wandb: Currently logged in as: harsh21122. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.3
wandb: Run data is saved locally in /ssd-scratch/harsh21122/ContextPromptPart/wandb/run-20230206_004030-37o4fjdw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run beaming-tiger-27
wandb: ‚≠êÔ∏è View project at https://wandb.ai/harsh21122/part_segmentation
wandb: üöÄ View run at https://wandb.ai/harsh21122/part_segmentation/runs/37o4fjdw
/home/harsh21122/.conda/envs/part/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
/home/harsh21122/.conda/envs/part/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Arguments are :  Namespace(batch_size=10, epochs=300, starting_epoch=1, base_lr=0.03, clip_model='RN50', result_dir='./Results/', model_dir='../ContextPromptPart_model', dataset_dir='/home/harsh21122/tmp/cat_dataset', resume=False, model_name='../ContextPromptPart_model/last_model', calc_accuracy_training=False, multi_step_scheduler=True, lr_decay=0.5, milestones=[50, 100, 150, 200, 250, 300], wandb=True, temperature=0.19, layer_len=-1, ref_layer1='relu3_2', ref_layer2='relu5_4', ref_weight1=0.33, ref_weight2=1.0, lamda_contrastive=50.0, lamda_cross=1.0, weight_decay=0.01)
Using device :  cuda
Length of Train dataset : 646 and Test dataset : 35
Length of Train loader : 65 and Test loader : 4
self.partnames :  ['background', 'head', 'neck', 'torso', 'tail', 'legs']
prompts :  [' the background of the cat.', ' the head of the cat.', ' the neck of the cat.', ' the torso of the cat.', ' the tail of the cat.', ' the legs of the cat.']
model device :  cuda
self.prompts : torch.Size([6, 77])
self.prompts : torch.Size([6, 1024])
self.prompts rquires grad :  <built-in method requires_grad_ of Tensor object at 0x7fdd0e3bf450>
layer: relu3_2,relu5_4
weighs: [0.33, 1.0]
Printing parameters and their gradient
gamma True
image_encoder.0.weight False
image_encoder.1.weight False
image_encoder.1.bias False
image_encoder.3.weight False
image_encoder.4.weight False
image_encoder.4.bias False
image_encoder.6.weight False
image_encoder.7.weight False
image_encoder.7.bias False
image_encoder.10.0.conv1.weight False
image_encoder.10.0.bn1.weight False
image_encoder.10.0.bn1.bias False
image_encoder.10.0.conv2.weight False
image_encoder.10.0.bn2.weight False
image_encoder.10.0.bn2.bias False
image_encoder.10.0.conv3.weight False
image_encoder.10.0.bn3.weight False
image_encoder.10.0.bn3.bias False
image_encoder.10.0.downsample.0.weight False
image_encoder.10.0.downsample.1.weight False
image_encoder.10.0.downsample.1.bias False
image_encoder.10.1.conv1.weight False
image_encoder.10.1.bn1.weight False
image_encoder.10.1.bn1.bias False
image_encoder.10.1.conv2.weight False
image_encoder.10.1.bn2.weight False
image_encoder.10.1.bn2.bias False
image_encoder.10.1.conv3.weight False
image_encoder.10.1.bn3.weight False
image_encoder.10.1.bn3.bias False
image_encoder.10.2.conv1.weight False
image_encoder.10.2.bn1.weight False
image_encoder.10.2.bn1.bias False
image_encoder.10.2.conv2.weight False
image_encoder.10.2.bn2.weight False
image_encoder.10.2.bn2.bias False
image_encoder.10.2.conv3.weight False
image_encoder.10.2.bn3.weight False
image_encoder.10.2.bn3.bias False
image_encoder.11.0.conv1.weight False
image_encoder.11.0.bn1.weight False
image_encoder.11.0.bn1.bias False
image_encoder.11.0.conv2.weight False
image_encoder.11.0.bn2.weight False
image_encoder.11.0.bn2.bias False
image_encoder.11.0.conv3.weight False
image_encoder.11.0.bn3.weight False
image_encoder.11.0.bn3.bias False
image_encoder.11.0.downsample.0.weight False
image_encoder.11.0.downsample.1.weight False
image_encoder.11.0.downsample.1.bias False
image_encoder.11.1.conv1.weight False
image_encoder.11.1.bn1.weight False
image_encoder.11.1.bn1.bias False
image_encoder.11.1.conv2.weight False
image_encoder.11.1.bn2.weight False
image_encoder.11.1.bn2.bias False
image_encoder.11.1.conv3.weight False
image_encoder.11.1.bn3.weight False
image_encoder.11.1.bn3.bias False
image_encoder.11.2.conv1.weight False
image_encoder.11.2.bn1.weight False
image_encoder.11.2.bn1.bias False
image_encoder.11.2.conv2.weight False
image_encoder.11.2.bn2.weight False
image_encoder.11.2.bn2.bias False
image_encoder.11.2.conv3.weight False
image_encoder.11.2.bn3.weight False
image_encoder.11.2.bn3.bias False
image_encoder.11.3.conv1.weight False
image_encoder.11.3.bn1.weight False
image_encoder.11.3.bn1.bias False
image_encoder.11.3.conv2.weight False
image_encoder.11.3.bn2.weight False
image_encoder.11.3.bn2.bias False
image_encoder.11.3.conv3.weight False
image_encoder.11.3.bn3.weight False
image_encoder.11.3.bn3.bias False
image_encoder.12.0.conv1.weight False
image_encoder.12.0.bn1.weight False
image_encoder.12.0.bn1.bias False
image_encoder.12.0.conv2.weight False
image_encoder.12.0.bn2.weight False
image_encoder.12.0.bn2.bias False
image_encoder.12.0.conv3.weight False
image_encoder.12.0.bn3.weight False
image_encoder.12.0.bn3.bias False
image_encoder.12.0.downsample.0.weight False
image_encoder.12.0.downsample.1.weight False
image_encoder.12.0.downsample.1.bias False
image_encoder.12.1.conv1.weight False
image_encoder.12.1.bn1.weight False
image_encoder.12.1.bn1.bias False
image_encoder.12.1.conv2.weight False
image_encoder.12.1.bn2.weight False
image_encoder.12.1.bn2.bias False
image_encoder.12.1.conv3.weight False
image_encoder.12.1.bn3.weight False
image_encoder.12.1.bn3.bias False
image_encoder.12.2.conv1.weight False
image_encoder.12.2.bn1.weight False
image_encoder.12.2.bn1.bias False
image_encoder.12.2.conv2.weight False
image_encoder.12.2.bn2.weight False
image_encoder.12.2.bn2.bias False
image_encoder.12.2.conv3.weight False
image_encoder.12.2.bn3.weight False
image_encoder.12.2.bn3.bias False
image_encoder.12.3.conv1.weight False
image_encoder.12.3.bn1.weight False
image_encoder.12.3.bn1.bias False
image_encoder.12.3.conv2.weight False
image_encoder.12.3.bn2.weight False
image_encoder.12.3.bn2.bias False
image_encoder.12.3.conv3.weight False
image_encoder.12.3.bn3.weight False
image_encoder.12.3.bn3.bias False
image_encoder.12.4.conv1.weight False
image_encoder.12.4.bn1.weight False
image_encoder.12.4.bn1.bias False
image_encoder.12.4.conv2.weight False
image_encoder.12.4.bn2.weight False
image_encoder.12.4.bn2.bias False
image_encoder.12.4.conv3.weight False
image_encoder.12.4.bn3.weight False
image_encoder.12.4.bn3.bias False
image_encoder.12.5.conv1.weight False
image_encoder.12.5.bn1.weight False
image_encoder.12.5.bn1.bias False
image_encoder.12.5.conv2.weight False
image_encoder.12.5.bn2.weight False
image_encoder.12.5.bn2.bias False
image_encoder.12.5.conv3.weight False
image_encoder.12.5.bn3.weight False
image_encoder.12.5.bn3.bias False
image_encoder.13.0.conv1.weight False
image_encoder.13.0.bn1.weight False
image_encoder.13.0.bn1.bias False
image_encoder.13.0.conv2.weight False
image_encoder.13.0.bn2.weight False
image_encoder.13.0.bn2.bias False
image_encoder.13.0.conv3.weight False
image_encoder.13.0.bn3.weight False
image_encoder.13.0.bn3.bias False
image_encoder.13.0.downsample.0.weight False
image_encoder.13.0.downsample.1.weight False
image_encoder.13.0.downsample.1.bias False
image_encoder.13.1.conv1.weight False
image_encoder.13.1.bn1.weight False
image_encoder.13.1.bn1.bias False
image_encoder.13.1.conv2.weight False
image_encoder.13.1.bn2.weight False
image_encoder.13.1.bn2.bias False
image_encoder.13.1.conv3.weight False
image_encoder.13.1.bn3.weight False
image_encoder.13.1.bn3.bias False
image_encoder.13.2.conv1.weight False
image_encoder.13.2.bn1.weight False
image_encoder.13.2.bn1.bias False
image_encoder.13.2.conv2.weight False
image_encoder.13.2.bn2.weight False
image_encoder.13.2.bn2.bias False
image_encoder.13.2.conv3.weight False
image_encoder.13.2.bn3.weight False
image_encoder.13.2.bn3.bias False
attnpool.positional_embedding True
attnpool.k_proj.weight True
attnpool.k_proj.bias True
attnpool.q_proj.weight True
attnpool.q_proj.bias True
attnpool.v_proj.weight True
attnpool.v_proj.bias True
attnpool.c_proj.weight True
attnpool.c_proj.bias True
align_context.memory_proj.0.weight True
align_context.memory_proj.0.bias True
align_context.memory_proj.1.weight True
align_context.memory_proj.1.bias True
align_context.memory_proj.2.weight True
align_context.memory_proj.2.bias True
align_context.text_proj.0.weight True
align_context.text_proj.0.bias True
align_context.text_proj.1.weight True
align_context.text_proj.1.bias True
align_context.decoder.0.self_attn.q_proj.weight True
align_context.decoder.0.self_attn.k_proj.weight True
align_context.decoder.0.self_attn.v_proj.weight True
align_context.decoder.0.self_attn.proj.weight True
align_context.decoder.0.self_attn.proj.bias True
align_context.decoder.0.cross_attn.q_proj.weight True
align_context.decoder.0.cross_attn.k_proj.weight True
align_context.decoder.0.cross_attn.v_proj.weight True
align_context.decoder.0.cross_attn.proj.weight True
align_context.decoder.0.cross_attn.proj.bias True
align_context.decoder.0.norm1.weight True
align_context.decoder.0.norm1.bias True
align_context.decoder.0.norm2.weight True
align_context.decoder.0.norm2.bias True
align_context.decoder.0.norm3.weight True
align_context.decoder.0.norm3.bias True
align_context.decoder.0.mlp.0.weight True
align_context.decoder.0.mlp.0.bias True
align_context.decoder.0.mlp.3.weight True
align_context.decoder.0.mlp.3.bias True
align_context.decoder.1.self_attn.q_proj.weight True
align_context.decoder.1.self_attn.k_proj.weight True
align_context.decoder.1.self_attn.v_proj.weight True
align_context.decoder.1.self_attn.proj.weight True
align_context.decoder.1.self_attn.proj.bias True
align_context.decoder.1.cross_attn.q_proj.weight True
align_context.decoder.1.cross_attn.k_proj.weight True
align_context.decoder.1.cross_attn.v_proj.weight True
align_context.decoder.1.cross_attn.proj.weight True
align_context.decoder.1.cross_attn.proj.bias True
align_context.decoder.1.norm1.weight True
align_context.decoder.1.norm1.bias True
align_context.decoder.1.norm2.weight True
align_context.decoder.1.norm2.bias True
align_context.decoder.1.norm3.weight True
align_context.decoder.1.norm3.bias True
align_context.decoder.1.mlp.0.weight True
align_context.decoder.1.mlp.0.bias True
align_context.decoder.1.mlp.3.weight True
align_context.decoder.1.mlp.3.bias True
align_context.decoder.2.self_attn.q_proj.weight True
align_context.decoder.2.self_attn.k_proj.weight True
align_context.decoder.2.self_attn.v_proj.weight True
align_context.decoder.2.self_attn.proj.weight True
align_context.decoder.2.self_attn.proj.bias True
align_context.decoder.2.cross_attn.q_proj.weight True
align_context.decoder.2.cross_attn.k_proj.weight True
align_context.decoder.2.cross_attn.v_proj.weight True
align_context.decoder.2.cross_attn.proj.weight True
align_context.decoder.2.cross_attn.proj.bias True
align_context.decoder.2.norm1.weight True
align_context.decoder.2.norm1.bias True
align_context.decoder.2.norm2.weight True
align_context.decoder.2.norm2.bias True
align_context.decoder.2.norm3.weight True
align_context.decoder.2.norm3.bias True
align_context.decoder.2.mlp.0.weight True
align_context.decoder.2.mlp.0.bias True
align_context.decoder.2.mlp.3.weight True
align_context.decoder.2.mlp.3.bias True
align_context.decoder.3.self_attn.q_proj.weight True
align_context.decoder.3.self_attn.k_proj.weight True
align_context.decoder.3.self_attn.v_proj.weight True
align_context.decoder.3.self_attn.proj.weight True
align_context.decoder.3.self_attn.proj.bias True
align_context.decoder.3.cross_attn.q_proj.weight True
align_context.decoder.3.cross_attn.k_proj.weight True
align_context.decoder.3.cross_attn.v_proj.weight True
align_context.decoder.3.cross_attn.proj.weight True
align_context.decoder.3.cross_attn.proj.bias True
align_context.decoder.3.norm1.weight True
align_context.decoder.3.norm1.bias True
align_context.decoder.3.norm2.weight True
align_context.decoder.3.norm2.bias True
align_context.decoder.3.norm3.weight True
align_context.decoder.3.norm3.bias True
align_context.decoder.3.mlp.0.weight True
align_context.decoder.3.mlp.0.bias True
align_context.decoder.3.mlp.3.weight True
align_context.decoder.3.mlp.3.bias True
align_context.decoder.4.self_attn.q_proj.weight True
align_context.decoder.4.self_attn.k_proj.weight True
align_context.decoder.4.self_attn.v_proj.weight True
align_context.decoder.4.self_attn.proj.weight True
align_context.decoder.4.self_attn.proj.bias True
align_context.decoder.4.cross_attn.q_proj.weight True
align_context.decoder.4.cross_attn.k_proj.weight True
align_context.decoder.4.cross_attn.v_proj.weight True
align_context.decoder.4.cross_attn.proj.weight True
align_context.decoder.4.cross_attn.proj.bias True
align_context.decoder.4.norm1.weight True
align_context.decoder.4.norm1.bias True
align_context.decoder.4.norm2.weight True
align_context.decoder.4.norm2.bias True
align_context.decoder.4.norm3.weight True
align_context.decoder.4.norm3.bias True
align_context.decoder.4.mlp.0.weight True
align_context.decoder.4.mlp.0.bias True
align_context.decoder.4.mlp.3.weight True
align_context.decoder.4.mlp.3.bias True
align_context.decoder.5.self_attn.q_proj.weight True
align_context.decoder.5.self_attn.k_proj.weight True
align_context.decoder.5.self_attn.v_proj.weight True
align_context.decoder.5.self_attn.proj.weight True
align_context.decoder.5.self_attn.proj.bias True
align_context.decoder.5.cross_attn.q_proj.weight True
align_context.decoder.5.cross_attn.k_proj.weight True
align_context.decoder.5.cross_attn.v_proj.weight True
align_context.decoder.5.cross_attn.proj.weight True
align_context.decoder.5.cross_attn.proj.bias True
align_context.decoder.5.norm1.weight True
align_context.decoder.5.norm1.bias True
align_context.decoder.5.norm2.weight True
align_context.decoder.5.norm2.bias True
align_context.decoder.5.norm3.weight True
align_context.decoder.5.norm3.bias True
align_context.decoder.5.mlp.0.weight True
align_context.decoder.5.mlp.0.bias True
align_context.decoder.5.mlp.3.weight True
align_context.decoder.5.mlp.3.bias True
align_context.out_proj.0.weight True
align_context.out_proj.0.bias True
align_context.out_proj.1.weight True
align_context.out_proj.1.bias True
decoder.conv_layers.0.weight True
decoder.conv_layers.0.bias True
decoder.conv_layers.2.weight True
decoder.conv_layers.2.bias True
decoder.conv_layers.3.weight True
decoder.conv_layers.3.bias True
decoder.conv_layers.5.weight True
decoder.conv_layers.5.bias True
Total epochs to be executed :  300
EPOCH 1:
Loss :  2.014411449432373 4.318336486816406 217.9312286376953
Loss :  2.0927324295043945 4.91580867767334 247.88316345214844
Loss :  2.162729024887085 4.701120376586914 237.21875
Loss :  2.016035556793213 5.1530537605285645 259.668701171875
Loss :  1.9963628053665161 4.527472972869873 228.37001037597656
Loss :  2.2193055152893066 4.494320869445801 226.9353485107422
Loss :  2.1987366676330566 4.9669060707092285 250.54403686523438
Loss :  2.1060216426849365 4.327931880950928 218.5026092529297
Loss :  2.117387533187866 4.719497203826904 238.0922393798828
Loss :  2.1131815910339355 4.421999454498291 223.21315002441406
Loss :  2.110229015350342 4.537476062774658 228.98403930664062
Loss :  2.0500521659851074 4.413298606872559 222.71498107910156
Loss :  2.0592100620269775 4.423568248748779 223.23760986328125
Loss :  2.0075576305389404 4.524599075317383 228.2375030517578
Loss :  1.9860930442810059 4.478728294372559 225.92251586914062
Loss :  2.0566625595092773 4.527162551879883 228.41477966308594
Loss :  1.9123731851577759 4.469003200531006 225.36253356933594
Loss :  1.9900764226913452 4.468539714813232 225.4170684814453
Loss :  1.9986263513565063 4.351846694946289 219.59095764160156
Loss :  1.9592641592025757 4.409804344177246 222.44947814941406
  batch 20 loss: 1.9592641592025757, 4.409804344177246, 222.44947814941406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8832920789718628 4.495343208312988 226.65045166015625
Loss :  2.0393624305725098 4.519270420074463 228.0028839111328
Loss :  1.9614850282669067 4.6653218269348145 235.2275848388672
Loss :  2.1006693840026855 4.632843971252441 233.74285888671875
Loss :  2.071030616760254 4.630725383758545 233.6072998046875
Loss :  2.02437424659729 5.230931758880615 263.57098388671875
Loss :  1.9910343885421753 4.853784084320068 244.68023681640625
Loss :  2.101050615310669 4.890835285186768 246.64280700683594
Loss :  1.9935203790664673 4.466479778289795 225.3175048828125
Loss :  1.9312427043914795 4.4536356925964355 224.61302185058594
Loss :  2.019012689590454 4.521580696105957 228.09805297851562
Loss :  1.922211766242981 4.676831245422363 235.76377868652344
Loss :  1.925683617591858 4.3638997077941895 220.1206817626953
Loss :  1.9949724674224854 4.4419074058532715 224.09034729003906
Loss :  1.8986433744430542 4.541462421417236 228.9717559814453
Loss :  1.9574843645095825 4.510000228881836 227.45750427246094
Loss :  1.9205411672592163 4.36085319519043 219.9632110595703
Loss :  2.0004093647003174 4.396987438201904 221.8497772216797
Loss :  1.909775972366333 4.7502312660217285 239.4213409423828
Loss :  1.9584803581237793 4.3339104652404785 218.6540069580078
  batch 40 loss: 1.9584803581237793, 4.3339104652404785, 218.6540069580078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.8590623140335083 4.609044075012207 232.311279296875
Loss :  1.8639777898788452 4.362857341766357 220.00685119628906
Loss :  1.8521901369094849 4.410616397857666 222.3830108642578
Loss :  1.8444023132324219 4.685236930847168 236.1062469482422
Loss :  1.8848762512207031 4.344162940979004 219.093017578125
Loss :  1.8206647634506226 4.438295841217041 223.73545837402344
Loss :  1.9423716068267822 4.580739498138428 230.97933959960938
Loss :  1.8125630617141724 4.558570861816406 229.74110412597656
Loss :  1.895913004875183 4.480847358703613 225.93829345703125
Loss :  1.8353468179702759 4.508211612701416 227.2459259033203
Loss :  1.9021090269088745 4.73934268951416 238.86924743652344
Loss :  1.8975603580474854 4.521096229553223 227.95237731933594
Loss :  1.8849432468414307 4.233389854431152 213.554443359375
Loss :  1.8287615776062012 4.396240234375 221.64077758789062
Loss :  1.766994595527649 4.420173645019531 222.7756805419922
Loss :  1.8720934391021729 4.490120887756348 226.37814331054688
Loss :  1.833822250366211 4.671769618988037 235.42230224609375
Loss :  1.7763216495513916 4.48659610748291 226.1061248779297
Loss :  1.7992645502090454 4.778221607208252 240.71035766601562
Loss :  1.9149730205535889 4.480837821960449 225.9568634033203
  batch 60 loss: 1.9149730205535889, 4.480837821960449, 225.9568634033203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.7816013097763062 4.557722091674805 229.66770935058594
Loss :  1.7457255125045776 4.486474514007568 226.0694580078125
Loss :  1.7312238216400146 4.346740245819092 219.06822204589844
Loss :  1.7199465036392212 4.61052131652832 232.24600219726562
Loss :  1.7199082374572754 4.149438858032227 209.1918487548828
Loss :  1.5385338068008423 4.418368339538574 222.45693969726562
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5597022771835327 4.438263416290283 223.47288513183594
Loss :  1.5374482870101929 4.264749526977539 214.77491760253906
Loss :  1.6896164417266846 4.241121768951416 213.74571228027344
Total LOSS train 229.05056739220254 valid 218.61261367797852
CE LOSS train 1.9470452767152053 valid 0.42240411043167114
Contrastive LOSS train 4.542070440145639 valid 1.060280442237854
Saved best model. Old loss 1000000.0 and new best loss 218.61261367797852
EPOCH 2:
Loss :  1.869659662246704 4.434593200683594 223.5993194580078
Loss :  1.8349342346191406 4.705601215362549 237.114990234375
Loss :  1.7739933729171753 4.27078104019165 215.3130340576172
Loss :  1.8111382722854614 4.4882731437683105 226.22479248046875
Loss :  1.811706781387329 4.165681838989258 210.09579467773438
Loss :  1.7039538621902466 4.2532429695129395 214.36610412597656
Loss :  1.7296113967895508 4.443613529205322 223.91029357910156
Loss :  1.6739124059677124 4.322188854217529 217.78335571289062
Loss :  1.6760908365249634 4.362496852874756 219.80093383789062
Loss :  1.748905897140503 4.3316779136657715 218.33279418945312
Loss :  1.7183150053024292 4.565609455108643 229.998779296875
Loss :  1.677758812904358 4.537777900695801 228.56666564941406
Loss :  1.7067898511886597 4.438776969909668 223.6456298828125
Loss :  1.7147496938705444 4.543341159820557 228.88180541992188
Loss :  1.7049212455749512 4.561223983764648 229.76612854003906
Loss :  1.7708513736724854 4.434238910675049 223.48280334472656
Loss :  1.6724917888641357 4.3812761306762695 220.73629760742188
Loss :  1.7048618793487549 4.323633670806885 217.8865509033203
Loss :  1.6273554563522339 4.563352584838867 229.79498291015625
Loss :  1.7292413711547852 4.400335311889648 221.7460174560547
  batch 20 loss: 1.7292413711547852, 4.400335311889648, 221.7460174560547
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6597872972488403 4.388848304748535 221.10220336914062
Loss :  1.6008604764938354 4.502371788024902 226.71945190429688
Loss :  1.6518367528915405 4.409998416900635 222.15176391601562
Loss :  1.7113834619522095 4.49713134765625 226.5679473876953
Loss :  1.772535800933838 4.618557453155518 232.70040893554688
Loss :  1.6743676662445068 4.40081787109375 221.7152557373047
Loss :  1.6883028745651245 4.687159061431885 236.0462646484375
Loss :  1.634369134902954 4.382192134857178 220.7439727783203
Loss :  1.5593467950820923 4.441644191741943 223.6415557861328
Loss :  1.664582371711731 4.427672863006592 223.0482177734375
Loss :  1.5154666900634766 4.520174980163574 227.5242156982422
Loss :  1.6232930421829224 4.700037002563477 236.62513732910156
Loss :  1.6203150749206543 4.442507743835449 223.74569702148438
Loss :  1.612300157546997 4.432407855987549 223.23269653320312
Loss :  1.4992444515228271 4.483017921447754 225.65013122558594
Loss :  1.524706482887268 4.514937877655029 227.27159118652344
Loss :  1.5493043661117554 4.419368267059326 222.51771545410156
Loss :  1.6467938423156738 4.286630630493164 215.9783172607422
Loss :  1.7516528367996216 4.404167175292969 221.9600067138672
Loss :  1.7186774015426636 4.3907341957092285 221.25538635253906
  batch 40 loss: 1.7186774015426636, 4.3907341957092285, 221.25538635253906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6519107818603516 4.567895889282227 230.0467071533203
Loss :  1.606766700744629 4.511321067810059 227.17282104492188
Loss :  1.5420010089874268 4.3577399253845215 219.4290008544922
Loss :  1.5691962242126465 4.481791973114014 225.65879821777344
Loss :  1.5515016317367554 4.43926477432251 223.51473999023438
Loss :  1.6173985004425049 4.529723167419434 228.1035614013672
Loss :  1.7144564390182495 4.523068428039551 227.86788940429688
Loss :  1.5660189390182495 4.519785404205322 227.5552978515625
Loss :  1.7024095058441162 4.433071613311768 223.35598754882812
Loss :  1.5879285335540771 4.212146759033203 212.19525146484375
Loss :  1.6439573764801025 4.576064109802246 230.44715881347656
Loss :  1.6956666707992554 4.494080066680908 226.39967346191406
Loss :  1.6101181507110596 4.404805660247803 221.85040283203125
Loss :  1.6770527362823486 4.482393264770508 225.7967071533203
Loss :  1.5840078592300415 4.547055721282959 228.93679809570312
Loss :  1.8092466592788696 4.493346214294434 226.4765625
Loss :  1.6777546405792236 4.4457807540893555 223.96678161621094
Loss :  1.5928223133087158 4.521340847015381 227.6598663330078
Loss :  1.6301199197769165 4.495326519012451 226.39645385742188
Loss :  1.7820671796798706 4.470109939575195 225.28756713867188
  batch 60 loss: 1.7820671796798706, 4.470109939575195, 225.28756713867188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
Loss :  1.6046619415283203 4.481149196624756 225.66212463378906
Loss :  1.5922096967697144 4.391218662261963 221.15313720703125
Loss :  1.594557523727417 4.438316822052002 223.51040649414062
Loss :  1.6202280521392822 4.468863487243652 225.0634002685547
Loss :  1.6449954509735107 4.141502857208252 208.7201385498047
Loss :  3.0446386337280273 4.469017505645752 226.49551391601562
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  3.00510311126709 4.441831111907959 225.09664916992188
Loss :  2.7470240592956543 4.503739356994629 227.93399047851562
Loss :  3.0284223556518555 4.2544732093811035 215.75209045410156
Total LOSS train 224.20726529634916 valid 223.81956100463867
CE LOSS train 1.6647603786908662 valid 0.7571055889129639
Contrastive LOSS train 4.450850105285644 valid 1.0636183023452759
EPOCH 3:
Loss :  1.7257136106491089 4.312224864959717 217.33694458007812
Loss :  1.712143063545227 4.481307506561279 225.7775115966797
Loss :  1.755459189414978 4.5328779220581055 228.39935302734375
Loss :  1.7199599742889404 4.469302654266357 225.18508911132812
Loss :  1.72467041015625 4.451025009155273 224.2759246826172
Loss :  1.686832070350647 4.29196834564209 216.28524780273438
Loss :  1.7347111701965332 4.467513084411621 225.11036682128906
Loss :  1.669115424156189 4.523604869842529 227.84934997558594
Loss :  1.6560691595077515 4.493353366851807 226.32373046875
Loss :  1.7154936790466309 4.357458114624023 219.58840942382812
Loss :  1.6759971380233765 4.590172290802002 231.1846160888672
Loss :  1.530981183052063 4.4114251136779785 222.10223388671875
Loss :  1.7043018341064453 4.427590370178223 223.0838165283203
Loss :  1.5503995418548584 4.5923614501953125 231.16847229003906
Loss :  1.7221156358718872 4.377540111541748 220.59912109375
Loss :  1.8170511722564697 4.517290115356445 227.68154907226562
Loss :  1.6074540615081787 4.436534404754639 223.43417358398438
Loss :  1.6682548522949219 4.367790699005127 220.05780029296875
Loss :  1.6788547039031982 4.401925563812256 221.77513122558594
Loss :  1.691358208656311 4.499826431274414 226.68267822265625
  batch 20 loss: 1.691358208656311, 4.499826431274414, 226.68267822265625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6344034671783447 4.32183313369751 217.72605895996094
Loss :  1.608842372894287 4.458797931671143 224.54873657226562
Loss :  1.6741634607315063 4.580289840698242 230.68865966796875
Loss :  1.6851493120193481 4.366755962371826 220.02294921875
Loss :  1.82597815990448 4.614367485046387 232.54434204101562
Loss :  1.7398089170455933 4.639171123504639 233.6983642578125
Loss :  1.8253583908081055 4.58920955657959 231.2858428955078
Loss :  1.6792171001434326 4.267693996429443 215.0639190673828
Loss :  1.5707498788833618 4.462981700897217 224.71983337402344
Loss :  1.7870938777923584 4.554798603057861 229.5270233154297
Loss :  1.6130735874176025 4.529045581817627 228.0653533935547
Loss :  1.6499288082122803 4.86002779006958 244.6513214111328
Loss :  1.6305214166641235 4.393064975738525 221.2837677001953
Loss :  1.6128768920898438 4.614144802093506 232.32012939453125
Loss :  1.5691423416137695 4.615660667419434 232.3521728515625
Loss :  1.6393332481384277 4.346761226654053 218.97738647460938
Loss :  1.5952728986740112 4.474447727203369 225.31765747070312
Loss :  1.7744358777999878 4.4397783279418945 223.7633514404297
Loss :  1.7672302722930908 4.333378791809082 218.43617248535156
Loss :  1.8165547847747803 4.553286075592041 229.48086547851562
  batch 40 loss: 1.8165547847747803, 4.553286075592041, 229.48086547851562
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
Loss :  1.7306244373321533 4.506870746612549 227.07415771484375
Loss :  1.7061333656311035 4.384341239929199 220.92318725585938
Loss :  1.70530104637146 4.376291751861572 220.5198974609375
Loss :  1.7658171653747559 4.498763084411621 226.7039794921875
Loss :  1.7108408212661743 4.3917646408081055 221.299072265625
Loss :  1.8320751190185547 4.440629959106445 223.8635711669922
Loss :  1.8599975109100342 4.510019302368164 227.3609619140625
Loss :  1.7423163652420044 4.507378578186035 227.11123657226562
Loss :  1.8854224681854248 4.474133491516113 225.59210205078125
Loss :  1.7275047302246094 4.481649875640869 225.80999755859375
Loss :  1.7361303567886353 4.532321453094482 228.35220336914062
Loss :  1.8233349323272705 4.38944149017334 221.29541015625
Loss :  1.747023105621338 4.35766077041626 219.63006591796875
Loss :  1.798853874206543 4.3170857429504395 217.6531524658203
Loss :  1.6468470096588135 4.456850051879883 224.48934936523438
Loss :  1.9653942584991455 4.453557968139648 224.64329528808594
Loss :  1.7931015491485596 4.428578853607178 223.2220458984375
Loss :  1.8048288822174072 4.52577543258667 228.09359741210938
Loss :  1.748538613319397 4.50832986831665 227.16502380371094
Loss :  1.830716848373413 4.5119147300720215 227.42645263671875
  batch 60 loss: 1.830716848373413, 4.5119147300720215, 227.42645263671875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
Loss :  1.806177020072937 4.562271595001221 229.91976928710938
Loss :  1.7154452800750732 4.411620140075684 222.29644775390625
Loss :  1.7753289937973022 4.418086051940918 222.67962646484375
Loss :  1.7502391338348389 4.487879753112793 226.14422607421875
Loss :  1.6622767448425293 4.073168754577637 205.32070922851562
Loss :  2.2352592945098877 4.483935356140137 226.43202209472656
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
Loss :  2.4603846073150635 4.525435924530029 228.732177734375
Loss :  2.1920149326324463 4.330327987670898 218.7084197998047
Loss :  2.1585114002227783 4.151314735412598 209.7242431640625
Total LOSS train 224.8148456280048 valid 220.8992156982422
CE LOSS train 1.718712935080895 valid 0.5396278500556946
Contrastive LOSS train 4.46192266024076 valid 1.0378286838531494
EPOCH 4:
Loss :  1.7081331014633179 4.415091037750244 222.46267700195312
Loss :  1.7994145154953003 4.484999179840088 226.0493621826172
Loss :  1.8049139976501465 4.399530410766602 221.78143310546875
Loss :  1.8289573192596436 4.345641136169434 219.11102294921875
Loss :  1.7638680934906006 4.523952960968018 227.96151733398438
Loss :  1.8568943738937378 4.395325183868408 221.62315368652344
Loss :  1.8035749197006226 4.44020938873291 223.8140411376953
Loss :  1.8297252655029297 4.415734767913818 222.61647033691406
Loss :  1.7883161306381226 4.386906147003174 221.1336212158203
Loss :  1.8126022815704346 4.325673580169678 218.09628295898438
Loss :  1.812709927558899 4.527997016906738 228.2125701904297
Loss :  1.741368293762207 4.487015724182129 226.09214782714844
Loss :  1.7978599071502686 4.540946960449219 228.84521484375
Loss :  1.7073917388916016 4.561147212982178 229.76475524902344
Loss :  1.8515349626541138 4.449825763702393 224.3428192138672
Loss :  1.92446768283844 4.426462173461914 223.24757385253906
Loss :  1.8201799392700195 4.512354373931885 227.43789672851562
Loss :  1.853899598121643 4.348498344421387 219.27880859375
Loss :  1.8286582231521606 4.4881415367126465 226.23573303222656
Loss :  1.8683959245681763 4.417230606079102 222.72991943359375
  batch 20 loss: 1.8683959245681763, 4.417230606079102, 222.72991943359375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9159311056137085 4.362715721130371 220.0517120361328
Loss :  1.8843586444854736 4.577339172363281 230.75131225585938
Loss :  1.926316499710083 4.491767406463623 226.51467895507812
Loss :  1.8979103565216064 4.416837692260742 222.7397918701172
Loss :  1.95309317111969 4.570549011230469 230.4805450439453
Loss :  1.8946232795715332 4.371022701263428 220.4457550048828
Loss :  1.9070518016815186 4.63627815246582 233.72096252441406
Loss :  1.8486518859863281 4.580413818359375 230.8693389892578
Loss :  1.8691186904907227 4.198843002319336 211.811279296875
Loss :  1.8391207456588745 4.459481239318848 224.8131866455078
Loss :  1.832832932472229 4.496655464172363 226.66561889648438
Loss :  1.8989529609680176 4.677364826202393 235.76719665527344
Loss :  1.8946729898452759 4.466254234313965 225.20738220214844
Loss :  1.8547464609146118 4.375222206115723 220.61585998535156
Loss :  1.787907600402832 4.576799392700195 230.62786865234375
Loss :  1.8843270540237427 4.579968452453613 230.88275146484375
Loss :  1.8855868577957153 4.430233478546143 223.3972625732422
Loss :  1.9568032026290894 4.730206489562988 238.46713256835938
Loss :  1.935407042503357 4.543622970581055 229.11656188964844
Loss :  2.0091135501861572 4.397951126098633 221.9066619873047
  batch 40 loss: 2.0091135501861572, 4.397951126098633, 221.9066619873047
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  2.0294909477233887 4.4278564453125 223.4223175048828
Loss :  1.9432259798049927 4.530825138092041 228.48448181152344
Loss :  1.887859582901001 4.40089750289917 221.9327392578125
Loss :  1.945206642150879 4.445371627807617 224.2137908935547
Loss :  1.92735755443573 4.380151748657227 220.9349365234375
Loss :  1.9704842567443848 4.373720169067383 220.656494140625
Loss :  1.798302173614502 4.591166973114014 231.35665893554688
Loss :  1.8380614519119263 4.643906593322754 234.03338623046875
Loss :  1.8127552270889282 4.420254707336426 222.82550048828125
Loss :  1.8179510831832886 4.534801006317139 228.55799865722656
Loss :  1.8028028011322021 4.491492748260498 226.37742614746094
Loss :  1.8044233322143555 4.464847087860107 225.04678344726562
Loss :  1.7558155059814453 4.460609436035156 224.78628540039062
Loss :  1.8952239751815796 4.474249839782715 225.6077117919922
Loss :  1.8019630908966064 4.465013027191162 225.0526123046875
Loss :  1.961992859840393 4.484002113342285 226.16209411621094
Loss :  1.8638358116149902 4.410804748535156 222.40406799316406
Loss :  1.807617425918579 4.499812602996826 226.79824829101562
Loss :  1.7374227046966553 4.450682640075684 224.2715606689453
Loss :  1.8228920698165894 4.364974021911621 220.07159423828125
  batch 60 loss: 1.8228920698165894, 4.364974021911621, 220.07159423828125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.9876410961151123 4.6572394371032715 234.849609375
Loss :  1.810698390007019 4.394067764282227 221.5140838623047
Loss :  1.822191834449768 4.56196403503418 229.92039489746094
Loss :  1.9019708633422852 4.38433837890625 221.118896484375
Loss :  1.9385448694229126 4.133272647857666 208.6021728515625
Loss :  6.908263683319092 4.5746307373046875 235.63980102539062
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  7.089354991912842 4.39335298538208 226.7570037841797
Loss :  7.740772247314453 4.518216133117676 233.65158081054688
Loss :  8.891341209411621 4.465612888336182 232.17198181152344
Total LOSS train 225.14910348745494 valid 232.05509185791016
CE LOSS train 1.8579249932215764 valid 2.2228353023529053
Contrastive LOSS train 4.4658235770005446 valid 1.1164032220840454
EPOCH 5:
Loss :  1.8155549764633179 4.300509452819824 216.8410186767578
Loss :  2.0274643898010254 4.752175331115723 239.63623046875
Loss :  1.822045922279358 4.673490047454834 235.49655151367188
Loss :  1.915123462677002 4.465123176574707 225.17129516601562
Loss :  1.7534044981002808 4.26763916015625 215.13536071777344
Loss :  1.871660828590393 4.557190418243408 229.73118591308594
Loss :  1.8887615203857422 4.557484149932861 229.76296997070312
Loss :  1.978825330734253 4.345639705657959 219.26080322265625
Loss :  1.8994519710540771 4.419015407562256 222.8502197265625
Loss :  1.9721519947052002 4.40503454208374 222.223876953125
Loss :  1.9149158000946045 4.4048542976379395 222.1576385498047
Loss :  1.9288920164108276 4.656626224517822 234.7602081298828
Loss :  1.8868157863616943 4.604105472564697 232.0920867919922
Loss :  1.8370893001556396 4.481524467468262 225.91329956054688
Loss :  1.883726716041565 4.5477519035339355 229.2713165283203
Loss :  1.9031137228012085 4.488049030303955 226.30555725097656
Loss :  1.8276947736740112 4.3884711265563965 221.25125122070312
Loss :  1.8650026321411133 4.429346561431885 223.33233642578125
Loss :  1.8700389862060547 4.3387227058410645 218.80618286132812
Loss :  1.8335388898849487 4.347578525543213 219.21246337890625
  batch 20 loss: 1.8335388898849487, 4.347578525543213, 219.21246337890625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.946738600730896 4.153990268707275 209.646240234375
Loss :  1.9233413934707642 4.358855247497559 219.86610412597656
Loss :  1.979303240776062 4.449777603149414 224.4681854248047
Loss :  1.8889070749282837 4.4310078620910645 223.43930053710938
Loss :  1.9016172885894775 4.447415828704834 224.27239990234375
Loss :  1.922257900238037 4.465272426605225 225.18588256835938
Loss :  1.8859260082244873 4.419438362121582 222.85784912109375
Loss :  1.802511215209961 4.165570259094238 210.08102416992188
Loss :  1.8624829053878784 4.373773574829102 220.55116271972656
Loss :  1.7717947959899902 4.266302585601807 215.0869140625
Loss :  1.8834033012390137 4.444694995880127 224.1181640625
Loss :  1.795283317565918 4.302215576171875 216.90606689453125
Loss :  1.812711477279663 4.224031448364258 213.0142822265625
Loss :  1.8013527393341064 4.443883895874023 223.99554443359375
Loss :  1.8467642068862915 4.426629066467285 223.17822265625
Loss :  1.805883526802063 4.2285990715026855 213.23582458496094
Loss :  1.8784234523773193 4.2750983238220215 215.63333129882812
Loss :  1.7830556631088257 4.261923789978027 214.87924194335938
Loss :  1.8425848484039307 4.242586135864258 213.97189331054688
Loss :  1.805997371673584 4.298887729644775 216.75038146972656
  batch 40 loss: 1.805997371673584, 4.298887729644775, 216.75038146972656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.817501187324524 4.205668926239014 212.1009521484375
Loss :  1.7854233980178833 4.315426826477051 217.55677795410156
Loss :  1.8449450731277466 4.1573805809021 209.71397399902344
Loss :  1.837915301322937 4.2568488121032715 214.68035888671875
Loss :  1.8533058166503906 4.108473300933838 207.27696228027344
Loss :  1.9347596168518066 4.367532253265381 220.31137084960938
Loss :  1.974382758140564 4.263091087341309 215.12893676757812
Loss :  1.9996005296707153 4.394834995269775 221.74134826660156
Loss :  2.009385824203491 4.309722900390625 217.4955291748047
Loss :  1.931242823600769 4.218783855438232 212.8704376220703
Loss :  1.8371264934539795 4.2859883308410645 216.1365509033203
Loss :  1.9100873470306396 4.508571147918701 227.33863830566406
Loss :  2.024071216583252 4.205108165740967 212.27947998046875
Loss :  1.9544281959533691 4.424203872680664 223.16461181640625
Loss :  1.984224557876587 4.2319111824035645 213.5797882080078
Loss :  2.0093324184417725 4.27923059463501 215.9708709716797
Loss :  2.084913492202759 4.474343299865723 225.8020782470703
Loss :  2.1569912433624268 4.555112361907959 229.91261291503906
Loss :  2.2207043170928955 4.545027256011963 229.47206115722656
Loss :  2.0529887676239014 4.429453372955322 223.52566528320312
  batch 60 loss: 2.0529887676239014, 4.429453372955322, 223.52566528320312
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  2.0862276554107666 4.346527099609375 219.41258239746094
Loss :  1.963207721710205 4.363698959350586 220.14816284179688
Loss :  2.179719924926758 4.432488441467285 223.80413818359375
Loss :  2.030034303665161 4.429562091827393 223.5081329345703
Loss :  2.2081804275512695 4.1191816329956055 208.16725158691406
Loss :  2.1620099544525146 4.411984920501709 222.76124572753906
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  2.106416702270508 4.435791969299316 223.89601135253906
Loss :  2.1563639640808105 4.425296306610107 223.42117309570312
Loss :  1.7995326519012451 4.198306560516357 211.71485900878906
Total LOSS train 220.63767911470853 valid 220.44832229614258
CE LOSS train 1.91578945013193 valid 0.4498831629753113
Contrastive LOSS train 4.374437801654523 valid 1.0495766401290894
EPOCH 6:
Loss :  2.1150269508361816 4.4661784172058105 225.42393493652344
Loss :  1.8856427669525146 4.523112773895264 228.04127502441406
Loss :  1.9501768350601196 4.333746433258057 218.6374969482422
Loss :  2.096428394317627 4.5371623039245605 228.9545440673828
Loss :  2.2249019145965576 4.2741546630859375 215.93263244628906
Loss :  1.928117036819458 4.3691887855529785 220.38755798339844
Loss :  1.8077263832092285 4.471211910247803 225.36831665039062
Loss :  2.084766149520874 4.270002841949463 215.58489990234375
Loss :  2.044931173324585 4.324735641479492 218.28172302246094
Loss :  1.9931756258010864 4.314418315887451 217.71409606933594
Loss :  2.0407636165618896 4.545530319213867 229.31727600097656
Loss :  1.9905586242675781 4.5664167404174805 230.31138610839844
Loss :  2.001925468444824 4.539352893829346 228.96957397460938
Loss :  2.171344518661499 4.473367691040039 225.8397216796875
Loss :  2.041325569152832 4.363614082336426 220.22203063964844
Loss :  1.904144287109375 4.450324535369873 224.4203643798828
Loss :  2.1677372455596924 4.371795177459717 220.7574920654297
Loss :  2.104496717453003 4.4327239990234375 223.74069213867188
Loss :  2.104034185409546 4.3258957862854 218.39881896972656
Loss :  1.981210708618164 4.545865058898926 229.27447509765625
  batch 20 loss: 1.981210708618164, 4.545865058898926, 229.27447509765625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 3, 4], device='cuda:0')
Loss :  2.037031888961792 4.403645038604736 222.2192840576172
Loss :  2.068023443222046 4.494685649871826 226.80230712890625
Loss :  1.9763081073760986 4.5255656242370605 228.25457763671875
Loss :  2.1542603969573975 4.430026531219482 223.6555938720703
Loss :  2.0297844409942627 4.622053623199463 233.13246154785156
Loss :  2.104592800140381 4.499109745025635 227.06008911132812
Loss :  1.8502719402313232 4.489771842956543 226.33885192871094
Loss :  2.0771594047546387 4.524237632751465 228.28904724121094
Loss :  2.1056697368621826 4.4138102531433105 222.79617309570312
Loss :  2.020616054534912 4.6514763832092285 234.5944366455078
Loss :  2.161754846572876 4.380086898803711 221.16610717773438
Loss :  2.0525567531585693 4.629554271697998 233.53025817871094
Loss :  2.0502684116363525 4.425177097320557 223.30911254882812
Loss :  2.024313449859619 4.528825283050537 228.465576171875
Loss :  2.0737719535827637 4.627096176147461 233.4285888671875
Loss :  2.0500168800354004 4.386838436126709 221.39193725585938
Loss :  2.0319290161132812 4.426373481750488 223.35061645507812
Loss :  1.999586582183838 4.456403732299805 224.8197784423828
Loss :  1.8661339282989502 4.46113395690918 224.92283630371094
Loss :  1.8937300443649292 4.601269245147705 231.95718383789062
  batch 40 loss: 1.8937300443649292, 4.601269245147705, 231.95718383789062
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  2.005593776702881 4.196691036224365 211.84014892578125
Loss :  1.9890148639678955 4.149838924407959 209.48095703125
Loss :  1.9985089302062988 4.303609848022461 217.1790008544922
Loss :  2.015831232070923 4.237058162689209 213.86874389648438
Loss :  2.0312488079071045 4.147259712219238 209.3942413330078
Loss :  1.9999439716339111 4.175401210784912 210.77000427246094
Loss :  1.9622728824615479 4.276208877563477 215.77272033691406
Loss :  2.023350238800049 4.107542037963867 207.40045166015625
Loss :  1.8746644258499146 4.137104511260986 208.72988891601562
Loss :  2.005648374557495 4.253712177276611 214.69125366210938
Loss :  1.9725481271743774 4.466991901397705 225.3221435546875
Loss :  2.012627363204956 4.33866024017334 218.94564819335938
Loss :  2.0636816024780273 4.314967155456543 217.81202697753906
Loss :  2.1563427448272705 4.4077467918396 222.54368591308594
Loss :  2.1492087841033936 4.325772285461426 218.43783569335938
Loss :  2.079751491546631 4.174376010894775 210.79855346679688
Loss :  2.0503017902374268 4.19677209854126 211.888916015625
Loss :  2.031036376953125 4.298715591430664 216.96681213378906
Loss :  2.0436594486236572 4.331509590148926 218.619140625
Loss :  1.9534757137298584 4.162268161773682 210.06687927246094
  batch 60 loss: 1.9534757137298584, 4.162268161773682, 210.06687927246094
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  2.016381025314331 4.167172431945801 210.37501525878906
Loss :  1.9197674989700317 4.181516647338867 210.99560546875
Loss :  2.0218112468719482 4.173609733581543 210.70228576660156
Loss :  2.0114567279815674 3.86321759223938 195.17233276367188
Loss :  2.0584847927093506 4.037563800811768 203.93667602539062
Loss :  1.96830153465271 4.083296298980713 206.13311767578125
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.9735605716705322 4.2516303062438965 214.55506896972656
Loss :  1.9296696186065674 4.042824745178223 204.07090759277344
Loss :  1.969700813293457 3.959543466567993 199.94686889648438
Total LOSS train 220.41190913273738 valid 206.1764907836914
CE LOSS train 2.0263511767754188 valid 0.49242520332336426
Contrastive LOSS train 4.367711166235117 valid 0.9898858666419983
Saved best model. Old loss 218.61261367797852 and new best loss 206.1764907836914
EPOCH 7:
Loss :  1.910007357597351 3.6428658962249756 184.0532989501953
Loss :  1.944053053855896 4.109661102294922 207.42710876464844
Loss :  1.9051626920700073 4.3012213706970215 216.9662322998047
Loss :  1.9610819816589355 4.50745964050293 227.3340606689453
Loss :  1.913127064704895 4.766346454620361 240.23045349121094
Loss :  2.035938024520874 4.397671222686768 221.91949462890625
Loss :  1.948460340499878 4.33847713470459 218.872314453125
Loss :  1.958897352218628 4.551144599914551 229.51612854003906
Loss :  1.9716845750808716 4.322180271148682 218.0806884765625
Loss :  1.8942773342132568 3.8351902961730957 193.65379333496094
Loss :  2.025700092315674 4.730554103851318 238.55340576171875
Loss :  2.099069595336914 4.053533554077148 204.7757568359375
Loss :  2.0750999450683594 4.090934753417969 206.62184143066406
Loss :  2.130019426345825 4.24908971786499 214.58450317382812
Loss :  2.0549371242523193 4.053431987762451 204.72653198242188
Loss :  2.1384432315826416 3.8277692794799805 193.5269012451172
Loss :  2.1312808990478516 4.243007183074951 214.28164672851562
Loss :  2.139669418334961 4.135676383972168 208.92347717285156
Loss :  2.0352840423583984 3.9453322887420654 199.30189514160156
Loss :  2.0817768573760986 3.916403293609619 197.9019317626953
  batch 20 loss: 2.0817768573760986, 3.916403293609619, 197.9019317626953
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  2.1010758876800537 3.9935050010681152 201.7763214111328
Loss :  2.1765034198760986 4.219204902648926 213.13674926757812
Loss :  2.078113317489624 4.600501537322998 232.10317993164062
Loss :  2.2207648754119873 4.6123738288879395 232.83946228027344
Loss :  2.152745246887207 4.561682224273682 230.23684692382812
Loss :  2.1892995834350586 4.456989765167236 225.03878784179688
Loss :  2.1464498043060303 4.68790864944458 236.54188537597656
Loss :  2.019862651824951 4.378007411956787 220.9202423095703
Loss :  2.134018659591675 4.601481914520264 232.20811462402344
Loss :  2.040424346923828 3.929964780807495 198.53866577148438
Loss :  2.0740764141082764 4.066473960876465 205.3977813720703
Loss :  2.0265369415283203 4.468019962310791 225.4275360107422
Loss :  2.1071221828460693 3.7109293937683105 187.65357971191406
Loss :  2.1194286346435547 4.237016677856445 213.9702606201172
Loss :  2.0678625106811523 4.225614547729492 213.3485870361328
Loss :  2.029332160949707 4.183175086975098 211.18807983398438
Loss :  2.0292000770568848 4.008939266204834 202.47616577148438
Loss :  2.0058999061584473 3.9058890342712402 197.30035400390625
Loss :  2.011054515838623 4.055567741394043 204.7894287109375
Loss :  2.0061521530151367 4.038293838500977 203.92083740234375
  batch 40 loss: 2.0061521530151367, 4.038293838500977, 203.92083740234375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3, 4], device='cuda:0')
Loss :  2.0222742557525635 4.1435465812683105 209.19960021972656
Loss :  2.03769850730896 3.971115827560425 200.59349060058594
Loss :  2.037517547607422 3.80147647857666 192.11134338378906
Loss :  2.0025532245635986 4.453686714172363 224.6868896484375
Loss :  2.050157308578491 4.133291721343994 208.71473693847656
Loss :  2.0459585189819336 4.450117588043213 224.5518341064453
Loss :  2.052708625793457 4.449371814727783 224.52130126953125
Loss :  2.037604808807373 4.122373580932617 208.15628051757812
Loss :  2.0238053798675537 4.094244956970215 206.73605346679688
Loss :  2.05114483833313 3.866551637649536 195.37872314453125
Loss :  2.01470947265625 4.195778846740723 211.80364990234375
Loss :  2.1102006435394287 3.702061653137207 187.21328735351562
Loss :  2.0729269981384277 3.9202606678009033 198.08595275878906
Loss :  2.1343705654144287 4.513678073883057 227.81826782226562
Loss :  2.1149046421051025 4.653436660766602 234.78672790527344
Loss :  2.0973331928253174 4.594698429107666 231.83226013183594
Loss :  2.115116834640503 4.485310077667236 226.380615234375
Loss :  2.1501896381378174 4.481224060058594 226.21139526367188
Loss :  2.1702194213867188 4.777682304382324 241.0543212890625
Loss :  2.1303815841674805 4.4868645668029785 226.47361755371094
  batch 60 loss: 2.1303815841674805, 4.4868645668029785, 226.47361755371094
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3, 5], device='cuda:0')
Loss :  2.2134571075439453 4.542435646057129 229.33523559570312
Loss :  2.1022133827209473 4.486936569213867 226.4490509033203
Loss :  2.1415293216705322 4.5331807136535645 228.80056762695312
Loss :  2.146907091140747 4.4212870597839355 223.2112579345703
Loss :  2.181994676589966 4.201681613922119 212.2660675048828
Loss :  6.250086784362793 4.52627420425415 232.5637969970703
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3, 5], device='cuda:0')
Loss :  5.878918647766113 4.577799320220947 234.76889038085938
Loss :  5.85524845123291 4.587684154510498 235.2394561767578
Loss :  7.863520622253418 4.42216157913208 228.9716033935547
Total LOSS train 214.71441321739783 valid 232.88593673706055
CE LOSS train 2.0668887890302217 valid 1.9658801555633545
Contrastive LOSS train 4.252950521615835 valid 1.10554039478302
EPOCH 8:
Loss :  2.1824915409088135 4.32803201675415 218.58409118652344
Loss :  2.096916913986206 4.221090793609619 213.15145874023438
Loss :  2.1448960304260254 3.8504796028137207 194.66888427734375
Loss :  2.1326959133148193 4.213380813598633 212.80172729492188
Loss :  2.112724781036377 4.087035655975342 206.46450805664062
Loss :  2.187997341156006 4.3706488609313965 220.72044372558594
Loss :  2.1056928634643555 4.193939685821533 211.8026885986328
Loss :  2.137190818786621 4.0954270362854 206.90853881835938
Loss :  2.1087393760681152 4.254146099090576 214.8160400390625
Loss :  2.0921807289123535 4.018176555633545 203.00100708007812
Loss :  2.1573410034179688 4.165755271911621 210.44509887695312
Loss :  2.1054983139038086 3.9894845485687256 201.57972717285156
Loss :  2.159388780593872 4.056317329406738 204.9752655029297
Loss :  2.15252423286438 4.48252010345459 226.27853393554688
Loss :  2.05411696434021 4.431023597717285 223.6053009033203
Loss :  2.126805067062378 4.533625602722168 228.80807495117188
Loss :  2.0666022300720215 4.289042949676514 216.5187530517578
Loss :  2.025440216064453 4.291975975036621 216.62423706054688
Loss :  2.061969757080078 4.346517562866211 219.38784790039062
Loss :  1.9800126552581787 4.452934741973877 224.6267547607422
  batch 20 loss: 1.9800126552581787, 4.452934741973877, 224.6267547607422
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  2.055166244506836 4.608001708984375 232.4552459716797
Loss :  1.9926283359527588 4.41679048538208 222.8321533203125
Loss :  2.052325963973999 4.50473690032959 227.28916931152344
Loss :  2.0187249183654785 4.3841705322265625 221.2272491455078
Loss :  2.077127456665039 4.663724899291992 235.2633819580078
Loss :  2.0887837409973145 4.3323655128479 218.70706176757812
Loss :  2.003706216812134 4.55009651184082 229.50852966308594
Loss :  2.0579118728637695 4.352256774902344 219.67074584960938
Loss :  2.0885355472564697 4.420600414276123 223.11854553222656
Loss :  2.1105144023895264 4.404970645904541 222.3590545654297
Loss :  2.0855495929718018 4.694918155670166 236.8314666748047
Loss :  1.8297394514083862 4.510637283325195 227.36160278320312
Loss :  2.0223352909088135 4.428451061248779 223.44488525390625
Loss :  2.068399667739868 4.6416544914245605 234.151123046875
Loss :  1.953964352607727 4.624680519104004 233.18798828125
Loss :  2.018272638320923 4.459611892700195 224.99887084960938
Loss :  1.957854151725769 4.450197219848633 224.46771240234375
Loss :  2.1047379970550537 4.249061107635498 214.5577850341797
Loss :  2.1484804153442383 4.212482929229736 212.7726287841797
Loss :  2.1069440841674805 4.748091697692871 239.51153564453125
  batch 40 loss: 2.1069440841674805, 4.748091697692871, 239.51153564453125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 5], device='cuda:0')
Loss :  2.009143829345703 4.437101364135742 223.8642120361328
Loss :  2.0604612827301025 4.401740550994873 222.14747619628906
Loss :  1.9360421895980835 4.446818828582764 224.2769775390625
Loss :  1.9784367084503174 4.513365745544434 227.646728515625
Loss :  1.9565774202346802 4.330130100250244 218.46307373046875
Loss :  2.063162326812744 4.362340450286865 220.18017578125
Loss :  2.1114895343780518 4.659881114959717 235.1055450439453
Loss :  2.0508177280426025 4.482693672180176 226.1855010986328
Loss :  2.1271066665649414 4.375657081604004 220.9099578857422
Loss :  1.9523426294326782 4.6439619064331055 234.1504364013672
Loss :  1.9471169710159302 4.505486488342285 227.221435546875
Loss :  2.0147929191589355 4.477141380310059 225.87185668945312
Loss :  2.0530290603637695 4.597022533416748 231.90414428710938
Loss :  2.127516031265259 4.344229698181152 219.33900451660156
Loss :  1.9171611070632935 4.503762245178223 227.1052703857422
Loss :  1.9725149869918823 4.5185065269470215 227.89784240722656
Loss :  2.0664756298065186 4.446073532104492 224.3701629638672
Loss :  1.9079985618591309 4.410000801086426 222.40805053710938
Loss :  2.0448575019836426 4.539824485778809 229.0360870361328
Loss :  2.1606478691101074 4.457253456115723 225.0233154296875
  batch 60 loss: 2.1606478691101074, 4.457253456115723, 225.0233154296875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 5], device='cuda:0')
Loss :  2.030430316925049 4.459039211273193 224.98239135742188
Loss :  1.8825600147247314 4.488318920135498 226.29849243164062
Loss :  2.005342721939087 4.488740921020508 226.4423828125
Loss :  2.0110929012298584 4.335559368133545 218.7890625
Loss :  2.0244133472442627 4.249898910522461 214.5193634033203
Loss :  1.6546285152435303 4.18828821182251 211.0690460205078
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.6854406595230103 4.408395290374756 222.10520935058594
Loss :  1.6700280904769897 4.36217737197876 219.77890014648438
Loss :  1.6845860481262207 3.8823421001434326 195.80169677734375
Total LOSS train 221.90191791240986 valid 212.18871307373047
CE LOSS train 2.052991663492643 valid 0.4211465120315552
Contrastive LOSS train 4.396978536018959 valid 0.9705855250358582
EPOCH 9:
Loss :  1.9386866092681885 4.500145435333252 226.9459686279297
Loss :  2.032508373260498 4.675793170928955 235.82215881347656
Loss :  2.0926668643951416 4.5938897132873535 231.7871551513672
Loss :  2.027836799621582 4.676178932189941 235.83677673339844
Loss :  2.1124043464660645 4.35181999206543 219.7034149169922
Loss :  1.9678122997283936 4.456453800201416 224.79051208496094
Loss :  2.0010595321655273 4.553153991699219 229.65875244140625
Loss :  1.9717631340026855 4.286232948303223 216.2834014892578
Loss :  1.9215565919876099 4.454657554626465 224.65443420410156
Loss :  1.935172200202942 4.7035932540893555 237.1148223876953
Loss :  2.0301311016082764 4.6199564933776855 233.0279541015625
Loss :  1.8872859477996826 4.668867588043213 235.33065795898438
Loss :  2.0619966983795166 4.523413181304932 228.23265075683594
Loss :  2.0309128761291504 4.433936595916748 223.7277374267578
Loss :  1.9630931615829468 4.148135185241699 209.36984252929688
Loss :  2.096680164337158 4.096493244171143 206.9213409423828
Loss :  2.054945945739746 4.2320027351379395 213.65509033203125
Loss :  2.1234099864959717 4.255502223968506 214.89852905273438
Loss :  1.9307624101638794 4.111212730407715 207.49139404296875
Loss :  2.040924072265625 4.144007205963135 209.2412872314453
  batch 20 loss: 2.040924072265625, 4.144007205963135, 209.2412872314453
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.9187105894088745 4.396104335784912 221.72393798828125
Loss :  2.0659656524658203 4.476034641265869 225.86769104003906
Loss :  1.9721674919128418 4.450567245483398 224.5005340576172
Loss :  1.9217197895050049 4.599419593811035 231.8927001953125
Loss :  1.936090111732483 4.635466575622559 233.7094268798828
Loss :  2.027503728866577 4.52354097366333 228.2045440673828
Loss :  2.0125672817230225 4.677479267120361 235.88653564453125
Loss :  1.984297275543213 4.256117343902588 214.7901611328125
Loss :  2.0302700996398926 3.9761345386505127 200.83700561523438
Loss :  1.9245831966400146 3.9727210998535156 200.5606231689453
Loss :  2.0225751399993896 4.197476387023926 211.89639282226562
Loss :  2.0378706455230713 4.6505889892578125 234.56732177734375
Loss :  2.032254457473755 4.5708699226379395 230.5757598876953
Loss :  2.10153865814209 4.50946569442749 227.57481384277344
Loss :  2.1372618675231934 4.632134437561035 233.74398803710938
Loss :  2.187298059463501 4.500818252563477 227.22821044921875
Loss :  2.1532793045043945 4.38623046875 221.4647979736328
Loss :  1.9395841360092163 4.05152702331543 204.5159454345703
Loss :  1.9327642917633057 4.033551216125488 203.61033630371094
Loss :  1.9344571828842163 4.095738410949707 206.7213897705078
  batch 40 loss: 1.9344571828842163, 4.095738410949707, 206.7213897705078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3, 4, 5], device='cuda:0')
Loss :  2.175130605697632 4.42764949798584 223.55760192871094
Loss :  2.0546047687530518 4.343286514282227 219.21893310546875
Loss :  2.106055736541748 4.369124412536621 220.56227111816406
Loss :  2.0581278800964355 4.491694450378418 226.64283752441406
Loss :  2.1095879077911377 4.268043518066406 215.5117645263672
Loss :  2.119729518890381 4.550782203674316 229.65884399414062
Loss :  2.029621124267578 4.552088260650635 229.634033203125
Loss :  2.116795539855957 4.451186180114746 224.6761016845703
Loss :  2.120938539505005 4.550313949584961 229.6366424560547
Loss :  2.0828335285186768 4.699827194213867 237.07420349121094
Loss :  2.068692445755005 4.5826520919799805 231.2012939453125
Loss :  2.1420698165893555 4.348931789398193 219.5886688232422
Loss :  2.1236109733581543 4.34904146194458 219.57568359375
Loss :  2.1419990062713623 4.507807731628418 227.53237915039062
Loss :  2.111354351043701 4.418059825897217 223.01434326171875
Loss :  1.936764121055603 4.352452754974365 219.5594024658203
Loss :  2.089887857437134 4.443498611450195 224.2648162841797
Loss :  2.14731764793396 4.452786445617676 224.78665161132812
Loss :  2.0594348907470703 4.599891185760498 232.05398559570312
Loss :  2.027209758758545 4.455575466156006 224.8059844970703
  batch 60 loss: 2.027209758758545, 4.455575466156006, 224.8059844970703
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  2.0954701900482178 4.603567600250244 232.2738494873047
Loss :  2.1214253902435303 4.400804042816162 222.16163635253906
Loss :  2.050966501235962 4.490074157714844 226.55467224121094
Loss :  2.127575635910034 4.3839802742004395 221.32659912109375
Loss :  2.096153974533081 4.0417962074279785 204.18597412109375
Loss :  1.4916625022888184 4.410268783569336 222.00511169433594
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1], device='cuda:0')
Loss :  1.5299912691116333 4.442468643188477 223.65342712402344
Loss :  1.533821940422058 4.349152565002441 218.991455078125
Loss :  1.3981736898422241 4.125298500061035 207.66310119628906
Total LOSS train 222.97577185997596 valid 218.07827377319336
CE LOSS train 2.043195781340966 valid 0.34954342246055603
Contrastive LOSS train 4.418651511118962 valid 1.0313246250152588
EPOCH 10:
Loss :  2.0953094959259033 4.240628719329834 214.12673950195312
Loss :  2.108792781829834 4.543068885803223 229.26223754882812
Loss :  2.1214911937713623 4.121705055236816 208.2067413330078
Loss :  2.1340839862823486 4.063161373138428 205.29214477539062
Loss :  2.124072790145874 4.2903828620910645 216.64321899414062
Loss :  2.0875661373138428 4.535545349121094 228.86483764648438
Loss :  2.0983362197875977 4.458583354949951 225.0275115966797
Loss :  2.1120755672454834 4.625182628631592 233.37120056152344
Loss :  2.1017069816589355 4.30063009262085 217.13320922851562
Loss :  2.1081721782684326 4.3550872802734375 219.86253356933594
Loss :  2.0701310634613037 4.598909378051758 232.01559448242188
Loss :  2.1000635623931885 4.575658321380615 230.8829803466797
Loss :  2.0471138954162598 4.509880542755127 227.54115295410156
Loss :  2.031891107559204 4.52885103225708 228.4744415283203
Loss :  2.1067450046539307 4.444214820861816 224.31748962402344
Loss :  2.023669958114624 4.454310894012451 224.7392120361328
Loss :  2.0554401874542236 4.419797420501709 223.04530334472656
Loss :  2.043545961380005 4.3467278480529785 219.37994384765625
Loss :  2.1038565635681152 4.389035701751709 221.55563354492188
Loss :  2.1080334186553955 4.439675331115723 224.091796875
  batch 20 loss: 2.1080334186553955, 4.439675331115723, 224.091796875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 4, 5], device='cuda:0')
Loss :  2.1179826259613037 4.429721355438232 223.6040496826172
Loss :  2.0615346431732178 4.6872429847717285 236.42369079589844
Loss :  2.056608200073242 4.5716047286987305 230.6368408203125
Loss :  2.076266288757324 4.482336044311523 226.1930694580078
Loss :  2.0037379264831543 4.576353549957275 230.8214111328125
Loss :  2.057514190673828 4.397800922393799 221.9475555419922
Loss :  1.9871677160263062 4.660134315490723 234.99388122558594
Loss :  2.0466649532318115 4.315181255340576 217.80572509765625
Loss :  2.0096423625946045 4.430493354797363 223.53431701660156
Loss :  2.0672922134399414 4.391941547393799 221.66436767578125
Loss :  2.0185251235961914 4.513184070587158 227.677734375
Loss :  2.084270715713501 4.559525012969971 230.06053161621094
Loss :  2.037761926651001 4.703566551208496 237.21609497070312
Loss :  2.0236809253692627 4.446572303771973 224.352294921875
Loss :  2.016850709915161 4.569843769073486 230.509033203125
Loss :  2.021573066711426 4.609589099884033 232.50103759765625
Loss :  2.0099878311157227 4.641298770904541 234.07493591308594
Loss :  2.099475145339966 4.535916805267334 228.8953094482422
Loss :  1.9760080575942993 4.4405975341796875 224.00588989257812
Loss :  1.9367529153823853 4.267888069152832 215.33116149902344
  batch 40 loss: 1.9367529153823853, 4.267888069152832, 215.33116149902344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  2.013658046722412 4.403306484222412 222.17898559570312
Loss :  1.9641056060791016 4.444793224334717 224.20376586914062
Loss :  2.0127017498016357 4.517700672149658 227.89773559570312
Loss :  2.076496124267578 4.54403018951416 229.2779998779297
Loss :  2.006373643875122 4.51962947845459 227.98785400390625
Loss :  1.9812864065170288 4.299746513366699 216.96861267089844
Loss :  1.9512889385223389 4.485376358032227 226.22010803222656
Loss :  2.0341897010803223 4.503747940063477 227.22158813476562
Loss :  2.0361814498901367 4.850244522094727 244.54840087890625
Loss :  1.9501492977142334 4.521969318389893 228.04861450195312
Loss :  2.110403537750244 4.5544843673706055 229.83460998535156
Loss :  1.9571598768234253 4.583427906036377 231.12855529785156
Loss :  1.8785583972930908 4.3888959884643555 221.32334899902344
Loss :  1.9487532377243042 4.367171287536621 220.30731201171875
Loss :  1.9917808771133423 4.443188190460205 224.15118408203125
Loss :  1.9813601970672607 4.409155368804932 222.43911743164062
Loss :  1.8671847581863403 4.490025997161865 226.3684844970703
Loss :  1.9410706758499146 4.535096645355225 228.69590759277344
Loss :  1.7808277606964111 4.586104393005371 231.08604431152344
Loss :  2.042818069458008 4.350364685058594 219.56105041503906
  batch 60 loss: 2.042818069458008, 4.350364685058594, 219.56105041503906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.9890544414520264 4.624331474304199 233.20562744140625
Loss :  2.001101016998291 4.447896480560303 224.3959197998047
Loss :  1.9096145629882812 4.437692642211914 223.79425048828125
Loss :  2.0106523036956787 4.455684185028076 224.79486083984375
Loss :  1.9211901426315308 4.304057598114014 217.1240692138672
Loss :  1.9756232500076294 4.440552711486816 224.0032501220703
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 4], device='cuda:0')
Loss :  2.01816987991333 4.340910911560059 219.063720703125
Loss :  2.0355401039123535 4.299893856048584 217.0302276611328
Loss :  1.8245646953582764 4.255560874938965 214.6026153564453
Total LOSS train 225.52072096604567 valid 218.67495346069336
CE LOSS train 2.0284516371213472 valid 0.4561411738395691
Contrastive LOSS train 4.469845397655781 valid 1.0638902187347412
EPOCH 11:
Loss :  1.925932765007019 4.28364372253418 216.10812377929688
Loss :  1.929396629333496 4.5748772621154785 230.6732635498047
Loss :  1.9250340461730957 4.473441123962402 225.5970916748047
Loss :  1.893703579902649 4.431070327758789 223.4472198486328
Loss :  1.8147960901260376 4.24817419052124 214.22349548339844
Loss :  1.8829410076141357 4.324678897857666 218.11688232421875
Loss :  1.9665950536727905 4.430521011352539 223.49264526367188
Loss :  1.8900260925292969 4.333704948425293 218.5752716064453
Loss :  1.9047281742095947 4.329689025878906 218.38917541503906
Loss :  1.902622938156128 4.418103218078613 222.8077850341797
Loss :  1.858223795890808 4.52994966506958 228.355712890625
Loss :  1.8735103607177734 4.503610610961914 227.0540313720703
Loss :  1.8877947330474854 4.498382568359375 226.8069305419922
Loss :  1.8813751935958862 4.499852180480957 226.87399291992188
Loss :  1.911780834197998 4.521370887756348 227.98031616210938
Loss :  1.8017857074737549 4.425683498382568 223.08596801757812
Loss :  1.8476159572601318 4.450579643249512 224.3765869140625
Loss :  1.8434075117111206 4.386717319488525 221.1792755126953
Loss :  1.8939746618270874 4.289230823516846 216.35552978515625
Loss :  1.8510923385620117 4.434399127960205 223.571044921875
  batch 20 loss: 1.8510923385620117, 4.434399127960205, 223.571044921875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 4, 5], device='cuda:0')
Loss :  1.8610899448394775 4.360270977020264 219.8746337890625
Loss :  1.8768926858901978 4.382795333862305 221.01666259765625
Loss :  1.8975441455841064 4.613595962524414 232.57733154296875
Loss :  1.8815691471099854 4.422237873077393 222.99346923828125
Loss :  1.8160886764526367 4.7003936767578125 236.8357696533203
Loss :  1.8711286783218384 4.334775924682617 218.60992431640625
Loss :  1.8273471593856812 4.492138862609863 226.43429565429688
Loss :  1.9098494052886963 4.2920451164245605 216.51210021972656
Loss :  1.8329765796661377 4.203482627868652 212.00711059570312
Loss :  1.8896352052688599 4.2682719230651855 215.30322265625
Loss :  1.8688732385635376 4.464757919311523 225.1067657470703
Loss :  1.9293017387390137 4.462284564971924 225.0435333251953
Loss :  1.8965599536895752 4.274015426635742 215.5973358154297
Loss :  1.878555178642273 4.274256229400635 215.59136962890625
Loss :  1.8218772411346436 4.081456184387207 205.8946990966797
Loss :  1.8337332010269165 4.394128322601318 221.5401611328125
Loss :  1.8239233493804932 4.375620365142822 220.60494995117188
Loss :  1.8282026052474976 3.978616237640381 200.75901794433594
Loss :  1.8116780519485474 4.24150276184082 213.88681030273438
Loss :  1.8332793712615967 4.276149272918701 215.6407470703125
  batch 40 loss: 1.8332793712615967, 4.276149272918701, 215.6407470703125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 4, 5], device='cuda:0')
Loss :  1.867362141609192 4.074426651000977 205.58868408203125
Loss :  1.8549020290374756 3.929185628890991 198.31417846679688
Loss :  1.9227852821350098 4.035563945770264 203.70098876953125
Loss :  1.8823665380477905 3.830983877182007 193.4315643310547
Loss :  1.8612546920776367 4.095836639404297 206.65309143066406
Loss :  1.7995641231536865 3.96549654006958 200.0743865966797
Loss :  1.750212550163269 4.053556442260742 204.42803955078125
Loss :  1.8332605361938477 4.1828508377075195 210.97581481933594
Loss :  1.7581045627593994 4.151572227478027 209.3367156982422
Loss :  1.81980299949646 4.212630271911621 212.45132446289062
Loss :  1.8390237092971802 3.9018826484680176 196.9331512451172
Loss :  1.7866992950439453 3.8283169269561768 193.20254516601562
Loss :  1.799145221710205 4.306093215942383 217.1038055419922
Loss :  1.7764158248901367 3.925626039505005 198.05770874023438
Loss :  1.8497132062911987 4.022416591644287 202.97055053710938
Loss :  1.730072021484375 3.635920286178589 183.52609252929688
Loss :  1.784815788269043 3.936889171600342 198.6292724609375
Loss :  1.819462776184082 3.9558768272399902 199.61329650878906
Loss :  1.7904036045074463 4.248099327087402 214.19537353515625
Loss :  1.7807167768478394 4.065243721008301 205.04290771484375
  batch 60 loss: 1.7807167768478394, 4.065243721008301, 205.04290771484375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.7978991270065308 4.016663551330566 202.63107299804688
Loss :  1.8360239267349243 3.926391124725342 198.15557861328125
Loss :  1.8084763288497925 3.6696319580078125 185.29006958007812
Loss :  1.870949625968933 3.7434566020965576 189.04379272460938
Loss :  1.8635822534561157 3.78163743019104 190.94546508789062
Loss :  1.4420835971832275 4.404176235198975 221.65089416503906
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.4887601137161255 4.342702865600586 218.6239013671875
Loss :  1.4903820753097534 4.265142917633057 214.74752807617188
Loss :  1.4252561330795288 4.093893527984619 206.11993408203125
Total LOSS train 213.21839576134315 valid 215.28556442260742
CE LOSS train 1.8516839687640851 valid 0.3563140332698822
Contrastive LOSS train 4.227334216924814 valid 1.0234733819961548
EPOCH 12:
Loss :  1.8066660165786743 3.4596590995788574 174.7896270751953
Loss :  1.7838444709777832 4.126679420471191 208.11781311035156
Loss :  1.7904751300811768 4.184437274932861 211.01234436035156
Loss :  1.80584716796875 4.211975574493408 212.40463256835938
Loss :  1.7833343744277954 4.17702579498291 210.63462829589844
Loss :  1.7407580614089966 4.3644118309021 219.9613494873047
Loss :  1.783457636833191 3.596231698989868 181.5950469970703
Loss :  1.7928880453109741 3.4227097034454346 172.92837524414062
Loss :  1.8179558515548706 3.889071226119995 196.2715301513672
Loss :  1.8116415739059448 4.217348575592041 212.6790771484375
Loss :  1.7854506969451904 4.225813388824463 213.07611083984375
Loss :  1.801086664199829 4.5595622062683105 229.77919006347656
Loss :  1.7338968515396118 4.204402446746826 211.9540252685547
Loss :  1.8378478288650513 4.593678951263428 231.52178955078125
Loss :  1.8168200254440308 4.483632564544678 225.99844360351562
Loss :  1.6944632530212402 4.651364326477051 234.26268005371094
Loss :  1.7579631805419922 4.549933433532715 229.254638671875
Loss :  1.7749069929122925 4.4087748527526855 222.21363830566406
Loss :  1.786816954612732 4.429974555969238 223.28555297851562
Loss :  1.754590392112732 4.551001071929932 229.3046417236328
  batch 20 loss: 1.754590392112732, 4.551001071929932, 229.3046417236328
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.8378897905349731 4.3567304611206055 219.67440795898438
Loss :  1.7846158742904663 4.359777927398682 219.7735137939453
Loss :  1.7628989219665527 4.355038166046143 219.51480102539062
Loss :  1.842760682106018 4.759355545043945 239.81053161621094
Loss :  1.7499229907989502 4.681774616241455 235.83865356445312
Loss :  1.8416324853897095 4.38985013961792 221.33413696289062
Loss :  1.7071174383163452 4.844252109527588 243.9197235107422
Loss :  1.7894128561019897 4.3491621017456055 219.2475128173828
Loss :  1.7397515773773193 4.475240230560303 225.5017547607422
Loss :  1.8196207284927368 4.475606441497803 225.59994506835938
Loss :  1.8165569305419922 4.4245381355285645 223.04347229003906
Loss :  1.8068335056304932 4.562285423278809 229.92111206054688
Loss :  1.7707210779190063 4.409272193908691 222.2343292236328
Loss :  1.8418635129928589 4.696464538574219 236.6650848388672
Loss :  1.797121286392212 4.612502574920654 232.4222412109375
Loss :  1.7670464515686035 4.499068737030029 226.72047424316406
Loss :  1.816793441772461 4.5508503913879395 229.35931396484375
Loss :  1.8412038087844849 4.379724979400635 220.82745361328125
Loss :  1.7829536199569702 4.391979694366455 221.38194274902344
Loss :  1.7893784046173096 4.429821491241455 223.28045654296875
  batch 40 loss: 1.7893784046173096, 4.429821491241455, 223.28045654296875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.7489372491836548 4.506521224975586 227.0749969482422
Loss :  1.7457153797149658 4.383130073547363 220.9022216796875
Loss :  1.9136401414871216 4.510337829589844 227.43052673339844
Loss :  1.9035885334014893 4.444703102111816 224.1387481689453
Loss :  1.8047927618026733 4.357696056365967 219.68959045410156
Loss :  1.7771246433258057 4.4537858963012695 224.4664306640625
Loss :  1.7505189180374146 4.386717796325684 221.08641052246094
Loss :  1.8284145593643188 4.4407057762146 223.86370849609375
Loss :  1.8289453983306885 4.4840989112854 226.0338897705078
Loss :  1.8243112564086914 4.449517250061035 224.3001708984375
Loss :  1.8780728578567505 4.528640270233154 228.31007385253906
Loss :  1.7875356674194336 4.355624198913574 219.56874084472656
Loss :  1.792360544204712 4.419373512268066 222.7610321044922
Loss :  1.7844130992889404 4.407874584197998 222.17813110351562
Loss :  1.8680204153060913 4.452356815338135 224.48587036132812
Loss :  1.8048640489578247 4.394820690155029 221.5458984375
Loss :  1.7819311618804932 4.459336280822754 224.74874877929688
Loss :  1.8450459241867065 4.365036964416504 220.09689331054688
Loss :  1.7591789960861206 4.3690643310546875 220.21240234375
Loss :  1.7918171882629395 4.258922100067139 214.73793029785156
  batch 60 loss: 1.7918171882629395, 4.258922100067139, 214.73793029785156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4], device='cuda:0')
Loss :  1.7840399742126465 4.438930988311768 223.7305908203125
Loss :  1.8646495342254639 4.383145809173584 221.02194213867188
Loss :  1.8033380508422852 4.400402545928955 221.82347106933594
Loss :  1.8579258918762207 4.367100238800049 220.2129364013672
Loss :  1.8286097049713135 4.198535919189453 211.75540161132812
Loss :  2.0759336948394775 4.47501277923584 225.82656860351562
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4], device='cuda:0')
Loss :  2.008868932723999 4.53009557723999 228.51364135742188
Loss :  2.1825685501098633 4.274512767791748 215.908203125
Loss :  1.9979703426361084 4.219902992248535 212.9931182861328
Total LOSS train 220.6660423865685 valid 220.81038284301758
CE LOSS train 1.7988399762373704 valid 0.4994925856590271
Contrastive LOSS train 4.377344047106229 valid 1.0549757480621338
EPOCH 13:
Loss :  1.8460239171981812 4.275683879852295 215.63021850585938
Loss :  1.7906131744384766 4.4916863441467285 226.37493896484375
Loss :  1.8566718101501465 4.3263840675354 218.17587280273438
Loss :  1.9076007604599 3.9476840496063232 199.29180908203125
Loss :  1.856285572052002 4.366083145141602 220.1604461669922
Loss :  1.7894625663757324 4.506101131439209 227.09451293945312
Loss :  1.8410447835922241 4.6581950187683105 234.75079345703125
Loss :  1.8495306968688965 4.064993381500244 205.0991973876953
Loss :  1.8836299180984497 4.218428134918213 212.80503845214844
Loss :  1.8778389692306519 4.130849361419678 208.42030334472656
Loss :  1.8178284168243408 4.099449634552002 206.79031372070312
Loss :  1.852195143699646 4.120631217956543 207.8837432861328
Loss :  1.806091547012329 4.157845973968506 209.69839477539062
Loss :  1.872867226600647 4.320352554321289 217.89048767089844
Loss :  1.936021327972412 4.141676425933838 209.01983642578125
Loss :  1.811689019203186 4.0803141593933105 205.827392578125
Loss :  1.806068778038025 4.174283504486084 210.52024841308594
Loss :  1.8049538135528564 3.8672749996185303 195.168701171875
Loss :  1.856181025505066 3.7897698879241943 191.3446807861328
Loss :  1.8397945165634155 3.947585105895996 199.21905517578125
  batch 20 loss: 1.8397945165634155, 3.947585105895996, 199.21905517578125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 4], device='cuda:0')
Loss :  1.8855053186416626 4.111734390258789 207.4722137451172
Loss :  1.8386611938476562 3.9758338928222656 200.63034057617188
Loss :  1.8783316612243652 4.275166988372803 215.6366729736328
Loss :  1.8220309019088745 4.4717278480529785 225.40843200683594
Loss :  1.8056604862213135 4.747496604919434 239.1804962158203
Loss :  1.847790002822876 4.488415241241455 226.2685546875
Loss :  1.794844627380371 4.669975757598877 235.29364013671875
Loss :  1.8731358051300049 4.380159854888916 220.88113403320312
Loss :  1.829100489616394 4.393502235412598 221.50421142578125
Loss :  1.8709598779678345 4.558088302612305 229.77537536621094
Loss :  1.8334424495697021 4.333508491516113 218.5088653564453
Loss :  1.8479130268096924 4.440255165100098 223.8606719970703
Loss :  1.8537243604660034 4.649625301361084 234.33499145507812
Loss :  1.898932933807373 4.4008331298828125 221.94058227539062
Loss :  1.849207878112793 4.377699375152588 220.7341766357422
Loss :  1.8681976795196533 4.276033878326416 215.66989135742188
Loss :  1.8553942441940308 4.127213001251221 208.21604919433594
Loss :  1.8770689964294434 3.936173677444458 198.68576049804688
Loss :  1.8710030317306519 4.06045389175415 204.89369201660156
Loss :  1.8914164304733276 4.271951198577881 215.48898315429688
  batch 40 loss: 1.8914164304733276, 4.271951198577881, 215.48898315429688
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.8649721145629883 3.936527967453003 198.69137573242188
Loss :  1.858435034751892 4.271791934967041 215.44802856445312
Loss :  1.8667407035827637 3.892167806625366 196.47512817382812
Loss :  1.8743305206298828 3.7310903072357178 188.42884826660156
Loss :  1.831228256225586 3.5795600414276123 180.80921936035156
Loss :  1.832236647605896 3.952251672744751 199.44480895996094
Loss :  1.837436318397522 4.264704704284668 215.07266235351562
Loss :  1.8551740646362305 4.351713180541992 219.4408416748047
Loss :  1.872583270072937 4.087845802307129 206.26487731933594
Loss :  1.8810985088348389 3.9101250171661377 197.38735961914062
Loss :  1.9083607196807861 4.052095413208008 204.51312255859375
Loss :  1.901711106300354 3.597938060760498 181.79861450195312
Loss :  1.886161208152771 3.8771307468414307 195.74269104003906
Loss :  1.8674603700637817 4.073862075805664 205.56056213378906
Loss :  1.8843833208084106 4.126354217529297 208.2021026611328
Loss :  1.9407575130462646 4.113780498504639 207.62977600097656
Loss :  1.8785474300384521 3.8336737155914307 193.56222534179688
Loss :  1.877234935760498 4.006493091583252 202.20188903808594
Loss :  1.785054087638855 4.427712917327881 223.1707000732422
Loss :  1.7924376726150513 4.4919819831848145 226.39154052734375
  batch 60 loss: 1.7924376726150513, 4.4919819831848145, 226.39154052734375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.882723331451416 4.638613700866699 233.8134002685547
Loss :  1.9043478965759277 3.9077229499816895 197.29049682617188
Loss :  1.8821629285812378 3.84578013420105 194.17115783691406
Loss :  1.8703268766403198 3.794856071472168 191.61312866210938
Loss :  1.841251254081726 3.905256986618042 197.10409545898438
Loss :  1.839145541191101 4.418691635131836 222.7737274169922
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 4], device='cuda:0')
Loss :  1.8554631471633911 4.476788520812988 225.6949005126953
Loss :  1.8882540464401245 4.427178382873535 223.24717712402344
Loss :  1.7493869066238403 4.3268890380859375 218.09384155273438
Total LOSS train 210.55045189490684 valid 222.45241165161133
CE LOSS train 1.8564903149238 valid 0.4373467266559601
Contrastive LOSS train 4.173879249279316 valid 1.0817222595214844
EPOCH 14:
Loss :  1.9573971033096313 4.045392036437988 204.2270050048828
Loss :  1.9250340461730957 3.9003093242645264 196.9405059814453
Loss :  1.8886085748672485 3.9169249534606934 197.73486328125
Loss :  1.9155142307281494 3.412923574447632 172.5616912841797
Loss :  1.8979804515838623 4.080419063568115 205.91893005371094
Loss :  1.7928955554962158 4.39874267578125 221.7300262451172
Loss :  1.7916980981826782 4.451959133148193 224.38966369628906
Loss :  1.827966570854187 4.253587245941162 214.50733947753906
Loss :  1.7928208112716675 4.4011993408203125 221.852783203125
Loss :  1.831937313079834 4.251152038574219 214.38954162597656
Loss :  1.7622231245040894 4.586036682128906 231.06405639648438
Loss :  1.7758591175079346 4.522935390472412 227.92263793945312
Loss :  1.778083324432373 4.431349754333496 223.34556579589844
Loss :  1.8200411796569824 4.288367748260498 216.23841857910156
Loss :  1.8519829511642456 4.111048221588135 207.40440368652344
Loss :  1.8042246103286743 4.007998943328857 202.2041778564453
Loss :  1.7824010848999023 3.9670321941375732 200.13400268554688
Loss :  1.7962617874145508 4.055987358093262 204.5956268310547
Loss :  1.730766773223877 4.153082370758057 209.3848876953125
Loss :  1.8293157815933228 4.163474082946777 210.00302124023438
  batch 20 loss: 1.8293157815933228, 4.163474082946777, 210.00302124023438
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.7782814502716064 4.186934947967529 211.12501525878906
Loss :  1.7426173686981201 4.131746292114258 208.32992553710938
Loss :  1.751171588897705 4.22090482711792 212.79641723632812
Loss :  1.777887225151062 4.043813705444336 203.9685821533203
Loss :  1.7616701126098633 4.295132160186768 216.51828002929688
Loss :  1.7502517700195312 4.168147087097168 210.1575927734375
Loss :  1.7630884647369385 4.396650314331055 221.59561157226562
Loss :  1.7794283628463745 4.136853218078613 208.62210083007812
Loss :  1.7067203521728516 4.107903957366943 207.1019287109375
Loss :  1.7684051990509033 4.068845272064209 205.21066284179688
Loss :  1.678006887435913 4.103242874145508 206.84014892578125
Loss :  1.801622748374939 4.15684175491333 209.64370727539062
Loss :  1.7150026559829712 4.376122951507568 220.52114868164062
Loss :  1.7282798290252686 4.146338939666748 209.04522705078125
Loss :  1.6903420686721802 4.2314677238464355 213.2637176513672
Loss :  1.6953754425048828 4.183467388153076 210.86874389648438
Loss :  1.7311675548553467 4.137049674987793 208.58364868164062
Loss :  1.812591552734375 4.1560587882995605 209.6155242919922
Loss :  1.7865206003189087 3.939965009689331 198.78475952148438
Loss :  1.8080370426177979 4.00631856918335 202.1239776611328
  batch 40 loss: 1.8080370426177979, 4.00631856918335, 202.1239776611328
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.777034044265747 4.286218166351318 216.08795166015625
Loss :  1.7063368558883667 4.112370491027832 207.32485961914062
Loss :  1.7011773586273193 4.187010765075684 211.0517120361328
Loss :  1.703743577003479 4.328657150268555 218.13661193847656
Loss :  1.7151190042495728 4.134325981140137 208.43141174316406
Loss :  1.7617357969284058 4.524334907531738 227.97848510742188
Loss :  1.7887341976165771 4.148229598999023 209.20021057128906
Loss :  1.7297974824905396 3.9400317668914795 198.73138427734375
Loss :  1.8278241157531738 4.02376127243042 203.01588439941406
Loss :  1.6988195180892944 4.2864766120910645 216.0226593017578
Loss :  1.7892546653747559 4.302949905395508 216.93675231933594
Loss :  1.7787870168685913 4.210072994232178 212.28244018554688
Loss :  1.7414395809173584 4.473058223724365 225.39434814453125
Loss :  1.7379398345947266 4.295694828033447 216.52268981933594
Loss :  1.726910948753357 4.422739028930664 222.86386108398438
Loss :  1.7039114236831665 4.116889476776123 207.5483856201172
Loss :  1.6806893348693848 4.304910659790039 216.9262237548828
Loss :  1.7141685485839844 4.400783061981201 221.75332641601562
Loss :  1.684900164604187 4.416768550872803 222.52333068847656
Loss :  1.7143466472625732 4.186706066131592 211.0496368408203
  batch 60 loss: 1.7143466472625732, 4.186706066131592, 211.0496368408203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.6596266031265259 4.120574951171875 207.68836975097656
Loss :  1.727034091949463 4.15255880355835 209.35498046875
Loss :  1.676956295967102 4.291558742523193 216.25489807128906
Loss :  1.7145774364471436 4.146960258483887 209.06259155273438
Loss :  1.709596037864685 3.7904560565948486 191.23239135742188
Loss :  1.7059242725372314 4.3484883308410645 219.13034057617188
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.7276175022125244 4.302765369415283 216.8658905029297
Loss :  1.6924781799316406 4.194910049438477 211.43797302246094
Loss :  1.6654560565948486 3.9756805896759033 200.44947814941406
Total LOSS train 211.14832716721756 valid 211.97092056274414
CE LOSS train 1.765383713061993 valid 0.41636401414871216
Contrastive LOSS train 4.1876588601332445 valid 0.9939201474189758
EPOCH 15:
Loss :  1.7291250228881836 3.8749587535858154 195.47706604003906
Loss :  1.7084306478500366 4.2135701179504395 212.38694763183594
Loss :  1.7156057357788086 4.125709056854248 208.0010528564453
Loss :  1.7218027114868164 4.1879448890686035 211.11904907226562
Loss :  1.7349976301193237 4.2987871170043945 216.6743621826172
Loss :  1.7053128480911255 4.222080230712891 212.80931091308594
Loss :  1.758133888244629 4.294190406799316 216.4676513671875
Loss :  1.745571494102478 4.019330978393555 202.71212768554688
Loss :  1.769117832183838 4.1938252449035645 211.46038818359375
Loss :  1.7695099115371704 4.065492630004883 205.0441436767578
Loss :  1.7506698369979858 4.083054542541504 205.9033966064453
Loss :  1.7275058031082153 4.297152996063232 216.5851593017578
Loss :  1.7014248371124268 4.38770866394043 221.0868682861328
Loss :  1.7578809261322021 4.372968673706055 220.40631103515625
Loss :  1.804753303527832 4.377171516418457 220.663330078125
Loss :  1.833631157875061 4.460326671600342 224.84996032714844
Loss :  1.7823984622955322 4.148355007171631 209.2001495361328
Loss :  1.7938284873962402 3.9501161575317383 199.2996368408203
Loss :  1.784790277481079 3.816072702407837 192.5884246826172
Loss :  1.8083484172821045 4.016369342803955 202.62681579589844
  batch 20 loss: 1.8083484172821045, 4.016369342803955, 202.62681579589844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.761234164237976 4.115696430206299 207.54605102539062
Loss :  1.7218061685562134 4.329334735870361 218.18853759765625
Loss :  1.7490900754928589 4.203117370605469 211.9049530029297
Loss :  1.7842148542404175 4.479605674743652 225.76449584960938
Loss :  1.7423042058944702 4.472725868225098 225.37860107421875
Loss :  1.7211799621582031 4.193599700927734 211.4011688232422
Loss :  1.7173625230789185 4.474149703979492 225.4248504638672
Loss :  1.712387204170227 4.097129821777344 206.56887817382812
Loss :  1.6887989044189453 4.1238298416137695 207.8802947998047
Loss :  1.7742444276809692 4.068918704986572 205.22018432617188
Loss :  1.7219228744506836 4.33846378326416 218.64511108398438
Loss :  1.7267807722091675 4.51865291595459 227.659423828125
Loss :  1.7469202280044556 4.327159404754639 218.10488891601562
Loss :  1.739669680595398 4.3293914794921875 218.20924377441406
Loss :  1.6998037099838257 4.285001277923584 215.94985961914062
Loss :  1.7219866514205933 4.223270893096924 212.88552856445312
Loss :  1.7159886360168457 4.187656879425049 211.0988311767578
Loss :  1.736323595046997 4.074897289276123 205.48118591308594
Loss :  1.7697303295135498 4.121752738952637 207.85736083984375
Loss :  1.7703229188919067 4.137017250061035 208.62118530273438
  batch 40 loss: 1.7703229188919067, 4.137017250061035, 208.62118530273438
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.7438710927963257 4.506505489349365 227.0691375732422
Loss :  1.7502604722976685 4.519599914550781 227.73025512695312
Loss :  1.7637226581573486 4.406624794006348 222.09495544433594
Loss :  1.7494436502456665 4.530375003814697 228.26820373535156
Loss :  1.704759955406189 4.041845321655273 203.79702758789062
Loss :  1.7100903987884521 4.060180187225342 204.71908569335938
Loss :  1.7336069345474243 4.245724201202393 214.0198211669922
Loss :  1.7262591123580933 4.334865093231201 218.46951293945312
Loss :  1.7489546537399292 4.15333890914917 209.4158935546875
Loss :  1.724360466003418 4.32425594329834 217.93716430664062
Loss :  1.757186770439148 4.16475772857666 209.9950714111328
Loss :  1.729562520980835 4.140398979187012 208.74951171875
Loss :  1.7142714262008667 4.431736469268799 223.30108642578125
Loss :  1.7422199249267578 4.476811408996582 225.58279418945312
Loss :  1.70204496383667 4.383542537689209 220.87916564941406
Loss :  1.7925732135772705 4.641220569610596 233.85360717773438
Loss :  1.7520763874053955 4.41202974319458 222.3535614013672
Loss :  1.7217668294906616 3.969914436340332 200.21749877929688
Loss :  1.7178285121917725 4.473226070404053 225.37913513183594
Loss :  1.759933590888977 4.215161323547363 212.51800537109375
  batch 60 loss: 1.759933590888977, 4.215161323547363, 212.51800537109375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4, 5], device='cuda:0')
Loss :  1.680547833442688 4.327400207519531 218.05055236816406
Loss :  1.7667365074157715 4.4157609939575195 222.55479431152344
Loss :  1.7274798154830933 4.33623743057251 218.5393524169922
Loss :  1.6900742053985596 4.497812271118164 226.5806884765625
Loss :  1.627463459968567 4.198060989379883 211.53050231933594
Loss :  1.514366626739502 4.435319900512695 223.28036499023438
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
Loss :  1.5292779207229614 4.365324974060059 219.7955322265625
Loss :  1.5285415649414062 4.346566200256348 218.85684204101562
Loss :  1.490404486656189 4.164151191711426 209.69796752929688
Total LOSS train 214.59629493126502 valid 217.90767669677734
CE LOSS train 1.7394154842083271 valid 0.37260112166404724
Contrastive LOSS train 4.257137592022236 valid 1.0410377979278564
EPOCH 16:
Loss :  1.7041454315185547 4.0869364738464355 206.05096435546875
Loss :  1.6807615756988525 4.663915157318115 234.8765106201172
Loss :  1.6720620393753052 4.58474588394165 230.9093475341797
Loss :  1.698607087135315 4.4879350662231445 226.09536743164062
Loss :  1.7080754041671753 4.288443088531494 216.13021850585938
Loss :  1.6615245342254639 4.44042444229126 223.68275451660156
Loss :  1.678446650505066 4.541600227355957 228.7584686279297
Loss :  1.6108238697052002 4.191723823547363 211.197021484375
Loss :  1.6283379793167114 4.411097526550293 222.18321228027344
Loss :  1.669984221458435 4.328628063201904 218.10137939453125
Loss :  1.6035677194595337 4.535412311553955 228.37417602539062
Loss :  1.597065806388855 4.466875076293945 224.94081115722656
Loss :  1.6038486957550049 4.454998970031738 224.3538055419922
Loss :  1.6288307905197144 4.472670078277588 225.2623291015625
Loss :  1.6314460039138794 4.508395671844482 227.0512237548828
Loss :  1.6285725831985474 4.594599723815918 231.35855102539062
Loss :  1.5786488056182861 4.396021842956543 221.37973022460938
Loss :  1.6165611743927002 4.700277328491211 236.63043212890625
Loss :  1.5805751085281372 4.364525318145752 219.80685424804688
Loss :  1.671129584312439 4.437164306640625 223.5293426513672
  batch 20 loss: 1.671129584312439, 4.437164306640625, 223.5293426513672
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.625296950340271 4.336950778961182 218.4728240966797
Loss :  1.608860969543457 4.536183834075928 228.4180450439453
Loss :  1.6012150049209595 4.565241813659668 229.86329650878906
Loss :  1.6261848211288452 4.4163665771484375 222.44451904296875
Loss :  1.6278167963027954 4.68623161315918 235.9394073486328
Loss :  1.5816973447799683 4.45159387588501 224.16139221191406
Loss :  1.5795868635177612 4.541191577911377 228.63917541503906
Loss :  1.5988099575042725 4.405195713043213 221.8585968017578
Loss :  1.52223801612854 4.6245551109313965 232.74998474121094
Loss :  1.6466425657272339 4.368533134460449 220.0732879638672
Loss :  1.5311064720153809 4.566034317016602 229.83282470703125
Loss :  1.6035393476486206 4.62577486038208 232.8922882080078
Loss :  1.588279366493225 4.402512073516846 221.71388244628906
Loss :  1.5820151567459106 4.420740127563477 222.6190185546875
Loss :  1.480606198310852 4.5030341148376465 226.63230895996094
Loss :  1.526097059249878 4.457101821899414 224.3811798095703
Loss :  1.509718894958496 4.511624813079834 227.09095764160156
Loss :  1.5692884922027588 4.53151798248291 228.1451873779297
Loss :  1.6538808345794678 4.351279258728027 219.2178497314453
Loss :  1.6174557209014893 4.654219627380371 234.3284454345703
  batch 40 loss: 1.6174557209014893, 4.654219627380371, 234.3284454345703
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.5920361280441284 4.534034252166748 228.2937469482422
Loss :  1.5286142826080322 4.440751075744629 223.566162109375
Loss :  1.5508514642715454 4.458395004272461 224.47061157226562
Loss :  1.5744696855545044 4.5097479820251465 227.06185913085938
Loss :  1.5468815565109253 4.449513912200928 224.02256774902344
Loss :  1.5567190647125244 4.378854751586914 220.49945068359375
Loss :  1.5753525495529175 4.5413031578063965 228.6405029296875
Loss :  1.5805052518844604 4.464966297149658 224.82882690429688
Loss :  1.6202303171157837 4.43479061126709 223.35975646972656
Loss :  1.5629909038543701 4.487218856811523 225.92393493652344
Loss :  1.600739598274231 4.432573318481445 223.22940063476562
Loss :  1.6182036399841309 4.470259666442871 225.13119506835938
Loss :  1.544467568397522 4.325127124786377 217.8008270263672
Loss :  1.5877622365951538 4.474642276763916 225.31988525390625
Loss :  1.5802853107452393 4.464907169342041 224.82565307617188
Loss :  1.617583155632019 4.36406135559082 219.82064819335938
Loss :  1.5498329401016235 4.5664448738098145 229.87208557128906
Loss :  1.561678171157837 4.384264945983887 220.77491760253906
Loss :  1.533395528793335 4.657715797424316 234.419189453125
Loss :  1.6468299627304077 4.322180271148682 217.75584411621094
  batch 60 loss: 1.6468299627304077, 4.322180271148682, 217.75584411621094
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.505570411682129 4.545557022094727 228.78341674804688
Loss :  1.5478264093399048 4.453427314758301 224.2191925048828
Loss :  1.509687066078186 4.368118762969971 219.91563415527344
Loss :  1.5348782539367676 4.406708717346191 221.8703155517578
Loss :  1.4675664901733398 4.165036678314209 209.71939086914062
Loss :  1.487860083580017 4.408577919006348 221.916748046875
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5101982355117798 4.3648810386657715 219.75424194335938
Loss :  1.520620346069336 4.313304424285889 217.1858367919922
Loss :  1.491100788116455 4.071709156036377 205.07656860351562
Total LOSS train 224.68110750638522 valid 215.98334884643555
CE LOSS train 1.594281689937298 valid 0.37277519702911377
Contrastive LOSS train 4.461736532358023 valid 1.0179272890090942
EPOCH 17:
Loss :  1.5994287729263306 4.4484076499938965 224.01980590820312
Loss :  1.557451844215393 4.599300861358643 231.52249145507812
Loss :  1.587141752243042 4.43259859085083 223.21707153320312
Loss :  1.5471802949905396 4.473438739776611 225.2191162109375
Loss :  1.5384382009506226 4.586934566497803 230.88516235351562
Loss :  1.4978190660476685 4.3799214363098145 220.493896484375
Loss :  1.5657240152359009 4.577115058898926 230.42147827148438
Loss :  1.5446678400039673 4.297417640686035 216.41554260253906
Loss :  1.4746763706207275 4.37791109085083 220.37022399902344
Loss :  1.5914939641952515 4.282378673553467 215.71041870117188
Loss :  1.500690221786499 4.469193935394287 224.96038818359375
Loss :  1.5075210332870483 4.591042518615723 231.0596466064453
Loss :  1.4940489530563354 4.587363243103027 230.86221313476562
Loss :  1.5026123523712158 4.390824794769287 221.04385375976562
Loss :  1.637708067893982 4.222537517547607 212.76458740234375
Loss :  1.5909054279327393 4.426083564758301 222.8950958251953
Loss :  1.459863305091858 4.451603412628174 224.0400390625
Loss :  1.5106796026229858 4.45167350769043 224.0943603515625
Loss :  1.440528392791748 4.493696689605713 226.12535095214844
Loss :  1.5905251502990723 4.390411853790283 221.1111297607422
  batch 20 loss: 1.5905251502990723, 4.390411853790283, 221.1111297607422
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.50877845287323 4.6621479988098145 234.61618041992188
Loss :  1.4726638793945312 4.6194329261779785 232.44430541992188
Loss :  1.4233644008636475 4.673311710357666 235.08895874023438
Loss :  1.497450828552246 4.496057033538818 226.30030822753906
Loss :  1.5259653329849243 4.512126445770264 227.13229370117188
Loss :  1.4544761180877686 4.540712833404541 228.49012756347656
Loss :  1.3973267078399658 4.636765480041504 233.235595703125
Loss :  1.4375009536743164 4.380332946777344 220.4541473388672
Loss :  1.3469048738479614 4.499157905578613 226.3048095703125
Loss :  1.5712655782699585 4.444086074829102 223.7755584716797
Loss :  1.3413866758346558 4.379429340362549 220.31285095214844
Loss :  1.5083205699920654 4.502972602844238 226.65695190429688
Loss :  1.4584369659423828 4.366737365722656 219.79530334472656
Loss :  1.4940272569656372 4.280909538269043 215.5395050048828
Loss :  1.3665692806243896 4.186721324920654 210.70262145996094
Loss :  1.423427939414978 4.379257678985596 220.38632202148438
Loss :  1.4009226560592651 4.38340425491333 220.57113647460938
Loss :  1.4561485052108765 4.346551895141602 218.7837371826172
Loss :  1.5214803218841553 4.327388286590576 217.89089965820312
Loss :  1.5318468809127808 4.503763198852539 226.72000122070312
  batch 40 loss: 1.5318468809127808, 4.503763198852539, 226.72000122070312
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.4389184713363647 4.241222858428955 213.50006103515625
Loss :  1.3806606531143188 4.253162860870361 214.03880310058594
Loss :  1.3849165439605713 4.24065637588501 213.41773986816406
Loss :  1.3790892362594604 4.224887847900391 212.62347412109375
Loss :  1.3802839517593384 4.316023349761963 217.1814422607422
Loss :  1.4501465559005737 4.019374370574951 202.4188690185547
Loss :  1.5262597799301147 4.330900192260742 218.07127380371094
Loss :  1.4082609415054321 4.555919170379639 229.2042236328125
Loss :  1.6037366390228271 4.405737400054932 221.89059448242188
Loss :  1.4608088731765747 4.554718494415283 229.19674682617188
Loss :  1.5013843774795532 4.631453037261963 233.07403564453125
Loss :  1.5254392623901367 4.602238655090332 231.6373748779297
Loss :  1.469724416732788 4.372218608856201 220.08065795898438
Loss :  1.4944738149642944 4.525021076202393 227.7455291748047
Loss :  1.4803718328475952 4.571176052093506 230.0391845703125
Loss :  1.600369930267334 4.5947794914245605 231.33934020996094
Loss :  1.4602622985839844 4.4540839195251465 224.16445922851562
Loss :  1.4510525465011597 4.470792293548584 224.99066162109375
Loss :  1.4748338460922241 4.576226234436035 230.28614807128906
Loss :  1.6116306781768799 4.2860107421875 215.91217041015625
  batch 60 loss: 1.6116306781768799, 4.2860107421875, 215.91217041015625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 5], device='cuda:0')
Loss :  1.4789295196533203 4.530485153198242 228.00318908691406
Loss :  1.5475261211395264 4.529342174530029 228.01463317871094
Loss :  1.4898948669433594 4.537573337554932 228.36856079101562
Loss :  1.5263704061508179 4.530831336975098 228.06793212890625
Loss :  1.4738746881484985 4.134711742401123 208.20945739746094
Loss :  1.4974027872085571 4.503533363342285 226.674072265625
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 5], device='cuda:0')
Loss :  1.5337408781051636 4.390843868255615 221.075927734375
Loss :  1.5463943481445312 4.366876125335693 219.89019775390625
Loss :  1.486690878868103 4.108205318450928 206.89695739746094
Total LOSS train 223.44476999136117 valid 218.6342887878418
CE LOSS train 1.4904090624589186 valid 0.37167271971702576
Contrastive LOSS train 4.439087214836707 valid 1.027051329612732
EPOCH 18:
Loss :  1.558102011680603 4.430664539337158 223.09133911132812
Loss :  1.5264849662780762 4.737936019897461 238.4232940673828
Loss :  1.488323450088501 4.351169109344482 219.04678344726562
Loss :  1.5540164709091187 4.385836601257324 220.84584045410156
Loss :  1.508437991142273 4.298864841461182 216.45167541503906
Loss :  1.46210515499115 4.438210487365723 223.3726348876953
Loss :  1.6531355381011963 4.420331954956055 222.66973876953125
Loss :  1.4903720617294312 4.3477373123168945 218.8772430419922
Loss :  1.5485771894454956 4.372647285461426 220.1809539794922
Loss :  1.6237183809280396 4.496066570281982 226.4270477294922
Loss :  1.5052443742752075 4.71793794631958 237.4021453857422
Loss :  1.5110549926757812 4.509598731994629 226.99099731445312
Loss :  1.4835402965545654 4.660200119018555 234.49354553222656
Loss :  1.5444731712341309 4.633646488189697 233.226806640625
Loss :  1.6717183589935303 4.479254722595215 225.63446044921875
Loss :  1.5741440057754517 4.488592147827148 226.00375366210938
Loss :  1.4506335258483887 4.4085493087768555 221.8780975341797
Loss :  1.537927508354187 4.545051097869873 228.7904815673828
Loss :  1.5176011323928833 4.373127460479736 220.17398071289062
Loss :  1.639102816581726 4.498324871063232 226.5553436279297
  batch 20 loss: 1.639102816581726, 4.498324871063232, 226.5553436279297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  1.5463120937347412 4.344654560089111 218.77903747558594
Loss :  1.4723094701766968 4.6726765632629395 235.10614013671875
Loss :  1.5520707368850708 4.651803970336914 234.14227294921875
Loss :  1.5918806791305542 4.406870365142822 221.93539428710938
Loss :  1.5665162801742554 4.907781600952148 246.95559692382812
Loss :  1.5590035915374756 4.719202041625977 237.51910400390625
Loss :  1.566789984703064 4.812523365020752 242.19296264648438
Loss :  1.5967508554458618 4.468538284301758 225.02366638183594
Loss :  1.4500256776809692 4.494225025177002 226.16128540039062
Loss :  1.6516920328140259 4.393304824829102 221.31692504882812
Loss :  1.4855633974075317 4.487181186676025 225.84461975097656
Loss :  1.5648607015609741 4.679821968078613 235.55596923828125
Loss :  1.5269850492477417 4.424654006958008 222.75967407226562
Loss :  1.5597673654556274 4.453847408294678 224.25213623046875
Loss :  1.5039591789245605 4.5632734298706055 229.66761779785156
Loss :  1.4649021625518799 4.464561462402344 224.69297790527344
Loss :  1.5152605772018433 4.395688056945801 221.29966735839844
Loss :  1.559454321861267 4.498619556427002 226.4904327392578
Loss :  1.5870996713638306 4.370044231414795 220.0893096923828
Loss :  1.5869027376174927 4.387703895568848 220.9720916748047
  batch 40 loss: 1.5869027376174927, 4.387703895568848, 220.9720916748047
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5922597646713257 4.466375827789307 224.91104125976562
Loss :  1.5375938415527344 4.376933574676514 220.38427734375
Loss :  1.5382908582687378 4.477393627166748 225.407958984375
Loss :  1.5786632299423218 4.503889560699463 226.77313232421875
Loss :  1.5092072486877441 4.233824729919434 213.200439453125
Loss :  1.5692920684814453 4.357914447784424 219.4650115966797
Loss :  1.6543073654174805 4.502559185028076 226.7822723388672
Loss :  1.5812207460403442 4.496975421905518 226.42999267578125
Loss :  1.6402229070663452 4.251439571380615 214.2122039794922
Loss :  1.601538062095642 4.447922229766846 223.99765014648438
Loss :  1.6070374250411987 4.375246524810791 220.36936950683594
Loss :  1.6314098834991455 4.250792026519775 214.17100524902344
Loss :  1.581721305847168 4.44276762008667 223.72010803222656
Loss :  1.59145987033844 4.365045547485352 219.84373474121094
Loss :  1.5733110904693604 4.474420547485352 225.29434204101562
Loss :  1.6304563283920288 4.577723503112793 230.51663208007812
Loss :  1.4846909046173096 4.455933094024658 224.28135681152344
Loss :  1.5212748050689697 4.287393569946289 215.8909454345703
Loss :  1.5189225673675537 4.652419090270996 234.13987731933594
Loss :  1.6666772365570068 4.350017070770264 219.1675262451172
  batch 60 loss: 1.6666772365570068, 4.350017070770264, 219.1675262451172
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.5607712268829346 4.5329670906066895 228.20913696289062
Loss :  1.5543113946914673 4.417819499969482 222.44528198242188
Loss :  1.587329626083374 4.467587471008301 224.96670532226562
Loss :  1.5421736240386963 4.45685338973999 224.3848419189453
Loss :  1.5177581310272217 4.180710792541504 210.5532989501953
Loss :  1.5387308597564697 4.446594715118408 223.86846923828125
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.5799628496170044 4.379326343536377 220.54627990722656
Loss :  1.573030948638916 4.2541680335998535 214.28143310546875
Loss :  1.5301735401153564 4.1210856437683105 207.58444213867188
Total LOSS train 225.08940288837138 valid 216.5701560974121
CE LOSS train 1.5558264842400185 valid 0.3825433850288391
Contrastive LOSS train 4.47067151436439 valid 1.0302714109420776
EPOCH 19:
Loss :  1.625874400138855 4.429896831512451 223.12071228027344
Loss :  1.6420481204986572 4.643495559692383 233.8168182373047
Loss :  1.5861310958862305 4.417336463928223 222.4529571533203
Loss :  1.655164122581482 4.532843589782715 228.29734802246094
Loss :  1.583907961845398 4.531890392303467 228.1784210205078
Loss :  1.612332820892334 4.4204840660095215 222.63653564453125
Loss :  1.6745469570159912 4.525180339813232 227.93356323242188
Loss :  1.6929585933685303 4.268368721008301 215.1114044189453
Loss :  1.5782781839370728 4.44587516784668 223.87203979492188
Loss :  1.700903058052063 4.327683448791504 218.08506774902344
Loss :  1.6487650871276855 4.349373817443848 219.11744689941406
Loss :  1.6228513717651367 4.3757643699646 220.41107177734375
Loss :  1.6770044565200806 4.393466472625732 221.35032653808594
Loss :  1.6681387424468994 4.486509799957275 225.99362182617188
Loss :  1.7397900819778442 4.343623161315918 218.9209442138672
Loss :  1.663853406906128 4.479970932006836 225.6623992919922
Loss :  1.5848350524902344 4.397115707397461 221.4406280517578
Loss :  1.6181585788726807 4.471344470977783 225.18539428710938
Loss :  1.6353873014450073 4.550362586975098 229.1535186767578
Loss :  1.6615900993347168 4.4568939208984375 224.50628662109375
  batch 20 loss: 1.6615900993347168, 4.4568939208984375, 224.50628662109375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6908491849899292 4.4575982093811035 224.5707550048828
Loss :  1.6020230054855347 4.423379421234131 222.77099609375
Loss :  1.6372324228286743 4.680016994476318 235.63809204101562
Loss :  1.7324600219726562 4.570519924163818 230.25845336914062
Loss :  1.738521695137024 4.485574722290039 226.0172576904297
Loss :  1.6463396549224854 4.445263862609863 223.9095458984375
Loss :  1.7067688703536987 4.702356338500977 236.8245849609375
Loss :  1.6542562246322632 4.299952507019043 216.65187072753906
Loss :  1.559893250465393 4.547194004058838 228.91958618164062
Loss :  1.7889426946640015 4.469021320343018 225.24000549316406
Loss :  1.5675206184387207 4.5909504890441895 231.11505126953125
Loss :  1.6923447847366333 4.6446638107299805 233.925537109375
Loss :  1.6656711101531982 4.453492641448975 224.34030151367188
Loss :  1.6154156923294067 4.5539326667785645 229.3120574951172
Loss :  1.5855964422225952 4.531146049499512 228.1428985595703
Loss :  1.6805424690246582 4.428168773651123 223.08897399902344
Loss :  1.6279642581939697 4.55418586730957 229.33724975585938
Loss :  1.7060874700546265 4.34684944152832 219.04855346679688
Loss :  1.757867693901062 4.761508464813232 239.8332977294922
Loss :  1.7086769342422485 4.61807918548584 232.61264038085938
  batch 40 loss: 1.7086769342422485, 4.61807918548584, 232.61264038085938
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.6438523530960083 4.458155155181885 224.55162048339844
Loss :  1.6135543584823608 4.594711780548096 231.34915161132812
Loss :  1.63210928440094 4.5239667892456055 227.8304443359375
Loss :  1.6643657684326172 4.544166564941406 228.87269592285156
Loss :  1.5880839824676514 4.2819952964782715 215.68785095214844
Loss :  1.658574104309082 4.369454383850098 220.13128662109375
Loss :  1.7878749370574951 4.485204696655273 226.04811096191406
Loss :  1.6093344688415527 4.509706020355225 227.09463500976562
Loss :  1.7271872758865356 4.421127796173096 222.78358459472656
Loss :  1.6262843608856201 4.530738353729248 228.16319274902344
Loss :  1.7073200941085815 4.539445877075195 228.6796112060547
Loss :  1.6474136114120483 4.42522668838501 222.90875244140625
Loss :  1.6519882678985596 4.425844669342041 222.94422912597656
Loss :  1.7539118528366089 4.6351213455200195 233.50997924804688
Loss :  1.571184754371643 4.570847034454346 230.11354064941406
Loss :  1.704734444618225 4.50927209854126 227.1683349609375
Loss :  1.6881968975067139 4.475968360900879 225.4866180419922
Loss :  1.6242624521255493 4.51810884475708 227.5297088623047
Loss :  1.681797981262207 4.919660568237305 247.66482543945312
Loss :  1.8145796060562134 4.645262718200684 234.0777130126953
  batch 60 loss: 1.8145796060562134, 4.645262718200684, 234.0777130126953
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.6259644031524658 4.510266304016113 227.1392822265625
Loss :  1.6286766529083252 4.536855220794678 228.471435546875
Loss :  1.6429840326309204 4.398223876953125 221.55418395996094
Loss :  1.5718803405761719 4.450348854064941 224.08932495117188
Loss :  1.5326216220855713 4.15971565246582 209.51840209960938
Loss :  1.6427477598190308 4.745291233062744 238.9073028564453
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6909695863723755 4.389360427856445 221.1589813232422
Loss :  1.6653164625167847 4.324063777923584 217.86849975585938
Loss :  1.725324273109436 4.238764762878418 213.66355895996094
Total LOSS train 226.15650353064905 valid 222.89958572387695
CE LOSS train 1.6559420292194074 valid 0.431331068277359
Contrastive LOSS train 4.490011222545917 valid 1.0596911907196045
EPOCH 20:
Loss :  1.6570183038711548 4.3852925300598145 220.92164611816406
Loss :  1.6961733102798462 4.740391254425049 238.71572875976562
Loss :  1.6968587636947632 4.514347553253174 227.4142303466797
Loss :  1.6713956594467163 4.429238319396973 223.13331604003906
Loss :  1.6011791229248047 4.254433631896973 214.32286071777344
Loss :  1.6133241653442383 4.532925128936768 228.25958251953125
Loss :  1.7032493352890015 4.823458194732666 242.87615966796875
Loss :  1.5790518522262573 4.2991414070129395 216.5361328125
Loss :  1.6047420501708984 4.397131443023682 221.4613037109375
Loss :  1.717680811882019 4.777440547943115 240.58970642089844
Loss :  1.5455570220947266 4.528618812561035 227.97650146484375
Loss :  1.513917326927185 4.621735095977783 232.60067749023438
Loss :  1.5265651941299438 4.5325422286987305 228.15367126464844
Loss :  1.648645043373108 4.533549785614014 228.32614135742188
Loss :  1.629722237586975 4.373035907745361 220.28150939941406
Loss :  1.6948119401931763 4.714382648468018 237.4139404296875
Loss :  1.572983980178833 4.433603286743164 223.25314331054688
Loss :  1.5706963539123535 4.427791118621826 222.9602508544922
Loss :  1.5967479944229126 4.379554271697998 220.57444763183594
Loss :  1.6648552417755127 4.440455913543701 223.68765258789062
  batch 20 loss: 1.6648552417755127, 4.440455913543701, 223.68765258789062
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.565499186515808 4.265970230102539 214.864013671875
Loss :  1.5597670078277588 4.597134113311768 231.41647338867188
Loss :  1.5535227060317993 4.622469902038574 232.67701721191406
Loss :  1.5966734886169434 4.448663711547852 224.0298614501953
Loss :  1.7110145092010498 4.542973041534424 228.8596649169922
Loss :  1.551658034324646 4.523031711578369 227.70323181152344
Loss :  1.625555396080017 4.700711727142334 236.6611328125
Loss :  1.6495471000671387 4.458512783050537 224.5751953125
Loss :  1.519510269165039 4.4182610511779785 222.4325714111328
Loss :  1.6905126571655273 4.6379923820495605 233.59011840820312
Loss :  1.4944077730178833 4.6161675453186035 232.30279541015625
Loss :  1.5996320247650146 4.718465805053711 237.52291870117188
Loss :  1.6050103902816772 4.396322250366211 221.42112731933594
Loss :  1.637606143951416 4.4354777336120605 223.41148376464844
Loss :  1.491042971611023 4.634352684020996 233.20867919921875
Loss :  1.4954198598861694 4.6740522384643555 235.19802856445312
Loss :  1.5692638158798218 4.640676975250244 233.6031036376953
Loss :  1.6689434051513672 4.531717777252197 228.2548370361328
Loss :  1.6982392072677612 4.3741865158081055 220.40756225585938
Loss :  1.6594443321228027 4.527429580688477 228.03091430664062
  batch 40 loss: 1.6594443321228027, 4.527429580688477, 228.03091430664062
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.679324984550476 4.507685661315918 227.0635986328125
Loss :  1.5760228633880615 4.463944435119629 224.7732391357422
Loss :  1.5834708213806152 4.474009990692139 225.28396606445312
Loss :  1.6240423917770386 4.475104331970215 225.37925720214844
Loss :  1.568466305732727 4.289534568786621 216.04519653320312
Loss :  1.6572037935256958 4.573495864868164 230.33200073242188
Loss :  1.6934058666229248 4.473947525024414 225.39077758789062
Loss :  1.6111468076705933 4.491171360015869 226.16970825195312
Loss :  1.761522650718689 4.329870223999023 218.25503540039062
Loss :  1.6635606288909912 4.43801212310791 223.5641632080078
Loss :  1.695825219154358 4.388762950897217 221.13397216796875
Loss :  1.7379000186920166 4.486291408538818 226.05247497558594
Loss :  1.6430411338806152 4.3874125480651855 221.01365661621094
Loss :  1.6990340948104858 4.64552640914917 233.97535705566406
Loss :  1.6485085487365723 4.478737831115723 225.5854034423828
Loss :  1.7728737592697144 4.494729995727539 226.50936889648438
Loss :  1.6193208694458008 4.685279369354248 235.88328552246094
Loss :  1.6632193326950073 4.507265090942383 227.02647399902344
Loss :  1.6622672080993652 4.64821720123291 234.0731201171875
Loss :  1.7436933517456055 4.52786922454834 228.1371612548828
  batch 60 loss: 1.7436933517456055, 4.52786922454834, 228.1371612548828
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 5], device='cuda:0')
Loss :  1.6067345142364502 4.504112720489502 226.8123779296875
Loss :  1.7217813730239868 4.642366409301758 233.84010314941406
Loss :  1.5985873937606812 4.394186973571777 221.3079376220703
Loss :  1.575241208076477 4.421393394470215 222.64491271972656
Loss :  1.6387895345687866 4.219958305358887 212.63670349121094
Loss :  1.5556174516677856 4.435487270355225 223.32998657226562
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6132827997207642 4.343479633331299 218.78726196289062
Loss :  1.602852702140808 4.237529277801514 213.4793243408203
Loss :  1.606402039527893 4.377093315124512 220.4610595703125
Total LOSS train 226.90028592623196 valid 219.01440811157227
CE LOSS train 1.629114317893982 valid 0.40160050988197327
Contrastive LOSS train 4.505423457805927 valid 1.094273328781128
EPOCH 21:
Loss :  1.6220890283584595 4.539427280426025 228.59344482421875
Loss :  1.6693416833877563 4.5540289878845215 229.37078857421875
Loss :  1.6565033197402954 4.5222272872924805 227.7678680419922
Loss :  1.655344009399414 4.5253005027771 227.92037963867188
Loss :  1.73117995262146 4.357522010803223 219.60728454589844
Loss :  1.6165409088134766 4.672650337219238 235.2490692138672
Loss :  1.72517728805542 4.55391263961792 229.42080688476562
Loss :  1.6471720933914185 4.335886001586914 218.44146728515625
Loss :  1.5250409841537476 4.414776802062988 222.26388549804688
Loss :  1.6175709962844849 4.168128967285156 210.02401733398438
Loss :  1.6101206541061401 4.643119812011719 233.76611328125
Loss :  1.5936046838760376 4.583329677581787 230.7600860595703
Loss :  1.5795947313308716 4.4720916748046875 225.18417358398438
Loss :  1.6124149560928345 4.559042930603027 229.56455993652344
Loss :  1.7294681072235107 4.57412576675415 230.4357452392578
Loss :  1.706016182899475 4.333181858062744 218.36509704589844
Loss :  1.590718150138855 4.663557052612305 234.76856994628906
Loss :  1.6124038696289062 4.486092567443848 225.91702270507812
Loss :  1.545597791671753 4.351014614105225 219.09632873535156
Loss :  1.6446293592453003 4.486365795135498 225.96290588378906
  batch 20 loss: 1.6446293592453003, 4.486365795135498, 225.96290588378906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.6160168647766113 4.476922512054443 225.46214294433594
Loss :  1.5322892665863037 4.525110721588135 227.78782653808594
Loss :  1.637052297592163 4.476999759674072 225.48704528808594
Loss :  1.6588164567947388 4.5906171798706055 231.18966674804688
Loss :  1.63886296749115 4.528114318847656 228.04458618164062
Loss :  1.5603357553482056 4.353908061981201 219.2557373046875
Loss :  1.5305426120758057 4.780271053314209 240.54409790039062
Loss :  1.5970079898834229 4.2949934005737305 216.3466796875
Loss :  1.5383285284042358 4.497732162475586 226.42494201660156
Loss :  1.7160993814468384 4.4500956535339355 224.2208709716797
Loss :  1.528373122215271 4.5330119132995605 228.178955078125
Loss :  1.6654235124588013 4.635900497436523 233.46044921875
Loss :  1.6073377132415771 4.4074931144714355 221.9819793701172
Loss :  1.6965785026550293 4.424187660217285 222.9059600830078
Loss :  1.5130037069320679 4.4233078956604 222.6783905029297
Loss :  1.5614707469940186 4.409058570861816 222.014404296875
Loss :  1.596234679222107 4.379862308502197 220.58935546875
Loss :  1.6020759344100952 4.388115406036377 221.0078582763672
Loss :  1.6830871105194092 4.375105381011963 220.4383544921875
Loss :  1.6815061569213867 4.575901508331299 230.47657775878906
  batch 40 loss: 1.6815061569213867, 4.575901508331299, 230.47657775878906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.6592459678649902 4.498341083526611 226.5762939453125
Loss :  1.5697709321975708 4.424647331237793 222.80213928222656
Loss :  1.5745453834533691 4.434812068939209 223.3151397705078
Loss :  1.6412436962127686 4.640120029449463 233.64724731445312
Loss :  1.546616792678833 4.367630481719971 219.9281463623047
Loss :  1.5309185981750488 4.4632768630981445 224.69476318359375
Loss :  1.6377428770065308 4.362451076507568 219.7602996826172
Loss :  1.5787187814712524 4.546960830688477 228.9267578125
Loss :  1.673740267753601 4.405501842498779 221.94882202148438
Loss :  1.7020632028579712 4.519625663757324 227.68333435058594
Loss :  1.7611665725708008 4.614688396453857 232.49559020996094
Loss :  1.7497133016586304 4.474662780761719 225.48284912109375
Loss :  1.5736472606658936 4.888145923614502 245.98095703125
Loss :  1.7031203508377075 4.484010219573975 225.9036407470703
Loss :  1.6237791776657104 4.387422561645508 220.99490356445312
Loss :  1.6288518905639648 4.468533515930176 225.05552673339844
Loss :  1.558592438697815 4.464460849761963 224.78163146972656
Loss :  1.583840250968933 4.73358154296875 238.26292419433594
Loss :  1.635831594467163 4.5005269050598145 226.6621856689453
Loss :  1.7381223440170288 4.458131790161133 224.64471435546875
  batch 60 loss: 1.7381223440170288, 4.458131790161133, 224.64471435546875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.6486623287200928 4.551820278167725 229.23968505859375
Loss :  1.5626277923583984 4.37818717956543 220.47198486328125
Loss :  1.5571728944778442 4.406074523925781 221.86090087890625
Loss :  1.6308125257492065 4.420619010925293 222.66175842285156
Loss :  1.5696628093719482 4.156293869018555 209.3843536376953
Loss :  2.7005832195281982 4.392667293548584 222.3339385986328
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 5], device='cuda:0')
Loss :  2.783444881439209 4.444181442260742 224.99252319335938
Loss :  2.507359027862549 4.300374984741211 217.52610778808594
Loss :  2.423893690109253 4.274775981903076 216.16268920898438
Total LOSS train 225.7560314471905 valid 220.25381469726562
CE LOSS train 1.6229412629054143 valid 0.6059734225273132
Contrastive LOSS train 4.4826618194580075 valid 1.068693995475769
EPOCH 22:
Loss :  1.7029246091842651 4.488227844238281 226.11431884765625
Loss :  1.634064793586731 4.413751125335693 222.32162475585938
Loss :  1.604988694190979 4.3567914962768555 219.44456481933594
Loss :  1.6169241666793823 4.444726467132568 223.85325622558594
Loss :  1.652753472328186 4.387487888336182 221.0271453857422
Loss :  1.6041535139083862 4.468308448791504 225.0195770263672
Loss :  1.6624979972839355 4.462905406951904 224.80775451660156
Loss :  1.5623359680175781 4.336751937866211 218.39993286132812
Loss :  1.5670826435089111 4.5822978019714355 230.6819610595703
Loss :  1.645122766494751 4.388092994689941 221.04977416992188
Loss :  1.5462706089019775 4.4754815101623535 225.32034301757812
Loss :  1.57673180103302 4.4538187980651855 224.26766967773438
Loss :  1.556857705116272 4.490720272064209 226.09286499023438
Loss :  1.5998479127883911 4.581184387207031 230.65907287597656
Loss :  1.6475036144256592 4.347156047821045 219.00531005859375
Loss :  1.6096022129058838 4.564959526062012 229.85757446289062
Loss :  1.5102009773254395 4.5057244300842285 226.7964324951172
Loss :  1.5587055683135986 4.476871967315674 225.4022979736328
Loss :  1.5127395391464233 4.3827080726623535 220.6481475830078
Loss :  1.595302939414978 4.382214546203613 220.70603942871094
  batch 20 loss: 1.595302939414978, 4.382214546203613, 220.70603942871094
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.553507685661316 4.3977179527282715 221.4394073486328
Loss :  1.4864534139633179 4.42997932434082 222.98541259765625
Loss :  1.5111457109451294 4.523794651031494 227.70086669921875
Loss :  1.585349678993225 4.470925807952881 225.1316375732422
Loss :  1.6058144569396973 4.614356517791748 232.32363891601562
Loss :  1.5234676599502563 4.373702526092529 220.20858764648438
Loss :  1.5321111679077148 4.7303056716918945 238.04739379882812
Loss :  1.5373283624649048 4.399502277374268 221.51243591308594
Loss :  1.4189084768295288 4.491186618804932 225.97824096679688
Loss :  1.589901328086853 4.496078968048096 226.39385986328125
Loss :  1.4036179780960083 4.251784324645996 213.9928436279297
Loss :  1.55592679977417 4.48909330368042 226.01058959960938
Loss :  1.5241477489471436 4.364508628845215 219.7495880126953
Loss :  1.518997311592102 4.478734493255615 225.45571899414062
Loss :  1.4299486875534058 4.486454486846924 225.75267028808594
Loss :  1.4691052436828613 4.430426597595215 222.9904327392578
Loss :  1.4938570261001587 4.451411724090576 224.06443786621094
Loss :  1.6035014390945435 4.2947492599487305 216.34095764160156
Loss :  1.5976333618164062 4.313809394836426 217.28811645507812
Loss :  1.6155966520309448 4.40368127822876 221.79966735839844
  batch 40 loss: 1.6155966520309448, 4.40368127822876, 221.79966735839844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5391309261322021 4.275214672088623 215.2998504638672
Loss :  1.501962661743164 4.4108381271362305 222.0438690185547
Loss :  1.5154391527175903 4.457225322723389 224.376708984375
Loss :  1.5450515747070312 4.424717903137207 222.78094482421875
Loss :  1.4770596027374268 4.283201217651367 215.6371307373047
Loss :  1.5400396585464478 4.428576946258545 222.96888732910156
Loss :  1.5879539251327515 4.377595901489258 220.46774291992188
Loss :  1.5305631160736084 4.360037803649902 219.53245544433594
Loss :  1.6304559707641602 4.486680030822754 225.96446228027344
Loss :  1.5406734943389893 4.483764171600342 225.7288818359375
Loss :  1.6293178796768188 4.477267265319824 225.49267578125
Loss :  1.5764120817184448 4.441996097564697 223.6762237548828
Loss :  1.5412789583206177 4.391382694244385 221.11041259765625
Loss :  1.5898092985153198 4.49550724029541 226.36517333984375
Loss :  1.556923270225525 4.356287002563477 219.37127685546875
Loss :  1.6584712266921997 4.534784317016602 228.39768981933594
Loss :  1.5152733325958252 4.498269557952881 226.4287567138672
Loss :  1.5266427993774414 4.531048774719238 228.07908630371094
Loss :  1.5283983945846558 4.423544883728027 222.7056427001953
Loss :  1.6434361934661865 4.475964069366455 225.44163513183594
  batch 60 loss: 1.6434361934661865, 4.475964069366455, 225.44163513183594
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.4893982410430908 4.3526482582092285 219.12181091308594
Loss :  1.5826208591461182 4.429021835327148 223.03372192382812
Loss :  1.525162696838379 4.389702796936035 221.0102996826172
Loss :  1.5209295749664307 4.347467422485352 218.89430236816406
Loss :  1.456750512123108 4.078412055969238 205.3773651123047
Loss :  1.9931538105010986 4.462128639221191 225.09957885742188
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.9679418802261353 4.357704162597656 219.8531494140625
Loss :  1.985300898551941 4.2818732261657715 216.07896423339844
Loss :  2.091764450073242 4.306718349456787 217.4276885986328
Total LOSS train 223.16844881497897 valid 219.6148452758789
CE LOSS train 1.5580321091871996 valid 0.5229411125183105
Contrastive LOSS train 4.432208325312688 valid 1.0766795873641968
EPOCH 23:
Loss :  1.5974174737930298 4.313365459442139 217.26568603515625
Loss :  1.6137639284133911 4.506155490875244 226.92153930664062
Loss :  1.5434366464614868 4.395499229431152 221.3184051513672
Loss :  1.5514880418777466 4.29583215713501 216.3430938720703
Loss :  1.5834777355194092 4.300837516784668 216.62535095214844
Loss :  1.4843471050262451 4.379333019256592 220.45098876953125
Loss :  1.5935314893722534 4.365891933441162 219.8881378173828
Loss :  1.528641700744629 4.218554973602295 212.45639038085938
Loss :  1.5233992338180542 4.451727867126465 224.1097869873047
Loss :  1.6087721586227417 4.388852596282959 221.0513916015625
Loss :  1.4798920154571533 4.64893913269043 233.92684936523438
Loss :  1.4768542051315308 4.378706932067871 220.41220092773438
Loss :  1.4890426397323608 4.337678909301758 218.37298583984375
Loss :  1.5807833671569824 4.514711856842041 227.31637573242188
Loss :  1.6375986337661743 4.451828956604004 224.2290496826172
Loss :  1.5969488620758057 4.504815578460693 226.83773803710938
Loss :  1.4750778675079346 4.458254337310791 224.38780212402344
Loss :  1.5331753492355347 4.489204406738281 225.99339294433594
Loss :  1.49875009059906 4.156174659729004 209.30747985839844
Loss :  1.5868418216705322 4.167490482330322 209.96136474609375
  batch 20 loss: 1.5868418216705322, 4.167490482330322, 209.96136474609375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.53801691532135 4.12328577041626 207.70230102539062
Loss :  1.4982446432113647 4.433704853057861 223.18348693847656
Loss :  1.4938890933990479 4.606668949127197 231.8273468017578
Loss :  1.5450409650802612 4.458474636077881 224.46878051757812
Loss :  1.565048098564148 4.497860431671143 226.45806884765625
Loss :  1.4920364618301392 4.399895191192627 221.48680114746094
Loss :  1.4575549364089966 4.572243690490723 230.06973266601562
Loss :  1.5508877038955688 4.3559770584106445 219.34974670410156
Loss :  1.3947253227233887 4.432112216949463 223.00033569335938
Loss :  1.5750513076782227 4.405488014221191 221.84945678710938
Loss :  1.396969199180603 4.383875370025635 220.5907440185547
Loss :  1.55901300907135 4.3169026374816895 217.40414428710938
Loss :  1.4811022281646729 4.239875316619873 213.47486877441406
Loss :  1.4856432676315308 4.08230447769165 205.60086059570312
Loss :  1.3880709409713745 4.119614601135254 207.36880493164062
Loss :  1.4249308109283447 3.964282989501953 199.63906860351562
Loss :  1.4118996858596802 3.7712013721466064 189.9719696044922
Loss :  1.5787291526794434 3.7018682956695557 186.67214965820312
Loss :  1.520833969116211 3.748446226119995 188.94314575195312
Loss :  1.5646711587905884 4.108850002288818 207.00717163085938
  batch 40 loss: 1.5646711587905884, 4.108850002288818, 207.00717163085938
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.4359441995620728 4.023780822753906 202.62498474121094
Loss :  1.4186289310455322 4.578186988830566 230.32797241210938
Loss :  1.4585679769515991 4.734135627746582 238.1653594970703
Loss :  1.4893286228179932 4.99405574798584 251.19212341308594
Loss :  1.4455491304397583 4.432636737823486 223.077392578125
Loss :  1.4721746444702148 4.309729099273682 216.9586181640625
Loss :  1.5641131401062012 4.462423801422119 224.685302734375
Loss :  1.4722827672958374 4.4715166091918945 225.04812622070312
Loss :  1.6687452793121338 4.398971080780029 221.61729431152344
Loss :  1.4718908071517944 4.335530757904053 218.24842834472656
Loss :  1.5868587493896484 4.0526275634765625 204.21823120117188
Loss :  1.5535340309143066 3.733107328414917 188.20889282226562
Loss :  1.4709774255752563 4.317093849182129 217.32566833496094
Loss :  1.6017554998397827 4.577436923980713 230.47360229492188
Loss :  1.4644582271575928 4.539353370666504 228.43212890625
Loss :  1.567030906677246 4.392355918884277 221.18482971191406
Loss :  1.4589033126831055 4.4827423095703125 225.5960235595703
Loss :  1.3968206644058228 4.486488342285156 225.7212371826172
Loss :  1.443881869316101 4.471755504608154 225.03164672851562
Loss :  1.7597335577011108 4.529938697814941 228.2566680908203
  batch 60 loss: 1.7597335577011108, 4.529938697814941, 228.2566680908203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.4065024852752686 4.441054821014404 223.45924377441406
Loss :  1.5363469123840332 4.326864242553711 217.8795623779297
Loss :  1.471834659576416 4.188209533691406 210.88230895996094
Loss :  1.3876984119415283 4.432989597320557 223.03717041015625
Loss :  1.3168308734893799 4.106292247772217 206.63143920898438
Loss :  1.413816213607788 4.358658313751221 219.34674072265625
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.4587363004684448 4.385950565338135 220.7562713623047
Loss :  1.4490454196929932 4.230195045471191 212.95880126953125
Loss :  1.425777792930603 4.20667839050293 211.75970458984375
Total LOSS train 218.6389115553636 valid 216.20537948608398
CE LOSS train 1.5111691126456628 valid 0.35644444823265076
Contrastive LOSS train 4.342554848010724 valid 1.0516695976257324
EPOCH 24:
Loss :  1.5023313760757446 4.30575704574585 216.79019165039062
Loss :  1.5861064195632935 4.352555274963379 219.2138671875
Loss :  1.4800753593444824 4.326517105102539 217.80592346191406
Loss :  1.5132454633712769 4.4193291664123535 222.47970581054688
Loss :  1.5894865989685059 4.263734817504883 214.77622985839844
Loss :  1.4141501188278198 4.200575351715088 211.4429168701172
Loss :  1.5436257123947144 4.39301061630249 221.19415283203125
Loss :  1.4662415981292725 4.259173393249512 214.42491149902344
Loss :  1.4411122798919678 4.286684036254883 215.7753143310547
Loss :  1.5974944829940796 4.10757303237915 206.97613525390625
Loss :  1.4624618291854858 4.219422817230225 212.43360900878906
Loss :  1.456059217453003 4.276749134063721 215.29351806640625
Loss :  1.4772359132766724 4.1186652183532715 207.41049194335938
Loss :  1.4465563297271729 4.0206499099731445 202.47906494140625
Loss :  1.697555422782898 4.150587558746338 209.2269287109375
Loss :  1.7656887769699097 4.225604057312012 213.04588317871094
Loss :  1.492432951927185 4.162460803985596 209.615478515625
Loss :  1.604403018951416 4.475046634674072 225.3567352294922
Loss :  1.4254907369613647 4.286962985992432 215.7736358642578
Loss :  1.689643383026123 4.404351711273193 221.9072265625
  batch 20 loss: 1.689643383026123, 4.404351711273193, 221.9072265625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.503830909729004 4.501039028167725 226.5557861328125
Loss :  1.4450920820236206 4.463041305541992 224.59716796875
Loss :  1.558422327041626 4.426136493682861 222.86524963378906
Loss :  1.6005048751831055 4.293179035186768 216.25946044921875
Loss :  1.705878734588623 4.32690954208374 218.0513458251953
Loss :  1.5609169006347656 4.238526821136475 213.4872589111328
Loss :  1.5801126956939697 4.453279972076416 224.24411010742188
Loss :  1.5409103631973267 4.361112117767334 219.5965118408203
Loss :  1.3834993839263916 4.4268012046813965 222.7235565185547
Loss :  1.673975944519043 4.336761474609375 218.51205444335938
Loss :  1.3994227647781372 4.3202033042907715 217.4095916748047
Loss :  1.637904167175293 4.368441104888916 220.05996704101562
Loss :  1.5696816444396973 4.238238334655762 213.48159790039062
Loss :  1.5705454349517822 4.365900039672852 219.86553955078125
Loss :  1.4075711965560913 4.5880513191223145 230.81015014648438
Loss :  1.499903917312622 4.343943119049072 218.6970672607422
Loss :  1.499312400817871 4.27902889251709 215.4507598876953
Loss :  1.748518943786621 4.266392707824707 215.0681610107422
Loss :  1.6846123933792114 4.212852478027344 212.32723999023438
Loss :  1.7547876834869385 4.254391193389893 214.47434997558594
  batch 40 loss: 1.7547876834869385, 4.254391193389893, 214.47434997558594
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4, 5], device='cuda:0')
Loss :  1.5815452337265015 4.1189093589782715 207.52700805664062
Loss :  1.6125799417495728 4.358457565307617 219.53546142578125
Loss :  1.5392968654632568 4.492370128631592 226.1577911376953
Loss :  1.6079890727996826 4.452430725097656 224.22952270507812
Loss :  1.5321952104568481 4.240267753601074 213.5455780029297
Loss :  1.6114856004714966 4.511648654937744 227.19390869140625
Loss :  1.7362048625946045 4.271093845367432 215.2908935546875
Loss :  1.565297245979309 4.463626384735107 224.74661254882812
Loss :  1.7954323291778564 4.098385810852051 206.7147216796875
Loss :  1.5966969728469849 4.7301459312438965 238.10398864746094
Loss :  1.7023682594299316 4.612033367156982 232.3040313720703
Loss :  1.7054649591445923 4.336832046508789 218.54705810546875
Loss :  1.6406159400939941 4.246640682220459 213.97264099121094
Loss :  1.790144681930542 4.412233352661133 222.4018096923828
Loss :  1.566209077835083 4.602487087249756 231.69056701660156
Loss :  1.7862788438796997 4.4604339599609375 224.8079833984375
Loss :  1.60799241065979 4.401778221130371 221.6968994140625
Loss :  1.509814739227295 4.258468151092529 214.43321228027344
Loss :  1.6153953075408936 4.166357040405273 209.93325805664062
Loss :  1.7888514995574951 4.109516143798828 207.2646484375
  batch 60 loss: 1.7888514995574951, 4.109516143798828, 207.2646484375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.5175158977508545 4.548731803894043 228.9541015625
Loss :  1.6271052360534668 4.3922648429870605 221.24034118652344
Loss :  1.572784185409546 4.57166051864624 230.1558074951172
Loss :  1.5032169818878174 4.488921642303467 225.9492950439453
Loss :  1.3922605514526367 4.216263294219971 212.20542907714844
Loss :  1.5349313020706177 4.400894641876221 221.5796661376953
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5927339792251587 4.439764499664307 223.58094787597656
Loss :  1.5545165538787842 4.2570905685424805 214.40904235839844
Loss :  1.7143521308898926 4.130207061767578 208.22470092773438
Total LOSS train 218.40863717886117 valid 216.94858932495117
CE LOSS train 1.5766391332332905 valid 0.42858803272247314
Contrastive LOSS train 4.3366399765014645 valid 1.0325517654418945
EPOCH 25:
Loss :  1.5717543363571167 4.326582431793213 217.90086364746094
Loss :  1.7094391584396362 4.4863433837890625 226.026611328125
Loss :  1.5972718000411987 4.623504638671875 232.7725067138672
Loss :  1.5874032974243164 4.5432915687561035 228.75198364257812
Loss :  1.7114824056625366 4.299077033996582 216.66534423828125
Loss :  1.5353482961654663 4.258805751800537 214.47564697265625
Loss :  1.6354345083236694 4.274895668029785 215.38021850585938
Loss :  1.5185773372650146 4.367708206176758 219.9039764404297
Loss :  1.486425757408142 4.531687259674072 228.07078552246094
Loss :  1.5796321630477905 4.38178825378418 220.66905212402344
Loss :  1.4725862741470337 4.475181579589844 225.23165893554688
Loss :  1.4333511590957642 4.4711456298828125 224.99063110351562
Loss :  1.4396084547042847 4.5171637535095215 227.29779052734375
Loss :  1.3401367664337158 4.374276638031006 220.05397033691406
Loss :  1.6749489307403564 4.17501163482666 210.42552185058594
Loss :  1.6412553787231445 4.3796539306640625 220.6239471435547
Loss :  1.384097695350647 4.434572696685791 223.11273193359375
Loss :  1.5442076921463013 4.4128642082214355 222.18740844726562
Loss :  1.4128808975219727 4.4033050537109375 221.57814025878906
Loss :  1.6178745031356812 4.3844499588012695 220.8403778076172
  batch 20 loss: 1.6178745031356812, 4.3844499588012695, 220.8403778076172
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.5013785362243652 4.383910655975342 220.6968994140625
Loss :  1.3936423063278198 4.366872310638428 219.7372589111328
Loss :  1.4276635646820068 4.475008487701416 225.17808532714844
Loss :  1.4883325099945068 4.377788066864014 220.3777313232422
Loss :  1.5231194496154785 4.5995774269104 231.50198364257812
Loss :  1.4270395040512085 4.437119960784912 223.2830352783203
Loss :  1.4930506944656372 4.559588432312012 229.47247314453125
Loss :  1.3898210525512695 4.353135585784912 219.04660034179688
Loss :  1.2585842609405518 4.564478397369385 229.48251342773438
Loss :  1.641956090927124 4.496724605560303 226.47817993164062
Loss :  1.2336914539337158 4.585944652557373 230.53091430664062
Loss :  1.4392510652542114 4.593791484832764 231.1288299560547
Loss :  1.426205039024353 4.445225238800049 223.68746948242188
Loss :  1.35332190990448 4.698165416717529 236.26158142089844
Loss :  1.324144959449768 4.510979652404785 226.8731231689453
Loss :  1.3618988990783691 4.42593240737915 222.65850830078125
Loss :  1.3467315435409546 4.513268947601318 227.0101776123047
Loss :  1.4579344987869263 4.240800857543945 213.4979705810547
Loss :  1.4620647430419922 4.115187168121338 207.22142028808594
Loss :  1.5135698318481445 4.104520797729492 206.73960876464844
  batch 40 loss: 1.5135698318481445, 4.104520797729492, 206.73960876464844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 5], device='cuda:0')
Loss :  1.386420726776123 4.101053714752197 206.43910217285156
Loss :  1.438241720199585 4.258273124694824 214.35189819335938
Loss :  1.369722604751587 4.380768775939941 220.4081573486328
Loss :  1.4124168157577515 4.604227542877197 231.62379455566406
Loss :  1.3363640308380127 4.3720927238464355 219.9409942626953
Loss :  1.4001662731170654 4.182509899139404 210.52565002441406
Loss :  1.5208582878112793 4.179238796234131 210.48280334472656
Loss :  1.3851488828659058 4.465491771697998 224.65972900390625
Loss :  1.509283423423767 4.20151948928833 211.5852508544922
Loss :  1.4446699619293213 4.433218479156494 223.1055908203125
Loss :  1.4472182989120483 4.405512809753418 221.7228546142578
Loss :  1.429512858390808 4.237107753753662 213.284912109375
Loss :  1.4077701568603516 4.408483982086182 221.83197021484375
Loss :  1.5652648210525513 4.471749305725098 225.15272521972656
Loss :  1.396251916885376 4.5084943771362305 226.8209686279297
Loss :  1.5272173881530762 4.393911838531494 221.22280883789062
Loss :  1.3834158182144165 4.402295112609863 221.49818420410156
Loss :  1.3794176578521729 4.372859477996826 220.02239990234375
Loss :  1.409946084022522 4.368536472320557 219.83676147460938
Loss :  1.5252845287322998 4.419797420501709 222.51515197753906
  batch 60 loss: 1.5252845287322998, 4.419797420501709, 222.51515197753906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.347502589225769 4.279575824737549 215.3262939453125
Loss :  1.395402193069458 4.4156341552734375 222.17710876464844
Loss :  1.4211938381195068 4.308756351470947 216.8590087890625
Loss :  1.3229645490646362 4.46644401550293 224.64517211914062
Loss :  1.2522369623184204 4.0743231773376465 204.96839904785156
Loss :  1.614268183708191 4.427502155303955 222.9893798828125
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.7930033206939697 4.472419738769531 225.4139862060547
Loss :  1.4966161251068115 4.366979598999023 219.84559631347656
Loss :  1.3478373289108276 4.164692401885986 209.58245849609375
Total LOSS train 221.21278803898738 valid 219.45785522460938
CE LOSS train 1.4580155556018537 valid 0.3369593322277069
Contrastive LOSS train 4.3950954804053675 valid 1.0411731004714966
EPOCH 26:
Loss :  1.5185909271240234 4.555112361907959 229.27420043945312
Loss :  1.5169371366500854 4.492092609405518 226.12156677246094
Loss :  1.410253643989563 4.431448936462402 222.98269653320312
Loss :  1.4610211849212646 4.443145275115967 223.61827087402344
Loss :  1.478969931602478 4.436767101287842 223.31732177734375
Loss :  1.4156500101089478 4.385632514953613 220.69728088378906
Loss :  1.5254895687103271 4.397738456726074 221.4123992919922
Loss :  1.410196304321289 4.43682861328125 223.2516326904297
Loss :  1.4346944093704224 4.323770999908447 217.6232452392578
Loss :  1.4363281726837158 4.096705436706543 206.27159118652344
Loss :  1.3412864208221436 4.187093257904053 210.69595336914062
Loss :  1.3459522724151611 4.417954921722412 222.2436981201172
Loss :  1.3194141387939453 4.407402515411377 221.68954467773438
Loss :  1.3451242446899414 4.385406970977783 220.615478515625
Loss :  1.4559855461120605 4.331010341644287 218.00650024414062
Loss :  1.4873028993606567 4.417466163635254 222.36061096191406
Loss :  1.371648907661438 4.266719341278076 214.70761108398438
Loss :  1.4557067155838013 4.05031156539917 203.97128295898438
Loss :  1.3362356424331665 4.33717679977417 218.19508361816406
Loss :  1.5190231800079346 4.024899005889893 202.76397705078125
  batch 20 loss: 1.5190231800079346, 4.024899005889893, 202.76397705078125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.4099820852279663 3.936183452606201 198.2191619873047
Loss :  1.3591889142990112 3.8163325786590576 192.1758270263672
Loss :  1.4263075590133667 3.8562216758728027 194.2373809814453
Loss :  1.426599383354187 4.119438648223877 207.39854431152344
Loss :  1.4683343172073364 4.257364749908447 214.33657836914062
Loss :  1.4156410694122314 3.964036464691162 199.61746215820312
Loss :  1.4671558141708374 4.223047256469727 212.61952209472656
Loss :  1.4791474342346191 4.001882553100586 201.57327270507812
Loss :  1.3726928234100342 4.242765426635742 213.51097106933594
Loss :  1.566629409790039 3.9252398014068604 197.82862854003906
Loss :  1.3489030599594116 4.067255973815918 204.71170043945312
Loss :  1.5095319747924805 3.9856693744659424 200.79299926757812
Loss :  1.4598444700241089 3.9489171504974365 198.90570068359375
Loss :  1.5014042854309082 4.099276542663574 206.46522521972656
Loss :  1.3707362413406372 3.905555009841919 196.64849853515625
Loss :  1.4579811096191406 4.050897598266602 204.0028533935547
Loss :  1.4281957149505615 3.9798195362091064 200.41917419433594
Loss :  1.5821725130081177 3.61849308013916 182.50682067871094
Loss :  1.5879559516906738 3.9986867904663086 201.5222930908203
Loss :  1.6020325422286987 3.9517385959625244 199.18896484375
  batch 40 loss: 1.6020325422286987, 3.9517385959625244, 199.18896484375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.5111116170883179 3.8877575397491455 195.89898681640625
Loss :  1.5177913904190063 3.8456029891967773 193.79794311523438
Loss :  1.4302386045455933 4.10082483291626 206.4714813232422
Loss :  1.4810272455215454 4.148180961608887 208.89007568359375
Loss :  1.4375699758529663 3.867785692214966 194.8268585205078
Loss :  1.5287153720855713 3.916389226913452 197.34817504882812
Loss :  1.5863580703735352 3.9035370349884033 196.76321411132812
Loss :  1.4180262088775635 4.235557556152344 213.19590759277344
Loss :  1.5677136182785034 4.0266594886779785 202.90069580078125
Loss :  1.4318712949752808 4.204106330871582 211.63719177246094
Loss :  1.4292916059494019 4.52911901473999 227.88523864746094
Loss :  1.6430273056030273 4.398593425750732 221.57269287109375
Loss :  1.3983874320983887 4.4379096031188965 223.2938690185547
Loss :  1.5088759660720825 4.599639892578125 231.49087524414062
Loss :  1.3663161993026733 4.681921482086182 235.46238708496094
Loss :  1.5130099058151245 4.436100959777832 223.3180694580078
Loss :  1.4697920083999634 4.470430374145508 224.99130249023438
Loss :  1.4117757081985474 4.455622673034668 224.19290161132812
Loss :  1.4481948614120483 4.517202854156494 227.30833435058594
Loss :  1.5328131914138794 4.447072982788086 223.88645935058594
  batch 60 loss: 1.5328131914138794, 4.447072982788086, 223.88645935058594
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.317339301109314 4.537902355194092 228.2124481201172
Loss :  1.3507615327835083 4.544044017791748 228.55296325683594
Loss :  1.435901165008545 4.464961051940918 224.68394470214844
Loss :  1.3705718517303467 4.510550498962402 226.89810180664062
Loss :  1.3143950700759888 4.251914024353027 213.91009521484375
Loss :  1.3201006650924683 4.393129348754883 220.9765625
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.3411301374435425 4.396454811096191 221.1638641357422
Loss :  1.3376154899597168 4.3432135581970215 218.498291015625
Loss :  1.2653454542160034 4.166280746459961 209.5793914794922
Total LOSS train 212.36756521371694 valid 217.55452728271484
CE LOSS train 1.4499557605156532 valid 0.31633636355400085
Contrastive LOSS train 4.218352189430823 valid 1.0415701866149902
EPOCH 27:
Loss :  1.3844690322875977 4.068845748901367 204.82676696777344
Loss :  1.511399745941162 4.332472324371338 218.135009765625
Loss :  1.4180165529251099 3.8870317935943604 195.7696075439453
Loss :  1.4208297729492188 4.119781970977783 207.40994262695312
Loss :  1.4577112197875977 4.067183494567871 204.81689453125
Loss :  1.3884258270263672 4.327325820922852 217.7547149658203
Loss :  1.4775676727294922 4.483556747436523 225.65541076660156
Loss :  1.395612120628357 4.154433250427246 209.11727905273438
Loss :  1.374132513999939 4.116604328155518 207.204345703125
Loss :  1.4816139936447144 3.7902140617370605 190.9923095703125
Loss :  1.3650490045547485 4.0309600830078125 202.91305541992188
Loss :  1.3809916973114014 3.9181084632873535 197.2864227294922
Loss :  1.377179741859436 4.03012228012085 202.88330078125
Loss :  1.3491358757019043 4.073955059051514 205.04689025878906
Loss :  1.4710683822631836 4.00615119934082 201.77862548828125
Loss :  1.5110701322555542 4.081077575683594 205.56494140625
Loss :  1.3366115093231201 4.042592525482178 203.4662322998047
Loss :  1.3982216119766235 3.955444574356079 199.1704559326172
Loss :  1.3204621076583862 4.030086040496826 202.82476806640625
Loss :  1.5029078722000122 4.073251247406006 205.1654815673828
  batch 20 loss: 1.5029078722000122, 4.073251247406006, 205.1654815673828
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.3528945446014404 4.08198881149292 205.45233154296875
Loss :  1.301130771636963 4.062296390533447 204.41595458984375
Loss :  1.342951774597168 4.128498554229736 207.76788330078125
Loss :  1.3370274305343628 3.724275827407837 187.55081176757812
Loss :  1.4137898683547974 4.269921779632568 214.90988159179688
Loss :  1.3310719728469849 4.140684127807617 208.3652801513672
Loss :  1.3760371208190918 4.330517768859863 217.9019317626953
Loss :  1.3878756761550903 4.115976810455322 207.1867218017578
Loss :  1.3447805643081665 4.0824294090271 205.4662628173828
Loss :  1.4533321857452393 4.01081657409668 201.99417114257812
Loss :  1.3210006952285767 4.191872596740723 210.9146270751953
Loss :  1.4825571775436401 3.9695096015930176 199.95803833007812
Loss :  1.4448167085647583 3.768756628036499 189.88265991210938
Loss :  1.5321968793869019 3.9428117275238037 198.6727752685547
Loss :  1.3958995342254639 4.000483989715576 201.42010498046875
Loss :  1.4838701486587524 3.9770188331604004 200.33480834960938
Loss :  1.4085214138031006 4.0380144119262695 203.3092498779297
Loss :  1.5887231826782227 3.8535072803497314 194.26409912109375
Loss :  1.5782527923583984 4.33784818649292 218.4706573486328
Loss :  1.50820791721344 4.563703536987305 229.69338989257812
  batch 40 loss: 1.50820791721344, 4.563703536987305, 229.69338989257812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.471398949623108 4.252246856689453 214.083740234375
Loss :  1.4228105545043945 3.875481128692627 195.19686889648438
Loss :  1.4690040349960327 3.824549674987793 192.6964874267578
Loss :  1.4663079977035522 4.087151050567627 205.82386779785156
Loss :  1.487907886505127 4.057345390319824 204.3551788330078
Loss :  1.5477564334869385 4.300677299499512 216.5816192626953
Loss :  1.607679843902588 4.063979625701904 204.80665588378906
Loss :  1.5118218660354614 4.069511890411377 204.9874267578125
Loss :  1.6099169254302979 3.612271785736084 182.2235107421875
Loss :  1.517810344696045 4.131289005279541 208.08226013183594
Loss :  1.560542106628418 3.9989497661590576 201.50804138183594
Loss :  1.5789694786071777 3.952420949935913 199.20001220703125
Loss :  1.501261591911316 3.979139566421509 200.458251953125
Loss :  1.5900287628173828 4.1147027015686035 207.32516479492188
Loss :  1.5534838438034058 4.023603916168213 202.73367309570312
Loss :  1.6729227304458618 4.053743839263916 204.36012268066406
Loss :  1.567771553993225 4.328752040863037 218.00537109375
Loss :  1.438726782798767 3.9737279415130615 200.1251220703125
Loss :  1.5148099660873413 4.1334309577941895 208.18637084960938
Loss :  1.5971095561981201 4.042010307312012 203.69761657714844
  batch 60 loss: 1.5971095561981201, 4.042010307312012, 203.69761657714844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.4379526376724243 3.9964396953582764 201.25994873046875
Loss :  1.5012164115905762 4.106566905975342 206.82955932617188
Loss :  1.465687870979309 4.192336559295654 211.08250427246094
Loss :  1.4572031497955322 3.856396436691284 194.2770233154297
Loss :  1.4328804016113281 3.5404837131500244 178.45706176757812
Loss :  1.3635969161987305 4.404840469360352 221.60562133789062
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.39993155002594 4.4716925621032715 224.98455810546875
Loss :  1.3884364366531372 4.368683815002441 219.8226318359375
Loss :  1.3599425554275513 4.23620080947876 213.16998291015625
Total LOSS train 204.33934701772836 valid 219.89569854736328
CE LOSS train 1.4567753296632033 valid 0.3399856388568878
Contrastive LOSS train 4.057651391396155 valid 1.05905020236969
EPOCH 28:
Loss :  1.5465420484542847 3.7337584495544434 188.23446655273438
Loss :  1.5259480476379395 4.249849319458008 214.01841735839844
Loss :  1.4895572662353516 4.038671970367432 203.42315673828125
Loss :  1.5403157472610474 4.17150354385376 210.11549377441406
Loss :  1.557166576385498 4.062902927398682 204.70230102539062
Loss :  1.5287104845046997 4.100614547729492 206.5594482421875
Loss :  1.6019960641860962 4.076527118682861 205.4283447265625
Loss :  1.5489078760147095 3.962616443634033 199.6797332763672
Loss :  1.5794793367385864 3.972011089324951 200.18003845214844
Loss :  1.6655381917953491 4.142519950866699 208.79153442382812
Loss :  1.5749411582946777 4.304507732391357 216.80032348632812
Loss :  1.5355867147445679 4.255153179168701 214.29324340820312
Loss :  1.5372754335403442 4.056803226470947 204.37744140625
Loss :  1.539453148841858 4.090903282165527 206.08462524414062
Loss :  1.6307119131088257 3.8358356952667236 193.4224853515625
Loss :  1.6316931247711182 4.187835693359375 211.0234832763672
Loss :  1.530651569366455 3.6315531730651855 183.10830688476562
Loss :  1.575304388999939 3.7902657985687256 191.08859252929688
Loss :  1.5409643650054932 4.151705265045166 209.12623596191406
Loss :  1.6661055088043213 4.086942195892334 206.01321411132812
  batch 20 loss: 1.6661055088043213, 4.086942195892334, 206.01321411132812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.6228662729263306 4.089076042175293 206.07666015625
Loss :  1.563294529914856 4.093029975891113 206.2147979736328
Loss :  1.6190800666809082 4.348175048828125 219.02783203125
Loss :  1.6148918867111206 4.147751331329346 209.00247192382812
Loss :  1.698759913444519 4.568339824676514 230.11575317382812
Loss :  1.60499107837677 4.525202751159668 227.86512756347656
Loss :  1.6366727352142334 4.548896312713623 229.08148193359375
Loss :  1.597252368927002 4.461792469024658 224.6868896484375
Loss :  1.5305255651474 4.316327095031738 217.34689331054688
Loss :  1.6588994264602661 4.577006816864014 230.50924682617188
Loss :  1.5226664543151855 4.559493064880371 229.497314453125
Loss :  1.618589997291565 4.360654354095459 219.65130615234375
Loss :  1.6298315525054932 4.228619575500488 213.06082153320312
Loss :  1.642098307609558 4.299567699432373 216.6204833984375
Loss :  1.5521795749664307 4.291928768157959 216.14862060546875
Loss :  1.5922924280166626 4.1346235275268555 208.32345581054688
Loss :  1.572126030921936 4.223675727844238 212.75592041015625
Loss :  1.6717718839645386 4.162012100219727 209.77236938476562
Loss :  1.6816149950027466 4.104136943817139 206.88845825195312
Loss :  1.6902741193771362 3.9943642616271973 201.4084930419922
  batch 40 loss: 1.6902741193771362, 3.9943642616271973, 201.4084930419922
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.6345611810684204 4.305854320526123 216.92727661132812
Loss :  1.6227325201034546 3.7256593704223633 187.90570068359375
Loss :  1.6088522672653198 4.00185489654541 201.70159912109375
Loss :  1.5958964824676514 4.07847261428833 205.51953125
Loss :  1.5851322412490845 3.7691526412963867 190.04275512695312
Loss :  1.6276354789733887 4.23170804977417 213.21304321289062
Loss :  1.6857267618179321 3.8731162548065186 195.34153747558594
Loss :  1.618876576423645 4.141698837280273 208.70382690429688
Loss :  1.6988742351531982 4.082951068878174 205.84642028808594
Loss :  1.6134825944900513 4.093151569366455 206.27105712890625
Loss :  1.6524122953414917 3.8932955265045166 196.3171844482422
Loss :  1.650590181350708 4.187682628631592 211.0347137451172
Loss :  1.6086887121200562 3.942298650741577 198.72361755371094
Loss :  1.6737375259399414 3.7656984329223633 189.9586639404297
Loss :  1.5706219673156738 4.124624729156494 207.80184936523438
Loss :  1.6890774965286255 4.04972505569458 204.17532348632812
Loss :  1.6193469762802124 3.88991641998291 196.11517333984375
Loss :  1.5939443111419678 4.230745315551758 213.13121032714844
Loss :  1.6874821186065674 4.455137252807617 224.44435119628906
Loss :  1.7208640575408936 4.363032341003418 219.8724822998047
  batch 60 loss: 1.7208640575408936, 4.363032341003418, 219.8724822998047
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.6068493127822876 4.25972318649292 214.59300231933594
Loss :  1.6138428449630737 4.083463191986084 205.78700256347656
Loss :  1.6410915851593018 4.088804721832275 206.08132934570312
Loss :  1.603639841079712 4.229809761047363 213.09413146972656
Loss :  1.532633662223816 3.7829208374023438 190.67868041992188
Loss :  1.499677062034607 4.4058661460876465 221.79298400878906
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.496599793434143 4.2964935302734375 216.32127380371094
Loss :  1.5005946159362793 4.283556938171387 215.67843627929688
Loss :  1.5320149660110474 4.002070903778076 201.63555908203125
Total LOSS train 208.21241149902343 valid 213.85706329345703
CE LOSS train 1.60652494430542 valid 0.38300374150276184
Contrastive LOSS train 4.132117722584651 valid 1.000517725944519
EPOCH 29:
Loss :  1.574535846710205 4.356031894683838 219.37612915039062
Loss :  1.6052558422088623 4.332351207733154 218.22280883789062
Loss :  1.5617049932479858 4.325544357299805 217.83892822265625
Loss :  1.6037251949310303 4.265164375305176 214.86195373535156
Loss :  1.6089614629745483 4.031247615814209 203.1713409423828
Loss :  1.5800881385803223 4.047619819641113 203.96109008789062
Loss :  1.6201106309890747 4.403268337249756 221.78353881835938
Loss :  1.5884019136428833 3.8601317405700684 194.59500122070312
Loss :  1.602487325668335 4.065880298614502 204.89651489257812
Loss :  1.5716731548309326 3.709200143814087 187.03167724609375
Loss :  1.5746150016784668 3.863386392593384 194.7439422607422
Loss :  1.606929898262024 4.149901866912842 209.10202026367188
Loss :  1.5364080667495728 3.880795478820801 195.57618713378906
Loss :  1.580387830734253 4.1673197746276855 209.9463653564453
Loss :  1.6543304920196533 4.35630989074707 219.46981811523438
Loss :  1.6787683963775635 4.410308837890625 222.1942138671875
Loss :  1.5998297929763794 4.318800449371338 217.5398406982422
Loss :  1.7326300144195557 3.7563135623931885 189.54830932617188
Loss :  1.7296477556228638 3.58882212638855 181.17074584960938
Loss :  1.840421438217163 4.112148761749268 207.44786071777344
  batch 20 loss: 1.840421438217163, 4.112148761749268, 207.44786071777344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.789098858833313 3.8431649208068848 193.9473419189453
Loss :  1.7567269802093506 3.9221689701080322 197.86517333984375
Loss :  1.7886319160461426 4.017886638641357 202.68296813964844
Loss :  1.7820430994033813 4.083714962005615 205.96778869628906
Loss :  1.8298516273498535 4.4476704597473145 224.21337890625
Loss :  1.639509916305542 4.4089837074279785 222.0886993408203
Loss :  1.7342803478240967 4.351510047912598 219.30978393554688
Loss :  1.700867772102356 4.4036545753479 221.8835906982422
Loss :  1.6806319952011108 4.242044925689697 213.7828826904297
Loss :  1.780580759048462 4.407208442687988 222.14100646972656
Loss :  1.7116948366165161 4.196104049682617 211.51690673828125
Loss :  1.8000620603561401 4.097713947296143 206.68576049804688
Loss :  1.6327059268951416 4.20889139175415 212.0772705078125
Loss :  1.6282068490982056 4.23150634765625 213.20352172851562
Loss :  1.6110426187515259 4.100579738616943 206.64002990722656
Loss :  1.671677827835083 4.051148891448975 204.2291259765625
Loss :  1.6374422311782837 3.9871773719787598 200.99630737304688
Loss :  1.7346128225326538 4.190480709075928 211.25865173339844
Loss :  1.721336007118225 4.021020412445068 202.77235412597656
Loss :  1.724665880203247 4.081236362457275 205.78648376464844
  batch 40 loss: 1.724665880203247, 4.081236362457275, 205.78648376464844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.648883581161499 3.9179329872131348 197.5455322265625
Loss :  1.629347324371338 3.807708978652954 192.01480102539062
Loss :  1.6244417428970337 4.027344703674316 202.99166870117188
Loss :  1.680668830871582 4.102748870849609 206.818115234375
Loss :  1.6646678447723389 3.847834587097168 194.056396484375
Loss :  1.6492871046066284 4.03623104095459 203.46084594726562
Loss :  1.6939082145690918 4.049788475036621 204.18333435058594
Loss :  1.6432018280029297 3.837693214416504 193.52786254882812
Loss :  1.6867913007736206 3.9668781757354736 200.03070068359375
Loss :  1.6941027641296387 4.4265642166137695 223.02232360839844
Loss :  1.682844877243042 4.394454479217529 221.4055633544922
Loss :  1.6623591184616089 4.394330024719238 221.3788604736328
Loss :  1.6435703039169312 4.293067455291748 216.29693603515625
Loss :  1.7593706846237183 4.155176639556885 209.51820373535156
Loss :  1.7461576461791992 4.304929256439209 216.99261474609375
Loss :  1.7280738353729248 3.8248579502105713 192.97097778320312
Loss :  1.6836662292480469 3.823772668838501 192.87229919433594
Loss :  1.65653395652771 4.24393367767334 213.8532257080078
Loss :  1.682555079460144 4.261491775512695 214.75714111328125
Loss :  1.7309924364089966 3.8141071796417236 192.43634033203125
  batch 60 loss: 1.7309924364089966, 3.8141071796417236, 192.43634033203125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.6293370723724365 3.7362992763519287 188.4442901611328
Loss :  1.6378397941589355 3.7699790000915527 190.13677978515625
Loss :  1.646864414215088 4.003365993499756 201.81517028808594
Loss :  1.587336778640747 4.09250545501709 206.21261596679688
Loss :  1.5412603616714478 3.6088342666625977 181.98297119140625
Loss :  1.537324070930481 4.109684467315674 207.02154541015625
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.5479421615600586 4.127485752105713 207.92222595214844
Loss :  1.5594730377197266 4.076588153839111 205.38888549804688
Loss :  1.599213719367981 3.843986749649048 193.79855346679688
Total LOSS train 206.31158282940206 valid 203.5328025817871
CE LOSS train 1.6683176407447229 valid 0.39980342984199524
Contrastive LOSS train 4.09286529467656 valid 0.960996687412262
Saved best model. Old loss 206.1764907836914 and new best loss 203.5328025817871
EPOCH 30:
Loss :  1.6201612949371338 3.8184869289398193 192.5445098876953
Loss :  1.658413052558899 4.027344226837158 203.025634765625
Loss :  1.6055467128753662 4.0161213874816895 202.41162109375
Loss :  1.6249765157699585 4.060944080352783 204.67218017578125
Loss :  1.684388279914856 3.9910261631011963 201.23570251464844
Loss :  1.6728326082229614 4.037562847137451 203.5509796142578
Loss :  1.7427643537521362 4.138096332550049 208.6475830078125
Loss :  1.70177161693573 3.927455425262451 198.0745391845703
Loss :  1.6460399627685547 3.851919412612915 194.2420196533203
Loss :  1.6464916467666626 4.075737953186035 205.43338012695312
Loss :  1.635437250137329 3.803805351257324 191.82569885253906
Loss :  1.6625652313232422 4.0788984298706055 205.60748291015625
Loss :  1.6212871074676514 4.332683086395264 218.2554473876953
Loss :  1.6734204292297363 4.092483043670654 206.2975616455078
Loss :  1.7687475681304932 4.182584285736084 210.89796447753906
Loss :  1.767377257347107 4.35573673248291 219.55421447753906
Loss :  1.7329362630844116 4.399884223937988 221.72715759277344
Loss :  1.7509829998016357 4.310088157653809 217.25538635253906
Loss :  1.7157753705978394 4.018810272216797 202.6562957763672
Loss :  1.8483766317367554 4.237121105194092 213.7044219970703
  batch 20 loss: 1.8483766317367554, 4.237121105194092, 213.7044219970703
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.7826429605484009 4.040298938751221 203.79759216308594
Loss :  1.7620943784713745 4.069756984710693 205.2499542236328
Loss :  1.782658576965332 4.38265323638916 220.91531372070312
Loss :  1.7930225133895874 4.2582807540893555 214.70706176757812
Loss :  1.6881095170974731 3.841400623321533 193.75814819335938
Loss :  1.6839444637298584 3.7960622310638428 191.487060546875
Loss :  1.7195965051651 4.0562520027160645 204.53219604492188
Loss :  1.728365421295166 3.887378215789795 196.09727478027344
Loss :  1.64394211769104 4.355658054351807 219.4268341064453
Loss :  1.7306214570999146 3.798114776611328 191.6363525390625
Loss :  1.6410824060440063 4.0538010597229 204.3311309814453
Loss :  1.8039374351501465 3.9809720516204834 200.8525390625
Loss :  1.7911009788513184 4.169867038726807 210.28445434570312
Loss :  1.8218932151794434 4.048099994659424 204.22689819335938
Loss :  1.7099292278289795 4.594727516174316 231.44630432128906
Loss :  1.8011118173599243 4.317383289337158 217.6702880859375
Loss :  1.7715082168579102 4.255972862243652 214.57015991210938
Loss :  1.8483744859695435 4.021193504333496 202.90805053710938
Loss :  1.8608672618865967 3.710139036178589 187.36782836914062
Loss :  1.8384231328964233 3.8528261184692383 194.479736328125
  batch 40 loss: 1.8384231328964233, 3.8528261184692383, 194.479736328125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.763408899307251 4.310126781463623 217.26974487304688
Loss :  1.8200445175170898 4.009869575500488 202.3135223388672
Loss :  1.8116166591644287 4.142043113708496 208.9137725830078
Loss :  1.770288348197937 4.096237659454346 206.58218383789062
Loss :  1.7881474494934082 3.870769739151001 195.32662963867188
Loss :  1.7575417757034302 3.9373719692230225 198.62612915039062
Loss :  1.7799937725067139 4.357247352600098 219.64236450195312
Loss :  1.679805040359497 4.234650135040283 213.41232299804688
Loss :  1.7532786130905151 4.0856475830078125 206.03565979003906
Loss :  1.6223810911178589 4.25966739654541 214.60574340820312
Loss :  1.6173560619354248 4.25734281539917 214.4844970703125
Loss :  1.7193349599838257 4.331315040588379 218.2850799560547
Loss :  1.6648520231246948 4.351267337799072 219.2282257080078
Loss :  1.6509863138198853 4.156662464141846 209.48411560058594
Loss :  1.5731267929077148 4.144420623779297 208.79415893554688
Loss :  1.6998873949050903 4.184942245483398 210.94700622558594
Loss :  1.6546920537948608 3.9345996379852295 198.38467407226562
Loss :  1.6009914875030518 4.07575798034668 205.38890075683594
Loss :  1.6864818334579468 4.228579044342041 213.1154327392578
Loss :  1.7416232824325562 4.139093399047852 208.6962890625
  batch 60 loss: 1.7416232824325562, 4.139093399047852, 208.6962890625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.6531988382339478 4.166290760040283 209.96774291992188
Loss :  1.6822155714035034 4.014631271362305 202.41378784179688
Loss :  1.6810981035232544 3.956263303756714 199.4942626953125
Loss :  1.6501222848892212 4.0218682289123535 202.7435302734375
Loss :  1.592477798461914 3.6846580505371094 185.8253936767578
Loss :  1.3911552429199219 4.3085455894470215 216.8184356689453
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.4391001462936401 4.364469051361084 219.6625518798828
Loss :  1.4077731370925903 4.212706565856934 212.04310607910156
Loss :  1.4527592658996582 4.252152919769287 214.06040954589844
Total LOSS train 206.4822481595553 valid 215.64612579345703
CE LOSS train 1.713822603225708 valid 0.36318981647491455
Contrastive LOSS train 4.095368480682373 valid 1.0630382299423218
EPOCH 31:
Loss :  1.6811364889144897 3.792264699935913 191.29437255859375
Loss :  1.731012225151062 4.402878284454346 221.87493896484375
Loss :  1.6602833271026611 4.028400421142578 203.08029174804688
Loss :  1.708425521850586 4.215958595275879 212.50634765625
Loss :  1.7634629011154175 3.9090094566345215 197.21392822265625
Loss :  1.7847658395767212 4.126675128936768 208.11851501464844
Loss :  1.7595387697219849 3.9789323806762695 200.70616149902344
Loss :  1.7438247203826904 3.8401379585266113 193.75071716308594
Loss :  1.7864567041397095 3.8452439308166504 194.04864501953125
Loss :  1.7294747829437256 3.884476661682129 195.95330810546875
Loss :  1.8009498119354248 4.0249342918396 203.04766845703125
Loss :  1.8568192720413208 4.181711196899414 210.9423828125
Loss :  1.8068920373916626 4.24702787399292 214.1582794189453
Loss :  1.974295973777771 4.019507884979248 202.94967651367188
Loss :  2.0059056282043457 3.8734028339385986 195.67604064941406
Loss :  2.010662317276001 3.9984805583953857 201.9346923828125
Loss :  2.0244991779327393 4.026282787322998 203.33863830566406
Loss :  2.063194751739502 3.8636724948883057 195.246826171875
Loss :  2.0419373512268066 3.801173686981201 192.10061645507812
Loss :  2.038734197616577 3.73061203956604 188.5693359375
  batch 20 loss: 2.038734197616577, 3.73061203956604, 188.5693359375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.9991363286972046 4.275362491607666 215.7672576904297
Loss :  1.7779276371002197 4.255473613739014 214.55160522460938
Loss :  1.9293955564498901 4.415497303009033 222.7042694091797
Loss :  1.8140836954116821 4.399366855621338 221.7824249267578
Loss :  1.765363097190857 4.076601982116699 205.595458984375
Loss :  1.7136685848236084 3.854825735092163 194.4549560546875
Loss :  1.7559372186660767 4.205217361450195 212.0167999267578
Loss :  1.720144271850586 3.841473340988159 193.79380798339844
Loss :  1.785097360610962 3.9168004989624023 197.6251220703125
Loss :  1.750625729560852 4.401581764221191 221.8297119140625
Loss :  1.786417841911316 4.629313945770264 233.2521209716797
Loss :  1.8462506532669067 4.626358509063721 233.1641845703125
Loss :  1.866000771522522 4.484771251678467 226.10455322265625
Loss :  1.9449008703231812 4.481168270111084 226.00331115722656
Loss :  1.8810381889343262 4.476675510406494 225.71481323242188
Loss :  1.8780968189239502 4.427072525024414 223.23171997070312
Loss :  1.8637140989303589 4.213492393493652 212.538330078125
Loss :  1.8022090196609497 4.37238883972168 220.42166137695312
Loss :  1.902548909187317 4.318214416503906 217.81326293945312
Loss :  1.9181829690933228 4.107550621032715 207.29571533203125
  batch 40 loss: 1.9181829690933228, 4.107550621032715, 207.29571533203125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.9296766519546509 3.7676429748535156 190.3118133544922
Loss :  1.9529742002487183 4.17546272277832 210.72610473632812
Loss :  1.876234769821167 4.104971885681152 207.1248321533203
Loss :  1.8272285461425781 4.193167686462402 211.48561096191406
Loss :  1.798448920249939 4.197247505187988 211.66082763671875
Loss :  1.7915407419204712 4.06380033493042 204.98155212402344
Loss :  1.803443431854248 4.069118499755859 205.25936889648438
Loss :  1.8128374814987183 4.276679039001465 215.64678955078125
Loss :  1.8658989667892456 4.489619731903076 226.34689331054688
Loss :  1.8816168308258057 4.5689897537231445 230.33111572265625
Loss :  1.8241621255874634 3.933333158493042 198.49081420898438
Loss :  1.8564138412475586 4.071777820587158 205.4453125
Loss :  1.858621597290039 3.968963384628296 200.30679321289062
Loss :  1.8107038736343384 3.973609685897827 200.49118041992188
Loss :  1.8515346050262451 3.964656352996826 200.0843505859375
Loss :  1.8334323167800903 3.9532511234283447 199.49598693847656
Loss :  1.8494529724121094 3.9081740379333496 197.25816345214844
Loss :  1.8285225629806519 3.9655022621154785 200.1036376953125
Loss :  1.8820877075195312 4.115999698638916 207.68206787109375
Loss :  1.867749810218811 4.192346572875977 211.48507690429688
  batch 60 loss: 1.867749810218811, 4.192346572875977, 211.48507690429688
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.8639181852340698 4.247819423675537 214.25489807128906
Loss :  1.865539789199829 4.311659812927246 217.4485321044922
Loss :  1.8619545698165894 4.213772773742676 212.55059814453125
Loss :  1.840645432472229 4.3135905265808105 217.52017211914062
Loss :  1.862858533859253 4.16126823425293 209.92626953125
Loss :  1.648115873336792 4.2916483879089355 216.23052978515625
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.686517357826233 4.130102157592773 208.19163513183594
Loss :  1.6678882837295532 4.204139709472656 211.8748779296875
Loss :  1.6659483909606934 4.139613151550293 208.6466064453125
Total LOSS train 208.59371126615085 valid 211.23591232299805
CE LOSS train 1.8457001521037175 valid 0.41648709774017334
Contrastive LOSS train 4.134960236916175 valid 1.0349032878875732
EPOCH 32:
Loss :  1.8158563375473022 3.7762351036071777 190.6276092529297
Loss :  1.8283799886703491 4.125629901885986 208.10987854003906
Loss :  1.8301278352737427 4.450830936431885 224.37167358398438
Loss :  1.8214497566223145 4.3839874267578125 221.0208282470703
Loss :  1.8705476522445679 4.43253231048584 223.49716186523438
Loss :  1.7604390382766724 4.29094123840332 216.3074951171875
Loss :  1.8615508079528809 4.568989276885986 230.31101989746094
Loss :  1.8230607509613037 4.368177890777588 220.23194885253906
Loss :  1.8920493125915527 4.205176830291748 212.15087890625
Loss :  1.812244176864624 4.36842155456543 220.2333221435547
Loss :  1.8011760711669922 4.378523826599121 220.7273712158203
Loss :  1.8224706649780273 4.46812629699707 225.22877502441406
Loss :  1.7891685962677002 4.416337013244629 222.60601806640625
Loss :  1.836344599723816 4.186794757843018 211.17608642578125
Loss :  1.7834045886993408 4.203134059906006 211.9401092529297
Loss :  1.7930415868759155 4.271464824676514 215.3662872314453
Loss :  1.7963972091674805 4.399722099304199 221.78250122070312
Loss :  1.7806503772735596 4.297142505645752 216.63778686523438
Loss :  1.7954492568969727 4.059345245361328 204.76271057128906
Loss :  1.8415602445602417 4.22218132019043 212.95062255859375
  batch 20 loss: 1.8415602445602417, 4.22218132019043, 212.95062255859375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.7971436977386475 4.243931770324707 213.99374389648438
Loss :  1.7508935928344727 4.4065632820129395 222.07907104492188
Loss :  1.7156621217727661 4.215594291687012 212.49537658691406
Loss :  1.7675303220748901 4.319869041442871 217.760986328125
Loss :  1.816313624382019 4.493224620819092 226.4775390625
Loss :  1.82968008518219 4.422994136810303 222.97938537597656
Loss :  1.8085085153579712 4.668900489807129 235.25352478027344
Loss :  1.877799391746521 4.322604179382324 218.00799560546875
Loss :  1.84839928150177 4.455010890960693 224.5989532470703
Loss :  1.8556452989578247 4.364075183868408 220.05941772460938
Loss :  1.7873141765594482 4.61856746673584 232.71568298339844
Loss :  1.809579610824585 4.478663921356201 225.74278259277344
Loss :  1.794762134552002 4.445650577545166 224.07730102539062
Loss :  1.8245465755462646 4.482181072235107 225.93359375
Loss :  1.8371555805206299 4.500895977020264 226.8819580078125
Loss :  1.8537346124649048 4.652861595153809 234.49681091308594
Loss :  1.846891164779663 4.550534725189209 229.37362670898438
Loss :  1.8626903295516968 4.320674419403076 217.8964080810547
Loss :  1.8445253372192383 4.214200496673584 212.55455017089844
Loss :  1.882502555847168 4.582656383514404 231.01531982421875
  batch 40 loss: 1.882502555847168, 4.582656383514404, 231.01531982421875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.8781051635742188 4.485299110412598 226.14306640625
Loss :  1.8825585842132568 4.44596004486084 224.18055725097656
Loss :  1.8583335876464844 4.374812602996826 220.59896850585938
Loss :  1.8885071277618408 4.353163719177246 219.54669189453125
Loss :  1.8800023794174194 4.373629570007324 220.5614776611328
Loss :  1.9159079790115356 4.565670490264893 230.19943237304688
Loss :  1.8664517402648926 4.441159248352051 223.92442321777344
Loss :  1.8653813600540161 4.591925144195557 231.46163940429688
Loss :  1.90304696559906 4.339711666107178 218.8886260986328
Loss :  1.9258291721343994 4.398285865783691 221.84011840820312
Loss :  1.8298649787902832 4.474081993103027 225.53396606445312
Loss :  1.8387926816940308 4.157887935638428 209.73318481445312
Loss :  1.9183349609375 4.34170389175415 219.00352478027344
Loss :  1.8510640859603882 4.335226535797119 218.6123809814453
Loss :  1.8856983184814453 4.3241705894470215 218.09422302246094
Loss :  1.8948020935058594 4.025273323059082 203.15847778320312
Loss :  1.938644528388977 4.27726411819458 215.80184936523438
Loss :  1.9377355575561523 4.361798286437988 220.02764892578125
Loss :  1.8882672786712646 4.3526434898376465 219.5204315185547
Loss :  1.9577574729919434 4.210639476776123 212.48973083496094
  batch 60 loss: 1.9577574729919434, 4.210639476776123, 212.48973083496094
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.9505209922790527 4.1959357261657715 211.74729919433594
Loss :  1.9131540060043335 4.198484897613525 211.83738708496094
Loss :  1.8971211910247803 4.0543131828308105 204.61277770996094
Loss :  1.9165540933609009 4.338082790374756 218.82069396972656
Loss :  1.9656175374984741 3.8744120597839355 195.68621826171875
Loss :  1.8272686004638672 4.467883110046387 225.22142028808594
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.7839356660842896 4.330986976623535 218.33328247070312
Loss :  1.716128945350647 4.671456336975098 235.2889404296875
Loss :  1.6687666177749634 4.302022933959961 216.76991271972656
Total LOSS train 219.1762908935547 valid 223.90338897705078
CE LOSS train 1.8494569338285005 valid 0.41719165444374084
Contrastive LOSS train 4.346536687704233 valid 1.0755057334899902
EPOCH 33:
Loss :  1.8760260343551636 4.840685844421387 243.91030883789062
Loss :  1.8636095523834229 4.418153762817383 222.77130126953125
Loss :  1.8978458642959595 4.37599515914917 220.69760131835938
Loss :  1.8898537158966064 4.307936191558838 217.28665161132812
Loss :  1.8776750564575195 4.265952110290527 215.17527770996094
Loss :  1.8947455883026123 4.320783615112305 217.93392944335938
Loss :  1.840254306793213 4.504197597503662 227.05014038085938
Loss :  1.8648449182510376 4.437446594238281 223.73716735839844
Loss :  1.8841657638549805 4.320931911468506 217.93077087402344
Loss :  1.90084969997406 4.433806896209717 223.5911865234375
Loss :  1.8835281133651733 4.164559841156006 210.1115264892578
Loss :  1.8833839893341064 4.549427509307861 229.35475158691406
Loss :  1.9183849096298218 4.392669200897217 221.5518341064453
Loss :  1.9081124067306519 4.510810375213623 227.44862365722656
Loss :  1.8292262554168701 4.729704856872559 238.31446838378906
Loss :  1.8206924200057983 4.154141902923584 209.5277862548828
Loss :  1.8210803270339966 3.9562337398529053 199.6327667236328
Loss :  1.8276495933532715 3.687431573867798 186.19923400878906
Loss :  1.8409920930862427 3.7361485958099365 188.64842224121094
Loss :  1.8086668252944946 3.7927939891815186 191.4483642578125
  batch 20 loss: 1.8086668252944946, 3.7927939891815186, 191.4483642578125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.8540118932724 3.6884121894836426 186.27462768554688
Loss :  1.844404697418213 4.108656406402588 207.2772216796875
Loss :  1.852211594581604 3.801994562149048 191.9519500732422
Loss :  1.8931884765625 4.337907791137695 218.78857421875
Loss :  1.883284091949463 4.488931655883789 226.32986450195312
Loss :  1.8089632987976074 4.082842826843262 205.9510955810547
Loss :  1.8687669038772583 4.277491092681885 215.7433319091797
Loss :  1.8429869413375854 3.935358762741089 198.61093139648438
Loss :  1.8335504531860352 4.191435813903809 211.4053497314453
Loss :  1.796458125114441 3.822951316833496 192.94403076171875
Loss :  1.827060580253601 3.9959559440612793 201.62484741210938
Loss :  1.9187783002853394 4.700104713439941 236.92401123046875
Loss :  1.937419056892395 4.060751438140869 204.97499084472656
Loss :  1.949805736541748 4.329816818237305 218.44064331054688
Loss :  1.9447821378707886 4.246365070343018 214.26303100585938
Loss :  1.9922981262207031 3.921558141708374 198.07020568847656
Loss :  2.0050508975982666 4.332040309906006 218.60707092285156
Loss :  2.005218029022217 4.126081943511963 208.30931091308594
Loss :  1.9643776416778564 4.312309741973877 217.57986450195312
Loss :  2.01399302482605 4.265303611755371 215.2791748046875
  batch 40 loss: 2.01399302482605, 4.265303611755371, 215.2791748046875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9333053827285767 4.530187606811523 228.44268798828125
Loss :  1.9078212976455688 4.104594707489014 207.13755798339844
Loss :  1.9881607294082642 4.005685329437256 202.27243041992188
Loss :  1.9933110475540161 4.151144504547119 209.550537109375
Loss :  2.044773578643799 4.10675048828125 207.38229370117188
Loss :  2.0100252628326416 4.228005409240723 213.41029357910156
Loss :  2.017324447631836 3.8114678859710693 192.59071350097656
Loss :  2.0286436080932617 4.2894792556762695 216.5026092529297
Loss :  2.021184206008911 4.193676948547363 211.7050323486328
Loss :  2.055331230163574 3.8936986923217773 196.74026489257812
Loss :  2.038745164871216 4.097087383270264 206.8931121826172
Loss :  2.045543909072876 4.223626613616943 213.2268829345703
Loss :  2.02085542678833 4.261172294616699 215.0794677734375
Loss :  1.920069694519043 4.301147937774658 216.97747802734375
Loss :  1.9209843873977661 4.42719030380249 223.28050231933594
Loss :  1.9018404483795166 4.321290969848633 217.9663848876953
Loss :  1.9670436382293701 4.204861164093018 212.21009826660156
Loss :  1.9582829475402832 4.652127265930176 234.5646514892578
Loss :  1.9730253219604492 3.9889073371887207 201.41839599609375
Loss :  1.9179860353469849 4.226701736450195 213.25306701660156
  batch 60 loss: 1.9179860353469849, 4.226701736450195, 213.25306701660156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
Loss :  2.000802993774414 4.362081527709961 220.10488891601562
Loss :  1.9633067846298218 3.9865505695343018 201.29083251953125
Loss :  2.013906955718994 4.336885452270508 218.85816955566406
Loss :  2.023080587387085 3.9316070079803467 198.6034393310547
Loss :  2.0355796813964844 4.0655035972595215 205.31076049804688
Loss :  2.010734796524048 4.301412105560303 217.0813446044922
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
Loss :  2.030595541000366 4.341633319854736 219.1122589111328
Loss :  2.0250437259674072 4.440781593322754 224.06411743164062
Loss :  1.9833645820617676 3.9851558208465576 201.2411651611328
Total LOSS train 212.40684298001804 valid 215.3747215270996
CE LOSS train 1.9241404955203716 valid 0.4958411455154419
Contrastive LOSS train 4.2096540524409365 valid 0.9962889552116394
EPOCH 34:
Loss :  1.9878188371658325 4.498504638671875 226.91305541992188
Loss :  1.9631173610687256 4.537415027618408 228.83387756347656
Loss :  2.0069003105163574 4.072896480560303 205.65171813964844
Loss :  2.021695375442505 4.373297691345215 220.68658447265625
Loss :  1.985011100769043 4.0727128982543945 205.62066650390625
Loss :  2.0451860427856445 4.2775774002075195 215.92405700683594
Loss :  2.0586466789245605 4.681029319763184 236.110107421875
Loss :  2.0443315505981445 4.346377849578857 219.36322021484375
Loss :  2.099583387374878 4.149411201477051 209.5701446533203
Loss :  2.0743072032928467 4.328428268432617 218.4957275390625
Loss :  2.0949745178222656 4.1800432205200195 211.09713745117188
Loss :  2.0783212184906006 4.184237003326416 211.29017639160156
Loss :  2.1028430461883545 4.2870707511901855 216.4563751220703
Loss :  2.0703141689300537 4.174227237701416 210.78167724609375
Loss :  2.0396382808685303 4.122124195098877 208.14585876464844
Loss :  2.051375150680542 4.201110363006592 212.1068878173828
Loss :  2.0793516635894775 3.9602158069610596 200.0901336669922
Loss :  2.080824851989746 4.077513694763184 205.95651245117188
Loss :  2.0944395065307617 4.268591403961182 215.5240020751953
Loss :  2.0150654315948486 3.919104814529419 197.97030639648438
  batch 20 loss: 2.0150654315948486, 3.919104814529419, 197.97030639648438
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3, 4, 5], device='cuda:0')
Loss :  2.0284018516540527 4.27664852142334 215.86082458496094
Loss :  2.1026735305786133 4.257959842681885 215.00067138671875
Loss :  2.084096908569336 4.482957363128662 226.23196411132812
Loss :  2.0297255516052246 4.183276653289795 211.1935577392578
Loss :  1.9835293292999268 4.448149681091309 224.39102172851562
Loss :  2.0783138275146484 4.048863410949707 204.521484375
Loss :  2.0548460483551025 4.415213108062744 222.81549072265625
Loss :  2.076688766479492 4.304010391235352 217.27720642089844
Loss :  2.1449763774871826 4.380058288574219 221.14788818359375
Loss :  1.9633487462997437 4.407386779785156 222.3326873779297
Loss :  2.110153913497925 4.286408424377441 216.43057250976562
Loss :  2.04756236076355 4.748040676116943 239.44960021972656
Loss :  2.0465149879455566 4.49232816696167 226.6629180908203
Loss :  2.116478443145752 4.466508865356445 225.44192504882812
Loss :  2.1170997619628906 4.371671676635742 220.70068359375
Loss :  2.118098497390747 4.203361988067627 212.2862091064453
Loss :  2.1039671897888184 4.279514789581299 216.0797119140625
Loss :  2.0246381759643555 4.059036731719971 204.9764862060547
Loss :  1.9944761991500854 4.078429222106934 205.9159393310547
Loss :  1.9543098211288452 4.448808193206787 224.3947296142578
  batch 40 loss: 1.9543098211288452, 4.448808193206787, 224.3947296142578
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3, 5], device='cuda:0')
Loss :  2.0256266593933105 4.427234172821045 223.3873291015625
Loss :  2.07494854927063 4.228796005249023 213.51475524902344
Loss :  2.1051695346832275 4.141384124755859 209.17437744140625
Loss :  2.031019687652588 4.3213114738464355 218.09658813476562
Loss :  2.053091287612915 3.958817720413208 199.9939727783203
Loss :  1.9666988849639893 4.232334613800049 213.58343505859375
Loss :  1.922729730606079 4.070082664489746 205.42686462402344
Loss :  2.028716802597046 4.010387420654297 202.548095703125
Loss :  1.9242087602615356 4.618155479431152 232.8319854736328
Loss :  2.0367746353149414 4.5271477699279785 228.3941650390625
Loss :  1.987436294555664 4.434659481048584 223.7204132080078
Loss :  1.9329901933670044 4.312410354614258 217.55349731445312
Loss :  1.9629055261611938 4.203675270080566 212.14666748046875
Loss :  2.018249273300171 4.19040060043335 211.5382843017578
Loss :  2.0414187908172607 4.322902202606201 218.1865234375
Loss :  1.8958237171173096 4.572476387023926 230.5196533203125
Loss :  1.994965672492981 4.758886337280273 239.9392852783203
Loss :  1.8742012977600098 4.325503349304199 218.1493682861328
Loss :  1.9445652961730957 4.483895301818848 226.1393280029297
Loss :  1.8769280910491943 4.5621418952941895 229.98402404785156
  batch 60 loss: 1.8769280910491943, 4.5621418952941895, 229.98402404785156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.9767526388168335 4.593320369720459 231.64276123046875
Loss :  1.9279931783676147 4.305237770080566 217.18988037109375
Loss :  1.9226833581924438 4.516922473907471 227.76881408691406
Loss :  1.9954372644424438 4.308065891265869 217.3987274169922
Loss :  1.98533034324646 3.871568441390991 195.56375122070312
Loss :  1.4928621053695679 4.450192451477051 224.0024871826172
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3, 5], device='cuda:0')
Loss :  1.498586893081665 4.48132848739624 225.56500244140625
Loss :  1.4949125051498413 4.335042476654053 218.24703979492188
Loss :  1.458036184310913 4.305392742156982 216.72767639160156
Total LOSS train 217.13988224909855 valid 221.13555145263672
CE LOSS train 2.0258509452526385 valid 0.36450904607772827
Contrastive LOSS train 4.302280609424297 valid 1.0763481855392456
EPOCH 35:
Loss :  1.8451755046844482 4.107366561889648 207.2135009765625
Loss :  1.908797025680542 4.470068454742432 225.41221618652344
Loss :  1.9946638345718384 4.4716949462890625 225.57940673828125
Loss :  1.9209280014038086 4.265636920928955 215.20277404785156
Loss :  1.9252278804779053 3.8068673610687256 192.2686004638672
Loss :  1.920060157775879 4.230257511138916 213.4329376220703
Loss :  1.835439920425415 4.456970691680908 224.68397521972656
Loss :  1.850102424621582 4.243093490600586 214.00477600097656
Loss :  1.8751312494277954 4.182746410369873 211.012451171875
Loss :  1.8380259275436401 4.361366271972656 219.90634155273438
Loss :  1.8072806596755981 4.499137878417969 226.76417541503906
Loss :  1.8099290132522583 4.444207668304443 224.02032470703125
Loss :  1.7847740650177002 4.473721027374268 225.4708251953125
Loss :  1.8007599115371704 4.450475692749023 224.32455444335938
Loss :  1.7785817384719849 4.578341007232666 230.6956329345703
Loss :  1.7946351766586304 4.466133117675781 225.10128784179688
Loss :  1.8020719289779663 4.412625789642334 222.43336486816406
Loss :  1.7681925296783447 4.412038326263428 222.37010192871094
Loss :  1.7663387060165405 4.329413890838623 218.23703002929688
Loss :  1.7758121490478516 4.420213222503662 222.78648376464844
  batch 20 loss: 1.7758121490478516, 4.420213222503662, 222.78648376464844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 5], device='cuda:0')
Loss :  1.756161093711853 4.372590065002441 220.3856658935547
Loss :  1.7643836736679077 4.393331527709961 221.43096923828125
Loss :  1.7425059080123901 4.591049671173096 231.2949981689453
Loss :  1.73147714138031 4.454500675201416 224.45651245117188
Loss :  1.7339667081832886 4.485194206237793 225.99366760253906
Loss :  1.7199801206588745 4.16109037399292 209.77450561523438
Loss :  1.735129714012146 4.4295477867126465 223.21250915527344
Loss :  1.721846103668213 4.340912342071533 218.76747131347656
Loss :  1.7119873762130737 4.418442726135254 222.63412475585938
Loss :  1.7251651287078857 4.344518184661865 218.95106506347656
Loss :  1.7137751579284668 4.560302734375 229.72891235351562
Loss :  1.7198705673217773 4.594779968261719 231.4588623046875
Loss :  1.7066346406936646 4.358064651489258 219.60986328125
Loss :  1.7064976692199707 4.379903793334961 220.70169067382812
Loss :  1.715872883796692 4.309844017028809 217.20806884765625
Loss :  1.728631854057312 4.3614821434021 219.80274963378906
Loss :  1.7259827852249146 4.364570140838623 219.95448303222656
Loss :  1.748988389968872 4.297341346740723 216.61605834960938
Loss :  1.756103277206421 4.4310383796691895 223.3080291748047
Loss :  1.76708984375 4.371732711791992 220.35372924804688
  batch 40 loss: 1.76708984375, 4.371732711791992, 220.35372924804688
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 5], device='cuda:0')
Loss :  1.76292085647583 4.4914374351501465 226.3347930908203
Loss :  1.7698925733566284 4.45747184753418 224.64349365234375
Loss :  1.749172329902649 4.3414788246154785 218.8231201171875
Loss :  1.763370156288147 4.363058090209961 219.91627502441406
Loss :  1.7469877004623413 4.105731010437012 207.03353881835938
Loss :  1.7748161554336548 4.206485271453857 212.0990753173828
Loss :  1.796699047088623 4.353466987609863 219.4700469970703
Loss :  1.7595449686050415 4.323716640472412 217.9453887939453
Loss :  1.8132612705230713 4.431207180023193 223.37362670898438
Loss :  1.7638838291168213 4.468994617462158 225.213623046875
Loss :  1.7840431928634644 4.521822929382324 227.87518310546875
Loss :  1.7797691822052002 4.392730712890625 221.4163055419922
Loss :  1.7710033655166626 4.369691848754883 220.25558471679688
Loss :  1.7993355989456177 4.3817830085754395 220.88848876953125
Loss :  1.7673944234848022 4.4112443923950195 222.32962036132812
Loss :  1.8189966678619385 4.115297794342041 207.58389282226562
Loss :  1.7823060750961304 4.179110050201416 210.73780822753906
Loss :  1.7625311613082886 4.515783786773682 227.5517120361328
Loss :  1.7853957414627075 4.498533248901367 226.71206665039062
Loss :  1.8049540519714355 4.333260536193848 218.4679718017578
  batch 60 loss: 1.8049540519714355, 4.333260536193848, 218.4679718017578
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 5], device='cuda:0')
Loss :  1.759871482849121 4.363799571990967 219.94984436035156
Loss :  1.7887998819351196 4.44866943359375 224.22227478027344
Loss :  1.7258479595184326 4.577685832977295 230.61013793945312
Loss :  1.7511104345321655 4.426431179046631 223.0726776123047
Loss :  1.7365604639053345 4.123980522155762 207.93557739257812
Loss :  1.9728245735168457 4.48141622543335 226.04364013671875
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 4, 5], device='cuda:0')
Loss :  1.928418755531311 4.389484882354736 221.4026641845703
Loss :  1.9312081336975098 4.325930595397949 218.2277374267578
Loss :  1.9876132011413574 4.054192543029785 204.69723510742188
Total LOSS train 220.47733576847958 valid 217.5928192138672
CE LOSS train 1.7808068678929256 valid 0.49690330028533936
Contrastive LOSS train 4.373930560625516 valid 1.0135481357574463
EPOCH 36:
Loss :  1.7504416704177856 4.212327003479004 212.36679077148438
Loss :  1.772812843322754 4.528723239898682 228.20896911621094
Loss :  1.7467621564865112 4.382706165313721 220.882080078125
Loss :  1.7565536499023438 4.5553412437438965 229.52362060546875
Loss :  1.7352385520935059 4.211904048919678 212.3304443359375
Loss :  1.721645712852478 4.402096748352051 221.8264923095703
Loss :  1.7625610828399658 4.549008369445801 229.21298217773438
Loss :  1.7082757949829102 4.3376545906066895 218.5910186767578
Loss :  1.7038177251815796 4.348216533660889 219.11463928222656
Loss :  1.714261770248413 4.316838264465332 217.55618286132812
Loss :  1.6745471954345703 4.431875228881836 223.268310546875
Loss :  1.686174750328064 4.492316722869873 226.302001953125
Loss :  1.6409506797790527 4.487715244293213 226.02670288085938
Loss :  1.6285812854766846 4.483427047729492 225.79994201660156
Loss :  1.7170777320861816 4.331697463989258 218.30194091796875
Loss :  1.6682440042495728 4.4832353591918945 225.83001708984375
Loss :  1.6357762813568115 4.3400349617004395 218.6375274658203
Loss :  1.6481385231018066 4.44548225402832 223.9222412109375
Loss :  1.6243747472763062 4.532166004180908 228.23268127441406
Loss :  1.7090822458267212 4.457561016082764 224.58712768554688
  batch 20 loss: 1.7090822458267212, 4.457561016082764, 224.58712768554688
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4, 5], device='cuda:0')
Loss :  1.6489572525024414 4.290267467498779 216.16232299804688
Loss :  1.6108421087265015 4.402716636657715 221.74667358398438
Loss :  1.6205334663391113 4.479788303375244 225.6099395751953
Loss :  1.6985406875610352 4.371420383453369 220.26956176757812
Loss :  1.7628378868103027 4.548120021820068 229.16883850097656
Loss :  1.5808744430541992 4.400883197784424 221.62503051757812
Loss :  1.6805006265640259 4.6486687660217285 234.1139373779297
Loss :  1.579120397567749 4.435749530792236 223.36659240722656
Loss :  1.589011788368225 4.456857204437256 224.43186950683594
Loss :  1.731738805770874 4.500307559967041 226.7471160888672
Loss :  1.5694776773452759 4.613133907318115 232.22616577148438
Loss :  1.695366382598877 4.46042013168335 224.7163848876953
Loss :  1.6075843572616577 4.450926780700684 224.1539306640625
Loss :  1.612243413925171 4.3997578620910645 221.6001434326172
Loss :  1.7009934186935425 4.482024669647217 225.80221557617188
Loss :  1.661679744720459 4.473097801208496 225.3165740966797
Loss :  1.6891447305679321 4.419985294342041 222.68841552734375
Loss :  1.7178609371185303 4.463442325592041 224.88998413085938
Loss :  1.7717825174331665 4.4197564125061035 222.75961303710938
Loss :  1.7590888738632202 4.410675525665283 222.29287719726562
  batch 40 loss: 1.7590888738632202, 4.410675525665283, 222.29287719726562
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.7643593549728394 4.502401828765869 226.8844451904297
Loss :  1.7294472455978394 4.477998733520508 225.62937927246094
Loss :  1.6347869634628296 4.417906284332275 222.53009033203125
Loss :  1.649430513381958 4.522880554199219 227.79345703125
Loss :  1.6030937433242798 4.389409065246582 221.07354736328125
Loss :  1.6348646879196167 4.426745414733887 222.9721221923828
Loss :  1.6899538040161133 4.56570291519165 229.97509765625
Loss :  1.6516659259796143 4.6872711181640625 236.01522827148438
Loss :  1.7250754833221436 4.6989359855651855 236.671875
Loss :  1.6066941022872925 4.577314853668213 230.47242736816406
Loss :  1.6610089540481567 4.519280433654785 227.62503051757812
Loss :  1.6271966695785522 4.371630668640137 220.20872497558594
Loss :  1.6729470491409302 4.476314067840576 225.4886474609375
Loss :  1.5911892652511597 4.78468132019043 240.82525634765625
Loss :  1.584850549697876 4.562045574188232 229.6871337890625
Loss :  1.6747151613235474 4.610015392303467 232.17547607421875
Loss :  1.6195642948150635 4.559323310852051 229.5857391357422
Loss :  1.677717924118042 4.475270748138428 225.44125366210938
Loss :  1.6693569421768188 4.665627956390381 234.9507598876953
Loss :  1.6723543405532837 4.42019510269165 222.68209838867188
  batch 60 loss: 1.6723543405532837, 4.42019510269165, 222.68209838867188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
Loss :  1.5569229125976562 4.523101806640625 227.71200561523438
Loss :  1.6537549495697021 4.452986717224121 224.30308532714844
Loss :  1.6489816904067993 4.432531356811523 223.2755584716797
Loss :  1.6366174221038818 4.506719589233398 226.97259521484375
Loss :  1.610632061958313 4.07200288772583 205.2107696533203
Loss :  1.7458217144012451 4.430907726287842 223.29119873046875
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
Loss :  1.7232526540756226 4.525420188903809 227.9942626953125
Loss :  1.7266768217086792 4.463761329650879 224.91473388671875
Loss :  1.7236855030059814 4.354651927947998 219.45626831054688
Total LOSS train 224.8672569861779 valid 223.91411590576172
CE LOSS train 1.6698566143329328 valid 0.43092137575149536
Contrastive LOSS train 4.463948015066293 valid 1.0886629819869995
EPOCH 37:
Loss :  1.6444385051727295 4.330672264099121 218.1780548095703
Loss :  1.6604231595993042 4.423556327819824 222.83822631835938
Loss :  1.632218837738037 4.30551815032959 216.9081268310547
Loss :  1.6287168264389038 4.487697124481201 226.01358032226562
Loss :  1.6499791145324707 4.419586658477783 222.6293182373047
Loss :  1.6155731678009033 4.350224494934082 219.12680053710938
Loss :  1.6520953178405762 4.617349624633789 232.5195770263672
Loss :  1.6561720371246338 4.146215915679932 208.9669647216797
Loss :  1.603248119354248 4.452935695648193 224.25003051757812
Loss :  1.6672356128692627 4.294469356536865 216.3907012939453
Loss :  1.6143804788589478 4.3918843269348145 221.20860290527344
Loss :  1.6573303937911987 4.576930046081543 230.5038299560547
Loss :  1.5930501222610474 4.488993167877197 226.04270935058594
Loss :  1.5444133281707764 4.524317264556885 227.76028442382812
Loss :  1.6982827186584473 4.363613605499268 219.87896728515625
Loss :  1.6952025890350342 4.551848411560059 229.28762817382812
Loss :  1.561781883239746 4.527464866638184 227.93502807617188
Loss :  1.6310782432556152 4.756588459014893 239.4604949951172
Loss :  1.5928785800933838 4.433253765106201 223.2555694580078
Loss :  1.6807246208190918 4.43190860748291 223.27615356445312
  batch 20 loss: 1.6807246208190918, 4.43190860748291, 223.27615356445312
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.6196762323379517 4.28377103805542 215.8082275390625
Loss :  1.6034873723983765 4.423509120941162 222.77894592285156
Loss :  1.6224188804626465 4.58305549621582 230.7751922607422
Loss :  1.599672794342041 4.414561748504639 222.3277587890625
Loss :  1.6687860488891602 4.534974575042725 228.4175262451172
Loss :  1.6454659700393677 4.519510269165039 227.6209716796875
Loss :  1.6939727067947388 4.4903059005737305 226.20925903320312
Loss :  1.6206398010253906 4.455563545227051 224.39881896972656
Loss :  1.5834763050079346 4.4219841957092285 222.6826934814453
Loss :  1.7340186834335327 4.489672660827637 226.2176513671875
Loss :  1.588625192642212 4.42521333694458 222.8492889404297
Loss :  1.6514736413955688 4.655545234680176 234.42874145507812
Loss :  1.637781023979187 4.418877601623535 222.5816650390625
Loss :  1.6413203477859497 4.502795696258545 226.78111267089844
Loss :  1.6961015462875366 4.525639533996582 227.97808837890625
Loss :  1.6343618631362915 4.4923577308654785 226.25225830078125
Loss :  1.6457998752593994 4.418695449829102 222.58056640625
Loss :  1.6976839303970337 4.373525142669678 220.37393188476562
Loss :  1.7145315408706665 4.340052127838135 218.71714782714844
Loss :  1.7316179275512695 4.45573091506958 224.51815795898438
  batch 40 loss: 1.7316179275512695, 4.45573091506958, 224.51815795898438
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4, 5], device='cuda:0')
Loss :  1.6564631462097168 4.421751022338867 222.7440185546875
Loss :  1.7066985368728638 4.586442947387695 231.02883911132812
Loss :  1.6931161880493164 4.3932600021362305 221.35610961914062
Loss :  1.7234187126159668 4.679198741912842 235.683349609375
Loss :  1.651327133178711 4.361528396606445 219.7277374267578
Loss :  1.6844784021377563 4.450225830078125 224.19577026367188
Loss :  1.722917079925537 4.422260761260986 222.83595275878906
Loss :  1.643868088722229 4.465075492858887 224.89764404296875
Loss :  1.6967051029205322 4.4524431228637695 224.31886291503906
Loss :  1.6956771612167358 4.605507850646973 231.9710693359375
Loss :  1.6483551263809204 4.49434757232666 226.36573791503906
Loss :  1.623339056968689 4.3294148445129395 218.09408569335938
Loss :  1.6784600019454956 4.423689842224121 222.8629608154297
Loss :  1.627846598625183 4.369598865509033 220.10780334472656
Loss :  1.6151624917984009 4.619032859802246 232.56680297851562
Loss :  1.6339870691299438 4.38429069519043 220.84852600097656
Loss :  1.6357057094573975 4.627816200256348 233.02651977539062
Loss :  1.6145386695861816 4.490301132202148 226.1295928955078
Loss :  1.6294422149658203 4.556628704071045 229.46087646484375
Loss :  1.6850769519805908 4.539220333099365 228.64608764648438
  batch 60 loss: 1.6850769519805908, 4.539220333099365, 228.64608764648438
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
Loss :  1.6346635818481445 4.732510089874268 238.26016235351562
Loss :  1.6315958499908447 4.503274917602539 226.7953338623047
Loss :  1.6446399688720703 4.439388751983643 223.61407470703125
Loss :  1.650607705116272 4.449385166168213 224.11985778808594
Loss :  1.621569275856018 4.162590026855469 209.75106811523438
Loss :  1.760995626449585 4.420923233032227 222.80715942382812
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([5], device='cuda:0')
Loss :  1.7453625202178955 4.479418754577637 225.71629333496094
Loss :  1.7498854398727417 4.3159708976745605 217.5484161376953
Loss :  1.7499405145645142 4.185743808746338 211.03712463378906
Total LOSS train 224.77134610689603 valid 219.27724838256836
CE LOSS train 1.6486122333086455 valid 0.43748512864112854
Contrastive LOSS train 4.462454671126146 valid 1.0464359521865845
EPOCH 38:
Loss :  1.6863294839859009 4.364882946014404 219.9304656982422
Loss :  1.6561349630355835 4.536186218261719 228.46543884277344
Loss :  1.6186811923980713 4.451298236846924 224.18359375
Loss :  1.6156535148620605 4.4270195960998535 222.9666290283203
Loss :  1.724037528038025 4.3011155128479 216.77981567382812
Loss :  1.6062871217727661 4.15371036529541 209.29180908203125
Loss :  1.7180309295654297 4.231980800628662 213.31707763671875
Loss :  1.6530295610427856 4.310564994812012 217.1812744140625
Loss :  1.6647605895996094 4.42579460144043 222.95449829101562
Loss :  1.663490891456604 4.303345680236816 216.83078002929688
Loss :  1.731118083000183 4.4226579666137695 222.86402893066406
Loss :  1.633015513420105 4.3951802253723145 221.39202880859375
Loss :  1.6416497230529785 4.466126441955566 224.94796752929688
Loss :  1.7169482707977295 4.409948348999023 222.21437072753906
Loss :  1.7967292070388794 4.654513835906982 234.5224151611328
Loss :  1.673614740371704 4.518531322479248 227.6001739501953
Loss :  1.6835904121398926 4.44584321975708 223.9757537841797
Loss :  1.7116363048553467 4.416086673736572 222.51597595214844
Loss :  1.6646342277526855 4.423646450042725 222.84695434570312
Loss :  1.7082871198654175 4.401589870452881 221.78778076171875
  batch 20 loss: 1.7082871198654175, 4.401589870452881, 221.78778076171875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([5], device='cuda:0')
Loss :  1.6602451801300049 4.39382791519165 221.3516387939453
Loss :  1.6514179706573486 4.553292751312256 229.31605529785156
Loss :  1.6798456907272339 4.501557350158691 226.75770568847656
Loss :  1.665845513343811 4.474972248077393 225.41445922851562
Loss :  1.666251540184021 4.650712490081787 234.20187377929688
Loss :  1.6435577869415283 4.327942848205566 218.0406951904297
Loss :  1.6637288331985474 4.554753303527832 229.40139770507812
Loss :  1.6022011041641235 4.496850490570068 226.44473266601562
Loss :  1.6416337490081787 4.410545349121094 222.1688995361328
Loss :  1.6869947910308838 4.407623291015625 222.0681610107422
Loss :  1.6053683757781982 4.435986042022705 223.4046630859375
Loss :  1.6710745096206665 4.556275367736816 229.48484802246094
Loss :  1.665253758430481 4.671998977661133 235.26519775390625
Loss :  1.634958028793335 4.432319641113281 223.25094604492188
Loss :  1.6402603387832642 4.543095588684082 228.7950439453125
Loss :  1.6497893333435059 4.5040130615234375 226.85044860839844
Loss :  1.6397099494934082 4.462765216827393 224.77796936035156
Loss :  1.630260944366455 4.500075340270996 226.634033203125
Loss :  1.7099031209945679 4.303919792175293 216.9058837890625
Loss :  1.6684919595718384 4.39893913269043 221.61544799804688
  batch 40 loss: 1.6684919595718384, 4.39893913269043, 221.61544799804688
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.6428622007369995 4.4915313720703125 226.2194366455078
Loss :  1.6774379014968872 4.319292068481445 217.6420440673828
Loss :  1.6627967357635498 4.4239182472229 222.85870361328125
Loss :  1.6620246171951294 4.732041358947754 238.2640838623047
Loss :  1.61339271068573 4.368955135345459 220.06114196777344
Loss :  1.5891579389572144 4.4292402267456055 223.05116271972656
Loss :  1.7009986639022827 4.5350799560546875 228.4550018310547
Loss :  1.6519240140914917 4.477868556976318 225.54534912109375
Loss :  1.7094178199768066 4.329685688018799 218.19369506835938
Loss :  1.670972466468811 4.553225040435791 229.3322296142578
Loss :  1.6605535745620728 4.552943229675293 229.30770874023438
Loss :  1.683187484741211 4.32827615737915 218.09698486328125
Loss :  1.629998803138733 4.471738338470459 225.2169189453125
Loss :  1.6522984504699707 4.5287017822265625 228.08738708496094
Loss :  1.6471667289733887 4.375274658203125 220.41090393066406
Loss :  1.6429721117019653 4.612337589263916 232.25985717773438
Loss :  1.599502444267273 4.590892314910889 231.1441192626953
Loss :  1.6106749773025513 4.499855041503906 226.60342407226562
Loss :  1.673152208328247 4.647525310516357 234.04942321777344
Loss :  1.6776005029678345 4.639127254486084 233.6339569091797
  batch 60 loss: 1.6776005029678345, 4.639127254486084, 233.6339569091797
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.6276042461395264 4.506131649017334 226.93418884277344
Loss :  1.627205491065979 4.384499549865723 220.85218811035156
Loss :  1.698758602142334 4.412890911102295 222.3433074951172
Loss :  1.6813420057296753 4.425360202789307 222.9493408203125
Loss :  1.7376214265823364 4.155376434326172 209.50645446777344
Loss :  23.557968139648438 4.399982929229736 243.55711364746094
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([5], device='cuda:0')
Loss :  22.50771141052246 4.347496032714844 239.88250732421875
Loss :  21.717260360717773 4.397486686706543 241.59158325195312
Loss :  14.134076118469238 4.252077102661133 226.73793029785156
Total LOSS train 224.3036607008714 valid 237.9422836303711
CE LOSS train 1.662693476676941 valid 3.5335190296173096
Contrastive LOSS train 4.452819347381592 valid 1.0630192756652832
EPOCH 39:
Loss :  1.6797369718551636 4.282198905944824 215.7896728515625
Loss :  1.7205363512039185 4.464083671569824 224.92471313476562
Loss :  1.7029448747634888 4.403022289276123 221.8540496826172
Loss :  1.6593215465545654 4.405591011047363 221.93887329101562
Loss :  1.655950665473938 4.285407066345215 215.92630004882812
Loss :  1.6491501331329346 4.314650058746338 217.38165283203125
Loss :  1.6870211362838745 4.523243427276611 227.8491973876953
Loss :  1.6887084245681763 4.282468318939209 215.81211853027344
Loss :  1.692550778388977 4.552169322967529 229.30101013183594
Loss :  1.653707504272461 4.327940464019775 218.05072021484375
Loss :  1.6593165397644043 4.456751823425293 224.4969024658203
Loss :  1.7112786769866943 4.499083042144775 226.66542053222656
Loss :  1.662718653678894 4.4581756591796875 224.57150268554688
Loss :  1.6561462879180908 4.53278923034668 228.2956085205078
Loss :  1.7023776769638062 4.326864719390869 218.0456085205078
Loss :  1.6782745122909546 4.494899272918701 226.42323303222656
Loss :  1.6872469186782837 4.460057735443115 224.69012451171875
Loss :  1.650411605834961 4.3778581619262695 220.54331970214844
Loss :  1.6287418603897095 4.616052150726318 232.4313507080078
Loss :  1.7060604095458984 4.390584945678711 221.2353057861328
  batch 20 loss: 1.7060604095458984, 4.390584945678711, 221.2353057861328
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.6523863077163696 4.347511291503906 219.0279541015625
Loss :  1.6355277299880981 4.462588310241699 224.7649383544922
Loss :  1.662742257118225 4.43613862991333 223.46966552734375
Loss :  1.6850087642669678 4.460638046264648 224.7169189453125
Loss :  1.6632206439971924 4.665099143981934 234.91818237304688
Loss :  1.6148881912231445 4.437099933624268 223.46987915039062
Loss :  1.7549550533294678 4.522804260253906 227.89517211914062
Loss :  1.6773103475570679 4.396562576293945 221.50543212890625
Loss :  1.6639995574951172 4.474263668060303 225.37718200683594
Loss :  1.6442921161651611 4.574004173278809 230.34449768066406
Loss :  1.6763567924499512 4.508780479431152 227.11538696289062
Loss :  1.707943320274353 4.697329998016357 236.57444763183594
Loss :  1.6471468210220337 4.3841938972473145 220.85684204101562
Loss :  1.5959761142730713 4.5385026931762695 228.5211181640625
Loss :  1.6237702369689941 4.516676902770996 227.45761108398438
Loss :  1.5636295080184937 4.411245822906494 222.12591552734375
Loss :  1.6385959386825562 4.372833251953125 220.28025817871094
Loss :  1.6918039321899414 4.34885311126709 219.13446044921875
Loss :  1.661320686340332 4.381824970245361 220.7525634765625
Loss :  1.7095468044281006 4.410849571228027 222.2520294189453
  batch 40 loss: 1.7095468044281006, 4.410849571228027, 222.2520294189453
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.7363898754119873 4.546802043914795 229.0764923095703
Loss :  1.6946399211883545 4.4455742835998535 223.97335815429688
Loss :  1.6674150228500366 4.391603946685791 221.24761962890625
Loss :  1.6282243728637695 4.482918739318848 225.77415466308594
Loss :  1.6399067640304565 4.3093485832214355 217.10733032226562
Loss :  1.5912001132965088 4.414620876312256 222.32225036621094
Loss :  1.6023766994476318 4.560774326324463 229.64108276367188
Loss :  1.6136879920959473 4.472169399261475 225.22216796875
Loss :  1.700243592262268 4.380782127380371 220.73934936523438
Loss :  1.6782991886138916 4.433167457580566 223.336669921875
Loss :  1.659446358680725 4.527029037475586 228.01089477539062
Loss :  1.6615874767303467 4.383549690246582 220.83908081054688
Loss :  1.628483533859253 4.521751880645752 227.71607971191406
Loss :  1.5636038780212402 4.437446594238281 223.43592834472656
Loss :  1.613040804862976 4.387604713439941 220.99327087402344
Loss :  1.6208291053771973 4.420842170715332 222.66294860839844
Loss :  1.635241985321045 4.490732192993164 226.17184448242188
Loss :  1.6623650789260864 4.490191459655762 226.17193603515625
Loss :  1.6890058517456055 4.623851299285889 232.88157653808594
Loss :  1.6540628671646118 4.4671950340271 225.01382446289062
  batch 60 loss: 1.6540628671646118, 4.4671950340271, 225.01382446289062
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.5916391611099243 4.534697532653809 228.32652282714844
Loss :  1.6443125009536743 4.418917179107666 222.59017944335938
Loss :  1.6599417924880981 4.413107872009277 222.31533813476562
Loss :  1.5939719676971436 4.372006893157959 220.19432067871094
Loss :  1.5524215698242188 4.162909030914307 209.6978759765625
Loss :  1.6618351936340332 4.371842861175537 220.2539825439453
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.4788432121276855 4.431540489196777 223.0558624267578
Loss :  1.4225960969924927 4.365026950836182 219.67393493652344
Loss :  1.7606260776519775 4.242663383483887 213.89378356933594
Total LOSS train 223.88075749323917 valid 219.21939086914062
CE LOSS train 1.6566916942596435 valid 0.4401565194129944
Contrastive LOSS train 4.444481328817514 valid 1.0606658458709717
EPOCH 40:
Loss :  1.632398009300232 4.285086154937744 215.88670349121094
Loss :  1.6594408750534058 4.473348617553711 225.32687377929688
Loss :  1.6309545040130615 4.553713798522949 229.31663513183594
Loss :  1.6772706508636475 4.415765762329102 222.46556091308594
Loss :  1.615270733833313 4.242194175720215 213.7249755859375
Loss :  1.6288830041885376 4.467201232910156 224.9889373779297
Loss :  1.6445941925048828 4.509978771209717 227.14352416992188
Loss :  1.6922169923782349 4.326962947845459 218.0403594970703
Loss :  1.6629565954208374 4.362377643585205 219.78184509277344
Loss :  1.624668836593628 4.400059700012207 221.62765502929688
Loss :  1.6511303186416626 4.399137020111084 221.60797119140625
Loss :  1.673012614250183 4.525508403778076 227.9484405517578
Loss :  1.6292427778244019 4.5371246337890625 228.4854736328125
Loss :  1.6507737636566162 4.469240188598633 225.11277770996094
Loss :  1.7314504384994507 4.426330089569092 223.04794311523438
Loss :  1.667085886001587 4.481837272644043 225.75894165039062
Loss :  1.7251173257827759 4.321478843688965 217.79905700683594
Loss :  1.6906765699386597 4.347809314727783 219.08114624023438
Loss :  1.6949416399002075 4.400662422180176 221.7280731201172
Loss :  1.6981960535049438 4.387916564941406 221.09402465820312
  batch 20 loss: 1.6981960535049438, 4.387916564941406, 221.09402465820312
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.6965277194976807 4.557474613189697 229.5702667236328
Loss :  1.7323112487792969 4.485715389251709 226.01808166503906
Loss :  1.7191170454025269 4.674535274505615 235.4458770751953
Loss :  1.6632198095321655 4.492828369140625 226.3046417236328
Loss :  1.7123380899429321 4.6791534423828125 235.67001342773438
Loss :  1.673631191253662 4.466867923736572 225.01702880859375
Loss :  1.6516587734222412 4.559980392456055 229.6506805419922
Loss :  1.6784453392028809 4.339645862579346 218.66075134277344
Loss :  1.6603481769561768 4.496008396148682 226.4607696533203
Loss :  1.703762412071228 4.42840051651001 223.12379455566406
Loss :  1.6368900537490845 4.433922290802002 223.3330078125
Loss :  1.7286876440048218 4.551301956176758 229.2937774658203
Loss :  1.6996763944625854 4.424820423126221 222.94070434570312
Loss :  1.7153027057647705 4.510789394378662 227.25477600097656
Loss :  1.7017264366149902 4.387941837310791 221.09881591796875
Loss :  1.6638634204864502 4.389816761016846 221.1547088623047
Loss :  1.7028932571411133 4.407683372497559 222.08706665039062
Loss :  1.6348634958267212 4.7763447761535645 240.4521026611328
Loss :  1.639171838760376 4.356383323669434 219.45834350585938
Loss :  1.6962928771972656 4.345356464385986 218.964111328125
  batch 40 loss: 1.6962928771972656, 4.345356464385986, 218.964111328125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.6197017431259155 4.575033187866211 230.37136840820312
Loss :  1.6774171590805054 4.495961666107178 226.47549438476562
Loss :  1.688256859779358 4.472602367401123 225.31837463378906
Loss :  1.6817330121994019 4.465958595275879 224.9796600341797
Loss :  1.7115956544876099 4.23481559753418 213.45237731933594
Loss :  1.637553095817566 4.329444408416748 218.10977172851562
Loss :  1.742235541343689 4.649688243865967 234.2266387939453
Loss :  1.7070667743682861 4.475998401641846 225.50698852539062
Loss :  1.7505640983581543 4.7015485763549805 236.82798767089844
Loss :  1.5820882320404053 4.590913772583008 231.12777709960938
Loss :  1.7156447172164917 4.431900978088379 223.31068420410156
Loss :  1.7138845920562744 4.387411117553711 221.08444213867188
Loss :  1.6690561771392822 4.368277072906494 220.08290100097656
Loss :  1.7178058624267578 4.4141950607299805 222.42755126953125
Loss :  1.6695407629013062 4.412561416625977 222.297607421875
Loss :  1.66257905960083 4.399331092834473 221.62913513183594
Loss :  1.6165497303009033 4.480271816253662 225.63014221191406
Loss :  1.6363344192504883 4.518041133880615 227.53839111328125
Loss :  1.586661458015442 4.525577068328857 227.86550903320312
Loss :  1.6279103755950928 4.5191473960876465 227.5852813720703
  batch 60 loss: 1.6279103755950928, 4.5191473960876465, 227.5852813720703
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.6724274158477783 4.5882568359375 231.08526611328125
Loss :  1.6304343938827515 4.434199810028076 223.34042358398438
Loss :  1.6563783884048462 4.392073631286621 221.2600555419922
Loss :  1.6354173421859741 4.374976634979248 220.38424682617188
Loss :  1.6841007471084595 3.992701292037964 201.3191680908203
Loss :  2.3194611072540283 4.434107303619385 224.0248260498047
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2], device='cuda:0')
Loss :  2.2810521125793457 4.411126136779785 222.8373565673828
Loss :  2.2878432273864746 4.283551216125488 216.4654083251953
Loss :  2.224893569946289 4.3070878982543945 217.5792999267578
Total LOSS train 224.23328434870794 valid 220.22672271728516
CE LOSS train 1.6720607280731201 valid 0.5562233924865723
Contrastive LOSS train 4.451224484810463 valid 1.0767719745635986
EPOCH 41:
Loss :  1.5928187370300293 4.529627323150635 228.07418823242188
Loss :  1.6054617166519165 4.440315246582031 223.62123107910156
Loss :  1.6069719791412354 4.335076332092285 218.3607940673828
Loss :  1.6253763437271118 4.4008708000183105 221.66891479492188
Loss :  1.6913338899612427 4.385352611541748 220.95895385742188
Loss :  1.6197584867477417 4.4255547523498535 222.89749145507812
Loss :  1.6360021829605103 4.459767818450928 224.6243896484375
Loss :  1.6907377243041992 4.45594596862793 224.488037109375
Loss :  1.6497727632522583 4.334251403808594 218.3623504638672
Loss :  1.6393964290618896 4.152163028717041 209.24754333496094
Loss :  1.6993169784545898 4.553520202636719 229.3753204345703
Loss :  1.6562399864196777 4.373721599578857 220.34231567382812
Loss :  1.6949727535247803 4.624386787414551 232.91432189941406
Loss :  1.7542567253112793 4.51609992980957 227.5592498779297
Loss :  1.7053639888763428 4.38057804107666 220.73426818847656
Loss :  1.6730691194534302 4.460003852844238 224.6732635498047
Loss :  1.6762282848358154 4.5273895263671875 228.0457000732422
Loss :  1.6704399585723877 4.376642227172852 220.50254821777344
Loss :  1.6233644485473633 4.36334228515625 219.7904815673828
Loss :  1.7057607173919678 4.394232749938965 221.4174041748047
  batch 20 loss: 1.7057607173919678, 4.394232749938965, 221.4174041748047
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.6810532808303833 4.431272983551025 223.2447052001953
Loss :  1.6786704063415527 4.507754802703857 227.06640625
Loss :  1.6717861890792847 4.6722540855407715 235.28448486328125
Loss :  1.7045642137527466 4.521126747131348 227.76089477539062
Loss :  1.6946948766708374 4.585257053375244 230.95755004882812
Loss :  1.6327736377716064 4.4854888916015625 225.90721130371094
Loss :  1.6669974327087402 4.642749309539795 233.80445861816406
Loss :  1.632934808731079 4.352992057800293 219.28253173828125
Loss :  1.5875029563903809 4.455206394195557 224.3478240966797
Loss :  1.6466459035873413 4.502164840698242 226.75489807128906
Loss :  1.6265201568603516 4.454257488250732 224.3394012451172
Loss :  1.6689453125 4.674513816833496 235.39463806152344
Loss :  1.6061549186706543 4.765066623687744 239.85948181152344
Loss :  1.6269162893295288 4.439408302307129 223.5973358154297
Loss :  1.5869721174240112 4.500748157501221 226.6243896484375
Loss :  1.5866813659667969 4.365009784698486 219.83717346191406
Loss :  1.5905117988586426 4.347488880157471 218.9649658203125
Loss :  1.6790438890457153 4.366694450378418 220.01376342773438
Loss :  1.6426469087600708 4.3573503494262695 219.5101776123047
Loss :  1.628263235092163 4.6261162757873535 232.93408203125
  batch 40 loss: 1.628263235092163, 4.6261162757873535, 232.93408203125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.6513751745224 4.428522109985352 223.07748413085938
Loss :  1.5898584127426147 4.377645969390869 220.47215270996094
Loss :  1.5609487295150757 4.34985876083374 219.0538787841797
Loss :  1.5974888801574707 4.548920631408691 229.04351806640625
Loss :  1.544890284538269 4.293387413024902 216.21426391601562
Loss :  1.6414188146591187 4.4555535316467285 224.41909790039062
Loss :  1.6319900751113892 4.50896692276001 227.08033752441406
Loss :  1.5840094089508057 4.337666034698486 218.46731567382812
Loss :  1.6127012968063354 4.4019083976745605 221.70811462402344
Loss :  1.5622695684432983 4.482354640960693 225.6800079345703
Loss :  1.595009446144104 4.499037265777588 226.546875
Loss :  1.6634832620620728 4.45333194732666 224.330078125
Loss :  1.6167293787002563 4.352897644042969 219.26161193847656
Loss :  1.7111613750457764 4.365993022918701 220.0108184814453
Loss :  1.5514942407608032 4.4058990478515625 221.84645080566406
Loss :  1.7212085723876953 4.3600544929504395 219.72393798828125
Loss :  1.6306310892105103 4.507645606994629 227.01290893554688
Loss :  1.6915340423583984 4.507730007171631 227.07803344726562
Loss :  1.5766681432724 4.519062519073486 227.52980041503906
Loss :  1.705830454826355 4.401334762573242 221.77256774902344
  batch 60 loss: 1.705830454826355, 4.401334762573242, 221.77256774902344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.5071754455566406 4.552589416503906 229.1366424560547
Loss :  1.5146043300628662 4.491442680358887 226.08673095703125
Loss :  1.5663952827453613 4.491981506347656 226.16546630859375
Loss :  1.6572893857955933 4.574287414550781 230.3716583251953
Loss :  1.5959256887435913 4.075077533721924 205.3498077392578
Loss :  1.8942489624023438 4.521886825561523 227.98858642578125
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.9281383752822876 4.460213661193848 224.93881225585938
Loss :  1.7660140991210938 4.401080131530762 221.82000732421875
Loss :  1.7759239673614502 4.255687236785889 214.56028747558594
Total LOSS train 224.1628415621244 valid 222.32692337036133
CE LOSS train 1.6359848260879517 valid 0.44398099184036255
Contrastive LOSS train 4.450537124046913 valid 1.0639218091964722
EPOCH 42:
Loss :  1.5752153396606445 4.34302282333374 218.72634887695312
Loss :  1.6827034950256348 4.468512535095215 225.10833740234375
Loss :  1.6658830642700195 4.433450222015381 223.33839416503906
Loss :  1.6934276819229126 4.5441460609436035 228.90072631835938
Loss :  1.6745212078094482 4.38308048248291 220.8285369873047
Loss :  1.6333942413330078 4.516737461090088 227.4702606201172
Loss :  1.7197755575180054 4.444616317749023 223.95059204101562
Loss :  1.7310032844543457 4.2095866203308105 212.2103271484375
Loss :  1.6886385679244995 4.4076828956604 222.07278442382812
Loss :  1.6700602769851685 4.386277675628662 220.98394775390625
Loss :  1.6188851594924927 4.421431541442871 222.69046020507812
Loss :  1.6630640029907227 4.565374851226807 229.9318084716797
Loss :  1.63710355758667 4.481840133666992 225.72911071777344
Loss :  1.741315484046936 4.647689342498779 234.1257781982422
Loss :  1.7850275039672852 4.720560073852539 237.8130340576172
Loss :  1.6605908870697021 4.7093305587768555 237.12710571289062
Loss :  1.6896657943725586 4.352287292480469 219.3040313720703
Loss :  1.7234619855880737 4.524421691894531 227.94454956054688
Loss :  1.7426917552947998 4.3418707847595215 218.8362274169922
Loss :  1.739637017250061 4.549843788146973 229.23182678222656
  batch 20 loss: 1.739637017250061, 4.549843788146973, 229.23182678222656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
Loss :  1.667515516281128 4.358242988586426 219.5796661376953
Loss :  1.7538193464279175 4.46008825302124 224.7582244873047
Loss :  1.6777198314666748 4.522406101226807 227.7980194091797
Loss :  1.6113022565841675 4.449012279510498 224.06190490722656
Loss :  1.6126606464385986 4.594792366027832 231.35227966308594
Loss :  1.7673407793045044 4.421807289123535 222.85769653320312
Loss :  1.7879606485366821 4.7046589851379395 237.0209197998047
Loss :  1.7127851247787476 4.418675899505615 222.64657592773438
Loss :  1.7004834413528442 4.420271873474121 222.71408081054688
Loss :  1.8150290250778198 4.350931167602539 219.36158752441406
Loss :  1.7213603258132935 4.667182445526123 235.08047485351562
Loss :  1.7851579189300537 4.6793036460876465 235.75033569335938
Loss :  1.6059364080429077 4.451569080352783 224.1844024658203
Loss :  1.7425037622451782 4.427693843841553 223.127197265625
Loss :  1.7690097093582153 4.598359107971191 231.6869659423828
Loss :  1.775602102279663 4.465040683746338 225.0276336669922
Loss :  1.7035777568817139 4.562262058258057 229.81668090820312
Loss :  1.7785600423812866 4.290922164916992 216.32467651367188
Loss :  1.6921221017837524 4.384041786193848 220.8942108154297
Loss :  1.7610771656036377 4.372384548187256 220.38031005859375
  batch 40 loss: 1.7610771656036377, 4.372384548187256, 220.38031005859375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.7241978645324707 4.4839935302734375 225.9238739013672
Loss :  1.712078332901001 4.377676486968994 220.5959014892578
Loss :  1.7492709159851074 4.358358860015869 219.66720581054688
Loss :  1.6548526287078857 4.5337815284729 228.3439178466797
Loss :  1.706586480140686 4.35858154296875 219.6356658935547
Loss :  1.6227407455444336 4.5255961418151855 227.9025421142578
Loss :  1.6754882335662842 4.494834899902344 226.417236328125
Loss :  1.6521943807601929 4.534246921539307 228.36453247070312
Loss :  1.6950536966323853 4.482301712036133 225.8101348876953
Loss :  1.615079641342163 4.36554479598999 219.89231872558594
Loss :  1.712463140487671 4.728540897369385 238.13951110839844
Loss :  1.63804030418396 4.467113971710205 224.99374389648438
Loss :  1.7538472414016724 4.477777481079102 225.64271545410156
Loss :  1.7203010320663452 4.601701259613037 231.8053741455078
Loss :  1.6452746391296387 4.433973789215088 223.34396362304688
Loss :  1.7335137128829956 4.465327262878418 224.9998779296875
Loss :  1.7344099283218384 4.64565896987915 234.01734924316406
Loss :  1.5752899646759033 4.4203715324401855 222.59385681152344
Loss :  1.6668248176574707 4.868407726287842 245.0872039794922
Loss :  1.6870548725128174 4.653314590454102 234.352783203125
  batch 60 loss: 1.6870548725128174, 4.653314590454102, 234.352783203125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.5790865421295166 4.5405707359313965 228.6076202392578
Loss :  1.6746183633804321 4.755369663238525 239.44309997558594
Loss :  1.6903446912765503 4.186361312866211 211.00840759277344
Loss :  1.624597191810608 4.809573650360107 242.10328674316406
Loss :  1.5354323387145996 4.292303562164307 216.15060424804688
Loss :  1.9765321016311646 4.340851306915283 219.01910400390625
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.6891894340515137 4.630394458770752 233.20892333984375
Loss :  1.8200690746307373 4.477813720703125 225.71075439453125
Loss :  1.7971183061599731 4.213691711425781 212.48170471191406
Total LOSS train 226.23985783503605 valid 222.60512161254883
CE LOSS train 1.691634330382714 valid 0.4492795765399933
Contrastive LOSS train 4.490964500720684 valid 1.0534229278564453
EPOCH 43:
Loss :  1.704054594039917 4.395015239715576 221.45481872558594
Loss :  1.7523126602172852 4.568948268890381 230.19973754882812
Loss :  1.6617817878723145 4.321394920349121 217.73153686523438
Loss :  1.5793401002883911 4.53290319442749 228.22450256347656
Loss :  1.545711636543274 4.653352737426758 234.21334838867188
Loss :  1.5757592916488647 4.496827125549316 226.4171142578125
Loss :  1.5610853433609009 4.381531715393066 220.63766479492188
Loss :  1.5944395065307617 4.54302453994751 228.74566650390625
Loss :  1.6118377447128296 4.508065223693848 227.0150909423828
Loss :  1.6889265775680542 4.327005863189697 218.03921508789062
Loss :  1.5802220106124878 4.388702392578125 221.0153350830078
Loss :  1.6401997804641724 4.565777778625488 229.92909240722656
Loss :  1.6237027645111084 4.537197589874268 228.48358154296875
Loss :  1.5818637609481812 4.5983567237854 231.49969482421875
Loss :  1.6344294548034668 4.389297962188721 221.09933471679688
Loss :  1.6200146675109863 4.602189540863037 231.7294921875
Loss :  1.5374658107757568 4.253428936004639 214.2089080810547
Loss :  1.5909528732299805 4.1088995933532715 207.0359344482422
Loss :  1.636432409286499 4.402649402618408 221.76890563964844
Loss :  1.7030402421951294 4.787268161773682 241.06643676757812
  batch 20 loss: 1.7030402421951294, 4.787268161773682, 241.06643676757812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.670015573501587 4.38731575012207 221.03579711914062
Loss :  1.643659234046936 4.376340866088867 220.46070861816406
Loss :  1.6897015571594238 4.528613090515137 228.12034606933594
Loss :  1.6708667278289795 4.496274471282959 226.48458862304688
Loss :  1.762784481048584 4.549646854400635 229.24513244628906
Loss :  1.6489533185958862 4.357336044311523 219.51576232910156
Loss :  1.61786687374115 4.654219150543213 234.32882690429688
Loss :  1.632960557937622 4.566309452056885 229.9484405517578
Loss :  1.6167232990264893 4.6647868156433105 234.85606384277344
Loss :  1.678667426109314 4.363664627075195 219.8618927001953
Loss :  1.6700934171676636 4.48794412612915 226.06729125976562
Loss :  1.5790681838989258 4.645069599151611 233.83255004882812
Loss :  1.6862187385559082 4.449092388153076 224.14083862304688
Loss :  1.6942459344863892 4.350022315979004 219.1953582763672
Loss :  1.5900706052780151 4.500876426696777 226.63389587402344
Loss :  1.6030833721160889 4.472399711608887 225.2230682373047
Loss :  1.5670547485351562 4.434686660766602 223.3013916015625
Loss :  1.687598705291748 4.388103008270264 221.09274291992188
Loss :  1.5993974208831787 4.485944747924805 225.89663696289062
Loss :  1.6255302429199219 4.612492084503174 232.25013732910156
  batch 40 loss: 1.6255302429199219, 4.612492084503174, 232.25013732910156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.5775485038757324 4.436379909515381 223.39654541015625
Loss :  1.5400948524475098 4.445034503936768 223.7918243408203
Loss :  1.5515391826629639 4.48463249206543 225.78317260742188
Loss :  1.55517578125 4.545401096343994 228.82522583007812
Loss :  1.5278489589691162 4.340056419372559 218.53067016601562
Loss :  1.5823066234588623 4.390812397003174 221.1229248046875
Loss :  1.581013560295105 4.42371129989624 222.76657104492188
Loss :  1.544014811515808 4.4012017250061035 221.60411071777344
Loss :  1.6127902269363403 4.267305374145508 214.97805786132812
Loss :  1.5450636148452759 4.450630187988281 224.07656860351562
Loss :  1.5794637203216553 4.419131755828857 222.5360565185547
Loss :  1.5870459079742432 4.405894756317139 221.8817901611328
Loss :  1.5486185550689697 4.443835258483887 223.74037170410156
Loss :  1.5762479305267334 4.42662239074707 222.90736389160156
Loss :  1.541053295135498 4.420507431030273 222.56642150878906
Loss :  1.6232110261917114 4.524381160736084 227.84226989746094
Loss :  1.536729097366333 4.465773105621338 224.82537841796875
Loss :  1.5027904510498047 4.420448303222656 222.52520751953125
Loss :  1.5067470073699951 4.44944953918457 223.97921752929688
Loss :  1.595350980758667 4.224874973297119 212.83909606933594
  batch 60 loss: 1.595350980758667, 4.224874973297119, 212.83909606933594
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.4771747589111328 4.373046398162842 220.12948608398438
Loss :  1.5223653316497803 4.223532676696777 212.69900512695312
Loss :  1.4987220764160156 4.408989906311035 221.94821166992188
Loss :  1.4613828659057617 4.4039530754089355 221.65902709960938
Loss :  1.4412918090820312 4.047898292541504 203.83621215820312
Loss :  1.4058698415756226 4.452287197113037 224.02023315429688
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.4409046173095703 4.449289321899414 223.90536499023438
Loss :  1.4392967224121094 4.303159236907959 216.59725952148438
Loss :  1.3793237209320068 4.224666118621826 212.6126251220703
Total LOSS train 223.88919489933895 valid 219.28387069702148
CE LOSS train 1.5995957594651442 valid 0.3448309302330017
Contrastive LOSS train 4.445791992774376 valid 1.0561665296554565
EPOCH 44:
Loss :  1.5484713315963745 4.035131454467773 203.3050537109375
Loss :  1.5339454412460327 4.269211292266846 214.99452209472656
Loss :  1.5096664428710938 4.2111639976501465 212.06787109375
Loss :  1.5378572940826416 4.0253496170043945 202.8053436279297
Loss :  1.5527595281600952 4.189486503601074 211.02708435058594
Loss :  1.524547815322876 4.4567790031433105 224.36349487304688
Loss :  1.5842227935791016 4.087611675262451 205.96481323242188
Loss :  1.5531997680664062 4.101720809936523 206.63925170898438
Loss :  1.54672110080719 4.03462553024292 203.2779998779297
Loss :  1.6141494512557983 4.16431188583374 209.82974243164062
Loss :  1.5421768426895142 4.518728256225586 227.4785919189453
Loss :  1.5370041131973267 4.265718936920166 214.8229522705078
Loss :  1.5353174209594727 4.561640739440918 229.6173553466797
Loss :  1.571881651878357 4.4293999671936035 223.04188537597656
Loss :  1.6031615734100342 4.121577262878418 207.68202209472656
Loss :  1.6234997510910034 4.107006549835205 206.9738311767578
Loss :  1.5460939407348633 4.104851245880127 206.78866577148438
Loss :  1.5859713554382324 3.9855222702026367 200.86207580566406
Loss :  1.5488723516464233 4.358397006988525 219.46871948242188
Loss :  1.6569435596466064 4.403199195861816 221.81689453125
  batch 20 loss: 1.6569435596466064, 4.403199195861816, 221.81689453125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.617279052734375 4.010004043579102 202.1174774169922
Loss :  1.5495340824127197 4.2540059089660645 214.2498321533203
Loss :  1.57852041721344 4.3814568519592285 220.6513671875
Loss :  1.606397271156311 4.493557453155518 226.28427124023438
Loss :  1.6294431686401367 4.608830451965332 232.0709686279297
Loss :  1.5562928915023804 4.496882915496826 226.4004364013672
Loss :  1.5799115896224976 4.720325946807861 237.59620666503906
Loss :  1.5644992589950562 4.4048004150390625 221.8045196533203
Loss :  1.5062129497528076 4.193089008331299 211.16065979003906
Loss :  1.5916213989257812 4.158605098724365 209.52188110351562
Loss :  1.4533977508544922 4.137988567352295 208.3528289794922
Loss :  1.5450000762939453 4.4042582511901855 221.75790405273438
Loss :  1.5330733060836792 3.9309957027435303 198.08285522460938
Loss :  1.5334392786026 4.205656051635742 211.8162384033203
Loss :  1.4712721109390259 3.9799227714538574 200.4674072265625
Loss :  1.5034008026123047 3.784651279449463 190.7359619140625
Loss :  1.5137344598770142 3.848597526550293 193.943603515625
Loss :  1.5956588983535767 3.961846113204956 199.68795776367188
Loss :  1.6062852144241333 4.0872721672058105 205.9698944091797
Loss :  1.6108442544937134 4.156774520874023 209.44956970214844
  batch 40 loss: 1.6108442544937134, 4.156774520874023, 209.44956970214844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.5585049390792847 4.246291637420654 213.87307739257812
Loss :  1.5239146947860718 4.175780296325684 210.31292724609375
Loss :  1.5302624702453613 4.1286492347717285 207.9627227783203
Loss :  1.5211637020111084 4.144237518310547 208.73304748535156
Loss :  1.5192971229553223 3.6670351028442383 184.87106323242188
Loss :  1.5496716499328613 3.885324239730835 195.8158721923828
Loss :  1.5878318548202515 3.8047032356262207 191.822998046875
Loss :  1.5401419401168823 3.988852024078369 200.9827423095703
Loss :  1.6121759414672852 3.899484872817993 196.58642578125
Loss :  1.5276659727096558 4.037653923034668 203.4103546142578
Loss :  1.575868010520935 3.7635717391967773 189.75445556640625
Loss :  1.5943907499313354 4.134060859680176 208.2974395751953
Loss :  1.5587270259857178 3.8752026557922363 195.31886291503906
Loss :  1.6137726306915283 4.157312870025635 209.4794158935547
Loss :  1.5594589710235596 4.283280849456787 215.7235107421875
Loss :  1.6125236749649048 4.1443867683410645 208.8318634033203
Loss :  1.5223881006240845 4.488596439361572 225.95220947265625
Loss :  1.5283936262130737 4.298083782196045 216.43258666992188
Loss :  1.5352389812469482 4.277569770812988 215.41372680664062
Loss :  1.6333650350570679 4.1219940185546875 207.73306274414062
  batch 60 loss: 1.6333650350570679, 4.1219940185546875, 207.73306274414062
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.5270678997039795 4.308833122253418 216.96871948242188
Loss :  1.5729957818984985 3.9177870750427246 197.4623565673828
Loss :  1.56681227684021 4.224468231201172 212.7902374267578
Loss :  1.5500962734222412 3.9871208667755127 200.90614318847656
Loss :  1.5292831659317017 3.7716407775878906 190.11131286621094
Loss :  1.499039888381958 4.395836353302002 221.29086303710938
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.5418962240219116 4.54191780090332 228.63778686523438
Loss :  1.539717197418213 4.113343715667725 207.2069091796875
Loss :  1.4803968667984009 4.111749649047852 207.06787109375
Total LOSS train 209.54610994779148 valid 216.0508575439453
CE LOSS train 1.5592814500515277 valid 0.3700992166996002
Contrastive LOSS train 4.1597365562732405 valid 1.027937412261963
EPOCH 45:
Loss :  1.6271982192993164 4.139094829559326 208.58193969726562
Loss :  1.6380144357681274 4.37834358215332 220.55519104003906
Loss :  1.5818289518356323 4.057165622711182 204.4401092529297
Loss :  1.593891978263855 3.9365909099578857 198.42343139648438
Loss :  1.6029027700424194 3.9427649974823 198.74114990234375
Loss :  1.5547709465026855 3.9702303409576416 200.0662841796875
Loss :  1.6170752048492432 4.076027870178223 205.41847229003906
Loss :  1.5864592790603638 4.048172473907471 203.99508666992188
Loss :  1.5750603675842285 4.009920597076416 202.0710906982422
Loss :  1.6192021369934082 3.9224693775177 197.74267578125
Loss :  1.5488901138305664 4.125697135925293 207.833740234375
Loss :  1.5478075742721558 4.15092658996582 209.09413146972656
Loss :  1.5585232973098755 3.911635637283325 197.1403045654297
Loss :  1.5715352296829224 3.9794461727142334 200.54383850097656
Loss :  1.6251757144927979 3.902200222015381 196.73519897460938
Loss :  1.6420836448669434 4.594913959503174 231.38778686523438
Loss :  1.577009916305542 4.480152606964111 225.5846405029297
Loss :  1.6228646039962769 4.381963729858398 220.72105407714844
Loss :  1.5718886852264404 4.420610427856445 222.60240173339844
Loss :  1.649946689605713 4.4336161613464355 223.33074951171875
  batch 20 loss: 1.649946689605713, 4.4336161613464355, 223.33074951171875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6210213899612427 4.381936550140381 220.7178497314453
Loss :  1.5312583446502686 4.4542646408081055 224.24449157714844
Loss :  1.5826294422149658 4.588002681732178 230.98275756835938
Loss :  1.6366360187530518 4.308640003204346 217.0686492919922
Loss :  1.64314866065979 4.69354248046875 236.3202667236328
Loss :  1.6178438663482666 4.3396735191345215 218.6015167236328
Loss :  1.617112159729004 4.528418064117432 228.0380096435547
Loss :  1.623088002204895 4.5078535079956055 227.01576232910156
Loss :  1.5350418090820312 4.414612770080566 222.26568603515625
Loss :  1.6297255754470825 4.444394588470459 223.84945678710938
Loss :  1.5020273923873901 4.378875732421875 220.44581604003906
Loss :  1.6032460927963257 4.563337802886963 229.77012634277344
Loss :  1.5637011528015137 4.425346374511719 222.83102416992188
Loss :  1.5436456203460693 4.36090087890625 219.58868408203125
Loss :  1.478393316268921 4.50196647644043 226.57672119140625
Loss :  1.5048744678497314 4.3117780685424805 217.09376525878906
Loss :  1.5044416189193726 4.256220817565918 214.3154754638672
Loss :  1.575466275215149 4.109771251678467 207.06402587890625
Loss :  1.5783830881118774 3.848188638687134 193.98782348632812
Loss :  1.5823272466659546 4.006523132324219 201.90847778320312
  batch 40 loss: 1.5823272466659546, 4.006523132324219, 201.90847778320312
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5680420398712158 4.405618667602539 221.84896850585938
Loss :  1.5039128065109253 3.930180549621582 198.012939453125
Loss :  1.5091127157211304 4.003808975219727 201.69955444335938
Loss :  1.5054296255111694 4.051752090454102 204.09303283691406
Loss :  1.4972022771835327 3.863044261932373 194.6494140625
Loss :  1.5240811109542847 4.069337844848633 204.990966796875
Loss :  1.5607707500457764 3.936722755432129 198.39691162109375
Loss :  1.5142306089401245 3.84824800491333 193.9266357421875
Loss :  1.5848970413208008 4.093818187713623 206.2758026123047
Loss :  1.5175220966339111 3.8931643962860107 196.1757354736328
Loss :  1.5612040758132935 4.16200065612793 209.66123962402344
Loss :  1.5921558141708374 4.2589263916015625 214.53848266601562
Loss :  1.52536940574646 3.8572680950164795 194.38877868652344
Loss :  1.5714422464370728 3.805006980895996 191.82179260253906
Loss :  1.5250135660171509 4.3211140632629395 217.58071899414062
Loss :  1.604503870010376 4.167263507843018 209.96768188476562
Loss :  1.5205165147781372 4.129047870635986 207.97291564941406
Loss :  1.5164940357208252 4.357507705688477 219.39187622070312
Loss :  1.5213364362716675 4.226945877075195 212.86862182617188
Loss :  1.6012911796569824 3.822878122329712 192.7451934814453
  batch 60 loss: 1.6012911796569824, 3.822878122329712, 192.7451934814453
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5104539394378662 3.951900005340576 199.10545349121094
Loss :  1.5271449089050293 3.8853840827941895 195.79635620117188
Loss :  1.5098291635513306 4.100557327270508 206.53768920898438
Loss :  1.4941679239273071 4.107666492462158 206.87750244140625
Loss :  1.4679025411605835 3.9842042922973633 200.67811584472656
Loss :  1.4987183809280396 4.398979187011719 221.4476776123047
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5100573301315308 4.417291164398193 222.37461853027344
Loss :  1.4771063327789307 4.374258041381836 220.1900177001953
Loss :  1.4363853931427002 4.276993751525879 215.28607177734375
Total LOSS train 210.4261237511268 valid 219.8245964050293
CE LOSS train 1.564925661453834 valid 0.35909634828567505
Contrastive LOSS train 4.177223975841816 valid 1.0692484378814697
EPOCH 46:
Loss :  1.5941020250320435 3.6316611766815186 183.17715454101562
Loss :  1.583046793937683 4.027798175811768 202.97296142578125
Loss :  1.5553172826766968 3.7222201824188232 187.66632080078125
Loss :  1.53954017162323 4.34022331237793 218.5507049560547
Loss :  1.546012282371521 4.248397350311279 213.9658660888672
Loss :  1.5027713775634766 4.006998538970947 201.8527069091797
Loss :  1.5477136373519897 3.8769145011901855 195.3934326171875
Loss :  1.515179991722107 3.633983850479126 183.21437072753906
Loss :  1.499069094657898 4.043208599090576 203.6595001220703
Loss :  1.5959391593933105 4.027647495269775 202.97830200195312
Loss :  1.4946280717849731 3.8029136657714844 191.64031982421875
Loss :  1.487687349319458 3.9747228622436523 200.2238311767578
Loss :  1.5327200889587402 4.245954990386963 213.83045959472656
Loss :  1.5329314470291138 4.057133197784424 204.38958740234375
Loss :  1.5723965167999268 3.686583995819092 185.90159606933594
Loss :  1.5755687952041626 3.864159107208252 194.7835235595703
Loss :  1.4841575622558594 4.100501537322998 206.5092315673828
Loss :  1.517048954963684 4.04541540145874 203.78781127929688
Loss :  1.4879502058029175 3.696633815765381 186.31964111328125
Loss :  1.5542516708374023 3.7554209232330322 189.32528686523438
  batch 20 loss: 1.5542516708374023, 3.7554209232330322, 189.32528686523438
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5695598125457764 4.244857311248779 213.8124237060547
Loss :  1.4898979663848877 4.075804710388184 205.28013610839844
Loss :  1.5061087608337402 4.032821178436279 203.14715576171875
Loss :  1.5490976572036743 3.8515214920043945 194.12518310546875
Loss :  1.5649718046188354 4.143184185028076 208.72418212890625
Loss :  1.5095773935317993 4.11074161529541 207.04666137695312
Loss :  1.51125168800354 3.95135760307312 199.07913208007812
Loss :  1.5168431997299194 3.886770486831665 195.85537719726562
Loss :  1.4511929750442505 4.0133233070373535 202.1173553466797
Loss :  1.557178258895874 3.971217155456543 200.11802673339844
Loss :  1.4350510835647583 3.916752338409424 197.27267456054688
Loss :  1.5363092422485352 4.306422710418701 216.85745239257812
Loss :  1.5202745199203491 4.004662990570068 201.75343322753906
Loss :  1.5185707807540894 3.919318675994873 197.4844970703125
Loss :  1.453471064567566 3.985529661178589 200.72996520996094
Loss :  1.4823203086853027 4.1665263175964355 209.80862426757812
Loss :  1.5095577239990234 3.90449595451355 196.73434448242188
Loss :  1.5627446174621582 3.752904176712036 189.20794677734375
Loss :  1.5704796314239502 3.724853277206421 187.81314086914062
Loss :  1.5763709545135498 3.957902669906616 199.47149658203125
  batch 40 loss: 1.5763709545135498, 3.957902669906616, 199.47149658203125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5305051803588867 4.029379844665527 202.99949645996094
Loss :  1.4977695941925049 3.794264316558838 191.2109832763672
Loss :  1.5043046474456787 3.9188690185546875 197.44775390625
Loss :  1.4974753856658936 4.020214080810547 202.50819396972656
Loss :  1.4900612831115723 3.752154588699341 189.09779357910156
Loss :  1.5163623094558716 3.867863893508911 194.9095458984375
Loss :  1.5527985095977783 3.8223989009857178 192.67274475097656
Loss :  1.5118820667266846 4.081655025482178 205.59463500976562
Loss :  1.5798531770706177 4.106315612792969 206.8956298828125
Loss :  1.513492226600647 3.8977131843566895 196.39915466308594
Loss :  1.5569087266921997 3.858595371246338 194.48667907714844
Loss :  1.56149423122406 4.061229228973389 204.62295532226562
Loss :  1.5230265855789185 3.9567840099334717 199.3622283935547
Loss :  1.5687181949615479 3.7925732135772705 191.1973876953125
Loss :  1.5232387781143188 3.837254524230957 193.38597106933594
Loss :  1.5995209217071533 4.046201705932617 203.90960693359375
Loss :  1.514845371246338 4.07716178894043 205.37294006347656
Loss :  1.530436396598816 4.049700736999512 204.01547241210938
Loss :  1.5181891918182373 3.969803810119629 200.0083770751953
Loss :  1.5998919010162354 3.7520079612731934 189.20030212402344
  batch 60 loss: 1.5998919010162354, 3.7520079612731934, 189.20030212402344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.494381308555603 4.159282207489014 209.45849609375
Loss :  1.5476945638656616 3.9809813499450684 200.59677124023438
Loss :  1.511254906654358 4.1293840408325195 207.98046875
Loss :  1.4987977743148804 4.084004878997803 205.69903564453125
Loss :  1.4737446308135986 3.671992301940918 185.07334899902344
Loss :  18.839630126953125 4.124368667602539 225.0580596923828
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4, 5], device='cuda:0')
Loss :  19.076189041137695 4.291499614715576 233.6511688232422
Loss :  18.9622859954834 4.035340785980225 220.7293243408203
Loss :  18.933435440063477 3.832475185394287 210.5572052001953
Total LOSS train 199.54904292179987 valid 222.49893951416016
CE LOSS train 1.528084765947782 valid 4.733358860015869
Contrastive LOSS train 3.960419170673077 valid 0.9581187963485718
EPOCH 47:
Loss :  1.5602104663848877 3.578949213027954 180.50767517089844
Loss :  1.5609972476959229 3.9100751876831055 197.06475830078125
Loss :  1.53217351436615 4.1831817626953125 210.69126892089844
Loss :  1.545366883277893 3.8422608375549316 193.6584014892578
Loss :  1.5535448789596558 3.662548542022705 184.68096923828125
Loss :  1.513447880744934 4.412933349609375 222.1601104736328
Loss :  1.5576200485229492 4.282027721405029 215.65899658203125
Loss :  1.5318442583084106 4.569677829742432 230.01573181152344
Loss :  1.5095689296722412 4.123327732086182 207.6759490966797
Loss :  1.5654252767562866 4.132966995239258 208.21377563476562
Loss :  1.5052857398986816 3.8041608333587646 191.71331787109375
Loss :  1.498616099357605 4.136654376983643 208.33132934570312
Loss :  1.5125535726547241 3.9994428157806396 201.4846954345703
Loss :  1.5409719944000244 4.051175117492676 204.0997314453125
Loss :  1.579581379890442 4.224150657653809 212.787109375
Loss :  1.5826221704483032 4.060691833496094 204.61721801757812
Loss :  1.4915492534637451 3.857168674468994 194.3499755859375
Loss :  1.5420682430267334 4.036529064178467 203.36851501464844
Loss :  1.497666597366333 3.961505651473999 199.5729522705078
Loss :  1.5621449947357178 4.014176845550537 202.27099609375
  batch 20 loss: 1.5621449947357178, 4.014176845550537, 202.27099609375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5529842376708984 3.805889368057251 191.84744262695312
Loss :  1.4953356981277466 4.275958061218262 215.29322814941406
Loss :  1.5123876333236694 4.008881092071533 201.95645141601562
Loss :  1.5558220148086548 3.903010368347168 196.70632934570312
Loss :  1.5711219310760498 4.136133193969727 208.37777709960938
Loss :  1.5140469074249268 4.274947643280029 215.2614288330078
Loss :  1.518457055091858 3.995403528213501 201.28863525390625
Loss :  1.5241549015045166 4.024913787841797 202.7698516845703
Loss :  1.4604910612106323 4.058647155761719 204.39285278320312
Loss :  1.5659966468811035 4.130481719970703 208.09007263183594
Loss :  1.44449782371521 4.077610015869141 205.3249969482422
Loss :  1.5409154891967773 4.071930408477783 205.13743591308594
Loss :  1.5233076810836792 4.026057243347168 202.82615661621094
Loss :  1.5232411623001099 3.8704638481140137 195.0464324951172
Loss :  1.4601473808288574 4.089942455291748 205.95726013183594
Loss :  1.4898344278335571 4.27668571472168 215.32412719726562
Loss :  1.4933886528015137 4.12066125869751 207.52645874023438
Loss :  1.5706737041473389 4.095820903778076 206.36172485351562
Loss :  1.5769011974334717 4.021051406860352 202.6294708251953
Loss :  1.5796204805374146 3.8749759197235107 195.3284149169922
  batch 40 loss: 1.5796204805374146, 3.8749759197235107, 195.3284149169922
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.534085750579834 4.031315803527832 203.09988403320312
Loss :  1.5025259256362915 4.04105281829834 203.55517578125
Loss :  1.5093392133712769 3.9316301345825195 198.09085083007812
Loss :  1.5044260025024414 4.1315717697143555 208.0830078125
Loss :  1.4970200061798096 3.833815813064575 193.1878204345703
Loss :  1.5239295959472656 4.033621788024902 203.20501708984375
Loss :  1.5591773986816406 4.038122177124023 203.4652862548828
Loss :  1.515681266784668 3.8353664875030518 193.2840118408203
Loss :  1.58356773853302 3.773514747619629 190.25930786132812
Loss :  1.5180938243865967 3.779005289077759 190.46836853027344
Loss :  1.5629593133926392 4.0641865730285645 204.7722930908203
Loss :  1.5664764642715454 3.9749832153320312 200.31564331054688
Loss :  1.527329921722412 3.762362241744995 189.64544677734375
Loss :  1.5743012428283691 3.9988186359405518 201.51522827148438
Loss :  1.530491828918457 4.097566604614258 206.4088134765625
Loss :  1.6057908535003662 3.9530348777770996 199.25753784179688
Loss :  1.5211113691329956 4.017714500427246 202.40684509277344
Loss :  1.5176904201507568 3.9158811569213867 197.31173706054688
Loss :  1.5233352184295654 4.118386268615723 207.44264221191406
Loss :  1.6044777631759644 4.244580268859863 213.83349609375
  batch 60 loss: 1.6044777631759644, 4.244580268859863, 213.83349609375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.5018351078033447 4.209029197692871 211.9532928466797
Loss :  1.5311628580093384 4.191538333892822 211.1080780029297
Loss :  1.523080825805664 4.282220363616943 215.6341094970703
Loss :  1.5039628744125366 4.3056769371032715 216.78781127929688
Loss :  1.4673879146575928 3.8435232639312744 193.6435546875
Loss :  3.017394781112671 4.201356410980225 213.08522033691406
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
Loss :  2.93837571144104 3.988957166671753 202.38623046875
Loss :  2.961833953857422 4.115265369415283 208.72511291503906
Loss :  2.896139621734619 4.047670841217041 205.27967834472656
Total LOSS train 203.3708505483774 valid 207.36906051635742
CE LOSS train 1.5311665571652926 valid 0.7240349054336548
Contrastive LOSS train 4.036793683125422 valid 1.0119177103042603
EPOCH 48:
Loss :  1.562867283821106 4.036195755004883 203.37265014648438
Loss :  1.5686641931533813 4.1404643058776855 208.5918731689453
Loss :  1.53508460521698 4.3842692375183105 220.74853515625
Loss :  1.5445845127105713 3.9893264770507812 201.0109100341797
Loss :  1.5527725219726562 3.7944908142089844 191.27731323242188
Loss :  1.5123565196990967 3.860698938369751 194.54730224609375
Loss :  1.5573534965515137 4.2173871994018555 212.4267120361328
Loss :  1.5281177759170532 3.957127332687378 199.38449096679688
Loss :  1.5077400207519531 3.9968717098236084 201.351318359375
Loss :  1.5631495714187622 3.968085527420044 199.96743774414062
Loss :  1.4960637092590332 4.401716709136963 221.58189392089844
Loss :  1.5034347772598267 4.3590216636657715 219.4545135498047
Loss :  1.5050129890441895 4.089430332183838 205.97653198242188
Loss :  1.5339356660842896 3.952924966812134 199.18019104003906
Loss :  1.5747623443603516 3.844559907913208 193.80276489257812
Loss :  1.5830309391021729 4.256217002868652 214.39389038085938
Loss :  1.4885191917419434 4.33790397644043 218.38372802734375
Loss :  1.5216535329818726 4.3153486251831055 217.28907775878906
Loss :  1.492766261100769 3.939434051513672 198.4644775390625
Loss :  1.5597567558288574 3.969841241836548 200.05181884765625
  batch 20 loss: 1.5597567558288574, 3.969841241836548, 200.05181884765625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5481431484222412 4.173482418060303 210.22225952148438
Loss :  1.4863349199295044 3.9519076347351074 199.0817108154297
Loss :  1.5044221878051758 3.9761252403259277 200.31068420410156
Loss :  1.5472607612609863 4.055602073669434 204.32736206054688
Loss :  1.567638635635376 4.225457668304443 212.8405303955078
Loss :  1.510053277015686 4.249261379241943 213.97312927246094
Loss :  1.5140701532363892 4.553938388824463 229.2109832763672
Loss :  1.5143684148788452 4.055349349975586 204.28184509277344
Loss :  1.4483301639556885 4.0878777503967285 205.84222412109375
Loss :  1.5542335510253906 4.253371715545654 214.22280883789062
Loss :  1.4294700622558594 4.13828706741333 208.3438262939453
Loss :  1.5280659198760986 4.494316577911377 226.243896484375
Loss :  1.507986307144165 3.915703535079956 197.29315185546875
Loss :  1.5070608854293823 3.92830228805542 197.92218017578125
Loss :  1.4454002380371094 4.007755279541016 201.83316040039062
Loss :  1.472537875175476 4.1500139236450195 208.97323608398438
Loss :  1.4738925695419312 3.9937047958374023 201.1591339111328
Loss :  1.5545508861541748 4.339834213256836 218.5462646484375
Loss :  1.5659430027008057 3.6213722229003906 182.63455200195312
Loss :  1.570402979850769 3.964045286178589 199.77267456054688
  batch 40 loss: 1.570402979850769, 3.964045286178589, 199.77267456054688
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5218005180358887 4.1697797775268555 210.0107879638672
Loss :  1.4898663759231567 4.097040176391602 206.3418731689453
Loss :  1.4913175106048584 3.8154079914093018 192.26171875
Loss :  1.4855619668960571 4.075655937194824 205.2683563232422
Loss :  1.4786542654037476 3.7190279960632324 187.4300537109375
Loss :  1.509231686592102 4.132071018218994 208.11277770996094
Loss :  1.5496478080749512 3.67378830909729 185.23907470703125
Loss :  1.4975194931030273 3.9561727046966553 199.30615234375
Loss :  1.5770224332809448 4.1559834480285645 209.37620544433594
Loss :  1.504300832748413 4.3637919425964355 219.6938934326172
Loss :  1.5490171909332275 4.018640995025635 202.48106384277344
Loss :  1.5570027828216553 3.9348244667053223 198.29823303222656
Loss :  1.5126736164093018 4.037941932678223 203.40977478027344
Loss :  1.5642088651657104 3.70379376411438 186.75389099121094
Loss :  1.5056042671203613 3.8806347846984863 195.53733825683594
Loss :  1.5964503288269043 3.8033177852630615 191.76234436035156
Loss :  1.5033020973205566 4.500529766082764 226.52978515625
Loss :  1.4997974634170532 4.507248401641846 226.8622283935547
Loss :  1.5113838911056519 4.005666255950928 201.79469299316406
Loss :  1.5968323945999146 4.196871757507324 211.44041442871094
  batch 60 loss: 1.5968323945999146, 4.196871757507324, 211.44041442871094
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.4794166088104248 4.140354633331299 208.4971466064453
Loss :  1.511329174041748 3.98260760307312 200.64170837402344
Loss :  1.493288516998291 3.9845478534698486 200.72067260742188
Loss :  1.4730125665664673 3.9962499141693115 201.28550720214844
Loss :  1.448912501335144 3.633173942565918 183.10760498046875
Loss :  1.5126372575759888 4.221129894256592 212.56912231445312
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.540391206741333 4.151217460632324 209.10125732421875
Loss :  1.5330876111984253 4.033294200897217 203.19778442382812
Loss :  1.519740343093872 4.014083385467529 202.22390747070312
Total LOSS train 204.93009760929988 valid 206.77301788330078
CE LOSS train 1.5207530425145075 valid 0.379935085773468
Contrastive LOSS train 4.068186888327966 valid 1.0035208463668823
EPOCH 49:
Loss :  1.5486104488372803 3.629462480545044 183.02174377441406
Loss :  1.5499409437179565 4.221131801605225 212.60653686523438
Loss :  1.5207546949386597 3.9988315105438232 201.4623260498047
Loss :  1.5350797176361084 4.063998222351074 204.7349853515625
Loss :  1.5440047979354858 4.032273292541504 203.1576690673828
Loss :  1.4999617338180542 3.9755237102508545 200.27613830566406
Loss :  1.5421974658966064 4.2375569343566895 213.4200439453125
Loss :  1.509284496307373 4.000750541687012 201.5467987060547
Loss :  1.4893311262130737 4.032134056091309 203.09603881835938
Loss :  1.5552982091903687 3.936763048171997 198.39344787597656
Loss :  1.4958484172821045 4.15420389175415 209.20603942871094
Loss :  1.4876372814178467 4.535696029663086 228.27244567871094
Loss :  1.502242088317871 4.5133442878723145 227.16946411132812
Loss :  1.5273774862289429 4.2872724533081055 215.8909912109375
Loss :  1.5684126615524292 4.050648212432861 204.10081481933594
Loss :  1.5891519784927368 4.2140913009643555 212.29371643066406
Loss :  1.4827067852020264 4.11110782623291 207.03810119628906
Loss :  1.5258420705795288 4.090912818908691 206.0714874267578
Loss :  1.4853073358535767 3.932451009750366 198.10784912109375
Loss :  1.5675621032714844 4.010464668273926 202.09080505371094
  batch 20 loss: 1.5675621032714844, 4.010464668273926, 202.09080505371094
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.5471776723861694 3.7798354625701904 190.53895568847656
Loss :  1.4847114086151123 4.229243755340576 212.9468994140625
Loss :  1.5099055767059326 3.6811933517456055 185.56956481933594
Loss :  1.5691217184066772 3.5628557205200195 179.7119140625
Loss :  1.6164013147354126 4.295693874359131 216.40109252929688
Loss :  1.5347061157226562 3.447049617767334 173.88717651367188
Loss :  1.5581833124160767 3.8340001106262207 193.25819396972656
Loss :  1.5316967964172363 4.003259181976318 201.6946563720703
Loss :  1.46120285987854 3.6333565711975098 183.1290283203125
Loss :  1.596379280090332 4.190645694732666 211.128662109375
Loss :  1.4383715391159058 3.70103120803833 186.48992919921875
Loss :  1.5659338235855103 4.040898323059082 203.61085510253906
Loss :  1.5414167642593384 3.6085612773895264 181.969482421875
Loss :  1.5438905954360962 3.589066743850708 180.99722290039062
Loss :  1.455476999282837 3.7305707931518555 187.9840087890625
Loss :  1.488519310951233 3.544332265853882 178.70513916015625
Loss :  1.4833675622940063 3.4855637550354004 175.7615509033203
Loss :  1.5919852256774902 3.9392669200897217 198.55532836914062
Loss :  1.6150009632110596 3.491442918777466 176.18714904785156
Loss :  1.620031714439392 3.6624929904937744 184.7446746826172
  batch 40 loss: 1.620031714439392, 3.6624929904937744, 184.7446746826172
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4], device='cuda:0')
Loss :  1.558992862701416 3.6231982707977295 182.71890258789062
Loss :  1.4991194009780884 4.251964092254639 214.09732055664062
Loss :  1.5045381784439087 4.437188148498535 223.3639373779297
Loss :  1.4698456525802612 4.256725311279297 214.30612182617188
Loss :  1.4817148447036743 4.177031517028809 210.3332977294922
Loss :  1.498431921005249 4.315056324005127 217.25125122070312
Loss :  1.557260513305664 4.0877766609191895 205.94610595703125
Loss :  1.498104453086853 4.039816379547119 203.48892211914062
Loss :  1.6495774984359741 4.29771089553833 216.53512573242188
Loss :  1.5092353820800781 4.480311870574951 225.5248260498047
Loss :  1.5641127824783325 4.323782444000244 217.75323486328125
Loss :  1.6441867351531982 4.442826271057129 223.78549194335938
Loss :  1.5151234865188599 4.342437267303467 218.63697814941406
Loss :  1.7087681293487549 4.385356426239014 220.97659301757812
Loss :  1.5244107246398926 4.47136926651001 225.09288024902344
Loss :  1.6806116104125977 4.369287490844727 220.14498901367188
Loss :  1.5646283626556396 4.539670944213867 228.5481719970703
Loss :  1.5513142347335815 4.575474262237549 230.3250274658203
Loss :  1.6456091403961182 4.702216625213623 236.75643920898438
Loss :  1.7714439630508423 4.521966934204102 227.86978149414062
  batch 60 loss: 1.7714439630508423, 4.521966934204102, 227.86978149414062
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6266573667526245 4.440306663513184 223.64199829101562
Loss :  1.6260461807250977 4.278085231781006 215.5303192138672
Loss :  1.628731608390808 4.477833271026611 225.52040100097656
Loss :  1.5231664180755615 4.4267897605896 222.86265563964844
Loss :  1.514565110206604 4.121179580688477 207.57354736328125
Loss :  1.7041317224502563 4.474715232849121 225.4398956298828
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.7458093166351318 4.435028076171875 223.49720764160156
Loss :  1.7093697786331177 4.249378204345703 214.1782684326172
Loss :  2.0964529514312744 4.24975061416626 214.583984375
Total LOSS train 206.0586653489333 valid 219.4248390197754
CE LOSS train 1.5476342916488648 valid 0.5241132378578186
Contrastive LOSS train 4.090220623749953 valid 1.062437653541565
EPOCH 50:
Loss :  1.6312898397445679 4.416929244995117 222.47775268554688
Loss :  1.6658945083618164 4.658482551574707 234.59002685546875
Loss :  1.6250452995300293 4.367363929748535 219.9932403564453
Loss :  1.6181747913360596 4.431403160095215 223.18833923339844
Loss :  1.5735366344451904 4.392354488372803 221.19125366210938
Loss :  1.6256282329559326 4.337739944458008 218.5126190185547
Loss :  1.685391902923584 4.621009826660156 232.7358856201172
Loss :  1.60028076171875 4.439063549041748 223.55345153808594
Loss :  1.5726712942123413 4.244855880737305 213.8154754638672
Loss :  1.6548068523406982 4.9232988357543945 247.8197479248047
Loss :  1.5801900625228882 4.361863613128662 219.67337036132812
Loss :  1.589096188545227 4.550386428833008 229.10841369628906
Loss :  1.6280286312103271 4.720109939575195 237.63351440429688
Loss :  1.594918966293335 4.5394392013549805 228.56687927246094
Loss :  1.688457727432251 4.365468502044678 219.96188354492188
Loss :  1.7020909786224365 4.621514320373535 232.77780151367188
Loss :  1.6473567485809326 4.395355701446533 221.41514587402344
Loss :  1.6696466207504272 4.367280006408691 220.0336456298828
Loss :  1.5731788873672485 4.451514720916748 224.14891052246094
Loss :  1.674486517906189 4.445291042327881 223.9390411376953
  batch 20 loss: 1.674486517906189, 4.445291042327881, 223.9390411376953
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.652292013168335 4.309190273284912 217.11181640625
Loss :  1.5729329586029053 4.3905029296875 221.09808349609375
Loss :  1.5795116424560547 4.54259729385376 228.70938110351562
Loss :  1.690565824508667 4.544604301452637 228.9207763671875
Loss :  1.672537922859192 4.644411087036133 233.89308166503906
Loss :  1.5914058685302734 4.5230841636657715 227.74560546875
Loss :  1.6559959650039673 4.363574028015137 219.83468627929688
Loss :  1.606991171836853 4.251507759094238 214.18238830566406
Loss :  1.5219721794128418 4.247013568878174 213.87265014648438
Loss :  1.6577205657958984 4.357679843902588 219.5417022705078
Loss :  1.4906452894210815 4.250180721282959 213.9996795654297
Loss :  1.634001612663269 4.456729412078857 224.47047424316406
Loss :  1.6006685495376587 4.124290943145752 207.81521606445312
Loss :  1.604809284210205 4.1407647132873535 208.64305114746094
Loss :  1.5055612325668335 4.394556999206543 221.2333984375
Loss :  1.5789620876312256 4.413017272949219 222.22982788085938
Loss :  1.567421317100525 4.4448747634887695 223.81117248535156
Loss :  1.6210355758666992 4.14279317855835 208.7606964111328
Loss :  1.6943219900131226 4.233819007873535 213.38526916503906
Loss :  1.6772241592407227 4.543279647827148 228.84121704101562
  batch 40 loss: 1.6772241592407227, 4.543279647827148, 228.84121704101562
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.6313550472259521 4.710440158843994 237.15335083007812
Loss :  1.5504690408706665 4.479698657989502 225.53541564941406
Loss :  1.5511908531188965 4.46578311920166 224.84034729003906
Loss :  1.5681002140045166 4.444066047668457 223.7714080810547
Loss :  1.5108050107955933 4.385010242462158 220.76132202148438
Loss :  1.590224027633667 4.40504789352417 221.84262084960938
Loss :  1.5940556526184082 4.291610240936279 216.174560546875
Loss :  1.5102227926254272 4.431909561157227 223.10569763183594
Loss :  1.6162829399108887 4.3990631103515625 221.56944274902344
Loss :  1.5592342615127563 4.573338985443115 230.22618103027344
Loss :  1.583065390586853 4.474945068359375 225.330322265625
Loss :  1.632824182510376 4.486963748931885 225.98101806640625
Loss :  1.4815704822540283 4.527416706085205 227.85240173339844
Loss :  1.635728120803833 4.620644569396973 232.66795349121094
Loss :  1.5906827449798584 4.6739277839660645 235.28707885742188
Loss :  1.5605477094650269 4.471742153167725 225.1476593017578
Loss :  1.5097211599349976 4.47665548324585 225.34249877929688
Loss :  1.5821181535720825 4.655637741088867 234.364013671875
Loss :  1.4631803035736084 4.728949546813965 237.91065979003906
Loss :  1.628842830657959 4.628290176391602 233.04335021972656
  batch 60 loss: 1.628842830657959, 4.628290176391602, 233.04335021972656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.5271135568618774 4.709677219390869 237.01097106933594
Loss :  1.6735148429870605 4.317934513092041 217.5702362060547
Loss :  1.5497709512710571 4.661816596984863 234.64060974121094
Loss :  1.55353581905365 4.530425548553467 228.07481384277344
Loss :  1.5588345527648926 4.182645797729492 210.69113159179688
Loss :  1.750105857849121 4.441195011138916 223.8098602294922
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.8548266887664795 4.415468215942383 222.62823486328125
Loss :  1.558009147644043 4.370061874389648 220.0611114501953
Loss :  1.556082010269165 4.2578444480896 214.44830322265625
Total LOSS train 224.44817903958835 valid 220.23687744140625
CE LOSS train 1.5998421430587768 valid 0.38902050256729126
Contrastive LOSS train 4.456966730264517 valid 1.0644611120224
EPOCH 51:
Loss :  1.6516739130020142 4.8527750968933105 244.29042053222656
Loss :  1.7325550317764282 4.556744575500488 229.56979370117188
Loss :  1.6121602058410645 4.427589416503906 222.99163818359375
Loss :  1.6425198316574097 4.577925205230713 230.5387725830078
Loss :  1.44649338722229 4.222995758056641 212.5962677001953
Loss :  1.636218547821045 4.338644504547119 218.5684356689453
Loss :  1.524644374847412 4.549058437347412 228.97756958007812
Loss :  1.6397732496261597 4.273213863372803 215.3004608154297
Loss :  1.5751949548721313 4.522036552429199 227.67701721191406
Loss :  1.7550725936889648 4.238865852355957 213.6983642578125
Loss :  1.5029178857803345 4.56150484085083 229.57815551757812
Loss :  1.4974780082702637 4.537248134613037 228.35989379882812
Loss :  1.5913584232330322 4.392289161682129 221.205810546875
Loss :  1.6248537302017212 4.514040946960449 227.32688903808594
Loss :  1.7701716423034668 4.570581436157227 230.2992401123047
Loss :  1.568113923072815 4.551340103149414 229.13511657714844
Loss :  1.6141763925552368 4.4064130783081055 221.93482971191406
Loss :  1.6670663356781006 4.50684118270874 227.00912475585938
Loss :  1.6349502801895142 4.633260726928711 233.29798889160156
Loss :  1.7586387395858765 4.620846748352051 232.8009796142578
  batch 20 loss: 1.7586387395858765, 4.620846748352051, 232.8009796142578
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.674688458442688 4.344406604766846 218.89501953125
Loss :  1.5525144338607788 4.473592758178711 225.23216247558594
Loss :  1.56429123878479 4.619782447814941 232.55340576171875
Loss :  1.6835651397705078 4.530911445617676 228.22914123535156
Loss :  1.8073604106903076 4.500977993011475 226.85626220703125
Loss :  1.6170201301574707 4.420788764953613 222.65646362304688
Loss :  1.8092780113220215 4.79402494430542 241.51052856445312
Loss :  1.7326459884643555 4.523018836975098 227.8835906982422
Loss :  1.596865177154541 4.468447208404541 225.01922607421875
Loss :  1.8034136295318604 4.330511093139648 218.3289794921875
Loss :  1.6140409708023071 4.489022254943848 226.06515502929688
Loss :  1.6426937580108643 4.555779457092285 229.43167114257812
Loss :  1.6699857711791992 4.554164409637451 229.37820434570312
Loss :  1.6777534484863281 4.492209434509277 226.28822326660156
Loss :  1.5957026481628418 4.406297206878662 221.9105682373047
Loss :  1.7035216093063354 4.544820308685303 228.9445343017578
Loss :  1.598067283630371 4.497535705566406 226.474853515625
Loss :  1.738284707069397 4.422860145568848 222.88128662109375
Loss :  1.5590344667434692 4.306131839752197 216.86563110351562
Loss :  1.6271387338638306 4.6242475509643555 232.83950805664062
  batch 40 loss: 1.6271387338638306, 4.6242475509643555, 232.83950805664062
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.7553199529647827 4.536919593811035 228.60130310058594
Loss :  1.6541515588760376 4.408034801483154 222.05587768554688
Loss :  1.6462781429290771 4.618317604064941 232.56214904785156
Loss :  1.6673048734664917 4.584066390991211 230.87062072753906
Loss :  1.6327848434448242 4.330857276916504 218.17564392089844
Loss :  1.6916037797927856 4.465264797210693 224.95484924316406
Loss :  1.8191910982131958 4.510073661804199 227.3228759765625
Loss :  1.6603204011917114 4.558096885681152 229.56517028808594
Loss :  1.6423193216323853 4.672893047332764 235.28697204589844
Loss :  1.6059010028839111 4.698846340179443 236.5482177734375
Loss :  1.638911485671997 4.539950847625732 228.63645935058594
Loss :  1.7599436044692993 4.335776329040527 218.54876708984375
Loss :  1.665265440940857 4.407132625579834 222.0218963623047
Loss :  1.6473191976547241 4.304511070251465 216.8728790283203
Loss :  1.682309627532959 4.489562511444092 226.16043090820312
Loss :  1.6968579292297363 4.572327136993408 230.31321716308594
Loss :  1.674680471420288 5.13679838180542 258.51458740234375
Loss :  1.6332041025161743 4.583761215209961 230.82127380371094
Loss :  1.6632013320922852 4.5202555656433105 227.6759796142578
Loss :  1.768213152885437 4.483246803283691 225.93055725097656
  batch 60 loss: 1.768213152885437, 4.483246803283691, 225.93055725097656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.628298282623291 4.546254634857178 228.94102478027344
Loss :  1.729089617729187 4.3921003341674805 221.3341064453125
Loss :  1.642308235168457 4.508019924163818 227.04330444335938
Loss :  1.6144096851348877 4.481689453125 225.69888305664062
Loss :  1.557368516921997 4.537971496582031 228.45594787597656
Loss :  1.5949640274047852 4.483725070953369 225.78121948242188
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1], device='cuda:0')
Loss :  1.6309866905212402 4.432501316070557 223.25604248046875
Loss :  1.6301430463790894 4.313109397888184 217.28561401367188
Loss :  1.4925537109375 4.206766605377197 211.8308868408203
Total LOSS train 227.0202181302584 valid 219.5384407043457
CE LOSS train 1.653699278831482 valid 0.373138427734375
Contrastive LOSS train 4.507330380953276 valid 1.0516916513442993
EPOCH 52:
Loss :  1.582931637763977 4.86255407333374 244.71063232421875
Loss :  1.566858172416687 4.592587947845459 231.19625854492188
Loss :  1.5979657173156738 4.388509273529053 221.02342224121094
Loss :  1.6837832927703857 4.558859825134277 229.62677001953125
Loss :  1.5502707958221436 4.255738735198975 214.33721923828125
Loss :  1.531862735748291 4.5808281898498535 230.57327270507812
Loss :  1.6546348333358765 4.46164083480835 224.73667907714844
Loss :  1.5668699741363525 4.695669651031494 236.350341796875
Loss :  1.6266430616378784 4.478148460388184 225.53407287597656
Loss :  1.6295080184936523 4.26375675201416 214.8173370361328
Loss :  1.6375638246536255 4.484784126281738 225.87677001953125
Loss :  1.6019959449768066 4.558944225311279 229.5491943359375
Loss :  1.5870485305786133 4.449690818786621 224.07159423828125
Loss :  1.6194812059402466 4.758755207061768 239.55723571777344
Loss :  1.7169374227523804 4.535026550292969 228.46826171875
Loss :  1.6564688682556152 4.4986114501953125 226.5870361328125
Loss :  1.5548955202102661 4.373616695404053 220.23573303222656
Loss :  1.618324637413025 4.366873264312744 219.9619903564453
Loss :  1.6162962913513184 4.386598110198975 220.9462127685547
Loss :  1.572013258934021 4.421265602111816 222.63528442382812
  batch 20 loss: 1.572013258934021, 4.421265602111816, 222.63528442382812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.644362211227417 4.3683648109436035 220.06260681152344
Loss :  1.6223092079162598 4.536864757537842 228.46554565429688
Loss :  1.623003602027893 4.816472053527832 242.4466094970703
Loss :  1.6945323944091797 4.475032329559326 225.44615173339844
Loss :  1.7781658172607422 4.805081844329834 242.03225708007812
Loss :  1.606966257095337 4.621678352355957 232.69088745117188
Loss :  1.6826207637786865 4.660510063171387 234.70811462402344
Loss :  1.6434739828109741 4.331361770629883 218.21156311035156
Loss :  1.5409692525863647 4.449887752532959 224.0353546142578
Loss :  1.5698001384735107 4.443479537963867 223.7437744140625
Loss :  1.5018656253814697 4.528128147125244 227.90826416015625
Loss :  1.5745164155960083 4.638131141662598 233.4810791015625
Loss :  1.6228864192962646 4.393058776855469 221.27581787109375
Loss :  1.6313055753707886 4.4802680015563965 225.6446990966797
Loss :  1.501329779624939 4.619424819946289 232.47256469726562
Loss :  1.5107371807098389 4.506752967834473 226.848388671875
Loss :  1.5459238290786743 4.4519572257995605 224.14378356933594
Loss :  1.6458227634429932 4.418211936950684 222.55642700195312
Loss :  1.5605287551879883 4.34354829788208 218.73794555664062
Loss :  1.5373529195785522 4.924636363983154 247.7691650390625
  batch 40 loss: 1.5373529195785522, 4.924636363983154, 247.7691650390625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.584141492843628 4.597090721130371 231.4386749267578
Loss :  1.4874258041381836 4.489062309265137 225.94053649902344
Loss :  1.5344653129577637 4.451175212860107 224.09323120117188
Loss :  1.5547235012054443 4.908247470855713 246.9670867919922
Loss :  1.5346899032592773 4.333320140838623 218.20068359375
Loss :  1.626173973083496 4.428843021392822 223.06832885742188
Loss :  1.5059702396392822 4.551852703094482 229.09860229492188
Loss :  1.5327333211898804 4.637240409851074 233.39474487304688
Loss :  1.672363519668579 4.49141788482666 226.24325561523438
Loss :  1.5707429647445679 4.672211647033691 235.1813201904297
Loss :  1.5024800300598145 4.526115894317627 227.80828857421875
Loss :  1.6584306955337524 4.404752731323242 221.8960723876953
Loss :  1.5443214178085327 4.343904495239258 218.7395477294922
Loss :  1.5475661754608154 4.4810943603515625 225.60227966308594
Loss :  1.5409250259399414 4.388642311096191 220.97303771972656
Loss :  1.5841878652572632 4.469515323638916 225.05995178222656
Loss :  1.648026943206787 4.546950817108154 228.9955596923828
Loss :  1.5293461084365845 4.510435581207275 227.05111694335938
Loss :  1.5126914978027344 4.646913528442383 233.85836791992188
Loss :  1.7433724403381348 4.432666301727295 223.37669372558594
  batch 60 loss: 1.7433724403381348, 4.432666301727295, 223.37669372558594
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.449764370918274 4.561746120452881 229.53707885742188
Loss :  1.6264909505844116 4.60894250869751 232.07362365722656
Loss :  1.5946904420852661 4.795143127441406 241.3518524169922
Loss :  1.5309734344482422 4.285501003265381 215.8060302734375
Loss :  1.4665417671203613 4.355573654174805 219.24522399902344
Loss :  1.4626399278640747 4.408742904663086 221.89979553222656
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.4799129962921143 4.396955490112305 221.32769775390625
Loss :  1.4357173442840576 4.296120643615723 216.2417449951172
Loss :  1.6177664995193481 4.237867832183838 213.5111541748047
Total LOSS train 227.5150385929988 valid 218.24509811401367
CE LOSS train 1.5891394743552574 valid 0.40444162487983704
Contrastive LOSS train 4.5185180003826435 valid 1.0594669580459595
EPOCH 53:
Loss :  1.6510781049728394 4.363461494445801 219.82415771484375
Loss :  1.6024048328399658 4.6408843994140625 233.64662170410156
Loss :  1.5122891664505005 4.641735553741455 233.59906005859375
Loss :  1.6500518321990967 4.417738437652588 222.53697204589844
Loss :  1.543623447418213 4.196432113647461 211.365234375
Loss :  1.4826611280441284 4.370375633239746 220.00144958496094
Loss :  1.5494449138641357 4.514479160308838 227.2733917236328
Loss :  1.531325101852417 4.405679702758789 221.8153076171875
Loss :  1.5363694429397583 4.46712064743042 224.8924102783203
Loss :  1.6606096029281616 4.385453224182129 220.9332733154297
Loss :  1.5800279378890991 4.622420310974121 232.7010498046875
Loss :  1.4428783655166626 4.425880432128906 222.7368927001953
Loss :  1.4905949831008911 4.47761869430542 225.3715362548828
Loss :  1.5413168668746948 4.55119514465332 229.10107421875
Loss :  1.6138278245925903 4.413086891174316 222.26817321777344
Loss :  1.616842269897461 4.650418281555176 234.13775634765625
Loss :  1.4663467407226562 4.476841926574707 225.30844116210938
Loss :  1.5403494834899902 4.564204692840576 229.75057983398438
Loss :  1.4939223527908325 4.455460548400879 224.26695251464844
Loss :  1.6050348281860352 4.3956170082092285 221.38589477539062
  batch 20 loss: 1.6050348281860352, 4.3956170082092285, 221.38589477539062
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.5744361877441406 4.432207107543945 223.18478393554688
Loss :  1.5393264293670654 4.4670729637146 224.89297485351562
Loss :  1.5696808099746704 4.597472190856934 231.44329833984375
Loss :  1.6243871450424194 4.384617805480957 220.85528564453125
Loss :  1.6408034563064575 4.662648677825928 234.7732391357422
Loss :  1.5816352367401123 4.39395809173584 221.279541015625
Loss :  1.553174376487732 4.629061222076416 233.00624084472656
Loss :  1.5332510471343994 4.455680847167969 224.31729125976562
Loss :  1.4398306608200073 4.358949184417725 219.38729858398438
Loss :  1.5619735717773438 4.385190010070801 220.82147216796875
Loss :  1.4572385549545288 4.322700500488281 217.59226989746094
Loss :  1.5735068321228027 4.564929008483887 229.8199462890625
Loss :  1.5428080558776855 4.39233922958374 221.15975952148438
Loss :  1.5781424045562744 4.424458980560303 222.80108642578125
Loss :  1.4816279411315918 4.4833478927612305 225.64901733398438
Loss :  1.5252995491027832 4.500007629394531 226.5256805419922
Loss :  1.5006059408187866 4.641523361206055 233.5767822265625
Loss :  1.5809178352355957 4.489900588989258 226.07594299316406
Loss :  1.5748512744903564 4.262609958648682 214.70533752441406
Loss :  1.5879108905792236 4.364956378936768 219.83572387695312
  batch 40 loss: 1.5879108905792236, 4.364956378936768, 219.83572387695312
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.505628228187561 4.4588236808776855 224.44680786132812
Loss :  1.4454314708709717 4.416463375091553 222.2686004638672
Loss :  1.5489224195480347 4.688164234161377 235.95713806152344
Loss :  1.579764485359192 4.444883823394775 223.82394409179688
Loss :  1.5155221223831177 4.297835350036621 216.40728759765625
Loss :  1.5689226388931274 4.67238712310791 235.1882781982422
Loss :  1.5960419178009033 4.372565746307373 220.2243194580078
Loss :  1.554384469985962 4.522543430328369 227.68154907226562
Loss :  1.5597771406173706 4.517187118530273 227.4191436767578
Loss :  1.5323290824890137 4.3957839012146 221.321533203125
Loss :  1.5774040222167969 4.443719387054443 223.7633819580078
Loss :  1.5966863632202148 4.497169017791748 226.4551239013672
Loss :  1.5198179483413696 4.5014142990112305 226.5905303955078
Loss :  1.515153169631958 4.309835433959961 217.00692749023438
Loss :  1.5386788845062256 4.717402458190918 237.40879821777344
Loss :  1.6264110803604126 4.424941062927246 222.8734588623047
Loss :  1.4860563278198242 4.457882881164551 224.3802032470703
Loss :  1.551570177078247 4.458024978637695 224.45281982421875
Loss :  1.5603687763214111 4.602784633636475 231.69960021972656
Loss :  1.582993507385254 4.367922306060791 219.97911071777344
  batch 60 loss: 1.582993507385254, 4.367922306060791, 219.97911071777344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.556556224822998 4.5308356285095215 228.09832763671875
Loss :  1.527509331703186 4.644955158233643 233.7752685546875
Loss :  1.550112247467041 4.445694923400879 223.83485412597656
Loss :  1.477462887763977 4.424498558044434 222.702392578125
Loss :  1.483345866203308 4.146320819854736 208.7993927001953
Loss :  1.3662433624267578 4.425058364868164 222.61915588378906
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.4349595308303833 4.426257133483887 222.74781799316406
Loss :  1.4154852628707886 4.3397111892700195 218.4010467529297
Loss :  1.4121606349945068 4.2643842697143555 214.63136291503906
Total LOSS train 224.94119990422175 valid 219.59984588623047
CE LOSS train 1.5490655110432552 valid 0.3530401587486267
Contrastive LOSS train 4.46784269626324 valid 1.0660960674285889
EPOCH 54:
Loss :  1.5414170026779175 4.477999687194824 225.44139099121094
Loss :  1.61127507686615 4.652403831481934 234.23147583007812
Loss :  1.5779005289077759 4.566850662231445 229.92042541503906
Loss :  1.5427762269973755 4.471631050109863 225.12432861328125
Loss :  1.6429673433303833 4.404767036437988 221.88133239746094
Loss :  1.5253698825836182 4.418049335479736 222.42784118652344
Loss :  1.60408353805542 4.5071702003479 226.96258544921875
Loss :  1.5449421405792236 4.3138628005981445 217.2380828857422
Loss :  1.4960510730743408 4.453704833984375 224.18128967285156
Loss :  1.5499845743179321 4.333420753479004 218.2210235595703
Loss :  1.468037486076355 4.490618705749512 225.99896240234375
Loss :  1.5816906690597534 4.60681676864624 231.9225311279297
Loss :  1.4874051809310913 4.490677833557129 226.02130126953125
Loss :  1.5432921648025513 4.599426746368408 231.51463317871094
Loss :  1.548134684562683 4.511731147766113 227.13470458984375
Loss :  1.6781197786331177 4.504037380218506 226.87998962402344
Loss :  1.537996530532837 4.37864351272583 220.4701690673828
Loss :  1.550814151763916 4.420217037200928 222.56166076660156
Loss :  1.5188933610916138 4.381264686584473 220.58212280273438
Loss :  1.6724157333374023 4.464459419250488 224.8953857421875
  batch 20 loss: 1.6724157333374023, 4.464459419250488, 224.8953857421875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.5476104021072388 4.394678592681885 221.2815399169922
Loss :  1.5219120979309082 4.573198318481445 230.18182373046875
Loss :  1.5552512407302856 4.601292133331299 231.61985778808594
Loss :  1.552477478981018 4.548553943634033 228.9801788330078
Loss :  1.5509587526321411 4.795234203338623 241.3126678466797
Loss :  1.4901447296142578 4.556124210357666 229.29635620117188
Loss :  1.4734810590744019 4.6790337562561035 235.4251708984375
Loss :  1.5361814498901367 4.337332248687744 218.4027862548828
Loss :  1.4674925804138184 4.448701858520508 223.902587890625
Loss :  1.5848243236541748 4.442800521850586 223.724853515625
Loss :  1.4558284282684326 4.582674980163574 230.58956909179688
Loss :  1.5531378984451294 4.663322448730469 234.71925354003906
Loss :  1.5602562427520752 4.457974910736084 224.45899963378906
Loss :  1.5421658754348755 4.492741107940674 226.17921447753906
Loss :  1.4838968515396118 4.395928382873535 221.2803192138672
Loss :  1.5117275714874268 4.451636791229248 224.09356689453125
Loss :  1.4861574172973633 4.428281784057617 222.90025329589844
Loss :  1.6033425331115723 4.344287395477295 218.81771850585938
Loss :  1.537537693977356 4.3881611824035645 220.9456024169922
Loss :  1.6501132249832153 4.520988941192627 227.69956970214844
  batch 40 loss: 1.6501132249832153, 4.520988941192627, 227.69956970214844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.5354831218719482 4.492690086364746 226.16998291015625
Loss :  1.508304476737976 4.315086841583252 217.2626495361328
Loss :  1.5178589820861816 4.561857223510742 229.6107177734375
Loss :  1.5038427114486694 4.47376823425293 225.1922607421875
Loss :  1.5064839124679565 4.512096881866455 227.111328125
Loss :  1.562240719795227 4.414135932922363 222.26904296875
Loss :  1.6070016622543335 4.523017883300781 227.7578887939453
Loss :  1.5294102430343628 4.5345072746276855 228.2547607421875
Loss :  1.5769479274749756 4.482666492462158 225.7102813720703
Loss :  1.5342624187469482 4.546452045440674 228.8568572998047
Loss :  1.5677634477615356 4.5237812995910645 227.7568359375
Loss :  1.5383641719818115 4.462333679199219 224.65504455566406
Loss :  1.5906734466552734 4.4292988777160645 223.0556182861328
Loss :  1.5642958879470825 4.304671764373779 216.79788208007812
Loss :  1.5376951694488525 4.4850287437438965 225.78912353515625
Loss :  1.6180657148361206 4.392280578613281 221.2321014404297
Loss :  1.5469452142715454 4.59383487701416 231.2386932373047
Loss :  1.498293399810791 4.455046653747559 224.25062561035156
Loss :  1.4594320058822632 4.780344009399414 240.47662353515625
Loss :  1.692546010017395 4.40535306930542 221.960205078125
  batch 60 loss: 1.692546010017395, 4.40535306930542, 221.960205078125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.4566693305969238 4.461331367492676 224.5232391357422
Loss :  1.5044448375701904 4.426187992095947 222.8138427734375
Loss :  1.542085886001587 4.547142505645752 228.8992156982422
Loss :  1.5139148235321045 4.501466751098633 226.58724975585938
Loss :  1.4347952604293823 4.591262340545654 230.99790954589844
Loss :  1.457940697669983 4.466490745544434 224.78248596191406
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.4749759435653687 4.44399881362915 223.67491149902344
Loss :  1.4621691703796387 4.296764850616455 216.3004150390625
Loss :  1.555652379989624 4.357801914215088 219.44573974609375
Total LOSS train 225.97927856445312 valid 221.05088806152344
CE LOSS train 1.5436290117410514 valid 0.388913094997406
Contrastive LOSS train 4.48871299303495 valid 1.089450478553772
EPOCH 55:
Loss :  1.559460163116455 4.358119487762451 219.46543884277344
Loss :  1.6122443675994873 4.605641841888428 231.8943328857422
Loss :  1.5495473146438599 4.541560173034668 228.62754821777344
Loss :  1.572072148323059 4.767568111419678 239.95046997070312
Loss :  1.5936423540115356 4.6372599601745605 233.45663452148438
Loss :  1.4935638904571533 4.916828632354736 247.33499145507812
Loss :  1.5471222400665283 4.729112148284912 238.0027313232422
Loss :  1.5502878427505493 4.458274841308594 224.4640350341797
Loss :  1.5095443725585938 4.5187530517578125 227.44720458984375
Loss :  1.6161720752716064 4.291327476501465 216.1825408935547
Loss :  1.5019862651824951 4.4455952644348145 223.78175354003906
Loss :  1.5185320377349854 4.490536212921143 226.04534912109375
Loss :  1.4628444910049438 4.684272289276123 235.67645263671875
Loss :  1.5044363737106323 4.780543327331543 240.53160095214844
Loss :  1.558092474937439 4.708290100097656 236.97259521484375
Loss :  1.5327340364456177 4.633311748504639 233.1983184814453
Loss :  1.5091090202331543 4.471400737762451 225.0791473388672
Loss :  1.491938591003418 4.41509485244751 222.24668884277344
Loss :  1.4469071626663208 4.431375026702881 223.0156707763672
Loss :  1.6242139339447021 4.432349681854248 223.24168395996094
  batch 20 loss: 1.6242139339447021, 4.432349681854248, 223.24168395996094
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.5391879081726074 4.3212666511535645 217.60252380371094
Loss :  1.478351354598999 4.5090742111206055 226.9320526123047
Loss :  1.4673621654510498 4.557281494140625 229.33143615722656
Loss :  1.5067766904830933 4.5307087898254395 228.04222106933594
Loss :  1.5160118341445923 4.63818883895874 233.42544555664062
Loss :  1.4526728391647339 4.339474678039551 218.42640686035156
Loss :  1.4915534257888794 4.624191761016846 232.70114135742188
Loss :  1.455488920211792 4.358390808105469 219.37503051757812
Loss :  1.3944791555404663 4.480730056762695 225.4309844970703
Loss :  1.5175327062606812 4.480952262878418 225.5651397705078
Loss :  1.3717797994613647 4.584192276000977 230.58139038085938
Loss :  1.5406399965286255 4.883883953094482 245.73483276367188
Loss :  1.4839799404144287 4.500596046447754 226.51377868652344
Loss :  1.4419288635253906 4.4438395500183105 223.63389587402344
Loss :  1.3906022310256958 4.561126708984375 229.4469451904297
Loss :  1.4169831275939941 4.567434787750244 229.78871154785156
Loss :  1.4426158666610718 4.430542469024658 222.96974182128906
Loss :  1.5251867771148682 4.408923625946045 221.97137451171875
Loss :  1.498201608657837 4.338068008422852 218.40159606933594
Loss :  1.5002939701080322 4.962156295776367 249.6081085205078
  batch 40 loss: 1.5002939701080322, 4.962156295776367, 249.6081085205078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.4692741632461548 4.580999851226807 230.51925659179688
Loss :  1.4093271493911743 4.755772113800049 239.19793701171875
Loss :  1.4374988079071045 4.585244178771973 230.69970703125
Loss :  1.4575488567352295 4.523858070373535 227.65045166015625
Loss :  1.407204508781433 4.250471115112305 213.93077087402344
Loss :  1.4916460514068604 4.481330394744873 225.55816650390625
Loss :  1.494472622871399 4.604040145874023 231.6964874267578
Loss :  1.43422532081604 4.446233749389648 223.74591064453125
Loss :  1.5715049505233765 4.435088157653809 223.32591247558594
Loss :  1.457628607749939 4.511366844177246 227.02597045898438
Loss :  1.5572209358215332 4.5738139152526855 230.24790954589844
Loss :  1.4581044912338257 4.9252519607543945 247.720703125
Loss :  1.423572063446045 4.3652215003967285 219.6846466064453
Loss :  1.512216329574585 4.337695598602295 218.39700317382812
Loss :  1.426269769668579 4.502812385559082 226.56689453125
Loss :  1.5185999870300293 4.430160999298096 223.0266571044922
Loss :  1.4394973516464233 4.464576244354248 224.66830444335938
Loss :  1.4052191972732544 4.3921380043029785 221.01211547851562
Loss :  1.4606918096542358 4.434518337249756 223.18661499023438
Loss :  1.5760267972946167 4.298906326293945 216.52133178710938
  batch 60 loss: 1.5760267972946167, 4.298906326293945, 216.52133178710938
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.3886791467666626 4.440402030944824 223.4087677001953
Loss :  1.443642020225525 4.171923637390137 210.03982543945312
Loss :  1.4053623676300049 4.229251861572266 212.86795043945312
Loss :  1.3922101259231567 4.260436534881592 214.41403198242188
Loss :  1.3299193382263184 4.025242805480957 202.59207153320312
Loss :  1.3458343744277954 4.3895416259765625 220.8229217529297
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.3690303564071655 4.284865856170654 215.61231994628906
Loss :  1.3686258792877197 4.268128395080566 214.77503967285156
Loss :  1.3818951845169067 4.1226911544799805 207.51644897460938
Total LOSS train 226.76620530348558 valid 214.68168258666992
CE LOSS train 1.4854406631909884 valid 0.3454737961292267
Contrastive LOSS train 4.505615307734563 valid 1.0306727886199951
EPOCH 56:
Loss :  1.4604220390319824 4.143179893493652 208.61941528320312
Loss :  1.4525699615478516 4.311210632324219 217.0131072998047
Loss :  1.43667733669281 4.103830337524414 206.62818908691406
Loss :  1.4708101749420166 4.473118305206299 225.12672424316406
Loss :  1.5011699199676514 4.302429676055908 216.62266540527344
Loss :  1.3827263116836548 4.335517883300781 218.1586151123047
Loss :  1.4696398973464966 4.230032920837402 212.97128295898438
Loss :  1.4382221698760986 4.10557222366333 206.71682739257812
Loss :  1.3997015953063965 4.053970813751221 204.09825134277344
Loss :  1.4852999448776245 4.306366443634033 216.8036346435547
Loss :  1.406266212463379 4.354891300201416 219.1508331298828
Loss :  1.3796169757843018 4.4149699211120605 222.12811279296875
Loss :  1.4053667783737183 4.497503280639648 226.28053283691406
Loss :  1.4227538108825684 4.52545690536499 227.6956024169922
Loss :  1.5090399980545044 4.472411632537842 225.12960815429688
Loss :  1.5018839836120605 4.1456732749938965 208.78553771972656
Loss :  1.3887653350830078 4.1660990715026855 209.69371032714844
Loss :  1.4347600936889648 4.213875770568848 212.1285400390625
Loss :  1.392464518547058 3.970024824142456 199.89370727539062
Loss :  1.4935017824172974 3.879735231399536 195.48025512695312
  batch 20 loss: 1.4935017824172974, 3.879735231399536, 195.48025512695312
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.457932710647583 3.9591472148895264 199.41529846191406
Loss :  1.3940123319625854 4.202985763549805 211.54330444335938
Loss :  1.4412721395492554 4.444470405578613 223.664794921875
Loss :  1.466347575187683 4.3101806640625 216.9753875732422
Loss :  1.501998782157898 4.482355117797852 225.6197509765625
Loss :  1.442681908607483 4.321637153625488 217.52455139160156
Loss :  1.4377280473709106 4.549355983734131 228.90553283691406
Loss :  1.4540507793426514 4.424508094787598 222.67945861816406
Loss :  1.3469651937484741 4.55238676071167 228.96630859375
Loss :  1.522550344467163 4.3367919921875 218.36215209960938
Loss :  1.3525669574737549 4.508321285247803 226.7686309814453
Loss :  1.4979193210601807 4.519624710083008 227.47915649414062
Loss :  1.4482967853546143 4.290676593780518 215.9821319580078
Loss :  1.4489448070526123 4.259888648986816 214.44337463378906
Loss :  1.3794282674789429 4.375366687774658 220.14776611328125
Loss :  1.405474066734314 4.1858320236206055 210.6970672607422
Loss :  1.411712646484375 4.102686882019043 206.54605102539062
Loss :  1.5424559116363525 4.409694194793701 222.02716064453125
Loss :  1.526899814605713 4.359805107116699 219.51715087890625
Loss :  1.5517204999923706 4.579329967498779 230.51821899414062
  batch 40 loss: 1.5517204999923706, 4.579329967498779, 230.51821899414062
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.4750785827636719 4.612302780151367 232.09022521972656
Loss :  1.4407283067703247 4.265260219573975 214.70375061035156
Loss :  1.4469348192214966 4.243901252746582 213.64199829101562
Loss :  1.465561032295227 4.322835922241211 217.60736083984375
Loss :  1.4309475421905518 4.2327704429626465 213.06947326660156
Loss :  1.473307490348816 4.295039176940918 216.2252655029297
Loss :  1.5208030939102173 4.304057598114014 216.7236785888672
Loss :  1.4541525840759277 4.3405256271362305 218.4804229736328
Loss :  1.560023546218872 4.460744857788086 224.59727478027344
Loss :  1.449874758720398 4.417415142059326 222.3206329345703
Loss :  1.5087074041366577 4.4711503982543945 225.0662384033203
Loss :  1.5010539293289185 4.138975143432617 208.44981384277344
Loss :  1.47140634059906 4.265174865722656 214.7301483154297
Loss :  1.511343240737915 4.482934951782227 225.6580810546875
Loss :  1.4724905490875244 4.414633750915527 222.2041778564453
Loss :  1.5519157648086548 4.128642559051514 207.98403930664062
Loss :  1.437735676765442 4.265670299530029 214.7212371826172
Loss :  1.4283643960952759 4.359832763671875 219.4199981689453
Loss :  1.4656695127487183 4.566428184509277 229.78707885742188
Loss :  1.5855281352996826 4.466470241546631 224.90904235839844
  batch 60 loss: 1.5855281352996826, 4.466470241546631, 224.90904235839844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.4099265336990356 4.162432670593262 209.53155517578125
Loss :  1.4753648042678833 4.376837253570557 220.31723022460938
Loss :  1.4371534585952759 4.155041217803955 209.189208984375
Loss :  1.410874366760254 4.0863142013549805 205.72657775878906
Loss :  1.3671650886535645 3.6119565963745117 181.96499633789062
Loss :  1.4678086042404175 4.329298496246338 217.93272399902344
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.4979181289672852 4.271474838256836 215.07167053222656
Loss :  1.5269533395767212 4.287703514099121 215.91212463378906
Loss :  1.472000002861023 3.9596173763275146 199.45286560058594
Total LOSS train 216.5542755126953 valid 212.09234619140625
CE LOSS train 1.4556112106029804 valid 0.36800000071525574
Contrastive LOSS train 4.301973287875835 valid 0.9899043440818787
EPOCH 57:
Loss :  1.497759222984314 4.087219715118408 205.85874938964844
Loss :  1.5044760704040527 4.396681308746338 221.33853149414062
Loss :  1.4503388404846191 4.085870265960693 205.7438507080078
Loss :  1.4760406017303467 4.4191975593566895 222.43592834472656
Loss :  1.507534384727478 4.107062339782715 206.86065673828125
Loss :  1.4337491989135742 4.105221271514893 206.69480895996094
Loss :  1.5060831308364868 4.4763617515563965 225.3241729736328
Loss :  1.456878662109375 4.475057601928711 225.2097625732422
Loss :  1.4382667541503906 4.257288932800293 214.30270385742188
Loss :  1.5040374994277954 4.486708641052246 225.8394775390625
Loss :  1.4201432466506958 4.095632076263428 206.20175170898438
Loss :  1.4161906242370605 4.399096488952637 221.37100219726562
Loss :  1.4147264957427979 4.423638820648193 222.5966796875
Loss :  1.4328125715255737 3.952509641647339 199.0583038330078
Loss :  1.5315574407577515 3.8742196559906006 195.24253845214844
Loss :  1.520371675491333 3.941502332687378 198.59548950195312
Loss :  1.3985000848770142 3.6821367740631104 185.50534057617188
Loss :  1.452052116394043 3.7275190353393555 187.8280029296875
Loss :  1.397533893585205 3.590153932571411 180.9052276611328
Loss :  1.5070143938064575 3.7801475524902344 190.514404296875
  batch 20 loss: 1.5070143938064575, 3.7801475524902344, 190.514404296875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.4483879804611206 3.9506444931030273 198.98062133789062
Loss :  1.392460584640503 3.995351552963257 201.1600341796875
Loss :  1.4301453828811646 3.6121184825897217 182.03607177734375
Loss :  1.4696052074432373 3.641460418701172 183.54263305664062
Loss :  1.5064529180526733 3.8996973037719727 196.49131774902344
Loss :  1.4282770156860352 3.5584206581115723 179.3493194580078
Loss :  1.446121096611023 4.151134967803955 209.00286865234375
Loss :  1.4452810287475586 3.8015289306640625 191.521728515625
Loss :  1.332668423652649 4.015515327453613 202.1084442138672
Loss :  1.508671522140503 3.7397639751434326 188.4968719482422
Loss :  1.3335829973220825 3.9918630123138428 200.92674255371094
Loss :  1.492107629776001 4.011301040649414 202.05715942382812
Loss :  1.4233901500701904 3.791900157928467 191.0183868408203
Loss :  1.416947841644287 3.5289220809936523 177.86305236816406
Loss :  1.3462063074111938 3.6064400672912598 181.668212890625
Loss :  1.3751838207244873 3.5431110858917236 178.53073120117188
Loss :  1.3679674863815308 3.388171434402466 170.7765350341797
Loss :  1.5025380849838257 3.6931569576263428 186.16038513183594
Loss :  1.488211750984192 3.638779640197754 183.42718505859375
Loss :  1.515945315361023 3.8824331760406494 195.63760375976562
  batch 40 loss: 1.515945315361023, 3.8824331760406494, 195.63760375976562
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.4346487522125244 3.6465988159179688 183.76458740234375
Loss :  1.3947362899780273 3.4457178115844727 173.6806182861328
Loss :  1.3887169361114502 3.5245327949523926 177.6153564453125
Loss :  1.4077271223068237 3.6194300651550293 182.3792266845703
Loss :  1.3696116209030151 3.362079381942749 169.4735870361328
Loss :  1.4215072393417358 3.8753671646118164 195.1898651123047
Loss :  1.4878480434417725 3.778306484222412 190.40318298339844
Loss :  1.4043262004852295 3.6010210514068604 181.45538330078125
Loss :  1.5264204740524292 3.625681161880493 182.8104705810547
Loss :  1.4118123054504395 3.60153865814209 181.48875427246094
Loss :  1.4838286638259888 3.5186586380004883 177.41676330566406
Loss :  1.4726508855819702 3.6361753940582275 183.28143310546875
Loss :  1.4242860078811646 3.551476240158081 178.9980926513672
Loss :  1.4775227308273315 3.8596062660217285 194.4578399658203
Loss :  1.4042383432388306 3.770297050476074 189.91908264160156
Loss :  1.5237455368041992 3.540334463119507 178.54046630859375
Loss :  1.4030400514602661 3.7665607929229736 189.7310791015625
Loss :  1.38346266746521 3.663525104522705 184.55972290039062
Loss :  1.4105277061462402 3.6453661918640137 183.6788330078125
Loss :  1.5468045473098755 3.5401763916015625 178.5556182861328
  batch 60 loss: 1.5468045473098755, 3.5401763916015625, 178.5556182861328
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.3857223987579346 3.7207260131835938 187.42202758789062
Loss :  1.456851601600647 3.647054433822632 183.8095703125
Loss :  1.4150513410568237 3.7315175533294678 187.99093627929688
Loss :  1.3872544765472412 4.0495805740356445 203.8662872314453
Loss :  1.336965560913086 3.6604206562042236 184.3579864501953
Loss :  1.4770113229751587 3.926928997039795 197.82345581054688
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5018364191055298 3.9051849842071533 196.76107788085938
Loss :  1.4982094764709473 3.8373935222625732 193.36788940429688
Loss :  1.5092378854751587 3.719452142715454 187.48184204101562
Total LOSS train 193.12353938176082 valid 193.8585662841797
CE LOSS train 1.4414696455001832 valid 0.3773094713687897
Contrastive LOSS train 3.8336413786961483 valid 0.9298630356788635
Saved best model. Old loss 203.5328025817871 and new best loss 193.8585662841797
EPOCH 58:
Loss :  1.4650429487228394 3.793959140777588 191.16299438476562
Loss :  1.4961234331130981 4.065176010131836 204.7549285888672
Loss :  1.4314212799072266 3.484027624130249 175.6328125
Loss :  1.4479018449783325 3.7237555980682373 187.63568115234375
Loss :  1.4784212112426758 4.089105129241943 205.93368530273438
Loss :  1.4051122665405273 3.721055030822754 187.45785522460938
Loss :  1.4793287515640259 3.6153340339660645 182.24603271484375
Loss :  1.421510100364685 3.378655433654785 170.35427856445312
Loss :  1.4022256135940552 3.621751070022583 182.48977661132812
Loss :  1.4676272869110107 3.7455496788024902 188.74510192871094
Loss :  1.3757028579711914 3.8440539836883545 193.57839965820312
Loss :  1.3755171298980713 3.8500635623931885 193.87869262695312
Loss :  1.3744539022445679 3.621124267578125 182.4306640625
Loss :  1.3906021118164062 3.7243292331695557 187.6070556640625
Loss :  1.5007518529891968 3.601917266845703 181.5966033935547
Loss :  1.484493613243103 4.165547847747803 209.7618865966797
Loss :  1.3698087930679321 4.2244672775268555 212.59317016601562
Loss :  1.418054461479187 4.143260478973389 208.58108520507812
Loss :  1.3682312965393066 4.397299289703369 221.23318481445312
Loss :  1.4956836700439453 4.183217525482178 210.65655517578125
  batch 20 loss: 1.4956836700439453, 4.183217525482178, 210.65655517578125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.4265741109848022 4.07611608505249 205.2323760986328
Loss :  1.3661518096923828 3.866837501525879 194.70802307128906
Loss :  1.407511591911316 4.074379920959473 205.1265106201172
Loss :  1.455430030822754 4.342103958129883 218.5606231689453
Loss :  1.4943522214889526 4.441323757171631 223.560546875
Loss :  1.401795744895935 4.551376819610596 228.97064208984375
Loss :  1.423667311668396 4.761709213256836 239.50912475585938
Loss :  1.4474042654037476 4.148949146270752 208.89486694335938
Loss :  1.3184815645217896 4.2616190910339355 214.39942932128906
Loss :  1.512033224105835 4.155751705169678 209.29962158203125
Loss :  1.3219772577285767 4.023592472076416 202.50160217285156
Loss :  1.4908957481384277 4.244986534118652 213.74021911621094
Loss :  1.4292575120925903 4.5823540687561035 230.54696655273438
Loss :  1.4199539422988892 4.3734259605407715 220.09124755859375
Loss :  1.3466739654541016 4.277675151824951 215.23043823242188
Loss :  1.385095238685608 4.314738750457764 217.12203979492188
Loss :  1.3898217678070068 4.263376235961914 214.55862426757812
Loss :  1.5256339311599731 4.351078510284424 219.07955932617188
Loss :  1.5147398710250854 4.418646812438965 222.44708251953125
Loss :  1.5528967380523682 4.431162357330322 223.11102294921875
  batch 40 loss: 1.5528967380523682, 4.431162357330322, 223.11102294921875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.4741904735565186 4.416690349578857 222.3087158203125
Loss :  1.4216160774230957 4.3517913818359375 219.0111846923828
Loss :  1.4181640148162842 4.280972480773926 215.466796875
Loss :  1.437300682067871 4.364477634429932 219.6611785888672
Loss :  1.3936755657196045 4.348301410675049 218.80874633789062
Loss :  1.4412684440612793 4.0380072593688965 203.3416290283203
Loss :  1.4838767051696777 4.24680757522583 213.82424926757812
Loss :  1.4254050254821777 4.354148864746094 219.13284301757812
Loss :  1.5527288913726807 4.419643878936768 222.53492736816406
Loss :  1.4292430877685547 4.102579593658447 206.5582275390625
Loss :  1.504215121269226 4.1614813804626465 209.57827758789062
Loss :  1.4901145696640015 4.181164264678955 210.54832458496094
Loss :  1.4436231851577759 4.302996635437012 216.59344482421875
Loss :  1.4976388216018677 4.438912391662598 223.44325256347656
Loss :  1.417262315750122 4.386759281158447 220.75523376464844
Loss :  1.563337802886963 4.052493095397949 204.18798828125
Loss :  1.419339656829834 4.234342098236084 213.13644409179688
Loss :  1.3927745819091797 4.320795059204102 217.43252563476562
Loss :  1.4240877628326416 4.229712009429932 212.90968322753906
Loss :  1.5639424324035645 4.028607368469238 202.99432373046875
  batch 60 loss: 1.5639424324035645, 4.028607368469238, 202.99432373046875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.3805747032165527 4.150118350982666 208.88648986816406
Loss :  1.4593548774719238 3.873889446258545 195.15382385253906
Loss :  1.4142446517944336 3.869159460067749 194.87222290039062
Loss :  1.386938452720642 4.04373836517334 203.5738525390625
Loss :  1.338446021080017 3.7573933601379395 189.20811462402344
Loss :  1.3490036725997925 3.983218193054199 200.50990295410156
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.386856198310852 4.050745964050293 203.9241485595703
Loss :  1.3799227476119995 3.819582939147949 192.35906982421875
Loss :  1.3756873607635498 3.8475217819213867 193.75177001953125
Total LOSS train 207.52220787635216 valid 197.63622283935547
CE LOSS train 1.437718864587637 valid 0.34392184019088745
Contrastive LOSS train 4.121689792779776 valid 0.9618804454803467
EPOCH 59:
Loss :  1.5022345781326294 4.045853614807129 203.79490661621094
Loss :  1.5169453620910645 4.401884078979492 221.6111602783203
Loss :  1.4449585676193237 3.845583438873291 193.72413635253906
Loss :  1.4599835872650146 4.17624044418335 210.27200317382812
Loss :  1.495082974433899 3.8206231594085693 192.5262451171875
Loss :  1.4199037551879883 4.243104934692383 213.5751495361328
Loss :  1.4977892637252808 4.322915077209473 217.64353942871094
Loss :  1.4415404796600342 4.230183124542236 212.95069885253906
Loss :  1.412882924079895 4.2357282638549805 213.1992950439453
Loss :  1.4958133697509766 4.058994770050049 204.445556640625
Loss :  1.401165246963501 4.246084213256836 213.70538330078125
Loss :  1.3982410430908203 4.280520915985107 215.42428588867188
Loss :  1.4053343534469604 4.2337517738342285 213.09292602539062
Loss :  1.4200751781463623 3.7713537216186523 189.98776245117188
Loss :  1.5294464826583862 4.321654796600342 217.6121826171875
Loss :  1.5113104581832886 4.022552490234375 202.63893127441406
Loss :  1.3757675886154175 3.9150571823120117 197.1286163330078
Loss :  1.4306169748306274 3.9564225673675537 199.25173950195312
Loss :  1.350665807723999 4.124451160430908 207.57322692871094
Loss :  1.5092864036560059 3.875164270401001 195.2675018310547
  batch 20 loss: 1.5092864036560059, 3.875164270401001, 195.2675018310547
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.425868272781372 4.036452293395996 203.2484893798828
Loss :  1.3561936616897583 3.8787941932678223 195.29591369628906
Loss :  1.4071460962295532 3.9139585494995117 197.10507202148438
Loss :  1.4603981971740723 3.884608745574951 195.6908416748047
Loss :  1.499786376953125 4.0920000076293945 206.09979248046875
Loss :  1.4040701389312744 4.026918888092041 202.75001525878906
Loss :  1.441825032234192 4.036518573760986 203.2677459716797
Loss :  1.422819972038269 3.8457000255584717 193.70782470703125
Loss :  1.291610598564148 4.183183193206787 210.45077514648438
Loss :  1.5040233135223389 3.943092107772827 198.65863037109375
Loss :  1.2866592407226562 4.000553131103516 201.31430053710938
Loss :  1.4656264781951904 3.933361053466797 198.13368225097656
Loss :  1.4100227355957031 3.759765386581421 189.39828491210938
Loss :  1.4066389799118042 3.8259453773498535 192.7039031982422
Loss :  1.3064903020858765 3.8542256355285645 194.0177764892578
Loss :  1.3519622087478638 3.8068888187408447 191.69639587402344
Loss :  1.3479280471801758 3.681919813156128 185.4439239501953
Loss :  1.526504635810852 3.4800403118133545 175.5285186767578
Loss :  1.5451234579086304 3.584500312805176 180.7701416015625
Loss :  1.5660351514816284 3.5971434116363525 181.4232177734375
  batch 40 loss: 1.5660351514816284, 3.5971434116363525, 181.4232177734375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.451554775238037 3.6317460536956787 183.03884887695312
Loss :  1.4122140407562256 3.189499616622925 160.88719177246094
Loss :  1.3574767112731934 3.8877346515655518 195.7442169189453
Loss :  1.4069682359695435 3.7920267581939697 191.00830078125
Loss :  1.3442367315292358 3.442457914352417 173.46713256835938
Loss :  1.433634638786316 3.4396021366119385 173.4137420654297
Loss :  1.541326642036438 3.3232297897338867 167.7028045654297
Loss :  1.3810876607894897 3.384932279586792 170.62770080566406
Loss :  1.589906930923462 3.5703301429748535 180.10641479492188
Loss :  1.4087581634521484 3.417602062225342 172.28884887695312
Loss :  1.4904099702835083 3.9389429092407227 198.43756103515625
Loss :  1.5000214576721191 3.4273271560668945 172.8663787841797
Loss :  1.4401051998138428 3.498751163482666 176.37767028808594
Loss :  1.5264337062835693 3.5408248901367188 178.5676727294922
Loss :  1.3759187459945679 3.501617193222046 176.45677185058594
Loss :  1.6019229888916016 3.4202306270599365 172.61346435546875
Loss :  1.4234894514083862 3.2471764087677 163.78231811523438
Loss :  1.3720768690109253 3.6174874305725098 182.24644470214844
Loss :  1.427003264427185 3.589311122894287 180.89256286621094
Loss :  1.602534294128418 3.355937957763672 169.39944458007812
  batch 60 loss: 1.602534294128418, 3.355937957763672, 169.39944458007812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.3936564922332764 3.1199612617492676 157.3917236328125
Loss :  1.450873851776123 3.191666841506958 161.03421020507812
Loss :  1.4120151996612549 3.3332016468048096 168.0720977783203
Loss :  1.3586539030075073 3.5319719314575195 177.95726013183594
Loss :  1.3100630044937134 3.005722999572754 151.59620666503906
Loss :  1.374290108680725 3.717175245285034 187.23304748535156
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.4046684503555298 3.682673215866089 185.538330078125
Loss :  1.3972350358963013 3.623293876647949 182.56192016601562
Loss :  1.4405803680419922 3.731252431869507 188.00320434570312
Total LOSS train 190.58626896784855 valid 185.83412551879883
CE LOSS train 1.4378172342593853 valid 0.36014509201049805
Contrastive LOSS train 3.7829690272991474 valid 0.9328131079673767
Saved best model. Old loss 193.8585662841797 and new best loss 185.83412551879883
EPOCH 60:
Loss :  1.5070395469665527 3.17079758644104 160.0469207763672
Loss :  1.5388740301132202 3.4778594970703125 175.43185424804688
Loss :  1.4508923292160034 3.027437448501587 152.82276916503906
Loss :  1.4625593423843384 3.27714204788208 165.3196563720703
Loss :  1.5094106197357178 3.1790947914123535 160.4641571044922
Loss :  1.4331241846084595 3.093916893005371 156.12896728515625
Loss :  1.5155003070831299 3.181162118911743 160.5736083984375
Loss :  1.4420106410980225 3.4950006008148193 176.19204711914062
Loss :  1.4104243516921997 3.6995232105255127 186.3865966796875
Loss :  1.5107698440551758 3.3982481956481934 171.42318725585938
Loss :  1.402897596359253 3.6982874870300293 186.3172607421875
Loss :  1.388942837715149 3.953399658203125 199.05892944335938
Loss :  1.3941805362701416 4.299860954284668 216.38722229003906
Loss :  1.399247169494629 3.9687585830688477 199.83717346191406
Loss :  1.548829436302185 4.206400394439697 211.8688507080078
Loss :  1.562678337097168 4.127640724182129 207.94471740722656
Loss :  1.3755218982696533 3.9778096675872803 200.26600646972656
Loss :  1.4574228525161743 3.8856210708618164 195.7384796142578
Loss :  1.3671835660934448 3.773228883743286 190.02862548828125
Loss :  1.526605486869812 3.869474172592163 195.0003204345703
  batch 20 loss: 1.526605486869812, 3.869474172592163, 195.0003204345703
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.4431266784667969 3.90405011177063 196.6456298828125
Loss :  1.3672096729278564 3.727532386779785 187.7438201904297
Loss :  1.4157259464263916 3.2637507915496826 164.603271484375
Loss :  1.4724944829940796 3.6099636554718018 181.97067260742188
Loss :  1.5448646545410156 3.8833632469177246 195.71302795410156
Loss :  1.4294508695602417 3.4631917476654053 174.5890350341797
Loss :  1.4728620052337646 3.8282599449157715 192.88584899902344
Loss :  1.438214898109436 3.387024402618408 170.78944396972656
Loss :  1.3024479150772095 3.530329465866089 177.8189239501953
Loss :  1.527719259262085 3.765810251235962 189.8182373046875
Loss :  1.294189691543579 3.6744470596313477 185.01654052734375
Loss :  1.4848278760910034 3.8679189682006836 194.8807830810547
Loss :  1.4232486486434937 3.986065149307251 200.72650146484375
Loss :  1.41692316532135 3.489529848098755 175.89340209960938
Loss :  1.3189771175384521 3.6250455379486084 182.5712432861328
Loss :  1.3669954538345337 3.8698925971984863 194.8616180419922
Loss :  1.352832555770874 3.6640329360961914 184.55447387695312
Loss :  1.5226916074752808 3.961306571960449 199.5880126953125
Loss :  1.5563061237335205 3.677309274673462 185.42176818847656
Loss :  1.5745127201080322 3.4932806491851807 176.23854064941406
  batch 40 loss: 1.5745127201080322, 3.4932806491851807, 176.23854064941406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.4594144821166992 3.609119415283203 181.91537475585938
Loss :  1.4206891059875488 3.57574725151062 180.2080535888672
Loss :  1.382273554801941 3.64804744720459 183.78465270996094
Loss :  1.424608588218689 3.7228689193725586 187.56805419921875
Loss :  1.3698928356170654 3.5788395404815674 180.31185913085938
Loss :  1.4590179920196533 3.5666823387145996 179.7931365966797
Loss :  1.5468096733093262 3.901942253112793 196.6439208984375
Loss :  1.4043970108032227 3.523409843444824 177.57489013671875
Loss :  1.5876938104629517 4.074258327484131 205.3006134033203
Loss :  1.4286770820617676 3.8388686180114746 193.3721160888672
Loss :  1.5051063299179077 3.823333501815796 192.67178344726562
Loss :  1.5073931217193604 3.8571598529815674 194.36538696289062
Loss :  1.4434438943862915 3.7738311290740967 190.135009765625
Loss :  1.528585433959961 3.949613332748413 199.00924682617188
Loss :  1.3985674381256104 3.775073289871216 190.15223693847656
Loss :  1.5953094959259033 3.62600040435791 182.89532470703125
Loss :  1.4440902471542358 3.7518112659454346 189.03465270996094
Loss :  1.3890634775161743 3.5851166248321533 180.6448974609375
Loss :  1.4361836910247803 3.939807653427124 198.42657470703125
Loss :  1.6023513078689575 3.9008870124816895 196.64671325683594
  batch 60 loss: 1.6023513078689575, 3.9008870124816895, 196.64671325683594
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.3951585292816162 3.388725757598877 170.83145141601562
Loss :  1.4587793350219727 3.4744772911071777 175.18264770507812
Loss :  1.4138718843460083 3.4812278747558594 175.47528076171875
Loss :  1.372794508934021 3.892270088195801 195.98629760742188
Loss :  1.328574776649475 3.221100091934204 162.38357543945312
Loss :  1.4088937044143677 4.028122425079346 202.8150177001953
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.4480292797088623 4.047932147979736 203.84463500976562
Loss :  1.4371979236602783 3.7679362297058105 189.83399963378906
Loss :  1.4743424654006958 3.6456782817840576 183.75827026367188
Total LOSS train 185.2289522611178 valid 195.06298065185547
CE LOSS train 1.4497305209820086 valid 0.36858561635017395
Contrastive LOSS train 3.6755844262930064 valid 0.9114195704460144
EPOCH 61:
Loss :  1.5245323181152344 3.6403400897979736 183.54153442382812
Loss :  1.548258900642395 3.7165751457214355 187.37701416015625
Loss :  1.46743643283844 3.8837695121765137 195.65591430664062
Loss :  1.4783728122711182 3.8455870151519775 193.7577362060547
Loss :  1.5221060514450073 3.514342784881592 177.23924255371094
Loss :  1.4552868604660034 3.6000607013702393 181.4583282470703
Loss :  1.5194966793060303 3.585230588912964 180.78103637695312
Loss :  1.4594193696975708 3.367449998855591 169.83192443847656
Loss :  1.4313814640045166 3.4086101055145264 171.8618927001953
Loss :  1.5158621072769165 3.3483521938323975 168.9334716796875
Loss :  1.4233063459396362 3.775132656097412 190.17994689941406
Loss :  1.411948323249817 3.7466650009155273 188.7451934814453
Loss :  1.4186164140701294 3.4469244480133057 173.76483154296875
Loss :  1.413351058959961 3.5235249996185303 177.589599609375
Loss :  1.5390584468841553 3.542801856994629 178.6791534423828
Loss :  1.5708099603652954 3.481217861175537 175.6317138671875
Loss :  1.400362253189087 3.5320589542388916 178.00331115722656
Loss :  1.4704911708831787 3.774885892868042 190.21478271484375
Loss :  1.3896467685699463 3.586251974105835 180.70223999023438
Loss :  1.5412499904632568 3.280792474746704 165.58087158203125
  batch 20 loss: 1.5412499904632568, 3.280792474746704, 165.58087158203125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.4449608325958252 3.542371988296509 178.56356811523438
Loss :  1.3906835317611694 3.812194585800171 192.0004119873047
Loss :  1.426528811454773 3.643216133117676 183.58734130859375
Loss :  1.4981422424316406 3.8518946170806885 194.09286499023438
Loss :  1.5564675331115723 3.822497844696045 192.68136596679688
Loss :  1.4407150745391846 3.4047744274139404 171.679443359375
Loss :  1.4696847200393677 3.694289207458496 186.18414306640625
Loss :  1.453523874282837 3.5529966354370117 179.1033477783203
Loss :  1.3365075588226318 3.3542520999908447 169.04910278320312
Loss :  1.5434919595718384 3.4438722133636475 173.73709106445312
Loss :  1.3301126956939697 3.7800707817077637 190.33364868164062
Loss :  1.4905465841293335 3.584897041320801 180.7353973388672
Loss :  1.4493926763534546 3.4763054847717285 175.26466369628906
Loss :  1.444185733795166 3.4162416458129883 172.2562713623047
Loss :  1.3472586870193481 3.874945640563965 195.09454345703125
Loss :  1.3899292945861816 3.3076939582824707 166.77462768554688
Loss :  1.3860979080200195 3.4608471393585205 174.42845153808594
Loss :  1.542073130607605 3.4696435928344727 175.0242462158203
Loss :  1.5590249300003052 3.7255678176879883 187.83741760253906
Loss :  1.5867670774459839 3.5818827152252197 180.68089294433594
  batch 40 loss: 1.5867670774459839, 3.5818827152252197, 180.68089294433594
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.4779329299926758 3.6517062187194824 184.06324768066406
Loss :  1.4447094202041626 3.4577977657318115 174.3345947265625
Loss :  1.4056966304779053 3.619856834411621 182.39854431152344
Loss :  1.4444600343704224 3.445369243621826 173.71292114257812
Loss :  1.3922252655029297 3.551938056945801 178.9891357421875
Loss :  1.4722719192504883 3.6482999324798584 183.88726806640625
Loss :  1.5588479042053223 3.4583163261413574 174.47467041015625
Loss :  1.4230657815933228 3.521164894104004 177.48130798339844
Loss :  1.5996285676956177 3.414876699447632 172.3434600830078
Loss :  1.4438139200210571 3.5605876445770264 179.47320556640625
Loss :  1.5201470851898193 3.58298921585083 180.6696014404297
Loss :  1.5259795188903809 3.621647596359253 182.60836791992188
Loss :  1.469767689704895 3.4686169624328613 174.90061950683594
Loss :  1.5392358303070068 3.6388087272644043 183.4796600341797
Loss :  1.4214892387390137 3.336787223815918 168.26084899902344
Loss :  1.6079485416412354 3.269470453262329 165.08148193359375
Loss :  1.4619938135147095 3.552229881286621 179.073486328125
Loss :  1.414301872253418 3.5469300746917725 178.76080322265625
Loss :  1.4613521099090576 3.5697219371795654 179.94744873046875
Loss :  1.6137741804122925 3.5493886470794678 179.0832061767578
  batch 60 loss: 1.6137741804122925, 3.5493886470794678, 179.0832061767578
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.4291731119155884 3.4080708026885986 171.83270263671875
Loss :  1.4812793731689453 3.431067943572998 173.03466796875
Loss :  1.443180799484253 3.4396069049835205 173.42352294921875
Loss :  1.405389428138733 3.3490583896636963 168.8583221435547
Loss :  1.3609845638275146 2.9441466331481934 148.5683135986328
Loss :  1.4349998235702515 4.109071731567383 206.88858032226562
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.4651710987091064 3.9567527770996094 199.3028106689453
Loss :  1.4555885791778564 3.8458549976348877 193.7483367919922
Loss :  1.4784495830535889 3.9645373821258545 199.705322265625
Total LOSS train 178.72892291729266 valid 199.91126251220703
CE LOSS train 1.467780586389395 valid 0.3696123957633972
Contrastive LOSS train 3.5452228436103232 valid 0.9911343455314636
EPOCH 62:
Loss :  1.5343017578125 3.373866081237793 170.22760009765625
Loss :  1.5621562004089355 3.655856132507324 184.35494995117188
Loss :  1.4824365377426147 3.4064509868621826 171.80499267578125
Loss :  1.493160367012024 3.4115166664123535 172.06900024414062
Loss :  1.5339971780776978 3.4973442554473877 176.40121459960938
Loss :  1.4645453691482544 3.402574062347412 171.59324645996094
Loss :  1.5383892059326172 3.6390702724456787 183.4918975830078
Loss :  1.4760371446609497 3.5194785594940186 177.44996643066406
Loss :  1.446928858757019 3.349581003189087 168.9259796142578
Loss :  1.5386329889297485 3.242943286895752 163.68580627441406
Loss :  1.4431157112121582 3.725154161453247 187.70082092285156
Loss :  1.4269107580184937 3.4496452808380127 173.9091796875
Loss :  1.435577154159546 3.5459697246551514 178.73406982421875
Loss :  1.4423115253448486 3.7626023292541504 189.57241821289062
Loss :  1.5625495910644531 3.3454549312591553 168.83529663085938
Loss :  1.5856809616088867 3.425835132598877 172.87744140625
Loss :  1.4228715896606445 3.355903387069702 169.21803283691406
Loss :  1.4960498809814453 3.578693151473999 180.4307098388672
Loss :  1.4129782915115356 3.433262825012207 173.0761260986328
Loss :  1.5633983612060547 3.286205768585205 165.87368774414062
  batch 20 loss: 1.5633983612060547, 3.286205768585205, 165.87368774414062
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.476599097251892 3.456796169281006 174.31640625
Loss :  1.412813663482666 3.5813329219818115 180.47946166992188
Loss :  1.4615802764892578 3.457634687423706 174.3433074951172
Loss :  1.5084933042526245 3.4706196784973145 175.03948974609375
Loss :  1.5666556358337402 3.7154452800750732 187.3389129638672
Loss :  1.4700515270233154 3.256934642791748 164.3167724609375
Loss :  1.5016525983810425 3.614481210708618 182.2257080078125
Loss :  1.4746779203414917 3.323463201522827 167.6478271484375
Loss :  1.3603003025054932 3.557621479034424 179.2413787841797
Loss :  1.5511547327041626 3.324911594390869 167.79672241210938
Loss :  1.3547558784484863 3.6675031185150146 184.7299041748047
Loss :  1.5052062273025513 3.6871163845062256 185.86102294921875
Loss :  1.4648513793945312 3.456510066986084 174.29034423828125
Loss :  1.4594628810882568 3.3404641151428223 168.482666015625
Loss :  1.3710622787475586 3.3300559520721436 167.8738555908203
Loss :  1.4096139669418335 3.3020365238189697 166.5114288330078
Loss :  1.4066559076309204 3.3397715091705322 168.39523315429688
Loss :  1.5527034997940063 3.2634100914001465 164.72320556640625
Loss :  1.5637259483337402 3.315070867538452 167.3172607421875
Loss :  1.5916471481323242 3.4628565311431885 174.73446655273438
  batch 40 loss: 1.5916471481323242, 3.4628565311431885, 174.73446655273438
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.4925261735916138 3.6406428813934326 183.52467346191406
Loss :  1.4612892866134644 3.055612802505493 154.24192810058594
Loss :  1.4249978065490723 3.208111047744751 161.83055114746094
Loss :  1.4635608196258545 3.1303017139434814 157.97865295410156
Loss :  1.4154852628707886 3.4720420837402344 175.01759338378906
Loss :  1.4876221418380737 3.5110533237457275 177.04029846191406
Loss :  1.5671072006225586 3.452702045440674 174.20220947265625
Loss :  1.4419870376586914 3.5534627437591553 179.11512756347656
Loss :  1.6048378944396973 3.1517062187194824 159.19015502929688
Loss :  1.461044192314148 2.994396924972534 151.18089294433594
Loss :  1.5304380655288696 3.125833749771118 157.82212829589844
Loss :  1.5371984243392944 3.23222017288208 163.14820861816406
Loss :  1.4859813451766968 3.0982770919799805 156.3998260498047
Loss :  1.549083948135376 3.606147050857544 181.8564453125
Loss :  1.4400101900100708 3.5071780681610107 176.79891967773438
Loss :  1.6133233308792114 3.1378118991851807 158.50392150878906
Loss :  1.4784133434295654 3.164454936981201 159.70115661621094
Loss :  1.433889389038086 3.4883077144622803 175.84927368164062
Loss :  1.4774364233016968 3.3392183780670166 168.4383544921875
Loss :  1.6171870231628418 3.1752161979675293 160.37799072265625
  batch 60 loss: 1.6171870231628418, 3.1752161979675293, 160.37799072265625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.4490991830825806 3.077486515045166 155.32342529296875
Loss :  1.496425986289978 3.114954710006714 157.24417114257812
Loss :  1.4634331464767456 3.257931709289551 164.3600311279297
Loss :  1.425769567489624 3.6170599460601807 182.2787628173828
Loss :  1.3833822011947632 3.278597831726074 165.31326293945312
Loss :  1.5444551706314087 4.43022346496582 223.0556182861328
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.562854528427124 4.4721360206604 225.16964721679688
Loss :  1.5814448595046997 4.423626899719238 222.76280212402344
Loss :  1.4494866132736206 4.129034042358398 207.9011993408203
Total LOSS train 171.2713195800781 valid 219.72231674194336
CE LOSS train 1.4850341998613799 valid 0.36237165331840515
Contrastive LOSS train 3.3957257197453425 valid 1.0322585105895996
EPOCH 63:
Loss :  1.5049477815628052 3.8331620693206787 193.1630401611328
Loss :  1.5243819952011108 3.914970874786377 197.27293395996094
Loss :  1.462637186050415 4.086062431335449 205.7657470703125
Loss :  1.4800670146942139 4.101280689239502 206.5441131591797
Loss :  1.5133435726165771 4.468457221984863 224.9362030029297
Loss :  1.4490197896957397 3.941866159439087 198.54232788085938
Loss :  1.5096039772033691 4.437148094177246 223.36700439453125
Loss :  1.4548149108886719 4.34658145904541 218.7838897705078
Loss :  1.4353551864624023 4.432342052459717 223.0524444580078
Loss :  1.510733962059021 3.9888417720794678 200.95281982421875
Loss :  1.4259785413742065 4.213538646697998 212.1029052734375
Loss :  1.4246824979782104 4.138803005218506 208.36483764648438
Loss :  1.4284796714782715 4.37635612487793 220.2462921142578
Loss :  1.439610481262207 4.1664581298828125 209.76251220703125
Loss :  1.5447722673416138 4.056828498840332 204.38619995117188
Loss :  1.5481376647949219 4.254694938659668 214.2828826904297
Loss :  1.4176545143127441 4.043501853942871 203.59274291992188
Loss :  1.471668004989624 4.436770439147949 223.3101806640625
Loss :  1.4061617851257324 3.9490582942962646 198.85906982421875
Loss :  1.533158302307129 3.9022459983825684 196.6454620361328
  batch 20 loss: 1.533158302307129, 3.9022459983825684, 196.6454620361328
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.4647822380065918 3.8439149856567383 193.66053771972656
Loss :  1.406520128250122 3.8202595710754395 192.4195098876953
Loss :  1.4436779022216797 4.085179805755615 205.70266723632812
Loss :  1.488300085067749 4.130943775177002 208.03549194335938
Loss :  1.5448544025421143 4.397923946380615 221.44105529785156
Loss :  1.4538437128067017 4.053488254547119 204.1282501220703
Loss :  1.4779975414276123 4.182732105255127 210.61460876464844
Loss :  1.4561134576797485 3.6964566707611084 186.27894592285156
Loss :  1.3602176904678345 3.925873041152954 197.65386962890625
Loss :  1.525399088859558 4.0776448249816895 205.40765380859375
Loss :  1.3542338609695435 4.073774814605713 205.04296875
Loss :  1.490079641342163 4.093545436859131 206.1673583984375
Loss :  1.4526605606079102 4.0336198806762695 203.1336669921875
Loss :  1.4514577388763428 4.118322849273682 207.3675994873047
Loss :  1.374692440032959 4.104449272155762 206.59715270996094
Loss :  1.408964991569519 4.123486518859863 207.5832977294922
Loss :  1.406585693359375 4.150304317474365 208.9217987060547
Loss :  1.5239537954330444 3.8586440086364746 194.45616149902344
Loss :  1.5419445037841797 3.7865593433380127 190.8699188232422
Loss :  1.5552363395690918 3.8985705375671387 196.4837646484375
  batch 40 loss: 1.5552363395690918, 3.8985705375671387, 196.4837646484375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.4762390851974487 3.983876943588257 200.6700897216797
Loss :  1.450504183769226 3.8168959617614746 192.29530334472656
Loss :  1.427554965019226 3.959550619125366 199.4050750732422
Loss :  1.448184609413147 4.1579484939575195 209.34561157226562
Loss :  1.4153246879577637 3.808485269546509 191.839599609375
Loss :  1.4741054773330688 3.952195405960083 199.08387756347656
Loss :  1.5417319536209106 4.123988628387451 207.7411651611328
Loss :  1.4407984018325806 3.9464051723480225 198.76104736328125
Loss :  1.5685694217681885 3.8269455432891846 192.9158477783203
Loss :  1.4542169570922852 4.030698299407959 202.9891357421875
Loss :  1.5151456594467163 4.0339860916137695 203.21446228027344
Loss :  1.5180432796478271 3.9119274616241455 197.11441040039062
Loss :  1.472669005393982 4.014747142791748 202.21002197265625
Loss :  1.538926601409912 4.084927082061768 205.7852783203125
Loss :  1.4485580921173096 4.068793296813965 204.8882293701172
Loss :  1.5839215517044067 3.9521806240081787 199.1929473876953
Loss :  1.4689091444015503 3.996952533721924 201.3165283203125
Loss :  1.4381723403930664 3.964343547821045 199.6553497314453
Loss :  1.471328854560852 4.016181468963623 202.2803955078125
Loss :  1.5920946598052979 3.8927834033966064 196.2312774658203
  batch 60 loss: 1.5920946598052979, 3.8927834033966064, 196.2312774658203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.437752604484558 3.9502015113830566 198.9478302001953
Loss :  1.487593173980713 3.9466986656188965 198.82252502441406
Loss :  1.456475853919983 4.027202129364014 202.81658935546875
Loss :  1.4311366081237793 4.111499309539795 207.006103515625
Loss :  1.3962522745132446 3.636107921600342 183.20164489746094
Loss :  2784.304443359375 4.104135513305664 2989.51123046875
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
Loss :  3044.527099609375 4.108886241912842 3249.971435546875
Loss :  3292.2119140625 3.958505868911743 3490.13720703125
Loss :  2357.560791015625 3.8653838634490967 2550.830078125
Total LOSS train 203.5943112886869 valid 3070.1124877929688
CE LOSS train 1.4710912979566133 valid 589.3901977539062
Contrastive LOSS train 4.042464388333834 valid 0.9663459658622742
EPOCH 64:
Loss :  1.5268058776855469 3.8862924575805664 195.8414306640625
Loss :  1.5442402362823486 4.093152046203613 206.20184326171875
Loss :  1.4911478757858276 3.7944517135620117 191.21372985839844
Loss :  1.5049141645431519 3.913560628890991 197.1829376220703
Loss :  1.5338609218597412 3.67775821685791 185.42176818847656
Loss :  1.4703798294067383 3.7857253551483154 190.75665283203125
Loss :  1.5336034297943115 3.9939749240875244 201.2323455810547
Loss :  1.4856847524642944 3.899050235748291 196.43820190429688
Loss :  1.4652459621429443 3.8112268447875977 192.02658081054688
Loss :  1.5350615978240967 3.635373115539551 183.30372619628906
Loss :  1.456766128540039 3.993340015411377 201.123779296875
Loss :  1.4519331455230713 3.937291383743286 198.31649780273438
Loss :  1.4562181234359741 3.846238374710083 193.7681427001953
Loss :  1.4684704542160034 3.876384973526001 195.2877197265625
Loss :  1.5630321502685547 3.836129665374756 193.36952209472656
Loss :  1.5659902095794678 3.918009042739868 197.46644592285156
Loss :  1.4473133087158203 3.8250808715820312 192.70135498046875
Loss :  1.499080777168274 3.7937541007995605 191.18678283691406
Loss :  1.439082384109497 3.9266412258148193 197.77114868164062
Loss :  1.5551458597183228 3.9465537071228027 198.88282775878906
  batch 20 loss: 1.5551458597183228, 3.9465537071228027, 198.88282775878906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.492550015449524 3.7555413246154785 189.26962280273438
Loss :  1.4370592832565308 3.807145357131958 191.79432678222656
Loss :  1.469821572303772 3.935786724090576 198.2591552734375
Loss :  1.5095666646957397 4.357303142547607 219.37472534179688
Loss :  1.5616672039031982 4.213679313659668 212.24562072753906
Loss :  1.4787812232971191 4.306884765625 216.82301330566406
Loss :  1.500435471534729 4.298948764801025 216.4478759765625
Loss :  1.482043981552124 3.9994075298309326 201.45242309570312
Loss :  1.3922340869903564 3.883009910583496 195.542724609375
Loss :  1.54503333568573 3.9760565757751465 200.3478546142578
Loss :  1.3865464925765991 4.075402736663818 205.1566925048828
Loss :  1.511919379234314 4.233325481414795 213.17819213867188
Loss :  1.4766334295272827 4.065099239349365 204.73159790039062
Loss :  1.4751936197280884 3.9971578121185303 201.3330841064453
Loss :  1.4051257371902466 3.912275552749634 197.01890563964844
Loss :  1.4363387823104858 4.196318626403809 211.2522735595703
Loss :  1.4345718622207642 3.819634437561035 192.41629028320312
Loss :  1.5432580709457397 3.8038835525512695 191.73744201660156
Loss :  1.560410737991333 3.71281361579895 187.2010955810547
Loss :  1.5730066299438477 3.7285990715026855 188.00296020507812
  batch 40 loss: 1.5730066299438477, 3.7285990715026855, 188.00296020507812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.4992142915725708 4.2706298828125 215.0307159423828
Loss :  1.4766496419906616 4.045206546783447 203.7369842529297
Loss :  1.4543828964233398 3.967186212539673 199.81369018554688
Loss :  1.4719080924987793 4.279439926147461 215.44390869140625
Loss :  1.441269040107727 3.576448917388916 180.2637176513672
Loss :  1.496861219406128 4.247092247009277 213.85147094726562
Loss :  1.5601356029510498 4.402425289154053 221.681396484375
Loss :  1.4680670499801636 4.358724594116211 219.404296875
Loss :  1.5935198068618774 4.423325061798096 222.75978088378906
Loss :  1.4792665243148804 4.220398902893066 212.49920654296875
Loss :  1.5300204753875732 4.25542688369751 214.30136108398438
Loss :  1.5369352102279663 4.261271953582764 214.6005401611328
Loss :  1.498338222503662 4.222456455230713 212.62115478515625
Loss :  1.5649515390396118 4.367218971252441 219.9259033203125
Loss :  1.471045970916748 4.355291843414307 219.23562622070312
Loss :  1.6128507852554321 4.235835552215576 213.40463256835938
Loss :  1.499164342880249 4.259222030639648 214.46026611328125
Loss :  1.4678804874420166 4.460296630859375 224.4827117919922
Loss :  1.4955257177352905 4.375168800354004 220.25396728515625
Loss :  1.6182000637054443 4.224839687347412 212.8601837158203
  batch 60 loss: 1.6182000637054443, 4.224839687347412, 212.8601837158203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.4666188955307007 4.007201194763184 201.82667541503906
Loss :  1.5066660642623901 3.9557220935821533 199.2927703857422
Loss :  1.4745508432388306 4.052452564239502 204.09718322753906
Loss :  1.4523710012435913 4.331015586853027 218.00315856933594
Loss :  1.416799545288086 4.098385334014893 206.3360595703125
Loss :  1.4345861673355103 4.314343452453613 217.15176391601562
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.4750698804855347 4.248610019683838 213.9055633544922
Loss :  1.4979389905929565 4.261812686920166 214.5885772705078
Loss :  1.4515877962112427 4.06906270980835 204.90472412109375
Total LOSS train 203.58871812086838 valid 212.63765716552734
CE LOSS train 1.495682586156405 valid 0.36289694905281067
Contrastive LOSS train 4.0418607014876144 valid 1.0172656774520874
EPOCH 65:
Loss :  1.5422435998916626 4.1986985206604 211.47715759277344
Loss :  1.5587453842163086 4.390886306762695 221.10305786132812
Loss :  1.5071942806243896 4.134092330932617 208.21180725097656
Loss :  1.5184168815612793 4.112882614135742 207.1625518798828
Loss :  1.5463805198669434 3.789987087249756 191.04574584960938
Loss :  1.4846855401992798 3.966684579849243 199.81890869140625
Loss :  1.545018196105957 4.220335006713867 212.561767578125
Loss :  1.5001285076141357 4.389664649963379 220.9833526611328
Loss :  1.479430913925171 4.12343692779541 207.65127563476562
Loss :  1.5451937913894653 4.204954624176025 211.7929229736328
Loss :  1.4693368673324585 3.947378635406494 198.8382568359375
Loss :  1.4647226333618164 4.0525641441345215 204.09292602539062
Loss :  1.4693036079406738 4.173216819763184 210.13014221191406
Loss :  1.4793504476547241 3.920632839202881 197.51100158691406
Loss :  1.5725929737091064 4.572551250457764 230.2001495361328
Loss :  1.5734496116638184 4.424199104309082 222.78341674804688
Loss :  1.4620991945266724 4.28967809677124 215.9459991455078
Loss :  1.5091108083724976 3.8778293132781982 195.40057373046875
Loss :  1.457776665687561 3.9790732860565186 200.41143798828125
Loss :  1.5685075521469116 4.0438232421875 203.75967407226562
  batch 20 loss: 1.5685075521469116, 4.0438232421875, 203.75967407226562
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5087063312530518 3.7657227516174316 189.7948455810547
Loss :  1.4590184688568115 3.81048321723938 191.98316955566406
Loss :  1.4901947975158691 3.966088056564331 199.79458618164062
Loss :  1.5291320085525513 4.276438236236572 215.35104370117188
Loss :  1.5771151781082153 4.200380802154541 211.59616088867188
Loss :  1.5008296966552734 4.0635294914245605 204.6772918701172
Loss :  1.5204368829727173 4.44415807723999 223.72833251953125
Loss :  1.5057250261306763 3.831387758255005 193.07510375976562
Loss :  1.4223142862319946 3.9258921146392822 197.7169189453125
Loss :  1.565542221069336 3.9783060550689697 200.4808349609375
Loss :  1.4195116758346558 4.125191688537598 207.67909240722656
Loss :  1.5340489149093628 4.588192462921143 230.94366455078125
Loss :  1.5072883367538452 4.054311275482178 204.2228546142578
Loss :  1.5069087743759155 4.031001091003418 203.0569610595703
Loss :  1.4389852285385132 4.345032215118408 218.6905975341797
Loss :  1.4710952043533325 4.284070014953613 215.6746063232422
Loss :  1.4667749404907227 4.248595237731934 213.89654541015625
Loss :  1.5629161596298218 4.200809001922607 211.60336303710938
Loss :  1.586161732673645 4.233388900756836 213.255615234375
Loss :  1.5988192558288574 3.937598943710327 198.478759765625
  batch 40 loss: 1.5988192558288574, 3.937598943710327, 198.478759765625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.5174906253814697 3.9311559200286865 198.07528686523438
Loss :  1.5020235776901245 3.97790789604187 200.39743041992188
Loss :  1.4829695224761963 4.212389945983887 212.1024627685547
Loss :  1.4975380897521973 4.310457229614258 217.02040100097656
Loss :  1.4627779722213745 4.121991157531738 207.56234741210938
Loss :  1.5202453136444092 4.055806636810303 204.31057739257812
Loss :  1.581813097000122 4.037251949310303 203.4444122314453
Loss :  1.4950684309005737 4.168440818786621 209.9171142578125
Loss :  1.609620451927185 4.169386863708496 210.07896423339844
Loss :  1.5073015689849854 4.423967361450195 222.70567321777344
Loss :  1.5714409351348877 4.260312557220459 214.58706665039062
Loss :  1.5572550296783447 4.224161148071289 212.7653045654297
Loss :  1.5268245935440063 4.232207775115967 213.13720703125
Loss :  1.573934555053711 4.236259937286377 213.38693237304688
Loss :  1.5141067504882812 4.211664199829102 212.09732055664062
Loss :  1.6025999784469604 4.1732892990112305 210.26705932617188
Loss :  1.5184948444366455 4.048977375030518 203.9673614501953
Loss :  1.5013693571090698 4.176255226135254 210.3141326904297
Loss :  1.52321195602417 4.338893413543701 218.46788024902344
Loss :  1.6316578388214111 4.248905658721924 214.07693481445312
  batch 60 loss: 1.6316578388214111, 4.248905658721924, 214.07693481445312
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.5013859272003174 4.315775394439697 217.2901611328125
Loss :  1.5546618700027466 4.2807416915893555 215.59173583984375
Loss :  1.5167145729064941 4.277008533477783 215.3671417236328
Loss :  1.492249608039856 4.103044509887695 206.64447021484375
Loss :  1.4589502811431885 3.8429641723632812 193.60716247558594
Loss :  1.52351975440979 4.400761127471924 221.5615692138672
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.546414852142334 4.392037868499756 221.1483154296875
Loss :  1.5409053564071655 4.424349784851074 222.75839233398438
Loss :  1.5496739149093628 4.001952648162842 201.6472930908203
Total LOSS train 208.82715407151443 valid 216.77889251708984
CE LOSS train 1.5176449207159188 valid 0.3874184787273407
Contrastive LOSS train 4.1461902068211485 valid 1.0004881620407104
EPOCH 66:
Loss :  1.5673935413360596 3.9412569999694824 198.6302490234375
Loss :  1.5974276065826416 4.461808204650879 224.68783569335938
Loss :  1.5396422147750854 4.144444465637207 208.76187133789062
Loss :  1.5535556077957153 4.149971961975098 209.05215454101562
Loss :  1.578540325164795 4.107876777648926 206.97238159179688
Loss :  1.513196349143982 4.139619827270508 208.4941864013672
Loss :  1.5730900764465332 4.403141975402832 221.73019409179688
Loss :  1.5347354412078857 4.1323018074035645 208.1498260498047
Loss :  1.5180988311767578 4.108356952667236 206.93594360351562
Loss :  1.578626036643982 4.349715709686279 219.0644073486328
Loss :  1.5041676759719849 4.382093906402588 220.60885620117188
Loss :  1.5018929243087769 4.513575553894043 227.1806640625
Loss :  1.5030328035354614 4.510439395904541 227.02500915527344
Loss :  1.5120716094970703 4.504928112030029 226.7584686279297
Loss :  1.6058722734451294 4.432785987854004 223.2451629638672
Loss :  1.596778392791748 4.47164249420166 225.17889404296875
Loss :  1.4932881593704224 4.186120986938477 210.79933166503906
Loss :  1.5391024351119995 4.158892631530762 209.48373413085938
Loss :  1.4935585260391235 4.0685133934021 204.9192352294922
Loss :  1.5994700193405151 4.249798774719238 214.08941650390625
  batch 20 loss: 1.5994700193405151, 4.249798774719238, 214.08941650390625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5426673889160156 4.395030498504639 221.294189453125
Loss :  1.505818247795105 4.527944564819336 227.90304565429688
Loss :  1.554272174835205 4.578001499176025 230.454345703125
Loss :  1.5626988410949707 4.462087154388428 224.66705322265625
Loss :  1.6074786186218262 4.645401954650879 233.87757873535156
Loss :  1.5428906679153442 4.441421031951904 223.6139373779297
Loss :  1.5583868026733398 4.6632208824157715 234.71942138671875
Loss :  1.5441652536392212 4.332101821899414 218.1492462158203
Loss :  1.4493775367736816 4.470502853393555 224.97451782226562
Loss :  1.5929850339889526 4.420303821563721 222.60818481445312
Loss :  1.4345241785049438 4.542947292327881 228.58189392089844
Loss :  1.5550321340560913 4.724311351776123 237.77059936523438
Loss :  1.5112354755401611 4.450099468231201 224.01620483398438
Loss :  1.5032191276550293 4.474016189575195 225.2040252685547
Loss :  1.4330084323883057 4.4864068031311035 225.75335693359375
Loss :  1.4460453987121582 4.465795516967773 224.73582458496094
Loss :  1.4391204118728638 4.685838222503662 235.7310333251953
Loss :  1.5587459802627563 4.225330829620361 212.82528686523438
Loss :  1.5694403648376465 4.312736988067627 217.206298828125
Loss :  1.582406997680664 4.63336181640625 233.25050354003906
  batch 40 loss: 1.582406997680664, 4.63336181640625, 233.25050354003906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.4996994733810425 4.437375068664551 223.3684539794922
Loss :  1.4606835842132568 4.46284294128418 224.6028289794922
Loss :  1.436146855354309 4.409223556518555 221.89732360839844
Loss :  1.4764740467071533 4.488302707672119 225.8916015625
Loss :  1.4233052730560303 4.411860942840576 222.016357421875
Loss :  1.4878506660461426 4.404842853546143 221.72999572753906
Loss :  1.5465024709701538 4.327398777008057 217.91644287109375
Loss :  1.440173625946045 4.523082256317139 227.5942840576172
Loss :  1.5846306085586548 4.427361011505127 222.9526824951172
Loss :  1.4575145244598389 4.593453407287598 231.13018798828125
Loss :  1.5178511142730713 4.6477580070495605 233.90574645996094
Loss :  1.5117493867874146 4.425316333770752 222.77757263183594
Loss :  1.4573123455047607 4.418137073516846 222.36416625976562
Loss :  1.5173743963241577 4.451452255249023 224.08999633789062
Loss :  1.4284250736236572 4.44944429397583 223.900634765625
Loss :  1.5834728479385376 4.576467037200928 230.4068145751953
Loss :  1.4398193359375 4.535243511199951 228.20199584960938
Loss :  1.4014345407485962 4.600050926208496 231.4039764404297
Loss :  1.4303553104400635 4.5071492195129395 226.78782653808594
Loss :  1.5945457220077515 4.628946781158447 233.04188537597656
  batch 60 loss: 1.5945457220077515, 4.628946781158447, 233.04188537597656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.3868212699890137 4.428908348083496 222.83224487304688
Loss :  1.4615373611450195 4.497034549713135 226.31326293945312
Loss :  1.3975965976715088 4.555821895599365 229.18869018554688
Loss :  1.3794528245925903 4.378817081451416 220.3203125
Loss :  1.327471137046814 4.138991355895996 208.27703857421875
Loss :  1.4812846183776855 4.387049198150635 220.833740234375
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5155802965164185 4.4249491691589355 222.76303100585938
Loss :  1.5066198110580444 4.3998308181762695 221.4981689453125
Loss :  1.5238378047943115 4.300715923309326 216.55963134765625
Total LOSS train 222.338718355619 valid 220.41364288330078
CE LOSS train 1.5083886201565082 valid 0.3809594511985779
Contrastive LOSS train 4.416606594966008 valid 1.0751789808273315
EPOCH 67:
Loss :  1.4962209463119507 4.271711349487305 215.081787109375
Loss :  1.5195266008377075 4.508373737335205 226.93821716308594
Loss :  1.4449554681777954 4.3675031661987305 219.8201141357422
Loss :  1.4725030660629272 4.4204325675964355 222.49412536621094
Loss :  1.5018913745880127 4.215817451477051 212.2927703857422
Loss :  1.4070112705230713 4.438387870788574 223.32640075683594
Loss :  1.521424651145935 4.522547245025635 227.64878845214844
Loss :  1.435782551765442 4.3836870193481445 220.62013244628906
Loss :  1.4133715629577637 4.3976969718933105 221.2982177734375
Loss :  1.5039399862289429 4.470335483551025 225.0207061767578
Loss :  1.3899180889129639 4.614450931549072 232.1124725341797
Loss :  1.3869774341583252 4.458098411560059 224.29190063476562
Loss :  1.3814159631729126 4.47116756439209 224.93978881835938
Loss :  1.3965333700180054 4.541067600250244 228.4499053955078
Loss :  1.5374844074249268 4.466399669647217 224.8574676513672
Loss :  1.530603051185608 4.4739532470703125 225.228271484375
Loss :  1.3670490980148315 4.536128044128418 228.17344665527344
Loss :  1.4333503246307373 4.418321132659912 222.3494110107422
Loss :  1.358980655670166 4.43353796005249 223.03587341308594
Loss :  1.5140185356140137 4.510319232940674 227.0299835205078
  batch 20 loss: 1.5140185356140137, 4.510319232940674, 227.0299835205078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.424992561340332 4.266461372375488 214.74806213378906
Loss :  1.3530263900756836 4.33656644821167 218.1813507080078
Loss :  1.4034844636917114 4.664604187011719 234.63369750976562
Loss :  1.4426229000091553 4.474743366241455 225.17979431152344
Loss :  1.5126457214355469 4.878058433532715 245.4155731201172
Loss :  1.3982634544372559 4.382254600524902 220.51100158691406
Loss :  1.4198108911514282 4.682479381561279 235.5437774658203
Loss :  1.4136629104614258 4.555714130401611 229.19937133789062
Loss :  1.2748297452926636 4.3963799476623535 221.0938262939453
Loss :  1.4915270805358887 4.418576240539551 222.42034912109375
Loss :  1.2768292427062988 4.430260181427002 222.7898406982422
Loss :  1.4460797309875488 4.560506343841553 229.4713897705078
Loss :  1.384823203086853 4.281310081481934 215.45033264160156
Loss :  1.3803331851959229 4.447212219238281 223.74095153808594
Loss :  1.2882331609725952 4.295690536499023 216.07276916503906
Loss :  1.3252376317977905 4.29217529296875 215.9340057373047
Loss :  1.3233078718185425 4.23045015335083 212.84580993652344
Loss :  1.4774214029312134 4.14772367477417 208.8636016845703
Loss :  1.4934993982315063 4.183925151824951 210.68975830078125
Loss :  1.5156216621398926 4.004426002502441 201.73692321777344
  batch 40 loss: 1.5156216621398926, 4.004426002502441, 201.73692321777344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.412314772605896 4.300013065338135 216.4129638671875
Loss :  1.379439353942871 4.165345668792725 209.646728515625
Loss :  1.355465054512024 4.049256801605225 203.8183135986328
Loss :  1.3855997323989868 4.0017409324646 201.47265625
Loss :  1.3364624977111816 4.2654876708984375 214.61083984375
Loss :  1.4130853414535522 3.860908269882202 194.45849609375
Loss :  1.4986391067504883 4.010690212249756 202.0331573486328
Loss :  1.3797962665557861 4.31235408782959 216.99749755859375
Loss :  1.5406450033187866 4.1727614402771 210.17872619628906
Loss :  1.3990901708602905 4.211741924285889 211.98619079589844
Loss :  1.4862277507781982 4.235413551330566 213.25689697265625
Loss :  1.474807620048523 4.048371315002441 203.89337158203125
Loss :  1.4194092750549316 4.077004432678223 205.26962280273438
Loss :  1.4969159364700317 4.013900279998779 202.19192504882812
Loss :  1.4022243022918701 4.181631565093994 210.48379516601562
Loss :  1.554366111755371 3.75813889503479 189.46131896972656
Loss :  1.4214911460876465 4.209990978240967 211.92103576660156
Loss :  1.3871068954467773 4.160016059875488 209.38790893554688
Loss :  1.4263554811477661 4.239950656890869 213.42388916015625
Loss :  1.5853803157806396 3.907489538192749 196.95985412597656
  batch 60 loss: 1.5853803157806396, 3.907489538192749, 196.95985412597656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.3936316967010498 3.9123926162719727 197.0132598876953
Loss :  1.4628369808197021 4.02141809463501 202.5337371826172
Loss :  1.4092122316360474 4.230389595031738 212.92869567871094
Loss :  1.3857041597366333 4.235862731933594 213.17884826660156
Loss :  1.3367211818695068 3.9082767963409424 196.75054931640625
Loss :  1.412040114402771 4.123516082763672 207.5878448486328
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.4469791650772095 4.193775653839111 211.13575744628906
Loss :  1.4359028339385986 4.114823341369629 207.17706298828125
Loss :  1.4524120092391968 4.011855602264404 202.04518127441406
Total LOSS train 216.30464993990384 valid 206.9864616394043
CE LOSS train 1.4277252215605516 valid 0.3631030023097992
Contrastive LOSS train 4.297538485893837 valid 1.002963900566101
EPOCH 68:
Loss :  1.499585747718811 3.998929262161255 201.446044921875
Loss :  1.5288156270980835 4.314612865447998 217.2594451904297
Loss :  1.4548033475875854 3.7145543098449707 187.18252563476562
Loss :  1.4692126512527466 3.6933538913726807 186.13690185546875
Loss :  1.5077978372573853 3.6857855319976807 185.7970733642578
Loss :  1.419576644897461 3.7480082511901855 188.81997680664062
Loss :  1.5032826662063599 3.6744096279144287 185.2237548828125
Loss :  1.4430499076843262 3.443190097808838 173.60255432128906
Loss :  1.4168403148651123 4.066157341003418 204.72470092773438
Loss :  1.5050233602523804 3.332714796066284 168.14076232910156
Loss :  1.3950581550598145 3.618224859237671 182.30630493164062
Loss :  1.3914262056350708 3.6924569606781006 186.0142822265625
Loss :  1.3897556066513062 3.6719813346862793 184.9888153076172
Loss :  1.407932996749878 3.6930088996887207 186.05838012695312
Loss :  1.536340355873108 3.892455816268921 196.1591339111328
Loss :  1.5342838764190674 3.698042869567871 186.43643188476562
Loss :  1.382495641708374 3.5123302936553955 176.99900817871094
Loss :  1.4519495964050293 3.5506644248962402 178.98516845703125
Loss :  1.3782522678375244 3.316018581390381 167.17918395996094
Loss :  1.5248559713363647 3.499366283416748 176.4931640625
  batch 20 loss: 1.5248559713363647, 3.499366283416748, 176.4931640625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.4434599876403809 3.4120302200317383 172.04498291015625
Loss :  1.375307559967041 3.646001100540161 183.67535400390625
Loss :  1.4181091785430908 3.758916139602661 189.36390686035156
Loss :  1.4672526121139526 4.140267372131348 208.48062133789062
Loss :  1.5359755754470825 4.029012680053711 202.9866180419922
Loss :  1.43099045753479 4.327654838562012 217.813720703125
Loss :  1.4605375528335571 3.981228828430176 200.52198791503906
Loss :  1.4504520893096924 4.1504902839660645 208.9749755859375
Loss :  1.3205208778381348 4.314904689788818 217.06576538085938
Loss :  1.520132064819336 3.4875435829162598 175.89730834960938
Loss :  1.3144549131393433 4.4807047843933105 225.3496856689453
Loss :  1.4830517768859863 4.4110941886901855 222.03775024414062
Loss :  1.4269152879714966 4.304783344268799 216.66607666015625
Loss :  1.4210695028305054 4.0180230140686035 202.3222198486328
Loss :  1.3313161134719849 4.2180399894714355 212.23330688476562
Loss :  1.3709365129470825 4.289666175842285 215.854248046875
Loss :  1.3713690042495728 4.156294822692871 209.1861114501953
Loss :  1.518704891204834 3.6915283203125 186.09512329101562
Loss :  1.5339844226837158 3.609135389328003 181.99075317382812
Loss :  1.5528587102890015 3.68080472946167 185.5930938720703
  batch 40 loss: 1.5528587102890015, 3.68080472946167, 185.5930938720703
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.457107424736023 3.7571518421173096 189.314697265625
Loss :  1.4217700958251953 4.054733753204346 204.15846252441406
Loss :  1.3970175981521606 4.059390544891357 204.36654663085938
Loss :  1.4268345832824707 4.583621501922607 230.60791015625
Loss :  1.381517767906189 4.089366436004639 205.84983825683594
Loss :  1.4495762586593628 3.6105034351348877 181.97474670410156
Loss :  1.5277897119522095 4.335431098937988 218.29934692382812
Loss :  1.413074016571045 4.372011661529541 220.01365661621094
Loss :  1.5628749132156372 3.8964643478393555 196.38609313964844
Loss :  1.4248600006103516 3.857290744781494 194.28939819335938
Loss :  1.4994207620620728 4.349427223205566 218.9707794189453
Loss :  1.4911082983016968 3.943382740020752 198.66024780273438
Loss :  1.436751127243042 3.5882062911987305 180.84706115722656
Loss :  1.512099027633667 3.9029109477996826 196.65765380859375
Loss :  1.407092571258545 3.9138424396514893 197.09921264648438
Loss :  1.5689313411712646 3.8389265537261963 193.5152587890625
Loss :  1.4259260892868042 3.7561819553375244 189.23501586914062
Loss :  1.3839386701583862 3.811419725418091 191.95492553710938
Loss :  1.420380711555481 4.080888271331787 205.4647979736328
Loss :  1.5785993337631226 3.3313872814178467 168.14796447753906
  batch 60 loss: 1.5785993337631226, 3.3313872814178467, 168.14796447753906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.379379391670227 3.7726409435272217 190.0114288330078
Loss :  1.4472628831863403 4.077482223510742 205.32138061523438
Loss :  1.4030283689498901 3.668161630630493 184.8111114501953
Loss :  1.3716652393341064 3.8446643352508545 193.60487365722656
Loss :  1.3243803977966309 3.2510597705841064 163.87738037109375
Loss :  1.4135496616363525 4.154277324676514 209.12741088867188
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.4466692209243774 4.144250869750977 208.65921020507812
Loss :  1.4367907047271729 4.039198398590088 203.39671325683594
Loss :  1.4524755477905273 3.8663313388824463 194.76904296875
Total LOSS train 195.0391848050631 valid 203.98809432983398
CE LOSS train 1.4461557608384352 valid 0.36311888694763184
Contrastive LOSS train 3.871860592181866 valid 0.9665828347206116
EPOCH 69:
Loss :  1.4911059141159058 3.660515785217285 184.5168914794922
Loss :  1.5182510614395142 4.121000289916992 207.56826782226562
Loss :  1.4445055723190308 3.5755178928375244 180.22039794921875
Loss :  1.459448218345642 3.6307616233825684 182.99752807617188
Loss :  1.4987562894821167 3.658921003341675 184.44479370117188
Loss :  1.414707064628601 3.4735326766967773 175.09133911132812
Loss :  1.4983892440795898 3.6946778297424316 186.23226928710938
Loss :  1.4369901418685913 3.4752700328826904 175.20050048828125
Loss :  1.4116843938827515 3.4868204593658447 175.75270080566406
Loss :  1.5007429122924805 3.7412126064300537 188.56137084960938
Loss :  1.39370858669281 3.9201228618621826 197.3998565673828
Loss :  1.3903894424438477 3.749197483062744 188.8502655029297
Loss :  1.3882100582122803 3.88041615486145 195.40902709960938
Loss :  1.4070543050765991 3.716259479522705 187.22003173828125
Loss :  1.5301165580749512 3.5768227577209473 180.3712615966797
Loss :  1.5309693813323975 3.784980535507202 190.77999877929688
Loss :  1.376821756362915 3.6775612831115723 185.2548828125
Loss :  1.4456547498703003 3.668610095977783 184.87615966796875
Loss :  1.3725005388259888 3.5297818183898926 177.86158752441406
Loss :  1.519726276397705 3.6178975105285645 182.41461181640625
  batch 20 loss: 1.519726276397705, 3.6178975105285645, 182.41461181640625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.44081711769104 3.1146931648254395 157.17547607421875
Loss :  1.3727469444274902 3.4174392223358154 172.2447052001953
Loss :  1.4166511297225952 3.73006272315979 187.9197998046875
Loss :  1.4637408256530762 3.8328840732574463 193.1079559326172
Loss :  1.530782699584961 3.9173901081085205 197.40028381347656
Loss :  1.4271713495254517 3.6510932445526123 183.98182678222656
Loss :  1.4530820846557617 3.934795379638672 198.19285583496094
Loss :  1.434506893157959 3.6604645252227783 184.45773315429688
Loss :  1.3143656253814697 3.659316062927246 184.28016662597656
Loss :  1.516066551208496 3.6226372718811035 182.64793395996094
Loss :  1.3132295608520508 3.7490954399108887 188.76800537109375
Loss :  1.4788788557052612 3.8727188110351562 195.1148223876953
Loss :  1.4283472299575806 3.5640738010406494 179.6320343017578
Loss :  1.425668478012085 3.755305528640747 189.19094848632812
Loss :  1.3343521356582642 3.8722147941589355 194.94508361816406
Loss :  1.3736538887023926 3.70802640914917 186.7749786376953
Loss :  1.371549367904663 3.6542162895202637 184.08236694335938
Loss :  1.5139297246932983 3.5570003986358643 179.36395263671875
Loss :  1.5290842056274414 3.416247844696045 172.3414764404297
Loss :  1.5459622144699097 3.2463467121124268 163.86329650878906
  batch 40 loss: 1.5459622144699097, 3.2463467121124268, 163.86329650878906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.4492069482803345 3.542844295501709 178.59141540527344
Loss :  1.4145933389663696 3.325028419494629 167.666015625
Loss :  1.3887500762939453 3.7720532417297363 189.9914093017578
Loss :  1.4160714149475098 3.6553008556365967 184.18112182617188
Loss :  1.3748054504394531 3.512641191482544 177.00686645507812
Loss :  1.4457327127456665 3.6257524490356445 182.73336791992188
Loss :  1.5266886949539185 3.2898521423339844 166.01930236816406
Loss :  1.4140182733535767 3.4377217292785645 173.30010986328125
Loss :  1.5627493858337402 3.57655930519104 180.39071655273438
Loss :  1.432598352432251 3.7213857173919678 187.50189208984375
Loss :  1.5070267915725708 3.7352077960968018 188.26742553710938
Loss :  1.50236177444458 4.074913024902344 205.24801635742188
Loss :  1.45133376121521 3.7160258293151855 187.25262451171875
Loss :  1.5231809616088867 3.9050559997558594 196.77598571777344
Loss :  1.4239224195480347 3.9160923957824707 197.22854614257812
Loss :  1.5731264352798462 3.4534072875976562 174.2434844970703
Loss :  1.4422762393951416 3.6034762859344482 181.6160888671875
Loss :  1.4067217111587524 3.643590211868286 183.5862274169922
Loss :  1.4437881708145142 3.8695228099823 194.919921875
Loss :  1.589650273323059 3.518918037414551 177.53555297851562
  batch 60 loss: 1.589650273323059, 3.518918037414551, 177.53555297851562
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.4096393585205078 3.6240057945251465 182.60992431640625
Loss :  1.4698859453201294 3.656815528869629 184.31065368652344
Loss :  1.4290028810501099 3.5233664512634277 177.59732055664062
Loss :  1.399187445640564 3.3735909461975098 170.0787353515625
Loss :  1.3556180000305176 3.259669303894043 164.33908081054688
Loss :  1.4432454109191895 3.970395803451538 199.96304321289062
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.47322416305542 4.023568630218506 202.6516571044922
Loss :  1.464494228363037 4.01542329788208 202.23565673828125
Loss :  1.47873854637146 3.9524588584899902 199.1016845703125
Total LOSS train 183.68463463416467 valid 200.98801040649414
CE LOSS train 1.4471731717769916 valid 0.369684636592865
Contrastive LOSS train 3.644749215932993 valid 0.9881147146224976
EPOCH 70:
Loss :  1.5044941902160645 3.5261456966400146 177.81178283691406
Loss :  1.5309264659881592 3.556420087814331 179.3519287109375
Loss :  1.4675873517990112 3.5269649028778076 177.8158416748047
Loss :  1.4837257862091064 3.6513442993164062 184.05093383789062
Loss :  1.5197606086730957 3.4628798961639404 174.66375732421875
Loss :  1.4441428184509277 3.4606359004974365 174.47593688964844
Loss :  1.5190016031265259 3.613792657852173 182.20863342285156
Loss :  1.4629321098327637 3.5064878463745117 176.78732299804688
Loss :  1.439733862876892 3.7932727336883545 191.10336303710938
Loss :  1.5187233686447144 3.2732198238372803 165.17971801757812
Loss :  1.423701524734497 3.298407793045044 166.34410095214844
Loss :  1.42162024974823 3.411283016204834 171.9857635498047
Loss :  1.420082688331604 3.9504828453063965 198.94422912597656
Loss :  1.4363423585891724 3.6481704711914062 183.84486389160156
Loss :  1.548006534576416 3.942549228668213 198.6754608154297
Loss :  1.5501457452774048 3.9837751388549805 200.7388916015625
Loss :  1.4121707677841187 3.618229389190674 182.32363891601562
Loss :  1.4722256660461426 3.5237839221954346 177.6614227294922
Loss :  1.405293583869934 3.4699177742004395 174.90118408203125
Loss :  1.5361162424087524 3.4884603023529053 175.95913696289062
  batch 20 loss: 1.5361162424087524, 3.4884603023529053, 175.95913696289062
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.4654836654663086 3.435216188430786 173.22628784179688
Loss :  1.404719591140747 3.485591411590576 175.68429565429688
Loss :  1.4443144798278809 3.409409761428833 171.91481018066406
Loss :  1.4894249439239502 3.4903197288513184 176.0054168701172
Loss :  1.549296259880066 4.204752445220947 211.78692626953125
Loss :  1.4558621644973755 3.588811159133911 180.8964080810547
Loss :  1.4787318706512451 3.7598371505737305 189.4705810546875
Loss :  1.4617687463760376 3.443373441696167 173.63043212890625
Loss :  1.354358196258545 3.538038730621338 178.25628662109375
Loss :  1.5356175899505615 3.407100200653076 171.890625
Loss :  1.3512954711914062 3.5825839042663574 180.48049926757812
Loss :  1.4991240501403809 3.4237287044525146 172.68556213378906
Loss :  1.4526780843734741 3.4331934452056885 173.1123504638672
Loss :  1.4504306316375732 4.2789626121521 215.3985595703125
Loss :  1.3688960075378418 3.6825828552246094 185.498046875
Loss :  1.4060101509094238 3.465120792388916 174.66204833984375
Loss :  1.4060193300247192 3.3968687057495117 171.24945068359375
Loss :  1.5371919870376587 3.9120686054229736 197.14060974121094
Loss :  1.548193097114563 3.9001047611236572 196.5534210205078
Loss :  1.5645167827606201 4.123162269592285 207.72262573242188
  batch 40 loss: 1.5645167827606201, 4.123162269592285, 207.72262573242188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.4794789552688599 3.9025819301605225 196.6085662841797
Loss :  1.4455019235610962 3.624905824661255 182.69078063964844
Loss :  1.4219856262207031 3.573859930038452 180.11497497558594
Loss :  1.4444630146026611 3.5226292610168457 177.575927734375
Loss :  1.4058951139450073 3.507667064666748 176.78924560546875
Loss :  1.4696072340011597 3.782041311264038 190.57167053222656
Loss :  1.541599154472351 3.786803722381592 190.88177490234375
Loss :  1.4391287565231323 3.895197868347168 196.1990203857422
Loss :  1.5732892751693726 3.694495439529419 186.29806518554688
Loss :  1.4521658420562744 3.4739110469818115 175.14772033691406
Loss :  1.5203911066055298 3.72210431098938 187.62559509277344
Loss :  1.5156641006469727 3.805783271789551 191.80484008789062
Loss :  1.4683401584625244 3.5174612998962402 177.34140014648438
Loss :  1.5374598503112793 3.830404043197632 193.0576629638672
Loss :  1.4461086988449097 4.109519004821777 206.92205810546875
Loss :  1.5849210023880005 4.149381160736084 209.05397033691406
Loss :  1.4613585472106934 3.7481486797332764 188.86880493164062
Loss :  1.4278208017349243 3.669254779815674 184.89056396484375
Loss :  1.4605728387832642 3.522454023361206 177.58326721191406
Loss :  1.5953888893127441 3.4724318981170654 175.21697998046875
  batch 60 loss: 1.5953888893127441, 3.4724318981170654, 175.21697998046875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.4268137216567993 3.9401838779449463 198.43601989746094
Loss :  1.4839084148406982 3.5080454349517822 176.88616943359375
Loss :  1.4446799755096436 3.283999443054199 165.6446533203125
Loss :  1.4182895421981812 4.161168575286865 209.47671508789062
Loss :  1.3779062032699585 3.1023495197296143 156.49537658691406
Loss :  1.4640307426452637 3.724538564682007 187.6909637451172
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.4930531978607178 3.9312336444854736 198.0547332763672
Loss :  1.4845688343048096 3.735671043395996 188.26812744140625
Loss :  1.4965825080871582 3.768596887588501 189.92642211914062
Total LOSS train 183.7580765944261 valid 190.9850616455078
CE LOSS train 1.4705139288535485 valid 0.37414562702178955
Contrastive LOSS train 3.6457512818850004 valid 0.9421492218971252
EPOCH 71:
Loss :  1.5172364711761475 3.4386181831359863 173.44815063476562
Loss :  1.5406087636947632 3.6669342517852783 184.88731384277344
Loss :  1.4801989793777466 3.308177947998047 166.88909912109375
Loss :  1.4946339130401611 3.521444082260132 177.56683349609375
Loss :  1.5283527374267578 3.2847206592559814 165.76438903808594
Loss :  1.4569255113601685 3.2722878456115723 165.07131958007812
Loss :  1.5278409719467163 3.5102603435516357 177.04086303710938
Loss :  1.476831078529358 3.132258892059326 158.08978271484375
Loss :  1.4561445713043213 3.889641284942627 195.93821716308594
Loss :  1.531938076019287 3.224059581756592 162.7349090576172
Loss :  1.442994475364685 3.6932783126831055 186.10690307617188
Loss :  1.4400763511657715 3.5278944969177246 177.83480834960938
Loss :  1.4394783973693848 4.003292560577393 201.60411071777344
Loss :  1.455554485321045 3.1504995822906494 158.98052978515625
Loss :  1.5584014654159546 3.2773900032043457 165.4279022216797
Loss :  1.5589582920074463 3.805025339126587 191.8102264404297
Loss :  1.431373953819275 3.5081567764282227 176.83921813964844
Loss :  1.4878524541854858 3.523064136505127 177.6410675048828
Loss :  1.4255815744400024 3.3650121688842773 169.6761932373047
Loss :  1.546994924545288 3.4626989364624023 174.68194580078125
  batch 20 loss: 1.546994924545288, 3.4626989364624023, 174.68194580078125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.4818787574768066 3.0111887454986572 152.0413055419922
Loss :  1.4240854978561401 3.2134735584259033 162.09776306152344
Loss :  1.4597686529159546 3.7882449626922607 190.87200927734375
Loss :  1.5013647079467773 3.425952434539795 172.79898071289062
Loss :  1.557672381401062 3.5869944095611572 180.9073944091797
Loss :  1.4702211618423462 3.7190792560577393 187.42417907714844
Loss :  1.4914321899414062 3.4960174560546875 176.29229736328125
Loss :  1.4781999588012695 3.5516765117645264 179.06202697753906
Loss :  1.3784044981002808 3.745300769805908 188.64344787597656
Loss :  1.547247290611267 3.432208776473999 173.15768432617188
Loss :  1.375797152519226 3.3372340202331543 168.23748779296875
Loss :  1.514082670211792 4.345219135284424 218.77503967285156
Loss :  1.471204400062561 3.430866241455078 173.01451110839844
Loss :  1.4686638116836548 3.3733224868774414 170.13478088378906
Loss :  1.3919352293014526 3.484699010848999 175.62689208984375
Loss :  1.4247841835021973 3.5344390869140625 178.14674377441406
Loss :  1.4242149591445923 3.5543386936187744 179.14114379882812
Loss :  1.5446174144744873 3.5201330184936523 177.55126953125
Loss :  1.5581130981445312 3.945923328399658 198.85427856445312
Loss :  1.574079155921936 3.3506600856781006 169.10708618164062
  batch 40 loss: 1.574079155921936, 3.3506600856781006, 169.10708618164062
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.4946750402450562 3.3766167163848877 170.3255157470703
Loss :  1.4658211469650269 3.5262210369110107 177.77687072753906
Loss :  1.4465746879577637 3.869051218032837 194.89913940429688
Loss :  1.469616413116455 3.4546685218811035 174.2030487060547
Loss :  1.433992862701416 3.278717041015625 165.36984252929688
Loss :  1.4911491870880127 3.4555842876434326 174.27037048339844
Loss :  1.555495262145996 3.3743207454681396 170.2715301513672
Loss :  1.4620099067687988 3.481834650039673 175.55374145507812
Loss :  1.5850626230239868 3.0807559490203857 155.62286376953125
Loss :  1.4754830598831177 3.2110824584960938 162.02960205078125
Loss :  1.5387784242630005 3.494511842727661 176.2643585205078
Loss :  1.533980369567871 3.533015489578247 178.18475341796875
Loss :  1.4887826442718506 3.551506757736206 179.06411743164062
Loss :  1.550102710723877 3.531424045562744 178.12130737304688
Loss :  1.4668906927108765 3.2681660652160645 164.8751983642578
Loss :  1.5930960178375244 3.1520144939422607 159.19381713867188
Loss :  1.481121301651001 3.642042398452759 183.583251953125
Loss :  1.451964020729065 3.8484199047088623 193.87295532226562
Loss :  1.4827131032943726 3.3769712448120117 170.33126831054688
Loss :  1.6066725254058838 3.3565099239349365 169.4321746826172
  batch 60 loss: 1.6066725254058838, 3.3565099239349365, 169.4321746826172
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.4542186260223389 3.555478572845459 179.2281494140625
Loss :  1.5072191953659058 3.6207692623138428 182.54568481445312
Loss :  1.4709510803222656 3.481093168258667 175.52560424804688
Loss :  1.4474862813949585 3.437575340270996 173.3262481689453
Loss :  1.4100515842437744 2.9264962673187256 147.73486328125
Loss :  1.4938833713531494 4.289128303527832 215.95030212402344
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5206795930862427 4.341879844665527 218.6146697998047
Loss :  1.5125935077667236 4.150243759155273 209.0247802734375
Loss :  1.5241903066635132 4.211392879486084 212.0938262939453
Total LOSS train 175.56194434532753 valid 213.92089462280273
CE LOSS train 1.4876869751856878 valid 0.3810475766658783
Contrastive LOSS train 3.481485150410579 valid 1.052848219871521
EPOCH 72:
Loss :  1.5368067026138306 3.1382946968078613 158.4515380859375
Loss :  1.5570957660675049 3.447093963623047 173.91180419921875
Loss :  1.5026801824569702 3.307105541229248 166.8579559326172
Loss :  1.5167243480682373 3.5293586254119873 177.98464965820312
Loss :  1.547524333000183 3.340236186981201 168.55934143066406
Loss :  1.4831303358078003 3.393371343612671 171.1516876220703
Loss :  1.5480941534042358 3.5929572582244873 181.19595336914062
Loss :  1.5019359588623047 3.4459800720214844 173.8009490966797
Loss :  1.4830330610275269 3.4740359783172607 175.18482971191406
Loss :  1.5511316061019897 3.1047749519348145 156.78988647460938
Loss :  1.4700777530670166 3.562682628631592 179.6042022705078
Loss :  1.468374252319336 3.608391046524048 181.88792419433594
Loss :  1.4686174392700195 3.5768887996673584 180.31304931640625
Loss :  1.485068917274475 3.542316436767578 178.6008758544922
Loss :  1.577683687210083 3.4910805225372314 176.1317138671875
Loss :  1.5773930549621582 3.5969011783599854 181.42245483398438
Loss :  1.464037537574768 3.6038084030151367 181.65444946289062
Loss :  1.514485478401184 3.428631544113159 172.94606018066406
Loss :  1.4589611291885376 2.8973336219787598 146.3256378173828
Loss :  1.567367434501648 3.4649007320404053 174.81240844726562
  batch 20 loss: 1.567367434501648, 3.4649007320404053, 174.81240844726562
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.509813904762268 3.482402801513672 175.6299591064453
Loss :  1.4571160078048706 3.465209484100342 174.71759033203125
Loss :  1.487955093383789 3.4093704223632812 171.95648193359375
Loss :  1.525004267692566 3.779047727584839 190.47740173339844
Loss :  1.5758447647094727 3.6230154037475586 182.72662353515625
Loss :  1.497642993927002 3.483604669570923 175.67788696289062
Loss :  1.5171763896942139 3.6767935752868652 185.3568572998047
Loss :  1.505629301071167 3.869905710220337 195.00091552734375
Loss :  1.414841651916504 3.382175922393799 170.5236358642578
Loss :  1.5669443607330322 3.5471861362457275 178.92625427246094
Loss :  1.4129934310913086 3.4539313316345215 174.10955810546875
Loss :  1.53789222240448 3.5584869384765625 179.4622344970703
Loss :  1.4986931085586548 3.6044538021087646 181.72137451171875
Loss :  1.496346354484558 3.9570982456207275 199.3512725830078
Loss :  1.4274351596832275 3.6283023357391357 182.84254455566406
Loss :  1.4570645093917847 4.1748456954956055 210.1993408203125
Loss :  1.4564510583877563 3.7226369380950928 187.5883026123047
Loss :  1.564968466758728 3.525249481201172 177.82745361328125
Loss :  1.5752190351486206 3.340561628341675 168.60330200195312
Loss :  1.5891724824905396 3.154686689376831 159.32350158691406
  batch 40 loss: 1.5891724824905396, 3.154686689376831, 159.32350158691406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5172497034072876 3.5559487342834473 179.31468200683594
Loss :  1.4907983541488647 3.5788075923919678 180.43118286132812
Loss :  1.4731594324111938 3.78625226020813 190.7857666015625
Loss :  1.4933313131332397 3.4222841262817383 172.6075439453125
Loss :  1.460524559020996 3.3850371837615967 170.71238708496094
Loss :  1.5122342109680176 3.286137819290161 165.81912231445312
Loss :  1.571178913116455 3.9019906520843506 196.67071533203125
Loss :  1.4864073991775513 3.956084966659546 199.2906494140625
Loss :  1.5982969999313354 4.33774471282959 218.48553466796875
Loss :  1.4978747367858887 3.1268348693847656 157.83961486816406
Loss :  1.5554358959197998 3.7418200969696045 188.6464385986328
Loss :  1.5509566068649292 3.605504035949707 181.82615661621094
Loss :  1.510162591934204 3.6611082553863525 184.56558227539062
Loss :  1.5664805173873901 3.512094259262085 177.1711883544922
Loss :  1.491374135017395 3.555591106414795 179.27093505859375
Loss :  1.6061264276504517 3.110614776611328 157.13685607910156
Loss :  1.5036810636520386 3.5879693031311035 180.9021453857422
Loss :  1.477121353149414 3.6191253662109375 182.4333953857422
Loss :  1.504445195198059 3.687204599380493 185.8646697998047
Loss :  1.6164348125457764 3.434328556060791 173.33287048339844
  batch 60 loss: 1.6164348125457764, 3.434328556060791, 173.33287048339844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.4779268503189087 3.445868730545044 173.7713623046875
Loss :  1.5272753238677979 3.5481784343719482 178.9362030029297
Loss :  1.4941816329956055 3.8753271102905273 195.2605438232422
Loss :  1.4739270210266113 3.529407262802124 177.9442901611328
Loss :  1.439719319343567 3.1725988388061523 160.0696563720703
Loss :  1.5159598588943481 4.18785285949707 210.90859985351562
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.540895700454712 4.171206474304199 210.10121154785156
Loss :  1.5338338613510132 4.093055248260498 206.18658447265625
Loss :  1.543307900428772 3.9753663539886475 200.31161499023438
Total LOSS train 178.28768240121695 valid 206.87700271606445
CE LOSS train 1.5115497240653404 valid 0.385826975107193
Contrastive LOSS train 3.535522648004385 valid 0.9938415884971619
EPOCH 73:
Loss :  1.5551730394363403 3.308232545852661 166.966796875
Loss :  1.5737037658691406 3.5044569969177246 176.7965545654297
Loss :  1.523869276046753 3.4221441745758057 172.63107299804688
Loss :  1.5375993251800537 3.7236368656158447 187.7194366455078
Loss :  1.5649765729904175 3.555302381515503 179.33009338378906
Loss :  1.5047403573989868 3.3576173782348633 169.3856201171875
Loss :  1.5635440349578857 3.3201332092285156 167.5701904296875
Loss :  1.5216082334518433 3.3266024589538574 167.8517303466797
Loss :  1.5049077272415161 3.380352020263672 170.52252197265625
Loss :  1.56654691696167 3.2543883323669434 164.2859649658203
Loss :  1.4923738241195679 3.540245771408081 178.50465393066406
Loss :  1.489843726158142 3.760554313659668 189.51754760742188
Loss :  1.4896386861801147 3.769951581954956 189.98721313476562
Loss :  1.5041797161102295 3.443519115447998 173.6801300048828
Loss :  1.5897166728973389 3.1990137100219727 161.5404052734375
Loss :  1.5872533321380615 3.287442922592163 165.9593963623047
Loss :  1.4825057983398438 3.2345242500305176 163.20870971679688
Loss :  1.5290756225585938 4.081803321838379 205.61923217773438
Loss :  1.4782376289367676 3.574530601501465 180.20477294921875
Loss :  1.5796369314193726 3.6492884159088135 184.04405212402344
  batch 20 loss: 1.5796369314193726, 3.6492884159088135, 184.04405212402344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5257858037948608 3.4130821228027344 172.17990112304688
Loss :  1.4753649234771729 3.39270281791687 171.1105194091797
Loss :  1.5031806230545044 3.3379368782043457 168.4000244140625
Loss :  1.5385210514068604 3.3134567737579346 167.21136474609375
Loss :  1.5834786891937256 3.8838741779327393 195.77719116210938
Loss :  1.5089123249053955 3.3503031730651855 169.02406311035156
Loss :  1.526824951171875 3.6243796348571777 182.7458038330078
Loss :  1.5159345865249634 3.4217123985290527 172.60154724121094
Loss :  1.4281342029571533 3.3238906860351562 167.62266540527344
Loss :  1.5719317197799683 3.3368632793426514 168.41510009765625
Loss :  1.4227982759475708 4.039770126342773 203.41131591796875
Loss :  1.540084719657898 3.669999122619629 185.0400390625
Loss :  1.502469778060913 3.5701537132263184 180.01016235351562
Loss :  1.4996211528778076 3.595554828643799 181.27735900878906
Loss :  1.434250831604004 3.7395377159118652 188.4111328125
Loss :  1.4641516208648682 3.5801446437835693 180.4713897705078
Loss :  1.4637548923492432 3.408250093460083 171.8762664794922
Loss :  1.5688530206680298 3.8808059692382812 195.60914611816406
Loss :  1.5797739028930664 3.281996250152588 165.67958068847656
Loss :  1.5926557779312134 3.356842041015625 169.43475341796875
  batch 40 loss: 1.5926557779312134, 3.356842041015625, 169.43475341796875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.523808479309082 3.371234655380249 170.08554077148438
Loss :  1.4984431266784668 3.5469319820404053 178.8450469970703
Loss :  1.4810935258865356 3.444805145263672 173.7213592529297
Loss :  1.5013198852539062 3.4310262203216553 173.05264282226562
Loss :  1.4699149131774902 3.190707206726074 161.00526428222656
Loss :  1.5207054615020752 3.4416868686676025 173.6050567626953
Loss :  1.5763570070266724 3.5630340576171875 179.72805786132812
Loss :  1.4946531057357788 4.376212120056152 220.30526733398438
Loss :  1.6031196117401123 3.8702239990234375 195.11431884765625
Loss :  1.5044302940368652 4.031649589538574 203.08689880371094
Loss :  1.5598937273025513 3.632115364074707 183.16566467285156
Loss :  1.5537774562835693 3.237717628479004 163.4396514892578
Loss :  1.514120101928711 3.306337594985962 166.83099365234375
Loss :  1.5685925483703613 3.2415807247161865 163.6476287841797
Loss :  1.4949792623519897 3.6172943115234375 182.3596954345703
Loss :  1.6064660549163818 3.4265246391296387 172.9326934814453
Loss :  1.5056476593017578 3.6796483993530273 185.48806762695312
Loss :  1.4795526266098022 3.6940343379974365 186.1812744140625
Loss :  1.5060539245605469 3.588923692703247 180.95223999023438
Loss :  1.6166110038757324 3.544562578201294 178.84474182128906
  batch 60 loss: 1.6166110038757324, 3.544562578201294, 178.84474182128906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.479009747505188 3.492089033126831 176.0834503173828
Loss :  1.527029275894165 3.7746877670288086 190.26141357421875
Loss :  1.4937245845794678 3.4036102294921875 171.6742401123047
Loss :  1.4721746444702148 3.464378833770752 174.6911163330078
Loss :  1.4383572340011597 3.6304104328155518 182.95887756347656
Loss :  1.5022857189178467 3.561880588531494 179.5963134765625
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.526610255241394 3.461787462234497 174.61598205566406
Loss :  1.5196043252944946 3.457799196243286 174.40956115722656
Loss :  1.528377890586853 3.166628122329712 159.8597869873047
Total LOSS train 177.84148653470552 valid 172.12041091918945
CE LOSS train 1.521160758458651 valid 0.38209447264671326
Contrastive LOSS train 3.5264065265655518 valid 0.791657030582428
Saved best model. Old loss 185.83412551879883 and new best loss 172.12041091918945
EPOCH 74:
Loss :  1.5549777746200562 3.2096517086029053 162.03756713867188
Loss :  1.57414710521698 3.63027024269104 183.08766174316406
Loss :  1.5240800380706787 3.4493894577026367 173.99354553222656
Loss :  1.5355548858642578 3.376842498779297 170.377685546875
Loss :  1.5624269247055054 3.45328688621521 174.2267608642578
Loss :  1.5014421939849854 3.470832347869873 175.04306030273438
Loss :  1.5613282918930054 3.426692485809326 172.8959503173828
Loss :  1.5189894437789917 3.735457420349121 188.29185485839844
Loss :  1.5025348663330078 3.345156669616699 168.76036071777344
Loss :  1.5652953386306763 2.963991403579712 149.76486206054688
Loss :  1.4905006885528564 3.682891845703125 185.6350860595703
Loss :  1.4878002405166626 3.739288568496704 188.4522247314453
Loss :  1.4867645502090454 3.5948941707611084 181.23147583007812
Loss :  1.5012726783752441 3.3909647464752197 171.04949951171875
Loss :  1.5877768993377686 3.287898540496826 165.9827117919922
Loss :  1.5851445198059082 3.9867944717407227 200.92486572265625
Loss :  1.4798434972763062 3.586199998855591 180.7898406982422
Loss :  1.5257235765457153 3.3530492782592773 169.17819213867188
Loss :  1.4744014739990234 3.9715325832366943 200.051025390625
Loss :  1.5769115686416626 3.2405683994293213 163.60533142089844
  batch 20 loss: 1.5769115686416626, 3.2405683994293213, 163.60533142089844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5222340822219849 3.0948779582977295 156.26612854003906
Loss :  1.4730029106140137 3.2668182849884033 164.8139190673828
Loss :  1.5032016038894653 3.5253217220306396 177.769287109375
Loss :  1.5372365713119507 3.587679386138916 180.92120361328125
Loss :  1.5841912031173706 3.708601236343384 187.01426696777344
Loss :  1.5120664834976196 3.3925695419311523 171.1405487060547
Loss :  1.5299005508422852 3.588813304901123 180.97056579589844
Loss :  1.5197633504867554 3.0957608222961426 156.30780029296875
Loss :  1.435808777809143 3.068081855773926 154.83990478515625
Loss :  1.5774813890457153 3.285902738571167 165.87261962890625
Loss :  1.434093952178955 3.439296007156372 173.39889526367188
Loss :  1.5497931241989136 3.5513250827789307 179.1160430908203
Loss :  1.513187289237976 3.4424779415130615 173.6370849609375
Loss :  1.5111416578292847 3.552457571029663 179.13401794433594
Loss :  1.4491608142852783 3.60802960395813 181.8506317138672
Loss :  1.477442741394043 3.4879496097564697 175.8749237060547
Loss :  1.4774776697158813 3.4000513553619385 171.48004150390625
Loss :  1.5782134532928467 3.363961935043335 169.77630615234375
Loss :  1.5885237455368042 3.071112871170044 155.1441650390625
Loss :  1.6023024320602417 3.7183291912078857 187.5187530517578
  batch 40 loss: 1.6023024320602417, 3.7183291912078857, 187.5187530517578
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5373376607894897 3.897477149963379 196.41119384765625
Loss :  1.5126527547836304 3.4001810550689697 171.52169799804688
Loss :  1.4961355924606323 3.3673248291015625 169.8623809814453
Loss :  1.5142120122909546 3.498370409011841 176.43272399902344
Loss :  1.483819603919983 4.277075290679932 215.33758544921875
Loss :  1.5321441888809204 3.4803478717803955 175.54954528808594
Loss :  1.5854750871658325 3.8528823852539062 194.22959899902344
Loss :  1.5086361169815063 3.5271809101104736 177.86767578125
Loss :  1.610559344291687 4.226185321807861 212.91983032226562
Loss :  1.5177783966064453 3.4318490028381348 173.1102294921875
Loss :  1.5711493492126465 3.5987963676452637 181.51097106933594
Loss :  1.5658910274505615 3.332637310028076 168.19775390625
Loss :  1.5289692878723145 3.2971513271331787 166.38653564453125
Loss :  1.5791304111480713 3.906484365463257 196.90335083007812
Loss :  1.5100150108337402 3.5747015476226807 180.24508666992188
Loss :  1.61542546749115 3.5680742263793945 180.01914978027344
Loss :  1.5209954977035522 3.6074459552764893 181.89329528808594
Loss :  1.4977947473526 3.369032382965088 169.94940185546875
Loss :  1.5221308469772339 3.522381544113159 177.64120483398438
Loss :  1.6258797645568848 3.5597081184387207 179.61129760742188
  batch 60 loss: 1.6258797645568848, 3.5597081184387207, 179.61129760742188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.4964643716812134 3.67604923248291 185.29891967773438
Loss :  1.540908932685852 3.8520474433898926 194.14328002929688
Loss :  1.5097253322601318 3.931337594985962 198.07659912109375
Loss :  1.4918031692504883 3.646064043045044 183.79501342773438
Loss :  1.4602267742156982 3.9182722568511963 197.37384033203125
Loss :  1.5265989303588867 4.2398247718811035 213.51783752441406
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5494331121444702 4.23477840423584 213.28836059570312
Loss :  1.5431301593780518 4.098047733306885 206.44552612304688
Loss :  1.551053762435913 4.030036449432373 203.05287170410156
Total LOSS train 178.03868971604567 valid 209.0761489868164
CE LOSS train 1.527821555504432 valid 0.38776344060897827
Contrastive LOSS train 3.530217379790086 valid 1.0075091123580933
EPOCH 75:
Loss :  1.5681015253067017 2.953397274017334 149.2379608154297
Loss :  1.5850954055786133 3.552518367767334 179.2110137939453
Loss :  1.5390926599502563 3.4465508460998535 173.86663818359375
Loss :  1.550367832183838 3.504542827606201 176.7775115966797
Loss :  1.5755106210708618 3.672269105911255 185.18896484375
Loss :  1.5188559293746948 3.1476800441741943 158.90286254882812
Loss :  1.5739378929138184 3.1775946617126465 160.45367431640625
Loss :  1.5341795682907104 3.2352278232574463 163.2955780029297
Loss :  1.518843412399292 3.4714303016662598 175.09036254882812
Loss :  1.5773464441299438 3.1425936222076416 158.70703125
Loss :  1.5088361501693726 3.6803510189056396 185.52638244628906
Loss :  1.5070130825042725 3.5364601612091064 178.3300323486328
Loss :  1.5073676109313965 3.8206067085266113 192.53770446777344
Loss :  1.520516276359558 3.595292329788208 181.28514099121094
Loss :  1.6005266904830933 3.5738487243652344 180.29296875
Loss :  1.5960019826889038 3.717132806777954 187.45265197753906
Loss :  1.499311923980713 3.362410068511963 169.61981201171875
Loss :  1.542973279953003 3.134207010269165 158.25332641601562
Loss :  1.4970524311065674 3.011418581008911 152.06797790527344
Loss :  1.5916051864624023 3.3997581005096436 171.57949829101562
  batch 20 loss: 1.5916051864624023, 3.3997581005096436, 171.57949829101562
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5429847240447998 3.454604148864746 174.273193359375
Loss :  1.4967986345291138 3.6287717819213867 182.9353790283203
Loss :  1.524556040763855 3.630598306655884 183.05447387695312
Loss :  1.555403709411621 3.477893829345703 175.45008850097656
Loss :  1.597480297088623 3.4123241901397705 172.21368408203125
Loss :  1.5304913520812988 3.4173552989959717 172.39825439453125
Loss :  1.5461277961730957 3.57816481590271 180.45436096191406
Loss :  1.5368183851242065 3.3098700046539307 167.0303192138672
Loss :  1.4587653875350952 3.4502134323120117 173.9694366455078
Loss :  1.5895867347717285 3.5843398571014404 180.80657958984375
Loss :  1.4569261074066162 3.5183324813842773 177.37355041503906
Loss :  1.5649772882461548 4.071885585784912 205.1592559814453
Loss :  1.5315914154052734 3.344945192337036 168.77883911132812
Loss :  1.5293210744857788 3.379337787628174 170.4962158203125
Loss :  1.4711037874221802 3.4190704822540283 172.42462158203125
Loss :  1.496692180633545 3.4293746948242188 172.96542358398438
Loss :  1.4970731735229492 3.3763434886932373 170.31423950195312
Loss :  1.5898926258087158 3.2222981452941895 162.70480346679688
Loss :  1.5994882583618164 3.780338764190674 190.61642456054688
Loss :  1.611295461654663 3.467616081237793 174.99209594726562
  batch 40 loss: 1.611295461654663, 3.467616081237793, 174.99209594726562
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5506480932235718 3.6793813705444336 185.51971435546875
Loss :  1.5280046463012695 3.157186985015869 159.38734436035156
Loss :  1.5129737854003906 3.152956247329712 159.16078186035156
Loss :  1.5294493436813354 3.6973888874053955 186.39889526367188
Loss :  1.5011873245239258 3.8966166973114014 196.33203125
Loss :  1.5466090440750122 3.4471755027770996 173.9053955078125
Loss :  1.5965595245361328 3.5550971031188965 179.35140991210938
Loss :  1.5255622863769531 3.2792177200317383 165.4864501953125
Loss :  1.6201609373092651 3.350703239440918 169.1553192138672
Loss :  1.5346702337265015 3.6722779273986816 185.1485595703125
Loss :  1.5846054553985596 3.864797830581665 194.8245086669922
Loss :  1.578421711921692 3.5090172290802 177.02928161621094
Loss :  1.5423263311386108 3.5550811290740967 179.29638671875
Loss :  1.5884997844696045 3.9653871059417725 199.85784912109375
Loss :  1.5232056379318237 3.4646241664886475 174.75440979003906
Loss :  1.6209065914154053 3.4536850452423096 174.30516052246094
Loss :  1.5313301086425781 3.592883586883545 181.17550659179688
Loss :  1.5093305110931396 3.4578325748443604 174.4009552001953
Loss :  1.5330697298049927 3.5764029026031494 180.35321044921875
Loss :  1.631583333015442 3.492439031600952 176.25352478027344
  batch 60 loss: 1.631583333015442, 3.492439031600952, 176.25352478027344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5097538232803345 3.5177154541015625 177.39552307128906
Loss :  1.5525869131088257 3.6629538536071777 184.7002716064453
Loss :  1.5227539539337158 3.5811030864715576 180.57791137695312
Loss :  1.504425048828125 3.6168389320373535 182.34637451171875
Loss :  1.473374605178833 3.0875654220581055 155.8516387939453
Loss :  1.5339125394821167 4.156444072723389 209.35610961914062
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.5553147792816162 4.082574844360352 205.68405151367188
Loss :  1.5498758554458618 4.128455638885498 207.97265625
Loss :  1.556251883506775 3.817246913909912 192.41860961914062
Total LOSS train 175.67779611440804 valid 203.85785675048828
CE LOSS train 1.5429524476711567 valid 0.3890629708766937
Contrastive LOSS train 3.4826968889970047 valid 0.954311728477478
EPOCH 76:
Loss :  1.5762032270431519 3.10623836517334 156.88812255859375
Loss :  1.5923230648040771 3.4709908962249756 175.14186096191406
Loss :  1.5477513074874878 3.3216054439544678 167.62802124023438
Loss :  1.558695673942566 3.1912875175476074 161.12307739257812
Loss :  1.582992672920227 3.316324472427368 167.3992156982422
Loss :  1.5299901962280273 3.532745361328125 178.16725158691406
Loss :  1.5835485458374023 3.53316330909729 178.24171447753906
Loss :  1.5465524196624756 3.432368755340576 173.1649932861328
Loss :  1.5314300060272217 3.5601441860198975 179.53863525390625
Loss :  1.5864861011505127 3.5802180767059326 180.59739685058594
Loss :  1.519065499305725 3.4731602668762207 175.1770782470703
Loss :  1.516491413116455 3.5542893409729004 179.23095703125
Loss :  1.5160696506500244 3.3212289810180664 167.5775146484375
Loss :  1.5284072160720825 3.6679351329803467 184.9251708984375
Loss :  1.606329083442688 3.8446154594421387 193.83709716796875
Loss :  1.603617787361145 3.68255352973938 185.73129272460938
Loss :  1.511945366859436 3.5910022258758545 181.0620574951172
Loss :  1.5538159608840942 3.52492094039917 177.79986572265625
Loss :  1.5094941854476929 3.6376261711120605 183.3907928466797
Loss :  1.5999879837036133 3.251516580581665 164.1758270263672
  batch 20 loss: 1.5999879837036133, 3.251516580581665, 164.1758270263672
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5529038906097412 3.461266040802002 174.6162109375
Loss :  1.5103631019592285 3.709070920944214 186.9639129638672
Loss :  1.5366356372833252 3.504140615463257 176.74366760253906
Loss :  1.5650101900100708 3.531766414642334 178.15333557128906
Loss :  1.6050612926483154 3.7279810905456543 188.0041046142578
Loss :  1.5407644510269165 3.391216993331909 171.10162353515625
Loss :  1.5556801557540894 3.6373353004455566 183.4224395751953
Loss :  1.5480738878250122 4.110161304473877 207.05615234375
Loss :  1.472365379333496 3.1390953063964844 158.42713928222656
Loss :  1.5980403423309326 3.770512580871582 190.12367248535156
Loss :  1.4716988801956177 3.5821127891540527 180.57733154296875
Loss :  1.5746033191680908 3.9838318824768066 200.7661895751953
Loss :  1.5417664051055908 3.728455066680908 187.9645233154297
Loss :  1.5395199060440063 3.7431559562683105 188.6973114013672
Loss :  1.4829235076904297 3.3577513694763184 169.37049865722656
Loss :  1.505998134613037 3.5732667446136475 180.16932678222656
Loss :  1.5054888725280762 3.600513219833374 181.53115844726562
Loss :  1.5956952571868896 3.32429838180542 167.81060791015625
Loss :  1.6048282384872437 3.4788944721221924 175.54954528808594
Loss :  1.615097165107727 3.5902864933013916 181.12942504882812
  batch 40 loss: 1.615097165107727, 3.5902864933013916, 181.12942504882812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5550646781921387 3.4286587238311768 172.98800659179688
Loss :  1.531380295753479 3.4149904251098633 172.28091430664062
Loss :  1.5164105892181396 3.4031286239624023 171.67283630371094
Loss :  1.532231092453003 3.450686454772949 174.06654357910156
Loss :  1.5028412342071533 3.2260096073150635 162.80331420898438
Loss :  1.5460703372955322 3.275683641433716 165.3302459716797
Loss :  1.594896912574768 3.3130733966827393 167.24856567382812
Loss :  1.5233649015426636 3.50284481048584 176.6656036376953
Loss :  1.6189439296722412 3.4603536128997803 174.63662719726562
Loss :  1.5327212810516357 3.502051830291748 176.6352996826172
Loss :  1.5829498767852783 3.564615488052368 179.813720703125
Loss :  1.5765830278396606 3.4518558979034424 174.16937255859375
Loss :  1.5414310693740845 3.2224998474121094 162.6664276123047
Loss :  1.5879011154174805 3.3121843338012695 167.19712829589844
Loss :  1.5244485139846802 3.5628676414489746 179.66783142089844
Loss :  1.6217141151428223 3.504695177078247 176.85647583007812
Loss :  1.5342217683792114 3.563424825668335 179.70545959472656
Loss :  1.5134927034378052 3.3670783042907715 169.86740112304688
Loss :  1.5362987518310547 3.55877685546875 179.4751434326172
Loss :  1.6327800750732422 3.352433919906616 169.2544708251953
  batch 60 loss: 1.6327800750732422, 3.352433919906616, 169.2544708251953
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5120790004730225 3.2670419216156006 164.8641815185547
Loss :  1.5545400381088257 3.5845425128936768 180.7816619873047
Loss :  1.5249667167663574 3.511237621307373 177.0868377685547
Loss :  1.5080795288085938 3.6029088497161865 181.65353393554688
Loss :  1.4788190126419067 2.9441003799438477 148.683837890625
Loss :  1.535430908203125 4.0368452072143555 203.377685546875
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5566810369491577 3.793734312057495 191.243408203125
Loss :  1.5511815547943115 3.760841131210327 189.59323120117188
Loss :  1.5575474500656128 3.5943779945373535 181.2764434814453
Total LOSS train 176.07765479454628 valid 191.3726921081543
CE LOSS train 1.5493530145058265 valid 0.3893868625164032
Contrastive LOSS train 3.490566040919377 valid 0.8985944986343384
EPOCH 77:
Loss :  1.5807243585586548 3.288172721862793 165.98934936523438
Loss :  1.5973882675170898 3.5400125980377197 178.59800720214844
Loss :  1.5534504652023315 3.33087158203125 168.09703063964844
Loss :  1.564147710800171 3.74853777885437 188.9910430908203
Loss :  1.5872832536697388 3.07330060005188 155.25230407714844
Loss :  1.5342299938201904 3.5120115280151367 177.13479614257812
Loss :  1.5864651203155518 3.4949593544006348 176.33444213867188
Loss :  1.5504095554351807 3.1981797218322754 161.4593963623047
Loss :  1.5365785360336304 3.2639975547790527 164.7364501953125
Loss :  1.59064519405365 3.099088430404663 156.54507446289062
Loss :  1.5258574485778809 3.7480075359344482 188.92623901367188
Loss :  1.524092197418213 3.4256415367126465 172.80616760253906
Loss :  1.5239624977111816 3.304028272628784 166.72537231445312
Loss :  1.536719799041748 3.4955592155456543 176.31466674804688
Loss :  1.6125932931900024 3.308110475540161 167.0181121826172
Loss :  1.6089838743209839 3.6060755252838135 181.91275024414062
Loss :  1.519915223121643 3.164640188217163 159.75192260742188
Loss :  1.559934139251709 3.2603859901428223 164.57923889160156
Loss :  1.5167962312698364 3.1066982746124268 156.85171508789062
Loss :  1.604005217552185 3.584930658340454 180.8505401611328
  batch 20 loss: 1.604005217552185, 3.584930658340454, 180.8505401611328
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5582295656204224 3.436232328414917 173.36984252929688
Loss :  1.5173596143722534 4.039287567138672 203.48175048828125
Loss :  1.5427863597869873 3.4214422702789307 172.61489868164062
Loss :  1.571805477142334 3.331951379776001 168.16937255859375
Loss :  1.611127257347107 3.521353006362915 177.6787872314453
Loss :  1.549744963645935 3.520864725112915 177.59298706054688
Loss :  1.5638878345489502 3.964754819869995 199.8016357421875
Loss :  1.5566245317459106 3.615222215652466 182.31773376464844
Loss :  1.4838054180145264 4.084306716918945 205.6991424560547
Loss :  1.6051644086837769 3.353585720062256 169.28445434570312
Loss :  1.4824587106704712 3.463517427444458 174.6583251953125
Loss :  1.5814201831817627 3.7385480403900146 188.50881958007812
Loss :  1.5486268997192383 3.60024094581604 181.56068420410156
Loss :  1.5463812351226807 3.618978261947632 182.49530029296875
Loss :  1.4925713539123535 3.3067381381988525 166.82948303222656
Loss :  1.5154294967651367 3.3567614555358887 169.35350036621094
Loss :  1.5150007009506226 3.3715951442718506 170.09475708007812
Loss :  1.6011128425598145 3.497391939163208 176.47071838378906
Loss :  1.6095398664474487 3.281435251235962 165.68130493164062
Loss :  1.6211316585540771 3.6528894901275635 184.26559448242188
  batch 40 loss: 1.6211316585540771, 3.6528894901275635, 184.26559448242188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5642642974853516 3.8604798316955566 194.5882568359375
Loss :  1.5429742336273193 3.1615285873413086 159.61940002441406
Loss :  1.529155969619751 3.542029619216919 178.63064575195312
Loss :  1.5450444221496582 3.220771074295044 162.58360290527344
Loss :  1.5183100700378418 3.2980551719665527 166.4210662841797
Loss :  1.5597490072250366 3.476963758468628 175.40794372558594
Loss :  1.6055052280426025 3.4680938720703125 175.01019287109375
Loss :  1.5385302305221558 3.452899217605591 174.18348693847656
Loss :  1.627618670463562 3.4618101119995117 174.71812438964844
Loss :  1.5468485355377197 3.2440240383148193 163.748046875
Loss :  1.5946853160858154 3.300010919570923 166.59523010253906
Loss :  1.5895578861236572 3.3471999168395996 168.94955444335938
Loss :  1.5576238632202148 3.9577713012695312 199.44618225097656
Loss :  1.6011875867843628 3.8999147415161133 196.596923828125
Loss :  1.5433237552642822 3.4199106693267822 172.53884887695312
Loss :  1.6329425573349 3.4611706733703613 174.6914825439453
Loss :  1.5513538122177124 3.29931640625 166.51718139648438
Loss :  1.5315120220184326 3.357604742050171 169.4117431640625
Loss :  1.5518449544906616 3.3868794441223145 170.8958282470703
Loss :  1.6415348052978516 3.441880226135254 173.7355499267578
  batch 60 loss: 1.6415348052978516, 3.441880226135254, 173.7355499267578
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5308678150177002 3.9856860637664795 200.81517028808594
Loss :  1.571327805519104 3.320969581604004 167.61981201171875
Loss :  1.544394850730896 3.359290361404419 169.5089111328125
Loss :  1.528836727142334 3.389232635498047 170.990478515625
Loss :  1.5009058713912964 2.888031005859375 145.90245056152344
Loss :  1.5561603307724 4.120349884033203 207.5736541748047
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5757064819335938 4.185238361358643 210.83761596679688
Loss :  1.5708781480789185 4.121829032897949 207.66232299804688
Loss :  1.5763014554977417 3.8936562538146973 196.2591094970703
Total LOSS train 174.4296896127554 valid 205.5831756591797
CE LOSS train 1.5590506315231323 valid 0.3940753638744354
Contrastive LOSS train 3.4574127747462344 valid 0.9734140634536743
EPOCH 78:
Loss :  1.5939278602600098 3.120651960372925 157.62652587890625
Loss :  1.6080163717269897 3.8760528564453125 195.41065979003906
Loss :  1.5674399137496948 3.442908763885498 173.71287536621094
Loss :  1.5781223773956299 3.3600285053253174 169.5795440673828
Loss :  1.601081371307373 3.1286256313323975 158.0323486328125
Loss :  1.5528405904769897 3.197462320327759 161.42596435546875
Loss :  1.6017382144927979 3.311875343322754 167.1955108642578
Loss :  1.5681750774383545 3.1847870349884033 160.80752563476562
Loss :  1.55483078956604 3.7047133445739746 186.79049682617188
Loss :  1.6046720743179321 3.6570205688476562 184.45570373535156
Loss :  1.5449559688568115 3.6238114833831787 182.7355194091797
Loss :  1.5432910919189453 3.418570041656494 172.47178649902344
Loss :  1.543038010597229 3.2593541145324707 164.51075744628906
Loss :  1.5550881624221802 3.480738401412964 175.59201049804688
Loss :  1.6249184608459473 3.3892176151275635 171.08580017089844
Loss :  1.6203805208206177 3.458077907562256 174.52427673339844
Loss :  1.5377956628799438 3.5326850414276123 178.1720428466797
Loss :  1.5742508172988892 3.211242437362671 162.13636779785156
Loss :  1.5342519283294678 3.09434175491333 156.2513427734375
Loss :  1.6156450510025024 3.2266716957092285 162.94923400878906
  batch 20 loss: 1.6156450510025024, 3.2266716957092285, 162.94923400878906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.5720123052597046 3.5672688484191895 179.93545532226562
Loss :  1.5321917533874512 3.31247878074646 167.1561279296875
Loss :  1.555812120437622 3.477008581161499 175.40625
Loss :  1.5832093954086304 3.290705919265747 166.11849975585938
Loss :  1.6194136142730713 3.8989059925079346 196.56471252441406
Loss :  1.5616753101348877 3.241367816925049 163.63006591796875
Loss :  1.574821949005127 3.4883687496185303 175.99327087402344
Loss :  1.5685540437698364 3.2718138694763184 165.1592559814453
Loss :  1.5004236698150635 3.4780001640319824 175.4004364013672
Loss :  1.6143163442611694 3.436849355697632 173.456787109375
Loss :  1.4992480278015137 3.3341851234436035 168.20851135253906
Loss :  1.5921326875686646 3.3816380500793457 170.67404174804688
Loss :  1.5626684427261353 3.6126890182495117 182.19711303710938
Loss :  1.5610778331756592 3.2269809246063232 162.91012573242188
Loss :  1.5111761093139648 3.3903586864471436 171.0290985107422
Loss :  1.5331348180770874 3.844515562057495 193.75892639160156
Loss :  1.5330806970596313 3.337219715118408 168.39407348632812
Loss :  1.6136324405670166 3.3951306343078613 171.3701629638672
Loss :  1.6213700771331787 3.3570759296417236 169.47515869140625
Loss :  1.6324069499969482 3.1644508838653564 159.85494995117188
  batch 40 loss: 1.6324069499969482, 3.1644508838653564, 159.85494995117188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.57976496219635 3.384204864501953 170.7899932861328
Loss :  1.5602117776870728 3.0822033882141113 155.67037963867188
Loss :  1.5481925010681152 3.505481481552124 176.822265625
Loss :  1.5629043579101562 3.1792213916778564 160.52398681640625
Loss :  1.537915587425232 3.024632215499878 152.76953125
Loss :  1.576588749885559 3.1715714931488037 160.1551513671875
Loss :  1.6190885305404663 3.15179443359375 159.2088165283203
Loss :  1.5589408874511719 4.466488361358643 224.88336181640625
Loss :  1.6409335136413574 3.364684820175171 169.8751678466797
Loss :  1.5669809579849243 3.173798084259033 160.25689697265625
Loss :  1.6104933023452759 3.2320351600646973 163.21224975585938
Loss :  1.6050777435302734 3.4671051502227783 174.9603271484375
Loss :  1.5762275457382202 3.0695550441741943 155.05398559570312
Loss :  1.6149708032608032 4.07150936126709 205.19044494628906
Loss :  1.5636765956878662 3.685253143310547 185.8263397216797
Loss :  1.6453596353530884 3.1878821849823 161.03945922851562
Loss :  1.5714079141616821 3.204335927963257 161.7882080078125
Loss :  1.5535229444503784 3.4260447025299072 172.8557586669922
Loss :  1.5712261199951172 3.442448616027832 173.69366455078125
Loss :  1.6532793045043945 3.7279508113861084 188.05081176757812
  batch 60 loss: 1.6532793045043945, 3.7279508113861084, 188.05081176757812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.5497347116470337 3.5840530395507812 180.75238037109375
Loss :  1.586087703704834 3.1007907390594482 156.62562561035156
Loss :  1.559320092201233 3.3596982955932617 169.5442352294922
Loss :  1.5442320108413696 3.2544844150543213 164.26846313476562
Loss :  1.517325520515442 3.0885367393493652 155.94415283203125
Loss :  1.577878475189209 3.859792470932007 194.5675048828125
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.5965958833694458 3.842921257019043 193.74266052246094
Loss :  1.5922783613204956 3.7127058506011963 187.2275848388672
Loss :  1.5974735021591187 3.5075368881225586 176.9743194580078
Total LOSS train 171.26032268817607 valid 188.1280174255371
CE LOSS train 1.5744658873631403 valid 0.39936837553977966
Contrastive LOSS train 3.393717127579909 valid 0.8768842220306396
EPOCH 79:
Loss :  1.6050076484680176 3.347559690475464 168.98300170898438
Loss :  1.618881106376648 3.938196897506714 198.5287322998047
Loss :  1.57972252368927 3.0780527591705322 155.48236083984375
Loss :  1.5885376930236816 3.426255941390991 172.9013214111328
Loss :  1.6088286638259888 3.228536367416382 163.03564453125
Loss :  1.5621850490570068 3.415618419647217 172.3430938720703
Loss :  1.6087239980697632 3.353224754333496 169.26995849609375
Loss :  1.576393961906433 3.1914315223693848 161.14797973632812
Loss :  1.5637723207473755 3.659613847732544 184.54446411132812
Loss :  1.6108484268188477 3.068260669708252 155.02389526367188
Loss :  1.55191171169281 3.902998208999634 196.7018280029297
Loss :  1.5499628782272339 3.3776745796203613 170.43368530273438
Loss :  1.5497475862503052 3.1976211071014404 161.43080139160156
Loss :  1.5614758729934692 3.241981267929077 163.66053771972656
Loss :  1.628793716430664 3.3252742290496826 167.89251708984375
Loss :  1.6240195035934448 3.8262341022491455 192.93572998046875
Loss :  1.54391348361969 3.4642395973205566 174.75588989257812
Loss :  1.579640507698059 3.1757326126098633 160.36627197265625
Loss :  1.5416643619537354 3.3038456439971924 166.73394775390625
Loss :  1.6209205389022827 3.3652517795562744 169.88351440429688
  batch 20 loss: 1.6209205389022827, 3.3652517795562744, 169.88351440429688
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5781855583190918 3.0626320838928223 154.7097930908203
Loss :  1.5404349565505981 3.572706699371338 180.17576599121094
Loss :  1.563924789428711 3.5163302421569824 177.38043212890625
Loss :  1.5899102687835693 3.3378050327301025 168.48016357421875
Loss :  1.6251676082611084 3.7382893562316895 188.53964233398438
Loss :  1.5695241689682007 3.1996994018554688 161.5544891357422
Loss :  1.581634521484375 3.420177698135376 172.59051513671875
Loss :  1.57598078250885 3.3515162467956543 169.1517791748047
Loss :  1.5090571641921997 3.308948040008545 166.9564666748047
Loss :  1.6201609373092651 3.2933239936828613 166.28636169433594
Loss :  1.508811593055725 3.2661988735198975 164.81874084472656
Loss :  1.5990036725997925 3.3217203617095947 167.6850128173828
Loss :  1.5695945024490356 3.5931153297424316 181.22535705566406
Loss :  1.5676265954971313 3.5739855766296387 180.26690673828125
Loss :  1.5190409421920776 3.1963231563568115 161.335205078125
Loss :  1.539901852607727 3.3551015853881836 169.29498291015625
Loss :  1.5396959781646729 3.313544511795044 167.21693420410156
Loss :  1.6181844472885132 3.3466851711273193 168.9524383544922
Loss :  1.6257327795028687 3.306993007659912 166.9753875732422
Loss :  1.6365793943405151 3.691018581390381 186.18751525878906
  batch 40 loss: 1.6365793943405151, 3.691018581390381, 186.18751525878906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5846185684204102 3.2296552658081055 163.0673828125
Loss :  1.5650513172149658 3.2303402423858643 163.08206176757812
Loss :  1.5532994270324707 3.1210217475891113 157.60438537597656
Loss :  1.5677894353866577 3.5333809852600098 178.23684692382812
Loss :  1.5423389673233032 3.507711172103882 176.9279022216797
Loss :  1.580507516860962 3.2796194553375244 165.5614776611328
Loss :  1.6222972869873047 3.353044271469116 169.27450561523438
Loss :  1.5636454820632935 3.231602430343628 163.14376831054688
Loss :  1.6439675092697144 3.1342597007751465 158.35694885253906
Loss :  1.5710535049438477 3.0516068935394287 154.15139770507812
Loss :  1.614256501197815 3.399566411972046 171.5925750732422
Loss :  1.6070574522018433 3.187047004699707 160.95941162109375
Loss :  1.5781170129776 3.55614972114563 179.38558959960938
Loss :  1.61695396900177 3.560539484024048 179.64393615722656
Loss :  1.5647132396697998 3.310147762298584 167.0720977783203
Loss :  1.6451060771942139 3.5119271278381348 177.24147033691406
Loss :  1.5703084468841553 3.1315691471099854 158.14877319335938
Loss :  1.5516088008880615 3.461089849472046 174.60609436035156
Loss :  1.5693140029907227 3.469630002975464 175.0508270263672
Loss :  1.6517568826675415 3.6063344478607178 181.96849060058594
  batch 60 loss: 1.6517568826675415, 3.6063344478607178, 181.96849060058594
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5494322776794434 3.6244924068450928 182.77406311035156
Loss :  1.588098168373108 3.5581843852996826 179.49732971191406
Loss :  1.563335657119751 3.0731494426727295 155.22080993652344
Loss :  1.5509822368621826 3.5078673362731934 176.94435119628906
Loss :  1.5253740549087524 2.844517707824707 143.7512664794922
Loss :  1.5810664892196655 3.9950883388519287 201.33547973632812
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.5997453927993774 3.9514966011047363 199.17457580566406
Loss :  1.5954748392105103 3.8474676609039307 193.96885681152344
Loss :  1.60020112991333 3.7833847999572754 190.76943969726562
Total LOSS train 170.14004352276143 valid 196.3120880126953
CE LOSS train 1.5799090440456685 valid 0.4000502824783325
Contrastive LOSS train 3.37120266694289 valid 0.9458461999893188
EPOCH 80:
Loss :  1.6099704504013062 3.430657386779785 173.14283752441406
Loss :  1.6233536005020142 3.6427414417266846 183.7604217529297
Loss :  1.5848337411880493 3.5177996158599854 177.47482299804688
Loss :  1.5943845510482788 3.548835039138794 179.03614807128906
Loss :  1.6149795055389404 3.699129343032837 186.57144165039062
Loss :  1.5687538385391235 3.4563682079315186 174.3871612548828
Loss :  1.6152585744857788 3.6000304222106934 181.61679077148438
Loss :  1.5837763547897339 3.61037540435791 182.1025390625
Loss :  1.571671962738037 3.5399703979492188 178.5701904296875
Loss :  1.6181684732437134 3.643401622772217 183.78823852539062
Loss :  1.5599608421325684 3.8822131156921387 195.67062377929688
Loss :  1.5573869943618774 3.569570779800415 180.0359344482422
Loss :  1.5554022789001465 3.503185987472534 176.71470642089844
Loss :  1.5657482147216797 3.5240204334259033 177.7667694091797
Loss :  1.6335736513137817 3.666477680206299 184.95745849609375
Loss :  1.626707673072815 3.5876402854919434 181.00872802734375
Loss :  1.5481550693511963 3.5816164016723633 180.6289825439453
Loss :  1.5832923650741577 3.3732399940490723 170.24530029296875
Loss :  1.545490026473999 3.5046164989471436 176.77630615234375
Loss :  1.6241052150726318 3.497950553894043 176.52162170410156
  batch 20 loss: 1.6241052150726318, 3.497950553894043, 176.52162170410156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5806623697280884 3.4048101902008057 171.8211669921875
Loss :  1.5426512956619263 3.4970250129699707 176.39390563964844
Loss :  1.564814567565918 3.444952964782715 173.81246948242188
Loss :  1.588639736175537 3.8692703247070312 195.05215454101562
Loss :  1.6237576007843018 3.779715061187744 190.60951232910156
Loss :  1.5675832033157349 3.2617452144622803 164.65484619140625
Loss :  1.5797414779663086 3.919508457183838 197.55516052246094
Loss :  1.574926733970642 3.591742753982544 181.1620635986328
Loss :  1.5057235956192017 3.356741428375244 169.34278869628906
Loss :  1.6196932792663574 3.410108804702759 172.12513732910156
Loss :  1.5045716762542725 3.577242612838745 180.36671447753906
Loss :  1.5972932577133179 3.428981065750122 173.0463409423828
Loss :  1.5668319463729858 3.622103691101074 182.67201232910156
Loss :  1.5653661489486694 3.4192516803741455 172.5279541015625
Loss :  1.516232967376709 3.519636631011963 177.49806213378906
Loss :  1.5377967357635498 3.3705155849456787 170.06356811523438
Loss :  1.537845253944397 3.4350855350494385 173.2921142578125
Loss :  1.6180475950241089 3.269192934036255 165.0776824951172
Loss :  1.6248716115951538 3.5382044315338135 178.53509521484375
Loss :  1.6363271474838257 3.2998931407928467 166.6309814453125
  batch 40 loss: 1.6363271474838257, 3.2998931407928467, 166.6309814453125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5838513374328613 3.4891228675842285 176.0399932861328
Loss :  1.5642820596694946 2.9944040775299072 151.28448486328125
Loss :  1.5527971982955933 3.8876945972442627 195.93753051757812
Loss :  1.567462682723999 3.581176519393921 180.62628173828125
Loss :  1.5418412685394287 4.207090854644775 211.89637756347656
Loss :  1.5803279876708984 3.315178632736206 167.33924865722656
Loss :  1.6223955154418945 3.3589208126068115 169.5684356689453
Loss :  1.562887191772461 3.4933979511260986 176.23277282714844
Loss :  1.6439896821975708 3.6102700233459473 182.15750122070312
Loss :  1.5712393522262573 3.618468761444092 182.4946746826172
Loss :  1.6151167154312134 3.4546713829040527 174.3486785888672
Loss :  1.6081197261810303 3.3926408290863037 171.2401580810547
Loss :  1.579342007637024 3.2943685054779053 166.2977752685547
Loss :  1.618391513824463 3.4258437156677246 172.91058349609375
Loss :  1.5677918195724487 3.456805467605591 174.40806579589844
Loss :  1.6479253768920898 3.450791358947754 174.18748474121094
Loss :  1.5756410360336304 3.4704294204711914 175.09710693359375
Loss :  1.558182954788208 3.3666186332702637 169.8891143798828
Loss :  1.5756607055664062 3.385540723800659 170.85269165039062
Loss :  1.6563836336135864 3.253969430923462 164.3548583984375
  batch 60 loss: 1.6563836336135864, 3.253969430923462, 164.3548583984375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.5555864572525024 3.5064022541046143 176.87570190429688
Loss :  1.5924198627471924 3.421928644180298 172.68885803222656
Loss :  1.5671967267990112 3.327064037322998 167.92039489746094
Loss :  1.5543272495269775 4.095973491668701 206.35299682617188
Loss :  1.5299352407455444 2.940627098083496 148.56129455566406
Loss :  1.5793384313583374 3.9396679401397705 198.562744140625
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5970818996429443 3.814262628555298 192.31021118164062
Loss :  1.5928484201431274 3.71288800239563 187.23724365234375
Loss :  1.597546100616455 3.7009007930755615 186.64259338378906
Total LOSS train 177.11661259577824 valid 191.1881980895996
CE LOSS train 1.5820222597855789 valid 0.39938652515411377
Contrastive LOSS train 3.5106918188241814 valid 0.9252251982688904
EPOCH 81:
Loss :  1.6145429611206055 3.536635637283325 178.4463348388672
Loss :  1.6287779808044434 3.7587857246398926 189.5680694580078
Loss :  1.5904936790466309 3.4876184463500977 175.97142028808594
Loss :  1.5986554622650146 3.3864479064941406 170.92103576660156
Loss :  1.618152141571045 3.2500295639038086 164.11962890625
Loss :  1.5738176107406616 3.4419310092926025 173.67037963867188
Loss :  1.619384765625 3.4853858947753906 175.888671875
Loss :  1.589784026145935 3.669957399368286 185.087646484375
Loss :  1.5783790349960327 3.4494636058807373 174.0515594482422
Loss :  1.6228687763214111 3.4504051208496094 174.14312744140625
Loss :  1.5668531656265259 3.4326889514923096 173.2012939453125
Loss :  1.564821720123291 3.620255947113037 182.57762145996094
Loss :  1.563725233078003 3.294238805770874 166.27566528320312
Loss :  1.5742813348770142 3.247025728225708 163.92556762695312
Loss :  1.638677954673767 3.390767812728882 171.17706298828125
Loss :  1.6327204704284668 3.6405487060546875 183.66015625
Loss :  1.557956337928772 3.356313943862915 169.3736572265625
Loss :  1.5921660661697388 3.301440954208374 166.66421508789062
Loss :  1.5562400817871094 3.3288629055023193 167.9993896484375
Loss :  1.6311196088790894 3.402247667312622 171.74349975585938
  batch 20 loss: 1.6311196088790894, 3.402247667312622, 171.74349975585938
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.5902541875839233 3.4596474170684814 174.5726318359375
Loss :  1.554682970046997 3.5715854167938232 180.1339569091797
Loss :  1.5763304233551025 3.194810628890991 161.3168487548828
Loss :  1.6005032062530518 3.2875478267669678 165.9779052734375
Loss :  1.6334956884384155 3.4058825969696045 171.92762756347656
Loss :  1.5804070234298706 3.413376569747925 172.24923706054688
Loss :  1.591672658920288 3.49601411819458 176.3923797607422
Loss :  1.5872530937194824 3.390622854232788 171.11839294433594
Loss :  1.5235216617584229 3.2663090229034424 164.83897399902344
Loss :  1.6297128200531006 3.2625272274017334 164.75607299804688
Loss :  1.5240545272827148 3.591811180114746 181.11460876464844
Loss :  1.609616994857788 3.6482808589935303 184.02366638183594
Loss :  1.582376480102539 3.4981908798217773 176.49192810058594
Loss :  1.5808124542236328 3.4733951091766357 175.2505645751953
Loss :  1.5351240634918213 3.5279762744903564 177.93394470214844
Loss :  1.554740309715271 3.523902416229248 177.74984741210938
Loss :  1.5542821884155273 3.5909857749938965 181.1035614013672
Loss :  1.6279966831207275 3.354846239089966 169.37030029296875
Loss :  1.6343570947647095 3.319385051727295 167.60360717773438
Loss :  1.64480459690094 3.45430588722229 174.360107421875
  batch 40 loss: 1.64480459690094, 3.45430588722229, 174.360107421875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.595825433731079 3.651529550552368 184.17230224609375
Loss :  1.577868103981018 3.4000937938690186 171.58255004882812
Loss :  1.5670807361602783 3.461146593093872 174.62440490722656
Loss :  1.5800584554672241 3.2158572673797607 162.3729248046875
Loss :  1.5563609600067139 3.1047585010528564 156.79429626464844
Loss :  1.5917608737945557 3.511408805847168 177.16220092773438
Loss :  1.6309691667556763 3.3684909343719482 170.05551147460938
Loss :  1.5758771896362305 3.6551451683044434 184.33314514160156
Loss :  1.6514853239059448 3.4923369884490967 176.26834106445312
Loss :  1.583534598350525 3.3068270683288574 166.92489624023438
Loss :  1.6241893768310547 3.502258777618408 176.7371368408203
Loss :  1.6175132989883423 3.1999051570892334 161.61276245117188
Loss :  1.5900423526763916 3.8691930770874023 195.04969787597656
Loss :  1.6264945268630981 3.4148201942443848 172.3675079345703
Loss :  1.5782603025436401 3.4735260009765625 175.2545623779297
Loss :  1.6537513732910156 3.3472554683685303 169.0165252685547
Loss :  1.5850611925125122 3.2882535457611084 165.99774169921875
Loss :  1.568832516670227 3.139374017715454 158.53753662109375
Loss :  1.585919976234436 3.3921306133270264 171.1924591064453
Loss :  1.6625592708587646 4.196335792541504 211.47933959960938
  batch 60 loss: 1.6625592708587646, 4.196335792541504, 211.47933959960938
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.5689210891723633 3.8010661602020264 191.6222381591797
Loss :  1.6042128801345825 3.1683907508850098 160.0237579345703
Loss :  1.5806758403778076 3.4184248447418213 172.50192260742188
Loss :  1.5682307481765747 3.4268198013305664 172.9092254638672
Loss :  1.5440541505813599 2.991110324859619 151.0995635986328
Loss :  1.582721471786499 3.5925676822662354 181.2111053466797
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.599389910697937 3.5639848709106445 179.79864501953125
Loss :  1.5952414274215698 3.5222156047821045 177.70602416992188
Loss :  1.5989843606948853 3.2793374061584473 165.56585693359375
Total LOSS train 173.4837648831881 valid 176.07040786743164
CE LOSS train 1.5922916504052969 valid 0.3997460901737213
Contrastive LOSS train 3.4378294504605806 valid 0.8198343515396118
EPOCH 82:
Loss :  1.622668981552124 3.489424228668213 176.0938720703125
Loss :  1.6356582641601562 3.487361431121826 176.00372314453125
Loss :  1.5995945930480957 3.3606760501861572 169.63339233398438
Loss :  1.6074191331863403 3.2570645809173584 164.4606475830078
Loss :  1.6252038478851318 2.9705710411071777 150.15374755859375
Loss :  1.581973910331726 3.3647706508636475 169.82049560546875
Loss :  1.6247379779815674 3.3730967044830322 170.27957153320312
Loss :  1.5963988304138184 3.1282529830932617 158.00904846191406
Loss :  1.5854350328445435 3.2464144229888916 163.90615844726562
Loss :  1.6282904148101807 3.231991767883301 163.22789001464844
Loss :  1.57522451877594 3.4492547512054443 174.0379638671875
Loss :  1.5734210014343262 3.6229798793792725 182.722412109375
Loss :  1.573288083076477 3.557509660720825 179.4487762451172
Loss :  1.5845235586166382 3.207979679107666 161.98350524902344
Loss :  1.6463702917099 3.849853754043579 194.13906860351562
Loss :  1.6410833597183228 3.2507777214050293 164.17996215820312
Loss :  1.5705887079238892 3.399759531021118 171.55856323242188
Loss :  1.602341651916504 3.1712796688079834 160.16632080078125
Loss :  1.5680756568908691 3.4086389541625977 172.00001525878906
Loss :  1.6387089490890503 3.516083240509033 177.44287109375
  batch 20 loss: 1.6387089490890503, 3.516083240509033, 177.44287109375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.5995782613754272 3.5874147415161133 180.97032165527344
Loss :  1.5651979446411133 3.244410276412964 163.7857208251953
Loss :  1.5846961736679077 3.617438554763794 182.45663452148438
Loss :  1.606955647468567 3.7013509273529053 186.67449951171875
Loss :  1.63883376121521 3.915865421295166 197.43211364746094
Loss :  1.5884268283843994 3.2184717655181885 162.5120086669922
Loss :  1.5996054410934448 3.6360924243927 183.40423583984375
Loss :  1.5954147577285767 3.4476583003997803 173.97833251953125
Loss :  1.5334441661834717 3.5080175399780273 176.934326171875
Loss :  1.6356110572814941 3.5669655799865723 179.98388671875
Loss :  1.5324846506118774 3.471282482147217 175.0966033935547
Loss :  1.6151598691940308 3.658956289291382 184.56297302246094
Loss :  1.5879156589508057 3.4586286544799805 174.51934814453125
Loss :  1.5856438875198364 3.5745649337768555 180.3138885498047
Loss :  1.5410889387130737 3.611943244934082 182.1382598876953
Loss :  1.559478521347046 3.477653741836548 175.44216918945312
Loss :  1.5586390495300293 3.6658289432525635 184.85008239746094
Loss :  1.6298279762268066 3.729970693588257 188.12835693359375
Loss :  1.6357872486114502 3.622448444366455 182.75820922851562
Loss :  1.6455559730529785 3.5751688480377197 180.40399169921875
  batch 40 loss: 1.6455559730529785, 3.5751688480377197, 180.40399169921875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5980455875396729 3.357635259628296 169.4798126220703
Loss :  1.5800567865371704 3.214555263519287 162.30783081054688
Loss :  1.569928765296936 3.904099941253662 196.77493286132812
Loss :  1.5825039148330688 3.6310317516326904 183.13409423828125
Loss :  1.558555006980896 3.3023173809051514 166.67442321777344
Loss :  1.5929583311080933 3.4950525760650635 176.3455810546875
Loss :  1.6310087442398071 3.396930694580078 171.4775390625
Loss :  1.5731689929962158 3.591620445251465 181.15419006347656
Loss :  1.648996114730835 3.7743513584136963 190.3665771484375
Loss :  1.5786335468292236 3.6331393718719482 183.235595703125
Loss :  1.6203701496124268 3.5352227687835693 178.3815155029297
Loss :  1.6135969161987305 3.4581122398376465 174.5192108154297
Loss :  1.5861701965332031 3.440868616104126 173.6295928955078
Loss :  1.6240650415420532 3.661371946334839 184.6926727294922
Loss :  1.5742582082748413 3.6383302211761475 183.4907684326172
Loss :  1.6516011953353882 3.437110662460327 173.5071258544922
Loss :  1.579524278640747 3.4351840019226074 173.33872985839844
Loss :  1.5607088804244995 3.5170671939849854 177.41407775878906
Loss :  1.5773463249206543 3.595848798751831 181.36978149414062
Loss :  1.6571787595748901 3.2252767086029053 162.9210205078125
  batch 60 loss: 1.6571787595748901, 3.2252767086029053, 162.9210205078125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.557414174079895 3.38008189201355 170.56150817871094
Loss :  1.593440294265747 3.403522491455078 171.76956176757812
Loss :  1.5675249099731445 3.144270896911621 158.78106689453125
Loss :  1.5542467832565308 3.726902723312378 187.89938354492188
Loss :  1.528362512588501 3.111722469329834 157.11448669433594
Loss :  1.5773146152496338 3.8546524047851562 194.3099365234375
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.5948196649551392 3.816589832305908 192.42431640625
Loss :  1.590441107749939 3.676898241043091 185.4353485107422
Loss :  1.5953165292739868 3.8930160999298096 196.24612426757812
Total LOSS train 175.16853919396033 valid 192.10393142700195
CE LOSS train 1.5950771845304048 valid 0.3988291323184967
Contrastive LOSS train 3.471469233586238 valid 0.9732540249824524
EPOCH 83:
Loss :  1.611964225769043 3.4326486587524414 173.24440002441406
Loss :  1.625204086303711 3.6044704914093018 181.84872436523438
Loss :  1.586800217628479 3.414813995361328 172.32749938964844
Loss :  1.5956965684890747 3.502199411392212 176.70567321777344
Loss :  1.6160571575164795 3.3925042152404785 171.24127197265625
Loss :  1.569771647453308 3.375021457672119 170.3208465576172
Loss :  1.6151431798934937 3.385084867477417 170.869384765625
Loss :  1.5833362340927124 3.2024178504943848 161.70423889160156
Loss :  1.5702794790267944 3.500011444091797 176.57086181640625
Loss :  1.6159839630126953 3.4243202209472656 172.8319854736328
Loss :  1.5588194131851196 3.534006357192993 178.25914001464844
Loss :  1.5572584867477417 3.54933500289917 179.0240020751953
Loss :  1.5571956634521484 3.5272562503814697 177.9199981689453
Loss :  1.5697135925292969 3.4428110122680664 173.71026611328125
Loss :  1.6370552778244019 3.242633581161499 163.76873779296875
Loss :  1.6318864822387695 3.4343650341033936 173.35012817382812
Loss :  1.5564067363739014 3.527554512023926 177.93414306640625
Loss :  1.5906754732131958 3.5251874923706055 177.8500518798828
Loss :  1.555266261100769 3.3385767936706543 168.48410034179688
Loss :  1.6303167343139648 3.631162405014038 183.1884307861328
  batch 20 loss: 1.6303167343139648, 3.631162405014038, 183.1884307861328
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.5904144048690796 3.676905393600464 185.43568420410156
Loss :  1.5541942119598389 3.5011744499206543 176.6129150390625
Loss :  1.5754225254058838 3.8984694480895996 196.4989013671875
Loss :  1.5988645553588867 3.1505582332611084 159.12677001953125
Loss :  1.6323602199554443 3.5156919956207275 177.41696166992188
Loss :  1.5782972574234009 3.431199789047241 173.13827514648438
Loss :  1.589112401008606 3.488037109375 175.990966796875
Loss :  1.5839664936065674 3.716646671295166 187.4163055419922
Loss :  1.519722819328308 3.3440568447113037 168.72256469726562
Loss :  1.6280772686004639 3.3739891052246094 170.32754516601562
Loss :  1.5209294557571411 3.3447630405426025 168.75909423828125
Loss :  1.6084651947021484 3.7841637134552 190.816650390625
Loss :  1.5801570415496826 3.545619249343872 178.86111450195312
Loss :  1.5782517194747925 3.35250186920166 169.20333862304688
Loss :  1.5318244695663452 3.5206079483032227 177.56222534179688
Loss :  1.551156759262085 3.552403211593628 179.17132568359375
Loss :  1.551310420036316 3.544498920440674 178.77626037597656
Loss :  1.6272547245025635 3.4330663681030273 173.28057861328125
Loss :  1.6342222690582275 3.5006024837493896 176.66433715820312
Loss :  1.6447938680648804 3.5454626083374023 178.9179229736328
  batch 40 loss: 1.6447938680648804, 3.5454626083374023, 178.9179229736328
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5950416326522827 3.5292012691497803 178.05511474609375
Loss :  1.5753976106643677 3.3650221824645996 169.82650756835938
Loss :  1.5644148588180542 3.491729736328125 176.15089416503906
Loss :  1.5780925750732422 3.777970314025879 190.4766082763672
Loss :  1.5544298887252808 3.1562044620513916 159.36465454101562
Loss :  1.5899919271469116 3.785982847213745 190.88914489746094
Loss :  1.629941463470459 3.3034372329711914 166.8018035888672
Loss :  1.5733758211135864 3.3698067665100098 170.063720703125
Loss :  1.6493422985076904 3.290703058242798 166.1844940185547
Loss :  1.5791380405426025 3.4179205894470215 172.47515869140625
Loss :  1.6206223964691162 3.5785186290740967 180.5465545654297
Loss :  1.6145060062408447 3.3434929847717285 168.78915405273438
Loss :  1.587099552154541 3.250410795211792 164.10763549804688
Loss :  1.6241722106933594 3.430894613265991 173.1688995361328
Loss :  1.5762500762939453 3.3685243129730225 170.00245666503906
Loss :  1.6527568101882935 3.362938642501831 169.7996826171875
Loss :  1.5835238695144653 3.5953805446624756 181.35255432128906
Loss :  1.56681227684021 3.450451135635376 174.08937072753906
Loss :  1.583685040473938 3.896374225616455 196.4023895263672
Loss :  1.6610395908355713 3.161492347717285 159.73565673828125
  batch 60 loss: 1.6610395908355713, 3.161492347717285, 159.73565673828125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.5652776956558228 3.5203938484191895 177.58497619628906
Loss :  1.600325107574463 3.7362961769104004 188.41513061523438
Loss :  1.576411247253418 3.3534634113311768 169.2495880126953
Loss :  1.564433217048645 3.6383512020111084 183.48199462890625
Loss :  1.540391206741333 3.1769895553588867 160.38986206054688
Loss :  1.5852079391479492 3.6331565380096436 183.24302673339844
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.602067470550537 3.6249167919158936 182.847900390625
Loss :  1.5980069637298584 3.628781318664551 183.03707885742188
Loss :  1.6021263599395752 3.629488706588745 183.07656860351562
Total LOSS train 175.09627122145432 valid 183.05114364624023
CE LOSS train 1.5895400212361261 valid 0.4005315899848938
Contrastive LOSS train 3.4701346214000997 valid 0.9073721766471863
EPOCH 84:
Loss :  1.6203967332839966 3.6991777420043945 186.57928466796875
Loss :  1.6336976289749146 3.5848608016967773 180.87673950195312
Loss :  1.5976710319519043 3.4884979724884033 176.02256774902344
Loss :  1.6060932874679565 3.8920812606811523 196.2101593017578
Loss :  1.6244271993637085 3.1117777824401855 157.2133026123047
Loss :  1.5813241004943848 3.35847806930542 169.50523376464844
Loss :  1.6247122287750244 3.3726699352264404 170.25820922851562
Loss :  1.5958774089813232 3.383136510848999 170.75270080566406
Loss :  1.5846658945083618 4.170932292938232 210.13128662109375
Loss :  1.6276254653930664 3.4204864501953125 172.65194702148438
Loss :  1.5734715461730957 3.595426082611084 181.3447723388672
Loss :  1.571602702140808 3.831912040710449 193.16720581054688
Loss :  1.5708884000778198 3.382331132888794 170.6874542236328
Loss :  1.5818979740142822 3.850595712661743 194.11167907714844
Loss :  1.6446055173873901 3.8442389965057373 193.85655212402344
Loss :  1.639925241470337 3.4596829414367676 174.6240692138672
Loss :  1.569205403327942 3.3114120960235596 167.13980102539062
Loss :  1.6017156839370728 3.2121284008026123 162.2081298828125
Loss :  1.5679124593734741 3.198328733444214 161.48435974121094
Loss :  1.6389156579971313 3.4910192489624023 176.18988037109375
  batch 20 loss: 1.6389156579971313, 3.4910192489624023, 176.18988037109375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6005408763885498 3.472219228744507 175.2115020751953
Loss :  1.5665287971496582 3.7239115238189697 187.76210021972656
Loss :  1.5872666835784912 3.2026350498199463 161.71902465820312
Loss :  1.6098778247833252 3.278172731399536 165.5185089111328
Loss :  1.6408946514129639 3.5227267742156982 177.77723693847656
Loss :  1.590122103691101 3.832202196121216 193.20022583007812
Loss :  1.6005260944366455 3.592331886291504 181.2171173095703
Loss :  1.5961744785308838 3.5256857872009277 177.88046264648438
Loss :  1.5352870225906372 3.3048362731933594 166.77711486816406
Loss :  1.6365525722503662 3.376574993133545 170.46530151367188
Loss :  1.5351582765579224 3.488884210586548 175.9793701171875
Loss :  1.6173995733261108 3.7210028171539307 187.66754150390625
Loss :  1.5906076431274414 3.4606728553771973 174.62425231933594
Loss :  1.5883976221084595 3.5003201961517334 176.60440063476562
Loss :  1.5444936752319336 3.2775654792785645 165.4227752685547
Loss :  1.5633764266967773 3.2630529403686523 164.7160186767578
Loss :  1.563525915145874 3.178248405456543 160.47593688964844
Loss :  1.6351176500320435 3.510202169418335 177.1452178955078
Loss :  1.6407994031906128 3.3596904277801514 169.6253204345703
Loss :  1.6508657932281494 3.2173168659210205 162.51670837402344
  batch 40 loss: 1.6508657932281494, 3.2173168659210205, 162.51670837402344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6044343709945679 3.614293336868286 182.319091796875
Loss :  1.5865002870559692 3.4162063598632812 172.39682006835938
Loss :  1.5768423080444336 3.1419944763183594 158.67657470703125
Loss :  1.589988350868225 3.362409830093384 169.71047973632812
Loss :  1.5669987201690674 3.029606580734253 153.04733276367188
Loss :  1.6006771326065063 3.308579921722412 167.02967834472656
Loss :  1.638045310974121 3.490288019180298 176.15245056152344
Loss :  1.5849614143371582 3.1837470531463623 160.77230834960938
Loss :  1.6569002866744995 3.1481378078460693 159.0637969970703
Loss :  1.5912349224090576 3.910252332687378 197.10385131835938
Loss :  1.6303082704544067 3.3482000827789307 169.04031372070312
Loss :  1.6240047216415405 3.3174989223480225 167.4989471435547
Loss :  1.597278118133545 3.0490622520446777 154.05038452148438
Loss :  1.6316171884536743 3.5725958347320557 180.26141357421875
Loss :  1.5859092473983765 3.5662808418273926 179.8999481201172
Loss :  1.657941222190857 3.3723652362823486 170.2761993408203
Loss :  1.5921249389648438 3.298642873764038 166.52426147460938
Loss :  1.5765150785446167 3.3088393211364746 167.0184783935547
Loss :  1.5927526950836182 3.542623996734619 178.7239532470703
Loss :  1.6660864353179932 3.3260152339935303 167.96685791015625
  batch 60 loss: 1.6660864353179932, 3.3260152339935303, 167.96685791015625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.5754157304763794 3.259359836578369 164.54339599609375
Loss :  1.6090158224105835 3.356030225753784 169.4105224609375
Loss :  1.585741400718689 3.4825589656829834 175.71368408203125
Loss :  1.5737589597702026 3.4975671768188477 176.45211791992188
Loss :  1.5505503416061401 3.178924560546875 160.4967803955078
Loss :  1.5839768648147583 4.11899995803833 207.5339813232422
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.5997898578643799 4.02429723739624 202.8146514892578
Loss :  1.5958882570266724 3.9624030590057373 199.71603393554688
Loss :  1.5994336605072021 3.9331271648406982 198.2557830810547
Total LOSS train 173.56109407865085 valid 202.0801124572754
CE LOSS train 1.5999345834438616 valid 0.39985841512680054
Contrastive LOSS train 3.4392232014582707 valid 0.9832817912101746
EPOCH 85:
Loss :  1.627077341079712 3.234614849090576 163.35781860351562
Loss :  1.6408402919769287 3.572626829147339 180.27218627929688
Loss :  1.6073516607284546 3.4415359497070312 173.68414306640625
Loss :  1.6161649227142334 3.203589677810669 161.795654296875
Loss :  1.6340575218200684 2.9842238426208496 150.8452606201172
Loss :  1.5943043231964111 3.4579532146453857 174.49195861816406
Loss :  1.6343930959701538 3.6813719272613525 185.7030029296875
Loss :  1.607614278793335 3.4512317180633545 174.16920471191406
Loss :  1.596660852432251 3.314833164215088 167.33831787109375
Loss :  1.6362611055374146 3.3463287353515625 168.95269775390625
Loss :  1.5862865447998047 3.6091370582580566 182.0431365966797
Loss :  1.5852017402648926 3.499727725982666 176.57159423828125
Loss :  1.584765076637268 3.4361352920532227 173.3915252685547
Loss :  1.5947167873382568 3.5130655765533447 177.24798583984375
Loss :  1.6516337394714355 3.4716908931732178 175.23617553710938
Loss :  1.6463559865951538 3.7482192516326904 189.0573272705078
Loss :  1.5785330533981323 3.4723289012908936 175.19497680664062
Loss :  1.6091158390045166 3.2868804931640625 165.95314025878906
Loss :  1.5771843194961548 3.3414530754089355 168.6498260498047
Loss :  1.6443918943405151 3.257770299911499 164.5329132080078
  batch 20 loss: 1.6443918943405151, 3.257770299911499, 164.5329132080078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.608500361442566 3.359755277633667 169.5962677001953
Loss :  1.5770595073699951 3.437901258468628 173.4721221923828
Loss :  1.5971723794937134 3.3885576725006104 171.02505493164062
Loss :  1.6183308362960815 3.483668565750122 175.8017578125
Loss :  1.647843360900879 3.6677322387695312 185.03445434570312
Loss :  1.600468397140503 3.276487112045288 165.42481994628906
Loss :  1.6100294589996338 3.681748628616333 185.6974639892578
Loss :  1.6059465408325195 3.262441873550415 164.72804260253906
Loss :  1.5480719804763794 3.4309487342834473 173.0955047607422
Loss :  1.643108606338501 4.322837829589844 217.78500366210938
Loss :  1.5466374158859253 3.5733485221862793 180.21405029296875
Loss :  1.6255543231964111 3.5268499851226807 177.96804809570312
Loss :  1.6005442142486572 3.377767324447632 170.48890686035156
Loss :  1.5992376804351807 3.4067347049713135 171.93597412109375
Loss :  1.5577025413513184 3.7354605197906494 188.3307342529297
Loss :  1.575683832168579 3.841339111328125 193.64263916015625
Loss :  1.5766425132751465 3.5877902507781982 180.96615600585938
Loss :  1.6442615985870361 3.3287007808685303 168.0792999267578
Loss :  1.6503043174743652 3.2775144577026367 165.52601623535156
Loss :  1.6601582765579224 3.6653778553009033 184.92904663085938
  batch 40 loss: 1.6601582765579224, 3.6653778553009033, 184.92904663085938
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.617940068244934 3.606735944747925 181.95472717285156
Loss :  1.6017489433288574 3.1738173961639404 160.29261779785156
Loss :  1.5930460691452026 3.8579723834991455 194.49166870117188
Loss :  1.6048004627227783 3.337346076965332 168.47210693359375
Loss :  1.5835673809051514 3.320829153060913 167.62503051757812
Loss :  1.6146290302276611 3.294400215148926 166.3346405029297
Loss :  1.6487116813659668 3.5280072689056396 178.049072265625
Loss :  1.5991007089614868 3.1281862258911133 158.0084228515625
Loss :  1.6648920774459839 3.667769193649292 185.0533447265625
Loss :  1.6040199995040894 3.3229784965515137 167.75294494628906
Loss :  1.6406456232070923 3.5272462368011475 178.00294494628906
Loss :  1.6344702243804932 4.2501654624938965 214.1427459716797
Loss :  1.610455870628357 3.1934776306152344 161.2843475341797
Loss :  1.642443299293518 3.2964413166046143 166.46450805664062
Loss :  1.5998802185058594 3.3018572330474854 166.6927490234375
Loss :  1.6660172939300537 3.3064842224121094 166.990234375
Loss :  1.6045299768447876 3.4668707847595215 174.94805908203125
Loss :  1.5899564027786255 3.4623465538024902 174.707275390625
Loss :  1.604350209236145 3.4907002449035645 176.13937377929688
Loss :  1.6733981370925903 3.1068055629730225 157.013671875
  batch 60 loss: 1.6733981370925903, 3.1068055629730225, 157.013671875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.5885108709335327 3.5359771251678467 178.3873748779297
Loss :  1.620238184928894 3.2737348079681396 165.30697631835938
Loss :  1.5987993478775024 3.0942487716674805 156.3112335205078
Loss :  1.5881375074386597 3.426516532897949 172.91395568847656
Loss :  1.5672818422317505 2.902886152267456 146.71157836914062
Loss :  1.6126724481582642 4.061238765716553 204.6746063232422
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.6278576850891113 3.9822962284088135 200.74266052246094
Loss :  1.6240808963775635 3.8872313499450684 195.98565673828125
Loss :  1.6280159950256348 3.738785743713379 188.5673065185547
Total LOSS train 173.78895099346454 valid 197.49255752563477
CE LOSS train 1.610888306911175 valid 0.4070039987564087
Contrastive LOSS train 3.443561264184805 valid 0.9346964359283447
EPOCH 86:
Loss :  1.6387939453125 3.24644136428833 163.9608612060547
Loss :  1.6516278982162476 3.642512559890747 183.77725219726562
Loss :  1.6194733381271362 3.408215284347534 172.03024291992188
Loss :  1.627336025238037 3.073613166809082 155.30799865722656
Loss :  1.6434876918792725 3.3484818935394287 169.0675811767578
Loss :  1.605233907699585 3.588107109069824 181.01058959960938
Loss :  1.642853021621704 3.536189317703247 178.4523162841797
Loss :  1.6164933443069458 3.0047361850738525 151.8533172607422
Loss :  1.6058111190795898 3.4583451747894287 174.52305603027344
Loss :  1.643786907196045 3.3419692516326904 168.74224853515625
Loss :  1.5949981212615967 3.7208569049835205 187.63784790039062
Loss :  1.5931251049041748 3.4185855388641357 172.52239990234375
Loss :  1.5913448333740234 3.5505003929138184 179.11636352539062
Loss :  1.6009039878845215 3.150792121887207 159.14051818847656
Loss :  1.6577582359313965 3.2065584659576416 161.98568725585938
Loss :  1.6513944864273071 3.413090705871582 172.30593872070312
Loss :  1.586613416671753 3.2694523334503174 165.05921936035156
Loss :  1.6160047054290771 3.241114854812622 163.67173767089844
Loss :  1.5851058959960938 3.1473326683044434 158.95175170898438
Loss :  1.6502540111541748 3.6461637020111084 183.95843505859375
  batch 20 loss: 1.6502540111541748, 3.6461637020111084, 183.95843505859375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6146223545074463 3.512937068939209 177.261474609375
Loss :  1.5832215547561646 3.564012289047241 179.78382873535156
Loss :  1.601762056350708 3.4112253189086914 172.16302490234375
Loss :  1.621692419052124 3.3380210399627686 168.52273559570312
Loss :  1.651077151298523 3.526420831680298 177.9721221923828
Loss :  1.6039974689483643 3.904798746109009 196.8439483642578
Loss :  1.6130900382995605 3.698288679122925 186.5275115966797
Loss :  1.6095093488693237 3.404863119125366 171.8526611328125
Loss :  1.5502259731292725 3.368677854537964 169.984130859375
Loss :  1.6461987495422363 3.807568311691284 192.0246124267578
Loss :  1.5505205392837524 3.570134162902832 180.05723571777344
Loss :  1.6284809112548828 3.5305228233337402 178.1546173095703
Loss :  1.6024760007858276 3.3839504718780518 170.8000030517578
Loss :  1.6011520624160767 3.480093479156494 175.60581970214844
Loss :  1.5611624717712402 3.6764798164367676 185.38514709472656
Loss :  1.5792316198349 3.3862924575805664 170.89385986328125
Loss :  1.5794929265975952 3.2930891513824463 166.2339630126953
Loss :  1.6468967199325562 3.4174110889434814 172.5174560546875
Loss :  1.6510065793991089 3.4315569400787354 173.22885131835938
Loss :  1.6607650518417358 3.7610924243927 189.71539306640625
  batch 40 loss: 1.6607650518417358, 3.7610924243927, 189.71539306640625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6157299280166626 3.514397144317627 177.33558654785156
Loss :  1.5991038084030151 3.6595282554626465 184.5755157470703
Loss :  1.590951681137085 3.519455909729004 177.56375122070312
Loss :  1.6056889295578003 3.398021936416626 171.50677490234375
Loss :  1.584792137145996 3.0839593410491943 155.7827606201172
Loss :  1.6171873807907104 3.2128522396087646 162.25979614257812
Loss :  1.651666283607483 3.6711089611053467 185.20712280273438
Loss :  1.6039893627166748 3.504209280014038 176.814453125
Loss :  1.6695419549942017 3.355160713195801 169.42758178710938
Loss :  1.6103346347808838 3.144240379333496 158.82235717773438
Loss :  1.6468474864959717 3.4580771923065186 174.5507049560547
Loss :  1.6407127380371094 3.6980695724487305 186.544189453125
Loss :  1.6181546449661255 3.334733724594116 168.35482788085938
Loss :  1.6497845649719238 3.677999973297119 185.54977416992188
Loss :  1.610305905342102 3.367466688156128 169.983642578125
Loss :  1.6734308004379272 3.467043161392212 175.0255889892578
Loss :  1.6151459217071533 3.776449680328369 190.4376220703125
Loss :  1.6010500192642212 3.788799524307251 191.041015625
Loss :  1.614623785018921 3.3355798721313477 168.39361572265625
Loss :  1.6804358959197998 3.8090314865112305 192.1320037841797
  batch 60 loss: 1.6804358959197998, 3.8090314865112305, 192.1320037841797
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6000021696090698 3.444316864013672 173.81585693359375
Loss :  1.6298859119415283 3.2458784580230713 163.92381286621094
Loss :  1.6084955930709839 3.184026002883911 160.80978393554688
Loss :  1.598666787147522 3.346436023712158 168.92047119140625
Loss :  1.5781121253967285 3.5687801837921143 180.01712036132812
Loss :  1.6270027160644531 4.349484443664551 219.10122680664062
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.641352653503418 4.406613349914551 221.97203063964844
Loss :  1.6379671096801758 4.308036804199219 217.03981018066406
Loss :  1.6419057846069336 4.290670394897461 216.17543029785156
Total LOSS train 174.26768446702224 valid 218.57212448120117
CE LOSS train 1.6168249607086183 valid 0.4104764461517334
Contrastive LOSS train 3.4530171944544867 valid 1.0726675987243652
EPOCH 87:
Loss :  1.6452453136444092 3.258592367172241 164.57485961914062
Loss :  1.657124638557434 3.3811516761779785 170.71470642089844
Loss :  1.626459002494812 3.5582258701324463 179.5377655029297
Loss :  1.6341254711151123 3.577932834625244 180.53076171875
Loss :  1.6500188112258911 3.453784465789795 174.33924865722656
Loss :  1.6136811971664429 3.232151746749878 163.2212677001953
Loss :  1.6500046253204346 3.4907376766204834 176.1868896484375
Loss :  1.6260961294174194 3.104325532913208 156.84237670898438
Loss :  1.6157937049865723 3.4936904907226562 176.30032348632812
Loss :  1.651236653327942 4.107310771942139 207.0167694091797
Loss :  1.6123913526535034 3.5551655292510986 179.37066650390625
Loss :  1.6101499795913696 3.4106099605560303 172.14065551757812
Loss :  1.6152571439743042 3.267272472381592 164.97886657714844
Loss :  1.6224168539047241 3.685371160507202 185.89097595214844
Loss :  1.6595765352249146 3.447732448577881 174.04620361328125
Loss :  1.6710678339004517 3.3592207431793213 169.63211059570312
Loss :  1.6091039180755615 3.7711918354034424 190.16868591308594
Loss :  1.6338446140289307 3.4173622131347656 172.501953125
Loss :  1.6030843257904053 3.861811399459839 194.69366455078125
Loss :  1.6553795337677002 3.4382731914520264 173.5690460205078
  batch 20 loss: 1.6553795337677002, 3.4382731914520264, 173.5690460205078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6325631141662598 3.1788012981414795 160.5726318359375
Loss :  1.60239839553833 3.535536766052246 178.37924194335938
Loss :  1.6173715591430664 3.1644349098205566 159.839111328125
Loss :  1.6442043781280518 3.128161907196045 158.05230712890625
Loss :  1.6681307554244995 4.04689884185791 204.01307678222656
Loss :  1.6284788846969604 3.734341859817505 188.34556579589844
Loss :  1.6415343284606934 3.6090524196624756 182.0941619873047
Loss :  1.6202119588851929 3.9492599964141846 199.0832061767578
Loss :  1.5874935388565063 3.648817300796509 184.0283660888672
Loss :  1.6484436988830566 3.495213270187378 176.4091033935547
Loss :  1.5736660957336426 3.7242255210876465 187.78494262695312
Loss :  1.6281033754348755 3.4975409507751465 176.50514221191406
Loss :  1.6208035945892334 3.219623327255249 162.6019744873047
Loss :  1.6209381818771362 3.627319574356079 182.98692321777344
Loss :  1.5795389413833618 3.887356996536255 195.9473876953125
Loss :  1.5977351665496826 3.328176736831665 168.00657653808594
Loss :  1.59589684009552 3.439152479171753 173.55352783203125
Loss :  1.645704746246338 3.3768956661224365 170.49049377441406
Loss :  1.6605561971664429 3.2653517723083496 164.9281463623047
Loss :  1.6610556840896606 3.685373544692993 185.9297332763672
  batch 40 loss: 1.6610556840896606, 3.685373544692993, 185.9297332763672
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.6266182661056519 3.4186289310455322 172.5580596923828
Loss :  1.6141377687454224 3.4322826862335205 173.228271484375
Loss :  1.598899006843567 3.6836631298065186 185.78204345703125
Loss :  1.6043177843093872 3.2923924922943115 166.2239532470703
Loss :  1.5978405475616455 3.143392324447632 158.7674560546875
Loss :  1.6238754987716675 3.4937920570373535 176.3134765625
Loss :  1.6569876670837402 3.8185031414031982 192.58213806152344
Loss :  1.605268120765686 3.385267496109009 170.86865234375
Loss :  1.6664612293243408 3.489048957824707 176.11891174316406
Loss :  1.6129393577575684 3.274097204208374 165.31781005859375
Loss :  1.6324384212493896 3.8937127590179443 196.3180694580078
Loss :  1.6474231481552124 3.5081446170806885 177.05465698242188
Loss :  1.6219041347503662 3.1529269218444824 159.26824951171875
Loss :  1.6594908237457275 3.237583637237549 163.53866577148438
Loss :  1.6052933931350708 3.3880093097686768 171.00576782226562
Loss :  1.6835142374038696 3.718851327896118 187.62608337402344
Loss :  1.625685214996338 3.469017505645752 175.07656860351562
Loss :  1.6102429628372192 3.3968963623046875 171.45506286621094
Loss :  1.6313645839691162 3.5347087383270264 178.36680603027344
Loss :  1.6836038827896118 3.325915575027466 167.97938537597656
  batch 60 loss: 1.6836038827896118, 3.325915575027466, 167.97938537597656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.609278917312622 3.661891222000122 184.70384216308594
Loss :  1.6233807802200317 3.235546112060547 163.40069580078125
Loss :  1.6149686574935913 3.2155792713165283 162.39393615722656
Loss :  1.5975643396377563 3.4930765628814697 176.2513885498047
Loss :  1.585107445716858 3.017153739929199 152.4427947998047
Loss :  1.6128441095352173 4.152637958526611 209.24473571777344
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.626273274421692 4.119429111480713 207.59771728515625
Loss :  1.6188983917236328 3.9480695724487305 199.02236938476562
Loss :  1.6285480260849 3.9589545726776123 199.57627868652344
Total LOSS train 175.54541790301982 valid 203.8602752685547
CE LOSS train 1.6273152809876663 valid 0.407137006521225
Contrastive LOSS train 3.4783620247474083 valid 0.9897386431694031
EPOCH 88:
Loss :  1.6492520570755005 3.381775140762329 170.73800659179688
Loss :  1.6509239673614502 3.492053985595703 176.2536163330078
Loss :  1.6336003541946411 3.3305838108062744 168.1627960205078
Loss :  1.639482855796814 3.2738561630249023 165.33229064941406
Loss :  1.6509337425231934 3.495591402053833 176.43051147460938
Loss :  1.6252152919769287 3.5089375972747803 177.0720977783203
Loss :  1.6466070413589478 4.000362396240234 201.66473388671875
Loss :  1.6219178438186646 3.7435495853424072 188.7993927001953
Loss :  1.6089251041412354 3.48370099067688 175.79397583007812
Loss :  1.646869421005249 3.2674992084503174 165.02182006835938
Loss :  1.615235686302185 3.6592116355895996 184.57582092285156
Loss :  1.6104711294174194 3.6184723377227783 182.53408813476562
Loss :  1.618106484413147 3.4198949337005615 172.61285400390625
Loss :  1.6227376461029053 3.3919031620025635 171.2178955078125
Loss :  1.6538076400756836 3.632598638534546 183.2837371826172
Loss :  1.6735341548919678 3.366631031036377 170.00509643554688
Loss :  1.6088556051254272 3.0305395126342773 153.13583374023438
Loss :  1.6351104974746704 3.432750701904297 173.2726593017578
Loss :  1.6033812761306763 3.4682538509368896 175.0160675048828
Loss :  1.6544114351272583 3.468437910079956 175.07630920410156
  batch 20 loss: 1.6544114351272583, 3.468437910079956, 175.07630920410156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.632487416267395 3.4868643283843994 175.9757080078125
Loss :  1.601449728012085 3.6130177974700928 182.25234985351562
Loss :  1.6157993078231812 3.4758684635162354 175.4092254638672
Loss :  1.6427556276321411 3.8000097274780273 191.64324951171875
Loss :  1.6660679578781128 3.5433201789855957 178.8320770263672
Loss :  1.621923565864563 3.454991579055786 174.37149047851562
Loss :  1.6368072032928467 3.269000291824341 165.08682250976562
Loss :  1.6184359788894653 3.6550188064575195 184.369384765625
Loss :  1.5838488340377808 3.455970048904419 174.38235473632812
Loss :  1.650054693222046 3.8834919929504395 195.8246612548828
Loss :  1.5731669664382935 3.6975061893463135 186.44847106933594
Loss :  1.6316626071929932 3.462103843688965 174.7368621826172
Loss :  1.6214888095855713 3.5126380920410156 177.25338745117188
Loss :  1.6210155487060547 3.7530364990234375 189.27284240722656
Loss :  1.5796014070510864 3.7247228622436523 187.8157501220703
Loss :  1.5978572368621826 3.7447760105133057 188.83665466308594
Loss :  1.5964399576187134 3.6144585609436035 182.31936645507812
Loss :  1.6488783359527588 3.468015432357788 175.04965209960938
Loss :  1.6623262166976929 3.52457332611084 177.8909912109375
Loss :  1.66431725025177 4.00839376449585 202.08401489257812
  batch 40 loss: 1.66431725025177, 4.00839376449585, 202.08401489257812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6290005445480347 3.4322760105133057 173.2427978515625
Loss :  1.616378664970398 3.5682194232940674 180.02734375
Loss :  1.603257179260254 3.6004743576049805 181.62696838378906
Loss :  1.61032235622406 3.497513771057129 176.4860076904297
Loss :  1.600834846496582 3.148803949356079 159.04103088378906
Loss :  1.6275408267974854 3.673513889312744 185.30323791503906
Loss :  1.659563660621643 3.8095791339874268 192.13851928710938
Loss :  1.6098475456237793 3.552865505218506 179.2531280517578
Loss :  1.669236660003662 3.4344916343688965 173.39381408691406
Loss :  1.6176635026931763 3.5288338661193848 178.05935668945312
Loss :  1.6395280361175537 3.630446434020996 183.16184997558594
Loss :  1.649834156036377 3.4812498092651367 175.7123260498047
Loss :  1.625412106513977 3.447662830352783 174.00856018066406
Loss :  1.6611086130142212 3.4794397354125977 175.63308715820312
Loss :  1.6110625267028809 3.248255491256714 164.0238494873047
Loss :  1.68401038646698 3.1192476749420166 157.64639282226562
Loss :  1.6268998384475708 3.546461343765259 178.94998168945312
Loss :  1.610933542251587 3.530822515487671 178.1520538330078
Loss :  1.62987220287323 3.535841703414917 178.4219512939453
Loss :  1.6849617958068848 3.4121153354644775 172.29074096679688
  batch 60 loss: 1.6849617958068848, 3.4121153354644775, 172.29074096679688
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6108615398406982 3.5434176921844482 178.78173828125
Loss :  1.6288655996322632 3.3053300380706787 166.89535522460938
Loss :  1.6173819303512573 3.728916645050049 188.06321716308594
Loss :  1.6015037298202515 3.4083821773529053 172.02061462402344
Loss :  1.5870857238769531 3.5446481704711914 178.81948852539062
Loss :  1.6203584671020508 4.011674404144287 202.20408630371094
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.634070873260498 4.04459285736084 203.86370849609375
Loss :  1.6276439428329468 3.981060266494751 200.68064880371094
Loss :  1.6362870931625366 3.871070623397827 195.1898193359375
Total LOSS train 177.64628201998198 valid 200.48456573486328
CE LOSS train 1.6284420215166533 valid 0.40907177329063416
Contrastive LOSS train 3.5203567835000844 valid 0.9677676558494568
EPOCH 89:
Loss :  1.6510567665100098 2.9849600791931152 150.89906311035156
Loss :  1.6556776762008667 3.623291254043579 182.8202362060547
Loss :  1.6353483200073242 3.3913514614105225 171.20291137695312
Loss :  1.6412711143493652 3.324563503265381 167.86944580078125
Loss :  1.654179573059082 3.2201969623565674 162.6640167236328
Loss :  1.6265733242034912 3.53377628326416 178.3153839111328
Loss :  1.650565505027771 3.1283674240112305 158.0689239501953
Loss :  1.6265884637832642 3.6286110877990723 183.05714416503906
Loss :  1.615767002105713 3.5315048694610596 178.19100952148438
Loss :  1.6524772644042969 3.4381473064422607 173.55984497070312
Loss :  1.6177870035171509 3.5995380878448486 181.5946807861328
Loss :  1.612681269645691 3.88354229927063 195.789794921875
Loss :  1.6179736852645874 3.4148457050323486 172.36026000976562
Loss :  1.6231948137283325 3.277630567550659 165.50473022460938
Loss :  1.658509373664856 3.330950975418091 168.2060546875
Loss :  1.673003077507019 3.4744873046875 175.39736938476562
Loss :  1.609493613243103 3.4627878665924072 174.74888610839844
Loss :  1.6356829404830933 3.352980852127075 169.28472900390625
Loss :  1.6071025133132935 3.522728681564331 177.7435302734375
Loss :  1.6596578359603882 3.6120598316192627 182.2626495361328
  batch 20 loss: 1.6596578359603882, 3.6120598316192627, 182.2626495361328
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6369562149047852 3.4152653217315674 172.4002227783203
Loss :  1.6085587739944458 3.9143850803375244 197.32781982421875
Loss :  1.6232973337173462 3.9043502807617188 196.84080505371094
Loss :  1.6486424207687378 3.2543349266052246 164.36538696289062
Loss :  1.6728426218032837 3.517465114593506 177.5460968017578
Loss :  1.6321943998336792 3.5531163215637207 179.2880096435547
Loss :  1.645031213760376 3.809370517730713 192.11355590820312
Loss :  1.6299777030944824 2.9951109886169434 151.38552856445312
Loss :  1.5935393571853638 3.7993698120117188 191.56202697753906
Loss :  1.659157633781433 3.3187448978424072 167.59640502929688
Loss :  1.5856877565383911 3.826625108718872 192.9169464111328
Loss :  1.643080711364746 3.6384072303771973 183.56344604492188
Loss :  1.6320785284042358 3.43624210357666 173.44418334960938
Loss :  1.6315019130706787 3.280125856399536 165.63778686523438
Loss :  1.5943236351013184 3.431457281112671 173.1671905517578
Loss :  1.6118274927139282 3.982398509979248 200.73175048828125
Loss :  1.6110378503799438 3.518691062927246 177.54559326171875
Loss :  1.660826563835144 3.5671544075012207 180.0185546875
Loss :  1.6711071729660034 3.688854217529297 186.11383056640625
Loss :  1.6735565662384033 3.705716371536255 186.95936584472656
  batch 40 loss: 1.6735565662384033, 3.705716371536255, 186.95936584472656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.638954758644104 3.5704498291015625 180.1614532470703
Loss :  1.6252073049545288 3.4970741271972656 176.47891235351562
Loss :  1.612276315689087 3.5574562549591064 179.48509216308594
Loss :  1.6197645664215088 3.051734685897827 154.2064971923828
Loss :  1.6095441579818726 3.370471954345703 170.1331329345703
Loss :  1.6353172063827515 3.442636251449585 173.76712036132812
Loss :  1.665942668914795 3.4479310512542725 174.06248474121094
Loss :  1.6192963123321533 4.223478317260742 212.793212890625
Loss :  1.6770325899124146 3.5297999382019043 178.16702270507812
Loss :  1.6253206729888916 3.6526706218719482 184.25885009765625
Loss :  1.6477601528167725 3.780540943145752 190.67481994628906
Loss :  1.6540641784667969 3.5511178970336914 179.2099609375
Loss :  1.6302584409713745 3.9805705547332764 200.65879821777344
Loss :  1.663859486579895 3.666257619857788 184.97674560546875
Loss :  1.6141117811203003 3.5843989849090576 180.8340606689453
Loss :  1.6842362880706787 3.619274139404297 182.64794921875
Loss :  1.6246438026428223 3.5201361179351807 177.63145446777344
Loss :  1.6084238290786743 3.2781896591186523 165.51791381835938
Loss :  1.626505970954895 3.742375373840332 188.7452850341797
Loss :  1.683675765991211 3.6144492626190186 182.4061279296875
  batch 60 loss: 1.683675765991211, 3.6144492626190186, 182.4061279296875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6055079698562622 3.368844747543335 170.04774475097656
Loss :  1.6245557069778442 3.396807909011841 171.46495056152344
Loss :  1.6108825206756592 3.219785213470459 162.6001434326172
Loss :  1.5954020023345947 3.4976818561553955 176.4794921875
Loss :  1.579436182975769 3.611814260482788 182.17015075683594
Loss :  1.6106078624725342 3.5630452632904053 179.76287841796875
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.6251524686813354 3.584381580352783 180.84423828125
Loss :  1.6197376251220703 3.3951046466827393 171.37496948242188
Loss :  1.6274670362472534 3.552783489227295 179.2666473388672
Total LOSS train 177.84068533090445 valid 177.81218338012695
CE LOSS train 1.6334122712795551 valid 0.40686675906181335
Contrastive LOSS train 3.5241454674647406 valid 0.8881958723068237
EPOCH 90:
Loss :  1.6461197137832642 3.517014265060425 177.496826171875
Loss :  1.6508557796478271 3.379075527191162 170.60462951660156
Loss :  1.6267971992492676 2.9970245361328125 151.47802734375
Loss :  1.632520079612732 3.4230573177337646 172.78538513183594
Loss :  1.6473416090011597 3.1598503589630127 159.63986206054688
Loss :  1.6170899868011475 3.5145153999328613 177.34286499023438
Loss :  1.6446583271026611 3.556718111038208 179.48056030273438
Loss :  1.6196633577346802 3.322600841522217 167.74969482421875
Loss :  1.6068811416625977 3.4878087043762207 175.99732971191406
Loss :  1.6455318927764893 3.4002349376678467 171.65728759765625
Loss :  1.607107162475586 3.6512086391448975 184.1675262451172
Loss :  1.6025251150131226 3.5459656715393066 178.9008026123047
Loss :  1.6078460216522217 3.6449503898620605 183.85536193847656
Loss :  1.6140916347503662 3.345078468322754 168.86801147460938
Loss :  1.6545400619506836 3.7075231075286865 187.03070068359375
Loss :  1.6658416986465454 3.807772636413574 192.05447387695312
Loss :  1.5988692045211792 3.7207086086273193 187.63429260253906
Loss :  1.6262671947479248 3.3727853298187256 170.26553344726562
Loss :  1.5942022800445557 3.3942205905914307 171.30523681640625
Loss :  1.6518876552581787 3.4489283561706543 174.09829711914062
  batch 20 loss: 1.6518876552581787, 3.4489283561706543, 174.09829711914062
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.6267924308776855 3.392212390899658 171.23741149902344
Loss :  1.596920371055603 3.581834316253662 180.6886444091797
Loss :  1.6140844821929932 3.7375540733337402 188.49179077148438
Loss :  1.6397972106933594 3.525057792663574 177.89268493652344
Loss :  1.6648224592208862 3.783888578414917 190.8592529296875
Loss :  1.619737148284912 3.0171399116516113 152.4767303466797
Loss :  1.6330708265304565 3.611280679702759 182.19711303710938
Loss :  1.6194061040878296 3.7857463359832764 190.90672302246094
Loss :  1.5761882066726685 3.4285202026367188 173.002197265625
Loss :  1.6514942646026611 3.6018710136413574 181.7450408935547
Loss :  1.567637324333191 3.533363103866577 178.2357940673828
Loss :  1.6323972940444946 3.6940019130706787 186.33248901367188
Loss :  1.6164581775665283 3.632645845413208 183.24874877929688
Loss :  1.6148797273635864 3.7136738300323486 187.29856872558594
Loss :  1.572933554649353 3.3463492393493652 168.89039611816406
Loss :  1.5912786722183228 3.742750644683838 188.7288055419922
Loss :  1.589387059211731 3.5856454372406006 180.8716583251953
Loss :  1.6480649709701538 3.4493138790130615 174.11376953125
Loss :  1.6587872505187988 3.4067277908325195 171.99517822265625
Loss :  1.6644251346588135 3.394017219543457 171.36529541015625
  batch 40 loss: 1.6644251346588135, 3.394017219543457, 171.36529541015625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6258916854858398 3.6485142707824707 184.05160522460938
Loss :  1.6114095449447632 3.4421606063842773 173.7194366455078
Loss :  1.5980720520019531 3.4238433837890625 172.7902374267578
Loss :  1.607503056526184 3.450939178466797 174.1544647216797
Loss :  1.5919727087020874 3.42889404296875 173.03668212890625
Loss :  1.6219087839126587 3.4669954776763916 174.9716796875
Loss :  1.655046820640564 3.7268805503845215 187.9990692138672
Loss :  1.6020303964614868 3.490687370300293 176.1363983154297
Loss :  1.6687699556350708 3.2227020263671875 162.8038787841797
Loss :  1.6095439195632935 3.142634868621826 158.7412872314453
Loss :  1.6404907703399658 3.4286231994628906 173.07164001464844
Loss :  1.642797589302063 3.525613307952881 177.9234619140625
Loss :  1.6168228387832642 3.4237704277038574 172.8053436279297
Loss :  1.6526790857315063 3.444892644882202 173.89730834960938
Loss :  1.6026901006698608 3.570970058441162 180.1511993408203
Loss :  1.6779099702835083 3.5036420822143555 176.86001586914062
Loss :  1.615850806236267 3.5284576416015625 178.03872680664062
Loss :  1.5993081331253052 3.4191360473632812 172.5561065673828
Loss :  1.6176477670669556 3.2725906372070312 165.24717712402344
Loss :  1.6812717914581299 3.3224635124206543 167.804443359375
  batch 60 loss: 1.6812717914581299, 3.3224635124206543, 167.804443359375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.5979242324829102 3.318234443664551 167.50965881347656
Loss :  1.6216697692871094 3.6477246284484863 184.00790405273438
Loss :  1.6053279638290405 3.3552651405334473 169.36859130859375
Loss :  1.59049391746521 3.5742645263671875 180.30372619628906
Loss :  1.5709308385849 2.8392422199249268 143.53305053710938
Loss :  1.6078296899795532 4.115618705749512 207.38876342773438
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.6230738162994385 4.151801109313965 209.213134765625
Loss :  1.6179120540618896 4.075983047485352 205.41705322265625
Loss :  1.625610113143921 3.995234727859497 201.38734436035156
Total LOSS train 175.4534475473257 valid 205.8515739440918
CE LOSS train 1.622848681303171 valid 0.4064025282859802
Contrastive LOSS train 3.4766119736891525 valid 0.9988086819648743
EPOCH 91:
Loss :  1.6414823532104492 3.340292453765869 168.65609741210938
Loss :  1.6497032642364502 3.4661483764648438 174.95712280273438
Loss :  1.6230950355529785 3.3038363456726074 166.81491088867188
Loss :  1.6297141313552856 3.215895652770996 162.42449951171875
Loss :  1.645656704902649 3.38753604888916 171.0224609375
Loss :  1.6103882789611816 3.4024038314819336 171.73057556152344
Loss :  1.6433629989624023 3.4694836139678955 175.11753845214844
Loss :  1.6166752576828003 3.1863937377929688 160.9363555908203
Loss :  1.6039639711380005 3.359934091567993 169.6006622314453
Loss :  1.644404411315918 3.269251585006714 165.10699462890625
Loss :  1.6012554168701172 3.581528663635254 180.6776885986328
Loss :  1.5974682569503784 3.603748321533203 181.78488159179688
Loss :  1.600912094116211 3.4688453674316406 175.0431671142578
Loss :  1.608124852180481 3.8371548652648926 193.4658660888672
Loss :  1.6572006940841675 3.819284200668335 192.62139892578125
Loss :  1.6634455919265747 3.4583938121795654 174.58314514160156
Loss :  1.5954532623291016 3.347050905227661 168.947998046875
Loss :  1.624944806098938 3.5646934509277344 179.859619140625
Loss :  1.5920994281768799 3.489595651626587 176.07188415527344
Loss :  1.6533159017562866 3.2915899753570557 166.23281860351562
  batch 20 loss: 1.6533159017562866, 3.2915899753570557, 166.23281860351562
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6219223737716675 4.114441394805908 207.343994140625
Loss :  1.5908076763153076 3.293922185897827 166.2869110107422
Loss :  1.6088824272155762 3.3556759357452393 169.39268493652344
Loss :  1.6312251091003418 3.294910430908203 166.37673950195312
Loss :  1.659388780593872 3.4002654552459717 171.67266845703125
Loss :  1.6116427183151245 3.1770222187042236 160.46275329589844
Loss :  1.6241223812103271 3.746737241744995 188.9609832763672
Loss :  1.612282156944275 3.4829506874084473 175.75982666015625
Loss :  1.5620497465133667 3.343669891357422 168.74554443359375
Loss :  1.646357536315918 3.7995002269744873 191.62136840820312
Loss :  1.557428002357483 3.6017379760742188 181.6443328857422
Loss :  1.6347705125808716 4.646420001983643 233.9557647705078
Loss :  1.6106566190719604 3.321636915206909 167.6925048828125
Loss :  1.6073198318481445 3.6921935081481934 186.2169952392578
Loss :  1.5666090250015259 4.070009231567383 205.0670623779297
Loss :  1.5787897109985352 3.903668165206909 196.76220703125
Loss :  1.5817759037017822 3.8669097423553467 194.92726135253906
Loss :  1.645652413368225 3.8859403133392334 195.94265747070312
Loss :  1.652531385421753 4.145395278930664 208.9222869873047
Loss :  1.6588493585586548 4.004659652709961 201.89183044433594
  batch 40 loss: 1.6588493585586548, 4.004659652709961, 201.89183044433594
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.6109143495559692 3.99672269821167 201.44705200195312
Loss :  1.5919930934906006 3.7038304805755615 186.7835235595703
Loss :  1.576518177986145 3.6823034286499023 185.6916961669922
Loss :  1.5879026651382446 3.6023900508880615 181.70741271972656
Loss :  1.559100866317749 3.8805646896362305 195.5873260498047
Loss :  1.5943578481674194 3.7557373046875 189.3812255859375
Loss :  1.6347061395645142 3.7281060218811035 188.04000854492188
Loss :  1.5748631954193115 3.9565846920013428 199.4040985107422
Loss :  1.660577654838562 3.6649527549743652 184.90821838378906
Loss :  1.5775238275527954 3.7862908840179443 190.89207458496094
Loss :  1.6295756101608276 3.791562080383301 191.2076873779297
Loss :  1.6072380542755127 3.849245309829712 194.0695037841797
Loss :  1.5752407312393188 3.5044400691986084 176.7972412109375
Loss :  1.6131465435028076 4.132347106933594 208.23049926757812
Loss :  1.5635504722595215 4.262814521789551 214.70428466796875
Loss :  1.6410014629364014 4.088335037231445 206.05775451660156
Loss :  1.5540903806686401 3.8003108501434326 191.56964111328125
Loss :  1.5302646160125732 3.73831844329834 188.44618225097656
Loss :  1.542442798614502 3.9069933891296387 196.89212036132812
Loss :  1.6513878107070923 3.6372735500335693 183.5150604248047
  batch 60 loss: 1.6513878107070923, 3.6372735500335693, 183.5150604248047
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.522282600402832 3.583651304244995 180.70484924316406
Loss :  1.5801833868026733 3.577083110809326 180.43434143066406
Loss :  1.5344114303588867 3.515528440475464 177.3108367919922
Loss :  1.5231188535690308 3.564016819000244 179.7239532470703
Loss :  1.4823323488235474 3.2993059158325195 166.4476318359375
Loss :  1.5584081411361694 4.230924606323242 213.10464477539062
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.581289529800415 4.196046352386475 211.38360595703125
Loss :  1.5785590410232544 4.050042629241943 204.0806884765625
Loss :  1.5836154222488403 4.1779866218566895 210.4829559326172
Total LOSS train 183.92701979417066 valid 209.7629737854004
CE LOSS train 1.6043762353750375 valid 0.3959038555622101
Contrastive LOSS train 3.646452867067777 valid 1.0444966554641724
EPOCH 92:
Loss :  1.5900065898895264 3.6765928268432617 185.41964721679688
Loss :  1.6189415454864502 3.6654417514801025 184.8910369873047
Loss :  1.5554466247558594 3.4848902225494385 175.79995727539062
Loss :  1.5652176141738892 3.3663442134857178 169.88243103027344
Loss :  1.5969306230545044 3.489320993423462 176.06297302246094
Loss :  1.5264428853988647 3.3197546005249023 167.51417541503906
Loss :  1.5997674465179443 3.5475783348083496 178.9786834716797
Loss :  1.5602000951766968 3.4128525257110596 172.20281982421875
Loss :  1.5475581884384155 3.574965715408325 180.2958526611328
Loss :  1.602705478668213 3.4746248722076416 175.33395385742188
Loss :  1.5186740159988403 3.9490807056427 198.97271728515625
Loss :  1.5231554508209229 3.8282666206359863 192.93649291992188
Loss :  1.5110244750976562 3.5493624210357666 178.97915649414062
Loss :  1.5267888307571411 3.580573081970215 180.55545043945312
Loss :  1.6321327686309814 3.4527924060821533 174.27174377441406
Loss :  1.6030501127243042 3.74354887008667 188.78048706054688
Loss :  1.508472204208374 3.6004247665405273 181.5297088623047
Loss :  1.5578035116195679 3.4627842903137207 174.697021484375
Loss :  1.511063575744629 3.426722764968872 172.84719848632812
Loss :  1.6139341592788696 3.6316604614257812 183.19696044921875
  batch 20 loss: 1.6139341592788696, 3.6316604614257812, 183.19696044921875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5503528118133545 3.183830738067627 160.7418975830078
Loss :  1.507621169090271 3.55484676361084 179.2499542236328
Loss :  1.5394736528396606 3.4229013919830322 172.68453979492188
Loss :  1.564801573753357 3.6043832302093506 181.78396606445312
Loss :  1.6115226745605469 3.871861696243286 195.20460510253906
Loss :  1.545498251914978 3.5656607151031494 179.8285369873047
Loss :  1.5572539567947388 3.872594118118286 195.18695068359375
Loss :  1.5617140531539917 3.566190004348755 179.87120056152344
Loss :  1.463744878768921 3.4308409690856934 173.00579833984375
Loss :  1.6152727603912354 3.2627384662628174 164.752197265625
Loss :  1.4755464792251587 3.7245490550994873 187.70298767089844
Loss :  1.5978277921676636 3.7661774158477783 189.9066925048828
Loss :  1.547364354133606 3.4857661724090576 175.83567810058594
Loss :  1.5446221828460693 3.5390145778656006 178.49534606933594
Loss :  1.492102026939392 3.55525541305542 179.25486755371094
Loss :  1.5157215595245361 3.5784928798675537 180.4403533935547
Loss :  1.5168925523757935 3.4409470558166504 173.56423950195312
Loss :  1.6201050281524658 3.4986867904663086 176.554443359375
Loss :  1.6203383207321167 3.3051130771636963 166.87599182128906
Loss :  1.6395624876022339 3.366833448410034 169.98123168945312
  batch 40 loss: 1.6395624876022339, 3.366833448410034, 169.98123168945312
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5740578174591064 3.5545547008514404 179.30178833007812
Loss :  1.5534160137176514 3.7357897758483887 188.34291076660156
Loss :  1.5455551147460938 3.5114712715148926 177.11911010742188
Loss :  1.5674784183502197 3.421379804611206 172.63645935058594
Loss :  1.5297846794128418 3.5477490425109863 178.917236328125
Loss :  1.5758020877838135 3.7517480850219727 189.1632080078125
Loss :  1.6192091703414917 4.016683578491211 202.45338439941406
Loss :  1.564113736152649 3.8239095211029053 192.7595977783203
Loss :  1.6497588157653809 3.113041639328003 157.30184936523438
Loss :  1.5671764612197876 3.309643268585205 167.04933166503906
Loss :  1.6250351667404175 3.2312231063842773 163.18618774414062
Loss :  1.6017271280288696 3.625863552093506 182.89491271972656
Loss :  1.5715335607528687 3.4710159301757812 175.12232971191406
Loss :  1.6112749576568604 3.5825581550598145 180.73919677734375
Loss :  1.5576754808425903 4.0021491050720215 201.66513061523438
Loss :  1.641569972038269 3.1619374752044678 159.7384490966797
Loss :  1.561462163925171 3.426753282546997 172.8991241455078
Loss :  1.5384713411331177 3.8490548133850098 193.9912109375
Loss :  1.5571138858795166 3.714818000793457 187.2980194091797
Loss :  1.6547902822494507 3.5253114700317383 177.9203643798828
  batch 60 loss: 1.6547902822494507, 3.5253114700317383, 177.9203643798828
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5389182567596436 3.618072748184204 182.44256591796875
Loss :  1.5808888673782349 4.107474327087402 206.95460510253906
Loss :  1.55312979221344 3.2861506938934326 165.8606719970703
Loss :  1.531434178352356 3.7463295459747314 188.84791564941406
Loss :  1.501463532447815 3.4754903316497803 175.27598571777344
Loss :  1.5567841529846191 3.969553232192993 200.03443908691406
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5728622674942017 3.749636650085449 189.0546875
Loss :  1.5612984895706177 3.7044994831085205 186.78627014160156
Loss :  1.5824713706970215 3.754962682723999 189.3306121826172
Total LOSS train 179.59925372783954 valid 191.3015022277832
CE LOSS train 1.5650691637626062 valid 0.39561784267425537
Contrastive LOSS train 3.560683686916645 valid 0.9387406706809998
EPOCH 93:
Loss :  1.6092827320098877 3.1491339206695557 159.06597900390625
Loss :  1.6245489120483398 3.699751853942871 186.6121368408203
Loss :  1.5757815837860107 3.5147745609283447 177.3144989013672
Loss :  1.5849158763885498 3.9443349838256836 198.80166625976562
Loss :  1.6155494451522827 3.557117223739624 179.47142028808594
Loss :  1.566709041595459 4.122638702392578 207.69863891601562
Loss :  1.6204564571380615 3.5653724670410156 179.88906860351562
Loss :  1.5645509958267212 3.5302302837371826 178.07606506347656
Loss :  1.5686891078948975 3.7316415309906006 188.15077209472656
Loss :  1.607837438583374 3.4818246364593506 175.69906616210938
Loss :  1.5554786920547485 4.070505619049072 205.0807647705078
Loss :  1.5451040267944336 3.9067487716674805 196.88253784179688
Loss :  1.5469319820404053 4.011854648590088 202.13966369628906
Loss :  1.5465023517608643 3.8407464027404785 193.58383178710938
Loss :  1.624307632446289 3.8667945861816406 194.9640350341797
Loss :  1.6145539283752441 3.482821464538574 175.755615234375
Loss :  1.5230319499969482 3.3206441402435303 167.55523681640625
Loss :  1.5591819286346436 3.7211577892303467 187.61708068847656
Loss :  1.5153040885925293 3.687120199203491 185.87130737304688
Loss :  1.6057274341583252 3.418879508972168 172.54969787597656
  batch 20 loss: 1.6057274341583252, 3.418879508972168, 172.54969787597656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.551728367805481 3.4500393867492676 174.05369567871094
Loss :  1.5056078433990479 4.088321208953857 205.9216766357422
Loss :  1.5308393239974976 3.4464592933654785 173.8538055419922
Loss :  1.5627456903457642 3.5517468452453613 179.15008544921875
Loss :  1.6062300205230713 3.8172945976257324 192.47096252441406
Loss :  1.53493070602417 3.3434269428253174 168.70626831054688
Loss :  1.5526955127716064 3.508563280105591 176.98085021972656
Loss :  1.5397164821624756 3.216306447982788 162.35504150390625
Loss :  1.4602550268173218 3.070417881011963 154.98114013671875
Loss :  1.5949764251708984 3.790135383605957 191.10174560546875
Loss :  1.4560918807983398 3.8783113956451416 195.3716583251953
Loss :  1.5665093660354614 3.5112977027893066 177.1313934326172
Loss :  1.5330758094787598 3.896864414215088 196.3762969970703
Loss :  1.5302904844284058 3.5120558738708496 177.13308715820312
Loss :  1.466025471687317 3.4481825828552246 173.87515258789062
Loss :  1.4925686120986938 3.7034192085266113 186.6635284423828
Loss :  1.4900623559951782 3.3769185543060303 170.33599853515625
Loss :  1.5882599353790283 3.528817892074585 178.02914428710938
Loss :  1.6007026433944702 3.3810384273529053 170.6526336669922
Loss :  1.6121248006820679 3.1883580684661865 161.030029296875
  batch 40 loss: 1.6121248006820679, 3.1883580684661865, 161.030029296875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.544881820678711 3.381908893585205 170.64031982421875
Loss :  1.5211189985275269 3.2329280376434326 163.1675262451172
Loss :  1.5019909143447876 3.5187318325042725 177.43856811523438
Loss :  1.52090585231781 3.482550621032715 175.6484375
Loss :  1.4898080825805664 3.27864670753479 165.42214965820312
Loss :  1.5391368865966797 3.5187056064605713 177.47442626953125
Loss :  1.5931665897369385 3.3796615600585938 170.5762481689453
Loss :  1.5137797594070435 3.5688581466674805 179.95668029785156
Loss :  1.6187400817871094 3.5053060054779053 176.88404846191406
Loss :  1.5252052545547485 3.403932809829712 171.7218475341797
Loss :  1.5773847103118896 3.8497531414031982 194.06503295898438
Loss :  1.5738102197647095 3.206075429916382 161.87757873535156
Loss :  1.5362342596054077 3.1694982051849365 160.0111541748047
Loss :  1.5886274576187134 3.413841962814331 172.2807159423828
Loss :  1.518304467201233 3.456773519515991 174.3569793701172
Loss :  1.627724289894104 3.44667911529541 173.96168518066406
Loss :  1.5343835353851318 3.6886143684387207 185.96510314941406
Loss :  1.509032130241394 3.59222149848938 181.12010192871094
Loss :  1.5362557172775269 3.5822155475616455 180.64703369140625
Loss :  1.6381465196609497 3.584242820739746 180.8502960205078
  batch 60 loss: 1.6381465196609497, 3.584242820739746, 180.8502960205078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.513486385345459 3.4538323879241943 174.20510864257812
Loss :  1.5571292638778687 3.7883994579315186 190.9770965576172
Loss :  1.5278618335723877 3.490924596786499 176.0740966796875
Loss :  1.5114773511886597 3.442246437072754 173.62379455566406
Loss :  1.4825243949890137 3.9196650981903076 197.46578979492188
Loss :  1.5324833393096924 4.057316780090332 204.39833068847656
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5539096593856812 3.9234018325805664 197.7239990234375
Loss :  1.54904305934906 4.220602512359619 212.57916259765625
Loss :  1.542813777923584 4.037176609039307 203.40164184570312
Total LOSS train 179.77484764685997 valid 204.52578353881836
CE LOSS train 1.5530922944729144 valid 0.385703444480896
Contrastive LOSS train 3.5644351152273326 valid 1.0092941522598267
EPOCH 94:
Loss :  1.5883876085281372 3.8272063732147217 192.94871520996094
Loss :  1.6077780723571777 3.8404009342193604 193.62782287597656
Loss :  1.5622751712799072 3.141448497772217 158.6346893310547
Loss :  1.5727624893188477 3.4524877071380615 174.19715881347656
Loss :  1.5978600978851318 3.992802619934082 201.2379913330078
Loss :  1.5421584844589233 3.4047203063964844 171.77818298339844
Loss :  1.597566843032837 3.53176212310791 178.1856689453125
Loss :  1.5605758428573608 3.47802734375 175.46194458007812
Loss :  1.5493873357772827 3.508894205093384 176.99411010742188
Loss :  1.606613278388977 3.6775052547454834 185.48187255859375
Loss :  1.5281178951263428 3.761725425720215 189.61439514160156
Loss :  1.5282779932022095 3.6754205226898193 185.29930114746094
Loss :  1.5216554403305054 3.683058738708496 185.67459106445312
Loss :  1.5339796543121338 3.52644681930542 177.8563232421875
Loss :  1.6306840181350708 3.299935817718506 166.6274871826172
Loss :  1.6065003871917725 3.3384385108947754 168.52842712402344
Loss :  1.5115084648132324 3.8218016624450684 192.60159301757812
Loss :  1.5551999807357788 3.420894145965576 172.59991455078125
Loss :  1.50943922996521 3.2892112731933594 165.9700164794922
Loss :  1.6147730350494385 3.3660385608673096 169.9167022705078
  batch 20 loss: 1.6147730350494385, 3.3660385608673096, 169.9167022705078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5489228963851929 3.3415701389312744 168.62742614746094
Loss :  1.504012107849121 3.5493502616882324 178.97152709960938
Loss :  1.5352617502212524 3.485442638397217 175.80738830566406
Loss :  1.5598305463790894 3.3381435871124268 168.46701049804688
Loss :  1.6086547374725342 3.5287137031555176 178.04434204101562
Loss :  1.5387156009674072 3.327972412109375 167.9373321533203
Loss :  1.5492221117019653 3.992810010910034 201.18972778320312
Loss :  1.555127501487732 3.577138662338257 180.4120635986328
Loss :  1.4519054889678955 3.408550262451172 171.87942504882812
Loss :  1.61451256275177 3.565337657928467 179.8813934326172
Loss :  1.463724136352539 3.990428924560547 200.9851837158203
Loss :  1.5913026332855225 3.6951820850372314 186.3504180908203
Loss :  1.5401787757873535 3.4136292934417725 172.2216339111328
Loss :  1.535247802734375 3.3941807746887207 171.24429321289062
Loss :  1.476212739944458 3.715850591659546 187.26873779296875
Loss :  1.499224066734314 3.625042200088501 182.75132751464844
Loss :  1.4997453689575195 3.5418896675109863 178.59422302246094
Loss :  1.6092162132263184 3.5752575397491455 180.37210083007812
Loss :  1.6113210916519165 3.4207828044891357 172.6504669189453
Loss :  1.6322400569915771 3.38713002204895 170.98873901367188
  batch 40 loss: 1.6322400569915771, 3.38713002204895, 170.98873901367188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5611076354980469 3.6121902465820312 182.17062377929688
Loss :  1.5361418724060059 3.4306931495666504 173.07080078125
Loss :  1.526176929473877 3.239576578140259 163.50502014160156
Loss :  1.5521830320358276 3.4418177604675293 173.64306640625
Loss :  1.5083130598068237 3.293980598449707 166.2073516845703
Loss :  1.5598533153533936 3.4896399974823 176.04185485839844
Loss :  1.6124545335769653 3.649545431137085 184.0897216796875
Loss :  1.5444910526275635 3.5782039165496826 180.45469665527344
Loss :  1.6449179649353027 3.3833823204040527 170.81402587890625
Loss :  1.5538424253463745 3.8913686275482178 196.12228393554688
Loss :  1.619455099105835 3.9807121753692627 200.6550750732422
Loss :  1.5933793783187866 3.9204888343811035 197.61782836914062
Loss :  1.5617210865020752 3.3265864849090576 167.89105224609375
Loss :  1.6019033193588257 3.38594651222229 170.89923095703125
Loss :  1.5528628826141357 3.4666497707366943 174.88534545898438
Loss :  1.6360784769058228 3.409325122833252 172.1023406982422
Loss :  1.550952672958374 3.5075149536132812 176.92669677734375
Loss :  1.5294690132141113 3.618217706680298 182.4403533935547
Loss :  1.545663833618164 3.61497163772583 182.29425048828125
Loss :  1.654384732246399 3.5087597370147705 177.09237670898438
  batch 60 loss: 1.654384732246399, 3.5087597370147705, 177.09237670898438
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.530142903327942 3.158482789993286 159.4542694091797
Loss :  1.587475299835205 3.383237361907959 170.7493438720703
Loss :  1.5463818311691284 3.3047029972076416 166.7815399169922
Loss :  1.5389015674591064 3.4358060359954834 173.32919311523438
Loss :  1.5039644241333008 3.215365171432495 162.27223205566406
Loss :  1.5752588510513306 4.031930923461914 203.1717987060547
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.5973963737487793 3.9912538528442383 201.16009521484375
Loss :  1.595816731452942 3.886549472808838 195.92327880859375
Loss :  1.5983340740203857 3.740746021270752 188.63563537597656
Total LOSS train 177.83680372971756 valid 197.2227020263672
CE LOSS train 1.5600353516065157 valid 0.39958351850509644
Contrastive LOSS train 3.525535323069646 valid 0.935186505317688
EPOCH 95:
Loss :  1.6060765981674194 3.3610713481903076 169.65965270996094
Loss :  1.6337785720825195 3.514481544494629 177.35784912109375
Loss :  1.5785911083221436 3.690415620803833 186.09938049316406
Loss :  1.5891590118408203 3.689495086669922 186.0639190673828
Loss :  1.6181645393371582 3.3094594478607178 167.0911407470703
Loss :  1.5560436248779297 3.4303319454193115 173.07264709472656
Loss :  1.6206508874893188 3.4278573989868164 173.01351928710938
Loss :  1.5871955156326294 3.233121395111084 163.24325561523438
Loss :  1.57564115524292 3.499791383743286 176.56520080566406
Loss :  1.624456763267517 3.4011647701263428 171.6826934814453
Loss :  1.5496433973312378 3.4412143230438232 173.6103515625
Loss :  1.5506196022033691 3.625199794769287 182.81060791015625
Loss :  1.5429432392120361 3.3874762058258057 170.916748046875
Loss :  1.5572853088378906 3.4585840702056885 174.48648071289062
Loss :  1.65048348903656 3.320035219192505 167.65223693847656
Loss :  1.6233540773391724 3.5286240577697754 178.05455017089844
Loss :  1.540304183959961 3.5817975997924805 180.6301727294922
Loss :  1.5812257528305054 3.407595634460449 171.96099853515625
Loss :  1.5423063039779663 3.317586660385132 167.42164611816406
Loss :  1.6367048025131226 3.4056155681610107 171.91748046875
  batch 20 loss: 1.6367048025131226, 3.4056155681610107, 171.91748046875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5794609785079956 3.209275484085083 162.04324340820312
Loss :  1.5414013862609863 3.3912885189056396 171.10581970214844
Loss :  1.5683454275131226 3.58208966255188 180.67282104492188
Loss :  1.5886797904968262 3.498363971710205 176.5068817138672
Loss :  1.6307342052459717 3.6215462684631348 182.7080535888672
Loss :  1.572422742843628 3.2925195693969727 166.19839477539062
Loss :  1.5823485851287842 3.424163579940796 172.79052734375
Loss :  1.5888556241989136 3.3633322715759277 169.75546264648438
Loss :  1.501908779144287 3.262810707092285 164.64244079589844
Loss :  1.6389153003692627 3.506751298904419 176.9764862060547
Loss :  1.5136637687683105 3.529160261154175 177.97166442871094
Loss :  1.620884895324707 3.624398708343506 182.8408203125
Loss :  1.5772265195846558 3.6717374324798584 185.16409301757812
Loss :  1.5734665393829346 3.543736219406128 178.76028442382812
Loss :  1.5237793922424316 3.3419008255004883 168.6188201904297
Loss :  1.5439884662628174 3.94901704788208 198.99484252929688
Loss :  1.5436480045318604 3.3070881366729736 166.89805603027344
Loss :  1.635487675666809 3.6267216205596924 182.9715576171875
Loss :  1.6370877027511597 3.3580610752105713 169.54014587402344
Loss :  1.6534572839736938 3.4387965202331543 173.59327697753906
  batch 40 loss: 1.6534572839736938, 3.4387965202331543, 173.59327697753906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5925121307373047 3.6415178775787354 183.6684112548828
Loss :  1.5710521936416626 3.1235036849975586 157.74623107910156
Loss :  1.562268853187561 3.2531323432922363 164.21888732910156
Loss :  1.5848352909088135 3.572321653366089 180.200927734375
Loss :  1.5478883981704712 3.026275157928467 152.86163330078125
Loss :  1.5888816118240356 3.472458839416504 175.21182250976562
Loss :  1.633143424987793 3.237213611602783 163.49383544921875
Loss :  1.5738823413848877 3.8065571784973145 191.90174865722656
Loss :  1.661279320716858 3.534167766571045 178.3696746826172
Loss :  1.581829309463501 3.427227020263672 172.9431915283203
Loss :  1.6367778778076172 3.433093309402466 173.29144287109375
Loss :  1.615949273109436 3.5483622550964355 179.0340576171875
Loss :  1.587882399559021 3.5541865825653076 179.29721069335938
Loss :  1.6230108737945557 3.480767250061035 175.661376953125
Loss :  1.5779037475585938 3.91823410987854 197.4896240234375
Loss :  1.651425838470459 3.424177646636963 172.8603057861328
Loss :  1.5746495723724365 3.537046194076538 178.4269561767578
Loss :  1.5544542074203491 3.4502644538879395 174.06768798828125
Loss :  1.5682041645050049 3.625526189804077 182.84451293945312
Loss :  1.6625566482543945 3.443814754486084 173.85328674316406
  batch 60 loss: 1.6625566482543945, 3.443814754486084, 173.85328674316406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5520243644714355 3.478482961654663 175.47616577148438
Loss :  1.6011930704116821 3.255439043045044 164.37315368652344
Loss :  1.5650333166122437 3.1296610832214355 158.04808044433594
Loss :  1.5563597679138184 3.323141574859619 167.71343994140625
Loss :  1.5246859788894653 2.8846776485443115 145.75857543945312
Loss :  1.5961613655090332 4.291704177856445 216.18136596679688
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.6152926683425903 4.299076557159424 216.56912231445312
Loss :  1.61270272731781 4.1331305503845215 208.26922607421875
Loss :  1.6170226335525513 4.332678318023682 218.2509307861328
Total LOSS train 174.01348407451923 valid 214.8176612854004
CE LOSS train 1.58661653812115 valid 0.4042556583881378
Contrastive LOSS train 3.448537360704862 valid 1.0831695795059204
EPOCH 96:
Loss :  1.617628812789917 3.298734426498413 166.55435180664062
Loss :  1.6416600942611694 3.682748317718506 185.77908325195312
Loss :  1.5922529697418213 3.4067726135253906 171.93087768554688
Loss :  1.6018792390823364 3.6982791423797607 186.51583862304688
Loss :  1.626975655555725 3.060337543487549 154.6438446044922
Loss :  1.5722346305847168 3.4132158756256104 172.2330322265625
Loss :  1.6275968551635742 3.454624891281128 174.3588409423828
Loss :  1.597671389579773 3.3494436740875244 169.06985473632812
Loss :  1.5867410898208618 3.2463905811309814 163.90628051757812
Loss :  1.6325597763061523 3.11236310005188 157.25070190429688
Loss :  1.565021276473999 3.6569371223449707 184.41188049316406
Loss :  1.5656712055206299 3.5396955013275146 178.55044555664062
Loss :  1.558036208152771 3.2547531127929688 164.29568481445312
Loss :  1.5713813304901123 3.3816466331481934 170.65371704101562
Loss :  1.6543086767196655 3.3564870357513428 169.47866821289062
Loss :  1.6334292888641357 3.3899788856506348 171.1323699951172
Loss :  1.5546621084213257 3.510307788848877 177.07005310058594
Loss :  1.5922966003417969 3.297715425491333 166.4780731201172
Loss :  1.5555367469787598 3.3457839488983154 168.84474182128906
Loss :  1.6411181688308716 3.3808670043945312 170.68446350097656
  batch 20 loss: 1.6411181688308716, 3.3808670043945312, 170.68446350097656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5908533334732056 3.1795778274536133 160.5697479248047
Loss :  1.5559481382369995 3.628467321395874 182.9793243408203
Loss :  1.581034779548645 3.280135154724121 165.58779907226562
Loss :  1.6021710634231567 3.586117744445801 180.90806579589844
Loss :  1.6407281160354614 3.6804254055023193 185.66200256347656
Loss :  1.5863134860992432 3.5500261783599854 179.08763122558594
Loss :  1.5946871042251587 4.009472370147705 202.06829833984375
Loss :  1.5984855890274048 3.2906816005706787 166.13255310058594
Loss :  1.5192633867263794 3.260436534881592 164.54107666015625
Loss :  1.6433300971984863 3.369171380996704 170.10189819335938
Loss :  1.5267713069915771 3.6096436977386475 182.00894165039062
Loss :  1.6239815950393677 3.6252877712249756 182.88836669921875
Loss :  1.5869438648223877 3.965489149093628 199.8614044189453
Loss :  1.5838592052459717 3.33365797996521 168.26675415039062
Loss :  1.5385286808013916 3.4201788902282715 172.54747009277344
Loss :  1.5578196048736572 3.4843125343322754 175.7734375
Loss :  1.558500051498413 3.4097018241882324 172.04359436035156
Loss :  1.6420623064041138 3.284763813018799 165.8802490234375
Loss :  1.6447551250457764 3.3787386417388916 170.58169555664062
Loss :  1.6594239473342896 3.339700222015381 168.64443969726562
  batch 40 loss: 1.6594239473342896, 3.339700222015381, 168.64443969726562
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6041741371154785 3.5096638202667236 177.0873565673828
Loss :  1.584805965423584 3.6354734897613525 183.35848999023438
Loss :  1.5755691528320312 3.4168894290924072 172.4200439453125
Loss :  1.5946106910705566 3.2764580249786377 165.41751098632812
Loss :  1.5624076128005981 3.769803285598755 190.0525665283203
Loss :  1.6023399829864502 3.276845693588257 165.4446258544922
Loss :  1.6431058645248413 3.6530096530914307 184.29359436035156
Loss :  1.5882542133331299 3.2489078044891357 164.0336456298828
Loss :  1.667151927947998 3.549159288406372 179.12510681152344
Loss :  1.5957145690917969 3.475651502609253 175.3782958984375
Loss :  1.6430906057357788 3.2870216369628906 165.99417114257812
Loss :  1.6263262033462524 3.25595760345459 164.42420959472656
Loss :  1.599850058555603 3.318265438079834 167.51312255859375
Loss :  1.6338520050048828 3.3532769680023193 169.29769897460938
Loss :  1.591713786125183 3.4956138134002686 176.37240600585938
Loss :  1.6614634990692139 3.398233652114868 171.57315063476562
Loss :  1.5928603410720825 3.5531158447265625 179.2486572265625
Loss :  1.5744811296463013 3.349904775619507 169.06971740722656
Loss :  1.588431715965271 3.4580276012420654 174.48980712890625
Loss :  1.6717160940170288 3.422351837158203 172.789306640625
  batch 60 loss: 1.6717160940170288, 3.422351837158203, 172.789306640625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.5738142728805542 3.658569097518921 184.50225830078125
Loss :  1.61575186252594 3.303161144256592 166.7738037109375
Loss :  1.5854171514511108 3.5730926990509033 180.24005126953125
Loss :  1.576851487159729 3.281886339187622 165.67117309570312
Loss :  1.5504357814788818 2.919093608856201 147.50511169433594
Loss :  1.6080951690673828 4.1160688400268555 207.41152954101562
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6241352558135986 4.113249778747559 207.28662109375
Loss :  1.6221888065338135 3.9238297939300537 197.8136749267578
Loss :  1.6269335746765137 3.8569858074188232 194.47622680664062
Total LOSS train 173.10851440429687 valid 201.74701309204102
CE LOSS train 1.598897123336792 valid 0.4067333936691284
Contrastive LOSS train 3.4301923495072586 valid 0.9642464518547058
EPOCH 97:
Loss :  1.63437020778656 3.5191142559051514 177.590087890625
Loss :  1.653246521949768 3.561962127685547 179.75135803222656
Loss :  1.6107425689697266 3.3210268020629883 167.66209411621094
Loss :  1.6197714805603027 3.6183042526245117 182.53497314453125
Loss :  1.6379526853561401 3.7370851039886475 188.49220275878906
Loss :  1.5924581289291382 3.751551628112793 189.17002868652344
Loss :  1.6395955085754395 3.47035813331604 175.1575164794922
Loss :  1.6120679378509521 3.562727928161621 179.7484588623047
Loss :  1.5994125604629517 4.199182987213135 211.55856323242188
Loss :  1.6411235332489014 3.6637487411499023 184.8285675048828
Loss :  1.583972454071045 3.8497512340545654 194.071533203125
Loss :  1.584749698638916 3.604907274246216 181.83010864257812
Loss :  1.5790983438491821 4.102181434631348 206.68817138671875
Loss :  1.5901691913604736 4.3021931648254395 216.6998291015625
Loss :  1.661206841468811 3.43397855758667 173.36013793945312
Loss :  1.6465414762496948 3.667661428451538 185.0296173095703
Loss :  1.57376229763031 3.463059425354004 174.7267303466797
Loss :  1.607344150543213 3.972564458847046 200.23556518554688
Loss :  1.5720205307006836 3.8741021156311035 195.27713012695312
Loss :  1.6486212015151978 3.5081982612609863 177.05853271484375
  batch 20 loss: 1.6486212015151978, 3.5081982612609863, 177.05853271484375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.6029601097106934 3.403245210647583 171.76522827148438
Loss :  1.5685758590698242 3.551882028579712 179.1626739501953
Loss :  1.5885391235351562 3.7018845081329346 186.68276977539062
Loss :  1.6075712442398071 3.746640682220459 188.93960571289062
Loss :  1.6417170763015747 3.6838507652282715 185.83425903320312
Loss :  1.5874520540237427 3.352726459503174 169.22377014160156
Loss :  1.596809983253479 3.4975953102111816 176.47657775878906
Loss :  1.5940725803375244 3.4366796016693115 173.4280548095703
Loss :  1.5205262899398804 3.4519739151000977 174.1192169189453
Loss :  1.6385868787765503 3.4426350593566895 173.7703399658203
Loss :  1.5263079404830933 3.517815589904785 177.41708374023438
Loss :  1.6196491718292236 3.7071869373321533 186.97898864746094
Loss :  1.58682382106781 3.8053319454193115 191.85342407226562
Loss :  1.5809051990509033 4.064222812652588 204.7920379638672
Loss :  1.5359376668930054 3.5186381340026855 177.46783447265625
Loss :  1.5562211275100708 3.5815634727478027 180.6343994140625
Loss :  1.5553655624389648 3.2754385471343994 165.32728576660156
Loss :  1.6376256942749023 3.388390064239502 171.05712890625
Loss :  1.642043113708496 3.315882444381714 167.43617248535156
Loss :  1.6568281650543213 3.3998594284057617 171.64979553222656
  batch 40 loss: 1.6568281650543213, 3.3998594284057617, 171.64979553222656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.6029833555221558 4.270650386810303 215.135498046875
Loss :  1.5816541910171509 3.2826919555664062 165.71624755859375
Loss :  1.569984793663025 3.320375919342041 167.5887908935547
Loss :  1.5883054733276367 3.290959358215332 166.1362762451172
Loss :  1.5585384368896484 3.065509080886841 154.833984375
Loss :  1.5942085981369019 3.7601494789123535 189.6016845703125
Loss :  1.6338125467300415 3.9103174209594727 197.14968872070312
Loss :  1.5778499841690063 3.433136463165283 173.23468017578125
Loss :  1.6550332307815552 3.273606061935425 165.3353271484375
Loss :  1.5888060331344604 3.1642558574676514 159.80160522460938
Loss :  1.6275780200958252 3.639594793319702 183.60731506347656
Loss :  1.6204522848129272 3.285982370376587 165.91957092285156
Loss :  1.5942574739456177 3.091402530670166 156.1643829345703
Loss :  1.632341742515564 3.0487112998962402 154.06790161132812
Loss :  1.5815414190292358 3.3961331844329834 171.38819885253906
Loss :  1.6601818799972534 4.413711071014404 222.34573364257812
Loss :  1.5916060209274292 3.722564935684204 187.7198486328125
Loss :  1.5737032890319824 3.594346284866333 181.291015625
Loss :  1.5944374799728394 3.459953546524048 174.5921173095703
Loss :  1.6666656732559204 3.6478700637817383 184.0601806640625
  batch 60 loss: 1.6666656732559204, 3.6478700637817383, 184.0601806640625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.5779908895492554 3.9056010246276855 196.8580322265625
Loss :  1.6056069135665894 3.7565088272094727 189.43104553222656
Loss :  1.5860298871994019 3.4344446659088135 173.30825805664062
Loss :  1.5690486431121826 3.250533103942871 164.095703125
Loss :  1.5459154844284058 2.607466459274292 131.9192352294922
Loss :  1.58842134475708 4.044882297515869 203.83253479003906
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.619496464729309 4.211187362670898 212.17886352539062
Loss :  1.5987974405288696 3.955615282058716 199.3795623779297
Loss :  1.620079517364502 4.004090785980225 201.8246307373047
Total LOSS train 180.10446425217847 valid 204.30389785766602
CE LOSS train 1.601711995785053 valid 0.4050198793411255
Contrastive LOSS train 3.570055051950308 valid 1.0010226964950562
EPOCH 98:
Loss :  1.624254584312439 2.939943552017212 148.62142944335938
Loss :  1.6336344480514526 3.4617340564727783 174.7203369140625
Loss :  1.5991877317428589 2.843353509902954 143.76686096191406
Loss :  1.6086368560791016 3.19281268119812 161.24928283691406
Loss :  1.6255079507827759 3.055717706680298 154.41139221191406
Loss :  1.5890886783599854 3.32841157913208 168.00967407226562
Loss :  1.6252734661102295 3.3815104961395264 170.7008056640625
Loss :  1.597568154335022 3.2620787620544434 164.70150756835938
Loss :  1.581468939781189 3.621732473373413 182.6680908203125
Loss :  1.6252321004867554 3.077556848526001 155.50306701660156
Loss :  1.576424479484558 4.073972225189209 205.27503967285156
Loss :  1.5730853080749512 3.6368367671966553 183.41493225097656
Loss :  1.5693823099136353 3.589545488357544 181.04666137695312
Loss :  1.5787590742111206 3.44071888923645 173.61471557617188
Loss :  1.643946647644043 3.424706220626831 172.87925720214844
Loss :  1.6427034139633179 3.4177639484405518 172.53089904785156
Loss :  1.563029170036316 3.4010510444641113 171.61558532714844
Loss :  1.6007771492004395 3.3140461444854736 167.30308532714844
Loss :  1.5625722408294678 3.0041017532348633 151.76766967773438
Loss :  1.6338330507278442 3.285945177078247 165.93109130859375
  batch 20 loss: 1.6338330507278442, 3.285945177078247, 165.93109130859375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.5914547443389893 2.8864235877990723 145.9126434326172
Loss :  1.5559617280960083 3.3702962398529053 170.07078552246094
Loss :  1.5759133100509644 3.1105875968933105 157.10528564453125
Loss :  1.6012799739837646 3.3307461738586426 168.13858032226562
Loss :  1.6356449127197266 3.6219964027404785 182.7354736328125
Loss :  1.5800498723983765 3.412539005279541 172.20700073242188
Loss :  1.5954450368881226 3.336076021194458 168.3992462158203
Loss :  1.583174467086792 3.4401519298553467 173.5907745361328
Loss :  1.5202621221542358 3.5993075370788574 181.4856414794922
Loss :  1.6273609399795532 2.9778175354003906 150.5182342529297
Loss :  1.519804835319519 3.1467607021331787 158.8578338623047
Loss :  1.6081156730651855 3.4296061992645264 173.0884246826172
Loss :  1.5805957317352295 3.270630359649658 165.11212158203125
Loss :  1.5750601291656494 3.124272584915161 157.78868103027344
Loss :  1.5305533409118652 3.2263784408569336 162.84947204589844
Loss :  1.549435019493103 3.944387674331665 198.76882934570312
Loss :  1.548193097114563 3.3237195014953613 167.73416137695312
Loss :  1.6318870782852173 3.317913770675659 167.52757263183594
Loss :  1.6381970643997192 3.351940155029297 169.23521423339844
Loss :  1.6468727588653564 3.0944948196411133 156.37161254882812
  batch 40 loss: 1.6468727588653564, 3.0944948196411133, 156.37161254882812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.5950803756713867 3.3580806255340576 169.49911499023438
Loss :  1.5762206315994263 3.505636692047119 176.85804748535156
Loss :  1.5629210472106934 3.39786434173584 171.45614624023438
Loss :  1.5830270051956177 3.3566651344299316 169.41627502441406
Loss :  1.5539665222167969 3.0878820419311523 155.9480743408203
Loss :  1.585619330406189 2.9764673709869385 150.4089813232422
Loss :  1.629198670387268 3.033848285675049 153.3216094970703
Loss :  1.5712707042694092 2.9596240520477295 149.55247497558594
Loss :  1.6493327617645264 3.1958324909210205 161.4409637451172
Loss :  1.5851399898529053 3.38623046875 170.89666748046875
Loss :  1.6208605766296387 3.8305413722991943 193.14793395996094
Loss :  1.6193606853485107 3.3908920288085938 171.16395568847656
Loss :  1.591935396194458 3.446326494216919 173.90826416015625
Loss :  1.6298695802688599 3.979860782623291 200.62290954589844
Loss :  1.5760254859924316 3.4659931659698486 174.87567138671875
Loss :  1.6592098474502563 3.348515748977661 169.08499145507812
Loss :  1.5868600606918335 3.237935781478882 163.483642578125
Loss :  1.5709514617919922 3.4404969215393066 173.59579467773438
Loss :  1.5889062881469727 3.529109239578247 178.04437255859375
Loss :  1.6642951965332031 3.3535776138305664 169.34317016601562
  batch 60 loss: 1.6642951965332031, 3.3535776138305664, 169.34317016601562
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.571265697479248 3.287834405899048 165.96298217773438
Loss :  1.6043295860290527 3.392893075942993 171.2489776611328
Loss :  1.5811909437179565 3.2625606060028076 164.709228515625
Loss :  1.5623186826705933 3.9591832160949707 199.521484375
Loss :  1.540628433227539 2.614518880844116 132.26657104492188
Loss :  1.6066124439239502 3.9262731075286865 197.92027282714844
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.624599814414978 3.8814446926116943 195.69683837890625
Loss :  1.6154663562774658 3.7682085037231445 190.02589416503906
Loss :  1.632346749305725 3.888725519180298 196.06861877441406
Total LOSS train 168.4154965914213 valid 194.92790603637695
CE LOSS train 1.5939909623219417 valid 0.4080866873264313
Contrastive LOSS train 3.3364300984602706 valid 0.9721813797950745
EPOCH 99:
Loss :  1.628968596458435 3.1516594886779785 159.21194458007812
Loss :  1.637909173965454 3.6381077766418457 183.54330444335938
Loss :  1.6014759540557861 3.3679206371307373 169.99749755859375
Loss :  1.6064085960388184 3.1798183917999268 160.5973358154297
Loss :  1.6306812763214111 3.2515451908111572 164.2079315185547
Loss :  1.586447834968567 3.1219305992126465 157.68296813964844
Loss :  1.6301889419555664 3.581327199935913 180.69654846191406
Loss :  1.5971659421920776 3.1321847438812256 158.20640563964844
Loss :  1.5829687118530273 2.904038667678833 146.78489685058594
Loss :  1.6261454820632935 2.9517266750335693 149.2124786376953
Loss :  1.5787711143493652 3.2707109451293945 165.11431884765625
Loss :  1.5772292613983154 3.5343658924102783 178.29551696777344
Loss :  1.5746856927871704 3.2242162227630615 162.78550720214844
Loss :  1.5812417268753052 3.2532668113708496 164.2445831298828
Loss :  1.6404565572738647 3.1971940994262695 161.5001678466797
Loss :  1.644015908241272 3.0709357261657715 155.1907958984375
Loss :  1.5595487356185913 3.4316656589508057 173.14283752441406
Loss :  1.5996065139770508 3.9168028831481934 197.43975830078125
Loss :  1.555985689163208 2.984311819076538 150.77157592773438
Loss :  1.6268360614776611 3.175888776779175 160.4212646484375
  batch 20 loss: 1.6268360614776611, 3.175888776779175, 160.4212646484375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.5876134634017944 3.2558813095092773 164.38168334960938
Loss :  1.552664875984192 3.6665499210357666 184.88015747070312
Loss :  1.5767098665237427 3.3101649284362793 167.08494567871094
Loss :  1.604909896850586 3.3657796382904053 169.89389038085938
Loss :  1.6379735469818115 3.522937536239624 177.78485107421875
Loss :  1.5812691450119019 3.1563236713409424 159.39744567871094
Loss :  1.596402645111084 3.3876242637634277 170.9776153564453
Loss :  1.5861083269119263 3.5900778770446777 181.08999633789062
Loss :  1.5230000019073486 3.32313871383667 167.679931640625
Loss :  1.6266655921936035 3.9341282844543457 198.3330841064453
Loss :  1.5195976495742798 3.70686674118042 186.86293029785156
Loss :  1.6045438051223755 3.7207303047180176 187.64105224609375
Loss :  1.5734018087387085 3.485466957092285 175.84674072265625
Loss :  1.5716968774795532 3.601832389831543 181.66331481933594
Loss :  1.5198924541473389 4.1866455078125 210.8521728515625
Loss :  1.5424939393997192 3.16577410697937 159.83120727539062
Loss :  1.5394055843353271 3.4705283641815186 175.06581115722656
Loss :  1.6244608163833618 3.4384572505950928 173.54733276367188
Loss :  1.6300132274627686 3.358670949935913 169.56356811523438
Loss :  1.6413706541061401 3.299276113510132 166.6051788330078
  batch 40 loss: 1.6413706541061401, 3.299276113510132, 166.6051788330078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.5855883359909058 2.941054105758667 148.63829040527344
Loss :  1.5677369832992554 3.002821207046509 151.70880126953125
Loss :  1.550885558128357 3.4079909324645996 171.950439453125
Loss :  1.5692739486694336 3.8798985481262207 195.564208984375
Loss :  1.5402923822402954 3.489741563796997 176.02737426757812
Loss :  1.5750651359558105 3.1693673133850098 160.04342651367188
Loss :  1.619397521018982 3.3207345008850098 167.6561279296875
Loss :  1.55617094039917 3.4708709716796875 175.09971618652344
Loss :  1.6392037868499756 3.143216133117676 158.80001831054688
Loss :  1.5684577226638794 3.784975290298462 190.8172149658203
Loss :  1.6068658828735352 3.661417245864868 184.677734375
Loss :  1.6062068939208984 3.4965555667877197 176.43397521972656
Loss :  1.57792067527771 3.08941388130188 156.04861450195312
Loss :  1.6181055307388306 3.1257052421569824 157.9033660888672
Loss :  1.5589572191238403 3.475921154022217 175.35501098632812
Loss :  1.6454880237579346 3.263326406478882 164.8118133544922
Loss :  1.5665059089660645 3.2923266887664795 166.18284606933594
Loss :  1.544834852218628 3.350313663482666 169.06051635742188
Loss :  1.5641423463821411 3.2514851093292236 164.13839721679688
Loss :  1.6500691175460815 3.1693077087402344 160.11546325683594
  batch 60 loss: 1.6500691175460815, 3.1693077087402344, 160.11546325683594
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5434682369232178 3.0354163646698 153.3142852783203
Loss :  1.580096960067749 3.0475099086761475 153.95558166503906
Loss :  1.554787278175354 2.8989033699035645 146.49996948242188
Loss :  1.5369148254394531 3.2654879093170166 164.81130981445312
Loss :  1.5106534957885742 2.9767279624938965 150.3470458984375
Loss :  1.5655624866485596 3.911090612411499 197.12010192871094
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.583958625793457 3.8718197345733643 195.17494201660156
Loss :  1.5767654180526733 3.8011248111724854 191.6330108642578
Loss :  1.5884487628936768 3.8427534103393555 193.7261199951172
Total LOSS train 168.73833993765024 valid 194.41354370117188
CE LOSS train 1.586831100170429 valid 0.3971121907234192
Contrastive LOSS train 3.3430301812978893 valid 0.9606883525848389
EPOCH 100:
Loss :  1.6072824001312256 3.0437889099121094 153.79673767089844
Loss :  1.6209787130355835 3.330599784851074 168.1509552001953
Loss :  1.577713131904602 3.1838693618774414 160.77117919921875
Loss :  1.5873916149139404 3.273869276046753 165.28085327148438
Loss :  1.6079744100570679 3.7507576942443848 189.14585876464844
Loss :  1.5618845224380493 3.0417377948760986 153.64877319335938
Loss :  1.6062155961990356 3.406125068664551 171.9124755859375
Loss :  1.5734765529632568 2.8271706104278564 142.9320068359375
Loss :  1.5565537214279175 2.940843343734741 148.5987091064453
Loss :  1.605058193206787 2.9958689212799072 151.39849853515625
Loss :  1.5501298904418945 3.2965304851531982 166.37664794921875
Loss :  1.5519744157791138 3.529160499572754 178.00999450683594
Loss :  1.5466872453689575 3.111793041229248 157.13633728027344
Loss :  1.557668685913086 3.109245777130127 157.01995849609375
Loss :  1.6236777305603027 3.176591634750366 160.4532470703125
Loss :  1.6274645328521729 3.3871824741363525 170.98660278320312
Loss :  1.5385099649429321 3.1777327060699463 160.42515563964844
Loss :  1.5805543661117554 3.335449695587158 168.35304260253906
Loss :  1.5390996932983398 2.9247541427612305 147.77679443359375
Loss :  1.6163294315338135 3.068239688873291 155.0283203125
  batch 20 loss: 1.6163294315338135, 3.068239688873291, 155.0283203125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.574165940284729 3.1908106803894043 161.1147003173828
Loss :  1.5365749597549438 3.2648777961730957 164.7804718017578
Loss :  1.5592724084854126 3.09509015083313 156.3137664794922
Loss :  1.5893104076385498 2.986417531967163 150.91018676757812
Loss :  1.6272010803222656 3.2406165599823 163.65802001953125
Loss :  1.5671354532241821 3.158414363861084 159.48785400390625
Loss :  1.58241868019104 3.2017316818237305 161.6689910888672
Loss :  1.569348931312561 2.9136035442352295 147.24952697753906
Loss :  1.5023781061172485 2.924708127975464 147.73779296875
Loss :  1.6169764995574951 3.175945997238159 160.41427612304688
Loss :  1.503907561302185 3.492652177810669 176.1365203857422
Loss :  1.596317172050476 4.064154624938965 204.80404663085938
Loss :  1.5676441192626953 3.3633201122283936 169.733642578125
Loss :  1.564000129699707 3.0750527381896973 155.31663513183594
Loss :  1.5125980377197266 3.4471585750579834 173.8705291748047
Loss :  1.5358717441558838 3.904311418533325 196.75144958496094
Loss :  1.5340412855148315 3.164137125015259 159.74090576171875
Loss :  1.6202243566513062 2.756030321121216 139.42173767089844
Loss :  1.6278847455978394 2.7389137744903564 138.57357788085938
Loss :  1.639704704284668 4.056850910186768 204.4822540283203
  batch 40 loss: 1.639704704284668, 4.056850910186768, 204.4822540283203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.5835638046264648 3.127262830734253 157.9467010498047
Loss :  1.564988374710083 2.7742865085601807 140.27931213378906
Loss :  1.5509439706802368 2.988349676132202 150.9684295654297
Loss :  1.568131446838379 3.7186238765716553 187.49932861328125
Loss :  1.5432428121566772 3.5737199783325195 180.229248046875
Loss :  1.5798183679580688 2.892754554748535 146.21754455566406
Loss :  1.62315833568573 3.8352062702178955 193.3834686279297
Loss :  1.5630433559417725 3.0402603149414062 153.57606506347656
Loss :  1.645730972290039 2.8057475090026855 141.93310546875
Loss :  1.572293758392334 3.2850944995880127 165.8270263671875
Loss :  1.6124427318572998 3.458765983581543 174.5507354736328
Loss :  1.6090705394744873 3.1051645278930664 156.86729431152344
Loss :  1.578777551651001 2.9323718547821045 148.19737243652344
Loss :  1.6217290163040161 3.533080816268921 178.27577209472656
Loss :  1.5633926391601562 3.492030620574951 176.1649169921875
Loss :  1.6488169431686401 2.9904589653015137 151.17176818847656
Loss :  1.5731695890426636 3.4612531661987305 174.6358184814453
Loss :  1.553906798362732 3.0704994201660156 155.07887268066406
Loss :  1.573578119277954 3.675283670425415 185.3377685546875
Loss :  1.6576753854751587 2.730390787124634 138.17721557617188
  batch 60 loss: 1.6576753854751587, 2.730390787124634, 138.17721557617188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.5517456531524658 2.6963753700256348 136.37051391601562
Loss :  1.5870453119277954 2.920595407485962 147.6168212890625
Loss :  1.561449408531189 3.202624559402466 161.6926727294922
Loss :  1.541538953781128 3.90336275100708 196.7096710205078
Loss :  1.517411470413208 2.892686128616333 146.15171813964844
Loss :  1.5722904205322266 4.177576541900635 210.4511260986328
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.59088134765625 4.130643844604492 208.12307739257812
Loss :  1.5820746421813965 4.120211601257324 207.5926513671875
Loss :  1.596949577331543 4.0916972160339355 206.1818084716797
Total LOSS train 162.5265876183143 valid 208.08716583251953
CE LOSS train 1.5786194838010348 valid 0.39923739433288574
Contrastive LOSS train 3.2189593718602105 valid 1.0229243040084839
EPOCH 101:
Loss :  1.610921859741211 3.2983274459838867 166.52728271484375
Loss :  1.6243162155151367 3.3178021907806396 167.51441955566406
Loss :  1.5833793878555298 3.0169520378112793 152.43096923828125
Loss :  1.5916543006896973 3.4907987117767334 176.131591796875
Loss :  1.614829421043396 2.6757023334503174 135.39993286132812
Loss :  1.5668351650238037 2.984881639480591 150.8109130859375
Loss :  1.6141189336776733 3.7938737869262695 191.3078155517578
Loss :  1.5797457695007324 3.1781563758850098 160.48756408691406
Loss :  1.5650602579116821 3.119469165802002 157.5385284423828
Loss :  1.6126530170440674 2.765958309173584 139.9105682373047
Loss :  1.5565546751022339 3.0642716884613037 154.77012634277344
Loss :  1.5546456575393677 3.062028646469116 154.65606689453125
Loss :  1.5533692836761475 3.5350911617279053 178.3079376220703
Loss :  1.5633881092071533 2.902167558670044 146.67176818847656
Loss :  1.6316728591918945 3.031252861022949 153.19430541992188
Loss :  1.6331151723861694 3.0128793716430664 152.27708435058594
Loss :  1.5477656126022339 2.9532346725463867 149.20948791503906
Loss :  1.5894720554351807 2.545840263366699 128.88148498535156
Loss :  1.547708511352539 2.604874610900879 131.79144287109375
Loss :  1.6249079704284668 2.527135133743286 127.9816665649414
  batch 20 loss: 1.6249079704284668, 2.527135133743286, 127.9816665649414
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.582201361656189 2.6922268867492676 136.19354248046875
Loss :  1.5454634428024292 3.8398396968841553 193.53744506835938
Loss :  1.569404125213623 2.865283727645874 144.83358764648438
Loss :  1.5972588062286377 2.791691541671753 141.1818389892578
Loss :  1.633626103401184 3.0440022945404053 153.833740234375
Loss :  1.575801134109497 3.031987190246582 153.1751708984375
Loss :  1.5899827480316162 3.4429335594177246 173.73666381835938
Loss :  1.5791890621185303 3.3956398963928223 171.36119079589844
Loss :  1.5135811567306519 3.474881649017334 175.25765991210938
Loss :  1.62278151512146 3.3373239040374756 168.48898315429688
Loss :  1.5142264366149902 3.1319243907928467 158.11044311523438
Loss :  1.6058628559112549 3.4661355018615723 174.9126434326172
Loss :  1.5757651329040527 3.072807788848877 155.21615600585938
Loss :  1.5730104446411133 4.515626430511475 227.35433959960938
Loss :  1.5260717868804932 2.8713326454162598 145.09271240234375
Loss :  1.5476776361465454 2.7796566486358643 140.530517578125
Loss :  1.547018051147461 3.0611186027526855 154.60293579101562
Loss :  1.6298226118087769 2.9171195030212402 147.4857940673828
Loss :  1.6363581418991089 3.1445915699005127 158.86593627929688
Loss :  1.6474260091781616 2.8666913509368896 144.98199462890625
  batch 40 loss: 1.6474260091781616, 2.8666913509368896, 144.98199462890625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.5944732427597046 3.0614383220672607 154.6663818359375
Loss :  1.5758278369903564 4.084401607513428 205.7958984375
Loss :  1.5639841556549072 2.7357332706451416 138.35064697265625
Loss :  1.5800241231918335 3.4505722522735596 174.10862731933594
Loss :  1.5552282333374023 2.7335290908813477 138.23167419433594
Loss :  1.5911564826965332 2.992048978805542 151.193603515625
Loss :  1.6319080591201782 2.9652485847473145 149.89434814453125
Loss :  1.5748348236083984 2.99894380569458 151.5220184326172
Loss :  1.6527634859085083 2.9145925045013428 147.3824005126953
Loss :  1.5829938650131226 2.9822561740875244 150.69580078125
Loss :  1.6204125881195068 3.2134814262390137 162.2944793701172
Loss :  1.6164851188659668 2.9785430431365967 150.54364013671875
Loss :  1.5870568752288818 2.8689351081848145 145.0338134765625
Loss :  1.6268534660339355 3.1236116886138916 157.80743408203125
Loss :  1.5710598230361938 2.7009241580963135 136.6172637939453
Loss :  1.653656244277954 2.514333963394165 127.37035369873047
Loss :  1.5812876224517822 3.155259370803833 159.34425354003906
Loss :  1.563768982887268 2.953777313232422 149.2526397705078
Loss :  1.5826747417449951 3.071748971939087 155.1701202392578
Loss :  1.6642258167266846 2.660599708557129 134.6942138671875
  batch 60 loss: 1.6642258167266846, 2.660599708557129, 134.6942138671875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.5641063451766968 3.1405227184295654 158.59024047851562
Loss :  1.597963809967041 2.8025755882263184 141.72674560546875
Loss :  1.575760841369629 2.9441728591918945 148.78440856933594
Loss :  1.558040976524353 3.562166690826416 179.6663818359375
Loss :  1.5357295274734497 2.5861945152282715 130.845458984375
Loss :  1.585823655128479 4.302375793457031 216.70462036132812
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.6038072109222412 4.183150291442871 210.76132202148438
Loss :  1.5962862968444824 4.2490715980529785 214.04986572265625
Loss :  1.6086311340332031 4.112035274505615 207.21038818359375
Total LOSS train 155.29441692645733 valid 212.18154907226562
CE LOSS train 1.5873987674713135 valid 0.4021577835083008
Contrastive LOSS train 3.074140376311082 valid 1.0280088186264038
EPOCH 102:
Loss :  1.621997356414795 3.275165557861328 165.38026428222656
Loss :  1.6343493461608887 3.564955711364746 179.88214111328125
Loss :  1.5981130599975586 2.848571538925171 144.0266876220703
Loss :  1.6053885221481323 3.1061172485351562 156.9112548828125
Loss :  1.6280349493026733 2.896422863006592 146.4491729736328
Loss :  1.5833065509796143 3.0562634468078613 154.396484375
Loss :  1.6277995109558105 3.0588366985321045 154.5696258544922
Loss :  1.5955294370651245 2.8837311267852783 145.78208923339844
Loss :  1.582058072090149 3.8047449588775635 191.81930541992188
Loss :  1.6252714395523071 3.0536587238311768 154.30821228027344
Loss :  1.5761164426803589 3.4535133838653564 174.2517852783203
Loss :  1.5740822553634644 3.101921558380127 156.670166015625
Loss :  1.57217538356781 2.9766454696655273 150.40444946289062
Loss :  1.5815132856369019 3.241652011871338 163.6641082763672
Loss :  1.6428751945495605 2.933635950088501 148.3246612548828
Loss :  1.645110011100769 2.9517593383789062 149.2330780029297
Loss :  1.5663902759552002 2.957803964614868 149.4565887451172
Loss :  1.6050810813903809 3.8263089656829834 192.9205322265625
Loss :  1.5673950910568237 2.6341397762298584 133.27438354492188
Loss :  1.6388837099075317 2.7680532932281494 140.0415496826172
  batch 20 loss: 1.6388837099075317, 2.7680532932281494, 140.0415496826172
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.6028019189834595 2.704935312271118 136.8495635986328
Loss :  1.5698484182357788 2.9682071208953857 149.98020935058594
Loss :  1.5916334390640259 2.6549980640411377 134.34153747558594
Loss :  1.6126222610473633 3.1078402996063232 157.004638671875
Loss :  1.645520806312561 3.900498151779175 196.67042541503906
Loss :  1.5927356481552124 3.049037218093872 154.0446014404297
Loss :  1.6049978733062744 2.800457715988159 141.6278839111328
Loss :  1.597452998161316 3.319427728652954 167.56884765625
Loss :  1.533605694770813 2.856024980545044 144.33485412597656
Loss :  1.6377347707748413 3.593987226486206 181.33709716796875
Loss :  1.5312824249267578 2.986034631729126 150.8330078125
Loss :  1.6173532009124756 3.1967978477478027 161.45724487304688
Loss :  1.5866942405700684 3.0007317066192627 151.623291015625
Loss :  1.5832828283309937 2.8925516605377197 146.2108612060547
Loss :  1.535428762435913 3.2097935676574707 162.02511596679688
Loss :  1.5542874336242676 3.3616724014282227 169.63790893554688
Loss :  1.552241563796997 3.21026349067688 162.06541442871094
Loss :  1.6296935081481934 3.01710844039917 152.48512268066406
Loss :  1.6367260217666626 3.0679285526275635 155.03314208984375
Loss :  1.6476763486862183 3.4795849323272705 175.62692260742188
  batch 40 loss: 1.6476763486862183, 3.4795849323272705, 175.62692260742188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.596168875694275 3.425380229949951 172.8651885986328
Loss :  1.5796202421188354 3.1251769065856934 157.83847045898438
Loss :  1.5676301717758179 3.2486250400543213 163.99888610839844
Loss :  1.5824949741363525 3.196089506149292 161.386962890625
Loss :  1.5585048198699951 2.541144371032715 128.61572265625
Loss :  1.5963101387023926 2.5371172428131104 128.45217895507812
Loss :  1.635632872581482 3.5658862590789795 179.92994689941406
Loss :  1.5773686170578003 2.771087169647217 140.1317138671875
Loss :  1.6562823057174683 3.639331102371216 183.62283325195312
Loss :  1.5854308605194092 3.140920400619507 158.63145446777344
Loss :  1.6283633708953857 2.9661262035369873 149.93466186523438
Loss :  1.621877908706665 2.613281488418579 132.28594970703125
Loss :  1.5939977169036865 2.863952875137329 144.79164123535156
Loss :  1.631514072418213 2.82781720161438 143.02236938476562
Loss :  1.5806447267532349 2.6964170932769775 136.40150451660156
Loss :  1.6579676866531372 3.197608232498169 161.53839111328125
Loss :  1.5885580778121948 3.1880366802215576 160.9904022216797
Loss :  1.5702970027923584 3.0662014484405518 154.88037109375
Loss :  1.5887082815170288 3.3408753871917725 168.63247680664062
Loss :  1.6674575805664062 2.7950870990753174 141.42181396484375
  batch 60 loss: 1.6674575805664062, 2.7950870990753174, 141.42181396484375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.5708483457565308 2.834653377532959 143.3035125732422
Loss :  1.604761004447937 2.7560431957244873 139.40692138671875
Loss :  1.5822858810424805 3.041594982147217 153.6620330810547
Loss :  1.5679690837860107 3.040543556213379 153.5951385498047
Loss :  1.5441402196884155 2.745260715484619 138.8071746826172
Loss :  1.581011176109314 3.8691534996032715 195.03868103027344
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.5975316762924194 3.872194528579712 195.20726013183594
Loss :  1.5929591655731201 3.7012808322906494 186.65699768066406
Loss :  1.6016240119934082 3.810715436935425 192.13739013671875
Total LOSS train 155.39495309682994 valid 192.26008224487305
CE LOSS train 1.597998861166147 valid 0.40040600299835205
Contrastive LOSS train 3.075939083099365 valid 0.9526788592338562
EPOCH 103:
Loss :  1.6246395111083984 3.3003225326538086 166.64076232910156
Loss :  1.6376038789749146 3.031306266784668 153.20291137695312
Loss :  1.603445053100586 2.7808802127838135 140.64744567871094
Loss :  1.6103415489196777 3.1130518913269043 157.26292419433594
Loss :  1.6322534084320068 2.981022834777832 150.6833953857422
Loss :  1.5890617370605469 3.535297393798828 178.3539276123047
Loss :  1.6305924654006958 3.751619577407837 189.21157836914062
Loss :  1.6032031774520874 3.190016984939575 161.10406494140625
Loss :  1.589643120765686 3.486983060836792 175.9387969970703
Loss :  1.6322717666625977 3.3692803382873535 170.09629821777344
Loss :  1.5821454524993896 2.7901861667633057 141.09144592285156
Loss :  1.57894766330719 2.729353666305542 138.046630859375
Loss :  1.579485297203064 3.1406428813934326 158.61163330078125
Loss :  1.588853359222412 3.490511178970337 176.11441040039062
Loss :  1.65068781375885 2.8272383213043213 143.01260375976562
Loss :  1.6450705528259277 3.0609326362609863 154.6916961669922
Loss :  1.5698214769363403 3.1014437675476074 156.6420135498047
Loss :  1.6033258438110352 3.213991403579712 162.3029022216797
Loss :  1.5652786493301392 2.419428825378418 122.5367202758789
Loss :  1.6382867097854614 2.548576831817627 129.06712341308594
  batch 20 loss: 1.6382867097854614, 2.548576831817627, 129.06712341308594
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.5993170738220215 2.792914390563965 141.2450408935547
Loss :  1.564327359199524 2.994107723236084 151.26971435546875
Loss :  1.5849422216415405 2.8387515544891357 143.52252197265625
Loss :  1.6090134382247925 3.442031145095825 173.7105712890625
Loss :  1.6426057815551758 2.7499780654907227 139.14151000976562
Loss :  1.5904885530471802 2.993825674057007 151.28176879882812
Loss :  1.6035118103027344 3.000659942626953 151.63650512695312
Loss :  1.596622347831726 3.0172998905181885 152.46160888671875
Loss :  1.5348929166793823 3.1481525897979736 158.94252014160156
Loss :  1.6365879774093628 3.3225529193878174 167.76422119140625
Loss :  1.5348618030548096 2.902400016784668 146.6548614501953
Loss :  1.6190218925476074 3.4141037464141846 172.32420349121094
Loss :  1.5931968688964844 3.3919618129730225 171.1912841796875
Loss :  1.589919924736023 2.79561448097229 141.3706512451172
Loss :  1.5449119806289673 2.930745840072632 148.0821990966797
Loss :  1.5645313262939453 2.763718843460083 139.75047302246094
Loss :  1.5641533136367798 3.586510181427002 180.88966369628906
Loss :  1.638458490371704 3.1371023654937744 158.4935760498047
Loss :  1.6453152894973755 3.5345776081085205 178.3741912841797
Loss :  1.6560429334640503 3.362600564956665 169.78607177734375
  batch 40 loss: 1.6560429334640503, 3.362600564956665, 169.78607177734375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.607058048248291 2.958747386932373 149.54441833496094
Loss :  1.589934229850769 3.175184488296509 160.3491668701172
Loss :  1.5782222747802734 3.3172292709350586 167.43968200683594
Loss :  1.5920040607452393 2.9136297702789307 147.27349853515625
Loss :  1.5693299770355225 2.917307138442993 147.4346923828125
Loss :  1.6060724258422852 2.619976043701172 132.60488891601562
Loss :  1.6425271034240723 3.291738748550415 166.22947692871094
Loss :  1.5878276824951172 3.228013515472412 162.98851013183594
Loss :  1.6630531549453735 3.380803346633911 170.70321655273438
Loss :  1.5954948663711548 3.0498881340026855 154.0898895263672
Loss :  1.6364272832870483 3.585735559463501 180.92320251464844
Loss :  1.631316065788269 3.0241987705230713 152.8412628173828
Loss :  1.605622410774231 2.7470057010650635 138.95590209960938
Loss :  1.6417267322540283 2.959467887878418 149.6151123046875
Loss :  1.594447374343872 2.9942281246185303 151.3058624267578
Loss :  1.6677188873291016 3.4853808879852295 175.936767578125
Loss :  1.602005958557129 3.1373918056488037 158.47158813476562
Loss :  1.584101676940918 2.8105239868164062 142.1103057861328
Loss :  1.600724458694458 3.0702803134918213 155.11474609375
Loss :  1.6738252639770508 3.661792516708374 184.76345825195312
  batch 60 loss: 1.6738252639770508, 3.661792516708374, 184.76345825195312
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.5832924842834473 3.2977499961853027 166.47079467773438
Loss :  1.6154389381408691 3.8199145793914795 192.6111602783203
Loss :  1.5949944257736206 2.6372873783111572 133.45936584472656
Loss :  1.5812939405441284 2.987548589706421 150.95872497558594
Loss :  1.5582242012023926 2.239013671875 113.5089111328125
Loss :  1.6011676788330078 3.805109977722168 191.85665893554688
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.6165153980255127 3.7126729488372803 187.2501678466797
Loss :  1.6110836267471313 3.7112655639648438 187.1743621826172
Loss :  1.6214524507522583 3.6074905395507812 181.99598693847656
Total LOSS train 156.44395458514873 valid 187.06929397583008
CE LOSS train 1.6041594725388748 valid 0.4053631126880646
Contrastive LOSS train 3.0967959037193884 valid 0.9018726348876953
EPOCH 104:
Loss :  1.6329079866409302 2.9767801761627197 150.47190856933594
Loss :  1.6459859609603882 3.4581544399261475 174.55369567871094
Loss :  1.6129473447799683 2.8629212379455566 144.75900268554688
Loss :  1.6188291311264038 3.321571111679077 167.6973876953125
Loss :  1.6393355131149292 3.180415630340576 160.6601104736328
Loss :  1.5973658561706543 2.8579628467559814 144.49551391601562
Loss :  1.6389639377593994 3.4576125144958496 174.51959228515625
Loss :  1.61154043674469 3.694777727127075 186.35043334960938
Loss :  1.5994242429733276 2.9430270195007324 148.7507781982422
Loss :  1.6399234533309937 2.835893392562866 143.43458557128906
Loss :  1.5931764841079712 3.022331953048706 152.7097625732422
Loss :  1.5909830331802368 3.3284780979156494 168.014892578125
Loss :  1.5906187295913696 3.0714590549468994 155.16357421875
Loss :  1.5994168519973755 2.952829599380493 149.2408905029297
Loss :  1.6568280458450317 3.5375518798828125 178.534423828125
Loss :  1.6527270078659058 2.8395440578460693 143.6299285888672
Loss :  1.5836563110351562 3.0124504566192627 152.2061767578125
Loss :  1.615437626838684 2.770033597946167 140.1171112060547
Loss :  1.5815941095352173 2.6975433826446533 136.45875549316406
Loss :  1.6498315334320068 3.3319664001464844 168.24815368652344
  batch 20 loss: 1.6498315334320068, 3.3319664001464844, 168.24815368652344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.613499641418457 3.170675754547119 160.14727783203125
Loss :  1.5821869373321533 3.075824022293091 155.37338256835938
Loss :  1.6003726720809937 2.598527669906616 131.52674865722656
Loss :  1.621356725692749 2.730436086654663 138.14315795898438
Loss :  1.6522232294082642 2.680272102355957 135.66583251953125
Loss :  1.603063941001892 2.711203098297119 137.1632080078125
Loss :  1.6143790483474731 2.947859764099121 149.0073699951172
Loss :  1.6071817874908447 2.8076694011688232 141.9906463623047
Loss :  1.5495164394378662 2.921771287918091 147.63807678222656
Loss :  1.6456081867218018 3.3692078590393066 170.1060028076172
Loss :  1.5484505891799927 2.8557302951812744 144.3349609375
Loss :  1.627787470817566 2.841613531112671 143.70846557617188
Loss :  1.6013221740722656 2.717207193374634 137.46168518066406
Loss :  1.5987695455551147 3.22647762298584 162.9226531982422
Loss :  1.5564804077148438 3.4358460903167725 173.34878540039062
Loss :  1.5751471519470215 3.3409974575042725 168.62501525878906
Loss :  1.5743714570999146 3.1379404067993164 158.4713897705078
Loss :  1.6441094875335693 2.4905765056610107 126.17293548583984
Loss :  1.6505272388458252 3.0626635551452637 154.78370666503906
Loss :  1.6611989736557007 2.5512351989746094 129.22296142578125
  batch 40 loss: 1.6611989736557007, 2.5512351989746094, 129.22296142578125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.615364670753479 2.930007219314575 148.11573791503906
Loss :  1.5991617441177368 2.7778799533843994 140.4931640625
Loss :  1.5884751081466675 4.472826957702637 225.2298126220703
Loss :  1.6013678312301636 3.3610000610351562 169.6513671875
Loss :  1.5801005363464355 3.2138962745666504 162.27490234375
Loss :  1.6132330894470215 2.988257884979248 151.026123046875
Loss :  1.6491209268569946 2.840468168258667 143.6725311279297
Loss :  1.5973472595214844 2.9610164165496826 149.64817810058594
Loss :  1.6680127382278442 2.813739538192749 142.35499572753906
Loss :  1.6052528619766235 3.1021194458007812 156.7112274169922
Loss :  1.6440541744232178 3.0399420261383057 153.6411590576172
Loss :  1.638137936592102 3.1444172859191895 158.8590087890625
Loss :  1.6141774654388428 2.998584747314453 151.5434112548828
Loss :  1.6473006010055542 2.66228985786438 134.76177978515625
Loss :  1.6024969816207886 3.0428178310394287 153.74337768554688
Loss :  1.6712111234664917 3.663004159927368 184.8214111328125
Loss :  1.6101996898651123 2.9642841815948486 149.82440185546875
Loss :  1.5932953357696533 3.286733388900757 165.92996215820312
Loss :  1.6088790893554688 3.012054681777954 152.21160888671875
Loss :  1.6776247024536133 2.602627754211426 131.80902099609375
  batch 60 loss: 1.6776247024536133, 2.602627754211426, 131.80902099609375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.5914487838745117 2.523589611053467 127.77092742919922
Loss :  1.6211819648742676 2.489818811416626 126.11212921142578
Loss :  1.6003810167312622 3.4726788997650146 175.2343292236328
Loss :  1.5876511335372925 2.8428685665130615 143.7310791015625
Loss :  1.5654170513153076 2.2733707427978516 115.23394775390625
Loss :  1.6064953804016113 3.632760763168335 183.24452209472656
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.621252417564392 3.550441265106201 179.143310546875
Loss :  1.616684079170227 3.651761293411255 184.20474243164062
Loss :  1.6247186660766602 3.6000168323516846 181.6255645751953
Total LOSS train 152.61902360182543 valid 182.05453491210938
CE LOSS train 1.613383700297429 valid 0.40617966651916504
Contrastive LOSS train 3.0201128299419695 valid 0.9000042080879211
EPOCH 105:
Loss :  1.6390615701675415 2.7913107872009277 141.20460510253906
Loss :  1.6517221927642822 3.7242727279663086 187.8653564453125
Loss :  1.6204564571380615 3.606520652770996 181.9464874267578
Loss :  1.6259208917617798 4.553735733032227 229.3126983642578
Loss :  1.646044373512268 2.7617714405059814 139.734619140625
Loss :  1.6066884994506836 3.3597488403320312 169.59413146972656
Loss :  1.6492730379104614 3.451197624206543 174.2091522216797
Loss :  1.6225368976593018 2.9286928176879883 148.05718994140625
Loss :  1.6115723848342896 3.4131622314453125 172.26968383789062
Loss :  1.6474066972732544 3.470574378967285 175.17611694335938
Loss :  1.6009974479675293 3.654393434524536 184.32066345214844
Loss :  1.59615159034729 3.231656789779663 163.17898559570312
Loss :  1.5951015949249268 2.7760307788848877 140.3966522216797
Loss :  1.6030385494232178 3.2217907905578613 162.6925811767578
Loss :  1.6614642143249512 3.1201164722442627 157.66729736328125
Loss :  1.6563050746917725 2.9972214698791504 151.5173797607422
Loss :  1.5883467197418213 3.3888955116271973 171.0331268310547
Loss :  1.618875503540039 3.123246669769287 157.78121948242188
Loss :  1.5827455520629883 3.0075619220733643 151.96084594726562
Loss :  1.6499451398849487 3.0295114517211914 153.12551879882812
  batch 20 loss: 1.6499451398849487, 3.0295114517211914, 153.12551879882812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6136493682861328 3.027971029281616 153.01219177246094
Loss :  1.582405686378479 2.8368563652038574 143.42523193359375
Loss :  1.6014196872711182 2.9217684268951416 147.68984985351562
Loss :  1.6235567331314087 3.503857135772705 176.81640625
Loss :  1.6543887853622437 3.331688404083252 168.2388153076172
Loss :  1.606966257095337 3.3328988552093506 168.2519073486328
Loss :  1.6188585758209229 2.8957176208496094 146.40475463867188
Loss :  1.613391399383545 3.318500280380249 167.5384063720703
Loss :  1.554797649383545 2.96793270111084 149.95143127441406
Loss :  1.6478397846221924 3.3216891288757324 167.7322998046875
Loss :  1.5513627529144287 3.1270103454589844 157.90188598632812
Loss :  1.6277419328689575 4.217747211456299 212.51510620117188
Loss :  1.600200891494751 2.7867655754089355 140.9384765625
Loss :  1.5974185466766357 2.8676986694335938 144.9823455810547
Loss :  1.5534313917160034 3.115705966949463 157.33872985839844
Loss :  1.5727556943893433 3.1350901126861572 158.32725524902344
Loss :  1.5717835426330566 3.0480453968048096 153.9740447998047
Loss :  1.6415899991989136 3.527653694152832 178.02427673339844
Loss :  1.6483298540115356 3.1699836254119873 160.1475067138672
Loss :  1.6576929092407227 3.0920298099517822 156.25918579101562
  batch 40 loss: 1.6576929092407227, 3.0920298099517822, 156.25918579101562
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.6099791526794434 3.578404188156128 180.5301971435547
Loss :  1.5919673442840576 3.2180535793304443 162.49464416503906
Loss :  1.578701138496399 2.8006529808044434 141.61135864257812
Loss :  1.5944983959197998 2.8527486324310303 144.23193359375
Loss :  1.5708602666854858 3.3433380126953125 168.73776245117188
Loss :  1.6061490774154663 3.0446276664733887 153.83753967285156
Loss :  1.6444967985153198 3.1709091663360596 160.18995666503906
Loss :  1.5900065898895264 3.1058225631713867 156.88113403320312
Loss :  1.6638702154159546 3.419072389602661 172.6174774169922
Loss :  1.5976969003677368 3.095228433609009 156.359130859375
Loss :  1.634372353553772 3.0118248462677 152.22561645507812
Loss :  1.63077974319458 3.030508279800415 153.1562042236328
Loss :  1.6063989400863647 2.869039297103882 145.05836486816406
Loss :  1.6404554843902588 3.4388186931610107 173.58139038085938
Loss :  1.5911797285079956 2.987443447113037 150.96336364746094
Loss :  1.6669777631759644 3.0536656379699707 154.3502655029297
Loss :  1.6024291515350342 2.884296417236328 145.81724548339844
Loss :  1.5851901769638062 2.8119053840637207 142.1804656982422
Loss :  1.6016095876693726 3.296196222305298 166.4114227294922
Loss :  1.6732903718948364 3.011040449142456 152.22531127929688
  batch 60 loss: 1.6732903718948364, 3.011040449142456, 152.22531127929688
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.5839444398880005 3.3195414543151855 167.56100463867188
Loss :  1.6152873039245605 2.968998432159424 150.06520080566406
Loss :  1.5944373607635498 2.4974851608276367 126.46869659423828
Loss :  1.5786269903182983 2.7779362201690674 140.47543334960938
Loss :  1.556046485900879 2.558703899383545 129.49124145507812
Loss :  1.5985162258148193 3.9677822589874268 199.9876251220703
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.6137477159500122 3.970215320587158 200.12452697753906
Loss :  1.6075383424758911 3.8253607749938965 192.87557983398438
Loss :  1.618611454963684 3.800567150115967 191.64695739746094
Total LOSS train 159.53902740478514 valid 196.15867233276367
CE LOSS train 1.6126536552722637 valid 0.404652863740921
Contrastive LOSS train 3.158527451295119 valid 0.9501417875289917
EPOCH 106:
Loss :  1.632743239402771 3.220400333404541 162.6527557373047
Loss :  1.6455049514770508 3.440295457839966 173.6602783203125
Loss :  1.6117416620254517 2.975611686706543 150.39231872558594
Loss :  1.6186332702636719 3.486886501312256 175.9629669189453
Loss :  1.637393593788147 3.185269594192505 160.90086364746094
Loss :  1.5964621305465698 3.797416925430298 191.46731567382812
Loss :  1.6339457035064697 3.154223680496216 159.34512329101562
Loss :  1.6064590215682983 3.212944746017456 162.25369262695312
Loss :  1.593265175819397 3.0403859615325928 153.61256408691406
Loss :  1.6338026523590088 3.729849100112915 188.1262664794922
Loss :  1.5884345769882202 2.9579124450683594 149.48406982421875
Loss :  1.5889778137207031 3.0969364643096924 156.435791015625
Loss :  1.5871156454086304 2.8948707580566406 146.3306427001953
Loss :  1.5968209505081177 3.277042865753174 165.44895935058594
Loss :  1.653183937072754 2.9956557750701904 151.43597412109375
Loss :  1.6538323163986206 3.4761321544647217 175.46044921875
Loss :  1.5838249921798706 3.4888193607330322 176.02479553222656
Loss :  1.617673397064209 2.698181390762329 136.52674865722656
Loss :  1.5832821130752563 2.923175096511841 147.74203491210938
Loss :  1.649652123451233 2.8426451683044434 143.78192138671875
  batch 20 loss: 1.649652123451233, 2.8426451683044434, 143.78192138671875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.6140663623809814 2.417468309402466 122.48748016357422
Loss :  1.5831047296524048 2.7326478958129883 138.2154998779297
Loss :  1.6018034219741821 3.27010440826416 165.10702514648438
Loss :  1.6240917444229126 3.0769386291503906 155.47100830078125
Loss :  1.6548881530761719 3.1939876079559326 161.35427856445312
Loss :  1.6078977584838867 3.1594793796539307 159.5818634033203
Loss :  1.6201329231262207 3.1575281620025635 159.4965362548828
Loss :  1.6119316816329956 3.0319361686706543 153.208740234375
Loss :  1.5578551292419434 2.90828537940979 146.97213745117188
Loss :  1.6490830183029175 3.050368070602417 154.16748046875
Loss :  1.5582576990127563 2.717390775680542 137.42779541015625
Loss :  1.6344083547592163 2.8384804725646973 143.55844116210938
Loss :  1.6098707914352417 2.7401647567749023 138.61810302734375
Loss :  1.6072169542312622 2.9293501377105713 148.07473754882812
Loss :  1.566954255104065 3.2290728092193604 163.02059936523438
Loss :  1.5845800638198853 3.2993900775909424 166.5540771484375
Loss :  1.5831477642059326 3.150966167449951 159.13145446777344
Loss :  1.6495081186294556 2.8262178897857666 142.9604034423828
Loss :  1.65522038936615 2.947453498840332 149.0279083251953
Loss :  1.6644623279571533 2.88462495803833 145.8957061767578
  batch 40 loss: 1.6644623279571533, 2.88462495803833, 145.8957061767578
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.6195518970489502 3.101588726043701 156.69898986816406
Loss :  1.604011058807373 3.27852201461792 165.5301055908203
Loss :  1.5929996967315674 2.6094326972961426 132.06463623046875
Loss :  1.6068456172943115 2.5077614784240723 126.99491882324219
Loss :  1.585605263710022 2.7446787357330322 138.8195343017578
Loss :  1.617831826210022 3.578862190246582 180.56094360351562
Loss :  1.6530449390411377 3.0012879371643066 151.71743774414062
Loss :  1.6026703119277954 2.73091197013855 138.1482696533203
Loss :  1.6702288389205933 3.0192227363586426 152.63136291503906
Loss :  1.6107068061828613 3.0683929920196533 155.0303497314453
Loss :  1.6465083360671997 3.385369300842285 170.91497802734375
Loss :  1.6412075757980347 3.4590532779693604 174.5938720703125
Loss :  1.6173356771469116 3.0377771854400635 153.50619506835938
Loss :  1.6498546600341797 2.6362311840057373 133.46141052246094
Loss :  1.6057249307632446 2.64603328704834 133.9073944091797
Loss :  1.6732892990112305 2.7196900844573975 137.6577911376953
Loss :  1.613761067390442 3.0974233150482178 156.48492431640625
Loss :  1.5974866151809692 3.3205080032348633 167.62289428710938
Loss :  1.614313006401062 3.8461098670959473 193.91981506347656
Loss :  1.682465672492981 2.8876523971557617 146.06507873535156
  batch 60 loss: 1.682465672492981, 2.8876523971557617, 146.06507873535156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.6000992059707642 3.116896629333496 157.44493103027344
Loss :  1.6281698942184448 2.7862391471862793 140.94012451171875
Loss :  1.6098744869232178 2.7155072689056396 137.38523864746094
Loss :  1.5983511209487915 3.0180063247680664 152.49867248535156
Loss :  1.5769882202148438 2.6982038021087646 136.4871826171875
Loss :  1.6083835363388062 3.8118417263031006 192.20046997070312
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6222983598709106 3.9153971672058105 197.39215087890625
Loss :  1.6183085441589355 3.774787425994873 190.357666015625
Loss :  1.6273250579833984 3.762986660003662 189.7766571044922
Total LOSS train 154.28405937781702 valid 192.43173599243164
CE LOSS train 1.6164639527981097 valid 0.4068312644958496
Contrastive LOSS train 3.053351901127742 valid 0.9407466650009155
EPOCH 107:
Loss :  1.6455881595611572 2.6424672603607178 133.76895141601562
Loss :  1.6579780578613281 3.0843842029571533 155.87718200683594
Loss :  1.6273162364959717 2.714933156967163 137.3739776611328
Loss :  1.632743000984192 3.3656387329101562 169.9146728515625
Loss :  1.6510809659957886 2.819899797439575 142.6460723876953
Loss :  1.61217200756073 3.1791152954101562 160.56793212890625
Loss :  1.6483492851257324 3.036653995513916 153.48104858398438
Loss :  1.623258352279663 2.8749325275421143 145.36988830566406
Loss :  1.6104437112808228 2.9498813152313232 149.10450744628906
Loss :  1.6476980447769165 3.0419085025787354 153.74313354492188
Loss :  1.6024680137634277 3.203831434249878 161.79403686523438
Loss :  1.6014810800552368 2.969658374786377 150.08441162109375
Loss :  1.599374771118164 2.9015958309173584 146.67916870117188
Loss :  1.6082676649093628 2.6933324337005615 136.27488708496094
Loss :  1.6634904146194458 2.9401438236236572 148.67068481445312
Loss :  1.6607232093811035 2.921755313873291 147.7484893798828
Loss :  1.5975148677825928 3.5132596492767334 177.260498046875
Loss :  1.6279990673065186 3.05122447013855 154.18922424316406
Loss :  1.5962576866149902 2.9405677318573 148.6246337890625
Loss :  1.659894347190857 2.8398096561431885 143.65037536621094
  batch 20 loss: 1.659894347190857, 2.8398096561431885, 143.65037536621094
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.625898838043213 2.6217808723449707 132.71495056152344
Loss :  1.5952845811843872 3.2010228633880615 161.6464385986328
Loss :  1.6127089262008667 3.7687408924102783 190.04974365234375
Loss :  1.6327873468399048 3.285935640335083 165.9295654296875
Loss :  1.662127137184143 3.43454909324646 173.3895721435547
Loss :  1.6179932355880737 3.06707763671875 154.9718780517578
Loss :  1.6294440031051636 3.1842195987701416 160.84042358398438
Loss :  1.622575044631958 3.1164255142211914 157.44384765625
Loss :  1.5680748224258423 3.036334753036499 153.3848114013672
Loss :  1.6545876264572144 2.9506962299346924 149.18939208984375
Loss :  1.5668439865112305 3.3593478202819824 169.53424072265625
Loss :  1.6400972604751587 3.162379026412964 159.75904846191406
Loss :  1.6158573627471924 2.655437707901001 134.3877410888672
Loss :  1.6132563352584839 2.8566620349884033 144.44635009765625
Loss :  1.572841763496399 3.1286518573760986 158.00543212890625
Loss :  1.5901076793670654 3.212451934814453 162.2126922607422
Loss :  1.5883208513259888 3.287400722503662 165.95835876464844
Loss :  1.6529765129089355 2.808265447616577 142.0662384033203
Loss :  1.659024715423584 2.989823341369629 151.1501922607422
Loss :  1.6684694290161133 3.1274397373199463 158.04046630859375
  batch 40 loss: 1.6684694290161133, 3.1274397373199463, 158.04046630859375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.625266432762146 3.2596752643585205 164.60902404785156
Loss :  1.610546350479126 2.8938000202178955 146.30055236816406
Loss :  1.5995829105377197 2.913778781890869 147.28851318359375
Loss :  1.613106608390808 3.175300359725952 160.3781280517578
Loss :  1.5926176309585571 3.09637451171875 156.41134643554688
Loss :  1.6259299516677856 3.037041664123535 153.47801208496094
Loss :  1.6587250232696533 3.031742572784424 153.245849609375
Loss :  1.607938528060913 2.6774404048919678 135.47996520996094
Loss :  1.6753867864608765 3.026423215866089 152.99655151367188
Loss :  1.613603115081787 3.471229314804077 175.17506408691406
Loss :  1.646489143371582 3.2196998596191406 162.6314697265625
Loss :  1.6433457136154175 3.0941357612609863 156.35012817382812
Loss :  1.6202365159988403 2.8686304092407227 145.0517578125
Loss :  1.6523550748825073 3.1258292198181152 157.94381713867188
Loss :  1.6069691181182861 3.201789140701294 161.69642639160156
Loss :  1.6755506992340088 2.94561505317688 148.956298828125
Loss :  1.6176759004592896 3.4298136234283447 173.1083526611328
Loss :  1.6019872426986694 2.815099000930786 142.35693359375
Loss :  1.618357539176941 3.4681308269500732 175.02490234375
Loss :  1.6849474906921387 2.9965813159942627 151.51402282714844
  batch 60 loss: 1.6849474906921387, 2.9965813159942627, 151.51402282714844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6021915674209595 2.6353747844696045 133.3709259033203
Loss :  1.6309864521026611 3.1173911094665527 157.5005340576172
Loss :  1.6120145320892334 3.016042470932007 152.4141387939453
Loss :  1.5998270511627197 3.1446521282196045 158.83242797851562
Loss :  1.578408122062683 2.5983240604400635 131.49461364746094
Loss :  1.6138200759887695 3.9568138122558594 199.4545135498047
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.6282378435134888 3.923872709274292 197.82186889648438
Loss :  1.625266194343567 3.845306158065796 193.89056396484375
Loss :  1.631930947303772 3.981685161590576 200.7161865234375
Total LOSS train 154.08546025202824 valid 197.97078323364258
CE LOSS train 1.6242680292863112 valid 0.407982736825943
Contrastive LOSS train 3.049223863161527 valid 0.995421290397644
EPOCH 108:
Loss :  1.6472759246826172 2.727139711380005 138.00425720214844
Loss :  1.659590482711792 3.4540512561798096 174.36215209960938
Loss :  1.629565715789795 3.2537224292755127 164.31568908691406
Loss :  1.633859395980835 3.0647759437561035 154.87266540527344
Loss :  1.6519824266433716 3.1180789470672607 157.55592346191406
Loss :  1.615182638168335 2.5721662044525146 130.22349548339844
Loss :  1.6496952772140503 3.146559953689575 158.97769165039062
Loss :  1.625394582748413 3.5644032955169678 179.84556579589844
Loss :  1.6131377220153809 3.6331610679626465 183.2711944580078
Loss :  1.6498264074325562 3.005134105682373 151.90652465820312
Loss :  1.6051862239837646 3.1600019931793213 159.60528564453125
Loss :  1.6028449535369873 2.9040064811706543 146.80316162109375
Loss :  1.6004247665405273 3.404716968536377 171.83627319335938
Loss :  1.6085187196731567 2.8161699771881104 142.41702270507812
Loss :  1.661942481994629 3.3220505714416504 167.76446533203125
Loss :  1.6603306531906128 3.3597023487091064 169.64544677734375
Loss :  1.5961556434631348 3.2023279666900635 161.71255493164062
Loss :  1.627162218093872 2.98282790184021 150.7685546875
Loss :  1.5960050821304321 2.7754580974578857 140.36891174316406
Loss :  1.6602915525436401 2.83087420463562 143.20401000976562
  batch 20 loss: 1.6602915525436401, 2.83087420463562, 143.20401000976562
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6280914545059204 4.1117753982543945 207.2168731689453
Loss :  1.600301742553711 3.821049213409424 192.6527557373047
Loss :  1.619962453842163 3.557316541671753 179.4857940673828
Loss :  1.6395576000213623 3.5228800773620605 177.78355407714844
Loss :  1.6685402393341064 3.574885845184326 180.41282653808594
Loss :  1.6236162185668945 3.738522529602051 188.54974365234375
Loss :  1.6351724863052368 3.9990315437316895 201.58676147460938
Loss :  1.626275658607483 3.1684958934783936 160.0510711669922
Loss :  1.5705797672271729 3.401076078414917 171.6243896484375
Loss :  1.6571334600448608 3.382319927215576 170.77313232421875
Loss :  1.5661299228668213 3.44936203956604 174.03424072265625
Loss :  1.6387470960617065 3.3702597618103027 170.1517333984375
Loss :  1.612037181854248 3.4161810874938965 172.42108154296875
Loss :  1.6082723140716553 2.935147762298584 148.36566162109375
Loss :  1.566158652305603 3.575939655303955 180.36314392089844
Loss :  1.582802653312683 2.93637752532959 148.4016876220703
Loss :  1.5803488492965698 3.2076454162597656 161.96261596679688
Loss :  1.6474469900131226 3.650334119796753 184.16415405273438
Loss :  1.6538232564926147 3.557997226715088 179.55368041992188
Loss :  1.6628608703613281 3.0844075679779053 155.88323974609375
  batch 40 loss: 1.6628608703613281, 3.0844075679779053, 155.88323974609375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.6174935102462769 3.408846378326416 172.059814453125
Loss :  1.602728009223938 3.1430742740631104 158.75643920898438
Loss :  1.5909391641616821 3.0839855670928955 155.79022216796875
Loss :  1.6042604446411133 2.9481735229492188 149.012939453125
Loss :  1.5818469524383545 3.1809747219085693 160.63058471679688
Loss :  1.6150693893432617 3.2439568042755127 163.8129119873047
Loss :  1.6498523950576782 2.767204761505127 140.01010131835938
Loss :  1.5967533588409424 2.7573635578155518 139.46493530273438
Loss :  1.6678400039672852 2.790310859680176 141.1833953857422
Loss :  1.603682518005371 3.0488712787628174 154.0472412109375
Loss :  1.6412055492401123 3.1843461990356445 160.8585205078125
Loss :  1.6368684768676758 2.8476946353912354 144.0216064453125
Loss :  1.6119410991668701 3.0577304363250732 154.4984588623047
Loss :  1.6453651189804077 2.9989020824432373 151.59046936035156
Loss :  1.5994514226913452 3.1533749103546143 159.26820373535156
Loss :  1.6696301698684692 3.3145437240600586 167.39682006835938
Loss :  1.6071752309799194 3.045240879058838 153.8692169189453
Loss :  1.5901715755462646 3.046762704849243 153.9282989501953
Loss :  1.6064307689666748 3.128633975982666 158.0381317138672
Loss :  1.677601933479309 3.130307197570801 158.19296264648438
  batch 60 loss: 1.677601933479309, 3.130307197570801, 158.19296264648438
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5898598432540894 3.0191802978515625 152.5488739013672
Loss :  1.6193664073944092 2.6367077827453613 133.4547576904297
Loss :  1.5998107194900513 2.9641199111938477 149.80580139160156
Loss :  1.586247444152832 3.0412402153015137 153.64825439453125
Loss :  1.5659102201461792 2.361239433288574 119.62788391113281
Loss :  1.6033579111099243 3.425468683242798 172.87680053710938
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6177884340286255 3.3756015300750732 170.39785766601562
Loss :  1.6132889986038208 3.3560712337493896 169.41685485839844
Loss :  1.6216869354248047 3.2000656127929688 161.62496948242188
Total LOSS train 160.8987051156851 valid 168.57912063598633
CE LOSS train 1.6209189763435952 valid 0.40542173385620117
Contrastive LOSS train 3.185555703823383 valid 0.8000164031982422
Saved best model. Old loss 172.12041091918945 and new best loss 168.57912063598633
EPOCH 109:
Loss :  1.6393043994903564 2.9680821895599365 150.0434112548828
Loss :  1.653067708015442 3.132392406463623 158.27267456054688
Loss :  1.6216118335723877 3.0133554935455322 152.2893829345703
Loss :  1.628131628036499 3.00305438041687 151.78085327148438
Loss :  1.6484596729278564 2.8636906147003174 144.83297729492188
Loss :  1.607957124710083 2.9472548961639404 148.970703125
Loss :  1.647472620010376 3.307140588760376 167.00450134277344
Loss :  1.6199979782104492 3.226789951324463 162.95948791503906
Loss :  1.60740327835083 2.7966134548187256 141.43807983398438
Loss :  1.6446754932403564 2.9400134086608887 148.6453399658203
Loss :  1.5983482599258423 3.53635311126709 178.41600036621094
Loss :  1.5971873998641968 3.1897189617156982 161.0831298828125
Loss :  1.5950725078582764 2.8967888355255127 146.4345245361328
Loss :  1.6043040752410889 3.0142970085144043 152.31915283203125
Loss :  1.6610956192016602 3.2056171894073486 161.94195556640625
Loss :  1.6588236093521118 2.966291904449463 149.97341918945312
Loss :  1.5943180322647095 3.0332489013671875 153.2567596435547
Loss :  1.624886393547058 2.8374791145324707 143.4988555908203
Loss :  1.5925793647766113 3.1491434574127197 159.04974365234375
Loss :  1.6564428806304932 2.9156813621520996 147.44052124023438
  batch 20 loss: 1.6564428806304932, 2.9156813621520996, 147.44052124023438
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6222118139266968 2.9103124141693115 147.13783264160156
Loss :  1.591361165046692 3.610687017440796 182.12570190429688
Loss :  1.6087144613265991 3.1120119094848633 157.20932006835938
Loss :  1.6288434267044067 3.323554754257202 167.80657958984375
Loss :  1.658275842666626 3.4489593505859375 174.1062469482422
Loss :  1.612819790840149 3.1493418216705322 159.0799102783203
Loss :  1.6248444318771362 3.330463409423828 168.14801025390625
Loss :  1.615771770477295 3.0287857055664062 153.0550537109375
Loss :  1.563085913658142 2.9205329418182373 147.5897216796875
Loss :  1.649693489074707 3.1693193912506104 160.11566162109375
Loss :  1.5619691610336304 3.433290958404541 173.2265167236328
Loss :  1.6343042850494385 3.575392246246338 180.40391540527344
Loss :  1.6096246242523193 3.0793991088867188 155.57957458496094
Loss :  1.6072287559509277 3.0037267208099365 151.79356384277344
Loss :  1.567891240119934 3.4745614528656006 175.29595947265625
Loss :  1.585782766342163 3.0469751358032227 153.93453979492188
Loss :  1.5848636627197266 2.653780698776245 134.27391052246094
Loss :  1.6518601179122925 2.6091549396514893 132.10960388183594
Loss :  1.6572567224502563 2.8485946655273438 144.0869903564453
Loss :  1.6672992706298828 3.210627794265747 162.1986846923828
  batch 40 loss: 1.6672992706298828, 3.210627794265747, 162.1986846923828
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.622726559638977 3.3878467082977295 171.0150604248047
Loss :  1.6065338850021362 3.216763973236084 162.44473266601562
Loss :  1.595292568206787 2.6402199268341064 133.60629272460938
Loss :  1.6094236373901367 2.6390020847320557 133.5595245361328
Loss :  1.5878791809082031 2.4799890518188477 125.58732604980469
Loss :  1.6200727224349976 3.2273755073547363 162.9888458251953
Loss :  1.6534947156906128 2.9275553226470947 148.03125
Loss :  1.6038228273391724 2.9030277729034424 146.7552032470703
Loss :  1.6730962991714478 3.012582540512085 152.30221557617188
Loss :  1.61247718334198 2.7109861373901367 137.16177368164062
Loss :  1.647692322731018 2.9135046005249023 147.3229217529297
Loss :  1.6439613103866577 3.6733884811401367 185.31338500976562
Loss :  1.6220227479934692 2.465125799179077 124.87831115722656
Loss :  1.6528375148773193 2.813605785369873 142.33311462402344
Loss :  1.6090232133865356 2.6206650733947754 132.64227294921875
Loss :  1.674533724784851 3.9490129947662354 199.12518310546875
Loss :  1.615115761756897 3.3917365074157715 171.20193481445312
Loss :  1.5990867614746094 3.094648838043213 156.33152770996094
Loss :  1.6147849559783936 3.31845760345459 167.5376739501953
Loss :  1.6813303232192993 3.103248119354248 156.84373474121094
  batch 60 loss: 1.6813303232192993, 3.103248119354248, 156.84373474121094
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.5991493463516235 3.214773416519165 162.33782958984375
Loss :  1.6273901462554932 3.2141215801239014 162.33348083496094
Loss :  1.6087360382080078 2.4801712036132812 125.61729431152344
Loss :  1.5960257053375244 2.7156789302825928 137.37997436523438
Loss :  1.5761973857879639 2.07602596282959 105.37749481201172
Loss :  1.6141403913497925 3.851768970489502 194.2025909423828
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.6290181875228882 3.854170083999634 194.3375244140625
Loss :  1.6237486600875854 3.765450954437256 189.89630126953125
Loss :  1.631761908531189 3.657309055328369 184.49720764160156
Total LOSS train 153.98395585280198 valid 190.73340606689453
CE LOSS train 1.6208853758298434 valid 0.40794047713279724
Contrastive LOSS train 3.047261439836942 valid 0.9143272638320923
EPOCH 110:
Loss :  1.6446226835250854 2.863943338394165 144.841796875
Loss :  1.6558148860931396 3.0820436477661133 155.75799560546875
Loss :  1.625859260559082 2.903698205947876 146.81076049804688
Loss :  1.631054162979126 2.83646821975708 143.4544677734375
Loss :  1.6493074893951416 2.4293465614318848 123.11663818359375
Loss :  1.6125203371047974 2.982905864715576 150.7578125
Loss :  1.6498547792434692 3.1100234985351562 157.15103149414062
Loss :  1.6256009340286255 2.746655225753784 138.95835876464844
Loss :  1.6148054599761963 3.001966953277588 151.71315002441406
Loss :  1.6519780158996582 2.8661162853240967 144.95779418945312
Loss :  1.6067687273025513 3.084813356399536 155.84742736816406
Loss :  1.6063812971115112 3.51971435546875 177.59210205078125
Loss :  1.6049020290374756 3.0501739978790283 154.1136016845703
Loss :  1.6130614280700684 3.3644425868988037 169.83518981933594
Loss :  1.6675958633422852 3.2010397911071777 161.71958923339844
Loss :  1.6638457775115967 2.5806539058685303 130.69654846191406
Loss :  1.6023401021957397 3.410862922668457 172.14549255371094
Loss :  1.6312882900238037 2.9199917316436768 147.63087463378906
Loss :  1.6030558347702026 2.8396859169006348 143.5873565673828
Loss :  1.663360595703125 3.094085693359375 156.36764526367188
  batch 20 loss: 1.663360595703125, 3.094085693359375, 156.36764526367188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.630893349647522 2.7590129375457764 139.58154296875
Loss :  1.600909948348999 3.832580804824829 193.22994995117188
Loss :  1.6175388097763062 2.9633448123931885 149.78477478027344
Loss :  1.6343066692352295 3.162538766860962 159.76124572753906
Loss :  1.6616014242172241 3.3778860569000244 170.555908203125
Loss :  1.616454005241394 2.944016456604004 148.81727600097656
Loss :  1.6261764764785767 3.4015390872955322 171.703125
Loss :  1.6191563606262207 3.0720040798187256 155.2193603515625
Loss :  1.567594051361084 3.333735227584839 168.25436401367188
Loss :  1.656331181526184 3.413332939147949 172.32296752929688
Loss :  1.568642258644104 3.8151848316192627 192.32789611816406
Loss :  1.6407583951950073 2.7433958053588867 138.810546875
Loss :  1.6175247430801392 2.672755002975464 135.25527954101562
Loss :  1.6158831119537354 2.9999940395355225 151.61558532714844
Loss :  1.5774730443954468 3.027324676513672 152.94371032714844
Loss :  1.5937696695327759 3.44582200050354 173.88487243652344
Loss :  1.592038631439209 3.0029830932617188 151.74119567871094
Loss :  1.6548925638198853 3.051936626434326 154.25172424316406
Loss :  1.6606663465499878 3.06329345703125 154.82533264160156
Loss :  1.6697921752929688 3.0909297466278076 156.21627807617188
  batch 40 loss: 1.6697921752929688, 3.0909297466278076, 156.21627807617188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.626265048980713 3.2068238258361816 161.9674530029297
Loss :  1.6112474203109741 3.5103750228881836 177.1300048828125
Loss :  1.6001207828521729 3.1574666500091553 159.4734649658203
Loss :  1.6126575469970703 3.4272005558013916 172.97268676757812
Loss :  1.5910285711288452 2.8523080348968506 144.20643615722656
Loss :  1.6221082210540771 2.6513469219207764 134.189453125
Loss :  1.6559258699417114 2.5406548976898193 128.6886749267578
Loss :  1.6075793504714966 2.453773021697998 124.29623413085938
Loss :  1.673488974571228 3.977600574493408 200.55352783203125
Loss :  1.6150826215744019 3.009932518005371 152.11170959472656
Loss :  1.6514482498168945 2.8082597255706787 142.06442260742188
Loss :  1.6440370082855225 2.95094895362854 149.19149780273438
Loss :  1.6192933320999146 2.701636791229248 136.7011260986328
Loss :  1.650064468383789 3.0452303886413574 153.91159057617188
Loss :  1.6052945852279663 3.0531163215637207 154.26112365722656
Loss :  1.6716883182525635 3.700338840484619 186.68862915039062
Loss :  1.6116259098052979 2.800006628036499 141.61196899414062
Loss :  1.5962004661560059 2.7967352867126465 141.43296813964844
Loss :  1.612079381942749 2.964738130569458 149.84898376464844
Loss :  1.6795341968536377 2.981612205505371 150.76014709472656
  batch 60 loss: 1.6795341968536377, 2.981612205505371, 150.76014709472656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.59537672996521 2.8180253505706787 142.49664306640625
Loss :  1.6241317987442017 2.595231533050537 131.38571166992188
Loss :  1.6038838624954224 3.327997922897339 168.0037841796875
Loss :  1.592228889465332 2.9900717735290527 151.09580993652344
Loss :  1.5715115070343018 2.7907614707946777 141.10958862304688
Loss :  1.6027348041534424 4.27974796295166 215.5901336669922
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.616999626159668 4.339242458343506 218.57913208007812
Loss :  1.6114643812179565 4.206790447235107 211.95098876953125
Loss :  1.6187654733657837 4.305680274963379 216.90277099609375
Total LOSS train 154.06634169358475 valid 215.75575637817383
CE LOSS train 1.6244665274253258 valid 0.4046913683414459
Contrastive LOSS train 3.048837474676279 valid 1.0764200687408447
EPOCH 111:
Loss :  1.6426739692687988 2.8947877883911133 146.38206481933594
Loss :  1.6547703742980957 3.067044258117676 155.00698852539062
Loss :  1.6238012313842773 2.8213934898376465 142.69346618652344
Loss :  1.6303246021270752 2.701457977294922 136.70323181152344
Loss :  1.6490039825439453 3.6557319164276123 184.4355926513672
Loss :  1.6111539602279663 2.5396642684936523 128.59437561035156
Loss :  1.6485254764556885 3.4601454734802246 174.6558074951172
Loss :  1.621756672859192 2.8368759155273438 143.46554565429688
Loss :  1.6088768243789673 3.231994152069092 163.2085723876953
Loss :  1.6455473899841309 3.0272319316864014 153.0071563720703
Loss :  1.599625587463379 2.929237127304077 148.0614776611328
Loss :  1.5994738340377808 2.879711389541626 145.5850372314453
Loss :  1.596643090248108 2.8490073680877686 144.04701232910156
Loss :  1.6060417890548706 4.500336647033691 226.6228790283203
Loss :  1.6603621244430542 3.0714569091796875 155.2332000732422
Loss :  1.6581790447235107 3.2803053855895996 165.67344665527344
Loss :  1.593157172203064 3.581270933151245 180.65670776367188
Loss :  1.6229051351547241 2.776233673095703 140.43458557128906
Loss :  1.5925447940826416 2.965979814529419 149.89154052734375
Loss :  1.6555546522140503 3.0932328701019287 156.3171844482422
  batch 20 loss: 1.6555546522140503, 3.0932328701019287, 156.3171844482422
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.621850848197937 4.393434047698975 221.29356384277344
Loss :  1.593004822731018 3.380643367767334 170.6251678466797
Loss :  1.6119064092636108 3.301300287246704 166.6769256591797
Loss :  1.6326152086257935 3.48764705657959 176.0149688720703
Loss :  1.6614973545074463 2.9434659481048584 148.8347930908203
Loss :  1.617606520652771 3.081186056137085 155.67689514160156
Loss :  1.6285964250564575 3.0276803970336914 153.0126190185547
Loss :  1.623658537864685 2.784597873687744 140.85354614257812
Loss :  1.572558879852295 2.997241973876953 151.4346466064453
Loss :  1.659206509590149 2.808631420135498 142.0907745361328
Loss :  1.5736489295959473 3.382031202316284 170.6752166748047
Loss :  1.6434744596481323 3.0727391242980957 155.2804412841797
Loss :  1.6201832294464111 2.937164545059204 148.47840881347656
Loss :  1.6178659200668335 2.6834230422973633 135.7890167236328
Loss :  1.5799169540405273 2.9872615337371826 150.9429931640625
Loss :  1.596220850944519 4.03839635848999 203.5160369873047
Loss :  1.5949546098709106 2.7580370903015137 139.49681091308594
Loss :  1.6571208238601685 2.627376079559326 133.0259246826172
Loss :  1.6618667840957642 3.4761838912963867 175.47105407714844
Loss :  1.6700714826583862 3.365062952041626 169.9232177734375
  batch 40 loss: 1.6700714826583862, 3.365062952041626, 169.9232177734375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6273940801620483 3.2102210521698 162.13844299316406
Loss :  1.613306999206543 3.015315532684326 152.37908935546875
Loss :  1.603290319442749 3.333282947540283 168.26744079589844
Loss :  1.6170892715454102 3.102623701095581 156.74827575683594
Loss :  1.5963361263275146 2.896716833114624 146.4321746826172
Loss :  1.6249725818634033 3.152782678604126 159.26409912109375
Loss :  1.6592360734939575 3.8328793048858643 193.30320739746094
Loss :  1.6127315759658813 4.0901594161987305 206.12069702148438
Loss :  1.6746965646743774 3.1260416507720947 157.97677612304688
Loss :  1.61960768699646 3.2596633434295654 164.602783203125
Loss :  1.6535601615905762 3.255464792251587 164.4268035888672
Loss :  1.6487081050872803 3.094254493713379 156.36143493652344
Loss :  1.6275529861450195 3.0807385444641113 155.6644744873047
Loss :  1.6571264266967773 2.789475440979004 141.13088989257812
Loss :  1.618687391281128 3.8286242485046387 193.04989624023438
Loss :  1.679082989692688 2.9153831005096436 147.44822692871094
Loss :  1.6229462623596191 2.939248561859131 148.5853729248047
Loss :  1.6078951358795166 2.888040781021118 146.0099334716797
Loss :  1.621809482574463 3.3587303161621094 169.55833435058594
Loss :  1.68471360206604 3.1183860301971436 157.60400390625
  batch 60 loss: 1.68471360206604, 3.1183860301971436, 157.60400390625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6055983304977417 3.339451313018799 168.57815551757812
Loss :  1.6338518857955933 3.396615743637085 171.46463012695312
Loss :  1.612794041633606 2.769519090652466 140.0887451171875
Loss :  1.6014610528945923 3.565803289413452 179.89161682128906
Loss :  1.5799484252929688 2.7879350185394287 140.9766845703125
Loss :  1.617631196975708 3.646939277648926 183.964599609375
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6312862634658813 3.507272243499756 176.99490356445312
Loss :  1.6275677680969238 3.478421688079834 175.54864501953125
Loss :  1.6327866315841675 3.4057977199554443 171.92266845703125
Total LOSS train 159.9670935997596 valid 177.10770416259766
CE LOSS train 1.6255868434906007 valid 0.40819665789604187
Contrastive LOSS train 3.166830165569599 valid 0.8514494299888611
EPOCH 112:
Loss :  1.647912621498108 2.600355386734009 131.6656951904297
Loss :  1.6595605611801147 3.319694995880127 167.64431762695312
Loss :  1.6281497478485107 2.990492582321167 151.15277099609375
Loss :  1.6342576742172241 3.5215964317321777 177.71408081054688
Loss :  1.6508593559265137 3.0357577800750732 153.43875122070312
Loss :  1.6153039932250977 3.2384214401245117 163.536376953125
Loss :  1.6510952711105347 2.9254300594329834 147.92259216308594
Loss :  1.6276919841766357 2.9465036392211914 148.95286560058594
Loss :  1.617936372756958 2.8879873752593994 146.01730346679688
Loss :  1.6541714668273926 2.46763277053833 125.03581237792969
Loss :  1.610231637954712 3.3657422065734863 169.8973388671875
Loss :  1.6093089580535889 3.185513734817505 160.88499450683594
Loss :  1.6079200506210327 3.1463165283203125 158.9237518310547
Loss :  1.6162019968032837 3.509577989578247 177.0950927734375
Loss :  1.6692192554473877 2.7280516624450684 138.07180786132812
Loss :  1.6653107404708862 2.9129221439361572 147.31141662597656
Loss :  1.6055841445922852 2.50919246673584 127.0652084350586
Loss :  1.6335853338241577 3.350407600402832 169.1539764404297
Loss :  1.6044691801071167 3.2628228664398193 164.74560546875
Loss :  1.6641559600830078 3.090287685394287 156.1785430908203
  batch 20 loss: 1.6641559600830078, 3.090287685394287, 156.1785430908203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.6320687532424927 2.9128165245056152 147.27288818359375
Loss :  1.6044533252716064 4.282318592071533 215.7203826904297
Loss :  1.6224050521850586 3.6156861782073975 182.40670776367188
Loss :  1.6417778730392456 2.935257911682129 148.40467834472656
Loss :  1.6686694622039795 3.043034553527832 153.82040405273438
Loss :  1.6264489889144897 3.128237009048462 158.03829956054688
Loss :  1.6363825798034668 3.0086801052093506 152.0703887939453
Loss :  1.6315805912017822 3.0233864784240723 152.8009033203125
Loss :  1.5799674987792969 3.484034776687622 175.78170776367188
Loss :  1.6624929904937744 3.244917869567871 163.90838623046875
Loss :  1.5778799057006836 3.552976131439209 179.2266845703125
Loss :  1.6459953784942627 3.241893768310547 163.74069213867188
Loss :  1.6254862546920776 3.6772754192352295 185.4892578125
Loss :  1.62384831905365 3.1182427406311035 157.53599548339844
Loss :  1.5882854461669922 3.179847002029419 160.5806427001953
Loss :  1.6041210889816284 3.671053647994995 185.1568145751953
Loss :  1.6025575399398804 3.214402675628662 162.32269287109375
Loss :  1.6620867252349854 3.5221006870269775 177.7671356201172
Loss :  1.6683852672576904 2.890120267868042 146.1743927001953
Loss :  1.677383542060852 3.09689998626709 156.5223846435547
  batch 40 loss: 1.677383542060852, 3.09689998626709, 156.5223846435547
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.637898325920105 3.259290933609009 164.60244750976562
Loss :  1.6231648921966553 4.411886692047119 222.21749877929688
Loss :  1.6134523153305054 2.6862919330596924 135.92803955078125
Loss :  1.6239216327667236 2.706881046295166 136.9679718017578
Loss :  1.6054670810699463 2.5001370906829834 126.61231994628906
Loss :  1.6349681615829468 3.440758466720581 173.67288208007812
Loss :  1.665418028831482 3.368455410003662 170.08819580078125
Loss :  1.62114679813385 3.114253520965576 157.3338165283203
Loss :  1.6799838542938232 4.131292819976807 208.24461364746094
Loss :  1.6254756450653076 2.602971076965332 131.77403259277344
Loss :  1.6578598022460938 3.227349281311035 163.02532958984375
Loss :  1.6524226665496826 2.6744117736816406 135.3730010986328
Loss :  1.630798578262329 2.6506283283233643 134.16221618652344
Loss :  1.659455418586731 3.277376890182495 165.52830505371094
Loss :  1.6199548244476318 3.2994799613952637 166.5939483642578
Loss :  1.6792607307434082 3.2394373416900635 163.651123046875
Loss :  1.6235997676849365 3.0791330337524414 155.5802459716797
Loss :  1.6089286804199219 3.3031980991363525 166.7688446044922
Loss :  1.6230450868606567 3.875598192214966 195.4029541015625
Loss :  1.6858209371566772 3.8902742862701416 196.1995391845703
  batch 60 loss: 1.6858209371566772, 3.8902742862701416, 196.1995391845703
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.6084877252578735 3.312344551086426 167.22572326660156
Loss :  1.6364322900772095 3.0301833152770996 153.14559936523438
Loss :  1.6187678575515747 3.0479328632354736 154.01541137695312
Loss :  1.6089773178100586 3.0938382148742676 156.30088806152344
Loss :  1.5901389122009277 2.7860240936279297 140.89134216308594
Loss :  1.6269479990005493 4.146712303161621 208.9625701904297
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6409298181533813 4.200448036193848 211.663330078125
Loss :  1.6364142894744873 3.9666357040405273 199.96820068359375
Loss :  1.6428133249282837 4.173680782318115 210.32684326171875
Total LOSS train 160.74547741229716 valid 207.7302360534668
CE LOSS train 1.6321546187767615 valid 0.4107033312320709
Contrastive LOSS train 3.1822664444263165 valid 1.0434201955795288
EPOCH 113:
Loss :  1.652876853942871 3.366349220275879 169.9703369140625
Loss :  1.663572072982788 3.0200085639953613 152.66400146484375
Loss :  1.6356931924819946 3.1909849643707275 161.18495178222656
Loss :  1.6405059099197388 3.0528066158294678 154.2808380126953
Loss :  1.6576765775680542 2.856031656265259 144.45925903320312
Loss :  1.6215250492095947 2.836024284362793 143.4227294921875
Loss :  1.6567645072937012 4.601792812347412 231.7464141845703
Loss :  1.6321290731430054 3.030266046524048 153.1454315185547
Loss :  1.6217198371887207 3.419001817703247 172.57180786132812
Loss :  1.6556485891342163 3.2877426147460938 166.04278564453125
Loss :  1.6126292943954468 3.1508851051330566 159.15687561035156
Loss :  1.6118794679641724 3.3087728023529053 167.05052185058594
Loss :  1.6104083061218262 3.2932446002960205 166.27264404296875
Loss :  1.6180859804153442 4.296216011047363 216.42889404296875
Loss :  1.670037865638733 3.1239006519317627 157.86508178710938
Loss :  1.6665608882904053 3.0196917057037354 152.65115356445312
Loss :  1.604770541191101 3.1083872318267822 157.0241241455078
Loss :  1.6329432725906372 3.216193675994873 162.442626953125
Loss :  1.603336215019226 2.9918923377990723 151.1979522705078
Loss :  1.6618634462356567 2.6987433433532715 136.59902954101562
  batch 20 loss: 1.6618634462356567, 2.6987433433532715, 136.59902954101562
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.630353331565857 2.596040964126587 131.43240356445312
Loss :  1.6031628847122192 2.9650044441223145 149.8533935546875
Loss :  1.6210496425628662 2.93068265914917 148.15518188476562
Loss :  1.6394721269607544 3.600811243057251 181.68002319335938
Loss :  1.6663089990615845 3.2385928630828857 163.595947265625
Loss :  1.6243928670883179 3.2513184547424316 164.1903076171875
Loss :  1.6336016654968262 3.073903799057007 155.32879638671875
Loss :  1.6289942264556885 2.963273763656616 149.7926788330078
Loss :  1.5782263278961182 2.854609966278076 144.30873107910156
Loss :  1.6615400314331055 3.1240086555480957 157.8619842529297
Loss :  1.5782458782196045 3.1605560779571533 159.60604858398438
Loss :  1.6461055278778076 3.171837329864502 160.23797607421875
Loss :  1.6228723526000977 3.015139579772949 152.37985229492188
Loss :  1.6207772493362427 2.7335000038146973 138.2957763671875
Loss :  1.5840438604354858 3.3985166549682617 171.50987243652344
Loss :  1.6003483533859253 2.7548370361328125 139.34219360351562
Loss :  1.5995149612426758 2.6225976943969727 132.72940063476562
Loss :  1.6596599817276 2.9943251609802246 151.37591552734375
Loss :  1.6660869121551514 3.1536972522735596 159.3509521484375
Loss :  1.6743500232696533 3.0267088413238525 153.00979614257812
  batch 40 loss: 1.6743500232696533, 3.0267088413238525, 153.00979614257812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.6339184045791626 3.234196424484253 163.34373474121094
Loss :  1.620053768157959 2.72286319732666 137.76321411132812
Loss :  1.6101031303405762 3.2182934284210205 162.5247802734375
Loss :  1.622714638710022 3.2351925373077393 163.38233947753906
Loss :  1.6047412157058716 2.6825873851776123 135.73410034179688
Loss :  1.6342527866363525 4.13137674331665 208.2030792236328
Loss :  1.665629267692566 3.3417952060699463 168.75540161132812
Loss :  1.621635913848877 3.750183582305908 189.13082885742188
Loss :  1.681938648223877 3.0473170280456543 154.04779052734375
Loss :  1.6281622648239136 2.769155263900757 140.08592224121094
Loss :  1.6609737873077393 2.869239091873169 145.12294006347656
Loss :  1.6556012630462646 4.127115249633789 208.0113525390625
Loss :  1.63531494140625 3.009986400604248 152.13462829589844
Loss :  1.6634173393249512 3.365621328353882 169.94448852539062
Loss :  1.6266868114471436 3.9726169109344482 200.25753784179688
Loss :  1.6849470138549805 3.2950150966644287 166.43569946289062
Loss :  1.6321110725402832 3.3922324180603027 171.2437286376953
Loss :  1.6183133125305176 3.4918205738067627 176.2093505859375
Loss :  1.6312305927276611 3.4179129600524902 172.52687072753906
Loss :  1.6895945072174072 3.0990028381347656 156.6397247314453
  batch 60 loss: 1.6895945072174072, 3.0990028381347656, 156.6397247314453
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.6139720678329468 3.072856903076172 155.25682067871094
Loss :  1.6413438320159912 3.110802173614502 157.18145751953125
Loss :  1.6234290599822998 3.131141424179077 158.1804962158203
Loss :  1.6128103733062744 3.2929835319519043 166.26197814941406
Loss :  1.5935184955596924 2.7339038848876953 138.28871154785156
Loss :  1.6383358240127563 4.347612380981445 219.01895141601562
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6518739461898804 4.339162826538086 218.61001586914062
Loss :  1.6489397287368774 4.248995304107666 214.0987091064453
Loss :  1.6533632278442383 4.180323123931885 210.66952514648438
Total LOSS train 160.87510258601262 valid 215.59930038452148
CE LOSS train 1.6334792100466216 valid 0.41334080696105957
Contrastive LOSS train 3.1848324628976674 valid 1.0450807809829712
EPOCH 114:
Loss :  1.6574105024337769 3.4797165393829346 175.6432342529297
Loss :  1.667822241783142 3.2135188579559326 162.34376525878906
Loss :  1.6397956609725952 2.82816481590271 143.04803466796875
Loss :  1.6442257165908813 3.0506443977355957 154.17645263671875
Loss :  1.6616802215576172 2.7883799076080322 141.08067321777344
Loss :  1.6253817081451416 3.1671266555786133 159.98171997070312
Loss :  1.6607283353805542 3.6369872093200684 183.5100860595703
Loss :  1.6368943452835083 3.7657053470611572 189.9221649169922
Loss :  1.6273272037506104 3.0099666118621826 152.12567138671875
Loss :  1.660951018333435 2.9967987537384033 151.50088500976562
Loss :  1.6203749179840088 3.4040467739105225 171.8227081298828
Loss :  1.619681477546692 3.2034523487091064 161.79229736328125
Loss :  1.6184386014938354 2.760389804840088 139.63792419433594
Loss :  1.6256215572357178 3.1652143001556396 159.88633728027344
Loss :  1.6761143207550049 3.1595547199249268 159.6538543701172
Loss :  1.6720777750015259 2.999812364578247 151.66268920898438
Loss :  1.614441156387329 3.2361972332000732 163.42430114746094
Loss :  1.6395630836486816 3.1784770488739014 160.56341552734375
Loss :  1.6097038984298706 2.7294113636016846 138.0802764892578
Loss :  1.665825366973877 2.7071533203125 137.02349853515625
  batch 20 loss: 1.665825366973877, 2.7071533203125, 137.02349853515625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6339675188064575 3.334075927734375 168.3377685546875
Loss :  1.6069053411483765 2.917536497116089 147.48373413085938
Loss :  1.624129295349121 3.7022500038146973 186.73663330078125
Loss :  1.6420495510101318 2.980811357498169 150.6826171875
Loss :  1.6689774990081787 3.3241734504699707 167.87765502929688
Loss :  1.628783941268921 3.5971665382385254 181.4871063232422
Loss :  1.6389973163604736 3.72683048248291 187.9805145263672
Loss :  1.6349395513534546 2.9723074436187744 150.25030517578125
Loss :  1.5868779420852661 2.9288015365600586 148.02696228027344
Loss :  1.6671440601348877 3.7311956882476807 188.2269287109375
Loss :  1.5880502462387085 3.0228078365325928 152.7284393310547
Loss :  1.6533597707748413 2.9996957778930664 151.63815307617188
Loss :  1.6327592134475708 2.944261312484741 148.8458251953125
Loss :  1.6316109895706177 3.0361106395721436 153.4371337890625
Loss :  1.598105549812317 3.524932622909546 177.8447265625
Loss :  1.6124904155731201 3.836791515350342 193.45205688476562
Loss :  1.610626220703125 2.749431848526001 139.08221435546875
Loss :  1.6673492193222046 2.7509584426879883 139.21527099609375
Loss :  1.6714017391204834 3.139641284942627 158.65347290039062
Loss :  1.6793893575668335 3.0401017665863037 153.68446350097656
  batch 40 loss: 1.6793893575668335, 3.0401017665863037, 153.68446350097656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.640913963317871 3.4921982288360596 176.25082397460938
Loss :  1.6274518966674805 3.149400234222412 159.09747314453125
Loss :  1.6187933683395386 3.196120023727417 161.42478942871094
Loss :  1.6299697160720825 3.3032615184783936 166.7930450439453
Loss :  1.6102312803268433 2.934009075164795 148.31068420410156
Loss :  1.6378096342086792 3.173123359680176 160.29397583007812
Loss :  1.6672478914260864 3.040897846221924 153.71214294433594
Loss :  1.6226719617843628 2.8158340454101562 142.41436767578125
Loss :  1.6816786527633667 3.1407580375671387 158.71957397460938
Loss :  1.627855896949768 2.9330203533172607 148.27886962890625
Loss :  1.6596176624298096 3.145993232727051 158.95928955078125
Loss :  1.655505895614624 2.544684410095215 128.8897247314453
Loss :  1.6354472637176514 2.824817180633545 142.87631225585938
Loss :  1.6634517908096313 3.1666109561920166 159.99400329589844
Loss :  1.626421570777893 3.135490894317627 158.40097045898438
Loss :  1.6840672492980957 3.7109503746032715 187.23158264160156
Loss :  1.6320621967315674 3.4097163677215576 172.11788940429688
Loss :  1.6187700033187866 3.7065892219543457 186.9482421875
Loss :  1.632510781288147 3.20772647857666 162.01882934570312
Loss :  1.6923216581344604 3.046354055404663 154.01002502441406
  batch 60 loss: 1.6923216581344604, 3.046354055404663, 154.01002502441406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.6188217401504517 3.577387809753418 180.4882049560547
Loss :  1.6446220874786377 3.2567319869995117 164.48121643066406
Loss :  1.6257611513137817 2.812595844268799 142.25555419921875
Loss :  1.6138757467269897 2.927284002304077 147.9780731201172
Loss :  1.5940055847167969 2.761251211166382 139.6565704345703
Loss :  1.6342705488204956 4.384607315063477 220.86463928222656
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6470603942871094 4.393075466156006 221.30084228515625
Loss :  1.6427507400512695 4.225692272186279 212.92735290527344
Loss :  1.6490747928619385 4.290555953979492 216.1768798828125
Total LOSS train 159.44852623572717 valid 217.8174285888672
CE LOSS train 1.6382132383493277 valid 0.4122686982154846
Contrastive LOSS train 3.1562062630286585 valid 1.072638988494873
EPOCH 115:
Loss :  1.6572297811508179 2.9369683265686035 148.50564575195312
Loss :  1.6680856943130493 3.0929765701293945 156.31692504882812
Loss :  1.6393249034881592 3.0674023628234863 155.0094451904297
Loss :  1.6445789337158203 2.6256206035614014 132.9256134033203
Loss :  1.6613519191741943 2.6114280223846436 132.2327423095703
Loss :  1.6271133422851562 2.6605687141418457 134.65554809570312
Loss :  1.661820411682129 3.174567222595215 160.3901824951172
Loss :  1.639769196510315 3.4688971042633057 175.08462524414062
Loss :  1.6304796934127808 3.2142891883850098 162.34494018554688
Loss :  1.6633021831512451 2.779047727584839 140.61569213867188
Loss :  1.6238138675689697 2.9745595455169678 150.35179138183594
Loss :  1.6220163106918335 2.867506742477417 144.99734497070312
Loss :  1.6212809085845947 3.36452579498291 169.84756469726562
Loss :  1.628430724143982 2.6041572093963623 131.83628845214844
Loss :  1.6786547899246216 3.6115312576293945 182.25521850585938
Loss :  1.673445701599121 3.6304502487182617 183.19595336914062
Loss :  1.61679208278656 3.437068462371826 173.47021484375
Loss :  1.6423345804214478 2.8229410648345947 142.7893829345703
Loss :  1.613350749015808 3.4158895015716553 172.4078369140625
Loss :  1.6697391271591187 3.648724317550659 184.10595703125
  batch 20 loss: 1.6697391271591187, 3.648724317550659, 184.10595703125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6396152973175049 2.5373423099517822 128.50672912597656
Loss :  1.6137338876724243 3.050083875656128 154.11793518066406
Loss :  1.6296833753585815 3.411959409713745 172.2276611328125
Loss :  1.646068811416626 3.0250508785247803 152.89862060546875
Loss :  1.672343134880066 3.412306547164917 172.2876739501953
Loss :  1.6324496269226074 4.118588447570801 207.56187438964844
Loss :  1.6419423818588257 3.0144741535186768 152.3656463623047
Loss :  1.6377065181732178 3.424062490463257 172.84083557128906
Loss :  1.5892506837844849 2.883639097213745 145.77120971679688
Loss :  1.6694557666778564 3.1833837032318115 160.83863830566406
Loss :  1.5906068086624146 3.220757484436035 162.62847900390625
Loss :  1.6566951274871826 3.408024311065674 172.0579071044922
Loss :  1.631983995437622 3.235137939453125 163.38888549804688
Loss :  1.6294715404510498 3.3919899463653564 171.22897338867188
Loss :  1.5941836833953857 3.69002366065979 186.09536743164062
Loss :  1.6081230640411377 3.704582452774048 186.83724975585938
Loss :  1.6059274673461914 3.095905303955078 156.40118408203125
Loss :  1.6643179655075073 3.084606170654297 155.89463806152344
Loss :  1.6680253744125366 2.949645519256592 149.15029907226562
Loss :  1.677047610282898 3.0110857486724854 152.23133850097656
  batch 40 loss: 1.677047610282898, 3.0110857486724854, 152.23133850097656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6383934020996094 3.3278393745422363 168.03036499023438
Loss :  1.6238064765930176 2.76491379737854 139.8695068359375
Loss :  1.6146793365478516 2.8392603397369385 143.57769775390625
Loss :  1.626871109008789 2.874678611755371 145.36080932617188
Loss :  1.6077240705490112 3.7613258361816406 189.67401123046875
Loss :  1.6368181705474854 2.9898681640625 151.13023376464844
Loss :  1.6668106317520142 2.762424945831299 139.78805541992188
Loss :  1.6220053434371948 2.892348289489746 146.2394256591797
Loss :  1.6833631992340088 3.08309006690979 155.83787536621094
Loss :  1.6287986040115356 2.8101859092712402 142.13809204101562
Loss :  1.6622658967971802 2.8851287364959717 145.918701171875
Loss :  1.6570262908935547 3.0935511589050293 156.33457946777344
Loss :  1.6371835470199585 2.784874200820923 140.88088989257812
Loss :  1.6648343801498413 3.5369889736175537 178.5142822265625
Loss :  1.6281245946884155 3.152606725692749 159.2584686279297
Loss :  1.685555100440979 3.107292890548706 157.05020141601562
Loss :  1.633482575416565 3.0998470783233643 156.62583923339844
Loss :  1.6198689937591553 3.2100672721862793 162.12322998046875
Loss :  1.6327677965164185 3.1367273330688477 158.46913146972656
Loss :  1.6915587186813354 4.124238014221191 207.90345764160156
  batch 60 loss: 1.6915587186813354, 4.124238014221191, 207.90345764160156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.6166642904281616 4.163692951202393 209.8013153076172
Loss :  1.6420561075210571 2.759280204772949 139.60606384277344
Loss :  1.6244641542434692 3.689028024673462 186.07586669921875
Loss :  1.6125339269638062 3.82245135307312 192.735107421875
Loss :  1.5946002006530762 2.8724985122680664 145.2195281982422
Loss :  1.6206918954849243 3.8428544998168945 193.763427734375
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6335175037384033 3.858114719390869 194.53924560546875
Loss :  1.6292909383773804 3.6881983280181885 186.03919982910156
Loss :  1.6353960037231445 3.5868330001831055 180.97703552246094
Total LOSS train 160.41281221829928 valid 188.82972717285156
CE LOSS train 1.6389815990741436 valid 0.40884900093078613
Contrastive LOSS train 3.1754765877356896 valid 0.8967082500457764
EPOCH 116:
Loss :  1.6582398414611816 2.4789609909057617 125.60629272460938
Loss :  1.6696844100952148 3.299898624420166 166.66461181640625
Loss :  1.64128577709198 2.9017839431762695 146.73048400878906
Loss :  1.6458860635757446 3.8146653175354004 192.379150390625
Loss :  1.66278874874115 3.1261582374572754 157.970703125
Loss :  1.6270990371704102 3.2101738452911377 162.13580322265625
Loss :  1.6618564128875732 3.793985605239868 191.3611297607422
Loss :  1.6381404399871826 4.301361560821533 216.7062225341797
Loss :  1.6282215118408203 2.981239080429077 150.69017028808594
Loss :  1.6616485118865967 3.0305540561676025 153.18936157226562
Loss :  1.6200616359710693 3.5280299186706543 178.02154541015625
Loss :  1.6182729005813599 3.6677675247192383 185.00665283203125
Loss :  1.6170717477798462 3.1170814037323 157.47113037109375
Loss :  1.6239413022994995 3.0770325660705566 155.47557067871094
Loss :  1.6747815608978271 3.3786020278930664 170.60487365722656
Loss :  1.6685487031936646 2.643871307373047 133.86212158203125
Loss :  1.6094032526016235 3.006303071975708 151.924560546875
Loss :  1.6356441974639893 3.5958712100982666 181.42921447753906
Loss :  1.6062573194503784 2.7414286136627197 138.6776885986328
Loss :  1.664176344871521 3.456648588180542 174.49659729003906
  batch 20 loss: 1.664176344871521, 3.456648588180542, 174.49659729003906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  1.6305536031723022 2.716567277908325 137.45892333984375
Loss :  1.6029812097549438 2.7337849140167236 138.29222106933594
Loss :  1.620678424835205 3.5747873783111572 180.36004638671875
Loss :  1.638920545578003 2.9262771606445312 147.95277404785156
Loss :  1.666496753692627 3.6442084312438965 183.87692260742188
Loss :  1.6251213550567627 3.8267273902893066 192.96148681640625
Loss :  1.6350008249282837 2.983182191848755 150.79409790039062
Loss :  1.631170392036438 3.738760471343994 188.56918334960938
Loss :  1.5821478366851807 2.8972461223602295 146.4444580078125
Loss :  1.6654151678085327 3.1223056316375732 157.78070068359375
Loss :  1.584684133529663 3.128481388092041 158.00875854492188
Loss :  1.6521997451782227 3.511387348175049 177.22157287597656
Loss :  1.6285791397094727 3.1056225299835205 156.9097137451172
Loss :  1.6273726224899292 2.857882022857666 144.52146911621094
Loss :  1.5934540033340454 3.192086696624756 161.1977996826172
Loss :  1.6084797382354736 3.280937433242798 165.6553497314453
Loss :  1.607793927192688 4.459605693817139 224.58807373046875
Loss :  1.665888786315918 2.8723502159118652 145.2834014892578
Loss :  1.6713933944702148 2.784332513809204 140.8880157470703
Loss :  1.680363416671753 2.8622050285339355 144.7906036376953
  batch 40 loss: 1.680363416671753, 2.8622050285339355, 144.7906036376953
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  1.6435155868530273 3.3223443031311035 167.76072692871094
Loss :  1.630427598953247 3.644533395767212 183.8571014404297
Loss :  1.6224334239959717 2.7768630981445312 140.46559143066406
Loss :  1.6331733465194702 2.79502010345459 141.38418579101562
Loss :  1.6152998208999634 2.747394561767578 138.98501586914062
Loss :  1.6432735919952393 2.8580007553100586 144.54331970214844
Loss :  1.6733288764953613 3.211583375930786 162.2524871826172
Loss :  1.6317170858383179 2.44553542137146 123.90848541259766
Loss :  1.6878119707107544 3.1578354835510254 159.57957458496094
Loss :  1.6348484754562378 2.671687602996826 135.21922302246094
Loss :  1.6660083532333374 2.9122369289398193 147.27786254882812
Loss :  1.6606011390686035 3.1372034549713135 158.52076721191406
Loss :  1.6414895057678223 2.857253074645996 144.504150390625
Loss :  1.6672881841659546 2.679330348968506 135.63380432128906
Loss :  1.62952721118927 3.1028778553009033 156.77342224121094
Loss :  1.6860179901123047 2.8558170795440674 144.47686767578125
Loss :  1.6345633268356323 2.7519490718841553 139.23202514648438
Loss :  1.6217247247695923 2.897608757019043 146.5021514892578
Loss :  1.6354838609695435 3.474926710128784 175.38182067871094
Loss :  1.6932367086410522 2.7011234760284424 136.74940490722656
  batch 60 loss: 1.6932367086410522, 2.7011234760284424, 136.74940490722656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.6224056482315063 2.653090715408325 134.27694702148438
Loss :  1.6487159729003906 3.070784568786621 155.1879425048828
Loss :  1.6317886114120483 3.433838367462158 173.32371520996094
Loss :  1.6207667589187622 3.3369140625 168.4664764404297
Loss :  1.6020586490631104 2.626833915710449 132.94375610351562
Loss :  1.6363637447357178 4.241442680358887 213.70849609375
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6478856801986694 4.199026107788086 211.5991973876953
Loss :  1.6444162130355835 4.183655261993408 210.82717895507812
Loss :  1.6505191326141357 3.980034589767456 200.65223693847656
Total LOSS train 158.17178896390476 valid 209.19677734375
CE LOSS train 1.6389109409772433 valid 0.41262978315353394
Contrastive LOSS train 3.130657566510714 valid 0.995008647441864
EPOCH 117:
Loss :  1.6628303527832031 2.9866840839385986 150.9970245361328
Loss :  1.6739236116409302 3.0368807315826416 153.51795959472656
Loss :  1.6449393033981323 3.4809935092926025 175.6946258544922
Loss :  1.6495344638824463 2.9674489498138428 150.02198791503906
Loss :  1.665541172027588 2.972259283065796 150.27850341796875
Loss :  1.6316187381744385 2.972604274749756 150.2618408203125
Loss :  1.665048360824585 2.9399218559265137 148.66114807128906
Loss :  1.6429672241210938 2.773270606994629 140.30648803710938
Loss :  1.6334363222122192 3.5609889030456543 179.68287658691406
Loss :  1.6656771898269653 3.1435036659240723 158.8408660888672
Loss :  1.624370813369751 3.729590654373169 188.10391235351562
Loss :  1.6230254173278809 3.0025150775909424 151.748779296875
Loss :  1.6199729442596436 2.783825397491455 140.81124877929688
Loss :  1.627266526222229 2.652465343475342 134.2505340576172
Loss :  1.6764577627182007 2.9358668327331543 148.46978759765625
Loss :  1.6722331047058105 2.9218666553497314 147.76556396484375
Loss :  1.6176172494888306 3.511023759841919 177.16880798339844
Loss :  1.64291250705719 3.3739302158355713 170.3394317626953
Loss :  1.6161022186279297 2.7510416507720947 139.16818237304688
Loss :  1.672452688217163 2.6607160568237305 134.708251953125
  batch 20 loss: 1.672452688217163, 2.6607160568237305, 134.708251953125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.642377257347107 2.4601526260375977 124.65000915527344
Loss :  1.617165446281433 2.843778371810913 143.80609130859375
Loss :  1.6343501806259155 2.899143695831299 146.59153747558594
Loss :  1.6494109630584717 2.6967875957489014 136.48880004882812
Loss :  1.6746082305908203 2.6711342334747314 135.2313232421875
Loss :  1.635940670967102 2.572453498840332 130.2586212158203
Loss :  1.6450145244598389 2.7472379207611084 139.0069122314453
Loss :  1.6414159536361694 3.4971718788146973 176.50001525878906
Loss :  1.5940147638320923 3.32301664352417 167.74484252929688
Loss :  1.6706300973892212 4.03206205368042 203.2737274169922
Loss :  1.5925836563110352 3.5192275047302246 177.55397033691406
Loss :  1.6566128730773926 3.4185147285461426 172.5823516845703
Loss :  1.6357064247131348 3.1268725395202637 157.97933959960938
Loss :  1.6328681707382202 3.9709885120391846 200.1822967529297
Loss :  1.5998032093048096 3.259072780609131 164.55345153808594
Loss :  1.6127564907073975 3.1936697959899902 161.29624938964844
Loss :  1.611835241317749 2.6004340648651123 131.63352966308594
Loss :  1.6683611869812012 3.0406441688537598 153.70057678222656
Loss :  1.6722629070281982 2.715426206588745 137.44357299804688
Loss :  1.6810332536697388 2.8830151557922363 145.831787109375
  batch 40 loss: 1.6810332536697388, 2.8830151557922363, 145.831787109375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.6432031393051147 2.7999420166015625 141.6403045654297
Loss :  1.6289875507354736 2.5393998622894287 128.59898376464844
Loss :  1.6197869777679443 2.6985740661621094 136.54849243164062
Loss :  1.6317253112792969 2.940417766571045 148.65261840820312
Loss :  1.6133949756622314 2.5878477096557617 131.00576782226562
Loss :  1.6409600973129272 3.0533697605133057 154.3094482421875
Loss :  1.6703598499298096 3.667473316192627 185.04403686523438
Loss :  1.6284068822860718 2.944546937942505 148.85574340820312
Loss :  1.6853488683700562 2.916714906692505 147.52108764648438
Loss :  1.6353448629379272 2.93876314163208 148.57350158691406
Loss :  1.6667490005493164 2.911323308944702 147.23291015625
Loss :  1.6618417501449585 2.69140625 136.23214721679688
Loss :  1.6429057121276855 3.3960864543914795 171.4472198486328
Loss :  1.6696109771728516 3.606113910675049 181.97531127929688
Loss :  1.635769009590149 2.73702073097229 138.48681640625
Loss :  1.689527153968811 3.156418800354004 159.51046752929688
Loss :  1.6392204761505127 2.6874847412109375 136.01345825195312
Loss :  1.6257697343826294 2.4823219776153564 125.74186706542969
Loss :  1.6374717950820923 3.7471978664398193 188.9973602294922
Loss :  1.6931616067886353 2.681769371032715 135.78163146972656
  batch 60 loss: 1.6931616067886353, 2.681769371032715, 135.78163146972656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.6238046884536743 3.005798578262329 151.9137420654297
Loss :  1.6497220993041992 3.578742265701294 180.5868377685547
Loss :  1.6329329013824463 2.929171323776245 148.0915069580078
Loss :  1.6232118606567383 2.607741594314575 132.0102996826172
Loss :  1.6059141159057617 3.9230663776397705 197.7592315673828
Loss :  1.6319442987442017 3.4802231788635254 175.64309692382812
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6440685987472534 3.481735944747925 175.73086547851562
Loss :  1.6400701999664307 3.4813451766967773 175.70733642578125
Loss :  1.6459460258483887 3.3486733436584473 169.07962036132812
Total LOSS train 153.8405787541316 valid 174.04022979736328
CE LOSS train 1.642950936464163 valid 0.41148650646209717
Contrastive LOSS train 3.0439525310809796 valid 0.8371683359146118
EPOCH 118:
Loss :  1.6635280847549438 2.9227864742279053 147.8028564453125
Loss :  1.673824667930603 3.1474571228027344 159.04669189453125
Loss :  1.6482930183410645 3.2935893535614014 166.32777404785156
Loss :  1.6532782316207886 2.777019500732422 140.50425720214844
Loss :  1.6696722507476807 2.4862797260284424 125.98365783691406
Loss :  1.638270378112793 2.365817070007324 119.92912292480469
Loss :  1.6705387830734253 2.6128134727478027 132.3112030029297
Loss :  1.6491498947143555 3.8431131839752197 193.8048095703125
Loss :  1.6399215459823608 3.9756417274475098 200.42201232910156
Loss :  1.6700042486190796 3.2042107582092285 161.8805389404297
Loss :  1.6339157819747925 3.363367795944214 169.80230712890625
Loss :  1.633400559425354 3.0262558460235596 152.94619750976562
Loss :  1.6329524517059326 2.938817024230957 148.5738067626953
Loss :  1.6398993730545044 2.6817750930786133 135.72865295410156
Loss :  1.685872197151184 3.3189098834991455 167.63136291503906
Loss :  1.679519534111023 2.6947710514068604 136.41807556152344
Loss :  1.6272321939468384 3.487147808074951 175.984619140625
Loss :  1.6512458324432373 2.6732470989227295 135.3135986328125
Loss :  1.6252855062484741 2.5419816970825195 128.724365234375
Loss :  1.6785869598388672 3.164238691329956 159.89051818847656
  batch 20 loss: 1.6785869598388672, 3.164238691329956, 159.89051818847656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.649595022201538 2.860826253890991 144.69090270996094
Loss :  1.6264092922210693 2.7313668727874756 138.1947479248047
Loss :  1.6422884464263916 3.1665358543395996 159.96908569335938
Loss :  1.6572989225387573 3.255100727081299 164.41233825683594
Loss :  1.6805315017700195 3.3080286979675293 167.0819549560547
Loss :  1.6445640325546265 2.7232513427734375 137.80712890625
Loss :  1.6529474258422852 3.578645944595337 180.5852508544922
Loss :  1.6492983102798462 2.8091723918914795 142.10791015625
Loss :  1.6061630249023438 3.0109381675720215 152.153076171875
Loss :  1.6781673431396484 3.243652820587158 163.86080932617188
Loss :  1.6069355010986328 3.0352883338928223 153.37135314941406
Loss :  1.6654901504516602 3.1424577236175537 158.7883758544922
Loss :  1.6458090543746948 2.6737277507781982 135.3321990966797
Loss :  1.644832730293274 3.254344940185547 164.36209106445312
Loss :  1.614945411682129 2.893481731414795 146.28903198242188
Loss :  1.6275361776351929 2.7668445110321045 139.96975708007812
Loss :  1.626047134399414 2.5457675457000732 128.9144287109375
Loss :  1.6773219108581543 3.251939535140991 164.2742919921875
Loss :  1.6799858808517456 3.0006179809570312 151.7108917236328
Loss :  1.6874666213989258 3.7962849140167236 191.501708984375
  batch 40 loss: 1.6874666213989258, 3.7962849140167236, 191.501708984375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.6519618034362793 3.0081629753112793 152.0601043701172
Loss :  1.6396955251693726 2.901925563812256 146.73597717285156
Loss :  1.632153868675232 2.894745111465454 146.36941528320312
Loss :  1.6422498226165771 3.3232309818267822 167.8037872314453
Loss :  1.625034213066101 2.7869515419006348 140.9726104736328
Loss :  1.6509382724761963 2.9844768047332764 150.87478637695312
Loss :  1.6785070896148682 2.899993419647217 146.6781768798828
Loss :  1.6397508382797241 2.627326726913452 133.00608825683594
Loss :  1.6918483972549438 3.031381130218506 153.2609100341797
Loss :  1.6447287797927856 3.0867910385131836 155.98428344726562
Loss :  1.6723068952560425 3.1735873222351074 160.35166931152344
Loss :  1.6682024002075195 3.1981303691864014 161.57472229003906
Loss :  1.6510692834854126 3.173720121383667 160.33706665039062
Loss :  1.6764705181121826 3.784372091293335 190.8950653076172
Loss :  1.6425312757492065 2.9865071773529053 150.9678955078125
Loss :  1.6941838264465332 3.411015510559082 172.24496459960938
Loss :  1.6471773386001587 3.004645824432373 151.87945556640625
Loss :  1.6337052583694458 2.999136209487915 151.5905303955078
Loss :  1.6451927423477173 3.2962875366210938 166.45956420898438
Loss :  1.6974127292633057 3.396519660949707 171.52340698242188
  batch 60 loss: 1.6974127292633057, 3.396519660949707, 171.52340698242188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.6313154697418213 3.2440593242645264 163.83428955078125
Loss :  1.6557806730270386 3.421536445617676 172.73260498046875
Loss :  1.639559268951416 3.0233802795410156 152.80856323242188
Loss :  1.6308783292770386 3.610896110534668 182.17567443847656
Loss :  1.6143643856048584 3.6073179244995117 181.98025512695312
Loss :  1.6440256834030151 3.857647180557251 194.52638244628906
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.6557514667510986 3.840085506439209 193.66001892089844
Loss :  1.6528786420822144 3.3692338466644287 170.11456298828125
Loss :  1.657146692276001 3.5871517658233643 181.01473999023438
Total LOSS train 155.8385481614333 valid 184.82892608642578
CE LOSS train 1.6511545291313758 valid 0.41428667306900024
Contrastive LOSS train 3.0837478711054875 valid 0.8967879414558411
EPOCH 119:
Loss :  1.6692636013031006 3.9484505653381348 199.091796875
Loss :  1.6771677732467651 3.2598066329956055 164.66749572753906
Loss :  1.651300311088562 2.5805511474609375 130.67886352539062
Loss :  1.654211401939392 2.9006757736206055 146.68798828125
Loss :  1.6683162450790405 3.222858190536499 162.8112335205078
Loss :  1.6370387077331543 2.715942859649658 137.43418884277344
Loss :  1.6673665046691895 3.3625247478485107 169.79360961914062
Loss :  1.6464018821716309 3.056758165359497 154.48431396484375
Loss :  1.6373183727264404 3.055774211883545 154.426025390625
Loss :  1.6670652627944946 2.6994123458862305 136.63768005371094
Loss :  1.6287142038345337 3.1250202655792236 157.8797149658203
Loss :  1.6293286085128784 3.1327273845672607 158.2657012939453
Loss :  1.6269152164459229 2.803460121154785 141.7999267578125
Loss :  1.634466528892517 3.286175012588501 165.94320678710938
Loss :  1.6814409494400024 3.2694685459136963 165.15487670898438
Loss :  1.676853895187378 3.178535223007202 160.60360717773438
Loss :  1.624311923980713 2.6318552494049072 133.21707153320312
Loss :  1.6489378213882446 2.6410603523254395 133.70196533203125
Loss :  1.6233973503112793 2.681013822555542 135.67408752441406
Loss :  1.676406979560852 3.038670301437378 153.60992431640625
  batch 20 loss: 1.676406979560852, 3.038670301437378, 153.60992431640625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.647873878479004 4.422567367553711 222.7762451171875
Loss :  1.6241730451583862 3.0327999591827393 153.26417541503906
Loss :  1.6397466659545898 2.944636344909668 148.87155151367188
Loss :  1.6546528339385986 2.9984681606292725 151.5780487060547
Loss :  1.6778624057769775 2.965625047683716 149.9591064453125
Loss :  1.6397154331207275 2.743028402328491 138.79112243652344
Loss :  1.647783875465393 3.3208436965942383 167.68997192382812
Loss :  1.643079161643982 3.233825206756592 163.33433532714844
Loss :  1.5970488786697388 3.232133626937866 163.20372009277344
Loss :  1.6729124784469604 3.7172305583953857 187.53443908691406
Loss :  1.5985593795776367 2.9921696186065674 151.20703125
Loss :  1.65945303440094 3.181040048599243 160.71145629882812
Loss :  1.639025330543518 3.144561529159546 158.86709594726562
Loss :  1.6374289989471436 3.155029773712158 159.38893127441406
Loss :  1.6066101789474487 3.2950918674468994 166.3612060546875
Loss :  1.6209394931793213 3.265357255935669 164.88880920410156
Loss :  1.6199800968170166 2.9959540367126465 151.4176788330078
Loss :  1.673552393913269 3.193904399871826 161.3687744140625
Loss :  1.6788369417190552 3.215928792953491 162.4752655029297
Loss :  1.6877692937850952 3.4063496589660645 172.00526428222656
  batch 40 loss: 1.6877692937850952, 3.4063496589660645, 172.00526428222656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.652913212776184 3.442889928817749 173.7974090576172
Loss :  1.6406257152557373 3.464134693145752 174.8473663330078
Loss :  1.6322609186172485 3.574948787689209 180.37969970703125
Loss :  1.6436601877212524 3.72845721244812 188.0665283203125
Loss :  1.6256047487258911 3.4907162189483643 176.1614227294922
Loss :  1.6483707427978516 3.27165150642395 165.23095703125
Loss :  1.6786092519760132 3.063141107559204 154.83566284179688
Loss :  1.6382343769073486 3.2878575325012207 166.03111267089844
Loss :  1.6893501281738281 3.12516450881958 157.94757080078125
Loss :  1.6379586458206177 3.2453243732452393 163.9041748046875
Loss :  1.6682440042495728 3.300851345062256 166.7108154296875
Loss :  1.6596096754074097 3.664229393005371 184.87107849121094
Loss :  1.637359380722046 3.0647268295288086 154.8737030029297
Loss :  1.661500334739685 3.1311864852905273 158.2208251953125
Loss :  1.6224186420440674 3.4952659606933594 176.38572692871094
Loss :  1.6781542301177979 3.1284496784210205 158.10064697265625
Loss :  1.622387170791626 3.28605580329895 165.92518615722656
Loss :  1.6063696146011353 2.810760974884033 142.14442443847656
Loss :  1.6186195611953735 3.117021322250366 157.4696807861328
Loss :  1.6817046403884888 2.937997341156006 148.58157348632812
  batch 60 loss: 1.6817046403884888, 2.937997341156006, 148.58157348632812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.6020336151123047 3.213125467300415 162.25831604003906
Loss :  1.6306241750717163 2.8733067512512207 145.2959747314453
Loss :  1.6104538440704346 2.7405989170074463 138.64041137695312
Loss :  1.5987224578857422 3.3573925495147705 169.46835327148438
Loss :  1.5789731740951538 2.874718189239502 145.3148956298828
Loss :  1.620314598083496 4.247601509094238 214.00039672851562
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.6337124109268188 4.2277703285217285 213.02223205566406
Loss :  1.629732370376587 4.072486400604248 205.25404357910156
Loss :  1.6362745761871338 4.084531784057617 205.8628692626953
Total LOSS train 159.90340036245493 valid 209.53488540649414
CE LOSS train 1.643969073662391 valid 0.40906864404678345
Contrastive LOSS train 3.1651886023007907 valid 1.0211329460144043
EPOCH 120:
Loss :  1.6455168724060059 2.8832638263702393 145.8087158203125
Loss :  1.6585549116134644 2.887498617172241 146.03347778320312
Loss :  1.629713773727417 3.0603156089782715 154.64549255371094
Loss :  1.6360057592391968 3.037628650665283 153.51744079589844
Loss :  1.654107689857483 3.0364081859588623 153.47451782226562
Loss :  1.6186538934707642 2.594959259033203 131.36660766601562
Loss :  1.655867099761963 3.284278392791748 165.86978149414062
Loss :  1.6313989162445068 2.775338649749756 140.39833068847656
Loss :  1.6218020915985107 2.6523044109344482 134.2370147705078
Loss :  1.6559642553329468 2.631290912628174 133.2205047607422
Loss :  1.614378571510315 3.7166213989257812 187.44544982910156
Loss :  1.6136934757232666 3.1712770462036133 160.17755126953125
Loss :  1.6121243238449097 3.259530544281006 164.58865356445312
Loss :  1.6203467845916748 3.1790285110473633 160.57177734375
Loss :  1.6734561920166016 2.7989089488983154 141.61891174316406
Loss :  1.6687935590744019 2.789890766143799 141.163330078125
Loss :  1.613219141960144 4.036100387573242 203.41824340820312
Loss :  1.6403276920318604 3.635352373123169 183.407958984375
Loss :  1.611977458000183 3.2141988277435303 162.32192993164062
Loss :  1.6705033779144287 3.1224138736724854 157.79119873046875
  batch 20 loss: 1.6705033779144287, 3.1224138736724854, 157.79119873046875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.6392234563827515 2.992177724838257 151.24810791015625
Loss :  1.6142640113830566 2.976184129714966 150.4234619140625
Loss :  1.6315319538116455 2.829526424407959 143.10784912109375
Loss :  1.6466240882873535 3.537046194076538 178.49893188476562
Loss :  1.6729869842529297 3.2097387313842773 162.15992736816406
Loss :  1.6342854499816895 2.8282134532928467 143.0449676513672
Loss :  1.6422199010849 2.940563917160034 148.67042541503906
Loss :  1.6372288465499878 2.7757339477539062 140.42391967773438
Loss :  1.585300087928772 2.61686635017395 132.42861938476562
Loss :  1.6649534702301025 2.930546283721924 148.1922607421875
Loss :  1.582942008972168 3.8951642513275146 196.34115600585938
Loss :  1.6485363245010376 2.9397807121276855 148.63755798339844
Loss :  1.623935341835022 2.7692453861236572 140.08619689941406
Loss :  1.6215795278549194 3.1589303016662598 159.56809997558594
Loss :  1.5855214595794678 2.9079084396362305 146.98094177246094
Loss :  1.6013120412826538 3.5500917434692383 179.1059112548828
Loss :  1.6001554727554321 2.760838270187378 139.64207458496094
Loss :  1.6604024171829224 2.914849281311035 147.40286254882812
Loss :  1.6652511358261108 2.935598134994507 148.44515991210938
Loss :  1.6753795146942139 2.9229042530059814 147.8206024169922
  batch 40 loss: 1.6753795146942139, 2.9229042530059814, 147.8206024169922
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6365270614624023 3.428720712661743 173.0725555419922
Loss :  1.6233445405960083 3.0980911254882812 156.5279083251953
Loss :  1.6147664785385132 3.2534167766571045 164.2855987548828
Loss :  1.6276332139968872 3.304004430770874 166.82786560058594
Loss :  1.6089859008789062 2.7747368812561035 140.3458251953125
Loss :  1.638494849205017 2.8473141193389893 144.0041961669922
Loss :  1.66876220703125 3.032505512237549 153.29403686523438
Loss :  1.6257801055908203 2.914529323577881 147.3522491455078
Loss :  1.684627652168274 3.353280544281006 169.34866333007812
Loss :  1.63176691532135 3.043158531188965 153.78968811035156
Loss :  1.6628122329711914 3.060485363006592 154.68707275390625
Loss :  1.6557844877243042 3.4049758911132812 171.90457153320312
Loss :  1.635276436805725 3.2178456783294678 162.52755737304688
Loss :  1.661787986755371 3.0982232093811035 156.5729522705078
Loss :  1.6219903230667114 3.0460619926452637 153.9250946044922
Loss :  1.68131422996521 3.0865867137908936 156.01065063476562
Loss :  1.6274319887161255 3.289088249206543 166.0818328857422
Loss :  1.6124992370605469 3.1065914630889893 156.94207763671875
Loss :  1.6260933876037598 3.077005624771118 155.47637939453125
Loss :  1.68694269657135 2.8794736862182617 145.66061401367188
  batch 60 loss: 1.68694269657135, 2.8794736862182617, 145.66061401367188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.6124351024627686 3.2857017517089844 165.8975372314453
Loss :  1.640393853187561 3.314767599105835 167.37876892089844
Loss :  1.6226297616958618 3.1852548122406006 160.8853759765625
Loss :  1.6118196249008179 2.936133623123169 148.4185028076172
Loss :  1.5935111045837402 3.2197399139404297 162.58050537109375
Loss :  1.6354228258132935 4.302763938903809 216.77362060546875
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.6477948427200317 4.207343101501465 212.01495361328125
Loss :  1.6445499658584595 4.124125957489014 207.85084533691406
Loss :  1.6512047052383423 4.082510948181152 205.77674865722656
Total LOSS train 155.80160006009615 valid 210.60404205322266
CE LOSS train 1.6352838571255024 valid 0.41280117630958557
Contrastive LOSS train 3.083326317713811 valid 1.020627737045288
EPOCH 121:
Loss :  1.6564112901687622 3.1311326026916504 158.21304321289062
Loss :  1.6686549186706543 3.328853130340576 168.11131286621094
Loss :  1.6417540311813354 3.4079792499542236 172.04071044921875
Loss :  1.6482079029083252 2.9139411449432373 147.3452606201172
Loss :  1.6655681133270264 3.0286099910736084 153.0960693359375
Loss :  1.6320858001708984 3.141190767288208 158.69161987304688
Loss :  1.6655362844467163 3.2153666019439697 162.43386840820312
Loss :  1.6436923742294312 2.810518741607666 142.1696319580078
Loss :  1.634036898612976 3.829516649246216 193.10986328125
Loss :  1.6664193868637085 2.76155161857605 139.74398803710938
Loss :  1.6267999410629272 3.14523983001709 158.8887939453125
Loss :  1.6255807876586914 4.099424362182617 206.5968017578125
Loss :  1.623624563217163 3.371777057647705 170.2124786376953
Loss :  1.6304301023483276 4.028416156768799 203.05123901367188
Loss :  1.6789567470550537 3.60298228263855 181.82806396484375
Loss :  1.6734027862548828 3.149178981781006 159.13235473632812
Loss :  1.6180225610733032 3.207962989807129 162.01617431640625
Loss :  1.6439810991287231 3.4063026905059814 171.95912170410156
Loss :  1.6124526262283325 3.1657612323760986 159.9005126953125
Loss :  1.6697803735733032 3.04325795173645 153.8326873779297
  batch 20 loss: 1.6697803735733032, 3.04325795173645, 153.8326873779297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6355515718460083 2.945570468902588 148.91407775878906
Loss :  1.608229637145996 2.8720381259918213 145.21014404296875
Loss :  1.6233859062194824 2.969569206237793 150.10183715820312
Loss :  1.6402945518493652 3.071678400039673 155.22421264648438
Loss :  1.6678410768508911 3.3866019248962402 170.99794006347656
Loss :  1.6263445615768433 3.0127899646759033 152.26583862304688
Loss :  1.6364080905914307 3.076153039932251 155.44406127929688
Loss :  1.6303118467330933 3.0969014167785645 156.4753875732422
Loss :  1.5791575908660889 3.068171501159668 154.98773193359375
Loss :  1.6622732877731323 3.4708127975463867 175.20291137695312
Loss :  1.5787293910980225 3.386094093322754 170.88343811035156
Loss :  1.6462337970733643 3.488589286804199 176.07569885253906
Loss :  1.6231303215026855 3.2093429565429688 162.09027099609375
Loss :  1.6226040124893188 2.97998309135437 150.62176513671875
Loss :  1.5865530967712402 3.3572895526885986 169.45101928710938
Loss :  1.6023706197738647 2.9025957584381104 146.73216247558594
Loss :  1.6010305881500244 3.315772294998169 167.3896484375
Loss :  1.6616712808609009 2.9991114139556885 151.6172332763672
Loss :  1.6664520502090454 2.9059898853302 146.96595764160156
Loss :  1.6753417253494263 3.071666955947876 155.25868225097656
  batch 40 loss: 1.6753417253494263, 3.071666955947876, 155.25868225097656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.6349990367889404 2.943099021911621 148.78994750976562
Loss :  1.6211106777191162 3.361117362976074 169.67697143554688
Loss :  1.610172152519226 3.147860288619995 159.00318908691406
Loss :  1.6227447986602783 2.8847649097442627 145.86099243164062
Loss :  1.6013000011444092 3.909602642059326 197.08143615722656
Loss :  1.6306052207946777 2.9072465896606445 146.99293518066406
Loss :  1.6644612550735474 2.8502814769744873 144.17852783203125
Loss :  1.6180834770202637 2.742000102996826 138.7180938720703
Loss :  1.6824935674667358 2.8857879638671875 145.97189331054688
Loss :  1.6258138418197632 2.7567710876464844 139.46437072753906
Loss :  1.6585065126419067 3.4669315814971924 175.0050811767578
Loss :  1.6531898975372314 3.1786792278289795 160.58714294433594
Loss :  1.6307659149169922 3.980510711669922 200.65631103515625
Loss :  1.6587021350860596 3.2487425804138184 164.09584045410156
Loss :  1.6165437698364258 3.053291082382202 154.28109741210938
Loss :  1.6808366775512695 3.0550880432128906 154.4352264404297
Loss :  1.6246755123138428 3.042163372039795 153.73284912109375
Loss :  1.6081398725509644 2.7074227333068848 136.97927856445312
Loss :  1.6235918998718262 2.9787497520446777 150.5610809326172
Loss :  1.68727445602417 3.0770022869110107 155.53738403320312
  batch 60 loss: 1.68727445602417, 3.0770022869110107, 155.53738403320312
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.6086091995239258 2.7446084022521973 138.8390350341797
Loss :  1.6372945308685303 3.2376930713653564 163.52195739746094
Loss :  1.618138313293457 3.1002085208892822 156.62855529785156
Loss :  1.6063299179077148 3.305717945098877 166.89222717285156
Loss :  1.587765097618103 2.3021950721740723 116.69752502441406
Loss :  1.6316230297088623 3.8758955001831055 195.4263916015625
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.6438490152359009 3.672542095184326 185.2709503173828
Loss :  1.640893816947937 3.619293451309204 182.60557556152344
Loss :  1.6486320495605469 3.9854519367218018 200.92123413085938
Total LOSS train 159.5149625338041 valid 191.05603790283203
CE LOSS train 1.6355609435301561 valid 0.4121580123901367
Contrastive LOSS train 3.1575880307417648 valid 0.9963629841804504
EPOCH 122:
Loss :  1.6545921564102173 2.86225962638855 144.76756286621094
Loss :  1.6679695844650269 3.0933403968811035 156.33499145507812
Loss :  1.6390628814697266 3.4486052989959717 174.0693359375
Loss :  1.6451115608215332 3.3815383911132812 170.72203063964844
Loss :  1.6627942323684692 2.847316265106201 144.0286102294922
Loss :  1.6274890899658203 2.6096203327178955 132.10850524902344
Loss :  1.6629222631454468 3.268948793411255 165.1103515625
Loss :  1.638452410697937 3.305830717086792 166.92999267578125
Loss :  1.6274027824401855 3.839305877685547 193.5926971435547
Loss :  1.6599819660186768 3.4826624393463135 175.79310607910156
Loss :  1.6185574531555176 2.9173409938812256 147.48561096191406
Loss :  1.6167398691177368 3.041118860244751 153.6726837158203
Loss :  1.615369439125061 2.5546414852142334 129.34744262695312
Loss :  1.6229318380355835 3.2774202823638916 165.4939422607422
Loss :  1.6759021282196045 3.2706942558288574 165.2106170654297
Loss :  1.6708968877792358 3.075681209564209 155.4549560546875
Loss :  1.6128864288330078 2.8668370246887207 144.95474243164062
Loss :  1.6391050815582275 2.682966470718384 135.7874298095703
Loss :  1.6082481145858765 3.1128015518188477 157.24832153320312
Loss :  1.6672638654708862 3.1822350025177 160.77902221679688
  batch 20 loss: 1.6672638654708862, 3.1822350025177, 160.77902221679688
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.6338635683059692 3.3629069328308105 169.77920532226562
Loss :  1.6069259643554688 2.7608954906463623 139.65170288085938
Loss :  1.6238161325454712 2.763205051422119 139.7840576171875
Loss :  1.6424285173416138 2.7999048233032227 141.63766479492188
Loss :  1.6704415082931519 3.4360241889953613 173.47164916992188
Loss :  1.6291756629943848 2.7557373046875 139.41604614257812
Loss :  1.6398658752441406 3.6635985374450684 184.81979370117188
Loss :  1.6345293521881104 3.3274474143981934 168.0069122314453
Loss :  1.5832444429397583 2.5765810012817383 130.4123077392578
Loss :  1.665859341621399 2.591744899749756 131.25311279296875
Loss :  1.5824843645095825 2.872265577316284 145.19577026367188
Loss :  1.6496851444244385 3.3371505737304688 168.50721740722656
Loss :  1.6270232200622559 2.794020175933838 141.32803344726562
Loss :  1.626175045967102 2.823216199874878 142.7869873046875
Loss :  1.5909863710403442 3.045044183731079 153.84320068359375
Loss :  1.606877326965332 2.8347156047821045 143.3426513671875
Loss :  1.6060876846313477 3.1911282539367676 161.16250610351562
Loss :  1.6664998531341553 2.727430582046509 138.0380401611328
Loss :  1.6720999479293823 3.2944743633270264 166.3958282470703
Loss :  1.6811405420303345 3.1758949756622314 160.47589111328125
  batch 40 loss: 1.6811405420303345, 3.1758949756622314, 160.47589111328125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.6407824754714966 2.7665839195251465 139.969970703125
Loss :  1.6270809173583984 2.5493781566619873 129.0959930419922
Loss :  1.6169458627700806 3.602581739425659 181.74603271484375
Loss :  1.6296077966690063 2.56128191947937 129.69371032714844
Loss :  1.611673355102539 3.274759531021118 165.3496551513672
Loss :  1.6407042741775513 3.0060529708862305 151.94334411621094
Loss :  1.6713517904281616 3.2366318702697754 163.50294494628906
Loss :  1.6271418333053589 3.6908085346221924 186.1675567626953
Loss :  1.6878266334533691 3.332280158996582 168.3018341064453
Loss :  1.6320070028305054 4.003015041351318 201.7827606201172
Loss :  1.6637033224105835 3.1556689739227295 159.4471435546875
Loss :  1.6590185165405273 3.5196354389190674 177.64077758789062
Loss :  1.6388574838638306 3.3830957412719727 170.79364013671875
Loss :  1.6683249473571777 4.0170793533325195 202.5222930908203
Loss :  1.6312588453292847 2.9075143337249756 147.00697326660156
Loss :  1.6898895502090454 2.721083641052246 137.74407958984375
Loss :  1.6371983289718628 4.0883378982543945 206.05409240722656
Loss :  1.621363639831543 3.3591108322143555 169.576904296875
Loss :  1.6348285675048828 3.2253081798553467 162.90023803710938
Loss :  1.6942764520645142 2.920877695083618 147.7381591796875
  batch 60 loss: 1.6942764520645142, 2.920877695083618, 147.7381591796875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6195368766784668 3.319579601287842 167.5985107421875
Loss :  1.6446179151535034 3.1868293285369873 160.986083984375
Loss :  1.625813364982605 3.022620677947998 152.7568359375
Loss :  1.6135083436965942 2.960312604904175 149.62913513183594
Loss :  1.5935430526733398 3.0967307090759277 156.43006896972656
Loss :  1.6271984577178955 3.7136590480804443 187.31015014648438
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6412906646728516 3.8388476371765137 193.58367919921875
Loss :  1.6365236043930054 3.684584379196167 185.86573791503906
Loss :  1.6447875499725342 3.4677984714508057 175.0347137451172
Total LOSS train 157.9166041447566 valid 185.44857025146484
CE LOSS train 1.6383654007544883 valid 0.41119688749313354
Contrastive LOSS train 3.1255647732661322 valid 0.8669496178627014
EPOCH 123:
Loss :  1.6551129817962646 3.1648244857788086 159.89633178710938
Loss :  1.665569543838501 2.953382968902588 149.334716796875
Loss :  1.6377545595169067 3.0830113887786865 155.788330078125
Loss :  1.64237642288208 2.9103567600250244 147.16021728515625
Loss :  1.6599527597427368 2.769406318664551 140.13027954101562
Loss :  1.624666452407837 2.9911813735961914 151.18373107910156
Loss :  1.6601039171218872 3.1990883350372314 161.61453247070312
Loss :  1.6368086338043213 3.154111623764038 159.34239196777344
Loss :  1.6281057596206665 3.2265498638153076 162.9556121826172
Loss :  1.6612722873687744 2.847909450531006 144.05674743652344
Loss :  1.618784785270691 3.4399545192718506 173.61651611328125
Loss :  1.6181362867355347 4.009702205657959 202.10324096679688
Loss :  1.6166349649429321 3.164118528366089 159.82257080078125
Loss :  1.624139666557312 3.126023769378662 157.9253387451172
Loss :  1.6762770414352417 3.140611171722412 158.7068328857422
Loss :  1.671069622039795 3.1731159687042236 160.3268585205078
Loss :  1.61549973487854 3.013096809387207 152.2703399658203
Loss :  1.641638159751892 2.813164234161377 142.2998504638672
Loss :  1.6125661134719849 2.9727232456207275 150.2487335205078
Loss :  1.6713356971740723 2.5992212295532227 131.6324005126953
  batch 20 loss: 1.6713356971740723, 2.5992212295532227, 131.6324005126953
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.6383200883865356 2.70215106010437 136.74588012695312
Loss :  1.612257957458496 2.779827833175659 140.60365295410156
Loss :  1.6290479898452759 3.6188712120056152 182.57260131835938
Loss :  1.643028736114502 3.1498117446899414 159.1336212158203
Loss :  1.6690092086791992 3.493811845779419 176.35960388183594
Loss :  1.6284371614456177 2.8621249198913574 144.73468017578125
Loss :  1.6378542184829712 2.711994171142578 137.237548828125
Loss :  1.6339831352233887 3.073651075363159 155.31654357910156
Loss :  1.5848575830459595 3.10292387008667 156.73104858398438
Loss :  1.66653573513031 4.2959370613098145 216.46339416503906
Loss :  1.5854618549346924 3.07830548286438 155.500732421875
Loss :  1.6520545482635498 3.2013261318206787 161.71835327148438
Loss :  1.6283756494522095 3.2013659477233887 161.69667053222656
Loss :  1.6272156238555908 3.164947748184204 159.87460327148438
Loss :  1.5928398370742798 3.728785991668701 188.03213500976562
Loss :  1.6071745157241821 2.9341061115264893 148.31248474121094
Loss :  1.606128454208374 3.1702523231506348 160.11874389648438
Loss :  1.6651668548583984 3.1960127353668213 161.46580505371094
Loss :  1.6687870025634766 3.0311198234558105 153.2247772216797
Loss :  1.6772624254226685 2.724191188812256 137.88682556152344
  batch 40 loss: 1.6772624254226685, 2.724191188812256, 137.88682556152344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.6370604038238525 3.0716233253479004 155.2182159423828
Loss :  1.6237292289733887 3.4001898765563965 171.6332244873047
Loss :  1.6146124601364136 2.5931718349456787 131.273193359375
Loss :  1.6260238885879517 2.795091390609741 141.38058471679688
Loss :  1.607197642326355 2.6817069053649902 135.69253540039062
Loss :  1.6360373497009277 3.222205638885498 162.74630737304688
Loss :  1.6669342517852783 2.9209306240081787 147.7134552001953
Loss :  1.6227338314056396 2.494580030441284 126.35173797607422
Loss :  1.6826621294021606 3.84594988822937 193.98016357421875
Loss :  1.6271032094955444 2.6039445400238037 131.82432556152344
Loss :  1.660599708557129 3.1197314262390137 157.6471710205078
Loss :  1.6543852090835571 3.0769338607788086 155.50108337402344
Loss :  1.6349891424179077 2.7997851371765137 141.62425231933594
Loss :  1.6631964445114136 3.227774143218994 163.05189514160156
Loss :  1.6259099245071411 3.4370689392089844 173.4793701171875
Loss :  1.6835548877716064 3.1498138904571533 159.1742401123047
Loss :  1.6320821046829224 2.8949244022369385 146.3782958984375
Loss :  1.6186168193817139 2.4263041019439697 122.93382263183594
Loss :  1.6317882537841797 2.8773560523986816 145.4995880126953
Loss :  1.6910628080368042 2.7581863403320312 139.60037231445312
  batch 60 loss: 1.6910628080368042, 2.7581863403320312, 139.60037231445312
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6181330680847168 2.5683796405792236 130.037109375
Loss :  1.644235610961914 3.884159803390503 195.85223388671875
Loss :  1.6278605461120605 3.000972032546997 151.67645263671875
Loss :  1.6180495023727417 3.0940115451812744 156.31861877441406
Loss :  1.5982413291931152 2.589934825897217 131.094970703125
Loss :  1.636834979057312 4.305311679840088 216.9024200439453
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.649051308631897 4.3429274559021 218.79542541503906
Loss :  1.6453003883361816 4.163663387298584 209.82846069335938
Loss :  1.6511555910110474 4.035417556762695 203.42202758789062
Total LOSS train 155.1050538283128 valid 212.2370834350586
CE LOSS train 1.6375139034711397 valid 0.41278889775276184
Contrastive LOSS train 3.0693508111513577 valid 1.0088543891906738
EPOCH 124:
Loss :  1.6598345041275024 2.9631080627441406 149.8152313232422
Loss :  1.6726378202438354 2.9362308979034424 148.4841766357422
Loss :  1.645772099494934 2.7091686725616455 137.1042022705078
Loss :  1.6498132944107056 3.621483564376831 182.72398376464844
Loss :  1.6643015146255493 2.4944281578063965 126.38570404052734
Loss :  1.6292767524719238 2.4965178966522217 126.4551773071289
Loss :  1.66085946559906 3.226226329803467 162.97216796875
Loss :  1.6380282640457153 2.74709153175354 138.99261474609375
Loss :  1.6282203197479248 3.053805351257324 154.3184814453125
Loss :  1.660903811454773 2.955078125 149.41481018066406
Loss :  1.6204400062561035 3.400176525115967 171.62925720214844
Loss :  1.6208906173706055 3.3472535610198975 168.9835662841797
Loss :  1.6182109117507935 3.105248212814331 156.880615234375
Loss :  1.6250966787338257 3.01997971534729 152.62408447265625
Loss :  1.6759312152862549 3.109954595565796 157.1736602783203
Loss :  1.6710916757583618 3.1022679805755615 156.7845001220703
Loss :  1.6158019304275513 3.3934056758880615 171.2860870361328
Loss :  1.6425457000732422 3.020573854446411 152.67123413085938
Loss :  1.6152629852294922 2.914206027984619 147.3255615234375
Loss :  1.6725900173187256 4.0963826179504395 206.49172973632812
  batch 20 loss: 1.6725900173187256, 4.0963826179504395, 206.49172973632812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.6412544250488281 3.991687774658203 201.2256317138672
Loss :  1.617842197418213 3.8699111938476562 195.1134033203125
Loss :  1.6351512670516968 2.940981149673462 148.6842041015625
Loss :  1.6509307622909546 2.8946123123168945 146.3815460205078
Loss :  1.6759858131408691 3.8012542724609375 191.7386932373047
Loss :  1.6364437341690063 3.4859092235565186 175.93190002441406
Loss :  1.64531672000885 3.2074086666107178 162.0157470703125
Loss :  1.6421390771865845 3.1456596851348877 158.9251251220703
Loss :  1.5933362245559692 3.247466802597046 163.9666748046875
Loss :  1.6713989973068237 3.301589250564575 166.75086975097656
Loss :  1.592463731765747 3.3520407676696777 169.1945037841797
Loss :  1.6554961204528809 2.79223370552063 141.26718139648438
Loss :  1.6317243576049805 3.5740115642547607 180.33230590820312
Loss :  1.630142092704773 3.0139410495758057 152.3271942138672
Loss :  1.5955687761306763 3.1975479125976562 161.47296142578125
Loss :  1.609873652458191 3.044832944869995 153.85153198242188
Loss :  1.608371376991272 3.0559585094451904 154.4062957763672
Loss :  1.665706753730774 3.0602729320526123 154.67935180664062
Loss :  1.6701689958572388 3.4392027854919434 173.63031005859375
Loss :  1.6791541576385498 3.0452139377593994 153.93984985351562
  batch 40 loss: 1.6791541576385498, 3.0452139377593994, 153.93984985351562
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.6400916576385498 3.123791456222534 157.8296661376953
Loss :  1.6265658140182495 2.4701194763183594 125.13253784179688
Loss :  1.6176296472549438 3.140207052230835 158.6279754638672
Loss :  1.6305627822875977 3.0441110134124756 153.83612060546875
Loss :  1.612276554107666 2.5411458015441895 128.66957092285156
Loss :  1.640820026397705 2.652212381362915 134.25144958496094
Loss :  1.6709030866622925 3.7773356437683105 190.5376739501953
Loss :  1.628816843032837 3.0610859394073486 154.68310546875
Loss :  1.6863738298416138 2.6008448600769043 131.72860717773438
Loss :  1.6334426403045654 3.0010197162628174 151.68441772460938
Loss :  1.6651923656463623 3.0823445320129395 155.7824249267578
Loss :  1.658401370048523 2.971555471420288 150.23617553710938
Loss :  1.6383581161499023 2.9046010971069336 146.868408203125
Loss :  1.6646641492843628 3.3932878971099854 171.3290557861328
Loss :  1.627191424369812 3.2017300128936768 161.7136993408203
Loss :  1.6833714246749878 3.058257579803467 154.5962371826172
Loss :  1.6314486265182495 3.3027796745300293 166.7704315185547
Loss :  1.61824369430542 2.8930728435516357 146.27188110351562
Loss :  1.631510853767395 3.0338728427886963 153.32516479492188
Loss :  1.690654993057251 3.3539388179779053 169.38760375976562
  batch 60 loss: 1.690654993057251, 3.3539388179779053, 169.38760375976562
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6195439100265503 3.117042303085327 157.47164916992188
Loss :  1.6472989320755005 3.1526663303375244 159.28060913085938
Loss :  1.630039930343628 2.527050495147705 127.9825668334961
Loss :  1.622283935546875 3.1166024208068848 157.45240783691406
Loss :  1.6053051948547363 2.4020376205444336 121.70719146728516
Loss :  1.6237685680389404 4.126832008361816 207.96536254882812
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6354520320892334 4.132394313812256 208.2551727294922
Loss :  1.6325066089630127 4.088270664215088 206.04603576660156
Loss :  1.6368433237075806 3.9352924823760986 198.40145874023438
Total LOSS train 157.10013474684496 valid 205.16700744628906
CE LOSS train 1.640876471079313 valid 0.40921083092689514
Contrastive LOSS train 3.109185185799232 valid 0.9838231205940247
EPOCH 125:
Loss :  1.663716435432434 2.755781888961792 139.4528045654297
Loss :  1.6747159957885742 3.501474142074585 176.7484130859375
Loss :  1.6464624404907227 3.006615161895752 151.97723388671875
Loss :  1.65079665184021 2.7730631828308105 140.303955078125
Loss :  1.6650828123092651 3.3755686283111572 170.44351196289062
Loss :  1.6323086023330688 3.242619276046753 163.76327514648438
Loss :  1.6638009548187256 2.892655849456787 146.29660034179688
Loss :  1.641240119934082 3.1138756275177 157.33502197265625
Loss :  1.6311274766921997 2.7474968433380127 139.0059814453125
Loss :  1.6614598035812378 2.9329729080200195 148.3101043701172
Loss :  1.619946002960205 3.9392802715301514 198.58396911621094
Loss :  1.6190897226333618 2.9646058082580566 149.84938049316406
Loss :  1.6149203777313232 2.8109374046325684 142.1617889404297
Loss :  1.622459888458252 3.3032634258270264 166.78564453125
Loss :  1.6745272874832153 3.069406032562256 155.14483642578125
Loss :  1.6689999103546143 3.173365592956543 160.3372802734375
Loss :  1.613838791847229 3.6530044078826904 184.26406860351562
Loss :  1.6396729946136475 2.918743371963501 147.57684326171875
Loss :  1.6109977960586548 2.7887868881225586 141.0503387451172
Loss :  1.6681525707244873 3.0794575214385986 155.64102172851562
  batch 20 loss: 1.6681525707244873, 3.0794575214385986, 155.64102172851562
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.6357629299163818 2.7757627964019775 140.4239044189453
Loss :  1.6104109287261963 2.9534950256347656 149.28515625
Loss :  1.6275826692581177 3.139808416366577 158.6179962158203
Loss :  1.6439517736434937 3.054673194885254 154.3776092529297
Loss :  1.67088782787323 3.2867672443389893 166.00924682617188
Loss :  1.630902647972107 2.950253486633301 149.14358520507812
Loss :  1.6388031244277954 3.184803009033203 160.8789520263672
Loss :  1.6339709758758545 2.88150691986084 145.70932006835938
Loss :  1.5839207172393799 3.520238161087036 177.5958251953125
Loss :  1.6652203798294067 3.318321466445923 167.581298828125
Loss :  1.5857239961624146 2.80049204826355 141.61032104492188
Loss :  1.6514921188354492 4.084132671356201 205.85812377929688
Loss :  1.6293432712554932 3.1874780654907227 161.0032501220703
Loss :  1.6287020444869995 3.1762304306030273 160.4402313232422
Loss :  1.5974900722503662 2.798755407333374 141.53526306152344
Loss :  1.612289547920227 3.4592843055725098 174.57650756835938
Loss :  1.6117968559265137 3.6757469177246094 185.39915466308594
Loss :  1.6686065196990967 3.0853116512298584 155.93418884277344
Loss :  1.673123836517334 3.0694611072540283 155.14617919921875
Loss :  1.6827682256698608 3.311225652694702 167.24404907226562
  batch 40 loss: 1.6827682256698608, 3.311225652694702, 167.24404907226562
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.6466628313064575 3.4654343128204346 174.9183807373047
Loss :  1.634199857711792 3.517803430557251 177.5243682861328
Loss :  1.6265405416488647 2.703897714614868 136.82142639160156
Loss :  1.6374601125717163 2.648033857345581 134.03915405273438
Loss :  1.618228793144226 2.882122278213501 145.72433471679688
Loss :  1.644458293914795 3.8822553157806396 195.75721740722656
Loss :  1.672924518585205 2.7241532802581787 137.88058471679688
Loss :  1.6308056116104126 2.6856207847595215 135.91183471679688
Loss :  1.687681794166565 2.668626308441162 135.11900329589844
Loss :  1.6357909440994263 2.9466049671173096 148.96603393554688
Loss :  1.66885507106781 2.943676710128784 148.85269165039062
Loss :  1.6625396013259888 2.8902649879455566 146.17578125
Loss :  1.6431599855422974 2.731703281402588 138.2283172607422
Loss :  1.668653964996338 4.269857406616211 215.16152954101562
Loss :  1.6315478086471558 3.0355985164642334 153.41146850585938
Loss :  1.6869033575057983 2.9313671588897705 148.25526428222656
Loss :  1.6372897624969482 3.8942646980285645 196.35052490234375
Loss :  1.624356746673584 2.919792652130127 147.61399841308594
Loss :  1.6371859312057495 3.2053418159484863 161.90428161621094
Loss :  1.6945416927337646 3.6855199337005615 185.9705352783203
  batch 60 loss: 1.6945416927337646, 3.6855199337005615, 185.9705352783203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.6246002912521362 3.4490811824798584 174.0786590576172
Loss :  1.650553822517395 2.784945011138916 140.89781188964844
Loss :  1.6334775686264038 2.9251344203948975 147.89019775390625
Loss :  1.6242692470550537 3.020308494567871 152.6396942138672
Loss :  1.6069111824035645 2.6194963455200195 132.5817413330078
Loss :  1.6420477628707886 4.076315402984619 205.4578094482422
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6533915996551514 4.12331485748291 207.8191375732422
Loss :  1.6500099897384644 4.023341178894043 202.8170623779297
Loss :  1.656662106513977 3.8932816982269287 196.32073974609375
Total LOSS train 157.939647263747 valid 203.10368728637695
CE LOSS train 1.641471791267395 valid 0.41416552662849426
Contrastive LOSS train 3.125963493493887 valid 0.9733204245567322
EPOCH 126:
Loss :  1.6648234128952026 2.8822593688964844 145.77780151367188
Loss :  1.6759703159332275 3.542715549468994 178.81173706054688
Loss :  1.6476843357086182 3.3984522819519043 171.57029724121094
Loss :  1.6521154642105103 3.2451741695404053 163.91082763671875
Loss :  1.6670864820480347 2.8057713508605957 141.95565795898438
Loss :  1.6348614692687988 3.0663979053497314 154.9547576904297
Loss :  1.6667946577072144 3.4417684078216553 173.75521850585938
Loss :  1.6462507247924805 3.1015753746032715 156.7250213623047
Loss :  1.6386831998825073 3.065046787261963 154.89102172851562
Loss :  1.669927954673767 2.9484047889709473 149.0901641845703
Loss :  1.6301764249801636 3.5590083599090576 179.58059692382812
Loss :  1.6288630962371826 3.1095755100250244 157.10763549804688
Loss :  1.6259253025054932 2.661106824874878 134.6812744140625
Loss :  1.6312474012374878 3.040503978729248 153.65643310546875
Loss :  1.6814134120941162 2.8759260177612305 145.4777069091797
Loss :  1.6738107204437256 3.2008206844329834 161.71484375
Loss :  1.6195117235183716 2.843528985977173 143.79595947265625
Loss :  1.6443284749984741 3.115192174911499 157.40394592285156
Loss :  1.615850567817688 3.6330461502075195 183.26815795898438
Loss :  1.6731005907058716 3.43369460105896 173.35781860351562
  batch 20 loss: 1.6731005907058716, 3.43369460105896, 173.35781860351562
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.6383999586105347 2.936403751373291 148.45858764648438
Loss :  1.6127595901489258 3.196387767791748 161.43214416503906
Loss :  1.6293805837631226 2.855954885482788 144.4271240234375
Loss :  1.6442468166351318 2.8468198776245117 143.9852294921875
Loss :  1.6708065271377563 3.417951822280884 172.5684051513672
Loss :  1.6316418647766113 3.078843355178833 155.5738067626953
Loss :  1.6403056383132935 3.2249834537506104 162.8894805908203
Loss :  1.6364127397537231 3.0554585456848145 154.4093475341797
Loss :  1.5855594873428345 2.7868921756744385 140.93016052246094
Loss :  1.666223168373108 3.105045795440674 156.91851806640625
Loss :  1.5872008800506592 3.0974926948547363 156.4618377685547
Loss :  1.6538431644439697 2.9787118434906006 150.5894317626953
Loss :  1.6311016082763672 3.0720162391662598 155.23191833496094
Loss :  1.629927635192871 3.2493646144866943 164.09815979003906
Loss :  1.5970412492752075 3.28080677986145 165.63739013671875
Loss :  1.610457181930542 2.758218288421631 139.52137756347656
Loss :  1.6089742183685303 2.9293646812438965 148.07720947265625
Loss :  1.6668798923492432 3.9069571495056152 197.01473999023438
Loss :  1.6702059507369995 2.8964784145355225 146.49412536621094
Loss :  1.6800601482391357 2.780503273010254 140.70521545410156
  batch 40 loss: 1.6800601482391357, 2.780503273010254, 140.70521545410156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.6434932947158813 3.2115378379821777 162.2203826904297
Loss :  1.6298201084136963 3.1401607990264893 158.6378631591797
Loss :  1.620527982711792 2.6857190132141113 135.90647888183594
Loss :  1.6310237646102905 2.871825695037842 145.22230529785156
Loss :  1.6116925477981567 2.6099092960357666 132.10716247558594
Loss :  1.6400377750396729 2.9926154613494873 151.27081298828125
Loss :  1.6696772575378418 3.0213398933410645 152.73667907714844
Loss :  1.6272696256637573 3.196927785873413 161.47366333007812
Loss :  1.684941291809082 2.9244425296783447 147.9070587158203
Loss :  1.6337960958480835 3.596691608428955 181.46836853027344
Loss :  1.6654767990112305 2.9105961322784424 147.19528198242188
Loss :  1.6595737934112549 2.9069526195526123 147.0072021484375
Loss :  1.6404227018356323 3.1262054443359375 157.95069885253906
Loss :  1.6663631200790405 2.705612897872925 136.94700622558594
Loss :  1.6296465396881104 2.9374444484710693 148.5018768310547
Loss :  1.6854368448257446 2.641723394393921 133.7716064453125
Loss :  1.6354930400848389 3.23065447807312 163.16822814941406
Loss :  1.6223417520523071 2.846938371658325 143.96926879882812
Loss :  1.635743498802185 2.804645538330078 141.86801147460938
Loss :  1.6936665773391724 3.449108123779297 174.14907836914062
  batch 60 loss: 1.6936665773391724, 3.449108123779297, 174.14907836914062
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.6243674755096436 3.2658495903015137 164.91685485839844
Loss :  1.649635672569275 2.883429527282715 145.8211212158203
Loss :  1.630287766456604 2.7868847846984863 140.9745330810547
Loss :  1.6196483373641968 2.937147855758667 148.47703552246094
Loss :  1.6010963916778564 2.754899263381958 139.34605407714844
Loss :  1.6330454349517822 4.12617826461792 207.94195556640625
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.6452767848968506 4.191277980804443 211.2091827392578
Loss :  1.641662359237671 3.978032350540161 200.54327392578125
Loss :  1.6475064754486084 4.079299449920654 205.6124725341797
Total LOSS train 154.61424184945915 valid 206.32672119140625
CE LOSS train 1.6420205244651207 valid 0.4118766188621521
Contrastive LOSS train 3.0594444164863 valid 1.0198248624801636
EPOCH 127:
Loss :  1.6585084199905396 3.0644538402557373 154.88119506835938
Loss :  1.669541597366333 3.3083736896514893 167.08822631835938
Loss :  1.6414319276809692 3.0722081661224365 155.25184631347656
Loss :  1.6475164890289307 2.7743215560913086 140.3636016845703
Loss :  1.6629239320755005 2.687361240386963 136.03097534179688
Loss :  1.630663275718689 2.7171123027801514 137.4862823486328
Loss :  1.6634430885314941 3.710252523422241 187.17605590820312
Loss :  1.6419456005096436 2.648343801498413 134.05914306640625
Loss :  1.6336480379104614 2.741941213607788 138.730712890625
Loss :  1.665885329246521 2.74906325340271 139.11903381347656
Loss :  1.628003478050232 2.837789297103882 143.51747131347656
Loss :  1.6289433240890503 3.019911289215088 152.62449645996094
Loss :  1.6278096437454224 3.0326573848724365 153.26068115234375
Loss :  1.6342840194702148 2.774350881576538 140.35182189941406
Loss :  1.6835711002349854 3.0845446586608887 155.9108123779297
Loss :  1.6766607761383057 3.0012240409851074 151.7378692626953
Loss :  1.6244596242904663 2.6729555130004883 135.27224731445312
Loss :  1.648760199546814 2.7495198249816895 139.124755859375
Loss :  1.6222256422042847 3.232410192489624 163.24273681640625
Loss :  1.6759302616119385 2.992306709289551 151.29127502441406
  batch 20 loss: 1.6759302616119385, 2.992306709289551, 151.29127502441406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6438777446746826 3.012014865875244 152.24461364746094
Loss :  1.6197212934494019 3.2212328910827637 162.68136596679688
Loss :  1.636383056640625 2.5848515033721924 130.8789520263672
Loss :  1.6511883735656738 3.156841278076172 159.49325561523438
Loss :  1.6768510341644287 3.1588680744171143 159.62025451660156
Loss :  1.640173316001892 2.587618827819824 131.02110290527344
Loss :  1.64870285987854 2.6564700603485107 134.47219848632812
Loss :  1.6454721689224243 3.058241844177246 154.5575714111328
Loss :  1.5994583368301392 2.8910157680511475 146.15023803710938
Loss :  1.6748273372650146 2.8308157920837402 143.21560668945312
Loss :  1.6009904146194458 2.82157564163208 142.67977905273438
Loss :  1.66135573387146 2.7100844383239746 137.16558837890625
Loss :  1.6408939361572266 3.0299110412597656 153.13644409179688
Loss :  1.6399478912353516 3.006621837615967 151.97103881835938
Loss :  1.6099884510040283 3.1260452270507812 157.91224670410156
Loss :  1.6234898567199707 3.1332154273986816 158.2842559814453
Loss :  1.62274169921875 3.2621169090270996 164.7285919189453
Loss :  1.6762107610702515 2.661013126373291 134.72686767578125
Loss :  1.6799510717391968 2.864114284515381 144.8856658935547
Loss :  1.6885652542114258 3.094548463821411 156.41598510742188
  batch 40 loss: 1.6885652542114258, 3.094548463821411, 156.41598510742188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.6545147895812988 3.218445062637329 162.57676696777344
Loss :  1.6424082517623901 2.632390260696411 133.2619171142578
Loss :  1.6348015069961548 2.60469913482666 131.8697509765625
Loss :  1.6466318368911743 2.679414987564087 135.6173858642578
Loss :  1.629875659942627 2.618887424468994 132.57424926757812
Loss :  1.655090093612671 3.242422342300415 163.77621459960938
Loss :  1.682076096534729 2.7968454360961914 141.52435302734375
Loss :  1.6434550285339355 3.0727016925811768 155.27853393554688
Loss :  1.6948273181915283 3.598799705505371 181.6348114013672
Loss :  1.6462479829788208 2.6732070446014404 135.30661010742188
Loss :  1.6736663579940796 3.070770263671875 155.21217346191406
Loss :  1.668096661567688 2.7617592811584473 139.7560577392578
Loss :  1.6493796110153198 2.597367525100708 131.51776123046875
Loss :  1.6729257106781006 3.317646026611328 167.5552215576172
Loss :  1.638000249862671 3.5360565185546875 178.44082641601562
Loss :  1.691274881362915 2.87703537940979 145.5430450439453
Loss :  1.6425211429595947 2.814211130142212 142.3530731201172
Loss :  1.630223274230957 2.6729896068573 135.27969360351562
Loss :  1.6421480178833008 2.7584691047668457 139.56561279296875
Loss :  1.6972949504852295 3.215066432952881 162.45062255859375
  batch 60 loss: 1.6972949504852295, 3.215066432952881, 162.45062255859375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.6279568672180176 3.2467236518859863 163.96414184570312
Loss :  1.6530585289001465 2.967095375061035 150.00782775878906
Loss :  1.6361662149429321 3.2587080001831055 164.57156372070312
Loss :  1.6252615451812744 3.3838489055633545 170.8177032470703
Loss :  1.6078460216522217 2.364976406097412 119.8566665649414
Loss :  1.6342750787734985 3.9132044315338135 197.29449462890625
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6456642150878906 3.8794963359832764 195.6204833984375
Loss :  1.6426955461502075 3.80035400390625 191.660400390625
Loss :  1.647998571395874 3.734203815460205 188.35818481445312
Total LOSS train 149.12423764742337 valid 193.23339080810547
CE LOSS train 1.6481953070713924 valid 0.4119996428489685
Contrastive LOSS train 2.9495208520155685 valid 0.9335509538650513
EPOCH 128:
Loss :  1.6659517288208008 2.8431243896484375 143.82217407226562
Loss :  1.6769620180130005 3.3314554691314697 168.24972534179688
Loss :  1.651823878288269 2.7459213733673096 138.94789123535156
Loss :  1.657029151916504 2.9454076290130615 148.9274139404297
Loss :  1.6744303703308105 2.9565987586975098 149.50436401367188
Loss :  1.642417311668396 3.0632095336914062 154.80288696289062
Loss :  1.6739850044250488 2.982384443283081 150.79319763183594
Loss :  1.6525167226791382 2.646307945251465 133.96791076660156
Loss :  1.643712043762207 2.641596794128418 133.72354125976562
Loss :  1.6725890636444092 2.5746662616729736 130.40589904785156
Loss :  1.633872151374817 3.3647139072418213 169.86956787109375
Loss :  1.6333675384521484 3.8223912715911865 192.7529296875
Loss :  1.630465030670166 2.9170048236846924 147.48069763183594
Loss :  1.6369410753250122 3.0336921215057373 153.32154846191406
Loss :  1.6835905313491821 3.02089786529541 152.72848510742188
Loss :  1.679266095161438 3.0229616165161133 152.8273468017578
Loss :  1.6288319826126099 2.732647657394409 138.26121520996094
Loss :  1.653493881225586 3.171983242034912 160.25265502929688
Loss :  1.6282243728637695 2.999478578567505 151.60214233398438
Loss :  1.6805227994918823 3.1557223796844482 159.46664428710938
  batch 20 loss: 1.6805227994918823, 3.1557223796844482, 159.46664428710938
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.6515424251556396 2.4991157054901123 126.60733032226562
Loss :  1.6295616626739502 2.966630458831787 149.96109008789062
Loss :  1.645944595336914 3.3175861835479736 167.52525329589844
Loss :  1.6601790189743042 3.1576426029205322 159.54229736328125
Loss :  1.6829805374145508 3.1845192909240723 160.90895080566406
Loss :  1.646657109260559 2.9876534938812256 151.02932739257812
Loss :  1.6534546613693237 3.4341938495635986 173.36314392089844
Loss :  1.647416353225708 3.9904394149780273 201.1693878173828
Loss :  1.6032387018203735 3.3145077228546143 167.32862854003906
Loss :  1.6755709648132324 3.388796091079712 171.11537170410156
Loss :  1.6052864789962769 2.928952932357788 148.0529327392578
Loss :  1.6631392240524292 2.961616039276123 149.74392700195312
Loss :  1.6425541639328003 2.9828391075134277 150.7845001220703
Loss :  1.6427093744277954 3.287156105041504 166.00051879882812
Loss :  1.6128652095794678 3.1113390922546387 157.17982482910156
Loss :  1.6270310878753662 3.1909995079040527 161.177001953125
Loss :  1.6252113580703735 3.4358983039855957 173.42013549804688
Loss :  1.6772655248641968 3.422990083694458 172.82676696777344
Loss :  1.6805577278137207 3.9341094493865967 198.3860321044922
Loss :  1.6893421411514282 3.209442138671875 162.1614532470703
  batch 40 loss: 1.6893421411514282, 3.209442138671875, 162.1614532470703
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6542333364486694 3.067230701446533 155.01577758789062
Loss :  1.642944097518921 3.120737314224243 157.6798095703125
Loss :  1.6349343061447144 3.387457847595215 171.00782775878906
Loss :  1.6452791690826416 4.2369465827941895 213.49261474609375
Loss :  1.6248607635498047 3.293299913406372 166.28985595703125
Loss :  1.647457242012024 3.302471160888672 166.77102661132812
Loss :  1.673366904258728 3.19138765335083 161.2427520751953
Loss :  1.6332327127456665 3.916827440261841 197.474609375
Loss :  1.686117172241211 3.4053115844726562 171.95169067382812
Loss :  1.6410284042358398 4.034038066864014 203.34292602539062
Loss :  1.6620697975158691 3.5981481075286865 181.56947326660156
Loss :  1.6611844301223755 3.408916711807251 172.1070098876953
Loss :  1.6400642395019531 3.172877311706543 160.28392028808594
Loss :  1.6657369136810303 3.2273948192596436 163.0354766845703
Loss :  1.626899242401123 3.6002373695373535 181.63876342773438
Loss :  1.6820064783096313 3.252959728240967 164.32998657226562
Loss :  1.6304810047149658 3.3948721885681152 171.37408447265625
Loss :  1.6146928071975708 3.396183967590332 171.4239044189453
Loss :  1.6297457218170166 3.7312207221984863 188.19078063964844
Loss :  1.6895925998687744 3.5332231521606445 178.3507537841797
  batch 60 loss: 1.6895925998687744, 3.5332231521606445, 178.3507537841797
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.615684986114502 3.2720131874084473 165.2163543701172
Loss :  1.6419987678527832 3.2855708599090576 165.92054748535156
Loss :  1.6227359771728516 3.0630712509155273 154.77630615234375
Loss :  1.6120927333831787 3.3660247325897217 169.913330078125
Loss :  1.594657063484192 2.836578845977783 143.42359924316406
Loss :  1.6307567358016968 4.389823913574219 221.1219482421875
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6433669328689575 4.376474857330322 220.4671173095703
Loss :  1.6380219459533691 4.215068340301514 212.3914337158203
Loss :  1.6453207731246948 4.323947429656982 217.8426971435547
Total LOSS train 163.01254295935996 valid 217.9557991027832
CE LOSS train 1.6482399683732254 valid 0.4113301932811737
Contrastive LOSS train 3.2272860747117265 valid 1.0809868574142456
EPOCH 129:
Loss :  1.6553566455841064 2.92132830619812 147.72177124023438
Loss :  1.6658577919006348 3.4015231132507324 171.7420196533203
Loss :  1.638264775276184 2.9517934322357178 149.22793579101562
Loss :  1.6429203748703003 2.9198415279388428 147.63499450683594
Loss :  1.659830093383789 3.0420403480529785 153.76185607910156
Loss :  1.6246761083602905 2.9816274642944336 150.7060546875
Loss :  1.6603754758834839 4.282767295837402 215.79873657226562
Loss :  1.636443853378296 3.0736734867095947 155.3201141357422
Loss :  1.6282418966293335 3.521139621734619 177.68521118164062
Loss :  1.6602774858474731 3.094923496246338 156.4064483642578
Loss :  1.618973970413208 3.2464182376861572 163.93988037109375
Loss :  1.6218675374984741 3.3747661113739014 170.3601837158203
Loss :  1.6165134906768799 3.5130860805511475 177.27081298828125
Loss :  1.6235601902008057 3.5240495204925537 177.82603454589844
Loss :  1.6711982488632202 3.197354793548584 161.5389404296875
Loss :  1.669647216796875 3.809333562850952 192.13632202148438
Loss :  1.6114115715026855 3.127161979675293 157.96949768066406
Loss :  1.6375194787979126 3.046377420425415 153.95639038085938
Loss :  1.6097773313522339 3.2763049602508545 165.42501831054688
Loss :  1.6667331457138062 3.419945001602173 172.6639862060547
  batch 20 loss: 1.6667331457138062, 3.419945001602173, 172.6639862060547
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.6345858573913574 2.876753568649292 145.47225952148438
Loss :  1.6093438863754272 3.163099765777588 159.7643280029297
Loss :  1.6277533769607544 3.318607807159424 167.55813598632812
Loss :  1.6433217525482178 3.5834829807281494 180.81747436523438
Loss :  1.670357346534729 3.1267483234405518 158.00778198242188
Loss :  1.6314077377319336 3.2509357929229736 164.17819213867188
Loss :  1.6410404443740845 4.073402404785156 205.3111572265625
Loss :  1.6363450288772583 3.0575430393218994 154.5135040283203
Loss :  1.5891430377960205 2.892547845840454 146.21653747558594
Loss :  1.6684812307357788 3.6749250888824463 185.4147491455078
Loss :  1.5905373096466064 3.163109302520752 159.74600219726562
Loss :  1.6547682285308838 3.038482904434204 153.57891845703125
Loss :  1.6315248012542725 3.2727890014648438 165.27098083496094
Loss :  1.6299428939819336 3.0612518787384033 154.69253540039062
Loss :  1.5956079959869385 3.3115594387054443 167.173583984375
Loss :  1.609983205795288 3.3293979167938232 168.0798797607422
Loss :  1.6073418855667114 3.146503448486328 158.93251037597656
Loss :  1.6650004386901855 3.249955892562866 164.16278076171875
Loss :  1.6689717769622803 3.7276506423950195 188.051513671875
Loss :  1.677892804145813 3.0472333431243896 154.03955078125
  batch 40 loss: 1.677892804145813, 3.0472333431243896, 154.03955078125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.6382789611816406 3.181265115737915 160.7015380859375
Loss :  1.6264735460281372 3.0667364597320557 154.9633026123047
Loss :  1.6163352727890015 3.0217390060424805 152.70327758789062
Loss :  1.6283819675445557 3.0327885150909424 153.26780700683594
Loss :  1.6099032163619995 3.5541763305664062 179.3187255859375
Loss :  1.6377555131912231 3.326878547668457 167.981689453125
Loss :  1.6689785718917847 3.754739284515381 189.40594482421875
Loss :  1.6255476474761963 3.0801749229431152 155.63429260253906
Loss :  1.6858144998550415 2.9474220275878906 149.05691528320312
Loss :  1.6327258348464966 2.6733031272888184 135.29788208007812
Loss :  1.6653228998184204 2.59692120552063 131.51138305664062
Loss :  1.6579114198684692 3.1561384201049805 159.46482849121094
Loss :  1.6379168033599854 2.9151713848114014 147.39649963378906
Loss :  1.6655042171478271 3.206512689590454 161.9911346435547
Loss :  1.6254677772521973 3.167907953262329 160.0208740234375
Loss :  1.6832091808319092 3.1320276260375977 158.2845916748047
Loss :  1.6295034885406494 3.194188117980957 161.3389129638672
Loss :  1.6158913373947144 3.0304782390594482 153.13980102539062
Loss :  1.6300702095031738 3.0595767498016357 154.60890197753906
Loss :  1.6917102336883545 3.318586587905884 167.6210479736328
  batch 60 loss: 1.6917102336883545, 3.318586587905884, 167.6210479736328
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6175318956375122 2.660128355026245 134.62396240234375
Loss :  1.6444272994995117 3.4758386611938477 175.4363555908203
Loss :  1.6283668279647827 2.8702218532562256 145.13946533203125
Loss :  1.6166685819625854 3.2837588787078857 165.8046112060547
Loss :  1.598634123802185 2.7534146308898926 139.2693634033203
Loss :  1.6346049308776855 4.06984806060791 205.1269989013672
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6462796926498413 4.087599277496338 206.0262451171875
Loss :  1.6423699855804443 4.024525165557861 202.86862182617188
Loss :  1.6490930318832397 3.8977534770965576 196.53677368164062
Total LOSS train 162.0624260535607 valid 202.6396598815918
CE LOSS train 1.6381712161577664 valid 0.41227325797080994
Contrastive LOSS train 3.20848508981558 valid 0.9744383692741394
EPOCH 130:
Loss :  1.6613537073135376 3.5134642124176025 177.33456420898438
Loss :  1.6722018718719482 3.236783742904663 163.51138305664062
Loss :  1.641599178314209 2.6775054931640625 135.51687622070312
Loss :  1.6484652757644653 3.2868144512176514 165.98919677734375
Loss :  1.662462592124939 2.8793787956237793 145.6313934326172
Loss :  1.633103609085083 3.087153673171997 155.99078369140625
Loss :  1.6623233556747437 4.075631618499756 205.44390869140625
Loss :  1.639962911605835 3.0596585273742676 154.62289428710938
Loss :  1.629223346710205 3.2075767517089844 162.00807189941406
Loss :  1.659601092338562 2.9769206047058105 150.50563049316406
Loss :  1.6249947547912598 3.1998631954193115 161.6181640625
Loss :  1.6246120929718018 3.268277645111084 165.0384979248047
Loss :  1.6208317279815674 3.023761034011841 152.8088836669922
Loss :  1.6274518966674805 3.124728202819824 157.86386108398438
Loss :  1.6736273765563965 3.470627546310425 175.2050018310547
Loss :  1.676157832145691 3.4504685401916504 174.1995849609375
Loss :  1.6176199913024902 3.2059364318847656 161.9144287109375
Loss :  1.6477051973342896 3.046079397201538 153.95167541503906
Loss :  1.6185518503189087 2.8516733646392822 144.20220947265625
Loss :  1.6697741746902466 3.0435354709625244 153.84654235839844
  batch 20 loss: 1.6697741746902466, 3.0435354709625244, 153.84654235839844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6392189264297485 3.607196807861328 181.99905395507812
Loss :  1.6163358688354492 3.549203634262085 179.07650756835938
Loss :  1.6334813833236694 2.495671033859253 126.41703796386719
Loss :  1.6509217023849487 2.520768404006958 127.68934631347656
Loss :  1.676961064338684 2.940608501434326 148.70738220214844
Loss :  1.63900625705719 3.6995046138763428 186.61424255371094
Loss :  1.6501595973968506 3.1420698165893555 158.75364685058594
Loss :  1.6415455341339111 3.085564613342285 155.91976928710938
Loss :  1.5980417728424072 3.040288209915161 153.61244201660156
Loss :  1.6710039377212524 2.996870756149292 151.51454162597656
Loss :  1.6006388664245605 3.3571102619171143 169.45614624023438
Loss :  1.661105990409851 2.820420503616333 142.68212890625
Loss :  1.6389801502227783 2.7850282192230225 140.890380859375
Loss :  1.6367230415344238 2.8753461837768555 145.40402221679688
Loss :  1.6068028211593628 2.8560099601745605 144.40728759765625
Loss :  1.6199034452438354 2.9497323036193848 149.1065216064453
Loss :  1.617379069328308 3.1298584938049316 158.1103057861328
Loss :  1.6747348308563232 2.992814302444458 151.31544494628906
Loss :  1.676641821861267 2.9264285564422607 147.99806213378906
Loss :  1.684851050376892 2.893298387527466 146.34976196289062
  batch 40 loss: 1.684851050376892, 2.893298387527466, 146.34976196289062
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.646410346031189 3.2676002979278564 165.02642822265625
Loss :  1.6336512565612793 2.9100136756896973 147.13433837890625
Loss :  1.6230237483978271 3.053887367248535 154.3173828125
Loss :  1.6363509893417358 3.0294783115386963 153.1102752685547
Loss :  1.6165215969085693 3.01865291595459 152.54916381835938
Loss :  1.6408637762069702 2.692621946334839 136.27197265625
Loss :  1.6702415943145752 2.7292916774749756 138.13482666015625
Loss :  1.6305241584777832 2.629225015640259 133.09178161621094
Loss :  1.6867953538894653 2.6283481121063232 133.1042022705078
Loss :  1.638089895248413 3.240976095199585 163.6868896484375
Loss :  1.6648765802383423 3.4096055030822754 172.1451416015625
Loss :  1.6612821817398071 3.3498313426971436 169.15284729003906
Loss :  1.6430495977401733 3.360168218612671 169.65145874023438
Loss :  1.6698156595230103 3.18436861038208 160.88824462890625
Loss :  1.6310442686080933 3.070498466491699 155.1559600830078
Loss :  1.6871551275253296 2.943317174911499 148.85301208496094
Loss :  1.637434482574463 3.4448652267456055 173.8806915283203
Loss :  1.6243250370025635 3.007338523864746 151.9912567138672
Loss :  1.6371341943740845 3.5183584690093994 177.5550537109375
Loss :  1.6946743726730347 3.2038955688476562 161.8894500732422
  batch 60 loss: 1.6946743726730347, 3.2038955688476562, 161.8894500732422
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.623758316040039 3.8028900623321533 191.7682647705078
Loss :  1.6484631299972534 3.08247709274292 155.77232360839844
Loss :  1.632162094116211 2.8510873317718506 144.1865234375
Loss :  1.618645191192627 3.127769947052002 158.0071563720703
Loss :  1.6024703979492188 2.57766056060791 130.48550415039062
Loss :  1.6221383810043335 4.155121326446533 209.37820434570312
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6343830823898315 4.251082897186279 214.1885223388672
Loss :  1.628469705581665 4.147111892700195 208.9840545654297
Loss :  1.6383121013641357 3.90956974029541 197.11679077148438
Total LOSS train 156.6313497690054 valid 207.4168930053711
CE LOSS train 1.6437665279094988 valid 0.40957802534103394
Contrastive LOSS train 3.0997516888838548 valid 0.9773924350738525
EPOCH 131:
Loss :  1.6632000207901 2.9804725646972656 150.6868133544922
Loss :  1.6719917058944702 3.7916982173919678 191.2569122314453
Loss :  1.6451282501220703 3.474536657333374 175.37196350097656
Loss :  1.6503543853759766 3.456028699874878 174.45179748535156
Loss :  1.6657347679138184 2.923672676086426 147.84938049316406
Loss :  1.6337196826934814 2.991316080093384 151.19952392578125
Loss :  1.6642093658447266 3.5300941467285156 178.16891479492188
Loss :  1.6423115730285645 3.948943853378296 199.08950805664062
Loss :  1.6324963569641113 3.237133741378784 163.4891815185547
Loss :  1.6621147394180298 3.040294647216797 153.67684936523438
Loss :  1.6259018182754517 4.138920307159424 208.57191467285156
Loss :  1.625807285308838 3.17777156829834 160.51438903808594
Loss :  1.620174765586853 3.0034384727478027 151.79209899902344
Loss :  1.626602292060852 3.9143896102905273 197.34608459472656
Loss :  1.6750684976577759 3.2189676761627197 162.62344360351562
Loss :  1.6753051280975342 3.046147584915161 153.98268127441406
Loss :  1.61948561668396 3.532174587249756 178.2282257080078
Loss :  1.647791862487793 3.5769660472869873 180.49609375
Loss :  1.620676040649414 3.2218520641326904 162.71328735351562
Loss :  1.6707651615142822 3.3229823112487793 167.8198699951172
  batch 20 loss: 1.6707651615142822, 3.3229823112487793, 167.8198699951172
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.639980673789978 3.0812900066375732 155.70448303222656
Loss :  1.6166415214538574 3.602139949798584 181.7236328125
Loss :  1.631543755531311 2.9417927265167236 148.72117614746094
Loss :  1.6470484733581543 2.8334851264953613 143.32130432128906
Loss :  1.6732597351074219 2.9936718940734863 151.3568572998047
Loss :  1.6327457427978516 3.054835319519043 154.37451171875
Loss :  1.6412500143051147 3.478867530822754 175.58462524414062
Loss :  1.6350271701812744 3.1205379962921143 157.66192626953125
Loss :  1.5854138135910034 2.8838069438934326 145.77577209472656
Loss :  1.665392279624939 2.983048915863037 150.81784057617188
Loss :  1.586685299873352 3.1475701332092285 158.96519470214844
Loss :  1.6509242057800293 3.390296459197998 171.16574096679688
Loss :  1.6271554231643677 3.337688446044922 168.51158142089844
Loss :  1.6254907846450806 3.2762041091918945 165.43569946289062
Loss :  1.592029333114624 2.9657206535339355 149.8780517578125
Loss :  1.6066726446151733 3.2526888847351074 164.24111938476562
Loss :  1.6052780151367188 3.0341098308563232 153.31076049804688
Loss :  1.6636817455291748 3.3418185710906982 168.75460815429688
Loss :  1.6682432889938354 2.7562143802642822 139.4789581298828
Loss :  1.6782759428024292 2.9282774925231934 148.09214782714844
  batch 40 loss: 1.6782759428024292, 2.9282774925231934, 148.09214782714844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6396929025650024 3.0474257469177246 154.010986328125
Loss :  1.6276681423187256 2.857933759689331 144.52435302734375
Loss :  1.6179450750350952 3.140796661376953 158.65777587890625
Loss :  1.6303125619888306 2.7727906703948975 140.26983642578125
Loss :  1.6117738485336304 2.6399316787719727 133.6083526611328
Loss :  1.640225887298584 3.022461414337158 152.7633056640625
Loss :  1.671289086341858 3.3003458976745605 166.68858337402344
Loss :  1.6293784379959106 3.0300674438476562 153.13275146484375
Loss :  1.6867139339447021 3.7183737754821777 187.6053924560547
Loss :  1.6332895755767822 3.207674980163574 162.01702880859375
Loss :  1.6636706590652466 2.8294625282287598 143.1367950439453
Loss :  1.657312035560608 2.7040140628814697 136.85801696777344
Loss :  1.638146996498108 2.9235167503356934 147.81399536132812
Loss :  1.6654244661331177 3.185877799987793 160.9593048095703
Loss :  1.62883722782135 3.1573758125305176 159.49761962890625
Loss :  1.6852701902389526 2.892402410507202 146.30538940429688
Loss :  1.6344212293624878 2.8566088676452637 144.46485900878906
Loss :  1.620829701423645 2.765059232711792 139.87379455566406
Loss :  1.6347322463989258 3.0431947708129883 153.7944793701172
Loss :  1.6937909126281738 4.302529811859131 216.82028198242188
  batch 60 loss: 1.6937909126281738, 4.302529811859131, 216.82028198242188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.623584508895874 3.220552682876587 162.65121459960938
Loss :  1.6505402326583862 2.8879666328430176 146.0488739013672
Loss :  1.63307785987854 3.3650519847869873 169.8856658935547
Loss :  1.6238471269607544 3.5498881340026855 179.1182403564453
Loss :  1.6044400930404663 3.1628286838531494 159.74588012695312
Loss :  1.6409415006637573 4.130035400390625 208.14271545410156
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  1.6532468795776367 4.007179260253906 202.01220703125
Loss :  1.6500213146209717 3.91001558303833 197.1508026123047
Loss :  1.6553479433059692 4.004781246185303 201.8944091796875
Total LOSS train 161.2685800405649 valid 202.30003356933594
CE LOSS train 1.6408891402758086 valid 0.4138369858264923
Contrastive LOSS train 3.1925538319807787 valid 1.0011953115463257
EPOCH 132:
Loss :  1.662751317024231 4.08788537979126 206.05702209472656
Loss :  1.6750333309173584 3.801236867904663 191.73687744140625
Loss :  1.6485683917999268 4.09509801864624 206.40347290039062
Loss :  1.6556161642074585 3.606717348098755 181.99147033691406
Loss :  1.6723381280899048 3.5049118995666504 176.9179229736328
Loss :  1.6378628015518188 3.3537299633026123 169.32435607910156
Loss :  1.6698172092437744 3.4433178901672363 173.83570861816406
Loss :  1.6432769298553467 3.2806594371795654 165.67625427246094
Loss :  1.6373704671859741 3.1214118003845215 157.7079620361328
Loss :  1.6642447710037231 3.206578493118286 161.9931640625
Loss :  1.6175984144210815 3.551257371902466 179.1804656982422
Loss :  1.6178301572799683 3.2387590408325195 163.5557861328125
Loss :  1.6111146211624146 2.986499071121216 150.93606567382812
Loss :  1.6173371076583862 3.504871129989624 176.86090087890625
Loss :  1.670560359954834 3.459294080734253 174.63526916503906
Loss :  1.6635499000549316 3.5181453227996826 177.57081604003906
Loss :  1.6045972108840942 3.3361001014709473 168.40960693359375
Loss :  1.6314637660980225 3.2995083332061768 166.6068878173828
Loss :  1.6017369031906128 3.1385490894317627 158.52919006347656
Loss :  1.6614861488342285 3.1926231384277344 161.2926483154297
  batch 20 loss: 1.6614861488342285, 3.1926231384277344, 161.2926483154297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6257463693618774 3.055047035217285 154.3780975341797
Loss :  1.598607063293457 3.480961561203003 175.6466827392578
Loss :  1.616595983505249 3.381951332092285 170.7141571044922
Loss :  1.631638526916504 3.46358060836792 174.8106689453125
Loss :  1.6596407890319824 3.5645365715026855 179.88645935058594
Loss :  1.616914987564087 3.6334211826324463 183.28797912597656
Loss :  1.6266379356384277 3.5759501457214355 180.42413330078125
Loss :  1.6225132942199707 3.2695910930633545 165.10206604003906
Loss :  1.5687038898468018 2.9161794185638428 147.377685546875
Loss :  1.6572595834732056 3.5928022861480713 181.29737854003906
Loss :  1.5715140104293823 3.1940722465515137 161.27513122558594
Loss :  1.642561435699463 3.201630115509033 161.7240753173828
Loss :  1.6163527965545654 3.222303628921509 162.73153686523438
Loss :  1.6153935194015503 2.7744967937469482 140.34022521972656
Loss :  1.5804811716079712 2.888765573501587 146.0187530517578
Loss :  1.5965741872787476 3.788372755050659 191.0152130126953
Loss :  1.5951365232467651 2.9496896266937256 149.07962036132812
Loss :  1.656883716583252 3.0573647022247314 154.52513122558594
Loss :  1.6611777544021606 2.907365083694458 147.02943420410156
Loss :  1.6708096265792847 2.855281352996826 144.43487548828125
  batch 40 loss: 1.6708096265792847, 2.855281352996826, 144.43487548828125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6279232501983643 3.9675533771514893 200.00559997558594
Loss :  1.6143606901168823 3.9556663036346436 199.39767456054688
Loss :  1.603297233581543 3.164799213409424 159.84326171875
Loss :  1.615617275238037 3.3261220455169678 167.92172241210938
Loss :  1.5942342281341553 3.2848641872406006 165.8374481201172
Loss :  1.6246397495269775 2.5194406509399414 127.59667205810547
Loss :  1.657092571258545 2.7990200519561768 141.60809326171875
Loss :  1.6102418899536133 3.2385458946228027 163.53753662109375
Loss :  1.67449951171875 3.2115585803985596 162.25242614746094
Loss :  1.6153532266616821 2.7779998779296875 140.51535034179688
Loss :  1.6504660844802856 2.8940577507019043 146.3533477783203
Loss :  1.6437658071517944 3.431635856628418 173.22555541992188
Loss :  1.6228030920028687 2.929039239883423 148.07476806640625
Loss :  1.6532539129257202 2.8403141498565674 143.66896057128906
Loss :  1.6127902269363403 2.889878749847412 146.1067352294922
Loss :  1.6751437187194824 2.6852498054504395 135.93763732910156
Loss :  1.6196603775024414 3.0814778804779053 155.6935577392578
Loss :  1.6043449640274048 2.9128801822662354 147.24835205078125
Loss :  1.6184370517730713 2.9373674392700195 148.48681640625
Loss :  1.6823992729187012 2.7113759517669678 137.25120544433594
  batch 60 loss: 1.6823992729187012, 2.7113759517669678, 137.25120544433594
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.603089690208435 2.8595993518829346 144.5830535888672
Loss :  1.6333445310592651 3.0387792587280273 153.5723114013672
Loss :  1.6141741275787354 3.1763389110565186 160.43112182617188
Loss :  1.6024982929229736 3.347602128982544 168.98260498046875
Loss :  1.582276463508606 2.6322197914123535 133.19326782226562
Loss :  1.6164188385009766 3.7317893505096436 188.2058868408203
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6298561096191406 3.6970372200012207 186.48171997070312
Loss :  1.6255320310592651 3.6351099014282227 183.3810272216797
Loss :  1.633296012878418 3.418583393096924 172.56246948242188
Total LOSS train 163.25609588623047 valid 182.65777587890625
CE LOSS train 1.6300154539254996 valid 0.4083240032196045
Contrastive LOSS train 3.2325215926537147 valid 0.854645848274231
EPOCH 133:
Loss :  1.6476902961730957 3.1361083984375 158.45310974121094
Loss :  1.659642219543457 3.012340784072876 152.27667236328125
Loss :  1.6288222074508667 2.84403395652771 143.83050537109375
Loss :  1.634371042251587 3.478610038757324 175.5648651123047
Loss :  1.6523876190185547 2.8660430908203125 144.9545440673828
Loss :  1.6161494255065918 2.5589029788970947 129.56129455566406
Loss :  1.6530826091766357 2.8062164783477783 141.96389770507812
Loss :  1.628568410873413 2.868475914001465 145.0523681640625
Loss :  1.6197916269302368 2.8825957775115967 145.7495880126953
Loss :  1.6540476083755493 2.7025694847106934 136.78253173828125
Loss :  1.609618902206421 3.014441728591919 152.3317108154297
Loss :  1.6095143556594849 2.912351131439209 147.22706604003906
Loss :  1.6062431335449219 3.1812829971313477 160.67039489746094
Loss :  1.6134510040283203 3.0893664360046387 156.08177185058594
Loss :  1.6688402891159058 2.96885347366333 150.11151123046875
Loss :  1.6630052328109741 3.087918281555176 156.05892944335938
Loss :  1.6036399602890015 2.803924322128296 141.7998504638672
Loss :  1.6323832273483276 3.1192917823791504 157.5969696044922
Loss :  1.6020212173461914 2.6323447227478027 133.21925354003906
Loss :  1.6629449129104614 3.140700101852417 158.6979522705078
  batch 20 loss: 1.6629449129104614, 3.140700101852417, 158.6979522705078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6287541389465332 2.654503583908081 134.3539276123047
Loss :  1.602739691734314 2.9704809188842773 150.1267852783203
Loss :  1.621476650238037 2.5142433643341064 127.3336410522461
Loss :  1.637345790863037 3.7970826625823975 191.49147033691406
Loss :  1.6657958030700684 2.9598801136016846 149.65980529785156
Loss :  1.624516487121582 2.763720750808716 139.810546875
Loss :  1.6350809335708618 3.33123517036438 168.19683837890625
Loss :  1.6311404705047607 3.22821307182312 163.0417938232422
Loss :  1.580432415008545 3.227327823638916 162.9468231201172
Loss :  1.663884162902832 3.3663346767425537 169.98060607910156
Loss :  1.5805495977401733 3.2329015731811523 163.2256317138672
Loss :  1.6477718353271484 4.35189151763916 219.24234008789062
Loss :  1.622785210609436 3.242262125015259 163.73590087890625
Loss :  1.6208943128585815 2.7040581703186035 136.8238067626953
Loss :  1.5867962837219238 2.9443552494049072 148.80455017089844
Loss :  1.6022710800170898 3.0492823123931885 154.06637573242188
Loss :  1.6020219326019287 3.185236692428589 160.86386108398438
Loss :  1.6633220911026 3.0608139038085938 154.70401000976562
Loss :  1.6680539846420288 3.110220432281494 157.1790771484375
Loss :  1.6789944171905518 3.574381113052368 180.39805603027344
  batch 40 loss: 1.6789944171905518, 3.574381113052368, 180.39805603027344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6397124528884888 3.0944466590881348 156.36204528808594
Loss :  1.6272945404052734 2.8088667392730713 142.0706329345703
Loss :  1.6183286905288696 2.820709228515625 142.65379333496094
Loss :  1.6301525831222534 2.7008628845214844 136.67330932617188
Loss :  1.6090015172958374 2.983187437057495 150.7683868408203
Loss :  1.636305332183838 3.3322012424468994 168.24636840820312
Loss :  1.667723536491394 3.0654032230377197 154.93788146972656
Loss :  1.6237815618515015 2.8817059993743896 145.70907592773438
Loss :  1.6838560104370117 3.5157694816589355 177.47232055664062
Loss :  1.6294108629226685 2.862863302230835 144.77256774902344
Loss :  1.6612437963485718 3.5148274898529053 177.40261840820312
Loss :  1.6542823314666748 3.8810720443725586 195.7078857421875
Loss :  1.6349045038223267 2.921786069869995 147.72421264648438
Loss :  1.6625810861587524 2.9731740951538086 150.3212890625
Loss :  1.6252168416976929 3.255492925643921 164.3998565673828
Loss :  1.68276846408844 3.0595738887786865 154.66146850585938
Loss :  1.6310250759124756 3.1377804279327393 158.52005004882812
Loss :  1.616491436958313 3.4822769165039062 175.73033142089844
Loss :  1.6297913789749146 4.097009658813477 206.4802703857422
Loss :  1.6902936697006226 3.080874443054199 155.7340087890625
  batch 60 loss: 1.6902936697006226, 3.080874443054199, 155.7340087890625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6167157888412476 2.742063283920288 138.71987915039062
Loss :  1.6439677476882935 2.8062632083892822 141.95712280273438
Loss :  1.6257438659667969 2.6614530086517334 134.69839477539062
Loss :  1.614869236946106 3.1404967308044434 158.63970947265625
Loss :  1.5959607362747192 3.2855443954467773 165.87318420410156
Loss :  1.6320666074752808 3.798114061355591 191.5377655029297
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6447690725326538 3.8041462898254395 191.8520965576172
Loss :  1.6419254541397095 3.683518171310425 185.8178253173828
Loss :  1.6477528810501099 3.6089625358581543 182.0958709716797
Total LOSS train 155.8493430504432 valid 187.82588958740234
CE LOSS train 1.633573317527771 valid 0.41193822026252747
Contrastive LOSS train 3.084315413695115 valid 0.9022406339645386
EPOCH 134:
Loss :  1.6567846536636353 2.645958423614502 133.9547119140625
Loss :  1.668208360671997 3.1317152976989746 158.2539825439453
Loss :  1.6402076482772827 3.1023671627044678 156.75857543945312
Loss :  1.6453778743743896 2.8728086948394775 145.2858123779297
Loss :  1.6628895998001099 2.6691718101501465 135.12147521972656
Loss :  1.6287918090820312 3.098140239715576 156.53579711914062
Loss :  1.6636101007461548 3.212743043899536 162.30075073242188
Loss :  1.6394795179367065 2.775669813156128 140.4229736328125
Loss :  1.6299935579299927 3.1408796310424805 158.67396545410156
Loss :  1.660396933555603 3.6839189529418945 185.85635375976562
Loss :  1.6190649271011353 2.9523167610168457 149.2349090576172
Loss :  1.6194044351577759 3.3578951358795166 169.51416015625
Loss :  1.6154131889343262 2.85331392288208 144.28111267089844
Loss :  1.6225335597991943 3.6936800479888916 186.30653381347656
Loss :  1.6739468574523926 3.447300910949707 174.03900146484375
Loss :  1.6702295541763306 3.3380725383758545 168.5738525390625
Loss :  1.614331603050232 3.767829418182373 190.00579833984375
Loss :  1.6407724618911743 2.8352622985839844 143.40390014648438
Loss :  1.6121999025344849 3.0326151847839355 153.24295043945312
Loss :  1.6688311100006104 3.2194387912750244 162.64077758789062
  batch 20 loss: 1.6688311100006104, 3.2194387912750244, 162.64077758789062
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6367425918579102 2.5078647136688232 127.02997589111328
Loss :  1.613451361656189 3.4681808948516846 175.02249145507812
Loss :  1.6320582628250122 2.957092523574829 149.4866943359375
Loss :  1.646683692932129 3.2539222240448 164.34278869628906
Loss :  1.6726855039596558 3.6723666191101074 185.291015625
Loss :  1.6336480379104614 2.5874032974243164 131.00381469726562
Loss :  1.6417341232299805 4.015730381011963 202.42825317382812
Loss :  1.6372408866882324 3.167459011077881 160.01019287109375
Loss :  1.587760329246521 2.657892942428589 134.48240661621094
Loss :  1.6680853366851807 2.998084783554077 151.57232666015625
Loss :  1.5883711576461792 3.422203302383423 172.6985321044922
Loss :  1.6526201963424683 3.419700860977173 172.63766479492188
Loss :  1.627967357635498 3.706899881362915 186.97296142578125
Loss :  1.6265610456466675 3.032381534576416 153.24563598632812
Loss :  1.5935323238372803 3.290788173675537 166.13294982910156
Loss :  1.607390284538269 3.5105199813842773 177.13339233398438
Loss :  1.6063205003738403 3.126296281814575 157.921142578125
Loss :  1.6651358604431152 3.1583263874053955 159.58145141601562
Loss :  1.669787049293518 3.026637554168701 153.0016632080078
Loss :  1.6804287433624268 3.350446939468384 169.20278930664062
  batch 40 loss: 1.6804287433624268, 3.350446939468384, 169.20278930664062
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6426299810409546 3.9929893016815186 201.29208374023438
Loss :  1.6309458017349243 2.6651883125305176 134.89036560058594
Loss :  1.6241214275360107 3.1098008155822754 157.11415100097656
Loss :  1.6372591257095337 3.066253662109375 154.94993591308594
Loss :  1.6162397861480713 3.0221595764160156 152.72421264648438
Loss :  1.6441283226013184 3.65057635307312 184.17295837402344
Loss :  1.6730619668960571 3.2231881618499756 162.8324737548828
Loss :  1.629498839378357 3.113171100616455 157.28805541992188
Loss :  1.6865569353103638 3.120032787322998 157.6881866455078
Loss :  1.6323349475860596 3.1821658611297607 160.74063110351562
Loss :  1.6643764972686768 3.326087236404419 167.96875
Loss :  1.6563736200332642 3.9291274547576904 198.1127471923828
Loss :  1.6356191635131836 3.3082327842712402 167.04725646972656
Loss :  1.6624397039413452 3.1850481033325195 160.91485595703125
Loss :  1.624295949935913 3.0134899616241455 152.29879760742188
Loss :  1.6820993423461914 2.9186770915985107 147.61595153808594
Loss :  1.628701090812683 3.025679588317871 152.91268920898438
Loss :  1.6131165027618408 3.7510948181152344 189.16786193847656
Loss :  1.6268212795257568 2.9154269695281982 147.39816284179688
Loss :  1.688012719154358 2.7309255599975586 138.2342987060547
  batch 60 loss: 1.688012719154358, 2.7309255599975586, 138.2342987060547
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.6128780841827393 2.692431926727295 136.23448181152344
Loss :  1.6408790349960327 2.5838139057159424 130.83157348632812
Loss :  1.6222081184387207 2.856781244277954 144.46127319335938
Loss :  1.6115878820419312 3.0075342655181885 151.98829650878906
Loss :  1.59181809425354 2.599043607711792 131.5439910888672
Loss :  1.6251503229141235 3.7810683250427246 190.67857360839844
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.63751220703125 3.8199446201324463 192.63475036621094
Loss :  1.6344044208526611 3.7254862785339355 187.90870666503906
Loss :  1.641329050064087 3.5505502223968506 179.16883850097656
Total LOSS train 159.44660891019382 valid 187.59771728515625
CE LOSS train 1.6387488695291372 valid 0.41033226251602173
Contrastive LOSS train 3.1561571818131666 valid 0.8876375555992126
EPOCH 135:
Loss :  1.6546244621276855 2.7959256172180176 141.45089721679688
Loss :  1.6669411659240723 3.540377616882324 178.68582153320312
Loss :  1.6371591091156006 3.5860722064971924 180.94076538085938
Loss :  1.641894817352295 3.059011697769165 154.5924835205078
Loss :  1.6594759225845337 2.711132287979126 137.21607971191406
Loss :  1.6244200468063354 3.144340991973877 158.8414764404297
Loss :  1.660717487335205 3.016932249069214 152.50733947753906
Loss :  1.6373728513717651 4.000241279602051 201.64944458007812
Loss :  1.6306225061416626 3.0613620281219482 154.69871520996094
Loss :  1.6634607315063477 2.873305082321167 145.32872009277344
Loss :  1.6202901601791382 2.9264981746673584 147.9451904296875
Loss :  1.6195012331008911 2.899061441421509 146.5725860595703
Loss :  1.6166049242019653 3.0345075130462646 153.34197998046875
Loss :  1.6224042177200317 3.5860986709594727 180.92733764648438
Loss :  1.6756312847137451 3.193902015686035 161.3707275390625
Loss :  1.6683622598648071 2.8703761100769043 145.18716430664062
Loss :  1.610277771949768 3.6262106895446777 182.92080688476562
Loss :  1.6368076801300049 3.0491740703582764 154.09552001953125
Loss :  1.6068562269210815 2.669591188430786 135.08641052246094
Loss :  1.665616512298584 2.82181453704834 142.75634765625
  batch 20 loss: 1.665616512298584, 2.82181453704834, 142.75634765625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.6316279172897339 3.7539291381835938 189.3280792236328
Loss :  1.605474591255188 3.136464834213257 158.4287109375
Loss :  1.62423837184906 3.274658203125 165.35714721679688
Loss :  1.6400333642959595 3.4188177585601807 172.58091735839844
Loss :  1.6681394577026367 3.7171123027801514 187.5237579345703
Loss :  1.6282488107681274 3.2647602558135986 164.8662567138672
Loss :  1.63796865940094 2.9195361137390137 147.61477661132812
Loss :  1.634063959121704 2.717233180999756 137.4957275390625
Loss :  1.5834182500839233 3.921506404876709 197.6587371826172
Loss :  1.6649948358535767 3.051046133041382 154.21730041503906
Loss :  1.5843576192855835 3.1043319702148438 156.8009490966797
Loss :  1.6497385501861572 3.1138229370117188 157.34088134765625
Loss :  1.6258445978164673 2.905426025390625 146.8971405029297
Loss :  1.6239806413650513 3.639674663543701 183.6077117919922
Loss :  1.5908178091049194 3.278052568435669 165.4934539794922
Loss :  1.6059496402740479 3.0586419105529785 154.53805541992188
Loss :  1.6041204929351807 3.407177686691284 171.9630126953125
Loss :  1.663178563117981 2.9149842262268066 147.41238403320312
Loss :  1.6669812202453613 2.933788537979126 148.3563995361328
Loss :  1.6771502494812012 3.271270275115967 165.24066162109375
  batch 40 loss: 1.6771502494812012, 3.271270275115967, 165.24066162109375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.6371361017227173 3.0435984134674072 153.81704711914062
Loss :  1.6249045133590698 3.111262559890747 157.1880340576172
Loss :  1.6161704063415527 3.2515852451324463 164.1954345703125
Loss :  1.6293092966079712 2.9589128494262695 149.574951171875
Loss :  1.6092990636825562 2.688065767288208 136.01258850097656
Loss :  1.6374194622039795 3.4110755920410156 172.19119262695312
Loss :  1.6690316200256348 2.790825605392456 141.21031188964844
Loss :  1.6270040273666382 3.0896902084350586 156.11151123046875
Loss :  1.6863256692886353 2.55654239654541 129.51344299316406
Loss :  1.6351641416549683 2.852916955947876 144.281005859375
Loss :  1.667480230331421 3.902733564376831 196.8041534423828
Loss :  1.6607717275619507 3.10893177986145 157.10736083984375
Loss :  1.642316222190857 2.7887094020843506 141.07778930664062
Loss :  1.6678529977798462 3.064866542816162 154.9111785888672
Loss :  1.6319319009780884 2.9433066844940186 148.79725646972656
Loss :  1.686922311782837 2.59199595451355 131.28671264648438
Loss :  1.6367641687393188 3.540174722671509 178.6455078125
Loss :  1.6223422288894653 2.5869288444519043 130.96878051757812
Loss :  1.6354749202728271 3.42215895652771 172.743408203125
Loss :  1.6939142942428589 2.845048189163208 143.94631958007812
  batch 60 loss: 1.6939142942428589, 2.845048189163208, 143.94631958007812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.6224695444107056 2.7746002674102783 140.35247802734375
Loss :  1.6487513780593872 3.1194002628326416 157.6187744140625
Loss :  1.6305185556411743 3.4352529048919678 173.39317321777344
Loss :  1.6193506717681885 2.9882988929748535 151.0343017578125
Loss :  1.600082278251648 2.577376365661621 130.46890258789062
Loss :  1.6325123310089111 4.055334091186523 204.3992156982422
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.6444478034973145 4.0180487632751465 202.54689025878906
Loss :  1.6406174898147583 3.878993511199951 195.59030151367188
Loss :  1.6482056379318237 3.826456308364868 192.9710235595703
Total LOSS train 157.5706075815054 valid 198.87685775756836
CE LOSS train 1.6379704108605018 valid 0.41205140948295593
Contrastive LOSS train 3.118652761899508 valid 0.956614077091217
EPOCH 136:
Loss :  1.6595516204833984 3.02026104927063 152.67259216308594
Loss :  1.671320915222168 3.0651659965515137 154.92962646484375
Loss :  1.6430751085281372 3.3663601875305176 169.96109008789062
Loss :  1.647593379020691 2.8609654903411865 144.6958770751953
Loss :  1.6648824214935303 3.3008105754852295 166.70541381835938
Loss :  1.6327275037765503 2.647244691848755 133.9949493408203
Loss :  1.6664663553237915 2.7359092235565186 138.46192932128906
Loss :  1.6443692445755005 2.91062068939209 147.17539978027344
Loss :  1.6363273859024048 3.049002170562744 154.08642578125
Loss :  1.6676632165908813 3.367332696914673 170.0343017578125
Loss :  1.627972960472107 3.2587649822235107 164.56622314453125
Loss :  1.6284066438674927 3.186427354812622 160.94976806640625
Loss :  1.6258965730667114 2.9299962520599365 148.12571716308594
Loss :  1.6320003271102905 3.237224578857422 163.4932403564453
Loss :  1.6830095052719116 3.201303005218506 161.7481689453125
Loss :  1.676444411277771 3.045712471008301 153.96206665039062
Loss :  1.6247650384902954 3.098464012145996 156.5479736328125
Loss :  1.6492329835891724 3.3380346298217773 168.55096435546875
Loss :  1.6235239505767822 2.7996041774749756 141.60372924804688
Loss :  1.6788394451141357 2.7739169597625732 140.3746795654297
  batch 20 loss: 1.6788394451141357, 2.7739169597625732, 140.3746795654297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.6471697092056274 3.0148086547851562 152.38760375976562
Loss :  1.6228562593460083 3.256248712539673 164.435302734375
Loss :  1.639237880706787 3.012986660003662 152.28857421875
Loss :  1.6506786346435547 3.5061752796173096 176.95944213867188
Loss :  1.6756350994110107 3.124083995819092 157.87982177734375
Loss :  1.6373798847198486 2.863759756088257 144.8253631591797
Loss :  1.6459540128707886 3.080874443054199 155.68966674804688
Loss :  1.6408928632736206 3.2770943641662598 165.49562072753906
Loss :  1.5931713581085205 3.296830177307129 166.43467712402344
Loss :  1.6708818674087524 2.6723310947418213 135.28744506835938
Loss :  1.5947004556655884 3.645714282989502 183.8804168701172
Loss :  1.6584059000015259 3.066267490386963 154.97177124023438
Loss :  1.636428713798523 2.8046791553497314 141.87039184570312
Loss :  1.6361504793167114 2.7523674964904785 139.25453186035156
Loss :  1.6047576665878296 3.6879286766052246 186.00119018554688
Loss :  1.6184231042861938 3.3425240516662598 168.74462890625
Loss :  1.6159220933914185 2.713301181793213 137.28097534179688
Loss :  1.6703848838806152 3.009643077850342 152.15252685546875
Loss :  1.6736809015274048 3.3865959644317627 171.00347900390625
Loss :  1.683001160621643 3.284968376159668 165.93141174316406
  batch 40 loss: 1.683001160621643, 3.284968376159668, 165.93141174316406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6449021100997925 3.166576862335205 159.97373962402344
Loss :  1.631739854812622 3.006514549255371 151.9574737548828
Loss :  1.6210170984268188 2.682631015777588 135.7525634765625
Loss :  1.6328498125076294 3.2056052684783936 161.91310119628906
Loss :  1.6113563776016235 3.0779242515563965 155.507568359375
Loss :  1.6384302377700806 3.128371000289917 158.05697631835938
Loss :  1.6687403917312622 3.515929698944092 177.46522521972656
Loss :  1.6255580186843872 3.107133388519287 156.98223876953125
Loss :  1.6856458187103271 3.051982879638672 154.2847900390625
Loss :  1.6316512823104858 2.4781365394592285 125.53848266601562
Loss :  1.6647553443908691 3.315741777420044 167.45184326171875
Loss :  1.657945990562439 3.010049343109131 152.16041564941406
Loss :  1.6394599676132202 4.218027591705322 212.5408477783203
Loss :  1.6658499240875244 2.6090087890625 132.1162872314453
Loss :  1.629294514656067 3.4846808910369873 175.8633270263672
Loss :  1.6851599216461182 2.9712653160095215 150.24842834472656
Loss :  1.6340725421905518 2.9230806827545166 147.78811645507812
Loss :  1.6203656196594238 3.6307663917541504 183.15867614746094
Loss :  1.633872628211975 3.6937713623046875 186.3224334716797
Loss :  1.6925439834594727 3.1786768436431885 160.6263885498047
  batch 60 loss: 1.6925439834594727, 3.1786768436431885, 160.6263885498047
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.6194684505462646 2.7481791973114014 139.02842712402344
Loss :  1.6462863683700562 3.409414529800415 172.1170196533203
Loss :  1.6269350051879883 3.1217799186706543 157.71592712402344
Loss :  1.6174908876419067 3.8119091987609863 192.21295166015625
Loss :  1.5973447561264038 3.3137078285217285 167.28274536132812
Loss :  1.6252878904342651 3.6840195655822754 185.82626342773438
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  1.6383610963821411 3.554333448410034 179.35504150390625
Loss :  1.633885383605957 3.568037509918213 180.03575134277344
Loss :  1.6401827335357666 3.3969807624816895 171.48922729492188
Total LOSS train 158.45355341984674 valid 179.17657089233398
CE LOSS train 1.643423366546631 valid 0.41004568338394165
Contrastive LOSS train 3.1362026031200703 valid 0.8492451906204224
EPOCH 137:
Loss :  1.6572041511535645 3.097292184829712 156.52182006835938
Loss :  1.6691904067993164 3.269735813140869 165.15597534179688
Loss :  1.6420810222625732 3.1611251831054688 159.69833374023438
Loss :  1.6461877822875977 2.851783275604248 144.2353515625
Loss :  1.6636183261871338 3.001675605773926 151.74740600585938
Loss :  1.6287412643432617 3.3451924324035645 168.88836669921875
Loss :  1.663003921508789 2.815922498703003 142.45913696289062
Loss :  1.6391304731369019 2.5895416736602783 131.1162109375
Loss :  1.6307190656661987 3.1312248706817627 158.1919708251953
Loss :  1.6627402305603027 2.817810297012329 142.55325317382812
Loss :  1.6205871105194092 3.4742538928985596 175.33328247070312
Loss :  1.6218369007110596 2.754467248916626 139.34519958496094
Loss :  1.6188236474990845 2.5222482681274414 127.73123168945312
Loss :  1.6255723237991333 3.1346962451934814 158.3603973388672
Loss :  1.6771011352539062 2.8987059593200684 146.61239624023438
Loss :  1.6713850498199463 2.7607955932617188 139.71116638183594
Loss :  1.6159995794296265 2.91450572013855 147.34127807617188
Loss :  1.6422396898269653 2.7338497638702393 138.33473205566406
Loss :  1.6150795221328735 3.4205708503723145 172.6436309814453
Loss :  1.6721758842468262 3.1337544918060303 158.3599090576172
  batch 20 loss: 1.6721758842468262, 3.1337544918060303, 158.3599090576172
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6397279500961304 3.2149722576141357 162.38833618164062
Loss :  1.6154649257659912 4.201033592224121 211.66714477539062
Loss :  1.6326465606689453 2.5807912349700928 130.67221069335938
Loss :  1.6463520526885986 2.6488289833068848 134.08779907226562
Loss :  1.6728073358535767 3.2561137676239014 164.47850036621094
Loss :  1.634577751159668 3.1396543979644775 158.6173095703125
Loss :  1.644221305847168 2.91668701171875 147.47857666015625
Loss :  1.6408884525299072 3.9451637268066406 198.89906311035156
Loss :  1.5942302942276 3.6120336055755615 182.19590759277344
Loss :  1.6731901168823242 2.717470407485962 137.5467071533203
Loss :  1.5978362560272217 2.758979558944702 139.54681396484375
Loss :  1.6609729528427124 3.0013527870178223 151.72862243652344
Loss :  1.6379244327545166 3.241614580154419 163.71865844726562
Loss :  1.6355040073394775 2.948267698287964 149.04888916015625
Loss :  1.603775143623352 3.204993963241577 161.8534698486328
Loss :  1.616218090057373 2.6846015453338623 135.84628295898438
Loss :  1.6147149801254272 3.0324318408966064 153.23631286621094
Loss :  1.670480489730835 3.424649953842163 172.90298461914062
Loss :  1.673989176750183 3.003506898880005 151.84933471679688
Loss :  1.6839958429336548 2.7375235557556152 138.56016540527344
  batch 40 loss: 1.6839958429336548, 2.7375235557556152, 138.56016540527344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.6477277278900146 2.986628532409668 150.97914123535156
Loss :  1.6360859870910645 3.1738014221191406 160.32615661621094
Loss :  1.6273821592330933 3.175157070159912 160.38523864746094
Loss :  1.6405011415481567 3.3646116256713867 169.87107849121094
Loss :  1.6216295957565308 3.0081353187561035 152.0283966064453
Loss :  1.6480261087417603 3.1561172008514404 159.45388793945312
Loss :  1.6764719486236572 3.0809521675109863 155.7240753173828
Loss :  1.634536623954773 3.044297695159912 153.84942626953125
Loss :  1.6892813444137573 2.9884071350097656 151.10963439941406
Loss :  1.6378774642944336 3.108452796936035 157.06051635742188
Loss :  1.668109655380249 3.243457794189453 163.8409881591797
Loss :  1.6619117259979248 3.9138762950897217 197.35572814941406
Loss :  1.6444777250289917 2.938612461090088 148.57508850097656
Loss :  1.669627070426941 3.2102487087249756 162.18206787109375
Loss :  1.6345189809799194 3.2912914752960205 166.1990966796875
Loss :  1.688485026359558 2.9332802295684814 148.35250854492188
Loss :  1.6398353576660156 2.995171546936035 151.39840698242188
Loss :  1.6264013051986694 3.7456743717193604 188.91012573242188
Loss :  1.6391468048095703 2.9451911449432373 148.89869689941406
Loss :  1.6959376335144043 2.505890130996704 126.99044799804688
  batch 60 loss: 1.6959376335144043, 2.505890130996704, 126.99044799804688
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.626186728477478 3.3453640937805176 168.89439392089844
Loss :  1.6534411907196045 3.2168753147125244 162.49720764160156
Loss :  1.6362227201461792 3.0810160636901855 155.68701171875
Loss :  1.6275805234909058 3.5108680725097656 177.1709747314453
Loss :  1.6110047101974487 2.3779680728912354 120.50941467285156
Loss :  1.6452723741531372 4.1737260818481445 210.3315887451172
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.656786561012268 4.1962080001831055 211.46717834472656
Loss :  1.6537078619003296 4.058961391448975 204.60177612304688
Loss :  1.6596968173980713 3.903406858444214 196.83004760742188
Total LOSS train 155.8294438288762 valid 205.80764770507812
CE LOSS train 1.6439283517690806 valid 0.4149242043495178
Contrastive LOSS train 3.0837103073413554 valid 0.9758517146110535
EPOCH 138:
Loss :  1.6683249473571777 3.226008176803589 162.96873474121094
Loss :  1.6808544397354126 3.28971529006958 166.16661071777344
Loss :  1.6542505025863647 2.992966651916504 151.30258178710938
Loss :  1.6584343910217285 3.6605734825134277 184.68710327148438
Loss :  1.6737697124481201 3.3154091835021973 167.44422912597656
Loss :  1.6394169330596924 3.3331985473632812 168.29934692382812
Loss :  1.670480728149414 3.808011531829834 192.07106018066406
Loss :  1.647306203842163 3.093806505203247 156.33763122558594
Loss :  1.6372559070587158 3.3652212619781494 169.8983154296875
Loss :  1.6670459508895874 3.1989331245422363 161.61370849609375
Loss :  1.6294196844100952 3.077606201171875 155.50973510742188
Loss :  1.628404974937439 3.3106281757354736 167.15980529785156
Loss :  1.626739263534546 3.2237424850463867 162.81385803222656
Loss :  1.6322764158248901 3.8080477714538574 192.03466796875
Loss :  1.6837048530578613 3.157508611679077 159.5591278076172
Loss :  1.6754947900772095 3.18546462059021 160.94871520996094
Loss :  1.6222182512283325 4.002407550811768 201.7425994873047
Loss :  1.646528959274292 2.9750125408172607 150.39715576171875
Loss :  1.6185228824615479 2.9244940280914307 147.84323120117188
Loss :  1.675517201423645 2.9619994163513184 149.77549743652344
  batch 20 loss: 1.675517201423645, 2.9619994163513184, 149.77549743652344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.6420031785964966 3.2560055255889893 164.44227600097656
Loss :  1.6165846586227417 3.1828739643096924 160.76026916503906
Loss :  1.63176691532135 3.1552889347076416 159.39620971679688
Loss :  1.643577218055725 3.5373685359954834 178.51199340820312
Loss :  1.671072006225586 3.246753215789795 164.00872802734375
Loss :  1.6314643621444702 3.0441575050354004 153.83934020996094
Loss :  1.6397647857666016 3.62068772315979 182.67416381835938
Loss :  1.6360114812850952 3.0775671005249023 155.51437377929688
Loss :  1.5868983268737793 3.0514626502990723 154.1600341796875
Loss :  1.668776273727417 3.0521013736724854 154.2738494873047
Loss :  1.5894224643707275 3.3284718990325928 168.0130157470703
Loss :  1.6543816328048706 3.1209869384765625 157.7037353515625
Loss :  1.6296550035476685 3.41086483001709 172.1728973388672
Loss :  1.6276369094848633 2.859092950820923 144.58229064941406
Loss :  1.595497488975525 3.069483757019043 155.06968688964844
Loss :  1.610725998878479 3.124329090118408 157.8271942138672
Loss :  1.6100932359695435 2.7982113361358643 141.52066040039062
Loss :  1.6674411296844482 2.8396081924438477 143.64784240722656
Loss :  1.6705960035324097 3.7431130409240723 188.8262481689453
Loss :  1.6799503564834595 3.700754404067993 186.71766662597656
  batch 40 loss: 1.6799503564834595, 3.700754404067993, 186.71766662597656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.6415411233901978 3.0967583656311035 156.47946166992188
Loss :  1.6285841464996338 3.205754041671753 161.91629028320312
Loss :  1.6193573474884033 2.848381519317627 144.03843688964844
Loss :  1.6327135562896729 3.3824808597564697 170.7567596435547
Loss :  1.6120048761367798 3.087235450744629 155.97377014160156
Loss :  1.6392070055007935 3.0418715476989746 153.7327880859375
Loss :  1.6691633462905884 3.173649787902832 160.35165405273438
Loss :  1.625592589378357 2.798903226852417 141.5707550048828
Loss :  1.6839416027069092 2.8819892406463623 145.7834014892578
Loss :  1.6304740905761719 2.7266054153442383 137.96075439453125
Loss :  1.6617958545684814 3.399569034576416 171.64024353027344
Loss :  1.6554737091064453 3.0750207901000977 155.40650939941406
Loss :  1.637265682220459 2.6525824069976807 134.26638793945312
Loss :  1.664221167564392 2.6771605014801025 135.52224731445312
Loss :  1.6289819478988647 3.3037986755371094 166.8189239501953
Loss :  1.6862965822219849 2.866577625274658 145.0151824951172
Loss :  1.635318636894226 2.7520811557769775 139.2393798828125
Loss :  1.6210029125213623 3.850389003753662 194.1404571533203
Loss :  1.634520411491394 2.8717455863952637 145.2218017578125
Loss :  1.6928761005401611 2.7513155937194824 139.25865173339844
  batch 60 loss: 1.6928761005401611, 2.7513155937194824, 139.25865173339844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6219009160995483 2.7396085262298584 138.60232543945312
Loss :  1.6489540338516235 2.551154375076294 129.2066650390625
Loss :  1.630462646484375 3.6351592540740967 183.388427734375
Loss :  1.6204321384429932 3.86858868598938 195.04986572265625
Loss :  1.6025505065917969 2.4358625411987305 123.39567565917969
Loss :  1.6404284238815308 4.183757305145264 210.8282928466797
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.65065336227417 4.127894401550293 208.0453643798828
Loss :  1.6485117673873901 4.099961280822754 206.64657592773438
Loss :  1.6549984216690063 3.9552977085113525 199.41989135742188
Total LOSS train 159.95343088003304 valid 206.2350311279297
CE LOSS train 1.6425218362074632 valid 0.4137496054172516
Contrastive LOSS train 3.1662181744208704 valid 0.9888244271278381
EPOCH 139:
Loss :  1.6633145809173584 3.21687650680542 162.50714111328125
Loss :  1.6757903099060059 3.756618022918701 189.50669860839844
Loss :  1.646783471107483 2.723604679107666 137.8270263671875
Loss :  1.6527565717697144 3.2460219860076904 163.953857421875
Loss :  1.6685967445373535 3.068406820297241 155.08892822265625
Loss :  1.6362433433532715 3.228814125061035 163.0769500732422
Loss :  1.6676663160324097 2.906641721725464 146.999755859375
Loss :  1.647309422492981 3.0240020751953125 152.847412109375
Loss :  1.6383110284805298 2.9669880867004395 149.9877166748047
Loss :  1.6684956550598145 2.836601495742798 143.4985809326172
Loss :  1.6299136877059937 3.2609944343566895 164.6796417236328
Loss :  1.6268552541732788 4.0470380783081055 203.978759765625
Loss :  1.6237255334854126 3.696183443069458 186.43289184570312
Loss :  1.630420207977295 3.9997575283050537 201.6182861328125
Loss :  1.6803178787231445 3.236074686050415 163.4840545654297
Loss :  1.6747936010360718 3.2988076210021973 166.61517333984375
Loss :  1.6227335929870605 2.9209303855895996 147.66925048828125
Loss :  1.648861289024353 3.033707857131958 153.33425903320312
Loss :  1.6195030212402344 3.390515089035034 171.145263671875
Loss :  1.6778067350387573 3.1980409622192383 161.57986450195312
  batch 20 loss: 1.6778067350387573, 3.1980409622192383, 161.57986450195312
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.642853021621704 3.1177380084991455 157.52975463867188
Loss :  1.6189929246902466 2.881023406982422 145.670166015625
Loss :  1.633283257484436 3.151407480239868 159.2036590576172
Loss :  1.6461150646209717 3.8338730335235596 193.3397674560547
Loss :  1.6726890802383423 3.308812141418457 167.11329650878906
Loss :  1.634190320968628 3.128587484359741 158.0635528564453
Loss :  1.6445871591567993 3.2415995597839355 163.7245635986328
Loss :  1.6393593549728394 3.935979127883911 198.4383087158203
Loss :  1.5912765264511108 3.106523036956787 156.9174346923828
Loss :  1.6701737642288208 3.447232961654663 174.03182983398438
Loss :  1.5930068492889404 3.075435161590576 155.36476135253906
Loss :  1.6562857627868652 3.072922945022583 155.30242919921875
Loss :  1.6324374675750732 3.1776344776153564 160.51416015625
Loss :  1.6305327415466309 3.049393653869629 154.1002197265625
Loss :  1.5988678932189941 3.19191575050354 161.1946563720703
Loss :  1.6130450963974 3.5808968544006348 180.65789794921875
Loss :  1.6123842000961304 3.049257516860962 154.07525634765625
Loss :  1.670021891593933 3.3312578201293945 168.23292541503906
Loss :  1.6733018159866333 3.532306671142578 178.28863525390625
Loss :  1.6824251413345337 3.3731534481048584 170.340087890625
  batch 40 loss: 1.6824251413345337, 3.3731534481048584, 170.340087890625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6448434591293335 3.851374626159668 194.21356201171875
Loss :  1.6321548223495483 3.935307025909424 198.3975067138672
Loss :  1.6230024099349976 2.7106263637542725 137.15431213378906
Loss :  1.635335087776184 2.7299091815948486 138.1307830810547
Loss :  1.613908290863037 3.021198034286499 152.67381286621094
Loss :  1.6400035619735718 2.7797622680664062 140.62811279296875
Loss :  1.6697072982788086 3.0944862365722656 156.39401245117188
Loss :  1.625031590461731 3.0040838718414307 151.8292236328125
Loss :  1.6840651035308838 3.199948310852051 161.68148803710938
Loss :  1.6298288106918335 3.674063205718994 185.33297729492188
Loss :  1.661454677581787 2.9619836807250977 149.76063537597656
Loss :  1.6551603078842163 2.8209755420684814 142.70394897460938
Loss :  1.6362110376358032 3.153873920440674 159.3299102783203
Loss :  1.6636378765106201 2.7129969596862793 137.3134765625
Loss :  1.6266125440597534 3.6949610710144043 186.37466430664062
Loss :  1.6863672733306885 2.8616089820861816 144.76681518554688
Loss :  1.6332132816314697 2.759639024734497 139.6151580810547
Loss :  1.6173195838928223 2.709385871887207 137.0866241455078
Loss :  1.6314834356307983 4.30072546005249 216.66775512695312
Loss :  1.6923153400421143 3.334679126739502 168.42628479003906
  batch 60 loss: 1.6923153400421143, 3.334679126739502, 168.42628479003906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.6178244352340698 2.785555601119995 140.89561462402344
Loss :  1.64561128616333 2.706728219985962 136.98202514648438
Loss :  1.6268724203109741 2.674508571624756 135.35231018066406
Loss :  1.6133462190628052 3.1360883712768555 158.41775512695312
Loss :  1.5936486721038818 2.08054256439209 105.62078094482422
Loss :  1.6351336240768433 4.270814895629883 215.17587280273438
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6475557088851929 4.244070053100586 213.85105895996094
Loss :  1.6389949321746826 4.1410813331604 208.69305419921875
Loss :  1.6767266988754272 4.0181050300598145 202.5819854736328
Total LOSS train 161.13360654390775 valid 210.07549285888672
CE LOSS train 1.6423844062365018 valid 0.4191816747188568
Contrastive LOSS train 3.1898244344271145 valid 1.0045262575149536
EPOCH 140:
Loss :  1.6642484664916992 2.7647297382354736 139.90072631835938
Loss :  1.680029273033142 3.2688212394714355 165.12107849121094
Loss :  1.6484246253967285 3.7682366371154785 190.0602569580078
Loss :  1.6510359048843384 2.677932024002075 135.54763793945312
Loss :  1.6760766506195068 2.524921178817749 127.92213439941406
Loss :  1.638293981552124 2.930206775665283 148.1486358642578
Loss :  1.672818899154663 3.0707333087921143 155.20948791503906
Loss :  1.6462056636810303 2.7516140937805176 139.22691345214844
Loss :  1.6333832740783691 3.170677900314331 160.16726684570312
Loss :  1.673905849456787 3.0176289081573486 152.5553436279297
Loss :  1.6301276683807373 3.510683536529541 177.164306640625
Loss :  1.6267660856246948 3.2770471572875977 165.4791259765625
Loss :  1.6223094463348389 3.2298073768615723 163.11268615722656
Loss :  1.6206415891647339 3.3148090839385986 167.361083984375
Loss :  1.6844041347503662 2.91556453704834 147.46263122558594
Loss :  1.693643569946289 2.887233018875122 146.0552978515625
Loss :  1.6185007095336914 3.7066030502319336 186.9486541748047
Loss :  1.6549367904663086 3.0535953044891357 154.33470153808594
Loss :  1.6109142303466797 2.576371431350708 130.4294891357422
Loss :  1.6855272054672241 3.9407501220703125 198.72303771972656
  batch 20 loss: 1.6855272054672241, 3.9407501220703125, 198.72303771972656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6380407810211182 3.5650887489318848 179.89248657226562
Loss :  1.61318039894104 4.012917518615723 202.25904846191406
Loss :  1.6332809925079346 3.751190185546875 189.1927947998047
Loss :  1.6537574529647827 3.27431321144104 165.3694305419922
Loss :  1.6926037073135376 3.2727103233337402 165.32810974121094
Loss :  1.6381462812423706 3.1374077796936035 158.508544921875
Loss :  1.656294345855713 3.317303419113159 167.52146911621094
Loss :  1.635103702545166 3.02052640914917 152.6614227294922
Loss :  1.5778785943984985 2.972327470779419 150.1942596435547
Loss :  1.6730233430862427 3.4482345581054688 174.08474731445312
Loss :  1.5729702711105347 3.7481253147125244 188.97923278808594
Loss :  1.6480960845947266 3.4954614639282227 176.42117309570312
Loss :  1.6199946403503418 3.1972579956054688 161.48289489746094
Loss :  1.614742636680603 2.9251139163970947 147.8704376220703
Loss :  1.5693336725234985 2.999852418899536 151.56195068359375
Loss :  1.5873782634735107 3.487565517425537 175.9656524658203
Loss :  1.5839462280273438 3.1782073974609375 160.49432373046875
Loss :  1.6583244800567627 3.1955041885375977 161.43353271484375
Loss :  1.670380711555481 3.050851345062256 154.21295166015625
Loss :  1.680619239807129 3.5443334579467773 178.8972930908203
  batch 40 loss: 1.680619239807129, 3.5443334579467773, 178.8972930908203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6279361248016357 3.2367656230926514 163.46621704101562
Loss :  1.6158316135406494 2.6323158740997314 133.23162841796875
Loss :  1.5915542840957642 3.806903839111328 191.93673706054688
Loss :  1.6140164136886597 3.318005323410034 167.5142822265625
Loss :  1.5863791704177856 3.201744794845581 161.67361450195312
Loss :  1.628934383392334 3.1097233295440674 157.11509704589844
Loss :  1.6724563837051392 2.9618868827819824 149.7667999267578
Loss :  1.5993152856826782 3.722113609313965 187.7050018310547
Loss :  1.6858465671539307 3.5452122688293457 178.94647216796875
Loss :  1.6097491979599 3.5324394702911377 178.2317352294922
Loss :  1.64534592628479 3.25647234916687 164.46896362304688
Loss :  1.6436357498168945 2.9821836948394775 150.75282287597656
Loss :  1.6152536869049072 3.366306781768799 169.9305877685547
Loss :  1.6549861431121826 3.1086697578430176 157.08847045898438
Loss :  1.5913820266723633 3.220262050628662 162.6044921875
Loss :  1.6835598945617676 3.4964778423309326 176.50746154785156
Loss :  1.6141762733459473 3.2296361923217773 163.0959930419922
Loss :  1.5879724025726318 3.115427255630493 157.3593292236328
Loss :  1.6155805587768555 3.2141342163085938 162.32229614257812
Loss :  1.694043755531311 3.202672004699707 161.82765197753906
  batch 60 loss: 1.694043755531311, 3.202672004699707, 161.82765197753906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.5968117713928223 3.358853816986084 169.5395050048828
Loss :  1.6270368099212646 2.834383249282837 143.34619140625
Loss :  1.6075137853622437 2.779168128967285 140.56591796875
Loss :  1.5878849029541016 3.078514337539673 155.51361083984375
Loss :  1.567574381828308 3.004751682281494 151.80516052246094
Loss :  1.6049548387527466 3.9931674003601074 201.26332092285156
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.617260456085205 3.95402193069458 199.318359375
Loss :  1.612912893295288 3.904299020767212 196.82786560058594
Loss :  1.6298125982284546 3.691519021987915 186.2057647705078
Total LOSS train 162.60889681302584 valid 195.90382766723633
CE LOSS train 1.6340625744599562 valid 0.40745314955711365
Contrastive LOSS train 3.219496668302096 valid 0.9228797554969788
EPOCH 141:
Loss :  1.6482908725738525 2.8675167560577393 145.0241241455078
Loss :  1.666133999824524 3.0892059803009033 156.12643432617188
Loss :  1.629475474357605 3.233685255050659 163.31373596191406
Loss :  1.6347779035568237 3.0765039920806885 155.45997619628906
Loss :  1.6585578918457031 3.1584699153900146 159.58204650878906
Loss :  1.6178488731384277 2.78651762008667 140.9437255859375
Loss :  1.6539925336837769 3.301865816116333 166.74728393554688
Loss :  1.6261482238769531 2.8816349506378174 145.7078857421875
Loss :  1.613150954246521 3.4765701293945312 175.441650390625
Loss :  1.6547157764434814 4.2371063232421875 213.51002502441406
Loss :  1.6094768047332764 3.2347092628479004 163.34494018554688
Loss :  1.6083203554153442 4.102203845977783 206.71852111816406
Loss :  1.605977177619934 2.7724359035491943 140.22776794433594
Loss :  1.6092339754104614 2.990544319152832 151.13645935058594
Loss :  1.6713286638259888 3.7307088375091553 188.20677185058594
Loss :  1.672232747077942 3.521233320236206 177.73388671875
Loss :  1.6026039123535156 3.147158145904541 158.96051025390625
Loss :  1.6356462240219116 3.725832462310791 187.92727661132812
Loss :  1.5960972309112549 3.3228840827941895 167.7403106689453
Loss :  1.6656244993209839 2.850088596343994 144.1700439453125
  batch 20 loss: 1.6656244993209839, 2.850088596343994, 144.1700439453125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6238209009170532 3.4438812732696533 173.81788635253906
Loss :  1.597589373588562 3.370413303375244 170.11825561523438
Loss :  1.6159145832061768 2.627122640609741 132.9720458984375
Loss :  1.6348645687103271 2.9483258724212646 149.0511474609375
Loss :  1.66484797000885 2.9502270221710205 149.1761932373047
Loss :  1.619360327720642 3.0358781814575195 153.41326904296875
Loss :  1.6309164762496948 3.089647054672241 156.11326599121094
Loss :  1.6269376277923584 2.929049491882324 148.07940673828125
Loss :  1.575010895729065 3.3392229080200195 168.53616333007812
Loss :  1.655850887298584 3.100554943084717 156.68359375
Loss :  1.57525634765625 3.4611260890960693 174.63156127929688
Loss :  1.643295407295227 3.0693445205688477 155.1105194091797
Loss :  1.6156190633773804 3.1996426582336426 161.59774780273438
Loss :  1.613814353942871 3.069880962371826 155.1078643798828
Loss :  1.5817265510559082 3.945467948913574 198.85511779785156
Loss :  1.6023578643798828 3.180100917816162 160.60740661621094
Loss :  1.607078194618225 3.1179263591766357 157.50338745117188
Loss :  1.6614218950271606 2.787074089050293 141.01512145996094
Loss :  1.6671102046966553 3.0008914470672607 151.71168518066406
Loss :  1.6777942180633545 3.075913190841675 155.47344970703125
  batch 40 loss: 1.6777942180633545, 3.075913190841675, 155.47344970703125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.6347352266311646 3.4102671146392822 172.14808654785156
Loss :  1.6187173128128052 3.38718318939209 170.97787475585938
Loss :  1.6048359870910645 2.9502484798431396 149.1172637939453
Loss :  1.6212437152862549 3.088672161102295 156.0548553466797
Loss :  1.5987164974212646 3.26216983795166 164.7071990966797
Loss :  1.6339349746704102 3.753587484359741 189.3133087158203
Loss :  1.6689808368682861 2.945537567138672 148.94586181640625
Loss :  1.6146690845489502 2.905359983444214 146.88267517089844
Loss :  1.6845961809158325 3.908635139465332 197.11636352539062
Loss :  1.6218317747116089 3.131427526473999 158.19320678710938
Loss :  1.6564617156982422 3.780057430267334 190.65933227539062
Loss :  1.6520135402679443 2.862729072570801 144.78846740722656
Loss :  1.6312644481658936 2.79184627532959 141.2235870361328
Loss :  1.6618069410324097 3.20479679107666 161.90164184570312
Loss :  1.6166272163391113 3.230959415435791 163.1645965576172
Loss :  1.6858651638031006 2.9541642665863037 149.39407348632812
Loss :  1.6272000074386597 3.361924886703491 169.7234344482422
Loss :  1.6078194379806519 3.2043874263763428 161.8271942138672
Loss :  1.6247321367263794 3.33583402633667 168.4164276123047
Loss :  1.6915147304534912 2.935582160949707 148.4706268310547
  batch 60 loss: 1.6915147304534912, 2.935582160949707, 148.4706268310547
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.6093604564666748 3.2023189067840576 161.72531127929688
Loss :  1.6382262706756592 2.9501912593841553 149.14779663085938
Loss :  1.6187106370925903 3.227760076522827 163.0067138671875
Loss :  1.6036412715911865 3.285635471343994 165.88540649414062
Loss :  1.5827195644378662 2.566556930541992 129.9105682373047
Loss :  1.6207560300827026 3.3094301223754883 167.09226989746094
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.6327897310256958 3.2045674324035645 161.86117553710938
Loss :  1.6291017532348633 2.976386070251465 150.4484100341797
Loss :  1.6430288553237915 3.1206424236297607 157.67515563964844
Total LOSS train 161.54308213454027 valid 159.2692527770996
CE LOSS train 1.6304684143800001 valid 0.4107572138309479
Contrastive LOSS train 3.198252296447754 valid 0.7801606059074402
Saved best model. Old loss 168.57912063598633 and new best loss 159.2692527770996
EPOCH 142:
Loss :  1.652012586593628 2.980201482772827 150.66207885742188
Loss :  1.6673712730407715 3.0627059936523438 154.80267333984375
Loss :  1.6359187364578247 2.8742103576660156 145.346435546875
Loss :  1.641927719116211 2.7803945541381836 140.66165161132812
Loss :  1.662033200263977 3.2027599811553955 161.80003356933594
Loss :  1.6249765157699585 3.218073844909668 162.52865600585938
Loss :  1.6604329347610474 2.9834446907043457 150.83267211914062
Loss :  1.635473370552063 3.0366690158843994 153.4689178466797
Loss :  1.6253643035888672 3.092017889022827 156.22625732421875
Loss :  1.6599228382110596 2.820467233657837 142.68328857421875
Loss :  1.6167149543762207 3.190007448196411 161.11708068847656
Loss :  1.6146758794784546 3.5098936557769775 177.10935974121094
Loss :  1.6120775938034058 3.198889970779419 161.55657958984375
Loss :  1.6182726621627808 3.0672757625579834 154.9820556640625
Loss :  1.6737861633300781 2.9087204933166504 147.10980224609375
Loss :  1.6723167896270752 2.941422462463379 148.74343872070312
Loss :  1.6112672090530396 2.975649118423462 150.3937225341797
Loss :  1.6395263671875 2.8598263263702393 144.63084411621094
Loss :  1.6054695844650269 3.099207639694214 156.56585693359375
Loss :  1.6709613800048828 2.816890239715576 142.51547241210938
  batch 20 loss: 1.6709613800048828, 2.816890239715576, 142.51547241210938
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.6309431791305542 2.7772207260131836 140.49197387695312
Loss :  1.6049926280975342 3.678520679473877 185.53103637695312
Loss :  1.6232430934906006 3.112734317779541 157.2599639892578
Loss :  1.6393656730651855 3.790088653564453 191.14378356933594
Loss :  1.6706880331039429 3.532931327819824 178.31724548339844
Loss :  1.6274293661117554 3.1788811683654785 160.5714874267578
Loss :  1.639359474182129 3.2671926021575928 164.99899291992188
Loss :  1.6321525573730469 2.872734546661377 145.26889038085938
Loss :  1.5782403945922852 3.4810032844543457 175.62841796875
Loss :  1.6648861169815063 3.3460631370544434 168.96804809570312
Loss :  1.578124761581421 2.9999196529388428 151.57411193847656
Loss :  1.6493642330169678 3.2320525646209717 163.2519989013672
Loss :  1.623292088508606 2.692521572113037 136.24937438964844
Loss :  1.6219208240509033 3.099191188812256 156.58148193359375
Loss :  1.5857473611831665 3.4179179668426514 172.48165893554688
Loss :  1.6023914813995361 3.401803731918335 171.69256591796875
Loss :  1.6026110649108887 2.8680410385131836 145.00466918945312
Loss :  1.6649688482284546 3.588885545730591 181.10923767089844
Loss :  1.6705615520477295 3.754525661468506 189.3968505859375
Loss :  1.6805670261383057 2.8783624172210693 145.59869384765625
  batch 40 loss: 1.6805670261383057, 2.8783624172210693, 145.59869384765625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6406528949737549 3.0598304271698 154.63217163085938
Loss :  1.6260368824005127 2.7767937183380127 140.46572875976562
Loss :  1.6136964559555054 3.497131586074829 176.47027587890625
Loss :  1.628559947013855 3.097428798675537 156.5
Loss :  1.606688380241394 3.006467342376709 151.9300537109375
Loss :  1.6387836933135986 3.1982109546661377 161.54933166503906
Loss :  1.6719321012496948 2.852602958679199 144.3020782470703
Loss :  1.6214686632156372 3.8168649673461914 192.4647216796875
Loss :  1.6867815256118774 2.7335569858551025 138.36463928222656
Loss :  1.628598928451538 3.925766706466675 197.91693115234375
Loss :  1.661710500717163 3.0662033557891846 154.9718780517578
Loss :  1.6570525169372559 3.640383720397949 183.67623901367188
Loss :  1.6370220184326172 2.783358335494995 140.80494689941406
Loss :  1.6660393476486206 2.991981267929077 151.26510620117188
Loss :  1.6234372854232788 3.1639511585235596 159.8209991455078
Loss :  1.6886987686157227 3.060246467590332 154.70103454589844
Loss :  1.633155345916748 2.8614189624786377 144.7041015625
Loss :  1.616205096244812 3.330155611038208 168.12399291992188
Loss :  1.6327686309814453 3.1457250118255615 158.9190216064453
Loss :  1.6954641342163086 3.4256341457366943 172.9771728515625
  batch 60 loss: 1.6954641342163086, 3.4256341457366943, 172.9771728515625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.6197500228881836 3.2058868408203125 161.91409301757812
Loss :  1.6479469537734985 2.5738961696624756 130.34275817871094
Loss :  1.6280437707901 3.5006208419799805 176.65907287597656
Loss :  1.6161885261535645 2.9253671169281006 147.88455200195312
Loss :  1.596368432044983 2.963557720184326 149.77426147460938
Loss :  1.6318224668502808 4.266717433929443 214.9676971435547
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6429390907287598 4.238743305206299 213.58010864257812
Loss :  1.639359951019287 4.114022254943848 207.34046936035156
Loss :  1.6512621641159058 3.9919283390045166 201.2476806640625
Total LOSS train 158.70757727989783 valid 209.28398895263672
CE LOSS train 1.6365292714192317 valid 0.41281554102897644
Contrastive LOSS train 3.1414209402524507 valid 0.9979820847511292
EPOCH 143:
Loss :  1.660078763961792 3.173557758331299 160.3379669189453
Loss :  1.6727793216705322 3.0661981105804443 154.98268127441406
Loss :  1.6433144807815552 2.7983155250549316 141.55908203125
Loss :  1.6488347053527832 3.3784854412078857 170.57310485839844
Loss :  1.6668651103973389 2.678177833557129 135.5757598876953
Loss :  1.6325342655181885 2.840864896774292 143.67578125
Loss :  1.6666926145553589 3.748986005783081 189.11598205566406
Loss :  1.6438816785812378 3.052649974822998 154.2763671875
Loss :  1.6353158950805664 2.8592605590820312 144.5983428955078
Loss :  1.6678236722946167 2.9711084365844727 150.22323608398438
Loss :  1.6270852088928223 3.0638186931610107 154.81802368164062
Loss :  1.6249797344207764 3.2921335697174072 166.23165893554688
Loss :  1.6236932277679443 3.0732533931732178 155.28636169433594
Loss :  1.6294195652008057 3.604323148727417 181.8455810546875
Loss :  1.6818516254425049 3.156543493270874 159.509033203125
Loss :  1.679349422454834 2.838573694229126 143.6080322265625
Loss :  1.625720500946045 3.9248623847961426 197.86883544921875
Loss :  1.652326226234436 3.3106582164764404 167.18524169921875
Loss :  1.6218998432159424 2.8149001598358154 142.36691284179688
Loss :  1.6813071966171265 2.828892230987549 143.12591552734375
  batch 20 loss: 1.6813071966171265, 2.828892230987549, 143.12591552734375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.6436129808425903 2.6496429443359375 134.12576293945312
Loss :  1.6188589334487915 3.379213571548462 170.5795440673828
Loss :  1.6347932815551758 2.783264398574829 140.7980194091797
Loss :  1.6502861976623535 3.150787353515625 159.1896514892578
Loss :  1.6785266399383545 3.2655394077301025 164.95550537109375
Loss :  1.6389400959014893 3.389000654220581 171.08897399902344
Loss :  1.6497328281402588 3.202310085296631 161.76524353027344
Loss :  1.644602656364441 3.1236252784729004 157.82586669921875
Loss :  1.594186544418335 2.882153034210205 145.70184326171875
Loss :  1.6745802164077759 3.080268144607544 155.68798828125
Loss :  1.5939960479736328 3.0226213932037354 152.72506713867188
Loss :  1.6598114967346191 3.316387176513672 167.4791717529297
Loss :  1.636386752128601 3.1006457805633545 156.66867065429688
Loss :  1.6353821754455566 3.3559234142303467 169.43154907226562
Loss :  1.6020421981811523 3.1464266777038574 158.92337036132812
Loss :  1.6170545816421509 3.0773866176605225 155.4863739013672
Loss :  1.6172512769699097 3.0059750080108643 151.91600036621094
Loss :  1.6734369993209839 2.863192558288574 144.8330535888672
Loss :  1.678712248802185 3.2554779052734375 164.45260620117188
Loss :  1.6878958940505981 2.912058115005493 147.29080200195312
  batch 40 loss: 1.6878958940505981, 2.912058115005493, 147.29080200195312
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6510576009750366 3.3972134590148926 171.51173400878906
Loss :  1.6371638774871826 3.0608723163604736 154.68077087402344
Loss :  1.6268188953399658 2.946389675140381 148.94630432128906
Loss :  1.6396065950393677 2.7321906089782715 138.24913024902344
Loss :  1.6205494403839111 2.8545968532562256 144.3503875732422
Loss :  1.6501375436782837 2.9579460620880127 149.5474395751953
Loss :  1.6808245182037354 3.1062324047088623 156.99244689941406
Loss :  1.6360458135604858 3.2693698406219482 165.1045379638672
Loss :  1.6942925453186035 3.0103671550750732 152.212646484375
Loss :  1.6411429643630981 3.221252679824829 162.7037811279297
Loss :  1.6719694137573242 2.9907355308532715 151.208740234375
Loss :  1.6664869785308838 3.2697653770446777 165.15475463867188
Loss :  1.6486973762512207 4.105576992034912 206.92755126953125
Loss :  1.6752753257751465 3.221280574798584 162.7393035888672
Loss :  1.6367080211639404 3.251329183578491 164.20315551757812
Loss :  1.696597695350647 2.842440128326416 143.818603515625
Loss :  1.6462501287460327 3.549133777618408 179.1029510498047
Loss :  1.6313058137893677 3.1762428283691406 160.4434356689453
Loss :  1.6459826231002808 3.953745126724243 199.33323669433594
Loss :  1.7032381296157837 3.480478525161743 175.72715759277344
  batch 60 loss: 1.7032381296157837, 3.480478525161743, 175.72715759277344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.6344208717346191 2.8333628177642822 143.30255126953125
Loss :  1.6601768732070923 2.7814390659332275 140.7321319580078
Loss :  1.6429226398468018 2.882411479949951 145.7635040283203
Loss :  1.632779598236084 3.3273253440856934 167.99905395507812
Loss :  1.6142079830169678 2.7326135635375977 138.24488830566406
Loss :  1.6455157995224 3.762026786804199 189.74685668945312
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.6556488275527954 3.8123574256896973 192.27352905273438
Loss :  1.6521673202514648 3.671410083770752 185.22267150878906
Loss :  1.6621617078781128 3.550964832305908 179.2104034423828
Total LOSS train 158.1029102032001 valid 186.61336517333984
CE LOSS train 1.6477000364890466 valid 0.4155404269695282
Contrastive LOSS train 3.1291042217841514 valid 0.887741208076477
EPOCH 144:
Loss :  1.6722172498703003 3.1260695457458496 157.97569274902344
Loss :  1.685465931892395 3.5335519313812256 178.36306762695312
Loss :  1.6587728261947632 3.125981330871582 157.9578399658203
Loss :  1.6642568111419678 2.914106607437134 147.36959838867188
Loss :  1.6784727573394775 2.868910074234009 145.1239776611328
Loss :  1.6472890377044678 2.9502718448638916 149.160888671875
Loss :  1.6769952774047852 3.3348793983459473 168.4209747314453
Loss :  1.6553343534469604 3.606940507888794 182.0023651123047
Loss :  1.6460598707199097 2.850771903991699 144.1846466064453
Loss :  1.6756223440170288 2.835883617401123 143.4698028564453
Loss :  1.6369167566299438 3.0513298511505127 154.2034149169922
Loss :  1.6345057487487793 3.3562402725219727 169.44651794433594
Loss :  1.632959008216858 3.021300792694092 152.697998046875
Loss :  1.6382114887237549 3.113330841064453 157.30474853515625
Loss :  1.6886494159698486 3.3238165378570557 167.8794708251953
Loss :  1.6830581426620483 2.8406054973602295 143.7133331298828
Loss :  1.6306766271591187 2.928607940673828 148.06106567382812
Loss :  1.6555209159851074 2.8345677852630615 143.3839111328125
Loss :  1.6265990734100342 2.6415657997131348 133.70489501953125
Loss :  1.683806300163269 2.969878673553467 150.177734375
  batch 20 loss: 1.683806300163269, 2.969878673553467, 150.177734375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6476026773452759 2.8824667930603027 145.77093505859375
Loss :  1.623558521270752 3.2588791847229004 164.56752014160156
Loss :  1.6389962434768677 3.1440351009368896 158.8407440185547
Loss :  1.653222680091858 3.147615671157837 159.0340118408203
Loss :  1.680291771888733 3.644874095916748 183.9239959716797
Loss :  1.6418536901474 3.133678674697876 158.32579040527344
Loss :  1.6519426107406616 3.2200937271118164 162.65663146972656
Loss :  1.6468920707702637 2.8965678215026855 146.47528076171875
Loss :  1.5977418422698975 2.677699327468872 135.4827117919922
Loss :  1.6766691207885742 3.3513681888580322 169.2450714111328
Loss :  1.5966475009918213 3.380845785140991 170.63893127441406
Loss :  1.6610568761825562 3.2936854362487793 166.34532165527344
Loss :  1.6368156671524048 3.2504730224609375 164.16046142578125
Loss :  1.6362683773040771 3.1736109256744385 160.31680297851562
Loss :  1.6037019491195679 3.3005361557006836 166.63050842285156
Loss :  1.6180707216262817 3.001478433609009 151.69200134277344
Loss :  1.6187442541122437 3.4644346237182617 174.84046936035156
Loss :  1.6736701726913452 3.097675323486328 156.55743408203125
Loss :  1.6772565841674805 2.99870228767395 151.6123809814453
Loss :  1.6859511137008667 3.955679416656494 199.46990966796875
  batch 40 loss: 1.6859511137008667, 3.955679416656494, 199.46990966796875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6487783193588257 3.1215927600860596 157.72840881347656
Loss :  1.6346845626831055 2.8736798763275146 145.3186798095703
Loss :  1.6248472929000854 2.880617618560791 145.65573120117188
Loss :  1.6375157833099365 2.889256000518799 146.10031127929688
Loss :  1.618628978729248 2.8546369075775146 144.3504638671875
Loss :  1.6483259201049805 3.114840269088745 157.39035034179688
Loss :  1.67910897731781 3.0610804557800293 154.73312377929688
Loss :  1.6327546834945679 3.9493348598480225 199.0994873046875
Loss :  1.691277027130127 3.3318405151367188 168.28330993652344
Loss :  1.6381125450134277 3.7260513305664062 187.940673828125
Loss :  1.6678129434585571 3.0273866653442383 153.0371551513672
Loss :  1.6619549989700317 2.8247616291046143 142.90003967285156
Loss :  1.6437582969665527 2.962411403656006 149.7643280029297
Loss :  1.669736385345459 3.4127838611602783 172.30892944335938
Loss :  1.6311039924621582 2.8943514823913574 146.3486785888672
Loss :  1.6915149688720703 3.186427354812622 161.01287841796875
Loss :  1.6395899057388306 3.783048152923584 190.7919921875
Loss :  1.624132752418518 2.938154458999634 148.5318603515625
Loss :  1.6386758089065552 3.71217942237854 187.24765014648438
Loss :  1.6988722085952759 3.6435065269470215 183.8741912841797
  batch 60 loss: 1.6988722085952759, 3.6435065269470215, 183.8741912841797
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.625959873199463 3.103990077972412 156.82546997070312
Loss :  1.6535921096801758 3.145137310028076 158.91046142578125
Loss :  1.635414719581604 2.8055684566497803 141.91384887695312
Loss :  1.6256041526794434 2.927973508834839 148.0242919921875
Loss :  1.6070889234542847 3.9358112812042236 198.39764404296875
Loss :  1.6229126453399658 4.336335182189941 218.43966674804688
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.636646032333374 4.405514240264893 221.912353515625
Loss :  1.6346242427825928 4.173923015594482 210.33078002929688
Loss :  1.6426942348480225 4.1431379318237305 208.79959106445312
Total LOSS train 159.81044334998498 valid 214.87059783935547
CE LOSS train 1.6493413925170899 valid 0.4106735587120056
Contrastive LOSS train 3.163222045164842 valid 1.0357844829559326
EPOCH 145:
Loss :  1.666369915008545 2.669743061065674 135.1535186767578
Loss :  1.678691029548645 3.342778205871582 168.81761169433594
Loss :  1.6506413221359253 3.6945762634277344 186.37945556640625
Loss :  1.6565665006637573 3.1694886684417725 160.13099670410156
Loss :  1.6707887649536133 3.8294472694396973 193.14315795898438
Loss :  1.6394915580749512 2.863079786300659 144.79348754882812
Loss :  1.6717073917388916 3.154829502105713 159.41317749023438
Loss :  1.649319052696228 3.3301892280578613 168.15878295898438
Loss :  1.6412709951400757 2.80039119720459 141.66082763671875
Loss :  1.6712875366210938 3.3113646507263184 167.23953247070312
Loss :  1.6304125785827637 2.977548360824585 150.50782775878906
Loss :  1.6297483444213867 3.7073168754577637 186.99559020996094
Loss :  1.6262016296386719 2.9417402744293213 148.71322631835938
Loss :  1.6313221454620361 3.2274537086486816 163.00399780273438
Loss :  1.6819195747375488 2.9862539768218994 150.99461364746094
Loss :  1.676763892173767 3.2099430561065674 162.1739044189453
Loss :  1.6242693662643433 3.6491034030914307 184.07943725585938
Loss :  1.6488128900527954 2.9486286640167236 149.0802459716797
Loss :  1.6208703517913818 2.680549383163452 135.64833068847656
Loss :  1.6776999235153198 2.9468584060668945 149.0206298828125
  batch 20 loss: 1.6776999235153198, 2.9468584060668945, 149.0206298828125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6425038576126099 2.8822720050811768 145.756103515625
Loss :  1.6196633577346802 3.4341697692871094 173.32815551757812
Loss :  1.6354331970214844 3.6044199466705322 181.85643005371094
Loss :  1.6500600576400757 3.5877768993377686 181.0388946533203
Loss :  1.6773046255111694 3.1645944118499756 159.9070281982422
Loss :  1.638977289199829 3.127418041229248 158.00987243652344
Loss :  1.6480059623718262 3.0673370361328125 155.01486206054688
Loss :  1.6427534818649292 2.97078013420105 150.18174743652344
Loss :  1.5908204317092896 3.2230658531188965 162.74411010742188
Loss :  1.6711297035217285 3.3905301094055176 171.1976318359375
Loss :  1.589253544807434 3.538978338241577 178.5381622314453
Loss :  1.6548261642456055 3.2264344692230225 162.97654724121094
Loss :  1.6294530630111694 3.6362695693969727 183.44293212890625
Loss :  1.6281523704528809 3.574702262878418 180.36326599121094
Loss :  1.593329906463623 3.051814079284668 154.18402099609375
Loss :  1.608202338218689 2.970353603363037 150.12588500976562
Loss :  1.608778953552246 3.0980772972106934 156.5126495361328
Loss :  1.6672067642211914 3.057734727859497 154.55393981933594
Loss :  1.672165036201477 3.184476613998413 160.89599609375
Loss :  1.682146430015564 3.5856847763061523 180.9663848876953
  batch 40 loss: 1.682146430015564, 3.5856847763061523, 180.9663848876953
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6442277431488037 3.058333396911621 154.56089782714844
Loss :  1.6298785209655762 3.0453710556030273 153.8984375
Loss :  1.619555950164795 3.544339656829834 178.83653259277344
Loss :  1.6331974267959595 3.1903069019317627 161.14854431152344
Loss :  1.613533616065979 3.0474541187286377 153.9862518310547
Loss :  1.6443264484405518 2.897006034851074 146.49462890625
Loss :  1.6761139631271362 3.7727417945861816 190.31320190429688
Loss :  1.6288102865219116 2.859301805496216 144.59390258789062
Loss :  1.6901123523712158 3.0195648670196533 152.66835021972656
Loss :  1.6351462602615356 3.2620673179626465 164.73851013183594
Loss :  1.666835069656372 3.4434103965759277 173.8373565673828
Loss :  1.661605954170227 3.9461143016815186 198.96731567382812
Loss :  1.6438820362091064 2.727440357208252 138.01589965820312
Loss :  1.6696043014526367 3.058403730392456 154.58978271484375
Loss :  1.6319397687911987 3.264925241470337 164.87820434570312
Loss :  1.691207766532898 3.1648635864257812 159.93438720703125
Loss :  1.6388492584228516 2.8542468547821045 144.3511962890625
Loss :  1.6235074996948242 2.9033920764923096 146.79310607910156
Loss :  1.6371656656265259 3.4803640842437744 175.65536499023438
Loss :  1.69687819480896 3.7360754013061523 188.5006561279297
  batch 60 loss: 1.69687819480896, 3.7360754013061523, 188.5006561279297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6232306957244873 3.0371382236480713 153.4801483154297
Loss :  1.6515424251556396 2.988865375518799 151.0948028564453
Loss :  1.6319674253463745 2.698120594024658 136.5380096435547
Loss :  1.6215603351593018 3.0089657306671143 152.06985473632812
Loss :  1.602129340171814 2.588263750076294 131.01531982421875
Loss :  1.6400713920593262 3.8351540565490723 193.3977813720703
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6509792804718018 3.802638530731201 191.7829132080078
Loss :  1.6483967304229736 3.7172470092773438 187.5107421875
Loss :  1.6565080881118774 3.773421049118042 190.3275604248047
Total LOSS train 161.1948251577524 valid 190.7547492980957
CE LOSS train 1.6446327631290143 valid 0.41412702202796936
Contrastive LOSS train 3.1910038544581485 valid 0.9433552622795105
EPOCH 146:
Loss :  1.6631801128387451 3.0404860973358154 153.68748474121094
Loss :  1.6761587858200073 3.178051233291626 160.57872009277344
Loss :  1.6480441093444824 2.8819100856781006 145.74354553222656
Loss :  1.6551216840744019 3.0278122425079346 153.0457305908203
Loss :  1.670654296875 2.944000720977783 148.87069702148438
Loss :  1.6384117603302002 2.7475504875183105 139.01593017578125
Loss :  1.6704658269882202 2.742469549179077 138.7939453125
Loss :  1.647936463356018 2.9697484970092773 150.13536071777344
Loss :  1.6407896280288696 2.856886386871338 144.485107421875
Loss :  1.6706607341766357 2.8541362285614014 144.37747192382812
Loss :  1.6312839984893799 2.802700996398926 141.76634216308594
Loss :  1.6301720142364502 2.8743197917938232 145.34616088867188
Loss :  1.6275674104690552 3.878455638885498 195.5503387451172
Loss :  1.633638620376587 3.2801733016967773 165.64230346679688
Loss :  1.6848160028457642 3.2968156337738037 166.5255889892578
Loss :  1.67764413356781 3.005106210708618 151.93295288085938
Loss :  1.6245989799499512 3.6728219985961914 185.2657012939453
Loss :  1.64793062210083 3.142303943634033 158.7631378173828
Loss :  1.6199331283569336 2.860517978668213 144.6458282470703
Loss :  1.6797136068344116 3.2751405239105225 165.43673706054688
  batch 20 loss: 1.6797136068344116, 3.2751405239105225, 165.43673706054688
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6436891555786133 3.0524520874023438 154.26629638671875
Loss :  1.619844913482666 3.296250581741333 166.432373046875
Loss :  1.6363883018493652 3.208425760269165 162.05767822265625
Loss :  1.6505155563354492 3.259610652923584 164.63104248046875
Loss :  1.677488923072815 3.2363133430480957 163.4931640625
Loss :  1.6388455629348755 3.1366257667541504 158.47012329101562
Loss :  1.6478699445724487 3.100184440612793 156.65708923339844
Loss :  1.6439331769943237 2.943837881088257 148.83583068847656
Loss :  1.5948765277862549 3.29004168510437 166.0969696044922
Loss :  1.675057053565979 2.9262688159942627 147.98851013183594
Loss :  1.5975700616836548 3.493530750274658 176.27410888671875
Loss :  1.6611089706420898 3.7263875007629395 187.98048400878906
Loss :  1.6373528242111206 3.466128349304199 174.9437713623047
Loss :  1.6354293823242188 3.677816867828369 185.52627563476562
Loss :  1.6030919551849365 4.669716835021973 235.08892822265625
Loss :  1.6153874397277832 3.32542085647583 167.8864288330078
Loss :  1.6156076192855835 3.2072768211364746 161.9794464111328
Loss :  1.671198844909668 3.0302810668945312 153.1852569580078
Loss :  1.6756246089935303 3.13504695892334 158.427978515625
Loss :  1.6847546100616455 3.274685859680176 165.41905212402344
  batch 40 loss: 1.6847546100616455, 3.274685859680176, 165.41905212402344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  1.6464629173278809 3.2786877155303955 165.5808563232422
Loss :  1.6307671070098877 3.875704765319824 195.41600036621094
Loss :  1.6202445030212402 3.2489843368530273 164.0694580078125
Loss :  1.6326699256896973 3.27276873588562 165.2711181640625
Loss :  1.6128071546554565 2.8685576915740967 145.0406951904297
Loss :  1.643577218055725 2.9740512371063232 150.34613037109375
Loss :  1.674914002418518 2.805551290512085 141.9524688720703
Loss :  1.6271915435791016 3.2437875270843506 163.8165740966797
Loss :  1.6883742809295654 2.814798593521118 142.4282989501953
Loss :  1.6323350667953491 3.075559377670288 155.41030883789062
Loss :  1.6641404628753662 3.2315824031829834 163.24325561523438
Loss :  1.6569595336914062 3.1032066345214844 156.81729125976562
Loss :  1.63792085647583 3.1699137687683105 160.13360595703125
Loss :  1.6633808612823486 3.0862412452697754 155.97543334960938
Loss :  1.6253254413604736 3.13864803314209 158.55772399902344
Loss :  1.6865495443344116 2.9842159748077393 150.89735412597656
Loss :  1.6321321725845337 3.2861554622650146 165.9398956298828
Loss :  1.616288423538208 3.368298292160034 170.0312042236328
Loss :  1.631001353263855 3.1643998622894287 159.85098266601562
Loss :  1.6930955648422241 2.869823694229126 145.1842803955078
  batch 60 loss: 1.6930955648422241, 2.869823694229126, 145.1842803955078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6179139614105225 3.244372606277466 163.8365478515625
Loss :  1.6473625898361206 3.430819272994995 173.18833923339844
Loss :  1.6267426013946533 3.2538223266601562 164.31785583496094
Loss :  1.6159652471542358 3.165846586227417 159.90829467773438
Loss :  1.596156358718872 3.1519277095794678 159.1925506591797
Loss :  1.6384847164154053 3.9765431880950928 200.4656524658203
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6497422456741333 4.003254413604736 201.81246948242188
Loss :  1.6472992897033691 3.913167715072632 197.30567932128906
Loss :  1.65521240234375 3.8544228076934814 194.37635803222656
Total LOSS train 160.79471412071814 valid 198.49003982543945
CE LOSS train 1.6443790160692655 valid 0.4138031005859375
Contrastive LOSS train 3.1830067011026237 valid 0.9636057019233704
EPOCH 147:
Loss :  1.6591936349868774 3.1409761905670166 158.7080078125
Loss :  1.672440528869629 3.1543853282928467 159.39170837402344
Loss :  1.6422911882400513 2.8674824237823486 145.0164031982422
Loss :  1.6499704122543335 3.841850757598877 193.7425079345703
Loss :  1.6651983261108398 2.6944453716278076 136.38746643066406
Loss :  1.631799340248108 2.823951005935669 142.82936096191406
Loss :  1.6659637689590454 2.8470618724823 144.01905822753906
Loss :  1.641999363899231 3.0151944160461426 152.40171813964844
Loss :  1.6330050230026245 4.007541179656982 202.01007080078125
Loss :  1.6641489267349243 3.200737714767456 161.70103454589844
Loss :  1.6219596862792969 3.157985210418701 159.52122497558594
Loss :  1.620384693145752 3.6560137271881104 184.42108154296875
Loss :  1.6174863576889038 3.0313146114349365 153.1832275390625
Loss :  1.6247351169586182 2.905259847640991 146.88772583007812
Loss :  1.6786953210830688 3.1558501720428467 159.47120666503906
Loss :  1.6735886335372925 2.9779202938079834 150.56959533691406
Loss :  1.618943452835083 3.5614068508148193 179.6892852783203
Loss :  1.6446022987365723 2.896118402481079 146.45053100585938
Loss :  1.613525629043579 2.9395527839660645 148.59117126464844
Loss :  1.674123764038086 3.0716657638549805 155.2574005126953
  batch 20 loss: 1.674123764038086, 3.0716657638549805, 155.2574005126953
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6375974416732788 2.9628684520721436 149.78102111816406
Loss :  1.6117925643920898 2.881256103515625 145.67459106445312
Loss :  1.6283899545669556 3.385439157485962 170.9003448486328
Loss :  1.6437941789627075 3.3087003231048584 167.0788116455078
Loss :  1.6700828075408936 3.0218286514282227 152.7615203857422
Loss :  1.6296215057373047 3.254497766494751 164.35450744628906
Loss :  1.6376539468765259 2.9752092361450195 150.3981170654297
Loss :  1.634134292602539 2.868692636489868 145.0687713623047
Loss :  1.5816831588745117 2.9640281200408936 149.7830810546875
Loss :  1.6664061546325684 2.946547269821167 148.9937744140625
Loss :  1.582513689994812 4.24005651473999 213.58534240722656
Loss :  1.650195837020874 2.985504150390625 150.92539978027344
Loss :  1.6254945993423462 2.7434170246124268 138.7963409423828
Loss :  1.623937964439392 2.9004201889038086 146.6449432373047
Loss :  1.5895819664001465 3.89898419380188 196.53878784179688
Loss :  1.6044600009918213 3.184067487716675 160.80783081054688
Loss :  1.605043888092041 3.1007025241851807 156.64016723632812
Loss :  1.6650824546813965 3.256688356399536 164.49949645996094
Loss :  1.6693494319915771 3.468074083328247 175.0730438232422
Loss :  1.6792877912521362 3.314458131790161 167.40219116210938
  batch 40 loss: 1.6792877912521362, 3.314458131790161, 167.40219116210938
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6409239768981934 2.9631922245025635 149.800537109375
Loss :  1.6255137920379639 2.562443256378174 129.7476806640625
Loss :  1.6155182123184204 2.931671142578125 148.19908142089844
Loss :  1.629010796546936 3.213595390319824 162.30877685546875
Loss :  1.6084460020065308 2.8402061462402344 143.61875915527344
Loss :  1.6384057998657227 3.0051920413970947 151.89801025390625
Loss :  1.6701734066009521 2.8444972038269043 143.89501953125
Loss :  1.6231926679611206 2.9109561443328857 147.17100524902344
Loss :  1.6850165128707886 3.004443883895874 151.90721130371094
Loss :  1.6287932395935059 4.088498115539551 206.0537109375
Loss :  1.6618965864181519 3.139662742614746 158.64503479003906
Loss :  1.6553021669387817 3.6396453380584717 183.6375732421875
Loss :  1.63631010055542 3.0146238803863525 152.3675079345703
Loss :  1.6637780666351318 2.966599464416504 149.99374389648438
Loss :  1.6255500316619873 3.3941280841827393 171.3319549560547
Loss :  1.6855616569519043 2.774970293045044 140.43408203125
Loss :  1.6312392950057983 3.020216226577759 152.64205932617188
Loss :  1.615449070930481 3.1227097511291504 157.7509307861328
Loss :  1.628930926322937 3.1337382793426514 158.31585693359375
Loss :  1.6905454397201538 3.0400660037994385 153.69384765625
  batch 60 loss: 1.6905454397201538, 3.0400660037994385, 153.69384765625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6149085760116577 3.5428154468536377 178.7556915283203
Loss :  1.6438730955123901 2.9959876537323 151.44325256347656
Loss :  1.6232551336288452 2.9104483127593994 147.1456756591797
Loss :  1.6110904216766357 3.2950613498687744 166.36415100097656
Loss :  1.590224027633667 2.472066640853882 125.19355773925781
Loss :  1.6304177045822144 4.350478172302246 219.15432739257812
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6432135105133057 4.414234638214111 222.35494995117188
Loss :  1.6406322717666626 4.291574478149414 216.21934509277344
Loss :  1.64687180519104 4.232532024383545 213.27346801757812
Total LOSS train 158.1272705078125 valid 217.7505226135254
CE LOSS train 1.6383548938311063 valid 0.41171795129776
Contrastive LOSS train 3.129778297130878 valid 1.0581330060958862
EPOCH 148:
Loss :  1.6535587310791016 2.6928117275238037 136.2941436767578
Loss :  1.6666561365127563 3.366602897644043 169.99679565429688
Loss :  1.6376553773880005 3.024508476257324 152.86306762695312
Loss :  1.645060658454895 2.879227638244629 145.6064453125
Loss :  1.660932183265686 2.837486982345581 143.5352783203125
Loss :  1.6270030736923218 3.0061004161834717 151.93202209472656
Loss :  1.6619954109191895 2.8642001152038574 144.87200927734375
Loss :  1.6391222476959229 2.755199670791626 139.39910888671875
Loss :  1.6318871974945068 3.1272757053375244 157.99566650390625
Loss :  1.664428949356079 3.3722987174987793 170.27935791015625
Loss :  1.6222161054611206 2.934868812561035 148.36566162109375
Loss :  1.62142014503479 2.978328227996826 150.53782653808594
Loss :  1.619720458984375 3.3706469535827637 170.15206909179688
Loss :  1.6264334917068481 2.9795405864715576 150.6034698486328
Loss :  1.6790071725845337 3.0385656356811523 153.60728454589844
Loss :  1.672057867050171 3.289501905441284 166.14715576171875
Loss :  1.6185977458953857 3.4879279136657715 176.01498413085938
Loss :  1.6432474851608276 2.8039450645446777 141.8404998779297
Loss :  1.616223931312561 2.954540729522705 149.34326171875
Loss :  1.6747114658355713 2.7988102436065674 141.61521911621094
  batch 20 loss: 1.6747114658355713, 2.7988102436065674, 141.61521911621094
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.639965295791626 2.6328256130218506 133.28125
Loss :  1.6153266429901123 3.122910499572754 157.76084899902344
Loss :  1.6321525573730469 2.6821532249450684 135.7398223876953
Loss :  1.6472913026809692 2.72165584564209 137.73008728027344
Loss :  1.6739246845245361 2.8198959827423096 142.66871643066406
Loss :  1.6356418132781982 3.215301752090454 162.40072631835938
Loss :  1.6452038288116455 3.2503111362457275 164.1607666015625
Loss :  1.641719937324524 2.9382405281066895 148.5537567138672
Loss :  1.5943984985351562 2.8725078105926514 145.21978759765625
Loss :  1.6741176843643188 3.3225584030151367 167.80203247070312
Loss :  1.5954129695892334 3.8993031978607178 196.56057739257812
Loss :  1.6579042673110962 3.35451340675354 169.38357543945312
Loss :  1.6353448629379272 3.2391273975372314 163.5917205810547
Loss :  1.6346862316131592 3.260474920272827 164.65843200683594
Loss :  1.6023858785629272 3.31318736076355 167.26174926757812
Loss :  1.6164969205856323 2.9856607913970947 150.8995361328125
Loss :  1.6164014339447021 2.9760587215423584 150.41932678222656
Loss :  1.670304536819458 3.3913023471832275 171.2354278564453
Loss :  1.6741334199905396 3.143836736679077 158.865966796875
Loss :  1.6817940473556519 2.8146777153015137 142.41567993164062
  batch 40 loss: 1.6817940473556519, 2.8146777153015137, 142.41567993164062
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6441468000411987 3.094951629638672 156.39173889160156
Loss :  1.6298177242279053 3.249833822250366 164.1215057373047
Loss :  1.62075674533844 3.349918842315674 169.11669921875
Loss :  1.6319838762283325 3.6261963844299316 182.94180297851562
Loss :  1.612748146057129 2.9670095443725586 149.96322631835938
Loss :  1.641564965248108 3.189110040664673 161.09707641601562
Loss :  1.671692967414856 3.323723793029785 167.85787963867188
Loss :  1.6269176006317139 3.864366292953491 194.84523010253906
Loss :  1.687461018562317 2.803788423538208 141.8768768310547
Loss :  1.6312425136566162 2.8885226249694824 146.057373046875
Loss :  1.6655025482177734 2.996772050857544 151.5041046142578
Loss :  1.657165288925171 3.448232889175415 174.06881713867188
Loss :  1.637035846710205 3.0862553119659424 155.94979858398438
Loss :  1.6658098697662354 3.3883399963378906 171.0828094482422
Loss :  1.6263761520385742 3.6070263385772705 181.97769165039062
Loss :  1.6845030784606934 3.324388027191162 167.90391540527344
Loss :  1.6307308673858643 3.355044364929199 169.38294982910156
Loss :  1.617029070854187 3.6103243827819824 182.1332550048828
Loss :  1.6304885149002075 3.454197883605957 174.34039306640625
Loss :  1.6897974014282227 3.245537281036377 163.9666748046875
  batch 60 loss: 1.6897974014282227, 3.245537281036377, 163.9666748046875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6176376342773438 3.3242123126983643 167.8282470703125
Loss :  1.6451871395111084 3.1151814460754395 157.40426635742188
Loss :  1.6241652965545654 2.620704412460327 132.6593780517578
Loss :  1.6127808094024658 2.6935033798217773 136.28794860839844
Loss :  1.5911803245544434 2.2419440746307373 113.68838500976562
Loss :  1.6251481771469116 3.705580234527588 186.90415954589844
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6370047330856323 3.6194326877593994 182.608642578125
Loss :  1.635073184967041 3.5457730293273926 178.92372131347656
Loss :  1.6391955614089966 3.3825948238372803 170.76893615722656
Total LOSS train 157.32398705115685 valid 179.80136489868164
CE LOSS train 1.6409271515332735 valid 0.40979889035224915
Contrastive LOSS train 3.1136611901796782 valid 0.8456487059593201
EPOCH 149:
Loss :  1.6551241874694824 3.164414644241333 159.8758544921875
Loss :  1.667197585105896 3.0693624019622803 155.13531494140625
Loss :  1.6361052989959717 3.1325178146362305 158.26199340820312
Loss :  1.6436011791229248 3.2066638469696045 161.97679138183594
Loss :  1.6583794355392456 2.870828866958618 145.1998291015625
Loss :  1.6226173639297485 2.879957914352417 145.62051391601562
Loss :  1.6595443487167358 3.410952091217041 172.2071533203125
Loss :  1.6333523988723755 3.276005268096924 165.43360900878906
Loss :  1.626055121421814 3.157273054122925 159.4897003173828
Loss :  1.6593486070632935 3.6234757900238037 182.8331298828125
Loss :  1.6135327816009521 2.869368553161621 145.0819549560547
Loss :  1.6143033504486084 2.936375141143799 148.4330596923828
Loss :  1.6126216650009155 2.798328161239624 141.52903747558594
Loss :  1.6201378107070923 2.781545400619507 140.69740295410156
Loss :  1.6746577024459839 3.4659364223480225 174.97146606445312
Loss :  1.6676026582717896 3.215247869491577 162.42999267578125
Loss :  1.6118580102920532 3.009535789489746 152.08865356445312
Loss :  1.63742995262146 3.0408666133880615 153.68077087402344
Loss :  1.60995614528656 2.779921293258667 140.60601806640625
Loss :  1.6706969738006592 2.67254638671875 135.2980194091797
  batch 20 loss: 1.6706969738006592, 2.67254638671875, 135.2980194091797
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.6360554695129395 3.184102773666382 160.84120178222656
Loss :  1.6118347644805908 3.819638967514038 192.59378051757812
Loss :  1.6293284893035889 3.282680034637451 165.76333618164062
Loss :  1.644918441772461 4.02616024017334 202.9529266357422
Loss :  1.6722197532653809 3.991689443588257 201.25669860839844
Loss :  1.6332043409347534 3.2185544967651367 162.56092834472656
Loss :  1.6416159868240356 3.4559810161590576 174.440673828125
Loss :  1.6368069648742676 3.118173837661743 157.54550170898438
Loss :  1.5861213207244873 2.9601340293884277 149.5928192138672
Loss :  1.6690442562103271 3.9550516605377197 199.42161560058594
Loss :  1.587896466255188 3.1794066429138184 160.5582275390625
Loss :  1.6526764631271362 3.4483273029327393 174.0690460205078
Loss :  1.6297458410263062 3.5767626762390137 180.46788024902344
Loss :  1.6271030902862549 3.7018048763275146 186.71734619140625
Loss :  1.590468168258667 3.605217218399048 181.85133361816406
Loss :  1.6034029722213745 3.6246771812438965 182.83726501464844
Loss :  1.6020084619522095 3.2850096225738525 165.8524932861328
Loss :  1.660153865814209 4.402407169342041 221.780517578125
Loss :  1.6648699045181274 3.1603751182556152 159.68362426757812
Loss :  1.6745069026947021 3.327939748764038 168.0714874267578
  batch 40 loss: 1.6745069026947021, 3.327939748764038, 168.0714874267578
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  1.6330498456954956 3.2029197216033936 161.77903747558594
Loss :  1.6181858777999878 3.1706418991088867 160.1502685546875
Loss :  1.608035683631897 3.2659687995910645 164.90647888183594
Loss :  1.6187671422958374 3.2970848083496094 166.4730224609375
Loss :  1.5971758365631104 3.3133199214935303 167.26318359375
Loss :  1.6278588771820068 3.260563850402832 164.6560516357422
Loss :  1.6602168083190918 3.5667483806610107 179.9976348876953
Loss :  1.6090922355651855 3.2200284004211426 162.61050415039062
Loss :  1.674767017364502 2.80499267578125 141.92440795898438
Loss :  1.6117912530899048 4.109612464904785 207.0924072265625
Loss :  1.6489559412002563 3.0130162239074707 152.29977416992188
Loss :  1.6396745443344116 3.5433170795440674 178.80552673339844
Loss :  1.6164919137954712 3.312541961669922 167.24359130859375
Loss :  1.6468026638031006 3.200591802597046 161.6763916015625
Loss :  1.6045148372650146 3.274117946624756 165.31040954589844
Loss :  1.6706868410110474 3.1467883586883545 159.01010131835938
Loss :  1.610263466835022 3.350041151046753 169.11231994628906
Loss :  1.5931575298309326 3.351611614227295 169.17373657226562
Loss :  1.6089507341384888 3.4670891761779785 174.96340942382812
Loss :  1.6791441440582275 3.8589932918548584 194.62879943847656
  batch 60 loss: 1.6791441440582275, 3.8589932918548584, 194.62879943847656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.5944241285324097 2.979081153869629 150.54847717285156
Loss :  1.6277238130569458 2.979809522628784 150.6182098388672
Loss :  1.605027437210083 2.8557469844818115 144.3923797607422
Loss :  1.5935437679290771 2.9703307151794434 150.11007690429688
Loss :  1.5708181858062744 2.7615532875061035 139.6484832763672
Loss :  1.6273376941680908 4.350314140319824 219.14303588867188
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.641369104385376 4.381719589233398 220.72735595703125
Loss :  1.6402043104171753 4.180975914001465 210.68899536132812
Loss :  1.6446223258972168 4.018979072570801 202.5935821533203
Total LOSS train 165.4477485069862 valid 213.2882423400879
CE LOSS train 1.6310342311859132 valid 0.4111555814743042
Contrastive LOSS train 3.276334285736084 valid 1.0047447681427002
EPOCH 150:
Loss :  1.6406688690185547 3.0745131969451904 155.3663330078125
Loss :  1.6548410654067993 3.1889212131500244 161.1009063720703
Loss :  1.6206120252609253 3.204914093017578 161.86630249023438
Loss :  1.6286462545394897 2.9350035190582275 148.3788299560547
Loss :  1.6455801725387573 3.1038849353790283 156.83982849121094
Loss :  1.6078345775604248 2.785245656967163 140.8701171875
Loss :  1.6466196775436401 3.1841633319854736 160.8547821044922
Loss :  1.6213001012802124 2.760984182357788 139.67051696777344
Loss :  1.6127125024795532 2.887894868850708 146.00746154785156
Loss :  1.6501215696334839 2.755467176437378 139.42347717285156
Loss :  1.603004813194275 2.941124200820923 148.6592254638672
Loss :  1.6012459993362427 3.118290662765503 157.51577758789062
Loss :  1.599010705947876 2.9635961055755615 149.77882385253906
Loss :  1.6067043542861938 3.1423933506011963 158.72637939453125
Loss :  1.6646121740341187 2.936501979827881 148.48971557617188
Loss :  1.6590144634246826 3.218560218811035 162.58702087402344
Loss :  1.5973386764526367 3.5410492420196533 178.64979553222656
Loss :  1.6256574392318726 2.85728120803833 144.48971557617188
Loss :  1.5942878723144531 3.304298162460327 166.80918884277344
Loss :  1.6609961986541748 3.5140225887298584 177.36212158203125
  batch 20 loss: 1.6609961986541748, 3.5140225887298584, 177.36212158203125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.6222227811813354 3.151740312576294 159.20924377441406
Loss :  1.5935444831848145 3.1109681129455566 157.14195251464844
Loss :  1.6125317811965942 2.9480862617492676 149.016845703125
Loss :  1.6302647590637207 3.4494099617004395 174.10076904296875
Loss :  1.6598907709121704 3.2129571437835693 162.30775451660156
Loss :  1.6146876811981201 2.8841476440429688 145.8220672607422
Loss :  1.6237603425979614 3.2057151794433594 161.90953063964844
Loss :  1.6188329458236694 3.3188180923461914 167.5597381591797
Loss :  1.5609909296035767 3.2595322132110596 164.53759765625
Loss :  1.6533305644989014 3.5126311779022217 177.28489685058594
Loss :  1.55929696559906 2.8543295860290527 144.27577209472656
Loss :  1.6352814435958862 2.984553575515747 150.8629608154297
Loss :  1.6074920892715454 2.9949662685394287 151.35580444335938
Loss :  1.6061416864395142 3.320488691329956 167.6305694580078
Loss :  1.5661922693252563 3.101428270339966 156.63760375976562
Loss :  1.5833607912063599 3.157078266143799 159.43727111816406
Loss :  1.5841426849365234 3.225198268890381 162.84405517578125
Loss :  1.6505091190338135 2.935553550720215 148.42819213867188
Loss :  1.6566050052642822 2.977147102355957 150.5139617919922
Loss :  1.6673616170883179 3.130589008331299 158.19680786132812
  batch 40 loss: 1.6673616170883179, 3.130589008331299, 158.19680786132812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.62563955783844 3.253952741622925 164.32327270507812
Loss :  1.6091291904449463 3.1391096115112305 158.56460571289062
Loss :  1.5988481044769287 2.6740341186523438 135.30055236816406
Loss :  1.6133683919906616 3.6519079208374023 184.20877075195312
Loss :  1.5918540954589844 2.4876081943511963 125.97225952148438
Loss :  1.6250241994857788 2.877647876739502 145.50743103027344
Loss :  1.6602518558502197 3.075758695602417 155.44818115234375
Loss :  1.6102882623672485 2.9783523082733154 150.5279083251953
Loss :  1.6762723922729492 2.917825222015381 147.56753540039062
Loss :  1.6169064044952393 3.2905044555664062 166.1421356201172
Loss :  1.6521321535110474 3.03554105758667 153.42918395996094
Loss :  1.645997405052185 3.64654541015625 183.9732666015625
Loss :  1.6246029138565063 2.8997528553009033 146.61224365234375
Loss :  1.654675841331482 2.9451212882995605 148.91073608398438
Loss :  1.6135642528533936 4.185837268829346 210.9054412841797
Loss :  1.678836464881897 3.0962820053100586 156.49293518066406
Loss :  1.6204328536987305 2.9291443824768066 148.07765197753906
Loss :  1.604009747505188 3.553332805633545 179.27064514160156
Loss :  1.6187748908996582 3.2801804542541504 165.62779235839844
Loss :  1.685256838798523 3.5897507667541504 181.17279052734375
  batch 60 loss: 1.685256838798523, 3.5897507667541504, 181.17279052734375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.6034033298492432 3.2655680179595947 164.88180541992188
Loss :  1.634210228919983 2.9569716453552246 149.48280334472656
Loss :  1.6125614643096924 3.6706132888793945 185.1432342529297
Loss :  1.6004676818847656 3.795545816421509 191.3777618408203
Loss :  1.5781431198120117 3.6486995220184326 184.01312255859375
Loss :  1.6247426271438599 4.203834056854248 211.81643676757812
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6368305683135986 4.129575252532959 208.11558532714844
Loss :  1.6342639923095703 4.0964460372924805 206.45655822753906
Loss :  1.6416423320770264 4.0200724601745605 202.645263671875
Total LOSS train 159.3146735558143 valid 207.25846099853516
CE LOSS train 1.623106182538546 valid 0.4104105830192566
Contrastive LOSS train 3.1538313278785117 valid 1.0050181150436401
EPOCH 151:
Loss :  1.646807074546814 3.035578489303589 153.4257354736328
Loss :  1.660812258720398 3.260993003845215 164.71046447753906
Loss :  1.6281366348266602 2.968998908996582 150.07809448242188
Loss :  1.6353981494903564 3.0474870204925537 154.00973510742188
Loss :  1.652478814125061 3.7136905193328857 187.33700561523438
Loss :  1.6144452095031738 2.84104585647583 143.66673278808594
Loss :  1.6524021625518799 3.099034309387207 156.6041259765625
Loss :  1.6265114545822144 2.991886854171753 151.22085571289062
Loss :  1.6173721551895142 3.0955393314361572 156.3943328857422
Loss :  1.6529701948165894 3.2441623210906982 163.861083984375
Loss :  1.6064351797103882 3.062896490097046 154.75125122070312
Loss :  1.6045781373977661 2.978541851043701 150.53167724609375
Loss :  1.6031743288040161 3.9008374214172363 196.64505004882812
Loss :  1.6108535528182983 3.0211873054504395 152.67022705078125
Loss :  1.668758749961853 3.0899839401245117 156.16795349121094
Loss :  1.6622918844223022 2.9470231533050537 149.01344299316406
Loss :  1.6019797325134277 3.6634104251861572 184.77249145507812
Loss :  1.6298123598098755 2.8337254524230957 143.3160858154297
Loss :  1.5985596179962158 2.5676791667938232 129.98251342773438
Loss :  1.6643155813217163 2.919018268585205 147.615234375
  batch 20 loss: 1.6643155813217163, 2.919018268585205, 147.615234375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6266387701034546 2.8448386192321777 143.86856079101562
Loss :  1.5991005897521973 3.1449904441833496 158.8486328125
Loss :  1.6173635721206665 3.166051149368286 159.919921875
Loss :  1.6347873210906982 4.040422439575195 203.65589904785156
Loss :  1.6637619733810425 3.89766263961792 196.54689025878906
Loss :  1.620442271232605 3.2961370944976807 166.4272918701172
Loss :  1.630328893661499 3.126328468322754 157.94674682617188
Loss :  1.6266616582870483 2.690873622894287 136.17034912109375
Loss :  1.572163462638855 2.9004852771759033 146.59642028808594
Loss :  1.660933017730713 3.2876782417297363 166.0448455810547
Loss :  1.5726838111877441 3.123638153076172 157.7545928955078
Loss :  1.6444501876831055 3.0041072368621826 151.84982299804688
Loss :  1.6187126636505127 3.73451566696167 188.34449768066406
Loss :  1.6174849271774292 3.3253369331359863 167.8843231201172
Loss :  1.5803114175796509 3.1458961963653564 158.8751220703125
Loss :  1.5962870121002197 2.997368812561035 151.4647216796875
Loss :  1.5969511270523071 3.0615859031677246 154.67625427246094
Loss :  1.6589316129684448 2.780531167984009 140.6855010986328
Loss :  1.6643664836883545 3.595733165740967 181.45101928710938
Loss :  1.6741379499435425 4.105761528015137 206.9622039794922
  batch 40 loss: 1.6741379499435425, 4.105761528015137, 206.9622039794922
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6337157487869263 3.2720117568969727 165.2342987060547
Loss :  1.6180245876312256 3.6338891983032227 183.31248474121094
Loss :  1.6081339120864868 3.0009756088256836 151.65692138671875
Loss :  1.621569037437439 3.1036760807037354 156.8053741455078
Loss :  1.6006064414978027 2.7089757919311523 137.0493927001953
Loss :  1.6318074464797974 2.7480568885803223 139.03465270996094
Loss :  1.6655277013778687 3.0205202102661133 152.69154357910156
Loss :  1.616025447845459 3.683340311050415 185.78305053710938
Loss :  1.6803698539733887 3.1665711402893066 160.00892639160156
Loss :  1.621547818183899 2.916689395904541 147.45602416992188
Loss :  1.6555644273757935 3.0362772941589355 153.46942138671875
Loss :  1.6492688655853271 2.879143714904785 145.6064453125
Loss :  1.6281023025512695 2.9949729442596436 151.37673950195312
Loss :  1.6568105220794678 3.596879482269287 181.50079345703125
Loss :  1.6159685850143433 2.9596879482269287 149.60035705566406
Loss :  1.6805733442306519 3.893909454345703 196.37603759765625
Loss :  1.62327241897583 3.207181453704834 161.9823455810547
Loss :  1.6070263385772705 2.792646884918213 141.23936462402344
Loss :  1.6217660903930664 3.166881799697876 159.96585083007812
Loss :  1.6874492168426514 3.238616704940796 163.6182861328125
  batch 60 loss: 1.6874492168426514, 3.238616704940796, 163.6182861328125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6076444387435913 3.4075734615325928 171.986328125
Loss :  1.637737512588501 2.754417896270752 139.358642578125
Loss :  1.6173434257507324 2.8144924640655518 142.3419647216797
Loss :  1.6063458919525146 3.4062225818634033 171.91746520996094
Loss :  1.5861389636993408 2.511159658432007 127.14411926269531
Loss :  1.6244090795516968 3.6580872535705566 184.5287628173828
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.637184500694275 3.5944557189941406 181.35997009277344
Loss :  1.634541392326355 3.453348398208618 174.3019561767578
Loss :  1.6405861377716064 3.549970865249634 179.13912963867188
Total LOSS train 159.68099294809195 valid 179.83245468139648
CE LOSS train 1.6291220353199884 valid 0.4101465344429016
Contrastive LOSS train 3.161037430396447 valid 0.8874927163124084
EPOCH 152:
Loss :  1.6518609523773193 2.910388708114624 147.17129516601562
Loss :  1.6654051542282104 3.0509026050567627 154.21054077148438
Loss :  1.634613037109375 3.869107484817505 195.08998107910156
Loss :  1.6425540447235107 2.715301752090454 137.4076385498047
Loss :  1.6585849523544312 3.2439866065979004 163.85791015625
Loss :  1.6241438388824463 3.568164110183716 180.0323486328125
Loss :  1.659614086151123 3.784027099609375 190.8609619140625
Loss :  1.6357721090316772 3.5619845390319824 179.73500061035156
Loss :  1.6276637315750122 3.0029759407043457 151.77647399902344
Loss :  1.661328673362732 3.5060248374938965 176.9625701904297
Loss :  1.618338942527771 4.035012245178223 203.3689422607422
Loss :  1.6171939373016357 3.107560634613037 156.99522399902344
Loss :  1.6159065961837769 2.908957004547119 147.06375122070312
Loss :  1.6225030422210693 3.0342390537261963 153.33445739746094
Loss :  1.6750913858413696 3.7780306339263916 190.57662963867188
Loss :  1.6698111295700073 3.4859442710876465 175.96702575683594
Loss :  1.612321138381958 3.10188627243042 156.70663452148438
Loss :  1.637558937072754 2.743488311767578 138.8119659423828
Loss :  1.6082655191421509 3.0473647117614746 153.97650146484375
Loss :  1.6701518297195435 3.3191819190979004 167.62924194335938
  batch 20 loss: 1.6701518297195435, 3.3191819190979004, 167.62924194335938
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6342062950134277 2.9933981895446777 151.30410766601562
Loss :  1.6084492206573486 2.9106664657592773 147.1417694091797
Loss :  1.625879168510437 2.7962920665740967 141.44049072265625
Loss :  1.6420350074768066 3.260751247406006 164.67959594726562
Loss :  1.6695863008499146 3.171452283859253 160.24220275878906
Loss :  1.6290792226791382 3.257924795150757 164.5253143310547
Loss :  1.6383650302886963 3.0098650455474854 152.13162231445312
Loss :  1.6347362995147705 3.7869462966918945 190.9820556640625
Loss :  1.584144949913025 3.722550392150879 187.711669921875
Loss :  1.6682466268539429 3.306994676589966 167.01797485351562
Loss :  1.585108757019043 2.967895269393921 149.97987365722656
Loss :  1.6521517038345337 3.116166830062866 157.46047973632812
Loss :  1.6283656358718872 2.9118127822875977 147.21900939941406
Loss :  1.6274082660675049 2.8843276500701904 145.8437957763672
Loss :  1.5923830270767212 3.3875956535339355 170.97215270996094
Loss :  1.607557773590088 3.3095622062683105 167.08566284179688
Loss :  1.6077150106430054 3.0683419704437256 155.02481079101562
Loss :  1.6656206846237183 2.7850770950317383 140.9194793701172
Loss :  1.6709033250808716 2.9148995876312256 147.41587829589844
Loss :  1.6801789999008179 2.733534336090088 138.3568878173828
  batch 40 loss: 1.6801789999008179, 2.733534336090088, 138.3568878173828
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6423245668411255 3.3771746158599854 170.5010528564453
Loss :  1.6280593872070312 2.6482133865356445 134.03872680664062
Loss :  1.6182361841201782 2.8994028568267822 146.58837890625
Loss :  1.6310354471206665 2.9577202796936035 149.51705932617188
Loss :  1.6109750270843506 3.3609204292297363 169.65699768066406
Loss :  1.6402095556259155 2.7661314010620117 139.94677734375
Loss :  1.6721441745758057 2.8251986503601074 142.9320831298828
Loss :  1.626212477684021 2.5920121669769287 131.226806640625
Loss :  1.686141014099121 3.084383726119995 155.90533447265625
Loss :  1.6316137313842773 2.687674045562744 136.0153045654297
Loss :  1.6632429361343384 2.891482353210449 146.2373504638672
Loss :  1.6574289798736572 3.9615631103515625 199.73558044433594
Loss :  1.6376466751098633 3.0870676040649414 155.99102783203125
Loss :  1.664727807044983 2.714620351791382 137.395751953125
Loss :  1.62662672996521 3.4959681034088135 176.42503356933594
Loss :  1.6868857145309448 2.651979684829712 134.28587341308594
Loss :  1.6336116790771484 2.8966476917266846 146.4659881591797
Loss :  1.618546962738037 3.9868836402893066 200.9627227783203
Loss :  1.6324454545974731 2.8929684162139893 146.28086853027344
Loss :  1.6931570768356323 3.6447527408599854 183.93080139160156
  batch 60 loss: 1.6931570768356323, 3.6447527408599854, 183.93080139160156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6196081638336182 2.741482973098755 138.69375610351562
Loss :  1.6478873491287231 3.1035194396972656 156.8238525390625
Loss :  1.6287013292312622 2.5974910259246826 131.50326538085938
Loss :  1.6184991598129272 3.5022764205932617 176.73231506347656
Loss :  1.599115014076233 2.3135030269622803 117.27426147460938
Loss :  1.640744686126709 4.031891822814941 203.23533630371094
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.652963638305664 3.929865837097168 198.14625549316406
Loss :  1.650109887123108 3.8727853298187256 195.2893829345703
Loss :  1.656347393989563 3.876023530960083 195.45751953125
Total LOSS train 158.37010615422176 valid 198.03212356567383
CE LOSS train 1.6380905683224019 valid 0.41408684849739075
Contrastive LOSS train 3.134640334202693 valid 0.9690058827400208
EPOCH 153:
Loss :  1.6608718633651733 3.2168171405792236 162.50172424316406
Loss :  1.6732345819473267 3.377180576324463 170.53225708007812
Loss :  1.6451923847198486 3.0312724113464355 153.20880126953125
Loss :  1.652554988861084 2.91723895072937 147.51451110839844
Loss :  1.6672345399856567 2.7734789848327637 140.3411865234375
Loss :  1.6348413228988647 3.1467201709747314 158.97085571289062
Loss :  1.667281150817871 3.4743142127990723 175.38299560546875
Loss :  1.6445955038070679 2.562189817428589 129.75408935546875
Loss :  1.6366560459136963 2.984841823577881 150.87875366210938
Loss :  1.6681972742080688 3.0881083011627197 156.0736083984375
Loss :  1.6273356676101685 2.911428213119507 147.19874572753906
Loss :  1.6269218921661377 2.888425588607788 146.04820251464844
Loss :  1.6252304315567017 2.917482614517212 147.49935913085938
Loss :  1.632153868675232 2.9239933490753174 147.83181762695312
Loss :  1.6817927360534668 3.190910816192627 161.2273406982422
Loss :  1.677001953125 3.5947747230529785 181.41574096679688
Loss :  1.6242046356201172 3.543853759765625 178.81689453125
Loss :  1.6485342979431152 2.7255706787109375 137.92706298828125
Loss :  1.6220842599868774 3.5412259101867676 178.68338012695312
Loss :  1.678971290588379 2.8904523849487305 146.2015838623047
  batch 20 loss: 1.678971290588379, 2.8904523849487305, 146.2015838623047
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6462066173553467 2.5782127380371094 130.55685424804688
Loss :  1.6224861145019531 3.1741878986358643 160.33187866210938
Loss :  1.6389881372451782 3.3818464279174805 170.73130798339844
Loss :  1.6536827087402344 3.6936986446380615 186.338623046875
Loss :  1.6787968873977661 3.1123404502868652 157.2958221435547
Loss :  1.6401938199996948 3.0325088500976562 153.26564025878906
Loss :  1.6481930017471313 3.4641542434692383 174.8559112548828
Loss :  1.644045114517212 3.046787977218628 153.9834442138672
Loss :  1.5958269834518433 3.0439693927764893 153.79429626464844
Loss :  1.674363374710083 3.4448771476745605 173.918212890625
Loss :  1.5956645011901855 3.733898878097534 188.2906036376953
Loss :  1.658814787864685 3.2041850090026855 161.86805725097656
Loss :  1.6358286142349243 2.889547824859619 146.11322021484375
Loss :  1.634913682937622 3.6554832458496094 184.40908813476562
Loss :  1.6018859148025513 3.2220065593719482 162.70220947265625
Loss :  1.6162052154541016 3.2601470947265625 164.62356567382812
Loss :  1.6167851686477661 3.1708195209503174 160.1577606201172
Loss :  1.6724332571029663 3.0223188400268555 152.7883758544922
Loss :  1.6772571802139282 2.953982353210449 149.37637329101562
Loss :  1.686671495437622 3.4933173656463623 176.3525390625
  batch 40 loss: 1.686671495437622, 3.4933173656463623, 176.3525390625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6513452529907227 3.4268414974212646 172.99342346191406
Loss :  1.6374577283859253 3.263046979904175 164.789794921875
Loss :  1.6282416582107544 3.4536635875701904 174.31141662597656
Loss :  1.6395562887191772 2.8241050243377686 142.8448028564453
Loss :  1.6208796501159668 2.6628315448760986 134.762451171875
Loss :  1.6489061117172241 2.839661121368408 143.63197326660156
Loss :  1.6791380643844604 3.5833323001861572 180.8457489013672
Loss :  1.635412335395813 3.0325677394866943 153.2637939453125
Loss :  1.692145824432373 3.192030668258667 161.29367065429688
Loss :  1.6404708623886108 4.044173717498779 203.84915161132812
Loss :  1.6707298755645752 3.8359017372131348 193.4658203125
Loss :  1.664929986000061 2.8736023902893066 145.3450469970703
Loss :  1.6468168497085571 2.757000207901001 139.496826171875
Loss :  1.6721398830413818 2.977367639541626 150.54051208496094
Loss :  1.637367844581604 3.8830270767211914 195.78872680664062
Loss :  1.6934295892715454 2.8703367710113525 145.2102813720703
Loss :  1.643345594406128 3.387463331222534 171.01651000976562
Loss :  1.628793478012085 3.223465919494629 162.80209350585938
Loss :  1.641434669494629 3.2884089946746826 166.0618896484375
Loss :  1.6988285779953003 2.807124376296997 142.05503845214844
  batch 60 loss: 1.6988285779953003, 2.807124376296997, 142.05503845214844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.6289470195770264 3.012634515762329 152.26068115234375
Loss :  1.6554378271102905 3.334670066833496 168.38894653320312
Loss :  1.63692045211792 2.71713924407959 137.49388122558594
Loss :  1.6269830465316772 3.403773307800293 171.81564331054688
Loss :  1.6085009574890137 2.635878562927246 133.40243530273438
Loss :  1.638400673866272 4.194216251373291 211.34921264648438
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6494858264923096 4.124343395233154 207.8666534423828
Loss :  1.6464756727218628 4.1196699142456055 207.6299591064453
Loss :  1.651793360710144 4.007275104522705 202.0155487060547
Total LOSS train 159.80758854792668 valid 207.2153434753418
CE LOSS train 1.6471121953083918 valid 0.412948340177536
Contrastive LOSS train 3.16320952635545 valid 1.0018187761306763
EPOCH 154:
Loss :  1.6667941808700562 2.669177532196045 135.12567138671875
Loss :  1.678888201713562 3.1283297538757324 158.0953826904297
Loss :  1.6516252756118774 3.1619911193847656 159.7511749267578
Loss :  1.6586768627166748 3.7695839405059814 190.13787841796875
Loss :  1.6722060441970825 2.706953287124634 137.01988220214844
Loss :  1.6422758102416992 2.9472177028656006 149.00315856933594
Loss :  1.6734932661056519 3.8018946647644043 191.76821899414062
Loss :  1.6524145603179932 2.788074016571045 141.05612182617188
Loss :  1.6455451250076294 2.8628830909729004 144.78968811035156
Loss :  1.6751008033752441 3.4657962322235107 174.96490478515625
Loss :  1.6365381479263306 3.2528116703033447 164.27711486816406
Loss :  1.6356306076049805 3.1519176959991455 159.2315216064453
Loss :  1.6334929466247559 3.389273166656494 171.09715270996094
Loss :  1.6397104263305664 2.889068365097046 146.09312438964844
Loss :  1.6870638132095337 3.093461513519287 156.36013793945312
Loss :  1.6821494102478027 3.006708860397339 152.01759338378906
Loss :  1.63136625289917 3.512051820755005 177.23394775390625
Loss :  1.6541537046432495 2.819821357727051 142.64523315429688
Loss :  1.6286364793777466 3.1096205711364746 157.1096649169922
Loss :  1.6828771829605103 3.0286576747894287 153.11575317382812
  batch 20 loss: 1.6828771829605103, 3.0286576747894287, 153.11575317382812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6511013507843018 3.2140650749206543 162.35435485839844
Loss :  1.6282107830047607 2.914036512374878 147.3300323486328
Loss :  1.6439497470855713 2.8933072090148926 146.30931091308594
Loss :  1.6579022407531738 3.366558074951172 169.98580932617188
Loss :  1.6825002431869507 3.3700501918792725 170.18499755859375
Loss :  1.646765112876892 3.0554511547088623 154.4193115234375
Loss :  1.6549010276794434 3.9046590328216553 196.8878631591797
Loss :  1.6519814729690552 3.2089805603027344 162.10101318359375
Loss :  1.607487678527832 2.9478962421417236 149.00228881835938
Loss :  1.681929349899292 3.0080809593200684 152.0859832763672
Loss :  1.609420657157898 3.8595504760742188 194.58694458007812
Loss :  1.6682708263397217 3.2017762660980225 161.757080078125
Loss :  1.6466290950775146 3.231971025466919 163.24517822265625
Loss :  1.6455633640289307 3.6030828952789307 181.79971313476562
Loss :  1.6146631240844727 2.901139259338379 146.671630859375
Loss :  1.6277234554290771 2.930687427520752 148.16209411621094
Loss :  1.6275634765625 3.6486449241638184 184.059814453125
Loss :  1.6793187856674194 2.914313316345215 147.39498901367188
Loss :  1.6836344003677368 3.1215360164642334 157.76043701171875
Loss :  1.6918237209320068 3.0385234355926514 153.6179962158203
  batch 40 loss: 1.6918237209320068, 3.0385234355926514, 153.6179962158203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6581758260726929 2.8628125190734863 144.79879760742188
Loss :  1.6455143690109253 3.09783673286438 156.53733825683594
Loss :  1.6366009712219238 3.959585666656494 199.61587524414062
Loss :  1.647218942642212 3.2083580493927 162.06512451171875
Loss :  1.6286393404006958 2.6134145259857178 132.29937744140625
Loss :  1.6549559831619263 2.7680153846740723 140.05572509765625
Loss :  1.683427333831787 2.7100179195404053 137.184326171875
Loss :  1.6428942680358887 2.9914581775665283 151.21580505371094
Loss :  1.6961870193481445 3.1281919479370117 158.10577392578125
Loss :  1.6476178169250488 2.876210927963257 145.45816040039062
Loss :  1.67633056640625 4.090219974517822 206.1873321533203
Loss :  1.6701347827911377 3.021308422088623 152.7355499267578
Loss :  1.6528798341751099 2.9634523391723633 149.82550048828125
Loss :  1.6773970127105713 2.9039931297302246 146.87705993652344
Loss :  1.6435747146606445 2.907038927078247 146.99551391601562
Loss :  1.696486473083496 3.344111442565918 168.9020538330078
Loss :  1.6488935947418213 3.4489471912384033 174.09625244140625
Loss :  1.6354247331619263 3.2962381839752197 166.44732666015625
Loss :  1.6478445529937744 3.727017402648926 187.99871826171875
Loss :  1.7025277614593506 4.0857367515563965 205.98936462402344
  batch 60 loss: 1.7025277614593506, 4.0857367515563965, 205.98936462402344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6360361576080322 2.852900266647339 144.2810516357422
Loss :  1.6614165306091309 3.0358452796936035 153.4536895751953
Loss :  1.6442184448242188 3.0272204875946045 153.0052490234375
Loss :  1.6355091333389282 3.2390248775482178 163.58676147460938
Loss :  1.6183947324752808 2.755614757537842 139.3991241455078
Loss :  1.6575956344604492 4.3525590896606445 219.28555297851562
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.668453574180603 4.283702373504639 215.85357666015625
Loss :  1.6661015748977661 4.289034843444824 216.1178436279297
Loss :  1.6715070009231567 4.040555000305176 203.6992645263672
Total LOSS train 159.96503108097957 valid 213.7390594482422
CE LOSS train 1.6541273832321166 valid 0.4178767502307892
Contrastive LOSS train 3.1662180827214166 valid 1.010138750076294
EPOCH 155:
Loss :  1.6734085083007812 2.8181545734405518 142.58114624023438
Loss :  1.6843394041061401 3.0991575717926025 156.64222717285156
Loss :  1.658915638923645 3.012504816055298 152.28416442871094
Loss :  1.665367603302002 3.4799087047576904 175.6608123779297
Loss :  1.6782842874526978 2.922691583633423 147.8128662109375
Loss :  1.6491141319274902 2.91507887840271 147.40304565429688
Loss :  1.6786562204360962 3.403764009475708 171.86685180664062
Loss :  1.6581199169158936 2.9943740367889404 151.3768310546875
Loss :  1.651502251625061 2.804091453552246 141.8560791015625
Loss :  1.6796036958694458 3.1848649978637695 160.92286682128906
Loss :  1.6424224376678467 3.3768608570098877 170.4854736328125
Loss :  1.64197838306427 2.9054596424102783 146.9149627685547
Loss :  1.6398171186447144 2.6907777786254883 136.1787109375
Loss :  1.6457289457321167 2.9250454902648926 147.8979949951172
Loss :  1.6911990642547607 3.261199474334717 164.75115966796875
Loss :  1.6871427297592163 2.7381625175476074 138.59527587890625
Loss :  1.6389459371566772 3.0090110301971436 152.08949279785156
Loss :  1.660993218421936 2.877803325653076 145.55116271972656
Loss :  1.6366945505142212 2.6287248134613037 133.0729217529297
Loss :  1.6876237392425537 2.7484755516052246 139.1114044189453
  batch 20 loss: 1.6876237392425537, 2.7484755516052246, 139.1114044189453
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6576735973358154 2.813521385192871 142.333740234375
Loss :  1.6364587545394897 2.7822227478027344 140.7476043701172
Loss :  1.651600956916809 2.9486889839172363 149.08604431152344
Loss :  1.664831280708313 3.5970067977905273 181.51516723632812
Loss :  1.687699317932129 2.8308613300323486 143.2307586669922
Loss :  1.6536149978637695 2.8180463314056396 142.55592346191406
Loss :  1.661624550819397 3.0271155834198 153.01739501953125
Loss :  1.6581753492355347 2.7960996627807617 141.46315002441406
Loss :  1.6158925294876099 3.482236385345459 175.7277069091797
Loss :  1.6858211755752563 3.5742990970611572 180.40077209472656
Loss :  1.6166832447052002 2.8504726886749268 144.14031982421875
Loss :  1.6727008819580078 3.0406951904296875 153.70745849609375
Loss :  1.6526626348495483 3.3123412132263184 167.2697296142578
Loss :  1.6519372463226318 3.751006841659546 189.2022705078125
Loss :  1.6230790615081787 3.029852867126465 153.11572265625
Loss :  1.635853886604309 2.9027633666992188 146.77401733398438
Loss :  1.6360546350479126 2.8609702587127686 144.68455505371094
Loss :  1.6849875450134277 3.084956407546997 155.93280029296875
Loss :  1.6894501447677612 3.9550962448120117 199.4442596435547
Loss :  1.697666049003601 3.4130899906158447 172.35215759277344
  batch 40 loss: 1.697666049003601, 3.4130899906158447, 172.35215759277344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6658586263656616 2.9455790519714355 148.94480895996094
Loss :  1.6539663076400757 2.7806406021118164 140.6859893798828
Loss :  1.6460422277450562 3.0700414180755615 155.1481170654297
Loss :  1.65546452999115 3.1996428966522217 161.6376190185547
Loss :  1.6393572092056274 2.943955421447754 148.83712768554688
Loss :  1.6645301580429077 3.81030535697937 192.1798095703125
Loss :  1.690869688987732 2.7853922843933105 140.96047973632812
Loss :  1.6524344682693481 2.871345281600952 145.21969604492188
Loss :  1.7019692659378052 2.798691511154175 141.63653564453125
Loss :  1.6568186283111572 3.259265899658203 164.62010192871094
Loss :  1.6833152770996094 2.8306894302368164 143.21778869628906
Loss :  1.6783227920532227 2.7945432662963867 141.40548706054688
Loss :  1.6619614362716675 2.9159324169158936 147.4585723876953
Loss :  1.6845391988754272 3.0175929069519043 152.56417846679688
Loss :  1.652794361114502 3.2298576831817627 163.14569091796875
Loss :  1.7017663717269897 3.4544339179992676 174.4234619140625
Loss :  1.657245397567749 2.758007526397705 139.5576171875
Loss :  1.6445367336273193 2.6932895183563232 136.3090057373047
Loss :  1.6562522649765015 3.153905153274536 159.35150146484375
Loss :  1.7072035074234009 2.6504456996917725 134.22947692871094
  batch 60 loss: 1.7072035074234009, 2.6504456996917725, 134.22947692871094
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.645118236541748 3.0377793312072754 153.53407287597656
Loss :  1.6686702966690063 3.5967133045196533 181.50433349609375
Loss :  1.6523292064666748 2.963397741317749 149.8222198486328
Loss :  1.6441490650177002 3.5743749141693115 180.36289978027344
Loss :  1.6278231143951416 3.1934149265289307 161.29856872558594
Loss :  1.6538419723510742 4.142339706420898 208.7708282470703
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6640156507492065 4.084603786468506 205.8942108154297
Loss :  1.660967230796814 4.062089443206787 204.76544189453125
Loss :  1.6665935516357422 3.989840030670166 201.15859985351562
Total LOSS train 154.7355717585637 valid 205.14727020263672
CE LOSS train 1.6611948306743916 valid 0.41664838790893555
Contrastive LOSS train 3.061487568341769 valid 0.9974600076675415
EPOCH 156:
Loss :  1.6795066595077515 3.563532590866089 179.85614013671875
Loss :  1.6897916793823242 3.3264503479003906 168.01229858398438
Loss :  1.6659904718399048 2.7784905433654785 140.59051513671875
Loss :  1.6721107959747314 2.81569242477417 142.45672607421875
Loss :  1.6842126846313477 2.546495199203491 129.00897216796875
Loss :  1.6561236381530762 2.753342866897583 139.32327270507812
Loss :  1.6843814849853516 3.0110723972320557 152.23800659179688
Loss :  1.66498863697052 3.457698345184326 174.54991149902344
Loss :  1.6590681076049805 3.87760066986084 195.5391082763672
Loss :  1.685426115989685 3.477768898010254 175.57386779785156
Loss :  1.6501716375350952 3.4939727783203125 176.34881591796875
Loss :  1.6493401527404785 3.305712938308716 166.9349822998047
Loss :  1.6467218399047852 3.4634995460510254 174.8217010498047
Loss :  1.6514339447021484 3.1653382778167725 159.9183349609375
Loss :  1.6946916580200195 2.9982807636260986 151.6087188720703
Loss :  1.6901332139968872 3.4195821285247803 172.66925048828125
Loss :  1.6433509588241577 2.960090398788452 149.6478729248047
Loss :  1.6644057035446167 3.05855655670166 154.59222412109375
Loss :  1.6405463218688965 3.867755651473999 195.02833557128906
Loss :  1.6906126737594604 2.952345371246338 149.30787658691406
  batch 20 loss: 1.6906126737594604, 2.952345371246338, 149.30787658691406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6609991788864136 2.8318428993225098 143.25314331054688
Loss :  1.64000403881073 2.9358370304107666 148.43185424804688
Loss :  1.655071496963501 2.8660778999328613 144.95896911621094
Loss :  1.667845606803894 2.9417107105255127 148.75338745117188
Loss :  1.690393328666687 3.255509376525879 164.4658660888672
Loss :  1.6574139595031738 3.824120044708252 192.86341857910156
Loss :  1.664584994316101 2.9446685314178467 148.89801025390625
Loss :  1.6617460250854492 2.8321642875671387 143.26995849609375
Loss :  1.6206032037734985 3.651047468185425 184.1729736328125
Loss :  1.689191222190857 2.8941073417663574 146.39456176757812
Loss :  1.6231433153152466 3.1474950313568115 158.99789428710938
Loss :  1.6769806146621704 3.6951417922973633 186.43408203125
Loss :  1.6573951244354248 2.9840941429138184 150.8621063232422
Loss :  1.6560348272323608 3.278916597366333 165.60186767578125
Loss :  1.6282795667648315 3.1871049404144287 160.9835205078125
Loss :  1.6402356624603271 2.8538622856140137 144.33334350585938
Loss :  1.6401435136795044 2.8597142696380615 144.6258544921875
Loss :  1.6877062320709229 3.1524407863616943 159.30975341796875
Loss :  1.691266655921936 2.6599578857421875 134.6891632080078
Loss :  1.699044942855835 3.692707061767578 186.3343963623047
  batch 40 loss: 1.699044942855835, 3.692707061767578, 186.3343963623047
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6680920124053955 3.163090944290161 159.8226318359375
Loss :  1.6563611030578613 3.102083444595337 156.76052856445312
Loss :  1.6482115983963013 2.952965259552002 149.29647827148438
Loss :  1.657835602760315 2.9047844409942627 146.89706420898438
Loss :  1.6407158374786377 2.7007789611816406 136.67965698242188
Loss :  1.6646758317947388 2.836190700531006 143.47421264648438
Loss :  1.6908711194992065 2.7279465198516846 138.08819580078125
Loss :  1.6529618501663208 2.6586015224456787 134.58303833007812
Loss :  1.702319622039795 4.449860572814941 224.19534301757812
Loss :  1.6576625108718872 3.904381036758423 196.87672424316406
Loss :  1.684065580368042 3.332587480545044 168.31344604492188
Loss :  1.6789464950561523 2.951975107192993 149.27769470214844
Loss :  1.6632287502288818 2.972804069519043 150.3034210205078
Loss :  1.6855134963989258 3.3848588466644287 170.92845153808594
Loss :  1.654720425605774 3.2999470233917236 166.65206909179688
Loss :  1.7027586698532104 4.083226680755615 205.8640899658203
Loss :  1.6598992347717285 3.0753743648529053 155.42861938476562
Loss :  1.6476331949234009 2.863276481628418 144.8114471435547
Loss :  1.6587246656417847 3.5436787605285645 178.84266662597656
Loss :  1.7094072103500366 3.056605100631714 154.5396728515625
  batch 60 loss: 1.7094072103500366, 3.056605100631714, 154.5396728515625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6475110054016113 3.087397575378418 156.0173797607422
Loss :  1.670771837234497 3.0710256099700928 155.22206115722656
Loss :  1.6540780067443848 3.686767339706421 185.99244689941406
Loss :  1.6456077098846436 3.2533438205718994 164.31280517578125
Loss :  1.6287702322006226 2.5916059017181396 131.2090606689453
Loss :  1.653268814086914 4.261383533477783 214.7224578857422
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6633305549621582 4.344836711883545 218.90516662597656
Loss :  1.6601980924606323 4.231717109680176 213.24606323242188
Loss :  1.6659445762634277 3.9879729747772217 201.06459045410156
Total LOSS train 160.46231173001803 valid 211.98456954956055
CE LOSS train 1.6646528537456806 valid 0.41648614406585693
Contrastive LOSS train 3.1759531791393574 valid 0.9969932436943054
EPOCH 157:
Loss :  1.6789380311965942 3.0373799800872803 153.54794311523438
Loss :  1.6894336938858032 3.0915136337280273 156.26512145996094
Loss :  1.6657830476760864 2.794571876525879 141.39437866210938
Loss :  1.6715843677520752 3.293923854827881 166.36778259277344
Loss :  1.6841918230056763 3.0335159301757812 153.3599853515625
Loss :  1.6566617488861084 3.7845346927642822 190.88339233398438
Loss :  1.6846264600753784 2.942906141281128 148.82994079589844
Loss :  1.6655950546264648 2.842010021209717 143.76608276367188
Loss :  1.6595538854599 3.500887870788574 176.70394897460938
Loss :  1.6858810186386108 3.2292022705078125 163.14599609375
Loss :  1.651476502418518 3.0023109912872314 151.76702880859375
Loss :  1.651168942451477 3.6547958850860596 184.39096069335938
Loss :  1.649147391319275 3.0943689346313477 156.3675994873047
Loss :  1.654318928718567 2.8404412269592285 143.67637634277344
Loss :  1.6968324184417725 2.896182060241699 146.5059356689453
Loss :  1.6928876638412476 3.039705753326416 153.6781768798828
Loss :  1.6475356817245483 3.003472328186035 151.82115173339844
Loss :  1.66779363155365 3.6470091342926025 184.0182647705078
Loss :  1.6444969177246094 2.7980828285217285 141.54864501953125
Loss :  1.69290292263031 2.904454469680786 146.91561889648438
  batch 20 loss: 1.69290292263031, 2.904454469680786, 146.91561889648438
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6638414859771729 2.853410005569458 144.3343505859375
Loss :  1.6434848308563232 2.9552934169769287 149.40814208984375
Loss :  1.657615303993225 3.09834885597229 156.57505798339844
Loss :  1.669601559638977 3.9284274578094482 198.09097290039062
Loss :  1.6917541027069092 3.5747263431549072 180.42807006835938
Loss :  1.6595522165298462 3.104525089263916 156.88580322265625
Loss :  1.667084813117981 3.5416624546051025 178.75021362304688
Loss :  1.6641439199447632 2.707503318786621 137.039306640625
Loss :  1.6237576007843018 2.886190891265869 145.9333038330078
Loss :  1.6907422542572021 3.198923349380493 161.63690185546875
Loss :  1.6252949237823486 3.7169721126556396 187.47389221191406
Loss :  1.6783424615859985 3.128864049911499 158.12155151367188
Loss :  1.65903639793396 3.015651226043701 152.4416046142578
Loss :  1.6586799621582031 2.8548712730407715 144.40223693847656
Loss :  1.6318249702453613 3.134373664855957 158.3505096435547
Loss :  1.6438697576522827 2.9925262928009033 151.2701873779297
Loss :  1.6435760259628296 2.9668421745300293 149.9856719970703
Loss :  1.689500331878662 2.840816020965576 143.7303009033203
Loss :  1.6929121017456055 2.7901337146759033 141.19960021972656
Loss :  1.7006760835647583 3.416663408279419 172.5338592529297
  batch 40 loss: 1.7006760835647583, 3.416663408279419, 172.5338592529297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6705018281936646 3.1598751544952393 159.6642608642578
Loss :  1.6597905158996582 2.725430965423584 137.93133544921875
Loss :  1.6520408391952515 3.2632038593292236 164.81222534179688
Loss :  1.662468433380127 2.854468584060669 144.3859100341797
Loss :  1.646569848060608 2.767259120941162 140.00953674316406
Loss :  1.6693764925003052 3.1304872035980225 158.1937255859375
Loss :  1.6946415901184082 3.7593727111816406 189.66326904296875
Loss :  1.6587445735931396 2.634922742843628 133.40487670898438
Loss :  1.706037998199463 2.753751277923584 139.3936004638672
Loss :  1.6637073755264282 3.2321743965148926 163.27243041992188
Loss :  1.6888930797576904 2.9888570308685303 151.13174438476562
Loss :  1.68436861038208 2.8306150436401367 143.21511840820312
Loss :  1.669585108757019 2.6638705730438232 134.8631134033203
Loss :  1.690766453742981 3.517115354537964 177.54653930664062
Loss :  1.6613976955413818 3.6453137397766113 183.9270782470703
Loss :  1.7071560621261597 3.4910380840301514 176.25906372070312
Loss :  1.6662641763687134 3.569638967514038 180.14820861816406
Loss :  1.6547808647155762 3.3792872428894043 170.619140625
Loss :  1.6651571989059448 3.150585651397705 159.19444274902344
Loss :  1.712854027748108 3.353205919265747 169.37315368652344
  batch 60 loss: 1.712854027748108, 3.353205919265747, 169.37315368652344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  1.6548758745193481 2.8674747943878174 145.0286102294922
Loss :  1.6767137050628662 2.907686710357666 147.06105041503906
Loss :  1.660272479057312 2.8361785411834717 143.46920776367188
Loss :  1.6514991521835327 3.2850308418273926 165.90304565429688
Loss :  1.6355197429656982 2.436704635620117 123.47075653076172
Loss :  1.6672083139419556 3.999021053314209 201.61825561523438
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6769802570343018 3.8708975315093994 195.22186279296875
Loss :  1.674956202507019 3.6638169288635254 184.8657989501953
Loss :  1.6794463396072388 3.709872245788574 187.1730499267578
Total LOSS train 157.3151894202599 valid 192.21974182128906
CE LOSS train 1.6678474609668439 valid 0.4198615849018097
Contrastive LOSS train 3.1129468330970176 valid 0.9274680614471436
EPOCH 158:
Loss :  1.683943510055542 2.712038993835449 137.285888671875
Loss :  1.6935725212097168 3.263397693634033 164.86346435546875
Loss :  1.6711838245391846 2.834986925125122 143.4205322265625
Loss :  1.6767905950546265 3.12960147857666 158.1568603515625
Loss :  1.6887531280517578 3.61448335647583 182.4129180908203
Loss :  1.6622498035430908 2.8657329082489014 144.9488983154297
Loss :  1.6896952390670776 3.0734708309173584 155.3632354736328
Loss :  1.6715587377548218 2.9186158180236816 147.6023406982422
Loss :  1.6661183834075928 3.211367130279541 162.23448181152344
Loss :  1.690900444984436 2.6577818393707275 134.5800018310547
Loss :  1.6584473848342896 2.999563694000244 151.63662719726562
Loss :  1.6587555408477783 3.558487892150879 179.58314514160156
Loss :  1.6568013429641724 2.939967632293701 148.65518188476562
Loss :  1.6619333028793335 2.8699073791503906 145.15728759765625
Loss :  1.702170729637146 3.677255392074585 185.56492614746094
Loss :  1.6978611946105957 3.7418978214263916 188.79275512695312
Loss :  1.6555097103118896 3.12459659576416 157.8853302001953
Loss :  1.6750783920288086 3.5129916667938232 177.3246612548828
Loss :  1.6533294916152954 2.62860107421875 133.08338928222656
Loss :  1.6990190744400024 2.9393956661224365 148.66880798339844
  batch 20 loss: 1.6990190744400024, 2.9393956661224365, 148.66880798339844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6720006465911865 3.644273042678833 183.88565063476562
Loss :  1.6528986692428589 3.471717596054077 175.23876953125
Loss :  1.6659899950027466 3.0231173038482666 152.8218536376953
Loss :  1.6774287223815918 3.2417385578155518 163.7643585205078
Loss :  1.6983364820480347 3.9690239429473877 200.1495361328125
Loss :  1.668325662612915 3.0677947998046875 155.0580596923828
Loss :  1.6747599840164185 2.8993144035339355 146.64047241210938
Loss :  1.6721199750900269 2.7709972858428955 140.22198486328125
Loss :  1.6336027383804321 3.101264715194702 156.69683837890625
Loss :  1.6962536573410034 3.059556484222412 154.67408752441406
Loss :  1.6344060897827148 3.7609188556671143 189.6803436279297
Loss :  1.684533715248108 3.554673671722412 179.41822814941406
Loss :  1.6653250455856323 3.0373716354370117 153.53390502929688
Loss :  1.6648178100585938 3.125626564025879 157.94613647460938
Loss :  1.6383224725723267 3.3423309326171875 168.75486755371094
Loss :  1.6498609781265259 3.064807415008545 154.89022827148438
Loss :  1.6494816541671753 3.3807802200317383 170.68849182128906
Loss :  1.6931525468826294 2.764803886413574 139.93333435058594
Loss :  1.6967589855194092 2.799268960952759 141.66021728515625
Loss :  1.7043691873550415 2.752171754837036 139.31295776367188
  batch 40 loss: 1.7043691873550415, 2.752171754837036, 139.31295776367188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.67582368850708 3.6597635746002197 184.66400146484375
Loss :  1.665596842765808 2.9784767627716064 150.58944702148438
Loss :  1.6579843759536743 3.5463500022888184 178.97549438476562
Loss :  1.6672152280807495 3.843412160873413 193.83782958984375
Loss :  1.65093195438385 3.03145694732666 153.22377014160156
Loss :  1.67236328125 3.370022773742676 170.1735076904297
Loss :  1.6966315507888794 2.7954885959625244 141.47105407714844
Loss :  1.660935401916504 3.1769306659698486 160.50746154785156
Loss :  1.707266926765442 2.778480291366577 140.6312713623047
Loss :  1.6645113229751587 2.831878423690796 143.2584228515625
Loss :  1.689542293548584 3.1696925163269043 160.17416381835938
Loss :  1.6844069957733154 3.3184170722961426 167.60525512695312
Loss :  1.6692262887954712 2.8684816360473633 145.0933074951172
Loss :  1.690041422843933 2.966562032699585 150.0181427001953
Loss :  1.660722017288208 3.5810816287994385 180.7147979736328
Loss :  1.7065482139587402 2.782362937927246 140.82469177246094
Loss :  1.6656700372695923 3.0171122550964355 152.52127075195312
Loss :  1.6539406776428223 2.8075010776519775 142.0290069580078
Loss :  1.664728045463562 3.1306002140045166 158.1947479248047
Loss :  1.7122447490692139 2.7732841968536377 140.37646484375
  batch 60 loss: 1.7122447490692139, 2.7732841968536377, 140.37646484375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.654652714729309 2.8690054416656494 145.10491943359375
Loss :  1.6769930124282837 3.183955430984497 160.874755859375
Loss :  1.6618248224258423 2.7329516410827637 138.3094024658203
Loss :  1.654517650604248 2.9007513523101807 146.69207763671875
Loss :  1.639568567276001 2.4411001205444336 123.694580078125
Loss :  1.6706516742706299 3.9735286235809326 200.3470916748047
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6801148653030396 4.0313286781311035 203.24655151367188
Loss :  1.678136944770813 3.9248416423797607 197.9202117919922
Loss :  1.6827120780944824 3.7874042987823486 191.05291748046875
Total LOSS train 157.56539846567009 valid 198.14169311523438
CE LOSS train 1.6724662377284123 valid 0.4206780195236206
Contrastive LOSS train 3.1178586703080398 valid 0.9468510746955872
EPOCH 159:
Loss :  1.6873096227645874 2.9941649436950684 151.39556884765625
Loss :  1.6968891620635986 3.073421001434326 155.36793518066406
Loss :  1.67448890209198 3.0576579570770264 154.55738830566406
Loss :  1.679715633392334 2.8986775875091553 146.6136016845703
Loss :  1.6911647319793701 3.686187744140625 186.00054931640625
Loss :  1.665125846862793 3.379563570022583 170.643310546875
Loss :  1.691279649734497 2.8864083290100098 146.01170349121094
Loss :  1.6732759475708008 2.8375439643859863 143.55047607421875
Loss :  1.6673107147216797 2.785818338394165 140.95823669433594
Loss :  1.6916056871414185 3.02666974067688 153.02508544921875
Loss :  1.6590909957885742 2.946650505065918 148.99160766601562
Loss :  1.65901780128479 3.111531972885132 157.23561096191406
Loss :  1.656546950340271 3.0668716430664062 155.0001220703125
Loss :  1.6615550518035889 3.316206455230713 167.4718780517578
Loss :  1.7018089294433594 3.1580522060394287 159.6044158935547
Loss :  1.6975104808807373 2.943164348602295 148.85572814941406
Loss :  1.6555014848709106 3.0309181213378906 153.20140075683594
Loss :  1.6751009225845337 2.9855527877807617 150.95272827148438
Loss :  1.6539753675460815 2.9905624389648438 151.18209838867188
Loss :  1.6989156007766724 3.1231043338775635 157.8541259765625
  batch 20 loss: 1.6989156007766724, 3.1231043338775635, 157.8541259765625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.672149419784546 2.859160900115967 144.63018798828125
Loss :  1.6535282135009766 2.925759792327881 147.9415283203125
Loss :  1.666499376296997 2.7280638217926025 138.0697021484375
Loss :  1.677965760231018 2.9096498489379883 147.16046142578125
Loss :  1.6984906196594238 3.2934155464172363 166.3692626953125
Loss :  1.6682380437850952 2.9260735511779785 147.971923828125
Loss :  1.6747080087661743 2.9379377365112305 148.57159423828125
Loss :  1.672269582748413 2.904038906097412 146.8742218017578
Loss :  1.6339796781539917 3.131638765335083 158.21591186523438
Loss :  1.6966978311538696 3.115506172180176 157.47201538085938
Loss :  1.6350516080856323 3.1840600967407227 160.8380584716797
Loss :  1.6848053932189941 3.0290701389312744 153.1383056640625
Loss :  1.6664890050888062 3.0030324459075928 151.818115234375
Loss :  1.6660572290420532 2.7549335956573486 139.41273498535156
Loss :  1.6411226987838745 3.54121994972229 178.70213317871094
Loss :  1.6526055335998535 3.9573850631713867 199.5218505859375
Loss :  1.652396321296692 2.7841804027557373 140.8614044189453
Loss :  1.6956632137298584 2.6599442958831787 134.69287109375
Loss :  1.699246883392334 2.953002691268921 149.34938049316406
Loss :  1.7063008546829224 2.89982271194458 146.6974334716797
  batch 40 loss: 1.7063008546829224, 2.89982271194458, 146.6974334716797
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6781792640686035 3.122222661972046 157.789306640625
Loss :  1.6679726839065552 3.2095437049865723 162.14515686035156
Loss :  1.6606316566467285 3.4307339191436768 173.19732666015625
Loss :  1.6694893836975098 3.1733388900756836 160.33644104003906
Loss :  1.6546615362167358 3.041473150253296 153.7283172607422
Loss :  1.677247405052185 2.7638401985168457 139.8692626953125
Loss :  1.7009336948394775 2.7184932231903076 137.62559509277344
Loss :  1.6667436361312866 2.9489173889160156 149.11260986328125
Loss :  1.7114644050598145 3.098883628845215 156.65565490722656
Loss :  1.670315146446228 3.9815149307250977 200.74606323242188
Loss :  1.694202184677124 3.7681641578674316 190.10240173339844
Loss :  1.6889231204986572 2.943692684173584 148.87355041503906
Loss :  1.674851655960083 2.7935147285461426 141.3505859375
Loss :  1.6950299739837646 2.927863359451294 148.08819580078125
Loss :  1.6675453186035156 3.1666371822357178 159.99940490722656
Loss :  1.7112432718276978 3.4308526515960693 173.25387573242188
Loss :  1.672261357307434 3.4090240001678467 172.1234588623047
Loss :  1.6604760885238647 3.2835142612457275 165.83619689941406
Loss :  1.6695581674575806 3.251960515975952 164.267578125
Loss :  1.7145159244537354 2.90921688079834 147.1753692626953
  batch 60 loss: 1.7145159244537354, 2.90921688079834, 147.1753692626953
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.657595157623291 3.6012375354766846 181.71946716308594
Loss :  1.6788073778152466 3.3258745670318604 167.9725341796875
Loss :  1.6631512641906738 2.921832323074341 147.7547607421875
Loss :  1.6556415557861328 3.2224647998809814 162.7788848876953
Loss :  1.6401258707046509 2.434643030166626 123.37228393554688
Loss :  1.6652789115905762 3.51172137260437 177.25135803222656
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.674605369567871 3.3559646606445312 169.47283935546875
Loss :  1.672584056854248 3.0974411964416504 156.5446319580078
Loss :  1.6768302917480469 3.0731635093688965 155.3350067138672
Total LOSS train 156.04087618314304 valid 164.65095901489258
CE LOSS train 1.6746618747711182 valid 0.4192075729370117
Contrastive LOSS train 3.0873242891751804 valid 0.7682908773422241
EPOCH 160:
Loss :  1.6864386796951294 2.7306008338928223 138.2164764404297
Loss :  1.6963016986846924 3.1094908714294434 157.1708526611328
Loss :  1.6742966175079346 2.919215440750122 147.63507080078125
Loss :  1.6793968677520752 3.1011130809783936 156.73504638671875
Loss :  1.6913535594940186 2.9986650943756104 151.62461853027344
Loss :  1.66497004032135 3.2387895584106445 163.6044464111328
Loss :  1.692318320274353 2.7910027503967285 141.24246215820312
Loss :  1.6739424467086792 2.774205207824707 140.3842010498047
Loss :  1.6686022281646729 3.219503879547119 162.643798828125
Loss :  1.692838191986084 3.0717532634735107 155.28050231933594
Loss :  1.6603902578353882 2.98561692237854 150.9412384033203
Loss :  1.6600195169448853 3.5046937465667725 176.8946990966797
Loss :  1.6572355031967163 3.1187689304351807 157.59568786621094
Loss :  1.6621525287628174 3.1835989952087402 160.84210205078125
Loss :  1.702269434928894 3.3318326473236084 168.2938995361328
Loss :  1.6977843046188354 2.8486664295196533 144.131103515625
Loss :  1.6555888652801514 2.84283709526062 143.79745483398438
Loss :  1.6748008728027344 3.3553974628448486 169.44467163085938
Loss :  1.6533617973327637 2.6803576946258545 135.67124938964844
Loss :  1.698507308959961 3.319364070892334 167.6667022705078
  batch 20 loss: 1.698507308959961, 3.319364070892334, 167.6667022705078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.671194076538086 2.5223841667175293 127.7904052734375
Loss :  1.6520113945007324 3.3350846767425537 168.40623474121094
Loss :  1.6652017831802368 2.900444984436035 146.6874542236328
Loss :  1.676541805267334 3.1629133224487305 159.82220458984375
Loss :  1.6976313591003418 4.06278133392334 204.83670043945312
Loss :  1.6672732830047607 3.356881618499756 169.5113525390625
Loss :  1.6741083860397339 3.4618637561798096 174.7672882080078
Loss :  1.6715034246444702 2.7491679191589355 139.12989807128906
Loss :  1.6335088014602661 2.9394266605377197 148.60484313964844
Loss :  1.696170449256897 3.1872897148132324 161.06065368652344
Loss :  1.6346029043197632 3.4829251766204834 175.78085327148438
Loss :  1.6847882270812988 3.8229308128356934 192.83132934570312
Loss :  1.6659101247787476 3.7636160850524902 189.84671020507812
Loss :  1.665215015411377 3.2783291339874268 165.58168029785156
Loss :  1.639153003692627 3.1183595657348633 157.55714416503906
Loss :  1.6504578590393066 3.0480144023895264 154.05117797851562
Loss :  1.6501351594924927 3.3963472843170166 171.46749877929688
Loss :  1.693909764289856 3.222404718399048 162.81414794921875
Loss :  1.697346568107605 3.537388563156128 178.5667724609375
Loss :  1.7046414613723755 3.033524513244629 153.380859375
  batch 40 loss: 1.7046414613723755, 3.033524513244629, 153.380859375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6758462190628052 3.0089330673217773 152.12249755859375
Loss :  1.66490638256073 2.7954940795898438 141.43960571289062
Loss :  1.657212734222412 2.826795816421509 142.99700927734375
Loss :  1.6668038368225098 2.875924825668335 145.46304321289062
Loss :  1.6513556241989136 2.9301235675811768 148.15753173828125
Loss :  1.6736246347427368 3.0188353061676025 152.6154022216797
Loss :  1.6981292963027954 2.8478615283966064 144.09121704101562
Loss :  1.6631088256835938 2.7480573654174805 139.06597900390625
Loss :  1.7093411684036255 3.3243861198425293 167.9286346435547
Loss :  1.6682701110839844 2.922405242919922 147.78854370117188
Loss :  1.692287802696228 3.2145495414733887 162.41976928710938
Loss :  1.687727689743042 3.2663207054138184 165.00376892089844
Loss :  1.6730753183364868 3.192439556121826 161.29505920410156
Loss :  1.6932926177978516 3.010852813720703 152.23593139648438
Loss :  1.665091872215271 3.1831512451171875 160.82264709472656
Loss :  1.7095553874969482 2.9134018421173096 147.379638671875
Loss :  1.6701936721801758 3.311596393585205 167.25001525878906
Loss :  1.658738136291504 3.6671760082244873 185.0175323486328
Loss :  1.6690394878387451 3.1149110794067383 157.4145965576172
Loss :  1.7150359153747559 2.7425355911254883 138.84182739257812
  batch 60 loss: 1.7150359153747559, 2.7425355911254883, 138.84182739257812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6592212915420532 2.67933988571167 135.626220703125
Loss :  1.6808569431304932 2.8373172283172607 143.54672241210938
Loss :  1.6663157939910889 3.1409707069396973 158.71485900878906
Loss :  1.658290982246399 3.205486536026001 161.9326171875
Loss :  1.6433318853378296 2.5898079872131348 131.13372802734375
Loss :  1.6676658391952515 4.305080890655518 216.9217071533203
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.676878571510315 4.313303470611572 217.34205627441406
Loss :  1.6746814250946045 4.132242679595947 208.2868194580078
Loss :  1.6791189908981323 4.069604396820068 205.1593475341797
Total LOSS train 156.96332139235275 valid 211.92748260498047
CE LOSS train 1.6739158080174372 valid 0.4197797477245331
Contrastive LOSS train 3.105788098848783 valid 1.017401099205017
EPOCH 161:
Loss :  1.689232349395752 3.1862664222717285 161.0025634765625
Loss :  1.6989902257919312 3.149796962738037 159.1888427734375
Loss :  1.677483081817627 2.804119348526001 141.88345336914062
Loss :  1.6829036474227905 2.7748472690582275 140.42527770996094
Loss :  1.693880319595337 3.7055304050445557 186.97039794921875
Loss :  1.6689717769622803 2.763739585876465 139.85595703125
Loss :  1.6943968534469604 3.1415762901306152 158.77320861816406
Loss :  1.67702317237854 3.1733508110046387 160.3445587158203
Loss :  1.6713528633117676 3.0140411853790283 152.3734130859375
Loss :  1.695277214050293 2.627365827560425 133.06356811523438
Loss :  1.6640714406967163 3.203836679458618 161.8559112548828
Loss :  1.6640537977218628 3.4356377124786377 173.44593811035156
Loss :  1.6622649431228638 2.6655337810516357 134.93894958496094
Loss :  1.667432427406311 2.847776174545288 144.05624389648438
Loss :  1.7061944007873535 2.85556960105896 144.4846649169922
Loss :  1.7023425102233887 2.819225311279297 142.6636199951172
Loss :  1.6615058183670044 3.157731533050537 159.54808044433594
Loss :  1.6797834634780884 2.870412588119507 145.20040893554688
Loss :  1.6590369939804077 3.629225492477417 183.1203155517578
Loss :  1.7017550468444824 3.1680734157562256 160.1054229736328
  batch 20 loss: 1.7017550468444824, 3.1680734157562256, 160.1054229736328
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6757638454437256 2.7966578006744385 141.50865173339844
Loss :  1.656989574432373 2.677133083343506 135.51364135742188
Loss :  1.6693867444992065 2.726727247238159 138.00575256347656
Loss :  1.6803756952285767 3.2254509925842285 162.9529266357422
Loss :  1.7003040313720703 3.2047877311706543 161.93968200683594
Loss :  1.670544147491455 2.8022799491882324 141.7845458984375
Loss :  1.677065134048462 3.1528525352478027 159.31968688964844
Loss :  1.6744420528411865 3.007194995880127 152.03419494628906
Loss :  1.6374322175979614 2.6655192375183105 134.91339111328125
Loss :  1.6991617679595947 3.2305006980895996 163.2241973876953
Loss :  1.6389071941375732 3.9120259284973145 197.24020385742188
Loss :  1.6879318952560425 3.069640874862671 155.1699676513672
Loss :  1.6706864833831787 2.937289237976074 148.53514099121094
Loss :  1.6701653003692627 2.9561538696289062 149.4778594970703
Loss :  1.6456942558288574 2.9434874057769775 148.820068359375
Loss :  1.6567256450653076 2.8036820888519287 141.8408203125
Loss :  1.6566379070281982 3.4249489307403564 172.90408325195312
Loss :  1.6987229585647583 2.7973668575286865 141.56707763671875
Loss :  1.7017796039581299 2.7828550338745117 140.8445281982422
Loss :  1.708173394203186 2.7844889163970947 140.9326171875
  batch 40 loss: 1.708173394203186, 2.7844889163970947, 140.9326171875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6795132160186768 3.0457985401153564 153.96945190429688
Loss :  1.6687476634979248 2.7184574604034424 137.59161376953125
Loss :  1.6606559753417969 3.510899066925049 177.2056121826172
Loss :  1.6690593957901 3.3093087673187256 167.13449096679688
Loss :  1.6535390615463257 3.2043917179107666 161.8731231689453
Loss :  1.675338625907898 3.468094825744629 175.080078125
Loss :  1.6994898319244385 2.7645671367645264 139.9278564453125
Loss :  1.6638789176940918 2.7520947456359863 139.26861572265625
Loss :  1.7095699310302734 2.9432199001312256 148.8705596923828
Loss :  1.6679643392562866 3.6884498596191406 186.0904541015625
Loss :  1.6930046081542969 3.0727415084838867 155.330078125
Loss :  1.687972068786621 3.000523090362549 151.71412658691406
Loss :  1.672945499420166 3.1251637935638428 157.93113708496094
Loss :  1.6939113140106201 3.101952075958252 156.79151916503906
Loss :  1.6656360626220703 2.9317727088928223 148.2542724609375
Loss :  1.7103921175003052 2.832641124725342 143.34243774414062
Loss :  1.6705609560012817 2.947675943374634 149.0543670654297
Loss :  1.6590166091918945 4.091635704040527 206.2407989501953
Loss :  1.6693652868270874 3.0356783866882324 153.4532928466797
Loss :  1.7151798009872437 3.046088933944702 154.01962280273438
  batch 60 loss: 1.7151798009872437, 3.046088933944702, 154.01962280273438
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.659356713294983 2.852320909500122 144.27540588378906
Loss :  1.6806907653808594 3.0218498706817627 152.773193359375
Loss :  1.6664408445358276 3.942729949951172 198.80294799804688
Loss :  1.6589010953903198 2.9786019325256348 150.58900451660156
Loss :  1.644730806350708 3.011049747467041 152.1972198486328
Loss :  1.6714990139007568 3.9303643703460693 198.18971252441406
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6801539659500122 3.9173460006713867 197.54745483398438
Loss :  1.6784924268722534 3.9066429138183594 197.01065063476562
Loss :  1.6822351217269897 3.6882972717285156 186.0970916748047
Total LOSS train 154.97863253079927 valid 194.7112274169922
CE LOSS train 1.6767800569534301 valid 0.42055878043174744
Contrastive LOSS train 3.0660370386563813 valid 0.9220743179321289
EPOCH 162:
Loss :  1.6911371946334839 2.872681140899658 145.3251953125
Loss :  1.7007625102996826 2.9744226932525635 150.42189025878906
Loss :  1.679150938987732 2.982186794281006 150.7884979248047
Loss :  1.6845170259475708 3.2733700275421143 165.35302734375
Loss :  1.6958588361740112 3.003417730331421 151.8667449951172
Loss :  1.6709367036819458 3.2414088249206543 163.7413787841797
Loss :  1.6964137554168701 3.2909152507781982 166.24217224121094
Loss :  1.6786047220230103 2.623512029647827 132.8542022705078
Loss :  1.6726585626602173 3.424391269683838 172.8922119140625
Loss :  1.696065068244934 2.986649990081787 151.028564453125
Loss :  1.6647517681121826 2.961804151535034 149.7549591064453
Loss :  1.6641396284103394 2.996450662612915 151.48667907714844
Loss :  1.6622856855392456 3.214952230453491 162.40989685058594
Loss :  1.6667613983154297 2.8496484756469727 144.14918518066406
Loss :  1.7054619789123535 3.0823822021484375 155.82456970214844
Loss :  1.7015734910964966 3.2602486610412598 164.71400451660156
Loss :  1.660649061203003 2.9676196575164795 150.0416259765625
Loss :  1.6794248819351196 3.3349924087524414 168.42904663085938
Loss :  1.6588616371154785 3.112966775894165 157.3072052001953
Loss :  1.7024139165878296 3.2558541297912598 164.4951171875
  batch 20 loss: 1.7024139165878296, 3.2558541297912598, 164.4951171875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6762930154800415 3.006220579147339 151.98733520507812
Loss :  1.6582040786743164 2.9405856132507324 148.68748474121094
Loss :  1.670538067817688 2.765429973602295 139.94203186035156
Loss :  1.6819648742675781 2.9219465255737305 147.77928161621094
Loss :  1.7023556232452393 3.8846724033355713 195.9359893798828
Loss :  1.673278570175171 2.7131266593933105 137.32960510253906
Loss :  1.6801393032073975 3.1408610343933105 158.7231903076172
Loss :  1.6768890619277954 3.4766929149627686 175.51153564453125
Loss :  1.6401293277740479 3.2782557010650635 165.55291748046875
Loss :  1.700486421585083 3.316979169845581 167.5494384765625
Loss :  1.6403945684432983 3.327507257461548 168.01576232910156
Loss :  1.6887764930725098 2.8949992656707764 146.43875122070312
Loss :  1.6713560819625854 2.7524876594543457 139.29574584960938
Loss :  1.6709165573120117 2.9136993885040283 147.3558807373047
Loss :  1.6461251974105835 3.861056327819824 194.6989288330078
Loss :  1.6569958925247192 3.0355045795440674 153.43222045898438
Loss :  1.6568139791488647 3.7625720500946045 189.78541564941406
Loss :  1.698867917060852 3.7904152870178223 191.21963500976562
Loss :  1.7019668817520142 2.9150731563568115 147.45562744140625
Loss :  1.7086453437805176 2.9567711353302 149.54721069335938
  batch 40 loss: 1.7086453437805176, 2.9567711353302, 149.54721069335938
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6801717281341553 2.9883315563201904 151.0967559814453
Loss :  1.6701743602752686 2.7824604511260986 140.79319763183594
Loss :  1.6628937721252441 2.954904794692993 149.4081268310547
Loss :  1.6722956895828247 3.094360113143921 156.3903045654297
Loss :  1.6579304933547974 3.280040740966797 165.65997314453125
Loss :  1.6784794330596924 3.3874127864837646 171.0491180419922
Loss :  1.701848030090332 3.6910507678985596 186.25437927246094
Loss :  1.6679025888442993 2.7963600158691406 141.48590087890625
Loss :  1.711733102798462 2.7511508464813232 139.26927185058594
Loss :  1.6704823970794678 2.8884494304656982 146.09295654296875
Loss :  1.69489324092865 3.0757970809936523 155.48475646972656
Loss :  1.6896562576293945 2.9640705585479736 149.89317321777344
Loss :  1.6748958826065063 3.0841219425201416 155.88099670410156
Loss :  1.695398211479187 2.8468081951141357 144.0358123779297
Loss :  1.667772650718689 3.2466511726379395 164.00033569335938
Loss :  1.7114323377609253 3.05812406539917 154.6176300048828
Loss :  1.6729061603546143 2.8904387950897217 146.19485473632812
Loss :  1.6615359783172607 3.2960891723632812 166.4659881591797
Loss :  1.672127366065979 3.1315155029296875 158.24790954589844
Loss :  1.7172815799713135 3.753741502761841 189.40435791015625
  batch 60 loss: 1.7172815799713135, 3.753741502761841, 189.40435791015625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6627622842788696 3.8755223751068115 195.4388885498047
Loss :  1.68359375 3.8623099327087402 194.79908752441406
Loss :  1.6691126823425293 2.7652931213378906 139.9337615966797
Loss :  1.6617366075515747 2.888275146484375 146.07550048828125
Loss :  1.6477361917495728 2.5534543991088867 129.32046508789062
Loss :  1.671614646911621 4.378359794616699 220.589599609375
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6806470155715942 4.352093696594238 219.28533935546875
Loss :  1.6773160696029663 4.259579181671143 214.65628051757812
Loss :  1.6828234195709229 4.119722366333008 207.6689453125
Total LOSS train 158.04097947340745 valid 215.55004119873047
CE LOSS train 1.6783125969079824 valid 0.4207058548927307
Contrastive LOSS train 3.127253327002892 valid 1.029930591583252
EPOCH 163:
Loss :  1.6928220987319946 2.8175365924835205 142.5696563720703
Loss :  1.7020585536956787 2.878854274749756 145.644775390625
Loss :  1.6812357902526855 2.8433849811553955 143.85047912597656
Loss :  1.6866002082824707 4.076102256774902 205.49171447753906
Loss :  1.6977105140686035 3.4613876342773438 174.76708984375
Loss :  1.6736273765563965 3.6140103340148926 182.3741455078125
Loss :  1.6983717679977417 3.510429620742798 177.2198486328125
Loss :  1.6814595460891724 2.7581446170806885 139.58868408203125
Loss :  1.6757330894470215 2.7194104194641113 137.64625549316406
Loss :  1.6986794471740723 2.8898963928222656 146.19349670410156
Loss :  1.6680407524108887 3.114264965057373 157.38128662109375
Loss :  1.6678245067596436 3.052199125289917 154.2777862548828
Loss :  1.6660466194152832 2.9322423934936523 148.27816772460938
Loss :  1.6708638668060303 2.6962270736694336 136.4822235107422
Loss :  1.707797646522522 2.968247175216675 150.12014770507812
Loss :  1.7047226428985596 3.1566622257232666 159.537841796875
Loss :  1.664250135421753 2.87904691696167 145.61659240722656
Loss :  1.6823956966400146 2.629563093185425 133.16053771972656
Loss :  1.6622339487075806 2.7406740188598633 138.69593811035156
Loss :  1.704637885093689 3.2552731037139893 164.46829223632812
  batch 20 loss: 1.704637885093689, 3.2552731037139893, 164.46829223632812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6796897649765015 2.6916842460632324 136.26390075683594
Loss :  1.6623597145080566 3.0151138305664062 152.4180450439453
Loss :  1.6746494770050049 3.1646034717559814 159.9048309326172
Loss :  1.6855462789535522 3.3701982498168945 170.19546508789062
Loss :  1.7045944929122925 3.7297399044036865 188.19158935546875
Loss :  1.6757524013519287 2.8917646408081055 146.26397705078125
Loss :  1.68201744556427 3.36003041267395 169.6835479736328
Loss :  1.6788432598114014 2.909877300262451 147.17271423339844
Loss :  1.6427918672561646 2.648097038269043 134.04763793945312
Loss :  1.7020258903503418 3.1993184089660645 161.66795349121094
Loss :  1.6435307264328003 4.065674304962158 204.92724609375
Loss :  1.6906511783599854 3.0723321437835693 155.30726623535156
Loss :  1.6732053756713867 2.8275036811828613 143.0483856201172
Loss :  1.672154188156128 2.8375027179718018 143.5472869873047
Loss :  1.6478286981582642 2.966696262359619 149.98263549804688
Loss :  1.6582714319229126 2.95619535446167 149.46803283691406
Loss :  1.6579279899597168 4.193093776702881 211.3126220703125
Loss :  1.6994088888168335 3.264963388442993 164.94757080078125
Loss :  1.7019981145858765 3.0896596908569336 156.1849822998047
Loss :  1.70931077003479 3.0794260501861572 155.68060302734375
  batch 40 loss: 1.70931077003479, 3.0794260501861572, 155.68060302734375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6818928718566895 3.0236318111419678 152.86349487304688
Loss :  1.672149658203125 2.9281301498413086 148.0786590576172
Loss :  1.6649922132492065 2.9348158836364746 148.40579223632812
Loss :  1.6738545894622803 2.7635867595672607 139.8531951904297
Loss :  1.6586060523986816 3.2110674381256104 162.21197509765625
Loss :  1.6790536642074585 2.750457525253296 139.20191955566406
Loss :  1.7018791437149048 2.8851442337036133 145.95909118652344
Loss :  1.668338418006897 2.7690227031707764 140.11947631835938
Loss :  1.7118809223175049 2.925675868988037 147.9956817626953
Loss :  1.6720219850540161 2.777818441390991 140.5629425048828
Loss :  1.6952240467071533 2.97200870513916 150.295654296875
Loss :  1.6907423734664917 3.1670217514038086 160.0418243408203
Loss :  1.6766984462738037 2.767756700515747 140.0645294189453
Loss :  1.696042776107788 2.8980133533477783 146.59671020507812
Loss :  1.6686846017837524 3.226290225982666 162.9832000732422
Loss :  1.7117514610290527 2.767673969268799 140.09544372558594
Loss :  1.6735423803329468 3.0257833003997803 152.96270751953125
Loss :  1.662367582321167 2.9079673290252686 147.06072998046875
Loss :  1.6726003885269165 3.4401631355285645 173.68077087402344
Loss :  1.717275619506836 2.959780693054199 149.706298828125
  batch 60 loss: 1.717275619506836, 2.959780693054199, 149.706298828125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.663395643234253 2.6904428005218506 136.18553161621094
Loss :  1.6842468976974487 3.1538937091827393 159.37893676757812
Loss :  1.6704652309417725 2.7526469230651855 139.3028106689453
Loss :  1.6628844738006592 3.240229845046997 163.67437744140625
Loss :  1.648819088935852 2.4758026599884033 125.43894958496094
Loss :  1.6713433265686035 4.365687847137451 219.9557342529297
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6798806190490723 4.4022016525268555 221.7899627685547
Loss :  1.6785544157028198 4.396351337432861 221.49612426757812
Loss :  1.68208909034729 4.341382026672363 218.75119018554688
Total LOSS train 153.7892608642578 valid 220.49825286865234
CE LOSS train 1.6801397011830257 valid 0.4205222725868225
Contrastive LOSS train 3.0421824308542105 valid 1.0853455066680908
EPOCH 164:
Loss :  1.6931426525115967 2.754159688949585 139.401123046875
Loss :  1.7022883892059326 2.94769549369812 149.08706665039062
Loss :  1.6814669370651245 2.646077871322632 133.98536682128906
Loss :  1.6868170499801636 3.0880885124206543 156.0912322998047
Loss :  1.6977667808532715 2.6766531467437744 135.53042602539062
Loss :  1.6738860607147217 2.760287046432495 139.68824768066406
Loss :  1.6984682083129883 2.9985368251800537 151.62530517578125
Loss :  1.6819162368774414 3.102360248565674 156.7999267578125
Loss :  1.6768410205841064 3.1451215744018555 158.9329071044922
Loss :  1.699579119682312 3.1205599308013916 157.7275848388672
Loss :  1.669958233833313 2.9153904914855957 147.43948364257812
Loss :  1.6705944538116455 2.802217721939087 141.78147888183594
Loss :  1.6688517332077026 3.4730193614959717 175.31982421875
Loss :  1.6735621690750122 3.2118747234344482 162.26730346679688
Loss :  1.709527850151062 3.4353010654449463 173.47459411621094
Loss :  1.7062592506408691 3.080796957015991 155.74609375
Loss :  1.6676874160766602 3.009132146835327 152.12429809570312
Loss :  1.6853468418121338 2.7432432174682617 138.84750366210938
Loss :  1.66575026512146 2.705256223678589 136.92857360839844
Loss :  1.7059074640274048 3.382943630218506 170.85308837890625
  batch 20 loss: 1.7059074640274048, 3.382943630218506, 170.85308837890625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6813008785247803 2.8526556491851807 144.3140869140625
Loss :  1.663844347000122 3.423413038253784 172.83450317382812
Loss :  1.6755328178405762 2.850172281265259 144.1841583251953
Loss :  1.685600757598877 2.6965689659118652 136.51405334472656
Loss :  1.7045320272445679 3.005589485168457 151.9840087890625
Loss :  1.6766974925994873 2.9255573749542236 147.95455932617188
Loss :  1.682822823524475 3.5403361320495605 178.69961547851562
Loss :  1.680579662322998 2.8509373664855957 144.22744750976562
Loss :  1.64532470703125 3.3520164489746094 169.24615478515625
Loss :  1.703322172164917 2.9564499855041504 149.52581787109375
Loss :  1.6468098163604736 2.8527400493621826 144.2838134765625
Loss :  1.6929054260253906 2.8008840084075928 141.7371063232422
Loss :  1.6761820316314697 3.145698070526123 158.96107482910156
Loss :  1.675540566444397 3.7262609004974365 187.98858642578125
Loss :  1.6522390842437744 2.952587842941284 149.28163146972656
Loss :  1.662556767463684 2.9429514408111572 148.81011962890625
Loss :  1.662611722946167 2.888345241546631 146.0798797607422
Loss :  1.7030038833618164 2.755488157272339 139.4774169921875
Loss :  1.7057104110717773 2.7001466751098633 136.71304321289062
Loss :  1.7125715017318726 2.9348766803741455 148.45640563964844
  batch 40 loss: 1.7125715017318726, 2.9348766803741455, 148.45640563964844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6865519285202026 3.7239158153533936 187.88233947753906
Loss :  1.677233338356018 2.7201180458068848 137.68313598632812
Loss :  1.6704877614974976 2.7365939617156982 138.50018310546875
Loss :  1.6786001920700073 2.8235487937927246 142.85604858398438
Loss :  1.6650490760803223 3.4592161178588867 174.6258544921875
Loss :  1.6855281591415405 2.910494327545166 147.2102508544922
Loss :  1.7070029973983765 2.720111846923828 137.71258544921875
Loss :  1.675658941268921 3.9405953884124756 198.70542907714844
Loss :  1.7168623208999634 2.912484884262085 147.34109497070312
Loss :  1.6794087886810303 2.6749067306518555 135.42474365234375
Loss :  1.7010055780410767 2.9221692085266113 147.80946350097656
Loss :  1.6970648765563965 3.2841477394104004 165.90444946289062
Loss :  1.6834479570388794 3.4360854625701904 173.4877166748047
Loss :  1.702305793762207 3.177941083908081 160.59934997558594
Loss :  1.6755565404891968 3.354790687561035 169.4150848388672
Loss :  1.7164864540100098 3.14103364944458 158.76817321777344
Loss :  1.6789603233337402 3.127100944519043 158.03399658203125
Loss :  1.6676857471466064 2.8098690509796143 142.1611328125
Loss :  1.67689049243927 3.1899759769439697 161.17568969726562
Loss :  1.7199510335922241 3.131884813308716 158.31419372558594
  batch 60 loss: 1.7199510335922241, 3.131884813308716, 158.31419372558594
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.666702151298523 3.6743435859680176 185.38388061523438
Loss :  1.6860953569412231 2.887632131576538 146.0677032470703
Loss :  1.6721279621124268 2.7263858318328857 137.99142456054688
Loss :  1.664581298828125 2.978808879852295 150.6050262451172
Loss :  1.6505627632141113 2.842437744140625 143.77244567871094
Loss :  1.6750212907791138 4.298424243927002 216.5962371826172
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.683813452720642 4.2870635986328125 216.0369873046875
Loss :  1.682021975517273 4.1967034339904785 211.51719665527344
Loss :  1.6857025623321533 4.1092963218688965 207.1505126953125
Total LOSS train 153.5747124305138 valid 212.82523345947266
CE LOSS train 1.6831863825137798 valid 0.42142564058303833
Contrastive LOSS train 3.037830528846154 valid 1.0273240804672241
EPOCH 165:
Loss :  1.6944409608840942 2.818842887878418 142.63658142089844
Loss :  1.7032638788223267 3.0889699459075928 156.15176391601562
Loss :  1.6832305192947388 2.782627582550049 140.81460571289062
Loss :  1.6889808177947998 2.8273637294769287 143.05715942382812
Loss :  1.6994147300720215 2.689671516418457 136.18299865722656
Loss :  1.6768006086349487 3.112051486968994 157.2793731689453
Loss :  1.6995269060134888 2.8306732177734375 143.23318481445312
Loss :  1.6834824085235596 2.752967119216919 139.33184814453125
Loss :  1.6777456998825073 2.815335988998413 142.44454956054688
Loss :  1.70079505443573 2.763118028640747 139.856689453125
Loss :  1.6716824769973755 3.342768430709839 168.8101043701172
Loss :  1.6713460683822632 2.9153337478637695 147.4380340576172
Loss :  1.6699330806732178 2.635283946990967 133.4341278076172
Loss :  1.6742910146713257 2.8395259380340576 143.6505889892578
Loss :  1.7101609706878662 2.9121034145355225 147.31532287597656
Loss :  1.7071343660354614 2.9122910499572754 147.32168579101562
Loss :  1.6681076288223267 2.83235502243042 143.28585815429688
Loss :  1.6860960721969604 3.0844016075134277 155.9061737060547
Loss :  1.666327953338623 2.7421791553497314 138.77528381347656
Loss :  1.7067456245422363 2.9661293029785156 150.01319885253906
  batch 20 loss: 1.7067456245422363, 2.9661293029785156, 150.01319885253906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6833326816558838 2.7149136066436768 137.42901611328125
Loss :  1.6655794382095337 2.799569845199585 141.64405822753906
Loss :  1.6768934726715088 2.840728521347046 143.71331787109375
Loss :  1.6875998973846436 3.41561222076416 172.4682159423828
Loss :  1.7062458992004395 3.098795175552368 156.64601135253906
Loss :  1.67789888381958 2.9155566692352295 147.4557342529297
Loss :  1.6843491792678833 3.0755045413970947 155.45957946777344
Loss :  1.681512475013733 2.785508155822754 140.95692443847656
Loss :  1.6474039554595947 2.903795003890991 146.83714294433594
Loss :  1.7047390937805176 3.5498459339141846 179.19703674316406
Loss :  1.6481280326843262 2.8768341541290283 145.48983764648438
Loss :  1.6935893297195435 3.0842971801757812 155.908447265625
Loss :  1.677758812904358 2.835292100906372 143.44236755371094
Loss :  1.6769791841506958 3.0632550716400146 154.83973693847656
Loss :  1.6533821821212769 2.941540479660034 148.73040771484375
Loss :  1.6639890670776367 3.3539044857025146 169.3592071533203
Loss :  1.6634715795516968 3.455843925476074 174.45565795898438
Loss :  1.7029316425323486 2.9491026401519775 149.15806579589844
Loss :  1.7063400745391846 2.9920647144317627 151.30958557128906
Loss :  1.7123008966445923 2.898052453994751 146.6149139404297
  batch 40 loss: 1.7123008966445923, 2.898052453994751, 146.6149139404297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6857316493988037 2.79986310005188 141.6788787841797
Loss :  1.6760660409927368 2.6595609188079834 134.65411376953125
Loss :  1.669020652770996 3.192380428314209 161.2880401611328
Loss :  1.6773169040679932 3.0618159770965576 154.76812744140625
Loss :  1.6635693311691284 3.064373016357422 154.88223266601562
Loss :  1.6841952800750732 2.878619432449341 145.6151580810547
Loss :  1.7065390348434448 3.008901834487915 152.15164184570312
Loss :  1.6748205423355103 3.0559229850769043 154.47096252441406
Loss :  1.7161972522735596 2.8431458473205566 143.8734893798828
Loss :  1.6783753633499146 2.6613574028015137 134.74624633789062
Loss :  1.7003858089447021 2.8342807292938232 143.41441345214844
Loss :  1.696373462677002 2.983677864074707 150.88027954101562
Loss :  1.6830039024353027 2.9610202312469482 149.7340087890625
Loss :  1.7016867399215698 2.9820587635040283 150.80462646484375
Loss :  1.6758886575698853 3.1940038204193115 161.37608337402344
Loss :  1.716546893119812 3.588719367980957 181.15252685546875
Loss :  1.6804745197296143 2.9857051372528076 150.9657440185547
Loss :  1.6699867248535156 2.7071378231048584 137.02687072753906
Loss :  1.6792007684707642 2.9855566024780273 150.95703125
Loss :  1.7210668325424194 3.168095827102661 160.1258544921875
  batch 60 loss: 1.7210668325424194, 3.168095827102661, 160.1258544921875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6690006256103516 2.762545108795166 139.7962646484375
Loss :  1.6878340244293213 4.022165298461914 202.7960968017578
Loss :  1.674169659614563 2.853105306625366 144.3294219970703
Loss :  1.6673259735107422 3.15484619140625 159.40963745117188
Loss :  1.6537021398544312 2.4890692234039307 126.10716247558594
Loss :  1.6765613555908203 4.303030014038086 216.82806396484375
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6853952407836914 4.332016944885254 218.28623962402344
Loss :  1.6833500862121582 4.230619430541992 213.21432495117188
Loss :  1.6871858835220337 4.197221755981445 211.5482635498047
Total LOSS train 150.23168170635518 valid 214.96922302246094
CE LOSS train 1.6840370985177846 valid 0.4217964708805084
Contrastive LOSS train 2.970952895971445 valid 1.0493054389953613
EPOCH 166:
Loss :  1.6965205669403076 2.7727346420288086 140.333251953125
Loss :  1.705072045326233 2.9074931144714355 147.07972717285156
Loss :  1.685647964477539 3.044088125228882 153.8900604248047
Loss :  1.6904233694076538 2.873358964920044 145.35838317871094
Loss :  1.7008678913116455 4.383450031280518 220.8733673095703
Loss :  1.6776036024093628 2.6817336082458496 135.7642822265625
Loss :  1.7006187438964844 2.8703832626342773 145.21978759765625
Loss :  1.684177041053772 2.7390828132629395 138.63832092285156
Loss :  1.6784075498580933 3.026994466781616 153.0281219482422
Loss :  1.7005282640457153 3.008488893508911 152.12496948242188
Loss :  1.6704423427581787 3.8286938667297363 193.10513305664062
Loss :  1.6695506572723389 3.427438259124756 173.04147338867188
Loss :  1.6673955917358398 2.8624894618988037 144.79185485839844
Loss :  1.6714786291122437 3.145073413848877 158.92515563964844
Loss :  1.7079968452453613 2.9101083278656006 147.21340942382812
Loss :  1.7047381401062012 2.8897902965545654 146.1942596435547
Loss :  1.6643637418746948 2.8079075813293457 142.05975341796875
Loss :  1.6825625896453857 3.876124382019043 195.48876953125
Loss :  1.662350058555603 2.669506788253784 135.1376953125
Loss :  1.7046769857406616 3.284759521484375 165.94265747070312
  batch 20 loss: 1.7046769857406616, 3.284759521484375, 165.94265747070312
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6800235509872437 2.7506234645843506 139.21119689941406
Loss :  1.6625041961669922 3.1197030544281006 157.6476593017578
Loss :  1.6748344898223877 3.0876011848449707 156.05490112304688
Loss :  1.6864556074142456 3.3232924938201904 167.85108947753906
Loss :  1.7059881687164307 3.334214448928833 168.41671752929688
Loss :  1.6778103113174438 2.8507144451141357 144.21353149414062
Loss :  1.68475341796875 2.986712694168091 151.0203857421875
Loss :  1.681764841079712 2.7737693786621094 140.3702392578125
Loss :  1.6478954553604126 3.290729284286499 166.18435668945312
Loss :  1.704724669456482 3.6290361881256104 183.1565399169922
Loss :  1.6484979391098022 3.1316311359405518 158.2300567626953
Loss :  1.6939697265625 3.121978521347046 157.7928924560547
Loss :  1.6779855489730835 2.9484283924102783 149.09939575195312
Loss :  1.6776883602142334 2.774235963821411 140.3894805908203
Loss :  1.6542422771453857 3.2009990215301514 161.70419311523438
Loss :  1.6646764278411865 3.0915560722351074 156.2424774169922
Loss :  1.6643950939178467 2.873957395553589 145.36227416992188
Loss :  1.70366370677948 2.7154877185821533 137.47804260253906
Loss :  1.7076475620269775 2.886075258255005 146.0113983154297
Loss :  1.7141594886779785 2.7808525562286377 140.7567901611328
  batch 40 loss: 1.7141594886779785, 2.7808525562286377, 140.7567901611328
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6884487867355347 3.1255316734313965 157.96502685546875
Loss :  1.6796140670776367 3.105069875717163 156.93310546875
Loss :  1.6729891300201416 2.741441011428833 138.7450408935547
Loss :  1.6813355684280396 2.6929333209991455 136.3280029296875
Loss :  1.6684856414794922 2.6767196655273438 135.5044708251953
Loss :  1.6871435642242432 2.817852258682251 142.5797576904297
Loss :  1.7085037231445312 2.7223258018493652 137.82479858398438
Loss :  1.6772202253341675 2.7034687995910645 136.8506622314453
Loss :  1.7179844379425049 3.2552733421325684 164.48165893554688
Loss :  1.680284857749939 3.425786018371582 172.96958923339844
Loss :  1.7025296688079834 3.6890740394592285 186.15623474121094
Loss :  1.6980974674224854 2.9822633266448975 150.81126403808594
Loss :  1.684628963470459 2.725292444229126 137.94924926757812
Loss :  1.7029249668121338 2.800832986831665 141.7445831298828
Loss :  1.6772147417068481 2.7659552097320557 139.9749755859375
Loss :  1.7177540063858032 3.942546844482422 198.84510803222656
Loss :  1.6820932626724243 3.4967305660247803 176.5186309814453
Loss :  1.6716265678405762 3.3203654289245605 167.6898956298828
Loss :  1.6803842782974243 3.1558072566986084 159.4707489013672
Loss :  1.721345067024231 3.4817616939544678 175.80943298339844
  batch 60 loss: 1.721345067024231, 3.4817616939544678, 175.80943298339844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6694164276123047 3.170727252960205 160.20578002929688
Loss :  1.6882414817810059 3.5101442337036133 177.19546508789062
Loss :  1.6744922399520874 2.8869237899780273 146.02069091796875
Loss :  1.6668813228607178 3.112224817276001 157.2781219482422
Loss :  1.6528515815734863 2.376366138458252 120.47116088867188
Loss :  1.671674370765686 4.120809078216553 207.71212768554688
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6802083253860474 4.0862836837768555 205.994384765625
Loss :  1.6780699491500854 3.9909491539001465 201.22552490234375
Loss :  1.6822410821914673 3.8117849826812744 192.271484375
Total LOSS train 155.04196166992188 valid 201.8008804321289
CE LOSS train 1.6844860847179706 valid 0.4205602705478668
Contrastive LOSS train 3.0671494814065787 valid 0.9529462456703186
EPOCH 167:
Loss :  1.6950533390045166 3.274991273880005 165.44461059570312
Loss :  1.7039047479629517 2.9580979347229004 149.60879516601562
Loss :  1.6832810640335083 2.909811496734619 147.17385864257812
Loss :  1.687785267829895 3.1013107299804688 156.75332641601562
Loss :  1.6985334157943726 3.0376064777374268 153.578857421875
Loss :  1.6741549968719482 3.2219948768615723 162.77389526367188
Loss :  1.6987775564193726 3.4027602672576904 171.8367919921875
Loss :  1.6822322607040405 3.2193455696105957 162.64952087402344
Loss :  1.6766573190689087 2.855862855911255 144.46978759765625
Loss :  1.6998229026794434 2.7469210624694824 139.04588317871094
Loss :  1.6694368124008179 2.9578659534454346 149.56272888183594
Loss :  1.6691536903381348 2.9148778915405273 147.41305541992188
Loss :  1.6676967144012451 3.0178658962249756 152.5609893798828
Loss :  1.6725537776947021 3.2974066734313965 166.54287719726562
Loss :  1.7097046375274658 3.0667991638183594 155.04966735839844
Loss :  1.7060171365737915 3.136648654937744 158.5384521484375
Loss :  1.667655110359192 2.8843636512756348 145.88583374023438
Loss :  1.6853264570236206 3.2592124938964844 164.64596557617188
Loss :  1.6666772365570068 3.3881301879882812 171.07318115234375
Loss :  1.7079087495803833 3.244917869567871 163.9538116455078
  batch 20 loss: 1.7079087495803833, 3.244917869567871, 163.9538116455078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6837092638015747 3.734870672225952 188.42724609375
Loss :  1.6669608354568481 2.931516408920288 148.24278259277344
Loss :  1.6789262294769287 2.8454954624176025 143.95370483398438
Loss :  1.689562439918518 2.8598592281341553 144.68252563476562
Loss :  1.7084108591079712 3.8794705867767334 195.6819305419922
Loss :  1.6809382438659668 3.4819607734680176 175.7789764404297
Loss :  1.6870455741882324 3.245866298675537 163.98036193847656
Loss :  1.6840217113494873 3.4383761882781982 173.6028289794922
Loss :  1.6493339538574219 3.9641103744506836 199.8548583984375
Loss :  1.7058910131454468 2.8739640712738037 145.40408325195312
Loss :  1.649096965789795 3.4502978324890137 174.1639862060547
Loss :  1.6945750713348389 3.421302080154419 172.7596893310547
Loss :  1.6777507066726685 3.0920522212982178 156.28036499023438
Loss :  1.6769884824752808 2.929962635040283 148.1751251220703
Loss :  1.6526799201965332 3.1029248237609863 156.79891967773438
Loss :  1.6628191471099854 3.285064697265625 165.9160614013672
Loss :  1.6625258922576904 3.0628724098205566 154.80613708496094
Loss :  1.7026137113571167 2.8925416469573975 146.32968139648438
Loss :  1.706050992012024 3.133434772491455 158.37779235839844
Loss :  1.712058663368225 2.9854660034179688 150.9853515625
  batch 40 loss: 1.712058663368225, 2.9854660034179688, 150.9853515625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6853972673416138 2.8673362731933594 145.05221557617188
Loss :  1.6757023334503174 3.2848150730133057 165.9164581298828
Loss :  1.6683001518249512 2.855604410171509 144.4485321044922
Loss :  1.6767992973327637 2.890453338623047 146.19947814941406
Loss :  1.662585735321045 2.7564709186553955 139.4861297607422
Loss :  1.6829780340194702 2.858628273010254 144.61439514160156
Loss :  1.7057206630706787 2.915274143218994 147.46942138671875
Loss :  1.6730421781539917 3.080211639404297 155.68362426757812
Loss :  1.7155157327651978 2.755682945251465 139.49966430664062
Loss :  1.677345871925354 2.7060396671295166 136.97933959960938
Loss :  1.6999748945236206 3.9780519008636475 200.60256958007812
Loss :  1.69598388671875 2.973724842071533 150.38223266601562
Loss :  1.6818138360977173 3.181065559387207 160.73509216308594
Loss :  1.7013522386550903 3.7233119010925293 187.866943359375
Loss :  1.6748155355453491 2.9913341999053955 151.2415313720703
Loss :  1.7166845798492432 2.885984182357788 146.01589965820312
Loss :  1.6798182725906372 3.1697278022766113 160.1662139892578
Loss :  1.6692707538604736 3.061215877532959 154.7300567626953
Loss :  1.6788151264190674 3.831681966781616 193.26290893554688
Loss :  1.7210354804992676 2.6892518997192383 136.1836395263672
  batch 60 loss: 1.7210354804992676, 2.6892518997192383, 136.1836395263672
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.669018268585205 2.870551586151123 145.19659423828125
Loss :  1.688860297203064 3.4158010482788086 172.47891235351562
Loss :  1.6752463579177856 3.379612922668457 170.65589904785156
Loss :  1.6683295965194702 3.6709189414978027 185.2142791748047
Loss :  1.6549296379089355 2.405555486679077 121.93270111083984
Loss :  1.6821504831314087 4.266415119171143 215.00289916992188
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6904534101486206 4.223394870758057 212.86019897460938
Loss :  1.6878694295883179 4.148282051086426 209.1019744873047
Loss :  1.6927543878555298 4.08466911315918 205.92620849609375
Total LOSS train 158.38121584378757 valid 210.72282028198242
CE LOSS train 1.6839019830410298 valid 0.42318859696388245
Contrastive LOSS train 3.133946261039147 valid 1.021167278289795
EPOCH 168:
Loss :  1.698390007019043 2.9584145545959473 149.61912536621094
Loss :  1.7069675922393799 3.0869104862213135 156.052490234375
Loss :  1.6864157915115356 2.9208903312683105 147.73092651367188
Loss :  1.6910649538040161 3.4879040718078613 176.08627319335938
Loss :  1.7014473676681519 2.660230875015259 134.71299743652344
Loss :  1.6778674125671387 3.8779609203338623 195.57591247558594
Loss :  1.7013568878173828 3.0883078575134277 156.1167449951172
Loss :  1.6849499940872192 2.8473219871520996 144.05105590820312
Loss :  1.679526448249817 2.667076349258423 135.03334045410156
Loss :  1.7024357318878174 2.6422667503356934 133.81578063964844
Loss :  1.672376275062561 3.2640490531921387 164.8748321533203
Loss :  1.6716530323028564 3.1836984157562256 160.8565673828125
Loss :  1.6697572469711304 3.1579601764678955 159.56776428222656
Loss :  1.6736632585525513 3.1279194355010986 158.0696258544922
Loss :  1.7099577188491821 3.1417927742004395 158.7996063232422
Loss :  1.7065051794052124 3.4364774227142334 173.53038024902344
Loss :  1.6670145988464355 3.1171159744262695 157.52281188964844
Loss :  1.6848176717758179 3.6561572551727295 184.49267578125
Loss :  1.665313720703125 2.9184463024139404 147.58763122558594
Loss :  1.7070008516311646 3.40425181388855 171.91958618164062
  batch 20 loss: 1.7070008516311646, 3.40425181388855, 171.91958618164062
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.682580590248108 4.144741535186768 208.91966247558594
Loss :  1.6653172969818115 2.846402883529663 143.98545837402344
Loss :  1.676782250404358 2.943416118621826 148.84759521484375
Loss :  1.6882140636444092 2.84033465385437 143.7049560546875
Loss :  1.7072428464889526 3.6108992099761963 182.25221252441406
Loss :  1.679343819618225 3.820357084274292 192.6971893310547
Loss :  1.6853033304214478 3.7134909629821777 187.35984802246094
Loss :  1.6825052499771118 3.4486594200134277 174.115478515625
Loss :  1.647011160850525 3.0841994285583496 155.85699462890625
Loss :  1.7044658660888672 3.5840330123901367 180.90611267089844
Loss :  1.6468942165374756 3.3048934936523438 166.89157104492188
Loss :  1.6932475566864014 3.4100282192230225 172.1946563720703
Loss :  1.6767724752426147 2.861985445022583 144.7760467529297
Loss :  1.6764159202575684 3.207547426223755 162.0537872314453
Loss :  1.6520929336547852 3.0815987586975098 155.73204040527344
Loss :  1.6628532409667969 3.552353858947754 179.28054809570312
Loss :  1.663060188293457 3.1204147338867188 157.6837921142578
Loss :  1.7031641006469727 2.9929282665252686 151.34957885742188
Loss :  1.7060822248458862 3.069443702697754 155.1782684326172
Loss :  1.712336778640747 2.6993398666381836 136.67933654785156
  batch 40 loss: 1.712336778640747, 2.6993398666381836, 136.67933654785156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6855969429016113 3.761162042617798 189.7436981201172
Loss :  1.6761232614517212 3.116814613342285 157.516845703125
Loss :  1.6691228151321411 2.768690347671509 140.10365295410156
Loss :  1.677573561668396 2.775627374649048 140.4589385986328
Loss :  1.664077877998352 2.923917055130005 147.85992431640625
Loss :  1.6839526891708374 3.3627922534942627 169.82357788085938
Loss :  1.7060118913650513 3.4285387992858887 173.13294982910156
Loss :  1.6742980480194092 2.830225706100464 143.1855926513672
Loss :  1.7162729501724243 2.636910915374756 133.56182861328125
Loss :  1.6787340641021729 2.702227830886841 136.79013061523438
Loss :  1.7006044387817383 3.4698801040649414 175.19461059570312
Loss :  1.6971232891082764 3.548144817352295 179.1043701171875
Loss :  1.6829537153244019 3.134632110595703 158.41455078125
Loss :  1.7016816139221191 3.0690982341766357 155.15658569335938
Loss :  1.6755237579345703 2.906022548675537 146.97665405273438
Loss :  1.7169164419174194 2.639007568359375 133.66729736328125
Loss :  1.680073618888855 3.132106065750122 158.28536987304688
Loss :  1.6692150831222534 2.991905927658081 151.26451110839844
Loss :  1.6789805889129639 3.248358964920044 164.09693908691406
Loss :  1.7210882902145386 3.379770517349243 170.70960998535156
  batch 60 loss: 1.7210882902145386, 3.379770517349243, 170.70960998535156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6695959568023682 3.0626883506774902 154.80401611328125
Loss :  1.6891652345657349 3.1827027797698975 160.8242950439453
Loss :  1.6759440898895264 2.680915594100952 135.7217254638672
Loss :  1.6680171489715576 3.061873197555542 154.7616729736328
Loss :  1.654617190361023 2.6541264057159424 134.36093139648438
Loss :  1.6761590242385864 4.3886003494262695 221.10618591308594
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.685179352760315 4.342767715454102 218.8235626220703
Loss :  1.684050440788269 4.214163780212402 212.39224243164062
Loss :  1.6868139505386353 4.450924396514893 224.2330322265625
Total LOSS train 158.95386986365685 valid 219.13875579833984
CE LOSS train 1.6843604986484235 valid 0.4217034876346588
Contrastive LOSS train 3.145390169437115 valid 1.1127310991287231
EPOCH 169:
Loss :  1.6973645687103271 3.453153133392334 174.35501098632812
Loss :  1.705700397491455 3.1511406898498535 159.2627410888672
Loss :  1.6865156888961792 2.696882963180542 136.53065490722656
Loss :  1.6915204524993896 3.7351467609405518 188.4488525390625
Loss :  1.7021710872650146 2.3772003650665283 120.56218719482422
Loss :  1.6783820390701294 3.0691978931427 155.13827514648438
Loss :  1.7022461891174316 3.008308172225952 152.11764526367188
Loss :  1.686010718345642 3.412590265274048 172.31552124023438
Loss :  1.6802558898925781 3.8889434337615967 196.12742614746094
Loss :  1.7026293277740479 3.210334539413452 162.2193603515625
Loss :  1.673607349395752 3.4387619495391846 173.61170959472656
Loss :  1.6727020740509033 3.3623526096343994 169.7903289794922
Loss :  1.6711535453796387 3.903262138366699 196.83425903320312
Loss :  1.6751279830932617 3.5625803470611572 179.80413818359375
Loss :  1.7106965780258179 3.0412659645080566 153.77398681640625
Loss :  1.7082598209381104 3.1379029750823975 158.60340881347656
Loss :  1.6687103509902954 2.9881675243377686 151.07708740234375
Loss :  1.6861628293991089 3.2752110958099365 165.44671630859375
Loss :  1.666082739830017 3.9339330196380615 198.36273193359375
Loss :  1.707509994506836 3.107522487640381 157.08363342285156
  batch 20 loss: 1.707509994506836, 3.107522487640381, 157.08363342285156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6828733682632446 2.7478978633880615 139.07777404785156
Loss :  1.6647123098373413 3.0162746906280518 152.47845458984375
Loss :  1.6765990257263184 2.892132043838501 146.283203125
Loss :  1.6878989934921265 2.8153436183929443 142.455078125
Loss :  1.7067699432373047 3.058330535888672 154.62330627441406
Loss :  1.6782305240631104 3.473747968673706 175.36563110351562
Loss :  1.6847792863845825 3.179933786392212 160.6814727783203
Loss :  1.6815980672836304 2.885737657546997 145.96847534179688
Loss :  1.6471741199493408 3.2874271869659424 166.01852416992188
Loss :  1.7046434879302979 2.893005847930908 146.35494995117188
Loss :  1.6476457118988037 2.9835617542266846 150.8257293701172
Loss :  1.6936265230178833 4.018157005310059 202.6014862060547
Loss :  1.6780359745025635 3.0417895317077637 153.76751708984375
Loss :  1.677111268043518 2.9615960121154785 149.7569122314453
Loss :  1.653131365776062 2.9644594192504883 149.87611389160156
Loss :  1.6632381677627563 3.009194850921631 152.12298583984375
Loss :  1.6631159782409668 3.043663501739502 153.84629821777344
Loss :  1.7032018899917603 2.7724595069885254 140.326171875
Loss :  1.707349181175232 2.8220736980438232 142.81103515625
Loss :  1.713485836982727 2.7034897804260254 136.88796997070312
  batch 40 loss: 1.713485836982727, 2.7034897804260254, 136.88796997070312
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6870331764221191 3.079080104827881 155.6410369873047
Loss :  1.6775035858154297 2.8254458904266357 142.94979858398438
Loss :  1.6703182458877563 2.9011166095733643 146.7261505126953
Loss :  1.6787389516830444 3.278583288192749 165.60791015625
Loss :  1.6658480167388916 2.604536294937134 131.89266967773438
Loss :  1.6863815784454346 2.7724528312683105 140.30902099609375
Loss :  1.7084633111953735 3.2743866443634033 165.42779541015625
Loss :  1.6772749423980713 2.7192254066467285 137.6385498046875
Loss :  1.7182193994522095 2.9917242527008057 151.30442810058594
Loss :  1.6809386014938354 3.779003143310547 190.631103515625
Loss :  1.7017874717712402 3.0931646823883057 156.36001586914062
Loss :  1.6978263854980469 2.9604618549346924 149.72091674804688
Loss :  1.6833409070968628 2.7439444065093994 138.88055419921875
Loss :  1.701943039894104 2.9899628162384033 151.20008850097656
Loss :  1.6754029989242554 3.6558642387390137 184.46861267089844
Loss :  1.7172702550888062 3.0434000492095947 153.88726806640625
Loss :  1.6799752712249756 3.8303024768829346 193.19509887695312
Loss :  1.6682093143463135 3.407562494277954 172.0463409423828
Loss :  1.6772018671035767 3.591426134109497 181.24850463867188
Loss :  1.7202802896499634 3.0164058208465576 152.5405731201172
  batch 60 loss: 1.7202802896499634, 3.0164058208465576, 152.5405731201172
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6656414270401 3.1263749599456787 157.984375
Loss :  1.6849614381790161 3.0636510848999023 154.86752319335938
Loss :  1.6707820892333984 3.539942741394043 178.66790771484375
Loss :  1.6626328229904175 3.041823387145996 153.75379943847656
Loss :  1.648296594619751 2.740138530731201 138.6552276611328
Loss :  1.6820179224014282 4.38899040222168 221.1315460205078
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6917798519134521 4.436413288116455 223.51243591308594
Loss :  1.688774585723877 4.324206352233887 217.8990936279297
Loss :  1.693989872932434 4.224791526794434 212.93356323242188
Total LOSS train 158.91076977069562 valid 218.86915969848633
CE LOSS train 1.6845277327757615 valid 0.4234974682331085
Contrastive LOSS train 3.1445248420421894 valid 1.0561978816986084
EPOCH 170:
Loss :  1.6930245161056519 3.091766119003296 156.2813262939453
Loss :  1.7013686895370483 3.4445505142211914 173.92889404296875
Loss :  1.6799118518829346 2.922743082046509 147.81707763671875
Loss :  1.6849462985992432 3.4663405418395996 175.00198364257812
Loss :  1.6961113214492798 2.842819929122925 143.83709716796875
Loss :  1.6716011762619019 2.8524351119995117 144.29335021972656
Loss :  1.6962666511535645 3.269648551940918 165.17869567871094
Loss :  1.6792747974395752 3.2572762966156006 164.5430908203125
Loss :  1.6728712320327759 2.7852511405944824 140.9354248046875
Loss :  1.6964436769485474 2.9697654247283936 150.18470764160156
Loss :  1.6649359464645386 3.3040707111358643 166.86846923828125
Loss :  1.6644012928009033 3.020920753479004 152.71043395996094
Loss :  1.6629410982131958 2.9504776000976562 149.18682861328125
Loss :  1.6679338216781616 3.146436929702759 158.9897918701172
Loss :  1.706093430519104 3.213883638381958 162.40028381347656
Loss :  1.7028868198394775 3.0906450748443604 156.23513793945312
Loss :  1.6625479459762573 3.0100808143615723 152.16659545898438
Loss :  1.6810414791107178 2.885477304458618 145.9549102783203
Loss :  1.661038875579834 2.7877726554870605 141.04966735839844
Loss :  1.703839659690857 2.9068639278411865 147.0470428466797
  batch 20 loss: 1.703839659690857, 2.9068639278411865, 147.0470428466797
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6784284114837646 3.029517889022827 153.15431213378906
Loss :  1.6611407995224 2.8537585735321045 144.3490753173828
Loss :  1.6740057468414307 2.881190061569214 145.7335205078125
Loss :  1.6855608224868774 3.069567918777466 155.16395568847656
Loss :  1.7052226066589355 3.0130414962768555 152.35728454589844
Loss :  1.6770246028900146 3.10050106048584 156.7020721435547
Loss :  1.6837868690490723 3.0019149780273438 151.779541015625
Loss :  1.680978775024414 2.866513967514038 145.00668334960938
Loss :  1.645771861076355 2.6782827377319336 135.55990600585938
Loss :  1.70415461063385 3.79046630859375 191.2274627685547
Loss :  1.6467900276184082 3.1675539016723633 160.0244903564453
Loss :  1.6926528215408325 3.5244879722595215 177.91705322265625
Loss :  1.6759920120239258 2.806455135345459 141.99874877929688
Loss :  1.674731731414795 2.7305994033813477 138.20469665527344
Loss :  1.6505763530731201 3.1830241680145264 160.80178833007812
Loss :  1.6606464385986328 3.299821615219116 166.65171813964844
Loss :  1.6600676774978638 3.05928635597229 154.6243896484375
Loss :  1.7005195617675781 3.1165502071380615 157.5280303955078
Loss :  1.7038302421569824 3.2953245639801025 166.47006225585938
Loss :  1.7101492881774902 3.0204052925109863 152.73040771484375
  batch 40 loss: 1.7101492881774902, 3.0204052925109863, 152.73040771484375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6826847791671753 2.9656598567962646 149.96566772460938
Loss :  1.67276930809021 2.648545742034912 134.10006713867188
Loss :  1.6654541492462158 3.4606149196624756 174.69619750976562
Loss :  1.6739574670791626 2.978001832962036 150.57403564453125
Loss :  1.6596169471740723 2.745854616165161 138.9523468017578
Loss :  1.6806150674819946 2.913233757019043 147.34230041503906
Loss :  1.7038018703460693 2.7404282093048096 138.72520446777344
Loss :  1.6706867218017578 3.1526196002960205 159.30166625976562
Loss :  1.7140408754348755 3.3577237129211426 169.6002197265625
Loss :  1.6755952835083008 3.0113699436187744 152.2440948486328
Loss :  1.6984115839004517 2.813875675201416 142.39219665527344
Loss :  1.6949620246887207 3.1828207969665527 160.83599853515625
Loss :  1.681384801864624 3.6634576320648193 184.85426330566406
Loss :  1.700753927230835 3.1349024772644043 158.4458770751953
Loss :  1.6741598844528198 3.3587722778320312 169.61277770996094
Loss :  1.7163598537445068 2.9281578063964844 148.12425231933594
Loss :  1.6793123483657837 3.6161999702453613 182.4893035888672
Loss :  1.6684561967849731 3.8296408653259277 193.15049743652344
Loss :  1.6780169010162354 3.7499852180480957 189.1772918701172
Loss :  1.7207372188568115 2.9474661350250244 149.0940399169922
  batch 60 loss: 1.7207372188568115, 2.9474661350250244, 149.0940399169922
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6679702997207642 3.622934103012085 182.81466674804688
Loss :  1.687362551689148 2.9595680236816406 149.66575622558594
Loss :  1.673515796661377 2.9738879203796387 150.367919921875
Loss :  1.6656633615493774 3.1838200092315674 160.85665893554688
Loss :  1.6515768766403198 2.8477096557617188 144.0370635986328
Loss :  1.6751726865768433 4.164500713348389 209.90020751953125
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6837106943130493 4.2105302810668945 212.21023559570312
Loss :  1.6817569732666016 4.033949851989746 203.37925720214844
Loss :  1.6860653162002563 4.008971691131592 202.1346435546875
Total LOSS train 156.6771749643179 valid 206.90608596801758
CE LOSS train 1.6812211990356445 valid 0.4215163290500641
Contrastive LOSS train 3.099919084402231 valid 1.002242922782898
EPOCH 171:
Loss :  1.6950057744979858 3.5423991680145264 178.81497192382812
Loss :  1.70392906665802 3.45611310005188 174.50958251953125
Loss :  1.6834044456481934 3.2716026306152344 165.2635498046875
Loss :  1.6884344816207886 2.9594321250915527 149.6600341796875
Loss :  1.6989374160766602 3.2584447860717773 164.62118530273438
Loss :  1.675604224205017 2.8325870037078857 143.30494689941406
Loss :  1.6993229389190674 3.0535168647766113 154.3751678466797
Loss :  1.6837058067321777 2.7117486000061035 137.27113342285156
Loss :  1.6784006357192993 3.3457419872283936 168.9654998779297
Loss :  1.7007291316986084 2.8517465591430664 144.28805541992188
Loss :  1.671268343925476 3.3445560932159424 168.89906311035156
Loss :  1.6714990139007568 3.001286745071411 151.73582458496094
Loss :  1.6700069904327393 3.2774038314819336 165.5402069091797
Loss :  1.6748851537704468 2.983827829360962 150.86627197265625
Loss :  1.7117668390274048 2.939910650253296 148.70729064941406
Loss :  1.7086858749389648 3.3475091457366943 169.08413696289062
Loss :  1.6714136600494385 3.165778160095215 159.9603271484375
Loss :  1.68923819065094 3.3607640266418457 169.72744750976562
Loss :  1.669950246810913 3.465445041656494 174.94219970703125
Loss :  1.7098565101623535 3.2685325145721436 165.136474609375
  batch 20 loss: 1.7098565101623535, 3.2685325145721436, 165.136474609375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.685534119606018 2.792452812194824 141.30816650390625
Loss :  1.6689330339431763 3.4761769771575928 175.477783203125
Loss :  1.6802481412887573 3.4308621883392334 173.22335815429688
Loss :  1.6902371644973755 3.0950419902801514 156.4423370361328
Loss :  1.7087502479553223 3.096572160720825 156.53736877441406
Loss :  1.6819134950637817 3.9162514209747314 197.49449157714844
Loss :  1.6880381107330322 3.038531541824341 153.61460876464844
Loss :  1.6853493452072144 3.0196847915649414 152.66958618164062
Loss :  1.6511008739471436 2.9594833850860596 149.62527465820312
Loss :  1.7065426111221313 3.064213275909424 154.91720581054688
Loss :  1.651050329208374 3.163530111312866 159.82754516601562
Loss :  1.6955455541610718 3.4805495738983154 175.7230224609375
Loss :  1.678938865661621 3.0507824420928955 154.2180633544922
Loss :  1.678066372871399 3.256819248199463 164.51902770996094
Loss :  1.6546080112457275 3.393944025039673 171.351806640625
Loss :  1.664091944694519 3.4947402477264404 176.40110778808594
Loss :  1.663133978843689 2.799243927001953 141.6253204345703
Loss :  1.7023779153823853 2.908231496810913 147.11395263671875
Loss :  1.705262541770935 2.949545383453369 149.18252563476562
Loss :  1.7118806838989258 3.0005247592926025 151.73812866210938
  batch 40 loss: 1.7118806838989258, 3.0005247592926025, 151.73812866210938
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6850388050079346 3.0126681327819824 152.31845092773438
Loss :  1.6757512092590332 3.149977684020996 159.1746368408203
Loss :  1.6689962148666382 3.16762638092041 160.05030822753906
Loss :  1.6776217222213745 2.868562936782837 145.10577392578125
Loss :  1.6634478569030762 2.9766201972961426 150.4944610595703
Loss :  1.6835581064224243 2.9808080196380615 150.72396850585938
Loss :  1.7061965465545654 2.7625458240509033 139.83348083496094
Loss :  1.6739782094955444 2.620866537094116 132.71730041503906
Loss :  1.715369701385498 2.810830593109131 142.25689697265625
Loss :  1.6784567832946777 2.952146291732788 149.2857666015625
Loss :  1.7003984451293945 3.797473192214966 191.5740509033203
Loss :  1.6961171627044678 2.884674310684204 145.92984008789062
Loss :  1.6827311515808105 2.7718312740325928 140.2742919921875
Loss :  1.701289176940918 3.084428548812866 155.92271423339844
Loss :  1.6751818656921387 2.9682528972625732 150.08782958984375
Loss :  1.7155659198760986 3.3274614810943604 168.08863830566406
Loss :  1.678980827331543 3.4369239807128906 173.52517700195312
Loss :  1.667852759361267 3.2059404850006104 161.96487426757812
Loss :  1.677327036857605 3.442497491836548 173.8022003173828
Loss :  1.719921588897705 3.6419389247894287 183.81686401367188
  batch 60 loss: 1.719921588897705, 3.6419389247894287, 183.81686401367188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6675331592559814 3.585305690765381 180.9328155517578
Loss :  1.68710196018219 3.926034450531006 197.98883056640625
Loss :  1.672724723815918 3.3014371395111084 166.7445831298828
Loss :  1.6650229692459106 2.8714821338653564 145.2391357421875
Loss :  1.6506110429763794 2.889653444290161 146.13327026367188
Loss :  1.678714632987976 4.329552173614502 218.1563262939453
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6872317790985107 4.263371467590332 214.85580444335938
Loss :  1.6851341724395752 4.24360990524292 213.86563110351562
Loss :  1.6898988485336304 4.271728515625 215.2763214111328
Total LOSS train 159.5795417198768 valid 215.53852081298828
CE LOSS train 1.6845295851047222 valid 0.4224747121334076
Contrastive LOSS train 3.1579002563770002 valid 1.06793212890625
EPOCH 172:
Loss :  1.6941685676574707 3.7137043476104736 187.37937927246094
Loss :  1.702989935874939 3.1790730953216553 160.65664672851562
Loss :  1.6818310022354126 2.9930179119110107 151.3327178955078
Loss :  1.6866992712020874 2.986903667449951 151.03189086914062
Loss :  1.6972582340240479 3.0986533164978027 156.6299285888672
Loss :  1.6734131574630737 3.059596300125122 154.65322875976562
Loss :  1.6974886655807495 3.087958812713623 156.09542846679688
Loss :  1.681086778640747 3.333411931991577 168.3516845703125
Loss :  1.6758232116699219 2.9030683040618896 146.82923889160156
Loss :  1.6983963251113892 3.159998655319214 159.69833374023438
Loss :  1.6686071157455444 3.166593313217163 159.99827575683594
Loss :  1.6695586442947388 3.1505165100097656 159.19537353515625
Loss :  1.6673765182495117 2.9240834712982178 147.87155151367188
Loss :  1.6720961332321167 3.117061138153076 157.525146484375
Loss :  1.7092825174331665 3.056361198425293 154.52734375
Loss :  1.7055145502090454 2.782714605331421 140.84124755859375
Loss :  1.6673481464385986 3.1280198097229004 158.06832885742188
Loss :  1.6856681108474731 2.8135368824005127 142.36251831054688
Loss :  1.666658639907837 2.827479839324951 143.0406494140625
Loss :  1.707445502281189 2.859867572784424 144.70082092285156
  batch 20 loss: 1.707445502281189, 2.859867572784424, 144.70082092285156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6831021308898926 2.9185495376586914 147.61058044433594
Loss :  1.6666165590286255 2.7048213481903076 136.90768432617188
Loss :  1.6786847114562988 2.6077048778533936 132.0639190673828
Loss :  1.688724398612976 2.7773258686065674 140.5550079345703
Loss :  1.707829475402832 3.5202341079711914 177.7195281982422
Loss :  1.681239128112793 2.741079807281494 138.7352294921875
Loss :  1.6872111558914185 3.009432077407837 152.1588134765625
Loss :  1.6848857402801514 2.9066264629364014 147.01622009277344
Loss :  1.65119469165802 3.0446114540100098 153.88177490234375
Loss :  1.7068935632705688 2.837394952774048 143.57664489746094
Loss :  1.6528599262237549 4.085184097290039 205.9120635986328
Loss :  1.696820855140686 3.328482151031494 168.1209259033203
Loss :  1.6798511743545532 2.75791072845459 139.5753936767578
Loss :  1.679184913635254 2.7739222049713135 140.3752899169922
Loss :  1.6568689346313477 2.9695160388946533 150.13267517089844
Loss :  1.6667333841323853 2.777710437774658 140.55226135253906
Loss :  1.6666641235351562 3.4862074851989746 175.97705078125
Loss :  1.7051118612289429 3.2590718269348145 164.65870666503906
Loss :  1.7080832719802856 3.568800210952759 180.14810180664062
Loss :  1.7147061824798584 3.1630642414093018 159.867919921875
  batch 40 loss: 1.7147061824798584, 3.1630642414093018, 159.867919921875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6889979839324951 3.1264586448669434 158.01193237304688
Loss :  1.6797723770141602 3.7205076217651367 187.7051544189453
Loss :  1.6729551553726196 3.4205942153930664 172.70266723632812
Loss :  1.6811269521713257 3.126711368560791 158.01669311523438
Loss :  1.6674426794052124 3.142796039581299 158.8072509765625
Loss :  1.6868315935134888 3.1959636211395264 161.48501586914062
Loss :  1.708439588546753 3.0949625968933105 156.45655822753906
Loss :  1.6772547960281372 2.7695937156677246 140.15695190429688
Loss :  1.7181810140609741 2.856126308441162 144.52450561523438
Loss :  1.6814707517623901 2.7085185050964355 137.10739135742188
Loss :  1.7030394077301025 3.0466485023498535 154.03546142578125
Loss :  1.6989628076553345 3.1030290126800537 156.85040283203125
Loss :  1.686202049255371 3.190251588821411 161.1987762451172
Loss :  1.703878402709961 2.798149347305298 141.61134338378906
Loss :  1.6790368556976318 2.822331190109253 142.79559326171875
Loss :  1.7180341482162476 3.3193845748901367 167.687255859375
Loss :  1.6835142374038696 2.9713096618652344 150.24900817871094
Loss :  1.6734384298324585 2.869858741760254 145.16636657714844
Loss :  1.682806372642517 2.9662363529205322 149.99461364746094
Loss :  1.7233752012252808 3.070054292678833 155.22608947753906
  batch 60 loss: 1.7233752012252808, 3.070054292678833, 155.22608947753906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.674796462059021 3.519641876220703 177.65687561035156
Loss :  1.6940395832061768 3.1181387901306152 157.60098266601562
Loss :  1.6808415651321411 2.958305597305298 149.59613037109375
Loss :  1.6746958494186401 2.937077283859253 148.528564453125
Loss :  1.6619526147842407 2.26114821434021 114.7193603515625
Loss :  1.6879783868789673 4.3368072509765625 218.52833557128906
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6958478689193726 4.351503372192383 219.27101135253906
Loss :  1.6930941343307495 4.379065990447998 220.64639282226562
Loss :  1.6982026100158691 4.278591632843018 215.62777709960938
Total LOSS train 154.5265610914964 valid 218.51837921142578
CE LOSS train 1.6857394475203293 valid 0.4245506525039673
Contrastive LOSS train 3.0568164348602296 valid 1.0696479082107544
EPOCH 173:
Loss :  1.702620506286621 3.132145643234253 158.30990600585938
Loss :  1.710952639579773 3.4809205532073975 175.75697326660156
Loss :  1.6914997100830078 2.692885637283325 136.33578491210938
Loss :  1.6959283351898193 2.8621323108673096 144.8025360107422
Loss :  1.7052562236785889 2.6497890949249268 134.19471740722656
Loss :  1.6829230785369873 2.6307177543640137 133.21881103515625
Loss :  1.7055134773254395 3.2753875255584717 165.4748992919922
Loss :  1.6902656555175781 2.6523101329803467 134.30577087402344
Loss :  1.6857556104660034 2.978473663330078 150.60943603515625
Loss :  1.7060028314590454 3.0092034339904785 152.1661834716797
Loss :  1.6787029504776 3.1969971656799316 161.52854919433594
Loss :  1.6789735555648804 3.0793135166168213 155.6446533203125
Loss :  1.676532506942749 3.789839506149292 191.1685028076172
Loss :  1.6803157329559326 3.033640146255493 153.36231994628906
Loss :  1.7148514986038208 3.133436918258667 158.38670349121094
Loss :  1.7102819681167603 3.114417552947998 157.43115234375
Loss :  1.674865961074829 3.0177407264709473 152.56190490722656
Loss :  1.6914899349212646 2.893124580383301 146.34771728515625
Loss :  1.6734278202056885 3.2168920040130615 162.51803588867188
Loss :  1.7111879587173462 3.306234836578369 167.02291870117188
  batch 20 loss: 1.7111879587173462, 3.306234836578369, 167.02291870117188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.687931776046753 3.067692518234253 155.0725555419922
Loss :  1.6720255613327026 2.971240282058716 150.23403930664062
Loss :  1.6832935810089111 2.8276498317718506 143.06578063964844
Loss :  1.69151771068573 3.377781867980957 170.5806121826172
Loss :  1.7090102434158325 4.232911586761475 213.35459899902344
Loss :  1.6828587055206299 3.1564533710479736 159.50552368164062
Loss :  1.687707781791687 3.4325144290924072 173.3134307861328
Loss :  1.6856300830841064 3.204481840133667 161.9097137451172
Loss :  1.6522835493087769 3.0721235275268555 155.25845336914062
Loss :  1.7071444988250732 3.1371281147003174 158.56353759765625
Loss :  1.6531118154525757 2.9832046031951904 150.81333923339844
Loss :  1.6968525648117065 3.683865547180176 185.89013671875
Loss :  1.6803276538848877 3.1151559352874756 157.43812561035156
Loss :  1.6793544292449951 3.5237555503845215 177.86712646484375
Loss :  1.6563321352005005 2.9021494388580322 146.7637939453125
Loss :  1.6657629013061523 3.0444862842559814 153.89007568359375
Loss :  1.6650996208190918 3.2847211360931396 165.90115356445312
Loss :  1.7035187482833862 3.146350622177124 159.02105712890625
Loss :  1.7066575288772583 3.40423321723938 171.91831970214844
Loss :  1.7129948139190674 3.7738215923309326 190.40408325195312
  batch 40 loss: 1.7129948139190674, 3.7738215923309326, 190.40408325195312
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6868287324905396 3.709277629852295 187.1507110595703
Loss :  1.677383303642273 3.2582168579101562 164.58822631835938
Loss :  1.6705784797668457 2.9708428382873535 150.2127227783203
Loss :  1.678951382637024 2.792452335357666 141.30157470703125
Loss :  1.664945363998413 2.785947322845459 140.96231079101562
Loss :  1.6849509477615356 2.910839319229126 147.22691345214844
Loss :  1.7070339918136597 3.3216187953948975 167.7879638671875
Loss :  1.675398826599121 3.298633337020874 166.60707092285156
Loss :  1.7163116931915283 3.4741904735565186 175.4258270263672
Loss :  1.6791409254074097 2.760993480682373 139.7288055419922
Loss :  1.7014799118041992 2.9795851707458496 150.6807403564453
Loss :  1.6970584392547607 2.877345561981201 145.5643310546875
Loss :  1.6841700077056885 3.1706130504608154 160.21482849121094
Loss :  1.7019069194793701 3.1278417110443115 158.093994140625
Loss :  1.6774327754974365 3.224384069442749 162.89663696289062
Loss :  1.7162805795669556 2.8061177730560303 142.0221710205078
Loss :  1.6810541152954102 3.255869150161743 164.47451782226562
Loss :  1.6709939241409302 2.700425863265991 136.69227600097656
Loss :  1.6802315711975098 3.417264461517334 172.54345703125
Loss :  1.7214025259017944 2.7154006958007812 137.49143981933594
  batch 60 loss: 1.7214025259017944, 2.7154006958007812, 137.49143981933594
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6723973751068115 3.6661036014556885 184.97756958007812
Loss :  1.692144513130188 2.819633960723877 142.67384338378906
Loss :  1.678940773010254 2.7585830688476562 139.60809326171875
Loss :  1.6725268363952637 2.97115159034729 150.23011779785156
Loss :  1.6591964960098267 2.85270619392395 144.2945098876953
Loss :  1.6871092319488525 4.359208583831787 219.6475372314453
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.695501446723938 4.216477870941162 212.51939392089844
Loss :  1.6928837299346924 4.19420862197876 211.4033203125
Loss :  1.697287678718567 4.244236946105957 213.9091339111328
Total LOSS train 157.92867056039663 valid 214.36984634399414
CE LOSS train 1.6876231395281278 valid 0.4243219196796417
Contrastive LOSS train 3.124820958651029 valid 1.0610592365264893
EPOCH 174:
Loss :  1.6995567083358765 2.8718881607055664 145.29396057128906
Loss :  1.7081371545791626 3.4902825355529785 176.22225952148438
Loss :  1.688923954963684 2.8373732566833496 143.55758666992188
Loss :  1.6943135261535645 4.138565540313721 208.6226043701172
Loss :  1.7039008140563965 2.6396255493164062 133.6851806640625
Loss :  1.6827967166900635 3.0496327877044678 154.16444396972656
Loss :  1.7046505212783813 2.8853628635406494 145.97279357910156
Loss :  1.68896484375 2.804784059524536 141.92816162109375
Loss :  1.684417724609375 3.2485098838806152 164.1099090576172
Loss :  1.7042609453201294 3.208085536956787 162.10853576660156
Loss :  1.6765819787979126 3.5089731216430664 177.12522888183594
Loss :  1.676849603652954 3.8095953464508057 192.1566162109375
Loss :  1.673811435699463 2.7289369106292725 138.1206512451172
Loss :  1.6777770519256592 2.802485227584839 141.8020477294922
Loss :  1.7129021883010864 2.885915517807007 146.00868225097656
Loss :  1.708115577697754 2.9318647384643555 148.3013458251953
Loss :  1.672325611114502 3.2204558849334717 162.69512939453125
Loss :  1.688786268234253 3.1766488552093506 160.52122497558594
Loss :  1.6701048612594604 3.0380334854125977 153.57177734375
Loss :  1.7096941471099854 3.14384388923645 158.9019012451172
  batch 20 loss: 1.7096941471099854, 3.14384388923645, 158.9019012451172
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6860524415969849 3.3268203735351562 168.02706909179688
Loss :  1.6700350046157837 2.9644241333007812 149.8912353515625
Loss :  1.681792974472046 2.7646842002868652 139.91600036621094
Loss :  1.6908819675445557 2.9656007289886475 149.97091674804688
Loss :  1.7091916799545288 3.00651478767395 152.03494262695312
Loss :  1.6838157176971436 3.0564725399017334 154.5074462890625
Loss :  1.689544677734375 3.166149616241455 159.9970245361328
Loss :  1.6872788667678833 2.9842348098754883 150.89903259277344
Loss :  1.6540582180023193 2.8312251567840576 143.21531677246094
Loss :  1.7084981203079224 3.002885341644287 151.85276794433594
Loss :  1.6556718349456787 3.216395854949951 162.4754638671875
Loss :  1.6992045640945435 3.6529605388641357 184.34722900390625
Loss :  1.682446837425232 2.873791456222534 145.3720245361328
Loss :  1.6817368268966675 2.9039885997772217 146.88116455078125
Loss :  1.6600732803344727 3.332428216934204 168.281494140625
Loss :  1.6693458557128906 3.0113115310668945 152.23492431640625
Loss :  1.669063687324524 2.9628355503082275 149.81085205078125
Loss :  1.7061569690704346 2.812269926071167 142.3196563720703
Loss :  1.7089778184890747 3.141542673110962 158.78611755371094
Loss :  1.715559482574463 2.9565842151641846 149.54476928710938
  batch 40 loss: 1.715559482574463, 2.9565842151641846, 149.54476928710938
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.6908681392669678 3.1933608055114746 161.35891723632812
Loss :  1.6815539598464966 2.6804049015045166 135.70179748535156
Loss :  1.6750211715698242 2.88881516456604 146.11578369140625
Loss :  1.6832599639892578 2.837089776992798 143.53775024414062
Loss :  1.6693229675292969 3.2094736099243164 162.14300537109375
Loss :  1.6875089406967163 3.1252357959747314 157.94931030273438
Loss :  1.708739161491394 3.1532697677612305 159.37222290039062
Loss :  1.6778316497802734 3.2090797424316406 162.13180541992188
Loss :  1.717577338218689 4.164037704467773 209.91946411132812
Loss :  1.6800071001052856 2.9686639308929443 150.1132049560547
Loss :  1.7023589611053467 3.006070613861084 152.00588989257812
Loss :  1.6967252492904663 3.2962229251861572 166.50787353515625
Loss :  1.6834924221038818 3.2829642295837402 165.83169555664062
Loss :  1.7009284496307373 3.1431612968444824 158.85899353027344
Loss :  1.6764336824417114 3.5248231887817383 177.9176025390625
Loss :  1.7149274349212646 3.4313364028930664 173.28173828125
Loss :  1.6800239086151123 3.008136034011841 152.08682250976562
Loss :  1.6695704460144043 3.1460025310516357 158.96969604492188
Loss :  1.6787575483322144 3.725356101989746 187.94656372070312
Loss :  1.7206896543502808 3.1136069297790527 157.40103149414062
  batch 60 loss: 1.7206896543502808, 3.1136069297790527, 157.40103149414062
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  1.6703855991363525 2.888944149017334 146.11758422851562
Loss :  1.6903443336486816 2.825491428375244 142.96490478515625
Loss :  1.677063226699829 2.9819564819335938 150.77488708496094
Loss :  1.670917272567749 2.883490562438965 145.84544372558594
Loss :  1.6574245691299438 2.953568458557129 149.33584594726562
Loss :  1.6841908693313599 4.3422651290893555 218.7974395751953
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6921918392181396 4.324462890625 217.9153289794922
Loss :  1.689449429512024 4.275850772857666 215.48199462890625
Loss :  1.6944327354431152 4.083724498748779 205.88064575195312
Total LOSS train 157.06808189978966 valid 214.51885223388672
CE LOSS train 1.6876614093780518 valid 0.4236081838607788
Contrastive LOSS train 3.1076083990243766 valid 1.0209311246871948
EPOCH 175:
Loss :  1.6983627080917358 3.3252341747283936 167.96006774902344
Loss :  1.7071391344070435 3.2328150272369385 163.34788513183594
Loss :  1.6879409551620483 3.3012855052948 166.75221252441406
Loss :  1.6931222677230835 2.832576274871826 143.32192993164062
Loss :  1.7028416395187378 2.575917959213257 130.4987335205078
Loss :  1.6805715560913086 2.7243311405181885 137.89712524414062
Loss :  1.7032403945922852 3.1524720191955566 159.32684326171875
Loss :  1.6879216432571411 3.183903694152832 160.88311767578125
Loss :  1.6825997829437256 2.6702654361724854 135.1958770751953
Loss :  1.7030928134918213 2.9026505947113037 146.8356170654297
Loss :  1.6741223335266113 3.2705986499786377 165.2040557861328
Loss :  1.6744779348373413 3.16471266746521 159.9101104736328
Loss :  1.6717699766159058 2.767267942428589 140.03517150878906
Loss :  1.676190972328186 3.4564127922058105 174.496826171875
Loss :  1.7118258476257324 3.0318920612335205 153.30642700195312
Loss :  1.7069474458694458 3.078482151031494 155.6310577392578
Loss :  1.6706753969192505 3.419546604156494 172.6479949951172
Loss :  1.687644362449646 3.447028875350952 174.03907775878906
Loss :  1.6693459749221802 3.1894571781158447 161.14219665527344
Loss :  1.7084848880767822 3.037153720855713 153.566162109375
  batch 20 loss: 1.7084848880767822, 3.037153720855713, 153.566162109375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.685332179069519 3.039424419403076 153.65655517578125
Loss :  1.6694220304489136 3.3872625827789307 171.0325469970703
Loss :  1.6812373399734497 2.695082664489746 136.4353790283203
Loss :  1.6908578872680664 3.45959734916687 174.6707305908203
Loss :  1.7088501453399658 3.0967600345611572 156.54684448242188
Loss :  1.683529019355774 2.8606550693511963 144.71629333496094
Loss :  1.6893694400787354 3.3077445030212402 167.07659912109375
Loss :  1.6874620914459229 3.6671247482299805 185.043701171875
Loss :  1.654646873474121 3.3651793003082275 169.9136199951172
Loss :  1.7083110809326172 2.8496549129486084 144.19105529785156
Loss :  1.6559175252914429 3.1450908184051514 158.91046142578125
Loss :  1.6987276077270508 2.9469153881073 149.04449462890625
Loss :  1.6820496320724487 2.938079595565796 148.58602905273438
Loss :  1.6809165477752686 3.092231273651123 156.29248046875
Loss :  1.6586127281188965 3.771015167236328 190.20936584472656
Loss :  1.6678543090820312 3.1144514083862305 157.39041137695312
Loss :  1.6672898530960083 3.1179122924804688 157.5629119873047
Loss :  1.704697847366333 3.093539237976074 156.38165283203125
Loss :  1.7073694467544556 3.5508859157562256 179.2516632080078
Loss :  1.7140312194824219 3.2347476482391357 163.451416015625
  batch 40 loss: 1.7140312194824219, 3.2347476482391357, 163.451416015625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6889104843139648 3.359276056289673 169.6527099609375
Loss :  1.6797420978546143 3.0871689319610596 156.03819274902344
Loss :  1.673427939414978 3.2103209495544434 162.18948364257812
Loss :  1.6819465160369873 3.644439935684204 183.90394592285156
Loss :  1.6683628559112549 2.5928025245666504 131.30848693847656
Loss :  1.6872704029083252 2.8053126335144043 141.95289611816406
Loss :  1.708483338356018 3.1274449825286865 158.0807342529297
Loss :  1.6785788536071777 2.7955102920532227 141.45408630371094
Loss :  1.718030571937561 3.676556348800659 185.5458526611328
Loss :  1.6821881532669067 3.139782667160034 158.67132568359375
Loss :  1.703683853149414 2.851813316345215 144.2943572998047
Loss :  1.6992384195327759 2.9131131172180176 147.35488891601562
Loss :  1.6870903968811035 2.746244192123413 138.99929809570312
Loss :  1.704420566558838 4.03258752822876 203.33380126953125
Loss :  1.6810078620910645 3.0913546085357666 156.24874877929688
Loss :  1.7192087173461914 2.767327070236206 140.08555603027344
Loss :  1.6855576038360596 3.4608712196350098 174.7291259765625
Loss :  1.6761857271194458 2.9066426753997803 147.00833129882812
Loss :  1.6849781274795532 3.7065930366516113 187.01463317871094
Loss :  1.7241096496582031 2.8528223037719727 144.36521911621094
  batch 60 loss: 1.7241096496582031, 2.8528223037719727, 144.36521911621094
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.6771090030670166 3.0838141441345215 155.86781311035156
Loss :  1.6955592632293701 3.26127290725708 164.7592010498047
Loss :  1.6827276945114136 3.2009665966033936 161.73104858398438
Loss :  1.6765995025634766 3.144679546356201 158.91058349609375
Loss :  1.6640044450759888 2.7591824531555176 139.6231231689453
Loss :  1.6837966442108154 4.349780082702637 219.17279052734375
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6917082071304321 4.347225666046143 219.05299377441406
Loss :  1.6886622905731201 4.251783847808838 214.27784729003906
Loss :  1.6935967206954956 4.189017295837402 211.14447021484375
Total LOSS train 158.39209453876202 valid 215.91202545166016
CE LOSS train 1.6880496135124794 valid 0.4233991801738739
Contrastive LOSS train 3.134080905180711 valid 1.0472543239593506
EPOCH 176:
Loss :  1.703140139579773 2.952660322189331 149.33615112304688
Loss :  1.7108896970748901 3.219432830810547 162.6825408935547
Loss :  1.6918718814849854 3.4278156757354736 173.08265686035156
Loss :  1.696911334991455 3.1186130046844482 157.6275634765625
Loss :  1.7062106132507324 2.7439119815826416 138.9018096923828
Loss :  1.684899926185608 2.7696661949157715 140.168212890625
Loss :  1.7067569494247437 3.584418296813965 180.92767333984375
Loss :  1.6920520067214966 2.6172587871551514 132.55499267578125
Loss :  1.6872944831848145 2.75026798248291 139.20069885253906
Loss :  1.707148790359497 2.9947121143341064 151.44276428222656
Loss :  1.679702639579773 3.205970525741577 161.9782257080078
Loss :  1.6794836521148682 2.96195387840271 149.7771759033203
Loss :  1.677559494972229 2.818153142929077 142.58522033691406
Loss :  1.682124137878418 3.219048023223877 162.63453674316406
Loss :  1.716186761856079 3.5208280086517334 177.75758361816406
Loss :  1.7122772932052612 2.8674862384796143 145.0865936279297
Loss :  1.6778132915496826 2.9113643169403076 147.24603271484375
Loss :  1.6940279006958008 3.9564337730407715 199.51571655273438
Loss :  1.675924301147461 3.187971830368042 161.0745086669922
Loss :  1.7128572463989258 3.0399298667907715 153.7093505859375
  batch 20 loss: 1.7128572463989258, 3.0399298667907715, 153.7093505859375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.690454363822937 2.703803777694702 136.88064575195312
Loss :  1.6750277280807495 2.814659357070923 142.4080047607422
Loss :  1.6863329410552979 3.851935625076294 194.2831268310547
Loss :  1.6951106786727905 3.453437328338623 174.36697387695312
Loss :  1.7124358415603638 3.878657579421997 195.6453094482422
Loss :  1.687172770500183 2.897996187210083 146.5869903564453
Loss :  1.6923906803131104 3.1055848598480225 156.9716339111328
Loss :  1.690600037574768 2.688585042953491 136.11984252929688
Loss :  1.6582977771759033 3.010878562927246 152.2022247314453
Loss :  1.710715413093567 2.9061567783355713 147.0185546875
Loss :  1.659361481666565 3.8408162593841553 193.70018005371094
Loss :  1.700645923614502 3.109494924545288 157.17539978027344
Loss :  1.684659719467163 3.0554628372192383 154.4578094482422
Loss :  1.6839913129806519 3.2392964363098145 163.64881896972656
Loss :  1.6626369953155518 3.2792890071868896 165.62709045410156
Loss :  1.6715248823165894 3.3725380897521973 170.29843139648438
Loss :  1.6708838939666748 3.0231285095214844 152.8273162841797
Loss :  1.7076630592346191 3.0300896167755127 153.21214294433594
Loss :  1.709885597229004 3.462913990020752 174.8555908203125
Loss :  1.7164608240127563 2.807136297225952 142.07327270507812
  batch 40 loss: 1.7164608240127563, 2.807136297225952, 142.07327270507812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6919240951538086 2.8551831245422363 144.45108032226562
Loss :  1.6826515197753906 2.6955554485321045 136.46041870117188
Loss :  1.6763684749603271 2.7652244567871094 139.93759155273438
Loss :  1.6842834949493408 3.0698859691619873 155.17857360839844
Loss :  1.6710187196731567 3.183642864227295 160.85316467285156
Loss :  1.6897741556167603 3.0904529094696045 156.21241760253906
Loss :  1.7103418111801147 2.555485248565674 129.48460388183594
Loss :  1.681082844734192 2.8457846641540527 143.97030639648438
Loss :  1.7201223373413086 3.1187729835510254 157.6587677001953
Loss :  1.6848589181900024 2.688995361328125 136.13462829589844
Loss :  1.706140398979187 2.881728410720825 145.79257202148438
Loss :  1.7020214796066284 3.0003132820129395 151.7176971435547
Loss :  1.6900473833084106 2.8298776149749756 143.18392944335938
Loss :  1.7071268558502197 2.7950053215026855 141.45738220214844
Loss :  1.683523178100586 3.799238920211792 191.6454620361328
Loss :  1.7202180624008179 2.7664406299591064 140.04225158691406
Loss :  1.6868621110916138 3.2508015632629395 164.22694396972656
Loss :  1.6773550510406494 2.751232147216797 139.2389678955078
Loss :  1.685774326324463 3.132575273513794 158.31454467773438
Loss :  1.724983811378479 3.0078179836273193 152.1158905029297
  batch 60 loss: 1.724983811378479, 3.0078179836273193, 152.1158905029297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6779214143753052 3.4130287170410156 172.329345703125
Loss :  1.6960128545761108 2.8872904777526855 146.06053161621094
Loss :  1.682734489440918 2.9052836894989014 146.94692993164062
Loss :  1.675426959991455 3.534102201461792 178.3805389404297
Loss :  1.6614818572998047 2.4054038524627686 121.93167877197266
Loss :  1.685004711151123 4.076660633087158 205.51803588867188
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6929357051849365 4.155673027038574 209.47657775878906
Loss :  1.6914074420928955 3.872821092605591 195.33245849609375
Loss :  1.6946909427642822 3.9662981033325195 200.0095977783203
Total LOSS train 155.25193211482122 valid 202.58416748046875
CE LOSS train 1.6912529395176814 valid 0.42367273569107056
Contrastive LOSS train 3.0712135535020093 valid 0.9915745258331299
EPOCH 177:
Loss :  1.7002708911895752 2.8734114170074463 145.370849609375
Loss :  1.7082738876342773 3.6307947635650635 183.2480010986328
Loss :  1.6892470121383667 2.9052209854125977 146.95028686523438
Loss :  1.6938897371292114 3.270117998123169 165.19979858398438
Loss :  1.7038544416427612 2.578662872314453 130.63699340820312
Loss :  1.68207848072052 2.8123106956481934 142.29762268066406
Loss :  1.704390525817871 3.8227736949920654 192.84307861328125
Loss :  1.6892664432525635 2.582331657409668 130.80584716796875
Loss :  1.6841609477996826 2.801172971725464 141.74281311035156
Loss :  1.7043421268463135 2.708327054977417 137.12069702148438
Loss :  1.676053524017334 3.1352577209472656 158.43893432617188
Loss :  1.6760468482971191 3.021097421646118 152.7309112548828
Loss :  1.674052357673645 3.3168365955352783 167.51588439941406
Loss :  1.6783696413040161 3.1626813411712646 159.81243896484375
Loss :  1.7135125398635864 3.3152666091918945 167.4768524169922
Loss :  1.7087346315383911 3.0527892112731934 154.34820556640625
Loss :  1.672774314880371 3.1262924671173096 157.98739624023438
Loss :  1.6890510320663452 3.1517722606658936 159.2776641845703
Loss :  1.6706547737121582 3.2085089683532715 162.09609985351562
Loss :  1.7098257541656494 2.787933826446533 141.1065216064453
  batch 20 loss: 1.7098257541656494, 2.787933826446533, 141.1065216064453
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.686300277709961 2.6473731994628906 134.05494689941406
Loss :  1.6702772378921509 3.5471839904785156 179.0294647216797
Loss :  1.6819651126861572 3.0085179805755615 152.1078643798828
Loss :  1.69175124168396 2.6466927528381348 134.02639770507812
Loss :  1.7099183797836304 3.1341071128845215 158.41526794433594
Loss :  1.6845438480377197 2.819633722305298 142.66622924804688
Loss :  1.6900975704193115 3.421562671661377 172.7682342529297
Loss :  1.6886736154556274 2.993499994277954 151.36367797851562
Loss :  1.6562131643295288 2.6832356452941895 135.81800842285156
Loss :  1.7101744413375854 3.7282230854034424 188.12132263183594
Loss :  1.6582385301589966 2.8515820503234863 144.23733520507812
Loss :  1.700912594795227 2.964287281036377 149.915283203125
Loss :  1.6853660345077515 2.8474438190460205 144.05755615234375
Loss :  1.6849935054779053 3.2507736682891846 164.2236785888672
Loss :  1.6639856100082397 3.376133918762207 170.47068786621094
Loss :  1.6733527183532715 2.945084810256958 148.92759704589844
Loss :  1.6731441020965576 2.8687896728515625 145.1126251220703
Loss :  1.709575891494751 2.692821502685547 136.3506622314453
Loss :  1.711098313331604 3.718817710876465 187.65199279785156
Loss :  1.7169777154922485 4.082235813140869 205.82876586914062
  batch 40 loss: 1.7169777154922485, 4.082235813140869, 205.82876586914062
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  1.692298412322998 3.559842348098755 179.6844024658203
Loss :  1.6833546161651611 2.9415855407714844 148.76263427734375
Loss :  1.676964282989502 2.9116971492767334 147.26182556152344
Loss :  1.6845086812973022 2.857600212097168 144.56451416015625
Loss :  1.671343207359314 2.6231648921966553 132.82958984375
Loss :  1.6896733045578003 3.678462266921997 185.61277770996094
Loss :  1.709910273551941 2.97945237159729 150.6825408935547
Loss :  1.6807619333267212 2.801166534423828 141.73907470703125
Loss :  1.7192531824111938 2.9288835525512695 148.16343688964844
Loss :  1.6850230693817139 2.9847605228424072 150.9230499267578
Loss :  1.7055411338806152 3.4205610752105713 172.7335968017578
Loss :  1.7013113498687744 3.249845027923584 164.1935577392578
Loss :  1.6890729665756226 2.9920737743377686 151.29275512695312
Loss :  1.705727458000183 4.083466529846191 205.87905883789062
Loss :  1.6817325353622437 2.987816095352173 151.07254028320312
Loss :  1.7185367345809937 2.7803597450256348 140.7365264892578
Loss :  1.684470295906067 2.825413227081299 142.9551239013672
Loss :  1.6745872497558594 2.701294422149658 136.73931884765625
Loss :  1.6831685304641724 3.0271286964416504 153.0395965576172
Loss :  1.7226706743240356 3.1558687686920166 159.51611328125
  batch 60 loss: 1.7226706743240356, 3.1558687686920166, 159.51611328125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.674537181854248 2.9092719554901123 147.13812255859375
Loss :  1.6933397054672241 3.3826985359191895 170.82827758789062
Loss :  1.6799403429031372 2.915802240371704 147.47006225585938
Loss :  1.6739561557769775 3.7409121990203857 188.7195587158203
Loss :  1.6606143712997437 3.3785555362701416 170.58839416503906
Loss :  1.6832715272903442 4.147151947021484 209.04087829589844
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6908069849014282 4.100335597991943 206.7075958251953
Loss :  1.6888890266418457 4.018280029296875 202.60289001464844
Loss :  1.6922670602798462 3.8732807636260986 195.35629272460938
Total LOSS train 156.54278376652644 valid 203.4269142150879
CE LOSS train 1.6895185764019305 valid 0.42306676506996155
Contrastive LOSS train 3.0970652947059043 valid 0.9683201909065247
EPOCH 178:
Loss :  1.7004609107971191 2.8765461444854736 145.5277557373047
Loss :  1.709161639213562 3.1528937816619873 159.35385131835938
Loss :  1.6895273923873901 2.7378175258636475 138.5803985595703
Loss :  1.6943385601043701 3.2361581325531006 163.5022430419922
Loss :  1.703331470489502 3.592827081680298 181.34469604492188
Loss :  1.6819696426391602 2.7914552688598633 141.25474548339844
Loss :  1.704252004623413 2.8835904598236084 145.88377380371094
Loss :  1.6895524263381958 2.837285041809082 143.55381774902344
Loss :  1.6852357387542725 2.94816517829895 149.093505859375
Loss :  1.7048784494400024 3.4262096881866455 173.01536560058594
Loss :  1.6773192882537842 3.002243995666504 151.78952026367188
Loss :  1.6774935722351074 3.4132487773895264 172.33993530273438
Loss :  1.6750339269638062 2.9373762607574463 148.54385375976562
Loss :  1.6795682907104492 2.8859739303588867 145.97825622558594
Loss :  1.7150417566299438 2.972496747970581 150.33987426757812
Loss :  1.7096883058547974 2.8292853832244873 143.1739501953125
Loss :  1.675833821296692 3.248415470123291 164.0966033935547
Loss :  1.6915655136108398 3.3578288555145264 169.5830078125
Loss :  1.673693299293518 2.8527650833129883 144.31195068359375
Loss :  1.7117077112197876 2.808972120285034 142.16030883789062
  batch 20 loss: 1.7117077112197876, 2.808972120285034, 142.16030883789062
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.688118577003479 2.6285476684570312 133.11550903320312
Loss :  1.6726163625717163 2.8181281089782715 142.5790252685547
Loss :  1.6835403442382812 2.755862236022949 139.47665405273438
Loss :  1.6920709609985352 3.8519184589385986 194.28799438476562
Loss :  1.7094231843948364 3.1237473487854004 157.89678955078125
Loss :  1.6841508150100708 3.0079689025878906 152.0825958251953
Loss :  1.6891447305679321 3.0387537479400635 153.6268310546875
Loss :  1.6876726150512695 2.6889989376068115 136.1376190185547
Loss :  1.654478669166565 3.133446455001831 158.32679748535156
Loss :  1.7086845636367798 2.8062515258789062 142.02125549316406
Loss :  1.656132459640503 3.070232629776001 155.16775512695312
Loss :  1.6988526582717896 3.2139925956726074 162.3984832763672
Loss :  1.6828194856643677 2.9394733905792236 148.65647888183594
Loss :  1.6823006868362427 3.228692054748535 163.1168975830078
Loss :  1.661177396774292 3.0279946327209473 153.0609130859375
Loss :  1.670351505279541 2.984712839126587 150.90599060058594
Loss :  1.6703996658325195 2.936889410018921 148.51486206054688
Loss :  1.7076224088668823 3.0728037357330322 155.34780883789062
Loss :  1.7096027135849 2.742225170135498 138.82086181640625
Loss :  1.7162179946899414 2.690338134765625 136.23312377929688
  batch 40 loss: 1.7162179946899414, 2.690338134765625, 136.23312377929688
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6918163299560547 3.868884325027466 195.1360321044922
Loss :  1.68292236328125 2.648818254470825 134.12384033203125
Loss :  1.6770395040512085 3.068011999130249 155.07763671875
Loss :  1.685296654701233 2.714176654815674 137.39413452148438
Loss :  1.6722201108932495 2.6197597980499268 132.66021728515625
Loss :  1.6903579235076904 2.9687581062316895 150.12826538085938
Loss :  1.7104963064193726 3.712512493133545 187.33612060546875
Loss :  1.681844711303711 2.8158445358276367 142.47406005859375
Loss :  1.7205815315246582 3.1420249938964844 158.82183837890625
Loss :  1.6846368312835693 3.374316453933716 170.40045166015625
Loss :  1.7059381008148193 3.482224464416504 175.81715393066406
Loss :  1.7012913227081299 2.900779962539673 146.74029541015625
Loss :  1.6894370317459106 2.62027907371521 132.70338439941406
Loss :  1.7061079740524292 3.3240199089050293 167.90708923339844
Loss :  1.6825796365737915 2.9552037715911865 149.44277954101562
Loss :  1.7189891338348389 2.838268280029297 143.63241577148438
Loss :  1.6860857009887695 3.299804210662842 166.67628479003906
Loss :  1.6767364740371704 2.7088429927825928 137.118896484375
Loss :  1.6851412057876587 3.1658523082733154 159.97775268554688
Loss :  1.7241994142532349 4.037867069244385 203.6175537109375
  batch 60 loss: 1.7241994142532349, 4.037867069244385, 203.6175537109375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.6772414445877075 3.372378349304199 170.29615783691406
Loss :  1.6959455013275146 2.847660541534424 144.07896423339844
Loss :  1.6829404830932617 2.886582136154175 146.0120391845703
Loss :  1.6768078804016113 2.8078019618988037 142.06689453125
Loss :  1.664209008216858 2.5811374187469482 130.72108459472656
Loss :  1.6851460933685303 4.395024299621582 221.43637084960938
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.692795991897583 4.3947014808654785 221.42787170410156
Loss :  1.6901421546936035 4.3055853843688965 216.9694061279297
Loss :  1.6944644451141357 4.166543006896973 210.0216064453125
Total LOSS train 153.47020005446214 valid 217.46381378173828
CE LOSS train 1.6899368322812593 valid 0.42361611127853394
Contrastive LOSS train 3.0356052765479453 valid 1.0416357517242432
EPOCH 179:
Loss :  1.7030223608016968 3.8301806449890137 193.21205139160156
Loss :  1.710656762123108 3.451848268508911 174.30307006835938
Loss :  1.6911373138427734 2.8812062740325928 145.75144958496094
Loss :  1.695270299911499 2.853301763534546 144.3603515625
Loss :  1.7045087814331055 2.782520055770874 140.8305206298828
Loss :  1.6820241212844849 2.796205997467041 141.49232482910156
Loss :  1.704606533050537 3.0579721927642822 154.60321044921875
Loss :  1.6893984079360962 2.880894660949707 145.734130859375
Loss :  1.6847782135009766 3.009066581726074 152.1381072998047
Loss :  1.704821228981018 3.293922185897827 166.4009246826172
Loss :  1.6770260334014893 3.2275519371032715 163.05462646484375
Loss :  1.6779886484146118 3.0201222896575928 152.68411254882812
Loss :  1.6756523847579956 3.0554118156433105 154.4462432861328
Loss :  1.6803767681121826 2.894551992416382 146.40797424316406
Loss :  1.7147119045257568 2.891711711883545 146.30029296875
Loss :  1.71108877658844 3.089156150817871 156.1688995361328
Loss :  1.6765174865722656 3.2249419689178467 162.92361450195312
Loss :  1.6923164129257202 2.901897430419922 146.78720092773438
Loss :  1.6743532419204712 3.5157811641693115 177.46340942382812
Loss :  1.7119454145431519 2.756476879119873 139.53578186035156
  batch 20 loss: 1.7119454145431519, 2.756476879119873, 139.53578186035156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.688429594039917 2.81160306930542 142.26858520507812
Loss :  1.6721084117889404 3.0578789710998535 154.56605529785156
Loss :  1.6832318305969238 3.2561655044555664 164.4915008544922
Loss :  1.6917552947998047 3.0107219219207764 152.2278594970703
Loss :  1.7090250253677368 3.7391722202301025 188.6676483154297
Loss :  1.682832956314087 2.9066948890686035 147.017578125
Loss :  1.6873952150344849 3.2700295448303223 165.18887329101562
Loss :  1.6859400272369385 3.019895315170288 152.6807098388672
Loss :  1.6517033576965332 3.059994697570801 154.6514434814453
Loss :  1.7070614099502563 3.4323391914367676 173.3240203857422
Loss :  1.6528671979904175 2.9859185218811035 150.94879150390625
Loss :  1.6963415145874023 3.1350836753845215 158.4505157470703
Loss :  1.6800708770751953 3.1901981830596924 161.18997192382812
Loss :  1.6793509721755981 2.99206280708313 151.28248596191406
Loss :  1.657423973083496 3.045632839202881 153.93907165527344
Loss :  1.6666406393051147 2.967353105545044 150.0343017578125
Loss :  1.6663683652877808 2.8941214084625244 146.3724365234375
Loss :  1.7048828601837158 2.730421304702759 138.2259521484375
Loss :  1.7070552110671997 3.8209357261657715 192.75384521484375
Loss :  1.7140357494354248 2.8605363368988037 144.7408447265625
  batch 40 loss: 1.7140357494354248, 2.8605363368988037, 144.7408447265625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6890387535095215 2.8314123153686523 143.25965881347656
Loss :  1.6791437864303589 2.8649039268493652 144.92433166503906
Loss :  1.6728421449661255 2.9199702739715576 147.67135620117188
Loss :  1.6811555624008179 2.951993465423584 149.28082275390625
Loss :  1.6674587726593018 2.7637012004852295 139.85252380371094
Loss :  1.6865755319595337 2.908022165298462 147.08767700195312
Loss :  1.7075976133346558 2.9637205600738525 149.8936309814453
Loss :  1.677401065826416 3.122189521789551 157.78688049316406
Loss :  1.717345952987671 3.86381459236145 194.9080810546875
Loss :  1.6807868480682373 3.7618870735168457 189.775146484375
Loss :  1.7032026052474976 3.061884880065918 154.7974395751953
Loss :  1.6983904838562012 2.7838592529296875 140.891357421875
Loss :  1.6861472129821777 2.872403144836426 145.30630493164062
Loss :  1.7034797668457031 3.039302349090576 153.66859436035156
Loss :  1.6795601844787598 3.265886068344116 164.97386169433594
Loss :  1.7176765203475952 3.678077220916748 185.6215362548828
Loss :  1.682922124862671 3.3666083812713623 170.01333618164062
Loss :  1.6727726459503174 2.977644920349121 150.55502319335938
Loss :  1.6811784505844116 2.9936397075653076 151.36317443847656
Loss :  1.7215830087661743 3.014195680618286 152.43136596679688
  batch 60 loss: 1.7215830087661743, 3.014195680618286, 152.43136596679688
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.672711968421936 3.4015634059906006 171.75088500976562
Loss :  1.691521167755127 3.7236125469207764 187.87216186523438
Loss :  1.6779361963272095 2.9312679767608643 148.2413330078125
Loss :  1.6706267595291138 3.038273334503174 153.58428955078125
Loss :  1.656604290008545 2.5424373149871826 128.77847290039062
Loss :  1.6798505783081055 4.036137104034424 203.48670959472656
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6878173351287842 4.024653911590576 202.92051696777344
Loss :  1.6860120296478271 4.0291571617126465 203.14385986328125
Loss :  1.6897337436676025 3.8035643100738525 191.86795043945312
Total LOSS train 156.4909236027644 valid 200.3547592163086
CE LOSS train 1.6880366307038528 valid 0.42243343591690063
Contrastive LOSS train 3.096057730454665 valid 0.9508910775184631
EPOCH 180:
Loss :  1.6976313591003418 2.9798531532287598 150.69029235839844
Loss :  1.706070065498352 2.9518370628356934 149.2979278564453
Loss :  1.686310887336731 3.008145570755005 152.0935821533203
Loss :  1.6909767389297485 3.3173561096191406 167.55877685546875
Loss :  1.7007653713226318 2.6824941635131836 135.82546997070312
Loss :  1.6779502630233765 2.818587064743042 142.6072998046875
Loss :  1.7014809846878052 2.7729151248931885 140.34722900390625
Loss :  1.6858316659927368 3.6932260990142822 186.34713745117188
Loss :  1.680983543395996 3.8060688972473145 191.98443603515625
Loss :  1.7021291255950928 2.6851842403411865 135.9613494873047
Loss :  1.6732137203216553 2.9995076656341553 151.6486053466797
Loss :  1.6735056638717651 2.910824775695801 147.21475219726562
Loss :  1.6712640523910522 2.8251125812530518 142.92689514160156
Loss :  1.675718069076538 3.0143001079559326 152.39073181152344
Loss :  1.7118198871612549 3.033294916152954 153.37657165527344
Loss :  1.7071781158447266 2.8499999046325684 144.20718383789062
Loss :  1.6707398891448975 3.0580780506134033 154.57464599609375
Loss :  1.6878161430358887 2.9837875366210938 150.877197265625
Loss :  1.6688028573989868 3.363609790802002 169.84930419921875
Loss :  1.708531141281128 2.8275270462036133 143.0848846435547
  batch 20 loss: 1.708531141281128, 2.8275270462036133, 143.0848846435547
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6843994855880737 2.7839105129241943 140.8799285888672
Loss :  1.6682853698730469 2.8837475776672363 145.8556671142578
Loss :  1.6802380084991455 2.732287883758545 138.2946319580078
Loss :  1.6900029182434082 3.0825233459472656 155.816162109375
Loss :  1.708146333694458 2.8717122077941895 145.29376220703125
Loss :  1.6823530197143555 3.7467591762542725 189.0203094482422
Loss :  1.687943935394287 2.771461248397827 140.26100158691406
Loss :  1.6861519813537598 3.3739287853240967 170.38259887695312
Loss :  1.6524112224578857 3.240018367767334 163.6533203125
Loss :  1.7075755596160889 2.9219982624053955 147.8074951171875
Loss :  1.6543126106262207 3.39740252494812 171.52444458007812
Loss :  1.697912573814392 3.6594274044036865 184.66928100585938
Loss :  1.681383728981018 2.783151865005493 140.83897399902344
Loss :  1.680676817893982 2.8250839710235596 142.93487548828125
Loss :  1.6581283807754517 3.545720338821411 178.9441375732422
Loss :  1.6674516201019287 3.3970401287078857 171.5194549560547
Loss :  1.6670106649398804 3.4374008178710938 173.53704833984375
Loss :  1.7050750255584717 3.024094581604004 152.90980529785156
Loss :  1.707883358001709 2.8656725883483887 144.99151611328125
Loss :  1.7143723964691162 3.5184998512268066 177.6393585205078
  batch 40 loss: 1.7143723964691162, 3.5184998512268066, 177.6393585205078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6891677379608154 3.6713531017303467 185.25682067871094
Loss :  1.6799672842025757 2.995789051055908 151.46942138671875
Loss :  1.6734950542449951 3.2188384532928467 162.61541748046875
Loss :  1.6818774938583374 2.7442266941070557 138.89321899414062
Loss :  1.668160319328308 3.142505407333374 158.79344177246094
Loss :  1.6869829893112183 2.834791660308838 143.4265594482422
Loss :  1.7083271741867065 3.1696507930755615 160.1908721923828
Loss :  1.6776180267333984 2.7651498317718506 139.9351043701172
Loss :  1.7178417444229126 3.52801775932312 178.1187286376953
Loss :  1.681758165359497 2.6847000122070312 135.91676330566406
Loss :  1.702745795249939 3.2495453357696533 164.1800079345703
Loss :  1.6986134052276611 2.876818895339966 145.53955078125
Loss :  1.6856966018676758 2.954118251800537 149.39161682128906
Loss :  1.7034271955490112 3.0739529132843018 155.4010772705078
Loss :  1.6787166595458984 3.4507765769958496 174.21754455566406
Loss :  1.717570185661316 4.061026573181152 204.76890563964844
Loss :  1.682579517364502 3.305990695953369 166.98211669921875
Loss :  1.6723934412002563 2.7572524547576904 139.53501892089844
Loss :  1.6814464330673218 2.859245538711548 144.6437225341797
Loss :  1.7220646142959595 2.935560941696167 148.50010681152344
  batch 60 loss: 1.7220646142959595, 2.935560941696167, 148.50010681152344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6730122566223145 3.702744960784912 186.81027221679688
Loss :  1.6921294927597046 3.2210588455200195 162.7450714111328
Loss :  1.6790210008621216 2.6903865337371826 136.19834899902344
Loss :  1.6726343631744385 3.0249972343444824 152.92250061035156
Loss :  1.659679651260376 2.524888277053833 127.90409851074219
Loss :  1.6838150024414062 4.318430423736572 217.6053466796875
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.691851019859314 4.2966084480285645 216.52227783203125
Loss :  1.6893895864486694 4.2070794105529785 212.04336547851562
Loss :  1.6938682794570923 4.186450958251953 211.0164031982422
Total LOSS train 156.21529775766226 valid 214.29684829711914
CE LOSS train 1.6868824793742254 valid 0.42346706986427307
Contrastive LOSS train 3.090568278386043 valid 1.0466127395629883
EPOCH 181:
Loss :  1.7005009651184082 2.8783621788024902 145.6186065673828
Loss :  1.7082781791687012 3.1055140495300293 156.98397827148438
Loss :  1.6889872550964355 2.8097259998321533 142.17527770996094
Loss :  1.6938040256500244 3.4147231578826904 172.42996215820312
Loss :  1.7034615278244019 2.798166513442993 141.61178588867188
Loss :  1.6811209917068481 2.8748672008514404 145.4244842529297
Loss :  1.7036073207855225 3.3535776138305664 169.3824920654297
Loss :  1.6876329183578491 2.693892002105713 136.38223266601562
Loss :  1.6826733350753784 3.0264017581939697 153.0027618408203
Loss :  1.7035179138183594 2.854583501815796 144.4326934814453
Loss :  1.6751480102539062 3.1264538764953613 157.99783325195312
Loss :  1.675068974494934 3.1091907024383545 157.1345977783203
Loss :  1.6729544401168823 2.9229109287261963 147.81851196289062
Loss :  1.6774909496307373 3.0613932609558105 154.7471466064453
Loss :  1.7125341892242432 3.089545249938965 156.18980407714844
Loss :  1.7091293334960938 3.5496418476104736 179.19122314453125
Loss :  1.6727131605148315 3.8430025577545166 193.82284545898438
Loss :  1.6893585920333862 3.3428475856781006 168.8317413330078
Loss :  1.6714259386062622 2.7215969562530518 137.75128173828125
Loss :  1.7102614641189575 3.360696315765381 169.74508666992188
  batch 20 loss: 1.7102614641189575, 3.360696315765381, 169.74508666992188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6874332427978516 2.6493093967437744 134.1529083251953
Loss :  1.6713238954544067 3.5627849102020264 179.81057739257812
Loss :  1.6827189922332764 2.854668378829956 144.4161376953125
Loss :  1.6923145055770874 3.1269407272338867 158.0393524169922
Loss :  1.7102130651474 3.333803415298462 168.400390625
Loss :  1.68468177318573 3.000364065170288 151.702880859375
Loss :  1.6901956796646118 3.0384976863861084 153.61508178710938
Loss :  1.6883082389831543 3.728384494781494 188.10752868652344
Loss :  1.655549168586731 3.006394624710083 151.97528076171875
Loss :  1.7091912031173706 2.8696906566619873 145.1937255859375
Loss :  1.6566795110702515 3.4267942905426025 172.99639892578125
Loss :  1.6995627880096436 3.152631998062134 159.3311767578125
Loss :  1.6838513612747192 4.025991916656494 202.9834442138672
Loss :  1.6834567785263062 3.0491445064544678 154.14068603515625
Loss :  1.6618438959121704 2.9006187915802 146.6927947998047
Loss :  1.6716290712356567 3.768878698348999 190.11557006835938
Loss :  1.6713508367538452 3.8605241775512695 194.69757080078125
Loss :  1.708413004875183 3.2189762592315674 162.6572265625
Loss :  1.7111530303955078 2.7925655841827393 141.3394317626953
Loss :  1.717443823814392 3.396847724914551 171.55982971191406
  batch 40 loss: 1.717443823814392, 3.396847724914551, 171.55982971191406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  1.6932059526443481 3.0313961505889893 153.2630157470703
Loss :  1.684982419013977 2.780735492706299 140.7217559814453
Loss :  1.6790425777435303 2.869999647140503 145.1790313720703
Loss :  1.6867281198501587 3.5016305446624756 176.76824951171875
Loss :  1.6739747524261475 3.394092321395874 171.37860107421875
Loss :  1.6916658878326416 3.3863296508789062 171.00814819335938
Loss :  1.7119464874267578 2.786590099334717 141.04144287109375
Loss :  1.682375431060791 3.0727627277374268 155.3205108642578
Loss :  1.720963954925537 3.4051854610443115 171.98023986816406
Loss :  1.685247778892517 3.280733585357666 165.721923828125
Loss :  1.7066611051559448 2.965374231338501 149.97537231445312
Loss :  1.702535629272461 2.8752567768096924 145.46536254882812
Loss :  1.689987063407898 2.799201011657715 141.65003967285156
Loss :  1.7074244022369385 2.8291923999786377 143.16705322265625
Loss :  1.6836639642715454 3.917893171310425 197.5783233642578
Loss :  1.72093665599823 3.0872225761413574 156.08206176757812
Loss :  1.6874967813491821 3.115111827850342 157.44308471679688
Loss :  1.678062915802002 2.846428155899048 143.99948120117188
Loss :  1.6866475343704224 3.222343683242798 162.8038330078125
Loss :  1.7249102592468262 3.0419037342071533 153.82009887695312
  batch 60 loss: 1.7249102592468262, 3.0419037342071533, 153.82009887695312
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6778385639190674 3.292670726776123 166.31137084960938
Loss :  1.695386290550232 2.648472309112549 134.11900329589844
Loss :  1.6818795204162598 2.6698522567749023 135.17449951171875
Loss :  1.6754573583602905 2.8054184913635254 141.94638061523438
Loss :  1.6622577905654907 2.368135452270508 120.06902313232422
Loss :  1.6850008964538574 4.29780912399292 216.57545471191406
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6928952932357788 4.20732307434082 212.05905151367188
Loss :  1.6904141902923584 4.216490268707275 212.51492309570312
Loss :  1.694807529449463 4.149045944213867 209.14710998535156
Total LOSS train 157.6090807401217 valid 212.57413482666016
CE LOSS train 1.6899732699761023 valid 0.4237018823623657
Contrastive LOSS train 3.1183821238004246 valid 1.0372614860534668
EPOCH 182:
Loss :  1.7018733024597168 2.6777143478393555 135.58758544921875
Loss :  1.7095868587493896 3.2403922080993652 163.72918701171875
Loss :  1.6906991004943848 2.735146999359131 138.44805908203125
Loss :  1.6956181526184082 2.647125720977783 134.05191040039062
Loss :  1.7052003145217896 3.3390307426452637 168.65673828125
Loss :  1.6837912797927856 2.9310781955718994 148.23770141601562
Loss :  1.7060633897781372 2.763291120529175 139.87062072753906
Loss :  1.6913259029388428 2.673628568649292 135.3727569580078
Loss :  1.6867746114730835 2.803745985031128 141.8740692138672
Loss :  1.706955075263977 3.4745004177093506 175.43197631835938
Loss :  1.6797912120819092 2.9468111991882324 149.02035522460938
Loss :  1.6805452108383179 2.7960243225097656 141.48175048828125
Loss :  1.6789758205413818 2.621018648147583 132.7299041748047
Loss :  1.6834983825683594 2.9566962718963623 149.518310546875
Loss :  1.7163546085357666 3.4789700508117676 175.66485595703125
Loss :  1.7134387493133545 3.60421085357666 181.92398071289062
Loss :  1.6782735586166382 2.7483222484588623 139.09437561035156
Loss :  1.6939232349395752 2.9320878982543945 148.29832458496094
Loss :  1.6757042407989502 2.789402723312378 141.14584350585938
Loss :  1.7127832174301147 2.839728355407715 143.69920349121094
  batch 20 loss: 1.7127832174301147, 2.839728355407715, 143.69920349121094
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6901402473449707 2.75506591796875 139.4434356689453
Loss :  1.6748392581939697 2.9519925117492676 149.2744598388672
Loss :  1.6858046054840088 3.175753116607666 160.4734649658203
Loss :  1.694703221321106 3.110836982727051 157.23655700683594
Loss :  1.7115871906280518 4.095459461212158 206.4845733642578
Loss :  1.6857123374938965 3.015601873397827 152.46580505371094
Loss :  1.6904658079147339 3.197899103164673 161.58541870117188
Loss :  1.6884877681732178 3.2738611698150635 165.3815460205078
Loss :  1.6549948453903198 2.90259051322937 146.78453063964844
Loss :  1.7086423635482788 2.979997396469116 150.70851135253906
Loss :  1.6555043458938599 3.46165132522583 174.73806762695312
Loss :  1.6979178190231323 3.4634673595428467 174.8712921142578
Loss :  1.6820316314697266 3.045410394668579 153.9525604248047
Loss :  1.6814961433410645 2.950120449066162 149.18753051757812
Loss :  1.6598196029663086 3.00246524810791 151.7830810546875
Loss :  1.6696302890777588 2.9895482063293457 151.1470489501953
Loss :  1.6699079275131226 3.0764689445495605 155.49334716796875
Loss :  1.708182454109192 3.1680386066436768 160.110107421875
Loss :  1.7103639841079712 3.0631470680236816 154.86770629882812
Loss :  1.7168484926223755 2.934584379196167 148.44606018066406
  batch 40 loss: 1.7168484926223755, 2.934584379196167, 148.44606018066406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6922335624694824 3.1297671794891357 158.1805877685547
Loss :  1.6828856468200684 2.84894061088562 144.1299285888672
Loss :  1.6761927604675293 3.751220464706421 189.23721313476562
Loss :  1.6836507320404053 2.8947763442993164 146.42247009277344
Loss :  1.669784665107727 3.0020699501037598 151.77328491210938
Loss :  1.688880443572998 2.984280586242676 150.9029083251953
Loss :  1.7092763185501099 2.931910991668701 148.30482482910156
Loss :  1.6789792776107788 3.006058692932129 151.98191833496094
Loss :  1.7183005809783936 3.5338938236236572 178.41299438476562
Loss :  1.682421088218689 3.3911824226379395 171.24154663085938
Loss :  1.7035332918167114 3.347184658050537 169.06277465820312
Loss :  1.69870924949646 3.070543050765991 155.22586059570312
Loss :  1.6859290599822998 3.0036637783050537 151.86911010742188
Loss :  1.7034553289413452 3.059879779815674 154.69744873046875
Loss :  1.6790368556976318 3.225048303604126 162.9314422607422
Loss :  1.7178199291229248 3.1488494873046875 159.16029357910156
Loss :  1.683207631111145 2.7456889152526855 138.9676513671875
Loss :  1.6732443571090698 2.819948434829712 142.67066955566406
Loss :  1.6827484369277954 3.373802661895752 170.37289428710938
Loss :  1.7228941917419434 2.6938934326171875 136.41757202148438
  batch 60 loss: 1.7228941917419434, 2.6938934326171875, 136.41757202148438
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6747092008590698 3.4153459072113037 172.44200134277344
Loss :  1.6933777332305908 3.6635043621063232 184.86859130859375
Loss :  1.6801931858062744 2.8367319107055664 143.51678466796875
Loss :  1.6730690002441406 3.1250038146972656 157.92324829101562
Loss :  1.6596323251724243 2.4900870323181152 126.16397857666016
Loss :  1.6906564235687256 4.4226908683776855 222.8251953125
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6987985372543335 4.3586530685424805 219.63143920898438
Loss :  1.6971039772033691 4.36823844909668 220.10902404785156
Loss :  1.7010502815246582 4.265471935272217 214.97463989257812
Total LOSS train 154.84847095196065 valid 219.38507461547852
CE LOSS train 1.689883406345661 valid 0.42526257038116455
Contrastive LOSS train 3.0631717461806076 valid 1.0663679838180542
EPOCH 183:
Loss :  1.700600266456604 3.237208366394043 163.56101989746094
Loss :  1.7088371515274048 2.9300575256347656 148.21170043945312
Loss :  1.689526081085205 3.606714963912964 182.02528381347656
Loss :  1.6941581964492798 2.7771692276000977 140.5526123046875
Loss :  1.7038512229919434 2.600647211074829 131.73622131347656
Loss :  1.6812164783477783 3.100975751876831 156.72999572753906
Loss :  1.7041161060333252 2.8468477725982666 144.0465087890625
Loss :  1.6883288621902466 2.9098918437957764 147.18292236328125
Loss :  1.6833853721618652 2.7502124309539795 139.19400024414062
Loss :  1.704332947731018 2.8627190589904785 144.8402862548828
Loss :  1.67630934715271 2.8797945976257324 145.66604614257812
Loss :  1.676957368850708 2.848536968231201 144.1038055419922
Loss :  1.6753252744674683 2.8986434936523438 146.6074981689453
Loss :  1.6798139810562134 3.1950597763061523 161.43280029296875
Loss :  1.7133930921554565 3.1295559406280518 158.19119262695312
Loss :  1.7101653814315796 2.9996163845062256 151.69097900390625
Loss :  1.6733275651931763 2.978848457336426 150.61575317382812
Loss :  1.6897366046905518 3.1576502323150635 159.57225036621094
Loss :  1.670514702796936 2.734713315963745 138.40618896484375
Loss :  1.709583044052124 2.763298988342285 139.87452697753906
  batch 20 loss: 1.709583044052124, 2.763298988342285, 139.87452697753906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6854897737503052 2.6304502487182617 133.20799255371094
Loss :  1.6687654256820679 2.908003807067871 147.06895446777344
Loss :  1.680296778678894 3.657700538635254 184.56532287597656
Loss :  1.6904497146606445 3.4329216480255127 173.33653259277344
Loss :  1.7087724208831787 3.01293683052063 152.35560607910156
Loss :  1.6821256875991821 3.5135464668273926 177.3594512939453
Loss :  1.687957525253296 2.808846950531006 142.13031005859375
Loss :  1.6851749420166016 3.1177282333374023 157.57159423828125
Loss :  1.6514720916748047 3.153503656387329 159.32666015625
Loss :  1.7071011066436768 3.4375641345977783 173.58531188964844
Loss :  1.6525638103485107 3.227299213409424 163.01751708984375
Loss :  1.696393609046936 3.057748556137085 154.5838165283203
Loss :  1.6801117658615112 3.117070436477661 157.53363037109375
Loss :  1.6797454357147217 3.4032669067382812 171.8430938720703
Loss :  1.6574574708938599 2.970797538757324 150.19732666015625
Loss :  1.6674352884292603 3.5574467182159424 179.53976440429688
Loss :  1.6674926280975342 2.893832206726074 146.35910034179688
Loss :  1.705955147743225 3.1770529747009277 160.55859375
Loss :  1.7090189456939697 2.9963014125823975 151.52407836914062
Loss :  1.7156891822814941 3.4940850734710693 176.41993713378906
  batch 40 loss: 1.7156891822814941, 3.4940850734710693, 176.41993713378906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.69089937210083 3.181349992752075 160.75840759277344
Loss :  1.6820170879364014 3.861422061920166 194.7531280517578
Loss :  1.6756517887115479 3.20967960357666 162.15963745117188
Loss :  1.6840298175811768 2.979964017868042 150.68223571777344
Loss :  1.6708345413208008 2.9472124576568604 149.03146362304688
Loss :  1.6898915767669678 3.711627721786499 187.2712860107422
Loss :  1.7109016180038452 2.729703187942505 138.19606018066406
Loss :  1.681187629699707 2.6287643909454346 133.11940002441406
Loss :  1.7202723026275635 2.8176186084747314 142.60121154785156
Loss :  1.684786319732666 2.7044758796691895 136.90858459472656
Loss :  1.7057535648345947 2.88962984085083 146.18724060058594
Loss :  1.7014942169189453 3.314683675765991 167.4356689453125
Loss :  1.6891813278198242 2.8870058059692383 146.0394744873047
Loss :  1.7061784267425537 3.112379312515259 157.3251495361328
Loss :  1.6821447610855103 3.1922266483306885 161.29347229003906
Loss :  1.718859076499939 3.5823373794555664 180.83572387695312
Loss :  1.6850085258483887 3.7547285556793213 189.42144775390625
Loss :  1.6743360757827759 3.0896408557891846 156.1563720703125
Loss :  1.683185338973999 3.4273197650909424 173.04916381835938
Loss :  1.7232478857040405 2.935120105743408 148.47926330566406
  batch 60 loss: 1.7232478857040405, 2.935120105743408, 148.47926330566406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6747417449951172 2.989008665084839 151.12518310546875
Loss :  1.693647861480713 3.1903576850891113 161.21153259277344
Loss :  1.6807036399841309 3.1217522621154785 157.76832580566406
Loss :  1.6737505197525024 3.23860764503479 163.60414123535156
Loss :  1.660387396812439 2.5360796451568604 128.46437072753906
Loss :  1.6803938150405884 4.328763008117676 218.11854553222656
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6882307529449463 4.383552074432373 220.86582946777344
Loss :  1.685606837272644 4.248889923095703 214.13009643554688
Loss :  1.6902852058410645 4.178491592407227 210.6148681640625
Total LOSS train 156.15698664738582 valid 215.93233489990234
CE LOSS train 1.688554464853727 valid 0.4225713014602661
Contrastive LOSS train 3.089368640459501 valid 1.0446228981018066
EPOCH 184:
Loss :  1.7004324197769165 2.856433629989624 144.52212524414062
Loss :  1.7086570262908936 2.8780646324157715 145.6118927001953
Loss :  1.6884647607803345 2.941092014312744 148.74305725097656
Loss :  1.6932340860366821 2.8841652870178223 145.90150451660156
Loss :  1.702736735343933 2.7875540256500244 141.0804443359375
Loss :  1.6807104349136353 2.823718786239624 142.8666534423828
Loss :  1.7036772966384888 2.8375625610351562 143.58180236816406
Loss :  1.6881003379821777 2.7589869499206543 139.63743591308594
Loss :  1.683682918548584 3.270070791244507 165.18722534179688
Loss :  1.7045114040374756 2.7884819507598877 141.1286163330078
Loss :  1.67593514919281 2.967310667037964 150.04147338867188
Loss :  1.6762046813964844 3.1857378482818604 160.96310424804688
Loss :  1.6742362976074219 2.8321163654327393 143.28005981445312
Loss :  1.6786892414093018 3.0637929439544678 154.86834716796875
Loss :  1.7139606475830078 3.676161766052246 185.5220489501953
Loss :  1.7100036144256592 2.7613370418548584 139.77685546875
Loss :  1.6749494075775146 3.980778932571411 200.71388244628906
Loss :  1.6919515132904053 2.8727619647979736 145.33004760742188
Loss :  1.6739994287490845 2.7285032272338867 138.09915161132812
Loss :  1.7119367122650146 2.851701021194458 144.29698181152344
  batch 20 loss: 1.7119367122650146, 2.851701021194458, 144.29698181152344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6885755062103271 2.9669442176818848 150.03578186035156
Loss :  1.6729463338851929 3.028148651123047 153.08038330078125
Loss :  1.6839268207550049 3.25730562210083 164.54920959472656
Loss :  1.693226933479309 2.955751657485962 149.48080444335938
Loss :  1.710924506187439 3.2978570461273193 166.60377502441406
Loss :  1.685062289237976 3.463263988494873 174.84825134277344
Loss :  1.6903756856918335 3.0695221424102783 155.16647338867188
Loss :  1.6883647441864014 2.973595142364502 150.36813354492188
Loss :  1.6550711393356323 3.321471929550171 167.72866821289062
Loss :  1.708992838859558 3.6585378646850586 184.63589477539062
Loss :  1.656052589416504 3.533909797668457 178.35154724121094
Loss :  1.698961615562439 3.1476054191589355 159.0792236328125
Loss :  1.6822621822357178 2.9257774353027344 147.9711456298828
Loss :  1.6808481216430664 2.8835737705230713 145.8595428466797
Loss :  1.65839421749115 3.027604103088379 153.03860473632812
Loss :  1.667284369468689 2.9331748485565186 148.32601928710938
Loss :  1.6665414571762085 3.06575608253479 154.954345703125
Loss :  1.7049179077148438 3.7551963329315186 189.4647216796875
Loss :  1.7071222066879272 3.0380516052246094 153.60971069335938
Loss :  1.7136552333831787 2.966153383255005 150.0213165283203
  batch 40 loss: 1.7136552333831787, 2.966153383255005, 150.0213165283203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6882137060165405 3.0599772930145264 154.6870880126953
Loss :  1.6789391040802002 4.484878063201904 225.92283630371094
Loss :  1.6724755764007568 3.1639962196350098 159.87228393554688
Loss :  1.681660532951355 3.827104330062866 193.036865234375
Loss :  1.6673967838287354 2.841298818588257 143.7323455810547
Loss :  1.6869866847991943 2.9864892959594727 151.01144409179688
Loss :  1.7086924314498901 2.8546833992004395 144.44287109375
Loss :  1.6776241064071655 4.153515338897705 209.3533935546875
Loss :  1.717670202255249 3.6315958499908447 183.29745483398438
Loss :  1.6807094812393188 3.3303070068359375 168.19606018066406
Loss :  1.7025532722473145 3.3847737312316895 170.94125366210938
Loss :  1.697876214981079 2.9836411476135254 150.8799285888672
Loss :  1.6849161386489868 3.0831167697906494 155.84075927734375
Loss :  1.7031333446502686 3.3313112258911133 168.26870727539062
Loss :  1.6781177520751953 3.207599401473999 162.05809020996094
Loss :  1.7175465822219849 3.3751022815704346 170.47265625
Loss :  1.682305932044983 3.5017623901367188 176.7704315185547
Loss :  1.672014832496643 2.9735188484191895 150.34796142578125
Loss :  1.6809403896331787 3.0922720432281494 156.29454040527344
Loss :  1.7223044633865356 3.7656476497650146 190.0046844482422
  batch 60 loss: 1.7223044633865356, 3.7656476497650146, 190.0046844482422
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  1.6717714071273804 3.438302516937256 173.58689880371094
Loss :  1.6910676956176758 2.822613000869751 142.82171630859375
Loss :  1.678231120109558 2.860402822494507 144.69837951660156
Loss :  1.6712055206298828 3.309133291244507 167.12786865234375
Loss :  1.658153772354126 2.5269789695739746 128.00709533691406
Loss :  1.6738314628601074 4.203919410705566 211.8697967529297
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  1.6816306114196777 4.190447807312012 211.20401000976562
Loss :  1.680726170539856 4.107141017913818 207.03778076171875
Loss :  1.6832242012023926 4.014057636260986 202.3861083984375
Total LOSS train 159.38461350661058 valid 208.1244239807129
CE LOSS train 1.6880321209247295 valid 0.42080605030059814
Contrastive LOSS train 3.1539316177368164 valid 1.0035144090652466
EPOCH 185:
Loss :  1.6994342803955078 2.86264705657959 144.831787109375
Loss :  1.7081016302108765 3.340566396713257 168.73641967773438
Loss :  1.689136266708374 2.7520065307617188 139.28945922851562
Loss :  1.69435453414917 3.2283036708831787 163.10952758789062
Loss :  1.7038748264312744 3.0277676582336426 153.09225463867188
Loss :  1.6820108890533447 2.72013258934021 137.68862915039062
Loss :  1.7039610147476196 3.571216106414795 180.2647705078125
Loss :  1.688501000404358 3.63207745552063 183.29237365722656
Loss :  1.682877779006958 3.8246471881866455 192.9152374267578
Loss :  1.70400869846344 2.842926025390625 143.85031127929688
Loss :  1.6760445833206177 3.0602214336395264 154.68711853027344
Loss :  1.675388216972351 3.0332860946655273 153.33969116210938
Loss :  1.6735981702804565 3.1005136966705322 156.69927978515625
Loss :  1.6778699159622192 3.3713388442993164 170.24481201171875
Loss :  1.7130287885665894 3.4120538234710693 172.3157196044922
Loss :  1.7094957828521729 2.8026938438415527 141.8441925048828
Loss :  1.6715623140335083 3.2829384803771973 165.81849670410156
Loss :  1.6879743337631226 3.0758657455444336 155.48126220703125
Loss :  1.6683013439178467 3.4044835567474365 171.89248657226562
Loss :  1.7092187404632568 2.932305097579956 148.324462890625
  batch 20 loss: 1.7092187404632568, 2.932305097579956, 148.324462890625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6847299337387085 2.730740785598755 138.2217559814453
Loss :  1.6677885055541992 3.3502039909362793 169.177978515625
Loss :  1.6798675060272217 2.7984678745269775 141.603271484375
Loss :  1.6906473636627197 3.5260400772094727 177.99264526367188
Loss :  1.7094746828079224 3.6212575435638428 182.77235412597656
Loss :  1.6829754114151 3.8404219150543213 193.70407104492188
Loss :  1.6893054246902466 3.0312962532043457 153.25411987304688
Loss :  1.686805248260498 3.3035645484924316 166.86502075195312
Loss :  1.653644323348999 2.8565430641174316 144.4807891845703
Loss :  1.708785891532898 2.9688656330108643 150.15206909179688
Loss :  1.6551464796066284 3.135319471359253 158.42112731933594
Loss :  1.6986435651779175 3.972287893295288 200.3130340576172
Loss :  1.6835631132125854 2.7309484481811523 138.23098754882812
Loss :  1.6836227178573608 2.8497819900512695 144.1727294921875
Loss :  1.6619541645050049 3.1089794635772705 157.11093139648438
Loss :  1.6714938879013062 3.2583558559417725 164.5892791748047
Loss :  1.6710907220840454 3.6389479637145996 183.61849975585938
Loss :  1.708387851715088 2.967992067337036 150.1079864501953
Loss :  1.7107590436935425 3.047062873840332 154.06390380859375
Loss :  1.7170015573501587 2.7873787879943848 141.0859375
  batch 40 loss: 1.7170015573501587, 2.7873787879943848, 141.0859375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  1.6922475099563599 3.2016472816467285 161.7746124267578
Loss :  1.6836225986480713 2.6151461601257324 132.44093322753906
Loss :  1.6774853467941284 3.1047439575195312 156.91468811035156
Loss :  1.685428261756897 3.108856439590454 157.1282501220703
Loss :  1.6725094318389893 2.690263271331787 136.18568420410156
Loss :  1.69082772731781 3.20774507522583 162.0780792236328
Loss :  1.7112938165664673 2.7025563716888428 136.839111328125
Loss :  1.6819454431533813 3.5444271564483643 178.90330505371094
Loss :  1.7205828428268433 2.939896821975708 148.71542358398438
Loss :  1.6855460405349731 2.709667921066284 137.1689453125
Loss :  1.7058461904525757 2.9782750606536865 150.61959838867188
Loss :  1.7016093730926514 2.941075563430786 148.75538635253906
Loss :  1.6889371871948242 3.0361626148223877 153.4970703125
Loss :  1.7057362794876099 3.583319664001465 180.87171936035156
Loss :  1.681958556175232 2.78170108795166 140.7670135498047
Loss :  1.7189216613769531 3.16975998878479 160.20692443847656
Loss :  1.685149073600769 3.0191969871520996 152.64500427246094
Loss :  1.6748607158660889 3.4415900707244873 173.75436401367188
Loss :  1.6837856769561768 3.057621717453003 154.56488037109375
Loss :  1.7234457731246948 2.828871965408325 143.16705322265625
  batch 60 loss: 1.7234457731246948, 2.828871965408325, 143.16705322265625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  1.6757699251174927 3.6895296573638916 186.15225219726562
Loss :  1.6945382356643677 3.46393084526062 174.89108276367188
Loss :  1.681022047996521 2.6423699855804443 133.7995147705078
Loss :  1.6746554374694824 3.196791410446167 161.51422119140625
Loss :  1.6617594957351685 2.5555899143218994 129.44125366210938
Loss :  1.6877100467681885 3.96183705329895 199.77957153320312
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.695552110671997 3.954545259475708 199.42282104492188
Loss :  1.6932871341705322 3.749680995941162 189.17733764648438
Loss :  1.6974081993103027 3.7281229496002197 188.10354614257812
Total LOSS train 157.8531254695012 valid 194.12081909179688
CE LOSS train 1.689137156193073 valid 0.4243520498275757
Contrastive LOSS train 3.1232797659360445 valid 0.9320307374000549
EPOCH 186:
Loss :  1.7021417617797852 2.871389627456665 145.27163696289062
Loss :  1.7103488445281982 3.9991204738616943 201.66636657714844
Loss :  1.6912285089492798 2.7990524768829346 141.6438446044922
Loss :  1.6958057880401611 3.167656898498535 160.07864379882812
Loss :  1.7049145698547363 2.63545298576355 133.47755432128906
Loss :  1.6825698614120483 2.5679731369018555 130.0812225341797
Loss :  1.7050886154174805 3.0425379276275635 153.8319854736328
Loss :  1.6898163557052612 2.8638880252838135 144.88421630859375
Loss :  1.6849031448364258 2.7520053386688232 139.28517150878906
Loss :  1.70490562915802 2.7685399055480957 140.1319122314453
Loss :  1.677234172821045 3.022784948348999 152.8164825439453
Loss :  1.6766843795776367 2.9580676555633545 149.58006286621094
Loss :  1.6739497184753418 2.9553513526916504 149.44151306152344
Loss :  1.677859902381897 3.1223044395446777 157.79307556152344
Loss :  1.7133522033691406 3.2320361137390137 163.31515502929688
Loss :  1.708304524421692 2.9131951332092285 147.36805725097656
Loss :  1.6725682020187378 2.7092742919921875 137.1362762451172
Loss :  1.6892638206481934 2.837207555770874 143.54965209960938
Loss :  1.6706539392471313 3.0912246704101562 156.2318878173828
Loss :  1.7093538045883179 3.2165987491607666 162.53929138183594
  batch 20 loss: 1.7093538045883179, 3.2165987491607666, 162.53929138183594
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.685600757598877 3.0350077152252197 153.4359893798828
Loss :  1.6689441204071045 3.093029260635376 156.32040405273438
Loss :  1.6805027723312378 3.0301055908203125 153.18577575683594
Loss :  1.690261960029602 2.915456533432007 147.4630889892578
Loss :  1.7085747718811035 3.2628774642944336 164.85244750976562
Loss :  1.6822882890701294 3.8047935962677 191.92196655273438
Loss :  1.687735915184021 2.9335522651672363 148.36534118652344
Loss :  1.6853448152542114 2.838925361633301 143.63162231445312
Loss :  1.651902675628662 3.159257173538208 159.61476135253906
Loss :  1.7072243690490723 3.1506450176239014 159.23948669433594
Loss :  1.653225064277649 3.4963736534118652 176.47190856933594
Loss :  1.6968907117843628 3.1263766288757324 158.01571655273438
Loss :  1.6803559064865112 3.1586883068084717 159.61477661132812
Loss :  1.6796975135803223 3.6665971279144287 185.00955200195312
Loss :  1.6571675539016724 2.9120919704437256 147.2617645263672
Loss :  1.6671465635299683 3.210618734359741 162.1980743408203
Loss :  1.6667876243591309 2.9805760383605957 150.6956024169922
Loss :  1.7050127983093262 2.9332945346832275 148.3697509765625
Loss :  1.708003282546997 3.207557439804077 162.08587646484375
Loss :  1.7145874500274658 2.702124834060669 136.82083129882812
  batch 40 loss: 1.7145874500274658, 2.702124834060669, 136.82083129882812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6891982555389404 2.803147792816162 141.84658813476562
Loss :  1.680485725402832 3.4084486961364746 172.10292053222656
Loss :  1.6740479469299316 2.889997959136963 146.17393493652344
Loss :  1.6824767589569092 3.283062219619751 165.83558654785156
Loss :  1.6695797443389893 2.599041700363159 131.62167358398438
Loss :  1.6886954307556152 2.7500863075256348 139.19300842285156
Loss :  1.7096858024597168 2.7734081745147705 140.38009643554688
Loss :  1.6797062158584595 2.6253018379211426 132.94479370117188
Loss :  1.719323754310608 2.9746181964874268 150.4502410888672
Loss :  1.6838055849075317 3.554466962814331 179.4071502685547
Loss :  1.7045025825500488 2.9162511825561523 147.51705932617188
Loss :  1.7001038789749146 2.7526655197143555 139.3333740234375
Loss :  1.6869128942489624 2.766306161880493 140.00222778320312
Loss :  1.7042713165283203 3.106980800628662 157.05331420898438
Loss :  1.6795413494110107 2.9346399307250977 148.41152954101562
Loss :  1.7184336185455322 3.4247019290924072 172.95352172851562
Loss :  1.6836190223693848 3.026965379714966 153.03189086914062
Loss :  1.6730444431304932 2.8251566886901855 142.93087768554688
Loss :  1.6819170713424683 3.1933951377868652 161.35166931152344
Loss :  1.7225342988967896 2.7374463081359863 138.5948486328125
  batch 60 loss: 1.7225342988967896, 2.7374463081359863, 138.5948486328125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6728990077972412 2.9027209281921387 146.80894470214844
Loss :  1.6917812824249268 3.3527791500091553 169.33074951171875
Loss :  1.6790683269500732 3.4308862686157227 173.22337341308594
Loss :  1.6718615293502808 3.5242295265197754 177.88333129882812
Loss :  1.6592365503311157 2.4994966983795166 126.63407135009766
Loss :  1.6853948831558228 3.6866302490234375 186.01690673828125
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6937320232391357 3.738935708999634 188.64051818847656
Loss :  1.6911633014678955 3.6613974571228027 184.7610321044922
Loss :  1.6962590217590332 3.447132110595703 174.0528564453125
Total LOSS train 153.38023881178637 valid 183.36782836914062
CE LOSS train 1.6880759239196776 valid 0.4240647554397583
Contrastive LOSS train 3.0338432678809535 valid 0.8617830276489258
EPOCH 187:
Loss :  1.70071280002594 3.016385078430176 152.5199737548828
Loss :  1.7091598510742188 2.9444100856781006 148.92965698242188
Loss :  1.6900020837783813 3.0843241214752197 155.9062042236328
Loss :  1.6950421333312988 3.585336923599243 180.96188354492188
Loss :  1.7047518491744995 2.5741169452667236 130.4105987548828
Loss :  1.681785225868225 2.799018383026123 141.6326904296875
Loss :  1.704388976097107 2.9278018474578857 148.094482421875
Loss :  1.6885013580322266 3.2003042697906494 161.70372009277344
Loss :  1.6832948923110962 3.490410566329956 176.2038116455078
Loss :  1.7041513919830322 2.7271811962127686 138.06320190429688
Loss :  1.6760011911392212 3.1619443893432617 159.77320861816406
Loss :  1.675826907157898 3.1072003841400146 157.0358428955078
Loss :  1.673329472541809 2.9240520000457764 147.8759307861328
Loss :  1.6777074337005615 3.1830973625183105 160.8325653076172
Loss :  1.7125223875045776 2.8912441730499268 146.2747344970703
Loss :  1.708337664604187 3.0801174640655518 155.71421813964844
Loss :  1.6710689067840576 3.501814126968384 176.76177978515625
Loss :  1.6881457567214966 2.907589912414551 147.06764221191406
Loss :  1.6686700582504272 2.5816798210144043 130.75265502929688
Loss :  1.7091336250305176 2.907635450363159 147.09091186523438
  batch 20 loss: 1.7091336250305176, 2.907635450363159, 147.09091186523438
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6852741241455078 3.1418535709381104 158.7779541015625
Loss :  1.6683269739151 2.9079222679138184 147.06443786621094
Loss :  1.6795215606689453 2.7978999614715576 141.57452392578125
Loss :  1.690663456916809 3.1398813724517822 158.68472290039062
Loss :  1.709199070930481 4.245282173156738 213.9733123779297
Loss :  1.6805150508880615 3.6242666244506836 182.8938446044922
Loss :  1.6867053508758545 2.9148001670837402 147.4267120361328
Loss :  1.6828408241271973 3.10243821144104 156.8047637939453
Loss :  1.6482577323913574 3.0875332355499268 156.02491760253906
Loss :  1.7056530714035034 2.9681739807128906 150.11434936523438
Loss :  1.6491769552230835 3.032264232635498 153.2623748779297
Loss :  1.6947176456451416 2.977348566055298 150.56214904785156
Loss :  1.6796451807022095 3.280425786972046 165.700927734375
Loss :  1.6790050268173218 3.22501802444458 162.92990112304688
Loss :  1.6550040245056152 3.1963908672332764 161.47454833984375
Loss :  1.6653873920440674 3.085991144180298 155.96495056152344
Loss :  1.664642095565796 3.1373045444488525 158.52987670898438
Loss :  1.7030624151229858 2.696754217147827 136.540771484375
Loss :  1.7069969177246094 2.8686423301696777 145.1391143798828
Loss :  1.7130094766616821 3.413661479949951 172.39608764648438
  batch 40 loss: 1.7130094766616821, 3.413661479949951, 172.39608764648438
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  1.6864113807678223 3.218877077102661 162.63026428222656
Loss :  1.6768887042999268 2.7448058128356934 138.9171905517578
Loss :  1.6693706512451172 3.4304583072662354 173.19229125976562
Loss :  1.6783630847930908 3.1376402378082275 158.5603790283203
Loss :  1.663652777671814 2.99191951751709 151.25962829589844
Loss :  1.6828513145446777 2.9879872798919678 151.08221435546875
Loss :  1.705383062362671 2.910456418991089 147.22821044921875
Loss :  1.6721891164779663 2.8405239582061768 143.69839477539062
Loss :  1.714866042137146 2.974403142929077 150.4350128173828
Loss :  1.6758581399917603 3.4376111030578613 173.55641174316406
Loss :  1.6986337900161743 3.296478509902954 166.52256774902344
Loss :  1.6948258876800537 2.8487167358398438 144.1306610107422
Loss :  1.6807969808578491 2.655925989151001 134.4770965576172
Loss :  1.7000888586044312 3.0307250022888184 153.23634338378906
Loss :  1.6733359098434448 2.9186646938323975 147.6065673828125
Loss :  1.7158311605453491 2.975214719772339 150.47657775878906
Loss :  1.6785286664962769 3.4676573276519775 175.0614013671875
Loss :  1.6679843664169312 3.749042272567749 189.12010192871094
Loss :  1.6780096292495728 3.96545672416687 199.9508514404297
Loss :  1.7210850715637207 2.8221585750579834 142.82901000976562
  batch 60 loss: 1.7210850715637207, 2.8221585750579834, 142.82901000976562
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6688392162322998 3.1117796897888184 157.25782775878906
Loss :  1.6880358457565308 3.7977492809295654 191.57550048828125
Loss :  1.675004005432129 3.0145621299743652 152.40310668945312
Loss :  1.6664118766784668 3.1731138229370117 160.3220977783203
Loss :  1.653314471244812 2.98125958442688 150.71629333496094
Loss :  1.6780647039413452 4.405096054077148 221.93287658691406
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6865788698196411 4.434291362762451 223.40115356445312
Loss :  1.6829960346221924 4.231313705444336 213.24868774414062
Loss :  1.689078450202942 4.142266750335693 208.8024139404297
Total LOSS train 157.0106454702524 valid 216.84628295898438
CE LOSS train 1.6854876664968637 valid 0.4222696125507355
Contrastive LOSS train 3.1065031565152683 valid 1.0355666875839233
EPOCH 188:
Loss :  1.6966030597686768 3.468855619430542 175.13938903808594
Loss :  1.7053521871566772 3.3275179862976074 168.0812530517578
Loss :  1.684754490852356 3.1590943336486816 159.63946533203125
Loss :  1.6895867586135864 2.8952066898345947 146.44992065429688
Loss :  1.6997973918914795 2.863140106201172 144.8568115234375
Loss :  1.6764864921569824 2.932032346725464 148.27810668945312
Loss :  1.6996967792510986 3.7272610664367676 188.062744140625
Loss :  1.6837546825408936 2.938380479812622 148.602783203125
Loss :  1.678056001663208 3.1538069248199463 159.368408203125
Loss :  1.700647234916687 3.1891567707061768 161.1584930419922
Loss :  1.6708518266677856 3.362166404724121 169.7791748046875
Loss :  1.6710222959518433 2.893375873565674 146.33981323242188
Loss :  1.669468879699707 2.8943145275115967 146.38519287109375
Loss :  1.6743009090423584 2.7640442848205566 139.8765106201172
Loss :  1.7103239297866821 2.9737870693206787 150.39967346191406
Loss :  1.7078027725219727 2.8069918155670166 142.05740356445312
Loss :  1.6694906949996948 2.8313677310943604 143.23788452148438
Loss :  1.6875814199447632 2.878662347793579 145.62069702148438
Loss :  1.6681689023971558 2.8084664344787598 142.09149169921875
Loss :  1.7076163291931152 3.3521883487701416 169.31703186035156
  batch 20 loss: 1.7076163291931152, 3.3521883487701416, 169.31703186035156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6828798055648804 2.6131296157836914 132.33935546875
Loss :  1.6656978130340576 2.6220929622650146 132.7703399658203
Loss :  1.6774132251739502 2.684523105621338 135.903564453125
Loss :  1.688369631767273 3.032261371612549 153.3014373779297
Loss :  1.7069700956344604 2.9719860553741455 150.3062744140625
Loss :  1.6790400743484497 3.8096745014190674 192.1627655029297
Loss :  1.684916615486145 2.6904520988464355 136.20751953125
Loss :  1.681810736656189 3.045414686203003 153.95254516601562
Loss :  1.6474783420562744 3.3599843978881836 169.64669799804688
Loss :  1.7049314975738525 3.420353412628174 172.72259521484375
Loss :  1.6481664180755615 3.306635856628418 166.97994995117188
Loss :  1.6938159465789795 3.0374228954315186 153.56495666503906
Loss :  1.6769475936889648 3.160003900527954 159.67713928222656
Loss :  1.6766201257705688 3.3730342388153076 170.32833862304688
Loss :  1.651650071144104 3.8048689365386963 191.89511108398438
Loss :  1.6618387699127197 3.140441417694092 158.68389892578125
Loss :  1.661226511001587 2.8664474487304688 144.9835968017578
Loss :  1.7023601531982422 2.6547632217407227 134.44052124023438
Loss :  1.7062610387802124 3.6830947399139404 185.8610076904297
Loss :  1.7125517129898071 3.082354784011841 155.83029174804688
  batch 40 loss: 1.7125517129898071, 3.082354784011841, 155.83029174804688
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6850796937942505 3.013019561767578 152.33604431152344
Loss :  1.675369143486023 3.4961421489715576 176.48248291015625
Loss :  1.6676607131958008 2.755863666534424 139.46084594726562
Loss :  1.6765308380126953 2.838329315185547 143.59300231933594
Loss :  1.6623749732971191 3.1048858165740967 156.9066619873047
Loss :  1.6837044954299927 2.7889363765716553 141.13052368164062
Loss :  1.7064768075942993 2.7456207275390625 138.98751831054688
Loss :  1.6732985973358154 3.1176958084106445 157.55809020996094
Loss :  1.7160403728485107 2.9914424419403076 151.2881622314453
Loss :  1.6774494647979736 2.860203742980957 144.68763732910156
Loss :  1.6997911930084229 3.212998151779175 162.34970092773438
Loss :  1.6961814165115356 3.269880533218384 165.19021606445312
Loss :  1.6825189590454102 3.0340397357940674 153.38450622558594
Loss :  1.7020243406295776 3.024710178375244 152.93753051757812
Loss :  1.6755566596984863 3.4647653102874756 174.913818359375
Loss :  1.717240571975708 3.170074224472046 160.220947265625
Loss :  1.6806738376617432 3.3066887855529785 167.01512145996094
Loss :  1.6695891618728638 3.178373098373413 160.58824157714844
Loss :  1.6792948246002197 3.4230008125305176 172.82933044433594
Loss :  1.7218862771987915 2.8273890018463135 143.09133911132812
  batch 60 loss: 1.7218862771987915, 2.8273890018463135, 143.09133911132812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6703962087631226 3.0027482509613037 151.80780029296875
Loss :  1.6897379159927368 3.1892662048339844 161.15306091308594
Loss :  1.6767288446426392 3.156352996826172 159.494384765625
Loss :  1.6696081161499023 3.2711710929870605 165.2281494140625
Loss :  1.6561709642410278 2.721055269241333 137.7089385986328
Loss :  1.68741774559021 4.371475696563721 220.26121520996094
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.696522831916809 4.378510475158691 220.62203979492188
Loss :  1.6919151544570923 4.314566612243652 217.4202423095703
Loss :  1.6988990306854248 4.244897842407227 213.94378662109375
Total LOSS train 155.94837247408353 valid 218.06182098388672
CE LOSS train 1.6842106709113487 valid 0.4247247576713562
Contrastive LOSS train 3.0852832317352297 valid 1.0612244606018066
EPOCH 189:
Loss :  1.6981139183044434 3.051560640335083 154.27615356445312
Loss :  1.7063642740249634 3.410569667816162 172.23484802246094
Loss :  1.6866984367370605 3.4488141536712646 174.1273956298828
Loss :  1.6920026540756226 3.1304428577423096 158.21414184570312
Loss :  1.7021478414535522 2.6532959938049316 134.366943359375
Loss :  1.6797109842300415 2.9622409343719482 149.79176330566406
Loss :  1.7026516199111938 3.023334503173828 152.86936950683594
Loss :  1.6867517232894897 2.849929094314575 144.18321228027344
Loss :  1.6813668012619019 2.881580114364624 145.7603759765625
Loss :  1.7032749652862549 3.1319098472595215 158.29876708984375
Loss :  1.6743407249450684 3.3459861278533936 168.97364807128906
Loss :  1.6739202737808228 2.9778826236724854 150.56805419921875
Loss :  1.6727395057678223 2.6699066162109375 135.16807556152344
Loss :  1.6769393682479858 2.7498106956481934 139.16748046875
Loss :  1.712174654006958 3.1726861000061035 160.3464813232422
Loss :  1.7097256183624268 2.8474173545837402 144.08059692382812
Loss :  1.6710364818572998 3.6937782764434814 186.35995483398438
Loss :  1.6888017654418945 3.1069960594177246 157.03860473632812
Loss :  1.6697453260421753 2.9438977241516113 148.8646240234375
Loss :  1.709068775177002 3.094332456588745 156.4257049560547
  batch 20 loss: 1.709068775177002, 3.094332456588745, 156.4257049560547
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6845654249191284 3.299170970916748 166.6431121826172
Loss :  1.6674394607543945 3.1335675716400146 158.34580993652344
Loss :  1.6781288385391235 3.769040584564209 190.13015747070312
Loss :  1.6894246339797974 2.965409994125366 149.9599151611328
Loss :  1.7083548307418823 2.933443307876587 148.38052368164062
Loss :  1.6799333095550537 2.9971272945404053 151.5363006591797
Loss :  1.686812400817871 3.0935213565826416 156.36288452148438
Loss :  1.6827561855316162 2.829071283340454 143.13632202148438
Loss :  1.6490081548690796 2.9026565551757812 146.78182983398438
Loss :  1.7059381008148193 3.796295166015625 191.52069091796875
Loss :  1.6488940715789795 3.0169949531555176 152.49864196777344
Loss :  1.693853497505188 3.317828893661499 167.58529663085938
Loss :  1.6773697137832642 2.883772611618042 145.86599731445312
Loss :  1.6760233640670776 2.970402717590332 150.1961669921875
Loss :  1.651042103767395 3.0830743312835693 155.8047637939453
Loss :  1.6614148616790771 3.247792959213257 164.05105590820312
Loss :  1.661102056503296 3.54101300239563 178.7117462158203
Loss :  1.70144522190094 2.8643808364868164 144.9204864501953
Loss :  1.7054822444915771 3.2782585620880127 165.618408203125
Loss :  1.7117661237716675 3.3571012020111084 169.5668182373047
  batch 40 loss: 1.7117661237716675, 3.3571012020111084, 169.5668182373047
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.68482506275177 3.0807673931121826 155.72320556640625
Loss :  1.6752837896347046 3.0284550189971924 153.0980224609375
Loss :  1.6678436994552612 3.154571533203125 159.39642333984375
Loss :  1.676914095878601 3.53460431098938 178.40711975097656
Loss :  1.6630115509033203 2.519683599472046 127.64718627929688
Loss :  1.683755874633789 3.339150905609131 168.6413116455078
Loss :  1.7066431045532227 2.7780253887176514 140.60792541503906
Loss :  1.6732207536697388 3.0665786266326904 155.0021514892578
Loss :  1.715714454650879 2.9374353885650635 148.5874786376953
Loss :  1.6770939826965332 3.3335092067718506 168.35255432128906
Loss :  1.6993868350982666 3.1153314113616943 157.46595764160156
Loss :  1.695594310760498 2.8602893352508545 144.71005249023438
Loss :  1.6816052198410034 2.8111801147460938 142.24061584472656
Loss :  1.7007137537002563 3.042752504348755 153.8383331298828
Loss :  1.67336106300354 2.9415018558502197 148.74844360351562
Loss :  1.715459942817688 3.1588797569274902 159.65943908691406
Loss :  1.6783242225646973 2.9337892532348633 148.3677978515625
Loss :  1.6672203540802002 4.099686622619629 206.65155029296875
Loss :  1.6776748895645142 3.2478277683258057 164.06906127929688
Loss :  1.7208510637283325 3.3858728408813477 171.01449584960938
  batch 60 loss: 1.7208510637283325, 3.3858728408813477, 171.01449584960938
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6683272123336792 2.8619065284729004 144.76364135742188
Loss :  1.6879925727844238 3.198780059814453 161.62698364257812
Loss :  1.6743159294128418 2.6718928813934326 135.2689666748047
Loss :  1.667790174484253 3.052597999572754 154.2976837158203
Loss :  1.654338002204895 2.8443031311035156 143.86949157714844
Loss :  1.6767266988754272 3.9296226501464844 198.15786743164062
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6855669021606445 4.0193305015563965 202.65208435058594
Loss :  1.6827102899551392 3.8373544216156006 193.55043029785156
Loss :  1.6877682209014893 3.839447259902954 193.66014099121094
Total LOSS train 156.62752333420974 valid 197.00513076782227
CE LOSS train 1.684701418876648 valid 0.4219420552253723
Contrastive LOSS train 3.098856452795175 valid 0.9598618149757385
EPOCH 190:
Loss :  1.6973142623901367 3.717874050140381 187.5910186767578
Loss :  1.7060436010360718 3.054511785507202 154.43162536621094
Loss :  1.6869736909866333 2.7476437091827393 139.0691680908203
Loss :  1.691799283027649 2.9067585468292236 147.02972412109375
Loss :  1.702656865119934 2.65004301071167 134.20480346679688
Loss :  1.6799813508987427 2.8038418292999268 141.8720703125
Loss :  1.7026375532150269 3.1804323196411133 160.72425842285156
Loss :  1.68659245967865 3.1819112300872803 160.78216552734375
Loss :  1.68088960647583 3.235144853591919 163.43814086914062
Loss :  1.7024787664413452 3.6626815795898438 184.83656311035156
Loss :  1.6737884283065796 3.6313297748565674 183.24026489257812
Loss :  1.6734578609466553 3.5928165912628174 181.3142852783203
Loss :  1.671713948249817 2.7824418544769287 140.79379272460938
Loss :  1.6756095886230469 2.973393201828003 150.34527587890625
Loss :  1.7106983661651611 3.322002649307251 167.81082153320312
Loss :  1.7079026699066162 3.038745403289795 153.64517211914062
Loss :  1.6682363748550415 3.034313678741455 153.38392639160156
Loss :  1.6856510639190674 3.4413669109344482 173.75399780273438
Loss :  1.6654753684997559 3.013888120651245 152.35989379882812
Loss :  1.7065672874450684 3.1980292797088623 161.6080322265625
  batch 20 loss: 1.7065672874450684, 3.1980292797088623, 161.6080322265625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6820809841156006 2.784736394882202 140.9188995361328
Loss :  1.664549469947815 2.8886494636535645 146.09703063964844
Loss :  1.6767289638519287 2.7319650650024414 138.2749786376953
Loss :  1.6882648468017578 2.838718891143799 143.62420654296875
Loss :  1.7077606916427612 3.095000982284546 156.4578094482422
Loss :  1.679875135421753 3.0289106369018555 153.12539672851562
Loss :  1.6864889860153198 2.9088406562805176 147.12852478027344
Loss :  1.683005928993225 3.156808376312256 159.52342224121094
Loss :  1.6489129066467285 2.8060688972473145 141.95236206054688
Loss :  1.7057006359100342 2.8542916774749756 144.4202880859375
Loss :  1.6499906778335571 3.463794231414795 174.83970642089844
Loss :  1.6950147151947021 3.262680768966675 164.82904052734375
Loss :  1.679504156112671 2.990722417831421 151.2156219482422
Loss :  1.6791002750396729 2.9040091037750244 146.8795623779297
Loss :  1.655910849571228 2.960357666015625 149.67379760742188
Loss :  1.6661549806594849 2.9406092166900635 148.6966094970703
Loss :  1.665830373764038 3.3354878425598145 168.4402313232422
Loss :  1.705769658088684 2.827359437942505 143.07373046875
Loss :  1.7085741758346558 2.861250638961792 144.77110290527344
Loss :  1.7147495746612549 3.4863994121551514 176.03472900390625
  batch 40 loss: 1.7147495746612549, 3.4863994121551514, 176.03472900390625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.68827223777771 3.4730372428894043 175.3401336669922
Loss :  1.6788043975830078 3.3964076042175293 171.49917602539062
Loss :  1.6711931228637695 2.800307512283325 141.6865692138672
Loss :  1.6790695190429688 3.7546305656433105 189.41058349609375
Loss :  1.665409803390503 2.699441432952881 136.63748168945312
Loss :  1.686209797859192 2.938676595687866 148.62002563476562
Loss :  1.7083162069320679 3.5023679733276367 176.8267059326172
Loss :  1.6754237413406372 2.8707046508789062 145.21066284179688
Loss :  1.7178376913070679 3.4350342750549316 173.46954345703125
Loss :  1.6798402070999146 2.8532516956329346 144.34242248535156
Loss :  1.701737880706787 2.854346990585327 144.41908264160156
Loss :  1.6985136270523071 3.830284595489502 193.21275329589844
Loss :  1.6847361326217651 2.9438869953155518 148.87908935546875
Loss :  1.7038626670837402 2.9091436862945557 147.16104125976562
Loss :  1.6775226593017578 3.0856058597564697 155.9578094482422
Loss :  1.7188459634780884 3.0049614906311035 151.9669189453125
Loss :  1.6824259757995605 2.87963604927063 145.66421508789062
Loss :  1.6714764833450317 2.9083549976348877 147.0892333984375
Loss :  1.6813089847564697 3.150775909423828 159.2200927734375
Loss :  1.7233643531799316 2.941248893737793 148.78579711914062
  batch 60 loss: 1.7233643531799316, 2.941248893737793, 148.78579711914062
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6720592975616455 3.0376992225646973 153.55702209472656
Loss :  1.6906601190567017 3.8736088275909424 195.37109375
Loss :  1.6771396398544312 2.6241984367370605 132.88705444335938
Loss :  1.669817328453064 2.9117372035980225 147.2566680908203
Loss :  1.6561528444290161 2.496504783630371 126.48139190673828
Loss :  1.6754989624023438 4.154233932495117 209.38720703125
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6843478679656982 4.150052547454834 209.1869659423828
Loss :  1.6813219785690308 4.031158447265625 203.23924255371094
Loss :  1.6864173412322998 3.8269028663635254 193.03155517578125
Total LOSS train 155.89484029916616 valid 203.71124267578125
CE LOSS train 1.6858528779103206 valid 0.42160433530807495
Contrastive LOSS train 3.0841797791994536 valid 0.9567257165908813
EPOCH 191:
Loss :  1.697981357574463 3.0414907932281494 153.77252197265625
Loss :  1.7065746784210205 3.307262897491455 167.06971740722656
Loss :  1.6873725652694702 2.816263437271118 142.50054931640625
Loss :  1.692210078239441 3.2188992500305176 162.63717651367188
Loss :  1.7026344537734985 2.68558406829834 135.98184204101562
Loss :  1.6798770427703857 3.8145835399627686 192.40904235839844
Loss :  1.7027000188827515 3.0511577129364014 154.26058959960938
Loss :  1.6865253448486328 3.5976476669311523 181.56890869140625
Loss :  1.6806224584579468 3.2629706859588623 164.8291473388672
Loss :  1.7027837038040161 2.8365626335144043 143.53091430664062
Loss :  1.6729247570037842 3.12251353263855 157.79859924316406
Loss :  1.6725258827209473 3.1591007709503174 159.6275634765625
Loss :  1.6706910133361816 3.176445722579956 160.4929656982422
Loss :  1.674888253211975 3.5994925498962402 181.64950561523438
Loss :  1.710508108139038 3.0606071949005127 154.74087524414062
Loss :  1.7082242965698242 3.2794442176818848 165.68043518066406
Loss :  1.6681938171386719 3.075805902481079 155.45849609375
Loss :  1.6854832172393799 2.892211437225342 146.29605102539062
Loss :  1.6652008295059204 3.28092360496521 165.7113800048828
Loss :  1.7072951793670654 2.7111520767211914 137.264892578125
  batch 20 loss: 1.7072951793670654, 2.7111520767211914, 137.264892578125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6825170516967773 2.841520071029663 143.75851440429688
Loss :  1.6651867628097534 2.8495967388153076 144.14503479003906
Loss :  1.67741858959198 2.776691436767578 140.51197814941406
Loss :  1.6889888048171997 3.0183591842651367 152.60694885253906
Loss :  1.7086011171340942 3.376314640045166 170.5243377685547
Loss :  1.6809879541397095 3.1552586555480957 159.4439239501953
Loss :  1.6876132488250732 3.409147024154663 172.14495849609375
Loss :  1.6841517686843872 3.05719256401062 154.54379272460938
Loss :  1.6506270170211792 2.901301860809326 146.71571350097656
Loss :  1.7069323062896729 3.513852119445801 177.39955139160156
Loss :  1.6512272357940674 4.196455001831055 211.47398376464844
Loss :  1.6961348056793213 3.2529313564300537 164.3426971435547
Loss :  1.680415153503418 3.282747745513916 165.81781005859375
Loss :  1.679376482963562 3.1188249588012695 157.62063598632812
Loss :  1.6552737951278687 3.233154296875 163.31298828125
Loss :  1.665278673171997 4.194319248199463 211.38124084472656
Loss :  1.6644402742385864 3.4518187046051025 174.25538635253906
Loss :  1.703101634979248 2.850292921066284 144.21774291992188
Loss :  1.7072598934173584 2.8934948444366455 146.3820037841797
Loss :  1.7131354808807373 2.7597415447235107 139.70021057128906
  batch 40 loss: 1.7131354808807373, 2.7597415447235107, 139.70021057128906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6857601404190063 3.191477060317993 161.25961303710938
Loss :  1.6766762733459473 3.293473482131958 166.35035705566406
Loss :  1.6687318086624146 3.0936152935028076 156.34950256347656
Loss :  1.6772024631500244 3.441734552383423 173.76393127441406
Loss :  1.6637552976608276 2.563535451889038 129.8405303955078
Loss :  1.6842459440231323 2.937613010406494 148.5648956298828
Loss :  1.7069146633148193 3.6336441040039062 183.3891143798828
Loss :  1.6746317148208618 3.6501338481903076 184.18133544921875
Loss :  1.7172670364379883 2.8871731758117676 146.075927734375
Loss :  1.678876519203186 2.7298567295074463 138.17172241210938
Loss :  1.7011362314224243 3.208381175994873 162.1201934814453
Loss :  1.6981017589569092 3.048525094985962 154.12435913085938
Loss :  1.6840684413909912 3.5866339206695557 181.01576232910156
Loss :  1.7037204504013062 3.3421454429626465 168.8109893798828
Loss :  1.67638099193573 3.563615083694458 179.8571319580078
Loss :  1.7188953161239624 2.747910499572754 139.1144256591797
Loss :  1.6825120449066162 2.7475552558898926 139.06027221679688
Loss :  1.6712770462036133 3.068497896194458 155.09617614746094
Loss :  1.6811387538909912 3.0487725734710693 154.11976623535156
Loss :  1.7230767011642456 2.7827866077423096 140.86241149902344
  batch 60 loss: 1.7230767011642456, 2.7827866077423096, 140.86241149902344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.6712135076522827 3.4787039756774902 175.60641479492188
Loss :  1.6895630359649658 3.3827006816864014 170.82460021972656
Loss :  1.675916075706482 3.4902660846710205 176.18922424316406
Loss :  1.668288230895996 3.337937593460083 168.56517028808594
Loss :  1.6544461250305176 2.628122329711914 133.06056213378906
Loss :  1.680817723274231 4.2757768630981445 215.46966552734375
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.689616322517395 4.313397407531738 217.3594970703125
Loss :  1.6866216659545898 4.165201663970947 209.9467010498047
Loss :  1.6922475099563599 4.00731897354126 202.05819702148438
Total LOSS train 160.1531541090745 valid 211.20851516723633
CE LOSS train 1.6855027950727022 valid 0.42306187748908997
Contrastive LOSS train 3.1693530082702637 valid 1.001829743385315
EPOCH 192:
Loss :  1.6981948614120483 3.439025402069092 173.6494598388672
Loss :  1.7070213556289673 3.0008139610290527 151.74771118164062
Loss :  1.686883807182312 3.0278167724609375 153.07772827148438
Loss :  1.6921223402023315 3.1928255558013916 161.33340454101562
Loss :  1.7024003267288208 2.6426444053649902 133.83462524414062
Loss :  1.67945396900177 3.8151328563690186 192.43609619140625
Loss :  1.702016830444336 3.153290271759033 159.3665313720703
Loss :  1.6854287385940552 2.7201554775238037 137.6931915283203
Loss :  1.6793885231018066 2.9855692386627197 150.9578399658203
Loss :  1.7020186185836792 2.9413790702819824 148.77096557617188
Loss :  1.6722151041030884 2.9988749027252197 151.61595153808594
Loss :  1.672287106513977 2.9422483444213867 148.78469848632812
Loss :  1.6711584329605103 3.700685739517212 186.7054443359375
Loss :  1.6756333112716675 3.0348944664001465 153.42034912109375
Loss :  1.7113635540008545 3.375819444656372 170.50233459472656
Loss :  1.7094172239303589 2.9164860248565674 147.53370666503906
Loss :  1.6700032949447632 3.936814546585083 198.51072692871094
Loss :  1.6880475282669067 2.9083831310272217 147.10720825195312
Loss :  1.6680892705917358 2.687136173248291 136.02490234375
Loss :  1.707661509513855 3.2475640773773193 164.0858612060547
  batch 20 loss: 1.707661509513855, 3.2475640773773193, 164.0858612060547
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  1.6834664344787598 2.582738161087036 130.82037353515625
Loss :  1.666113257408142 3.287682056427002 166.0502166748047
Loss :  1.6778266429901123 3.119798421859741 157.66773986816406
Loss :  1.6889647245407104 3.2064852714538574 162.0132293701172
Loss :  1.7081001996994019 3.3785197734832764 170.63409423828125
Loss :  1.6801317930221558 2.75374698638916 139.3674774169922
Loss :  1.6864532232284546 3.048790454864502 154.1259765625
Loss :  1.6831737756729126 2.9278295040130615 148.07464599609375
Loss :  1.6488311290740967 2.7231602668762207 137.80685424804688
Loss :  1.7056975364685059 3.08481764793396 155.9465789794922
Loss :  1.650236964225769 3.368004083633423 170.05044555664062
Loss :  1.6947475671768188 3.4170405864715576 172.54678344726562
Loss :  1.6783276796340942 3.078188896179199 155.5877685546875
Loss :  1.6767096519470215 3.49945330619812 176.64938354492188
Loss :  1.6521029472351074 3.0027735233306885 151.790771484375
Loss :  1.6621828079223633 3.0393638610839844 153.63038635253906
Loss :  1.6614607572555542 2.9617207050323486 149.7474822998047
Loss :  1.7014776468276978 2.868426561355591 145.122802734375
Loss :  1.7051708698272705 3.0269083976745605 153.0505828857422
Loss :  1.7112641334533691 3.176042318344116 160.51336669921875
  batch 40 loss: 1.7112641334533691, 3.176042318344116, 160.51336669921875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.684100866317749 3.330869436264038 168.22756958007812
Loss :  1.67447829246521 3.1634533405303955 159.84715270996094
Loss :  1.6668403148651123 3.1237571239471436 157.8546905517578
Loss :  1.67536199092865 3.4134891033172607 172.34982299804688
Loss :  1.6609810590744019 2.7307233810424805 138.1971435546875
Loss :  1.682094693183899 2.9451773166656494 148.9409637451172
Loss :  1.7052925825119019 3.210660457611084 162.23831176757812
Loss :  1.6718882322311401 2.82816481590271 143.0801239013672
Loss :  1.7151850461959839 3.078176259994507 155.62399291992188
Loss :  1.6770397424697876 4.330333232879639 218.19369506835938
Loss :  1.6990658044815063 3.380711555480957 170.73464965820312
Loss :  1.6960982084274292 3.6673521995544434 185.06370544433594
Loss :  1.6814292669296265 2.9220468997955322 147.7837677001953
Loss :  1.7015526294708252 3.389679193496704 171.18551635742188
Loss :  1.6737334728240967 3.6066720485687256 182.00733947753906
Loss :  1.7171404361724854 3.3871753215789795 171.07591247558594
Loss :  1.679420828819275 2.8393514156341553 143.64700317382812
Loss :  1.667970061302185 3.5614233016967773 179.7391357421875
Loss :  1.6780637502670288 3.9277665615081787 198.06639099121094
Loss :  1.7218515872955322 2.8015127182006836 141.7974853515625
  batch 60 loss: 1.7218515872955322, 2.8015127182006836, 141.7974853515625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6681082248687744 3.2485663890838623 164.09642028808594
Loss :  1.6873807907104492 2.8061411380767822 141.9944305419922
Loss :  1.6741242408752441 2.8227643966674805 142.8123321533203
Loss :  1.6661741733551025 3.405339002609253 171.93312072753906
Loss :  1.6524137258529663 2.5031330585479736 126.80906677246094
Loss :  1.6779210567474365 3.8933990001678467 196.34786987304688
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6867138147354126 3.9271063804626465 198.04202270507812
Loss :  1.684125542640686 3.84773588180542 194.0709228515625
Loss :  1.689570665359497 3.688826084136963 186.13087463378906
Total LOSS train 159.10239140437199 valid 193.64792251586914
CE LOSS train 1.6843236226301928 valid 0.42239266633987427
Contrastive LOSS train 3.1483613894535947 valid 0.9222065210342407
EPOCH 193:
Loss :  1.6964294910430908 2.7846062183380127 140.92674255371094
Loss :  1.7049041986465454 3.086212158203125 156.01551818847656
Loss :  1.6846380233764648 2.625681161880493 132.96868896484375
Loss :  1.6895729303359985 2.9456987380981445 148.97451782226562
Loss :  1.700518250465393 2.669748067855835 135.1879119873047
Loss :  1.676819920539856 2.800219774246216 141.68780517578125
Loss :  1.700475811958313 2.969524383544922 150.17669677734375
Loss :  1.6839667558670044 3.639827013015747 183.67530822753906
Loss :  1.6780495643615723 2.6201894283294678 132.68753051757812
Loss :  1.702012300491333 2.9694101810455322 150.17251586914062
Loss :  1.6723803281784058 3.1334242820739746 158.34359741210938
Loss :  1.6720812320709229 2.828502893447876 143.09722900390625
Loss :  1.670864224433899 3.097442150115967 156.54296875
Loss :  1.6750543117523193 2.9956860542297363 151.4593505859375
Loss :  1.710820198059082 3.0043578147888184 151.9287109375
Loss :  1.709675908088684 2.937403917312622 148.57986450195312
Loss :  1.6694798469543457 2.9766452312469482 150.50173950195312
Loss :  1.6878491640090942 2.8539915084838867 144.38742065429688
Loss :  1.66801118850708 3.090094566345215 156.17274475097656
Loss :  1.7075905799865723 3.6526248455047607 184.33883666992188
  batch 20 loss: 1.7075905799865723, 3.6526248455047607, 184.33883666992188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6839652061462402 2.7503275871276855 139.20033264160156
Loss :  1.6665337085723877 2.868842363357544 145.10865783691406
Loss :  1.678017020225525 2.685431718826294 135.94961547851562
Loss :  1.689247727394104 2.7643303871154785 139.90577697753906
Loss :  1.7083361148834229 3.8667900562286377 195.0478515625
Loss :  1.6791348457336426 3.471980571746826 175.27816772460938
Loss :  1.6859304904937744 2.871377468109131 145.2548065185547
Loss :  1.6821191310882568 2.93986439704895 148.6753387451172
Loss :  1.6475297212600708 3.667736291885376 185.0343475341797
Loss :  1.7061008214950562 3.438413381576538 173.62677001953125
Loss :  1.6482774019241333 2.789645195007324 141.1305389404297
Loss :  1.6948233842849731 2.9842193126678467 150.90579223632812
Loss :  1.6777615547180176 3.096496820449829 156.5026092529297
Loss :  1.6770873069763184 3.528486728668213 178.10142517089844
Loss :  1.6537978649139404 2.7513678073883057 139.22218322753906
Loss :  1.6637805700302124 3.2692837715148926 165.1279754638672
Loss :  1.6624560356140137 3.8939332962036133 196.359130859375
Loss :  1.7030034065246582 3.127094030380249 158.05770874023438
Loss :  1.7071000337600708 2.9298105239868164 148.1976318359375
Loss :  1.7140130996704102 3.150332450866699 159.2306365966797
  batch 40 loss: 1.7140130996704102, 3.150332450866699, 159.2306365966797
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.6866177320480347 3.0167176723480225 152.52249145507812
Loss :  1.6773216724395752 3.4237897396087646 172.86680603027344
Loss :  1.6682307720184326 3.3876326084136963 171.04986572265625
Loss :  1.6778370141983032 2.953240156173706 149.33984375
Loss :  1.6613295078277588 3.535783529281616 178.45050048828125
Loss :  1.6829699277877808 3.0867819786071777 156.02206420898438
Loss :  1.7061147689819336 3.237938642501831 163.60304260253906
Loss :  1.6693499088287354 3.062519073486328 154.79530334472656
Loss :  1.7149794101715088 3.062495470046997 154.83975219726562
Loss :  1.671825647354126 3.490269899368286 176.18531799316406
Loss :  1.6956596374511719 2.986844301223755 151.03787231445312
Loss :  1.69045090675354 2.857150077819824 144.54794311523438
Loss :  1.6748117208480835 2.989314317703247 151.14051818847656
Loss :  1.6952717304229736 3.1079936027526855 157.09494018554688
Loss :  1.6656278371810913 2.953366756439209 149.33396911621094
Loss :  1.7111334800720215 2.869051694869995 145.16372680664062
Loss :  1.6708812713623047 2.9298431873321533 148.1630401611328
Loss :  1.6584439277648926 2.5868937969207764 131.00314331054688
Loss :  1.6694692373275757 3.0307319164276123 153.2060546875
Loss :  1.716442346572876 3.2127373218536377 162.3533172607422
  batch 60 loss: 1.716442346572876, 3.2127373218536377, 162.3533172607422
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6593552827835083 2.843806743621826 143.84970092773438
Loss :  1.6797751188278198 3.098191261291504 156.58934020996094
Loss :  1.664737582206726 3.572727918624878 180.30113220214844
Loss :  1.6554006338119507 2.6612844467163086 134.71961975097656
Loss :  1.640608787536621 2.547574043273926 129.01930236816406
Loss :  1.6691279411315918 3.715104579925537 187.4243621826172
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6777499914169312 3.806248426437378 191.99017333984375
Loss :  1.6752891540527344 3.5230069160461426 177.8256378173828
Loss :  1.684188723564148 3.815901517868042 192.47926330566406
Total LOSS train 154.78371699406551 valid 187.42985916137695
CE LOSS train 1.6823823928833008 valid 0.421047180891037
Contrastive LOSS train 3.0620266877687894 valid 0.9539753794670105
EPOCH 194:
Loss :  1.6887961626052856 3.050586223602295 154.21810913085938
Loss :  1.699311375617981 3.552957773208618 179.34719848632812
Loss :  1.6776505708694458 2.406925678253174 122.02393341064453
Loss :  1.682550311088562 2.8664679527282715 145.00595092773438
Loss :  1.6954611539840698 2.94330096244812 148.8605194091797
Loss :  1.6683142185211182 3.32179594039917 167.75811767578125
Loss :  1.6946892738342285 2.6670920848846436 135.04928588867188
Loss :  1.6761525869369507 2.786201000213623 140.98619079589844
Loss :  1.6694905757904053 2.2813045978546143 115.7347183227539
Loss :  1.6944677829742432 2.268641233444214 115.12652587890625
Loss :  1.6614915132522583 2.7840490341186523 140.86395263671875
Loss :  1.6614452600479126 2.991694688796997 151.2461700439453
Loss :  1.6591883897781372 3.018153190612793 152.5668487548828
Loss :  1.662360668182373 2.787696361541748 141.0471649169922
Loss :  1.7024179697036743 3.3923754692077637 171.32119750976562
Loss :  1.7025214433670044 3.467163562774658 175.06069946289062
Loss :  1.6584583520889282 2.74133038520813 138.7249755859375
Loss :  1.6786096096038818 2.6529550552368164 134.32635498046875
Loss :  1.655509352684021 2.6267809867858887 132.9945526123047
Loss :  1.7027450799942017 3.6347475051879883 183.44012451171875
  batch 20 loss: 1.7027450799942017, 3.6347475051879883, 183.44012451171875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6731587648391724 3.1002461910247803 156.6854705810547
Loss :  1.6549906730651855 3.3349835872650146 168.40415954589844
Loss :  1.6700016260147095 3.58766770362854 181.0533905029297
Loss :  1.6815025806427002 2.7203471660614014 137.69886779785156
Loss :  1.7057256698608398 2.959275722503662 149.6695098876953
Loss :  1.671887993812561 4.154333114624023 209.3885498046875
Loss :  1.6806567907333374 3.376166820526123 170.48899841308594
Loss :  1.6731281280517578 3.502948522567749 176.820556640625
Loss :  1.6345092058181763 3.1251609325408936 157.89254760742188
Loss :  1.698002576828003 3.030365228652954 153.2162628173828
Loss :  1.6322362422943115 3.056997776031494 154.48211669921875
Loss :  1.6832863092422485 3.0639779567718506 154.88218688964844
Loss :  1.6657646894454956 3.267209768295288 165.02626037597656
Loss :  1.664813756942749 3.2516562938690186 164.24761962890625
Loss :  1.6355335712432861 3.31441068649292 167.35606384277344
Loss :  1.6480828523635864 2.965009927749634 149.89859008789062
Loss :  1.6476876735687256 3.03932523727417 153.61395263671875
Loss :  1.6929869651794434 2.954158306121826 149.40090942382812
Loss :  1.6991122961044312 3.354731321334839 169.43568420410156
Loss :  1.7049094438552856 2.813062906265259 142.35806274414062
  batch 40 loss: 1.7049094438552856, 2.813062906265259, 142.35806274414062
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.674100637435913 2.8812255859375 145.73538208007812
Loss :  1.663611888885498 3.0036978721618652 151.84849548339844
Loss :  1.6544238328933716 2.820727825164795 142.69081115722656
Loss :  1.6636245250701904 2.9144198894500732 147.38461303710938
Loss :  1.6489673852920532 3.2283365726470947 163.0657958984375
Loss :  1.672287940979004 2.769819974899292 140.1632843017578
Loss :  1.6980527639389038 2.8014585971832275 141.77099609375
Loss :  1.6606636047363281 2.8092660903930664 142.12396240234375
Loss :  1.7087574005126953 3.1621322631835938 159.81536865234375
Loss :  1.6655216217041016 3.1001522541046143 156.6731414794922
Loss :  1.6896536350250244 2.856372594833374 144.50828552246094
Loss :  1.6874433755874634 2.7686514854431152 140.12001037597656
Loss :  1.670544147491455 2.835517168045044 143.4464111328125
Loss :  1.693721890449524 2.9752185344696045 150.45465087890625
Loss :  1.6621143817901611 2.8230395317077637 142.8140869140625
Loss :  1.710941195487976 2.8620457649230957 144.813232421875
Loss :  1.6685479879379272 3.263532876968384 164.84519958496094
Loss :  1.6564266681671143 3.583411693572998 180.82701110839844
Loss :  1.6677401065826416 3.0026979446411133 151.80264282226562
Loss :  1.714787483215332 2.9859657287597656 151.0130615234375
  batch 60 loss: 1.714787483215332, 2.9859657287597656, 151.0130615234375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6552808284759521 3.35710072517395 169.51031494140625
Loss :  1.6754262447357178 2.792503833770752 141.30062866210938
Loss :  1.6605052947998047 2.9004998207092285 146.6855010986328
Loss :  1.6521555185317993 3.306285858154297 166.96646118164062
Loss :  1.637352466583252 2.4400627613067627 123.64048767089844
Loss :  1.6617149114608765 3.8996102809906006 196.64222717285156
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6715478897094727 3.8667731285095215 195.0102081298828
Loss :  1.6694386005401611 3.8171439170837402 192.52662658691406
Loss :  1.6732234954833984 3.6695892810821533 185.15267944335938
Total LOSS train 152.79603365384617 valid 192.33293533325195
CE LOSS train 1.674188650571383 valid 0.4183058738708496
Contrastive LOSS train 3.022436893903292 valid 0.9173973202705383
EPOCH 195:
Loss :  1.6871857643127441 3.134760856628418 158.4252166748047
Loss :  1.695703148841858 3.4756669998168945 175.47906494140625
Loss :  1.6732447147369385 3.113834857940674 157.364990234375
Loss :  1.6782755851745605 3.671173095703125 185.23692321777344
Loss :  1.6900463104248047 2.8920977115631104 146.29493713378906
Loss :  1.6632468700408936 2.765803575515747 139.95343017578125
Loss :  1.6890791654586792 2.8422093391418457 143.79954528808594
Loss :  1.671065092086792 3.2324378490448 163.29295349121094
Loss :  1.6640681028366089 2.6581552028656006 134.5718231201172
Loss :  1.691684603691101 3.3732151985168457 170.35244750976562
Loss :  1.658711552619934 2.9974212646484375 151.52976989746094
Loss :  1.6578162908554077 3.267537832260132 165.0347137451172
Loss :  1.6579316854476929 2.920048952102661 147.66036987304688
Loss :  1.6635074615478516 3.33716082572937 168.5215606689453
Loss :  1.7007660865783691 3.114849090576172 157.44322204589844
Loss :  1.701252818107605 3.7258780002593994 187.99514770507812
Loss :  1.6557447910308838 4.153332710266113 209.3223876953125
Loss :  1.6762346029281616 3.445404291152954 173.9464569091797
Loss :  1.6551321744918823 2.7405500411987305 138.68263244628906
Loss :  1.6985816955566406 2.9300358295440674 148.2003631591797
  batch 20 loss: 1.6985816955566406, 2.9300358295440674, 148.2003631591797
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  1.6739526987075806 2.949673652648926 149.1576385498047
Loss :  1.6536728143692017 3.1941351890563965 161.3604278564453
Loss :  1.6659399271011353 3.267730712890625 165.05247497558594
Loss :  1.6807472705841064 3.041775703430176 153.76953125
Loss :  1.7011727094650269 3.3492815494537354 169.16525268554688
Loss :  1.6685700416564941 2.849778652191162 144.15750122070312
Loss :  1.6755740642547607 3.1432487964630127 158.8380126953125
Loss :  1.6704561710357666 2.874213695526123 145.38113403320312
Loss :  1.6328781843185425 2.8894262313842773 146.10418701171875
Loss :  1.695670485496521 3.9914469718933105 201.26800537109375
Loss :  1.6318273544311523 2.9760661125183105 150.43511962890625
Loss :  1.6826339960098267 2.950237274169922 149.1945037841797
Loss :  1.66665780544281 2.980844020843506 150.7088623046875
Loss :  1.6659250259399414 3.171527147293091 160.24227905273438
Loss :  1.6378512382507324 2.9888830184936523 151.08200073242188
Loss :  1.6505308151245117 3.867628812789917 195.03196716308594
Loss :  1.6507246494293213 2.7926981449127197 141.28562927246094
Loss :  1.694435954093933 2.888458251953125 146.1173553466797
Loss :  1.7002629041671753 2.794813394546509 141.44093322753906
Loss :  1.7060295343399048 3.7358620166778564 188.49913024902344
  batch 40 loss: 1.7060295343399048, 3.7358620166778564, 188.49913024902344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6773666143417358 3.569795846939087 180.1671600341797
Loss :  1.6670361757278442 3.4262709617614746 172.9805908203125
Loss :  1.6598254442214966 2.852034568786621 144.2615509033203
Loss :  1.6680452823638916 3.4311554431915283 173.22581481933594
Loss :  1.6550157070159912 3.371319055557251 170.22096252441406
Loss :  1.6765925884246826 3.349601984024048 169.1566925048828
Loss :  1.7006484270095825 2.753904104232788 139.39585876464844
Loss :  1.667155146598816 3.6687004566192627 185.10218811035156
Loss :  1.711413860321045 3.07433819770813 155.42831420898438
Loss :  1.6700334548950195 3.849478006362915 194.14393615722656
Loss :  1.6932618618011475 3.613793134689331 182.38291931152344
Loss :  1.6901437044143677 3.3576533794403076 169.57281494140625
Loss :  1.6735299825668335 3.363680601119995 169.85755920410156
Loss :  1.6952379941940308 3.311863422393799 167.2884063720703
Loss :  1.6651359796524048 3.2702138423919678 165.1758270263672
Loss :  1.7111228704452515 3.248920202255249 164.15713500976562
Loss :  1.6700552701950073 3.2537620067596436 164.358154296875
Loss :  1.6588143110275269 3.146192789077759 158.9684600830078
Loss :  1.6700608730316162 3.380037784576416 170.6719512939453
Loss :  1.715884804725647 3.4692046642303467 175.17611694335938
  batch 60 loss: 1.715884804725647, 3.4692046642303467, 175.17611694335938
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  1.6583001613616943 3.0300097465515137 153.15878295898438
Loss :  1.6777622699737549 3.143078088760376 158.8316650390625
Loss :  1.6630021333694458 2.8026390075683594 141.7949676513672
Loss :  1.6547914743423462 3.1342713832855225 158.36834716796875
Loss :  1.6403377056121826 2.858311891555786 144.55592346191406
Loss :  1.666411280632019 3.658097505569458 184.5712890625
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6766356229782104 3.610485792160034 182.200927734375
Loss :  1.6736626625061035 3.4248948097229004 172.91839599609375
Loss :  1.6782395839691162 3.3222711086273193 167.7917938232422
Total LOSS train 161.84310772235577 valid 176.87060165405273
CE LOSS train 1.674236403978788 valid 0.41955989599227905
Contrastive LOSS train 3.203377437591553 valid 0.8305677771568298
EPOCH 196:
Loss :  1.688400387763977 2.870377540588379 145.207275390625
Loss :  1.6976369619369507 3.1152420043945312 157.45973205566406
Loss :  1.6766372919082642 2.966339588165283 149.99362182617188
Loss :  1.6820534467697144 3.0339560508728027 153.37985229492188
Loss :  1.6933318376541138 2.6233773231506348 132.86219787597656
Loss :  1.6678626537322998 3.093334913253784 156.33460998535156
Loss :  1.692762851715088 2.916307210922241 147.50811767578125
Loss :  1.6750638484954834 2.7954928874969482 141.44970703125
Loss :  1.6685049533843994 3.1281867027282715 158.0778350830078
Loss :  1.694550633430481 2.7545812129974365 139.42361450195312
Loss :  1.6635103225708008 3.2625110149383545 164.7890625
Loss :  1.6624231338500977 3.3371293544769287 168.51889038085938
Loss :  1.662911295890808 3.0248773097991943 152.9067840576172
Loss :  1.668165683746338 3.8527932167053223 194.30783081054688
Loss :  1.7044434547424316 2.973339319229126 150.37139892578125
Loss :  1.704646110534668 2.818124294281006 142.61087036132812
Loss :  1.6603822708129883 3.2621359825134277 164.76718139648438
Loss :  1.6799174547195435 2.7956395149230957 141.46189880371094
Loss :  1.6592693328857422 3.1466453075408936 158.9915313720703
Loss :  1.7021669149398804 2.5702357292175293 130.2139434814453
  batch 20 loss: 1.7021669149398804, 2.5702357292175293, 130.2139434814453
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6780372858047485 2.878382444381714 145.59716796875
Loss :  1.6583482027053833 3.0652170181274414 154.91920471191406
Loss :  1.6698946952819824 3.656552314758301 184.4975128173828
Loss :  1.6839884519577026 2.8525776863098145 144.31288146972656
Loss :  1.7035911083221436 2.9395201206207275 148.6796112060547
Loss :  1.67239511013031 2.9373974800109863 148.54226684570312
Loss :  1.6791459321975708 3.1184544563293457 157.6018829345703
Loss :  1.6749917268753052 3.800046443939209 191.67730712890625
Loss :  1.6388763189315796 2.7992892265319824 141.60333251953125
Loss :  1.699293613433838 2.92969012260437 148.18380737304688
Loss :  1.6374696493148804 2.815706968307495 142.42282104492188
Loss :  1.6863094568252563 3.6657826900482178 184.97544860839844
Loss :  1.670644998550415 2.8636326789855957 144.85227966308594
Loss :  1.6698226928710938 3.2092514038085938 162.13238525390625
Loss :  1.6424270868301392 3.135138988494873 158.3993682861328
Loss :  1.6545076370239258 3.1793606281280518 160.62254333496094
Loss :  1.654877781867981 3.1268630027770996 157.99803161621094
Loss :  1.6969283819198608 2.7170565128326416 137.5497589111328
Loss :  1.702476978302002 2.9875028133392334 151.07762145996094
Loss :  1.7081493139266968 3.5452871322631836 178.97250366210938
  batch 40 loss: 1.7081493139266968, 3.5452871322631836, 178.97250366210938
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  1.6799637079238892 3.078383207321167 155.59912109375
Loss :  1.6698256731033325 2.6857757568359375 135.9586181640625
Loss :  1.6629809141159058 2.8668813705444336 145.00704956054688
Loss :  1.6706191301345825 3.4497456550598145 174.1579132080078
Loss :  1.6579736471176147 2.5670688152313232 130.01141357421875
Loss :  1.678998589515686 3.0527422428131104 154.3161163330078
Loss :  1.7019792795181274 2.544006824493408 128.90231323242188
Loss :  1.6685922145843506 2.812411308288574 142.28915405273438
Loss :  1.7122371196746826 2.885795831680298 146.0020294189453
Loss :  1.6732444763183594 2.971710443496704 150.25877380371094
Loss :  1.696028470993042 3.4230763912200928 172.849853515625
Loss :  1.6941158771514893 2.9534902572631836 149.36863708496094
Loss :  1.6783843040466309 2.5274312496185303 128.04994201660156
Loss :  1.69944429397583 2.7246885299682617 137.93386840820312
Loss :  1.671505331993103 2.627063512802124 133.02468872070312
Loss :  1.7148079872131348 3.6532723903656006 184.37843322753906
Loss :  1.675663709640503 3.2793447971343994 165.6428985595703
Loss :  1.6649779081344604 2.717268705368042 137.52841186523438
Loss :  1.6749382019042969 3.3733761310577393 170.34375
Loss :  1.719253420829773 2.74186635017395 138.8125762939453
  batch 60 loss: 1.719253420829773, 2.74186635017395, 138.8125762939453
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  1.6644980907440186 3.0076801776885986 152.0485076904297
Loss :  1.683462142944336 3.060920000076294 154.72946166992188
Loss :  1.6699005365371704 2.807706117630005 142.05520629882812
Loss :  1.6631852388381958 3.558851718902588 179.60577392578125
Loss :  1.6499801874160767 3.155306100845337 159.415283203125
Loss :  1.6708357334136963 3.920642614364624 197.70297241210938
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  1.680601716041565 3.8631577491760254 194.83848571777344
Loss :  1.6778502464294434 3.7002413272857666 186.68992614746094
Loss :  1.6808820962905884 3.681389808654785 185.7503662109375
Total LOSS train 153.28528442382813 valid 191.2454376220703
CE LOSS train 1.678205811060392 valid 0.4202205240726471
Contrastive LOSS train 3.0321415461026704 valid 0.9203474521636963
EPOCH 197:
Loss :  1.6947541236877441 2.954171895980835 149.40333557128906
Loss :  1.7029004096984863 3.707801103591919 187.09295654296875
Loss :  1.6829205751419067 2.5804429054260254 130.70506286621094
Loss :  1.6877814531326294 4.302286148071289 216.8020782470703
Loss :  1.6977605819702148 3.066809892654419 155.0382537841797
Loss :  1.6732311248779297 3.5561909675598145 179.4827880859375
Loss :  1.6965515613555908 3.0161044597625732 152.50177001953125
Loss :  1.6800254583358765 2.8238890171051025 142.87448120117188
Loss :  1.6732090711593628 3.1112868785858154 157.237548828125
Loss :  1.6978139877319336 3.934704065322876 198.43301391601562
Loss :  1.6676005125045776 3.529533863067627 178.14430236816406
Loss :  1.6660189628601074 2.863140344619751 144.82302856445312
Loss :  1.6652941703796387 2.9837071895599365 150.8506622314453
Loss :  1.6701068878173828 3.0773794651031494 155.53907775878906
Loss :  1.7053041458129883 2.7966034412384033 141.5354766845703
Loss :  1.7041256427764893 3.0446970462799072 153.93898010253906
Loss :  1.6607884168624878 3.609527587890625 182.1371612548828
Loss :  1.679340124130249 2.8671014308929443 145.03440856933594
Loss :  1.658549427986145 2.838270902633667 143.5720977783203
Loss :  1.7015691995620728 3.095505714416504 156.4768524169922
  batch 20 loss: 1.7015691995620728, 3.095505714416504, 156.4768524169922
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.678022027015686 2.7263343334198 137.99473571777344
Loss :  1.6580766439437866 3.8117973804473877 192.24795532226562
Loss :  1.669706106185913 2.8378334045410156 143.56137084960938
Loss :  1.6836371421813965 2.9994359016418457 151.6554412841797
Loss :  1.702944040298462 3.0469512939453125 154.05050659179688
Loss :  1.671726942062378 3.0413811206817627 153.74078369140625
Loss :  1.6789289712905884 3.3765952587127686 170.50868225097656
Loss :  1.6746677160263062 2.740164041519165 138.68287658691406
Loss :  1.638654351234436 2.8228349685668945 142.78041076660156
Loss :  1.6992411613464355 2.915245532989502 147.46151733398438
Loss :  1.6378074884414673 3.394291400909424 171.3523712158203
Loss :  1.6866909265518188 2.8837764263153076 145.87551879882812
Loss :  1.6720800399780273 2.9488420486450195 149.1141815185547
Loss :  1.6715569496154785 2.8061299324035645 141.97805786132812
Loss :  1.6448441743850708 3.15600848197937 159.44528198242188
Loss :  1.656863808631897 3.1970930099487305 161.51150512695312
Loss :  1.6569610834121704 2.8629202842712402 144.802978515625
Loss :  1.6988561153411865 3.132439136505127 158.32081604003906
Loss :  1.704240083694458 2.833000421524048 143.35426330566406
Loss :  1.709701657295227 3.2869558334350586 166.0574951171875
  batch 40 loss: 1.709701657295227, 3.2869558334350586, 166.0574951171875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.6815294027328491 3.0580453872680664 154.58380126953125
Loss :  1.6707838773727417 2.646211862564087 133.9813690185547
Loss :  1.663158893585205 3.417126178741455 172.51947021484375
Loss :  1.670492172241211 3.32342267036438 167.84161376953125
Loss :  1.6573013067245483 2.8612189292907715 144.71824645996094
Loss :  1.6783901453018188 3.673487424850464 185.35276794433594
Loss :  1.702089548110962 3.0938260555267334 156.3933868408203
Loss :  1.6683056354522705 3.236029624938965 163.46978759765625
Loss :  1.7113219499588013 3.6267786026000977 183.0502471923828
Loss :  1.6726243495941162 2.933257818222046 148.33551025390625
Loss :  1.6947236061096191 3.805764675140381 191.9829559326172
Loss :  1.6920424699783325 2.941981792449951 148.7911376953125
Loss :  1.6762582063674927 2.935107707977295 148.431640625
Loss :  1.6969736814498901 3.8309245109558105 193.24319458007812
Loss :  1.6687604188919067 3.5585274696350098 179.5951385498047
Loss :  1.7129980325698853 2.8973612785339355 146.5810546875
Loss :  1.673297643661499 2.9236135482788086 147.85397338867188
Loss :  1.6624079942703247 2.6929166316986084 136.30824279785156
Loss :  1.6725373268127441 3.553854465484619 179.36524963378906
Loss :  1.7171192169189453 3.0158839225769043 152.5113067626953
  batch 60 loss: 1.7171192169189453, 3.0158839225769043, 152.5113067626953
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6610918045043945 3.269430637359619 165.1326141357422
Loss :  1.680242657661438 3.349543809890747 169.1574249267578
Loss :  1.6671650409698486 3.5807454586029053 180.70443725585938
Loss :  1.6598405838012695 3.321221351623535 167.7209014892578
Loss :  1.6466057300567627 2.4870550632476807 125.99935913085938
Loss :  1.6724433898925781 4.335573673248291 218.4511260986328
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.682524561882019 4.36997652053833 220.1813507080078
Loss :  1.6785683631896973 4.319423198699951 217.6497344970703
Loss :  1.6832983493804932 4.146599292755127 209.01327514648438
Total LOSS train 159.0729987511268 valid 216.32387161254883
CE LOSS train 1.678752537874075 valid 0.4208245873451233
Contrastive LOSS train 3.1478849447690522 valid 1.0366498231887817
EPOCH 198:
Loss :  1.6924670934677124 2.706961154937744 137.04052734375
Loss :  1.7005231380462646 3.420720100402832 172.7365264892578
Loss :  1.680479884147644 3.0146186351776123 152.41140747070312
Loss :  1.6856569051742554 3.1388890743255615 158.63011169433594
Loss :  1.6960183382034302 3.337383985519409 168.56521606445312
Loss :  1.6716973781585693 3.017462968826294 152.5448455810547
Loss :  1.6954894065856934 3.5686676502227783 180.12887573242188
Loss :  1.678903579711914 2.86726450920105 145.04212951660156
Loss :  1.6719880104064941 2.8008246421813965 141.7132110595703
Loss :  1.6961328983306885 3.3819220066070557 170.792236328125
Loss :  1.6660661697387695 3.0659568309783936 154.96389770507812
Loss :  1.664450764656067 3.5288565158843994 178.10726928710938
Loss :  1.6643651723861694 3.379241704940796 170.62644958496094
Loss :  1.6698634624481201 3.1020658016204834 156.7731475830078
Loss :  1.7053554058074951 3.0847115516662598 155.94093322753906
Loss :  1.7045478820800781 3.321629524230957 167.78602600097656
Loss :  1.6615136861801147 3.1221835613250732 157.77069091796875
Loss :  1.6802688837051392 3.6281251907348633 183.08653259277344
Loss :  1.6601022481918335 4.270373344421387 215.1787567138672
Loss :  1.7008271217346191 2.695854902267456 136.49356079101562
  batch 20 loss: 1.7008271217346191, 2.695854902267456, 136.49356079101562
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6774040460586548 2.6866540908813477 136.01010131835938
Loss :  1.6578214168548584 3.6832597255706787 185.82080078125
Loss :  1.6696048974990845 3.0816245079040527 155.75082397460938
Loss :  1.684182047843933 3.529895544052124 178.17897033691406
Loss :  1.7029905319213867 3.153916120529175 159.39878845214844
Loss :  1.6728250980377197 3.3454997539520264 168.94781494140625
Loss :  1.6796886920928955 3.7785873413085938 190.6090545654297
Loss :  1.6751669645309448 3.2209692001342773 162.7236328125
Loss :  1.6391665935516357 3.1871163845062256 160.99497985839844
Loss :  1.6985024213790894 2.9998133182525635 151.6891632080078
Loss :  1.63720703125 3.0744199752807617 155.3582000732422
Loss :  1.6856720447540283 3.2707366943359375 165.22250366210938
Loss :  1.6698336601257324 2.982428789138794 150.79127502441406
Loss :  1.6691792011260986 2.8588690757751465 144.6126251220703
Loss :  1.6419095993041992 3.3273870944976807 168.01126098632812
Loss :  1.6538703441619873 3.1789910793304443 160.60342407226562
Loss :  1.654011607170105 3.218924045562744 162.60020446777344
Loss :  1.6958705186843872 3.4833381175994873 175.86277770996094
Loss :  1.7006964683532715 3.3594248294830322 169.67193603515625
Loss :  1.706722378730774 3.3899729251861572 171.2053680419922
  batch 40 loss: 1.706722378730774, 3.3899729251861572, 171.2053680419922
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6791142225265503 2.9368109703063965 148.5196533203125
Loss :  1.6687928438186646 2.8051581382751465 141.92669677734375
Loss :  1.6622227430343628 2.85597825050354 144.4611358642578
Loss :  1.670223593711853 3.3999226093292236 171.66635131835938
Loss :  1.657124638557434 2.920832633972168 147.69874572753906
Loss :  1.6775752305984497 2.8810713291168213 145.7311553955078
Loss :  1.701167345046997 2.874922037124634 145.44728088378906
Loss :  1.667765736579895 3.0642077922821045 154.87815856933594
Loss :  1.7115529775619507 3.207191228866577 162.07110595703125
Loss :  1.6719270944595337 3.4515488147735596 174.24935913085938
Loss :  1.6939301490783691 2.9819204807281494 150.78994750976562
Loss :  1.6910901069641113 2.9265904426574707 148.02061462402344
Loss :  1.6749070882797241 2.8778347969055176 145.566650390625
Loss :  1.695318579673767 3.1057591438293457 156.9832763671875
Loss :  1.6666618585586548 3.3934543132781982 171.33937072753906
Loss :  1.7107651233673096 2.918976306915283 147.6595916748047
Loss :  1.6710309982299805 3.0268099308013916 153.01153564453125
Loss :  1.6607320308685303 4.359385967254639 219.63003540039062
Loss :  1.670835018157959 2.9618523120880127 149.76345825195312
Loss :  1.716048002243042 3.5739076137542725 180.4114227294922
  batch 60 loss: 1.716048002243042, 3.5739076137542725, 180.4114227294922
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6608963012695312 2.7319228649139404 138.25704956054688
Loss :  1.680782675743103 2.876519203186035 145.50674438476562
Loss :  1.6670466661453247 2.7830350399017334 140.8188018798828
Loss :  1.6593137979507446 3.441075563430786 173.7130889892578
Loss :  1.6451034545898438 2.444024085998535 123.84630584716797
Loss :  1.6762512922286987 4.259064197540283 214.6294708251953
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  1.686371088027954 4.323759078979492 217.87432861328125
Loss :  1.68449866771698 4.359896183013916 219.67930603027344
Loss :  1.6871380805969238 4.249756336212158 214.17495727539062
Total LOSS train 160.19020913931038 valid 216.58951568603516
CE LOSS train 1.6777072503016546 valid 0.42178452014923096
Contrastive LOSS train 3.1702500636761006 valid 1.0624390840530396
EPOCH 199:
Loss :  1.691389799118042 2.871187925338745 145.25079345703125
Loss :  1.6996235847473145 2.80489444732666 141.94435119628906
Loss :  1.6798800230026245 2.538093328475952 128.5845489501953
Loss :  1.6854033470153809 2.83974027633667 143.67242431640625
Loss :  1.6961627006530762 3.0356647968292236 153.47940063476562
Loss :  1.6717143058776855 3.1232824325561523 157.83583068847656
Loss :  1.695622205734253 3.5561225414276123 179.50173950195312
Loss :  1.6789642572402954 3.761641502380371 189.76104736328125
Loss :  1.6726154088974 2.7963221073150635 141.4887237548828
Loss :  1.6975542306900024 2.669353485107422 135.1652374267578
Loss :  1.6668497323989868 3.2812955379486084 165.73162841796875
Loss :  1.6653422117233276 2.972568988800049 150.29379272460938
Loss :  1.664564847946167 3.0496344566345215 154.1462860107422
Loss :  1.6695845127105713 2.9027066230773926 146.80491638183594
Loss :  1.7045576572418213 2.9609625339508057 149.752685546875
Loss :  1.7038098573684692 2.893022298812866 146.35491943359375
Loss :  1.6604968309402466 3.458008050918579 174.56089782714844
Loss :  1.6788415908813477 2.8980801105499268 146.58285522460938
Loss :  1.6580506563186646 2.636681079864502 133.4921112060547
Loss :  1.7011315822601318 2.810431718826294 142.22271728515625
  batch 20 loss: 1.7011315822601318, 2.810431718826294, 142.22271728515625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.677547574043274 3.6335864067077637 183.35687255859375
Loss :  1.6578325033187866 3.2611639499664307 164.71603393554688
Loss :  1.6698635816574097 2.803241014480591 141.8319091796875
Loss :  1.6842279434204102 2.752607583999634 139.3146209716797
Loss :  1.7036668062210083 4.165877819061279 209.99755859375
Loss :  1.673844337463379 2.9339964389801025 148.37367248535156
Loss :  1.6811237335205078 2.8836445808410645 145.8633575439453
Loss :  1.677193522453308 3.1901695728302 161.18568420410156
Loss :  1.6426546573638916 2.8743393421173096 145.359619140625
Loss :  1.7010852098464966 2.9053988456726074 146.9710235595703
Loss :  1.6417040824890137 3.031073808670044 153.19540405273438
Loss :  1.6890441179275513 3.0129201412200928 152.33505249023438
Loss :  1.6737676858901978 3.0747835636138916 155.41294860839844
Loss :  1.6730828285217285 2.8792011737823486 145.6331329345703
Loss :  1.646527647972107 2.910341501235962 147.16360473632812
Loss :  1.6579842567443848 3.64760160446167 184.03807067871094
Loss :  1.6577069759368896 2.882171869277954 145.76629638671875
Loss :  1.6980472803115845 2.9913535118103027 151.26571655273438
Loss :  1.7028465270996094 3.4678590297698975 175.09579467773438
Loss :  1.708187222480774 2.9348788261413574 148.45213317871094
  batch 40 loss: 1.708187222480774, 2.9348788261413574, 148.45213317871094
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6804252862930298 2.8953895568847656 146.44989013671875
Loss :  1.6701236963272095 3.9564578533172607 199.49301147460938
Loss :  1.6630573272705078 2.918536424636841 147.58987426757812
Loss :  1.6705763339996338 2.935336112976074 148.4373779296875
Loss :  1.6567273139953613 2.8212053775787354 142.7169952392578
Loss :  1.6767295598983765 3.3685994148254395 170.10670471191406
Loss :  1.6995488405227661 2.758636713027954 139.6313934326172
Loss :  1.6671298742294312 2.985240936279297 150.92918395996094
Loss :  1.7099082469940186 3.1082639694213867 157.12310791015625
Loss :  1.6710917949676514 3.392827272415161 171.3124542236328
Loss :  1.6939083337783813 2.920701026916504 147.7289581298828
Loss :  1.6909412145614624 2.703763246536255 136.8791046142578
Loss :  1.6754851341247559 2.7795674800872803 140.65386962890625
Loss :  1.6962581872940063 2.8042612075805664 141.90931701660156
Loss :  1.6689573526382446 3.160846710205078 159.71128845214844
Loss :  1.712704062461853 3.601133108139038 181.7693634033203
Loss :  1.6738860607147217 2.774134874343872 140.38063049316406
Loss :  1.6636579036712646 2.969874382019043 150.15736389160156
Loss :  1.6734081506729126 3.3186516761779785 167.60598754882812
Loss :  1.7176601886749268 2.932919979095459 148.36366271972656
  batch 60 loss: 1.7176601886749268, 2.932919979095459, 148.36366271972656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6639121770858765 2.929419994354248 148.13490295410156
Loss :  1.6831390857696533 4.13097620010376 208.23194885253906
Loss :  1.6705398559570312 2.870089054107666 145.17498779296875
Loss :  1.6632640361785889 3.2099921703338623 162.16287231445312
Loss :  1.6500593423843384 2.800431251525879 141.67161560058594
Loss :  1.6769694089889526 4.198417663574219 211.5978546142578
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6865012645721436 4.173642635345459 210.36863708496094
Loss :  1.6836528778076172 4.142453193664551 208.8063201904297
Loss :  1.6868491172790527 3.9487338066101074 199.12353515625
Total LOSS train 154.86586585411658 valid 207.4740867614746
CE LOSS train 1.6788184184294481 valid 0.4217122793197632
Contrastive LOSS train 3.0637409356924206 valid 0.9871834516525269
EPOCH 200:
Loss :  1.6945056915283203 2.8471598625183105 144.052490234375
Loss :  1.7023916244506836 3.207423448562622 162.0735626220703
Loss :  1.6825189590454102 3.73036789894104 188.200927734375
Loss :  1.688173532485962 3.007566213607788 152.0664825439453
Loss :  1.6977332830429077 2.950305938720703 149.21302795410156
Loss :  1.6744328737258911 3.3959412574768066 171.47149658203125
Loss :  1.69687020778656 3.0169029235839844 152.54202270507812
Loss :  1.680814266204834 3.3693222999572754 170.1469268798828
Loss :  1.6742210388183594 3.454572916030884 174.4028778076172
Loss :  1.6984548568725586 3.215682029724121 162.48255920410156
Loss :  1.6684741973876953 3.1900672912597656 161.1718292236328
Loss :  1.6678581237792969 3.470284938812256 175.18211364746094
Loss :  1.6674096584320068 2.849012851715088 144.1180419921875
Loss :  1.6731709241867065 3.2359158992767334 163.46896362304688
Loss :  1.7072608470916748 3.5973868370056152 181.57659912109375
Loss :  1.7059465646743774 2.814479112625122 142.42990112304688
Loss :  1.665130376815796 3.2247512340545654 162.90269470214844
Loss :  1.682801365852356 2.7415809631347656 138.7618408203125
Loss :  1.6634016036987305 2.9549524784088135 149.41102600097656
Loss :  1.70395827293396 3.350684881210327 169.2382049560547
  batch 20 loss: 1.70395827293396, 3.350684881210327, 169.2382049560547
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6822389364242554 2.825333595275879 142.94891357421875
Loss :  1.6632146835327148 3.423030138015747 172.81471252441406
Loss :  1.6740663051605225 3.815709352493286 192.45953369140625
Loss :  1.6869488954544067 3.007814645767212 152.0776824951172
Loss :  1.7043569087982178 3.781100273132324 190.75936889648438
Loss :  1.6754238605499268 3.815119981765747 192.43142700195312
Loss :  1.680920124053955 3.142993450164795 158.83059692382812
Loss :  1.6782898902893066 2.944003105163574 148.87843322753906
Loss :  1.6431905031204224 2.944995403289795 148.89295959472656
Loss :  1.7008378505706787 3.2420146465301514 163.80157470703125
Loss :  1.6421974897384644 3.364560842514038 169.8702392578125
Loss :  1.6886645555496216 3.1484036445617676 159.1088409423828
Loss :  1.6731770038604736 2.876875638961792 145.51695251464844
Loss :  1.6722996234893799 3.1495940685272217 159.15200805664062
Loss :  1.6458885669708252 3.041017770767212 153.69677734375
Loss :  1.6572325229644775 2.974733591079712 150.39390563964844
Loss :  1.657125473022461 3.0038793087005615 151.85108947753906
Loss :  1.6980245113372803 2.9585745334625244 149.6267547607422
Loss :  1.702072262763977 2.754241704940796 139.41415405273438
Loss :  1.7078019380569458 2.845446825027466 143.9801483154297
  batch 40 loss: 1.7078019380569458, 2.845446825027466, 143.9801483154297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6804171800613403 2.9146807193756104 147.41445922851562
Loss :  1.6697604656219482 2.730275869369507 138.1835479736328
Loss :  1.6634222269058228 2.7435848712921143 138.84266662597656
Loss :  1.6706739664077759 3.174004554748535 160.3708953857422
Loss :  1.6578763723373413 2.630510091781616 133.18338012695312
Loss :  1.678132176399231 3.1944589614868164 161.4010772705078
Loss :  1.7002809047698975 2.966282844543457 150.01443481445312
Loss :  1.6687731742858887 3.0353775024414062 153.43765258789062
Loss :  1.7111445665359497 3.3555374145507812 169.48802185058594
Loss :  1.6733139753341675 2.891731023788452 146.25985717773438
Loss :  1.6954092979431152 3.329685926437378 168.17970275878906
Loss :  1.6923571825027466 2.740272045135498 138.70594787597656
Loss :  1.6770832538604736 2.552001953125 129.2771759033203
Loss :  1.697271466255188 2.749016523361206 139.14808654785156
Loss :  1.670702338218689 2.9585137367248535 149.5963897705078
Loss :  1.712538719177246 3.0484366416931152 154.13436889648438
Loss :  1.6741539239883423 2.9049017429351807 146.9192352294922
Loss :  1.6645029783248901 2.718411445617676 137.5850830078125
Loss :  1.673902153968811 2.9524612426757812 149.29696655273438
Loss :  1.7168347835540771 2.7805612087249756 140.74488830566406
  batch 60 loss: 1.7168347835540771, 2.7805612087249756, 140.74488830566406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.6638034582138062 2.6961464881896973 136.47113037109375
Loss :  1.683714747428894 2.886568784713745 146.0121612548828
Loss :  1.669636845588684 3.0972530841827393 156.53228759765625
Loss :  1.662564754486084 3.5214996337890625 177.737548828125
Loss :  1.648547649383545 2.478736639022827 125.58537292480469
Loss :  1.6754474639892578 3.8599298000335693 194.67193603515625
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.685483694076538 3.9368977546691895 198.53038024902344
Loss :  1.683172345161438 3.7141432762145996 187.3903350830078
Loss :  1.6852995157241821 3.749314546585083 189.15103149414062
Total LOSS train 155.32218463604266 valid 192.43592071533203
CE LOSS train 1.6800972113242516 valid 0.42132487893104553
Contrastive LOSS train 3.072841765330388 valid 0.9373286366462708
EPOCH 201:
Loss :  1.693331241607666 2.9815266132354736 150.7696533203125
Loss :  1.7005586624145508 3.051617383956909 154.28143310546875
Loss :  1.6805061101913452 3.4325246810913086 173.30674743652344
Loss :  1.6857722997665405 3.1821959018707275 160.7955780029297
Loss :  1.69575834274292 3.8528506755828857 194.33828735351562
Loss :  1.6716891527175903 3.1906495094299316 161.20416259765625
Loss :  1.69584059715271 3.1420505046844482 158.79837036132812
Loss :  1.679125428199768 2.925121784210205 147.93521118164062
Loss :  1.672579050064087 3.033203363418579 153.33274841308594
Loss :  1.6968587636947632 2.67415714263916 135.4047088623047
Loss :  1.6669255495071411 3.5614230632781982 179.7380828857422
Loss :  1.6662598848342896 2.7418665885925293 138.75958251953125
Loss :  1.6661796569824219 2.5962371826171875 131.47804260253906
Loss :  1.671985387802124 2.81611704826355 142.4778289794922
Loss :  1.7057169675827026 2.975994348526001 150.50543212890625
Loss :  1.7020008563995361 2.965367317199707 149.97036743164062
Loss :  1.6633644104003906 3.176137685775757 160.47024536132812
Loss :  1.6800683736801147 3.561310291290283 179.74559020996094
Loss :  1.662742018699646 3.566594362258911 179.99244689941406
Loss :  1.7009607553482056 3.340972661972046 168.7495880126953
  batch 20 loss: 1.7009607553482056, 3.340972661972046, 168.7495880126953
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6811927556991577 3.4057562351226807 171.96900939941406
Loss :  1.6623733043670654 3.5554921627044678 179.43698120117188
Loss :  1.6736791133880615 3.557494640350342 179.54840087890625
Loss :  1.6852484941482544 3.526332378387451 178.00186157226562
Loss :  1.7005690336227417 2.8736462593078613 145.38287353515625
Loss :  1.6741834878921509 2.8436455726623535 143.85646057128906
Loss :  1.6778792142868042 3.416149377822876 172.48533630371094
Loss :  1.678208827972412 2.8119001388549805 142.27320861816406
Loss :  1.6431251764297485 2.9205338954925537 147.66981506347656
Loss :  1.7000653743743896 2.991089105606079 151.2545166015625
Loss :  1.6425520181655884 3.2309300899505615 163.18905639648438
Loss :  1.689095139503479 2.939506769180298 148.66444396972656
Loss :  1.67447030544281 2.8105812072753906 142.20352172851562
Loss :  1.6742616891860962 2.4476778507232666 124.05815887451172
Loss :  1.64780592918396 3.1716692447662354 160.2312774658203
Loss :  1.6598738431930542 2.8974380493164062 146.53176879882812
Loss :  1.6592897176742554 2.8234798908233643 142.83328247070312
Loss :  1.703049898147583 2.869997978210449 145.20294189453125
Loss :  1.7079691886901855 2.6639719009399414 134.90655517578125
Loss :  1.7148780822753906 2.8466484546661377 144.04730224609375
  batch 40 loss: 1.7148780822753906, 2.8466484546661377, 144.04730224609375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
Loss :  1.6847513914108276 2.8844709396362305 145.90829467773438
Loss :  1.6764754056930542 2.8201935291290283 142.68614196777344
Loss :  1.665030837059021 3.482862710952759 175.80816650390625
Loss :  1.6752138137817383 2.7292892932891846 138.13967895507812
Loss :  1.6599305868148804 3.1195473670959473 157.63729858398438
Loss :  1.6840587854385376 3.8326973915100098 193.3189239501953
Loss :  1.709275722503662 2.913541078567505 147.38632202148438
Loss :  1.6704570055007935 2.637371301651001 133.5390167236328
Loss :  1.719547986984253 2.869091510772705 145.1741180419922
Loss :  1.6776083707809448 3.070911169052124 155.22317504882812
Loss :  1.6997238397598267 3.7780566215515137 190.60255432128906
Loss :  1.6994984149932861 3.793203830718994 191.35968017578125
Loss :  1.6827834844589233 2.807529926300049 142.0592803955078
Loss :  1.7059471607208252 3.016270875930786 152.5194854736328
Loss :  1.6725469827651978 2.8206350803375244 142.7042999267578
Loss :  1.7185274362564087 3.0576677322387695 154.60191345214844
Loss :  1.6787053346633911 2.8448784351348877 143.92263793945312
Loss :  1.667799472808838 3.063443422317505 154.8399658203125
Loss :  1.6758954524993896 3.092949867248535 156.32337951660156
Loss :  1.7190226316452026 3.052589178085327 154.34848022460938
  batch 60 loss: 1.7190226316452026, 3.052589178085327, 154.34848022460938
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6669995784759521 2.5225532054901123 127.79466247558594
Loss :  1.6834921836853027 2.516993284225464 127.53315734863281
Loss :  1.671299934387207 3.3568320274353027 169.5128936767578
Loss :  1.6646829843521118 3.2533161640167236 164.3304901123047
Loss :  1.6509824991226196 2.4553744792938232 124.41970825195312
Loss :  1.6635345220565796 3.923098087310791 197.8184356689453
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
Loss :  1.6717839241027832 3.9506723880767822 199.2053985595703
Loss :  1.6699035167694092 3.774827718734741 190.41128540039062
Loss :  1.673262119293213 3.646484851837158 183.9975128173828
Total LOSS train 154.88453240027795 valid 192.85815811157227
CE LOSS train 1.68135771384606 valid 0.4183155298233032
Contrastive LOSS train 3.0640635343698355 valid 0.9116212129592896
EPOCH 202:
Loss :  1.6958510875701904 2.5255932807922363 127.97551727294922
Loss :  1.7043496370315552 2.961970806121826 149.80288696289062
Loss :  1.6842652559280396 2.84989857673645 144.17919921875
Loss :  1.6893928050994873 3.5244147777557373 177.91012573242188
Loss :  1.6985142230987549 2.9643232822418213 149.91468811035156
Loss :  1.6757370233535767 3.118577241897583 157.60459899902344
Loss :  1.7001358270645142 2.5276215076446533 128.08120727539062
Loss :  1.683665156364441 2.5068233013153076 127.02482604980469
Loss :  1.6769918203353882 3.088547468185425 156.10435485839844
Loss :  1.7025797367095947 2.850768804550171 144.2410125732422
Loss :  1.6707290410995483 2.7419283390045166 138.76715087890625
Loss :  1.669958233833313 2.7459447383880615 138.96719360351562
Loss :  1.6701443195343018 3.3503339290618896 169.1868438720703
Loss :  1.6739871501922607 3.231273651123047 163.2376708984375
Loss :  1.707735538482666 3.0259017944335938 153.00282287597656
Loss :  1.7061625719070435 3.8361170291900635 193.5120086669922
Loss :  1.665435552597046 2.4656872749328613 124.94979858398438
Loss :  1.685238242149353 2.49883770942688 126.62712860107422
Loss :  1.6649271249771118 2.6864171028137207 135.9857940673828
Loss :  1.7055402994155884 2.7049341201782227 136.95223999023438
  batch 20 loss: 1.7055402994155884, 2.7049341201782227, 136.95223999023438
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
Loss :  1.6829967498779297 2.715837240219116 137.474853515625
Loss :  1.6638439893722534 2.8129353523254395 142.3106231689453
Loss :  1.6758875846862793 2.4511404037475586 124.23291015625
Loss :  1.6874078512191772 2.5866777896881104 131.02130126953125
Loss :  1.7047532796859741 2.8869612216949463 146.05282592773438
Loss :  1.6779042482376099 3.7465217113494873 189.0039825439453
Loss :  1.683164119720459 2.962012767791748 149.78379821777344
Loss :  1.6812759637832642 3.765099287033081 189.9362335205078
Loss :  1.6457014083862305 2.7353944778442383 138.41543579101562
Loss :  1.7023999691009521 3.7114109992980957 187.27294921875
Loss :  1.64450204372406 3.2353756427764893 163.4132843017578
Loss :  1.6918522119522095 3.0732390880584717 155.3538055419922
Loss :  1.6772035360336304 2.6330692768096924 133.33065795898438
Loss :  1.67633855342865 2.498790979385376 126.61589050292969
Loss :  1.6488999128341675 2.9004600048065186 146.67189025878906
Loss :  1.6594802141189575 2.7242438793182373 137.87167358398438
Loss :  1.658573865890503 2.8159019947052 142.45367431640625
Loss :  1.700087308883667 2.5649077892303467 129.9454803466797
Loss :  1.7032800912857056 2.607576847076416 132.08212280273438
Loss :  1.7093764543533325 3.0494489669799805 154.18182373046875
  batch 40 loss: 1.7093764543533325, 3.0494489669799805, 154.18182373046875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
Loss :  1.6808427572250366 3.077631950378418 155.56243896484375
Loss :  1.672041654586792 2.8817293643951416 145.75851440429688
Loss :  1.6644209623336792 2.9614644050598145 149.73764038085938
Loss :  1.6722149848937988 3.0424232482910156 153.79336547851562
Loss :  1.6585683822631836 2.840359926223755 143.6765594482422
Loss :  1.67881441116333 3.143315553665161 158.84458923339844
Loss :  1.7026665210723877 2.6617910861968994 134.79222106933594
Loss :  1.6686676740646362 2.4782228469848633 125.5798110961914
Loss :  1.713285207748413 2.6112921237945557 132.27789306640625
Loss :  1.673935890197754 2.340181827545166 118.68302917480469
Loss :  1.6958043575286865 3.523928165435791 177.8922119140625
Loss :  1.6939233541488647 2.525678873062134 127.97786712646484
Loss :  1.6785882711410522 2.542609691619873 128.80906677246094
Loss :  1.7001410722732544 3.324655055999756 167.93289184570312
Loss :  1.6704332828521729 3.196042776107788 161.4725799560547
Loss :  1.7146642208099365 3.238819122314453 163.65560913085938
Loss :  1.6758118867874146 3.4154069423675537 172.44615173339844
Loss :  1.665946364402771 2.8454554080963135 143.93870544433594
Loss :  1.6744253635406494 3.0639939308166504 154.87411499023438
Loss :  1.7180850505828857 3.1205031871795654 157.7432403564453
  batch 60 loss: 1.7180850505828857, 3.1205031871795654, 157.7432403564453
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6660524606704712 3.54282283782959 178.80718994140625
Loss :  1.6837843656539917 2.7515740394592285 139.26248168945312
Loss :  1.671480417251587 2.328871011734009 118.11502838134766
Loss :  1.6652204990386963 3.4217638969421387 172.75341796875
Loss :  1.6519763469696045 3.755967855453491 189.4503631591797
Loss :  1.6744685173034668 3.826068639755249 192.9779052734375
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6832069158554077 3.811319589614868 192.2491912841797
Loss :  1.6807286739349365 3.7649550437927246 189.92848205566406
Loss :  1.684134840965271 3.694749116897583 186.4215850830078
Total LOSS train 148.85060413067157 valid 190.39429092407227
CE LOSS train 1.681816396346459 valid 0.42103371024131775
Contrastive LOSS train 2.9433757781982424 valid 0.9236872792243958
EPOCH 203:
Loss :  1.6967157125473022 2.9644124507904053 149.91734313964844
Loss :  1.7043135166168213 2.9390869140625 148.65866088867188
Loss :  1.6839791536331177 2.6707987785339355 135.22390747070312
Loss :  1.688805103302002 2.6414742469787598 133.7625274658203
Loss :  1.6965562105178833 2.3582100868225098 119.60706329345703
Loss :  1.672372817993164 2.676982879638672 135.5215301513672
Loss :  1.6963346004486084 2.929473638534546 148.17001342773438
Loss :  1.6790578365325928 2.7505645751953125 139.20729064941406
Loss :  1.6718204021453857 2.4144017696380615 122.39190673828125
Loss :  1.6977390050888062 2.530677080154419 128.23159790039062
Loss :  1.6651512384414673 2.939161777496338 148.62322998046875
Loss :  1.6642816066741943 2.754533052444458 139.39093017578125
Loss :  1.6641825437545776 2.8244879245758057 142.88858032226562
Loss :  1.66841459274292 2.644601583480835 133.8984832763672
Loss :  1.7042484283447266 2.579744815826416 130.69149780273438
Loss :  1.7012869119644165 2.6801435947418213 135.70848083496094
Loss :  1.658669352531433 2.664807081222534 134.89903259277344
Loss :  1.6786354780197144 2.69399094581604 136.37818908691406
Loss :  1.656859040260315 3.0249266624450684 152.9031982421875
Loss :  1.6999833583831787 2.7614376544952393 139.77186584472656
  batch 20 loss: 1.6999833583831787, 2.7614376544952393, 139.77186584472656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
Loss :  1.6762185096740723 2.8197457790374756 142.66351318359375
Loss :  1.6564022302627563 2.8639252185821533 144.8526611328125
Loss :  1.6691303253173828 2.92290997505188 147.8146209716797
Loss :  1.6819562911987305 2.6147727966308594 132.4206085205078
Loss :  1.7004280090332031 3.192330837249756 161.3169708251953
Loss :  1.6714013814926147 2.723236322402954 137.83322143554688
Loss :  1.6773908138275146 2.651951789855957 134.2749786376953
Loss :  1.6749039888381958 2.3049604892730713 116.92292785644531
Loss :  1.6381064653396606 2.3959264755249023 121.43443298339844
Loss :  1.6976873874664307 2.8951451778411865 146.4549560546875
Loss :  1.6363418102264404 3.135636568069458 158.4181671142578
Loss :  1.6858831644058228 3.0089874267578125 152.13525390625
Loss :  1.670578956604004 2.481227397918701 125.73194885253906
Loss :  1.6700570583343506 3.43109393119812 173.22476196289062
Loss :  1.6420345306396484 3.157102108001709 159.49713134765625
Loss :  1.6541365385055542 2.7975282669067383 141.53054809570312
Loss :  1.6540086269378662 2.841156244277954 143.71182250976562
Loss :  1.6970802545547485 2.291553020477295 116.27473449707031
Loss :  1.7008776664733887 2.9039485454559326 146.8983154296875
Loss :  1.7074551582336426 2.5692925453186035 130.17208862304688
  batch 40 loss: 1.7074551582336426, 2.5692925453186035, 130.17208862304688
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6793142557144165 3.7135512828826904 187.3568878173828
Loss :  1.6704496145248413 3.0664899349212646 154.9949493408203
Loss :  1.6624921560287476 2.8515360355377197 144.23928833007812
Loss :  1.670719861984253 2.8459560871124268 143.96852111816406
Loss :  1.6565192937850952 2.576540231704712 130.48353576660156
Loss :  1.6773232221603394 2.8879053592681885 146.0725860595703
Loss :  1.70122492313385 2.8644661903381348 144.92453002929688
Loss :  1.6668626070022583 2.4310057163238525 123.21714782714844
Loss :  1.711673617362976 2.3662493228912354 120.02413940429688
Loss :  1.6718045473098755 4.097324371337891 206.5380096435547
Loss :  1.693788766860962 2.8395421504974365 143.6708984375
Loss :  1.6923519372940063 3.2887473106384277 166.1297149658203
Loss :  1.6761493682861328 2.5752475261688232 130.4385223388672
Loss :  1.697894811630249 2.8469808101654053 144.04693603515625
Loss :  1.6668680906295776 2.8208134174346924 142.70753479003906
Loss :  1.7124812602996826 2.550379991531372 129.23147583007812
Loss :  1.671380639076233 2.925743341445923 147.95855712890625
Loss :  1.6610188484191895 3.1946394443511963 161.39300537109375
Loss :  1.6699727773666382 2.6722183227539062 135.2808837890625
Loss :  1.715710163116455 2.5425941944122314 128.8454132080078
  batch 60 loss: 1.715710163116455, 2.5425941944122314, 128.8454132080078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6612415313720703 2.747649669647217 139.04371643066406
Loss :  1.6791785955429077 2.5951733589172363 131.43785095214844
Loss :  1.665853500366211 2.426607608795166 122.9962387084961
Loss :  1.6586545705795288 2.550339937210083 129.1756591796875
Loss :  1.6440595388412476 2.721977710723877 137.74295043945312
Loss :  1.6696478128433228 4.076177597045898 205.47853088378906
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
Loss :  1.6791330575942993 3.9911351203918457 201.23590087890625
Loss :  1.6772476434707642 3.8831162452697754 195.8330535888672
Loss :  1.6811338663101196 3.9104926586151123 197.2057647705078
Total LOSS train 141.25150686410757 valid 199.93831253051758
CE LOSS train 1.6776380703999445 valid 0.4202834665775299
Contrastive LOSS train 2.7914773500882664 valid 0.9776231646537781
EPOCH 204:
Loss :  1.6902028322219849 3.186455011367798 161.01295471191406
Loss :  1.699364185333252 2.9382312297821045 148.61093139648438
Loss :  1.6788561344146729 2.5790398120880127 130.630859375
Loss :  1.6845684051513672 2.418888568878174 122.62899780273438
Loss :  1.6939047574996948 2.544076681137085 128.89773559570312
Loss :  1.6701960563659668 2.584754467010498 130.9079132080078
Loss :  1.6951017379760742 2.6287872791290283 133.13446044921875
Loss :  1.6777492761611938 2.3931422233581543 121.3348617553711
Loss :  1.6704719066619873 2.270907163619995 115.21582794189453
Loss :  1.696687936782837 2.152534008026123 109.3233871459961
Loss :  1.6641920804977417 3.1268227100372314 158.0053253173828
Loss :  1.6640487909317017 3.629027843475342 183.1154327392578
Loss :  1.6644679307937622 2.790916681289673 141.21031188964844
Loss :  1.6693902015686035 2.9975156784057617 151.545166015625
Loss :  1.7051206827163696 2.8093926906585693 142.1747589111328
Loss :  1.7032651901245117 2.916189670562744 147.5127410888672
Loss :  1.6619833707809448 2.4995858669281006 126.64127349853516
Loss :  1.682047963142395 2.708263874053955 137.09524536132812
Loss :  1.6614224910736084 2.799363136291504 141.62957763671875
Loss :  1.7038135528564453 3.3187685012817383 167.64224243164062
  batch 20 loss: 1.7038135528564453, 3.3187685012817383, 167.64224243164062
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6805120706558228 2.1717376708984375 110.26739501953125
Loss :  1.6606988906860352 2.4412245750427246 123.721923828125
Loss :  1.6731455326080322 2.3944942951202393 121.39786529541016
Loss :  1.685249924659729 2.529205799102783 128.14553833007812
Loss :  1.70325767993927 3.624025583267212 182.904541015625
Loss :  1.6756761074066162 2.395890951156616 121.47022247314453
Loss :  1.6808900833129883 3.1578140258789062 159.57159423828125
Loss :  1.6792830228805542 2.6750388145446777 135.43121337890625
Loss :  1.643978476524353 3.000788450241089 151.68341064453125
Loss :  1.7017767429351807 3.150470018386841 159.22528076171875
Loss :  1.64299738407135 2.7829856872558594 140.7922821044922
Loss :  1.6909300088882446 3.26603364944458 164.99261474609375
Loss :  1.6765445470809937 2.8954508304595947 146.44908142089844
Loss :  1.6762877702713013 3.6411356925964355 183.73306274414062
Loss :  1.6504719257354736 2.725149393081665 137.90794372558594
Loss :  1.6618224382400513 2.812033176422119 142.2634735107422
Loss :  1.6614867448806763 2.857058048248291 144.51438903808594
Loss :  1.7026602029800415 2.921912670135498 147.7982940673828
Loss :  1.7053565979003906 2.3994345664978027 121.67707824707031
Loss :  1.7114511728286743 2.78910756111145 141.16683959960938
  batch 40 loss: 1.7114511728286743, 2.78910756111145, 141.16683959960938
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.683346152305603 2.635956287384033 133.48117065429688
Loss :  1.6742801666259766 2.2763192653656006 115.49024200439453
Loss :  1.6661173105239868 2.7764174938201904 140.48699951171875
Loss :  1.6736398935317993 3.09887957572937 156.6176300048828
Loss :  1.659777045249939 2.609375476837158 132.12855529785156
Loss :  1.6808409690856934 3.109058141708374 157.13375854492188
Loss :  1.7046422958374023 3.458812952041626 174.64527893066406
Loss :  1.670434832572937 2.610882043838501 132.21453857421875
Loss :  1.7157502174377441 2.7589306831359863 139.6622772216797
Loss :  1.676499366760254 2.257373571395874 114.54517364501953
Loss :  1.6977065801620483 2.898385763168335 146.6169891357422
Loss :  1.6973817348480225 2.751532554626465 139.27401733398438
Loss :  1.681406855583191 2.6157422065734863 132.46852111816406
Loss :  1.70307195186615 3.7748184204101562 190.44400024414062
Loss :  1.6725010871887207 2.8647124767303467 144.9081268310547
Loss :  1.7164623737335205 2.331613063812256 118.29711151123047
Loss :  1.6770391464233398 2.4742441177368164 125.38924407958984
Loss :  1.6665183305740356 2.6547939777374268 134.40621948242188
Loss :  1.675187110900879 3.1215932369232178 157.75485229492188
Loss :  1.7186771631240845 2.473940849304199 125.41571807861328
  batch 60 loss: 1.7186771631240845, 2.473940849304199, 125.41571807861328
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6655299663543701 2.468183994293213 125.0747299194336
Loss :  1.6827826499938965 3.387256622314453 171.0456085205078
Loss :  1.669856309890747 4.0607123374938965 204.70547485351562
Loss :  1.662922739982605 3.0227391719818115 152.7998809814453
Loss :  1.6492118835449219 2.156416416168213 109.47003173828125
Loss :  1.67299485206604 4.131741046905518 208.26004028320312
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6822185516357422 4.077971935272217 205.580810546875
Loss :  1.6801761388778687 3.9755518436431885 200.457763671875
Loss :  1.6848376989364624 3.9681601524353027 200.0928497314453
Total LOSS train 142.1209261380709 valid 203.5978660583496
CE LOSS train 1.6806602606406578 valid 0.4212094247341156
Contrastive LOSS train 2.80880531164316 valid 0.9920400381088257
EPOCH 205:
Loss :  1.6946483850479126 2.658352851867676 134.61228942871094
Loss :  1.7031437158584595 2.6592278480529785 134.66453552246094
Loss :  1.6835215091705322 2.1444945335388184 108.90824890136719
Loss :  1.6890006065368652 2.401419162750244 121.75996398925781
Loss :  1.6985437870025635 2.601029396057129 131.75001525878906
Loss :  1.6748846769332886 2.877145290374756 145.5321502685547
Loss :  1.6991323232650757 2.7668466567993164 140.0414581298828
Loss :  1.682085633277893 2.4437708854675293 123.8706283569336
Loss :  1.674877405166626 2.36126971244812 119.73836517333984
Loss :  1.70049250125885 2.1950199604034424 111.45149230957031
Loss :  1.6690950393676758 2.4421274662017822 123.77547454833984
Loss :  1.6681668758392334 2.4140307903289795 122.36970520019531
Loss :  1.667841911315918 2.7738735675811768 140.3615264892578
Loss :  1.6728304624557495 3.5261950492858887 177.9825897216797
Loss :  1.7088998556137085 2.586949348449707 131.05636596679688
Loss :  1.7073346376419067 2.6118290424346924 132.2987823486328
Loss :  1.6652907133102417 2.6680080890655518 135.06568908691406
Loss :  1.685231328010559 2.5295984745025635 128.1651611328125
Loss :  1.663851022720337 2.3784193992614746 120.5848159790039
Loss :  1.7062472105026245 2.397719144821167 121.59220123291016
  batch 20 loss: 1.7062472105026245, 2.397719144821167, 121.59220123291016
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.68245530128479 2.794684648513794 141.41668701171875
Loss :  1.663354516029358 2.392918348312378 121.30927276611328
Loss :  1.6754120588302612 2.1054115295410156 106.94599151611328
Loss :  1.6881109476089478 2.3475382328033447 119.06502532958984
Loss :  1.705879807472229 2.6359140872955322 133.5015869140625
Loss :  1.677908182144165 2.9036593437194824 146.86087036132812
Loss :  1.6831638813018799 2.471209764480591 125.24365234375
Loss :  1.6808879375457764 2.5294525623321533 128.1535186767578
Loss :  1.644191026687622 2.5247421264648438 127.88129425048828
Loss :  1.7035170793533325 2.7786097526550293 140.63400268554688
Loss :  1.6430058479309082 2.3732552528381348 120.30577087402344
Loss :  1.6912857294082642 2.5820186138153076 130.79222106933594
Loss :  1.6752843856811523 2.741905927658081 138.77056884765625
Loss :  1.6750915050506592 2.475330352783203 125.44161224365234
Loss :  1.6480244398117065 2.8457155227661133 143.93380737304688
Loss :  1.6602163314819336 2.9438905715942383 148.85475158691406
Loss :  1.6601202487945557 2.5783026218414307 130.57525634765625
Loss :  1.7025662660598755 2.759894609451294 139.69729614257812
Loss :  1.7058813571929932 2.7319846153259277 138.30511474609375
Loss :  1.7120274305343628 2.4758381843566895 125.50393676757812
  batch 40 loss: 1.7120274305343628, 2.4758381843566895, 125.50393676757812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6837455034255981 3.4816489219665527 175.7661895751953
Loss :  1.674649715423584 3.2401318550109863 163.68124389648438
Loss :  1.6658629179000854 2.776766538619995 140.5041961669922
Loss :  1.6742914915084839 3.9637815952301025 199.86337280273438
Loss :  1.6603747606277466 2.423645496368408 122.8426513671875
Loss :  1.6823087930679321 2.8475022315979004 144.0574188232422
Loss :  1.705328106880188 2.847282648086548 144.0694580078125
Loss :  1.6708769798278809 2.495527505874634 126.44725036621094
Loss :  1.715988278388977 2.66546368598938 134.98916625976562
Loss :  1.6761126518249512 2.47236704826355 125.29446411132812
Loss :  1.6982765197753906 2.612931489944458 132.3448486328125
Loss :  1.6964950561523438 2.6103663444519043 132.21481323242188
Loss :  1.6806981563568115 3.3281381130218506 168.0876007080078
Loss :  1.70248544216156 2.926989793777466 148.05197143554688
Loss :  1.6724516153335571 2.6041834354400635 131.88162231445312
Loss :  1.7176564931869507 2.485382080078125 125.98676300048828
Loss :  1.6777563095092773 2.4563660621643066 124.49606323242188
Loss :  1.6673197746276855 3.0575077533721924 154.54269409179688
Loss :  1.6774736642837524 2.7136037349700928 137.357666015625
Loss :  1.7212245464324951 2.4638638496398926 124.91441345214844
  batch 60 loss: 1.7212245464324951, 2.4638638496398926, 124.91441345214844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6674021482467651 3.5809617042541504 180.71548461914062
Loss :  1.685049057006836 3.694531202316284 186.41160583496094
Loss :  1.6718742847442627 2.5311906337738037 128.2313995361328
Loss :  1.6639755964279175 2.6315906047821045 133.24349975585938
Loss :  1.6500025987625122 2.0392067432403564 103.61033630371094
Loss :  1.6727805137634277 4.192762851715088 211.3109130859375
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
Loss :  1.6819255352020264 4.1651692390441895 209.94039916992188
Loss :  1.679370641708374 4.159189701080322 209.63885498046875
Loss :  1.6858755350112915 4.016435146331787 202.5076446533203
Total LOSS train 135.82122908372145 valid 208.3494529724121
CE LOSS train 1.6823566821905283 valid 0.4214688837528229
Contrastive LOSS train 2.682777452468872 valid 1.0041087865829468
EPOCH 206:
Loss :  1.6963400840759277 2.458864450454712 124.63956451416016
Loss :  1.7040855884552002 2.6952028274536133 136.4642333984375
Loss :  1.684051513671875 2.5495190620422363 129.16000366210938
Loss :  1.6891114711761475 2.8623228073120117 144.8052520751953
Loss :  1.6987136602401733 2.66788649559021 135.09303283691406
Loss :  1.6742409467697144 2.5529491901397705 129.3217010498047
Loss :  1.6998932361602783 2.770411491394043 140.220458984375
Loss :  1.6825029850006104 2.879398822784424 145.65245056152344
Loss :  1.6756223440170288 2.744792938232422 138.915283203125
Loss :  1.7019075155258179 2.347106456756592 119.0572280883789
Loss :  1.6697214841842651 3.071535348892212 155.24649047851562
Loss :  1.6690694093704224 2.8999218940734863 146.6651611328125
Loss :  1.6685627698898315 2.768826961517334 140.1099090576172
Loss :  1.6733530759811401 2.548922300338745 129.1194610595703
Loss :  1.709183931350708 2.7266311645507812 138.04074096679688
Loss :  1.708414912223816 2.6737723350524902 135.39703369140625
Loss :  1.6655514240264893 2.779035806655884 140.6173553466797
Loss :  1.6859830617904663 2.6023683547973633 131.80441284179688
Loss :  1.6647827625274658 2.3745522499084473 120.39239501953125
Loss :  1.7075450420379639 2.7641825675964355 139.9166717529297
  batch 20 loss: 1.7075450420379639, 2.7641825675964355, 139.9166717529297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.683463215827942 2.359302043914795 119.64856719970703
Loss :  1.6641994714736938 2.423281669616699 122.82828521728516
Loss :  1.6763838529586792 2.69952654838562 136.6527099609375
Loss :  1.689612865447998 3.499128818511963 176.6460418701172
Loss :  1.707051396369934 2.551676034927368 129.2908477783203
Loss :  1.6794798374176025 2.5462186336517334 128.9904022216797
Loss :  1.6848175525665283 2.513955593109131 127.3825912475586
Loss :  1.6823673248291016 2.739776611328125 138.67120361328125
Loss :  1.6464619636535645 2.8362386226654053 143.45840454101562
Loss :  1.7046113014221191 3.1139492988586426 157.40206909179688
Loss :  1.6452641487121582 3.5048577785491943 176.88815307617188
Loss :  1.692765235900879 2.9715540409088135 150.2704620361328
Loss :  1.677879810333252 2.5928070545196533 131.3182373046875
Loss :  1.6778749227523804 2.6373796463012695 133.54685974121094
Loss :  1.6514244079589844 3.0972187519073486 156.51235961914062
Loss :  1.6631243228912354 2.686688184738159 135.99754333496094
Loss :  1.6629530191421509 2.6865897178649902 135.992431640625
Loss :  1.7051118612289429 2.6040029525756836 131.90525817871094
Loss :  1.7080731391906738 2.791066884994507 141.26141357421875
Loss :  1.71329927444458 3.5832529067993164 180.87594604492188
  batch 40 loss: 1.71329927444458, 3.5832529067993164, 180.87594604492188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6858114004135132 2.4642586708068848 124.89875030517578
Loss :  1.6767057180404663 2.0782470703125 105.58905792236328
Loss :  1.6685640811920166 2.446455955505371 123.99136352539062
Loss :  1.6767269372940063 2.31465220451355 117.40933990478516
Loss :  1.6623855829238892 2.4011378288269043 121.71927642822266
Loss :  1.6831209659576416 2.5283944606781006 128.10284423828125
Loss :  1.7056467533111572 2.395509958267212 121.48114776611328
Loss :  1.6724177598953247 3.2656338214874268 164.95411682128906
Loss :  1.7163375616073608 3.4157650470733643 172.5045928955078
Loss :  1.677417278289795 3.5485966205596924 179.10723876953125
Loss :  1.6989034414291382 2.905963897705078 146.99708557128906
Loss :  1.6971324682235718 2.6212761402130127 132.7609405517578
Loss :  1.681775689125061 2.7438600063323975 138.87477111816406
Loss :  1.7035197019577026 2.5957112312316895 131.4890899658203
Loss :  1.6733804941177368 2.878342628479004 145.59051513671875
Loss :  1.7183692455291748 2.6669256687164307 135.0646514892578
Loss :  1.6791044473648071 3.267277956008911 165.04299926757812
Loss :  1.6684837341308594 2.7517058849334717 139.2537841796875
Loss :  1.6785327196121216 2.7730371952056885 140.33038330078125
Loss :  1.7215746641159058 2.538790225982666 128.6610870361328
  batch 60 loss: 1.7215746641159058, 2.538790225982666, 128.6610870361328
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6684436798095703 2.7459895610809326 138.96792602539062
Loss :  1.6861531734466553 3.4200334548950195 172.68783569335938
Loss :  1.6724761724472046 3.712341547012329 187.28955078125
Loss :  1.6646459102630615 2.873089551925659 145.31912231445312
Loss :  1.6503692865371704 2.264887809753418 114.89476013183594
Loss :  1.6715242862701416 4.398118019104004 221.57742309570312
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.681065320968628 4.346279621124268 218.9950408935547
Loss :  1.6779842376708984 4.2653961181640625 214.94778442382812
Loss :  1.684566855430603 4.05888557434082 204.62884521484375
Total LOSS train 139.98709012545072 valid 215.03727340698242
CE LOSS train 1.683582384769733 valid 0.42114171385765076
Contrastive LOSS train 2.7660701494950515 valid 1.014721393585205
EPOCH 207:
Loss :  1.6968539953231812 2.5996651649475098 131.68011474609375
Loss :  1.7037584781646729 3.2451353073120117 163.9605255126953
Loss :  1.6831238269805908 2.4947381019592285 126.42002868652344
Loss :  1.6883833408355713 2.7245538234710693 137.91607666015625
Loss :  1.6973766088485718 2.3379645347595215 118.5956039428711
Loss :  1.6735230684280396 2.841111660003662 143.72911071777344
Loss :  1.6978999376296997 2.4808590412139893 125.74085235595703
Loss :  1.6808316707611084 2.323249578475952 117.84330749511719
Loss :  1.6737772226333618 2.479447841644287 125.64616394042969
Loss :  1.699859619140625 2.6388909816741943 133.6444091796875
Loss :  1.6681705713272095 2.9229321479797363 147.8147735595703
Loss :  1.667317271232605 2.688046455383301 136.06964111328125
Loss :  1.6669343709945679 2.865995168685913 144.96669006347656
Loss :  1.6715388298034668 2.9164958000183105 147.49632263183594
Loss :  1.7075810432434082 2.723010540008545 137.8581085205078
Loss :  1.7073215246200562 2.5885331630706787 131.13397216796875
Loss :  1.6630313396453857 2.5458264350891113 128.954345703125
Loss :  1.6837153434753418 2.749528169631958 139.16012573242188
Loss :  1.661022424697876 2.604390859603882 131.8805694580078
Loss :  1.7035900354385376 2.7644269466400146 139.9249267578125
  batch 20 loss: 1.7035900354385376, 2.7644269466400146, 139.9249267578125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6793583631515503 2.6286985874176025 133.11428833007812
Loss :  1.660071849822998 2.6357734203338623 133.44873046875
Loss :  1.6729464530944824 2.4724791049957275 125.2968978881836
Loss :  1.6861454248428345 2.589468240737915 131.15956115722656
Loss :  1.7040343284606934 2.6977055072784424 136.5893096923828
Loss :  1.6760624647140503 2.689197540283203 136.13592529296875
Loss :  1.681235432624817 2.564612627029419 129.911865234375
Loss :  1.6795940399169922 2.817578077316284 142.55850219726562
Loss :  1.6427557468414307 2.5482983589172363 129.05767822265625
Loss :  1.7030928134918213 2.7167093753814697 137.53855895996094
Loss :  1.6419782638549805 3.0429635047912598 153.7901611328125
Loss :  1.6907665729522705 2.6606431007385254 134.72291564941406
Loss :  1.6752959489822388 2.378282070159912 120.58939361572266
Loss :  1.675079107284546 2.50358247756958 126.85420227050781
Loss :  1.6479815244674683 2.921980857849121 147.7470245361328
Loss :  1.6600757837295532 2.94728946685791 149.02455139160156
Loss :  1.6600620746612549 2.7392501831054688 138.62257385253906
Loss :  1.702701210975647 2.8242547512054443 142.91543579101562
Loss :  1.706169605255127 2.65073823928833 134.2430877685547
Loss :  1.711782455444336 2.680302858352661 135.72691345214844
  batch 40 loss: 1.711782455444336, 2.680302858352661, 135.72691345214844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6842023134231567 2.7101528644561768 137.1918487548828
Loss :  1.67534601688385 2.3379580974578857 118.57324981689453
Loss :  1.666487455368042 2.4527831077575684 124.3056411743164
Loss :  1.6751025915145874 2.592637300491333 131.30697631835938
Loss :  1.6608871221542358 3.3438212871551514 168.85195922851562
Loss :  1.6818569898605347 2.537825107574463 128.5731201171875
Loss :  1.7048039436340332 2.504956007003784 126.95260620117188
Loss :  1.6713521480560303 2.3231918811798096 117.83094024658203
Loss :  1.7159210443496704 2.836242437362671 143.52804565429688
Loss :  1.6761900186538696 2.338771104812622 118.61474609375
Loss :  1.6977245807647705 2.7798802852630615 140.69174194335938
Loss :  1.6955851316452026 4.015990257263184 202.49510192871094
Loss :  1.6799720525741577 2.469810724258423 125.1705093383789
Loss :  1.701656460762024 2.2632153034210205 114.8624267578125
Loss :  1.6707226037979126 2.6570703983306885 134.52423095703125
Loss :  1.7156919240951538 2.583702802658081 130.9008331298828
Loss :  1.6752394437789917 2.617621660232544 132.5563201904297
Loss :  1.664976954460144 2.5542194843292236 129.37594604492188
Loss :  1.6743828058242798 2.626495599746704 132.99916076660156
Loss :  1.7189412117004395 2.62398099899292 132.91799926757812
  batch 60 loss: 1.7189412117004395, 2.62398099899292, 132.91799926757812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6648504734039307 2.5241892337799072 127.87431335449219
Loss :  1.682651162147522 2.3909504413604736 121.23017120361328
Loss :  1.6695587635040283 2.4034335613250732 121.84123229980469
Loss :  1.662267804145813 2.6618919372558594 134.75686645507812
Loss :  1.6485357284545898 2.8261754512786865 142.95730590820312
Loss :  1.6663850545883179 3.9253058433532715 197.93167114257812
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
Loss :  1.6756935119628906 3.971228837966919 200.2371368408203
Loss :  1.672664999961853 3.847752809524536 194.060302734375
Loss :  1.6782947778701782 3.7808802127838135 190.72230529785156
Total LOSS train 134.92871598463793 valid 195.73785400390625
CE LOSS train 1.681349334349999 valid 0.41957369446754456
Contrastive LOSS train 2.6649473447066088 valid 0.9452200531959534
EPOCH 208:
Loss :  1.6950982809066772 4.12944221496582 208.16720581054688
Loss :  1.7035236358642578 3.490755796432495 176.24131774902344
Loss :  1.6835126876831055 2.645946741104126 133.98085021972656
Loss :  1.6889528036117554 3.0080859661102295 152.09324645996094
Loss :  1.6983386278152466 2.7692134380340576 140.1590118408203
Loss :  1.6751497983932495 2.9509568214416504 149.22299194335938
Loss :  1.699601650238037 3.6609911918640137 184.74916076660156
Loss :  1.682914137840271 2.129366159439087 108.1512222290039
Loss :  1.6759414672851562 2.1931955814361572 111.33572387695312
Loss :  1.701250672340393 3.2815937995910645 165.78094482421875
Loss :  1.6700150966644287 2.5607869625091553 129.70936584472656
Loss :  1.6692076921463013 2.3817126750946045 120.75483703613281
Loss :  1.6688892841339111 3.1283090114593506 158.08433532714844
Loss :  1.6733516454696655 2.883995532989502 145.87313842773438
Loss :  1.7083797454833984 3.2547948360443115 164.4481201171875
Loss :  1.7071517705917358 2.771348237991333 140.27456665039062
Loss :  1.6650291681289673 2.729311227798462 138.13058471679688
Loss :  1.6847033500671387 2.9156274795532227 147.46607971191406
Loss :  1.6636741161346436 2.8201546669006348 142.67141723632812
Loss :  1.705378770828247 2.8836889266967773 145.88983154296875
  batch 20 loss: 1.705378770828247, 2.8836889266967773, 145.88983154296875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6821647882461548 2.3801093101501465 120.68762969970703
Loss :  1.6628751754760742 2.749861001968384 139.1559295654297
Loss :  1.6753449440002441 3.0769202709198 155.52134704589844
Loss :  1.688415288925171 3.281569480895996 165.7668914794922
Loss :  1.7064203023910522 2.8826744556427 145.84014892578125
Loss :  1.678560733795166 3.3198282718658447 167.6699676513672
Loss :  1.6836131811141968 2.6158692836761475 132.47706604003906
Loss :  1.6815751791000366 2.521726369857788 127.76789093017578
Loss :  1.6454086303710938 2.537027597427368 128.49679565429688
Loss :  1.704543113708496 2.624804735183716 132.9447784423828
Loss :  1.6450568437576294 2.785430431365967 140.91656494140625
Loss :  1.6925327777862549 3.2132058143615723 162.3528289794922
Loss :  1.677669644355774 3.4675538539886475 175.05535888671875
Loss :  1.6771607398986816 2.6111176013946533 132.2330322265625
Loss :  1.6501243114471436 2.8352956771850586 143.4149169921875
Loss :  1.661474585533142 2.9438374042510986 148.85333251953125
Loss :  1.6608155965805054 2.505023956298828 126.9120101928711
Loss :  1.7025598287582397 2.485949993133545 126.00006103515625
Loss :  1.7058837413787842 2.5407910346984863 128.7454376220703
Loss :  1.7113853693008423 2.5413992404937744 128.78134155273438
  batch 40 loss: 1.7113853693008423, 2.5413992404937744, 128.78134155273438
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6838562488555908 2.8940694332122803 146.3873291015625
Loss :  1.6754000186920166 2.197002649307251 111.5255355834961
Loss :  1.6669141054153442 3.220219612121582 162.6779022216797
Loss :  1.6752091646194458 3.2477495670318604 164.0626983642578
Loss :  1.6612509489059448 2.3783912658691406 120.580810546875
Loss :  1.681984305381775 2.8877604007720947 146.07000732421875
Loss :  1.7047947645187378 2.0898396968841553 106.19677734375
Loss :  1.6711252927780151 2.8324944972991943 143.2958526611328
Loss :  1.7153551578521729 2.8325142860412598 143.34107971191406
Loss :  1.675821304321289 2.418696641921997 122.61064910888672
Loss :  1.6976619958877563 3.588203191757202 181.10781860351562
Loss :  1.6955054998397827 2.5539655685424805 129.39378356933594
Loss :  1.6804766654968262 2.462785482406616 124.81974792480469
Loss :  1.7020604610443115 2.5083107948303223 127.11759948730469
Loss :  1.6721305847167969 2.423658847808838 122.85507202148438
Loss :  1.7166533470153809 2.4029130935668945 121.8623046875
Loss :  1.6771676540374756 2.3334012031555176 118.34722900390625
Loss :  1.667130470275879 2.392612934112549 121.29777526855469
Loss :  1.6769421100616455 3.496143102645874 176.48410034179688
Loss :  1.721994400024414 2.3851897716522217 120.98148345947266
  batch 60 loss: 1.721994400024414, 2.3851897716522217, 120.98148345947266
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6689099073410034 2.669682741165161 135.15304565429688
Loss :  1.6867948770523071 2.9121999740600586 147.2967987060547
Loss :  1.6740065813064575 2.5942797660827637 131.38800048828125
Loss :  1.6658750772476196 2.6800549030303955 135.6686248779297
Loss :  1.6521883010864258 2.5674707889556885 130.02572631835938
Loss :  1.6695940494537354 4.343576908111572 218.84844970703125
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6789896488189697 4.395379543304443 221.44796752929688
Loss :  1.6753880977630615 4.3098859786987305 217.169677734375
Loss :  1.682805061340332 4.234724521636963 213.4190216064453
Total LOSS train 141.28192361684947 valid 217.7212791442871
CE LOSS train 1.6827824372511644 valid 0.420701265335083
Contrastive LOSS train 2.7919828194838305 valid 1.0586811304092407
EPOCH 209:
Loss :  1.6981571912765503 2.7752292156219482 140.45960998535156
Loss :  1.7054519653320312 2.4340388774871826 123.40739440917969
Loss :  1.6859734058380127 2.1011340618133545 106.74267578125
Loss :  1.6907497644424438 3.3903815746307373 171.20982360839844
Loss :  1.7002538442611694 2.5960917472839355 131.5048370361328
Loss :  1.676881194114685 2.6660044193267822 134.9770965576172
Loss :  1.7009443044662476 2.4900591373443604 126.20390319824219
Loss :  1.684409260749817 2.410088300704956 122.18882751464844
Loss :  1.677202820777893 2.6246390342712402 132.90914916992188
Loss :  1.7026541233062744 2.5453543663024902 128.97036743164062
Loss :  1.6708260774612427 3.0363709926605225 153.48936462402344
Loss :  1.6694786548614502 2.7263429164886475 137.9866180419922
Loss :  1.669007420539856 2.8036255836486816 141.85028076171875
Loss :  1.6733657121658325 2.831261396408081 143.23643493652344
Loss :  1.7092490196228027 2.459155321121216 124.6670150756836
Loss :  1.7072246074676514 2.4830613136291504 125.86029052734375
Loss :  1.6649138927459717 2.4918642044067383 126.25812530517578
Loss :  1.6845359802246094 2.54195499420166 128.78228759765625
Loss :  1.6626797914505005 2.488494396209717 126.08740234375
Loss :  1.7051035165786743 2.5769832134246826 130.5542755126953
  batch 20 loss: 1.7051035165786743, 2.5769832134246826, 130.5542755126953
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
Loss :  1.6806117296218872 2.5065486431121826 127.00804138183594
Loss :  1.6611294746398926 2.6928300857543945 136.30264282226562
Loss :  1.6734676361083984 2.41552734375 122.44983673095703
Loss :  1.686586618423462 2.563416004180908 129.85739135742188
Loss :  1.7049634456634521 2.6989941596984863 136.6546630859375
Loss :  1.6768674850463867 2.4314963817596436 123.2516860961914
Loss :  1.682205080986023 2.424485206604004 122.90646362304688
Loss :  1.679568886756897 2.353748083114624 119.3669662475586
Loss :  1.6436678171157837 2.3175814151763916 117.52274322509766
Loss :  1.7027958631515503 2.5966222286224365 131.53390502929688
Loss :  1.6429057121276855 2.971287488937378 150.207275390625
Loss :  1.690708875656128 3.418428897857666 172.61215209960938
Loss :  1.6760104894638062 2.376690626144409 120.51054382324219
Loss :  1.6757299900054932 2.5042333602905273 126.88739776611328
Loss :  1.6490049362182617 2.6855647563934326 135.92724609375
Loss :  1.6609002351760864 2.2567672729492188 114.499267578125
Loss :  1.6607565879821777 2.381244659423828 120.72299194335938
Loss :  1.7034087181091309 2.352663993835449 119.33660888671875
Loss :  1.7065778970718384 3.357982873916626 169.605712890625
Loss :  1.7124454975128174 2.5629029273986816 129.8575897216797
  batch 40 loss: 1.7124454975128174, 2.5629029273986816, 129.8575897216797
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6852173805236816 2.6265389919281006 133.0121612548828
Loss :  1.676640510559082 2.5872321128845215 131.03823852539062
Loss :  1.6680572032928467 2.667767286300659 135.05642700195312
Loss :  1.6766722202301025 2.372443675994873 120.29885864257812
Loss :  1.6626968383789062 2.4310314655303955 123.21427154541016
Loss :  1.683090090751648 3.0289103984832764 153.1286163330078
Loss :  1.7057862281799316 2.7985634803771973 141.6339569091797
Loss :  1.6726462841033936 2.7548670768737793 139.41600036621094
Loss :  1.7167047262191772 2.3476076126098633 119.09708404541016
Loss :  1.6775320768356323 2.054675579071045 104.41130828857422
Loss :  1.699193000793457 2.504941701889038 126.94628143310547
Loss :  1.6972815990447998 2.381289005279541 120.76173400878906
Loss :  1.6823407411575317 2.348097324371338 119.08721160888672
Loss :  1.7037670612335205 3.4999442100524902 176.7009735107422
Loss :  1.6737654209136963 2.650165319442749 134.18203735351562
Loss :  1.718383550643921 2.5893168449401855 131.18421936035156
Loss :  1.6785547733306885 2.8028759956359863 141.82235717773438
Loss :  1.66842520236969 2.6181459426879883 132.5757293701172
Loss :  1.6781384944915771 2.8485233783721924 144.1042938232422
Loss :  1.7217981815338135 2.5578317642211914 129.61338806152344
  batch 60 loss: 1.7217981815338135, 2.5578317642211914, 129.61338806152344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6691837310791016 2.574928045272827 130.41558837890625
Loss :  1.6863627433776855 3.9278573989868164 198.0792236328125
Loss :  1.6737496852874756 2.2173678874969482 112.54214477539062
Loss :  1.6662005186080933 3.679447650909424 185.63858032226562
Loss :  1.6521962881088257 2.2180659770965576 112.55549621582031
Loss :  1.6609914302825928 4.351650714874268 219.2435302734375
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
Loss :  1.670044183731079 4.273858547210693 215.36297607421875
Loss :  1.66623055934906 4.243929862976074 213.8627166748047
Loss :  1.673262596130371 4.133415699005127 208.34405517578125
Total LOSS train 133.5520474947416 valid 214.20331954956055
CE LOSS train 1.6831347703933717 valid 0.4183156490325928
Contrastive LOSS train 2.637378270809467 valid 1.0333539247512817
EPOCH 210:
Loss :  1.697127103805542 2.496049404144287 126.49959564208984
Loss :  1.70479416847229 3.094365119934082 156.4230499267578
Loss :  1.6855378150939941 2.7106857299804688 137.21981811523438
Loss :  1.6905604600906372 2.586120128631592 130.99656677246094
Loss :  1.700368046760559 3.6318938732147217 183.29505920410156
Loss :  1.6773865222930908 2.6328628063201904 133.32052612304688
Loss :  1.701734185218811 2.9385571479797363 148.6295928955078
Loss :  1.6848758459091187 2.483273506164551 125.84854888916016
Loss :  1.6780390739440918 2.5632150173187256 129.8387908935547
Loss :  1.7033824920654297 2.6302249431610107 133.21463012695312
Loss :  1.6730523109436035 3.107839345932007 157.0650177001953
Loss :  1.6721770763397217 3.46628999710083 174.98667907714844
Loss :  1.6716238260269165 2.634856939315796 133.4144744873047
Loss :  1.6759898662567139 2.722338914871216 137.79293823242188
Loss :  1.7103636264801025 2.7526960372924805 139.34515380859375
Loss :  1.708970308303833 2.365107774734497 119.96435546875
Loss :  1.6679213047027588 2.678356885910034 135.5857696533203
Loss :  1.6870933771133423 2.7225403785705566 137.81410217285156
Loss :  1.6662169694900513 3.493558406829834 176.34413146972656
Loss :  1.7085710763931274 2.4746692180633545 125.44203186035156
  batch 20 loss: 1.7085710763931274, 2.4746692180633545, 125.44203186035156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6849080324172974 2.5717456340789795 130.27218627929688
Loss :  1.6657638549804688 2.887233018875122 146.02740478515625
Loss :  1.678060531616211 2.3411831855773926 118.73722076416016
Loss :  1.6909573078155518 2.1600637435913086 109.69414520263672
Loss :  1.7081071138381958 2.4224464893341064 122.83042907714844
Loss :  1.680327296257019 3.0356194972991943 153.4613037109375
Loss :  1.685266137123108 2.703662872314453 136.868408203125
Loss :  1.6828138828277588 2.852717638015747 144.31869506835938
Loss :  1.6470881700515747 2.995537281036377 151.42396545410156
Loss :  1.7050414085388184 2.6928815841674805 136.34912109375
Loss :  1.6451866626739502 2.70893931388855 137.09214782714844
Loss :  1.6917140483856201 2.554873466491699 129.43539428710938
Loss :  1.6761881113052368 2.959322929382324 149.642333984375
Loss :  1.6759692430496216 3.0322909355163574 153.29051208496094
Loss :  1.6501678228378296 2.738379955291748 138.56915283203125
Loss :  1.6615805625915527 3.544987916946411 178.9109649658203
Loss :  1.6617757081985474 2.3668859004974365 120.00606536865234
Loss :  1.7026526927947998 2.1862900257110596 111.01715087890625
Loss :  1.7064980268478394 2.3305375576019287 118.2333755493164
Loss :  1.7121689319610596 2.693802833557129 136.40231323242188
  batch 40 loss: 1.7121689319610596, 2.693802833557129, 136.40231323242188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.686234951019287 2.4508705139160156 124.2297592163086
Loss :  1.6773415803909302 2.3005001544952393 116.70235443115234
Loss :  1.669864535331726 3.0710127353668213 155.2205047607422
Loss :  1.6782374382019043 2.780441999435425 140.70033264160156
Loss :  1.6651101112365723 2.33343505859375 118.33686065673828
Loss :  1.6843452453613281 2.6696553230285645 135.1671142578125
Loss :  1.706231713294983 2.4972283840179443 126.5676498413086
Loss :  1.6747595071792603 2.613579034805298 132.3537139892578
Loss :  1.7177129983901978 3.0080816745758057 152.12179565429688
Loss :  1.6795133352279663 2.434356212615967 123.39732360839844
Loss :  1.7015330791473389 3.0540809631347656 154.40557861328125
Loss :  1.699441909790039 2.6235835552215576 132.87863159179688
Loss :  1.6844027042388916 2.4402594566345215 123.69737243652344
Loss :  1.7054561376571655 2.777564764022827 140.5836944580078
Loss :  1.67557692527771 3.1622345447540283 159.7873077392578
Loss :  1.720198631286621 2.7134604454040527 137.39321899414062
Loss :  1.6808030605316162 2.663353204727173 134.8484649658203
Loss :  1.6704379320144653 2.4639699459075928 124.86893463134766
Loss :  1.6805477142333984 2.749040126800537 139.13255310058594
Loss :  1.7237741947174072 2.5523736476898193 129.3424530029297
  batch 60 loss: 1.7237741947174072, 2.5523736476898193, 129.3424530029297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.671202301979065 2.63242769241333 133.29258728027344
Loss :  1.6885007619857788 3.2465624809265137 164.01663208007812
Loss :  1.676353096961975 2.3279366493225098 118.07318878173828
Loss :  1.6691309213638306 3.17185115814209 160.26168823242188
Loss :  1.6560289859771729 2.804832935333252 141.89768981933594
Loss :  1.6798994541168213 4.029515743255615 203.1556854248047
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
Loss :  1.6887261867523193 4.0144548416137695 202.41146850585938
Loss :  1.686134934425354 3.8863041400909424 196.0013427734375
Loss :  1.693251609802246 3.8535635471343994 194.37142944335938
Total LOSS train 138.22923889160157 valid 198.98498153686523
CE LOSS train 1.6849347811478834 valid 0.4233129024505615
Contrastive LOSS train 2.730886092552772 valid 0.9633908867835999
EPOCH 211:
Loss :  1.7005889415740967 2.6807005405426025 135.73562622070312
Loss :  1.7078306674957275 2.7222301959991455 137.8193359375
Loss :  1.688674807548523 2.897792339324951 146.5782928466797
Loss :  1.6935276985168457 2.5248067378997803 127.9338607788086
Loss :  1.7026326656341553 2.0616986751556396 104.78755950927734
Loss :  1.6804782152175903 2.4719250202178955 125.2767333984375
Loss :  1.7037954330444336 2.617421865463257 132.57489013671875
Loss :  1.6865955591201782 2.854854106903076 144.42930603027344
Loss :  1.6794167757034302 2.3397679328918457 118.66781616210938
Loss :  1.702996850013733 2.7065446376800537 137.0302276611328
Loss :  1.6719450950622559 3.1365652084350586 158.50021362304688
Loss :  1.6710222959518433 2.687206506729126 136.03134155273438
Loss :  1.6700161695480347 3.262470006942749 164.79351806640625
Loss :  1.6753228902816772 3.1307923793792725 158.21493530273438
Loss :  1.7093701362609863 3.104694366455078 156.94407653808594
Loss :  1.7082933187484741 3.185209035873413 160.96875
Loss :  1.6670509576797485 2.637451410293579 133.5396270751953
Loss :  1.685687780380249 2.491118907928467 126.24163055419922
Loss :  1.6662527322769165 2.3256187438964844 117.94718933105469
Loss :  1.706939935684204 2.676206350326538 135.5172576904297
  batch 20 loss: 1.706939935684204, 2.676206350326538, 135.5172576904297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6847726106643677 2.5294578075408936 128.1576690673828
Loss :  1.6663367748260498 3.1333887577056885 158.3357696533203
Loss :  1.678757905960083 3.351099967956543 169.23374938964844
Loss :  1.6905474662780762 2.7268283367156982 138.03196716308594
Loss :  1.7082245349884033 2.8859403133392334 146.00523376464844
Loss :  1.6811347007751465 2.5678904056549072 130.07565307617188
Loss :  1.686156988143921 3.0036044120788574 151.8663787841797
Loss :  1.6842840909957886 2.9300754070281982 148.18804931640625
Loss :  1.6492888927459717 3.0132296085357666 152.31077575683594
Loss :  1.7070995569229126 2.792853355407715 141.34976196289062
Loss :  1.6487212181091309 2.875117063522339 145.4045867919922
Loss :  1.695414662361145 3.0117201805114746 152.28143310546875
Loss :  1.6809738874435425 3.562121629714966 179.78704833984375
Loss :  1.6804448366165161 2.8144826889038086 142.4045867919922
Loss :  1.6543244123458862 3.3651559352874756 169.91212463378906
Loss :  1.665446400642395 3.9033377170562744 196.83233642578125
Loss :  1.6649105548858643 3.093569278717041 156.3433837890625
Loss :  1.706084966659546 2.9370338916778564 148.5577850341797
Loss :  1.7089688777923584 2.6801767349243164 135.71780395507812
Loss :  1.7149039506912231 2.550748348236084 129.2523193359375
  batch 40 loss: 1.7149039506912231, 2.550748348236084, 129.2523193359375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6875883340835571 2.309074640274048 117.14132690429688
Loss :  1.6781408786773682 2.324824094772339 117.91934204101562
Loss :  1.669484257698059 2.5549545288085938 129.41720581054688
Loss :  1.6773898601531982 3.6860527992248535 185.9800262451172
Loss :  1.6638312339782715 2.941471815109253 148.7374267578125
Loss :  1.6852083206176758 2.6008894443511963 131.7296905517578
Loss :  1.7075552940368652 2.5158700942993164 127.50106048583984
Loss :  1.6741340160369873 3.343400001525879 168.84413146972656
Loss :  1.7182152271270752 2.5047948360443115 126.95795440673828
Loss :  1.6787697076797485 2.1173882484436035 107.54818725585938
Loss :  1.7011750936508179 3.1008429527282715 156.74331665039062
Loss :  1.6990429162979126 3.8089706897735596 192.14756774902344
Loss :  1.6829673051834106 2.406172037124634 121.99156951904297
Loss :  1.7041648626327515 2.6195688247680664 132.68260192871094
Loss :  1.6745697259902954 2.613245725631714 132.3368682861328
Loss :  1.7190098762512207 2.674668788909912 135.45245361328125
Loss :  1.6799733638763428 2.7168068885803223 137.52032470703125
Loss :  1.66927170753479 3.225458860397339 162.9422149658203
Loss :  1.679451823234558 2.7125914096832275 137.3090362548828
Loss :  1.7222223281860352 2.4694418907165527 125.1943130493164
  batch 60 loss: 1.7222223281860352, 2.4694418907165527, 125.1943130493164
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6694605350494385 2.723651885986328 137.85205078125
Loss :  1.6870366334915161 2.365241289138794 119.9490966796875
Loss :  1.6740493774414062 2.5706393718719482 130.20602416992188
Loss :  1.6664302349090576 3.5241751670837402 177.87518310546875
Loss :  1.652856469154358 3.9306766986846924 198.1866912841797
Loss :  1.6779524087905884 4.32834529876709 218.09521484375
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.687117576599121 4.293642044067383 216.3692169189453
Loss :  1.6850107908248901 4.213066101074219 212.33831787109375
Loss :  1.6912074089050293 4.091800212860107 206.28121948242188
Total LOSS train 143.6884041419396 valid 213.27099227905273
CE LOSS train 1.6854959322856022 valid 0.4228018522262573
Contrastive LOSS train 2.8400581506582405 valid 1.0229500532150269
EPOCH 212:
Loss :  1.6986072063446045 2.5990641117095947 131.6518096923828
Loss :  1.706194519996643 2.789935350418091 141.2029571533203
Loss :  1.6867414712905884 2.300138235092163 116.69365692138672
Loss :  1.6921918392181396 2.648894786834717 134.1369171142578
Loss :  1.7016915082931519 2.6638877391815186 134.8960723876953
Loss :  1.6789151430130005 2.6022677421569824 131.79229736328125
Loss :  1.702433466911316 2.558227062225342 129.61378479003906
Loss :  1.6860095262527466 3.7328295707702637 188.32748413085938
Loss :  1.6794289350509644 2.86232328414917 144.79559326171875
Loss :  1.704708218574524 2.381344795227051 120.7719497680664
Loss :  1.6740037202835083 2.600120782852173 131.6800537109375
Loss :  1.6729265451431274 2.5710322856903076 130.22454833984375
Loss :  1.6723031997680664 2.4386749267578125 123.60604858398438
Loss :  1.676735758781433 2.9054081439971924 146.9471435546875
Loss :  1.7111674547195435 2.65205717086792 134.31402587890625
Loss :  1.7100400924682617 2.8139963150024414 142.40985107421875
Loss :  1.6684828996658325 2.8122825622558594 142.28262329101562
Loss :  1.6876682043075562 2.3836543560028076 120.8703842163086
Loss :  1.6665502786636353 2.7499969005584717 139.16639709472656
Loss :  1.7083005905151367 2.5517051219940186 129.29356384277344
  batch 20 loss: 1.7083005905151367, 2.5517051219940186, 129.29356384277344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.684287190437317 2.32790470123291 118.07952117919922
Loss :  1.6651227474212646 3.251317262649536 164.23097229003906
Loss :  1.6773431301116943 2.414611577987671 122.40792083740234
Loss :  1.6901907920837402 2.480297327041626 125.70506286621094
Loss :  1.708113431930542 2.6297991275787354 133.1980743408203
Loss :  1.6803327798843384 2.80704402923584 142.03253173828125
Loss :  1.6853495836257935 2.729936122894287 138.18215942382812
Loss :  1.6832164525985718 2.570932626724243 130.22984313964844
Loss :  1.6476746797561646 3.2192180156707764 162.60858154296875
Loss :  1.706386923789978 2.9289603233337402 148.15440368652344
Loss :  1.6476351022720337 2.566413402557373 129.96829223632812
Loss :  1.6947576999664307 2.542003870010376 128.79495239257812
Loss :  1.680073857307434 2.46024751663208 124.69245147705078
Loss :  1.679279088973999 2.4793195724487305 125.64525604248047
Loss :  1.652881383895874 2.717942714691162 137.55001831054688
Loss :  1.6635794639587402 3.5065481662750244 176.99098205566406
Loss :  1.663342833518982 3.5486936569213867 179.0980224609375
Loss :  1.7034955024719238 4.01090669631958 202.2488250732422
Loss :  1.7068367004394531 2.9578449726104736 149.5990753173828
Loss :  1.7128645181655884 2.4912428855895996 126.27500915527344
  batch 40 loss: 1.7128645181655884, 2.4912428855895996, 126.27500915527344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.686355471611023 2.9218251705169678 147.77761840820312
Loss :  1.6770466566085815 2.3730409145355225 120.32909393310547
Loss :  1.6694227457046509 2.5337977409362793 128.35931396484375
Loss :  1.6778123378753662 2.441800832748413 123.76785278320312
Loss :  1.6635816097259521 2.4668447971343994 125.00582122802734
Loss :  1.683773159980774 2.6924960613250732 136.30857849121094
Loss :  1.7067402601242065 3.0178143978118896 152.5974578857422
Loss :  1.6730583906173706 3.3886289596557617 171.10450744628906
Loss :  1.716977834701538 2.769530773162842 140.19351196289062
Loss :  1.6782774925231934 2.269169807434082 115.13676452636719
Loss :  1.6996930837631226 2.3900091648101807 121.20014953613281
Loss :  1.6982097625732422 2.7354817390441895 138.47230529785156
Loss :  1.6833683252334595 2.5638887882232666 129.8778076171875
Loss :  1.7049232721328735 2.678769588470459 135.64340209960938
Loss :  1.6751339435577393 2.634188413619995 133.3845672607422
Loss :  1.7185096740722656 2.715564012527466 137.4967041015625
Loss :  1.6795613765716553 3.0359768867492676 153.47840881347656
Loss :  1.670003056526184 2.0726938247680664 105.30469512939453
Loss :  1.6801538467407227 2.882695198059082 145.81492614746094
Loss :  1.723036289215088 2.33091402053833 118.26873779296875
  batch 60 loss: 1.723036289215088, 2.33091402053833, 118.26873779296875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6710346937179565 2.841947317123413 143.76840209960938
Loss :  1.688778042793274 4.078647136688232 205.6211395263672
Loss :  1.675970196723938 2.2461142539978027 113.98168182373047
Loss :  1.6690778732299805 2.6227505207061768 132.80661010742188
Loss :  1.6558462381362915 2.8226161003112793 142.78665161132812
Loss :  1.673600435256958 4.238734245300293 213.6103057861328
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6826156377792358 4.300357341766357 216.7004852294922
Loss :  1.6793315410614014 4.173935413360596 210.37611389160156
Loss :  1.6860806941986084 4.148557662963867 209.1139678955078
Total LOSS train 138.44393580510066 valid 212.4502182006836
CE LOSS train 1.6852955396358784 valid 0.4215201735496521
Contrastive LOSS train 2.7351728035853458 valid 1.0371394157409668
EPOCH 213:
Loss :  1.7004079818725586 2.890158176422119 146.20831298828125
Loss :  1.707876443862915 2.869598627090454 145.18780517578125
Loss :  1.6883533000946045 2.983126401901245 150.8446807861328
Loss :  1.6937412023544312 2.6280267238616943 133.09507751464844
Loss :  1.7029356956481934 4.1387104988098145 208.6384735107422
Loss :  1.6804827451705933 2.7252614498138428 137.9435577392578
Loss :  1.7035526037216187 2.4954445362091064 126.47577667236328
Loss :  1.687406301498413 2.283156156539917 115.84521484375
Loss :  1.6802767515182495 3.277705669403076 165.56556701660156
Loss :  1.7043176889419556 2.365018367767334 119.95523071289062
Loss :  1.6743264198303223 2.8718605041503906 145.26734924316406
Loss :  1.6730214357376099 3.2584824562072754 164.59713745117188
Loss :  1.6722251176834106 2.9819326400756836 150.76885986328125
Loss :  1.6763842105865479 2.584301710128784 130.8914794921875
Loss :  1.7110480070114136 2.483167886734009 125.86943817138672
Loss :  1.7092722654342651 2.687469244003296 136.08273315429688
Loss :  1.6679563522338867 2.7925238609313965 141.2941436767578
Loss :  1.6869003772735596 2.4163057804107666 122.50218963623047
Loss :  1.666030764579773 2.3772902488708496 120.5305404663086
Loss :  1.707525372505188 2.4446823596954346 123.94164276123047
  batch 20 loss: 1.707525372505188, 2.4446823596954346, 123.94164276123047
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6845393180847168 3.6614878177642822 184.75892639160156
Loss :  1.6656217575073242 2.572770833969116 130.3041534423828
Loss :  1.677531361579895 3.039050340652466 153.6300506591797
Loss :  1.6903687715530396 3.0741496086120605 155.39784240722656
Loss :  1.7080512046813965 3.165062665939331 159.961181640625
Loss :  1.680748462677002 3.151761293411255 159.26881408691406
Loss :  1.6862221956253052 2.2985408306121826 116.61326599121094
Loss :  1.6840285062789917 2.441103458404541 123.73920440673828
Loss :  1.649277687072754 2.8372273445129395 143.51065063476562
Loss :  1.7065057754516602 2.722323417663574 137.8226776123047
Loss :  1.6485780477523804 2.740122079849243 138.65467834472656
Loss :  1.6949424743652344 2.479565382003784 125.6732177734375
Loss :  1.6802390813827515 2.368685722351074 120.1145248413086
Loss :  1.6800322532653809 2.4821503162384033 125.78754425048828
Loss :  1.65395987033844 2.669975519180298 135.15274047851562
Loss :  1.6652607917785645 3.5148239135742188 177.40646362304688
Loss :  1.6651058197021484 2.4248759746551514 122.90890502929688
Loss :  1.7054482698440552 2.504239559173584 126.91742706298828
Loss :  1.7090400457382202 2.5576372146606445 129.5908966064453
Loss :  1.7146090269088745 2.5910136699676514 131.2653045654297
  batch 40 loss: 1.7146090269088745, 2.5910136699676514, 131.2653045654297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.687981128692627 2.5232293605804443 127.84944915771484
Loss :  1.679219126701355 2.3325793743133545 118.30818939208984
Loss :  1.671431541442871 3.179151773452759 160.6290283203125
Loss :  1.679928183555603 2.966881036758423 150.02398681640625
Loss :  1.6663457155227661 3.10530686378479 156.93170166015625
Loss :  1.6859368085861206 2.5805563926696777 130.71376037597656
Loss :  1.708178162574768 2.861076831817627 144.76202392578125
Loss :  1.6763713359832764 3.9408552646636963 198.71914672851562
Loss :  1.7185187339782715 3.242314577102661 163.83424377441406
Loss :  1.6812949180603027 2.322514772415161 117.80703735351562
Loss :  1.7018110752105713 3.233675241470337 163.3855743408203
Loss :  1.6996768712997437 3.1443467140197754 158.91700744628906
Loss :  1.6845859289169312 2.543606758117676 128.86492919921875
Loss :  1.7049988508224487 2.5182414054870605 127.61707305908203
Loss :  1.6759320497512817 3.4658074378967285 174.96630859375
Loss :  1.7191880941390991 2.569169282913208 130.1776580810547
Loss :  1.681283712387085 2.6794822216033936 135.6553955078125
Loss :  1.671085000038147 2.41963791847229 122.6529769897461
Loss :  1.6808031797409058 3.53261399269104 178.31150817871094
Loss :  1.7232153415679932 2.3449594974517822 118.97119140625
  batch 60 loss: 1.7232153415679932, 2.3449594974517822, 118.97119140625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.671704649925232 2.328768491744995 118.1101303100586
Loss :  1.6887856721878052 2.949493885040283 149.16348266601562
Loss :  1.6766222715377808 1.9383997917175293 98.59661102294922
Loss :  1.669487476348877 3.1029672622680664 156.81785583496094
Loss :  1.6565430164337158 2.104773759841919 106.89522552490234
Loss :  1.6769460439682007 4.330531597137451 218.20352172851562
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6857569217681885 4.313223838806152 217.34695434570312
Loss :  1.6826601028442383 4.314778804779053 217.42160034179688
Loss :  1.6886996030807495 4.035825252532959 203.47996520996094
Total LOSS train 140.74869502140925 valid 214.11301040649414
CE LOSS train 1.6862320093008187 valid 0.4221749007701874
Contrastive LOSS train 2.7812492333925687 valid 1.0089563131332397
EPOCH 214:
Loss :  1.700888991355896 3.263838768005371 164.892822265625
Loss :  1.708215594291687 3.3724400997161865 170.33023071289062
Loss :  1.6890071630477905 2.382450819015503 120.81155395507812
Loss :  1.6939778327941895 3.7152018547058105 187.45407104492188
Loss :  1.7032946348190308 2.471057653427124 125.25617218017578
Loss :  1.6811188459396362 2.5079445838928223 127.07835388183594
Loss :  1.7041326761245728 2.647099733352661 134.05911254882812
Loss :  1.6880959272384644 2.228358268737793 113.10601043701172
Loss :  1.6813554763793945 2.4437854290008545 123.8706283569336
Loss :  1.7057772874832153 3.2521612644195557 164.3138427734375
Loss :  1.675866961479187 2.7405686378479004 138.7042999267578
Loss :  1.674515724182129 2.9910857677459717 151.2288055419922
Loss :  1.6738064289093018 2.3225672245025635 117.80216979980469
Loss :  1.6782675981521606 2.53178071975708 128.26730346679688
Loss :  1.712506890296936 2.6870362758636475 136.06431579589844
Loss :  1.71149480342865 2.659545660018921 134.68878173828125
Loss :  1.6701796054840088 2.792170286178589 141.27870178222656
Loss :  1.6892595291137695 2.564099073410034 129.8942108154297
Loss :  1.668638825416565 2.4902610778808594 126.18169403076172
Loss :  1.7096034288406372 2.7669644355773926 140.05783081054688
  batch 20 loss: 1.7096034288406372, 2.7669644355773926, 140.05783081054688
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6862068176269531 2.5817980766296387 130.77610778808594
Loss :  1.6674774885177612 2.776508092880249 140.49288940429688
Loss :  1.6792606115341187 2.9909050464630127 151.22451782226562
Loss :  1.692168116569519 2.92960262298584 148.17230224609375
Loss :  1.7093536853790283 3.3779172897338867 170.60520935058594
Loss :  1.6824097633361816 2.3986856937408447 121.61669921875
Loss :  1.6877676248550415 2.751634120941162 139.2694854736328
Loss :  1.6855541467666626 3.210934638977051 162.23228454589844
Loss :  1.6516441106796265 2.8364880084991455 143.47604370117188
Loss :  1.7080044746398926 2.6266372203826904 133.0398712158203
Loss :  1.651225209236145 3.2725636959075928 165.2794189453125
Loss :  1.6970876455307007 2.96736478805542 150.06532287597656
Loss :  1.6829029321670532 2.4393386840820312 123.64983367919922
Loss :  1.6826343536376953 2.5165631771087646 127.51078796386719
Loss :  1.6572082042694092 3.136187791824341 158.4665985107422
Loss :  1.668016791343689 2.435821056365967 123.45906829833984
Loss :  1.6675587892532349 2.708991050720215 137.1171112060547
Loss :  1.7073432207107544 2.5659570693969727 130.00518798828125
Loss :  1.7105262279510498 2.573002576828003 130.36065673828125
Loss :  1.715923547744751 2.5678720474243164 130.10952758789062
  batch 40 loss: 1.715923547744751, 2.5678720474243164, 130.10952758789062
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6896541118621826 3.6753714084625244 185.45822143554688
Loss :  1.6809014081954956 2.860171318054199 144.68946838378906
Loss :  1.6730046272277832 2.758200168609619 139.5830078125
Loss :  1.6810804605484009 2.819288492202759 142.6455078125
Loss :  1.6678917407989502 2.460869073867798 124.71134948730469
Loss :  1.6875653266906738 2.6030969619750977 131.8424072265625
Loss :  1.7092747688293457 2.927826166152954 148.1005859375
Loss :  1.6776033639907837 2.9068877696990967 147.02198791503906
Loss :  1.7191967964172363 2.9612982273101807 149.7841033935547
Loss :  1.6825826168060303 2.5049502849578857 126.93009185791016
Loss :  1.7025960683822632 3.30967378616333 167.186279296875
Loss :  1.7013150453567505 3.070551633834839 155.22889709472656
Loss :  1.6868752241134644 2.653108835220337 134.34231567382812
Loss :  1.7069547176361084 2.4978508949279785 126.59950256347656
Loss :  1.6784521341323853 2.8832852840423584 145.84271240234375
Loss :  1.721042275428772 2.6356537342071533 133.50372314453125
Loss :  1.683335542678833 3.05733585357666 154.5501251220703
Loss :  1.6734591722488403 2.582706928253174 130.80880737304688
Loss :  1.6824419498443604 3.006317615509033 151.9983367919922
Loss :  1.7239755392074585 2.6050820350646973 131.9780731201172
  batch 60 loss: 1.7239755392074585, 2.6050820350646973, 131.9780731201172
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6733942031860352 2.7670679092407227 140.02679443359375
Loss :  1.6901416778564453 2.5299289226531982 128.18658447265625
Loss :  1.677857518196106 2.252650260925293 114.31037139892578
Loss :  1.6704875230789185 2.8164548873901367 142.49322509765625
Loss :  1.6576741933822632 2.2748467922210693 115.40001678466797
Loss :  1.6787855625152588 3.46404767036438 174.88116455078125
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.687477707862854 3.365851402282715 169.9800567626953
Loss :  1.684919834136963 3.2747559547424316 165.42271423339844
Loss :  1.6896988153457642 3.168609857559204 160.1201934814453
Total LOSS train 140.0844974224384 valid 167.60103225708008
CE LOSS train 1.6878313229634212 valid 0.42242470383644104
Contrastive LOSS train 2.7679333173311673 valid 0.792152464389801
EPOCH 215:
Loss :  1.7014445066452026 2.7893338203430176 141.1681365966797
Loss :  1.7087239027023315 3.1651523113250732 159.96633911132812
Loss :  1.6892282962799072 2.435870885848999 123.48277282714844
Loss :  1.693724274635315 2.5225939750671387 127.82342529296875
Loss :  1.7030683755874634 2.6717498302459717 135.29055786132812
Loss :  1.6804664134979248 3.150320529937744 159.1964874267578
Loss :  1.702605962753296 3.0306098461151123 153.23309326171875
Loss :  1.6873221397399902 2.944676399230957 148.921142578125
Loss :  1.6813982725143433 2.7107086181640625 137.21682739257812
Loss :  1.7045392990112305 2.4326605796813965 123.33756256103516
Loss :  1.675710678100586 2.741516590118408 138.7515411376953
Loss :  1.674788475036621 3.264998197555542 164.92469787597656
Loss :  1.6739435195922852 2.272925853729248 115.32023620605469
Loss :  1.6788262128829956 2.685511589050293 135.95440673828125
Loss :  1.7121047973632812 2.629523754119873 133.18829345703125
Loss :  1.710424542427063 2.582164764404297 130.81866455078125
Loss :  1.6708141565322876 2.451859712600708 124.26380157470703
Loss :  1.6884632110595703 2.773279905319214 140.3524627685547
Loss :  1.6691176891326904 2.8258731365203857 142.9627685546875
Loss :  1.7086557149887085 2.614936590194702 132.45547485351562
  batch 20 loss: 1.7086557149887085, 2.614936590194702, 132.45547485351562
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6870064735412598 2.58870530128479 131.12228393554688
Loss :  1.6688618659973145 2.7524778842926025 139.2927703857422
Loss :  1.68037748336792 2.2653603553771973 114.94839477539062
Loss :  1.69265615940094 2.904160261154175 146.90066528320312
Loss :  1.7099424600601196 2.664565324783325 134.93821716308594
Loss :  1.6838746070861816 2.330317974090576 118.19977569580078
Loss :  1.6890631914138794 3.340329647064209 168.70553588867188
Loss :  1.6868526935577393 3.2262632846832275 163.00003051757812
Loss :  1.653709053993225 2.4131650924682617 122.31196594238281
Loss :  1.7087618112564087 2.8817503452301025 145.79627990722656
Loss :  1.6542490720748901 2.744251012802124 138.86680603027344
Loss :  1.699048399925232 2.8866467475891113 146.03138732910156
Loss :  1.6853760480880737 2.7473273277282715 139.05174255371094
Loss :  1.684808611869812 2.571075916290283 130.23861694335938
Loss :  1.6597586870193481 3.2682390213012695 165.07171630859375
Loss :  1.670185923576355 4.026195049285889 202.9799346923828
Loss :  1.6699180603027344 2.990316152572632 151.18572998046875
Loss :  1.7086067199707031 2.7616384029388428 139.79052734375
Loss :  1.7115414142608643 2.4279489517211914 123.1089859008789
Loss :  1.7168729305267334 2.3396878242492676 118.70126342773438
  batch 40 loss: 1.7168729305267334, 2.3396878242492676, 118.70126342773438
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6913185119628906 2.6433935165405273 133.86099243164062
Loss :  1.683406949043274 2.5133185386657715 127.34933471679688
Loss :  1.6756309270858765 3.569000720977783 180.12567138671875
Loss :  1.6828911304473877 3.3884339332580566 171.10458374023438
Loss :  1.6701884269714355 2.752662420272827 139.3032989501953
Loss :  1.6890040636062622 2.4979088306427 126.58444213867188
Loss :  1.7101621627807617 2.0400655269622803 103.71343231201172
Loss :  1.6788278818130493 2.572869300842285 130.32229614257812
Loss :  1.7195850610733032 2.9263932704925537 148.03924560546875
Loss :  1.6832529306411743 3.0306882858276367 153.21766662597656
Loss :  1.7035539150238037 2.66568922996521 134.98800659179688
Loss :  1.7017608880996704 2.4955804347991943 126.48078155517578
Loss :  1.6866793632507324 2.4874603748321533 126.0596923828125
Loss :  1.7069082260131836 2.732177734375 138.3157958984375
Loss :  1.6782619953155518 3.1702685356140137 160.1916961669922
Loss :  1.720556378364563 3.7187674045562744 187.65892028808594
Loss :  1.682779312133789 2.9511189460754395 149.23873901367188
Loss :  1.6728156805038452 2.4072651863098145 122.03607177734375
Loss :  1.6821507215499878 2.64652156829834 134.0082244873047
Loss :  1.7235361337661743 2.1313114166259766 108.28910064697266
  batch 60 loss: 1.7235361337661743, 2.1313114166259766, 108.28910064697266
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.672703742980957 2.4704689979553223 125.19615936279297
Loss :  1.6901836395263672 2.319873332977295 117.68385314941406
Loss :  1.6776410341262817 3.247246026992798 164.03994750976562
Loss :  1.6705580949783325 2.875070095062256 145.424072265625
Loss :  1.6577472686767578 2.660346269607544 134.67506408691406
Loss :  1.6762256622314453 3.9053151607513428 196.94198608398438
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
Loss :  1.6849638223648071 3.9054152965545654 196.9557342529297
Loss :  1.6819369792938232 3.932229518890381 198.2934112548828
Loss :  1.6875590085983276 3.7805190086364746 190.71351623535156
Total LOSS train 139.950437105619 valid 195.7261619567871
CE LOSS train 1.6884453315001267 valid 0.4218897521495819
Contrastive LOSS train 2.765239825615516 valid 0.9451297521591187
EPOCH 216:
Loss :  1.7013747692108154 2.964801788330078 149.9414520263672
Loss :  1.7086386680603027 3.760554075241089 189.73634338378906
Loss :  1.6897425651550293 3.058831214904785 154.6313018798828
Loss :  1.6946877241134644 2.509934186935425 127.19139862060547
Loss :  1.703385829925537 2.066490888595581 105.0279312133789
Loss :  1.6814167499542236 2.280641794204712 115.71350860595703
Loss :  1.7042688131332397 2.5732786655426025 130.3682098388672
Loss :  1.6884523630142212 3.1621603965759277 159.7964630126953
Loss :  1.6815446615219116 2.786665439605713 141.0148162841797
Loss :  1.7055246829986572 2.6956276893615723 136.48690795898438
Loss :  1.6756197214126587 3.504558563232422 176.90354919433594
Loss :  1.6744023561477661 2.828787326812744 143.11376953125
Loss :  1.6739436388015747 3.2064664363861084 161.9972686767578
Loss :  1.6786715984344482 2.801417589187622 141.74954223632812
Loss :  1.7128156423568726 2.7114131450653076 137.28347778320312
Loss :  1.711753487586975 2.632561206817627 133.33981323242188
Loss :  1.6716282367706299 2.8835175037384033 145.84750366210938
Loss :  1.6905595064163208 2.78170108795166 140.77561950683594
Loss :  1.6702712774276733 2.3255245685577393 117.94650268554688
Loss :  1.7104992866516113 2.5138959884643555 127.40530395507812
  batch 20 loss: 1.7104992866516113, 2.5138959884643555, 127.40530395507812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6881736516952515 3.0546679496765137 154.42156982421875
Loss :  1.6698569059371948 2.5132460594177246 127.3321533203125
Loss :  1.6815773248672485 2.6762566566467285 135.49441528320312
Loss :  1.6941981315612793 2.8199050426483154 142.689453125
Loss :  1.7112040519714355 2.807610273361206 142.09170532226562
Loss :  1.6850312948226929 2.4016945362091064 121.7697525024414
Loss :  1.6902183294296265 2.5287911891937256 128.12977600097656
Loss :  1.6879929304122925 2.52480149269104 127.92807006835938
Loss :  1.654361367225647 2.625277519226074 132.91822814941406
Loss :  1.7092690467834473 2.6988065242767334 136.64959716796875
Loss :  1.653397798538208 3.3476128578186035 169.03404235839844
Loss :  1.6982454061508179 3.852303981781006 194.31344604492188
Loss :  1.6837708950042725 2.711419105529785 137.25473022460938
Loss :  1.6828914880752563 2.56300687789917 129.83323669433594
Loss :  1.6584584712982178 2.452751398086548 124.29602813720703
Loss :  1.6685221195220947 2.9280436038970947 148.07069396972656
Loss :  1.6681780815124512 2.4911510944366455 126.2257308959961
Loss :  1.7065234184265137 3.276502847671509 165.53167724609375
Loss :  1.7095590829849243 2.5046169757843018 126.9404067993164
Loss :  1.7148998975753784 3.734450101852417 188.43740844726562
  batch 40 loss: 1.7148998975753784, 3.734450101852417, 188.43740844726562
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6896775960922241 2.775799512863159 140.4796600341797
Loss :  1.680659294128418 2.4946131706237793 126.41131591796875
Loss :  1.6741368770599365 2.708810329437256 137.11465454101562
Loss :  1.6815334558486938 2.5398292541503906 128.67298889160156
Loss :  1.6690571308135986 2.17612624168396 110.47537231445312
Loss :  1.6882773637771606 3.0862245559692383 155.99951171875
Loss :  1.7095484733581543 2.465914487838745 125.0052719116211
Loss :  1.6780649423599243 3.307875394821167 167.07183837890625
Loss :  1.7198845148086548 2.6846508979797363 135.95242309570312
Loss :  1.6825413703918457 2.3037009239196777 116.86758422851562
Loss :  1.7036491632461548 2.6101410388946533 132.210693359375
Loss :  1.701646327972412 2.6144185066223145 132.42257690429688
Loss :  1.6871533393859863 2.180985450744629 110.7364273071289
Loss :  1.707564353942871 2.6800882816314697 135.71197509765625
Loss :  1.679549217224121 3.41676664352417 172.51788330078125
Loss :  1.7218097448349 2.4978127479553223 126.6124496459961
Loss :  1.6848679780960083 2.4933292865753174 126.35133361816406
Loss :  1.6753572225570679 2.443101167678833 123.83041381835938
Loss :  1.6843565702438354 2.9783575534820557 150.60223388671875
Loss :  1.7249611616134644 2.738237142562866 138.63681030273438
  batch 60 loss: 1.7249611616134644, 2.738237142562866, 138.63681030273438
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.675511360168457 2.430873155593872 123.21916961669922
Loss :  1.69222891330719 2.619725465774536 132.67849731445312
Loss :  1.680482029914856 3.312274694442749 167.29421997070312
Loss :  1.6733450889587402 2.9958043098449707 151.46356201171875
Loss :  1.6610007286071777 2.027719736099243 103.04698944091797
Loss :  1.6822859048843384 4.156994819641113 209.5320281982422
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6907896995544434 4.209486961364746 212.16514587402344
Loss :  1.6879993677139282 4.064479351043701 204.91197204589844
Loss :  1.6956394910812378 4.015377044677734 202.46449279785156
Total LOSS train 139.49259479229266 valid 207.2684097290039
CE LOSS train 1.6888060844861543 valid 0.42390987277030945
Contrastive LOSS train 2.756075778374305 valid 1.0038442611694336
EPOCH 217:
Loss :  1.7037346363067627 2.3437397480010986 118.8907241821289
Loss :  1.7107124328613281 2.694880247116089 136.45472717285156
Loss :  1.6922534704208374 2.514647960662842 127.42465209960938
Loss :  1.6965569257736206 2.6415772438049316 133.77542114257812
Loss :  1.7054593563079834 2.2360241413116455 113.50666809082031
Loss :  1.6832255125045776 2.3022172451019287 116.7940902709961
Loss :  1.7059656381607056 2.5035786628723145 126.88489532470703
Loss :  1.6898518800735474 3.0014281272888184 151.76126098632812
Loss :  1.6832977533340454 2.3918566703796387 121.27613067626953
Loss :  1.7064510583877563 2.3301749229431152 118.21519470214844
Loss :  1.6781632900238037 2.833143949508667 143.33535766601562
Loss :  1.6771289110183716 2.6375977993011475 133.5570068359375
Loss :  1.6763627529144287 2.523453950881958 127.84906005859375
Loss :  1.6813178062438965 2.7218241691589355 137.77252197265625
Loss :  1.7144112586975098 3.539740562438965 178.70144653320312
Loss :  1.7132805585861206 3.5525543689727783 179.34100341796875
Loss :  1.673916220664978 2.5423316955566406 128.79049682617188
Loss :  1.6919968128204346 2.506096363067627 126.99681091308594
Loss :  1.6726133823394775 2.3936030864715576 121.35276794433594
Loss :  1.7116395235061646 2.386695384979248 121.0464096069336
  batch 20 loss: 1.7116395235061646, 2.386695384979248, 121.0464096069336
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6898064613342285 2.4569597244262695 124.53778839111328
Loss :  1.6719955205917358 2.814157009124756 142.37985229492188
Loss :  1.683470368385315 2.397779941558838 121.57247161865234
Loss :  1.6948989629745483 3.767944097518921 190.09210205078125
Loss :  1.711710810661316 2.3126423358917236 117.34382629394531
Loss :  1.6858776807785034 2.3420796394348145 118.78985595703125
Loss :  1.6909101009368896 2.5889503955841064 131.138427734375
Loss :  1.688806414604187 2.59358286857605 131.36795043945312
Loss :  1.6551426649093628 2.489715099334717 126.14089965820312
Loss :  1.70973801612854 3.1893155574798584 161.17550659179688
Loss :  1.6543678045272827 3.525735378265381 177.94114685058594
Loss :  1.6991088390350342 2.747520923614502 139.07516479492188
Loss :  1.685428261756897 2.370030641555786 120.18695831298828
Loss :  1.685283899307251 2.395779609680176 121.4742660522461
Loss :  1.6604582071304321 2.7896227836608887 141.1416015625
Loss :  1.671242117881775 2.758347749710083 139.58863830566406
Loss :  1.6708356142044067 2.5537984371185303 129.36074829101562
Loss :  1.7093749046325684 3.2328848838806152 163.35362243652344
Loss :  1.7124661207199097 2.6267611980438232 133.05052185058594
Loss :  1.7178312540054321 2.234877586364746 113.46171569824219
  batch 40 loss: 1.7178312540054321, 2.234877586364746, 113.46171569824219
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.692010521888733 2.291571855545044 116.27059936523438
Loss :  1.6836044788360596 1.9188036918640137 97.62378692626953
Loss :  1.6758004426956177 2.0544097423553467 104.39629364013672
Loss :  1.683349609375 3.1382904052734375 158.59786987304688
Loss :  1.6703979969024658 2.2066147327423096 112.00112915039062
Loss :  1.6891498565673828 2.412773609161377 122.32782745361328
Loss :  1.710371971130371 3.0052990913391113 151.97532653808594
Loss :  1.6794509887695312 2.5154669284820557 127.45279693603516
Loss :  1.7205555438995361 2.6655514240264893 134.9981231689453
Loss :  1.6843037605285645 2.4624502658843994 124.80681610107422
Loss :  1.7045013904571533 2.7458205223083496 138.9955291748047
Loss :  1.702926516532898 2.5309488773345947 128.2503662109375
Loss :  1.6883747577667236 2.6489675045013428 134.13674926757812
Loss :  1.708450198173523 2.6097424030303955 132.19557189941406
Loss :  1.6802035570144653 3.3311872482299805 168.23956298828125
Loss :  1.7217531204223633 2.6794424057006836 135.69387817382812
Loss :  1.68526029586792 2.4537744522094727 124.37397766113281
Loss :  1.6757053136825562 2.2476871013641357 114.06005859375
Loss :  1.6850117444992065 2.796643018722534 141.5171661376953
Loss :  1.725856065750122 2.345449209213257 118.9983139038086
  batch 60 loss: 1.725856065750122, 2.345449209213257, 118.9983139038086
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6765581369400024 2.4043185710906982 121.89248657226562
Loss :  1.6934270858764648 4.0470147132873535 204.04415893554688
Loss :  1.6813732385635376 3.6579697132110596 184.57984924316406
Loss :  1.6746418476104736 2.870851993560791 145.2172393798828
Loss :  1.6623643636703491 3.1001553535461426 156.67013549804688
Loss :  1.6864184141159058 4.148138999938965 209.09336853027344
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6947448253631592 4.079779624938965 205.68373107910156
Loss :  1.6919053792953491 3.9885919094085693 201.1215057373047
Loss :  1.698535680770874 4.003347873687744 201.8659210205078
Total LOSS train 135.48023576002853 valid 204.44113159179688
CE LOSS train 1.6903460924441998 valid 0.4246339201927185
Contrastive LOSS train 2.6757977999173677 valid 1.000836968421936
EPOCH 218:
Loss :  1.7051748037338257 2.731886625289917 138.29949951171875
Loss :  1.7120211124420166 3.010108232498169 152.21743774414062
Loss :  1.6936719417572021 3.2121102809906006 162.29917907714844
Loss :  1.6985969543457031 4.1361799240112305 208.50758361816406
Loss :  1.7067629098892212 2.4381041526794434 123.61196899414062
Loss :  1.6856318712234497 2.388723850250244 121.121826171875
Loss :  1.7076672315597534 2.696685552597046 136.5419464111328
Loss :  1.692153811454773 2.7563672065734863 139.51051330566406
Loss :  1.685368299484253 2.8579065799713135 144.5806884765625
Loss :  1.7085505723953247 2.7601687908172607 139.7169952392578
Loss :  1.6792535781860352 2.6268258094787598 133.0205535888672
Loss :  1.6780622005462646 2.89723801612854 146.5399627685547
Loss :  1.6774858236312866 2.4757070541381836 125.46283721923828
Loss :  1.681600570678711 2.826615571975708 143.0123748779297
Loss :  1.714824914932251 2.7849316596984863 140.96141052246094
Loss :  1.7128762006759644 2.808458089828491 142.13577270507812
Loss :  1.674302339553833 2.713940143585205 137.37130737304688
Loss :  1.69253408908844 2.797612190246582 141.57315063476562
Loss :  1.6728545427322388 3.433590888977051 173.35240173339844
Loss :  1.7113935947418213 2.6657321453094482 134.9980010986328
  batch 20 loss: 1.7113935947418213, 2.6657321453094482, 134.9980010986328
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6895551681518555 3.13547420501709 158.46327209472656
Loss :  1.6719169616699219 3.445106029510498 173.92721557617188
Loss :  1.6831070184707642 2.5623838901519775 129.80230712890625
Loss :  1.694993495941162 2.7693095207214355 140.16046142578125
Loss :  1.7120825052261353 3.104377031326294 156.93093872070312
Loss :  1.6860859394073486 2.39136004447937 121.25408935546875
Loss :  1.6914535760879517 2.5981433391571045 131.59861755371094
Loss :  1.6895133256912231 2.815392255783081 142.45912170410156
Loss :  1.6562467813491821 2.9285857677459717 148.08554077148438
Loss :  1.7098388671875 3.3567659854888916 169.5481414794922
Loss :  1.6547901630401611 2.642439603805542 133.77676391601562
Loss :  1.698708415031433 2.6397159099578857 133.68450927734375
Loss :  1.6846840381622314 2.520469903945923 127.70818328857422
Loss :  1.6843889951705933 2.2732579708099365 115.34728240966797
Loss :  1.6594364643096924 2.7057414054870605 136.94650268554688
Loss :  1.6702016592025757 3.4332540035247803 173.33290100097656
Loss :  1.6699237823486328 2.8336517810821533 143.35250854492188
Loss :  1.7090864181518555 2.5302555561065674 128.22186279296875
Loss :  1.7123682498931885 2.9673728942871094 150.08102416992188
Loss :  1.7178300619125366 2.3121588230133057 117.32576751708984
  batch 40 loss: 1.7178300619125366, 2.3121588230133057, 117.32576751708984
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6922610998153687 2.995516538619995 151.4680938720703
Loss :  1.6838343143463135 2.3572094440460205 119.5443115234375
Loss :  1.6760179996490479 2.6882050037384033 136.08627319335938
Loss :  1.6838603019714355 3.0321850776672363 153.29310607910156
Loss :  1.6710944175720215 2.5076324939727783 127.05271911621094
Loss :  1.6902079582214355 2.705357551574707 136.9580841064453
Loss :  1.7117265462875366 2.37481689453125 120.45256805419922
Loss :  1.680100440979004 2.408729076385498 122.1165542602539
Loss :  1.7192981243133545 2.7269482612609863 138.06671142578125
Loss :  1.6835845708847046 2.9801225662231445 150.68971252441406
Loss :  1.7040348052978516 2.7711377143859863 140.26092529296875
Loss :  1.7020875215530396 3.2363836765289307 163.52127075195312
Loss :  1.6879041194915771 1.9428974390029907 98.83277893066406
Loss :  1.707930088043213 2.70007061958313 136.71145629882812
Loss :  1.6798150539398193 2.4017646312713623 121.76805114746094
Loss :  1.7215505838394165 2.330554485321045 118.24927520751953
Loss :  1.684568166732788 2.5593411922454834 129.65162658691406
Loss :  1.6747485399246216 2.753551483154297 139.35232543945312
Loss :  1.683956503868103 3.5029561519622803 176.83177185058594
Loss :  1.7248860597610474 2.7066550254821777 137.05763244628906
  batch 60 loss: 1.7248860597610474, 2.7066550254821777, 137.05763244628906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6756337881088257 2.683821201324463 135.86668395996094
Loss :  1.6928184032440186 2.6132802963256836 132.35684204101562
Loss :  1.6808934211730957 2.420154571533203 122.6886215209961
Loss :  1.6742810010910034 2.602947950363159 131.82168579101562
Loss :  1.6620062589645386 3.15297532081604 159.31077575683594
Loss :  1.6827071905136108 3.6060421466827393 181.9848175048828
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.6910964250564575 3.532031297683716 178.29266357421875
Loss :  1.6884937286376953 3.418780565261841 172.6275177001953
Loss :  1.6942474842071533 3.4895131587982178 176.16990661621094
Total LOSS train 140.25926584097056 valid 177.26872634887695
CE LOSS train 1.690555374438946 valid 0.42356187105178833
Contrastive LOSS train 2.771374205442575 valid 0.8723782896995544
EPOCH 219:
Loss :  1.7046276330947876 2.9389142990112305 148.65032958984375
Loss :  1.7117427587509155 2.546807289123535 129.05210876464844
Loss :  1.6936426162719727 3.16898250579834 160.1427764892578
Loss :  1.6987038850784302 3.5710997581481934 180.25369262695312
Loss :  1.7074625492095947 2.940744161605835 148.74465942382812
Loss :  1.6865681409835815 2.5443737506866455 128.90525817871094
Loss :  1.7080154418945312 2.694904327392578 136.45321655273438
Loss :  1.6931185722351074 2.4823639392852783 125.81130981445312
Loss :  1.6866687536239624 2.246065378189087 113.98993682861328
Loss :  1.7096490859985352 2.4289019107818604 123.15474700927734
Loss :  1.6808356046676636 2.863003730773926 144.83102416992188
Loss :  1.6792774200439453 2.498444080352783 126.60147857666016
Loss :  1.6783372163772583 2.427338123321533 123.04524230957031
Loss :  1.6823315620422363 2.6043505668640137 131.8998565673828
Loss :  1.7147166728973389 3.2606704235076904 164.7482452392578
Loss :  1.7134941816329956 3.0810954570770264 155.76828002929688
Loss :  1.673897624015808 3.1239633560180664 157.8720703125
Loss :  1.691603422164917 2.980109930038452 150.6970977783203
Loss :  1.6715596914291382 2.259490489959717 114.64608764648438
Loss :  1.7107313871383667 2.485504388809204 125.98595428466797
  batch 20 loss: 1.7107313871383667, 2.485504388809204, 125.98595428466797
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
Loss :  1.6885228157043457 2.6966404914855957 136.5205535888672
Loss :  1.6706364154815674 2.44342303276062 123.84178924560547
Loss :  1.682145595550537 2.529895067214966 128.17689514160156
Loss :  1.6942068338394165 2.5871644020080566 131.05242919921875
Loss :  1.7112171649932861 2.5435667037963867 128.88955688476562
Loss :  1.685380458831787 2.8406717777252197 143.71896362304688
Loss :  1.6907083988189697 2.479536294937134 125.66752624511719
Loss :  1.6886605024337769 2.7241873741149902 137.8980255126953
Loss :  1.655879259109497 2.6948277950286865 136.39727783203125
Loss :  1.7098685503005981 3.192826271057129 161.35118103027344
Loss :  1.6557568311691284 2.441162347793579 123.71387481689453
Loss :  1.6995893716812134 2.9619834423065186 149.7987518310547
Loss :  1.6862831115722656 2.497427463531494 126.55766296386719
Loss :  1.6860543489456177 2.485330104827881 125.95256042480469
Loss :  1.6620151996612549 2.600933313369751 131.70867919921875
Loss :  1.6725744009017944 2.366698741912842 120.00751495361328
Loss :  1.672378659248352 2.4189295768737793 122.61885833740234
Loss :  1.7107537984848022 2.3693435192108154 120.17793273925781
Loss :  1.7134876251220703 2.825141191482544 142.97055053710938
Loss :  1.718943476676941 3.0747480392456055 155.4563446044922
  batch 40 loss: 1.718943476676941, 3.0747480392456055, 155.4563446044922
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.693795919418335 2.517134666442871 127.55052947998047
Loss :  1.685562252998352 2.0171947479248047 102.54530334472656
Loss :  1.6777070760726929 2.169088840484619 110.13214874267578
Loss :  1.6848212480545044 2.323686361312866 117.869140625
Loss :  1.6718742847442627 2.1493330001831055 109.1385269165039
Loss :  1.6907089948654175 2.4270455837249756 123.0429916381836
Loss :  1.7114347219467163 2.436232805252075 123.5230712890625
Loss :  1.6807940006256104 2.693528413772583 136.3572235107422
Loss :  1.7216439247131348 2.943804979324341 148.91189575195312
Loss :  1.6855511665344238 2.975738286972046 150.4724578857422
Loss :  1.7057769298553467 2.590728998184204 131.2422332763672
Loss :  1.7041255235671997 3.2117249965667725 162.29037475585938
Loss :  1.6900699138641357 2.5122876167297363 127.30445098876953
Loss :  1.7097690105438232 2.6360509395599365 133.51231384277344
Loss :  1.682677984237671 2.3705592155456543 120.21063995361328
Loss :  1.723462700843811 2.3386712074279785 118.65702819824219
Loss :  1.6877636909484863 2.6740987300872803 135.3927001953125
Loss :  1.6782889366149902 2.6244819164276123 132.90237426757812
Loss :  1.687279224395752 3.039382219314575 153.65640258789062
Loss :  1.7269678115844727 3.573866128921509 180.4202880859375
  batch 60 loss: 1.7269678115844727, 3.573866128921509, 180.4202880859375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6782941818237305 2.7337777614593506 138.3671875
Loss :  1.6940631866455078 2.6005961894989014 131.723876953125
Loss :  1.6823376417160034 2.3507044315338135 119.21755981445312
Loss :  1.6759517192840576 2.5491020679473877 129.1310577392578
Loss :  1.663558006286621 2.740931272506714 138.7101287841797
Loss :  1.6819761991500854 4.1391801834106445 208.6409912109375
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
Loss :  1.6900206804275513 4.207486629486084 212.06434631347656
Loss :  1.6875927448272705 4.036997318267822 203.53746032714844
Loss :  1.6929988861083984 3.9465746879577637 199.021728515625
Total LOSS train 134.86172626201923 valid 205.81613159179688
CE LOSS train 1.691481955234821 valid 0.4232497215270996
Contrastive LOSS train 2.663404864531297 valid 0.9866436719894409
EPOCH 220:
Loss :  1.7044622898101807 2.603696346282959 131.8892822265625
Loss :  1.7123602628707886 3.4877946376800537 176.10208129882812
Loss :  1.6946072578430176 2.5752477645874023 130.45700073242188
Loss :  1.699123740196228 2.816553831100464 142.52682495117188
Loss :  1.7085742950439453 2.3624987602233887 119.83351135253906
Loss :  1.687279224395752 2.5738558769226074 130.3800811767578
Loss :  1.709730625152588 2.7062323093414307 137.02134704589844
Loss :  1.6946126222610474 2.6108312606811523 132.23617553710938
Loss :  1.688298225402832 3.7509548664093018 189.2360382080078
Loss :  1.7113313674926758 2.6076552867889404 132.09410095214844
Loss :  1.6835699081420898 2.520153522491455 127.69124603271484
Loss :  1.6823786497116089 2.7485291957855225 139.10882568359375
Loss :  1.6816093921661377 2.5939574241638184 131.37948608398438
Loss :  1.6856447458267212 2.572242259979248 130.29774475097656
Loss :  1.7171883583068848 2.99839448928833 151.6369171142578
Loss :  1.71657133102417 4.025095462799072 202.97134399414062
Loss :  1.6780112981796265 3.0933425426483154 156.3451385498047
Loss :  1.6957648992538452 3.076206684112549 155.506103515625
Loss :  1.6764051914215088 3.3078367710113525 167.06825256347656
Loss :  1.7143983840942383 3.7311582565307617 188.27230834960938
  batch 20 loss: 1.7143983840942383, 3.7311582565307617, 188.27230834960938
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6925411224365234 2.8600893020629883 144.69700622558594
Loss :  1.67530357837677 2.8129119873046875 142.32090759277344
Loss :  1.6863951683044434 3.6270930767059326 183.0410614013672
Loss :  1.6980090141296387 2.747028350830078 139.04942321777344
Loss :  1.7141913175582886 2.9741032123565674 150.41934204101562
Loss :  1.6891498565673828 3.0372796058654785 153.55313110351562
Loss :  1.6940820217132568 2.630748748779297 133.2315216064453
Loss :  1.6913669109344482 2.43941068649292 123.66190338134766
Loss :  1.658503532409668 2.5602493286132812 129.6709747314453
Loss :  1.711787223815918 2.8560092449188232 144.5122528076172
Loss :  1.6571283340454102 3.3184726238250732 167.5807647705078
Loss :  1.700641393661499 2.928431510925293 148.12220764160156
Loss :  1.686748743057251 2.832045555114746 143.28903198242188
Loss :  1.68666672706604 2.5901174545288086 131.19253540039062
Loss :  1.6617372035980225 2.681128978729248 135.7181854248047
Loss :  1.672214388847351 2.492366075515747 126.29051208496094
Loss :  1.671901822090149 2.8255035877227783 142.94708251953125
Loss :  1.7101582288742065 2.576447010040283 130.5325164794922
Loss :  1.7133562564849854 2.3826253414154053 120.84461975097656
Loss :  1.7188739776611328 2.545010566711426 128.9694061279297
  batch 40 loss: 1.7188739776611328, 2.545010566711426, 128.9694061279297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6936373710632324 2.7637693881988525 139.88211059570312
Loss :  1.685239315032959 2.630683422088623 133.2194061279297
Loss :  1.6776312589645386 3.2841787338256836 165.88656616210938
Loss :  1.6854941844940186 2.728321075439453 138.10154724121094
Loss :  1.672946572303772 2.8532652854919434 144.33621215820312
Loss :  1.6921617984771729 2.5356390476226807 128.47412109375
Loss :  1.7130333185195923 3.485407829284668 175.98341369628906
Loss :  1.682792067527771 2.994194746017456 151.39251708984375
Loss :  1.7228144407272339 2.5346858501434326 128.4571075439453
Loss :  1.6879663467407227 2.8994734287261963 146.66165161132812
Loss :  1.707567572593689 2.6251423358917236 132.9646759033203
Loss :  1.7060884237289429 2.9714200496673584 150.27708435058594
Loss :  1.6919105052947998 3.9260916709899902 197.99649047851562
Loss :  1.7113001346588135 2.4727444648742676 125.34852600097656
Loss :  1.683845043182373 2.8021228313446045 141.78997802734375
Loss :  1.724247932434082 3.5149600505828857 177.4722442626953
Loss :  1.688294768333435 2.601431131362915 131.75985717773438
Loss :  1.6787666082382202 2.6301968097686768 133.18861389160156
Loss :  1.6875909566879272 2.782419443130493 140.80856323242188
Loss :  1.7271593809127808 2.764754295349121 139.96487426757812
  batch 60 loss: 1.7271593809127808, 2.764754295349121, 139.96487426757812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6786764860153198 3.776217460632324 190.4895477294922
Loss :  1.694791555404663 3.3441994190216064 168.90476989746094
Loss :  1.6826906204223633 2.97160267829895 150.26283264160156
Loss :  1.6755931377410889 2.37776255607605 120.563720703125
Loss :  1.6629670858383179 2.128659725189209 108.095947265625
Loss :  1.6823065280914307 3.909196615219116 197.1421356201172
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
Loss :  1.690374493598938 3.8948352336883545 196.43212890625
Loss :  1.687939167022705 3.6855123043060303 185.96356201171875
Loss :  1.6932125091552734 3.6108484268188477 182.23562622070312
Total LOSS train 145.44585500863883 valid 190.44336318969727
CE LOSS train 1.6930443965471709 valid 0.42330312728881836
Contrastive LOSS train 2.8750562080970177 valid 0.9027121067047119
EPOCH 221:
Loss :  1.7045336961746216 2.3655521869659424 119.98214721679688
Loss :  1.7117259502410889 2.956406831741333 149.53207397460938
Loss :  1.6937847137451172 3.572524070739746 180.3199920654297
Loss :  1.6982970237731934 3.0107438564300537 152.23548889160156
Loss :  1.7069101333618164 2.3433802127838135 118.87592315673828
Loss :  1.6858662366867065 2.80362868309021 141.86729431152344
Loss :  1.7075303792953491 2.743560791015625 138.8855743408203
Loss :  1.6926259994506836 3.411025047302246 172.24388122558594
Loss :  1.6861677169799805 3.8422775268554688 193.800048828125
Loss :  1.7096152305603027 2.4567201137542725 124.54562377929688
Loss :  1.681497573852539 2.7135980129241943 137.3614044189453
Loss :  1.6805753707885742 4.025143623352051 202.93775939941406
Loss :  1.6799814701080322 3.2431800365448 163.83897399902344
Loss :  1.6848911046981812 2.7547829151153564 139.42404174804688
Loss :  1.7169204950332642 3.7736823558807373 190.40103149414062
Loss :  1.7163736820220947 2.696718692779541 136.55230712890625
Loss :  1.6775171756744385 2.8128488063812256 142.31996154785156
Loss :  1.6950263977050781 2.104377508163452 106.91389465332031
Loss :  1.6755907535552979 2.8092589378356934 142.1385498046875
Loss :  1.7142868041992188 2.583954334259033 130.91201782226562
  batch 20 loss: 1.7142868041992188, 2.583954334259033, 130.91201782226562
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6922473907470703 2.51788067817688 127.5862808227539
Loss :  1.6746195554733276 2.5019025802612305 126.7697525024414
Loss :  1.685973882675171 2.64072322845459 133.72213745117188
Loss :  1.6972236633300781 2.449427366256714 124.16859436035156
Loss :  1.7134568691253662 2.8791096210479736 145.66893005371094
Loss :  1.6885110139846802 2.61334490776062 132.3557586669922
Loss :  1.6931259632110596 2.8224494457244873 142.8155975341797
Loss :  1.6914273500442505 2.3359756469726562 118.4902114868164
Loss :  1.659282922744751 2.4240012168884277 122.85934448242188
Loss :  1.712203025817871 2.607898235321045 132.10711669921875
Loss :  1.6586061716079712 2.8265347480773926 142.98533630371094
Loss :  1.7015935182571411 2.66743540763855 135.0733642578125
Loss :  1.68863844871521 2.7236568927764893 137.87149047851562
Loss :  1.6885181665420532 2.4594106674194336 124.65904998779297
Loss :  1.6645383834838867 3.149142026901245 159.12164306640625
Loss :  1.6749640703201294 3.463012456893921 174.82557678222656
Loss :  1.6749299764633179 2.8348214626312256 143.41600036621094
Loss :  1.7132762670516968 2.53257155418396 128.34185791015625
Loss :  1.7160230875015259 2.6641266345977783 134.92234802246094
Loss :  1.7212003469467163 2.5454611778259277 128.9942626953125
  batch 40 loss: 1.7212003469467163, 2.5454611778259277, 128.9942626953125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6970226764678955 2.735978364944458 138.49594116210938
Loss :  1.6886695623397827 2.178497076034546 110.61351776123047
Loss :  1.6809097528457642 2.7648849487304688 139.92515563964844
Loss :  1.6883385181427002 2.724608898162842 137.9187774658203
Loss :  1.6755566596984863 2.729752540588379 138.16317749023438
Loss :  1.6935677528381348 2.863539934158325 144.87057495117188
Loss :  1.714098334312439 2.3467648029327393 119.05233764648438
Loss :  1.6837023496627808 2.3257522583007812 117.9713134765625
Loss :  1.7238489389419556 2.70599627494812 137.02366638183594
Loss :  1.6878633499145508 2.4827117919921875 125.82345581054688
Loss :  1.7076029777526855 3.0309815406799316 153.2566680908203
Loss :  1.705574631690979 2.677434206008911 135.57728576660156
Loss :  1.691234827041626 2.4711849689483643 125.25048828125
Loss :  1.7104682922363281 2.342184543609619 118.8197021484375
Loss :  1.6829164028167725 2.8214993476867676 142.7578887939453
Loss :  1.7233699560165405 2.166388511657715 110.04280090332031
Loss :  1.6875293254852295 2.544083595275879 128.89170837402344
Loss :  1.6780401468276978 2.816962480545044 142.5261688232422
Loss :  1.68683922290802 3.0539867877960205 154.3861846923828
Loss :  1.726817011833191 2.530602216720581 128.25692749023438
  batch 60 loss: 1.726817011833191, 2.530602216720581, 128.25692749023438
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6786143779754639 2.4842398166656494 125.8906021118164
Loss :  1.6949609518051147 2.707415819168091 137.0657501220703
Loss :  1.6830116510391235 2.4290931224823 123.1376724243164
Loss :  1.675864338874817 2.7397308349609375 138.6623992919922
Loss :  1.6633007526397705 2.090548276901245 106.19071197509766
Loss :  1.6837940216064453 3.709423542022705 187.15496826171875
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.6923003196716309 3.704272747039795 186.90594482421875
Loss :  1.68980872631073 3.630681037902832 183.22386169433594
Loss :  1.6937288045883179 3.419929027557373 172.69017028808594
Total LOSS train 138.19097724327673 valid 182.49373626708984
CE LOSS train 1.6932277037547185 valid 0.42343220114707947
Contrastive LOSS train 2.7299549763019266 valid 0.8549822568893433
EPOCH 222:
Loss :  1.7052068710327148 2.8062095642089844 142.01568603515625
Loss :  1.7120695114135742 3.038970470428467 153.66058349609375
Loss :  1.6933071613311768 2.2301595211029053 113.20127868652344
Loss :  1.6986805200576782 2.272351026535034 115.31623077392578
Loss :  1.7077895402908325 2.605565071105957 131.98605346679688
Loss :  1.6870431900024414 2.661874532699585 134.78076171875
Loss :  1.7083606719970703 2.725938558578491 138.00527954101562
Loss :  1.6930272579193115 2.948206663131714 149.10336303710938
Loss :  1.6866512298583984 2.471794843673706 125.27639770507812
Loss :  1.7096693515777588 2.472412586212158 125.3302993774414
Loss :  1.6812273263931274 2.8271398544311523 143.03822326660156
Loss :  1.6803486347198486 2.689256191253662 136.14315795898438
Loss :  1.6797654628753662 2.4458634853363037 123.97293853759766
Loss :  1.6842447519302368 2.846721649169922 144.0203399658203
Loss :  1.7168126106262207 3.384248971939087 170.92926025390625
Loss :  1.7153544425964355 2.72541880607605 137.9862823486328
Loss :  1.6773227453231812 2.6471076011657715 134.03269958496094
Loss :  1.6951380968093872 2.5613925457000732 129.7647705078125
Loss :  1.6756442785263062 2.361541271209717 119.7527084350586
Loss :  1.7136555910110474 2.432135820388794 123.32044219970703
  batch 20 loss: 1.7136555910110474, 2.432135820388794, 123.32044219970703
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6918762922286987 2.4725310802459717 125.31843566894531
Loss :  1.6741995811462402 2.6428463459014893 133.81651306152344
Loss :  1.685412049293518 2.5906732082366943 131.2190704345703
Loss :  1.6968756914138794 2.0700275897979736 105.19825744628906
Loss :  1.7134817838668823 2.5244455337524414 127.93575286865234
Loss :  1.688016414642334 2.5177578926086426 127.57591247558594
Loss :  1.6930395364761353 3.071565866470337 155.27133178710938
Loss :  1.6906439065933228 2.5411083698272705 128.74606323242188
Loss :  1.6578530073165894 2.5629754066467285 129.80662536621094
Loss :  1.7115837335586548 2.705573797225952 136.99026489257812
Loss :  1.6569812297821045 2.6118898391723633 132.25148010253906
Loss :  1.7005139589309692 2.6710240840911865 135.25172424316406
Loss :  1.6867181062698364 2.31868052482605 117.6207504272461
Loss :  1.6865179538726807 2.1616628170013428 109.7696533203125
Loss :  1.6620705127716064 2.3996307849884033 121.64360809326172
Loss :  1.672677993774414 2.4184939861297607 122.59737396240234
Loss :  1.6726574897766113 2.3259897232055664 117.9721450805664
Loss :  1.7111238241195679 2.4826438426971436 125.84331512451172
Loss :  1.7141889333724976 3.356910228729248 169.5596923828125
Loss :  1.7195515632629395 3.03629469871521 153.53428649902344
  batch 40 loss: 1.7195515632629395, 3.03629469871521, 153.53428649902344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4], device='cuda:0')
Loss :  1.6945390701293945 2.9809794425964355 150.74349975585938
Loss :  1.6855950355529785 2.4948599338531494 126.4285888671875
Loss :  1.6777819395065308 2.5461535453796387 128.98545837402344
Loss :  1.6853610277175903 2.1361172199249268 108.49122619628906
Loss :  1.6722410917282104 2.258406639099121 114.59257507324219
Loss :  1.6911323070526123 2.152031898498535 109.292724609375
Loss :  1.7117270231246948 3.3477258682250977 169.0980224609375
Loss :  1.680803894996643 2.8555409908294678 144.45785522460938
Loss :  1.722163200378418 2.4931082725524902 126.37757110595703
Loss :  1.685431957244873 2.5131425857543945 127.34255981445312
Loss :  1.7060219049453735 2.5419130325317383 128.80166625976562
Loss :  1.7040852308273315 3.0614302158355713 154.77560424804688
Loss :  1.6899783611297607 2.251624345779419 114.27119445800781
Loss :  1.7095979452133179 2.448709011077881 124.1450424194336
Loss :  1.6821168661117554 2.79826283454895 141.5952606201172
Loss :  1.7236183881759644 2.5448501110076904 128.96612548828125
Loss :  1.6872198581695557 2.542881727218628 128.83131408691406
Loss :  1.6776751279830933 2.8976457118988037 146.55995178222656
Loss :  1.6867936849594116 2.7024545669555664 136.8095245361328
Loss :  1.7269432544708252 2.3638296127319336 119.91842651367188
  batch 60 loss: 1.7269432544708252, 2.3638296127319336, 119.91842651367188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6783033609390259 2.4412193298339844 123.73927307128906
Loss :  1.694525122642517 2.5395126342773438 128.67015075683594
Loss :  1.682342767715454 2.3151066303253174 117.43767547607422
Loss :  1.6755069494247437 2.651156187057495 134.2333221435547
Loss :  1.6627435684204102 1.9309711456298828 98.21129608154297
Loss :  1.6794507503509521 3.8851165771484375 195.93527221679688
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6880253553390503 3.970447540283203 200.21038818359375
Loss :  1.6848573684692383 3.8177993297576904 192.5748291015625
Loss :  1.6907655000686646 3.8708906173706055 195.23529052734375
Total LOSS train 131.26669111985427 valid 195.98894500732422
CE LOSS train 1.6923315653434166 valid 0.42269137501716614
Contrastive LOSS train 2.59148720227755 valid 0.9677226543426514
EPOCH 223:
Loss :  1.7046253681182861 2.2196357250213623 112.68641662597656
Loss :  1.7117196321487427 2.309023141860962 117.16287994384766
Loss :  1.6931320428848267 2.219719171524048 112.67909240722656
Loss :  1.6979399919509888 2.7037012577056885 136.88299560546875
Loss :  1.7067475318908691 2.3260152339935303 118.00750732421875
Loss :  1.6856375932693481 2.4540023803710938 124.38575744628906
Loss :  1.7074275016784668 2.802262783050537 141.82057189941406
Loss :  1.6923896074295044 2.3799750804901123 120.69114685058594
Loss :  1.6859915256500244 2.495316982269287 126.45183563232422
Loss :  1.709436297416687 2.9581151008605957 149.61520385742188
Loss :  1.680955410003662 2.447844982147217 124.07320404052734
Loss :  1.6798409223556519 2.306210517883301 116.99036407470703
Loss :  1.6794986724853516 3.3870272636413574 171.03086853027344
Loss :  1.683940052986145 2.4354703426361084 123.45745086669922
Loss :  1.7164230346679688 2.798536777496338 141.64324951171875
Loss :  1.7156908512115479 3.227309465408325 163.0811767578125
Loss :  1.6774234771728516 3.4197134971618652 172.66310119628906
Loss :  1.6957173347473145 2.582737445831299 130.8325958251953
Loss :  1.6764472723007202 2.6578738689422607 134.5701446533203
Loss :  1.7147276401519775 2.356835126876831 119.55648803710938
  batch 20 loss: 1.7147276401519775, 2.356835126876831, 119.55648803710938
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6930598020553589 2.455214023590088 124.45376586914062
Loss :  1.6759181022644043 2.6142568588256836 132.38876342773438
Loss :  1.6868796348571777 2.2397005558013916 113.67191314697266
Loss :  1.6976574659347534 3.8057849407196045 191.98690795898438
Loss :  1.714410424232483 2.519690990447998 127.69895935058594
Loss :  1.690041184425354 2.118199586868286 107.60002136230469
Loss :  1.6945109367370605 2.1359434127807617 108.49168395996094
Loss :  1.692081332206726 2.0509397983551025 104.23906707763672
Loss :  1.6593096256256104 2.6600329875946045 134.6609649658203
Loss :  1.7123501300811768 2.5705294609069824 130.23883056640625
Loss :  1.6582213640213013 2.631652593612671 133.2408447265625
Loss :  1.7013452053070068 2.60719895362854 132.06129455566406
Loss :  1.6877278089523315 2.585350513458252 130.95526123046875
Loss :  1.6874816417694092 2.750005006790161 139.18772888183594
Loss :  1.6633234024047852 2.744331121444702 138.8798828125
Loss :  1.6740330457687378 2.6501007080078125 134.17906188964844
Loss :  1.6740305423736572 3.2030043601989746 161.82424926757812
Loss :  1.712164044380188 2.30968976020813 117.1966552734375
Loss :  1.7152799367904663 2.508444309234619 127.13749694824219
Loss :  1.7207762002944946 2.466698408126831 125.05570220947266
  batch 40 loss: 1.7207762002944946, 2.466698408126831, 125.05570220947266
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.695864200592041 2.607943058013916 132.093017578125
Loss :  1.6876282691955566 2.2815284729003906 115.76405334472656
Loss :  1.6798882484436035 2.5677707195281982 130.06842041015625
Loss :  1.6872856616973877 2.706547975540161 137.01467895507812
Loss :  1.6747682094573975 2.081385374069214 105.74403381347656
Loss :  1.6933162212371826 2.4776828289031982 125.57745361328125
Loss :  1.714038610458374 2.661939859390259 134.81103515625
Loss :  1.683959722518921 2.820388078689575 142.703369140625
Loss :  1.7240726947784424 2.725064516067505 137.977294921875
Loss :  1.6883916854858398 2.985370635986328 150.9569091796875
Loss :  1.7078596353530884 2.6891701221466064 136.16636657714844
Loss :  1.7059650421142578 2.4823219776153564 125.82205963134766
Loss :  1.6918624639511108 2.6495301723480225 134.16836547851562
Loss :  1.7107993364334106 2.7816762924194336 140.79461669921875
Loss :  1.6833035945892334 2.8247506618499756 142.92083740234375
Loss :  1.7240569591522217 2.6442952156066895 133.93882751464844
Loss :  1.687870979309082 3.715085744857788 187.44215393066406
Loss :  1.6782301664352417 2.595989465713501 131.47769165039062
Loss :  1.687095284461975 2.719158887863159 137.64503479003906
Loss :  1.7267600297927856 2.2632343769073486 114.88848114013672
  batch 60 loss: 1.7267600297927856, 2.2632343769073486, 114.88848114013672
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6783496141433716 2.3958756923675537 121.47213745117188
Loss :  1.694766640663147 2.744877576828003 138.9386444091797
Loss :  1.682968258857727 2.5374977588653564 128.557861328125
Loss :  1.676245093345642 2.6165034770965576 132.5014190673828
Loss :  1.6640387773513794 2.4979333877563477 126.56070709228516
Loss :  1.6826070547103882 4.122775554656982 207.82138061523438
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
Loss :  1.6907966136932373 4.021859169006348 202.78375244140625
Loss :  1.6881930828094482 3.937915325164795 198.58395385742188
Loss :  1.6935369968414307 3.911181688308716 197.25262451171875
Total LOSS train 132.60671656681941 valid 201.6104278564453
CE LOSS train 1.6931645998587974 valid 0.42338424921035767
Contrastive LOSS train 2.6182710280785195 valid 0.977795422077179
EPOCH 224:
Loss :  1.7056127786636353 2.750009775161743 139.2061004638672
Loss :  1.7125287055969238 2.760383367538452 139.731689453125
Loss :  1.6947540044784546 2.292083978652954 116.29895782470703
Loss :  1.6994284391403198 2.3861372470855713 121.00628662109375
Loss :  1.708149790763855 2.2234177589416504 112.87904357910156
Loss :  1.6871440410614014 2.3721811771392822 120.29620361328125
Loss :  1.709240198135376 2.4242095947265625 122.91972351074219
Loss :  1.6938669681549072 3.1539371013641357 159.39071655273438
Loss :  1.6875503063201904 2.398690938949585 121.62210083007812
Loss :  1.7103943824768066 2.5945372581481934 131.437255859375
Loss :  1.6822749376296997 2.7165772914886475 137.51113891601562
Loss :  1.6815228462219238 2.2601122856140137 114.68714141845703
Loss :  1.6815320253372192 2.5033345222473145 126.84825897216797
Loss :  1.685750126838684 2.866546630859375 145.01307678222656
Loss :  1.7173097133636475 3.662108898162842 184.82275390625
Loss :  1.7161728143692017 2.4356017112731934 123.49625396728516
Loss :  1.6788591146469116 2.5829689502716064 130.8273162841797
Loss :  1.6965668201446533 2.5489578247070312 129.1444549560547
Loss :  1.6777945756912231 2.4094972610473633 122.15265655517578
Loss :  1.7154269218444824 2.60471248626709 131.9510498046875
  batch 20 loss: 1.7154269218444824, 2.60471248626709, 131.9510498046875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.694211483001709 2.914135456085205 147.40098571777344
Loss :  1.6771944761276245 3.017486572265625 152.55152893066406
Loss :  1.6883089542388916 3.0255138874053955 152.96400451660156
Loss :  1.6990617513656616 2.4754858016967773 125.47335052490234
Loss :  1.7152477502822876 2.58031964302063 130.73121643066406
Loss :  1.6908998489379883 2.438117742538452 123.59678649902344
Loss :  1.695906639099121 3.1268835067749023 158.0400848388672
Loss :  1.6939111948013306 3.8120291233062744 192.2953643798828
Loss :  1.6624369621276855 2.0625784397125244 104.7913589477539
Loss :  1.7136576175689697 2.863001823425293 144.86373901367188
Loss :  1.661333680152893 2.5315370559692383 128.23818969726562
Loss :  1.7033586502075195 2.579540729522705 130.68038940429688
Loss :  1.6902309656143188 2.40960431098938 122.17044830322266
Loss :  1.6897046566009521 3.3237602710723877 167.87771606445312
Loss :  1.6660664081573486 3.351283550262451 169.23023986816406
Loss :  1.6762192249298096 2.602062225341797 131.7793426513672
Loss :  1.6760261058807373 2.244187116622925 113.88538360595703
Loss :  1.7134356498718262 2.781179904937744 140.77243041992188
Loss :  1.7163268327713013 2.452662706375122 124.34945678710938
Loss :  1.721706748008728 2.781081438064575 140.77578735351562
  batch 40 loss: 1.721706748008728, 2.781081438064575, 140.77578735351562
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6973520517349243 2.8468356132507324 144.0391387939453
Loss :  1.6888766288757324 2.689899444580078 136.183837890625
Loss :  1.6816213130950928 3.047027587890625 154.0330047607422
Loss :  1.6892595291137695 2.4619710445404053 124.78781127929688
Loss :  1.6769860982894897 2.0566790103912354 104.51094055175781
Loss :  1.6956599950790405 2.37392258644104 120.39179229736328
Loss :  1.7158409357070923 2.685368299484253 135.9842529296875
Loss :  1.6865938901901245 2.6684718132019043 135.1101837158203
Loss :  1.7260513305664062 2.751297950744629 139.29095458984375
Loss :  1.6912686824798584 2.4753851890563965 125.46052551269531
Loss :  1.7106952667236328 3.2680463790893555 165.11300659179688
Loss :  1.7091032266616821 2.562725305557251 129.84536743164062
Loss :  1.695820689201355 2.6997578144073486 136.68370056152344
Loss :  1.7150334119796753 2.4170150756835938 122.56578826904297
Loss :  1.6886202096939087 2.8234143257141113 142.8593292236328
Loss :  1.7279374599456787 3.284534215927124 165.95465087890625
Loss :  1.693616271018982 2.584831953048706 130.93521118164062
Loss :  1.6839931011199951 2.329390287399292 118.15350341796875
Loss :  1.692399263381958 2.7447826862335205 138.93153381347656
Loss :  1.730322241783142 2.4480860233306885 124.13462829589844
  batch 60 loss: 1.730322241783142, 2.4480860233306885, 124.13462829589844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6831998825073242 2.6090927124023438 132.13783264160156
Loss :  1.6984751224517822 3.0207407474517822 152.73550415039062
Loss :  1.6869345903396606 2.7584493160247803 139.60940551757812
Loss :  1.6800204515457153 3.003878355026245 151.8739471435547
Loss :  1.6680269241333008 2.301692485809326 116.75265502929688
Loss :  1.6880449056625366 4.265074729919434 214.94178771972656
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6962064504623413 4.304520606994629 216.9222412109375
Loss :  1.6937111616134644 4.163995265960693 209.8934783935547
Loss :  1.6996116638183594 4.193785667419434 211.38890075683594
Total LOSS train 135.7198229276217 valid 213.28660202026367
CE LOSS train 1.6953974412037776 valid 0.42490291595458984
Contrastive LOSS train 2.6804885167341967 valid 1.0484464168548584
EPOCH 225:
Loss :  1.7086814641952515 2.1461150646209717 109.01443481445312
Loss :  1.7151683568954468 2.4153168201446533 122.48101043701172
Loss :  1.6971025466918945 2.524897336959839 127.94197082519531
Loss :  1.7016249895095825 2.521918773651123 127.79756164550781
Loss :  1.7098757028579712 2.405266284942627 121.97319030761719
Loss :  1.6894291639328003 2.458198308944702 124.5993423461914
Loss :  1.7105662822723389 2.670254945755005 135.2233123779297
Loss :  1.6955523490905762 2.385861873626709 120.98863983154297
Loss :  1.6891587972640991 2.5522165298461914 129.29998779296875
Loss :  1.7116190195083618 2.663973093032837 134.9102783203125
Loss :  1.6838459968566895 2.6072676181793213 132.0472412109375
Loss :  1.6827142238616943 2.962059736251831 149.7856903076172
Loss :  1.6824008226394653 2.9144937992095947 147.40708923339844
Loss :  1.686661958694458 2.7779507637023926 140.58419799804688
Loss :  1.7184109687805176 2.4102978706359863 122.23330688476562
Loss :  1.717267394065857 2.519476890563965 127.69111633300781
Loss :  1.6797270774841309 2.5430750846862793 128.83348083496094
Loss :  1.6973084211349487 2.678772211074829 135.63592529296875
Loss :  1.67839777469635 2.7338054180145264 138.36866760253906
Loss :  1.7156840562820435 2.4404313564300537 123.73725128173828
  batch 20 loss: 1.7156840562820435, 2.4404313564300537, 123.73725128173828
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6943469047546387 2.422891139984131 122.83889770507812
Loss :  1.6773265600204468 2.5210092067718506 127.72779083251953
Loss :  1.6883047819137573 2.2168259620666504 112.52960205078125
Loss :  1.6992319822311401 2.6490845680236816 134.15345764160156
Loss :  1.715258240699768 2.4587275981903076 124.6516342163086
Loss :  1.6904628276824951 2.546736001968384 129.0272674560547
Loss :  1.6948041915893555 2.8398218154907227 143.68589782714844
Loss :  1.6919376850128174 2.4489009380340576 124.1369857788086
Loss :  1.6596243381500244 3.1029067039489746 156.80496215820312
Loss :  1.7121268510818481 2.6696183681488037 135.1930389404297
Loss :  1.6582772731781006 2.7938568592071533 141.3511199951172
Loss :  1.7009716033935547 2.8738088607788086 145.39141845703125
Loss :  1.6878794431686401 3.3432815074920654 168.85195922851562
Loss :  1.6876847743988037 2.6834802627563477 135.8616943359375
Loss :  1.6636273860931396 2.7117092609405518 137.24908447265625
Loss :  1.6740399599075317 3.0102591514587402 152.18699645996094
Loss :  1.6738282442092896 3.1161718368530273 157.482421875
Loss :  1.7115072011947632 2.0970892906188965 106.56597137451172
Loss :  1.7146949768066406 2.169642210006714 110.19680786132812
Loss :  1.7201179265975952 2.291151523590088 116.27769470214844
  batch 40 loss: 1.7201179265975952, 2.291151523590088, 116.27769470214844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6955269575119019 3.537550687789917 178.57305908203125
Loss :  1.6870441436767578 2.9367165565490723 148.5228729248047
Loss :  1.679943323135376 2.486953020095825 126.02759552001953
Loss :  1.6871767044067383 2.5235748291015625 127.86592102050781
Loss :  1.674696683883667 3.712005615234375 187.2749786376953
Loss :  1.692456603050232 3.6289658546447754 183.1407470703125
Loss :  1.7130926847457886 2.770339012145996 140.23004150390625
Loss :  1.6840165853500366 2.5067977905273438 127.0239028930664
Loss :  1.7232284545898438 2.5881617069244385 131.13131713867188
Loss :  1.6886285543441772 2.9818084239959717 150.779052734375
Loss :  1.7082183361053467 2.617448329925537 132.5806427001953
Loss :  1.7061409950256348 2.5313217639923096 128.27223205566406
Loss :  1.6921744346618652 3.011754035949707 152.27987670898438
Loss :  1.7106560468673706 2.8148045539855957 142.45089721679688
Loss :  1.6849086284637451 2.756338357925415 139.5018310546875
Loss :  1.7239115238189697 2.569354295730591 130.19161987304688
Loss :  1.6896425485610962 2.8506152629852295 144.22039794921875
Loss :  1.6804455518722534 2.8117854595184326 142.2697296142578
Loss :  1.6888706684112549 3.0450918674468994 153.94346618652344
Loss :  1.7275103330612183 3.0273871421813965 153.09686279296875
  batch 60 loss: 1.7275103330612183, 3.0273871421813965, 153.09686279296875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6811084747314453 3.0358619689941406 153.4741973876953
Loss :  1.697567343711853 2.594378709793091 131.41650390625
Loss :  1.686430811882019 2.456926107406616 124.5327377319336
Loss :  1.6795666217803955 2.767458438873291 140.052490234375
Loss :  1.6675760746002197 2.1784157752990723 110.58837127685547
Loss :  1.6838759183883667 4.030777454376221 203.22274780273438
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6920114755630493 3.9340550899505615 198.394775390625
Loss :  1.6889028549194336 4.011542320251465 202.26602172851562
Loss :  1.69569993019104 3.7276341915130615 188.07740783691406
Total LOSS train 136.340919142503 valid 197.99023818969727
CE LOSS train 1.6944275324161235 valid 0.42392498254776
Contrastive LOSS train 2.6929298217479998 valid 0.9319085478782654
EPOCH 226:
Loss :  1.7084976434707642 3.2319109439849854 163.30404663085938
Loss :  1.7151941061019897 2.9015769958496094 146.79405212402344
Loss :  1.6974068880081177 2.5116217136383057 127.27849578857422
Loss :  1.701680064201355 2.488664150238037 126.1348876953125
Loss :  1.7096953392028809 2.285665273666382 115.99295806884766
Loss :  1.689292311668396 2.5326192378997803 128.32025146484375
Loss :  1.7107053995132446 2.9114694595336914 147.2841796875
Loss :  1.6954450607299805 2.647503137588501 134.0706024169922
Loss :  1.6892832517623901 2.97772479057312 150.57553100585938
Loss :  1.7120561599731445 2.3795361518859863 120.6888656616211
Loss :  1.6837947368621826 2.3808202743530273 120.72480773925781
Loss :  1.682483196258545 3.1616275310516357 159.76385498046875
Loss :  1.682158350944519 3.263352632522583 164.84979248046875
Loss :  1.6867610216140747 3.257791042327881 164.57632446289062
Loss :  1.719002366065979 2.3077127933502197 117.10464477539062
Loss :  1.7180763483047485 2.368194818496704 120.12782287597656
Loss :  1.6798287630081177 2.6750683784484863 135.43324279785156
Loss :  1.697472095489502 2.4278268814086914 123.08881378173828
Loss :  1.6776796579360962 2.8573713302612305 144.54623413085938
Loss :  1.716385841369629 2.9909157752990723 151.26217651367188
  batch 20 loss: 1.716385841369629, 2.9909157752990723, 151.26217651367188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6939704418182373 3.0295283794403076 153.17039489746094
Loss :  1.6762815713882446 3.5438640117645264 178.86949157714844
Loss :  1.686946988105774 2.513437032699585 127.35880279541016
Loss :  1.698811650276184 2.9603195190429688 149.71478271484375
Loss :  1.715040683746338 3.464315176010132 174.93080139160156
Loss :  1.6886863708496094 2.854806423187256 144.42901611328125
Loss :  1.6934595108032227 3.0061445236206055 152.0006866455078
Loss :  1.6904902458190918 2.692211151123047 136.30105590820312
Loss :  1.6589386463165283 3.2951467037200928 166.41627502441406
Loss :  1.712537169456482 3.4558796882629395 174.50653076171875
Loss :  1.6584148406982422 2.77420711517334 140.3687744140625
Loss :  1.7017570734024048 2.758286237716675 139.61605834960938
Loss :  1.688258171081543 2.286600351333618 116.01827239990234
Loss :  1.6875905990600586 2.5986168384552 131.61843872070312
Loss :  1.663120985031128 2.8601739406585693 144.67181396484375
Loss :  1.6733684539794922 3.2641098499298096 164.8788604736328
Loss :  1.6730014085769653 2.50579833984375 126.96292114257812
Loss :  1.710861325263977 2.5568196773529053 129.55184936523438
Loss :  1.7137144804000854 3.698803663253784 186.65390014648438
Loss :  1.718957543373108 2.5893449783325195 131.18621826171875
  batch 40 loss: 1.718957543373108, 2.5893449783325195, 131.18621826171875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
Loss :  1.6937060356140137 2.617725133895874 132.57997131347656
Loss :  1.6858007907867432 2.2851552963256836 115.94356536865234
Loss :  1.6784096956253052 3.02365779876709 152.86129760742188
Loss :  1.6862825155258179 2.2743616104125977 115.40435791015625
Loss :  1.674227237701416 2.4279491901397705 123.07168579101562
Loss :  1.6933513879776 2.8726792335510254 145.32730102539062
Loss :  1.714180588722229 3.846705436706543 194.04945373535156
Loss :  1.6843639612197876 2.7173657417297363 137.55264282226562
Loss :  1.724351167678833 2.9452950954437256 148.98910522460938
Loss :  1.6897863149642944 2.5163917541503906 127.5093765258789
Loss :  1.7091286182403564 2.67543363571167 135.48080444335938
Loss :  1.7078572511672974 2.97577166557312 150.49644470214844
Loss :  1.6936951875686646 2.473158597946167 125.35162353515625
Loss :  1.7127667665481567 2.836362600326538 143.53089904785156
Loss :  1.685879111289978 2.9227969646453857 147.8257293701172
Loss :  1.7259541749954224 2.356935739517212 119.57273864746094
Loss :  1.6906095743179321 2.8297958374023438 143.18040466308594
Loss :  1.6808629035949707 2.495551347732544 126.45842742919922
Loss :  1.6895694732666016 2.801314353942871 141.7552947998047
Loss :  1.728545904159546 2.4987006187438965 126.66357421875
  batch 60 loss: 1.728545904159546, 2.4987006187438965, 126.66357421875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6803592443466187 3.0662169456481934 154.9912109375
Loss :  1.69615638256073 2.508991003036499 127.14570617675781
Loss :  1.6842447519302368 3.033594846725464 153.36399841308594
Loss :  1.6774884462356567 3.085745334625244 155.96475219726562
Loss :  1.6649786233901978 3.7440168857574463 188.86582946777344
Loss :  1.6839812994003296 3.136660575866699 158.51699829101562
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
Loss :  1.691717505455017 3.2034432888031006 161.86387634277344
Loss :  1.6895112991333008 3.006070375442505 151.99302673339844
Loss :  1.6946830749511719 2.9044978618621826 146.91958618164062
Total LOSS train 142.53973423884466 valid 154.82337188720703
CE LOSS train 1.6943025057132428 valid 0.42367076873779297
Contrastive LOSS train 2.816908608950101 valid 0.7261244654655457
Saved best model. Old loss 159.2692527770996 and new best loss 154.82337188720703
EPOCH 227:
Loss :  1.7065385580062866 2.749932289123535 139.20315551757812
Loss :  1.7133527994155884 2.4908270835876465 126.25470733642578
Loss :  1.6952688694000244 2.4877676963806152 126.08364868164062
Loss :  1.6999481916427612 2.8787496089935303 145.63743591308594
Loss :  1.7082321643829346 2.3321266174316406 118.31455993652344
Loss :  1.6872526407241821 2.3915390968322754 121.26421356201172
Loss :  1.7089183330535889 2.572047233581543 130.311279296875
Loss :  1.693425178527832 3.0981333255767822 156.60008239746094
Loss :  1.6867907047271729 2.2200887203216553 112.69122314453125
Loss :  1.7098057270050049 2.217244863510132 112.57205200195312
Loss :  1.6817455291748047 2.199648857116699 111.66419219970703
Loss :  1.6803710460662842 2.5925049781799316 131.3056182861328
Loss :  1.679785132408142 2.5648062229156494 129.9200897216797
Loss :  1.6842962503433228 2.77754545211792 140.5615692138672
Loss :  1.716772437095642 2.797968864440918 141.61520385742188
Loss :  1.7157529592514038 2.6769492626190186 135.56321716308594
Loss :  1.676893711090088 2.6954243183135986 136.44810485839844
Loss :  1.6949527263641357 2.5301408767700195 128.20199584960938
Loss :  1.6755231618881226 2.2919414043426514 116.27259063720703
Loss :  1.714228868484497 2.5860514640808105 131.0167999267578
  batch 20 loss: 1.714228868484497, 2.5860514640808105, 131.0167999267578
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6927286386489868 3.0732758045196533 155.3565216064453
Loss :  1.6754419803619385 3.1885340213775635 161.10214233398438
Loss :  1.6869248151779175 2.5874557495117188 131.05970764160156
Loss :  1.6990286111831665 2.7247140407562256 137.9347381591797
Loss :  1.7154327630996704 2.5715363025665283 130.29225158691406
Loss :  1.6907356977462769 2.4445462226867676 123.91804504394531
Loss :  1.695909023284912 2.7624194622039795 139.81687927246094
Loss :  1.693611979484558 3.2095348834991455 162.1703643798828
Loss :  1.6619362831115723 2.549082040786743 129.1160430908203
Loss :  1.713916301727295 2.7563858032226562 139.533203125
Loss :  1.6612558364868164 2.2787601947784424 115.5992660522461
Loss :  1.7033334970474243 2.4182512760162354 122.61589813232422
Loss :  1.6902413368225098 2.3211758136749268 117.74903106689453
Loss :  1.6899104118347168 2.3349850177764893 118.43916320800781
Loss :  1.6658258438110352 2.81514310836792 142.42298889160156
Loss :  1.6760238409042358 3.655717134475708 184.46188354492188
Loss :  1.6755050420761108 2.6282780170440674 133.0894012451172
Loss :  1.7125341892242432 2.6578149795532227 134.60328674316406
Loss :  1.7148815393447876 3.081631898880005 155.7964630126953
Loss :  1.7201720476150513 2.9970483779907227 151.5725860595703
  batch 40 loss: 1.7201720476150513, 2.9970483779907227, 151.5725860595703
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6953705549240112 2.2868282794952393 116.03678894042969
Loss :  1.6874669790267944 2.567394733428955 130.0572052001953
Loss :  1.6797215938568115 2.54581880569458 128.9706573486328
Loss :  1.6873242855072021 2.8702239990234375 145.19851684570312
Loss :  1.675044298171997 2.461412191390991 124.74565124511719
Loss :  1.693497896194458 2.3860623836517334 120.99661254882812
Loss :  1.7140069007873535 3.422118663787842 172.81993103027344
Loss :  1.6842516660690308 2.472815752029419 125.32503509521484
Loss :  1.7238024473190308 2.309464931488037 117.1970443725586
Loss :  1.6891072988510132 2.3592758178710938 119.65290069580078
Loss :  1.7086063623428345 2.7557153701782227 139.49436950683594
Loss :  1.70746910572052 2.747192859649658 139.06712341308594
Loss :  1.6939791440963745 2.839877128601074 143.68783569335938
Loss :  1.7133922576904297 3.0020315647125244 151.81497192382812
Loss :  1.686592936515808 3.0118980407714844 152.28150939941406
Loss :  1.7267580032348633 2.740239143371582 138.7387237548828
Loss :  1.6921024322509766 2.8765902519226074 145.52162170410156
Loss :  1.6828944683074951 2.6750006675720215 135.43292236328125
Loss :  1.6918357610702515 2.728426218032837 138.11314392089844
Loss :  1.7299500703811646 2.41034197807312 122.2470474243164
  batch 60 loss: 1.7299500703811646, 2.41034197807312, 122.2470474243164
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.683605432510376 2.3246350288391113 117.91535949707031
Loss :  1.698535680770874 3.1857235431671143 160.98471069335938
Loss :  1.6871892213821411 2.8096394538879395 142.16917419433594
Loss :  1.680202841758728 2.467555046081543 125.0579605102539
Loss :  1.6679370403289795 2.334134578704834 118.37466430664062
Loss :  1.6882102489471436 4.322134971618652 217.7949676513672
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.696352243423462 4.332773685455322 218.3350372314453
Loss :  1.6940582990646362 4.290322780609131 216.210205078125
Loss :  1.7000519037246704 4.222555637359619 212.8278350830078
Total LOSS train 134.61620131272537 valid 216.29201126098633
CE LOSS train 1.694551528417147 valid 0.4250129759311676
Contrastive LOSS train 2.6584329971900353 valid 1.0556389093399048
EPOCH 228:
Loss :  1.7087384462356567 3.2685301303863525 165.13525390625
Loss :  1.7157645225524902 2.395702362060547 121.50088500976562
Loss :  1.6976354122161865 2.248824119567871 114.13883972167969
Loss :  1.702225923538208 2.7373926639556885 138.5718536376953
Loss :  1.710057258605957 2.6593666076660156 134.67837524414062
Loss :  1.689462423324585 2.543560028076172 128.86746215820312
Loss :  1.7101047039031982 2.639646291732788 133.69241333007812
Loss :  1.6949516534805298 3.0964322090148926 156.5165557861328
Loss :  1.6889727115631104 2.643054723739624 133.8417205810547
Loss :  1.7114225625991821 2.888462781906128 146.1345672607422
Loss :  1.683683156967163 2.785193920135498 140.94337463378906
Loss :  1.6824785470962524 2.5064094066619873 127.0029525756836
Loss :  1.682004690170288 2.843566656112671 143.86033630371094
Loss :  1.686894416809082 2.7407925128936768 138.7265167236328
Loss :  1.7188912630081177 2.7274463176727295 138.09120178222656
Loss :  1.717940092086792 3.0545501708984375 154.44544982910156
Loss :  1.6805686950683594 2.864375591278076 144.89935302734375
Loss :  1.6985408067703247 2.4633140563964844 124.86424255371094
Loss :  1.679458498954773 2.381344795227051 120.74669647216797
Loss :  1.7171462774276733 2.3527777194976807 119.35603332519531
  batch 20 loss: 1.7171462774276733, 2.3527777194976807, 119.35603332519531
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6958649158477783 2.744600534439087 138.92588806152344
Loss :  1.6794599294662476 2.5501158237457275 129.1852569580078
Loss :  1.6905373334884644 3.4945497512817383 176.41802978515625
Loss :  1.7019429206848145 3.066174268722534 155.0106658935547
Loss :  1.7177633047103882 2.8478055000305176 144.1080322265625
Loss :  1.693666934967041 2.4803810119628906 125.71271514892578
Loss :  1.6982462406158447 2.5626981258392334 129.83314514160156
Loss :  1.6964645385742188 2.4275968074798584 123.07630157470703
Loss :  1.6649891138076782 3.1596808433532715 159.64903259277344
Loss :  1.71523118019104 2.9095611572265625 147.1932830810547
Loss :  1.66364324092865 2.8286635875701904 143.09683227539062
Loss :  1.7045466899871826 2.5020346641540527 126.8062744140625
Loss :  1.6913787126541138 2.6691720485687256 135.1499786376953
Loss :  1.6907904148101807 3.1266236305236816 158.02197265625
Loss :  1.6672523021697998 2.8655688762664795 144.94569396972656
Loss :  1.6770453453063965 2.6194162368774414 132.64785766601562
Loss :  1.6763954162597656 2.4889450073242188 126.12364196777344
Loss :  1.7135816812515259 2.8313565254211426 143.28140258789062
Loss :  1.7164344787597656 2.5189077854156494 127.66181945800781
Loss :  1.7218047380447388 2.585054636001587 130.9745330810547
  batch 40 loss: 1.7218047380447388, 2.585054636001587, 130.9745330810547
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6969321966171265 3.129601240158081 158.17698669433594
Loss :  1.6887203454971313 2.5803346633911133 130.70545959472656
Loss :  1.6810283660888672 2.927748680114746 148.06846618652344
Loss :  1.68856680393219 3.2922379970550537 166.3004608154297
Loss :  1.6762146949768066 2.736706495285034 138.51153564453125
Loss :  1.695082426071167 3.231780767440796 163.28411865234375
Loss :  1.7153469324111938 3.2433207035064697 163.88137817382812
Loss :  1.685669183731079 2.571471691131592 130.25924682617188
Loss :  1.7254984378814697 2.9345719814300537 148.45408630371094
Loss :  1.6904404163360596 2.4954376220703125 126.46231842041016
Loss :  1.7098023891448975 2.8281021118164062 143.1149139404297
Loss :  1.7082264423370361 2.164120674133301 109.91426086425781
Loss :  1.6944557428359985 2.3103413581848145 117.2115249633789
Loss :  1.713482141494751 2.532911539077759 128.3590545654297
Loss :  1.6865768432617188 2.719404935836792 137.65682983398438
Loss :  1.7261836528778076 2.685612440109253 136.00680541992188
Loss :  1.691141128540039 3.2537758350372314 164.37994384765625
Loss :  1.6819214820861816 3.0602009296417236 154.69195556640625
Loss :  1.6901822090148926 2.909127950668335 147.14657592773438
Loss :  1.7284623384475708 2.2736783027648926 115.4123764038086
  batch 60 loss: 1.7284623384475708, 2.2736783027648926, 115.4123764038086
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6818690299987793 2.4990921020507812 126.636474609375
Loss :  1.6973893642425537 2.4474751949310303 124.0711441040039
Loss :  1.685990333557129 2.7250640392303467 137.93919372558594
Loss :  1.679215431213379 2.687629222869873 136.0606689453125
Loss :  1.6673122644424438 2.748487710952759 139.09170532226562
Loss :  1.6841962337493896 3.440526008605957 173.7104949951172
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
Loss :  1.6921420097351074 3.327714204788208 168.07785034179688
Loss :  1.6894819736480713 3.2345950603485107 163.4192352294922
Loss :  1.6964764595031738 2.966033935546875 149.9981689453125
Total LOSS train 138.70206040602463 valid 163.8014373779297
CE LOSS train 1.6959952629529513 valid 0.42411911487579346
Contrastive LOSS train 2.7401213242457465 valid 0.7415084838867188
EPOCH 229:
Loss :  1.7083332538604736 2.5694022178649902 130.17843627929688
Loss :  1.7152812480926514 2.7079691886901855 137.11373901367188
Loss :  1.6975270509719849 2.145979404449463 108.99649810791016
Loss :  1.7020518779754639 2.832003593444824 143.30223083496094
Loss :  1.7105568647384644 3.1599817276000977 159.7096405029297
Loss :  1.6901382207870483 3.2111332416534424 162.24679565429688
Loss :  1.7115768194198608 3.7015280723571777 186.78797912597656
Loss :  1.6968141794204712 2.62837290763855 133.11544799804688
Loss :  1.6906837224960327 2.6883385181427 136.1076202392578
Loss :  1.7135566473007202 2.2216737270355225 112.7972412109375
Loss :  1.6861615180969238 2.6445419788360596 133.9132537841797
Loss :  1.6846290826797485 2.5441529750823975 128.89227294921875
Loss :  1.6844046115875244 2.5283117294311523 128.09999084472656
Loss :  1.6886906623840332 2.856461763381958 144.51177978515625
Loss :  1.7201504707336426 2.7633731365203857 139.88880920410156
Loss :  1.7189936637878418 2.6921277046203613 136.32537841796875
Loss :  1.681501865386963 2.770416259765625 140.2023162841797
Loss :  1.6983728408813477 2.4537413120269775 124.38543701171875
Loss :  1.6792577505111694 2.339190721511841 118.6387939453125
Loss :  1.7164682149887085 2.438941717147827 123.6635513305664
  batch 20 loss: 1.7164682149887085, 2.438941717147827, 123.6635513305664
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6951136589050293 2.3181982040405273 117.60502624511719
Loss :  1.6778454780578613 2.382296085357666 120.79265594482422
Loss :  1.6889526844024658 3.3834054470062256 170.85922241210938
Loss :  1.7006185054779053 2.4387097358703613 123.63610076904297
Loss :  1.716629981994629 3.1773626804351807 160.5847625732422
Loss :  1.691839575767517 2.9342751502990723 148.4055938720703
Loss :  1.696500539779663 2.3754701614379883 120.47000885009766
Loss :  1.6942508220672607 2.5387961864471436 128.63406372070312
Loss :  1.6625664234161377 2.606013536453247 131.96324157714844
Loss :  1.7146100997924805 2.9437460899353027 148.90191650390625
Loss :  1.6620122194290161 2.7370452880859375 138.5142822265625
Loss :  1.7039275169372559 2.184170961380005 110.9124755859375
Loss :  1.6909011602401733 2.5014588832855225 126.76384735107422
Loss :  1.6906185150146484 2.8245747089385986 142.91934204101562
Loss :  1.667026162147522 2.512564182281494 127.29523468017578
Loss :  1.677272081375122 3.0306129455566406 153.20791625976562
Loss :  1.6770286560058594 2.804062604904175 141.88015747070312
Loss :  1.7141207456588745 2.8960423469543457 146.51625061035156
Loss :  1.7172858715057373 2.4796814918518066 125.70136260986328
Loss :  1.722318172454834 2.744898796081543 138.96725463867188
  batch 40 loss: 1.722318172454834, 2.744898796081543, 138.96725463867188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.698394775390625 2.674738883972168 135.43533325195312
Loss :  1.6903890371322632 2.0811285972595215 105.74681854248047
Loss :  1.6828935146331787 2.252772569656372 114.32151794433594
Loss :  1.690533995628357 3.409144163131714 172.1477508544922
Loss :  1.6781080961227417 3.143216371536255 158.8389129638672
Loss :  1.6959002017974854 2.5299952030181885 128.19566345214844
Loss :  1.7160029411315918 2.7201335430145264 137.72268676757812
Loss :  1.686590313911438 2.552008867263794 129.2870330810547
Loss :  1.7252521514892578 2.898005962371826 146.62554931640625
Loss :  1.6902928352355957 2.584934949874878 130.93704223632812
Loss :  1.7089030742645264 2.7857327461242676 140.99554443359375
Loss :  1.7071328163146973 3.4247004985809326 172.94216918945312
Loss :  1.6935137510299683 2.96010422706604 149.69873046875
Loss :  1.712164044380188 2.846860647201538 144.05519104003906
Loss :  1.6850204467773438 3.2142515182495117 162.3975830078125
Loss :  1.7251404523849487 2.775975227355957 140.52391052246094
Loss :  1.6898247003555298 2.9521543979644775 149.29754638671875
Loss :  1.6802699565887451 2.8166792392730713 142.5142364501953
Loss :  1.689303994178772 3.0495316982269287 154.16587829589844
Loss :  1.7284178733825684 2.540109395980835 128.73388671875
  batch 60 loss: 1.7284178733825684, 2.540109395980835, 128.73388671875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6813905239105225 2.4276483058929443 123.06380462646484
Loss :  1.697640061378479 2.5371713638305664 128.55621337890625
Loss :  1.686152696609497 2.502634286880493 126.81786346435547
Loss :  1.6796948909759521 2.8594138622283936 144.65037536621094
Loss :  1.6676890850067139 3.0665135383605957 154.99337768554688
Loss :  1.687483549118042 3.9660630226135254 199.99063110351562
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
Loss :  1.6951735019683838 3.93888783454895 198.6395721435547
Loss :  1.6926568746566772 3.7705142498016357 190.21836853027344
Loss :  1.6983304023742676 3.7670233249664307 190.04949951171875
Total LOSS train 138.09342381403997 valid 194.72451782226562
CE LOSS train 1.6960493179468008 valid 0.4245826005935669
Contrastive LOSS train 2.7279474992018478 valid 0.9417558312416077
EPOCH 230:
Loss :  1.7090636491775513 2.7495148181915283 139.18479919433594
Loss :  1.7158843278884888 2.433885097503662 123.4101333618164
Loss :  1.6981472969055176 2.327488899230957 118.07259368896484
Loss :  1.7027727365493774 3.303853750228882 166.8954620361328
Loss :  1.7108182907104492 2.670203685760498 135.2209930419922
Loss :  1.6910808086395264 2.3787052631378174 120.6263427734375
Loss :  1.7118836641311646 2.943751096725464 148.89944458007812
Loss :  1.6968162059783936 2.416999101638794 122.54676818847656
Loss :  1.6902527809143066 2.678431749343872 135.61183166503906
Loss :  1.7124027013778687 2.585618019104004 130.99330139160156
Loss :  1.6847940683364868 3.0091452598571777 152.14205932617188
Loss :  1.6834871768951416 2.812286853790283 142.29783630371094
Loss :  1.6831756830215454 2.578254222869873 130.59588623046875
Loss :  1.687131404876709 2.6361732482910156 133.49578857421875
Loss :  1.7185261249542236 2.4661705493927 125.02705383300781
Loss :  1.7174441814422607 2.5699305534362793 130.21395874023438
Loss :  1.6800562143325806 3.0318312644958496 153.27162170410156
Loss :  1.6975867748260498 2.8220162391662598 142.79840087890625
Loss :  1.6786212921142578 2.4105308055877686 122.20516204833984
Loss :  1.7163734436035156 2.453798770904541 124.40631103515625
  batch 20 loss: 1.7163734436035156, 2.453798770904541, 124.40631103515625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6950886249542236 3.067946672439575 155.09242248535156
Loss :  1.6781342029571533 2.5424294471740723 128.7996063232422
Loss :  1.6893398761749268 2.191159963607788 111.2473373413086
Loss :  1.7005674839019775 2.7192742824554443 137.66427612304688
Loss :  1.7165886163711548 2.9205517768859863 147.74417114257812
Loss :  1.6924554109573364 2.3767037391662598 120.52764892578125
Loss :  1.697306752204895 3.189298391342163 161.1622314453125
Loss :  1.6952675580978394 2.395207643508911 121.45565032958984
Loss :  1.6640510559082031 2.6237432956695557 132.85121154785156
Loss :  1.7153428792953491 2.433105707168579 123.3706283569336
Loss :  1.6634846925735474 2.441256284713745 123.7262954711914
Loss :  1.7049498558044434 3.108729600906372 157.1414337158203
Loss :  1.691951036453247 2.496764659881592 126.53018188476562
Loss :  1.6913403272628784 2.4557113647460938 124.4769058227539
Loss :  1.6678727865219116 2.7939751148223877 141.36663818359375
Loss :  1.677669882774353 2.1172091960906982 107.53813171386719
Loss :  1.6774219274520874 2.1907119750976562 111.21302032470703
Loss :  1.7142177820205688 2.8230388164520264 142.8661651611328
Loss :  1.7167556285858154 2.411350965499878 122.28430938720703
Loss :  1.721692442893982 2.9391543865203857 148.67941284179688
  batch 40 loss: 1.721692442893982, 2.9391543865203857, 148.67941284179688
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.697357177734375 2.747645854949951 139.07965087890625
Loss :  1.6888401508331299 2.808072328567505 142.0924530029297
Loss :  1.681402564048767 2.1911540031433105 111.23910522460938
Loss :  1.688900351524353 2.091320037841797 106.2549057006836
Loss :  1.6768262386322021 2.8235220909118652 142.85292053222656
Loss :  1.6953591108322144 2.8573575019836426 144.563232421875
Loss :  1.715584635734558 2.5706560611724854 130.24839782714844
Loss :  1.6863821744918823 2.3770229816436768 120.53752899169922
Loss :  1.7253390550613403 2.5326857566833496 128.359619140625
Loss :  1.691223382949829 2.776036262512207 140.4930419921875
Loss :  1.7105374336242676 2.410435914993286 122.23233795166016
Loss :  1.7090237140655518 2.4944827556610107 126.43315887451172
Loss :  1.6959292888641357 2.8477022647857666 144.08103942871094
Loss :  1.7149444818496704 2.737910747528076 138.61048889160156
Loss :  1.6883046627044678 2.9756407737731934 150.47035217285156
Loss :  1.727675437927246 2.826746940612793 143.0650177001953
Loss :  1.692732334136963 2.78440260887146 140.91285705566406
Loss :  1.683363676071167 2.5709726810455322 130.23199462890625
Loss :  1.6911016702651978 2.5448625087738037 128.93423461914062
Loss :  1.7292454242706299 2.2107656002044678 112.26752471923828
  batch 60 loss: 1.7292454242706299, 2.2107656002044678, 112.26752471923828
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6826049089431763 2.4081788063049316 122.09154510498047
Loss :  1.6976264715194702 2.5161540508270264 127.50532531738281
Loss :  1.686454176902771 2.410550594329834 122.21398162841797
Loss :  1.679162859916687 3.097212314605713 156.53977966308594
Loss :  1.6668682098388672 2.1278693675994873 108.06034088134766
Loss :  1.6807631254196167 3.875521421432495 195.4568328857422
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.688709020614624 3.816725015640259 192.52496337890625
Loss :  1.6862493753433228 3.701709747314453 186.771728515625
Loss :  1.6929131746292114 3.7196555137634277 187.67568969726562
Total LOSS train 132.66191171499398 valid 190.60730361938477
CE LOSS train 1.696317034501296 valid 0.42322829365730286
Contrastive LOSS train 2.6193118975712704 valid 0.9299138784408569
EPOCH 231:
Loss :  1.7077223062515259 2.4949088096618652 126.45316314697266
Loss :  1.7144230604171753 2.8197734355926514 142.70309448242188
Loss :  1.6964352130889893 2.823199510574341 142.85641479492188
Loss :  1.7006996870040894 2.7442712783813477 138.9142608642578
Loss :  1.7093135118484497 2.13741397857666 108.58000946044922
Loss :  1.688920259475708 2.0515382289886475 104.26583099365234
Loss :  1.7102664709091187 2.7814016342163086 140.7803497314453
Loss :  1.6955915689468384 3.180837392807007 160.73745727539062
Loss :  1.6892844438552856 3.057408571243286 154.55970764160156
Loss :  1.712006688117981 3.2088987827301025 162.15695190429688
Loss :  1.684649109840393 3.1012072563171387 156.74501037597656
Loss :  1.6836917400360107 2.929027795791626 148.13507080078125
Loss :  1.6833051443099976 2.4871695041656494 126.04177856445312
Loss :  1.6876516342163086 2.731588840484619 138.26708984375
Loss :  1.7191269397735596 2.577869176864624 130.6125946044922
Loss :  1.7184518575668335 2.6375739574432373 133.59713745117188
Loss :  1.68112051486969 2.5982980728149414 131.5960235595703
Loss :  1.698860764503479 2.4211113452911377 122.75442504882812
Loss :  1.679571509361267 2.4247632026672363 122.91773223876953
Loss :  1.7166599035263062 2.871051549911499 145.2692413330078
  batch 20 loss: 1.7166599035263062, 2.871051549911499, 145.2692413330078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6957124471664429 3.721825122833252 187.78697204589844
Loss :  1.6788583993911743 2.7073512077331543 137.04641723632812
Loss :  1.6901265382766724 2.407001256942749 122.0401840209961
Loss :  1.7010672092437744 2.5341389179229736 128.40802001953125
Loss :  1.7171599864959717 2.6159117221832275 132.51275634765625
Loss :  1.692851185798645 2.4891538619995117 126.15054321289062
Loss :  1.697470784187317 2.5193374156951904 127.66434478759766
Loss :  1.6952059268951416 2.4467263221740723 124.03152465820312
Loss :  1.6637966632843018 3.153477430343628 159.33767700195312
Loss :  1.7146929502487183 2.409754991531372 122.20243835449219
Loss :  1.6627132892608643 3.2617456912994385 164.75
Loss :  1.7039347887039185 2.695791006088257 136.4934844970703
Loss :  1.6909786462783813 2.4362101554870605 123.5014877319336
Loss :  1.6908187866210938 2.6142590045928955 132.40377807617188
Loss :  1.6678277254104614 2.7150332927703857 137.41949462890625
Loss :  1.6777914762496948 3.297534942626953 166.55453491210938
Loss :  1.6778863668441772 2.4383442401885986 123.59510040283203
Loss :  1.7145143747329712 2.4676692485809326 125.09797668457031
Loss :  1.7174330949783325 2.6164774894714355 132.5413055419922
Loss :  1.7227489948272705 2.5534257888793945 129.39404296875
  batch 40 loss: 1.7227489948272705, 2.5534257888793945, 129.39404296875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.699012279510498 2.5801010131835938 130.7040557861328
Loss :  1.691144585609436 2.5001988410949707 126.70108795166016
Loss :  1.6839773654937744 3.420910358428955 172.7294921875
Loss :  1.69121515750885 2.4284965991973877 123.11604309082031
Loss :  1.678932547569275 2.02170729637146 102.76429748535156
Loss :  1.6969492435455322 2.321763277053833 117.78511810302734
Loss :  1.7168281078338623 2.6040780544281006 131.9207305908203
Loss :  1.6878982782363892 2.7930309772491455 141.33944702148438
Loss :  1.7265586853027344 2.857239246368408 144.58853149414062
Loss :  1.6924171447753906 3.0460500717163086 153.9949188232422
Loss :  1.7116222381591797 3.7422969341278076 188.82647705078125
Loss :  1.7097817659378052 3.299454689025879 166.68251037597656
Loss :  1.6963942050933838 2.800832986831665 141.73805236816406
Loss :  1.7149107456207275 2.675851345062256 135.50747680664062
Loss :  1.6890054941177368 2.594825267791748 131.43026733398438
Loss :  1.727763056755066 3.2736222743988037 165.40887451171875
Loss :  1.693755865097046 2.6037051677703857 131.87901306152344
Loss :  1.6848914623260498 2.3161721229553223 117.49349975585938
Loss :  1.6935040950775146 2.774106502532959 140.39881896972656
Loss :  1.7309774160385132 2.6263957023620605 133.05075073242188
  batch 60 loss: 1.7309774160385132, 2.6263957023620605, 133.05075073242188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6853443384170532 2.4635918140411377 124.86492919921875
Loss :  1.7005544900894165 2.7894914150238037 141.1751251220703
Loss :  1.6892756223678589 2.5339927673339844 128.388916015625
Loss :  1.6824802160263062 3.181971311569214 160.7810516357422
Loss :  1.6705924272537231 1.8659735918045044 94.96927642822266
Loss :  1.6893905401229858 3.5498743057250977 179.18310546875
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
Loss :  1.69709050655365 3.4490602016448975 174.1501007080078
Loss :  1.6948745250701904 3.366990089416504 170.04437255859375
Loss :  1.7001873254776 3.2823987007141113 165.8201141357422
Total LOSS train 137.3094490638146 valid 172.29942321777344
CE LOSS train 1.6968789045627302 valid 0.4250468313694
Contrastive LOSS train 2.712251400947571 valid 0.8205996751785278
EPOCH 232:
Loss :  1.7099748849868774 2.701899290084839 136.80494689941406
Loss :  1.7169402837753296 2.974318027496338 150.43283081054688
Loss :  1.6998199224472046 2.575218439102173 130.4607391357422
Loss :  1.7043977975845337 3.2694668769836426 165.177734375
Loss :  1.7127809524536133 2.50331449508667 126.87850952148438
Loss :  1.6931506395339966 2.538141965866089 128.60025024414062
Loss :  1.7136447429656982 2.6522514820098877 134.3262176513672
Loss :  1.6989784240722656 2.7403364181518555 138.71578979492188
Loss :  1.6926759481430054 3.1075422763824463 157.06979370117188
Loss :  1.714234709739685 2.321681261062622 117.79829406738281
Loss :  1.6872615814208984 2.2155110836029053 112.46281433105469
Loss :  1.6859679222106934 2.159376382827759 109.65478515625
Loss :  1.6850574016571045 2.5374135971069336 128.5557403564453
Loss :  1.688928246498108 2.7293195724487305 138.1549072265625
Loss :  1.7198822498321533 2.7346203327178955 138.45089721679688
Loss :  1.7185430526733398 2.6052956581115723 131.9833221435547
Loss :  1.6818726062774658 2.7864797115325928 141.005859375
Loss :  1.6992089748382568 2.7640037536621094 139.89939880371094
Loss :  1.6798374652862549 2.4127185344696045 122.31576538085938
Loss :  1.7171688079833984 2.481229305267334 125.77863311767578
  batch 20 loss: 1.7171688079833984, 2.481229305267334, 125.77863311767578
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6958096027374268 3.0561375617980957 154.50270080566406
Loss :  1.6791510581970215 3.0881216526031494 156.08523559570312
Loss :  1.690299153327942 2.61059308052063 132.21994018554688
Loss :  1.701168179512024 2.7138984203338623 137.39608764648438
Loss :  1.7172796726226807 3.006971836090088 152.0658721923828
Loss :  1.6930077075958252 2.633814811706543 133.3837432861328
Loss :  1.6976364850997925 2.950972557067871 149.2462615966797
Loss :  1.6954655647277832 3.1440505981445312 158.8979949951172
Loss :  1.6644556522369385 2.9416069984436035 148.74481201171875
Loss :  1.7152577638626099 2.9486849308013916 149.14950561523438
Loss :  1.6642060279846191 3.1908679008483887 161.2075958251953
Loss :  1.70541512966156 2.3356730937957764 118.48906707763672
Loss :  1.692321538925171 2.622645378112793 132.8245849609375
Loss :  1.6918268203735352 2.718935966491699 137.6386260986328
Loss :  1.6686331033706665 3.644357919692993 183.88653564453125
Loss :  1.678511619567871 2.4808948040008545 125.72325134277344
Loss :  1.6782195568084717 2.8557169437408447 144.4640655517578
Loss :  1.7146962881088257 2.5662155151367188 130.0254669189453
Loss :  1.7176774740219116 2.4056589603424072 122.00062561035156
Loss :  1.7227537631988525 2.483074426651001 125.87648010253906
  batch 40 loss: 1.7227537631988525, 2.483074426651001, 125.87648010253906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6988775730133057 2.66355562210083 134.8766632080078
Loss :  1.691100835800171 2.8101236820220947 142.19728088378906
Loss :  1.6837342977523804 2.6006133556365967 131.7144012451172
Loss :  1.6909528970718384 2.423487424850464 122.86532592773438
Loss :  1.6790714263916016 2.9464173316955566 148.99993896484375
Loss :  1.6971935033798218 2.3170292377471924 117.54866027832031
Loss :  1.7171276807785034 2.370144844055176 120.224365234375
Loss :  1.6882209777832031 3.555903196334839 179.48338317871094
Loss :  1.7272709608078003 2.684906244277954 135.9725799560547
Loss :  1.6932265758514404 2.5493314266204834 129.1597900390625
Loss :  1.7119801044464111 2.7774064540863037 140.58229064941406
Loss :  1.7101061344146729 2.3766512870788574 120.54267120361328
Loss :  1.6967546939849854 2.2149498462677 112.44424438476562
Loss :  1.7149369716644287 2.3506979942321777 119.24983215332031
Loss :  1.6887167692184448 2.7378649711608887 138.58197021484375
Loss :  1.7272064685821533 2.50911283493042 127.18284606933594
Loss :  1.6929939985275269 2.623985528945923 132.89227294921875
Loss :  1.68369460105896 2.5735435485839844 130.3608856201172
Loss :  1.6916735172271729 2.658691167831421 134.62623596191406
Loss :  1.7293087244033813 2.1126210689544678 107.36035919189453
  batch 60 loss: 1.7293087244033813, 2.1126210689544678, 107.36035919189453
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6832627058029175 2.5708701610565186 130.2267608642578
Loss :  1.6988630294799805 2.3191847801208496 117.65809631347656
Loss :  1.6876474618911743 2.433155059814453 123.34539794921875
Loss :  1.680909514427185 2.7504899501800537 139.2053985595703
Loss :  1.66896653175354 2.6017324924468994 131.75558471679688
Loss :  1.6870359182357788 3.9671266078948975 200.04336547851562
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6952139139175415 3.9056570529937744 196.9780731201172
Loss :  1.6923229694366455 3.8885881900787354 196.12173461914062
Loss :  1.698124885559082 3.854536533355713 194.42494201660156
Total LOSS train 135.34481412447417 valid 196.89202880859375
CE LOSS train 1.697506411258991 valid 0.4245312213897705
Contrastive LOSS train 2.672946174328144 valid 0.9636341333389282
EPOCH 233:
Loss :  1.708846092224121 3.1031734943389893 156.86752319335938
Loss :  1.7154836654663086 2.827103853225708 143.0706787109375
Loss :  1.6980628967285156 2.1008646488189697 106.74130249023438
Loss :  1.7027363777160645 2.276648759841919 115.53517150878906
Loss :  1.7112597227096558 2.2549073696136475 114.45662689208984
Loss :  1.691650152206421 2.3502397537231445 119.2036361694336
Loss :  1.7122713327407837 3.4118330478668213 172.30392456054688
Loss :  1.698026418685913 2.2981040477752686 116.60323333740234
Loss :  1.6919549703598022 2.4040162563323975 121.89276885986328
Loss :  1.7137343883514404 2.642322540283203 133.82984924316406
Loss :  1.6874630451202393 2.843878984451294 143.8814239501953
Loss :  1.686440110206604 2.6031384468078613 131.84336853027344
Loss :  1.6857972145080566 2.56119966506958 129.7457733154297
Loss :  1.6895201206207275 3.6274468898773193 183.06185913085938
Loss :  1.7203867435455322 2.8061156272888184 142.0261688232422
Loss :  1.718719720840454 2.659451723098755 134.69129943847656
Loss :  1.6827106475830078 2.7991387844085693 141.6396484375
Loss :  1.6998261213302612 2.767486572265625 140.07415771484375
Loss :  1.6809250116348267 2.3276443481445312 118.06314086914062
Loss :  1.7174196243286133 2.5998141765594482 131.7081298828125
  batch 20 loss: 1.7174196243286133, 2.5998141765594482, 131.7081298828125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
Loss :  1.6966583728790283 2.7996113300323486 141.67721557617188
Loss :  1.6800307035446167 2.4322304725646973 123.29155731201172
Loss :  1.6911876201629639 2.3570640087127686 119.54438781738281
Loss :  1.7016633749008179 2.764753580093384 139.93934631347656
Loss :  1.7174631357192993 2.829638719558716 143.19940185546875
Loss :  1.693929672241211 2.5699374675750732 130.1907958984375
Loss :  1.6985859870910645 2.8238308429718018 142.89013671875
Loss :  1.6964112520217896 3.3820674419403076 170.79978942871094
Loss :  1.6654837131500244 2.534585475921631 128.39476013183594
Loss :  1.7156575918197632 2.679946184158325 135.7129669189453
Loss :  1.664493203163147 2.5061357021331787 126.97127532958984
Loss :  1.7054592370986938 2.659379243850708 134.67442321777344
Loss :  1.692398190498352 2.5274243354797363 128.06361389160156
Loss :  1.6917845010757446 3.1522903442382812 159.30630493164062
Loss :  1.6687475442886353 3.2778518199920654 165.56134033203125
Loss :  1.6782338619232178 2.7302961349487305 138.1930389404297
Loss :  1.6779106855392456 2.389739751815796 121.1648941040039
Loss :  1.7140315771102905 2.4900970458984375 126.21888732910156
Loss :  1.716198444366455 2.4300811290740967 123.22026062011719
Loss :  1.7218129634857178 2.7166178226470947 137.55270385742188
  batch 40 loss: 1.7218129634857178, 2.7166178226470947, 137.55270385742188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.697554349899292 2.621305227279663 132.7628173828125
Loss :  1.6898887157440186 2.821657419204712 142.77276611328125
Loss :  1.6822388172149658 2.358546733856201 119.60957336425781
Loss :  1.6896729469299316 2.3511111736297607 119.24523162841797
Loss :  1.6773062944412231 2.3664252758026123 119.99857330322266
Loss :  1.6955193281173706 2.5010690689086914 126.74897003173828
Loss :  1.7158478498458862 2.399395227432251 121.68561553955078
Loss :  1.686155080795288 2.157313585281372 109.55183410644531
Loss :  1.7251585721969604 3.068359375 155.14312744140625
Loss :  1.6909246444702148 2.3598082065582275 119.68133544921875
Loss :  1.7103537321090698 2.6490368843078613 134.16220092773438
Loss :  1.7087020874023438 2.666747808456421 135.04608154296875
Loss :  1.69542396068573 2.7261762619018555 138.0042266845703
Loss :  1.7143492698669434 2.832462787628174 143.33749389648438
Loss :  1.687712550163269 2.797548532485962 141.5651397705078
Loss :  1.7274878025054932 3.2850606441497803 165.98052978515625
Loss :  1.692773699760437 3.1639394760131836 159.88975524902344
Loss :  1.6830506324768066 2.711364507675171 137.2512664794922
Loss :  1.6915518045425415 2.7878823280334473 141.08567810058594
Loss :  1.7293789386749268 2.0492537021636963 104.19206237792969
  batch 60 loss: 1.7293789386749268, 2.0492537021636963, 104.19206237792969
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.682615876197815 2.169628858566284 110.1640625
Loss :  1.6985633373260498 2.3084638118743896 117.12174987792969
Loss :  1.68680739402771 2.554866313934326 129.4301300048828
Loss :  1.6802843809127808 2.8333585262298584 143.34820556640625
Loss :  1.6680238246917725 2.57185697555542 130.26087951660156
Loss :  1.6888271570205688 3.8684422969818115 195.11094665527344
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.697046160697937 3.7884514331817627 191.11962890625
Loss :  1.6943775415420532 3.763956308364868 189.89219665527344
Loss :  1.6991161108016968 3.69686222076416 186.54222106933594
Total LOSS train 134.33609372652495 valid 190.6662483215332
CE LOSS train 1.6970572599997886 valid 0.4247790277004242
Contrastive LOSS train 2.6527807162358212 valid 0.92421555519104
EPOCH 234:
Loss :  1.708040475845337 2.566161632537842 130.01611328125
Loss :  1.7148849964141846 2.618238925933838 132.6268310546875
Loss :  1.6970158815383911 2.4395320415496826 123.67361450195312
Loss :  1.7014421224594116 2.581486463546753 130.77577209472656
Loss :  1.7102121114730835 2.949761390686035 149.19827270507812
Loss :  1.6893913745880127 2.2811715602874756 115.74797058105469
Loss :  1.7104427814483643 2.4597573280334473 124.69831085205078
Loss :  1.696030616760254 3.0292232036590576 153.15719604492188
Loss :  1.6901044845581055 2.3948216438293457 121.43118286132812
Loss :  1.7118191719055176 2.3175244331359863 117.58804321289062
Loss :  1.684952735900879 3.222884178161621 162.82916259765625
Loss :  1.684049367904663 3.327474355697632 168.05776977539062
Loss :  1.6837422847747803 3.060537099838257 154.71060180664062
Loss :  1.688681721687317 2.8192033767700195 142.6488494873047
Loss :  1.7194993495941162 2.7503530979156494 139.23715209960938
Loss :  1.7183961868286133 3.2855539321899414 165.99609375
Loss :  1.6825863122940063 2.8563292026519775 144.49905395507812
Loss :  1.6988470554351807 3.0183374881744385 152.61572265625
Loss :  1.6811877489089966 3.362029552459717 169.78265380859375
Loss :  1.716612696647644 2.208112955093384 112.12226104736328
  batch 20 loss: 1.716612696647644, 2.208112955093384, 112.12226104736328
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
Loss :  1.696513295173645 2.5338151454925537 128.38726806640625
Loss :  1.6802418231964111 2.708604335784912 137.11045837402344
Loss :  1.690987229347229 2.2837164402008057 115.8768081665039
Loss :  1.7016841173171997 2.479931116104126 125.6982421875
Loss :  1.7176475524902344 2.649947166442871 134.2150115966797
Loss :  1.6934235095977783 2.5823915004730225 130.81298828125
Loss :  1.6983201503753662 3.587493896484375 181.07301330566406
Loss :  1.696020483970642 2.3365683555603027 118.5244369506836
Loss :  1.6652063131332397 2.7716987133026123 140.25013732910156
Loss :  1.715796947479248 2.7977752685546875 141.60455322265625
Loss :  1.6649219989776611 2.7037723064422607 136.85353088378906
Loss :  1.7061667442321777 3.4935221672058105 176.38226318359375
Loss :  1.692990779876709 2.5873072147369385 131.058349609375
Loss :  1.692639708518982 2.971750497817993 150.28016662597656
Loss :  1.6697882413864136 2.6934852600097656 136.3440399169922
Loss :  1.6797677278518677 2.6046996116638184 131.9147491455078
Loss :  1.6795495748519897 3.0909640789031982 156.22775268554688
Loss :  1.7155085802078247 2.5941293239593506 131.42198181152344
Loss :  1.718306064605713 2.6099860668182373 132.2176055908203
Loss :  1.7233976125717163 2.5234572887420654 127.8962631225586
  batch 40 loss: 1.7233976125717163, 2.5234572887420654, 127.8962631225586
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.700111985206604 2.6132595539093018 132.36309814453125
Loss :  1.6926958560943604 2.254981279373169 114.44175720214844
Loss :  1.6854184865951538 2.2792465686798096 115.64774322509766
Loss :  1.693030834197998 2.872612953186035 145.32366943359375
Loss :  1.6811174154281616 2.3938486576080322 121.37355041503906
Loss :  1.6988012790679932 2.554582357406616 129.42791748046875
Loss :  1.7186967134475708 2.4746270179748535 125.4500503540039
Loss :  1.6901310682296753 2.2925381660461426 116.3170394897461
Loss :  1.7278586626052856 2.6888909339904785 136.1724090576172
Loss :  1.6947427988052368 3.0176312923431396 152.57630920410156
Loss :  1.7129428386688232 3.295985460281372 166.51220703125
Loss :  1.7110958099365234 2.722748279571533 137.8485107421875
Loss :  1.6981372833251953 2.1988861560821533 111.64244079589844
Loss :  1.7161065340042114 2.499878406524658 126.71002960205078
Loss :  1.6904428005218506 3.452666759490967 174.3237762451172
Loss :  1.7284142971038818 2.568641185760498 130.16046142578125
Loss :  1.695041298866272 2.5736043453216553 130.37525939941406
Loss :  1.6862645149230957 2.602590322494507 131.81578063964844
Loss :  1.6945607662200928 2.97884202003479 150.6366729736328
Loss :  1.7319685220718384 2.994013786315918 151.43264770507812
  batch 60 loss: 1.7319685220718384, 2.994013786315918, 151.43264770507812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6869003772735596 2.6492788791656494 134.15084838867188
Loss :  1.7020002603530884 3.433124303817749 173.35821533203125
Loss :  1.6909078359603882 2.4192922115325928 122.655517578125
Loss :  1.6840875148773193 2.807783365249634 142.07325744628906
Loss :  1.672508955001831 2.0734827518463135 105.34664916992188
Loss :  1.6915760040283203 4.280044078826904 215.6937713623047
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6993637084960938 4.147435665130615 209.07113647460938
Loss :  1.6968395709991455 4.11482572555542 207.43812561035156
Loss :  1.7035075426101685 3.8288683891296387 193.1469268798828
Total LOSS train 137.74920102632962 valid 206.3374900817871
CE LOSS train 1.698012348321768 valid 0.4258768856525421
Contrastive LOSS train 2.7210237943209137 valid 0.9572170972824097
EPOCH 235:
Loss :  1.7114839553833008 2.6902315616607666 136.2230682373047
Loss :  1.718390941619873 3.410548210144043 172.24578857421875
Loss :  1.7013428211212158 3.3164072036743164 167.52169799804688
Loss :  1.705306053161621 3.546344041824341 179.0225067138672
Loss :  1.7132447957992554 3.0437862873077393 153.90255737304688
Loss :  1.6930012702941895 2.9199132919311523 147.6886749267578
Loss :  1.7136030197143555 2.2620084285736084 114.81401824951172
Loss :  1.698978304862976 2.022859811782837 102.84196472167969
Loss :  1.692608118057251 2.492945671081543 126.33989715576172
Loss :  1.7138001918792725 2.7207157611846924 137.7495880126953
Loss :  1.6872934103012085 3.378422498703003 170.60841369628906
Loss :  1.6850864887237549 2.689851999282837 136.1776885986328
Loss :  1.6838176250457764 2.5080370903015137 127.0856704711914
Loss :  1.68740975856781 2.3955531120300293 121.4650650024414
Loss :  1.7187434434890747 2.1717610359191895 110.30679321289062
Loss :  1.716840386390686 2.3199594020843506 117.71481323242188
Loss :  1.6799097061157227 2.232079029083252 113.28385925292969
Loss :  1.696723461151123 2.186297655105591 111.01160430908203
Loss :  1.6784939765930176 2.5171914100646973 127.53807067871094
Loss :  1.7152957916259766 2.7018847465515137 136.80953979492188
  batch 20 loss: 1.7152957916259766, 2.7018847465515137, 136.80953979492188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6947070360183716 2.3677070140838623 120.08006286621094
Loss :  1.6781424283981323 2.574472188949585 130.40174865722656
Loss :  1.688625454902649 2.3388819694519043 118.63272857666016
Loss :  1.6994878053665161 2.486924648284912 126.04571533203125
Loss :  1.7156479358673096 2.843305826187134 143.88095092773438
Loss :  1.6911934614181519 2.377012014389038 120.54179382324219
Loss :  1.696165919303894 2.506134271621704 127.00288391113281
Loss :  1.693650484085083 3.6941559314727783 186.4014434814453
Loss :  1.6629316806793213 2.5850889682769775 130.91738891601562
Loss :  1.714332938194275 2.825835704803467 143.00611877441406
Loss :  1.6634695529937744 2.6366379261016846 133.495361328125
Loss :  1.7045344114303589 2.269857883453369 115.19743347167969
Loss :  1.6910285949707031 2.8165605068206787 142.51904296875
Loss :  1.690477728843689 2.7304847240448 138.21470642089844
Loss :  1.6679767370224 2.7601094245910645 139.6734619140625
Loss :  1.677563190460205 2.4549636840820312 124.42575073242188
Loss :  1.6774073839187622 4.012296676635742 202.29225158691406
Loss :  1.7140638828277588 2.543980836868286 128.91310119628906
Loss :  1.7171392440795898 2.5599522590637207 129.71475219726562
Loss :  1.7225170135498047 2.1298327445983887 108.21415710449219
  batch 40 loss: 1.7225170135498047, 2.1298327445983887, 108.21415710449219
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
Loss :  1.6981172561645508 2.682805299758911 135.83837890625
Loss :  1.6901805400848389 2.6558279991149902 134.48158264160156
Loss :  1.6827316284179688 2.58170223236084 130.76785278320312
Loss :  1.6904103755950928 2.548118829727173 129.0963592529297
Loss :  1.6783119440078735 2.9074435234069824 147.0504913330078
Loss :  1.6966055631637573 3.274700403213501 165.43162536621094
Loss :  1.7167201042175293 2.7805230617523193 140.7428741455078
Loss :  1.6874855756759644 3.8744521141052246 195.41009521484375
Loss :  1.7266160249710083 3.4608139991760254 174.76731872558594
Loss :  1.6924406290054321 2.277782440185547 115.5815658569336
Loss :  1.7113864421844482 2.648561954498291 134.1394805908203
Loss :  1.7099292278289795 2.5744121074676514 130.4305419921875
Loss :  1.6965728998184204 2.63082218170166 133.23768615722656
Loss :  1.7155052423477173 3.100708246231079 156.75091552734375
Loss :  1.6887894868850708 2.6579484939575195 134.5862274169922
Loss :  1.7279051542282104 3.0075337886810303 152.10459899902344
Loss :  1.6935842037200928 2.81706166267395 142.5466766357422
Loss :  1.68431556224823 2.3220300674438477 117.78582000732422
Loss :  1.6927478313446045 2.919853448867798 147.6854248046875
Loss :  1.7304999828338623 1.9590574502944946 99.6833724975586
  batch 60 loss: 1.7304999828338623, 1.9590574502944946, 99.6833724975586
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6842825412750244 2.4628989696502686 124.82923126220703
Loss :  1.6998052597045898 2.441462278366089 123.77291870117188
Loss :  1.6882140636444092 2.873950481414795 145.3857421875
Loss :  1.6812928915023804 2.6503098011016846 134.19677734375
Loss :  1.669164776802063 2.0057554244995117 101.95693969726562
Loss :  1.688454508781433 4.231889724731445 213.28294372558594
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6966111660003662 4.2283244132995605 213.11282348632812
Loss :  1.6939221620559692 4.108633518218994 207.12559509277344
Loss :  1.6984593868255615 4.177443027496338 210.5706024169922
Total LOSS train 136.43357896071214 valid 211.02299118041992
CE LOSS train 1.6970161786446205 valid 0.4246148467063904
Contrastive LOSS train 2.694731226334205 valid 1.0443607568740845
EPOCH 236:
Loss :  1.7087223529815674 2.6476051807403564 134.0889892578125
Loss :  1.7152618169784546 2.266077995300293 115.0191650390625
Loss :  1.6973905563354492 2.172244071960449 110.3095932006836
Loss :  1.7016961574554443 2.2918667793273926 116.29503631591797
Loss :  1.7101868391036987 3.093291997909546 156.37478637695312
Loss :  1.6900885105133057 2.435718059539795 123.47599029541016
Loss :  1.7111350297927856 2.634289503097534 133.4256134033203
Loss :  1.6964216232299805 2.469059705734253 125.14940643310547
Loss :  1.6906508207321167 2.4039461612701416 121.8879623413086
Loss :  1.7124526500701904 2.363142490386963 119.86958312988281
Loss :  1.6856683492660522 2.714247465133667 137.39804077148438
Loss :  1.6846036911010742 2.9213011264801025 147.74966430664062
Loss :  1.6839978694915771 3.306619882583618 167.01498413085938
Loss :  1.6881123781204224 2.8058419227600098 141.98020935058594
Loss :  1.719382405281067 2.721538782119751 137.7963104248047
Loss :  1.7183514833450317 2.7721526622772217 140.32598876953125
Loss :  1.6818233728408813 2.7613332271575928 139.7484893798828
Loss :  1.699232578277588 2.48099422454834 125.74894714355469
Loss :  1.68062162399292 2.2415764331817627 113.75943756103516
Loss :  1.718233346939087 2.5968222618103027 131.55934143066406
  batch 20 loss: 1.718233346939087, 2.5968222618103027, 131.55934143066406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6972384452819824 2.808684825897217 142.1314697265625
Loss :  1.6804109811782837 3.1119346618652344 157.2771453857422
Loss :  1.6912362575531006 2.7662999629974365 140.00624084472656
Loss :  1.7020174264907837 2.8311798572540283 143.26100158691406
Loss :  1.7179768085479736 2.560443878173828 129.7401580810547
Loss :  1.6944515705108643 2.319361448287964 117.66252136230469
Loss :  1.699000597000122 3.388855457305908 171.14178466796875
Loss :  1.696816325187683 3.6747350692749023 185.43357849121094
Loss :  1.6659049987792969 3.163961887359619 159.86399841308594
Loss :  1.7164151668548584 2.8202881813049316 142.73081970214844
Loss :  1.6656060218811035 2.753920316696167 139.3616180419922
Loss :  1.706519603729248 2.5426905155181885 128.84104919433594
Loss :  1.693820595741272 2.8646650314331055 144.92706298828125
Loss :  1.6934001445770264 3.4081308841705322 172.09994506835938
Loss :  1.6709145307540894 2.794832706451416 141.4125518798828
Loss :  1.6808037757873535 2.8441898822784424 143.89028930664062
Loss :  1.6803739070892334 3.6933674812316895 186.3487548828125
Loss :  1.716383695602417 2.7834527492523193 140.88902282714844
Loss :  1.7188382148742676 2.7735657691955566 140.39712524414062
Loss :  1.7239621877670288 2.811291456222534 142.28854370117188
  batch 40 loss: 1.7239621877670288, 2.811291456222534, 142.28854370117188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7001440525054932 2.6114165782928467 132.27098083496094
Loss :  1.6923387050628662 2.506073236465454 126.99600219726562
Loss :  1.6847385168075562 2.752964496612549 139.3329620361328
Loss :  1.692126750946045 2.646111488342285 133.99769592285156
Loss :  1.6797268390655518 2.388749599456787 121.11720275878906
Loss :  1.6974893808364868 2.5718600749969482 130.29049682617188
Loss :  1.7174110412597656 2.313617706298828 117.39830017089844
Loss :  1.6880208253860474 2.49930739402771 126.65338897705078
Loss :  1.7267372608184814 2.5871593952178955 131.08470153808594
Loss :  1.6924856901168823 2.4213778972625732 122.7613754272461
Loss :  1.7112501859664917 2.958737850189209 149.64813232421875
Loss :  1.7095633745193481 2.456040382385254 124.51158142089844
Loss :  1.6963783502578735 2.758831024169922 139.637939453125
Loss :  1.7149590253829956 2.6274077892303467 133.08535766601562
Loss :  1.6887468099594116 2.5161213874816895 127.49481201171875
Loss :  1.7275443077087402 2.7370352745056152 138.5792999267578
Loss :  1.6934465169906616 2.7335102558135986 138.36895751953125
Loss :  1.6843419075012207 2.3799285888671875 120.68077087402344
Loss :  1.6927305459976196 2.5780067443847656 130.5930633544922
Loss :  1.7302674055099487 2.2439775466918945 113.92914581298828
  batch 60 loss: 1.7302674055099487, 2.2439775466918945, 113.92914581298828
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.684462070465088 2.9065582752227783 147.0123748779297
Loss :  1.7003791332244873 2.3700413703918457 120.20244598388672
Loss :  1.6891636848449707 2.653012990951538 134.33981323242188
Loss :  1.682460904121399 2.61600923538208 132.48292541503906
Loss :  1.6706370115280151 1.9475585222244263 99.04856872558594
Loss :  1.6881581544876099 4.01067590713501 202.22195434570312
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.695931077003479 4.013786792755127 202.38528442382812
Loss :  1.6933152675628662 3.890496253967285 196.21812438964844
Loss :  1.6999822854995728 3.827272891998291 193.06362915039062
Total LOSS train 136.00308485764722 valid 198.47224807739258
CE LOSS train 1.697748846274156 valid 0.4249955713748932
Contrastive LOSS train 2.6861067240054792 valid 0.9568182229995728
EPOCH 237:
Loss :  1.710981011390686 2.6727187633514404 135.346923828125
Loss :  1.7175182104110718 3.065072774887085 154.9711456298828
Loss :  1.7000298500061035 2.43104887008667 123.25247192382812
Loss :  1.7045103311538696 3.0603158473968506 154.72030639648438
Loss :  1.7126555442810059 2.373568296432495 120.39106750488281
Loss :  1.6929062604904175 2.776332139968872 140.50950622558594
Loss :  1.7135193347930908 2.3078742027282715 117.10722351074219
Loss :  1.6988918781280518 1.9156025648117065 97.47901916503906
Loss :  1.6929036378860474 2.367774724960327 120.08163452148438
Loss :  1.714345932006836 2.5527775287628174 129.3532257080078
Loss :  1.6879130601882935 2.767817497253418 140.0787811279297
Loss :  1.6867929697036743 3.303504705429077 166.86203002929688
Loss :  1.6860510110855103 2.5988290309906006 131.62750244140625
Loss :  1.6900702714920044 2.5100080966949463 127.19047546386719
Loss :  1.7212587594985962 2.5065841674804688 127.05046844482422
Loss :  1.7200754880905151 2.6635544300079346 134.89779663085938
Loss :  1.6842153072357178 2.5605523586273193 129.7118377685547
Loss :  1.7011862993240356 2.5198521614074707 127.69379425048828
Loss :  1.6828609704971313 3.1712183952331543 160.2437744140625
Loss :  1.7190289497375488 2.047374725341797 104.0877685546875
  batch 20 loss: 1.7190289497375488, 2.047374725341797, 104.0877685546875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.698443055152893 1.9843673706054688 100.91680908203125
Loss :  1.6822937726974487 3.4510111808776855 174.23284912109375
Loss :  1.6928635835647583 2.681497097015381 135.76773071289062
Loss :  1.7038885354995728 2.7256555557250977 137.98666381835938
Loss :  1.719607949256897 2.4997076988220215 126.70498657226562
Loss :  1.6962822675704956 3.440187931060791 173.7056884765625
Loss :  1.7011876106262207 2.554715394973755 129.4369659423828
Loss :  1.6989020109176636 2.402480363845825 121.82291412353516
Loss :  1.6687090396881104 2.361969232559204 119.76717376708984
Loss :  1.718443512916565 2.5162196159362793 127.52942657470703
Loss :  1.6675947904586792 2.8433642387390137 143.83580017089844
Loss :  1.7076364755630493 2.9323296546936035 148.32412719726562
Loss :  1.6943378448486328 3.3760147094726562 170.4950714111328
Loss :  1.6935820579528809 2.373370409011841 120.36209869384766
Loss :  1.6700454950332642 2.5323173999786377 128.28591918945312
Loss :  1.6790430545806885 2.9879629611968994 151.0771942138672
Loss :  1.6782214641571045 2.7736237049102783 140.35940551757812
Loss :  1.713395118713379 3.0055453777313232 151.99066162109375
Loss :  1.7162712812423706 2.513300895690918 127.38131713867188
Loss :  1.721495509147644 2.543346643447876 128.88882446289062
  batch 40 loss: 1.721495509147644, 2.543346643447876, 128.88882446289062
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6977111101150513 2.810755491256714 142.23548889160156
Loss :  1.689456582069397 2.3221054077148438 117.79472351074219
Loss :  1.6826612949371338 2.474494218826294 125.4073715209961
Loss :  1.6901707649230957 2.5310921669006348 128.24478149414062
Loss :  1.6781353950500488 2.3010661602020264 116.7314453125
Loss :  1.696128487586975 2.414673089981079 122.4297866821289
Loss :  1.716056227684021 2.4598701000213623 124.70956420898438
Loss :  1.6871044635772705 2.8376519680023193 143.5697021484375
Loss :  1.7251191139221191 2.6055500507354736 132.0026092529297
Loss :  1.6914732456207275 2.365687370300293 119.97584533691406
Loss :  1.710527777671814 2.548661231994629 129.14358520507812
Loss :  1.7085283994674683 2.4833104610443115 125.8740463256836
Loss :  1.695197582244873 2.6646296977996826 134.9266815185547
Loss :  1.7127891778945923 2.4760704040527344 125.51631164550781
Loss :  1.6884253025054932 2.4481935501098633 124.09809875488281
Loss :  1.725513219833374 2.424288749694824 122.93994903564453
Loss :  1.6923760175704956 2.80116605758667 141.7506866455078
Loss :  1.6839392185211182 3.1207616329193115 157.72203063964844
Loss :  1.691688895225525 3.174513339996338 160.4173583984375
Loss :  1.7293444871902466 2.514780282974243 127.4683609008789
  batch 60 loss: 1.7293444871902466, 2.514780282974243, 127.4683609008789
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6832224130630493 2.6147053241729736 132.41848754882812
Loss :  1.699344277381897 2.6932475566864014 136.36172485351562
Loss :  1.6881721019744873 2.6477549076080322 134.07591247558594
Loss :  1.681713342666626 2.952996015548706 149.33151245117188
Loss :  1.6697391271591187 2.201993942260742 111.76943969726562
Loss :  1.6893388032913208 3.8216567039489746 192.77218627929688
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6973819732666016 3.8079259395599365 192.09368896484375
Loss :  1.6948288679122925 3.7468934059143066 189.03948974609375
Loss :  1.700203537940979 3.4391114711761475 173.65577697753906
Total LOSS train 133.66836747389573 valid 186.89028549194336
CE LOSS train 1.698069192812993 valid 0.42505088448524475
Contrastive LOSS train 2.6394059676390427 valid 0.8597778677940369
EPOCH 238:
Loss :  1.7090729475021362 3.331662178039551 168.2921905517578
Loss :  1.7162460088729858 2.6977810859680176 136.6053009033203
Loss :  1.6985540390014648 3.1157875061035156 157.4879150390625
Loss :  1.7024623155593872 3.136248826980591 158.51490783691406
Loss :  1.7110313177108765 2.5427937507629395 128.85072326660156
Loss :  1.6908490657806396 3.3739099502563477 170.38633728027344
Loss :  1.7113406658172607 2.9649109840393066 149.95687866210938
Loss :  1.6970736980438232 3.072019338607788 155.29803466796875
Loss :  1.691921353340149 2.5507397651672363 129.22891235351562
Loss :  1.7124041318893433 2.735180139541626 138.47140502929688
Loss :  1.6869490146636963 2.736415386199951 138.50772094726562
Loss :  1.6863489151000977 2.2168502807617188 112.52886199951172
Loss :  1.6858433485031128 2.483586311340332 125.86515808105469
Loss :  1.6902152299880981 2.432051181793213 123.29277801513672
Loss :  1.7205300331115723 2.7302162647247314 138.23135375976562
Loss :  1.7185025215148926 2.7782301902770996 140.63002014160156
Loss :  1.6829566955566406 3.076138496398926 155.48988342285156
Loss :  1.6988691091537476 3.024136543273926 152.90570068359375
Loss :  1.6812527179718018 2.316511631011963 117.5068359375
Loss :  1.7162081003189087 2.7912437915802 141.2783966064453
  batch 20 loss: 1.7162081003189087, 2.7912437915802, 141.2783966064453
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6960408687591553 2.649972438812256 134.19467163085938
Loss :  1.6788266897201538 2.8576314449310303 144.56040954589844
Loss :  1.6892950534820557 3.0667572021484375 155.02716064453125
Loss :  1.6996299028396606 3.5222249031066895 177.81088256835938
Loss :  1.7154885530471802 3.436188220977783 173.52490234375
Loss :  1.6914910078048706 2.968367099761963 150.10984802246094
Loss :  1.6955897808074951 3.006079912185669 151.9995880126953
Loss :  1.6930903196334839 3.9862515926361084 201.0056610107422
Loss :  1.6612656116485596 4.042159080505371 203.76922607421875
Loss :  1.7130768299102783 2.6816515922546387 135.795654296875
Loss :  1.659387469291687 3.9568865299224854 199.50372314453125
Loss :  1.701244831085205 3.091386556625366 156.27056884765625
Loss :  1.6867787837982178 2.8456971645355225 143.9716339111328
Loss :  1.686668872833252 3.0931050777435303 156.34193420410156
Loss :  1.6620233058929443 3.108184576034546 157.0712432861328
Loss :  1.672589898109436 3.784472703933716 190.89622497558594
Loss :  1.672044038772583 2.8115670680999756 142.25039672851562
Loss :  1.7094086408615112 2.7680587768554688 140.1123504638672
Loss :  1.7125380039215088 2.721458911895752 137.78549194335938
Loss :  1.7182024717330933 2.8069002628326416 142.06321716308594
  batch 40 loss: 1.7182024717330933, 2.8069002628326416, 142.06321716308594
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
Loss :  1.6927711963653564 3.40301775932312 171.84365844726562
Loss :  1.684969186782837 2.5190324783325195 127.63658905029297
Loss :  1.6767139434814453 2.6933183670043945 136.34263610839844
Loss :  1.6847089529037476 3.205819845199585 161.97569274902344
Loss :  1.6717051267623901 2.387538194656372 121.04861450195312
Loss :  1.6914482116699219 2.6849944591522217 135.94117736816406
Loss :  1.7130646705627441 2.6333425045013428 133.38018798828125
Loss :  1.681242823600769 2.6729447841644287 135.32847595214844
Loss :  1.7229417562484741 2.9782841205596924 150.63714599609375
Loss :  1.6861735582351685 2.2783570289611816 115.6040267944336
Loss :  1.7067656517028809 2.4984824657440186 126.63088989257812
Loss :  1.7045336961746216 2.557953357696533 129.60220336914062
Loss :  1.6910923719406128 2.4181883335113525 122.60050964355469
Loss :  1.710396409034729 2.730529546737671 138.23687744140625
Loss :  1.683144211769104 2.7987942695617676 141.62286376953125
Loss :  1.7245079278945923 2.7443439960479736 138.9416961669922
Loss :  1.688617467880249 2.798590898513794 141.6181640625
Loss :  1.6788160800933838 2.6152050495147705 132.43907165527344
Loss :  1.6881874799728394 2.9616944789886475 149.7729034423828
Loss :  1.7279201745986938 2.204864978790283 111.9711685180664
  batch 60 loss: 1.7279201745986938, 2.204864978790283, 111.9711685180664
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
Loss :  1.6801068782806396 3.136916160583496 158.52590942382812
Loss :  1.69697105884552 2.6310744285583496 133.25070190429688
Loss :  1.6852282285690308 2.4100091457366943 122.18568420410156
Loss :  1.678188443183899 3.1272025108337402 158.03831481933594
Loss :  1.6661914587020874 2.7561864852905273 139.47552490234375
Loss :  1.6884863376617432 4.073149681091309 205.34597778320312
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
Loss :  1.6961853504180908 4.044262886047363 203.90933227539062
Loss :  1.6935009956359863 4.025269508361816 202.95697021484375
Loss :  1.7009543180465698 3.9439473152160645 198.89833068847656
Total LOSS train 145.6622275719276 valid 202.77765274047852
CE LOSS train 1.6944572173632109 valid 0.42523857951164246
Contrastive LOSS train 2.879355390255268 valid 0.9859868288040161
EPOCH 239:
Loss :  1.7082726955413818 2.542161464691162 128.81634521484375
Loss :  1.7156336307525635 2.8102176189422607 142.2265167236328
Loss :  1.6976243257522583 2.719529390335083 137.67410278320312
Loss :  1.7023124694824219 2.5814130306243896 130.77296447753906
Loss :  1.7107192277908325 2.495713949203491 126.49641418457031
Loss :  1.6910881996154785 3.0526771545410156 154.32493591308594
Loss :  1.7119462490081787 2.7812178134918213 140.77284240722656
Loss :  1.697139024734497 2.925936460494995 147.9939727783203
Loss :  1.6909040212631226 2.8018670082092285 141.7842559814453
Loss :  1.7131128311157227 2.898646354675293 146.6454315185547
Loss :  1.685508370399475 2.7971384525299072 141.54241943359375
Loss :  1.6843268871307373 2.564826488494873 129.92564392089844
Loss :  1.683581829071045 2.91571307182312 147.46923828125
Loss :  1.6873431205749512 3.432748317718506 173.32476806640625
Loss :  1.7198998928070068 2.9123778343200684 147.3387908935547
Loss :  1.7184442281723022 2.3671629428863525 120.07659149169922
Loss :  1.6813591718673706 2.5237648487091064 127.86959838867188
Loss :  1.6986199617385864 2.4111411571502686 122.25568389892578
Loss :  1.6792771816253662 2.6234517097473145 132.85186767578125
Loss :  1.7169948816299438 2.588996171951294 131.16680908203125
  batch 20 loss: 1.7169948816299438, 2.588996171951294, 131.16680908203125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6947382688522339 2.9568724632263184 149.53836059570312
Loss :  1.6781015396118164 2.7597949504852295 139.6678466796875
Loss :  1.6894540786743164 2.8054983615875244 141.96437072753906
Loss :  1.7002149820327759 3.5181000232696533 177.60520935058594
Loss :  1.7168227434158325 2.7467286586761475 139.05325317382812
Loss :  1.6919924020767212 2.7305712699890137 138.22055053710938
Loss :  1.6969541311264038 2.7748918533325195 140.44155883789062
Loss :  1.6944727897644043 2.303697347640991 116.87934112548828
Loss :  1.6625603437423706 2.7073614597320557 137.0306396484375
Loss :  1.7151812314987183 2.9334537982940674 148.3878631591797
Loss :  1.6625055074691772 2.669203996658325 135.12271118164062
Loss :  1.704591989517212 2.9698526859283447 150.1972198486328
Loss :  1.69082772731781 2.953474283218384 149.3645477294922
Loss :  1.6903899908065796 2.693096160888672 136.34519958496094
Loss :  1.667232632637024 3.018981456756592 152.61630249023438
Loss :  1.6769906282424927 2.684566020965576 135.90528869628906
Loss :  1.676512598991394 2.7263834476470947 137.9956817626953
Loss :  1.7139418125152588 3.039792060852051 153.70355224609375
Loss :  1.7166235446929932 2.5497326850891113 129.20326232910156
Loss :  1.7219953536987305 2.4554765224456787 124.49581909179688
  batch 40 loss: 1.7219953536987305, 2.4554765224456787, 124.49581909179688
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
Loss :  1.6970999240875244 2.625478506088257 132.9710235595703
Loss :  1.6887266635894775 3.171549081802368 160.26617431640625
Loss :  1.6808878183364868 2.5283596515655518 128.098876953125
Loss :  1.6889456510543823 3.0762012004852295 155.49900817871094
Loss :  1.6765121221542358 2.8791465759277344 145.63385009765625
Loss :  1.6954339742660522 2.5501277446746826 129.20181274414062
Loss :  1.7159217596054077 2.439789295196533 123.70538330078125
Loss :  1.6861802339553833 2.821035861968994 142.73797607421875
Loss :  1.7256056070327759 2.5164380073547363 127.5475082397461
Loss :  1.6908541917800903 2.6918787956237793 136.2847900390625
Loss :  1.710565447807312 3.1373817920684814 158.5796661376953
Loss :  1.7087130546569824 3.333486318588257 168.38302612304688
Loss :  1.6949962377548218 3.93159818649292 198.27490234375
Loss :  1.713875412940979 2.9541478157043457 149.42127990722656
Loss :  1.6866425275802612 2.562969207763672 129.83511352539062
Loss :  1.7269413471221924 2.5827224254608154 130.86306762695312
Loss :  1.6924097537994385 2.6410372257232666 133.74427795410156
Loss :  1.68272066116333 2.9469258785247803 149.02902221679688
Loss :  1.691652536392212 3.0914034843444824 156.26182556152344
Loss :  1.7300430536270142 2.3642995357513428 119.94501495361328
  batch 60 loss: 1.7300430536270142, 2.3642995357513428, 119.94501495361328
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
Loss :  1.6836740970611572 2.45289945602417 124.32865142822266
Loss :  1.699276328086853 2.3457672595977783 118.98764038085938
Loss :  1.6879379749298096 2.0984253883361816 106.60920715332031
Loss :  1.681046962738037 3.5282838344573975 178.09523010253906
Loss :  1.669193148612976 2.187169075012207 111.02764129638672
Loss :  1.6870416402816772 3.8611018657684326 194.7421417236328
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.6946792602539062 3.865591526031494 194.9742431640625
Loss :  1.6924031972885132 3.7041895389556885 186.90187072753906
Loss :  1.6984374523162842 3.7059340476989746 186.99514770507812
Total LOSS train 140.313842186561 valid 190.90335083007812
CE LOSS train 1.6963395228752722 valid 0.42460936307907104
Contrastive LOSS train 2.7723500361809363 valid 0.9264835119247437
EPOCH 240:
Loss :  1.7098711729049683 2.4084651470184326 122.13312530517578
Loss :  1.7172497510910034 2.7658026218414307 140.00738525390625
Loss :  1.6999151706695557 2.6496927738189697 134.18455505371094
Loss :  1.704392910003662 2.4446215629577637 123.93547058105469
Loss :  1.7130886316299438 2.6081700325012207 132.12159729003906
Loss :  1.6936023235321045 2.603248119354248 131.8560028076172
Loss :  1.7144337892532349 2.6260526180267334 133.01705932617188
Loss :  1.6996986865997314 2.1224145889282227 107.82042694091797
Loss :  1.6937230825424194 2.0430827140808105 103.84786224365234
Loss :  1.7152159214019775 2.050549030303955 104.24266815185547
Loss :  1.6884084939956665 3.1486966609954834 159.1232452392578
Loss :  1.687238335609436 2.4205732345581055 122.71590423583984
Loss :  1.686169147491455 2.8007187843322754 141.72210693359375
Loss :  1.6898448467254639 2.575702667236328 130.4749755859375
Loss :  1.7214444875717163 2.573357105255127 130.38931274414062
Loss :  1.720038890838623 2.602827787399292 131.86141967773438
Loss :  1.6831958293914795 2.5132250785827637 127.34445190429688
Loss :  1.7001479864120483 2.419912576675415 122.6957778930664
Loss :  1.6809896230697632 2.285296678543091 115.9458236694336
Loss :  1.7184991836547852 2.7380666732788086 138.62184143066406
  batch 20 loss: 1.7184991836547852, 2.7380666732788086, 138.62184143066406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.696412444114685 2.5939853191375732 131.3956756591797
Loss :  1.6802904605865479 3.0504720211029053 154.2039031982422
Loss :  1.6914480924606323 2.8664140701293945 145.0121612548828
Loss :  1.7016987800598145 2.831071376800537 143.25527954101562
Loss :  1.7180386781692505 2.7059648036956787 137.01626586914062
Loss :  1.6944384574890137 2.658088445663452 134.59886169433594
Loss :  1.6991487741470337 2.7047293186187744 136.93560791015625
Loss :  1.6967854499816895 3.190390110015869 161.21629333496094
Loss :  1.6657687425613403 2.831820011138916 143.25677490234375
Loss :  1.7161797285079956 2.7894606590270996 141.18922424316406
Loss :  1.6652582883834839 2.7166597843170166 137.4982452392578
Loss :  1.70623779296875 2.6481592655181885 134.11419677734375
Loss :  1.6930794715881348 2.6579630374908447 134.5912322998047
Loss :  1.6927129030227661 2.566843271255493 130.03488159179688
Loss :  1.6701072454452515 3.173413038253784 160.34075927734375
Loss :  1.6797512769699097 2.9705991744995117 150.20970153808594
Loss :  1.6792635917663574 2.166637897491455 110.01115417480469
Loss :  1.7158019542694092 2.062432050704956 104.83740997314453
Loss :  1.7181532382965088 2.647115468978882 134.0739288330078
Loss :  1.7238165140151978 2.4405431747436523 123.7509765625
  batch 40 loss: 1.7238165140151978, 2.4405431747436523, 123.7509765625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6996123790740967 2.5818891525268555 130.7940673828125
Loss :  1.6921602487564087 2.592665910720825 131.32545471191406
Loss :  1.6842930316925049 2.614325523376465 132.40057373046875
Loss :  1.6922438144683838 2.5973494052886963 131.55972290039062
Loss :  1.6797269582748413 2.394869804382324 121.4232177734375
Loss :  1.6980981826782227 2.100802183151245 106.73820495605469
Loss :  1.7181611061096191 1.9667309522628784 100.0547103881836
Loss :  1.688330054283142 2.2120020389556885 112.28843688964844
Loss :  1.727660059928894 2.3091938495635986 117.1873550415039
Loss :  1.6934807300567627 2.370513677597046 120.21916198730469
Loss :  1.7123738527297974 2.6504147052764893 134.2331085205078
Loss :  1.7108134031295776 3.0613017082214355 154.77589416503906
Loss :  1.6979084014892578 2.368008613586426 120.09833526611328
Loss :  1.7164907455444336 2.683662176132202 135.89959716796875
Loss :  1.690083622932434 2.876664161682129 145.52328491210938
Loss :  1.7295340299606323 3.5655136108398438 180.00521850585938
Loss :  1.695197582244873 2.4018678665161133 121.78858947753906
Loss :  1.685628056526184 2.41432523727417 122.40189361572266
Loss :  1.694439172744751 2.68438458442688 135.91366577148438
Loss :  1.7323001623153687 2.4031667709350586 121.89064025878906
  batch 60 loss: 1.7323001623153687, 2.4031667709350586, 121.89064025878906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6866123676300049 2.5450730323791504 128.9402618408203
Loss :  1.7020516395568848 3.3260297775268555 168.0035400390625
Loss :  1.690693974494934 3.107778310775757 157.07960510253906
Loss :  1.6842269897460938 2.573187828063965 130.3436279296875
Loss :  1.672321081161499 2.1464273929595947 108.99369049072266
Loss :  1.6878389120101929 3.600255250930786 181.70059204101562
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6953151226043701 3.6444826126098633 183.91944885253906
Loss :  1.6927958726882935 3.4926917552948 176.32737731933594
Loss :  1.6998450756072998 3.33442759513855 168.4212188720703
Total LOSS train 131.86900623028095 valid 177.59215927124023
CE LOSS train 1.6987077199495755 valid 0.42496126890182495
Contrastive LOSS train 2.6034059542876022 valid 0.8336068987846375
EPOCH 241:
Loss :  1.7113195657730103 2.5274384021759033 128.08323669433594
Loss :  1.71823251247406 2.6921637058258057 136.326416015625
Loss :  1.700971245765686 2.9247500896453857 147.9384765625
Loss :  1.7048721313476562 2.699549436569214 136.68234252929688
Loss :  1.7137346267700195 2.376706838607788 120.54907989501953
Loss :  1.6934770345687866 2.74303936958313 138.84544372558594
Loss :  1.7140941619873047 3.007517099380493 152.08995056152344
Loss :  1.6991326808929443 2.5815443992614746 130.77635192871094
Loss :  1.6925193071365356 2.518326997756958 127.60887145996094
Loss :  1.713448166847229 2.6095736026763916 132.192138671875
Loss :  1.687114953994751 2.8306541442871094 143.21983337402344
Loss :  1.6858232021331787 2.456437110900879 124.50767517089844
Loss :  1.6846750974655151 2.6509528160095215 134.23231506347656
Loss :  1.6886842250823975 3.1214165687561035 157.759521484375
Loss :  1.7205077409744263 2.8883020877838135 146.13560485839844
Loss :  1.719219446182251 2.8208541870117188 142.76193237304688
Loss :  1.6820697784423828 2.5938398838043213 131.3740692138672
Loss :  1.6992194652557373 3.092658281326294 156.33213806152344
Loss :  1.679789423942566 2.3898918628692627 121.17437744140625
Loss :  1.7174547910690308 2.4007253646850586 121.75372314453125
  batch 20 loss: 1.7174547910690308, 2.4007253646850586, 121.75372314453125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
Loss :  1.6951090097427368 2.3823699951171875 120.81360626220703
Loss :  1.6784130334854126 2.5122132301330566 127.28907775878906
Loss :  1.6896953582763672 2.351386547088623 119.25902557373047
Loss :  1.7001254558563232 2.8843116760253906 145.9156951904297
Loss :  1.7168452739715576 2.472134590148926 125.32357025146484
Loss :  1.6923905611038208 2.298405170440674 116.6126480102539
Loss :  1.6974283456802368 2.400392770767212 121.7170639038086
Loss :  1.6953381299972534 2.7348146438598633 138.4360809326172
Loss :  1.663834571838379 2.542046308517456 128.7661590576172
Loss :  1.7158912420272827 2.6070592403411865 132.06886291503906
Loss :  1.663794994354248 3.0765457153320312 155.49107360839844
Loss :  1.7052717208862305 2.606381893157959 132.0243682861328
Loss :  1.6919283866882324 2.441746950149536 123.7792739868164
Loss :  1.6914926767349243 2.5251402854919434 127.94850158691406
Loss :  1.6683166027069092 2.8513128757476807 144.2339630126953
Loss :  1.6782798767089844 2.6738734245300293 135.3719482421875
Loss :  1.6778903007507324 2.566708564758301 130.01332092285156
Loss :  1.7147709131240845 2.695432424545288 136.48638916015625
Loss :  1.7176353931427002 2.6230976581573486 132.8725128173828
Loss :  1.7233874797821045 2.398242235183716 121.635498046875
  batch 40 loss: 1.7233874797821045, 2.398242235183716, 121.635498046875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
Loss :  1.6993792057037354 2.657029628753662 134.55087280273438
Loss :  1.6919811964035034 2.893760919570923 146.38003540039062
Loss :  1.6840840578079224 2.9860572814941406 150.9869384765625
Loss :  1.6924149990081787 2.7785375118255615 140.61929321289062
Loss :  1.6796932220458984 3.364135265350342 169.88644409179688
Loss :  1.6978981494903564 3.0031049251556396 151.85313415527344
Loss :  1.7182040214538574 2.955451250076294 149.4907684326172
Loss :  1.6885744333267212 2.3665568828582764 120.01641845703125
Loss :  1.7278974056243896 2.539952516555786 128.72552490234375
Loss :  1.6934823989868164 2.6258444786071777 132.98570251464844
Loss :  1.7120550870895386 2.5826263427734375 130.84336853027344
Loss :  1.710577130317688 2.3273518085479736 118.07817077636719
Loss :  1.6974231004714966 2.3192858695983887 117.6617202758789
Loss :  1.7158724069595337 2.741420030593872 138.786865234375
Loss :  1.6892448663711548 2.7574102878570557 139.55975341796875
Loss :  1.7290812730789185 2.542618989944458 128.8600311279297
Loss :  1.6947612762451172 2.8707993030548096 145.23472595214844
Loss :  1.6850392818450928 2.304016351699829 116.88585662841797
Loss :  1.6939994096755981 2.9381978511810303 148.60389709472656
Loss :  1.7319961786270142 1.9986042976379395 101.6622085571289
  batch 60 loss: 1.7319961786270142, 1.9986042976379395, 101.6622085571289
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.686486840248108 2.6876158714294434 136.06729125976562
Loss :  1.7021968364715576 2.1688475608825684 110.14456939697266
Loss :  1.6912211179733276 2.3693833351135254 120.16039276123047
Loss :  1.6845989227294922 2.9245104789733887 147.91012573242188
Loss :  1.6733801364898682 2.036503314971924 103.49854278564453
Loss :  1.6881870031356812 4.243077278137207 213.84205627441406
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6956267356872559 4.181816101074219 210.78643798828125
Loss :  1.6932921409606934 4.059995651245117 204.69308471679688
Loss :  1.699011206626892 3.9762678146362305 200.51239013671875
Total LOSS train 133.47468907282902 valid 207.45849227905273
CE LOSS train 1.6980883359909058 valid 0.424752801656723
Contrastive LOSS train 2.6355320123525767 valid 0.9940669536590576
EPOCH 242:
Loss :  1.7133175134658813 2.529026985168457 128.16465759277344
Loss :  1.719786286354065 3.0605268478393555 154.74612426757812
Loss :  1.7026264667510986 2.520953416824341 127.75029754638672
Loss :  1.7070884704589844 2.959042549133301 149.6592254638672
Loss :  1.7151613235473633 2.6436150074005127 133.8959197998047
Loss :  1.6963040828704834 3.242392063140869 163.81590270996094
Loss :  1.7162132263183594 2.966461181640625 150.03927612304688
Loss :  1.7020586729049683 2.6601784229278564 134.7109832763672
Loss :  1.6957314014434814 2.623568534851074 132.8741455078125
Loss :  1.7164194583892822 2.5478062629699707 129.1067352294922
Loss :  1.6902352571487427 2.7550113201141357 139.4407958984375
Loss :  1.6885415315628052 2.806605100631714 142.018798828125
Loss :  1.6875512599945068 2.397799253463745 121.5775146484375
Loss :  1.690695881843567 2.6988816261291504 136.634765625
Loss :  1.7220231294631958 2.619206190109253 132.68234252929688
Loss :  1.7204285860061646 2.7652599811553955 139.98342895507812
Loss :  1.6841789484024048 2.686955213546753 136.0319366455078
Loss :  1.701448678970337 2.540379762649536 128.72044372558594
Loss :  1.6822177171707153 3.178204298019409 160.59243774414062
Loss :  1.7194751501083374 2.354217052459717 119.43032836914062
  batch 20 loss: 1.7194751501083374, 2.354217052459717, 119.43032836914062
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.697359561920166 2.350445508956909 119.21963500976562
Loss :  1.681032657623291 2.47891902923584 125.62698364257812
Loss :  1.6921889781951904 2.4001624584198 121.7003173828125
Loss :  1.7023228406906128 2.256115436553955 114.50809478759766
Loss :  1.7186280488967896 3.4523472785949707 174.33599853515625
Loss :  1.6945698261260986 2.3046352863311768 116.92633819580078
Loss :  1.6994645595550537 2.60127854347229 131.76339721679688
Loss :  1.697176218032837 2.5575568675994873 129.5750274658203
Loss :  1.6661479473114014 2.705886125564575 136.96046447753906
Loss :  1.717334270477295 2.6334211826324463 133.38839721679688
Loss :  1.665887475013733 3.1908891201019287 161.21034240722656
Loss :  1.706735372543335 3.1509175300598145 159.25262451171875
Loss :  1.693617343902588 2.242619752883911 113.8246078491211
Loss :  1.6933759450912476 2.218958616256714 112.64130401611328
Loss :  1.6708548069000244 3.0236997604370117 152.8558349609375
Loss :  1.680810809135437 3.0303115844726562 153.19639587402344
Loss :  1.6807128190994263 2.8316521644592285 143.26332092285156
Loss :  1.717498779296875 2.4294593334198 123.19046783447266
Loss :  1.7202284336090088 2.6139450073242188 132.41748046875
Loss :  1.7259180545806885 2.3883748054504395 121.14466094970703
  batch 40 loss: 1.7259180545806885, 2.3883748054504395, 121.14466094970703
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7022981643676758 2.780285358428955 140.71656799316406
Loss :  1.6949833631515503 2.6436333656311035 133.87664794921875
Loss :  1.6870722770690918 3.1930489540100098 161.3395233154297
Loss :  1.695645809173584 2.664888620376587 134.94007873535156
Loss :  1.6831656694412231 2.5343658924102783 128.40145874023438
Loss :  1.701098084449768 2.704134225845337 136.90780639648438
Loss :  1.7203025817871094 2.6920278072357178 136.3217010498047
Loss :  1.6911897659301758 2.6113030910491943 132.25634765625
Loss :  1.7291849851608276 2.5851080417633057 130.98458862304688
Loss :  1.6952629089355469 3.0081446170806885 152.1024932861328
Loss :  1.713987946510315 2.751504421234131 139.28921508789062
Loss :  1.7119102478027344 2.9347219467163086 148.44801330566406
Loss :  1.6987507343292236 2.3647735118865967 119.93743133544922
Loss :  1.7171287536621094 2.5836665630340576 130.9004669189453
Loss :  1.69033682346344 2.56937837600708 130.1592559814453
Loss :  1.7300108671188354 2.6145050525665283 132.45526123046875
Loss :  1.6959593296051025 2.7330257892608643 138.3472442626953
Loss :  1.6862579584121704 2.9595835208892822 149.66543579101562
Loss :  1.694873571395874 2.77608585357666 140.49916076660156
Loss :  1.7323176860809326 2.4137837886810303 122.42150115966797
  batch 60 loss: 1.7323176860809326, 2.4137837886810303, 122.42150115966797
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6873230934143066 2.7119174003601074 137.28318786621094
Loss :  1.702987790107727 2.454552173614502 124.43059539794922
Loss :  1.6917980909347534 2.016749620437622 102.5292739868164
Loss :  1.685020089149475 2.4382824897766113 123.5991439819336
Loss :  1.6733938455581665 2.034376859664917 103.3922348022461
Loss :  1.692623496055603 3.904141902923584 196.89971923828125
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.7001160383224487 3.893916606903076 196.3959503173828
Loss :  1.6974201202392578 3.6808159351348877 185.73822021484375
Loss :  1.705429196357727 3.6527302265167236 184.34193420410156
Total LOSS train 134.92434445894682 valid 190.84395599365234
CE LOSS train 1.7000865881259626 valid 0.42635729908943176
Contrastive LOSS train 2.664485135445228 valid 0.9131825566291809
EPOCH 243:
Loss :  1.7129777669906616 2.3495559692382812 119.1907730102539
Loss :  1.7197091579437256 2.761181354522705 139.77877807617188
Loss :  1.7027671337127686 2.4183719158172607 122.6213607788086
Loss :  1.7067945003509521 2.4723312854766846 125.32335662841797
Loss :  1.7150766849517822 2.769702196121216 140.20018005371094
Loss :  1.6953966617584229 4.1575727462768555 209.57403564453125
Loss :  1.7162408828735352 2.865516185760498 144.99205017089844
Loss :  1.7020559310913086 2.725445032119751 137.97430419921875
Loss :  1.6964813470840454 2.4876046180725098 126.07671356201172
Loss :  1.7176573276519775 2.5021214485168457 126.82373046875
Loss :  1.6919296979904175 2.610358953475952 132.20986938476562
Loss :  1.6905946731567383 2.2885239124298096 116.11679077148438
Loss :  1.6901464462280273 4.3505635261535645 219.21832275390625
Loss :  1.6936414241790771 2.405438184738159 121.9655532836914
Loss :  1.7240087985992432 3.0960652828216553 156.52728271484375
Loss :  1.7227778434753418 2.5359082221984863 128.5181884765625
Loss :  1.6876600980758667 3.2395920753479004 163.66725158691406
Loss :  1.7047884464263916 2.7399208545684814 138.70083618164062
Loss :  1.6864285469055176 2.4795658588409424 125.66472625732422
Loss :  1.72147798538208 3.286127805709839 166.0278778076172
  batch 20 loss: 1.72147798538208, 3.286127805709839, 166.0278778076172
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.700194239616394 2.5484001636505127 129.1201934814453
Loss :  1.6847755908966064 2.763610601425171 139.86529541015625
Loss :  1.695353627204895 2.7328169345855713 138.33621215820312
Loss :  1.7049281597137451 3.2773003578186035 165.5699462890625
Loss :  1.7208025455474854 2.6347978115081787 133.460693359375
Loss :  1.6975589990615845 2.3474321365356445 119.06916046142578
Loss :  1.7021950483322144 2.4422690868377686 123.8156509399414
Loss :  1.7000945806503296 3.25080943107605 164.2405548095703
Loss :  1.6699097156524658 2.6487226486206055 134.1060333251953
Loss :  1.7193840742111206 2.6366496086120605 133.55186462402344
Loss :  1.6692203283309937 2.898894786834717 146.61395263671875
Loss :  1.7089236974716187 2.444908857345581 123.9543685913086
Loss :  1.695975422859192 2.5053954124450684 126.96574401855469
Loss :  1.6954618692398071 2.878084897994995 145.59971618652344
Loss :  1.6733142137527466 2.974595308303833 150.403076171875
Loss :  1.6826529502868652 2.991961717605591 151.28073120117188
Loss :  1.682241439819336 2.5087926387786865 127.12187194824219
Loss :  1.7177537679672241 2.617274522781372 132.58148193359375
Loss :  1.7204060554504395 2.4508726596832275 124.2640380859375
Loss :  1.725693702697754 2.612208127975464 132.3361053466797
  batch 40 loss: 1.725693702697754, 2.612208127975464, 132.3361053466797
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.702528715133667 3.1435189247131348 158.87847900390625
Loss :  1.6951204538345337 2.237248182296753 113.55753326416016
Loss :  1.6875547170639038 2.463665246963501 124.87081909179688
Loss :  1.695578694343567 2.692241907119751 136.3076629638672
Loss :  1.6833181381225586 2.982943058013916 150.83047485351562
Loss :  1.7007489204406738 2.584275960922241 130.91453552246094
Loss :  1.7201954126358032 2.1347949504852295 108.4599380493164
Loss :  1.6914762258529663 2.2835443019866943 115.86869049072266
Loss :  1.729174017906189 2.628474473953247 133.15289306640625
Loss :  1.6960045099258423 2.2799832820892334 115.6951675415039
Loss :  1.7142254114151 2.952303647994995 149.32940673828125
Loss :  1.7123267650604248 2.9918015003204346 151.30239868164062
Loss :  1.6995251178741455 2.3584420680999756 119.62162780761719
Loss :  1.7173800468444824 2.6920535564422607 136.32005310058594
Loss :  1.6910206079483032 2.5579922199249268 129.59063720703125
Loss :  1.7296994924545288 2.603297233581543 131.89456176757812
Loss :  1.6961119174957275 2.5988657474517822 131.63938903808594
Loss :  1.686631202697754 2.471809148788452 125.27708435058594
Loss :  1.695157527923584 2.70424222946167 136.9072723388672
Loss :  1.732507348060608 3.820176362991333 192.7413330078125
  batch 60 loss: 1.732507348060608, 3.820176362991333, 192.7413330078125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.687407374382019 2.595954418182373 131.48512268066406
Loss :  1.7028230428695679 2.598388671875 131.62225341796875
Loss :  1.6915385723114014 2.0552761554718018 104.4553451538086
Loss :  1.6847984790802002 2.7828009128570557 140.82484436035156
Loss :  1.6729217767715454 1.86729896068573 95.0378646850586
Loss :  1.691132664680481 4.026507377624512 203.01649475097656
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.698655128479004 4.086103439331055 206.0038299560547
Loss :  1.6960259675979614 3.957397937774658 199.56593322753906
Loss :  1.7036739587783813 4.126662731170654 208.03680419921875
Total LOSS train 136.9232932457557 valid 204.15576553344727
CE LOSS train 1.7012496288006123 valid 0.42591848969459534
Contrastive LOSS train 2.704440896327679 valid 1.0316656827926636
EPOCH 244:
Loss :  1.71234929561615 2.453584909439087 124.39159393310547
Loss :  1.7191139459609985 3.6473350524902344 184.08587646484375
Loss :  1.701428771018982 3.001701593399048 151.78651428222656
Loss :  1.7054115533828735 2.7849740982055664 140.95411682128906
Loss :  1.7141729593276978 2.4968080520629883 126.55457305908203
Loss :  1.6943144798278809 2.609985589981079 132.193603515625
Loss :  1.7148851156234741 2.9052445888519287 146.97711181640625
Loss :  1.7004939317703247 2.6179864406585693 132.59982299804688
Loss :  1.6949634552001953 3.055370569229126 154.46348571777344
Loss :  1.716630220413208 2.6990270614624023 136.66798400878906
Loss :  1.6899877786636353 2.5676138401031494 130.0706787109375
Loss :  1.6884684562683105 2.83113431930542 143.24517822265625
Loss :  1.6876901388168335 2.443845272064209 123.87995147705078
Loss :  1.691400408744812 2.203519582748413 111.86737823486328
Loss :  1.722501277923584 2.1633806228637695 109.89153289794922
Loss :  1.7213215827941895 2.4628987312316895 124.86625671386719
Loss :  1.6852284669876099 3.304203987121582 166.8954315185547
Loss :  1.701731562614441 3.094209909439087 156.4122314453125
Loss :  1.6832268238067627 2.367252826690674 120.04586791992188
Loss :  1.7193933725357056 2.786979913711548 141.06838989257812
  batch 20 loss: 1.7193933725357056, 2.786979913711548, 141.06838989257812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.698272705078125 3.1612422466278076 159.76039123535156
Loss :  1.6819483041763306 3.0047783851623535 151.92086791992188
Loss :  1.692330241203308 3.3279953002929688 168.09210205078125
Loss :  1.7029720544815063 2.577517032623291 130.57882690429688
Loss :  1.7192260026931763 2.4618799686431885 124.81322479248047
Loss :  1.6951916217803955 2.7960762977600098 141.49900817871094
Loss :  1.7001584768295288 2.9271249771118164 148.05641174316406
Loss :  1.6975064277648926 2.8451597690582275 143.95550537109375
Loss :  1.6669138669967651 2.8316810131073 143.25096130371094
Loss :  1.7174230813980103 2.7379751205444336 138.61618041992188
Loss :  1.6663788557052612 2.4793014526367188 125.63145446777344
Loss :  1.7070376873016357 2.5042002201080322 126.91705322265625
Loss :  1.6939741373062134 2.3896870613098145 121.17832946777344
Loss :  1.6935763359069824 2.4125921726226807 122.32318115234375
Loss :  1.671288013458252 2.842603921890259 143.80149841308594
Loss :  1.6808879375457764 2.95218563079834 149.29017639160156
Loss :  1.6806144714355469 2.848045587539673 144.08290100097656
Loss :  1.7166569232940674 2.234930992126465 113.46321105957031
Loss :  1.7194143533706665 2.7148778438568115 137.46331787109375
Loss :  1.7246787548065186 2.3982114791870117 121.63525390625
  batch 40 loss: 1.7246787548065186, 2.3982114791870117, 121.63525390625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7015414237976074 2.6587986946105957 134.6414794921875
Loss :  1.6940499544143677 3.340822696685791 168.7351837158203
Loss :  1.6867460012435913 2.7868828773498535 141.03089904785156
Loss :  1.6947236061096191 2.569958448410034 130.19264221191406
Loss :  1.6826194524765015 1.9480818510055542 99.08670806884766
Loss :  1.700108528137207 2.4111273288726807 122.25647735595703
Loss :  1.719364881515503 2.336888074874878 118.56377410888672
Loss :  1.6907315254211426 3.149224042892456 159.1519317626953
Loss :  1.7289069890975952 2.4877560138702393 126.11670684814453
Loss :  1.6947568655014038 2.2747814655303955 115.43383026123047
Loss :  1.713465690612793 2.422332286834717 122.830078125
Loss :  1.7116591930389404 2.5775949954986572 130.59140014648438
Loss :  1.6987252235412598 2.41444730758667 122.42108917236328
Loss :  1.7170426845550537 3.6264560222625732 183.0398406982422
Loss :  1.690665364265442 2.9255151748657227 147.96641540527344
Loss :  1.729884386062622 2.4457883834838867 124.01930236816406
Loss :  1.6963586807250977 2.2867321968078613 116.03296661376953
Loss :  1.686659336090088 2.552452564239502 129.3092803955078
Loss :  1.6954278945922852 2.7701151371002197 140.20118713378906
Loss :  1.7329444885253906 2.8005781173706055 141.7618408203125
  batch 60 loss: 1.7329444885253906, 2.8005781173706055, 141.7618408203125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.68721604347229 2.4578096866607666 124.57770538330078
Loss :  1.702596664428711 2.257080078125 114.55660247802734
Loss :  1.6915555000305176 2.4091572761535645 122.14942169189453
Loss :  1.6845009326934814 2.674494981765747 135.40924072265625
Loss :  1.6730024814605713 2.162625551223755 109.80428314208984
Loss :  1.6922235488891602 4.111837863922119 207.28411865234375
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6996909379959106 4.016972064971924 202.5482940673828
Loss :  1.6971787214279175 3.9406473636627197 198.7295379638672
Loss :  1.7049777507781982 3.724071979522705 187.9085693359375
Total LOSS train 135.30965728759764 valid 199.1176300048828
CE LOSS train 1.699944886794457 valid 0.42624443769454956
Contrastive LOSS train 2.6721942259715155 valid 0.9310179948806763
EPOCH 245:
Loss :  1.7125993967056274 2.467694044113159 125.09730529785156
Loss :  1.7191632986068726 2.909529685974121 147.19564819335938
Loss :  1.7018887996673584 3.6736092567443848 185.38235473632812
Loss :  1.7057949304580688 2.951220750808716 149.26683044433594
Loss :  1.7137821912765503 2.732760429382324 138.35179138183594
Loss :  1.694377064704895 2.4005162715911865 121.72018432617188
Loss :  1.7144742012023926 3.0132484436035156 152.37689208984375
Loss :  1.6997486352920532 2.617778778076172 132.5886993408203
Loss :  1.6937894821166992 2.9665350914001465 150.02053833007812
Loss :  1.7149274349212646 2.72487473487854 137.9586639404297
Loss :  1.688155174255371 2.767228841781616 140.04959106445312
Loss :  1.686991572380066 2.948258399963379 149.09991455078125
Loss :  1.6862282752990723 2.639430046081543 133.65773010253906
Loss :  1.6899361610412598 2.672952890396118 135.33758544921875
Loss :  1.7206395864486694 3.0581393241882324 154.6276092529297
Loss :  1.719527006149292 2.926351308822632 148.03709411621094
Loss :  1.6831879615783691 2.5051157474517822 126.93898010253906
Loss :  1.7001845836639404 2.5009195804595947 126.74617004394531
Loss :  1.6815173625946045 2.3085312843322754 117.10808563232422
Loss :  1.7184675931930542 2.322612762451172 117.84910583496094
  batch 20 loss: 1.7184675931930542, 2.322612762451172, 117.84910583496094
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6968662738800049 2.6696438789367676 135.17906188964844
Loss :  1.6807318925857544 2.9739766120910645 150.3795623779297
Loss :  1.6913175582885742 2.5126540660858154 127.32402038574219
Loss :  1.7017617225646973 2.526991128921509 128.0513153076172
Loss :  1.7178833484649658 2.698514699935913 136.64361572265625
Loss :  1.6942341327667236 2.6689257621765137 135.14051818847656
Loss :  1.6992734670639038 3.747102975845337 189.05442810058594
Loss :  1.6971544027328491 3.284118413925171 165.903076171875
Loss :  1.6661951541900635 2.7896735668182373 141.14987182617188
Loss :  1.7167192697525024 2.499596118927002 126.69652557373047
Loss :  1.6655555963516235 2.7961926460266113 141.47518920898438
Loss :  1.7068954706192017 2.883574962615967 145.88563537597656
Loss :  1.6938250064849854 2.6567349433898926 134.53057861328125
Loss :  1.6938055753707886 3.111832857131958 157.2854461669922
Loss :  1.6710658073425293 3.133857011795044 158.36392211914062
Loss :  1.6809196472167969 2.5029852390289307 126.83018493652344
Loss :  1.6807655096054077 2.796370267868042 141.49928283691406
Loss :  1.7171140909194946 2.6276450157165527 133.099365234375
Loss :  1.7199885845184326 2.402088165283203 121.82439422607422
Loss :  1.7251942157745361 2.4412286281585693 123.78662872314453
  batch 40 loss: 1.7251942157745361, 2.4412286281585693, 123.78662872314453
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7018990516662598 2.8473076820373535 144.06729125976562
Loss :  1.6940861940383911 2.341083288192749 118.74824523925781
Loss :  1.6870251893997192 2.589599609375 131.16700744628906
Loss :  1.6949490308761597 2.815119981765747 142.45094299316406
Loss :  1.6831746101379395 2.4171767234802246 122.54200744628906
Loss :  1.7009599208831787 3.500478982925415 176.72491455078125
Loss :  1.7203478813171387 2.41276216506958 122.35845184326172
Loss :  1.6923856735229492 2.567538261413574 130.0692901611328
Loss :  1.7295876741409302 2.3572893142700195 119.59405517578125
Loss :  1.6968332529067993 2.6275033950805664 133.07200622558594
Loss :  1.714996337890625 2.752361536026001 139.33306884765625
Loss :  1.7129039764404297 2.6394755840301514 133.6866912841797
Loss :  1.700100302696228 2.3666725158691406 120.03372955322266
Loss :  1.7175935506820679 3.4017486572265625 171.80502319335938
Loss :  1.6915329694747925 2.9777991771698 150.58148193359375
Loss :  1.7298072576522827 3.5615203380584717 179.8058319091797
Loss :  1.696323275566101 2.995522975921631 151.47247314453125
Loss :  1.68715238571167 3.1033775806427 156.8560333251953
Loss :  1.6955565214157104 2.856074810028076 144.49929809570312
Loss :  1.732669711112976 3.2904818058013916 166.2567596435547
  batch 60 loss: 1.732669711112976, 3.2904818058013916, 166.2567596435547
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6882411241531372 2.6165659427642822 132.51654052734375
Loss :  1.7035304307937622 2.7289421558380127 138.15065002441406
Loss :  1.6924338340759277 2.52811861038208 128.09835815429688
Loss :  1.6855560541152954 3.410959482192993 172.23353576660156
Loss :  1.6737629175186157 2.2043066024780273 111.88909149169922
Loss :  1.6910476684570312 3.960432767868042 199.71267700195312
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6985222101211548 3.935020923614502 198.44956970214844
Loss :  1.6959503889083862 3.7747933864593506 190.4356231689453
Loss :  1.7029340267181396 3.697908878326416 186.59837341308594
Total LOSS train 140.7311719454252 valid 193.7990608215332
CE LOSS train 1.699785485634437 valid 0.4257335066795349
Contrastive LOSS train 2.7806277201725886 valid 0.924477219581604
EPOCH 246:
Loss :  1.712673544883728 2.448533296585083 124.13934326171875
Loss :  1.7189795970916748 2.3975491523742676 121.596435546875
Loss :  1.7019257545471191 2.4350199699401855 123.45292663574219
Loss :  1.7058898210525513 3.4841527938842773 175.9135284423828
Loss :  1.7143303155899048 2.4155681133270264 122.49273681640625
Loss :  1.6948778629302979 2.397918224334717 121.59078979492188
Loss :  1.7152259349822998 2.7413647174835205 138.78346252441406
Loss :  1.7007144689559937 2.5753824710845947 130.46983337402344
Loss :  1.6947225332260132 2.5578811168670654 129.58877563476562
Loss :  1.7156105041503906 2.4478514194488525 124.10818481445312
Loss :  1.6893858909606934 2.682257890701294 135.8022918701172
Loss :  1.688179850578308 3.027346134185791 153.0554962158203
Loss :  1.687346339225769 2.5065605640411377 127.01537322998047
Loss :  1.6910818815231323 2.2458879947662354 113.98548126220703
Loss :  1.7221488952636719 3.0200891494750977 152.7266082763672
Loss :  1.720877766609192 2.6536242961883545 134.40208435058594
Loss :  1.6850229501724243 2.532677412033081 128.3188934326172
Loss :  1.7020165920257568 2.5126841068267822 127.33622741699219
Loss :  1.6831386089324951 2.3953230381011963 121.44928741455078
Loss :  1.719727635383606 2.3589141368865967 119.66543579101562
  batch 20 loss: 1.719727635383606, 2.3589141368865967, 119.66543579101562
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6983530521392822 2.385357141494751 120.96621704101562
Loss :  1.68246591091156 3.181896924972534 160.77731323242188
Loss :  1.6932671070098877 3.804755449295044 191.93104553222656
Loss :  1.7037732601165771 3.265313148498535 164.96942138671875
Loss :  1.7198610305786133 2.2768847942352295 115.56410217285156
Loss :  1.6965115070343018 2.5172464847564697 127.558837890625
Loss :  1.7014659643173218 2.4180874824523926 122.60584259033203
Loss :  1.6990796327590942 2.524998426437378 127.94900512695312
Loss :  1.668692708015442 2.4849343299865723 125.91541290283203
Loss :  1.7186092138290405 2.622945547103882 132.8658905029297
Loss :  1.6685267686843872 3.538294792175293 178.58326721191406
Loss :  1.708638310432434 4.265095233917236 214.96339416503906
Loss :  1.6955915689468384 2.9670310020446777 150.04713439941406
Loss :  1.6951534748077393 2.803589344024658 141.8746337890625
Loss :  1.6734092235565186 2.9783883094787598 150.59283447265625
Loss :  1.682869553565979 2.6947009563446045 136.4179229736328
Loss :  1.6826902627944946 3.2124199867248535 162.30369567871094
Loss :  1.7183012962341309 3.173149824142456 160.37579345703125
Loss :  1.7204054594039917 2.9042344093322754 146.93211364746094
Loss :  1.7258511781692505 2.478722333908081 125.66197204589844
  batch 40 loss: 1.7258511781692505, 2.478722333908081, 125.66197204589844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7024558782577515 2.97541880607605 150.473388671875
Loss :  1.6950194835662842 2.4531354904174805 124.35179901123047
Loss :  1.6871421337127686 2.8594183921813965 144.65806579589844
Loss :  1.6942354440689087 2.735426425933838 138.46554565429688
Loss :  1.681801676750183 2.4407718181610107 123.72039031982422
Loss :  1.699773907661438 2.7186360359191895 137.63157653808594
Loss :  1.7192565202713013 3.2118732929229736 162.3129119873047
Loss :  1.6901929378509521 2.972184181213379 150.2993927001953
Loss :  1.7279549837112427 2.5486457347869873 129.16024780273438
Loss :  1.6945395469665527 2.553287982940674 129.3589324951172
Loss :  1.7128595113754272 2.497337579727173 126.57974243164062
Loss :  1.711319088935852 2.371396780014038 120.28115844726562
Loss :  1.6985692977905273 3.4805498123168945 175.72605895996094
Loss :  1.7166837453842163 2.7787115573883057 140.6522674560547
Loss :  1.6905319690704346 2.622112512588501 132.79615783691406
Loss :  1.7289994955062866 2.20926833152771 112.19241333007812
Loss :  1.695766806602478 2.577035427093506 130.54754638671875
Loss :  1.6864475011825562 2.680332660675049 135.7030792236328
Loss :  1.6947991847991943 2.723219394683838 137.8557586669922
Loss :  1.7321563959121704 2.9813108444213867 150.79769897460938
  batch 60 loss: 1.7321563959121704, 2.9813108444213867, 150.79769897460938
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
Loss :  1.6874877214431763 2.6269264221191406 133.03379821777344
Loss :  1.7023725509643555 2.5345072746276855 128.427734375
Loss :  1.6914265155792236 1.9671285152435303 100.0478515625
Loss :  1.6848137378692627 2.175557851791382 110.46270751953125
Loss :  1.6730774641036987 1.9569696187973022 99.52156066894531
Loss :  1.689518690109253 4.402054309844971 221.792236328125
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6970956325531006 4.336377143859863 218.51596069335938
Loss :  1.6947665214538574 4.375700950622559 220.4798126220703
Loss :  1.7011444568634033 4.318149089813232 217.6085968017578
Total LOSS train 137.0739512516902 valid 219.59915161132812
CE LOSS train 1.7002627189342792 valid 0.42528611421585083
Contrastive LOSS train 2.7074737640527577 valid 1.079537272453308
EPOCH 247:
Loss :  1.7117242813110352 3.539738178253174 178.69863891601562
Loss :  1.7191545963287354 2.69087290763855 136.26280212402344
Loss :  1.7023626565933228 2.794842481613159 141.44448852539062
Loss :  1.7069939374923706 2.646186113357544 134.0163116455078
Loss :  1.7151930332183838 2.516916036605835 127.56099700927734
Loss :  1.6959370374679565 2.4047675132751465 121.93431091308594
Loss :  1.715849757194519 3.6490931510925293 184.17050170898438
Loss :  1.7013359069824219 2.611248254776001 132.2637481689453
Loss :  1.6952687501907349 3.7148525714874268 187.43789672851562
Loss :  1.7164007425308228 2.7667832374572754 140.05555725097656
Loss :  1.6901113986968994 3.000227212905884 151.70147705078125
Loss :  1.688907265663147 2.461334466934204 124.75563049316406
Loss :  1.6878856420516968 2.4817774295806885 125.77676391601562
Loss :  1.6915833950042725 2.6407058238983154 133.7268829345703
Loss :  1.7228178977966309 2.8682808876037598 145.13687133789062
Loss :  1.7215301990509033 2.5453460216522217 128.98883056640625
Loss :  1.6858501434326172 3.139087200164795 158.6402130126953
Loss :  1.7026283740997314 2.373082160949707 120.35673522949219
Loss :  1.683903455734253 2.5618298053741455 129.775390625
Loss :  1.72085702419281 2.550501823425293 129.24595642089844
  batch 20 loss: 1.72085702419281, 2.550501823425293, 129.24595642089844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6991835832595825 2.458254814147949 124.61192321777344
Loss :  1.6831717491149902 2.5633704662323 129.8516845703125
Loss :  1.694009780883789 2.4557085037231445 124.47943115234375
Loss :  1.7040205001831055 3.149493455886841 159.17869567871094
Loss :  1.7201465368270874 2.303013563156128 116.8708267211914
Loss :  1.6965504884719849 2.268745183944702 115.13380432128906
Loss :  1.7014405727386475 2.4558236598968506 124.49262237548828
Loss :  1.6989805698394775 2.424240827560425 122.91102600097656
Loss :  1.6681602001190186 2.4457266330718994 123.9544906616211
Loss :  1.7184842824935913 2.7372894287109375 138.5829620361328
Loss :  1.6678252220153809 2.9738662242889404 150.36114501953125
Loss :  1.7083563804626465 2.659809112548828 134.6988067626953
Loss :  1.6948429346084595 3.185044527053833 160.9470672607422
Loss :  1.6943848133087158 2.5319466590881348 128.29171752929688
Loss :  1.6719188690185547 2.654811143875122 134.4124755859375
Loss :  1.6815382242202759 2.629289388656616 133.14599609375
Loss :  1.6813520193099976 2.810425043106079 142.20260620117188
Loss :  1.717471957206726 2.584984540939331 130.96669006347656
Loss :  1.7202634811401367 2.844207525253296 143.93063354492188
Loss :  1.7257323265075684 2.3538198471069336 119.4167251586914
  batch 40 loss: 1.7257323265075684, 2.3538198471069336, 119.4167251586914
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7024732828140259 2.775679111480713 140.48641967773438
Loss :  1.6949266195297241 2.1997179985046387 111.68082427978516
Loss :  1.6873189210891724 2.6898512840270996 136.1798858642578
Loss :  1.6952433586120605 2.4695351123809814 125.1719970703125
Loss :  1.6829599142074585 2.417726993560791 122.5693130493164
Loss :  1.7007827758789062 2.4495081901550293 124.17619323730469
Loss :  1.7202651500701904 2.8898513317108154 146.21282958984375
Loss :  1.6914595365524292 2.5285258293151855 128.1177520751953
Loss :  1.7295622825622559 3.2733917236328125 165.39915466308594
Loss :  1.696068286895752 2.8163557052612305 142.51385498046875
Loss :  1.7146291732788086 3.2255125045776367 162.99024963378906
Loss :  1.7128889560699463 2.5421743392944336 128.8216094970703
Loss :  1.700153112411499 2.474257707595825 125.41303253173828
Loss :  1.718098759651184 3.16540789604187 159.98849487304688
Loss :  1.6919337511062622 2.4449870586395264 123.9412841796875
Loss :  1.7305865287780762 2.4508955478668213 124.27536010742188
Loss :  1.6974432468414307 2.7221686840057373 137.80587768554688
Loss :  1.6877833604812622 4.043095111846924 203.84254455566406
Loss :  1.6963675022125244 4.1051344871521 206.95309448242188
Loss :  1.7334970235824585 2.9151389598846436 147.4904327392578
  batch 60 loss: 1.7334970235824585, 2.9151389598846436, 147.4904327392578
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6896108388900757 2.718215227127075 137.60037231445312
Loss :  1.7047821283340454 2.4470651149749756 124.05803680419922
Loss :  1.6939821243286133 2.383709192276001 120.87944793701172
Loss :  1.6871726512908936 2.7399404048919678 138.6842041015625
Loss :  1.67569899559021 3.1611030101776123 159.73085021972656
Loss :  1.6950644254684448 3.9282689094543457 198.1085205078125
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
Loss :  1.7022426128387451 3.829970598220825 193.20077514648438
Loss :  1.6998614072799683 3.8219873905181885 192.79922485351562
Loss :  1.7065287828445435 3.5916552543640137 181.28929138183594
Total LOSS train 139.0057607797476 valid 191.3494529724121
CE LOSS train 1.7009202810434194 valid 0.42663219571113586
Contrastive LOSS train 2.7460968054257906 valid 0.8979138135910034
EPOCH 248:
Loss :  1.7145193815231323 2.467780828475952 125.10355377197266
Loss :  1.721083164215088 2.6551475524902344 134.4784698486328
Loss :  1.7040001153945923 2.404218912124634 121.91494750976562
Loss :  1.7083697319030762 3.0958986282348633 156.50331115722656
Loss :  1.716293454170227 3.2423012256622314 163.83135986328125
Loss :  1.6976063251495361 3.6868817806243896 186.04168701171875
Loss :  1.7171744108200073 2.4614148139953613 124.78791046142578
Loss :  1.7034413814544678 2.4036035537719727 121.88361358642578
Loss :  1.6976958513259888 2.470080852508545 125.20173645019531
Loss :  1.718253254890442 2.770021915435791 140.21934509277344
Loss :  1.6926223039627075 3.266288995742798 165.007080078125
Loss :  1.6915438175201416 3.25075364112854 164.22923278808594
Loss :  1.6908119916915894 3.0772056579589844 155.5511016845703
Loss :  1.6941074132919312 2.5669445991516113 130.0413360595703
Loss :  1.7249857187271118 2.6148598194122314 132.46798706054688
Loss :  1.7232398986816406 2.8332436084747314 143.3854217529297
Loss :  1.6884944438934326 2.51694393157959 127.53569030761719
Loss :  1.7050963640213013 2.460846185684204 124.74740600585938
Loss :  1.6864486932754517 2.0679433345794678 105.08361053466797
Loss :  1.7218269109725952 2.1562368869781494 109.5336685180664
  batch 20 loss: 1.7218269109725952, 2.1562368869781494, 109.5336685180664
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7001194953918457 3.3646655082702637 169.9333953857422
Loss :  1.684523344039917 2.3132407665252686 117.34656524658203
Loss :  1.6948926448822021 2.1806271076202393 110.72624969482422
Loss :  1.7042886018753052 2.9939095973968506 151.39976501464844
Loss :  1.7199264764785767 2.700232982635498 136.7315673828125
Loss :  1.6965959072113037 2.6542551517486572 134.4093475341797
Loss :  1.7015223503112793 2.5061261653900146 127.00782775878906
Loss :  1.6992243528366089 2.6246023178100586 132.92933654785156
Loss :  1.6686501502990723 2.7856554985046387 140.95143127441406
Loss :  1.7178157567977905 3.4381163120269775 173.62364196777344
Loss :  1.6686923503875732 2.5950851440429688 131.42294311523438
Loss :  1.7089101076126099 2.988609790802002 151.139404296875
Loss :  1.6959049701690674 2.544719696044922 128.931884765625
Loss :  1.6955527067184448 3.018700361251831 152.6305694580078
Loss :  1.673606038093567 2.444599151611328 123.903564453125
Loss :  1.6827963590621948 2.1999154090881348 111.6785659790039
Loss :  1.6820611953735352 2.5096261501312256 127.16336822509766
Loss :  1.7174545526504517 2.1253345012664795 107.98417663574219
Loss :  1.7198899984359741 2.487226724624634 126.08122253417969
Loss :  1.7250957489013672 2.5240092277526855 127.9255599975586
  batch 40 loss: 1.7250957489013672, 2.5240092277526855, 127.9255599975586
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7013310194015503 3.3197073936462402 167.6866912841797
Loss :  1.6937048435211182 2.305117607116699 116.9495849609375
Loss :  1.6859095096588135 2.766948699951172 140.03335571289062
Loss :  1.6938995122909546 2.5367114543914795 128.5294647216797
Loss :  1.681514859199524 2.324019193649292 117.88247680664062
Loss :  1.6993465423583984 2.439021348953247 123.6504135131836
Loss :  1.7189257144927979 2.381932020187378 120.8155288696289
Loss :  1.6897989511489868 2.310225486755371 117.2010726928711
Loss :  1.7283353805541992 2.353849172592163 119.42079162597656
Loss :  1.694447636604309 2.228026866912842 113.09579467773438
Loss :  1.7132353782653809 2.4012234210968018 121.77440643310547
Loss :  1.7115352153778076 2.6390061378479004 133.66183471679688
Loss :  1.6992846727371216 2.8416295051574707 143.78076171875
Loss :  1.7178044319152832 2.7610130310058594 139.76846313476562
Loss :  1.6918708086013794 2.872537851333618 145.31875610351562
Loss :  1.730124592781067 2.907822608947754 147.12124633789062
Loss :  1.696939468383789 3.1508615016937256 159.24002075195312
Loss :  1.6874638795852661 2.5273208618164062 128.0535125732422
Loss :  1.6964309215545654 2.712353467941284 137.31410217285156
Loss :  1.7333567142486572 2.627782106399536 133.12245178222656
  batch 60 loss: 1.7333567142486572, 2.627782106399536, 133.12245178222656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6891051530838013 2.7259600162506104 137.9871063232422
Loss :  1.7045859098434448 2.657460927963257 134.57763671875
Loss :  1.693618893623352 2.394951343536377 121.4411849975586
Loss :  1.6869808435440063 2.769526958465576 140.163330078125
Loss :  1.6756666898727417 2.094297409057617 106.39054107666016
Loss :  1.6944044828414917 4.386670112609863 221.0279083251953
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.7017983198165894 4.317825794219971 217.5930938720703
Loss :  1.6993122100830078 4.223019599914551 212.8502960205078
Loss :  1.706239938735962 4.3732075691223145 220.3666229248047
Total LOSS train 134.40691363994893 valid 217.95948028564453
CE LOSS train 1.701390081185561 valid 0.4265599846839905
Contrastive LOSS train 2.654110472018902 valid 1.0933018922805786
EPOCH 249:
Loss :  1.7143051624298096 3.1544997692108154 159.43930053710938
Loss :  1.720895528793335 3.034956455230713 153.46871948242188
Loss :  1.7038494348526 2.454061985015869 124.40695190429688
Loss :  1.7079771757125854 2.333526134490967 118.38428497314453
Loss :  1.71597158908844 2.226898670196533 113.06090545654297
Loss :  1.6965564489364624 3.512880802154541 177.34060668945312
Loss :  1.7167466878890991 3.5772507190704346 180.57928466796875
Loss :  1.7025812864303589 2.6867196559906006 136.03855895996094
Loss :  1.6964900493621826 2.3402531147003174 118.70914459228516
Loss :  1.7172764539718628 2.4787018299102783 125.6523666381836
Loss :  1.6910871267318726 3.024235963821411 152.9028778076172
Loss :  1.6895873546600342 3.12882924079895 158.1310577392578
Loss :  1.6885253190994263 2.5982816219329834 131.60260009765625
Loss :  1.6919333934783936 2.4552359580993652 124.45372772216797
Loss :  1.7227755784988403 2.302781105041504 116.86183166503906
Loss :  1.7210246324539185 2.784524440765381 140.94725036621094
Loss :  1.6861186027526855 2.636600971221924 133.5161590576172
Loss :  1.7018059492111206 3.3736672401428223 170.3851776123047
Loss :  1.6840295791625977 2.626875162124634 133.02780151367188
Loss :  1.7192847728729248 2.5586771965026855 129.6531524658203
  batch 20 loss: 1.7192847728729248, 2.5586771965026855, 129.6531524658203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
Loss :  1.6982722282409668 2.4153900146484375 122.4677734375
Loss :  1.6822950839996338 2.5258119106292725 127.97289276123047
Loss :  1.6928292512893677 2.4903132915496826 126.20849609375
Loss :  1.7024213075637817 2.943912982940674 148.8980712890625
Loss :  1.7186955213546753 2.9349746704101562 148.46742248535156
Loss :  1.6951144933700562 2.641268253326416 133.75852966308594
Loss :  1.7001123428344727 3.938615322113037 198.63088989257812
Loss :  1.6972708702087402 2.4738876819610596 125.39165496826172
Loss :  1.6664403676986694 2.879782199859619 145.65554809570312
Loss :  1.716646432876587 3.368298053741455 170.1315460205078
Loss :  1.666150450706482 3.736780881881714 188.5052032470703
Loss :  1.7067148685455322 2.795624017715454 141.4879150390625
Loss :  1.693421483039856 2.6338448524475098 133.3856658935547
Loss :  1.6931753158569336 2.76944637298584 140.16549682617188
Loss :  1.670990228652954 2.816818952560425 142.51193237304688
Loss :  1.6809056997299194 2.654118776321411 134.3868408203125
Loss :  1.6807529926300049 3.2860772609710693 165.984619140625
Loss :  1.7170166969299316 2.7978506088256836 141.6095428466797
Loss :  1.7194715738296509 2.217960834503174 112.61751556396484
Loss :  1.7250072956085205 2.141644239425659 108.80722045898438
  batch 40 loss: 1.7250072956085205, 2.141644239425659, 108.80722045898438
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.70164954662323 2.212113380432129 112.30731964111328
Loss :  1.6941546201705933 2.3279876708984375 118.09353637695312
Loss :  1.686635136604309 2.6085317134857178 132.11322021484375
Loss :  1.694669246673584 2.6294174194335938 133.16554260253906
Loss :  1.6824641227722168 2.4285871982574463 123.11182403564453
Loss :  1.7000389099121094 2.781855344772339 140.79281616210938
Loss :  1.71927809715271 3.124760389328003 157.95730590820312
Loss :  1.690677285194397 2.351571559906006 119.26924896240234
Loss :  1.7271085977554321 2.9191932678222656 147.686767578125
Loss :  1.694519281387329 2.7360446453094482 138.4967498779297
Loss :  1.7131290435791016 2.1724820137023926 110.33722686767578
Loss :  1.7113206386566162 2.534252643585205 128.4239501953125
Loss :  1.698471188545227 2.3897132873535156 121.18413543701172
Loss :  1.7158886194229126 2.441711664199829 123.80147552490234
Loss :  1.691034197807312 2.3811378479003906 120.7479248046875
Loss :  1.728632926940918 2.939955711364746 148.72642517089844
Loss :  1.6956896781921387 2.5664618015289307 130.01878356933594
Loss :  1.6864713430404663 2.6872596740722656 136.04945373535156
Loss :  1.6952030658721924 2.7513468265533447 139.26254272460938
Loss :  1.7321094274520874 2.2647545337677 114.96983337402344
  batch 60 loss: 1.7321094274520874, 2.2647545337677, 114.96983337402344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6872944831848145 2.762024164199829 139.78851318359375
Loss :  1.7031700611114502 2.675889492034912 135.49765014648438
Loss :  1.691706895828247 2.4178085327148438 122.5821304321289
Loss :  1.6857194900512695 2.6440465450286865 133.88804626464844
Loss :  1.6740915775299072 1.9670442342758179 100.02630615234375
Loss :  1.6965434551239014 4.3456830978393555 218.98069763183594
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.7042748928070068 4.27028751373291 215.21864318847656
Loss :  1.7019788026809692 4.233159065246582 213.3599395751953
Loss :  1.7068055868148804 4.10745906829834 207.0797576904297
Total LOSS train 136.675465627817 valid 213.65975952148438
CE LOSS train 1.7002100632740902 valid 0.4267013967037201
Contrastive LOSS train 2.699505088879512 valid 1.026864767074585
EPOCH 250:
Loss :  1.711995244026184 2.259181261062622 114.67105865478516
Loss :  1.7190427780151367 3.6477038860321045 184.10423278808594
Loss :  1.7017371654510498 2.486773729324341 126.04042053222656
Loss :  1.7061221599578857 2.483462333679199 125.87924194335938
Loss :  1.7148371934890747 2.685863733291626 136.00802612304688
Loss :  1.6952866315841675 2.3645591735839844 119.92324829101562
Loss :  1.7151522636413574 2.538578748703003 128.6440887451172
Loss :  1.7011693716049194 3.078784942626953 155.64041137695312
Loss :  1.6959329843521118 2.781433343887329 140.76760864257812
Loss :  1.7156492471694946 2.4641852378845215 124.92491149902344
Loss :  1.6911160945892334 2.8034467697143555 141.8634490966797
Loss :  1.6896140575408936 2.7821996212005615 140.7996063232422
Loss :  1.6890000104904175 2.926389217376709 148.00845336914062
Loss :  1.6928510665893555 2.396899461746216 121.5378189086914
Loss :  1.723397135734558 3.39501690864563 171.4742431640625
Loss :  1.721922755241394 2.375847816467285 120.51431274414062
Loss :  1.687032699584961 2.8024117946624756 141.8076171875
Loss :  1.7028093338012695 2.4692471027374268 125.16516876220703
Loss :  1.6851389408111572 2.7494022846221924 139.15524291992188
Loss :  1.7199314832687378 2.513460874557495 127.39297485351562
  batch 20 loss: 1.7199314832687378, 2.513460874557495, 127.39297485351562
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.699832797050476 3.4153082370758057 172.46524047851562
Loss :  1.684598684310913 2.2269511222839355 113.03215789794922
Loss :  1.694915533065796 2.1113109588623047 107.26046752929688
Loss :  1.7043660879135132 2.3621790409088135 119.81332397460938
Loss :  1.720047116279602 3.1908764839172363 161.2638702392578
Loss :  1.6971064805984497 2.34979510307312 119.18685913085938
Loss :  1.7014110088348389 3.129356861114502 158.1692657470703
Loss :  1.6991571187973022 2.592362403869629 131.31727600097656
Loss :  1.6685619354248047 3.3381261825561523 168.5748748779297
Loss :  1.7179803848266602 2.7197623252868652 137.7061004638672
Loss :  1.6682167053222656 2.744218349456787 138.87913513183594
Loss :  1.7081255912780762 2.6442248821258545 133.91937255859375
Loss :  1.6947609186172485 2.5275399684906006 128.07176208496094
Loss :  1.6941370964050293 2.5261452198028564 128.0013885498047
Loss :  1.6723780632019043 2.795747995376587 141.45977783203125
Loss :  1.6814210414886475 3.0741074085235596 155.3867950439453
Loss :  1.6810740232467651 2.7758631706237793 140.47422790527344
Loss :  1.7164051532745361 2.5228307247161865 127.85794067382812
Loss :  1.7187654972076416 2.293476104736328 116.39257049560547
Loss :  1.7240008115768433 2.3204245567321777 117.74522399902344
  batch 40 loss: 1.7240008115768433, 2.3204245567321777, 117.74522399902344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7012227773666382 3.124128818511963 157.90765380859375
Loss :  1.6925469636917114 2.5988924503326416 131.63717651367188
Loss :  1.6863082647323608 2.3394088745117188 118.65675354003906
Loss :  1.6933451890945435 2.9871840476989746 151.05255126953125
Loss :  1.6819109916687012 2.874661445617676 145.4149932861328
Loss :  1.6990269422531128 3.1757419109344482 160.48611450195312
Loss :  1.7180284261703491 2.4694528579711914 125.19066619873047
Loss :  1.690618634223938 3.1669580936431885 160.03851318359375
Loss :  1.7267749309539795 2.381157875061035 120.78466796875
Loss :  1.6935303211212158 2.4096128940582275 122.1741714477539
Loss :  1.712634801864624 3.0022122859954834 151.8232421875
Loss :  1.7095921039581299 2.615175724029541 132.4683837890625
Loss :  1.6967098712921143 2.376572608947754 120.52533721923828
Loss :  1.7127456665039062 3.1335132122039795 158.38839721679688
Loss :  1.688860535621643 3.140956401824951 158.73667907714844
Loss :  1.725807785987854 2.437026262283325 123.57711791992188
Loss :  1.6926250457763672 2.6996638774871826 136.6758270263672
Loss :  1.6827677488327026 3.4432551860809326 173.8455352783203
Loss :  1.6911430358886719 3.3278231620788574 168.08230590820312
Loss :  1.7289481163024902 2.288771152496338 116.16751098632812
  batch 60 loss: 1.7289481163024902, 2.288771152496338, 116.16751098632812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
Loss :  1.6817128658294678 2.645113945007324 133.93740844726562
Loss :  1.6981881856918335 2.3922317028045654 121.30977630615234
Loss :  1.6859115362167358 2.5065419673919678 127.01300811767578
Loss :  1.679064154624939 3.849078893661499 194.1330108642578
Loss :  1.666936993598938 1.8277238607406616 93.05313110351562
Loss :  1.6878581047058105 4.01285457611084 202.33058166503906
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.6956709623336792 3.9413299560546875 198.7621612548828
Loss :  1.6931228637695312 3.8893983364105225 196.16302490234375
Loss :  1.7000312805175781 3.7579500675201416 189.5975341796875
Total LOSS train 137.7596876878005 valid 196.71332550048828
CE LOSS train 1.6994455777681792 valid 0.42500782012939453
Contrastive LOSS train 2.721204843887916 valid 0.9394875168800354
EPOCH 251:
Loss :  1.7061220407485962 2.2650043964385986 114.95634460449219
Loss :  1.7138218879699707 2.8899998664855957 146.2138214111328
Loss :  1.6957658529281616 2.5382039546966553 128.60595703125
Loss :  1.700171947479248 2.9528372287750244 149.34202575683594
Loss :  1.7097054719924927 2.3779733180999756 120.6083755493164
Loss :  1.688629150390625 2.2860190868377686 115.98958587646484
Loss :  1.7096831798553467 2.3822226524353027 120.82081604003906
Loss :  1.6951833963394165 2.4279837608337402 123.09436798095703
Loss :  1.6898056268692017 2.905893564224243 146.98448181152344
Loss :  1.710323452949524 2.32308292388916 117.86447143554688
Loss :  1.6841208934783936 2.559866189956665 129.67742919921875
Loss :  1.6837825775146484 2.5570476055145264 129.53616333007812
Loss :  1.682530403137207 2.748997926712036 139.13241577148438
Loss :  1.6870492696762085 2.7952475547790527 141.4494171142578
Loss :  1.7185972929000854 2.822277784347534 142.83248901367188
Loss :  1.716192364692688 2.656891107559204 134.5607452392578
Loss :  1.6811468601226807 2.3068830966949463 117.02529907226562
Loss :  1.697159767150879 2.3016343116760254 116.77887725830078
Loss :  1.6794238090515137 2.0603582859039307 104.69733428955078
Loss :  1.716004729270935 2.7496955394744873 139.20077514648438
  batch 20 loss: 1.716004729270935, 2.7496955394744873, 139.20077514648438
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6949604749679565 2.604564905166626 131.92320251464844
Loss :  1.6790412664413452 2.430399179458618 123.1989974975586
Loss :  1.6893161535263062 2.1163151264190674 107.50507354736328
Loss :  1.6996263265609741 4.334746360778809 218.43695068359375
Loss :  1.7161792516708374 3.0656659603118896 154.99948120117188
Loss :  1.6915208101272583 2.7179861068725586 137.59083557128906
Loss :  1.696912407875061 3.010596752166748 152.22674560546875
Loss :  1.6942633390426636 2.726231336593628 138.00582885742188
Loss :  1.6638073921203613 2.5493195056915283 129.12977600097656
Loss :  1.714133858680725 2.566563129425049 130.0422821044922
Loss :  1.6640547513961792 2.527329444885254 128.030517578125
Loss :  1.7045190334320068 2.685762882232666 135.99266052246094
Loss :  1.690502405166626 2.5110998153686523 127.24549865722656
Loss :  1.6899336576461792 2.4423561096191406 123.8077392578125
Loss :  1.6684167385101318 3.7452642917633057 188.93162536621094
Loss :  1.6778020858764648 3.286400556564331 165.99781799316406
Loss :  1.6776200532913208 2.4223318099975586 122.7942123413086
Loss :  1.7130016088485718 2.1591830253601074 109.67215728759766
Loss :  1.7162846326828003 2.739560604095459 138.69430541992188
Loss :  1.7216224670410156 2.5903728008270264 131.24026489257812
  batch 40 loss: 1.7216224670410156, 2.5903728008270264, 131.24026489257812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
Loss :  1.6982964277267456 2.7950563430786133 141.4511260986328
Loss :  1.68997323513031 1.9616776704788208 99.77385711669922
Loss :  1.683731198310852 2.9135794639587402 147.36270141601562
Loss :  1.690940260887146 2.124861001968384 107.93399047851562
Loss :  1.679463505744934 3.1501142978668213 159.1851806640625
Loss :  1.696799635887146 3.3884520530700684 171.11940002441406
Loss :  1.716146469116211 2.620861530303955 132.75921630859375
Loss :  1.688205361366272 2.2737576961517334 115.37608337402344
Loss :  1.7250770330429077 4.556947708129883 229.5724639892578
Loss :  1.69184148311615 2.681309461593628 135.75732421875
Loss :  1.7111166715621948 2.488600015640259 126.14111328125
Loss :  1.7085785865783691 2.689356803894043 136.17640686035156
Loss :  1.6961389780044556 2.626929759979248 133.04261779785156
Loss :  1.713044285774231 2.679326295852661 135.67935180664062
Loss :  1.6892739534378052 2.4081614017486572 122.09735107421875
Loss :  1.7258350849151611 2.690157175064087 136.2336883544922
Loss :  1.6933449506759644 2.705195903778076 136.95314025878906
Loss :  1.6847697496414185 2.566960334777832 130.0327911376953
Loss :  1.6930046081542969 2.9060235023498535 146.9941864013672
Loss :  1.7298511266708374 2.6916441917419434 136.31207275390625
  batch 60 loss: 1.7298511266708374, 2.6916441917419434, 136.31207275390625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6848725080490112 3.456434488296509 174.50660705566406
Loss :  1.701601266860962 2.998852491378784 151.64422607421875
Loss :  1.6897244453430176 2.6062381267547607 132.0016326904297
Loss :  1.6836682558059692 2.6518850326538086 134.27792358398438
Loss :  1.6716879606246948 2.2808752059936523 115.71544647216797
Loss :  1.6969237327575684 4.39077615737915 221.23573303222656
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.704643726348877 4.2893900871276855 216.1741485595703
Loss :  1.702089786529541 4.195753574371338 211.48976135253906
Loss :  1.7080708742141724 4.158999443054199 209.6580352783203
Total LOSS train 136.3528779249925 valid 214.63941955566406
CE LOSS train 1.696395780489995 valid 0.4270177185535431
Contrastive LOSS train 2.6931296586990356 valid 1.0397498607635498
EPOCH 252:
Loss :  1.7097082138061523 2.6579902172088623 134.6092071533203
Loss :  1.7167896032333374 2.951817512512207 149.30767822265625
Loss :  1.6992441415786743 2.218738555908203 112.63616943359375
Loss :  1.7035993337631226 2.652435541152954 134.32537841796875
Loss :  1.7126959562301636 2.436884641647339 123.55692291259766
Loss :  1.6923810243606567 2.4492406845092773 124.1544189453125
Loss :  1.7125588655471802 2.757361650466919 139.5806427001953
Loss :  1.6985499858856201 2.8001291751861572 141.7050018310547
Loss :  1.6935625076293945 3.1621971130371094 159.8034210205078
Loss :  1.7130005359649658 3.965745687484741 200.00027465820312
Loss :  1.688447117805481 2.7441821098327637 138.89755249023438
Loss :  1.6879169940948486 2.224503755569458 112.9131088256836
Loss :  1.6869817972183228 2.5356409549713135 128.4690399169922
Loss :  1.6911934614181519 3.1189658641815186 157.6394805908203
Loss :  1.7218472957611084 2.360184669494629 119.7310791015625
Loss :  1.7193101644515991 2.4205141067504883 122.74501037597656
Loss :  1.6856051683425903 2.8404531478881836 143.70826721191406
Loss :  1.7011369466781616 2.3740394115448 120.40310668945312
Loss :  1.6840150356292725 2.0179872512817383 102.5833740234375
Loss :  1.7188645601272583 2.7823781967163086 140.83778381347656
  batch 20 loss: 1.7188645601272583, 2.7823781967163086, 140.83778381347656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.698438286781311 2.643864631652832 133.8916778564453
Loss :  1.6832748651504517 3.486271858215332 175.9968719482422
Loss :  1.6933616399765015 4.07314395904541 205.35055541992188
Loss :  1.7033835649490356 2.9209282398223877 147.7498016357422
Loss :  1.71952223777771 2.6716604232788086 135.30255126953125
Loss :  1.6954883337020874 2.4316694736480713 123.27896118164062
Loss :  1.7007957696914673 2.9545507431030273 149.42832946777344
Loss :  1.697770357131958 3.328965902328491 168.14605712890625
Loss :  1.668595552444458 2.496243953704834 126.48078918457031
Loss :  1.7170958518981934 2.773324728012085 140.38333129882812
Loss :  1.668652057647705 2.756810426712036 139.50917053222656
Loss :  1.7076040506362915 2.5929207801818848 131.35365295410156
Loss :  1.6939641237258911 2.7044599056243896 136.91696166992188
Loss :  1.6934239864349365 3.092634916305542 156.32516479492188
Loss :  1.6725162267684937 2.6816940307617188 135.75721740722656
Loss :  1.6814321279525757 2.4893510341644287 126.14898681640625
Loss :  1.6811226606369019 2.4228029251098633 122.8212661743164
Loss :  1.7153903245925903 2.2585153579711914 114.64115905761719
Loss :  1.71857750415802 2.387794256210327 121.10828399658203
Loss :  1.723694086074829 2.791693687438965 141.30838012695312
  batch 40 loss: 1.723694086074829, 2.791693687438965, 141.30838012695312
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7009156942367554 3.311605930328369 167.2812042236328
Loss :  1.6927368640899658 3.360460042953491 169.71572875976562
Loss :  1.6861299276351929 3.9485232830047607 199.11228942871094
Loss :  1.69318687915802 2.9782440662384033 150.6053924560547
Loss :  1.6814481019973755 2.559666872024536 129.664794921875
Loss :  1.6988437175750732 2.675328254699707 135.4652557373047
Loss :  1.717961072921753 2.6112489700317383 132.28041076660156
Loss :  1.6894272565841675 2.468050241470337 125.0919418334961
Loss :  1.7260538339614868 2.4531474113464355 124.3834228515625
Loss :  1.6931782960891724 2.5313613414764404 128.26124572753906
Loss :  1.7120455503463745 2.5986318588256836 131.64364624023438
Loss :  1.7096705436706543 2.588515520095825 131.1354522705078
Loss :  1.697386384010315 2.2194411754608154 112.66944885253906
Loss :  1.7138373851776123 2.593968629837036 131.41226196289062
Loss :  1.6907083988189697 3.269989252090454 165.19017028808594
Loss :  1.7265726327896118 2.6324546337127686 133.34930419921875
Loss :  1.6945098638534546 2.6951606273651123 136.45252990722656
Loss :  1.6861228942871094 2.498522996902466 126.61227416992188
Loss :  1.6942030191421509 2.381301164627075 120.75926208496094
Loss :  1.7306596040725708 2.23728084564209 113.5947036743164
  batch 60 loss: 1.7306596040725708, 2.23728084564209, 113.5947036743164
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6861703395843506 2.1460776329040527 108.99005126953125
Loss :  1.702575445175171 2.224360942840576 112.92062377929688
Loss :  1.6909072399139404 2.168945550918579 110.13819122314453
Loss :  1.684839129447937 3.501152753829956 176.7424774169922
Loss :  1.6731513738632202 2.607893705368042 132.06784057617188
Loss :  1.7003039121627808 4.304249286651611 216.9127655029297
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.7080165147781372 4.30626106262207 217.0210723876953
Loss :  1.7053552865982056 4.2009172439575195 211.751220703125
Loss :  1.711190938949585 4.078553676605225 205.63888549804688
Total LOSS train 137.61609250582183 valid 212.83098602294922
CE LOSS train 1.6991500579393828 valid 0.42779773473739624
Contrastive LOSS train 2.7183388489943283 valid 1.0196384191513062
EPOCH 253:
Loss :  1.710878849029541 2.583155632019043 130.86865234375
Loss :  1.7176871299743652 2.621082067489624 132.77178955078125
Loss :  1.7005109786987305 2.127857208251953 108.09336853027344
Loss :  1.704890489578247 2.094961404800415 106.45295715332031
Loss :  1.7139577865600586 1.92996346950531 98.2121353149414
Loss :  1.694069504737854 2.459799289703369 124.68403625488281
Loss :  1.7136867046356201 2.2680439949035645 115.11588287353516
Loss :  1.7000634670257568 2.2879819869995117 116.09916687011719
Loss :  1.6948577165603638 2.6272971630096436 133.05970764160156
Loss :  1.7145129442214966 2.6259572505950928 133.0123748779297
Loss :  1.6893302202224731 3.2879631519317627 166.08749389648438
Loss :  1.6887544393539429 2.489952564239502 126.1863784790039
Loss :  1.6876885890960693 2.1897826194763184 111.17681884765625
Loss :  1.691748023033142 2.2185614109039307 112.61981964111328
Loss :  1.7220349311828613 2.3182225227355957 117.63316345214844
Loss :  1.7195323705673218 2.893643379211426 146.40170288085938
Loss :  1.6855785846710205 3.7410926818847656 188.74020385742188
Loss :  1.7017407417297363 2.6889612674713135 136.14979553222656
Loss :  1.68446683883667 2.247545003890991 114.06171417236328
Loss :  1.7184492349624634 2.7837300300598145 140.9049530029297
  batch 20 loss: 1.7184492349624634, 2.7837300300598145, 140.9049530029297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.698588490486145 2.934119701385498 148.40457153320312
Loss :  1.6829978227615356 2.4943606853485107 126.40103149414062
Loss :  1.692911148071289 2.2843849658966064 115.91215515136719
Loss :  1.7026609182357788 2.3641951084136963 119.91241455078125
Loss :  1.7187360525131226 2.8433239459991455 143.8849334716797
Loss :  1.694559097290039 3.169001340866089 160.14463806152344
Loss :  1.6995666027069092 2.8901185989379883 146.20550537109375
Loss :  1.6970887184143066 2.9127418994903564 147.3341827392578
Loss :  1.667301893234253 2.6975045204162598 136.5425262451172
Loss :  1.7162189483642578 3.0427629947662354 153.8543701171875
Loss :  1.6674696207046509 3.2158384323120117 162.45938110351562
Loss :  1.7067021131515503 2.462817668914795 124.84758758544922
Loss :  1.6931132078170776 2.624920606613159 132.93914794921875
Loss :  1.692617416381836 2.1692187786102295 110.15355682373047
Loss :  1.671470284461975 2.5971624851226807 131.5295867919922
Loss :  1.6807386875152588 3.568720817565918 180.1167755126953
Loss :  1.680596947669983 2.5984694957733154 131.6040802001953
Loss :  1.7151663303375244 2.5815088748931885 130.7906036376953
Loss :  1.7183666229248047 2.3887887001037598 121.15780639648438
Loss :  1.723439335823059 2.5463595390319824 129.04141235351562
  batch 40 loss: 1.723439335823059, 2.5463595390319824, 129.04141235351562
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.701021671295166 2.6877312660217285 136.08758544921875
Loss :  1.6927460432052612 2.5905256271362305 131.21902465820312
Loss :  1.6866028308868408 2.5943093299865723 131.40206909179688
Loss :  1.6934843063354492 2.670510768890381 135.21902465820312
Loss :  1.6821891069412231 2.229998826980591 113.18212890625
Loss :  1.699108362197876 2.8037705421447754 141.88763427734375
Loss :  1.7179092168807983 2.290015697479248 116.21869659423828
Loss :  1.6905382871627808 2.905064105987549 146.94374084472656
Loss :  1.7262911796569824 2.370650291442871 120.25880432128906
Loss :  1.6940152645111084 2.762355327606201 139.81178283691406
Loss :  1.7126924991607666 2.495020627975464 126.4637222290039
Loss :  1.710436224937439 3.3457117080688477 168.9960174560547
Loss :  1.6981208324432373 3.3104395866394043 167.2200927734375
Loss :  1.715542197227478 3.067251682281494 155.078125
Loss :  1.6915497779846191 2.7813076972961426 140.75692749023438
Loss :  1.7278777360916138 2.8060667514801025 142.03121948242188
Loss :  1.6959583759307861 2.702183723449707 136.80514526367188
Loss :  1.6871154308319092 2.118220806121826 107.59815979003906
Loss :  1.6954526901245117 2.8205647468566895 142.72369384765625
Loss :  1.7312461137771606 2.0743520259857178 105.44884490966797
  batch 60 loss: 1.7312461137771606, 2.0743520259857178, 105.44884490966797
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6875895261764526 2.300323486328125 116.70376586914062
Loss :  1.7036898136138916 2.775038242340088 140.45559692382812
Loss :  1.692283034324646 2.8920767307281494 146.29611206054688
Loss :  1.68660306930542 2.816821813583374 142.52769470214844
Loss :  1.675394058227539 2.258612871170044 114.60603332519531
Loss :  1.701729416847229 4.15001106262207 209.20228576660156
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.7093151807785034 4.097775459289551 206.5980987548828
Loss :  1.7070335149765015 3.9622042179107666 199.81724548339844
Loss :  1.7116270065307617 3.908928632736206 197.15805053710938
Total LOSS train 133.50015423114482 valid 203.19392013549805
CE LOSS train 1.6995416531196008 valid 0.42790675163269043
Contrastive LOSS train 2.6360122699003954 valid 0.9772321581840515
EPOCH 254:
Loss :  1.7126555442810059 3.4827451705932617 175.84991455078125
Loss :  1.7193580865859985 3.237215518951416 163.58013916015625
Loss :  1.7026722431182861 3.3014590740203857 166.77561950683594
Loss :  1.7070796489715576 3.38234806060791 170.82447814941406
Loss :  1.7159044742584229 2.8588459491729736 144.658203125
Loss :  1.6968187093734741 2.728749990463257 138.1343231201172
Loss :  1.7159005403518677 3.110569953918457 157.24440002441406
Loss :  1.7027146816253662 2.528839588165283 128.1446990966797
Loss :  1.6975632905960083 2.589092969894409 131.1522216796875
Loss :  1.7170089483261108 2.632220506668091 133.32803344726562
Loss :  1.6927707195281982 2.8257241249084473 142.97897338867188
Loss :  1.6916451454162598 2.79162335395813 141.27281188964844
Loss :  1.6910544633865356 2.7456369400024414 138.972900390625
Loss :  1.6949455738067627 2.465381622314453 124.96402740478516
Loss :  1.7241599559783936 3.106245279312134 157.03643798828125
Loss :  1.7216112613677979 2.7484285831451416 139.14305114746094
Loss :  1.688639521598816 2.450632333755493 124.22025299072266
Loss :  1.7036343812942505 2.4778122901916504 125.59425354003906
Loss :  1.686705231666565 2.2763547897338867 115.50444793701172
Loss :  1.720850944519043 2.67716383934021 135.57904052734375
  batch 20 loss: 1.720850944519043, 2.67716383934021, 135.57904052734375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7008755207061768 2.3511435985565186 119.258056640625
Loss :  1.6857439279556274 2.846590757369995 144.01528930664062
Loss :  1.695572018623352 2.8543381690979004 144.4124755859375
Loss :  1.70500910282135 2.5895347595214844 131.18174743652344
Loss :  1.72066330909729 2.994824171066284 151.4618682861328
Loss :  1.6973748207092285 2.584290027618408 130.91188049316406
Loss :  1.7026240825653076 2.5460219383239746 129.00372314453125
Loss :  1.7001608610153198 2.1441261768341064 108.90646362304688
Loss :  1.6713263988494873 2.5823445320129395 130.78855895996094
Loss :  1.718408226966858 3.416192054748535 172.52801513671875
Loss :  1.6711726188659668 3.012195587158203 152.28094482421875
Loss :  1.7094377279281616 2.7675580978393555 140.08734130859375
Loss :  1.6961230039596558 2.900148630142212 146.70355224609375
Loss :  1.6957371234893799 3.0291659832000732 153.15403747558594
Loss :  1.6756724119186401 2.710174083709717 137.1843719482422
Loss :  1.6844558715820312 2.6037509441375732 131.87200927734375
Loss :  1.6844573020935059 2.7663724422454834 140.00308227539062
Loss :  1.717668056488037 3.2028744220733643 161.86138916015625
Loss :  1.7207130193710327 2.9905788898468018 151.24966430664062
Loss :  1.725674033164978 3.278737783432007 165.66256713867188
  batch 40 loss: 1.725674033164978, 3.278737783432007, 165.66256713867188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.703895926475525 3.1439731121063232 158.90255737304688
Loss :  1.695224404335022 3.4370064735412598 173.54554748535156
Loss :  1.6901919841766357 2.4963793754577637 126.50916290283203
Loss :  1.6967417001724243 2.9504950046539307 149.22149658203125
Loss :  1.685563325881958 2.7377984523773193 138.5754852294922
Loss :  1.7017189264297485 2.501133441925049 126.75839233398438
Loss :  1.720113754272461 2.616872549057007 132.56373596191406
Loss :  1.6935385465621948 2.7870330810546875 141.04519653320312
Loss :  1.7283717393875122 2.7379634380340576 138.62655639648438
Loss :  1.6964350938796997 2.974040985107422 150.39849853515625
Loss :  1.7149966955184937 2.7890830039978027 141.1691436767578
Loss :  1.7123222351074219 3.015528917312622 152.48876953125
Loss :  1.6999517679214478 2.705010175704956 136.95045471191406
Loss :  1.7164112329483032 2.7111551761627197 137.274169921875
Loss :  1.6921573877334595 2.879267454147339 145.65553283691406
Loss :  1.7291362285614014 2.8221490383148193 142.8365936279297
Loss :  1.6969538927078247 2.761849880218506 139.78945922851562
Loss :  1.6877521276474 2.848195791244507 144.09754943847656
Loss :  1.6958760023117065 2.8571953773498535 144.55564880371094
Loss :  1.731988549232483 2.7598798274993896 139.72598266601562
  batch 60 loss: 1.731988549232483, 2.7598798274993896, 139.72598266601562
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6881290674209595 2.499082326889038 126.64224243164062
Loss :  1.703900933265686 2.435316324234009 123.46971893310547
Loss :  1.69251549243927 2.505608320236206 126.97293090820312
Loss :  1.6865431070327759 3.148613214492798 159.11720275878906
Loss :  1.6749805212020874 1.899538278579712 96.65189361572266
Loss :  1.697965383529663 3.8788137435913086 195.63865661621094
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.7054020166397095 3.877220869064331 195.56643676757812
Loss :  1.7031182050704956 3.7327775955200195 188.34201049804688
Loss :  1.7087252140045166 3.623542547225952 182.88584899902344
Total LOSS train 141.40044907789965 valid 190.60823822021484
CE LOSS train 1.7018149761053232 valid 0.42718130350112915
Contrastive LOSS train 2.7939726462730996 valid 0.905885636806488
EPOCH 255:
Loss :  1.712641716003418 2.4166946411132812 122.54737091064453
Loss :  1.7191917896270752 2.941016912460327 148.77003479003906
Loss :  1.7020407915115356 2.6710317134857178 135.25363159179688
Loss :  1.7061854600906372 2.5042827129364014 126.92031860351562
Loss :  1.7147278785705566 1.9366167783737183 98.54557037353516
Loss :  1.6951568126678467 2.3913984298706055 121.26508331298828
Loss :  1.7146588563919067 2.2257611751556396 113.00271606445312
Loss :  1.7011339664459229 2.515850067138672 127.49363708496094
Loss :  1.695756435394287 2.5125973224639893 127.32562255859375
Loss :  1.715578556060791 2.3695695400238037 120.19405364990234
Loss :  1.690763235092163 2.591945171356201 131.28802490234375
Loss :  1.689759373664856 2.430720806121826 123.22579956054688
Loss :  1.689204216003418 3.268702268600464 165.12432861328125
Loss :  1.69345223903656 3.044316530227661 153.90927124023438
Loss :  1.7225326299667358 2.619570732116699 132.70106506347656
Loss :  1.7203959226608276 2.126474380493164 108.04411315917969
Loss :  1.6863470077514648 2.202237606048584 111.79822540283203
Loss :  1.701772928237915 2.406486749649048 122.02611541748047
Loss :  1.684191107749939 2.5782840251922607 130.598388671875
Loss :  1.7192611694335938 2.2429161071777344 113.86506652832031
  batch 20 loss: 1.7192611694335938, 2.2429161071777344, 113.86506652832031
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6988993883132935 2.098891496658325 106.64347076416016
Loss :  1.6830536127090454 2.21710205078125 112.53815460205078
Loss :  1.6931225061416626 2.5904152393341064 131.21388244628906
Loss :  1.703052043914795 2.7392807006835938 138.66708374023438
Loss :  1.718802571296692 2.5065972805023193 127.04866790771484
Loss :  1.6952955722808838 2.4488685131073 124.13872528076172
Loss :  1.7000707387924194 3.0738346576690674 155.3917999267578
Loss :  1.697851538658142 2.9339330196380615 148.39450073242188
Loss :  1.668472170829773 2.723259687423706 137.83145141601562
Loss :  1.7167009115219116 3.0118165016174316 152.30752563476562
Loss :  1.6684095859527588 3.122234582901001 157.78013610839844
Loss :  1.707234263420105 2.5401389598846436 128.7141876220703
Loss :  1.6942287683486938 2.574059247970581 130.39718627929688
Loss :  1.6938837766647339 2.403059720993042 121.84687042236328
Loss :  1.6728674173355103 2.500932455062866 126.71949005126953
Loss :  1.6819415092468262 2.496177911758423 126.49083709716797
Loss :  1.6818456649780273 2.7390224933624268 138.63296508789062
Loss :  1.7160881757736206 3.5332000255584717 178.3760986328125
Loss :  1.7188969850540161 2.444474935531616 123.94264221191406
Loss :  1.724014163017273 2.0319619178771973 103.32211303710938
  batch 40 loss: 1.724014163017273, 2.0319619178771973, 103.32211303710938
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7015612125396729 2.2063355445861816 112.01834106445312
Loss :  1.6930949687957764 1.9845101833343506 100.9186019897461
Loss :  1.6867589950561523 2.5533645153045654 129.35498046875
Loss :  1.693903923034668 2.6453375816345215 133.96078491210938
Loss :  1.6823710203170776 2.281317949295044 115.7482681274414
Loss :  1.6991677284240723 3.8198015689849854 192.6892547607422
Loss :  1.7177674770355225 2.3993680477142334 121.68616485595703
Loss :  1.6903892755508423 2.3086304664611816 117.12191772460938
Loss :  1.7262580394744873 2.4146738052368164 122.45994567871094
Loss :  1.693780779838562 2.609466552734375 132.1671142578125
Loss :  1.7125359773635864 2.7546238899230957 139.44374084472656
Loss :  1.710312008857727 2.548135280609131 129.11708068847656
Loss :  1.6980177164077759 2.036818742752075 103.53895568847656
Loss :  1.7148678302764893 2.339139223098755 118.67182922363281
Loss :  1.6913206577301025 2.522918462753296 127.83724212646484
Loss :  1.727771282196045 2.6130011081695557 132.37782287597656
Loss :  1.6959667205810547 2.2309367656707764 113.24280548095703
Loss :  1.687278151512146 2.0904250144958496 106.20852661132812
Loss :  1.6956210136413574 2.3529000282287598 119.34062194824219
Loss :  1.7314263582229614 2.4190475940704346 122.68380737304688
  batch 60 loss: 1.7314263582229614, 2.4190475940704346, 122.68380737304688
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6876522302627563 2.608520030975342 132.1136474609375
Loss :  1.7037699222564697 2.909492254257202 147.17837524414062
Loss :  1.6924304962158203 2.5164554119110107 127.51519775390625
Loss :  1.686484456062317 2.538372755050659 128.60511779785156
Loss :  1.675045132637024 2.1046714782714844 106.90862274169922
Loss :  1.701540231704712 4.140208721160889 208.71197509765625
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.7090932130813599 4.218091011047363 212.6136474609375
Loss :  1.7066295146942139 4.039120674133301 203.6626739501953
Loss :  1.7122551202774048 4.0250654220581055 202.96551513671875
Total LOSS train 128.26469221848708 valid 206.98845291137695
CE LOSS train 1.700077489706186 valid 0.4280637800693512
Contrastive LOSS train 2.5312922972899217 valid 1.0062663555145264
EPOCH 256:
Loss :  1.7124212980270386 2.8755834102630615 145.49159240722656
Loss :  1.7188529968261719 2.8862192630767822 146.02981567382812
Loss :  1.702118158340454 2.105107545852661 106.9574966430664
Loss :  1.706558108329773 2.0223183631896973 102.82247924804688
Loss :  1.7149368524551392 2.9596011638641357 149.6949920654297
Loss :  1.69550359249115 2.9204471111297607 147.71786499023438
Loss :  1.7147884368896484 2.226300001144409 113.02979278564453
Loss :  1.701513648033142 2.1212825775146484 107.7656478881836
Loss :  1.6964576244354248 2.3149654865264893 117.44473266601562
Loss :  1.7158992290496826 2.520781993865967 127.75499725341797
Loss :  1.6911728382110596 2.594461679458618 131.4142608642578
Loss :  1.6902824640274048 4.4070281982421875 222.04168701171875
Loss :  1.6899514198303223 1.9866987466812134 101.02488708496094
Loss :  1.6942768096923828 2.6681511402130127 135.10183715820312
Loss :  1.7231501340866089 2.35400128364563 119.4232177734375
Loss :  1.7209548950195312 2.231311798095703 113.28654479980469
Loss :  1.688077449798584 2.083066463470459 105.84140014648438
Loss :  1.7031387090682983 2.3120970726013184 117.30799102783203
Loss :  1.6868990659713745 2.1422739028930664 108.80059051513672
Loss :  1.720612645149231 2.669403314590454 135.19078063964844
  batch 20 loss: 1.720612645149231, 2.669403314590454, 135.19078063964844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7015235424041748 2.7185146808624268 137.62725830078125
Loss :  1.686338186264038 2.6945457458496094 136.41363525390625
Loss :  1.6959104537963867 2.249178886413574 114.15485382080078
Loss :  1.7057493925094604 3.6246063709259033 182.93606567382812
Loss :  1.7210462093353271 3.188581705093384 161.15013122558594
Loss :  1.6980082988739014 2.416332483291626 122.51463317871094
Loss :  1.7030482292175293 3.161616802215576 159.7838897705078
Loss :  1.7005261182785034 3.528513193130493 178.12619018554688
Loss :  1.6724209785461426 3.7389609813690186 188.62046813964844
Loss :  1.7191274166107178 3.0315985679626465 153.29905700683594
Loss :  1.6724547147750854 2.5912508964538574 131.23500061035156
Loss :  1.7101490497589111 3.5859575271606445 181.00802612304688
Loss :  1.6972599029541016 2.89284610748291 146.33956909179688
Loss :  1.69692862033844 2.5742247104644775 130.40817260742188
Loss :  1.6769822835922241 3.1812143325805664 160.73770141601562
Loss :  1.6856999397277832 2.9709904193878174 150.23521423339844
Loss :  1.685755729675293 2.6409900188446045 133.73526000976562
Loss :  1.7186075448989868 2.733928680419922 138.41505432128906
Loss :  1.721630573272705 2.5181283950805664 127.6280517578125
Loss :  1.7266312837600708 3.000802755355835 151.7667694091797
  batch 40 loss: 1.7266312837600708, 3.000802755355835, 151.7667694091797
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7050659656524658 2.319878339767456 117.69898223876953
Loss :  1.6971884965896606 2.5212249755859375 127.75843811035156
Loss :  1.6913257837295532 2.3911020755767822 121.24642944335938
Loss :  1.6979960203170776 2.6969804763793945 136.54702758789062
Loss :  1.6872279644012451 2.824934482574463 142.93394470214844
Loss :  1.7033721208572388 3.1315762996673584 158.2821807861328
Loss :  1.7213397026062012 2.947392702102661 149.09097290039062
Loss :  1.6953107118606567 1.8241304159164429 92.9018325805664
Loss :  1.7295472621917725 2.2403762340545654 113.74835968017578
Loss :  1.698642611503601 2.3709380626678467 120.24554443359375
Loss :  1.7164539098739624 2.3582656383514404 119.6297378540039
Loss :  1.7142032384872437 2.4626638889312744 124.84739685058594
Loss :  1.7025539875030518 2.219521999359131 112.67864990234375
Loss :  1.718289852142334 2.560014486312866 129.71900939941406
Loss :  1.6957653760910034 3.222076654434204 162.7996063232422
Loss :  1.7303000688552856 2.952871084213257 149.3738555908203
Loss :  1.7000056505203247 2.7487034797668457 139.13519287109375
Loss :  1.6916178464889526 2.859945774078369 144.68890380859375
Loss :  1.6997591257095337 2.630714178085327 133.23545837402344
Loss :  1.7342385053634644 2.128840446472168 108.17626190185547
  batch 60 loss: 1.7342385053634644, 2.128840446472168, 108.17626190185547
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6924705505371094 2.851247549057007 144.25485229492188
Loss :  1.7078806161880493 2.179165840148926 110.66616821289062
Loss :  1.6971021890640259 2.0436182022094727 103.87801361083984
Loss :  1.6913561820983887 2.571113348007202 130.2470245361328
Loss :  1.6804860830307007 1.8664608001708984 95.00353240966797
Loss :  1.7077274322509766 3.866450071334839 195.03024291992188
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.7150083780288696 3.9583046436309814 199.6302490234375
Loss :  1.7129141092300415 3.7954015731811523 191.48300170898438
Loss :  1.71695876121521 3.856088161468506 194.52137756347656
Total LOSS train 134.29330749511718 valid 195.16621780395508
CE LOSS train 1.702813302553617 valid 0.4292396903038025
Contrastive LOSS train 2.651809864777785 valid 0.9640220403671265
EPOCH 257:
Loss :  1.7156076431274414 2.8462870121002197 144.0299530029297
Loss :  1.721967339515686 3.6368935108184814 183.566650390625
Loss :  1.7057721614837646 2.4670979976654053 125.0606689453125
Loss :  1.7096368074417114 4.078737735748291 205.6465301513672
Loss :  1.7180187702178955 2.391686201095581 121.30233001708984
Loss :  1.6990684270858765 2.626694440841675 133.03378295898438
Loss :  1.7176374197006226 2.5766184329986572 130.54855346679688
Loss :  1.7045446634292603 2.510226011276245 127.2158432006836
Loss :  1.6993895769119263 2.4695253372192383 125.17565155029297
Loss :  1.717966079711914 2.702775478363037 136.85675048828125
Loss :  1.6945087909698486 2.8351759910583496 143.45330810546875
Loss :  1.6937004327774048 2.5530624389648438 129.34681701660156
Loss :  1.693049669265747 1.9250445365905762 97.94527435302734
Loss :  1.6969033479690552 2.3692173957824707 120.15777587890625
Loss :  1.7257384061813354 3.428107500076294 173.13111877441406
Loss :  1.723034381866455 2.6387226581573486 133.65916442871094
Loss :  1.6909122467041016 2.5878851413726807 131.08517456054688
Loss :  1.705226182937622 2.566948652267456 130.0526580810547
Loss :  1.6887263059616089 2.376025438308716 120.48999786376953
Loss :  1.7221382856369019 3.6563796997070312 184.54112243652344
  batch 20 loss: 1.7221382856369019, 3.6563796997070312, 184.54112243652344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7029122114181519 2.544741153717041 128.93997192382812
Loss :  1.6876107454299927 2.6643996238708496 134.9075927734375
Loss :  1.6971522569656372 2.1311962604522705 108.25696563720703
Loss :  1.7064316272735596 3.245082378387451 163.96055603027344
Loss :  1.7216747999191284 3.6601343154907227 184.7283935546875
Loss :  1.6989551782608032 2.841013193130493 143.74961853027344
Loss :  1.7039945125579834 2.517517328262329 127.57986450195312
Loss :  1.7013604640960693 2.1607706546783447 109.73989868164062
Loss :  1.6729196310043335 2.4955034255981445 126.44808959960938
Loss :  1.7192232608795166 2.455716371536255 124.50504302978516
Loss :  1.672843337059021 2.365997791290283 119.97273254394531
Loss :  1.7104039192199707 2.51222825050354 127.32181549072266
Loss :  1.697347640991211 2.2576186656951904 114.57828521728516
Loss :  1.696790337562561 2.88692569732666 146.04307556152344
Loss :  1.6767812967300415 3.3156864643096924 167.4611053466797
Loss :  1.6853035688400269 3.0110552310943604 152.23806762695312
Loss :  1.6852761507034302 2.688572406768799 136.1138916015625
Loss :  1.7179408073425293 3.8126165866851807 192.34877014160156
Loss :  1.7210875749588013 2.6693618297576904 135.18917846679688
Loss :  1.726111888885498 2.2759013175964355 115.52117919921875
  batch 40 loss: 1.726111888885498, 2.2759013175964355, 115.52117919921875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
Loss :  1.7042566537857056 2.7741074562072754 140.4096221923828
Loss :  1.6965521574020386 2.4691083431243896 125.15196228027344
Loss :  1.6904677152633667 2.4076640605926514 122.07366943359375
Loss :  1.697455883026123 2.315133571624756 117.45413208007812
Loss :  1.686358094215393 2.229403495788574 113.15653228759766
Loss :  1.7031558752059937 2.346298933029175 119.01810455322266
Loss :  1.7213177680969238 2.626082420349121 133.0254364013672
Loss :  1.6942673921585083 2.6277477741241455 133.0816650390625
Loss :  1.729175329208374 2.763725996017456 139.91546630859375
Loss :  1.6977375745773315 3.3315417766571045 168.2748260498047
Loss :  1.715551733970642 3.2618725299835205 164.80917358398438
Loss :  1.7131683826446533 2.8604190349578857 144.73411560058594
Loss :  1.7013574838638306 2.2818615436553955 115.79443359375
Loss :  1.7171472311019897 2.4295403957366943 123.19416809082031
Loss :  1.6941120624542236 2.322542667388916 117.82125091552734
Loss :  1.7294416427612305 2.4517245292663574 124.31566619873047
Loss :  1.6981136798858643 2.5365490913391113 128.52557373046875
Loss :  1.6892576217651367 3.0513720512390137 154.2578582763672
Loss :  1.6971793174743652 2.7715909481048584 140.27671813964844
Loss :  1.7323888540267944 2.451759099960327 124.32034301757812
  batch 60 loss: 1.7323888540267944, 2.451759099960327, 124.32034301757812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6892839670181274 2.5374836921691895 128.56346130371094
Loss :  1.7049192190170288 2.4049458503723145 121.95220947265625
Loss :  1.694055438041687 2.546342611312866 129.0111846923828
Loss :  1.6879898309707642 2.5274813175201416 128.0620574951172
Loss :  1.6768183708190918 2.624474048614502 132.90052795410156
Loss :  1.6952440738677979 4.093894958496094 206.38999938964844
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.70220947265625 4.118321418762207 207.6182861328125
Loss :  1.6996561288833618 4.058485507965088 204.62393188476562
Loss :  1.7066868543624878 3.9413540363311768 198.77438354492188
Total LOSS train 136.0923596895658 valid 204.3516502380371
CE LOSS train 1.7031876527346097 valid 0.42667171359062195
Contrastive LOSS train 2.6877834430107703 valid 0.9853385090827942
EPOCH 258:
Loss :  1.713369607925415 2.434319019317627 123.4293212890625
Loss :  1.7203248739242554 2.3182015419006348 117.63040161132812
Loss :  1.7035733461380005 2.2978594303131104 116.59654998779297
Loss :  1.7076836824417114 2.4347684383392334 123.44610595703125
Loss :  1.7164987325668335 2.8791451454162598 145.6737518310547
Loss :  1.697177767753601 3.6030609607696533 181.8502197265625
Loss :  1.7165124416351318 3.0368101596832275 153.55702209472656
Loss :  1.7035757303237915 2.671769618988037 135.2920684814453
Loss :  1.6980570554733276 2.3543317317962646 119.41464233398438
Loss :  1.7172163724899292 2.261993885040283 114.81690979003906
Loss :  1.693302035331726 2.391010046005249 121.2437973022461
Loss :  1.6925694942474365 2.370948553085327 120.239990234375
Loss :  1.692088007926941 2.5541982650756836 129.40200805664062
Loss :  1.6961889266967773 2.8626580238342285 144.82908630371094
Loss :  1.7250703573226929 2.2302534580230713 113.23773956298828
Loss :  1.722561001777649 2.1752941608428955 110.48727416992188
Loss :  1.6902493238449097 1.9302127361297607 98.20088195800781
Loss :  1.7051005363464355 2.018900156021118 102.65010833740234
Loss :  1.6883468627929688 2.1242270469665527 107.89969635009766
Loss :  1.7221640348434448 2.3940203189849854 121.42317962646484
  batch 20 loss: 1.7221640348434448, 2.3940203189849854, 121.42317962646484
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7025388479232788 3.025351047515869 152.9700927734375
Loss :  1.6875547170639038 2.5331549644470215 128.34530639648438
Loss :  1.6972053050994873 3.031338691711426 153.26414489746094
Loss :  1.7064865827560425 3.144951820373535 158.95407104492188
Loss :  1.7219330072402954 2.5841875076293945 130.9313201904297
Loss :  1.6992623805999756 2.4720299243927 125.3007583618164
Loss :  1.7043044567108154 2.8478219509124756 144.09539794921875
Loss :  1.701716661453247 2.266634702682495 115.033447265625
Loss :  1.673533320426941 2.4303030967712402 123.18868255615234
Loss :  1.7199311256408691 2.5368258953094482 128.56121826171875
Loss :  1.673983097076416 2.2596051692962646 114.65423583984375
Loss :  1.711281180381775 2.6837382316589355 135.898193359375
Loss :  1.698453426361084 2.2369871139526367 113.54781341552734
Loss :  1.698108434677124 2.3254637718200684 117.9712905883789
Loss :  1.678400993347168 2.646289587020874 133.99288940429688
Loss :  1.6869207620620728 2.1825153827667236 110.81269073486328
Loss :  1.6869087219238281 2.441406011581421 123.7572021484375
Loss :  1.7193323373794556 2.408259391784668 122.1323013305664
Loss :  1.7221934795379639 2.200697660446167 111.75707244873047
Loss :  1.7271504402160645 2.1757986545562744 110.51708221435547
  batch 40 loss: 1.7271504402160645, 2.1757986545562744, 110.51708221435547
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7054054737091064 2.3023264408111572 116.82173156738281
Loss :  1.6974505186080933 2.1702792644500732 110.21141052246094
Loss :  1.6915552616119385 2.2745559215545654 115.41935729980469
Loss :  1.6983423233032227 1.9943253993988037 101.41461181640625
Loss :  1.6873490810394287 2.15966796875 109.67074584960938
Loss :  1.703896403312683 2.386169910430908 121.01239013671875
Loss :  1.7215673923492432 2.75364089012146 139.4036102294922
Loss :  1.6954925060272217 3.533127784729004 178.3518829345703
Loss :  1.7299866676330566 2.3514492511749268 119.30245208740234
Loss :  1.6987459659576416 2.2345056533813477 113.42402648925781
Loss :  1.7165298461914062 2.362266778945923 119.82987213134766
Loss :  1.7142025232315063 2.5741755962371826 130.42298889160156
Loss :  1.7025748491287231 2.43354868888855 123.38001251220703
Loss :  1.7182828187942505 2.6433522701263428 133.88589477539062
Loss :  1.6961941719055176 2.459555149078369 124.67395782470703
Loss :  1.7298996448516846 2.3306665420532227 118.26322174072266
Loss :  1.6999974250793457 2.4243576526641846 122.91787719726562
Loss :  1.691771388053894 2.254108190536499 114.39717864990234
Loss :  1.699609398841858 2.5139381885528564 127.39651489257812
Loss :  1.7338762283325195 2.2826004028320312 115.86389923095703
  batch 60 loss: 1.7338762283325195, 2.2826004028320312, 115.86389923095703
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6917786598205566 2.2601683139801025 114.7001953125
Loss :  1.707181453704834 3.206071376800537 162.01075744628906
Loss :  1.696281909942627 2.1325578689575195 108.32417297363281
Loss :  1.6900912523269653 2.3474340438842773 119.06179809570312
Loss :  1.6790533065795898 2.6792807579040527 135.64308166503906
Loss :  1.6958247423171997 4.349541187286377 219.1728973388672
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.7026664018630981 4.44913911819458 224.1596221923828
Loss :  1.699934482574463 4.310012340545654 217.20054626464844
Loss :  1.7078169584274292 4.205242156982422 211.9699249267578
Total LOSS train 125.42784013014574 valid 218.12574768066406
CE LOSS train 1.7036299375387338 valid 0.4269542396068573
Contrastive LOSS train 2.4744842089139496 valid 1.0513105392456055
EPOCH 259:
Loss :  1.7148487567901611 2.757054090499878 139.5675506591797
Loss :  1.7213819026947021 2.790464162826538 141.2445831298828
Loss :  1.705095887184143 2.308239221572876 117.11705780029297
Loss :  1.7091885805130005 2.3136544227600098 117.39191436767578
Loss :  1.717535376548767 2.533015489578247 128.36830139160156
Loss :  1.6988105773925781 2.6096067428588867 132.17913818359375
Loss :  1.717668056488037 2.744577646255493 138.94654846191406
Loss :  1.7048944234848022 2.374269723892212 120.41838073730469
Loss :  1.699951410293579 2.5066616535186768 127.03303527832031
Loss :  1.7186962366104126 2.265876054763794 115.01249694824219
Loss :  1.6951253414154053 2.6450302600860596 133.94664001464844
Loss :  1.6941943168640137 2.777092695236206 140.548828125
Loss :  1.6937886476516724 2.501671314239502 126.77735137939453
Loss :  1.6976953744888306 2.9080822467803955 147.101806640625
Loss :  1.7266333103179932 2.5631842613220215 129.88584899902344
Loss :  1.7238352298736572 2.675567865371704 135.50222778320312
Loss :  1.692011833190918 2.549149751663208 129.14950561523438
Loss :  1.706891655921936 2.4490857124328613 124.16117858886719
Loss :  1.6906483173370361 2.123439073562622 107.86260223388672
Loss :  1.7231618165969849 2.461488962173462 124.797607421875
  batch 20 loss: 1.7231618165969849, 2.461488962173462, 124.797607421875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7046643495559692 2.761909246444702 139.8001251220703
Loss :  1.6902124881744385 2.562659740447998 129.8231964111328
Loss :  1.699573278427124 2.289001941680908 116.14966583251953
Loss :  1.7086585760116577 2.3248608112335205 117.95169830322266
Loss :  1.7237437963485718 2.3231418132781982 117.8808364868164
Loss :  1.7012615203857422 2.801717519760132 141.78713989257812
Loss :  1.7059828042984009 3.798701286315918 191.6410369873047
Loss :  1.703685998916626 2.4683687686920166 125.12213134765625
Loss :  1.6766326427459717 2.2191388607025146 112.63357543945312
Loss :  1.7215914726257324 2.66868257522583 135.1557159423828
Loss :  1.6762595176696777 2.8147735595703125 142.41493225097656
Loss :  1.7130459547042847 2.7672054767608643 140.0733184814453
Loss :  1.70046067237854 2.5476231575012207 129.0816192626953
Loss :  1.699925184249878 2.6617801189422607 134.78892517089844
Loss :  1.6805622577667236 2.738588809967041 138.61000061035156
Loss :  1.6890239715576172 2.703089475631714 136.843505859375
Loss :  1.6890733242034912 2.7082765102386475 137.10289001464844
Loss :  1.7211600542068481 2.6962883472442627 136.53558349609375
Loss :  1.7237409353256226 2.467460870742798 125.09678649902344
Loss :  1.7286838293075562 2.71990966796875 137.7241668701172
  batch 40 loss: 1.7286838293075562, 2.71990966796875, 137.7241668701172
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7079752683639526 3.147028684616089 159.05941772460938
Loss :  1.6998015642166138 3.5944905281066895 181.42433166503906
Loss :  1.6947767734527588 2.924544334411621 147.9219970703125
Loss :  1.7013764381408691 2.6255106925964355 132.97689819335938
Loss :  1.6908776760101318 2.624403715133667 132.9110565185547
Loss :  1.7063891887664795 2.811239719390869 142.26837158203125
Loss :  1.7237615585327148 2.205181837081909 111.98285675048828
Loss :  1.6981576681137085 2.5542218685150146 129.40924072265625
Loss :  1.7311623096466064 2.5749239921569824 130.47735595703125
Loss :  1.7007949352264404 1.9817867279052734 100.7901382446289
Loss :  1.7179073095321655 2.582793951034546 130.85760498046875
Loss :  1.7155637741088867 2.4984099864959717 126.63606262207031
Loss :  1.7038652896881104 2.3144495487213135 117.42634582519531
Loss :  1.719270944595337 2.479004144668579 125.66947937011719
Loss :  1.6972013711929321 2.41624116897583 122.50926208496094
Loss :  1.73056161403656 3.481574535369873 175.8092803955078
Loss :  1.7008774280548096 3.805388927459717 191.97032165527344
Loss :  1.6921899318695068 3.994442939758301 201.41433715820312
Loss :  1.70024836063385 3.7553415298461914 189.46731567382812
Loss :  1.7345898151397705 2.869734525680542 145.2213134765625
  batch 60 loss: 1.7345898151397705, 2.869734525680542, 145.2213134765625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6925357580184937 2.883359670639038 145.8605194091797
Loss :  1.7077305316925049 2.3876044750213623 121.08795928955078
Loss :  1.6969656944274902 2.2612314224243164 114.75853729248047
Loss :  1.6907027959823608 2.6257989406585693 132.98065185546875
Loss :  1.679734468460083 1.9891531467437744 101.13739013671875
Loss :  1.6923404932022095 4.372193813323975 220.30203247070312
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6992225646972656 4.345378398895264 218.9681396484375
Loss :  1.696780800819397 4.2283196449279785 213.11276245117188
Loss :  1.7034826278686523 4.163882255554199 209.8975830078125
Total LOSS train 135.00396153376653 valid 215.57012939453125
CE LOSS train 1.7053079715141883 valid 0.4258706569671631
Contrastive LOSS train 2.665973091125488 valid 1.0409705638885498
EPOCH 260:
Loss :  1.7153764963150024 2.9672000408172607 150.07537841796875
Loss :  1.7220247983932495 2.8203508853912354 142.7395782470703
Loss :  1.7056304216384888 2.517540216445923 127.5826416015625
Loss :  1.7094855308532715 2.5901787281036377 131.2184295654297
Loss :  1.7177798748016357 2.395413637161255 121.48846435546875
Loss :  1.6990798711776733 2.4687702655792236 125.1375961303711
Loss :  1.7179234027862549 2.649859666824341 134.21090698242188
Loss :  1.704967975616455 2.787215232849121 141.06573486328125
Loss :  1.7000821828842163 2.9503281116485596 149.21649169921875
Loss :  1.7184137105941772 2.8143608570098877 142.43646240234375
Loss :  1.6946066617965698 3.6486730575561523 184.12826538085938
Loss :  1.6943849325180054 2.556593656539917 129.52406311035156
Loss :  1.693455696105957 2.789301872253418 141.15853881835938
Loss :  1.6973466873168945 2.461015224456787 124.74810791015625
Loss :  1.7258614301681519 2.8179497718811035 142.62335205078125
Loss :  1.723742961883545 2.6023688316345215 131.84217834472656
Loss :  1.6920684576034546 2.6474862098693848 134.06637573242188
Loss :  1.7064576148986816 2.9094362258911133 147.1782684326172
Loss :  1.6905121803283691 2.2964861392974854 116.51482391357422
Loss :  1.7231721878051758 2.3908185958862305 121.26410675048828
  batch 20 loss: 1.7231721878051758, 2.3908185958862305, 121.26410675048828
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7042973041534424 3.3327696323394775 168.34278869628906
Loss :  1.6898335218429565 3.227501392364502 163.06491088867188
Loss :  1.6994285583496094 3.2339415550231934 163.39651489257812
Loss :  1.708370566368103 2.695645809173584 136.49066162109375
Loss :  1.72344970703125 2.6172986030578613 132.58837890625
Loss :  1.701206088066101 2.3998825550079346 121.69532775878906
Loss :  1.7062305212020874 2.438128709793091 123.61266326904297
Loss :  1.7039333581924438 2.771841526031494 140.29600524902344
Loss :  1.6763070821762085 3.359375 169.64505004882812
Loss :  1.7213281393051147 2.4916059970855713 126.30162811279297
Loss :  1.6760214567184448 2.2140843868255615 112.38023376464844
Loss :  1.7126113176345825 2.147223472595215 109.07378387451172
Loss :  1.6999963521957397 2.7473394870758057 139.0669708251953
Loss :  1.6994174718856812 2.9142887592315674 147.41384887695312
Loss :  1.6799213886260986 2.498777151107788 126.61878204345703
Loss :  1.6882585287094116 2.4301257133483887 123.19454193115234
Loss :  1.6882469654083252 3.9020936489105225 196.7929229736328
Loss :  1.7203375101089478 2.5226144790649414 127.85105895996094
Loss :  1.7232639789581299 2.6359364986419678 133.5200958251953
Loss :  1.728065848350525 2.9785044193267822 150.65328979492188
  batch 40 loss: 1.728065848350525, 2.9785044193267822, 150.65328979492188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7069631814956665 3.187868595123291 161.10040283203125
Loss :  1.6995124816894531 2.6385233402252197 133.62567138671875
Loss :  1.6935749053955078 2.651674509048462 134.2772979736328
Loss :  1.7004402875900269 2.696119785308838 136.5064239501953
Loss :  1.6897577047348022 1.932266116142273 98.30306243896484
Loss :  1.705687165260315 3.070343017578125 155.22283935546875
Loss :  1.7234222888946533 3.069547414779663 155.20079040527344
Loss :  1.6977444887161255 2.965006113052368 149.9480438232422
Loss :  1.7311029434204102 3.0357754230499268 153.51988220214844
Loss :  1.7009599208831787 2.194802761077881 111.44109344482422
Loss :  1.7181543111801147 2.6589932441711426 134.66781616210938
Loss :  1.716036081314087 3.7988712787628174 191.6595916748047
Loss :  1.70479154586792 2.2555809020996094 114.48383331298828
Loss :  1.7199805974960327 2.5100340843200684 127.2216796875
Loss :  1.6987409591674805 2.424887180328369 122.94309997558594
Loss :  1.7315504550933838 2.534968376159668 128.47996520996094
Loss :  1.7022494077682495 2.52423095703125 127.9137954711914
Loss :  1.6938879489898682 2.0952811241149902 106.45793914794922
Loss :  1.7016249895095825 2.5954298973083496 131.47312927246094
Loss :  1.735285997390747 2.1058173179626465 107.0261459350586
  batch 60 loss: 1.735285997390747, 2.1058173179626465, 107.0261459350586
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6939842700958252 2.8096611499786377 142.1770477294922
Loss :  1.708872675895691 3.1947059631347656 161.4441680908203
Loss :  1.6982842683792114 2.2780611515045166 115.60134887695312
Loss :  1.6922235488891602 3.394965171813965 171.44049072265625
Loss :  1.6812130212783813 2.207331418991089 112.04778289794922
Loss :  1.7053221464157104 3.7810964584350586 190.76014709472656
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.712310791015625 3.918621063232422 197.64337158203125
Loss :  1.7101476192474365 3.8293466567993164 193.17747497558594
Loss :  1.7144668102264404 3.749441385269165 189.18653869628906
Total LOSS train 137.45234715388372 valid 192.6918830871582
CE LOSS train 1.7053683721102202 valid 0.4286167025566101
Contrastive LOSS train 2.714939574094919 valid 0.9373603463172913
EPOCH 261:
Loss :  1.7164539098739624 2.818375825881958 142.63525390625
Loss :  1.7229257822036743 2.557828664779663 129.61436462402344
Loss :  1.7068519592285156 2.1716561317443848 110.28965759277344
Loss :  1.7107949256896973 2.4260990619659424 123.0157470703125
Loss :  1.7190992832183838 2.2074849605560303 112.09334564208984
Loss :  1.700826644897461 3.2301623821258545 163.2089385986328
Loss :  1.7191874980926514 4.025657653808594 203.0020751953125
Loss :  1.7066874504089355 2.471266031265259 125.26998901367188
Loss :  1.7020459175109863 2.5639660358428955 129.9003448486328
Loss :  1.7201502323150635 3.1111443042755127 157.27737426757812
Loss :  1.6971328258514404 2.853929281234741 144.39358520507812
Loss :  1.6961703300476074 2.806781768798828 142.03524780273438
Loss :  1.6953996419906616 2.7724668979644775 140.31875610351562
Loss :  1.6991267204284668 2.3605306148529053 119.72565460205078
Loss :  1.727600336074829 3.1164305210113525 157.54913330078125
Loss :  1.7250391244888306 2.428635597229004 123.15681457519531
Loss :  1.694248914718628 2.1075925827026367 107.07388305664062
Loss :  1.7083595991134644 1.9666783809661865 100.04227447509766
Loss :  1.6927233934402466 2.4347002506256104 123.42774200439453
Loss :  1.7249319553375244 2.132396697998047 108.34476470947266
  batch 20 loss: 1.7249319553375244, 2.132396697998047, 108.34476470947266
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.705973505973816 2.064310312271118 104.9214859008789
Loss :  1.6917146444320679 3.1973986625671387 161.5616455078125
Loss :  1.7009494304656982 2.7541005611419678 139.40597534179688
Loss :  1.7092434167861938 2.708359479904175 137.12721252441406
Loss :  1.724495768547058 2.399756908416748 121.71234130859375
Loss :  1.7024677991867065 2.362339973449707 119.81946563720703
Loss :  1.7073826789855957 2.604757070541382 131.9452362060547
Loss :  1.7048546075820923 3.128112316131592 158.11045837402344
Loss :  1.6771763563156128 2.968071699142456 150.08074951171875
Loss :  1.722128987312317 2.645641565322876 134.0041961669922
Loss :  1.6772401332855225 2.7147138118743896 137.41293334960938
Loss :  1.7135404348373413 2.5501604080200195 129.2215576171875
Loss :  1.7010241746902466 2.5330862998962402 128.35533142089844
Loss :  1.7007067203521729 2.491175889968872 126.2594985961914
Loss :  1.6818573474884033 2.6825170516967773 135.80770874023438
Loss :  1.6899964809417725 3.0403051376342773 153.70526123046875
Loss :  1.6899091005325317 2.0734994411468506 105.36488342285156
Loss :  1.7214587926864624 2.923656702041626 147.904296875
Loss :  1.7243279218673706 2.330307960510254 118.2397232055664
Loss :  1.729124665260315 2.290947437286377 116.27649688720703
  batch 40 loss: 1.729124665260315, 2.290947437286377, 116.27649688720703
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7084708213806152 2.121722936630249 107.79461669921875
Loss :  1.700843334197998 1.7380157709121704 88.60163116455078
Loss :  1.6952621936798096 2.5041353702545166 126.90203094482422
Loss :  1.7019141912460327 2.545527458190918 128.97828674316406
Loss :  1.6910642385482788 1.8386904001235962 93.62557983398438
Loss :  1.7068567276000977 2.0866904258728027 106.04137420654297
Loss :  1.7241325378417969 2.3766465187072754 120.55645751953125
Loss :  1.698553204536438 2.226646900177002 113.03089904785156
Loss :  1.7318527698516846 2.5115625858306885 127.30998229980469
Loss :  1.7020697593688965 2.128298759460449 108.11701202392578
Loss :  1.7189081907272339 2.505946636199951 127.01624298095703
Loss :  1.7167001962661743 2.4914865493774414 126.29102325439453
Loss :  1.705619215965271 2.1660149097442627 110.00636291503906
Loss :  1.7207902669906616 2.363076686859131 119.87461853027344
Loss :  1.6993049383163452 2.4100828170776367 122.20344543457031
Loss :  1.732427954673767 2.831799030303955 143.32237243652344
Loss :  1.7034107446670532 2.878577947616577 145.63230895996094
Loss :  1.6951556205749512 3.4843690395355225 175.91360473632812
Loss :  1.7027547359466553 2.932236909866333 148.31460571289062
Loss :  1.7358691692352295 2.4599807262420654 124.73490905761719
  batch 60 loss: 1.7358691692352295, 2.4599807262420654, 124.73490905761719
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6949894428253174 2.6609690189361572 134.74343872070312
Loss :  1.7098231315612793 2.411902666091919 122.3049545288086
Loss :  1.6989046335220337 2.4544870853424072 124.42326354980469
Loss :  1.693070888519287 2.657303810119629 134.55825805664062
Loss :  1.6823058128356934 2.9275424480438232 148.05943298339844
Loss :  1.6960633993148804 4.297149658203125 216.5535430908203
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.702701449394226 4.286679267883301 216.0366668701172
Loss :  1.7004629373550415 4.193361759185791 211.36856079101562
Loss :  1.7064566612243652 4.182312488555908 210.82208251953125
Total LOSS train 129.96874025785007 valid 213.6952133178711
CE LOSS train 1.7066520635898297 valid 0.4266141653060913
Contrastive LOSS train 2.565241780647865 valid 1.045578122138977
EPOCH 262:
Loss :  1.7169113159179688 3.133312463760376 158.38253784179688
Loss :  1.723415493965149 2.628223419189453 133.13458251953125
Loss :  1.707651138305664 2.31284236907959 117.34976959228516
Loss :  1.7114754915237427 2.466845989227295 125.05377960205078
Loss :  1.7196346521377563 2.242222547531128 113.83076477050781
Loss :  1.7010759115219116 2.4239518642425537 122.89866638183594
Loss :  1.7195980548858643 3.0041913986206055 151.92916870117188
Loss :  1.706639289855957 2.747012138366699 139.05723571777344
Loss :  1.7018511295318604 3.1321234703063965 158.3080291748047
Loss :  1.719620943069458 2.066845417022705 105.0618896484375
Loss :  1.6967532634735107 2.6969399452209473 136.5437469482422
Loss :  1.6961272954940796 2.756129503250122 139.50259399414062
Loss :  1.6951721906661987 2.147587776184082 109.0745620727539
Loss :  1.6989836692810059 2.5891449451446533 131.15623474121094
Loss :  1.7269831895828247 2.50958251953125 127.20610809326172
Loss :  1.724712610244751 2.0269103050231934 103.07022857666016
Loss :  1.6938905715942383 2.6854445934295654 135.96612548828125
Loss :  1.7078884840011597 2.549699306488037 129.19285583496094
Loss :  1.6922783851623535 2.5076868534088135 127.07662200927734
Loss :  1.7245453596115112 2.515718936920166 127.510498046875
  batch 20 loss: 1.7245453596115112, 2.515718936920166, 127.510498046875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.705643892288208 3.080227851867676 155.717041015625
Loss :  1.6913928985595703 2.1911001205444336 111.24639892578125
Loss :  1.7006181478500366 2.6497232913970947 134.18678283691406
Loss :  1.7096445560455322 2.8556113243103027 144.49020385742188
Loss :  1.7244807481765747 2.3730382919311523 120.37639617919922
Loss :  1.702692985534668 3.2009835243225098 161.7518768310547
Loss :  1.7076002359390259 3.4042139053344727 171.9182891845703
Loss :  1.7050373554229736 3.2810299396514893 165.75653076171875
Loss :  1.678137183189392 2.505401372909546 126.94820404052734
Loss :  1.7229387760162354 2.5833799839019775 130.89195251464844
Loss :  1.6783208847045898 2.663154363632202 134.83602905273438
Loss :  1.7141982316970825 2.999767780303955 151.7025909423828
Loss :  1.701462984085083 3.8675363063812256 195.07827758789062
Loss :  1.701033353805542 3.074824333190918 155.44224548339844
Loss :  1.6818865537643433 2.672452449798584 135.30450439453125
Loss :  1.689936637878418 3.1702985763549805 160.20486450195312
Loss :  1.6897461414337158 2.1399245262145996 108.68596649169922
Loss :  1.7213152647018433 2.3164544105529785 117.54403686523438
Loss :  1.7242820262908936 2.1999430656433105 111.721435546875
Loss :  1.7290042638778687 2.8778114318847656 145.61956787109375
  batch 40 loss: 1.7290042638778687, 2.8778114318847656, 145.61956787109375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7082655429840088 3.2019898891448975 161.80775451660156
Loss :  1.7006603479385376 3.214235544204712 162.4124298095703
Loss :  1.6952308416366577 2.0933268070220947 106.361572265625
Loss :  1.701694369316101 3.032160758972168 153.30972290039062
Loss :  1.6909765005111694 2.910090923309326 147.19552612304688
Loss :  1.7067126035690308 2.4552762508392334 124.47052001953125
Loss :  1.7241814136505127 2.1717729568481445 110.31282806396484
Loss :  1.6987154483795166 2.476963758468628 125.54690551757812
Loss :  1.7317914962768555 2.496898651123047 126.57672119140625
Loss :  1.7019693851470947 2.3727917671203613 120.34156036376953
Loss :  1.719254732131958 3.4713401794433594 175.28627014160156
Loss :  1.7167125940322876 4.0531907081604 204.37623596191406
Loss :  1.7052628993988037 3.3448145389556885 168.94598388671875
Loss :  1.7202870845794678 2.3971738815307617 121.5789794921875
Loss :  1.6988941431045532 2.48237681388855 125.81773376464844
Loss :  1.7321854829788208 2.3310582637786865 118.28509521484375
Loss :  1.7027734518051147 2.737983465194702 138.60194396972656
Loss :  1.6940902471542358 2.5080032348632812 127.09425354003906
Loss :  1.7021877765655518 2.6291728019714355 133.16082763671875
Loss :  1.7358624935150146 2.1730964183807373 110.39068603515625
  batch 60 loss: 1.7358624935150146, 2.1730964183807373, 110.39068603515625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6945412158966064 3.2462973594665527 164.0093994140625
Loss :  1.7096166610717773 2.4309422969818115 123.25672912597656
Loss :  1.6991698741912842 2.549849271774292 129.19163513183594
Loss :  1.6932909488677979 2.17022705078125 110.20464324951172
Loss :  1.682616949081421 1.6040852069854736 81.88687896728516
Loss :  1.6981236934661865 4.422605514526367 222.82839965820312
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.7046304941177368 4.474009037017822 225.40509033203125
Loss :  1.7022210359573364 4.306450843811035 217.02476501464844
Loss :  1.7091691493988037 4.210752487182617 212.24679565429688
Total LOSS train 135.40233893761268 valid 219.37626266479492
CE LOSS train 1.7066388625365037 valid 0.4272922873497009
Contrastive LOSS train 2.673914021712083 valid 1.0526881217956543
EPOCH 263:
Loss :  1.7170528173446655 2.063481092453003 104.89111328125
Loss :  1.7235561609268188 2.4740679264068604 125.42695617675781
Loss :  1.7077058553695679 2.2635233402252197 114.88387298583984
Loss :  1.7116271257400513 2.6611344814300537 134.76834106445312
Loss :  1.7199128866195679 2.8970987796783447 146.57484436035156
Loss :  1.701463222503662 2.5732831954956055 130.36561584472656
Loss :  1.7200348377227783 2.424255132675171 122.93278503417969
Loss :  1.7071013450622559 2.1981616020202637 111.61518096923828
Loss :  1.7021702527999878 2.6885173320770264 136.12803649902344
Loss :  1.7202210426330566 2.319279193878174 117.6841812133789
Loss :  1.6973458528518677 2.8031954765319824 141.85711669921875
Loss :  1.6965551376342773 2.795783042907715 141.48570251464844
Loss :  1.6958341598510742 4.345312595367432 218.96145629882812
Loss :  1.6995166540145874 2.661818027496338 134.79042053222656
Loss :  1.7274787425994873 2.517261505126953 127.5905532836914
Loss :  1.725215196609497 2.059645175933838 104.70747375488281
Loss :  1.6946196556091309 2.434049129486084 123.3970718383789
Loss :  1.708658218383789 2.480245351791382 125.7209243774414
Loss :  1.692989468574524 2.664926767349243 134.9393310546875
Loss :  1.7250096797943115 2.602465867996216 131.84829711914062
  batch 20 loss: 1.7250096797943115, 2.602465867996216, 131.84829711914062
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.706375002861023 2.6505398750305176 134.23336791992188
Loss :  1.6923341751098633 2.6937546730041504 136.38006591796875
Loss :  1.7012956142425537 2.3676717281341553 120.08487701416016
Loss :  1.7101799249649048 3.0748534202575684 155.45285034179688
Loss :  1.725243091583252 2.767559766769409 140.10324096679688
Loss :  1.7031649351119995 2.370837450027466 120.24503326416016
Loss :  1.708143711090088 2.495993137359619 126.50780487060547
Loss :  1.70550537109375 2.553729772567749 129.39199829101562
Loss :  1.6783998012542725 2.4064948558807373 122.00314331054688
Loss :  1.7229077816009521 2.7973315715789795 141.5894775390625
Loss :  1.678698182106018 3.457963705062866 174.57687377929688
Loss :  1.7142623662948608 2.6327710151672363 133.35281372070312
Loss :  1.7015659809112549 2.4607977867126465 124.741455078125
Loss :  1.7007701396942139 2.3793962001800537 120.67057800292969
Loss :  1.6813533306121826 2.4406096935272217 123.71183776855469
Loss :  1.689400315284729 2.6971418857574463 136.5465087890625
Loss :  1.6888761520385742 2.539987087249756 128.688232421875
Loss :  1.720409870147705 2.7229349613189697 137.86715698242188
Loss :  1.723323106765747 2.2951579093933105 116.48121643066406
Loss :  1.7281197309494019 2.266733407974243 115.06478881835938
  batch 40 loss: 1.7281197309494019, 2.266733407974243, 115.06478881835938
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7069430351257324 2.461104154586792 124.76214599609375
Loss :  1.6991724967956543 2.7417092323303223 138.78463745117188
Loss :  1.693446397781372 2.8048341274261475 141.93515014648438
Loss :  1.7001166343688965 2.283268690109253 115.86355590820312
Loss :  1.6890987157821655 2.1975009441375732 111.56414794921875
Loss :  1.7051347494125366 2.4117138385772705 122.29082489013672
Loss :  1.7225450277328491 2.6253623962402344 132.9906768798828
Loss :  1.696660041809082 3.711573362350464 187.27532958984375
Loss :  1.7302619218826294 4.2601423263549805 214.73736572265625
Loss :  1.6999307870864868 3.309734582901001 167.18666076660156
Loss :  1.7172349691390991 2.779531955718994 140.69383239746094
Loss :  1.7153373956680298 2.691319227218628 136.2812957763672
Loss :  1.703927993774414 2.3525173664093018 119.32979583740234
Loss :  1.7197209596633911 2.4932172298431396 126.38057708740234
Loss :  1.6973577737808228 2.4762585163116455 125.51028442382812
Loss :  1.731305480003357 2.1145741939544678 107.46001434326172
Loss :  1.70167076587677 2.6690094470977783 135.1521453857422
Loss :  1.6935251951217651 3.0577340126037598 154.58023071289062
Loss :  1.701299786567688 2.843205451965332 143.861572265625
Loss :  1.7344188690185547 3.074563503265381 155.4626007080078
  batch 60 loss: 1.7344188690185547, 3.074563503265381, 155.4626007080078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6937779188156128 3.039553165435791 153.6714324951172
Loss :  1.709290623664856 1.8726978302001953 95.34417724609375
Loss :  1.6985000371932983 2.1592795848846436 109.66248321533203
Loss :  1.6924948692321777 2.5201008319854736 127.69754028320312
Loss :  1.681681752204895 1.791932225227356 91.27828979492188
Loss :  1.7048264741897583 4.265224456787109 214.966064453125
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.7118661403656006 4.301911354064941 216.80743408203125
Loss :  1.7102253437042236 4.128291606903076 208.1248016357422
Loss :  1.7130582332611084 3.8666722774505615 195.0466766357422
Total LOSS train 133.04645174466646 valid 208.73624420166016
CE LOSS train 1.7062961706748376 valid 0.4282645583152771
Contrastive LOSS train 2.6268031248679526 valid 0.9666680693626404
EPOCH 264:
Loss :  1.716844081878662 2.148488759994507 109.14128112792969
Loss :  1.7233388423919678 2.24588680267334 114.0176773071289
Loss :  1.7071119546890259 2.026829719543457 103.04859924316406
Loss :  1.7111005783081055 2.2938451766967773 116.40335845947266
Loss :  1.7191630601882935 2.1623029708862305 109.83431243896484
Loss :  1.7008970975875854 2.2917351722717285 116.28765869140625
Loss :  1.7192013263702393 2.541275978088379 128.7830047607422
Loss :  1.706529974937439 2.450718641281128 124.24246215820312
Loss :  1.7015682458877563 2.6162312030792236 132.51312255859375
Loss :  1.719836711883545 2.158653736114502 109.65251922607422
Loss :  1.6966522932052612 2.8928306102752686 146.3381805419922
Loss :  1.695999026298523 2.5791521072387695 130.6536102294922
Loss :  1.6951274871826172 2.9334042072296143 148.36534118652344
Loss :  1.6990735530853271 2.9384520053863525 148.62167358398438
Loss :  1.7272545099258423 3.7463529109954834 189.04489135742188
Loss :  1.7248306274414062 4.048019886016846 204.12582397460938
Loss :  1.693398118019104 3.8148133754730225 192.43406677246094
Loss :  1.7078373432159424 2.486055612564087 126.0106201171875
Loss :  1.6913578510284424 2.4864916801452637 126.01594543457031
Loss :  1.7244563102722168 2.4688191413879395 125.16541290283203
  batch 20 loss: 1.7244563102722168, 2.4688191413879395, 125.16541290283203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7053308486938477 2.293471097946167 116.3788833618164
Loss :  1.6910964250564575 2.4981913566589355 126.60066223144531
Loss :  1.7005804777145386 2.0763261318206787 105.51688385009766
Loss :  1.7096790075302124 2.410936117172241 122.25648498535156
Loss :  1.7245280742645264 2.7553904056549072 139.49404907226562
Loss :  1.7026872634887695 3.0074918270111084 152.0772705078125
Loss :  1.7074941396713257 3.518815755844116 177.6482696533203
Loss :  1.705108404159546 2.2297163009643555 113.19092559814453
Loss :  1.6778408288955688 2.314081907272339 117.3819351196289
Loss :  1.7223342657089233 2.8538095951080322 144.41281127929688
Loss :  1.6778144836425781 2.8279879093170166 143.07720947265625
Loss :  1.713672161102295 2.246588706970215 114.04310607910156
Loss :  1.7010598182678223 2.169456720352173 110.17389678955078
Loss :  1.700726866722107 2.629291296005249 133.16529846191406
Loss :  1.6815361976623535 3.254530668258667 164.40806579589844
Loss :  1.689765453338623 2.3812689781188965 120.75321197509766
Loss :  1.6895273923873901 2.536634683609009 128.52125549316406
Loss :  1.7211532592773438 2.4531030654907227 124.37630462646484
Loss :  1.7242568731307983 2.5379996299743652 128.62423706054688
Loss :  1.7289459705352783 2.2626953125 114.86370849609375
  batch 40 loss: 1.7289459705352783, 2.2626953125, 114.86370849609375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7079055309295654 2.638679265975952 133.64186096191406
Loss :  1.7003734111785889 2.2377121448516846 113.58597564697266
Loss :  1.6946802139282227 3.2410898208618164 163.74917602539062
Loss :  1.701278805732727 2.5445480346679688 128.92868041992188
Loss :  1.6908048391342163 2.5422537326812744 128.80349731445312
Loss :  1.7061939239501953 2.545973300933838 129.00486755371094
Loss :  1.7234869003295898 2.238236427307129 113.63530731201172
Loss :  1.6983654499053955 2.624232530593872 132.9099884033203
Loss :  1.730911374092102 2.422797441482544 122.87078094482422
Loss :  1.701249361038208 2.1310791969299316 108.25521087646484
Loss :  1.7184661626815796 2.4262824058532715 123.03258514404297
Loss :  1.7161415815353394 2.1066195964813232 107.047119140625
Loss :  1.7049148082733154 2.0100343227386475 102.20663452148438
Loss :  1.7202279567718506 2.568453550338745 130.14291381835938
Loss :  1.6990907192230225 2.0936121940612793 106.37969970703125
Loss :  1.7318141460418701 2.204240560531616 111.94384002685547
Loss :  1.7027744054794312 2.3301005363464355 118.20780181884766
Loss :  1.6945619583129883 1.999940037727356 101.69156646728516
Loss :  1.7022523880004883 3.0238640308380127 152.8954620361328
Loss :  1.7356582880020142 2.0636813640594482 104.91972351074219
  batch 60 loss: 1.7356582880020142, 2.0636813640594482, 104.91972351074219
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.694883942604065 2.1051089763641357 106.95033264160156
Loss :  1.7098116874694824 2.060297727584839 104.72469329833984
Loss :  1.6993021965026855 2.2691683769226074 115.15772247314453
Loss :  1.6936637163162231 2.610178232192993 132.20257568359375
Loss :  1.6829875707626343 2.1247873306274414 107.92235565185547
Loss :  1.7113555669784546 4.349609375 219.1918182373047
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.718400478363037 4.259217739105225 214.67929077148438
Loss :  1.716668963432312 4.301248550415039 216.7790985107422
Loss :  1.7189639806747437 4.036852836608887 203.5615997314453
Total LOSS train 127.66883697509766 valid 213.55295181274414
CE LOSS train 1.7064387468191293 valid 0.4297409951686859
Contrastive LOSS train 2.519247973882235 valid 1.0092132091522217
EPOCH 265:
Loss :  1.7175087928771973 2.815399169921875 142.4874725341797
Loss :  1.7240194082260132 2.3577466011047363 119.6113510131836
Loss :  1.7082688808441162 2.215064525604248 112.46149444580078
Loss :  1.7120527029037476 2.3441452980041504 118.91931915283203
Loss :  1.7202754020690918 2.4625871181488037 124.8496322631836
Loss :  1.7020314931869507 1.9903873205184937 101.22139739990234
Loss :  1.7202482223510742 2.395752191543579 121.50785827636719
Loss :  1.7076728343963623 2.4488439559936523 124.14987182617188
Loss :  1.7028367519378662 2.4970548152923584 126.55557250976562
Loss :  1.7206354141235352 2.122074604034424 107.8243637084961
Loss :  1.6980689764022827 2.6844754219055176 135.92184448242188
Loss :  1.6974663734436035 2.7655856609344482 139.97674560546875
Loss :  1.6966694593429565 2.238898754119873 113.64160919189453
Loss :  1.700370192527771 3.0617129802703857 154.7860107421875
Loss :  1.7278354167938232 2.680858850479126 135.77076721191406
Loss :  1.725783109664917 2.3820455074310303 120.82805633544922
Loss :  1.69487726688385 2.146813154220581 109.03553771972656
Loss :  1.70901620388031 2.077967405319214 105.60738372802734
Loss :  1.6935248374938965 1.9728978872299194 100.33842468261719
Loss :  1.725496768951416 1.9040831327438354 96.92964935302734
  batch 20 loss: 1.725496768951416, 1.9040831327438354, 96.92964935302734
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7070249319076538 1.8520796298980713 94.31100463867188
Loss :  1.693123459815979 2.1724367141723633 110.31495666503906
Loss :  1.702211856842041 2.3870835304260254 121.05638885498047
Loss :  1.7111297845840454 2.2132513523101807 112.37369537353516
Loss :  1.725746989250183 3.600301504135132 181.74082946777344
Loss :  1.7040956020355225 2.365291118621826 119.9686508178711
Loss :  1.7089186906814575 2.394331455230713 121.42549133300781
Loss :  1.7064034938812256 2.6813788414001465 135.7753448486328
Loss :  1.6799148321151733 2.535158395767212 128.43783569335938
Loss :  1.72366464138031 2.630112648010254 133.2292938232422
Loss :  1.680198311805725 3.232827663421631 163.3215789794922
Loss :  1.7152633666992188 2.4072797298431396 122.0792465209961
Loss :  1.7025766372680664 2.357978105545044 119.60147857666016
Loss :  1.7018243074417114 2.624791145324707 132.94139099121094
Loss :  1.6824778318405151 2.5206122398376465 127.71308898925781
Loss :  1.6907212734222412 2.4867310523986816 126.02727508544922
Loss :  1.6904888153076172 2.8970532417297363 146.54315185546875
Loss :  1.7215489149093628 3.0442001819610596 153.93154907226562
Loss :  1.7249162197113037 3.0057404041290283 152.01193237304688
Loss :  1.7296233177185059 2.7552950382232666 139.494384765625
  batch 40 loss: 1.7296233177185059, 2.7552950382232666, 139.494384765625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7089860439300537 3.8523545265197754 194.3267059326172
Loss :  1.7016347646713257 2.43342661857605 123.37297058105469
Loss :  1.6961917877197266 2.6932730674743652 136.35984802246094
Loss :  1.702479600906372 2.765897035598755 139.99732971191406
Loss :  1.6921308040618896 2.5810816287994385 130.74620056152344
Loss :  1.7076771259307861 2.375347375869751 120.47505187988281
Loss :  1.7246556282043457 1.953527569770813 99.40103149414062
Loss :  1.6997699737548828 1.9967684745788574 101.53819274902344
Loss :  1.7322423458099365 2.2249135971069336 112.97792053222656
Loss :  1.702811360359192 2.7289936542510986 138.15248107910156
Loss :  1.7197908163070679 2.64377498626709 133.90853881835938
Loss :  1.7173607349395752 3.1311557292938232 158.275146484375
Loss :  1.7062444686889648 2.2217965126037598 112.79607391357422
Loss :  1.721008539199829 2.501061201095581 126.7740707397461
Loss :  1.6998215913772583 2.408120632171631 122.10585021972656
Loss :  1.7325422763824463 3.0650718212127686 154.9861297607422
Loss :  1.7033793926239014 2.31941294670105 117.67402648925781
Loss :  1.6950715780258179 2.156193971633911 109.50476837158203
Loss :  1.7026687860488892 3.3943686485290527 171.4210968017578
Loss :  1.735622763633728 2.261528491973877 114.81204986572266
  batch 60 loss: 1.735622763633728, 2.261528491973877, 114.81204986572266
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6949164867401123 2.787463426589966 141.06808471679688
Loss :  1.7101290225982666 2.5161242485046387 127.51634216308594
Loss :  1.6991469860076904 2.328313112258911 118.11480712890625
Loss :  1.6931328773498535 2.7064788341522217 137.01707458496094
Loss :  1.6824344396591187 1.9862333536148071 100.99410247802734
Loss :  1.699415683746338 4.4195451736450195 222.6766815185547
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.7059687376022339 4.424656867980957 222.93881225585938
Loss :  1.7034201622009277 4.327141284942627 218.06048583984375
Loss :  1.7102558612823486 4.091614246368408 206.2909698486328
Total LOSS train 127.67752039982723 valid 217.49173736572266
CE LOSS train 1.7075135689515335 valid 0.42756396532058716
Contrastive LOSS train 2.5194001509593082 valid 1.022903561592102
EPOCH 266:
Loss :  1.7170653343200684 2.4963831901550293 126.53622436523438
Loss :  1.7235153913497925 3.7429940700531006 188.8732147216797
Loss :  1.7077569961547852 2.4408185482025146 123.7486801147461
Loss :  1.7116482257843018 2.4952378273010254 126.47354125976562
Loss :  1.719780445098877 2.2788689136505127 115.66322326660156
Loss :  1.7016745805740356 2.394662380218506 121.4347915649414
Loss :  1.7200754880905151 2.3684515953063965 120.14265441894531
Loss :  1.7075879573822021 2.155400037765503 109.47759246826172
Loss :  1.7028844356536865 2.509260416030884 127.16590118408203
Loss :  1.720964789390564 2.3584060668945312 119.64126586914062
Loss :  1.6980807781219482 2.4797563552856445 125.68589782714844
Loss :  1.6972116231918335 2.319385290145874 117.66647338867188
Loss :  1.6963436603546143 2.2745213508605957 115.42240905761719
Loss :  1.7000106573104858 2.325376272201538 117.96882629394531
Loss :  1.7277120351791382 2.885078191757202 145.9816131591797
Loss :  1.7254234552383423 2.6321873664855957 133.3347930908203
Loss :  1.6945245265960693 3.205458164215088 161.96742248535156
Loss :  1.7087146043777466 2.3011486530303955 116.76615142822266
Loss :  1.6930452585220337 2.235028028488159 113.44445037841797
Loss :  1.7249058485031128 2.2960150241851807 116.5256576538086
  batch 20 loss: 1.7249058485031128, 2.2960150241851807, 116.5256576538086
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7065457105636597 2.118069887161255 107.61003875732422
Loss :  1.6926075220108032 2.274059534072876 115.39558410644531
Loss :  1.701664924621582 2.2240803241729736 112.90568542480469
Loss :  1.7106207609176636 2.1952812671661377 111.47467803955078
Loss :  1.72519850730896 2.5597615242004395 129.71327209472656
Loss :  1.7035307884216309 3.673715353012085 185.38929748535156
Loss :  1.7082821130752563 2.8451898097991943 143.9677734375
Loss :  1.7059013843536377 2.8478119373321533 144.09649658203125
Loss :  1.678862452507019 2.828134298324585 143.0855712890625
Loss :  1.723162055015564 2.9139487743377686 147.42059326171875
Loss :  1.6790739297866821 2.768932819366455 140.12571716308594
Loss :  1.7146520614624023 2.3748953342437744 120.45941925048828
Loss :  1.7023130655288696 2.546755790710449 129.04010009765625
Loss :  1.7019577026367188 2.306234836578369 117.01370239257812
Loss :  1.6832090616226196 2.804588556289673 141.9126434326172
Loss :  1.6913342475891113 2.5460636615753174 128.99452209472656
Loss :  1.6912673711776733 2.4600722789764404 124.69488525390625
Loss :  1.7225685119628906 2.0676093101501465 105.10302734375
Loss :  1.725386142730713 2.3870761394500732 121.07919311523438
Loss :  1.7298344373703003 3.6295015811920166 183.2049102783203
  batch 40 loss: 1.7298344373703003, 3.6295015811920166, 183.2049102783203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.70942223072052 2.690711736679077 136.24501037597656
Loss :  1.702495813369751 2.325380563735962 117.97152709960938
Loss :  1.6966947317123413 2.362607479095459 119.82706451416016
Loss :  1.7036432027816772 2.304521322250366 116.9297103881836
Loss :  1.693354606628418 2.129549980163574 108.17085266113281
Loss :  1.7086995840072632 2.365811347961426 119.999267578125
Loss :  1.7257318496704102 2.4392495155334473 123.6882095336914
Loss :  1.7014771699905396 2.45257830619812 124.33039093017578
Loss :  1.733246088027954 2.3737566471099854 120.42108154296875
Loss :  1.704742193222046 3.111380100250244 157.27374267578125
Loss :  1.7212963104248047 3.4000747203826904 171.72503662109375
Loss :  1.7185157537460327 3.163633108139038 159.90017700195312
Loss :  1.708009958267212 2.601719379425049 131.79397583007812
Loss :  1.723117709159851 2.661106824874878 134.77845764160156
Loss :  1.7012693881988525 2.4006881713867188 121.73567962646484
Loss :  1.734383463859558 3.217945098876953 162.6316375732422
Loss :  1.7052111625671387 3.3150200843811035 167.4562225341797
Loss :  1.69692063331604 2.9349424839019775 148.4440460205078
Loss :  1.7042986154556274 3.4521048069000244 174.30953979492188
Loss :  1.7367008924484253 2.9289534091949463 148.1843719482422
  batch 60 loss: 1.7367008924484253, 2.9289534091949463, 148.1843719482422
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.69728422164917 3.2912538051605225 166.2599639892578
Loss :  1.7119807004928589 2.890650987625122 146.24452209472656
Loss :  1.7014387845993042 2.52542781829834 127.97283172607422
Loss :  1.6962426900863647 2.6524710655212402 134.31979370117188
Loss :  1.685692548751831 2.2458302974700928 113.97720336914062
Loss :  1.7123361825942993 4.086836338043213 206.0541534423828
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.7188191413879395 4.132534980773926 208.3455810546875
Loss :  1.7173740863800049 3.994976043701172 201.4661865234375
Loss :  1.7183830738067627 4.0199761390686035 202.71719360351562
Total LOSS train 133.09535710261417 valid 204.64577865600586
CE LOSS train 1.707981063769414 valid 0.4295957684516907
Contrastive LOSS train 2.6277475357055664 valid 1.0049940347671509
EPOCH 267:
Loss :  1.7193384170532227 2.3160834312438965 117.52350616455078
Loss :  1.7255948781967163 2.7825677394866943 140.85398864746094
Loss :  1.70992910861969 2.6309664249420166 133.2582550048828
Loss :  1.713902473449707 2.7100300788879395 137.2154083251953
Loss :  1.7215443849563599 2.438920259475708 123.66755676269531
Loss :  1.7037874460220337 2.41024112701416 122.2158432006836
Loss :  1.7214372158050537 2.630689859390259 133.2559356689453
Loss :  1.7089828252792358 2.6578259468078613 134.60028076171875
Loss :  1.7041982412338257 2.1814825534820557 110.77832794189453
Loss :  1.7218482494354248 2.390932321548462 121.26846313476562
Loss :  1.699429988861084 2.6251721382141113 132.95803833007812
Loss :  1.6988109350204468 2.411223888397217 122.260009765625
Loss :  1.6981167793273926 2.124542713165283 107.92525482177734
Loss :  1.7019404172897339 2.6718924045562744 135.2965545654297
Loss :  1.7292685508728027 2.8111979961395264 142.28916931152344
Loss :  1.7271158695220947 2.3572983741760254 119.592041015625
Loss :  1.6971019506454468 2.5957343578338623 131.4838104248047
Loss :  1.7108534574508667 2.7113168239593506 137.2766876220703
Loss :  1.6956576108932495 2.490792751312256 126.23529052734375
Loss :  1.7269526720046997 2.6467998027801514 134.06695556640625
  batch 20 loss: 1.7269526720046997, 2.6467998027801514, 134.06695556640625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7088185548782349 2.154025077819824 109.41007232666016
Loss :  1.6950795650482178 2.263134002685547 114.85177612304688
Loss :  1.703988790512085 2.320049524307251 117.70646667480469
Loss :  1.7126452922821045 2.3083548545837402 117.13038635253906
Loss :  1.726771593093872 2.2845542430877686 115.95448303222656
Loss :  1.7055387496948242 2.063319444656372 104.87150573730469
Loss :  1.7101314067840576 2.619690418243408 132.6946563720703
Loss :  1.7077057361602783 3.1855309009552 160.9842529296875
Loss :  1.6815552711486816 2.609604835510254 132.1617889404297
Loss :  1.7244586944580078 2.442396879196167 123.84429931640625
Loss :  1.6814128160476685 3.09041166305542 156.20199584960938
Loss :  1.715968132019043 2.8747286796569824 145.45240783691406
Loss :  1.7038267850875854 2.5479848384857178 129.10305786132812
Loss :  1.703513503074646 2.143826723098755 108.89485168457031
Loss :  1.6848540306091309 2.6199052333831787 132.68011474609375
Loss :  1.6930052042007446 2.572608470916748 130.32342529296875
Loss :  1.6927807331085205 2.5086023807525635 127.1229019165039
Loss :  1.7233961820602417 2.7908358573913574 141.2651824951172
Loss :  1.7262909412384033 3.0920491218566895 156.32875061035156
Loss :  1.7312408685684204 2.4473791122436523 124.1001968383789
  batch 40 loss: 1.7312408685684204, 2.4473791122436523, 124.1001968383789
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7110154628753662 2.2926056385040283 116.34129333496094
Loss :  1.7036463022232056 1.9105069637298584 97.22898864746094
Loss :  1.6979777812957764 2.4344303607940674 123.41949462890625
Loss :  1.704634666442871 2.5153520107269287 127.47223663330078
Loss :  1.694196105003357 2.870115041732788 145.199951171875
Loss :  1.7096132040023804 2.61468505859375 132.44386291503906
Loss :  1.7261489629745483 2.2402005195617676 113.73617553710938
Loss :  1.701499342918396 2.3218750953674316 117.79525756835938
Loss :  1.7332112789154053 2.1953577995300293 111.5010986328125
Loss :  1.7042430639266968 2.07072114944458 105.24030303955078
Loss :  1.7208421230316162 2.469587564468384 125.2002182006836
Loss :  1.7187165021896362 2.890420436859131 146.23974609375
Loss :  1.7079699039459229 2.558238983154297 129.6199188232422
Loss :  1.7230063676834106 2.5401968955993652 128.73284912109375
Loss :  1.7014598846435547 2.6801271438598633 135.70782470703125
Loss :  1.7341911792755127 2.5246634483337402 127.96736145019531
Loss :  1.7056838274002075 2.5654237270355225 129.97686767578125
Loss :  1.6974761486053467 2.839700937271118 143.68252563476562
Loss :  1.7052568197250366 2.7041313648223877 136.91183471679688
Loss :  1.7375661134719849 2.322335958480835 117.85436248779297
  batch 60 loss: 1.7375661134719849, 2.322335958480835, 117.85436248779297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.69801664352417 2.8131394386291504 142.35498046875
Loss :  1.7124046087265015 2.326960802078247 118.06044006347656
Loss :  1.7022504806518555 2.200664758682251 111.73548889160156
Loss :  1.696625828742981 2.3966827392578125 121.53076171875
Loss :  1.6866469383239746 1.6953675746917725 86.45502471923828
Loss :  1.7046979665756226 4.076008319854736 205.50511169433594
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.7112902402877808 4.141314506530762 208.77700805664062
Loss :  1.7098060846328735 3.989623785018921 201.1909942626953
Loss :  1.713225245475769 4.05466890335083 204.44667053222656
Total LOSS train 126.73096642127403 valid 204.9799461364746
CE LOSS train 1.709278367115901 valid 0.42830631136894226
Contrastive LOSS train 2.500433764090905 valid 1.0136672258377075
EPOCH 268:
Loss :  1.7199994325637817 2.1290974617004395 108.17487335205078
Loss :  1.7263126373291016 2.265108346939087 114.98172760009766
Loss :  1.7105686664581299 3.3952434062957764 171.47274780273438
Loss :  1.714564561843872 2.2633771896362305 114.8834228515625
Loss :  1.7218161821365356 3.2706899642944336 165.25631713867188
Loss :  1.7040576934814453 2.7054574489593506 136.9769287109375
Loss :  1.7219691276550293 2.7448463439941406 138.9642791748047
Loss :  1.7094377279281616 2.688831329345703 136.1510009765625
Loss :  1.7044528722763062 2.3818130493164062 120.79510498046875
Loss :  1.7218873500823975 2.6531782150268555 134.38079833984375
Loss :  1.7000455856323242 2.580070972442627 130.70359802246094
Loss :  1.6989827156066895 2.51296067237854 127.34701538085938
Loss :  1.6981478929519653 2.336130380630493 118.50466918945312
Loss :  1.7015175819396973 2.9424355030059814 148.82330322265625
Loss :  1.7290401458740234 2.4752299785614014 125.49053955078125
Loss :  1.7266658544540405 2.638054370880127 133.62939453125
Loss :  1.6965293884277344 2.6166176795959473 132.5274200439453
Loss :  1.7103369235992432 2.546048402786255 129.01275634765625
Loss :  1.6952872276306152 2.6753311157226562 135.4618377685547
Loss :  1.7269493341445923 3.2416276931762695 163.80833435058594
  batch 20 loss: 1.7269493341445923, 3.2416276931762695, 163.80833435058594
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.708459734916687 2.7272841930389404 138.0726776123047
Loss :  1.6945712566375732 2.7540442943573 139.3967742919922
Loss :  1.7038100957870483 2.7617995738983154 139.79379272460938
Loss :  1.7119855880737305 3.3603861331939697 169.73129272460938
Loss :  1.7263767719268799 3.3146207332611084 167.45741271972656
Loss :  1.7052888870239258 3.1768288612365723 160.54673767089844
Loss :  1.709884762763977 3.0639023780822754 154.90499877929688
Loss :  1.70770263671875 2.6057655811309814 131.99598693847656
Loss :  1.6810095310211182 3.020939588546753 152.72799682617188
Loss :  1.7249239683151245 2.9317634105682373 148.31309509277344
Loss :  1.681036353111267 3.1652190685272217 159.94198608398438
Loss :  1.7163310050964355 3.208845615386963 162.15859985351562
Loss :  1.7040475606918335 3.19242000579834 161.32504272460938
Loss :  1.7036302089691162 3.774744987487793 190.4408721923828
Loss :  1.6850173473358154 3.143597364425659 158.86488342285156
Loss :  1.6929069757461548 2.747819423675537 139.08387756347656
Loss :  1.6929295063018799 3.334625482559204 168.42420959472656
Loss :  1.7235174179077148 3.238042116165161 163.6256103515625
Loss :  1.7266520261764526 2.312011480331421 117.32722473144531
Loss :  1.7311874628067017 2.1815993785858154 110.8111572265625
  batch 40 loss: 1.7311874628067017, 2.1815993785858154, 110.8111572265625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7108899354934692 2.4624297618865967 124.83238220214844
Loss :  1.7033661603927612 2.0505402088165283 104.23037719726562
Loss :  1.69784414768219 2.8495876789093018 144.17723083496094
Loss :  1.7042112350463867 2.7249155044555664 137.94998168945312
Loss :  1.6937552690505981 2.5878396034240723 131.0857391357422
Loss :  1.7089253664016724 3.3158581256866455 167.5018310546875
Loss :  1.7259331941604614 2.4448137283325195 123.96662139892578
Loss :  1.7013380527496338 2.5469963550567627 129.0511474609375
Loss :  1.7332707643508911 2.393599271774292 121.4132308959961
Loss :  1.7044376134872437 2.166327476501465 110.02081298828125
Loss :  1.7207247018814087 2.5020015239715576 126.82080078125
Loss :  1.7185951471328735 2.6529366970062256 134.3654327392578
Loss :  1.7076565027236938 2.385798931121826 120.99760437011719
Loss :  1.7222126722335815 2.5083909034729004 127.14176177978516
Loss :  1.7013167142868042 2.4844071865081787 125.92167663574219
Loss :  1.7335127592086792 2.4632420539855957 124.89561462402344
Loss :  1.7047263383865356 2.588582754135132 131.1338653564453
Loss :  1.6967880725860596 2.8333749771118164 143.36553955078125
Loss :  1.7040399312973022 2.499520778656006 126.6800765991211
Loss :  1.7366663217544556 2.723383903503418 137.90585327148438
  batch 60 loss: 1.7366663217544556, 2.723383903503418, 137.90585327148438
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6968790292739868 3.7074191570281982 187.06784057617188
Loss :  1.7117586135864258 2.6293764114379883 133.1805877685547
Loss :  1.7010685205459595 3.154891014099121 159.44561767578125
Loss :  1.6956589221954346 3.281191825866699 165.7552490234375
Loss :  1.68532395362854 2.007643222808838 102.06748962402344
Loss :  1.7031198740005493 4.097160816192627 206.56117248535156
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.7097597122192383 4.076476573944092 205.53358459472656
Loss :  1.7081605195999146 3.9869613647460938 201.0562286376953
Loss :  1.7109699249267578 3.876091241836548 195.51553344726562
Total LOSS train 139.43484097994292 valid 202.16662979125977
CE LOSS train 1.7090882759827835 valid 0.42774248123168945
Contrastive LOSS train 2.754515050007747 valid 0.969022810459137
EPOCH 269:
Loss :  1.7193162441253662 2.4485063552856445 124.1446304321289
Loss :  1.7256964445114136 2.487398624420166 126.09562683105469
Loss :  1.7099552154541016 2.4015843868255615 121.78916931152344
Loss :  1.7140731811523438 2.7888786792755127 141.15802001953125
Loss :  1.7218469381332397 2.3084304332733154 117.14337158203125
Loss :  1.7043967247009277 2.9741029739379883 150.4095458984375
Loss :  1.7220046520233154 2.4129409790039062 122.36905670166016
Loss :  1.7096726894378662 2.354123592376709 119.41584777832031
Loss :  1.7049777507781982 2.963482141494751 149.8790740966797
Loss :  1.7219789028167725 2.685692310333252 136.00660705566406
Loss :  1.699992060661316 2.8993005752563477 146.66502380371094
Loss :  1.6988563537597656 2.4627816677093506 124.83793640136719
Loss :  1.698076844215393 2.3235747814178467 117.87681579589844
Loss :  1.701439380645752 2.5202229022979736 127.71258544921875
Loss :  1.7289304733276367 2.4052770137786865 121.9927749633789
Loss :  1.7267625331878662 2.5951831340789795 131.4859161376953
Loss :  1.6960833072662354 2.646509885787964 134.02159118652344
Loss :  1.7100248336791992 2.7281103134155273 138.11553955078125
Loss :  1.694148063659668 2.2841134071350098 115.89981842041016
Loss :  1.7260065078735352 2.4104981422424316 122.25091552734375
  batch 20 loss: 1.7260065078735352, 2.4104981422424316, 122.25091552734375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7072227001190186 2.329298734664917 118.17215728759766
Loss :  1.6931101083755493 2.3850619792938232 120.94620513916016
Loss :  1.7022544145584106 3.0110585689544678 152.25518798828125
Loss :  1.710863471031189 2.378650188446045 120.64337158203125
Loss :  1.7254762649536133 2.5717039108276367 130.3106689453125
Loss :  1.7041436433792114 3.4201548099517822 172.71188354492188
Loss :  1.7089077234268188 2.5221059322357178 127.81420135498047
Loss :  1.7065478563308716 2.3474676609039307 119.0799331665039
Loss :  1.679774284362793 2.2630910873413086 114.8343276977539
Loss :  1.7240382432937622 2.674893379211426 135.46871948242188
Loss :  1.6801977157592773 2.952969789505005 149.32867431640625
Loss :  1.7157522439956665 2.2346150875091553 113.44650268554688
Loss :  1.7033113241195679 2.558375120162964 129.6220703125
Loss :  1.7027002573013306 3.444929599761963 173.9491729736328
Loss :  1.6839834451675415 2.410231113433838 122.19554138183594
Loss :  1.692026972770691 2.8362529277801514 143.5046844482422
Loss :  1.6920056343078613 2.7335712909698486 138.3705596923828
Loss :  1.7231898307800293 3.920405864715576 197.7434844970703
Loss :  1.7257145643234253 4.007585048675537 202.10496520996094
Loss :  1.7304733991622925 3.180567502975464 160.75885009765625
  batch 40 loss: 1.7304733991622925, 3.180567502975464, 160.75885009765625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.710034728050232 3.1525487899780273 159.3374786376953
Loss :  1.7028989791870117 2.3574776649475098 119.5767822265625
Loss :  1.6971529722213745 3.499516487121582 176.67298889160156
Loss :  1.7038803100585938 3.06937837600708 155.17279052734375
Loss :  1.6930769681930542 2.303105592727661 116.84835815429688
Loss :  1.7085708379745483 2.8811228275299072 145.76470947265625
Loss :  1.7258305549621582 2.978834867477417 150.66757202148438
Loss :  1.7003098726272583 2.6743342876434326 135.4170379638672
Loss :  1.7333012819290161 2.547980546951294 129.13232421875
Loss :  1.7033671140670776 2.624807834625244 132.94375610351562
Loss :  1.7203835248947144 3.0980188846588135 156.62132263183594
Loss :  1.7182137966156006 2.799436092376709 141.6900177001953
Loss :  1.70718252658844 2.289490222930908 116.18169403076172
Loss :  1.72193443775177 2.8233227729797363 142.88807678222656
Loss :  1.7007168531417847 3.144188642501831 158.91014099121094
Loss :  1.7330217361450195 2.6963791847229004 136.55197143554688
Loss :  1.7042186260223389 2.1812024116516113 110.76433563232422
Loss :  1.6961251497268677 2.2642180919647217 114.90703582763672
Loss :  1.7036219835281372 2.6080617904663086 132.10671997070312
Loss :  1.7363148927688599 2.221238136291504 112.7982177734375
  batch 60 loss: 1.7363148927688599, 2.221238136291504, 112.7982177734375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6963136196136475 2.5627968311309814 129.83616638183594
Loss :  1.7112114429473877 3.627869129180908 183.10467529296875
Loss :  1.7007619142532349 2.1161749362945557 107.50950622558594
Loss :  1.6947580575942993 2.3352060317993164 118.4550552368164
Loss :  1.6842902898788452 2.615712881088257 132.46994018554688
Loss :  1.7003306150436401 4.323325157165527 217.86659240722656
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.706801414489746 4.388998508453369 221.15672302246094
Loss :  1.704460620880127 4.2211103439331055 212.75997924804688
Loss :  1.7112008333206177 4.104701995849609 206.94630432128906
Total LOSS train 136.136303124061 valid 214.68239974975586
CE LOSS train 1.7085142410718477 valid 0.4278002083301544
Contrastive LOSS train 2.6885557724879336 valid 1.0261754989624023
EPOCH 270:
Loss :  1.7184399366378784 2.482780933380127 125.85748291015625
Loss :  1.725046157836914 2.550567388534546 129.25341796875
Loss :  1.7093689441680908 2.2423105239868164 113.82489013671875
Loss :  1.7132389545440674 2.659381151199341 134.6822967529297
Loss :  1.7213939428329468 2.4905858039855957 126.25068664550781
Loss :  1.7033939361572266 2.565528154373169 129.9798126220703
Loss :  1.7213698625564575 2.742357015609741 138.83921813964844
Loss :  1.7090363502502441 2.478823184967041 125.65019989013672
Loss :  1.7042659521102905 2.3814172744750977 120.77513122558594
Loss :  1.7219483852386475 2.577692747116089 130.60659790039062
Loss :  1.6993510723114014 3.373391628265381 170.3689422607422
Loss :  1.6986112594604492 3.4907915592193604 176.23818969726562
Loss :  1.6979598999023438 2.306299924850464 117.01295471191406
Loss :  1.7015100717544556 2.3762927055358887 120.51614379882812
Loss :  1.7288861274719238 2.807602643966675 142.1090087890625
Loss :  1.7264914512634277 2.9567692279815674 149.56494140625
Loss :  1.6962990760803223 3.6913681030273438 186.26470947265625
Loss :  1.7102071046829224 3.143141031265259 158.8672637939453
Loss :  1.694955587387085 4.076449871063232 205.5174560546875
Loss :  1.7266876697540283 2.8203132152557373 142.74234008789062
  batch 20 loss: 1.7266876697540283, 2.8203132152557373, 142.74234008789062
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7086408138275146 2.753274440765381 139.3723602294922
Loss :  1.6948868036270142 2.807279109954834 142.058837890625
Loss :  1.7040882110595703 2.956449270248413 149.52655029296875
Loss :  1.7129234075546265 2.9569945335388184 149.56265258789062
Loss :  1.726965308189392 2.5339014530181885 128.4220428466797
Loss :  1.7061249017715454 2.367504835128784 120.08136749267578
Loss :  1.7109324932098389 2.9990246295928955 151.66217041015625
Loss :  1.708739995956421 2.9218833446502686 147.8029022216797
Loss :  1.6826152801513672 2.3056437969207764 116.96480560302734
Loss :  1.725242018699646 2.6701254844665527 135.23150634765625
Loss :  1.6826597452163696 2.5640368461608887 129.88450622558594
Loss :  1.7172664403915405 2.725924491882324 138.01348876953125
Loss :  1.7051703929901123 3.3120276927948 167.30654907226562
Loss :  1.7046366930007935 3.484135389328003 175.91140747070312
Loss :  1.6863064765930176 2.821234703063965 142.748046875
Loss :  1.6939764022827148 2.679210662841797 135.65451049804688
Loss :  1.693806529045105 2.6029012203216553 131.8388671875
Loss :  1.7243387699127197 2.637462615966797 133.59747314453125
Loss :  1.7268822193145752 2.7014098167419434 136.79737854003906
Loss :  1.731595754623413 2.8808329105377197 145.7732391357422
  batch 40 loss: 1.731595754623413, 2.8808329105377197, 145.7732391357422
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7115238904953003 2.836806297302246 143.5518341064453
Loss :  1.704672932624817 2.308661460876465 117.13774871826172
Loss :  1.6989153623580933 2.6318018436431885 133.28900146484375
Loss :  1.7054033279418945 2.1720054149627686 110.30567932128906
Loss :  1.6950136423110962 2.1695847511291504 110.17425537109375
Loss :  1.710182547569275 2.5600392818450928 129.712158203125
Loss :  1.7270400524139404 2.227883815765381 113.12123107910156
Loss :  1.7024418115615845 2.593489170074463 131.37689208984375
Loss :  1.73428213596344 2.9970498085021973 151.58677673339844
Loss :  1.706331729888916 3.1510775089263916 159.2602081298828
Loss :  1.7224119901657104 3.2431540489196777 163.88011169433594
Loss :  1.7202324867248535 2.2345635890960693 113.44841003417969
Loss :  1.7092349529266357 2.9742822647094727 150.42333984375
Loss :  1.723828673362732 2.5327372550964355 128.36068725585938
Loss :  1.702760934829712 2.9456498622894287 148.98524475097656
Loss :  1.7345203161239624 3.0060932636260986 152.0391845703125
Loss :  1.7063263654708862 2.4757463932037354 125.49365234375
Loss :  1.6982030868530273 2.513598680496216 127.37813568115234
Loss :  1.7058461904525757 3.1305534839630127 158.2335205078125
Loss :  1.7379461526870728 2.083340644836426 105.90497589111328
  batch 60 loss: 1.7379461526870728, 2.083340644836426, 105.90497589111328
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6987571716308594 2.4092888832092285 122.1632080078125
Loss :  1.7130147218704224 2.7737197875976562 140.3990020751953
Loss :  1.7028485536575317 2.6851959228515625 135.962646484375
Loss :  1.697414517402649 3.2147488594055176 162.4348602294922
Loss :  1.6872005462646484 2.02323055267334 102.8487319946289
Loss :  1.7082180976867676 4.286723613739014 216.04440307617188
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.7148098945617676 4.372071266174316 220.31837463378906
Loss :  1.7132437229156494 4.294721603393555 216.44932556152344
Loss :  1.7156085968017578 4.003588676452637 201.89503479003906
Total LOSS train 138.47082836444562 valid 213.67678451538086
CE LOSS train 1.7097325306672317 valid 0.42890214920043945
Contrastive LOSS train 2.7352219104766844 valid 1.0008971691131592
EPOCH 271:
Loss :  1.720171570777893 2.6094424724578857 132.19229125976562
Loss :  1.7263156175613403 3.0488553047180176 154.16908264160156
Loss :  1.711118459701538 2.679945945739746 135.7084197998047
Loss :  1.7150732278823853 3.273554563522339 165.39280700683594
Loss :  1.7228847742080688 2.395094633102417 121.47761535644531
Loss :  1.7052335739135742 2.6400859355926514 133.70953369140625
Loss :  1.7228233814239502 2.72536039352417 137.9908447265625
Loss :  1.7104182243347168 2.3193137645721436 117.67610931396484
Loss :  1.705857515335083 2.2186877727508545 112.64024353027344
Loss :  1.7230137586593628 2.2819337844848633 115.8197021484375
Loss :  1.7008564472198486 2.5454208850860596 128.97189331054688
Loss :  1.7001676559448242 2.3444979190826416 118.92506408691406
Loss :  1.6994279623031616 2.843325138092041 143.86569213867188
Loss :  1.7031172513961792 3.7146780490875244 187.43701171875
Loss :  1.7297791242599487 2.8936960697174072 146.41458129882812
Loss :  1.7274197340011597 2.76554274559021 140.00454711914062
Loss :  1.6977622509002686 2.5652565956115723 129.96060180664062
Loss :  1.711586833000183 2.8316385746002197 143.29351806640625
Loss :  1.6961159706115723 2.4883041381835938 126.11132049560547
Loss :  1.7276525497436523 2.667590379714966 135.10716247558594
  batch 20 loss: 1.7276525497436523, 2.667590379714966, 135.10716247558594
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7093526124954224 2.9910731315612793 151.26300048828125
Loss :  1.6958026885986328 2.8442039489746094 143.906005859375
Loss :  1.7045950889587402 2.792715549468994 141.34036254882812
Loss :  1.7135132551193237 2.499321222305298 126.67958068847656
Loss :  1.7275733947753906 2.871729612350464 145.31405639648438
Loss :  1.706507682800293 3.976097583770752 200.5113983154297
Loss :  1.7111274003982544 3.7088844776153564 187.1553497314453
Loss :  1.708990216255188 2.5756897926330566 130.49346923828125
Loss :  1.6831921339035034 2.3791847229003906 120.64242553710938
Loss :  1.725553274154663 2.570905923843384 130.27085876464844
Loss :  1.6831705570220947 2.43668270111084 123.5173110961914
Loss :  1.7172654867172241 2.416337013244629 122.53411102294922
Loss :  1.7053979635238647 2.9828507900238037 150.84793090820312
Loss :  1.7048978805541992 2.2390503883361816 113.65741729736328
Loss :  1.6866099834442139 2.726579427719116 138.0155792236328
Loss :  1.6942510604858398 3.961254596710205 199.75697326660156
Loss :  1.6941558122634888 2.538623571395874 128.62533569335938
Loss :  1.7246583700180054 2.4207441806793213 122.7618637084961
Loss :  1.7271569967269897 2.4839730262756348 125.92581176757812
Loss :  1.7319148778915405 2.4190220832824707 122.68302154541016
  batch 40 loss: 1.7319148778915405, 2.4190220832824707, 122.68302154541016
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7119035720825195 2.6835248470306396 135.8881378173828
Loss :  1.7043660879135132 2.510606527328491 127.23469543457031
Loss :  1.698785662651062 2.38521671295166 120.9596176147461
Loss :  1.7052977085113525 2.2590317726135254 114.65689086914062
Loss :  1.6947085857391357 2.3731932640075684 120.3543701171875
Loss :  1.7098157405853271 2.593078374862671 131.3637237548828
Loss :  1.726374864578247 2.2443132400512695 113.94203186035156
Loss :  1.7019166946411133 3.1787116527557373 160.6374969482422
Loss :  1.7336008548736572 3.6797335147857666 185.72027587890625
Loss :  1.7049967050552368 2.338007688522339 118.60537719726562
Loss :  1.7215940952301025 2.7037084102630615 136.90701293945312
Loss :  1.7191802263259888 2.31809663772583 117.62400817871094
Loss :  1.7084044218063354 2.3842504024505615 120.9209213256836
Loss :  1.7229886054992676 2.468538999557495 125.14994049072266
Loss :  1.7021105289459229 2.501523017883301 126.77825927734375
Loss :  1.7339756488800049 2.160595417022705 109.76374816894531
Loss :  1.7057074308395386 3.3551459312438965 169.46299743652344
Loss :  1.6976107358932495 2.8002398014068604 141.70960998535156
Loss :  1.705161213874817 2.6449475288391113 133.95252990722656
Loss :  1.737223505973816 2.179788827896118 110.7266616821289
  batch 60 loss: 1.737223505973816, 2.179788827896118, 110.7266616821289
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.698076844215393 2.332942008972168 118.34517669677734
Loss :  1.712565541267395 2.3433640003204346 118.8807601928711
Loss :  1.7021667957305908 2.769031047821045 140.15371704101562
Loss :  1.696443796157837 2.604201078414917 131.906494140625
Loss :  1.6860359907150269 1.9009748697280884 96.73477935791016
Loss :  1.7095342874526978 4.296761989593506 216.54763793945312
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.716403841972351 4.273675441741943 215.40017700195312
Loss :  1.7148926258087158 4.2131547927856445 212.3726348876953
Loss :  1.7168500423431396 4.114816188812256 207.45765686035156
Total LOSS train 135.0946021446815 valid 212.94452667236328
CE LOSS train 1.7100537611888005 valid 0.4292125105857849
Contrastive LOSS train 2.6676909905213577 valid 1.028704047203064
EPOCH 272:
Loss :  1.719636082649231 2.3938636779785156 121.4128189086914
Loss :  1.725996732711792 2.7709500789642334 140.27349853515625
Loss :  1.710476040840149 2.3622286319732666 119.82191467285156
Loss :  1.714371919631958 2.3755431175231934 120.49152374267578
Loss :  1.722333550453186 2.1612439155578613 109.78453063964844
Loss :  1.704687237739563 2.12363600730896 107.88648986816406
Loss :  1.7223942279815674 2.505831718444824 127.01398468017578
Loss :  1.7102162837982178 2.2246971130371094 112.945068359375
Loss :  1.7057435512542725 1.788917899131775 91.1516342163086
Loss :  1.7230945825576782 2.612563133239746 132.35125732421875
Loss :  1.7009317874908447 2.30837082862854 117.11947631835938
Loss :  1.7002009153366089 2.5232958793640137 127.86499786376953
Loss :  1.6992039680480957 3.2214834690093994 162.77337646484375
Loss :  1.7030020952224731 3.3655202388763428 169.97901916503906
Loss :  1.7298961877822876 2.400329828262329 121.74639129638672
Loss :  1.7277981042861938 2.4199554920196533 122.72557067871094
Loss :  1.6977264881134033 2.524937152862549 127.944580078125
Loss :  1.711436152458191 2.2497785091400146 114.20035552978516
Loss :  1.696054458618164 2.1739726066589355 110.39468383789062
Loss :  1.7274101972579956 2.2210428714752197 112.77955627441406
  batch 20 loss: 1.7274101972579956, 2.2210428714752197, 112.77955627441406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7092868089675903 2.339843988418579 118.70149230957031
Loss :  1.6956753730773926 2.2808196544647217 115.73666381835938
Loss :  1.7046387195587158 2.128979206085205 108.15359497070312
Loss :  1.7132833003997803 2.2226414680480957 112.84535217285156
Loss :  1.7275176048278809 2.2960100173950195 116.52801513671875
Loss :  1.706717848777771 2.375659942626953 120.48971557617188
Loss :  1.711315393447876 2.7192094326019287 137.67178344726562
Loss :  1.7090846300125122 2.5051469802856445 126.9664306640625
Loss :  1.683468222618103 2.972595453262329 150.31324768066406
Loss :  1.7256518602371216 3.3400039672851562 168.72584533691406
Loss :  1.6834367513656616 2.3598525524139404 119.67606353759766
Loss :  1.7174737453460693 2.2853853702545166 115.98674774169922
Loss :  1.705580472946167 2.7207183837890625 137.7415008544922
Loss :  1.7048431634902954 2.2941372394561768 116.41170501708984
Loss :  1.6866848468780518 2.595191478729248 131.44625854492188
Loss :  1.6946433782577515 2.412369728088379 122.31312561035156
Loss :  1.6944159269332886 2.967784881591797 150.0836639404297
Loss :  1.7246342897415161 2.3109474182128906 117.27200317382812
Loss :  1.7274727821350098 1.8879939317703247 96.12716674804688
Loss :  1.7320194244384766 1.8027044534683228 91.86724090576172
  batch 40 loss: 1.7320194244384766, 1.8027044534683228, 91.86724090576172
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7121654748916626 2.508627414703369 127.14353942871094
Loss :  1.7049096822738647 2.1132524013519287 107.3675308227539
Loss :  1.6996408700942993 2.7117364406585693 137.28646850585938
Loss :  1.7058669328689575 2.547720193862915 129.0918731689453
Loss :  1.6955080032348633 2.890371799468994 146.21409606933594
Loss :  1.7105046510696411 2.390089750289917 121.2149887084961
Loss :  1.7270246744155884 3.4115283489227295 172.30343627929688
Loss :  1.7024950981140137 2.1301887035369873 108.21192932128906
Loss :  1.734514832496643 2.4360668659210205 123.53785705566406
Loss :  1.705767273902893 2.1915900707244873 111.28527069091797
Loss :  1.722241997718811 2.4579336643218994 124.61892700195312
Loss :  1.720059871673584 2.572664499282837 130.35328674316406
Loss :  1.7094485759735107 2.163029909133911 109.86094665527344
Loss :  1.7240606546401978 2.3533363342285156 119.39087677001953
Loss :  1.703355312347412 2.198232412338257 111.61497497558594
Loss :  1.7350218296051025 2.4125990867614746 122.36497497558594
Loss :  1.7071454524993896 2.3646504878997803 119.93966674804688
Loss :  1.6993709802627563 3.771752119064331 190.28697204589844
Loss :  1.7067842483520508 3.9524621963500977 199.32989501953125
Loss :  1.738139033317566 3.159672260284424 159.7217559814453
  batch 60 loss: 1.738139033317566, 3.159672260284424, 159.7217559814453
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6997590065002441 2.791694402694702 141.2844696044922
Loss :  1.7140514850616455 2.3476858139038086 119.09834289550781
Loss :  1.70350182056427 2.6030361652374268 131.85531616210938
Loss :  1.6975257396697998 2.4737741947174072 125.38623809814453
Loss :  1.687103271484375 1.7408688068389893 88.73054504394531
Loss :  1.6957122087478638 4.362680435180664 219.82972717285156
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.7017430067062378 4.374564170837402 220.42994689941406
Loss :  1.6996119022369385 4.391510486602783 221.275146484375
Loss :  1.7056152820587158 4.34046745300293 218.72898864746094
Total LOSS train 126.5109619140625 valid 220.0659523010254
CE LOSS train 1.7104064904726468 valid 0.42640382051467896
Contrastive LOSS train 2.496011108618516 valid 1.0851168632507324
EPOCH 273:
Loss :  1.7201809883117676 2.3137505054473877 117.40770721435547
Loss :  1.7262030839920044 2.5166938304901123 127.56089782714844
Loss :  1.7109460830688477 2.4180707931518555 122.61448669433594
Loss :  1.7145373821258545 3.4615654945373535 174.79281616210938
Loss :  1.7223340272903442 2.6892495155334473 136.184814453125
Loss :  1.7047717571258545 2.4549930095672607 124.45442199707031
Loss :  1.7227205038070679 2.3359718322753906 118.52130889892578
Loss :  1.71036696434021 2.384469747543335 120.93385314941406
Loss :  1.7057055234909058 2.577810049057007 130.59620666503906
Loss :  1.723095417022705 2.4677345752716064 125.10982513427734
Loss :  1.7010173797607422 2.6468024253845215 134.0411376953125
Loss :  1.7002454996109009 2.288503885269165 116.12544250488281
Loss :  1.6995713710784912 2.1087729930877686 107.13822174072266
Loss :  1.7030482292175293 2.284893274307251 115.94771575927734
Loss :  1.7300500869750977 2.6309993267059326 133.280029296875
Loss :  1.7279967069625854 2.6169450283050537 132.5752410888672
Loss :  1.6983143091201782 2.475667715072632 125.48169708251953
Loss :  1.711914300918579 3.157395601272583 159.58169555664062
Loss :  1.6967601776123047 2.852916717529297 144.3426055908203
Loss :  1.7275800704956055 3.08760142326355 156.10765075683594
  batch 20 loss: 1.7275800704956055, 3.08760142326355, 156.10765075683594
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7096318006515503 2.139247417449951 108.67200469970703
Loss :  1.696125864982605 2.7821462154388428 140.80343627929688
Loss :  1.7048362493515015 1.9916422367095947 101.28694915771484
Loss :  1.7136907577514648 2.691521167755127 136.2897491455078
Loss :  1.7275506258010864 2.5134875774383545 127.40193176269531
Loss :  1.7067328691482544 2.5349416732788086 128.4538116455078
Loss :  1.7111806869506836 2.1967852115631104 111.55044555664062
Loss :  1.7089354991912842 3.842344045639038 193.82614135742188
Loss :  1.6835018396377563 2.595973014831543 131.48214721679688
Loss :  1.7259232997894287 2.6353249549865723 133.49217224121094
Loss :  1.6835901737213135 2.7843215465545654 140.89967346191406
Loss :  1.7177261114120483 2.6096384525299072 132.19964599609375
Loss :  1.706118106842041 2.445010185241699 123.95662689208984
Loss :  1.7057428359985352 2.2299396991729736 113.20272827148438
Loss :  1.687418818473816 2.5392861366271973 128.6517333984375
Loss :  1.695371389389038 2.738856077194214 138.63818359375
Loss :  1.6952712535858154 3.1323471069335938 158.3126220703125
Loss :  1.7251629829406738 2.8514904975891113 144.2996826171875
Loss :  1.727910041809082 2.5052621364593506 126.99102020263672
Loss :  1.7324514389038086 2.3795530796051025 120.7101058959961
  batch 40 loss: 1.7324514389038086, 2.3795530796051025, 120.7101058959961
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.712803602218628 3.01676082611084 152.55084228515625
Loss :  1.705693244934082 2.4019925594329834 121.8053207397461
Loss :  1.7002949714660645 2.650080680847168 134.20433044433594
Loss :  1.706727147102356 2.2388722896575928 113.65033721923828
Loss :  1.6967952251434326 3.3351423740386963 168.45391845703125
Loss :  1.711699366569519 2.8642611503601074 144.9247589111328
Loss :  1.7280466556549072 3.57802677154541 180.62937927246094
Loss :  1.7042759656906128 2.607910394668579 132.09979248046875
Loss :  1.7346683740615845 3.00227427482605 151.84837341308594
Loss :  1.7072325944900513 2.592550754547119 131.3347625732422
Loss :  1.7233806848526 2.422327995300293 122.83978271484375
Loss :  1.7208313941955566 2.374213457107544 120.43150329589844
Loss :  1.7100063562393188 2.340869188308716 118.75346374511719
Loss :  1.7244793176651 2.549126386642456 129.18080139160156
Loss :  1.7032662630081177 3.4117023944854736 172.2883758544922
Loss :  1.734967589378357 2.504551410675049 126.96253967285156
Loss :  1.7069793939590454 2.7707176208496094 140.2428741455078
Loss :  1.6987665891647339 2.1942853927612305 111.41304016113281
Loss :  1.7061631679534912 3.4296000003814697 173.1861572265625
Loss :  1.7379875183105469 2.036344528198242 103.55522155761719
  batch 60 loss: 1.7379875183105469, 2.036344528198242, 103.55522155761719
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6992372274398804 2.180417537689209 110.72010803222656
Loss :  1.713474154472351 2.050182342529297 104.22258758544922
Loss :  1.7035123109817505 3.3195836544036865 167.6826934814453
Loss :  1.697981595993042 3.4831316471099854 175.8545684814453
Loss :  1.6878467798233032 2.220197916030884 112.69773864746094
Loss :  1.706364393234253 3.7217156887054443 187.79214477539062
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.7126891613006592 3.6738669872283936 185.40603637695312
Loss :  1.7109323740005493 3.505035161972046 176.9626922607422
Loss :  1.7142990827560425 3.583923578262329 180.9104766845703
Total LOSS train 133.6223362849309 valid 182.76783752441406
CE LOSS train 1.7107592307604276 valid 0.4285747706890106
Contrastive LOSS train 2.63823153422429 valid 0.8959808945655823
EPOCH 274:
Loss :  1.7207927703857422 2.3994944095611572 121.69551849365234
Loss :  1.7272555828094482 2.5526018142700195 129.3573455810547
Loss :  1.712312936782837 2.2376012802124023 113.59237670898438
Loss :  1.715928554534912 2.3142805099487305 117.4299545288086
Loss :  1.7238130569458008 2.327934503555298 118.12054443359375
Loss :  1.7064849138259888 2.3742947578430176 120.42121887207031
Loss :  1.7239035367965698 2.649479389190674 134.1978759765625
Loss :  1.7119581699371338 4.247137069702148 214.06881713867188
Loss :  1.7074662446975708 2.8551712036132812 144.46603393554688
Loss :  1.7244986295700073 2.181809902191162 110.81498718261719
Loss :  1.7030988931655884 2.4255294799804688 122.97957611083984
Loss :  1.7025498151779175 2.4210124015808105 122.753173828125
Loss :  1.7019394636154175 2.401233196258545 121.76360321044922
Loss :  1.705644965171814 2.3486416339874268 119.13772583007812
Loss :  1.7319438457489014 2.4625515937805176 124.8595199584961
Loss :  1.729529857635498 2.3756649494171143 120.51277923583984
Loss :  1.7009607553482056 3.057255268096924 154.563720703125
Loss :  1.7143319845199585 2.8037352561950684 141.90109252929688
Loss :  1.6992237567901611 2.5969669818878174 131.5475616455078
Loss :  1.7292333841323853 2.549177885055542 129.18812561035156
  batch 20 loss: 1.7292333841323853, 2.549177885055542, 129.18812561035156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7114566564559937 2.316100597381592 117.51648712158203
Loss :  1.6981475353240967 2.4826653003692627 125.83141326904297
Loss :  1.706375241279602 2.316553831100464 117.53406524658203
Loss :  1.7149096727371216 2.4126687049865723 122.34835052490234
Loss :  1.7285200357437134 3.3171117305755615 167.5841064453125
Loss :  1.7079463005065918 3.169980049133301 160.2069549560547
Loss :  1.7122647762298584 3.7652182579040527 189.97317504882812
Loss :  1.7100526094436646 2.7967617511749268 141.5481414794922
Loss :  1.6843963861465454 3.5905821323394775 181.21351623535156
Loss :  1.7260903120040894 2.684234142303467 135.9377899169922
Loss :  1.6838911771774292 2.757720470428467 139.5699005126953
Loss :  1.7177472114562988 2.754762887954712 139.4558868408203
Loss :  1.7059528827667236 2.6565358638763428 134.53274536132812
Loss :  1.7056578397750854 2.667314291000366 135.0713653564453
Loss :  1.6869081258773804 2.68505859375 135.93983459472656
Loss :  1.695045828819275 3.0606601238250732 154.72805786132812
Loss :  1.694776177406311 2.634726047515869 133.4310760498047
Loss :  1.724910855293274 3.2607765197753906 164.76373291015625
Loss :  1.7276886701583862 2.5941569805145264 131.435546875
Loss :  1.7322957515716553 2.617330312728882 132.59881591796875
  batch 40 loss: 1.7322957515716553, 2.617330312728882, 132.59881591796875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.712524175643921 3.121356248855591 157.78033447265625
Loss :  1.7054039239883423 2.1213908195495605 107.77494812011719
Loss :  1.6997358798980713 2.5443012714385986 128.914794921875
Loss :  1.706420660018921 2.371385097503662 120.27567291259766
Loss :  1.6961554288864136 3.2378334999084473 163.58782958984375
Loss :  1.710927128791809 2.883894443511963 145.9056396484375
Loss :  1.7276419401168823 2.26444935798645 114.95010375976562
Loss :  1.7039202451705933 2.267306089401245 115.06922149658203
Loss :  1.7350386381149292 2.4232583045959473 122.89795684814453
Loss :  1.7071853876113892 2.4153597354888916 122.47517395019531
Loss :  1.7232670783996582 1.9716640710830688 100.30647277832031
Loss :  1.7209073305130005 2.2635114192962646 114.89647674560547
Loss :  1.7102407217025757 2.3250739574432373 117.96394348144531
Loss :  1.7243191003799438 3.2869138717651367 166.07000732421875
Loss :  1.7037428617477417 3.6035947799682617 181.8834686279297
Loss :  1.7351511716842651 2.7299680709838867 138.23355102539062
Loss :  1.7070627212524414 3.06349778175354 154.8819580078125
Loss :  1.698910117149353 3.1635403633117676 159.8759307861328
Loss :  1.7064764499664307 3.2256057262420654 162.9867706298828
Loss :  1.7384916543960571 2.34374737739563 118.92586517333984
  batch 60 loss: 1.7384916543960571, 2.34374737739563, 118.92586517333984
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6994379758834839 2.4493489265441895 124.16688537597656
Loss :  1.713584542274475 2.575793743133545 130.50326538085938
Loss :  1.7034879922866821 2.2295126914978027 113.17912292480469
Loss :  1.6978250741958618 3.0759401321411133 155.49484252929688
Loss :  1.6877193450927734 2.8179521560668945 142.5853271484375
Loss :  1.70453679561615 4.289763927459717 216.19273376464844
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.7109572887420654 4.375218391418457 220.4718780517578
Loss :  1.7095282077789307 4.2062482833862305 212.02194213867188
Loss :  1.7115951776504517 4.16038179397583 209.73068237304688
Total LOSS train 136.24889350304238 valid 214.60430908203125
CE LOSS train 1.7114381955220148 valid 0.4278987944126129
Contrastive LOSS train 2.690749107874357 valid 1.0400954484939575
EPOCH 275:
Loss :  1.720836877822876 2.4788129329681396 125.66148376464844
Loss :  1.7271344661712646 3.3816075325012207 170.80751037597656
Loss :  1.7115613222122192 2.7462079524993896 139.02195739746094
Loss :  1.7155449390411377 2.8677940368652344 145.10525512695312
Loss :  1.7233010530471802 2.0265133380889893 103.0489730834961
Loss :  1.7057654857635498 2.0000712871551514 101.7093276977539
Loss :  1.7231441736221313 2.3122751712799072 117.33690643310547
Loss :  1.7108579874038696 2.1397693157196045 108.69932556152344
Loss :  1.706104040145874 3.6386804580688477 183.64012145996094
Loss :  1.7235674858093262 2.4669535160064697 125.07124328613281
Loss :  1.7012220621109009 2.7516376972198486 139.28309631347656
Loss :  1.700529932975769 2.597625494003296 131.58180236816406
Loss :  1.699860692024231 2.3131818771362305 117.35895538330078
Loss :  1.70329749584198 2.670518398284912 135.22921752929688
Loss :  1.7304147481918335 2.531055450439453 128.28318786621094
Loss :  1.727691650390625 2.5173048973083496 127.59293365478516
Loss :  1.698203682899475 2.1504294872283936 109.21968078613281
Loss :  1.711864948272705 2.6199305057525635 132.70838928222656
Loss :  1.6965153217315674 2.839322090148926 143.66262817382812
Loss :  1.7276294231414795 3.0265893936157227 153.05709838867188
  batch 20 loss: 1.7276294231414795, 3.0265893936157227, 153.05709838867188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
Loss :  1.7094916105270386 2.5899949073791504 131.209228515625
Loss :  1.695949673652649 3.2756524085998535 165.47857666015625
Loss :  1.7046668529510498 3.2742531299591064 165.41732788085938
Loss :  1.7130521535873413 2.593071460723877 131.36663818359375
Loss :  1.72720468044281 2.4973461627960205 126.59451293945312
Loss :  1.7064082622528076 2.704357147216797 136.9242706298828
Loss :  1.7109436988830566 3.0196659564971924 152.69422912597656
Loss :  1.708703637123108 2.485705852508545 125.9939956665039
Loss :  1.682202935218811 2.9148223400115967 147.42332458496094
Loss :  1.7252634763717651 2.6787047386169434 135.66050720214844
Loss :  1.6821486949920654 3.184985876083374 160.9314422607422
Loss :  1.7167258262634277 2.562669038772583 129.8501739501953
Loss :  1.7049195766448975 3.008634328842163 152.1366424560547
Loss :  1.704444408416748 2.4862358570098877 126.0162353515625
Loss :  1.686221718788147 2.464160680770874 124.89424896240234
Loss :  1.6941221952438354 2.7201008796691895 137.6991729736328
Loss :  1.6941980123519897 2.395207643508911 121.45458221435547
Loss :  1.724384069442749 2.3507609367370605 119.2624282836914
Loss :  1.726924180984497 2.5952506065368652 131.4894561767578
Loss :  1.7315394878387451 2.7141969203948975 137.44137573242188
  batch 40 loss: 1.7315394878387451, 2.7141969203948975, 137.44137573242188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.711868405342102 3.204934597015381 161.95860290527344
Loss :  1.7041300535202026 2.174656629562378 110.43696594238281
Loss :  1.6988246440887451 2.3624398708343506 119.82081604003906
Loss :  1.705190658569336 3.0204381942749023 152.7270965576172
Loss :  1.6949152946472168 2.3271644115448 118.05313873291016
Loss :  1.710025429725647 2.5847079753875732 130.94541931152344
Loss :  1.726711630821228 2.4154369831085205 122.49856567382812
Loss :  1.7026296854019165 2.375804901123047 120.49287414550781
Loss :  1.7339916229248047 2.562321186065674 129.8500518798828
Loss :  1.7055723667144775 2.4566736221313477 124.53925323486328
Loss :  1.7221944332122803 2.512321710586548 127.3382797241211
Loss :  1.7198206186294556 2.430985450744629 123.26908874511719
Loss :  1.7089217901229858 2.203198194503784 111.86883544921875
Loss :  1.7235697507858276 2.654191732406616 134.4331512451172
Loss :  1.70293128490448 2.707282543182373 137.06704711914062
Loss :  1.734531044960022 2.466092348098755 125.03914642333984
Loss :  1.7064379453659058 2.3377039432525635 118.59163665771484
Loss :  1.6986488103866577 2.8937833309173584 146.3878173828125
Loss :  1.7060555219650269 2.651305913925171 134.27134704589844
Loss :  1.7378287315368652 2.0927278995513916 106.37422943115234
  batch 60 loss: 1.7378287315368652, 2.0927278995513916, 106.37422943115234
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.698758840560913 2.3168747425079346 117.54249572753906
Loss :  1.7131071090698242 3.4114878177642822 172.28749084472656
Loss :  1.7028782367706299 2.271320104598999 115.26888275146484
Loss :  1.6975927352905273 2.518307685852051 127.61297607421875
Loss :  1.6873518228530884 2.232333183288574 113.30401611328125
Loss :  1.7043882608413696 4.05305290222168 204.35704040527344
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.7107386589050293 4.17899227142334 210.6603546142578
Loss :  1.708916187286377 3.9914371967315674 201.28077697753906
Loss :  1.7120717763900757 3.809645891189575 192.19436645507812
Total LOSS train 132.3076413668119 valid 202.1231346130371
CE LOSS train 1.7102930985964262 valid 0.4280179440975189
Contrastive LOSS train 2.611946964263916 valid 0.9524114727973938
EPOCH 276:
Loss :  1.7206058502197266 2.4671883583068848 125.08002471923828
Loss :  1.7268122434616089 2.6385204792022705 133.65283203125
Loss :  1.7117314338684082 2.1384670734405518 108.63508605957031
Loss :  1.7155401706695557 2.2402055263519287 113.72581481933594
Loss :  1.723265528678894 2.951305866241455 149.28855895996094
Loss :  1.705978512763977 2.3793561458587646 120.67378234863281
Loss :  1.7237128019332886 2.9743456840515137 150.4409942626953
Loss :  1.711689829826355 2.0453941822052 103.98139953613281
Loss :  1.7071794271469116 3.5825729370117188 180.83583068847656
Loss :  1.7243963479995728 2.764087200164795 139.9287567138672
Loss :  1.7026804685592651 2.697646379470825 136.5850067138672
Loss :  1.7020002603530884 2.8457369804382324 143.9888458251953
Loss :  1.7015341520309448 3.520291566848755 177.7161102294922
Loss :  1.7050539255142212 3.029834747314453 153.19677734375
Loss :  1.7311328649520874 2.409266471862793 122.1944580078125
Loss :  1.7291204929351807 2.460902452468872 124.77423858642578
Loss :  1.7002352476119995 2.7732632160186768 140.3634033203125
Loss :  1.7138993740081787 2.572145938873291 130.32119750976562
Loss :  1.6992169618606567 1.7811907529830933 90.75875854492188
Loss :  1.7292314767837524 2.0162739753723145 102.54293060302734
  batch 20 loss: 1.7292314767837524, 2.0162739753723145, 102.54293060302734
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7117446660995483 2.152036190032959 109.31355285644531
Loss :  1.6984776258468628 2.412432909011841 122.32012176513672
Loss :  1.70695960521698 3.05346941947937 154.38043212890625
Loss :  1.7152694463729858 2.6632745265960693 134.87899780273438
Loss :  1.7288448810577393 2.6462972164154053 134.04371643066406
Loss :  1.7080634832382202 3.9343254566192627 198.42434692382812
Loss :  1.7128630876541138 3.181689977645874 160.79736328125
Loss :  1.7103601694107056 3.2583889961242676 164.6298065185547
Loss :  1.6848715543746948 2.4243063926696777 122.90018463134766
Loss :  1.7267601490020752 2.655578851699829 134.50570678710938
Loss :  1.6843384504318237 2.9263176918029785 148.00022888183594
Loss :  1.718446969985962 2.9992291927337646 151.67990112304688
Loss :  1.7064201831817627 2.4699132442474365 125.20207977294922
Loss :  1.7059423923492432 2.8543360233306885 144.42274475097656
Loss :  1.6876604557037354 3.5553019046783447 179.4527587890625
Loss :  1.6953017711639404 2.5285143852233887 128.1210174560547
Loss :  1.6950827836990356 3.155775547027588 159.48385620117188
Loss :  1.7252099514007568 3.092599391937256 156.3551788330078
Loss :  1.72793710231781 2.8047876358032227 141.96731567382812
Loss :  1.732580542564392 1.8639800548553467 94.93158721923828
  batch 40 loss: 1.732580542564392, 1.8639800548553467, 94.93158721923828
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7129931449890137 2.240654230117798 113.7457046508789
Loss :  1.7060476541519165 2.167111396789551 110.06161499023438
Loss :  1.7007416486740112 2.945157527923584 148.9586181640625
Loss :  1.7071542739868164 3.2404379844665527 163.7290496826172
Loss :  1.6971197128295898 2.924807071685791 147.93746948242188
Loss :  1.7119941711425781 2.405787229537964 122.00135803222656
Loss :  1.7280652523040771 1.8431671857833862 93.88642120361328
Loss :  1.7046196460723877 2.2662289142608643 115.01606750488281
Loss :  1.7355955839157104 2.165576457977295 110.01441955566406
Loss :  1.707703709602356 2.604620933532715 131.93875122070312
Loss :  1.7238408327102661 2.7243082523345947 137.9392547607422
Loss :  1.7214295864105225 2.400195598602295 121.73120880126953
Loss :  1.7110893726348877 2.252870798110962 114.35462951660156
Loss :  1.7253609895706177 2.473621129989624 125.40641784667969
Loss :  1.705111026763916 2.350139856338501 119.21210479736328
Loss :  1.7361286878585815 2.924212694168091 147.94676208496094
Loss :  1.7085773944854736 2.513929605484009 127.40505981445312
Loss :  1.700741171836853 3.1698689460754395 160.19419860839844
Loss :  1.7078279256820679 2.4151980876922607 122.46772766113281
Loss :  1.738918423652649 2.1925690174102783 111.36737060546875
  batch 60 loss: 1.738918423652649, 2.1925690174102783, 111.36737060546875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7008650302886963 2.6844210624694824 135.9219207763672
Loss :  1.7149277925491333 2.718777656555176 137.65382385253906
Loss :  1.7050559520721436 2.6971423625946045 136.5621795654297
Loss :  1.6992690563201904 3.1692419052124023 160.16136169433594
Loss :  1.6892095804214478 1.8111311197280884 92.24576568603516
Loss :  1.7077412605285645 4.312233924865723 217.31944274902344
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.7139668464660645 4.314202308654785 217.42408752441406
Loss :  1.7125523090362549 4.208921909332275 212.1586456298828
Loss :  1.7146066427230835 3.9363248348236084 198.5308380126953
Total LOSS train 134.19007638784555 valid 211.3582534790039
CE LOSS train 1.7118236963565534 valid 0.4286516606807709
Contrastive LOSS train 2.649565045650189 valid 0.9840812087059021
EPOCH 277:
Loss :  1.7218595743179321 2.2123923301696777 112.34147644042969
Loss :  1.7280937433242798 2.6088736057281494 132.17176818847656
Loss :  1.7130128145217896 2.3322529792785645 118.3256607055664
Loss :  1.7163790464401245 2.384634256362915 120.94808959960938
Loss :  1.7236928939819336 2.186065435409546 111.02696228027344
Loss :  1.7066688537597656 2.216830253601074 112.54818725585938
Loss :  1.7238456010818481 2.8862431049346924 146.03599548339844
Loss :  1.712288498878479 2.3750250339508057 120.46353912353516
Loss :  1.7074928283691406 2.7520108222961426 139.3080291748047
Loss :  1.7243973016738892 2.4258313179016113 123.01596069335938
Loss :  1.7028465270996094 2.617856025695801 132.5956573486328
Loss :  1.7020378112792969 3.243238925933838 163.86398315429688
Loss :  1.7014318704605103 2.308880567550659 117.14546203613281
Loss :  1.7049188613891602 3.4006190299987793 171.73587036132812
Loss :  1.7308316230773926 2.227775812149048 113.11962890625
Loss :  1.7287309169769287 3.036073923110962 153.5324249267578
Loss :  1.6995774507522583 2.473012685775757 125.35021209716797
Loss :  1.713334560394287 2.54193115234375 128.8098907470703
Loss :  1.6982451677322388 2.5647735595703125 129.93692016601562
Loss :  1.7287968397140503 2.8032615184783936 141.89186096191406
  batch 20 loss: 1.7287968397140503, 2.8032615184783936, 141.89186096191406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7111132144927979 2.255281686782837 114.4751968383789
Loss :  1.6978942155838013 2.921738862991333 147.7848358154297
Loss :  1.7067989110946655 3.064077138900757 154.91065979003906
Loss :  1.7148624658584595 2.325265407562256 117.9781265258789
Loss :  1.7287755012512207 2.478868246078491 125.67218780517578
Loss :  1.7089873552322388 3.2916030883789062 166.2891387939453
Loss :  1.7133697271347046 2.2720963954925537 115.31819152832031
Loss :  1.711351752281189 2.178086757659912 110.61568450927734
Loss :  1.6861335039138794 2.232943534851074 113.33331298828125
Loss :  1.7274402379989624 2.5672378540039062 130.08934020996094
Loss :  1.6862082481384277 2.997483253479004 151.56036376953125
Loss :  1.7196840047836304 2.360443592071533 119.74185943603516
Loss :  1.7081338167190552 2.1810812950134277 110.76219940185547
Loss :  1.7077852487564087 2.813485860824585 142.38206481933594
Loss :  1.690172791481018 3.250248670578003 164.20260620117188
Loss :  1.6975483894348145 2.510033130645752 127.19920349121094
Loss :  1.697371006011963 2.719119071960449 137.6533203125
Loss :  1.726693034172058 2.222674608230591 112.86042022705078
Loss :  1.729066014289856 2.2390565872192383 113.68189239501953
Loss :  1.733510136604309 2.127030849456787 108.08505249023438
  batch 40 loss: 1.733510136604309, 2.127030849456787, 108.08505249023438
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7143042087554932 2.9736266136169434 150.39564514160156
Loss :  1.7073047161102295 3.9636662006378174 199.89060974121094
Loss :  1.702026605606079 2.9666619300842285 150.03512573242188
Loss :  1.7079991102218628 2.6883420944213867 136.12509155273438
Loss :  1.6978703737258911 2.5540521144866943 129.40048217773438
Loss :  1.712363362312317 2.873081684112549 145.36643981933594
Loss :  1.7287328243255615 3.5244529247283936 177.9513702392578
Loss :  1.7048847675323486 2.9502177238464355 149.21575927734375
Loss :  1.7357674837112427 2.3143763542175293 117.45458984375
Loss :  1.70828115940094 3.3960537910461426 171.51097106933594
Loss :  1.7245068550109863 2.9065468311309814 147.05184936523438
Loss :  1.7225009202957153 2.8204569816589355 142.74534606933594
Loss :  1.71219801902771 2.809847593307495 142.20458984375
Loss :  1.7263813018798828 2.390430450439453 121.2479019165039
Loss :  1.7061185836791992 2.6950416564941406 136.45819091796875
Loss :  1.7369208335876465 2.1733663082122803 110.40523529052734
Loss :  1.7097702026367188 4.001249313354492 201.77224731445312
Loss :  1.7020411491394043 2.3982596397399902 121.61502075195312
Loss :  1.7091946601867676 3.2513327598571777 164.2758331298828
Loss :  1.7399226427078247 2.32041335105896 117.76058959960938
  batch 60 loss: 1.7399226427078247, 2.32041335105896, 117.76058959960938
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7022349834442139 2.82131028175354 142.76776123046875
Loss :  1.7157173156738281 2.7420804500579834 138.81973266601562
Loss :  1.7060538530349731 2.775560140609741 140.4840545654297
Loss :  1.7005436420440674 2.821425199508667 142.7718048095703
Loss :  1.6906789541244507 2.4565439224243164 124.51787567138672
Loss :  1.709492564201355 4.152420997619629 209.33053588867188
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.7156734466552734 4.159306049346924 209.68096923828125
Loss :  1.714253544807434 4.005242824554443 201.9763946533203
Loss :  1.71658194065094 4.011734962463379 202.30332946777344
Total LOSS train 135.70780545748198 valid 205.82280731201172
CE LOSS train 1.7125492444405188 valid 0.429145485162735
Contrastive LOSS train 2.6799051468188946 valid 1.0029337406158447
EPOCH 278:
Loss :  1.7226741313934326 3.1858649253845215 161.0159149169922
Loss :  1.728914499282837 2.4869749546051025 126.0776596069336
Loss :  1.7142497301101685 2.101107597351074 106.7696304321289
Loss :  1.7178897857666016 2.610969066619873 132.26634216308594
Loss :  1.7253245115280151 2.3080170154571533 117.12617492675781
Loss :  1.7085623741149902 2.495415687561035 126.4793472290039
Loss :  1.7256571054458618 2.3834314346313477 120.89722442626953
Loss :  1.71378493309021 2.570176124572754 130.22259521484375
Loss :  1.7093913555145264 3.14666485786438 159.04263305664062
Loss :  1.725999116897583 3.204575538635254 161.95477294921875
Loss :  1.7048933506011963 3.3646414279937744 169.9369659423828
Loss :  1.7039926052093506 3.9688196182250977 200.1449737548828
Loss :  1.7033519744873047 2.2145261764526367 112.4296646118164
Loss :  1.7066079378128052 3.2871735095977783 166.06527709960938
Loss :  1.73259699344635 2.384333848953247 120.94928741455078
Loss :  1.7304269075393677 2.4392716884613037 123.69401550292969
Loss :  1.7014492750167847 2.8724775314331055 145.3253173828125
Loss :  1.7146373987197876 2.9110829830169678 147.26878356933594
Loss :  1.7000229358673096 3.2456858158111572 163.98431396484375
Loss :  1.7298067808151245 2.308682918548584 117.16394805908203
  batch 20 loss: 1.7298067808151245, 2.308682918548584, 117.16394805908203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.712396502494812 2.75073504447937 139.24916076660156
Loss :  1.699362874031067 3.057608127593994 154.5797576904297
Loss :  1.7077163457870483 2.9677679538726807 150.0961151123047
Loss :  1.7159335613250732 2.542954921722412 128.86367797851562
Loss :  1.7294318675994873 2.76540470123291 139.99966430664062
Loss :  1.709342122077942 2.582078695297241 130.81326293945312
Loss :  1.7138921022415161 2.6253957748413086 132.9836883544922
Loss :  1.7117708921432495 2.1203315258026123 107.72834777832031
Loss :  1.686478853225708 3.929640054702759 198.16848754882812
Loss :  1.7278276681900024 2.7714779376983643 140.30172729492188
Loss :  1.6863254308700562 2.7142081260681152 137.396728515625
Loss :  1.7198193073272705 2.5362966060638428 128.53465270996094
Loss :  1.708207130432129 3.1993319988250732 161.6748046875
Loss :  1.7078514099121094 3.1165010929107666 157.5329132080078
Loss :  1.690284252166748 3.666943073272705 185.0374298095703
Loss :  1.6979811191558838 2.582484245300293 130.8221893310547
Loss :  1.6978378295898438 2.3085906505584717 117.12737274169922
Loss :  1.727082371711731 2.388915777206421 121.17286682128906
Loss :  1.729548454284668 3.1515867710113525 159.30889892578125
Loss :  1.7338725328445435 4.264703750610352 214.96905517578125
  batch 40 loss: 1.7338725328445435, 4.264703750610352, 214.96905517578125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7148528099060059 3.1029915809631348 156.86444091796875
Loss :  1.708300232887268 2.056030035018921 104.50979614257812
Loss :  1.70292329788208 2.3515329360961914 119.27957153320312
Loss :  1.709112286567688 2.320500373840332 117.734130859375
Loss :  1.6990389823913574 2.6219770908355713 132.7978973388672
Loss :  1.7134218215942383 3.0679965019226074 155.11325073242188
Loss :  1.7296310663223267 2.8084778785705566 142.1535186767578
Loss :  1.7062275409698486 2.128481864929199 108.13032531738281
Loss :  1.7368521690368652 2.0557758808135986 104.52565002441406
Loss :  1.7093080282211304 2.1823160648345947 110.82511138916016
Loss :  1.7250730991363525 2.3943064212799072 121.44039916992188
Loss :  1.7226226329803467 2.8556320667266846 144.5042266845703
Loss :  1.712424874305725 2.510789632797241 127.25190734863281
Loss :  1.726593255996704 2.958385705947876 149.6458740234375
Loss :  1.7066034078598022 2.8842358589172363 145.91839599609375
Loss :  1.7370271682739258 2.211031198501587 112.28858947753906
Loss :  1.7100882530212402 2.6471645832061768 134.0683135986328
Loss :  1.702301025390625 2.5845859050750732 130.9315948486328
Loss :  1.70950186252594 2.6395061016082764 133.684814453125
Loss :  1.7401169538497925 2.3817059993743896 120.8254165649414
  batch 60 loss: 1.7401169538497925, 2.3817059993743896, 120.8254165649414
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.702466607093811 2.676445245742798 135.5247344970703
Loss :  1.7163513898849487 2.355214834213257 119.47709655761719
Loss :  1.7064963579177856 1.9637835025787354 99.89567565917969
Loss :  1.7011430263519287 2.3181231021881104 117.6072998046875
Loss :  1.6913416385650635 1.894888997077942 96.43579864501953
Loss :  1.7084522247314453 4.359333515167236 219.6751251220703
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.7145015001296997 4.318364143371582 217.63272094726562
Loss :  1.7131409645080566 4.312863826751709 217.3563232421875
Loss :  1.7146382331848145 4.0818281173706055 205.80604553222656
Total LOSS train 136.7170688335712 valid 215.1175537109375
CE LOSS train 1.7134310172154352 valid 0.4286595582962036
Contrastive LOSS train 2.700072752512418 valid 1.0204570293426514
EPOCH 279:
Loss :  1.723097562789917 2.43969988822937 123.70809173583984
Loss :  1.7290713787078857 2.7171428203582764 137.58621215820312
Loss :  1.7144120931625366 3.863631010055542 194.89596557617188
Loss :  1.7178850173950195 2.562251329421997 129.8304443359375
Loss :  1.7255440950393677 2.175438165664673 110.4974594116211
Loss :  1.708796739578247 2.017610788345337 102.5893325805664
Loss :  1.7258366346359253 2.2601981163024902 114.7357406616211
Loss :  1.7142002582550049 2.355562448501587 119.49232482910156
Loss :  1.7101207971572876 2.4774117469787598 125.5807113647461
Loss :  1.726523756980896 2.328145742416382 118.1338119506836
Loss :  1.7056325674057007 2.713634967803955 137.3873748779297
Loss :  1.7049293518066406 2.915095090866089 147.45968627929688
Loss :  1.704068660736084 2.514723777770996 127.44026184082031
Loss :  1.707417368888855 3.537564277648926 178.58563232421875
Loss :  1.7330801486968994 2.5749433040618896 130.48023986816406
Loss :  1.7309670448303223 2.379868745803833 120.72440338134766
Loss :  1.7024743556976318 2.407905340194702 122.09774017333984
Loss :  1.7155969142913818 2.5801076889038086 130.72097778320312
Loss :  1.7011005878448486 2.5274922847747803 128.07571411132812
Loss :  1.7299283742904663 3.0754590034484863 155.5028839111328
  batch 20 loss: 1.7299283742904663, 3.0754590034484863, 155.5028839111328
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7132095098495483 2.4817419052124023 125.80030822753906
Loss :  1.7004525661468506 2.455280303955078 124.46446990966797
Loss :  1.7088929414749146 2.8176681995391846 142.59230041503906
Loss :  1.7167493104934692 4.209102153778076 212.17185974121094
Loss :  1.7302813529968262 2.3704886436462402 120.25470733642578
Loss :  1.7104686498641968 2.3379831314086914 118.60962677001953
Loss :  1.7148041725158691 2.2924695014953613 116.3382797241211
Loss :  1.7125539779663086 2.3578810691833496 119.60660552978516
Loss :  1.6877920627593994 2.2770073413848877 115.53815460205078
Loss :  1.7282730340957642 2.527960777282715 128.12631225585938
Loss :  1.6880722045898438 2.4741032123565674 125.39323425292969
Loss :  1.7208774089813232 1.8939249515533447 96.41712951660156
Loss :  1.7094649076461792 2.7983131408691406 141.62510681152344
Loss :  1.708971619606018 2.393000602722168 121.35900115966797
Loss :  1.6915582418441772 2.8004369735717773 141.71340942382812
Loss :  1.6989870071411133 2.4420275688171387 123.80036926269531
Loss :  1.6987839937210083 2.475228786468506 125.46022033691406
Loss :  1.727530837059021 2.479376792907715 125.69637298583984
Loss :  1.730090618133545 2.0917086601257324 106.31552124023438
Loss :  1.7345129251480103 2.1134531497955322 107.40717315673828
  batch 40 loss: 1.7345129251480103, 2.1134531497955322, 107.40717315673828
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7155295610427856 3.0527195930480957 154.3515167236328
Loss :  1.7087308168411255 1.9515838623046875 99.28792572021484
Loss :  1.7032372951507568 3.1065893173217773 157.03269958496094
Loss :  1.709532618522644 2.4688568115234375 125.15237426757812
Loss :  1.6995189189910889 3.067535161972046 155.07627868652344
Loss :  1.7138928174972534 2.207117795944214 112.06977844238281
Loss :  1.7297687530517578 2.0415425300598145 103.80689239501953
Loss :  1.7067489624023438 2.3772799968719482 120.57074737548828
Loss :  1.736673355102539 2.5178167819976807 127.62751007080078
Loss :  1.7096213102340698 2.34647798538208 119.03351593017578
Loss :  1.7254886627197266 3.031175374984741 153.2842559814453
Loss :  1.7230368852615356 3.1735665798187256 160.4013671875
Loss :  1.7128714323043823 1.9684791564941406 100.13682556152344
Loss :  1.726859211921692 2.396348714828491 121.54429626464844
Loss :  1.7071739435195923 3.1367013454437256 158.542236328125
Loss :  1.737166404724121 3.7408270835876465 188.7785186767578
Loss :  1.7107064723968506 2.885423183441162 145.98187255859375
Loss :  1.7032026052474976 2.9085962772369385 147.1330108642578
Loss :  1.7103972434997559 2.8593153953552246 144.67617797851562
Loss :  1.7407193183898926 3.339069128036499 168.69418334960938
  batch 60 loss: 1.7407193183898926, 3.339069128036499, 168.69418334960938
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7037932872772217 2.6090304851531982 132.1553192138672
Loss :  1.7173936367034912 3.454876661300659 174.4612274169922
Loss :  1.7078173160552979 3.019570827484131 152.68637084960938
Loss :  1.7025206089019775 2.471766948699951 125.2908706665039
Loss :  1.6928733587265015 1.725124716758728 87.94910430908203
Loss :  1.7122361660003662 4.290650844573975 216.24478149414062
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.7180818319320679 4.347052574157715 219.07070922851562
Loss :  1.7168346643447876 4.278846740722656 215.65916442871094
Loss :  1.7183374166488647 4.169079780578613 210.17233276367188
Total LOSS train 132.7683084341196 valid 215.28674697875977
CE LOSS train 1.7141274745647723 valid 0.4295843541622162
Contrastive LOSS train 2.621083617210388 valid 1.0422699451446533
EPOCH 280:
Loss :  1.7243179082870483 2.743882894515991 138.91845703125
Loss :  1.730291724205017 2.5064878463745117 127.0546875
Loss :  1.7158392667770386 3.0213534832000732 152.78350830078125
Loss :  1.7194054126739502 2.496413469314575 126.54007720947266
Loss :  1.7268136739730835 2.5143678188323975 127.44520568847656
Loss :  1.7102779150009155 2.7518019676208496 139.30038452148438
Loss :  1.7269225120544434 2.20446515083313 111.95018005371094
Loss :  1.7152321338653564 2.332353353500366 118.33290100097656
Loss :  1.7109370231628418 2.5339157581329346 128.40672302246094
Loss :  1.7271808385849 2.295672655105591 116.51081085205078
Loss :  1.7065536975860596 2.3718268871307373 120.29789733886719
Loss :  1.7058995962142944 2.189643383026123 111.18807220458984
Loss :  1.705121397972107 2.137002468109131 108.55524444580078
Loss :  1.7084252834320068 2.458160638809204 124.61646270751953
Loss :  1.7339063882827759 2.320195436477661 117.74368286132812
Loss :  1.7317066192626953 2.468569278717041 125.16017150878906
Loss :  1.7039868831634521 2.544525623321533 128.93026733398438
Loss :  1.7168081998825073 2.9468295574188232 149.05828857421875
Loss :  1.7022000551223755 2.06656813621521 105.03060913085938
Loss :  1.7314422130584717 2.2784414291381836 115.65351867675781
  batch 20 loss: 1.7314422130584717, 2.2784414291381836, 115.65351867675781
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7143722772598267 2.2034780979156494 111.88827514648438
Loss :  1.7017728090286255 2.500523328781128 126.72794342041016
Loss :  1.710252285003662 2.1170413494110107 107.56231689453125
Loss :  1.7181239128112793 2.7144179344177246 137.43902587890625
Loss :  1.7313897609710693 4.054204940795898 204.44163513183594
Loss :  1.712026596069336 2.3688673973083496 120.1553955078125
Loss :  1.7164297103881836 2.3578097820281982 119.60691833496094
Loss :  1.7141125202178955 3.0812225341796875 155.77523803710938
Loss :  1.6898372173309326 2.3790528774261475 120.6424789428711
Loss :  1.7297148704528809 2.4936585426330566 126.41264343261719
Loss :  1.6901004314422607 2.7111527919769287 137.2477264404297
Loss :  1.7222933769226074 2.4074859619140625 122.09658813476562
Loss :  1.7109354734420776 2.4041361808776855 121.9177474975586
Loss :  1.7104727029800415 3.2167611122131348 162.5485382080078
Loss :  1.6932332515716553 3.820188283920288 192.70265197753906
Loss :  1.7005606889724731 2.6575634479522705 134.5787353515625
Loss :  1.700134515762329 2.738300085067749 138.61514282226562
Loss :  1.7286850214004517 2.903719663619995 146.9146728515625
Loss :  1.7312815189361572 3.118354558944702 157.6490020751953
Loss :  1.7356131076812744 2.512913942337036 127.38130950927734
  batch 40 loss: 1.7356131076812744, 2.512913942337036, 127.38130950927734
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.717028021812439 2.5863161087036133 131.0328369140625
Loss :  1.7105016708374023 2.6257197856903076 132.99649047851562
Loss :  1.7054111957550049 2.3943428993225098 121.42256164550781
Loss :  1.7117832899093628 2.8889238834381104 146.15797424316406
Loss :  1.7019507884979248 1.9704958200454712 100.22673797607422
Loss :  1.71600341796875 3.119457483291626 157.68887329101562
Loss :  1.7314658164978027 2.7301409244537354 138.23851013183594
Loss :  1.7087466716766357 3.3486149311065674 169.1394805908203
Loss :  1.7383153438568115 2.6545183658599854 134.4642333984375
Loss :  1.7119157314300537 2.81520676612854 142.47225952148438
Loss :  1.7271513938903809 2.595409631729126 131.4976348876953
Loss :  1.7241796255111694 3.413339376449585 172.39114379882812
Loss :  1.7149837017059326 3.1019833087921143 156.81414794921875
Loss :  1.7283718585968018 3.421488046646118 172.8027801513672
Loss :  1.709054946899414 2.1951189041137695 111.46499633789062
Loss :  1.7388036251068115 2.276689291000366 115.5732650756836
Loss :  1.7125043869018555 2.467190980911255 125.07205200195312
Loss :  1.7048431634902954 2.294562578201294 116.43296813964844
Loss :  1.7119089365005493 2.8642804622650146 144.92593383789062
Loss :  1.7418205738067627 2.440573215484619 123.7704849243164
  batch 60 loss: 1.7418205738067627, 2.440573215484619, 123.7704849243164
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7052537202835083 2.327375888824463 118.07405090332031
Loss :  1.7189157009124756 2.62711238861084 133.0745391845703
Loss :  1.7092605829238892 2.2810044288635254 115.75948333740234
Loss :  1.7039639949798584 2.8416836261749268 143.78814697265625
Loss :  1.694374918937683 2.6096906661987305 132.1789093017578
Loss :  1.7106958627700806 4.064670085906982 204.94419860839844
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.7165122032165527 4.107122898101807 207.07264709472656
Loss :  1.7149080038070679 3.9773049354553223 200.5801544189453
Loss :  1.718303918838501 3.8041000366210938 191.92330932617188
Total LOSS train 133.12679431621845 valid 201.13007736206055
CE LOSS train 1.715586521075322 valid 0.42957597970962524
Contrastive LOSS train 2.628224150951092 valid 0.9510250091552734
EPOCH 281:
Loss :  1.7250531911849976 4.134318828582764 208.4409942626953
Loss :  1.7307288646697998 2.7642946243286133 139.94546508789062
Loss :  1.7162586450576782 2.62903094291687 133.16781616210938
Loss :  1.7197329998016357 2.4980452060699463 126.62199401855469
Loss :  1.7272014617919922 2.725806951522827 138.01754760742188
Loss :  1.7103848457336426 2.97516131401062 150.4684600830078
Loss :  1.7268259525299072 3.812652349472046 192.35943603515625
Loss :  1.715315341949463 3.556502103805542 179.54042053222656
Loss :  1.7110614776611328 2.185364246368408 110.9792709350586
Loss :  1.7273579835891724 2.171877384185791 110.32122802734375
Loss :  1.706251621246338 2.99625563621521 151.51902770996094
Loss :  1.7057212591171265 2.863886594772339 144.90005493164062
Loss :  1.7048654556274414 3.089141845703125 156.16195678710938
Loss :  1.7081241607666016 2.703028678894043 136.85955810546875
Loss :  1.7338613271713257 2.9982426166534424 151.64598083496094
Loss :  1.7314475774765015 2.8424317836761475 143.85302734375
Loss :  1.7034046649932861 2.815892457962036 142.49801635742188
Loss :  1.716347575187683 2.5061404705047607 127.02336883544922
Loss :  1.7016770839691162 2.7834577560424805 140.8745574951172
Loss :  1.7311456203460693 2.2465429306030273 114.05829620361328
  batch 20 loss: 1.7311456203460693, 2.2465429306030273, 114.05829620361328
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.713820219039917 2.2329318523406982 113.36041259765625
Loss :  1.7011641263961792 2.4493408203125 124.16820526123047
Loss :  1.7095160484313965 2.359786033630371 119.69882202148438
Loss :  1.7175668478012085 2.509453296661377 127.19023132324219
Loss :  1.7312614917755127 2.7140393257141113 137.4332275390625
Loss :  1.711597204208374 4.044114589691162 203.91732788085938
Loss :  1.716259241104126 2.5199389457702637 127.71321105957031
Loss :  1.7141631841659546 2.409379243850708 122.1831283569336
Loss :  1.6897001266479492 2.6793558597564697 135.65748596191406
Loss :  1.7300549745559692 2.5849294662475586 130.97653198242188
Loss :  1.6899852752685547 2.574660062789917 130.42298889160156
Loss :  1.7221821546554565 2.66377854347229 134.91111755371094
Loss :  1.7108972072601318 2.5042757987976074 126.92469024658203
Loss :  1.7103749513626099 2.470179796218872 125.2193603515625
Loss :  1.6934586763381958 2.6636276245117188 134.87484741210938
Loss :  1.7004222869873047 3.2103052139282227 162.21568298339844
Loss :  1.7003097534179688 2.458430528640747 124.62183380126953
Loss :  1.7286262512207031 2.7071990966796875 137.0885772705078
Loss :  1.7311159372329712 2.2878634929656982 116.1242904663086
Loss :  1.7354437112808228 2.882157325744629 145.8433074951172
  batch 40 loss: 1.7354437112808228, 2.882157325744629, 145.8433074951172
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7167502641677856 3.045222282409668 153.9778594970703
Loss :  1.7098441123962402 2.729282855987549 138.17398071289062
Loss :  1.7047276496887207 3.423356056213379 172.87252807617188
Loss :  1.7109673023223877 2.7756714820861816 140.49453735351562
Loss :  1.701149821281433 2.0277011394500732 103.0862045288086
Loss :  1.715356469154358 2.4799487590789795 125.7127914428711
Loss :  1.7310359477996826 2.6015870571136475 131.8103790283203
Loss :  1.7081667184829712 2.9536569118499756 149.39100646972656
Loss :  1.7378792762756348 4.028683185577393 203.1720428466797
Loss :  1.7114439010620117 2.55379581451416 129.40122985839844
Loss :  1.7266614437103271 2.3793797492980957 120.69564819335938
Loss :  1.7245250940322876 2.6873996257781982 136.09449768066406
Loss :  1.714617133140564 2.953899621963501 149.4095916748047
Loss :  1.7284204959869385 2.5299651622772217 128.2266845703125
Loss :  1.7087903022766113 2.5974011421203613 131.57884216308594
Loss :  1.7385785579681396 2.5891382694244385 131.1954803466797
Loss :  1.712544322013855 2.3282155990600586 118.12332916259766
Loss :  1.7049115896224976 2.2493736743927 114.17359161376953
Loss :  1.7120144367218018 2.572007894515991 130.31240844726562
Loss :  1.741694688796997 3.049469470977783 154.21517944335938
  batch 60 loss: 1.741694688796997, 3.049469470977783, 154.21517944335938
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7054269313812256 2.3292205333709717 118.16645812988281
Loss :  1.7189247608184814 2.1676416397094727 110.10100555419922
Loss :  1.7093303203582764 2.107592821121216 107.0889663696289
Loss :  1.7049040794372559 2.5709664821624756 130.25323486328125
Loss :  1.6954262256622314 1.720455288887024 87.71819305419922
Loss :  1.7127799987792969 4.3142900466918945 217.4272918701172
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.7185020446777344 4.350672245025635 219.2521209716797
Loss :  1.7171095609664917 4.267682075500488 215.10121154785156
Loss :  1.7192790508270264 4.095028877258301 206.47073364257812
Total LOSS train 136.84996044452373 valid 214.56283950805664
CE LOSS train 1.7154581326704759 valid 0.4298197627067566
Contrastive LOSS train 2.7026900639900795 valid 1.0237572193145752
EPOCH 282:
Loss :  1.7259656190872192 2.12652850151062 108.0523910522461
Loss :  1.7318090200424194 2.728602409362793 138.16192626953125
Loss :  1.7174007892608643 2.725903272628784 138.0125732421875
Loss :  1.7210832834243774 2.4205331802368164 122.74774169921875
Loss :  1.7283865213394165 3.2574784755706787 164.60231018066406
Loss :  1.7120352983474731 2.129974126815796 108.21073913574219
Loss :  1.7280166149139404 3.099161386489868 156.6860809326172
Loss :  1.7165484428405762 1.8690756559371948 95.17032623291016
Loss :  1.7120146751403809 2.4989025592803955 126.65714263916016
Loss :  1.7278618812561035 2.372413158416748 120.34851837158203
Loss :  1.707215428352356 3.0523593425750732 154.32518005371094
Loss :  1.7065906524658203 2.5077106952667236 127.09212493896484
Loss :  1.7054916620254517 2.383087158203125 120.85984802246094
Loss :  1.7085247039794922 2.5388128757476807 128.649169921875
Loss :  1.733957052230835 2.5807652473449707 130.77223205566406
Loss :  1.7318851947784424 2.444059133529663 123.93484497070312
Loss :  1.7039178609848022 2.388587713241577 121.13330078125
Loss :  1.7167470455169678 2.4451804161071777 123.97576141357422
Loss :  1.7021135091781616 2.353616952896118 119.3829574584961
Loss :  1.731701135635376 2.5820956230163574 130.83648681640625
  batch 20 loss: 1.731701135635376, 2.5820956230163574, 130.83648681640625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.714342713356018 2.3220107555389404 117.81488037109375
Loss :  1.7018868923187256 2.310579538345337 117.23086547851562
Loss :  1.7104414701461792 2.263537645339966 114.88732147216797
Loss :  1.7181341648101807 2.3152434825897217 117.48030853271484
Loss :  1.7316350936889648 2.232590675354004 113.36116790771484
Loss :  1.7120616436004639 2.311276912689209 117.2759017944336
Loss :  1.7165966033935547 2.5166335105895996 127.54827117919922
Loss :  1.7142926454544067 2.5743603706359863 130.43231201171875
Loss :  1.690103530883789 2.940922260284424 148.73622131347656
Loss :  1.7298728227615356 2.585753917694092 131.01756286621094
Loss :  1.6905001401901245 2.4126319885253906 122.32209777832031
Loss :  1.7225122451782227 2.9726719856262207 150.3561248779297
Loss :  1.7111976146697998 3.619683265686035 182.6953582763672
Loss :  1.7109633684158325 2.923229694366455 147.87245178222656
Loss :  1.6940724849700928 3.3440349102020264 168.8958282470703
Loss :  1.7012773752212524 2.381157875061035 120.75917053222656
Loss :  1.7009658813476562 2.3296775817871094 118.18484497070312
Loss :  1.7291392087936401 2.474076986312866 125.43299102783203
Loss :  1.7314609289169312 2.494743824005127 126.4686508178711
Loss :  1.735827922821045 2.1934821605682373 111.4099349975586
  batch 40 loss: 1.735827922821045, 2.1934821605682373, 111.4099349975586
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7170664072036743 2.4765379428863525 125.54396057128906
Loss :  1.710522174835205 2.425685405731201 122.99479675292969
Loss :  1.7052017450332642 2.4894251823425293 126.17646026611328
Loss :  1.7113487720489502 2.3752052783966064 120.47161102294922
Loss :  1.7013150453567505 2.8617396354675293 144.7882843017578
Loss :  1.715493083000183 2.763058662414551 139.86843872070312
Loss :  1.7311760187149048 2.9418089389801025 148.82162475585938
Loss :  1.7079228162765503 2.6312832832336426 133.27207946777344
Loss :  1.737991452217102 3.7411949634552 188.79774475097656
Loss :  1.7111597061157227 2.542526960372925 128.83750915527344
Loss :  1.726607322692871 3.688833713531494 186.1682891845703
Loss :  1.7242857217788696 2.2841432094573975 115.93144989013672
Loss :  1.7144356966018677 2.1616668701171875 109.79778289794922
Loss :  1.7279777526855469 2.4340012073516846 123.42803955078125
Loss :  1.7084619998931885 2.9672317504882812 150.07005310058594
Loss :  1.738179087638855 2.590125799179077 131.2444610595703
Loss :  1.7120270729064941 2.6714823246002197 135.2861328125
Loss :  1.704464316368103 2.39890193939209 121.64956665039062
Loss :  1.7115366458892822 2.739901542663574 138.70660400390625
Loss :  1.7414700984954834 2.2321276664733887 113.34785461425781
  batch 60 loss: 1.7414700984954834, 2.2321276664733887, 113.34785461425781
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7049510478973389 2.664102077484131 134.91006469726562
Loss :  1.7185335159301758 2.3521010875701904 119.32359313964844
Loss :  1.709033727645874 2.2697594165802 115.1969985961914
Loss :  1.7038061618804932 2.412468194961548 122.32721710205078
Loss :  1.6942939758300781 2.7139689922332764 137.3927459716797
Loss :  1.710908055305481 4.337340354919434 218.5779266357422
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.7166333198547363 4.362700462341309 219.85165405273438
Loss :  1.7152819633483887 4.234637260437012 213.4471435546875
Loss :  1.7176085710525513 4.144707679748535 208.95298767089844
Total LOSS train 130.833035160945 valid 215.20742797851562
CE LOSS train 1.715781730871934 valid 0.4294021427631378
Contrastive LOSS train 2.582345065703759 valid 1.0361769199371338
EPOCH 283:
Loss :  1.7251709699630737 2.7931675910949707 141.38356018066406
Loss :  1.731221318244934 2.592947483062744 131.3785858154297
Loss :  1.7169126272201538 1.9941792488098145 101.42587280273438
Loss :  1.7204030752182007 2.9862873554229736 151.03475952148438
Loss :  1.727730631828308 1.9429643154144287 98.87594604492188
Loss :  1.7114461660385132 2.1155169010162354 107.48729705810547
Loss :  1.7279318571090698 2.550647020339966 129.26028442382812
Loss :  1.7165095806121826 2.1548304557800293 109.4580307006836
Loss :  1.7123339176177979 2.301070213317871 116.7658462524414
Loss :  1.7283351421356201 3.771667003631592 190.31167602539062
Loss :  1.70806884765625 2.8943939208984375 146.42776489257812
Loss :  1.7073523998260498 2.5663485527038574 130.0247802734375
Loss :  1.7064486742019653 2.422375202178955 122.82521057128906
Loss :  1.7096712589263916 2.5996856689453125 131.69395446777344
Loss :  1.734649419784546 3.138559103012085 158.66259765625
Loss :  1.7325972318649292 2.4054911136627197 122.00715637207031
Loss :  1.70518958568573 2.889765977859497 146.1934814453125
Loss :  1.7178237438201904 2.203718423843384 111.90374755859375
Loss :  1.7038277387619019 2.5663228034973145 130.0199737548828
Loss :  1.7323837280273438 2.245652914047241 114.01502990722656
  batch 20 loss: 1.7323837280273438, 2.245652914047241, 114.01502990722656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7153692245483398 2.512953519821167 127.36304473876953
Loss :  1.7030870914459229 2.4822769165039062 125.81693267822266
Loss :  1.7113386392593384 2.3958024978637695 121.50146484375
Loss :  1.719069242477417 2.5618295669555664 129.810546875
Loss :  1.7323530912399292 2.521538257598877 127.80926513671875
Loss :  1.71320641040802 3.0878677368164062 156.10659790039062
Loss :  1.7176012992858887 2.3587238788604736 119.65379333496094
Loss :  1.7154432535171509 2.720984697341919 137.76467895507812
Loss :  1.6915067434310913 2.9760243892669678 150.49273681640625
Loss :  1.7308462858200073 3.319265127182007 167.69410705566406
Loss :  1.6917089223861694 2.6172945499420166 132.5564422607422
Loss :  1.72319495677948 2.3222644329071045 117.83641815185547
Loss :  1.7120232582092285 2.3039157390594482 116.90780639648438
Loss :  1.7116496562957764 2.3505661487579346 119.23995208740234
Loss :  1.6951478719711304 3.2829184532165527 165.841064453125
Loss :  1.7022680044174194 2.6030232906341553 131.8534393310547
Loss :  1.7020306587219238 2.376359701156616 120.52001953125
Loss :  1.7297406196594238 2.316180944442749 117.53878784179688
Loss :  1.7321140766143799 2.7062864303588867 137.0464324951172
Loss :  1.7364684343338013 2.2555112838745117 114.51203155517578
  batch 40 loss: 1.7364684343338013, 2.2555112838745117, 114.51203155517578
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7180358171463013 2.5942139625549316 131.42872619628906
Loss :  1.711422085762024 2.3207242488861084 117.74763488769531
Loss :  1.7062848806381226 2.7591121196746826 139.66189575195312
Loss :  1.7124855518341064 2.4814295768737793 125.78396606445312
Loss :  1.702769160270691 3.5419843196868896 178.80198669433594
Loss :  1.7168596982955933 2.1345267295837402 108.44319152832031
Loss :  1.731933355331421 2.1821556091308594 110.83971405029297
Loss :  1.7095775604248047 2.4495208263397217 124.18562316894531
Loss :  1.7387608289718628 2.216517448425293 112.56463623046875
Loss :  1.71217679977417 2.7639150619506836 139.90792846679688
Loss :  1.7275233268737793 2.2840287685394287 115.92896270751953
Loss :  1.7249975204467773 3.0679867267608643 155.12432861328125
Loss :  1.7151963710784912 2.0927367210388184 106.3520278930664
Loss :  1.7286322116851807 2.351353406906128 119.29630279541016
Loss :  1.7094457149505615 2.3944544792175293 121.43216705322266
Loss :  1.7386441230773926 2.159818410873413 109.72956848144531
Loss :  1.7128640413284302 2.242675304412842 113.84663391113281
Loss :  1.7054498195648193 2.5428390502929688 128.84739685058594
Loss :  1.7124289274215698 2.881288766860962 145.77687072753906
Loss :  1.7418287992477417 2.2946994304656982 116.47679901123047
  batch 60 loss: 1.7418287992477417, 2.2946994304656982, 116.47679901123047
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7058391571044922 2.459139823913574 124.66283416748047
Loss :  1.7192872762680054 1.962814450263977 99.8600082397461
Loss :  1.709797739982605 2.5530319213867188 129.36138916015625
Loss :  1.7047065496444702 2.4026451110839844 121.83695983886719
Loss :  1.6952567100524902 1.8841195106506348 95.90123748779297
Loss :  1.7128831148147583 4.31233549118042 217.3296661376953
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.718611478805542 4.31718635559082 217.5779266357422
Loss :  1.7172938585281372 4.22169303894043 212.8019561767578
Loss :  1.7198525667190552 4.078527450561523 205.64622497558594
Total LOSS train 128.043383202186 valid 213.3389434814453
CE LOSS train 1.7165289181929368 valid 0.4299631416797638
Contrastive LOSS train 2.5265370864134566 valid 1.0196318626403809
EPOCH 284:
Loss :  1.7257825136184692 2.8581204414367676 144.63180541992188
Loss :  1.7317554950714111 2.9550564289093018 149.4845733642578
Loss :  1.717502236366272 2.519026279449463 127.66881561279297
Loss :  1.7210440635681152 2.3640520572662354 119.92365264892578
Loss :  1.7281956672668457 2.066481113433838 105.05225372314453
Loss :  1.7121357917785645 2.401506185531616 121.78744506835938
Loss :  1.7281749248504639 2.5740835666656494 130.43235778808594
Loss :  1.7169948816299438 2.368009090423584 120.11744689941406
Loss :  1.712843894958496 2.897555112838745 146.59060668945312
Loss :  1.7286455631256104 2.5113279819488525 127.2950439453125
Loss :  1.7085025310516357 2.1684930324554443 110.1331558227539
Loss :  1.7078553438186646 3.1809074878692627 160.75323486328125
Loss :  1.7070667743682861 2.359896659851074 119.701904296875
Loss :  1.710184097290039 3.5423495769500732 178.82766723632812
Loss :  1.7351770401000977 3.258965253829956 164.68344116210938
Loss :  1.7328720092773438 2.568375825881958 130.15167236328125
Loss :  1.7058117389678955 2.5647358894348145 129.94261169433594
Loss :  1.7183771133422852 2.9278948307037354 148.11312866210938
Loss :  1.704267144203186 2.0314345359802246 103.27599334716797
Loss :  1.7328804731369019 4.073331832885742 205.39947509765625
  batch 20 loss: 1.7328804731369019, 4.073331832885742, 205.39947509765625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7160942554473877 2.660310983657837 134.7316436767578
Loss :  1.70383882522583 2.1950111389160156 111.45439910888672
Loss :  1.711981177330017 2.557691812515259 129.59657287597656
Loss :  1.7195134162902832 2.9375007152557373 148.59454345703125
Loss :  1.732797622680664 2.6106367111206055 132.26463317871094
Loss :  1.7135093212127686 2.3959462642669678 121.51081848144531
Loss :  1.7178207635879517 2.3513236045837402 119.28399658203125
Loss :  1.7153400182724 2.85679030418396 144.5548553466797
Loss :  1.6914156675338745 3.488603115081787 176.12158203125
Loss :  1.73041570186615 2.5460474491119385 129.0327911376953
Loss :  1.691380262374878 2.9501595497131348 149.19935607910156
Loss :  1.7228469848632812 3.209357500076294 162.19073486328125
Loss :  1.7119040489196777 2.441818952560425 123.8028564453125
Loss :  1.711559534072876 2.566964864730835 130.05979919433594
Loss :  1.694669246673584 2.9281318187713623 148.10125732421875
Loss :  1.7018743753433228 2.9843192100524902 150.91783142089844
Loss :  1.7016056776046753 3.5926363468170166 181.3334197998047
Loss :  1.7295866012573242 2.401057481765747 121.78245544433594
Loss :  1.7319551706314087 2.3612782955169678 119.79586791992188
Loss :  1.7364565134048462 2.294696569442749 116.47128295898438
  batch 40 loss: 1.7364565134048462, 2.294696569442749, 116.47128295898438
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7178572416305542 2.346492290496826 119.04247283935547
Loss :  1.7110196352005005 2.163405179977417 109.88127899169922
Loss :  1.7059249877929688 2.2024614810943604 111.8290023803711
Loss :  1.712250828742981 2.4356043338775635 123.49246978759766
Loss :  1.7024123668670654 3.216644287109375 162.5346221923828
Loss :  1.716428279876709 2.2655930519104004 114.99608612060547
Loss :  1.7318768501281738 2.947380304336548 149.10089111328125
Loss :  1.7092502117156982 2.4150302410125732 122.46076202392578
Loss :  1.7385494709014893 2.4618566036224365 124.83137512207031
Loss :  1.712003231048584 2.952012300491333 149.3126220703125
Loss :  1.7270681858062744 2.637848377227783 133.61949157714844
Loss :  1.7248543500900269 2.5422792434692383 128.8388214111328
Loss :  1.7150583267211914 2.475081443786621 125.46913146972656
Loss :  1.7284730672836304 2.521287679672241 127.79285430908203
Loss :  1.7093594074249268 2.673863172531128 135.40252685546875
Loss :  1.7386202812194824 3.468747615814209 175.17599487304688
Loss :  1.7125153541564941 3.2348897457122803 163.45700073242188
Loss :  1.7051563262939453 2.3052332401275635 116.9668197631836
Loss :  1.712077021598816 2.345597982406616 118.99197387695312
Loss :  1.7415540218353271 2.0937418937683105 106.42864990234375
  batch 60 loss: 1.7415540218353271, 2.0937418937683105, 106.42864990234375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7052884101867676 2.3889238834381104 121.1514892578125
Loss :  1.7186009883880615 2.044205904006958 103.92889404296875
Loss :  1.709125280380249 2.425424814224243 122.98036193847656
Loss :  1.7038878202438354 2.457899570465088 124.59886932373047
Loss :  1.694537878036499 2.797476053237915 141.56834411621094
Loss :  1.7091290950775146 4.336859703063965 218.55210876464844
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.7148536443710327 4.424487113952637 222.939208984375
Loss :  1.7128132581710815 4.253054618835449 214.36553955078125
Loss :  1.7182414531707764 4.129077911376953 208.17213439941406
Total LOSS train 134.28645829420824 valid 216.0072479248047
CE LOSS train 1.7165597585531382 valid 0.4295603632926941
Contrastive LOSS train 2.651397947164682 valid 1.0322694778442383
EPOCH 285:
Loss :  1.7253233194351196 3.7262368202209473 188.03717041015625
Loss :  1.7312390804290771 2.9182403087615967 147.64324951171875
Loss :  1.7170830965042114 2.5566580295562744 129.54998779296875
Loss :  1.720362663269043 3.1281583309173584 158.12828063964844
Loss :  1.7279711961746216 2.775935649871826 140.52474975585938
Loss :  1.7115750312805176 2.136789560317993 108.55105590820312
Loss :  1.7282010316848755 2.3805360794067383 120.7550048828125
Loss :  1.7166519165039062 3.0127601623535156 152.35464477539062
Loss :  1.7124041318893433 2.143868923187256 108.90584564208984
Loss :  1.7284518480300903 2.134521007537842 108.45450592041016
Loss :  1.7082059383392334 2.951779842376709 149.2971954345703
Loss :  1.7077348232269287 2.5924923419952393 131.3323516845703
Loss :  1.707114815711975 1.8613277673721313 94.77350616455078
Loss :  1.710340142250061 2.4064507484436035 122.03288269042969
Loss :  1.734910249710083 2.7855424880981445 141.0120391845703
Loss :  1.7328795194625854 2.3764986991882324 120.55781555175781
Loss :  1.7052581310272217 3.004258871078491 151.91819763183594
Loss :  1.7180665731430054 2.366705894470215 120.05335998535156
Loss :  1.7038358449935913 2.2463982105255127 114.02374267578125
Loss :  1.7323334217071533 2.4349710941314697 123.48088836669922
  batch 20 loss: 1.7323334217071533, 2.4349710941314697, 123.48088836669922
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.715576410293579 2.3416011333465576 118.7956314086914
Loss :  1.7033493518829346 2.4438650608062744 123.89659881591797
Loss :  1.7115721702575684 2.3411977291107178 118.77145385742188
Loss :  1.719140887260437 2.3021857738494873 116.82843017578125
Loss :  1.7322379350662231 3.100494146347046 156.75694274902344
Loss :  1.7132209539413452 2.32966947555542 118.19669342041016
Loss :  1.717389702796936 2.5381853580474854 128.6266632080078
Loss :  1.715127944946289 2.499690532684326 126.69965362548828
Loss :  1.6907190084457397 2.361982583999634 119.78984832763672
Loss :  1.7302589416503906 2.4430313110351562 123.88182067871094
Loss :  1.6908462047576904 2.6128993034362793 132.33580017089844
Loss :  1.7229328155517578 2.8586111068725586 144.6534881591797
Loss :  1.7116643190383911 2.895609140396118 146.49212646484375
Loss :  1.7114224433898926 2.3016624450683594 116.79454803466797
Loss :  1.6946063041687012 2.6905035972595215 136.21978759765625
Loss :  1.7017993927001953 3.6585984230041504 184.6317138671875
Loss :  1.701724648475647 2.6620776653289795 134.80560302734375
Loss :  1.7299696207046509 2.3783280849456787 120.64637756347656
Loss :  1.7322787046432495 2.385152578353882 120.98990631103516
Loss :  1.7368077039718628 2.2137792110443115 112.42576599121094
  batch 40 loss: 1.7368077039718628, 2.2137792110443115, 112.42576599121094
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7182921171188354 2.4153833389282227 122.48745727539062
Loss :  1.711702823638916 2.373298168182373 120.3766098022461
Loss :  1.706571340560913 2.7252495288848877 137.96905517578125
Loss :  1.7126665115356445 3.355429172515869 169.48411560058594
Loss :  1.7029423713684082 3.901500940322876 196.77798461914062
Loss :  1.716896891593933 2.473644495010376 125.39912414550781
Loss :  1.7322585582733154 2.37168288230896 120.31640625
Loss :  1.709539771080017 2.423875093460083 122.9032974243164
Loss :  1.7391797304153442 2.4986836910247803 126.6733627319336
Loss :  1.7125588655471802 2.422579526901245 122.84153747558594
Loss :  1.7276664972305298 2.6656625270843506 135.0107879638672
Loss :  1.72545325756073 2.3004934787750244 116.75012969970703
Loss :  1.715556263923645 2.8109726905822754 142.26419067382812
Loss :  1.7289597988128662 2.3619236946105957 119.82514190673828
Loss :  1.709519386291504 2.1085236072540283 107.13569641113281
Loss :  1.7389336824417114 2.269167184829712 115.19729614257812
Loss :  1.712959885597229 2.6192755699157715 132.67674255371094
Loss :  1.705386996269226 2.3935787677764893 121.38432312011719
Loss :  1.7123578786849976 2.696585178375244 136.54161071777344
Loss :  1.7418261766433716 2.3081681728363037 117.15023803710938
  batch 60 loss: 1.7418261766433716, 2.3081681728363037, 117.15023803710938
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7057533264160156 2.2297873497009277 113.19511413574219
Loss :  1.7192506790161133 3.790802478790283 191.25938415527344
Loss :  1.7097880840301514 2.3252718448638916 117.97338104248047
Loss :  1.7045382261276245 2.4825685024261475 125.83296203613281
Loss :  1.6950702667236328 1.8769596815109253 95.54305267333984
Loss :  1.7137367725372314 4.235397815704346 213.48362731933594
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.7193011045455933 4.291630744934082 216.30084228515625
Loss :  1.71747624874115 4.204002857208252 211.91763305664062
Loss :  1.721256971359253 4.0879950523376465 206.12100219726562
Total LOSS train 130.5629896897536 valid 211.9557762145996
CE LOSS train 1.716587963471046 valid 0.43031424283981323
Contrastive LOSS train 2.576928047033457 valid 1.0219987630844116
EPOCH 286:
Loss :  1.7258485555648804 2.3387091159820557 118.66130065917969
Loss :  1.7317951917648315 2.3701303005218506 120.23831176757812
Loss :  1.7177420854568481 1.996385931968689 101.53704071044922
Loss :  1.7211188077926636 2.2530112266540527 114.37167358398438
Loss :  1.7286481857299805 1.9735251665115356 100.4049072265625
Loss :  1.7126250267028809 2.079768419265747 105.70104217529297
Loss :  1.7287986278533936 2.399826765060425 121.72013854980469
Loss :  1.7175447940826416 3.615126609802246 182.473876953125
Loss :  1.71336030960083 3.430473566055298 173.23704528808594
Loss :  1.7291228771209717 1.794548511505127 91.45655059814453
Loss :  1.7092044353485107 2.5162084102630615 127.51962280273438
Loss :  1.7085320949554443 2.22036075592041 112.72657012939453
Loss :  1.7076975107192993 2.0171427726745605 102.56483459472656
Loss :  1.7108829021453857 2.1487808227539062 109.1499252319336
Loss :  1.7355232238769531 2.5146467685699463 127.46786499023438
Loss :  1.7336188554763794 2.406277656555176 122.04750061035156
Loss :  1.7065240144729614 2.4736168384552 125.38736724853516
Loss :  1.7192058563232422 2.58798885345459 131.11865234375
Loss :  1.705325961112976 2.4580917358398438 124.60990905761719
Loss :  1.7333954572677612 2.86557936668396 145.01235961914062
  batch 20 loss: 1.7333954572677612, 2.86557936668396, 145.01235961914062
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7167906761169434 2.3632054328918457 119.87705993652344
Loss :  1.7045899629592896 2.4089949131011963 122.15433502197266
Loss :  1.7126911878585815 2.2250852584838867 112.96695709228516
Loss :  1.720248818397522 2.3195157051086426 117.69602966308594
Loss :  1.733035922050476 2.442263126373291 123.84619140625
Loss :  1.7143950462341309 2.5642435550689697 129.92657470703125
Loss :  1.718549132347107 2.4721226692199707 125.3246841430664
Loss :  1.7163466215133667 2.3478734493255615 119.11001586914062
Loss :  1.692671775817871 2.3822288513183594 120.80411529541016
Loss :  1.7313116788864136 2.7784981727600098 140.65621948242188
Loss :  1.692803978919983 2.7456109523773193 138.97335815429688
Loss :  1.723949670791626 2.150911569595337 109.26953125
Loss :  1.7130451202392578 3.086371421813965 156.0316162109375
Loss :  1.7127412557601929 2.654038906097412 134.41468811035156
Loss :  1.6964110136032104 2.581923484802246 130.79258728027344
Loss :  1.7034035921096802 2.812237501144409 142.31527709960938
Loss :  1.7032760381698608 2.533111810684204 128.35887145996094
Loss :  1.7309510707855225 2.21553897857666 112.50789642333984
Loss :  1.733204960823059 2.281675100326538 115.81696319580078
Loss :  1.7375836372375488 2.5189080238342285 127.68299102783203
  batch 40 loss: 1.7375836372375488, 2.5189080238342285, 127.68299102783203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.719686508178711 2.5996503829956055 131.7021942138672
Loss :  1.7132320404052734 2.5277373790740967 128.10009765625
Loss :  1.7082226276397705 1.9138693809509277 97.40168762207031
Loss :  1.714186191558838 2.270562171936035 115.24229431152344
Loss :  1.7045961618423462 2.185274600982666 110.96833038330078
Loss :  1.7182544469833374 2.4439139366149902 123.91394805908203
Loss :  1.733346939086914 2.1706748008728027 110.26708221435547
Loss :  1.7113367319107056 2.143636465072632 108.89315795898438
Loss :  1.73996102809906 2.251586437225342 114.31928253173828
Loss :  1.7142809629440308 1.6961712837219238 86.52284240722656
Loss :  1.7291096448898315 2.095754384994507 106.51683044433594
Loss :  1.7267221212387085 2.586923122406006 131.0728759765625
Loss :  1.7171189785003662 2.1353566646575928 108.48494720458984
Loss :  1.730204701423645 2.4976553916931152 126.61296844482422
Loss :  1.7113627195358276 2.6591169834136963 134.66722106933594
Loss :  1.739998459815979 2.9321184158325195 148.34593200683594
Loss :  1.7148019075393677 2.390296459197998 121.22962951660156
Loss :  1.7073065042495728 2.213737964630127 112.39420318603516
Loss :  1.7142893075942993 2.57028865814209 130.22872924804688
Loss :  1.743082046508789 2.4267823696136475 123.08219909667969
  batch 60 loss: 1.743082046508789, 2.4267823696136475, 123.08219909667969
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7076935768127441 2.9224979877471924 147.83258056640625
Loss :  1.720717191696167 3.2636804580688477 164.9047393798828
Loss :  1.711428165435791 2.3698952198028564 120.20618438720703
Loss :  1.7064529657363892 2.4754602909088135 125.4794692993164
Loss :  1.697316288948059 2.2564032077789307 114.5174789428711
Loss :  1.7135584354400635 4.280762672424316 215.75169372558594
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.7190074920654297 4.258220195770264 214.63002014160156
Loss :  1.7176223993301392 4.222296714782715 212.83245849609375
Loss :  1.7199132442474365 3.9279532432556152 198.11756896972656
Total LOSS train 123.5206055861253 valid 210.33293533325195
CE LOSS train 1.7178342177317694 valid 0.42997831106185913
Contrastive LOSS train 2.43605542916518 valid 0.9819883108139038
EPOCH 287:
Loss :  1.7269984483718872 2.858099937438965 144.6320037841797
Loss :  1.7326925992965698 2.5101161003112793 127.23849487304688
Loss :  1.718644142150879 2.3780357837677 120.62042999267578
Loss :  1.7218834161758423 2.4585304260253906 124.64840698242188
Loss :  1.7291696071624756 2.1774344444274902 110.60089111328125
Loss :  1.7130610942840576 2.190847635269165 111.25543975830078
Loss :  1.7292107343673706 3.1357429027557373 158.516357421875
Loss :  1.7178750038146973 2.4642958641052246 124.93266296386719
Loss :  1.7136508226394653 2.2495110034942627 114.18920135498047
Loss :  1.7296152114868164 2.1462438106536865 109.04180145263672
Loss :  1.7093713283538818 2.7124621868133545 137.3324737548828
Loss :  1.7090173959732056 2.5916972160339355 131.2938690185547
Loss :  1.7081999778747559 2.1700286865234375 110.20963287353516
Loss :  1.7111914157867432 2.5425736904144287 128.83987426757812
Loss :  1.7358512878417969 3.2816367149353027 165.81768798828125
Loss :  1.733710765838623 2.4542999267578125 124.4487075805664
Loss :  1.7071532011032104 2.374788999557495 120.44660186767578
Loss :  1.719815731048584 2.3351244926452637 118.47604370117188
Loss :  1.7058364152908325 2.2026724815368652 111.8394546508789
Loss :  1.7340419292449951 2.272876024246216 115.37783813476562
  batch 20 loss: 1.7340419292449951, 2.272876024246216, 115.37783813476562
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7175770998001099 2.1736960411071777 110.40237426757812
Loss :  1.7056204080581665 2.893939971923828 146.40261840820312
Loss :  1.7138105630874634 3.110823154449463 157.2549591064453
Loss :  1.7211756706237793 2.290454149246216 116.24388122558594
Loss :  1.7340161800384521 2.777219533920288 140.59498596191406
Loss :  1.7153676748275757 3.0966262817382812 156.5466766357422
Loss :  1.7194719314575195 2.7022674083709717 136.8328399658203
Loss :  1.7170124053955078 2.7973906993865967 141.5865478515625
Loss :  1.6938828229904175 2.697819232940674 136.5848388671875
Loss :  1.7321734428405762 2.5397450923919678 128.71942138671875
Loss :  1.6942495107650757 2.556384563446045 129.5134735107422
Loss :  1.725141167640686 3.4012579917907715 171.7880401611328
Loss :  1.7142174243927002 2.6597015857696533 134.6992950439453
Loss :  1.7139133214950562 2.5850303173065186 130.96542358398438
Loss :  1.6975635290145874 2.69329571723938 136.3623504638672
Loss :  1.7045321464538574 2.7731854915618896 140.36380004882812
Loss :  1.7043625116348267 2.3459761142730713 119.00316619873047
Loss :  1.7316064834594727 2.161665678024292 109.81488800048828
Loss :  1.7336617708206177 2.4623630046844482 124.85181427001953
Loss :  1.7379895448684692 3.001903772354126 151.8331756591797
  batch 40 loss: 1.7379895448684692, 3.001903772354126, 151.8331756591797
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7199287414550781 3.2512919902801514 164.28453063964844
Loss :  1.713221549987793 3.0706686973571777 155.2466583251953
Loss :  1.7081724405288696 2.763615369796753 139.88894653320312
Loss :  1.7140659093856812 2.6898646354675293 136.20729064941406
Loss :  1.7044377326965332 2.1575911045074463 109.58399200439453
Loss :  1.7184007167816162 2.370718479156494 120.25432586669922
Loss :  1.7334399223327637 2.2672083377838135 115.09385681152344
Loss :  1.7112796306610107 2.2864232063293457 116.03244018554688
Loss :  1.7400654554367065 2.5233895778656006 127.9095458984375
Loss :  1.7141796350479126 3.4786365032196045 175.64599609375
Loss :  1.7292540073394775 2.6710047721862793 135.27947998046875
Loss :  1.7267546653747559 2.5566797256469727 129.5607452392578
Loss :  1.7172030210494995 3.2434158325195312 163.88800048828125
Loss :  1.7303210496902466 2.3587467670440674 119.66766357421875
Loss :  1.7117111682891846 2.571354866027832 130.2794647216797
Loss :  1.740247368812561 2.489734649658203 126.22698211669922
Loss :  1.7148321866989136 2.493098735809326 126.36976623535156
Loss :  1.7074832916259766 2.615196704864502 132.4673309326172
Loss :  1.7143932580947876 2.6866917610168457 136.04898071289062
Loss :  1.742750644683838 2.773428201675415 140.41416931152344
  batch 60 loss: 1.742750644683838, 2.773428201675415, 140.41416931152344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7079296112060547 2.3223302364349365 117.8244400024414
Loss :  1.7208884954452515 2.476494312286377 125.54560089111328
Loss :  1.7114036083221436 3.2985918521881104 166.64100646972656
Loss :  1.7064166069030762 2.7069973945617676 137.05628967285156
Loss :  1.697052001953125 2.216243267059326 112.50921630859375
Loss :  1.7121490240097046 4.005527019500732 201.98849487304688
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.7177445888519287 3.9735159873962402 200.39353942871094
Loss :  1.715765357017517 3.8775553703308105 195.59352111816406
Loss :  1.721595287322998 3.8268001079559326 193.0615997314453
Total LOSS train 132.15460252028245 valid 197.7592887878418
CE LOSS train 1.7183102901165301 valid 0.4303988218307495
Contrastive LOSS train 2.6087258632366472 valid 0.9567000269889832
EPOCH 288:
Loss :  1.7271257638931274 2.5208218097686768 127.76821899414062
Loss :  1.7328224182128906 2.5627081394195557 129.86822509765625
Loss :  1.7188301086425781 2.292640447616577 116.35084533691406
Loss :  1.7220098972320557 3.2641007900238037 164.9270477294922
Loss :  1.7291978597640991 3.6926047801971436 186.35943603515625
Loss :  1.7133064270019531 3.289637565612793 166.19517517089844
Loss :  1.729247808456421 2.8942084312438965 146.43966674804688
Loss :  1.7180415391921997 2.1592652797698975 109.68130493164062
Loss :  1.7139467000961304 2.267437219619751 115.08580780029297
Loss :  1.7294895648956299 2.3934690952301025 121.40294647216797
Loss :  1.7095304727554321 3.13604474067688 158.5117645263672
Loss :  1.708870768547058 3.697741746902466 186.59596252441406
Loss :  1.7080192565917969 2.141829490661621 108.79949951171875
Loss :  1.7109787464141846 2.4024851322174072 121.83523559570312
Loss :  1.7359893321990967 2.5378923416137695 128.63059997558594
Loss :  1.734010100364685 2.5113675594329834 127.30238342285156
Loss :  1.7069454193115234 2.384139060974121 120.91390228271484
Loss :  1.7194397449493408 2.734297513961792 138.43431091308594
Loss :  1.7053190469741821 3.1126954555511475 157.340087890625
Loss :  1.7335622310638428 2.7948598861694336 141.4765625
  batch 20 loss: 1.7335622310638428, 2.7948598861694336, 141.4765625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7169965505599976 2.341580390930176 118.79601287841797
Loss :  1.7050117254257202 2.4543466567993164 124.4223403930664
Loss :  1.713085651397705 2.612685441970825 132.3473663330078
Loss :  1.720433235168457 2.7009031772613525 136.76559448242188
Loss :  1.7334524393081665 2.633418321609497 133.4043731689453
Loss :  1.7147303819656372 2.6638383865356445 134.9066619873047
Loss :  1.7192659378051758 2.711846113204956 137.3115692138672
Loss :  1.7167493104934692 2.682933807373047 135.8634490966797
Loss :  1.6931778192520142 2.220715045928955 112.72892761230469
Loss :  1.7319281101226807 2.5973563194274902 131.59974670410156
Loss :  1.6933557987213135 3.4949212074279785 176.43942260742188
Loss :  1.7245359420776367 2.723414421081543 137.89524841308594
Loss :  1.7135080099105835 2.302410364151001 116.83403015136719
Loss :  1.713200330734253 2.4144320487976074 122.43480682373047
Loss :  1.6967339515686035 2.4937453269958496 126.38399505615234
Loss :  1.7037712335586548 2.453160285949707 124.36178588867188
Loss :  1.7035335302352905 2.4279708862304688 123.10208129882812
Loss :  1.7309634685516357 3.0182392597198486 152.64291381835938
Loss :  1.7332372665405273 2.886545419692993 146.0605010986328
Loss :  1.7376437187194824 2.2944586277008057 116.4605712890625
  batch 40 loss: 1.7376437187194824, 2.2944586277008057, 116.4605712890625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7194876670837402 2.521864175796509 127.81269836425781
Loss :  1.713132381439209 2.3789100646972656 120.65863800048828
Loss :  1.7078813314437866 2.7746989727020264 140.44284057617188
Loss :  1.7141060829162598 2.4484541416168213 124.13681030273438
Loss :  1.7045960426330566 2.9387047290802 148.63983154296875
Loss :  1.7183763980865479 2.829592227935791 143.197998046875
Loss :  1.733567476272583 4.0106353759765625 202.2653350830078
Loss :  1.7112888097763062 2.2766456604003906 115.54357147216797
Loss :  1.7405123710632324 2.3871848583221436 121.0997543334961
Loss :  1.7144856452941895 2.197526693344116 111.5908203125
Loss :  1.7293367385864258 2.515319585800171 127.49531555175781
Loss :  1.727056622505188 2.590393543243408 131.24673461914062
Loss :  1.7176265716552734 2.2978498935699463 116.61012268066406
Loss :  1.7306967973709106 2.4966299533843994 126.56219482421875
Loss :  1.7122972011566162 2.49409556388855 126.41707611083984
Loss :  1.7403866052627563 2.5632169246673584 129.90122985839844
Loss :  1.7155081033706665 2.2563869953155518 114.53485870361328
Loss :  1.708493947982788 2.150294542312622 109.22322082519531
Loss :  1.7150884866714478 2.4616050720214844 124.79534149169922
Loss :  1.743570327758789 2.4132282733917236 122.40498352050781
  batch 60 loss: 1.743570327758789, 2.4132282733917236, 122.40498352050781
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7086325883865356 3.2974865436553955 166.5829620361328
Loss :  1.7214710712432861 2.653017520904541 134.37234497070312
Loss :  1.712297797203064 2.414071559906006 122.4158706665039
Loss :  1.7071822881698608 2.4402997493743896 123.72216796875
Loss :  1.6979256868362427 2.083155393600464 105.85569763183594
Loss :  1.7117196321487427 3.8738245964050293 195.40293884277344
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.7170426845550537 3.888089656829834 196.12152099609375
Loss :  1.7151861190795898 3.820434808731079 192.73692321777344
Loss :  1.719408392906189 3.8300564289093018 193.22222900390625
Total LOSS train 133.11090463491587 valid 194.37090301513672
CE LOSS train 1.7182615793668308 valid 0.42985209822654724
Contrastive LOSS train 2.6278528616978574 valid 0.9575141072273254
EPOCH 289:
Loss :  1.7277294397354126 3.007269859313965 152.09121704101562
Loss :  1.7335177659988403 2.503124475479126 126.8897476196289
Loss :  1.7197760343551636 2.6746535301208496 135.45245361328125
Loss :  1.722882628440857 3.0873594284057617 156.09085083007812
Loss :  1.7300305366516113 2.9448201656341553 148.97103881835938
Loss :  1.7145618200302124 2.313281536102295 117.3786392211914
Loss :  1.7304496765136719 3.9132168292999268 197.39129638671875
Loss :  1.7194998264312744 2.091215133666992 106.28025817871094
Loss :  1.7154935598373413 2.221865653991699 112.80877685546875
Loss :  1.7310731410980225 2.3342790603637695 118.44502258300781
Loss :  1.7114850282669067 2.548865795135498 129.15476989746094
Loss :  1.7110000848770142 3.4297292232513428 173.1974639892578
Loss :  1.7101342678070068 2.491222620010376 126.27127075195312
Loss :  1.7130353450775146 2.290393352508545 116.23270416259766
Loss :  1.7372090816497803 2.65148663520813 134.31153869628906
Loss :  1.7352553606033325 3.2549281120300293 164.48165893554688
Loss :  1.7089039087295532 2.5067548751831055 127.04664611816406
Loss :  1.7212516069412231 1.8405578136444092 93.7491455078125
Loss :  1.7073636054992676 1.7349129915237427 88.45301818847656
Loss :  1.7349029779434204 2.614657163619995 132.4677734375
  batch 20 loss: 1.7349029779434204, 2.614657163619995, 132.4677734375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7185778617858887 2.211639404296875 112.30054473876953
Loss :  1.706655740737915 3.060483694076538 154.7308349609375
Loss :  1.7145541906356812 2.292027235031128 116.31591796875
Loss :  1.7218384742736816 2.2700440883636475 115.22404479980469
Loss :  1.734574794769287 2.3996763229370117 121.71839141845703
Loss :  1.7161177396774292 2.188528299331665 111.14253234863281
Loss :  1.7203444242477417 2.3606467247009277 119.75267791748047
Loss :  1.7181439399719238 2.3808302879333496 120.75965881347656
Loss :  1.695122241973877 2.261725664138794 114.78140258789062
Loss :  1.732866644859314 2.501326560974121 126.7991943359375
Loss :  1.6954752206802368 2.681208372116089 135.7559051513672
Loss :  1.725754976272583 2.959418773651123 149.69668579101562
Loss :  1.7149524688720703 2.25780987739563 114.6054458618164
Loss :  1.7145497798919678 2.164475202560425 109.93830871582031
Loss :  1.6985416412353516 2.8029489517211914 141.8459930419922
Loss :  1.7054036855697632 2.3792309761047363 120.66695404052734
Loss :  1.7050578594207764 2.3226065635681152 117.83538055419922
Loss :  1.7319566011428833 2.456268310546875 124.54537200927734
Loss :  1.7342997789382935 2.400437355041504 121.75616455078125
Loss :  1.7385321855545044 2.414639949798584 122.47052764892578
  batch 40 loss: 1.7385321855545044, 2.414639949798584, 122.47052764892578
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7209094762802124 2.4875335693359375 126.09758758544922
Loss :  1.7145962715148926 2.0471506118774414 104.07212829589844
Loss :  1.709538221359253 2.2739999294281006 115.40953826904297
Loss :  1.7155543565750122 2.259193181991577 114.67520904541016
Loss :  1.7060900926589966 1.9876054525375366 101.08636474609375
Loss :  1.7197096347808838 2.2022905349731445 111.83423614501953
Loss :  1.7344828844070435 2.1649973392486572 109.9843521118164
Loss :  1.713091492652893 2.2119367122650146 112.30992126464844
Loss :  1.7409783601760864 2.4481863975524902 124.15029907226562
Loss :  1.7160882949829102 2.422595977783203 122.84588623046875
Loss :  1.7305715084075928 2.413477659225464 122.40444946289062
Loss :  1.7282966375350952 2.2231783866882324 112.88721466064453
Loss :  1.719007134437561 2.714660882949829 137.45205688476562
Loss :  1.731862187385559 2.4179818630218506 122.6309585571289
Loss :  1.7136249542236328 2.26979660987854 115.20345306396484
Loss :  1.741484522819519 2.5851259231567383 130.99778747558594
Loss :  1.716977834701538 2.6650595664978027 134.96995544433594
Loss :  1.709720253944397 4.237445831298828 213.58200073242188
Loss :  1.716614007949829 4.30396032333374 216.9146270751953
Loss :  1.7446047067642212 2.813925266265869 142.44085693359375
  batch 60 loss: 1.7446047067642212, 2.813925266265869, 142.44085693359375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.710041880607605 2.2310521602630615 113.26264953613281
Loss :  1.7226316928863525 2.4490153789520264 124.17340087890625
Loss :  1.7134841680526733 2.3294999599456787 118.18848419189453
Loss :  1.7081308364868164 2.667025566101074 135.0594024658203
Loss :  1.698969841003418 1.9530245065689087 99.35018920898438
Loss :  1.7127902507781982 4.176974773406982 210.5615234375
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.7183818817138672 4.252150058746338 214.3258819580078
Loss :  1.7163126468658447 3.926509141921997 198.04176330566406
Loss :  1.722514033317566 4.0335917472839355 203.402099609375
Total LOSS train 127.87369596041165 valid 206.58281707763672
CE LOSS train 1.7196298030706552 valid 0.4306285083293915
Contrastive LOSS train 2.5230813301526585 valid 1.0083979368209839
EPOCH 290:
Loss :  1.728597640991211 2.8199832439422607 142.72775268554688
Loss :  1.734249472618103 2.6447839736938477 133.97344970703125
Loss :  1.7206039428710938 2.1834404468536377 110.89262390136719
Loss :  1.723707914352417 3.5706846714019775 180.25794982910156
Loss :  1.7308101654052734 2.2787396907806396 115.66779327392578
Loss :  1.7153178453445435 2.1018404960632324 106.80734252929688
Loss :  1.7308894395828247 2.5840399265289307 130.93289184570312
Loss :  1.7199575901031494 2.4678218364715576 125.11104583740234
Loss :  1.7159478664398193 2.524111747741699 127.92153930664062
Loss :  1.7310491800308228 2.356013059616089 119.53170013427734
Loss :  1.7116498947143555 2.598447322845459 131.63401794433594
Loss :  1.7109863758087158 2.788360834121704 141.1290283203125
Loss :  1.7101788520812988 3.2234344482421875 162.88189697265625
Loss :  1.7130986452102661 2.4238686561584473 122.90653228759766
Loss :  1.7373052835464478 2.4982428550720215 126.64944458007812
Loss :  1.7354271411895752 2.5351507663726807 128.4929656982422
Loss :  1.70884370803833 2.341871500015259 118.80242156982422
Loss :  1.721049189567566 2.2456367015838623 114.00288391113281
Loss :  1.707394003868103 2.072298526763916 105.32232666015625
Loss :  1.734877109527588 2.3317246437072754 118.32111358642578
  batch 20 loss: 1.734877109527588, 2.3317246437072754, 118.32111358642578
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.718797206878662 2.0846173763275146 105.94966125488281
Loss :  1.707133412361145 2.8578784465789795 144.60105895996094
Loss :  1.7150301933288574 2.2919723987579346 116.31364440917969
Loss :  1.7222522497177124 2.3917453289031982 121.30951690673828
Loss :  1.7348593473434448 2.6088528633117676 132.17750549316406
Loss :  1.7165240049362183 2.631652593612671 133.2991485595703
Loss :  1.720657229423523 2.3768770694732666 120.56451416015625
Loss :  1.7185125350952148 2.212761878967285 112.35660552978516
Loss :  1.6956043243408203 2.404855966567993 121.93840026855469
Loss :  1.733418583869934 2.561022996902466 129.78456115722656
Loss :  1.6958707571029663 2.4662344455718994 125.0075912475586
Loss :  1.726247787475586 2.898728132247925 146.6626434326172
Loss :  1.7155086994171143 2.4616928100585938 124.8001480102539
Loss :  1.7151967287063599 2.315854787826538 117.5079345703125
Loss :  1.6991046667099 2.713322162628174 137.36521911621094
Loss :  1.7059801816940308 2.7697479724884033 140.19337463378906
Loss :  1.7058030366897583 2.6256797313690186 132.9897918701172
Loss :  1.732535481452942 2.6830759048461914 135.88632202148438
Loss :  1.734869360923767 2.420405149459839 122.755126953125
Loss :  1.7390716075897217 2.3041956424713135 116.94886016845703
  batch 40 loss: 1.7390716075897217, 2.3041956424713135, 116.94886016845703
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7215296030044556 3.5123403072357178 177.3385467529297
Loss :  1.7151901721954346 3.38002872467041 170.7166290283203
Loss :  1.710511326789856 2.415419578552246 122.48149108886719
Loss :  1.7166379690170288 2.3782076835632324 120.62702178955078
Loss :  1.7072032690048218 2.365191698074341 119.96678924560547
Loss :  1.7205790281295776 2.565830945968628 130.0121307373047
Loss :  1.7354398965835571 2.3406307697296143 118.76698303222656
Loss :  1.7133985757827759 2.343064308166504 118.86661529541016
Loss :  1.741660237312317 2.533449411392212 128.41412353515625
Loss :  1.7161881923675537 2.3679444789886475 120.11341094970703
Loss :  1.7304937839508057 2.5698587894439697 130.2234344482422
Loss :  1.7285116910934448 2.5611414909362793 129.78558349609375
Loss :  1.7193115949630737 2.295872688293457 116.51294708251953
Loss :  1.7320165634155273 2.7308008670806885 138.2720489501953
Loss :  1.7139841318130493 2.551570177078247 129.29249572753906
Loss :  1.741378664970398 2.4365103244781494 123.56689453125
Loss :  1.7171181440353394 2.6400160789489746 133.71792602539062
Loss :  1.7101789712905884 2.4307100772857666 123.24568939208984
Loss :  1.7166730165481567 2.1717519760131836 110.30427551269531
Loss :  1.7447266578674316 1.755815863609314 89.5355224609375
  batch 60 loss: 1.7447266578674316, 1.755815863609314, 89.5355224609375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.710579752922058 1.8788318634033203 95.65216827392578
Loss :  1.7231231927871704 2.035583257675171 103.50228118896484
Loss :  1.7141693830490112 3.2023682594299316 161.83258056640625
Loss :  1.7091134786605835 2.6036856174468994 131.8933868408203
Loss :  1.7000072002410889 1.7900439500808716 91.20220184326172
Loss :  1.7143502235412598 4.349540710449219 219.19139099121094
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.7196195125579834 4.350748538970947 219.25704956054688
Loss :  1.7178871631622314 4.2453532218933105 213.98553466796875
Loss :  1.7211042642593384 4.155120372772217 209.47711181640625
Total LOSS train 126.7418696476863 valid 215.4777717590332
CE LOSS train 1.7200714331406814 valid 0.4302760660648346
Contrastive LOSS train 2.500435972213745 valid 1.0387800931930542
EPOCH 291:
Loss :  1.7291783094406128 2.4692184925079346 125.19010162353516
Loss :  1.734696865081787 2.6383566856384277 133.65252685546875
Loss :  1.721288800239563 2.265338897705078 114.98823547363281
Loss :  1.724471926689148 2.339700222015381 118.70948028564453
Loss :  1.7314629554748535 1.9408886432647705 98.77589416503906
Loss :  1.7162963151931763 1.9400899410247803 98.72078704833984
Loss :  1.7317496538162231 2.1027698516845703 106.8702392578125
Loss :  1.721049427986145 2.1843199729919434 110.93704223632812
Loss :  1.7172114849090576 1.9458016157150269 99.00728607177734
Loss :  1.732097864151001 1.739729404449463 88.71857452392578
Loss :  1.7130296230316162 2.220844030380249 112.7552261352539
Loss :  1.7123444080352783 2.5458195209503174 129.00332641601562
Loss :  1.7113584280014038 3.16583251953125 160.00299072265625
Loss :  1.7141170501708984 2.1920039653778076 111.31431579589844
Loss :  1.7378286123275757 2.257794141769409 114.6275405883789
Loss :  1.7359527349472046 2.29113507270813 116.29270935058594
Loss :  1.7099593877792358 2.788691997528076 141.14456176757812
Loss :  1.7222944498062134 2.416745901107788 122.5595932006836
Loss :  1.7086905241012573 2.2139124870300293 112.40431213378906
Loss :  1.7358026504516602 2.135101079940796 108.49085235595703
  batch 20 loss: 1.7358026504516602, 2.135101079940796, 108.49085235595703
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7200596332550049 2.0038113594055176 101.91062927246094
Loss :  1.708685040473938 2.0994560718536377 106.68148803710938
Loss :  1.7164770364761353 2.4708364009857178 125.25829315185547
Loss :  1.7235922813415527 2.3841092586517334 120.9290542602539
Loss :  1.7360509634017944 2.5830445289611816 130.88827514648438
Loss :  1.7179458141326904 2.1416571140289307 108.8008041381836
Loss :  1.7221496105194092 2.1152517795562744 107.4847412109375
Loss :  1.7199987173080444 2.41656756401062 122.54837799072266
Loss :  1.697530746459961 2.4144163131713867 122.41835021972656
Loss :  1.73438560962677 2.172837972640991 110.37628173828125
Loss :  1.6977074146270752 2.4386703968048096 123.6312255859375
Loss :  1.7272950410842896 2.429410457611084 123.19781494140625
Loss :  1.7166668176651 2.382277011871338 120.83052062988281
Loss :  1.7163344621658325 2.490387201309204 126.2356948852539
Loss :  1.7007427215576172 2.4060463905334473 122.00306701660156
Loss :  1.707486629486084 2.56951904296875 130.18344116210938
Loss :  1.707306146621704 2.5641133785247803 129.91297912597656
Loss :  1.7337260246276855 2.335177183151245 118.49258422851562
Loss :  1.7359343767166138 2.074100971221924 105.44097900390625
Loss :  1.7400822639465332 2.317772388458252 117.62870025634766
  batch 40 loss: 1.7400822639465332, 2.317772388458252, 117.62870025634766
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7229801416397095 2.552276372909546 129.3367919921875
Loss :  1.7169560194015503 2.2579903602600098 114.6164779663086
Loss :  1.712090015411377 2.1550822257995605 109.46620178222656
Loss :  1.7178970575332642 2.075056314468384 105.47071075439453
Loss :  1.708741307258606 2.2449464797973633 113.95606231689453
Loss :  1.7217470407485962 2.4538302421569824 124.41326141357422
Loss :  1.7359732389450073 2.011155366897583 102.29373931884766
Loss :  1.7151013612747192 1.765020489692688 89.96612548828125
Loss :  1.7423362731933594 2.275176763534546 115.50117492675781
Loss :  1.7178326845169067 2.241532564163208 113.79446411132812
Loss :  1.7318965196609497 2.9325506687164307 148.35943603515625
Loss :  1.7296345233917236 2.4362778663635254 123.54353332519531
Loss :  1.7205204963684082 2.1410298347473145 108.77201080322266
Loss :  1.7331364154815674 2.325319766998291 117.99913024902344
Loss :  1.7149897813796997 2.2287802696228027 113.15399932861328
Loss :  1.7424689531326294 2.6789257526397705 135.68875122070312
Loss :  1.7185043096542358 2.6834876537323 135.89288330078125
Loss :  1.7114789485931396 1.8660905361175537 95.01600646972656
Loss :  1.7181062698364258 2.1836469173431396 110.90045166015625
Loss :  1.7456023693084717 1.7126184701919556 87.37652587890625
  batch 60 loss: 1.7456023693084717, 1.7126184701919556, 87.37652587890625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7119324207305908 2.372091770172119 120.31652069091797
Loss :  1.724388599395752 2.2071187496185303 112.080322265625
Loss :  1.7154642343521118 1.9212929010391235 97.78010559082031
Loss :  1.7108094692230225 2.341639995574951 118.79280853271484
Loss :  1.7020350694656372 1.6871260404586792 86.05833435058594
Loss :  1.7185494899749756 4.383334159851074 220.88525390625
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.7237879037857056 4.3594207763671875 219.69482421875
Loss :  1.7224313020706177 4.217560291290283 212.60044860839844
Loss :  1.725063443183899 4.0542988777160645 204.4400177001953
Total LOSS train 115.83945735051081 valid 214.40513610839844
CE LOSS train 1.7212871129696186 valid 0.43126586079597473
Contrastive LOSS train 2.282363409262437 valid 1.0135747194290161
EPOCH 292:
Loss :  1.730440616607666 2.1274573802948 108.10330963134766
Loss :  1.7358025312423706 2.0807979106903076 105.77569580078125
Loss :  1.722588062286377 2.1830241680145264 110.87379455566406
Loss :  1.725614309310913 4.114660739898682 207.45864868164062
Loss :  1.732418179512024 2.9794507026672363 150.7049560546875
Loss :  1.717310905456543 2.3141937255859375 117.42699432373047
Loss :  1.7324857711791992 3.034698724746704 153.46742248535156
Loss :  1.7218689918518066 2.407482385635376 122.09599304199219
Loss :  1.7179089784622192 2.2697579860687256 115.205810546875
Loss :  1.73273766040802 2.1381728649139404 108.6413803100586
Loss :  1.7137980461120605 2.4420015811920166 123.81388092041016
Loss :  1.7132163047790527 2.5663235187530518 130.02938842773438
Loss :  1.7122834920883179 2.706481695175171 137.03636169433594
Loss :  1.715211033821106 2.533970832824707 128.41375732421875
Loss :  1.7388473749160767 1.797283411026001 91.60301971435547
Loss :  1.7366842031478882 3.5998454093933105 181.72894287109375
Loss :  1.7116645574569702 2.508373975753784 127.13036346435547
Loss :  1.7235578298568726 3.27476167678833 165.46163940429688
Loss :  1.7102305889129639 2.099088668823242 106.66466522216797
Loss :  1.7363781929016113 3.880793333053589 195.7760467529297
  batch 20 loss: 1.7363781929016113, 3.880793333053589, 195.7760467529297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.720777153968811 2.5379364490509033 128.6175994873047
Loss :  1.7093136310577393 2.278459072113037 115.63226318359375
Loss :  1.717180848121643 2.320352792739868 117.73481750488281
Loss :  1.7240405082702637 2.437502145767212 123.5991439819336
Loss :  1.736423134803772 2.120514392852783 107.76213836669922
Loss :  1.718572974205017 2.4454691410064697 123.99203491210938
Loss :  1.7227365970611572 2.143038034439087 108.87464141845703
Loss :  1.7206134796142578 2.6799230575561523 135.71676635742188
Loss :  1.698368787765503 2.2247872352600098 112.93773651123047
Loss :  1.7348827123641968 2.5117814540863037 127.32395935058594
Loss :  1.6986767053604126 2.911931037902832 147.29522705078125
Loss :  1.7280409336090088 2.49308705329895 126.38239288330078
Loss :  1.7174036502838135 2.3892314434051514 121.1789779663086
Loss :  1.7169084548950195 2.692847967147827 136.3592987060547
Loss :  1.7013719081878662 2.365530252456665 119.9778823852539
Loss :  1.708081841468811 2.3774561882019043 120.58089447021484
Loss :  1.7077183723449707 2.5723066329956055 130.3230438232422
Loss :  1.7338427305221558 2.3200180530548096 117.7347412109375
Loss :  1.735987663269043 2.508991241455078 127.185546875
Loss :  1.740073800086975 2.9451839923858643 148.999267578125
  batch 40 loss: 1.740073800086975, 2.9451839923858643, 148.999267578125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7229149341583252 3.021958589553833 152.8208465576172
Loss :  1.717079520225525 1.6732163429260254 85.37789916992188
Loss :  1.7121644020080566 1.9201291799545288 97.7186279296875
Loss :  1.7179789543151855 2.492156505584717 126.3258056640625
Loss :  1.7087674140930176 2.293269634246826 116.37225341796875
Loss :  1.7219126224517822 2.033409357070923 103.39238739013672
Loss :  1.7361369132995605 2.055208444595337 104.4965591430664
Loss :  1.7151134014129639 2.6964502334594727 136.53762817382812
Loss :  1.7426061630249023 3.2872297763824463 166.10409545898438
Loss :  1.7179350852966309 2.083380937576294 105.88697814941406
Loss :  1.7323366403579712 3.0247483253479004 152.96974182128906
Loss :  1.7298412322998047 3.8883042335510254 196.14505004882812
Loss :  1.7207536697387695 2.2321667671203613 113.32909393310547
Loss :  1.7332483530044556 2.3526599407196045 119.36624145507812
Loss :  1.7152817249298096 2.2513186931610107 114.28121185302734
Loss :  1.742497205734253 2.294106960296631 116.44784545898438
Loss :  1.7184913158416748 2.336829900741577 118.55998229980469
Loss :  1.7115682363510132 2.0820181369781494 105.8124771118164
Loss :  1.718120813369751 2.5931200981140137 131.37413024902344
Loss :  1.745485782623291 2.178076982498169 110.64933013916016
  batch 60 loss: 1.745485782623291, 2.178076982498169, 110.64933013916016
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.712103247642517 2.2570834159851074 114.56627655029297
Loss :  1.7244901657104492 2.2912111282348633 116.28504180908203
Loss :  1.7157424688339233 2.217200517654419 112.57576751708984
Loss :  1.710929274559021 2.7884254455566406 141.13218688964844
Loss :  1.702088475227356 1.5319466590881348 78.29942321777344
Loss :  1.7191075086593628 4.189339637756348 211.1860809326172
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.7242478132247925 4.179734706878662 210.7109832763672
Loss :  1.722825050354004 4.070337772369385 205.23971557617188
Loss :  1.7261569499969482 4.110156536102295 207.23397827148438
Total LOSS train 126.49918964092548 valid 208.59268951416016
CE LOSS train 1.7218100236012386 valid 0.43153923749923706
Contrastive LOSS train 2.49554760822883 valid 1.0275391340255737
EPOCH 293:
Loss :  1.7304939031600952 2.3300530910491943 118.23314666748047
Loss :  1.735990285873413 2.3906843662261963 121.27021026611328
Loss :  1.72288978099823 2.099039316177368 106.67485809326172
Loss :  1.7260479927062988 2.396428108215332 121.54745483398438
Loss :  1.7330321073532104 1.8456101417541504 94.01354217529297
Loss :  1.7179423570632935 2.4572336673736572 124.57962799072266
Loss :  1.7331475019454956 2.335635185241699 118.51490783691406
Loss :  1.7226006984710693 2.590822458267212 131.2637176513672
Loss :  1.7187155485153198 2.1945135593414307 111.44438934326172
Loss :  1.733324646949768 2.261483669281006 114.80750274658203
Loss :  1.7147296667099 2.487595319747925 126.09449768066406
Loss :  1.7141629457473755 2.1310088634490967 108.26461029052734
Loss :  1.7133855819702148 2.468838691711426 125.15531921386719
Loss :  1.7161483764648438 2.4049618244171143 121.96424102783203
Loss :  1.7392059564590454 2.2356317043304443 113.52079010009766
Loss :  1.7374238967895508 2.395986795425415 121.5367660522461
Loss :  1.7121379375457764 2.2492332458496094 114.17379760742188
Loss :  1.7239980697631836 2.6824183464050293 135.84490966796875
Loss :  1.710746169090271 2.1228859424591064 107.85504150390625
Loss :  1.7371715307235718 2.222245216369629 112.84943389892578
  batch 20 loss: 1.7371715307235718, 2.222245216369629, 112.84943389892578
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7215486764907837 2.0125174522399902 102.34741973876953
Loss :  1.710334300994873 2.4352548122406006 123.47307586669922
Loss :  1.7178926467895508 2.616767644882202 132.5562744140625
Loss :  1.7248467206954956 2.1658644676208496 110.01806640625
Loss :  1.7371101379394531 2.380514621734619 120.76284790039062
Loss :  1.7195767164230347 2.386615037918091 121.05032348632812
Loss :  1.723685383796692 1.8696683645248413 95.20710754394531
Loss :  1.7215843200683594 1.9648157358169556 99.96237182617188
Loss :  1.6995218992233276 2.290809392929077 116.239990234375
Loss :  1.7355430126190186 2.2663958072662354 115.05533599853516
Loss :  1.6996492147445679 2.1747894287109375 110.43911743164062
Loss :  1.7286641597747803 2.311001777648926 117.27874755859375
Loss :  1.7182542085647583 2.887713670730591 146.10394287109375
Loss :  1.717911958694458 2.403714895248413 121.90365600585938
Loss :  1.7026245594024658 2.185317277908325 110.9684829711914
Loss :  1.7092089653015137 2.307621479034424 117.09027862548828
Loss :  1.7088751792907715 2.364323854446411 119.9250717163086
Loss :  1.734537124633789 3.011977434158325 152.3334197998047
Loss :  1.7366936206817627 2.061096429824829 104.79151916503906
Loss :  1.7407675981521606 2.1866655349731445 111.07404327392578
  batch 40 loss: 1.7407675981521606, 2.1866655349731445, 111.07404327392578
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7238045930862427 2.4344518184661865 123.44639587402344
Loss :  1.7178927659988403 1.6462922096252441 84.03250885009766
Loss :  1.7132200002670288 2.61694598197937 132.56053161621094
Loss :  1.7189619541168213 2.306196689605713 117.02880096435547
Loss :  1.709946870803833 1.8149770498275757 92.45880126953125
Loss :  1.7228434085845947 2.26759934425354 115.10281372070312
Loss :  1.736779808998108 3.063091993331909 154.89138793945312
Loss :  1.7162742614746094 2.178039789199829 110.61827087402344
Loss :  1.7431944608688354 2.343871831893921 118.93678283691406
Loss :  1.7190263271331787 1.9343483448028564 98.43643951416016
Loss :  1.7329339981079102 2.3089165687561035 117.17876434326172
Loss :  1.7307292222976685 2.7647361755371094 139.96754455566406
Loss :  1.7217466831207275 3.541555881500244 178.79953002929688
Loss :  1.7340469360351562 3.617478370666504 182.60797119140625
Loss :  1.716417908668518 2.5825953483581543 130.84617614746094
Loss :  1.7434707880020142 2.3597283363342285 119.72988891601562
Loss :  1.719744324684143 2.4243998527526855 122.93973541259766
Loss :  1.7128193378448486 2.2831931114196777 115.87247467041016
Loss :  1.7193777561187744 2.646322011947632 134.0354766845703
Loss :  1.7462321519851685 2.2629315853118896 114.89280700683594
  batch 60 loss: 1.7462321519851685, 2.2629315853118896, 114.89280700683594
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.713250756263733 2.4370553493499756 123.5660171508789
Loss :  1.7256561517715454 2.3481478691101074 119.13304901123047
Loss :  1.7169049978256226 2.1302778720855713 108.23079681396484
Loss :  1.7121840715408325 2.329786777496338 118.20152282714844
Loss :  1.7036722898483276 1.5311511754989624 78.26123046875
Loss :  1.7211662530899048 4.192557334899902 211.34902954101562
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.7262132167816162 4.182840824127197 210.86825561523438
Loss :  1.724744200706482 4.104020118713379 206.92575073242188
Loss :  1.7278931140899658 4.011538982391357 202.30484008789062
Total LOSS train 119.23023963341346 valid 207.86196899414062
CE LOSS train 1.722665495138902 valid 0.43197327852249146
Contrastive LOSS train 2.3501514764932487 valid 1.0028847455978394
EPOCH 294:
Loss :  1.7314373254776 2.600266933441162 131.74478149414062
Loss :  1.7367022037506104 2.6791975498199463 135.69659423828125
Loss :  1.723676323890686 2.5098767280578613 127.21751403808594
Loss :  1.726778268814087 2.998713493347168 151.66244506835938
Loss :  1.7335351705551147 2.387486696243286 121.10787200927734
Loss :  1.7187343835830688 2.2410495281219482 113.77120971679688
Loss :  1.7335890531539917 2.842390537261963 143.8531036376953
Loss :  1.7231484651565552 2.6896657943725586 136.20643615722656
Loss :  1.7193018198013306 2.537842035293579 128.6114044189453
Loss :  1.733764886856079 2.287083148956299 116.08792114257812
Loss :  1.7151391506195068 3.24391508102417 163.910888671875
Loss :  1.7145721912384033 3.1979644298553467 161.61279296875
Loss :  1.7137726545333862 2.875262498855591 145.47689819335938
Loss :  1.7163108587265015 2.199422597885132 111.68743896484375
Loss :  1.7395269870758057 2.5569326877593994 129.58616638183594
Loss :  1.7373998165130615 2.483928680419922 125.93383026123047
Loss :  1.7123682498931885 2.366908073425293 120.05777740478516
Loss :  1.7241802215576172 2.4382355213165283 123.63595581054688
Loss :  1.710932970046997 2.064215898513794 104.92172241210938
Loss :  1.7372701168060303 2.2278637886047363 113.13045501708984
  batch 20 loss: 1.7372701168060303, 2.2278637886047363, 113.13045501708984
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.721696138381958 2.2535223960876465 114.39781188964844
Loss :  1.7105262279510498 2.345423698425293 118.9817123413086
Loss :  1.7180116176605225 2.0184578895568848 102.64090728759766
Loss :  1.7248213291168213 2.3441097736358643 118.93031311035156
Loss :  1.7369282245635986 2.1056325435638428 107.0185546875
Loss :  1.7193193435668945 2.2490923404693604 114.17394256591797
Loss :  1.7232407331466675 2.3218114376068115 117.81381225585938
Loss :  1.7211883068084717 2.5741279125213623 130.42758178710938
Loss :  1.699180245399475 1.9606044292449951 99.72940063476562
Loss :  1.7352674007415771 2.388364315032959 121.15348052978516
Loss :  1.6992493867874146 2.4575793743133545 124.57821655273438
Loss :  1.7282613515853882 2.059011220932007 104.67882537841797
Loss :  1.7180299758911133 2.4146153926849365 122.44879913330078
Loss :  1.7177133560180664 2.472684383392334 125.3519287109375
Loss :  1.70234215259552 2.6629397869110107 134.84933471679688
Loss :  1.708911657333374 2.495734691619873 126.49564361572266
Loss :  1.7085832357406616 2.3704938888549805 120.2332763671875
Loss :  1.7343004941940308 2.5850229263305664 130.98544311523438
Loss :  1.736465334892273 2.1310858726501465 108.29075622558594
Loss :  1.7405072450637817 2.012489080429077 102.36495971679688
  batch 40 loss: 1.7405072450637817, 2.012489080429077, 102.36495971679688
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.723524808883667 2.5009748935699463 126.77227020263672
Loss :  1.717558741569519 2.031482458114624 103.29167938232422
Loss :  1.7128500938415527 2.2731199264526367 115.36885070800781
Loss :  1.7185585498809814 2.1044461727142334 106.94086456298828
Loss :  1.7094933986663818 2.118112564086914 107.61511993408203
Loss :  1.7222843170166016 2.137761354446411 108.6103515625
Loss :  1.7364718914031982 2.107381582260132 107.10555267333984
Loss :  1.7158112525939941 2.040783643722534 103.75499725341797
Loss :  1.74274480342865 2.209104299545288 112.19795989990234
Loss :  1.71872878074646 2.102912187576294 106.86433410644531
Loss :  1.7328389883041382 2.2964015007019043 116.55291748046875
Loss :  1.730471134185791 2.4525187015533447 124.35640716552734
Loss :  1.7214401960372925 1.8949486017227173 96.4688720703125
Loss :  1.7337664365768433 2.1181387901306152 107.64070129394531
Loss :  1.7161259651184082 3.095539093017578 156.49307250976562
Loss :  1.7430243492126465 2.350818157196045 119.283935546875
Loss :  1.7193868160247803 2.290919303894043 116.26535034179688
Loss :  1.7124391794204712 2.847883939743042 144.10662841796875
Loss :  1.7189335823059082 2.5858314037323 131.010498046875
Loss :  1.7460092306137085 2.178424835205078 110.66725158691406
  batch 60 loss: 1.7460092306137085, 2.178424835205078, 110.66725158691406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7129782438278198 1.9410816431045532 98.76705932617188
Loss :  1.7251437902450562 2.4052646160125732 121.98837280273438
Loss :  1.7166178226470947 2.2663636207580566 115.03480529785156
Loss :  1.7119312286376953 2.9971163272857666 151.5677490234375
Loss :  1.7031736373901367 1.611021876335144 82.25426483154297
Loss :  1.7164512872695923 4.242306709289551 213.831787109375
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.721562147140503 4.109589099884033 207.20101928710938
Loss :  1.719878077507019 4.102568626403809 206.8483123779297
Loss :  1.7243162393569946 3.93634033203125 198.5413360595703
Total LOSS train 120.65288884089543 valid 206.6056137084961
CE LOSS train 1.722599878677955 valid 0.43107905983924866
Contrastive LOSS train 2.378605793072627 valid 0.9840850830078125
EPOCH 295:
Loss :  1.7311688661575317 2.3304009437561035 118.251220703125
Loss :  1.736703634262085 2.377159595489502 120.59468078613281
Loss :  1.7238072156906128 2.196331024169922 111.54035949707031
Loss :  1.7268445491790771 2.5659401416778564 130.0238494873047
Loss :  1.7337770462036133 2.285506010055542 116.00907897949219
Loss :  1.7189068794250488 2.33900785446167 118.66930389404297
Loss :  1.733693242073059 2.071518659591675 105.30963134765625
Loss :  1.7234078645706177 2.2690300941467285 115.17491912841797
Loss :  1.719519019126892 2.30849289894104 117.1441650390625
Loss :  1.733747124671936 2.6009292602539062 131.78021240234375
Loss :  1.7154204845428467 2.5725975036621094 130.34530639648438
Loss :  1.7149503231048584 2.26859974861145 115.14493560791016
Loss :  1.7137821912765503 2.095806360244751 106.50410461425781
Loss :  1.716629981994629 2.8551225662231445 144.47276306152344
Loss :  1.7394291162490845 2.319833993911743 117.73112487792969
Loss :  1.7376741170883179 2.145740270614624 109.0246810913086
Loss :  1.7127877473831177 2.203967809677124 111.91117858886719
Loss :  1.7244588136672974 1.9496726989746094 99.20809173583984
Loss :  1.7115689516067505 2.6097991466522217 132.20152282714844
Loss :  1.7373923063278198 3.43292498588562 173.38365173339844
  batch 20 loss: 1.7373923063278198, 3.43292498588562, 173.38365173339844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.72195565700531 2.018303155899048 102.63711547851562
Loss :  1.7106653451919556 2.5982017517089844 131.62075805664062
Loss :  1.7182892560958862 2.2426936626434326 113.85297393798828
Loss :  1.725264310836792 2.316235303878784 117.53703308105469
Loss :  1.7375752925872803 2.586880683898926 131.0816192626953
Loss :  1.7198975086212158 2.7653748989105225 139.98863220214844
Loss :  1.7239937782287598 2.1494247913360596 109.19522857666016
Loss :  1.721714973449707 3.4537084102630615 174.40713500976562
Loss :  1.7000213861465454 3.298490285873413 166.62454223632812
Loss :  1.7359507083892822 2.6842494010925293 135.9484100341797
Loss :  1.700342059135437 2.3128294944763184 117.34181213378906
Loss :  1.7291688919067383 2.5517334938049316 129.3158416748047
Loss :  1.7189017534255981 2.531085729598999 128.27317810058594
Loss :  1.7185325622558594 2.550337553024292 129.23541259765625
Loss :  1.703519582748413 2.689347982406616 136.17091369628906
Loss :  1.7100088596343994 2.490429401397705 126.23147583007812
Loss :  1.7097017765045166 3.6238245964050293 182.9009246826172
Loss :  1.7353181838989258 3.809208869934082 192.19577026367188
Loss :  1.7373759746551514 3.286407232284546 166.0577392578125
Loss :  1.7413877248764038 2.111177682876587 107.3002700805664
  batch 40 loss: 1.7413877248764038, 2.111177682876587, 107.3002700805664
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.724613904953003 2.4029951095581055 121.87437438964844
Loss :  1.7189339399337769 2.005086898803711 101.97328186035156
Loss :  1.7138925790786743 2.159332752227783 109.68052673339844
Loss :  1.7198325395584106 2.3673768043518066 120.08867645263672
Loss :  1.7107672691345215 2.2651026248931885 114.96590423583984
Loss :  1.7235113382339478 2.5370824337005615 128.57763671875
Loss :  1.737423300743103 2.6862449645996094 136.0496826171875
Loss :  1.7168948650360107 3.418639659881592 172.64886474609375
Loss :  1.7436957359313965 1.8524714708328247 94.36727142333984
Loss :  1.7194520235061646 2.158649444580078 109.65192413330078
Loss :  1.733333945274353 2.178485870361328 110.65763092041016
Loss :  1.731095314025879 2.4562697410583496 124.5445785522461
Loss :  1.722348928451538 2.068476915359497 105.14619445800781
Loss :  1.7345573902130127 2.4147675037384033 122.47293090820312
Loss :  1.7171618938446045 1.803724765777588 91.90340423583984
Loss :  1.7435545921325684 2.4498579502105713 124.2364501953125
Loss :  1.7202820777893066 2.191020965576172 111.27133178710938
Loss :  1.7134664058685303 1.9644131660461426 99.93412017822266
Loss :  1.719933032989502 2.564124584197998 129.92616271972656
Loss :  1.7464569807052612 2.093675374984741 106.43022918701172
  batch 60 loss: 1.7464569807052612, 2.093675374984741, 106.43022918701172
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7138875722885132 2.9289968013763428 148.16372680664062
Loss :  1.7259207963943481 2.0237035751342773 102.91110229492188
Loss :  1.7173057794570923 2.71183443069458 137.30902099609375
Loss :  1.7125742435455322 2.5630102157592773 129.8630828857422
Loss :  1.7039717435836792 2.302870273590088 116.84748840332031
Loss :  1.7187825441360474 3.9834437370300293 200.89096069335938
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.7237718105316162 3.977888822555542 200.6182098388672
Loss :  1.722263216972351 3.813494920730591 192.39700317382812
Loss :  1.725904107093811 3.92874813079834 198.16331481933594
Total LOSS train 125.13595639742337 valid 198.01737213134766
CE LOSS train 1.7232326654287484 valid 0.43147602677345276
Contrastive LOSS train 2.468254465323228 valid 0.982187032699585
EPOCH 296:
Loss :  1.7317593097686768 2.3757290840148926 120.5182113647461
Loss :  1.7370903491973877 2.845302104949951 144.002197265625
Loss :  1.7242038249969482 2.2776074409484863 115.60457611083984
Loss :  1.7273658514022827 2.1523988246917725 109.34730529785156
Loss :  1.734251856803894 2.560913324356079 129.77992248535156
Loss :  1.7192267179489136 2.6518523693084717 134.3118438720703
Loss :  1.7343151569366455 2.4745092391967773 125.45977783203125
Loss :  1.7239834070205688 2.744784355163574 138.96319580078125
Loss :  1.720220685005188 2.174178123474121 110.42913055419922
Loss :  1.7345830202102661 2.4114010334014893 122.30463409423828
Loss :  1.7165000438690186 2.647258758544922 134.07945251464844
Loss :  1.7159545421600342 2.204955816268921 111.9637451171875
Loss :  1.7152056694030762 2.354381799697876 119.43429565429688
Loss :  1.7179769277572632 2.2471089363098145 114.07342529296875
Loss :  1.7405407428741455 3.7246253490448 187.9718017578125
Loss :  1.7386813163757324 4.24650239944458 214.0637969970703
Loss :  1.714153528213501 2.74481463432312 138.95489501953125
Loss :  1.7256536483764648 2.3670833110809326 120.07981872558594
Loss :  1.7128093242645264 2.166966199874878 110.06111907958984
Loss :  1.738567590713501 2.6813035011291504 135.80374145507812
  batch 20 loss: 1.738567590713501, 2.6813035011291504, 135.80374145507812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7234501838684082 2.8102195262908936 142.2344207763672
Loss :  1.712543249130249 2.594330072402954 131.42904663085938
Loss :  1.7199889421463013 2.456233024597168 124.5316390991211
Loss :  1.7266291379928589 2.940072774887085 148.73025512695312
Loss :  1.7385011911392212 2.4229326248168945 122.8851318359375
Loss :  1.721306324005127 2.1938493251800537 111.41377258300781
Loss :  1.7252141237258911 2.289189338684082 116.18467712402344
Loss :  1.7230701446533203 2.50581693649292 127.013916015625
Loss :  1.7014328241348267 2.570713996887207 130.2371368408203
Loss :  1.7367658615112305 3.0267179012298584 153.07266235351562
Loss :  1.7016527652740479 2.5168282985687256 127.5430679321289
Loss :  1.7299493551254272 2.440131425857544 123.73651885986328
Loss :  1.719813585281372 3.0843207836151123 155.93585205078125
Loss :  1.7193766832351685 2.8198487758636475 142.71180725097656
Loss :  1.704405665397644 2.5705831050872803 130.2335662841797
Loss :  1.7108982801437378 3.147695302963257 159.0956573486328
Loss :  1.7106186151504517 2.564161777496338 129.918701171875
Loss :  1.735793113708496 2.4352943897247314 123.5005111694336
Loss :  1.7379176616668701 2.421337842941284 122.8048095703125
Loss :  1.7418659925460815 2.385324239730835 121.0080795288086
  batch 40 loss: 1.7418659925460815, 2.385324239730835, 121.0080795288086
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7253639698028564 2.9410438537597656 148.7775421142578
Loss :  1.7194249629974365 1.9942280054092407 101.43082427978516
Loss :  1.7146718502044678 2.367508888244629 120.0901107788086
Loss :  1.7203036546707153 2.1624865531921387 109.84463500976562
Loss :  1.7114806175231934 2.333434820175171 118.38321685791016
Loss :  1.7242450714111328 2.1162610054016113 107.53729248046875
Loss :  1.737919569015503 2.2992825508117676 116.7020492553711
Loss :  1.717783808708191 2.5131585597991943 127.3757095336914
Loss :  1.7441396713256836 3.2771832942962646 165.60330200195312
Loss :  1.7204638719558716 1.9533131122589111 99.38612365722656
Loss :  1.7341891527175903 2.2293691635131836 113.20265197753906
Loss :  1.7320746183395386 2.395596504211426 121.51189422607422
Loss :  1.7233878374099731 2.4686174392700195 125.15425872802734
Loss :  1.7354066371917725 2.421887159347534 122.82976531982422
Loss :  1.718297004699707 2.415266752243042 122.48163604736328
Loss :  1.744350552558899 2.2887542247772217 116.18206787109375
Loss :  1.7213428020477295 2.2119288444519043 112.31778717041016
Loss :  1.714591383934021 2.2864773273468018 116.03845977783203
Loss :  1.7209203243255615 2.5894978046417236 131.19580078125
Loss :  1.7471734285354614 2.0572896003723145 104.61165618896484
  batch 60 loss: 1.7471734285354614, 2.0572896003723145, 104.61165618896484
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7149896621704102 2.1746482849121094 110.44740295410156
Loss :  1.726868987083435 2.124706983566284 107.96221923828125
Loss :  1.7183605432510376 2.013368844985962 102.38680267333984
Loss :  1.713761329650879 2.6144394874572754 132.43572998046875
Loss :  1.7052987813949585 2.081275224685669 105.76905822753906
Loss :  1.720773458480835 4.258650779724121 214.6533203125
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.7257007360458374 4.244933128356934 213.9723663330078
Loss :  1.7240958213806152 4.138583660125732 208.6532745361328
Loss :  1.7275241613388062 4.043126583099365 203.88385009765625
Total LOSS train 126.78594019963191 valid 210.29070281982422
CE LOSS train 1.7241699585547814 valid 0.43188104033470154
Contrastive LOSS train 2.5012354208872867 valid 1.0107816457748413
EPOCH 297:
Loss :  1.7326891422271729 2.217374324798584 112.60140228271484
Loss :  1.7378987073898315 2.363966226577759 119.93621063232422
Loss :  1.7252236604690552 2.1770620346069336 110.57833099365234
Loss :  1.7281324863433838 2.4033544063568115 121.8958511352539
Loss :  1.7348010540008545 1.9270155429840088 98.08557891845703
Loss :  1.720288872718811 2.034018039703369 103.42119598388672
Loss :  1.7348746061325073 2.198002576828003 111.63500213623047
Loss :  1.7247165441513062 1.9468170404434204 99.06556701660156
Loss :  1.7209720611572266 1.8253443241119385 92.98818969726562
Loss :  1.735107183456421 1.9063630104064941 97.05326080322266
Loss :  1.7172417640686035 2.4250998497009277 122.97222900390625
Loss :  1.7165480852127075 2.4261786937713623 123.02548217773438
Loss :  1.715769648551941 2.909557819366455 147.19366455078125
Loss :  1.7183367013931274 2.4814438819885254 125.79053497314453
Loss :  1.7408591508865356 2.0687756538391113 105.17964172363281
Loss :  1.7391752004623413 2.4297592639923096 123.22713470458984
Loss :  1.714412808418274 2.5047595500946045 126.952392578125
Loss :  1.725874900817871 2.6013522148132324 131.79348754882812
Loss :  1.7127236127853394 3.166912317276001 160.05833435058594
Loss :  1.7380868196487427 2.400928020477295 121.78449249267578
  batch 20 loss: 1.7380868196487427, 2.400928020477295, 121.78449249267578
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7228052616119385 2.3449480533599854 118.97021484375
Loss :  1.711672306060791 2.497013807296753 126.56236267089844
Loss :  1.718993902206421 2.3446919918060303 118.9535903930664
Loss :  1.7256354093551636 2.5164601802825928 127.54863739013672
Loss :  1.7376327514648438 2.3135221004486084 117.41373443603516
Loss :  1.720516562461853 3.34100341796875 168.77069091796875
Loss :  1.7245168685913086 2.807076930999756 142.078369140625
Loss :  1.7225866317749023 2.639397144317627 133.69244384765625
Loss :  1.7009172439575195 2.5445215702056885 128.927001953125
Loss :  1.7362838983535767 2.5369584560394287 128.58421325683594
Loss :  1.7011624574661255 2.7848031520843506 140.94131469726562
Loss :  1.7297521829605103 2.4726462364196777 125.362060546875
Loss :  1.719759225845337 2.8477399349212646 144.10675048828125
Loss :  1.7194665670394897 2.944319725036621 148.93545532226562
Loss :  1.7045804262161255 3.504765748977661 176.94285583496094
Loss :  1.7109779119491577 3.3442177772521973 168.921875
Loss :  1.7108045816421509 2.9085958003997803 147.14059448242188
Loss :  1.7361565828323364 2.4822373390197754 125.84803009033203
Loss :  1.7382960319519043 2.883131265640259 145.89486694335938
Loss :  1.7421929836273193 2.4361674785614014 123.55056762695312
  batch 40 loss: 1.7421929836273193, 2.4361674785614014, 123.55056762695312
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7259297370910645 2.5338103771209717 128.4164581298828
Loss :  1.7198681831359863 3.013334035873413 152.38656616210938
Loss :  1.7152352333068848 3.1029622554779053 156.8633575439453
Loss :  1.7207950353622437 3.237835168838501 163.612548828125
Loss :  1.7119114398956299 1.9893354177474976 101.1786880493164
Loss :  1.7243621349334717 2.909522294998169 147.2004852294922
Loss :  1.738215446472168 2.369370460510254 120.20673370361328
Loss :  1.7178239822387695 2.3504271507263184 119.23918151855469
Loss :  1.7440940141677856 2.3457353115081787 119.0308609008789
Loss :  1.72049880027771 2.329638957977295 118.20244598388672
Loss :  1.734002947807312 2.783608913421631 140.91445922851562
Loss :  1.7320092916488647 2.6300742626190186 133.2357177734375
Loss :  1.723239779472351 2.1469430923461914 109.07038879394531
Loss :  1.7353847026824951 2.4805593490600586 125.76335144042969
Loss :  1.7177696228027344 2.882685661315918 145.85205078125
Loss :  1.7443474531173706 2.3384435176849365 118.66651916503906
Loss :  1.7210170030593872 2.2806572914123535 115.7538833618164
Loss :  1.7141860723495483 2.0746817588806152 105.44827270507812
Loss :  1.720686912536621 2.189945697784424 111.21797180175781
Loss :  1.7471821308135986 1.739809513092041 88.73766326904297
  batch 60 loss: 1.7471821308135986, 1.739809513092041, 88.73766326904297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7147496938705444 2.3918745517730713 121.30847930908203
Loss :  1.726615309715271 2.6356213092803955 133.50767517089844
Loss :  1.7180207967758179 2.4059560298919678 122.01581573486328
Loss :  1.7133877277374268 2.416386842727661 122.53273010253906
Loss :  1.7047995328903198 3.9276301860809326 198.08631896972656
Loss :  1.716956615447998 3.781672954559326 190.80059814453125
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.7218115329742432 3.896273374557495 196.53549194335938
Loss :  1.7199733257293701 3.6959598064422607 186.51795959472656
Loss :  1.7244685888290405 3.767949342727661 190.12193298339844
Total LOSS train 128.19741903451774 valid 190.9939956665039
CE LOSS train 1.7242242427972647 valid 0.43111714720726013
Contrastive LOSS train 2.5294638817126933 valid 0.9419873356819153
EPOCH 298:
Loss :  1.7322477102279663 2.7268290519714355 138.07369995117188
Loss :  1.7374900579452515 2.3642284870147705 119.94891357421875
Loss :  1.7248420715332031 2.26942777633667 115.19622802734375
Loss :  1.7276968955993652 2.4313719272613525 123.29629516601562
Loss :  1.7345898151397705 2.247593402862549 114.1142578125
Loss :  1.7199376821517944 2.3644118309020996 119.9405288696289
Loss :  1.7345081567764282 2.5741171836853027 130.44036865234375
Loss :  1.7241712808609009 2.6543846130371094 134.4434051513672
Loss :  1.7201265096664429 2.759679079055786 139.70407104492188
Loss :  1.734263300895691 3.2993781566619873 166.7031707763672
Loss :  1.7162432670593262 3.014228582382202 152.42767333984375
Loss :  1.715726375579834 2.971256971359253 150.27857971191406
Loss :  1.7147105932235718 2.6846089363098145 135.94515991210938
Loss :  1.717481017112732 2.5436935424804688 128.90216064453125
Loss :  1.7402487993240356 2.5150985717773438 127.49517822265625
Loss :  1.7384274005889893 3.096097707748413 156.54331970214844
Loss :  1.7137974500656128 2.3146255016326904 117.44507598876953
Loss :  1.7253223657608032 2.5516703128814697 129.308837890625
Loss :  1.712571382522583 2.2717108726501465 115.29811096191406
Loss :  1.7383983135223389 2.2746124267578125 115.4690170288086
  batch 20 loss: 1.7383983135223389, 2.2746124267578125, 115.4690170288086
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.723040223121643 2.447024345397949 124.07425689697266
Loss :  1.7120189666748047 2.1910135746002197 111.26270294189453
Loss :  1.7195204496383667 2.0885705947875977 106.1480484008789
Loss :  1.726430058479309 2.297791004180908 116.61598205566406
Loss :  1.7383843660354614 2.0102508068084717 102.25093078613281
Loss :  1.721173644065857 2.344308376312256 118.93659210205078
Loss :  1.7251408100128174 2.2663962841033936 115.04496002197266
Loss :  1.7231812477111816 2.4760708808898926 125.52672576904297
Loss :  1.701839566230774 3.052283525466919 154.31602478027344
Loss :  1.7369543313980103 2.4212915897369385 122.8015365600586
Loss :  1.7020114660263062 2.260105848312378 114.70730590820312
Loss :  1.7302058935165405 2.2042136192321777 111.94088745117188
Loss :  1.7200868129730225 3.0272135734558105 153.0807647705078
Loss :  1.719694972038269 3.9211318492889404 197.7762908935547
Loss :  1.7052266597747803 2.8454792499542236 143.97918701171875
Loss :  1.7113453149795532 2.618799924850464 132.65135192871094
Loss :  1.7108609676361084 2.850806474685669 144.25119018554688
Loss :  1.7356916666030884 2.3259215354919434 118.03176879882812
Loss :  1.7376903295516968 2.238999366760254 113.68766021728516
Loss :  1.7415404319763184 2.0945916175842285 106.47112274169922
  batch 40 loss: 1.7415404319763184, 2.0945916175842285, 106.47112274169922
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7250123023986816 2.3072242736816406 117.08622741699219
Loss :  1.718928337097168 2.8532192707061768 144.37989807128906
Loss :  1.7142665386199951 2.8575870990753174 144.59361267089844
Loss :  1.7200090885162354 2.555295467376709 129.4847869873047
Loss :  1.710997223854065 2.4413650035858154 123.77925109863281
Loss :  1.7238961458206177 2.5301549434661865 128.2316436767578
Loss :  1.7379013299942017 2.229170560836792 113.19642639160156
Loss :  1.7169737815856934 3.008247137069702 152.12933349609375
Loss :  1.7439963817596436 3.339757204055786 168.7318572998047
Loss :  1.7200995683670044 2.316653251647949 117.55276489257812
Loss :  1.7336074113845825 2.6237711906433105 132.9221649169922
Loss :  1.7319841384887695 2.588794231414795 131.17169189453125
Loss :  1.7231413125991821 2.5024964809417725 126.84796905517578
Loss :  1.7354130744934082 2.19423246383667 111.44703674316406
Loss :  1.7178102731704712 2.1927490234375 111.35526275634766
Loss :  1.7443091869354248 3.5424797534942627 178.86830139160156
Loss :  1.7210586071014404 2.611091375350952 132.27561950683594
Loss :  1.714531421661377 2.3922924995422363 121.32915496826172
Loss :  1.7207717895507812 2.9490106105804443 149.17129516601562
Loss :  1.7471487522125244 2.0831258296966553 105.90343475341797
  batch 60 loss: 1.7471487522125244, 2.0831258296966553, 105.90343475341797
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7147681713104248 2.3245913982391357 117.9443359375
Loss :  1.726742148399353 2.1725077629089355 110.35213470458984
Loss :  1.7181483507156372 2.1820485591888428 110.82057189941406
Loss :  1.7138216495513916 3.2076613903045654 162.09689331054688
Loss :  1.705430269241333 1.9069679975509644 97.05382537841797
Loss :  1.7204492092132568 4.350066661834717 219.22377014160156
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.7252053022384644 4.35611629486084 219.53102111816406
Loss :  1.7233446836471558 4.277714729309082 215.6090850830078
Loss :  1.7264927625656128 4.150100231170654 209.2314910888672
Total LOSS train 129.28084364670974 valid 215.89884185791016
CE LOSS train 1.7240862442896916 valid 0.4316231906414032
Contrastive LOSS train 2.5511351346969606 valid 1.0375250577926636
EPOCH 299:
Loss :  1.7328650951385498 2.159484386444092 109.70708465576172
Loss :  1.7379094362258911 2.3240163326263428 117.938720703125
Loss :  1.7253038883209229 2.2611544132232666 114.78302764892578
Loss :  1.7283703088760376 2.759539842605591 139.70535278320312
Loss :  1.7348028421401978 2.201284646987915 111.79903411865234
Loss :  1.7203788757324219 2.545912981033325 129.01602172851562
Loss :  1.7349823713302612 2.5334854125976562 128.4092559814453
Loss :  1.7247381210327148 2.4257328510284424 123.01138305664062
Loss :  1.7208914756774902 2.6421709060668945 133.82943725585938
Loss :  1.7351853847503662 2.805699110031128 142.0201416015625
Loss :  1.7165794372558594 3.9217233657836914 197.80274963378906
Loss :  1.7162353992462158 3.0168497562408447 152.5587158203125
Loss :  1.7155948877334595 2.7520217895507812 139.31668090820312
Loss :  1.7182674407958984 2.790778875350952 141.2572021484375
Loss :  1.740699291229248 2.7598073482513428 139.73106384277344
Loss :  1.7389228343963623 2.7849884033203125 140.98834228515625
Loss :  1.7143107652664185 3.062195301055908 154.82408142089844
Loss :  1.7257555723190308 2.600552797317505 131.75338745117188
Loss :  1.712739109992981 2.5495219230651855 129.1888427734375
Loss :  1.7384730577468872 3.0997374057769775 156.72535705566406
  batch 20 loss: 1.7384730577468872, 3.0997374057769775, 156.72535705566406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7233198881149292 2.2855780124664307 116.0022201538086
Loss :  1.712407112121582 2.4935452938079834 126.3896713256836
Loss :  1.7197281122207642 2.5611512660980225 129.77728271484375
Loss :  1.7265664339065552 2.891719341278076 146.31253051757812
Loss :  1.738307237625122 2.370083808898926 120.24249267578125
Loss :  1.7211960554122925 2.210462808609009 112.24433898925781
Loss :  1.7250560522079468 2.3898305892944336 121.21659088134766
Loss :  1.7230284214019775 2.478156089782715 125.6308364868164
Loss :  1.7015808820724487 3.0676329135894775 155.08323669433594
Loss :  1.7365115880966187 2.5578415393829346 129.6285858154297
Loss :  1.7017138004302979 2.516364812850952 127.51995086669922
Loss :  1.729986548423767 2.4191696643829346 122.68846893310547
Loss :  1.7200881242752075 2.3707544803619385 120.2578125
Loss :  1.7197511196136475 2.3563690185546875 119.53820037841797
Loss :  1.7047783136367798 2.9893639087677 151.1729736328125
Loss :  1.7110837697982788 2.456791877746582 124.55067443847656
Loss :  1.7108479738235474 2.954770803451538 149.4493865966797
Loss :  1.7358875274658203 2.320002317428589 117.73600006103516
Loss :  1.7381136417388916 2.7082407474517822 137.150146484375
Loss :  1.7419930696487427 2.3593130111694336 119.70764923095703
  batch 40 loss: 1.7419930696487427, 2.3593130111694336, 119.70764923095703
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.725683569908142 2.654653787612915 134.4583740234375
Loss :  1.7197973728179932 2.072577476501465 105.34867095947266
Loss :  1.7152440547943115 2.304480791091919 116.93927764892578
Loss :  1.7206875085830688 2.1790266036987305 110.6720199584961
Loss :  1.7118993997573853 2.1714601516723633 110.28490447998047
Loss :  1.7243196964263916 3.4390170574188232 173.6751708984375
Loss :  1.7381900548934937 2.1837050914764404 110.92344665527344
Loss :  1.7174901962280273 2.2019171714782715 111.81334686279297
Loss :  1.744238018989563 2.204789638519287 111.98371887207031
Loss :  1.7203820943832397 2.6695287227630615 135.1968231201172
Loss :  1.7339853048324585 2.6261579990386963 133.04188537597656
Loss :  1.7322415113449097 2.4262893199920654 123.04670715332031
Loss :  1.7235825061798096 2.4340131282806396 123.42423248291016
Loss :  1.735580563545227 2.861159563064575 144.79356384277344
Loss :  1.718366265296936 2.4955718517303467 126.49696350097656
Loss :  1.744523048400879 2.432708978652954 123.37997436523438
Loss :  1.7215107679367065 3.0002517700195312 151.73410034179688
Loss :  1.7149039506912231 1.9464670419692993 99.03825378417969
Loss :  1.721100926399231 2.526028871536255 128.02255249023438
Loss :  1.7473249435424805 2.2918128967285156 116.33796691894531
  batch 60 loss: 1.7473249435424805, 2.2918128967285156, 116.33796691894531
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7152618169784546 2.348231554031372 119.12683868408203
Loss :  1.726998209953308 2.4252753257751465 122.99076080322266
Loss :  1.7186323404312134 2.10931134223938 107.1842041015625
Loss :  1.7139047384262085 2.419188976287842 122.67335510253906
Loss :  1.7055931091308594 1.9328008890151978 98.34564208984375
Loss :  1.7213959693908691 3.9846811294555664 200.9554443359375
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.7262808084487915 3.9054999351501465 197.00128173828125
Loss :  1.7245995998382568 3.7957677841186523 191.5129852294922
Loss :  1.7285213470458984 3.86537504196167 194.9972686767578
Total LOSS train 128.7322721041166 valid 196.1167449951172
CE LOSS train 1.724406049801753 valid 0.4321303367614746
Contrastive LOSS train 2.5401573254511907 valid 0.9663437604904175
EPOCH 300:
Loss :  1.7328604459762573 2.720789909362793 137.77235412597656
Loss :  1.7380974292755127 2.983560800552368 150.9161376953125
Loss :  1.7255136966705322 1.9431172609329224 98.88137817382812
Loss :  1.7284281253814697 2.6077535152435303 132.11610412597656
Loss :  1.7351469993591309 1.9446464776992798 98.96746826171875
Loss :  1.7206175327301025 2.097115993499756 106.576416015625
Loss :  1.7351139783859253 2.971574544906616 150.31382751464844
Loss :  1.7250181436538696 2.7350876331329346 138.47940063476562
Loss :  1.721171498298645 2.233177423477173 113.38004302978516
Loss :  1.734843373298645 2.743582248687744 138.91395568847656
Loss :  1.7170182466506958 2.7641117572784424 139.922607421875
Loss :  1.7165156602859497 2.4607033729553223 124.7516860961914
Loss :  1.7155777215957642 2.224696636199951 112.95040893554688
Loss :  1.718385100364685 2.09903621673584 106.67019653320312
Loss :  1.7406600713729858 2.621854543685913 132.83338928222656
Loss :  1.7387843132019043 2.311817169189453 117.32964324951172
Loss :  1.714281439781189 2.3180532455444336 117.616943359375
Loss :  1.7256885766983032 2.459761619567871 124.7137680053711
Loss :  1.712751865386963 2.1245789527893066 107.94170379638672
Loss :  1.738255500793457 2.387830972671509 121.12980651855469
  batch 20 loss: 1.738255500793457, 2.387830972671509, 121.12980651855469
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7233425378799438 2.256542205810547 114.55045318603516
Loss :  1.7123186588287354 2.6072113513946533 132.07289123535156
Loss :  1.7196218967437744 2.4131460189819336 122.37692260742188
Loss :  1.7265690565109253 2.281383752822876 115.79576110839844
Loss :  1.73834228515625 2.263758659362793 114.92627716064453
Loss :  1.7212218046188354 2.1616146564483643 109.80195617675781
Loss :  1.7250442504882812 2.2994027137756348 116.69518280029297
Loss :  1.7230242490768433 2.158086061477661 109.62732696533203
Loss :  1.70172119140625 2.2268879413604736 113.0461196899414
Loss :  1.736775279045105 2.4146182537078857 122.46768951416016
Loss :  1.7021440267562866 2.5564470291137695 129.5244903564453
Loss :  1.7304331064224243 2.3295693397521973 118.20890045166016
Loss :  1.7208000421524048 2.0323100090026855 103.3363037109375
Loss :  1.7205514907836914 2.3011906147003174 116.78008270263672
Loss :  1.7058669328689575 3.501877784729004 176.7997589111328
Loss :  1.7120881080627441 4.118974208831787 207.66079711914062
Loss :  1.7120758295059204 2.349696397781372 119.19689178466797
Loss :  1.7367843389511108 2.27972674369812 115.7231216430664
Loss :  1.7387278079986572 3.2104592323303223 162.26168823242188
Loss :  1.7425007820129395 2.396595001220703 121.57225036621094
  batch 40 loss: 1.7425007820129395, 2.396595001220703, 121.57225036621094
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.726426124572754 2.482530355453491 125.85294342041016
Loss :  1.7204923629760742 2.0964486598968506 106.54292297363281
Loss :  1.7159366607666016 2.584775686264038 130.95472717285156
Loss :  1.7214832305908203 3.2607407569885254 164.75851440429688
Loss :  1.7128746509552002 3.620966911315918 182.76121520996094
Loss :  1.7250779867172241 2.726940393447876 138.0720977783203
Loss :  1.7385790348052979 2.840606451034546 143.76890563964844
Loss :  1.718677043914795 3.124244451522827 157.93089294433594
Loss :  1.7443994283676147 2.1816887855529785 110.82884216308594
Loss :  1.7210710048675537 2.8907790184020996 146.26002502441406
Loss :  1.7344255447387695 2.475506544113159 125.50975799560547
Loss :  1.7325239181518555 2.5919158458709717 131.3283233642578
Loss :  1.7239351272583008 2.5610451698303223 129.7761993408203
Loss :  1.7358955144882202 2.331592559814453 118.31552124023438
Loss :  1.7186763286590576 3.1325042247772217 158.34388732910156
Loss :  1.7447874546051025 2.781648874282837 140.8272247314453
Loss :  1.7220309972763062 3.10461688041687 156.952880859375
Loss :  1.715384840965271 3.788071632385254 191.11895751953125
Loss :  1.7215797901153564 2.846076011657715 144.02537536621094
Loss :  1.7477355003356934 2.25911021232605 114.7032470703125
  batch 60 loss: 1.7477355003356934, 2.25911021232605, 114.7032470703125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7161738872528076 2.2974445819854736 116.5884017944336
Loss :  1.7278783321380615 2.839348793029785 143.6953125
Loss :  1.7196545600891113 3.0079565048217773 152.1174774169922
Loss :  1.7150331735610962 2.83697772026062 143.5639190673828
Loss :  1.70683753490448 1.9683138132095337 100.12252807617188
Loss :  1.71835458278656 3.816174030303955 192.5270538330078
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.7231632471084595 3.9914791584014893 201.297119140625
Loss :  1.7216603755950928 3.8264310359954834 193.043212890625
Loss :  1.7251282930374146 3.643216609954834 183.88595581054688
Total LOSS train 130.6049569936899 valid 192.68833541870117
CE LOSS train 1.7248038988846999 valid 0.43128207325935364
Contrastive LOSS train 2.5776030632165763 valid 0.9108041524887085
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 0.056 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: \ 0.181 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: | 0.321 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 0.321 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 0.353 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: \ 0.454 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: | 0.454 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 0.454 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 0.454 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: \ 0.454 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: | 0.532 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 0.532 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 0.618 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: \ 0.618 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: | 0.712 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 0.751 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 0.829 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: \ 0.915 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: | 1.009 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 1.110 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 1.204 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: \ 1.329 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: | 1.439 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 1.532 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 1.538 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: \ 1.538 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: | 1.538 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 1.538 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 1.538 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: \ 1.538 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: | 1.538 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 1.538 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 1.538 MB of 1.538 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                    epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:                       lr ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   train contrastive loss ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: train cross-entropy loss ‚ñÜ‚ñà‚ñÉ‚ñÇ‚ñÜ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ
wandb:               train loss ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:     val contrastive loss ‚ñà‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÇ‚ñÜ‚ñÜ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñá‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÑ
wandb:   val cross-entropy loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                 val loss ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÅ‚ñÜ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñá‚ñÖ‚ñÜ‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÑ
wandb: 
wandb: Run summary:
wandb:            best_val_loss 154.82337
wandb:                    epoch 300
wandb:                       lr 0.00094
wandb:   train contrastive loss 2.5776
wandb: train cross-entropy loss 1.7248
wandb:               train loss 130.60496
wandb:     val contrastive loss 0.9108
wandb:   val cross-entropy loss 0.43128
wandb:                 val loss 192.68834
wandb: 
wandb: Synced beaming-tiger-27: https://wandb.ai/harsh21122/part_segmentation/runs/37o4fjdw
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230206_004030-37o4fjdw/logs

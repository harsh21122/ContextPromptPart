wandb: Currently logged in as: harsh21122. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.3
wandb: Run data is saved locally in /ssd-scratch/harsh21122/ContextPromptPart/wandb/run-20230206_105659-16u9ivv6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run winter-bush-28
wandb: ‚≠êÔ∏è View project at https://wandb.ai/harsh21122/part_segmentation
wandb: üöÄ View run at https://wandb.ai/harsh21122/part_segmentation/runs/16u9ivv6
/home/harsh21122/.conda/envs/part/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
/home/harsh21122/.conda/envs/part/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Arguments are :  Namespace(batch_size=10, epochs=300, starting_epoch=1, base_lr=0.3, clip_model='RN50', result_dir='./Results/', model_dir='../ContextPromptPart_model', dataset_dir='/home/harsh21122/tmp/cat_dataset', resume=False, model_name='../ContextPromptPart_model/last_model', calc_accuracy_training=False, multi_step_scheduler=True, lr_decay=0.1, milestones=[20, 100, 150, 200, 250, 300], wandb=True, temperature=0.19, layer_len=-1, ref_layer1='relu3_2', ref_layer2='relu5_4', ref_weight1=0.33, ref_weight2=1.0, lamda_contrastive=1.0, lamda_cross=0.0, weight_decay=0.01)
Using device :  cuda
Length of Train dataset : 646 and Test dataset : 35
Length of Train loader : 65 and Test loader : 4
self.partnames :  ['background', 'head', 'neck', 'torso', 'tail', 'legs']
prompts :  [' the background of the cat.', ' the head of the cat.', ' the neck of the cat.', ' the torso of the cat.', ' the tail of the cat.', ' the legs of the cat.']
model device :  cuda
self.prompts : torch.Size([6, 77])
self.prompts : torch.Size([6, 1024])
self.prompts rquires grad :  <built-in method requires_grad_ of Tensor object at 0x7f32dc8c24a0>
layer: relu3_2,relu5_4
weighs: [0.33, 1.0]
Printing parameters and their gradient
gamma True
image_encoder.0.weight False
image_encoder.1.weight False
image_encoder.1.bias False
image_encoder.3.weight False
image_encoder.4.weight False
image_encoder.4.bias False
image_encoder.6.weight False
image_encoder.7.weight False
image_encoder.7.bias False
image_encoder.10.0.conv1.weight False
image_encoder.10.0.bn1.weight False
image_encoder.10.0.bn1.bias False
image_encoder.10.0.conv2.weight False
image_encoder.10.0.bn2.weight False
image_encoder.10.0.bn2.bias False
image_encoder.10.0.conv3.weight False
image_encoder.10.0.bn3.weight False
image_encoder.10.0.bn3.bias False
image_encoder.10.0.downsample.0.weight False
image_encoder.10.0.downsample.1.weight False
image_encoder.10.0.downsample.1.bias False
image_encoder.10.1.conv1.weight False
image_encoder.10.1.bn1.weight False
image_encoder.10.1.bn1.bias False
image_encoder.10.1.conv2.weight False
image_encoder.10.1.bn2.weight False
image_encoder.10.1.bn2.bias False
image_encoder.10.1.conv3.weight False
image_encoder.10.1.bn3.weight False
image_encoder.10.1.bn3.bias False
image_encoder.10.2.conv1.weight False
image_encoder.10.2.bn1.weight False
image_encoder.10.2.bn1.bias False
image_encoder.10.2.conv2.weight False
image_encoder.10.2.bn2.weight False
image_encoder.10.2.bn2.bias False
image_encoder.10.2.conv3.weight False
image_encoder.10.2.bn3.weight False
image_encoder.10.2.bn3.bias False
image_encoder.11.0.conv1.weight False
image_encoder.11.0.bn1.weight False
image_encoder.11.0.bn1.bias False
image_encoder.11.0.conv2.weight False
image_encoder.11.0.bn2.weight False
image_encoder.11.0.bn2.bias False
image_encoder.11.0.conv3.weight False
image_encoder.11.0.bn3.weight False
image_encoder.11.0.bn3.bias False
image_encoder.11.0.downsample.0.weight False
image_encoder.11.0.downsample.1.weight False
image_encoder.11.0.downsample.1.bias False
image_encoder.11.1.conv1.weight False
image_encoder.11.1.bn1.weight False
image_encoder.11.1.bn1.bias False
image_encoder.11.1.conv2.weight False
image_encoder.11.1.bn2.weight False
image_encoder.11.1.bn2.bias False
image_encoder.11.1.conv3.weight False
image_encoder.11.1.bn3.weight False
image_encoder.11.1.bn3.bias False
image_encoder.11.2.conv1.weight False
image_encoder.11.2.bn1.weight False
image_encoder.11.2.bn1.bias False
image_encoder.11.2.conv2.weight False
image_encoder.11.2.bn2.weight False
image_encoder.11.2.bn2.bias False
image_encoder.11.2.conv3.weight False
image_encoder.11.2.bn3.weight False
image_encoder.11.2.bn3.bias False
image_encoder.11.3.conv1.weight False
image_encoder.11.3.bn1.weight False
image_encoder.11.3.bn1.bias False
image_encoder.11.3.conv2.weight False
image_encoder.11.3.bn2.weight False
image_encoder.11.3.bn2.bias False
image_encoder.11.3.conv3.weight False
image_encoder.11.3.bn3.weight False
image_encoder.11.3.bn3.bias False
image_encoder.12.0.conv1.weight False
image_encoder.12.0.bn1.weight False
image_encoder.12.0.bn1.bias False
image_encoder.12.0.conv2.weight False
image_encoder.12.0.bn2.weight False
image_encoder.12.0.bn2.bias False
image_encoder.12.0.conv3.weight False
image_encoder.12.0.bn3.weight False
image_encoder.12.0.bn3.bias False
image_encoder.12.0.downsample.0.weight False
image_encoder.12.0.downsample.1.weight False
image_encoder.12.0.downsample.1.bias False
image_encoder.12.1.conv1.weight False
image_encoder.12.1.bn1.weight False
image_encoder.12.1.bn1.bias False
image_encoder.12.1.conv2.weight False
image_encoder.12.1.bn2.weight False
image_encoder.12.1.bn2.bias False
image_encoder.12.1.conv3.weight False
image_encoder.12.1.bn3.weight False
image_encoder.12.1.bn3.bias False
image_encoder.12.2.conv1.weight False
image_encoder.12.2.bn1.weight False
image_encoder.12.2.bn1.bias False
image_encoder.12.2.conv2.weight False
image_encoder.12.2.bn2.weight False
image_encoder.12.2.bn2.bias False
image_encoder.12.2.conv3.weight False
image_encoder.12.2.bn3.weight False
image_encoder.12.2.bn3.bias False
image_encoder.12.3.conv1.weight False
image_encoder.12.3.bn1.weight False
image_encoder.12.3.bn1.bias False
image_encoder.12.3.conv2.weight False
image_encoder.12.3.bn2.weight False
image_encoder.12.3.bn2.bias False
image_encoder.12.3.conv3.weight False
image_encoder.12.3.bn3.weight False
image_encoder.12.3.bn3.bias False
image_encoder.12.4.conv1.weight False
image_encoder.12.4.bn1.weight False
image_encoder.12.4.bn1.bias False
image_encoder.12.4.conv2.weight False
image_encoder.12.4.bn2.weight False
image_encoder.12.4.bn2.bias False
image_encoder.12.4.conv3.weight False
image_encoder.12.4.bn3.weight False
image_encoder.12.4.bn3.bias False
image_encoder.12.5.conv1.weight False
image_encoder.12.5.bn1.weight False
image_encoder.12.5.bn1.bias False
image_encoder.12.5.conv2.weight False
image_encoder.12.5.bn2.weight False
image_encoder.12.5.bn2.bias False
image_encoder.12.5.conv3.weight False
image_encoder.12.5.bn3.weight False
image_encoder.12.5.bn3.bias False
image_encoder.13.0.conv1.weight False
image_encoder.13.0.bn1.weight False
image_encoder.13.0.bn1.bias False
image_encoder.13.0.conv2.weight False
image_encoder.13.0.bn2.weight False
image_encoder.13.0.bn2.bias False
image_encoder.13.0.conv3.weight False
image_encoder.13.0.bn3.weight False
image_encoder.13.0.bn3.bias False
image_encoder.13.0.downsample.0.weight False
image_encoder.13.0.downsample.1.weight False
image_encoder.13.0.downsample.1.bias False
image_encoder.13.1.conv1.weight False
image_encoder.13.1.bn1.weight False
image_encoder.13.1.bn1.bias False
image_encoder.13.1.conv2.weight False
image_encoder.13.1.bn2.weight False
image_encoder.13.1.bn2.bias False
image_encoder.13.1.conv3.weight False
image_encoder.13.1.bn3.weight False
image_encoder.13.1.bn3.bias False
image_encoder.13.2.conv1.weight False
image_encoder.13.2.bn1.weight False
image_encoder.13.2.bn1.bias False
image_encoder.13.2.conv2.weight False
image_encoder.13.2.bn2.weight False
image_encoder.13.2.bn2.bias False
image_encoder.13.2.conv3.weight False
image_encoder.13.2.bn3.weight False
image_encoder.13.2.bn3.bias False
attnpool.positional_embedding True
attnpool.k_proj.weight True
attnpool.k_proj.bias True
attnpool.q_proj.weight True
attnpool.q_proj.bias True
attnpool.v_proj.weight True
attnpool.v_proj.bias True
attnpool.c_proj.weight True
attnpool.c_proj.bias True
align_context.memory_proj.0.weight True
align_context.memory_proj.0.bias True
align_context.memory_proj.1.weight True
align_context.memory_proj.1.bias True
align_context.memory_proj.2.weight True
align_context.memory_proj.2.bias True
align_context.text_proj.0.weight True
align_context.text_proj.0.bias True
align_context.text_proj.1.weight True
align_context.text_proj.1.bias True
align_context.decoder.0.self_attn.q_proj.weight True
align_context.decoder.0.self_attn.k_proj.weight True
align_context.decoder.0.self_attn.v_proj.weight True
align_context.decoder.0.self_attn.proj.weight True
align_context.decoder.0.self_attn.proj.bias True
align_context.decoder.0.cross_attn.q_proj.weight True
align_context.decoder.0.cross_attn.k_proj.weight True
align_context.decoder.0.cross_attn.v_proj.weight True
align_context.decoder.0.cross_attn.proj.weight True
align_context.decoder.0.cross_attn.proj.bias True
align_context.decoder.0.norm1.weight True
align_context.decoder.0.norm1.bias True
align_context.decoder.0.norm2.weight True
align_context.decoder.0.norm2.bias True
align_context.decoder.0.norm3.weight True
align_context.decoder.0.norm3.bias True
align_context.decoder.0.mlp.0.weight True
align_context.decoder.0.mlp.0.bias True
align_context.decoder.0.mlp.3.weight True
align_context.decoder.0.mlp.3.bias True
align_context.decoder.1.self_attn.q_proj.weight True
align_context.decoder.1.self_attn.k_proj.weight True
align_context.decoder.1.self_attn.v_proj.weight True
align_context.decoder.1.self_attn.proj.weight True
align_context.decoder.1.self_attn.proj.bias True
align_context.decoder.1.cross_attn.q_proj.weight True
align_context.decoder.1.cross_attn.k_proj.weight True
align_context.decoder.1.cross_attn.v_proj.weight True
align_context.decoder.1.cross_attn.proj.weight True
align_context.decoder.1.cross_attn.proj.bias True
align_context.decoder.1.norm1.weight True
align_context.decoder.1.norm1.bias True
align_context.decoder.1.norm2.weight True
align_context.decoder.1.norm2.bias True
align_context.decoder.1.norm3.weight True
align_context.decoder.1.norm3.bias True
align_context.decoder.1.mlp.0.weight True
align_context.decoder.1.mlp.0.bias True
align_context.decoder.1.mlp.3.weight True
align_context.decoder.1.mlp.3.bias True
align_context.decoder.2.self_attn.q_proj.weight True
align_context.decoder.2.self_attn.k_proj.weight True
align_context.decoder.2.self_attn.v_proj.weight True
align_context.decoder.2.self_attn.proj.weight True
align_context.decoder.2.self_attn.proj.bias True
align_context.decoder.2.cross_attn.q_proj.weight True
align_context.decoder.2.cross_attn.k_proj.weight True
align_context.decoder.2.cross_attn.v_proj.weight True
align_context.decoder.2.cross_attn.proj.weight True
align_context.decoder.2.cross_attn.proj.bias True
align_context.decoder.2.norm1.weight True
align_context.decoder.2.norm1.bias True
align_context.decoder.2.norm2.weight True
align_context.decoder.2.norm2.bias True
align_context.decoder.2.norm3.weight True
align_context.decoder.2.norm3.bias True
align_context.decoder.2.mlp.0.weight True
align_context.decoder.2.mlp.0.bias True
align_context.decoder.2.mlp.3.weight True
align_context.decoder.2.mlp.3.bias True
align_context.decoder.3.self_attn.q_proj.weight True
align_context.decoder.3.self_attn.k_proj.weight True
align_context.decoder.3.self_attn.v_proj.weight True
align_context.decoder.3.self_attn.proj.weight True
align_context.decoder.3.self_attn.proj.bias True
align_context.decoder.3.cross_attn.q_proj.weight True
align_context.decoder.3.cross_attn.k_proj.weight True
align_context.decoder.3.cross_attn.v_proj.weight True
align_context.decoder.3.cross_attn.proj.weight True
align_context.decoder.3.cross_attn.proj.bias True
align_context.decoder.3.norm1.weight True
align_context.decoder.3.norm1.bias True
align_context.decoder.3.norm2.weight True
align_context.decoder.3.norm2.bias True
align_context.decoder.3.norm3.weight True
align_context.decoder.3.norm3.bias True
align_context.decoder.3.mlp.0.weight True
align_context.decoder.3.mlp.0.bias True
align_context.decoder.3.mlp.3.weight True
align_context.decoder.3.mlp.3.bias True
align_context.decoder.4.self_attn.q_proj.weight True
align_context.decoder.4.self_attn.k_proj.weight True
align_context.decoder.4.self_attn.v_proj.weight True
align_context.decoder.4.self_attn.proj.weight True
align_context.decoder.4.self_attn.proj.bias True
align_context.decoder.4.cross_attn.q_proj.weight True
align_context.decoder.4.cross_attn.k_proj.weight True
align_context.decoder.4.cross_attn.v_proj.weight True
align_context.decoder.4.cross_attn.proj.weight True
align_context.decoder.4.cross_attn.proj.bias True
align_context.decoder.4.norm1.weight True
align_context.decoder.4.norm1.bias True
align_context.decoder.4.norm2.weight True
align_context.decoder.4.norm2.bias True
align_context.decoder.4.norm3.weight True
align_context.decoder.4.norm3.bias True
align_context.decoder.4.mlp.0.weight True
align_context.decoder.4.mlp.0.bias True
align_context.decoder.4.mlp.3.weight True
align_context.decoder.4.mlp.3.bias True
align_context.decoder.5.self_attn.q_proj.weight True
align_context.decoder.5.self_attn.k_proj.weight True
align_context.decoder.5.self_attn.v_proj.weight True
align_context.decoder.5.self_attn.proj.weight True
align_context.decoder.5.self_attn.proj.bias True
align_context.decoder.5.cross_attn.q_proj.weight True
align_context.decoder.5.cross_attn.k_proj.weight True
align_context.decoder.5.cross_attn.v_proj.weight True
align_context.decoder.5.cross_attn.proj.weight True
align_context.decoder.5.cross_attn.proj.bias True
align_context.decoder.5.norm1.weight True
align_context.decoder.5.norm1.bias True
align_context.decoder.5.norm2.weight True
align_context.decoder.5.norm2.bias True
align_context.decoder.5.norm3.weight True
align_context.decoder.5.norm3.bias True
align_context.decoder.5.mlp.0.weight True
align_context.decoder.5.mlp.0.bias True
align_context.decoder.5.mlp.3.weight True
align_context.decoder.5.mlp.3.bias True
align_context.out_proj.0.weight True
align_context.out_proj.0.bias True
align_context.out_proj.1.weight True
align_context.out_proj.1.bias True
decoder.conv_layers.0.weight True
decoder.conv_layers.0.bias True
decoder.conv_layers.2.weight True
decoder.conv_layers.2.bias True
decoder.conv_layers.3.weight True
decoder.conv_layers.3.bias True
decoder.conv_layers.5.weight True
decoder.conv_layers.5.bias True
Total epochs to be executed :  300
EPOCH 1:
Loss :  2.009134292602539 4.495472431182861 4.495472431182861
Loss :  2.0124409198760986 4.61210298538208 4.61210298538208
Loss :  1.9007240533828735 4.871708393096924 4.871708393096924
Loss :  1.6798627376556396 4.379696369171143 4.379696369171143
Loss :  1.6185498237609863 4.430668830871582 4.430668830871582
Loss :  1.6067951917648315 4.384462833404541 4.384462833404541
Loss :  1.8001114130020142 4.5496978759765625 4.5496978759765625
Loss :  1.7466435432434082 4.3927717208862305 4.3927717208862305
Loss :  1.786764144897461 4.513902187347412 4.513902187347412
Loss :  1.9602420330047607 4.345719814300537 4.345719814300537
Loss :  1.6718738079071045 4.744194507598877 4.744194507598877
Loss :  1.7728277444839478 4.534238338470459 4.534238338470459
Loss :  1.6762111186981201 4.68637228012085 4.68637228012085
Loss :  1.6419600248336792 4.542963981628418 4.542963981628418
Loss :  2.028430223464966 4.248270034790039 4.248270034790039
Loss :  1.848573923110962 4.352108955383301 4.352108955383301
Loss :  1.684338927268982 4.414246082305908 4.414246082305908
Loss :  1.7736955881118774 4.396430969238281 4.396430969238281
Loss :  1.5584862232208252 4.447972297668457 4.447972297668457
Loss :  1.4783293008804321 4.556230545043945 4.556230545043945
  batch 20 loss: 1.4783293008804321, 4.556230545043945, 4.556230545043945
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.5866988897323608 4.384665489196777 4.384665489196777
Loss :  1.6237318515777588 4.37023401260376 4.37023401260376
Loss :  1.6870023012161255 4.542076587677002 4.542076587677002
Loss :  1.591564655303955 4.472076892852783 4.472076892852783
Loss :  1.792284369468689 4.579281330108643 4.579281330108643
Loss :  1.8220188617706299 4.396960735321045 4.396960735321045
Loss :  2.0540919303894043 4.698962688446045 4.698962688446045
Loss :  1.8358659744262695 4.401300430297852 4.401300430297852
Loss :  1.6356323957443237 4.450249671936035 4.450249671936035
Loss :  1.8929935693740845 4.58767557144165 4.58767557144165
Loss :  1.679514765739441 4.466897964477539 4.466897964477539
Loss :  1.95138680934906 4.718124866485596 4.718124866485596
Loss :  1.8945502042770386 4.4007673263549805 4.4007673263549805
Loss :  1.9004799127578735 4.35803747177124 4.35803747177124
Loss :  1.9010140895843506 4.531254291534424 4.531254291534424
Loss :  1.9305875301361084 4.436232566833496 4.436232566833496
Loss :  1.934759497642517 4.284739971160889 4.284739971160889
Loss :  2.2966253757476807 4.347162246704102 4.347162246704102
Loss :  2.2177083492279053 4.3659563064575195 4.3659563064575195
Loss :  2.5643351078033447 4.5950927734375 4.5950927734375
  batch 40 loss: 2.5643351078033447, 4.5950927734375, 4.5950927734375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  2.2841317653656006 4.5704803466796875 4.5704803466796875
Loss :  2.4179232120513916 4.5011701583862305 4.5011701583862305
Loss :  2.266178846359253 4.470108985900879 4.470108985900879
Loss :  2.313025712966919 4.473893642425537 4.473893642425537
Loss :  2.1876463890075684 4.035993576049805 4.035993576049805
Loss :  2.5841376781463623 4.263178825378418 4.263178825378418
Loss :  2.4400534629821777 3.6109766960144043 3.6109766960144043
Loss :  2.4707703590393066 4.482231616973877 4.482231616973877
Loss :  2.5287153720855713 4.417634010314941 4.417634010314941
Loss :  2.371488332748413 4.538360595703125 4.538360595703125
Loss :  2.415613889694214 4.649327278137207 4.649327278137207
Loss :  2.4901270866394043 4.567504405975342 4.567504405975342
Loss :  2.2751784324645996 4.430248737335205 4.430248737335205
Loss :  2.3593132495880127 4.416809558868408 4.416809558868408
Loss :  2.2118706703186035 4.558288097381592 4.558288097381592
Loss :  2.2611541748046875 4.586149215698242 4.586149215698242
Loss :  2.2627317905426025 4.50791597366333 4.50791597366333
Loss :  1.809312105178833 4.137513637542725 4.137513637542725
Loss :  1.9540562629699707 4.279856204986572 4.279856204986572
Loss :  2.2230236530303955 4.211915969848633 4.211915969848633
  batch 60 loss: 2.2230236530303955, 4.211915969848633, 4.211915969848633
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4, 5], device='cuda:0')
Loss :  2.036738157272339 4.050809860229492 4.050809860229492
Loss :  2.2735254764556885 4.025209903717041 4.025209903717041
Loss :  2.5468502044677734 4.314778804779053 4.314778804779053
Loss :  2.6014761924743652 4.4744110107421875 4.4744110107421875
Loss :  2.8702280521392822 4.329458713531494 4.329458713531494
Loss :  1.2595210075378418 4.433720588684082 4.433720588684082
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.28114914894104 4.4507222175598145 4.4507222175598145
Loss :  1.2801719903945923 4.323834419250488 4.323834419250488
Loss :  1.3259867429733276 4.115337371826172 4.115337371826172
Total LOSS train 4.433710868542011 valid 4.330903649330139
CE LOSS train 2.023140184695904 valid 0.3314966857433319
Contrastive LOSS train 4.433710868542011 valid 1.028834342956543
Saved best model. Old loss 1000000.0 and new best loss 4.330903649330139
EPOCH 2:
Loss :  2.6714141368865967 3.72477388381958 3.72477388381958
Loss :  2.8111836910247803 4.266329765319824 4.266329765319824
Loss :  2.857452869415283 3.837360382080078 3.837360382080078
Loss :  2.7816073894500732 4.571568965911865 4.571568965911865
Loss :  2.866952896118164 4.28334903717041 4.28334903717041
Loss :  2.90043044090271 4.492317199707031 4.492317199707031
Loss :  2.848456859588623 4.508719444274902 4.508719444274902
Loss :  2.953692674636841 4.03684139251709 4.03684139251709
Loss :  3.039116382598877 4.42828369140625 4.42828369140625
Loss :  2.862152338027954 4.475862979888916 4.475862979888916
Loss :  2.8300044536590576 4.699387550354004 4.699387550354004
Loss :  3.027639627456665 4.786820888519287 4.786820888519287
Loss :  3.2783679962158203 4.50786828994751 4.50786828994751
Loss :  2.9168343544006348 4.5546441078186035 4.5546441078186035
Loss :  2.6456785202026367 4.3583197593688965 4.3583197593688965
Loss :  2.6189866065979004 4.48289680480957 4.48289680480957
Loss :  2.887316942214966 4.356592655181885 4.356592655181885
Loss :  2.783705949783325 4.4367899894714355 4.4367899894714355
Loss :  3.005563259124756 4.380465507507324 4.380465507507324
Loss :  2.827698230743408 4.5091986656188965 4.5091986656188965
  batch 20 loss: 2.827698230743408, 4.5091986656188965, 4.5091986656188965
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  2.7622478008270264 4.281401634216309 4.281401634216309
Loss :  2.8089804649353027 4.558215618133545 4.558215618133545
Loss :  2.688314914703369 4.716343402862549 4.716343402862549
Loss :  2.561901330947876 4.448787212371826 4.448787212371826
Loss :  2.6187210083007812 4.6659111976623535 4.6659111976623535
Loss :  2.6693694591522217 4.404797077178955 4.404797077178955
Loss :  2.4541006088256836 4.652410507202148 4.652410507202148
Loss :  2.7635209560394287 4.464457988739014 4.464457988739014
Loss :  2.7097156047821045 4.642336845397949 4.642336845397949
Loss :  2.5245141983032227 4.450809001922607 4.450809001922607
Loss :  2.7180819511413574 4.733191013336182 4.733191013336182
Loss :  2.6853973865509033 4.600666522979736 4.600666522979736
Loss :  2.782022714614868 4.411031246185303 4.411031246185303
Loss :  2.569887399673462 4.4759650230407715 4.4759650230407715
Loss :  2.734534740447998 4.500241756439209 4.500241756439209
Loss :  2.849576234817505 4.390957355499268 4.390957355499268
Loss :  2.846243381500244 4.502910137176514 4.502910137176514
Loss :  2.68135142326355 4.38364315032959 4.38364315032959
Loss :  2.495546817779541 4.431990623474121 4.431990623474121
Loss :  2.7022857666015625 4.439589023590088 4.439589023590088
  batch 40 loss: 2.7022857666015625, 4.439589023590088, 4.439589023590088
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2], device='cuda:0')
Loss :  2.9356038570404053 4.384181499481201 4.384181499481201
Loss :  2.9250597953796387 4.493677616119385 4.493677616119385
Loss :  2.9955925941467285 4.3571085929870605 4.3571085929870605
Loss :  3.275716543197632 4.462522029876709 4.462522029876709
Loss :  3.198817491531372 4.228814601898193 4.228814601898193
Loss :  2.9761805534362793 4.472895622253418 4.472895622253418
Loss :  2.8135733604431152 4.31254768371582 4.31254768371582
Loss :  3.18752384185791 4.471932411193848 4.471932411193848
Loss :  3.039311408996582 4.47789192199707 4.47789192199707
Loss :  3.0985312461853027 4.763669967651367 4.763669967651367
Loss :  3.0393519401550293 4.81709098815918 4.81709098815918
Loss :  2.9660050868988037 4.662393093109131 4.662393093109131
Loss :  3.1084585189819336 4.401545524597168 4.401545524597168
Loss :  2.869530200958252 4.6843390464782715 4.6843390464782715
Loss :  2.9863619804382324 4.409837245941162 4.409837245941162
Loss :  2.7709665298461914 4.360196113586426 4.360196113586426
Loss :  2.7973744869232178 4.261061668395996 4.261061668395996
Loss :  2.8906679153442383 4.436243534088135 4.436243534088135
Loss :  2.8966801166534424 4.492019176483154 4.492019176483154
Loss :  2.5014748573303223 4.57872200012207 4.57872200012207
  batch 60 loss: 2.5014748573303223, 4.57872200012207, 4.57872200012207
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  2.8842005729675293 4.529587268829346 4.529587268829346
Loss :  2.8342437744140625 4.401578903198242 4.401578903198242
Loss :  2.807755708694458 4.379053592681885 4.379053592681885
Loss :  2.874892234802246 4.331240653991699 4.331240653991699
Loss :  2.764331817626953 4.115835666656494 4.115835666656494
Loss :  3.2182559967041016 4.395466327667236 4.395466327667236
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3], device='cuda:0')
Loss :  3.1380605697631836 4.431257724761963 4.431257724761963
Loss :  3.0913729667663574 4.344099521636963 4.344099521636963
Loss :  3.323091745376587 4.315302848815918 4.315302848815918
Total LOSS train 4.448308211106521 valid 4.37153160572052
CE LOSS train 2.8381350187154917 valid 0.8307729363441467
Contrastive LOSS train 4.448308211106521 valid 1.0788257122039795
EPOCH 3:
Loss :  2.781311273574829 4.289564609527588 4.289564609527588
Loss :  2.731187105178833 4.663429260253906 4.663429260253906
Loss :  2.7817702293395996 4.276008129119873 4.276008129119873
Loss :  2.7958977222442627 4.510648727416992 4.510648727416992
Loss :  2.9835524559020996 4.29133415222168 4.29133415222168
Loss :  3.0586812496185303 4.372583389282227 4.372583389282227
Loss :  2.953123092651367 4.412962913513184 4.412962913513184
Loss :  2.9874625205993652 4.4314866065979 4.4314866065979
Loss :  3.0459046363830566 4.394802570343018 4.394802570343018
Loss :  3.1221389770507812 4.314887046813965 4.314887046813965
Loss :  3.32157826423645 4.375144004821777 4.375144004821777
Loss :  3.3011927604675293 4.715483665466309 4.715483665466309
Loss :  3.4257290363311768 4.436490535736084 4.436490535736084
Loss :  3.0736358165740967 4.547619819641113 4.547619819641113
Loss :  3.1464853286743164 4.456559658050537 4.456559658050537
Loss :  3.4738550186157227 4.453849792480469 4.453849792480469
Loss :  3.4460039138793945 4.4942426681518555 4.4942426681518555
Loss :  3.45746111869812 4.4158759117126465 4.4158759117126465
Loss :  3.3842647075653076 4.354608535766602 4.354608535766602
Loss :  3.3586020469665527 4.473913669586182 4.473913669586182
  batch 20 loss: 3.3586020469665527, 4.473913669586182, 4.473913669586182
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3, 5], device='cuda:0')
Loss :  3.2054049968719482 4.316713809967041 4.316713809967041
Loss :  3.3137471675872803 4.467718124389648 4.467718124389648
Loss :  3.3270249366760254 4.571063041687012 4.571063041687012
Loss :  3.206151008605957 4.372762680053711 4.372762680053711
Loss :  3.2358551025390625 4.593832492828369 4.593832492828369
Loss :  3.3036580085754395 4.329760551452637 4.329760551452637
Loss :  3.362788438796997 4.529042720794678 4.529042720794678
Loss :  3.117799758911133 4.4133405685424805 4.4133405685424805
Loss :  3.28165864944458 4.258580207824707 4.258580207824707
Loss :  2.923093795776367 4.492823123931885 4.492823123931885
Loss :  3.1474010944366455 4.483925819396973 4.483925819396973
Loss :  2.8827335834503174 4.5540666580200195 4.5540666580200195
Loss :  2.931640148162842 4.427914619445801 4.427914619445801
Loss :  2.9069085121154785 4.553530693054199 4.553530693054199
Loss :  2.938971519470215 4.50706672668457 4.50706672668457
Loss :  2.8938491344451904 4.451228618621826 4.451228618621826
Loss :  2.8844118118286133 4.374951362609863 4.374951362609863
Loss :  2.8354954719543457 4.426918983459473 4.426918983459473
Loss :  2.7133703231811523 4.375100612640381 4.375100612640381
Loss :  2.7828402519226074 4.294203758239746 4.294203758239746
  batch 40 loss: 2.7828402519226074, 4.294203758239746, 4.294203758239746
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([4, 5], device='cuda:0')
Loss :  2.740731716156006 4.457146644592285 4.457146644592285
Loss :  2.891469717025757 4.446910381317139 4.446910381317139
Loss :  2.799161434173584 4.428293704986572 4.428293704986572
Loss :  2.9108309745788574 4.454850673675537 4.454850673675537
Loss :  2.8266546726226807 4.2966718673706055 4.2966718673706055
Loss :  2.9064674377441406 4.327587604522705 4.327587604522705
Loss :  2.9000403881073 4.624560356140137 4.624560356140137
Loss :  2.665588140487671 4.469056129455566 4.469056129455566
Loss :  2.6518049240112305 4.482141971588135 4.482141971588135
Loss :  2.53908371925354 4.446089267730713 4.446089267730713
Loss :  2.373382329940796 4.4821929931640625 4.4821929931640625
Loss :  2.3955721855163574 4.512241840362549 4.512241840362549
Loss :  2.43538761138916 4.559686183929443 4.559686183929443
Loss :  2.318563222885132 4.48909854888916 4.48909854888916
Loss :  2.1334824562072754 4.514469623565674 4.514469623565674
Loss :  2.329796075820923 4.4666314125061035 4.4666314125061035
Loss :  2.2757210731506348 4.5962300300598145 4.5962300300598145
Loss :  2.0535309314727783 4.50375509262085 4.50375509262085
Loss :  2.294564962387085 4.678810119628906 4.678810119628906
Loss :  2.0475103855133057 4.421017646789551 4.421017646789551
  batch 60 loss: 2.0475103855133057, 4.421017646789551, 4.421017646789551
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4, 5], device='cuda:0')
Loss :  1.96501624584198 4.633980751037598 4.633980751037598
Loss :  2.0332775115966797 4.489449501037598 4.489449501037598
Loss :  1.9239864349365234 4.707662582397461 4.707662582397461
Loss :  1.8798829317092896 4.334129810333252 4.334129810333252
Loss :  1.7665119171142578 3.9572932720184326 3.9572932720184326
Loss :  1.752677083015442 4.626031875610352 4.626031875610352
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.3086204528808594 4.523962497711182 4.523962497711182
Loss :  1.4508099555969238 4.26993465423584 4.26993465423584
Loss :  1.384739875793457 4.068490505218506 4.068490505218506
Total LOSS train 4.450092289997981 valid 4.37210488319397
CE LOSS train 2.8289640059837926 valid 0.34618496894836426
Contrastive LOSS train 4.450092289997981 valid 1.0171226263046265
EPOCH 4:
Loss :  1.8833467960357666 4.4781813621521 4.4781813621521
Loss :  2.0038695335388184 4.6935906410217285 4.6935906410217285
Loss :  1.8530040979385376 4.440046310424805 4.440046310424805
Loss :  1.7551064491271973 4.480627536773682 4.480627536773682
Loss :  1.7995535135269165 4.310998916625977 4.310998916625977
Loss :  1.8075709342956543 4.343533515930176 4.343533515930176
Loss :  1.9788254499435425 4.441755294799805 4.441755294799805
Loss :  1.9016790390014648 4.542286396026611 4.542286396026611
Loss :  1.8362250328063965 4.565091609954834 4.565091609954834
Loss :  2.045642137527466 4.542617321014404 4.542617321014404
Loss :  1.963233470916748 4.777410984039307 4.777410984039307
Loss :  1.9581804275512695 4.840590000152588 4.840590000152588
Loss :  2.3495237827301025 4.597551345825195 4.597551345825195
Loss :  2.4530036449432373 4.612245559692383 4.612245559692383
Loss :  2.502999782562256 4.51069974899292 4.51069974899292
Loss :  2.564117193222046 4.627374172210693 4.627374172210693
Loss :  2.3136813640594482 4.60017204284668 4.60017204284668
Loss :  2.603379487991333 4.388569355010986 4.388569355010986
Loss :  2.7981135845184326 4.501308917999268 4.501308917999268
Loss :  2.581325054168701 4.4003753662109375 4.4003753662109375
  batch 20 loss: 2.581325054168701, 4.4003753662109375, 4.4003753662109375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  3.0332329273223877 4.315328598022461 4.315328598022461
Loss :  2.5893630981445312 4.523133277893066 4.523133277893066
Loss :  2.5295073986053467 4.4366865158081055 4.4366865158081055
Loss :  2.8650400638580322 4.47002649307251 4.47002649307251
Loss :  2.9166018962860107 4.496694087982178 4.496694087982178
Loss :  2.4999659061431885 4.225570201873779 4.225570201873779
Loss :  2.376608371734619 4.53331995010376 4.53331995010376
Loss :  2.9128434658050537 4.365366458892822 4.365366458892822
Loss :  2.594865322113037 4.58223295211792 4.58223295211792
Loss :  2.233534812927246 4.351651191711426 4.351651191711426
Loss :  2.319275379180908 4.382931232452393 4.382931232452393
Loss :  2.2349720001220703 4.515554904937744 4.515554904937744
Loss :  2.4066450595855713 4.380906105041504 4.380906105041504
Loss :  2.407195806503296 4.606882572174072 4.606882572174072
Loss :  2.0220866203308105 4.331720352172852 4.331720352172852
Loss :  2.0227298736572266 4.280220985412598 4.280220985412598
Loss :  2.1814138889312744 4.356363773345947 4.356363773345947
Loss :  2.505847692489624 4.404479503631592 4.404479503631592
Loss :  2.4457013607025146 4.376906394958496 4.376906394958496
Loss :  2.7606160640716553 4.4242472648620605 4.4242472648620605
  batch 40 loss: 2.7606160640716553, 4.4242472648620605, 4.4242472648620605
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4], device='cuda:0')
Loss :  2.8841354846954346 4.424347877502441 4.424347877502441
Loss :  2.918739080429077 4.51177453994751 4.51177453994751
Loss :  2.9302005767822266 4.3833160400390625 4.3833160400390625
Loss :  2.8852698802948 4.49311637878418 4.49311637878418
Loss :  3.0997836589813232 4.231271266937256 4.231271266937256
Loss :  3.1810457706451416 4.2739176750183105 4.2739176750183105
Loss :  3.123274087905884 4.595440864562988 4.595440864562988
Loss :  3.028259754180908 4.448535442352295 4.448535442352295
Loss :  2.7777178287506104 4.516510009765625 4.516510009765625
Loss :  3.3222126960754395 4.548439025878906 4.548439025878906
Loss :  3.3382058143615723 4.568734645843506 4.568734645843506
Loss :  3.3387255668640137 4.452932834625244 4.452932834625244
Loss :  3.3005130290985107 4.72385311126709 4.72385311126709
Loss :  3.136253833770752 4.302610397338867 4.302610397338867
Loss :  3.5676112174987793 4.443691730499268 4.443691730499268
Loss :  3.231194019317627 4.472244739532471 4.472244739532471
Loss :  3.562544822692871 4.580937385559082 4.580937385559082
Loss :  3.7022674083709717 4.50532341003418 4.50532341003418
Loss :  3.63620924949646 4.551424503326416 4.551424503326416
Loss :  3.222472667694092 4.457734107971191 4.457734107971191
  batch 60 loss: 3.222472667694092, 4.457734107971191, 4.457734107971191
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3], device='cuda:0')
Loss :  3.6408870220184326 4.564201354980469 4.564201354980469
Loss :  3.3620214462280273 4.437347412109375 4.437347412109375
Loss :  3.5956740379333496 4.476590633392334 4.476590633392334
Loss :  3.6002039909362793 4.443429946899414 4.443429946899414
Loss :  3.659557580947876 4.173769474029541 4.173769474029541
Loss :  13.469832420349121 4.413151264190674 4.413151264190674
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
Loss :  14.026873588562012 4.451562404632568 4.451562404632568
Loss :  13.703767776489258 4.275609970092773 4.275609970092773
Loss :  16.495668411254883 4.216988563537598 4.216988563537598
Total LOSS train 4.471334523421067 valid 4.339328050613403
CE LOSS train 2.690083204782926 valid 4.123917102813721
Contrastive LOSS train 4.471334523421067 valid 1.0542471408843994
EPOCH 5:
Loss :  3.320227861404419 4.368758201599121 4.368758201599121
Loss :  3.1195759773254395 4.569491863250732 4.569491863250732
Loss :  3.2139174938201904 4.317923069000244 4.317923069000244
Loss :  3.050577163696289 4.336402416229248 4.336402416229248
Loss :  3.0027973651885986 4.211574077606201 4.211574077606201
Loss :  3.179448366165161 4.332294940948486 4.332294940948486
Loss :  2.904550075531006 4.508817195892334 4.508817195892334
Loss :  2.985762596130371 4.313204288482666 4.313204288482666
Loss :  3.1523377895355225 4.388566017150879 4.388566017150879
Loss :  2.9915993213653564 4.262549877166748 4.262549877166748
Loss :  3.2115986347198486 4.470754623413086 4.470754623413086
Loss :  3.13456392288208 4.57678747177124 4.57678747177124
Loss :  3.1620078086853027 4.427669048309326 4.427669048309326
Loss :  3.114814281463623 4.4937663078308105 4.4937663078308105
Loss :  2.8986971378326416 4.3620734214782715 4.3620734214782715
Loss :  3.1111865043640137 4.281716346740723 4.281716346740723
Loss :  3.2100989818573 4.3411760330200195 4.3411760330200195
Loss :  3.2262682914733887 4.375472545623779 4.375472545623779
Loss :  3.2689738273620605 4.3623833656311035 4.3623833656311035
Loss :  3.0535261631011963 4.367109298706055 4.367109298706055
  batch 20 loss: 3.0535261631011963, 4.367109298706055, 4.367109298706055
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  3.097116231918335 4.43065071105957 4.43065071105957
Loss :  3.5190253257751465 4.643679141998291 4.643679141998291
Loss :  3.401674747467041 4.431028842926025 4.431028842926025
Loss :  3.4038708209991455 4.506557941436768 4.506557941436768
Loss :  3.316990852355957 4.449519157409668 4.449519157409668
Loss :  3.3253414630889893 4.418887138366699 4.418887138366699
Loss :  3.3773105144500732 4.622151851654053 4.622151851654053
Loss :  3.2453014850616455 4.57847261428833 4.57847261428833
Loss :  3.6540093421936035 4.477787494659424 4.477787494659424
Loss :  3.163719892501831 4.4275288581848145 4.4275288581848145
Loss :  3.533144474029541 4.537297248840332 4.537297248840332
Loss :  3.355441093444824 4.647279739379883 4.647279739379883
Loss :  3.1507933139801025 4.533488750457764 4.533488750457764
Loss :  3.0672624111175537 4.515363693237305 4.515363693237305
Loss :  3.189117670059204 4.493260860443115 4.493260860443115
Loss :  3.1989848613739014 4.581149101257324 4.581149101257324
Loss :  3.0905022621154785 4.493292331695557 4.493292331695557
Loss :  2.8565735816955566 4.560614585876465 4.560614585876465
Loss :  2.907515287399292 4.414618015289307 4.414618015289307
Loss :  2.8442187309265137 4.601454257965088 4.601454257965088
  batch 40 loss: 2.8442187309265137, 4.601454257965088, 4.601454257965088
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3, 4], device='cuda:0')
Loss :  3.022757053375244 4.413880825042725 4.413880825042725
Loss :  3.182706832885742 4.479640007019043 4.479640007019043
Loss :  3.1305816173553467 4.510639667510986 4.510639667510986
Loss :  3.2182374000549316 4.518641948699951 4.518641948699951
Loss :  3.255702257156372 4.487884521484375 4.487884521484375
Loss :  3.150300979614258 4.461606502532959 4.461606502532959
Loss :  3.2716710567474365 4.688413619995117 4.688413619995117
Loss :  3.10640025138855 4.458000659942627 4.458000659942627
Loss :  3.0069868564605713 4.5077409744262695 4.5077409744262695
Loss :  3.127901554107666 4.605419635772705 4.605419635772705
Loss :  3.2774524688720703 4.56500768661499 4.56500768661499
Loss :  3.2865891456604004 4.3869171142578125 4.3869171142578125
Loss :  3.5540223121643066 4.456366539001465 4.456366539001465
Loss :  3.5169107913970947 4.488345623016357 4.488345623016357
Loss :  3.4179322719573975 4.556675434112549 4.556675434112549
Loss :  3.3565571308135986 4.413565158843994 4.413565158843994
Loss :  3.554259777069092 4.538529872894287 4.538529872894287
Loss :  3.2140390872955322 4.53579044342041 4.53579044342041
Loss :  3.295288562774658 4.773590087890625 4.773590087890625
Loss :  2.9622461795806885 4.452651500701904 4.452651500701904
  batch 60 loss: 2.9622461795806885, 4.452651500701904, 4.452651500701904
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3, 4], device='cuda:0')
Loss :  3.3761260509490967 4.5232768058776855 4.5232768058776855
Loss :  3.238272190093994 4.356481552124023 4.356481552124023
Loss :  3.1215972900390625 4.413434028625488 4.413434028625488
Loss :  3.4007301330566406 4.377678394317627 4.377678394317627
Loss :  3.2577362060546875 4.094691753387451 4.094691753387451
Loss :  3.0395615100860596 4.417239189147949 4.417239189147949
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([4], device='cuda:0')
Loss :  3.039821147918701 4.384824275970459 4.384824275970459
Loss :  2.9957339763641357 4.383092403411865 4.383092403411865
Loss :  3.1811892986297607 4.2945404052734375 4.2945404052734375
Total LOSS train 4.463037586212158 valid 4.369924068450928
CE LOSS train 3.204822298196646 valid 0.7952973246574402
Contrastive LOSS train 4.463037586212158 valid 1.0736351013183594
EPOCH 6:
Loss :  2.998472213745117 4.445285320281982 4.445285320281982
Loss :  3.09014892578125 4.476473808288574 4.476473808288574
Loss :  3.1377360820770264 4.389227867126465 4.389227867126465
Loss :  3.123119831085205 4.399911403656006 4.399911403656006
Loss :  2.75022292137146 4.34058141708374 4.34058141708374
Loss :  2.951021909713745 4.433362007141113 4.433362007141113
Loss :  2.846266031265259 4.513315200805664 4.513315200805664
Loss :  3.0096065998077393 4.343369960784912 4.343369960784912
Loss :  3.080979347229004 4.360278129577637 4.360278129577637
Loss :  2.9118399620056152 4.321811199188232 4.321811199188232
Loss :  3.1208324432373047 4.456024646759033 4.456024646759033
Loss :  2.941462755203247 4.475472927093506 4.475472927093506
Loss :  2.8506340980529785 4.529176235198975 4.529176235198975
Loss :  2.8406548500061035 4.514604568481445 4.514604568481445
Loss :  2.499178409576416 4.34708833694458 4.34708833694458
Loss :  2.4324400424957275 4.465561866760254 4.465561866760254
Loss :  2.6354706287384033 4.461352825164795 4.461352825164795
Loss :  2.493767499923706 4.339184761047363 4.339184761047363
Loss :  2.5936198234558105 4.443912982940674 4.443912982940674
Loss :  2.275226593017578 4.528284072875977 4.528284072875977
  batch 20 loss: 2.275226593017578, 4.528284072875977, 4.528284072875977
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 3, 4], device='cuda:0')
Loss :  2.5732250213623047 4.376145839691162 4.376145839691162
Loss :  2.7051126956939697 4.409353256225586 4.409353256225586
Loss :  2.5872883796691895 4.520074844360352 4.520074844360352
Loss :  2.6407809257507324 4.555636405944824 4.555636405944824
Loss :  2.4192001819610596 4.568719863891602 4.568719863891602
Loss :  2.7642009258270264 4.4359002113342285 4.4359002113342285
Loss :  2.75895619392395 4.665634632110596 4.665634632110596
Loss :  2.9443342685699463 4.320413589477539 4.320413589477539
Loss :  3.180278778076172 4.371725082397461 4.371725082397461
Loss :  2.937188148498535 4.468439102172852 4.468439102172852
Loss :  3.20510196685791 4.4773712158203125 4.4773712158203125
Loss :  3.222554922103882 4.578734874725342 4.578734874725342
Loss :  3.1408603191375732 4.6555399894714355 4.6555399894714355
Loss :  3.271611452102661 4.564028739929199 4.564028739929199
Loss :  2.8288111686706543 4.715257167816162 4.715257167816162
Loss :  2.740015983581543 4.466050624847412 4.466050624847412
Loss :  2.6310770511627197 4.519343852996826 4.519343852996826
Loss :  2.412999391555786 4.380495548248291 4.380495548248291
Loss :  2.178257703781128 4.389638423919678 4.389638423919678
Loss :  2.2175240516662598 4.479191303253174 4.479191303253174
  batch 40 loss: 2.2175240516662598, 4.479191303253174, 4.479191303253174
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1], device='cuda:0')
Loss :  2.3389177322387695 4.417721271514893 4.417721271514893
Loss :  2.326453447341919 4.457475662231445 4.457475662231445
Loss :  2.4566235542297363 4.4245405197143555 4.4245405197143555
Loss :  2.295476198196411 4.485400199890137 4.485400199890137
Loss :  2.3916380405426025 4.2501301765441895 4.2501301765441895
Loss :  2.2198843955993652 4.346758842468262 4.346758842468262
Loss :  2.064424753189087 4.535006046295166 4.535006046295166
Loss :  2.3091509342193604 4.496530532836914 4.496530532836914
Loss :  2.0999042987823486 4.352034568786621 4.352034568786621
Loss :  2.2189955711364746 4.478245258331299 4.478245258331299
Loss :  2.1486825942993164 4.562748432159424 4.562748432159424
Loss :  2.1316120624542236 4.40689754486084 4.40689754486084
Loss :  2.1261885166168213 4.408409595489502 4.408409595489502
Loss :  2.3398749828338623 4.363176345825195 4.363176345825195
Loss :  2.6886115074157715 4.4669647216796875 4.4669647216796875
Loss :  2.454604148864746 4.478315353393555 4.478315353393555
Loss :  2.496337413787842 4.623264789581299 4.623264789581299
Loss :  2.5519754886627197 4.492306232452393 4.492306232452393
Loss :  2.329840660095215 4.950525760650635 4.950525760650635
Loss :  2.3127949237823486 4.403657913208008 4.403657913208008
  batch 60 loss: 2.3127949237823486, 4.403657913208008, 4.403657913208008
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2], device='cuda:0')
Loss :  2.2765793800354004 4.639096260070801 4.639096260070801
Loss :  2.1794044971466064 4.436712265014648 4.436712265014648
Loss :  2.2819316387176514 4.319113731384277 4.319113731384277
Loss :  2.269644260406494 4.3937883377075195 4.3937883377075195
Loss :  2.3033273220062256 4.47727632522583 4.47727632522583
Loss :  2.2411868572235107 4.4258222579956055 4.4258222579956055
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3], device='cuda:0')
Loss :  2.0976991653442383 4.435755729675293 4.435755729675293
Loss :  2.1996052265167236 4.309667110443115 4.309667110443115
Loss :  1.825942873954773 4.253220081329346 4.253220081329346
Total LOSS train 4.4641241660484905 valid 4.35611629486084
CE LOSS train 2.6085378280052773 valid 0.45648571848869324
Contrastive LOSS train 4.4641241660484905 valid 1.0633050203323364
EPOCH 7:
Loss :  2.0303752422332764 4.359006404876709 4.359006404876709
Loss :  2.1568126678466797 4.52390718460083 4.52390718460083
Loss :  2.198129177093506 4.452263355255127 4.452263355255127
Loss :  2.293768882751465 4.456703186035156 4.456703186035156
Loss :  2.26434588432312 4.518109321594238 4.518109321594238
Loss :  2.3984570503234863 4.450796604156494 4.450796604156494
Loss :  2.3685944080352783 4.6734538078308105 4.6734538078308105
Loss :  2.481874704360962 4.591296672821045 4.591296672821045
Loss :  2.5477352142333984 4.650517463684082 4.650517463684082
Loss :  2.5128262042999268 4.467727184295654 4.467727184295654
Loss :  2.558896780014038 4.487386226654053 4.487386226654053
Loss :  2.5136959552764893 4.636340618133545 4.636340618133545
Loss :  2.622847080230713 4.482096195220947 4.482096195220947
Loss :  2.5893633365631104 4.494639873504639 4.494639873504639
Loss :  2.5112946033477783 4.354649543762207 4.354649543762207
Loss :  2.6922366619110107 4.455862045288086 4.455862045288086
Loss :  2.6299657821655273 4.336459159851074 4.336459159851074
Loss :  2.733271837234497 4.436224937438965 4.436224937438965
Loss :  2.699191093444824 4.548948764801025 4.548948764801025
Loss :  2.700793981552124 4.428093910217285 4.428093910217285
  batch 20 loss: 2.700793981552124, 4.428093910217285, 4.428093910217285
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3, 5], device='cuda:0')
Loss :  2.6395766735076904 4.411802768707275 4.411802768707275
Loss :  2.724203109741211 4.410952091217041 4.410952091217041
Loss :  2.6753718852996826 4.52223014831543 4.52223014831543
Loss :  2.815471887588501 4.499964714050293 4.499964714050293
Loss :  2.832002878189087 4.585259437561035 4.585259437561035
Loss :  2.8345022201538086 4.459129333496094 4.459129333496094
Loss :  2.876485824584961 4.576779365539551 4.576779365539551
Loss :  2.758082389831543 4.338823318481445 4.338823318481445
Loss :  3.024667501449585 4.3762054443359375 4.3762054443359375
Loss :  2.809593915939331 4.340719223022461 4.340719223022461
Loss :  2.952249526977539 4.403519153594971 4.403519153594971
Loss :  2.7070846557617188 4.7540283203125 4.7540283203125
Loss :  2.811246871948242 4.437788009643555 4.437788009643555
Loss :  2.7792840003967285 4.441154956817627 4.441154956817627
Loss :  2.6592166423797607 4.540828227996826 4.540828227996826
Loss :  2.7341530323028564 4.545413494110107 4.545413494110107
Loss :  2.7381436824798584 4.443619251251221 4.443619251251221
Loss :  2.6063249111175537 4.377129554748535 4.377129554748535
Loss :  2.7810897827148438 4.391894340515137 4.391894340515137
Loss :  2.6116175651550293 4.440151691436768 4.440151691436768
  batch 40 loss: 2.6116175651550293, 4.440151691436768, 4.440151691436768
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  2.482160806655884 4.374866962432861 4.374866962432861
Loss :  2.663844108581543 4.366209030151367 4.366209030151367
Loss :  2.5005722045898438 4.393342971801758 4.393342971801758
Loss :  2.2939465045928955 4.535742282867432 4.535742282867432
Loss :  2.4297378063201904 4.490438461303711 4.490438461303711
Loss :  2.6612749099731445 4.437500476837158 4.437500476837158
Loss :  2.5525171756744385 4.563824653625488 4.563824653625488
Loss :  2.626225471496582 4.606436252593994 4.606436252593994
Loss :  2.4783873558044434 4.394461154937744 4.394461154937744
Loss :  2.6517271995544434 4.489896774291992 4.489896774291992
Loss :  2.6186368465423584 4.508189678192139 4.508189678192139
Loss :  2.7532553672790527 4.407553672790527 4.407553672790527
Loss :  2.485048294067383 4.520871162414551 4.520871162414551
Loss :  3.0306999683380127 4.342747688293457 4.342747688293457
Loss :  2.7628254890441895 4.5563249588012695 4.5563249588012695
Loss :  2.220046281814575 4.483187198638916 4.483187198638916
Loss :  2.508246421813965 4.445756912231445 4.445756912231445
Loss :  2.949963092803955 4.428475379943848 4.428475379943848
Loss :  2.5719504356384277 4.502318382263184 4.502318382263184
Loss :  3.024606704711914 4.664655685424805 4.664655685424805
  batch 60 loss: 3.024606704711914, 4.664655685424805, 4.664655685424805
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  2.5880072116851807 4.533447742462158 4.533447742462158
Loss :  2.3669233322143555 4.591644763946533 4.591644763946533
Loss :  2.822429656982422 4.4894561767578125 4.4894561767578125
Loss :  2.7752327919006348 4.464541435241699 4.464541435241699
Loss :  2.4098756313323975 4.092493057250977 4.092493057250977
Loss :  2.5091216564178467 4.397873878479004 4.397873878479004
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 4], device='cuda:0')
Loss :  2.5834646224975586 4.443474292755127 4.443474292755127
Loss :  2.5087552070617676 4.396028995513916 4.396028995513916
Loss :  2.67439341545105 4.345848083496094 4.345848083496094
Total LOSS train 4.473634741856501 valid 4.395806312561035
CE LOSS train 2.616969024218046 valid 0.6685983538627625
Contrastive LOSS train 4.473634741856501 valid 1.0864620208740234
EPOCH 8:
Loss :  2.4029016494750977 4.390381813049316 4.390381813049316
Loss :  2.2827816009521484 4.627181529998779 4.627181529998779
Loss :  2.41690993309021 4.3127593994140625 4.3127593994140625
Loss :  2.2647159099578857 4.617057800292969 4.617057800292969
Loss :  2.3679158687591553 4.404915809631348 4.404915809631348
Loss :  2.474651336669922 4.655261516571045 4.655261516571045
Loss :  1.99863862991333 4.515740394592285 4.515740394592285
Loss :  2.400303840637207 4.245110511779785 4.245110511779785
Loss :  2.6492528915405273 4.491212368011475 4.491212368011475
Loss :  2.5376474857330322 4.42689847946167 4.42689847946167
Loss :  2.307338237762451 4.477745056152344 4.477745056152344
Loss :  2.4646944999694824 4.497772216796875 4.497772216796875
Loss :  2.38590407371521 4.525145530700684 4.525145530700684
Loss :  2.5910816192626953 4.579768180847168 4.579768180847168
Loss :  2.542091131210327 4.389953136444092 4.389953136444092
Loss :  2.221322536468506 4.474037170410156 4.474037170410156
Loss :  2.3152778148651123 4.470571041107178 4.470571041107178
Loss :  2.60971736907959 4.312129497528076 4.312129497528076
Loss :  2.3885498046875 4.32494592666626 4.32494592666626
Loss :  2.3846631050109863 4.3825788497924805 4.3825788497924805
  batch 20 loss: 2.3846631050109863, 4.3825788497924805, 4.3825788497924805
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  2.4115328788757324 4.436657905578613 4.436657905578613
Loss :  2.524942636489868 4.686463832855225 4.686463832855225
Loss :  2.5877232551574707 4.55063009262085 4.55063009262085
Loss :  2.3950116634368896 4.456433296203613 4.456433296203613
Loss :  2.4745359420776367 4.517513751983643 4.517513751983643
Loss :  2.652219533920288 4.504833698272705 4.504833698272705
Loss :  2.3783950805664062 4.54473352432251 4.54473352432251
Loss :  2.5152528285980225 4.296102523803711 4.296102523803711
Loss :  2.6572399139404297 4.6926751136779785 4.6926751136779785
Loss :  2.4473679065704346 4.434210300445557 4.434210300445557
Loss :  2.6605424880981445 4.448849201202393 4.448849201202393
Loss :  2.601635217666626 4.542948246002197 4.542948246002197
Loss :  2.6318395137786865 4.382763385772705 4.382763385772705
Loss :  2.6999166011810303 4.443329811096191 4.443329811096191
Loss :  2.759443998336792 4.534454822540283 4.534454822540283
Loss :  2.6579859256744385 4.392599582672119 4.392599582672119
Loss :  2.631646156311035 4.401965141296387 4.401965141296387
Loss :  2.424956798553467 4.6055755615234375 4.6055755615234375
Loss :  2.309814929962158 4.2868547439575195 4.2868547439575195
Loss :  2.161137104034424 4.287806510925293 4.287806510925293
  batch 40 loss: 2.161137104034424, 4.287806510925293, 4.287806510925293
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 4], device='cuda:0')
Loss :  2.289057970046997 4.333440780639648 4.333440780639648
Loss :  2.1169586181640625 4.361705303192139 4.361705303192139
Loss :  2.205418586730957 4.384943962097168 4.384943962097168
Loss :  1.9789055585861206 4.411353588104248 4.411353588104248
Loss :  2.107600212097168 4.2143683433532715 4.2143683433532715
Loss :  2.040147066116333 4.2347612380981445 4.2347612380981445
Loss :  1.8594906330108643 4.384925365447998 4.384925365447998
Loss :  1.9495259523391724 4.409611225128174 4.409611225128174
Loss :  1.7614636421203613 4.312215805053711 4.312215805053711
Loss :  1.865536093711853 4.255256175994873 4.255256175994873
Loss :  1.974315881729126 4.290040969848633 4.290040969848633
Loss :  1.8915176391601562 4.058682918548584 4.058682918548584
Loss :  1.9282256364822388 4.308896541595459 4.308896541595459
Loss :  1.9653605222702026 4.240262985229492 4.240262985229492
Loss :  2.021470308303833 4.481165409088135 4.481165409088135
Loss :  2.085186004638672 4.333643913269043 4.333643913269043
Loss :  1.9554636478424072 4.495509147644043 4.495509147644043
Loss :  2.0467870235443115 4.616631031036377 4.616631031036377
Loss :  2.2027299404144287 4.588093280792236 4.588093280792236
Loss :  2.09328293800354 4.486486434936523 4.486486434936523
  batch 60 loss: 2.09328293800354, 4.486486434936523, 4.486486434936523
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 5], device='cuda:0')
Loss :  2.1840553283691406 4.457609176635742 4.457609176635742
Loss :  2.3707330226898193 4.5473127365112305 4.5473127365112305
Loss :  2.151568651199341 4.461542129516602 4.461542129516602
Loss :  2.162454843521118 4.381161689758301 4.381161689758301
Loss :  2.168853521347046 4.1588616371154785 4.1588616371154785
Loss :  2.4819042682647705 4.43461799621582 4.43461799621582
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([5], device='cuda:0')
Loss :  2.4142136573791504 4.474934101104736 4.474934101104736
Loss :  2.420513868331909 4.278960227966309 4.278960227966309
Loss :  2.418937921524048 4.312003135681152 4.312003135681152
Total LOSS train 4.427339216379019 valid 4.375128865242004
CE LOSS train 2.307163214683533 valid 0.604734480381012
Contrastive LOSS train 4.427339216379019 valid 1.078000783920288
EPOCH 9:
Loss :  2.037202835083008 4.3456549644470215 4.3456549644470215
Loss :  2.0030672550201416 4.597102642059326 4.597102642059326
Loss :  2.342758893966675 4.607443332672119 4.607443332672119
Loss :  2.4674084186553955 4.426064491271973 4.426064491271973
Loss :  2.4218320846557617 4.469613552093506 4.469613552093506
Loss :  2.2107932567596436 4.294644355773926 4.294644355773926
Loss :  2.3377084732055664 4.409087181091309 4.409087181091309
Loss :  2.457056760787964 4.34857177734375 4.34857177734375
Loss :  2.44345760345459 4.3957014083862305 4.3957014083862305
Loss :  2.1764814853668213 4.295283317565918 4.295283317565918
Loss :  2.498790740966797 4.419310092926025 4.419310092926025
Loss :  2.485677719116211 4.286381244659424 4.286381244659424
Loss :  2.5063600540161133 4.643453598022461 4.643453598022461
Loss :  2.283513307571411 4.651257038116455 4.651257038116455
Loss :  2.1864073276519775 4.426034450531006 4.426034450531006
Loss :  2.1313960552215576 4.424951553344727 4.424951553344727
Loss :  1.9767894744873047 4.4518513679504395 4.4518513679504395
Loss :  1.9213807582855225 4.402947425842285 4.402947425842285
Loss :  1.964239239692688 4.350193977355957 4.350193977355957
Loss :  1.8634815216064453 4.387242794036865 4.387242794036865
  batch 20 loss: 1.8634815216064453, 4.387242794036865, 4.387242794036865
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  2.158500909805298 4.341146469116211 4.341146469116211
Loss :  1.8184999227523804 4.591418266296387 4.591418266296387
Loss :  1.9262784719467163 4.506921768188477 4.506921768188477
Loss :  1.8919013738632202 4.330877304077148 4.330877304077148
Loss :  2.133112907409668 4.549545764923096 4.549545764923096
Loss :  2.0746989250183105 4.31173038482666 4.31173038482666
Loss :  2.129063606262207 4.595473766326904 4.595473766326904
Loss :  1.9728460311889648 4.389674186706543 4.389674186706543
Loss :  2.146989345550537 4.4649457931518555 4.4649457931518555
Loss :  1.96774160861969 4.496862888336182 4.496862888336182
Loss :  2.316154956817627 4.457669734954834 4.457669734954834
Loss :  2.252666711807251 4.5957231521606445 4.5957231521606445
Loss :  2.5148260593414307 4.41461181640625 4.41461181640625
Loss :  2.796400308609009 4.476980209350586 4.476980209350586
Loss :  3.0332913398742676 4.542090892791748 4.542090892791748
Loss :  3.21563458442688 4.5375494956970215 4.5375494956970215
Loss :  3.341479539871216 4.410825729370117 4.410825729370117
Loss :  3.331651449203491 4.381371974945068 4.381371974945068
Loss :  3.4117162227630615 4.363836765289307 4.363836765289307
Loss :  3.4086084365844727 4.4690775871276855 4.4690775871276855
  batch 40 loss: 3.4086084365844727, 4.4690775871276855, 4.4690775871276855
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4, 5], device='cuda:0')
Loss :  3.49812388420105 4.354094982147217 4.354094982147217
Loss :  2.970491409301758 4.3571085929870605 4.3571085929870605
Loss :  3.1363635063171387 4.405289649963379 4.405289649963379
Loss :  3.19256854057312 4.463157653808594 4.463157653808594
Loss :  3.564897298812866 4.205480575561523 4.205480575561523
Loss :  3.5379233360290527 4.4733195304870605 4.4733195304870605
Loss :  3.266230583190918 4.468826770782471 4.468826770782471
Loss :  3.501239776611328 4.522367000579834 4.522367000579834
Loss :  3.4463961124420166 4.3631062507629395 4.3631062507629395
Loss :  3.6855154037475586 4.6118316650390625 4.6118316650390625
Loss :  3.5465176105499268 4.571721076965332 4.571721076965332
Loss :  3.62837553024292 4.460762023925781 4.460762023925781
Loss :  3.3011865615844727 4.4357829093933105 4.4357829093933105
Loss :  3.5298824310302734 4.441734313964844 4.441734313964844
Loss :  3.679866313934326 4.464945316314697 4.464945316314697
Loss :  3.3871614933013916 4.376179218292236 4.376179218292236
Loss :  4.117578506469727 4.448197841644287 4.448197841644287
Loss :  3.8213202953338623 4.394134044647217 4.394134044647217
Loss :  4.168236255645752 4.491676330566406 4.491676330566406
Loss :  3.5774526596069336 4.402801513671875 4.402801513671875
  batch 60 loss: 3.5774526596069336, 4.402801513671875, 4.402801513671875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 3, 4], device='cuda:0')
Loss :  4.072097301483154 4.576345920562744 4.576345920562744
Loss :  3.6478514671325684 4.385955333709717 4.385955333709717
Loss :  3.894942283630371 4.437959671020508 4.437959671020508
Loss :  3.8473622798919678 4.403501987457275 4.403501987457275
Loss :  4.077073097229004 4.3662896156311035 4.3662896156311035
Loss :  3.0739586353302 4.422905445098877 4.422905445098877
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([4], device='cuda:0')
Loss :  3.1102399826049805 4.428086757659912 4.428086757659912
Loss :  3.043621301651001 4.334950923919678 4.334950923919678
Loss :  3.24013352394104 4.17510986328125 4.17510986328125
Total LOSS train 4.4422106816218445 valid 4.340263247489929
CE LOSS train 2.8408387679320115 valid 0.81003338098526
Contrastive LOSS train 4.4422106816218445 valid 1.0437774658203125
EPOCH 10:
Loss :  3.8848214149475098 4.363096237182617 4.363096237182617
Loss :  3.6739649772644043 4.441923141479492 4.441923141479492
Loss :  3.8753092288970947 4.319307327270508 4.319307327270508
Loss :  3.2581264972686768 4.3435516357421875 4.3435516357421875
Loss :  3.1153762340545654 4.3137431144714355 4.3137431144714355
Loss :  3.3077917098999023 4.346502304077148 4.346502304077148
Loss :  3.1151282787323 4.679659366607666 4.679659366607666
Loss :  3.1104252338409424 4.302480697631836 4.302480697631836
Loss :  2.8914506435394287 4.454516410827637 4.454516410827637
Loss :  2.8404295444488525 4.3487229347229 4.3487229347229
Loss :  2.7099204063415527 4.541995048522949 4.541995048522949
Loss :  2.806342601776123 4.443602561950684 4.443602561950684
Loss :  2.654160261154175 4.527406692504883 4.527406692504883
Loss :  2.6583433151245117 4.592508792877197 4.592508792877197
Loss :  2.5024776458740234 4.382552146911621 4.382552146911621
Loss :  2.5951526165008545 4.36269998550415 4.36269998550415
Loss :  2.5373122692108154 4.430070400238037 4.430070400238037
Loss :  2.401947259902954 4.7321014404296875 4.7321014404296875
Loss :  2.6124539375305176 4.437771320343018 4.437771320343018
Loss :  2.4405970573425293 4.512308120727539 4.512308120727539
  batch 20 loss: 2.4405970573425293, 4.512308120727539, 4.512308120727539
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3, 4], device='cuda:0')
Loss :  2.4657726287841797 4.343478679656982 4.343478679656982
Loss :  2.4185590744018555 4.519501209259033 4.519501209259033
Loss :  2.4976840019226074 4.541416645050049 4.541416645050049
Loss :  2.546269416809082 4.471413612365723 4.471413612365723
Loss :  2.790576219558716 4.583883285522461 4.583883285522461
Loss :  2.5106701850891113 4.354665756225586 4.354665756225586
Loss :  2.3879923820495605 4.6039347648620605 4.6039347648620605
Loss :  2.6273937225341797 4.46849250793457 4.46849250793457
Loss :  2.7009150981903076 4.676946640014648 4.676946640014648
Loss :  2.582059383392334 4.600892066955566 4.600892066955566
Loss :  2.4828176498413086 4.581770420074463 4.581770420074463
Loss :  2.2674758434295654 4.7013020515441895 4.7013020515441895
Loss :  2.5875585079193115 4.410841941833496 4.410841941833496
Loss :  2.4242146015167236 4.499828815460205 4.499828815460205
Loss :  2.4383561611175537 4.6391401290893555 4.6391401290893555
Loss :  2.451483964920044 4.532528400421143 4.532528400421143
Loss :  2.5048818588256836 4.538581848144531 4.538581848144531
Loss :  2.4726362228393555 4.547458171844482 4.547458171844482
Loss :  2.326774835586548 4.3396077156066895 4.3396077156066895
Loss :  2.1406373977661133 4.545377731323242 4.545377731323242
  batch 40 loss: 2.1406373977661133, 4.545377731323242, 4.545377731323242
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 4, 5], device='cuda:0')
Loss :  2.2786059379577637 4.400750160217285 4.400750160217285
Loss :  2.085193395614624 4.448459148406982 4.448459148406982
Loss :  2.1972782611846924 4.365645885467529 4.365645885467529
Loss :  2.145326614379883 4.384952068328857 4.384952068328857
Loss :  2.1259312629699707 4.423391819000244 4.423391819000244
Loss :  2.0902397632598877 4.4117045402526855 4.4117045402526855
Loss :  2.005972385406494 4.720505237579346 4.720505237579346
Loss :  2.160238027572632 4.429926872253418 4.429926872253418
Loss :  2.0340754985809326 4.385653972625732 4.385653972625732
Loss :  2.2074990272521973 4.455667018890381 4.455667018890381
Loss :  2.2177560329437256 4.5219268798828125 4.5219268798828125
Loss :  2.0894112586975098 4.398595333099365 4.398595333099365
Loss :  2.0713748931884766 4.345156669616699 4.345156669616699
Loss :  1.990782380104065 4.3626933097839355 4.3626933097839355
Loss :  2.329608678817749 4.556058406829834 4.556058406829834
Loss :  2.1217215061187744 4.446258068084717 4.446258068084717
Loss :  2.295872211456299 4.594936847686768 4.594936847686768
Loss :  2.5660319328308105 4.409023761749268 4.409023761749268
Loss :  2.43906569480896 4.50005578994751 4.50005578994751
Loss :  2.4508042335510254 4.620707035064697 4.620707035064697
  batch 60 loss: 2.4508042335510254, 4.620707035064697, 4.620707035064697
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  2.6388497352600098 4.525339603424072 4.525339603424072
Loss :  2.648869752883911 4.36907434463501 4.36907434463501
Loss :  2.612849235534668 4.357373237609863 4.357373237609863
Loss :  3.043736219406128 4.3850178718566895 4.3850178718566895
Loss :  2.8128702640533447 4.40245246887207 4.40245246887207
Loss :  4.196242809295654 4.536898136138916 4.536898136138916
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  4.175346374511719 4.464772701263428 4.464772701263428
Loss :  4.332546234130859 4.234880447387695 4.234880447387695
Loss :  3.804266929626465 4.010096073150635 4.010096073150635
Total LOSS train 4.470752437298114 valid 4.3116618394851685
CE LOSS train 2.5580957614458524 valid 0.9510667324066162
Contrastive LOSS train 4.470752437298114 valid 1.0025240182876587
Saved best model. Old loss 4.330903649330139 and new best loss 4.3116618394851685
EPOCH 11:
Loss :  2.679905652999878 4.248642921447754 4.248642921447754
Loss :  2.5949325561523438 4.7425923347473145 4.7425923347473145
Loss :  2.773768424987793 4.455351829528809 4.455351829528809
Loss :  2.6857378482818604 4.43620491027832 4.43620491027832
Loss :  2.9931960105895996 4.52041482925415 4.52041482925415
Loss :  2.7079691886901855 4.469237327575684 4.469237327575684
Loss :  2.649531841278076 4.56060266494751 4.56060266494751
Loss :  2.8848955631256104 4.569411754608154 4.569411754608154
Loss :  2.8719937801361084 4.503320217132568 4.503320217132568
Loss :  2.526912212371826 4.3233137130737305 4.3233137130737305
Loss :  2.4538862705230713 4.499976634979248 4.499976634979248
Loss :  2.6845126152038574 4.562092304229736 4.562092304229736
Loss :  2.713118314743042 4.601038932800293 4.601038932800293
Loss :  2.929281234741211 4.632185935974121 4.632185935974121
Loss :  2.769928216934204 4.39219856262207 4.39219856262207
Loss :  2.61156964302063 4.553915977478027 4.553915977478027
Loss :  2.981541633605957 4.445431709289551 4.445431709289551
Loss :  2.8641045093536377 4.667071342468262 4.667071342468262
Loss :  3.2771172523498535 4.395592212677002 4.395592212677002
Loss :  3.104454278945923 4.47663688659668 4.47663688659668
  batch 20 loss: 3.104454278945923, 4.47663688659668, 4.47663688659668
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  3.081312894821167 4.430213928222656 4.430213928222656
Loss :  3.026097536087036 4.386373996734619 4.386373996734619
Loss :  2.795210123062134 4.674257755279541 4.674257755279541
Loss :  2.554169178009033 4.5555925369262695 4.5555925369262695
Loss :  2.443053722381592 4.519012451171875 4.519012451171875
Loss :  2.4631879329681396 4.3952202796936035 4.3952202796936035
Loss :  2.428751230239868 4.489923477172852 4.489923477172852
Loss :  2.6210806369781494 4.388896465301514 4.388896465301514
Loss :  2.7507128715515137 4.487712383270264 4.487712383270264
Loss :  2.330561876296997 4.450730323791504 4.450730323791504
Loss :  2.8744890689849854 4.428216457366943 4.428216457366943
Loss :  2.6952481269836426 4.523850440979004 4.523850440979004
Loss :  2.646367073059082 4.37358283996582 4.37358283996582
Loss :  2.4336225986480713 4.460324287414551 4.460324287414551
Loss :  2.717602014541626 4.654412746429443 4.654412746429443
Loss :  2.646793842315674 4.444864749908447 4.444864749908447
Loss :  2.5486369132995605 4.382843971252441 4.382843971252441
Loss :  2.534904718399048 4.512217998504639 4.512217998504639
Loss :  2.3687639236450195 4.468679904937744 4.468679904937744
Loss :  2.505136728286743 4.5941972732543945 4.5941972732543945
  batch 40 loss: 2.505136728286743, 4.5941972732543945, 4.5941972732543945
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 5], device='cuda:0')
Loss :  2.7574760913848877 4.449198246002197 4.449198246002197
Loss :  2.7752201557159424 4.372801303863525 4.372801303863525
Loss :  2.514211416244507 4.0532546043396 4.0532546043396
Loss :  2.5449769496917725 4.524952411651611 4.524952411651611
Loss :  2.351823329925537 4.354394912719727 4.354394912719727
Loss :  2.403341293334961 4.367212772369385 4.367212772369385
Loss :  2.4212327003479004 4.337732791900635 4.337732791900635
Loss :  2.340419292449951 4.497261047363281 4.497261047363281
Loss :  2.342864513397217 4.504036903381348 4.504036903381348
Loss :  2.29599928855896 4.489988327026367 4.489988327026367
Loss :  2.3133809566497803 4.64709997177124 4.64709997177124
Loss :  2.261216163635254 4.487137317657471 4.487137317657471
Loss :  2.19931697845459 4.502392292022705 4.502392292022705
Loss :  2.21966290473938 4.582188606262207 4.582188606262207
Loss :  2.1633949279785156 4.511603355407715 4.511603355407715
Loss :  2.115901231765747 4.5100860595703125 4.5100860595703125
Loss :  2.0908446311950684 4.718976974487305 4.718976974487305
Loss :  1.960717797279358 4.3816914558410645 4.3816914558410645
Loss :  2.041975736618042 4.470337867736816 4.470337867736816
Loss :  2.3370723724365234 4.396451473236084 4.396451473236084
  batch 60 loss: 2.3370723724365234, 4.396451473236084, 4.396451473236084
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  2.09606671333313 4.534933090209961 4.534933090209961
Loss :  2.1550939083099365 4.6540141105651855 4.6540141105651855
Loss :  2.054028034210205 4.3409953117370605 4.3409953117370605
Loss :  1.9032530784606934 4.434686660766602 4.434686660766602
Loss :  1.7762610912322998 4.089406967163086 4.089406967163086
Loss :  1.6987178325653076 4.4536614418029785 4.4536614418029785
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.7051509618759155 4.463789463043213 4.463789463043213
Loss :  1.7108262777328491 4.421844482421875 4.421844482421875
Loss :  1.7763878107070923 4.230276584625244 4.230276584625244
Total LOSS train 4.475218340066763 valid 4.392392992973328
CE LOSS train 2.5332278710145215 valid 0.44409695267677307
Contrastive LOSS train 4.475218340066763 valid 1.057569146156311
EPOCH 12:
Loss :  2.075052261352539 4.311107158660889 4.311107158660889
Loss :  2.1878185272216797 4.492120265960693 4.492120265960693
Loss :  2.0117013454437256 4.3695292472839355 4.3695292472839355
Loss :  1.9619601964950562 4.55042839050293 4.55042839050293
Loss :  2.1675162315368652 4.2386860847473145 4.2386860847473145
Loss :  2.069636344909668 4.441202640533447 4.441202640533447
Loss :  2.142320394515991 4.403811931610107 4.403811931610107
Loss :  2.1338372230529785 4.3732757568359375 4.3732757568359375
Loss :  1.965767502784729 4.430362224578857 4.430362224578857
Loss :  2.3922414779663086 4.400521278381348 4.400521278381348
Loss :  2.127973794937134 4.576844692230225 4.576844692230225
Loss :  2.106816053390503 4.4290266036987305 4.4290266036987305
Loss :  2.225107431411743 4.489706516265869 4.489706516265869
Loss :  1.9325218200683594 4.583856105804443 4.583856105804443
Loss :  2.521446943283081 4.331357955932617 4.331357955932617
Loss :  2.155056953430176 4.53129243850708 4.53129243850708
Loss :  2.1403415203094482 4.480441093444824 4.480441093444824
Loss :  2.418241262435913 4.425947666168213 4.425947666168213
Loss :  2.4293510913848877 4.37625789642334 4.37625789642334
Loss :  2.843506336212158 4.4308061599731445 4.4308061599731445
  batch 20 loss: 2.843506336212158, 4.4308061599731445, 4.4308061599731445
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  2.2537434101104736 4.369100570678711 4.369100570678711
Loss :  2.449475049972534 4.539694309234619 4.539694309234619
Loss :  2.2829151153564453 4.38175106048584 4.38175106048584
Loss :  1.9061381816864014 4.548544406890869 4.548544406890869
Loss :  2.6140964031219482 4.436852931976318 4.436852931976318
Loss :  1.859249472618103 4.430545330047607 4.430545330047607
Loss :  1.8484574556350708 4.7639994621276855 4.7639994621276855
Loss :  1.900658130645752 4.476808547973633 4.476808547973633
Loss :  1.6423462629318237 4.44257926940918 4.44257926940918
Loss :  2.1482038497924805 4.39422607421875 4.39422607421875
Loss :  1.9091740846633911 4.497180461883545 4.497180461883545
Loss :  2.1559436321258545 4.678085803985596 4.678085803985596
Loss :  1.930273175239563 4.571325302124023 4.571325302124023
Loss :  1.8709783554077148 4.769455909729004 4.769455909729004
Loss :  1.8190909624099731 4.54710054397583 4.54710054397583
Loss :  1.9976269006729126 4.471479892730713 4.471479892730713
Loss :  2.152533769607544 4.601048469543457 4.601048469543457
Loss :  2.3840172290802 4.480626583099365 4.480626583099365
Loss :  2.363645315170288 4.472109794616699 4.472109794616699
Loss :  2.212430000305176 4.5108866691589355 4.5108866691589355
  batch 40 loss: 2.212430000305176, 4.5108866691589355, 4.5108866691589355
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  2.5317916870117188 4.608588695526123 4.608588695526123
Loss :  2.3965978622436523 4.46691370010376 4.46691370010376
Loss :  2.7314579486846924 4.423041343688965 4.423041343688965
Loss :  2.5909578800201416 4.483421325683594 4.483421325683594
Loss :  2.7148444652557373 4.313475131988525 4.313475131988525
Loss :  2.7039852142333984 4.401329040527344 4.401329040527344
Loss :  2.430556535720825 4.633026123046875 4.633026123046875
Loss :  2.8217060565948486 4.712730884552002 4.712730884552002
Loss :  2.4970219135284424 4.543769359588623 4.543769359588623
Loss :  2.792341709136963 4.603749752044678 4.603749752044678
Loss :  2.717958450317383 4.636573314666748 4.636573314666748
Loss :  2.750364303588867 4.395882606506348 4.395882606506348
Loss :  2.814802885055542 4.422642230987549 4.422642230987549
Loss :  2.6405460834503174 4.321620464324951 4.321620464324951
Loss :  2.940751552581787 4.531744956970215 4.531744956970215
Loss :  2.525106430053711 4.38801908493042 4.38801908493042
Loss :  2.71160626411438 4.515913486480713 4.515913486480713
Loss :  3.1263325214385986 4.520201206207275 4.520201206207275
Loss :  2.9915802478790283 4.548824787139893 4.548824787139893
Loss :  2.547632932662964 4.557816505432129 4.557816505432129
  batch 60 loss: 2.547632932662964, 4.557816505432129, 4.557816505432129
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 3, 4], device='cuda:0')
Loss :  2.904285192489624 4.561543941497803 4.561543941497803
Loss :  2.762211561203003 4.448563575744629 4.448563575744629
Loss :  2.8917925357818604 4.377641677856445 4.377641677856445
Loss :  2.8772623538970947 4.493077278137207 4.493077278137207
Loss :  3.0842161178588867 4.291644096374512 4.291644096374512
Loss :  1.5598798990249634 4.380771160125732 4.380771160125732
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
Loss :  1.6178889274597168 4.483059883117676 4.483059883117676
Loss :  1.5733808279037476 4.273440361022949 4.273440361022949
Loss :  1.6376216411590576 4.210295677185059 4.210295677185059
Total LOSS train 4.480795970329871 valid 4.336891770362854
CE LOSS train 2.3724141872846163 valid 0.4094054102897644
Contrastive LOSS train 4.480795970329871 valid 1.0525739192962646
EPOCH 13:
Loss :  2.874279737472534 4.278486251831055 4.278486251831055
Loss :  2.6150715351104736 4.403973579406738 4.403973579406738
Loss :  2.943246364593506 4.388864040374756 4.388864040374756
Loss :  2.9581003189086914 4.589387893676758 4.589387893676758
Loss :  2.5673396587371826 4.715198993682861 4.715198993682861
Loss :  2.4859683513641357 4.428362846374512 4.428362846374512
Loss :  2.4663052558898926 4.477959632873535 4.477959632873535
Loss :  2.5393497943878174 4.234060764312744 4.234060764312744
Loss :  2.6878933906555176 4.41193962097168 4.41193962097168
Loss :  2.5930752754211426 4.40702486038208 4.40702486038208
Loss :  2.4089725017547607 4.4527082443237305 4.4527082443237305
Loss :  2.5081820487976074 4.565595626831055 4.565595626831055
Loss :  2.6152613162994385 4.5033183097839355 4.5033183097839355
Loss :  2.8055508136749268 4.592724800109863 4.592724800109863
Loss :  2.3913304805755615 4.689965724945068 4.689965724945068
Loss :  2.38395357131958 4.557770729064941 4.557770729064941
Loss :  2.3927786350250244 4.714319229125977 4.714319229125977
Loss :  2.530113935470581 4.468448162078857 4.468448162078857
Loss :  2.3290066719055176 4.438990592956543 4.438990592956543
Loss :  2.007472038269043 4.541807174682617 4.541807174682617
  batch 20 loss: 2.007472038269043, 4.541807174682617, 4.541807174682617
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  2.3365278244018555 4.3456130027771 4.3456130027771
Loss :  2.152015209197998 4.520428657531738 4.520428657531738
Loss :  2.0811095237731934 4.469069480895996 4.469069480895996
Loss :  2.22599196434021 4.386442184448242 4.386442184448242
Loss :  2.199207305908203 4.64473295211792 4.64473295211792
Loss :  2.0658650398254395 4.429478168487549 4.429478168487549
Loss :  1.8507719039916992 4.656632423400879 4.656632423400879
Loss :  2.0074965953826904 4.486480236053467 4.486480236053467
Loss :  2.1847445964813232 4.6560869216918945 4.6560869216918945
Loss :  1.9701526165008545 4.302718639373779 4.302718639373779
Loss :  2.047271251678467 4.558223247528076 4.558223247528076
Loss :  2.160668134689331 4.788417339324951 4.788417339324951
Loss :  2.129345417022705 4.482771396636963 4.482771396636963
Loss :  2.1103479862213135 4.446274280548096 4.446274280548096
Loss :  2.080848455429077 4.564441204071045 4.564441204071045
Loss :  2.200953722000122 4.543478965759277 4.543478965759277
Loss :  2.1859302520751953 4.885861873626709 4.885861873626709
Loss :  2.088042974472046 4.669226169586182 4.669226169586182
Loss :  2.024360179901123 4.540807723999023 4.540807723999023
Loss :  2.2074406147003174 4.303337574005127 4.303337574005127
  batch 40 loss: 2.2074406147003174, 4.303337574005127, 4.303337574005127
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  2.2766799926757812 4.436001300811768 4.436001300811768
Loss :  2.3875155448913574 4.415974140167236 4.415974140167236
Loss :  2.383025646209717 4.360791206359863 4.360791206359863
Loss :  2.2334280014038086 4.606651782989502 4.606651782989502
Loss :  2.5531675815582275 4.464417934417725 4.464417934417725
Loss :  2.333228826522827 4.436404228210449 4.436404228210449
Loss :  2.050295829772949 4.616867542266846 4.616867542266846
Loss :  2.3187098503112793 4.531277656555176 4.531277656555176
Loss :  2.056485652923584 4.395105838775635 4.395105838775635
Loss :  2.1843106746673584 4.4887166023254395 4.4887166023254395
Loss :  2.0616283416748047 4.559489727020264 4.559489727020264
Loss :  2.1979868412017822 4.620138168334961 4.620138168334961
Loss :  2.242098808288574 4.602129936218262 4.602129936218262
Loss :  2.100161075592041 4.531996726989746 4.531996726989746
Loss :  2.1622490882873535 4.420131206512451 4.420131206512451
Loss :  2.05968976020813 4.411184787750244 4.411184787750244
Loss :  2.062901735305786 4.258841037750244 4.258841037750244
Loss :  2.399386167526245 4.436155319213867 4.436155319213867
Loss :  2.4370923042297363 4.6687469482421875 4.6687469482421875
Loss :  2.313774824142456 4.437074184417725 4.437074184417725
  batch 60 loss: 2.313774824142456, 4.437074184417725, 4.437074184417725
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 3, 5], device='cuda:0')
Loss :  2.5954995155334473 4.571863174438477 4.571863174438477
Loss :  2.584733009338379 4.49440860748291 4.49440860748291
Loss :  2.8360421657562256 4.386146068572998 4.386146068572998
Loss :  2.8969571590423584 4.336339473724365 4.336339473724365
Loss :  3.0671844482421875 4.047798156738281 4.047798156738281
Loss :  2.8564963340759277 4.462435245513916 4.462435245513916
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3], device='cuda:0')
Loss :  2.8595426082611084 4.41904354095459 4.41904354095459
Loss :  2.831456184387207 4.2721967697143555 4.2721967697143555
Loss :  2.762713670730591 4.253360271453857 4.253360271453857
Total LOSS train 4.493478173475999 valid 4.35175895690918
CE LOSS train 2.341639632445115 valid 0.6906784176826477
Contrastive LOSS train 4.493478173475999 valid 1.0633400678634644
EPOCH 14:
Loss :  2.9178874492645264 4.317809581756592 4.317809581756592
Loss :  2.6388180255889893 4.5929274559021 4.5929274559021
Loss :  2.9111275672912598 4.50146484375 4.50146484375
Loss :  2.9170305728912354 4.3408427238464355 4.3408427238464355
Loss :  2.633877754211426 4.319490432739258 4.319490432739258
Loss :  3.09533429145813 4.392699241638184 4.392699241638184
Loss :  2.901690721511841 4.447488307952881 4.447488307952881
Loss :  2.973456859588623 4.352846145629883 4.352846145629883
Loss :  2.966270685195923 4.507350444793701 4.507350444793701
Loss :  2.994175910949707 4.38908052444458 4.38908052444458
Loss :  3.017514944076538 4.521992206573486 4.521992206573486
Loss :  3.2518539428710938 4.504969596862793 4.504969596862793
Loss :  3.088963031768799 4.604218482971191 4.604218482971191
Loss :  3.045600414276123 4.5305562019348145 4.5305562019348145
Loss :  2.980173349380493 4.498374938964844 4.498374938964844
Loss :  2.7893917560577393 4.525381088256836 4.525381088256836
Loss :  3.1523163318634033 4.371262073516846 4.371262073516846
Loss :  3.0712873935699463 4.383448123931885 4.383448123931885
Loss :  3.1986258029937744 4.375603199005127 4.375603199005127
Loss :  3.0135467052459717 4.436018943786621 4.436018943786621
  batch 20 loss: 3.0135467052459717, 4.436018943786621, 4.436018943786621
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3, 5], device='cuda:0')
Loss :  3.163517475128174 4.313126087188721 4.313126087188721
Loss :  3.3548238277435303 4.548854351043701 4.548854351043701
Loss :  3.3150076866149902 4.449743270874023 4.449743270874023
Loss :  3.134605646133423 4.467933654785156 4.467933654785156
Loss :  3.0272419452667236 4.585688591003418 4.585688591003418
Loss :  3.2116851806640625 4.426477909088135 4.426477909088135
Loss :  3.2028627395629883 4.528324604034424 4.528324604034424
Loss :  3.215203046798706 4.379014492034912 4.379014492034912
Loss :  3.1239376068115234 4.4067912101745605 4.4067912101745605
Loss :  3.0694923400878906 4.462780475616455 4.462780475616455
Loss :  3.1332976818084717 4.497635364532471 4.497635364532471
Loss :  3.093295097351074 4.606551170349121 4.606551170349121
Loss :  2.8941025733947754 4.4394707679748535 4.4394707679748535
Loss :  2.82851243019104 4.386319637298584 4.386319637298584
Loss :  2.8944461345672607 4.398437976837158 4.398437976837158
Loss :  2.736834764480591 4.443164348602295 4.443164348602295
Loss :  2.618089437484741 4.331915378570557 4.331915378570557
Loss :  2.6397862434387207 4.29742431640625 4.29742431640625
Loss :  2.6563308238983154 4.273386478424072 4.273386478424072
Loss :  2.7332863807678223 4.435309886932373 4.435309886932373
  batch 40 loss: 2.7332863807678223, 4.435309886932373, 4.435309886932373
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  2.409634828567505 4.312306880950928 4.312306880950928
Loss :  2.4533960819244385 4.438553333282471 4.438553333282471
Loss :  2.325206756591797 4.371500492095947 4.371500492095947
Loss :  2.3927717208862305 4.472385883331299 4.472385883331299
Loss :  2.1710031032562256 4.324455261230469 4.324455261230469
Loss :  2.330734968185425 4.403322219848633 4.403322219848633
Loss :  2.4501090049743652 4.444111347198486 4.444111347198486
Loss :  2.3231313228607178 4.46016263961792 4.46016263961792
Loss :  2.4196135997772217 4.527886867523193 4.527886867523193
Loss :  2.3614983558654785 4.565504550933838 4.565504550933838
Loss :  2.348738670349121 4.549385070800781 4.549385070800781
Loss :  2.386629343032837 4.519908428192139 4.519908428192139
Loss :  2.341578245162964 4.367094993591309 4.367094993591309
Loss :  2.173576831817627 4.488236904144287 4.488236904144287
Loss :  2.3672823905944824 4.451765060424805 4.451765060424805
Loss :  2.5228686332702637 4.48240327835083 4.48240327835083
Loss :  2.657287120819092 4.47597074508667 4.47597074508667
Loss :  2.69403076171875 4.5044264793396 4.5044264793396
Loss :  2.804560661315918 4.617931842803955 4.617931842803955
Loss :  2.7587296962738037 4.432676792144775 4.432676792144775
  batch 60 loss: 2.7587296962738037, 4.432676792144775, 4.432676792144775
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4, 5], device='cuda:0')
Loss :  2.9791550636291504 4.639506816864014 4.639506816864014
Loss :  2.564635753631592 4.47785758972168 4.47785758972168
Loss :  2.7496190071105957 4.557673454284668 4.557673454284668
Loss :  2.686546802520752 4.361158847808838 4.361158847808838
Loss :  2.8742270469665527 3.9840054512023926 3.9840054512023926
Loss :  3.227693796157837 4.444176197052002 4.444176197052002
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([4], device='cuda:0')
Loss :  3.352201461791992 4.416555404663086 4.416555404663086
Loss :  3.294283151626587 4.383020877838135 4.383020877838135
Loss :  3.531165599822998 4.134427070617676 4.134427070617676
Total LOSS train 4.443421011704665 valid 4.344544887542725
CE LOSS train 2.802336436051589 valid 0.8827913999557495
Contrastive LOSS train 4.443421011704665 valid 1.033606767654419
EPOCH 15:
Loss :  2.601672649383545 4.486975193023682 4.486975193023682
Loss :  2.838714838027954 4.666486740112305 4.666486740112305
Loss :  2.5828375816345215 4.428264141082764 4.428264141082764
Loss :  2.551088809967041 4.450976848602295 4.450976848602295
Loss :  2.4380733966827393 4.493714332580566 4.493714332580566
Loss :  2.5937554836273193 4.361084938049316 4.361084938049316
Loss :  3.172471046447754 4.778937816619873 4.778937816619873
Loss :  2.951892375946045 4.372476100921631 4.372476100921631
Loss :  2.838373899459839 4.489609718322754 4.489609718322754
Loss :  2.8687028884887695 4.423673629760742 4.423673629760742
Loss :  2.876065492630005 4.533959865570068 4.533959865570068
Loss :  2.831605911254883 4.487061500549316 4.487061500549316
Loss :  2.6750130653381348 4.688424110412598 4.688424110412598
Loss :  2.8550333976745605 4.5756306648254395 4.5756306648254395
Loss :  3.0540771484375 4.411002159118652 4.411002159118652
Loss :  2.7838668823242188 4.43643856048584 4.43643856048584
Loss :  3.0129618644714355 4.732632637023926 4.732632637023926
Loss :  2.716085910797119 4.363909721374512 4.363909721374512
Loss :  2.8212599754333496 4.403591632843018 4.403591632843018
Loss :  2.890023708343506 4.425707817077637 4.425707817077637
  batch 20 loss: 2.890023708343506, 4.425707817077637, 4.425707817077637
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4, 5], device='cuda:0')
Loss :  2.3288419246673584 4.611217021942139 4.611217021942139
Loss :  2.3577749729156494 4.301750659942627 4.301750659942627
Loss :  2.2607078552246094 4.391879558563232 4.391879558563232
Loss :  2.3566861152648926 4.560160160064697 4.560160160064697
Loss :  2.264678716659546 4.627031326293945 4.627031326293945
Loss :  2.0367934703826904 4.384433269500732 4.384433269500732
Loss :  2.2842800617218018 4.562453269958496 4.562453269958496
Loss :  2.1690549850463867 4.361579895019531 4.361579895019531
Loss :  2.2311275005340576 4.481595993041992 4.481595993041992
Loss :  2.1360864639282227 4.538175106048584 4.538175106048584
Loss :  1.8715189695358276 4.526243686676025 4.526243686676025
Loss :  2.0254032611846924 4.553983688354492 4.553983688354492
Loss :  2.034085512161255 4.467790126800537 4.467790126800537
Loss :  1.9154495000839233 4.4990105628967285 4.4990105628967285
Loss :  1.836955189704895 4.406661510467529 4.406661510467529
Loss :  1.783907175064087 4.407229423522949 4.407229423522949
Loss :  1.77310311794281 4.526526927947998 4.526526927947998
Loss :  1.913649082183838 4.517962455749512 4.517962455749512
Loss :  2.0698766708374023 4.393549919128418 4.393549919128418
Loss :  1.9309289455413818 4.43881368637085 4.43881368637085
  batch 40 loss: 1.9309289455413818, 4.43881368637085, 4.43881368637085
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.8875471353530884 4.56986141204834 4.56986141204834
Loss :  2.1130359172821045 4.457605838775635 4.457605838775635
Loss :  1.8730862140655518 4.447046756744385 4.447046756744385
Loss :  1.9030038118362427 4.565983295440674 4.565983295440674
Loss :  2.0736567974090576 4.307455539703369 4.307455539703369
Loss :  2.22998309135437 4.593462944030762 4.593462944030762
Loss :  2.2028706073760986 4.353041172027588 4.353041172027588
Loss :  1.8879401683807373 4.505622863769531 4.505622863769531
Loss :  2.1514270305633545 4.504391193389893 4.504391193389893
Loss :  2.1477344036102295 4.517754554748535 4.517754554748535
Loss :  2.0206940174102783 4.531381607055664 4.531381607055664
Loss :  2.027188301086426 4.593835353851318 4.593835353851318
Loss :  2.0158214569091797 4.40978479385376 4.40978479385376
Loss :  1.9610528945922852 4.503455638885498 4.503455638885498
Loss :  1.76069176197052 4.426888942718506 4.426888942718506
Loss :  1.9725689888000488 4.330458641052246 4.330458641052246
Loss :  1.9302396774291992 4.415860652923584 4.415860652923584
Loss :  1.9398261308670044 4.411284923553467 4.411284923553467
Loss :  2.091939926147461 4.561135292053223 4.561135292053223
Loss :  2.105032205581665 4.696533203125 4.696533203125
  batch 60 loss: 2.105032205581665, 4.696533203125, 4.696533203125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  2.2573108673095703 4.567930221557617 4.567930221557617
Loss :  2.245986223220825 4.707747459411621 4.707747459411621
Loss :  2.30375075340271 4.449539661407471 4.449539661407471
Loss :  2.078036069869995 4.513058185577393 4.513058185577393
Loss :  2.0998642444610596 4.1809234619140625 4.1809234619140625
Loss :  2.9065115451812744 4.412849426269531 4.412849426269531
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2], device='cuda:0')
Loss :  2.924516201019287 4.401484489440918 4.401484489440918
Loss :  2.961717367172241 4.3675384521484375 4.3675384521484375
Loss :  2.7693185806274414 4.230707168579102 4.230707168579102
Total LOSS train 4.487548461327186 valid 4.353144884109497
CE LOSS train 2.2894580694345326 valid 0.6923296451568604
Contrastive LOSS train 4.487548461327186 valid 1.0576767921447754
EPOCH 16:
Loss :  1.9372860193252563 4.677881240844727 4.677881240844727
Loss :  2.117359161376953 4.510439872741699 4.510439872741699
Loss :  2.0523481369018555 4.419095516204834 4.419095516204834
Loss :  2.1152231693267822 4.531715393066406 4.531715393066406
Loss :  1.9204049110412598 4.575739860534668 4.575739860534668
Loss :  2.0069050788879395 4.347174167633057 4.347174167633057
Loss :  2.1516573429107666 4.455850124359131 4.455850124359131
Loss :  2.160170555114746 4.363988399505615 4.363988399505615
Loss :  2.333824396133423 4.473959445953369 4.473959445953369
Loss :  2.215810775756836 4.510226726531982 4.510226726531982
Loss :  2.3741512298583984 4.3653130531311035 4.3653130531311035
Loss :  2.5541412830352783 4.459086894989014 4.459086894989014
Loss :  2.5097107887268066 4.4306111335754395 4.4306111335754395
Loss :  2.336146831512451 4.548672199249268 4.548672199249268
Loss :  2.46058988571167 4.488790512084961 4.488790512084961
Loss :  2.218446969985962 4.494443893432617 4.494443893432617
Loss :  2.4328229427337646 4.439242839813232 4.439242839813232
Loss :  2.4070608615875244 4.430570125579834 4.430570125579834
Loss :  2.556912899017334 4.434323310852051 4.434323310852051
Loss :  2.330584764480591 4.4218926429748535 4.4218926429748535
  batch 20 loss: 2.330584764480591, 4.4218926429748535, 4.4218926429748535
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3, 4, 5], device='cuda:0')
Loss :  2.525522470474243 4.488935947418213 4.488935947418213
Loss :  2.6036829948425293 4.423018455505371 4.423018455505371
Loss :  2.658939838409424 4.535497665405273 4.535497665405273
Loss :  2.5003740787506104 4.527902603149414 4.527902603149414
Loss :  2.459132671356201 4.629810810089111 4.629810810089111
Loss :  2.7933990955352783 4.59512996673584 4.59512996673584
Loss :  2.8154680728912354 4.613491535186768 4.613491535186768
Loss :  2.9960458278656006 4.303469181060791 4.303469181060791
Loss :  3.2317962646484375 4.462761878967285 4.462761878967285
Loss :  3.103767156600952 4.398374557495117 4.398374557495117
Loss :  3.3493359088897705 4.4440999031066895 4.4440999031066895
Loss :  3.1120195388793945 4.754179954528809 4.754179954528809
Loss :  3.2215895652770996 4.44212532043457 4.44212532043457
Loss :  3.225316047668457 4.467550277709961 4.467550277709961
Loss :  3.391738176345825 4.493247985839844 4.493247985839844
Loss :  3.314704179763794 4.461982250213623 4.461982250213623
Loss :  3.2380824089050293 4.380967617034912 4.380967617034912
Loss :  3.1391043663024902 4.375640869140625 4.375640869140625
Loss :  3.0039899349212646 4.343739986419678 4.343739986419678
Loss :  3.0111024379730225 4.5374040603637695 4.5374040603637695
  batch 40 loss: 3.0111024379730225, 4.5374040603637695, 4.5374040603637695
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([4, 5], device='cuda:0')
Loss :  3.3930931091308594 4.422669887542725 4.422669887542725
Loss :  3.152273654937744 4.3995747566223145 4.3995747566223145
Loss :  3.334599494934082 4.586615562438965 4.586615562438965
Loss :  3.3774967193603516 4.458244323730469 4.458244323730469
Loss :  3.2636988162994385 4.346776485443115 4.346776485443115
Loss :  3.1014575958251953 4.46133279800415 4.46133279800415
Loss :  2.8046061992645264 4.49737024307251 4.49737024307251
Loss :  2.799910545349121 4.584303379058838 4.584303379058838
Loss :  2.891629695892334 4.5738959312438965 4.5738959312438965
Loss :  3.0015017986297607 4.51384162902832 4.51384162902832
Loss :  2.634993076324463 4.4872026443481445 4.4872026443481445
Loss :  2.8356196880340576 4.454973220825195 4.454973220825195
Loss :  2.756868362426758 4.425583839416504 4.425583839416504
Loss :  2.654973030090332 4.629631519317627 4.629631519317627
Loss :  2.6886370182037354 4.831174373626709 4.831174373626709
Loss :  2.7981557846069336 4.3564043045043945 4.3564043045043945
Loss :  2.72841739654541 4.6420745849609375 4.6420745849609375
Loss :  2.7495157718658447 4.4239726066589355 4.4239726066589355
Loss :  2.6146090030670166 4.634539604187012 4.634539604187012
Loss :  2.463975191116333 4.386231422424316 4.386231422424316
  batch 60 loss: 2.463975191116333, 4.386231422424316, 4.386231422424316
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3, 4, 5], device='cuda:0')
Loss :  2.687852144241333 4.589030742645264 4.589030742645264
Loss :  2.7078299522399902 4.420079231262207 4.420079231262207
Loss :  2.680683135986328 4.637949466705322 4.637949466705322
Loss :  2.6699185371398926 4.400184631347656 4.400184631347656
Loss :  2.7153611183166504 4.1745405197143555 4.1745405197143555
Loss :  2.4499869346618652 4.469640731811523 4.469640731811523
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3, 4, 5], device='cuda:0')
Loss :  2.3196945190429688 4.361191272735596 4.361191272735596
Loss :  2.3651657104492188 4.360812187194824 4.360812187194824
Loss :  2.5630393028259277 4.413113117218018 4.413113117218018
Total LOSS train 4.483023782876821 valid 4.40118932723999
CE LOSS train 2.7137591673777655 valid 0.6407598257064819
Contrastive LOSS train 4.483023782876821 valid 1.1032782793045044
EPOCH 17:
Loss :  2.688908576965332 4.369204521179199 4.369204521179199
Loss :  2.5995543003082275 4.61101770401001 4.61101770401001
Loss :  2.6774814128875732 4.440229892730713 4.440229892730713
Loss :  2.6252100467681885 4.498180866241455 4.498180866241455
Loss :  2.6557095050811768 4.462052822113037 4.462052822113037
Loss :  2.695281744003296 4.379200458526611 4.379200458526611
Loss :  2.5629124641418457 4.463099956512451 4.463099956512451
Loss :  2.5616321563720703 4.382199764251709 4.382199764251709
Loss :  2.579911947250366 4.238219738006592 4.238219738006592
Loss :  2.4068498611450195 4.609163284301758 4.609163284301758
Loss :  2.5962870121002197 4.6030964851379395 4.6030964851379395
Loss :  2.365415573120117 4.473459720611572 4.473459720611572
Loss :  2.3301773071289062 4.472853660583496 4.472853660583496
Loss :  2.2198781967163086 4.557348728179932 4.557348728179932
Loss :  2.273791551589966 4.600230693817139 4.600230693817139
Loss :  2.309221029281616 4.680791854858398 4.680791854858398
Loss :  2.3314528465270996 4.455214500427246 4.455214500427246
Loss :  2.318610668182373 4.3960371017456055 4.3960371017456055
Loss :  2.110410213470459 4.421963691711426 4.421963691711426
Loss :  2.2265121936798096 4.624892234802246 4.624892234802246
  batch 20 loss: 2.2265121936798096, 4.624892234802246, 4.624892234802246
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  2.1725332736968994 4.361086368560791 4.361086368560791
Loss :  2.1868770122528076 4.509555339813232 4.509555339813232
Loss :  2.1901967525482178 4.45778226852417 4.45778226852417
Loss :  2.403064250946045 4.458282947540283 4.458282947540283
Loss :  2.199327230453491 4.587748050689697 4.587748050689697
Loss :  2.246047019958496 4.678691387176514 4.678691387176514
Loss :  2.1091527938842773 4.623998641967773 4.623998641967773
Loss :  2.0612289905548096 4.369364261627197 4.369364261627197
Loss :  2.2719473838806152 4.425515651702881 4.425515651702881
Loss :  2.2582387924194336 4.410139083862305 4.410139083862305
Loss :  2.1835663318634033 4.544547080993652 4.544547080993652
Loss :  1.8221056461334229 4.7484211921691895 4.7484211921691895
Loss :  1.8424512147903442 4.545196056365967 4.545196056365967
Loss :  1.9027804136276245 4.477665424346924 4.477665424346924
Loss :  1.7564841508865356 4.549875259399414 4.549875259399414
Loss :  1.8446282148361206 4.567709922790527 4.567709922790527
Loss :  1.7702354192733765 4.402865886688232 4.402865886688232
Loss :  1.796938419342041 4.490012168884277 4.490012168884277
Loss :  1.8436167240142822 4.431997776031494 4.431997776031494
Loss :  1.8501890897750854 4.50449800491333 4.50449800491333
  batch 40 loss: 1.8501890897750854, 4.50449800491333, 4.50449800491333
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.7328649759292603 4.454770088195801 4.454770088195801
Loss :  1.7260196208953857 4.354716777801514 4.354716777801514
Loss :  1.5761319398880005 4.367561340332031 4.367561340332031
Loss :  1.6458097696304321 4.433009624481201 4.433009624481201
Loss :  1.5674549341201782 4.493301868438721 4.493301868438721
Loss :  1.7064675092697144 4.3555073738098145 4.3555073738098145
Loss :  1.955253005027771 4.487979412078857 4.487979412078857
Loss :  1.5670193433761597 4.544711589813232 4.544711589813232
Loss :  1.9413566589355469 4.461923122406006 4.461923122406006
Loss :  1.581486701965332 4.522277355194092 4.522277355194092
Loss :  1.6217663288116455 4.578289985656738 4.578289985656738
Loss :  1.763685703277588 4.348916530609131 4.348916530609131
Loss :  1.6455600261688232 4.398772239685059 4.398772239685059
Loss :  1.8055087327957153 4.392089366912842 4.392089366912842
Loss :  1.390968680381775 4.439623832702637 4.439623832702637
Loss :  1.927624225616455 4.425715923309326 4.425715923309326
Loss :  1.6851427555084229 4.525637626647949 4.525637626647949
Loss :  1.479190707206726 4.452714443206787 4.452714443206787
Loss :  1.6554937362670898 4.520681381225586 4.520681381225586
Loss :  1.8959341049194336 4.436562538146973 4.436562538146973
  batch 60 loss: 1.8959341049194336, 4.436562538146973, 4.436562538146973
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.5906715393066406 4.300410747528076 4.300410747528076
Loss :  1.5234137773513794 4.389825344085693 4.389825344085693
Loss :  1.4901123046875 4.4762091636657715 4.4762091636657715
Loss :  1.3736076354980469 4.332263469696045 4.332263469696045
Loss :  1.3064302206039429 4.2491774559021 4.2491774559021
Loss :  17.374813079833984 4.465144634246826 4.465144634246826
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2], device='cuda:0')
Loss :  16.48980712890625 4.459550857543945 4.459550857543945
Loss :  13.189465522766113 4.3150715827941895 4.3150715827941895
Loss :  29.855497360229492 4.232779502868652 4.232779502868652
Total LOSS train 4.471170139312744 valid 4.368136644363403
CE LOSS train 2.0154121949122503 valid 7.463874340057373
Contrastive LOSS train 4.471170139312744 valid 1.058194875717163
EPOCH 18:
Loss :  1.551571249961853 4.357716083526611 4.357716083526611
Loss :  1.4817146062850952 4.496957302093506 4.496957302093506
Loss :  1.4783953428268433 4.407334327697754 4.407334327697754
Loss :  1.5120904445648193 4.4414143562316895 4.4414143562316895
Loss :  1.5198899507522583 4.284794330596924 4.284794330596924
Loss :  1.4676241874694824 4.332405090332031 4.332405090332031
Loss :  1.5691959857940674 4.398843765258789 4.398843765258789
Loss :  1.5831162929534912 4.337172985076904 4.337172985076904
Loss :  1.581041932106018 4.3517889976501465 4.3517889976501465
Loss :  1.7018221616744995 4.244439125061035 4.244439125061035
Loss :  1.6677532196044922 4.308990478515625 4.308990478515625
Loss :  1.6535998582839966 4.475423812866211 4.475423812866211
Loss :  1.808735728263855 4.429168701171875 4.429168701171875
Loss :  1.9572057723999023 4.626368999481201 4.626368999481201
Loss :  1.839476466178894 4.522739887237549 4.522739887237549
Loss :  1.7735531330108643 4.53589391708374 4.53589391708374
Loss :  1.7938839197158813 4.473730087280273 4.473730087280273
Loss :  1.724212884902954 4.4615478515625 4.4615478515625
Loss :  1.8547866344451904 4.266684055328369 4.266684055328369
Loss :  1.7594832181930542 4.449620723724365 4.449620723724365
  batch 20 loss: 1.7594832181930542, 4.449620723724365, 4.449620723724365
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2], device='cuda:0')
Loss :  2.021092414855957 4.383939266204834 4.383939266204834
Loss :  1.9254378080368042 4.482369422912598 4.482369422912598
Loss :  1.8623803853988647 4.444669246673584 4.444669246673584
Loss :  1.9898713827133179 4.5001630783081055 4.5001630783081055
Loss :  1.8883806467056274 4.489075660705566 4.489075660705566
Loss :  1.9663166999816895 4.3843607902526855 4.3843607902526855
Loss :  1.9334062337875366 4.683782577514648 4.683782577514648
Loss :  2.1209380626678467 4.379364490509033 4.379364490509033
Loss :  2.2336997985839844 4.364075183868408 4.364075183868408
Loss :  2.2679290771484375 4.310928821563721 4.310928821563721
Loss :  2.5013160705566406 4.469400882720947 4.469400882720947
Loss :  2.4192287921905518 4.546223163604736 4.546223163604736
Loss :  2.5223701000213623 4.410480976104736 4.410480976104736
Loss :  2.4589595794677734 4.3366007804870605 4.3366007804870605
Loss :  2.584437131881714 4.49949312210083 4.49949312210083
Loss :  2.762603282928467 4.413245677947998 4.413245677947998
Loss :  2.7815279960632324 4.442608833312988 4.442608833312988
Loss :  2.8637332916259766 4.342513561248779 4.342513561248779
Loss :  2.9161312580108643 4.488654613494873 4.488654613494873
Loss :  3.042064905166626 4.458475589752197 4.458475589752197
  batch 40 loss: 3.042064905166626, 4.458475589752197, 4.458475589752197
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3], device='cuda:0')
Loss :  3.199340581893921 4.524657726287842 4.524657726287842
Loss :  2.924553394317627 4.3891072273254395 4.3891072273254395
Loss :  3.3379647731781006 4.358129024505615 4.358129024505615
Loss :  3.2812023162841797 4.4559221267700195 4.4559221267700195
Loss :  3.4829022884368896 4.345329761505127 4.345329761505127
Loss :  3.4109933376312256 4.420903205871582 4.420903205871582
Loss :  3.2799429893493652 4.2473063468933105 4.2473063468933105
Loss :  3.681882619857788 4.511258125305176 4.511258125305176
Loss :  2.974440813064575 4.564950466156006 4.564950466156006
Loss :  3.437171697616577 4.502113342285156 4.502113342285156
Loss :  3.217233419418335 4.605618000030518 4.605618000030518
Loss :  3.3954126834869385 4.467455863952637 4.467455863952637
Loss :  3.028103828430176 4.458268642425537 4.458268642425537
Loss :  2.889385461807251 4.688055515289307 4.688055515289307
Loss :  3.0306484699249268 4.560105800628662 4.560105800628662
Loss :  3.1249635219573975 4.51384973526001 4.51384973526001
Loss :  3.006956100463867 4.4427947998046875 4.4427947998046875
Loss :  3.248971700668335 4.474277973175049 4.474277973175049
Loss :  3.091599225997925 4.686253547668457 4.686253547668457
Loss :  2.8505077362060547 4.331780910491943 4.331780910491943
  batch 60 loss: 2.8505077362060547, 4.331780910491943, 4.331780910491943
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 3], device='cuda:0')
Loss :  2.8321800231933594 4.448819637298584 4.448819637298584
Loss :  2.680293321609497 4.464090824127197 4.464090824127197
Loss :  2.90157151222229 4.542099475860596 4.542099475860596
Loss :  2.8087143898010254 4.541797161102295 4.541797161102295
Loss :  2.5933194160461426 4.008549213409424 4.008549213409424
Loss :  2.320549488067627 4.414168834686279 4.414168834686279
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
Loss :  2.301520347595215 4.502594470977783 4.502594470977783
Loss :  2.231881856918335 4.249049186706543 4.249049186706543
Loss :  2.583362340927124 4.27836275100708 4.27836275100708
Total LOSS train 4.4397993087768555 valid 4.361043810844421
CE LOSS train 2.4315574389237624 valid 0.645840585231781
Contrastive LOSS train 4.4397993087768555 valid 1.06959068775177
EPOCH 19:
Loss :  2.494812250137329 4.497809886932373 4.497809886932373
Loss :  2.7955970764160156 4.481462478637695 4.481462478637695
Loss :  2.6734957695007324 4.384454250335693 4.384454250335693
Loss :  2.6486735343933105 4.749070644378662 4.749070644378662
Loss :  2.485262393951416 4.3367390632629395 4.3367390632629395
Loss :  2.9697487354278564 4.487133502960205 4.487133502960205
Loss :  2.467242479324341 4.534884452819824 4.534884452819824
Loss :  2.6841490268707275 4.286715507507324 4.286715507507324
Loss :  2.7058024406433105 4.413806438446045 4.413806438446045
Loss :  2.8002686500549316 4.3723578453063965 4.3723578453063965
Loss :  2.8459811210632324 4.53566312789917 4.53566312789917
Loss :  3.0108423233032227 4.515038967132568 4.515038967132568
Loss :  3.0198755264282227 4.505377292633057 4.505377292633057
Loss :  2.870659828186035 4.54066801071167 4.54066801071167
Loss :  2.672426462173462 4.3991780281066895 4.3991780281066895
Loss :  2.738213539123535 4.512711048126221 4.512711048126221
Loss :  2.868802309036255 4.4826741218566895 4.4826741218566895
Loss :  2.793135404586792 4.40261173248291 4.40261173248291
Loss :  2.9664804935455322 4.702902793884277 4.702902793884277
Loss :  2.5619521141052246 4.357151508331299 4.357151508331299
  batch 20 loss: 2.5619521141052246, 4.357151508331299, 4.357151508331299
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  2.922452211380005 4.487235069274902 4.487235069274902
Loss :  3.0489354133605957 4.3619465827941895 4.3619465827941895
Loss :  3.190336227416992 4.153001308441162 4.153001308441162
Loss :  2.7515437602996826 4.374244213104248 4.374244213104248
Loss :  3.052769184112549 4.831716537475586 4.831716537475586
Loss :  2.8021910190582275 4.403960227966309 4.403960227966309
Loss :  2.7174575328826904 4.754276752471924 4.754276752471924
Loss :  2.6330466270446777 4.379925727844238 4.379925727844238
Loss :  2.5771517753601074 4.517998218536377 4.517998218536377
Loss :  2.3619484901428223 4.478107929229736 4.478107929229736
Loss :  2.1755905151367188 4.669140338897705 4.669140338897705
Loss :  2.086729049682617 4.56599760055542 4.56599760055542
Loss :  2.1735153198242188 4.358488082885742 4.358488082885742
Loss :  2.0687592029571533 4.529191493988037 4.529191493988037
Loss :  2.1399083137512207 4.486905574798584 4.486905574798584
Loss :  2.0958194732666016 4.7003960609436035 4.7003960609436035
Loss :  1.899605393409729 4.497530937194824 4.497530937194824
Loss :  1.8806034326553345 4.397982120513916 4.397982120513916
Loss :  2.26389217376709 4.4476542472839355 4.4476542472839355
Loss :  2.027811288833618 4.493329048156738 4.493329048156738
  batch 40 loss: 2.027811288833618, 4.493329048156738, 4.493329048156738
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  2.2465732097625732 4.354391098022461 4.354391098022461
Loss :  2.098801851272583 4.464330196380615 4.464330196380615
Loss :  1.985221266746521 4.464329242706299 4.464329242706299
Loss :  1.7815794944763184 4.755385875701904 4.755385875701904
Loss :  1.7400377988815308 4.432593822479248 4.432593822479248
Loss :  1.8887170553207397 4.359131813049316 4.359131813049316
Loss :  1.9030522108078003 4.466258525848389 4.466258525848389
Loss :  1.993867039680481 4.585453987121582 4.585453987121582
Loss :  1.8272926807403564 4.446221351623535 4.446221351623535
Loss :  2.0383124351501465 4.4355998039245605 4.4355998039245605
Loss :  1.683330774307251 4.48649263381958 4.48649263381958
Loss :  2.0143802165985107 4.492568492889404 4.492568492889404
Loss :  1.723623514175415 4.464178085327148 4.464178085327148
Loss :  1.6202853918075562 4.314046859741211 4.314046859741211
Loss :  1.5082534551620483 4.434298515319824 4.434298515319824
Loss :  1.6129021644592285 4.693055152893066 4.693055152893066
Loss :  1.6150473356246948 4.431427478790283 4.431427478790283
Loss :  1.3881915807724 4.466225624084473 4.466225624084473
Loss :  1.2942882776260376 4.6089768409729 4.6089768409729
Loss :  1.5175038576126099 4.4682297706604 4.4682297706604
  batch 60 loss: 1.5175038576126099, 4.4682297706604, 4.4682297706604
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.3054735660552979 4.434998989105225 4.434998989105225
Loss :  1.288111686706543 4.4904561042785645 4.4904561042785645
Loss :  1.4740818738937378 4.401712894439697 4.401712894439697
Loss :  1.344738245010376 4.339881896972656 4.339881896972656
Loss :  1.3330358266830444 4.154220104217529 4.154220104217529
Loss :  212.9777374267578 4.438107490539551 4.438107490539551
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  127.13946533203125 4.400047302246094 4.400047302246094
Loss :  239.63931274414062 4.381667613983154 4.381667613983154
Loss :  173.47129821777344 4.110598564147949 4.110598564147949
Total LOSS train 4.474336983607365 valid 4.332605242729187
CE LOSS train 2.248772164491507 valid 43.36782455444336
Contrastive LOSS train 4.474336983607365 valid 1.0276496410369873
EPOCH 20:
Loss :  1.462976098060608 4.385458469390869 4.385458469390869
Loss :  1.416439414024353 4.5882792472839355 4.5882792472839355
Loss :  1.4126895666122437 4.381477355957031 4.381477355957031
Loss :  1.4293804168701172 4.469119548797607 4.469119548797607
Loss :  1.522011399269104 4.306823253631592 4.306823253631592
Loss :  1.516103982925415 4.529023170471191 4.529023170471191
Loss :  1.6714085340499878 4.523253917694092 4.523253917694092
Loss :  1.8267455101013184 4.297892093658447 4.297892093658447
Loss :  1.9062788486480713 4.429636001586914 4.429636001586914
Loss :  1.989328145980835 4.266247272491455 4.266247272491455
Loss :  2.101348638534546 4.4109368324279785 4.4109368324279785
Loss :  2.2895431518554688 4.493191719055176 4.493191719055176
Loss :  2.514193534851074 4.369129657745361 4.369129657745361
Loss :  2.7839467525482178 4.513171195983887 4.513171195983887
Loss :  2.643157482147217 4.412205219268799 4.412205219268799
Loss :  2.505754232406616 4.531012535095215 4.531012535095215
Loss :  2.808948040008545 4.479776859283447 4.479776859283447
Loss :  2.98177433013916 4.403192520141602 4.403192520141602
Loss :  2.9379005432128906 4.446529865264893 4.446529865264893
Loss :  2.6945621967315674 4.447445392608643 4.447445392608643
  batch 20 loss: 2.6945621967315674, 4.447445392608643, 4.447445392608643
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  3.041947841644287 4.321089267730713 4.321089267730713
Loss :  3.2905893325805664 4.534608840942383 4.534608840942383
Loss :  3.0886592864990234 4.473149299621582 4.473149299621582
Loss :  2.8230977058410645 4.3411993980407715 4.3411993980407715
Loss :  2.600235939025879 4.686310291290283 4.686310291290283
Loss :  2.905876636505127 4.36265754699707 4.36265754699707
Loss :  2.7367820739746094 4.6433000564575195 4.6433000564575195
Loss :  2.926511287689209 4.379572868347168 4.379572868347168
Loss :  3.1734116077423096 4.460815906524658 4.460815906524658
Loss :  2.7090530395507812 4.451733112335205 4.451733112335205
Loss :  3.1724188327789307 4.5560150146484375 4.5560150146484375
Loss :  2.7977049350738525 4.585088729858398 4.585088729858398
Loss :  2.8714749813079834 4.372080326080322 4.372080326080322
Loss :  2.8361682891845703 4.450988292694092 4.450988292694092
Loss :  3.0524849891662598 4.458225727081299 4.458225727081299
Loss :  3.010319709777832 4.4057698249816895 4.4057698249816895
Loss :  3.031109571456909 4.367553234100342 4.367553234100342
Loss :  2.6970410346984863 4.111482620239258 4.111482620239258
Loss :  2.5796451568603516 4.335855484008789 4.335855484008789
Loss :  2.5190603733062744 4.323915004730225 4.323915004730225
  batch 40 loss: 2.5190603733062744, 4.323915004730225, 4.323915004730225
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  2.6989388465881348 4.210573673248291 4.210573673248291
Loss :  2.616610050201416 4.264132499694824 4.264132499694824
Loss :  2.618671178817749 4.076779365539551 4.076779365539551
Loss :  2.7047598361968994 4.4735918045043945 4.4735918045043945
Loss :  2.8702235221862793 4.4116411209106445 4.4116411209106445
Loss :  2.9077014923095703 4.438952445983887 4.438952445983887
Loss :  3.041640043258667 4.612552165985107 4.612552165985107
Loss :  3.035207748413086 4.471229076385498 4.471229076385498
Loss :  3.130471706390381 4.263684272766113 4.263684272766113
Loss :  3.331571340560913 4.477912902832031 4.477912902832031
Loss :  3.277005910873413 4.456171035766602 4.456171035766602
Loss :  3.623824119567871 4.283038139343262 4.283038139343262
Loss :  3.50205397605896 4.366997241973877 4.366997241973877
Loss :  3.5040576457977295 4.501737594604492 4.501737594604492
Loss :  3.316324472427368 4.396064281463623 4.396064281463623
Loss :  3.8596479892730713 4.364785671234131 4.364785671234131
Loss :  3.8575239181518555 4.533942699432373 4.533942699432373
Loss :  3.8546087741851807 4.2983856201171875 4.2983856201171875
Loss :  4.170388698577881 4.441573143005371 4.441573143005371
Loss :  3.663008213043213 4.355668067932129 4.355668067932129
  batch 60 loss: 3.663008213043213, 4.355668067932129, 4.355668067932129
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 5], device='cuda:0')
Loss :  3.9410572052001953 4.3523430824279785 4.3523430824279785
Loss :  3.679170846939087 4.323456764221191 4.323456764221191
Loss :  3.793928623199463 4.369902610778809 4.369902610778809
Loss :  3.6670095920562744 4.2991132736206055 4.2991132736206055
Loss :  3.6765482425689697 4.097604274749756 4.097604274749756
Loss :  4.496572494506836 4.43693208694458 4.43693208694458
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3], device='cuda:0')
Loss :  4.483290672302246 4.425147533416748 4.425147533416748
Loss :  4.466243743896484 4.33421516418457 4.33421516418457
Loss :  4.808811187744141 4.297647476196289 4.297647476196289
Total LOSS train 4.406877627739539 valid 4.373485565185547
CE LOSS train 2.84030826825362 valid 1.2022027969360352
Contrastive LOSS train 4.406877627739539 valid 1.0744118690490723
EPOCH 21:
Loss :  3.5609686374664307 4.291117191314697 4.291117191314697
Loss :  3.3454747200012207 4.554542064666748 4.554542064666748
Loss :  3.5921971797943115 4.389278888702393 4.389278888702393
Loss :  3.5427756309509277 4.524362087249756 4.524362087249756
Loss :  3.5195980072021484 4.560621738433838 4.560621738433838
Loss :  3.666252374649048 4.372462272644043 4.372462272644043
Loss :  3.439070224761963 4.414242267608643 4.414242267608643
Loss :  3.490161657333374 4.385922431945801 4.385922431945801
Loss :  3.319547414779663 4.572184085845947 4.572184085845947
Loss :  3.3117918968200684 4.390326976776123 4.390326976776123
Loss :  3.446232795715332 4.487595558166504 4.487595558166504
Loss :  3.4595658779144287 4.429203510284424 4.429203510284424
Loss :  3.544786214828491 4.554215431213379 4.554215431213379
Loss :  3.429285764694214 4.429945468902588 4.429945468902588
Loss :  3.103947877883911 4.4091477394104 4.4091477394104
Loss :  3.6162073612213135 4.522633075714111 4.522633075714111
Loss :  3.4124042987823486 4.4316277503967285 4.4316277503967285
Loss :  3.4356563091278076 4.419118404388428 4.419118404388428
Loss :  3.2586233615875244 4.521523952484131 4.521523952484131
Loss :  3.3220438957214355 4.486697196960449 4.486697196960449
  batch 20 loss: 3.3220438957214355, 4.486697196960449, 4.486697196960449
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 3, 5], device='cuda:0')
Loss :  3.1340482234954834 4.256103992462158 4.256103992462158
Loss :  3.2309200763702393 4.528929710388184 4.528929710388184
Loss :  3.1956071853637695 4.543866157531738 4.543866157531738
Loss :  3.243178129196167 4.500258922576904 4.500258922576904
Loss :  3.220038890838623 4.686178207397461 4.686178207397461
Loss :  3.4229178428649902 4.480560302734375 4.480560302734375
Loss :  3.425976037979126 4.608351707458496 4.608351707458496
Loss :  3.2674918174743652 4.5284833908081055 4.5284833908081055
Loss :  3.5386321544647217 4.401649475097656 4.401649475097656
Loss :  3.163503646850586 4.422027587890625 4.422027587890625
Loss :  3.2534236907958984 4.499985694885254 4.499985694885254
Loss :  3.1573212146759033 4.6629533767700195 4.6629533767700195
Loss :  3.1803038120269775 4.488303184509277 4.488303184509277
Loss :  3.1291069984436035 4.637577533721924 4.637577533721924
Loss :  3.1363372802734375 4.359512805938721 4.359512805938721
Loss :  3.1249735355377197 4.5350751876831055 4.5350751876831055
Loss :  3.137795925140381 4.5024542808532715 4.5024542808532715
Loss :  3.0698490142822266 4.402251720428467 4.402251720428467
Loss :  3.274616241455078 4.5790629386901855 4.5790629386901855
Loss :  3.0007004737854004 4.3376898765563965 4.3376898765563965
  batch 40 loss: 3.0007004737854004, 4.3376898765563965, 4.3376898765563965
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3, 5], device='cuda:0')
Loss :  2.9841978549957275 4.479927062988281 4.479927062988281
Loss :  3.0127615928649902 4.550023555755615 4.550023555755615
Loss :  2.900881052017212 4.330255508422852 4.330255508422852
Loss :  2.956430435180664 4.575744152069092 4.575744152069092
Loss :  3.0906319618225098 4.298757076263428 4.298757076263428
Loss :  3.097601890563965 4.491836071014404 4.491836071014404
Loss :  3.239417791366577 4.602254867553711 4.602254867553711
Loss :  2.9803977012634277 4.571383953094482 4.571383953094482
Loss :  3.0098447799682617 4.446040153503418 4.446040153503418
Loss :  3.0293314456939697 4.558047294616699 4.558047294616699
Loss :  2.765507936477661 4.556446552276611 4.556446552276611
Loss :  2.836933135986328 4.553373336791992 4.553373336791992
Loss :  2.961653232574463 4.342728137969971 4.342728137969971
Loss :  3.038764476776123 4.442580223083496 4.442580223083496
Loss :  2.961660861968994 4.551638603210449 4.551638603210449
Loss :  3.192112445831299 4.352412700653076 4.352412700653076
Loss :  3.2266764640808105 4.456021308898926 4.456021308898926
Loss :  3.1598477363586426 4.407973766326904 4.407973766326904
Loss :  3.091712236404419 4.5041117668151855 4.5041117668151855
Loss :  2.9393420219421387 4.370375156402588 4.370375156402588
  batch 60 loss: 2.9393420219421387, 4.370375156402588, 4.370375156402588
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3, 5], device='cuda:0')
Loss :  3.1995785236358643 4.597343921661377 4.597343921661377
Loss :  2.9900124073028564 4.521174907684326 4.521174907684326
Loss :  3.132351875305176 4.446202754974365 4.446202754974365
Loss :  2.995931625366211 4.452469348907471 4.452469348907471
Loss :  3.0170910358428955 4.180790424346924 4.180790424346924
Loss :  2.8322246074676514 4.416831970214844 4.416831970214844
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3], device='cuda:0')
Loss :  2.90254807472229 4.32620906829834 4.32620906829834
Loss :  2.8138046264648438 4.2106828689575195 4.2106828689575195
Loss :  2.9456851482391357 4.200038433074951 4.200038433074951
Total LOSS train 4.473045488504263 valid 4.288440585136414
CE LOSS train 3.214369326371413 valid 0.7364212870597839
Contrastive LOSS train 4.473045488504263 valid 1.0500096082687378
Saved best model. Old loss 4.3116618394851685 and new best loss 4.288440585136414
EPOCH 22:
Loss :  3.0871026515960693 4.366283893585205 4.366283893585205
Loss :  2.9413905143737793 4.642656326293945 4.642656326293945
Loss :  3.0659947395324707 4.655693054199219 4.655693054199219
Loss :  2.9442317485809326 4.5314741134643555 4.5314741134643555
Loss :  3.089931011199951 4.337712287902832 4.337712287902832
Loss :  2.9537723064422607 4.3339409828186035 4.3339409828186035
Loss :  2.8561553955078125 4.388645648956299 4.388645648956299
Loss :  2.7706198692321777 4.481729984283447 4.481729984283447
Loss :  2.7551116943359375 4.3699164390563965 4.3699164390563965
Loss :  2.730297803878784 4.280593395233154 4.280593395233154
Loss :  2.8840653896331787 4.4107866287231445 4.4107866287231445
Loss :  2.890620470046997 4.645598888397217 4.645598888397217
Loss :  2.8789737224578857 4.41785192489624 4.41785192489624
Loss :  2.7364373207092285 4.402792930603027 4.402792930603027
Loss :  2.6299285888671875 4.502827167510986 4.502827167510986
Loss :  2.8161263465881348 4.41953182220459 4.41953182220459
Loss :  2.8228797912597656 4.419167995452881 4.419167995452881
Loss :  2.8441925048828125 4.589927673339844 4.589927673339844
Loss :  2.7810091972351074 4.35789680480957 4.35789680480957
Loss :  2.7567715644836426 4.39159631729126 4.39159631729126
  batch 20 loss: 2.7567715644836426, 4.39159631729126, 4.39159631729126
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3, 5], device='cuda:0')
Loss :  2.678149461746216 4.326809883117676 4.326809883117676
Loss :  2.834045171737671 4.3538079261779785 4.3538079261779785
Loss :  2.76874041557312 4.7638983726501465 4.7638983726501465
Loss :  2.738790988922119 4.545856952667236 4.545856952667236
Loss :  2.87325382232666 4.604829788208008 4.604829788208008
Loss :  2.773420572280884 4.6090617179870605 4.6090617179870605
Loss :  2.7662670612335205 4.628068447113037 4.628068447113037
Loss :  2.628589630126953 4.819614887237549 4.819614887237549
Loss :  2.8701467514038086 4.5766282081604 4.5766282081604
Loss :  2.6840476989746094 4.381045341491699 4.381045341491699
Loss :  2.753615379333496 4.554386615753174 4.554386615753174
Loss :  2.5451087951660156 4.639976501464844 4.639976501464844
Loss :  2.7535006999969482 4.4853973388671875 4.4853973388671875
Loss :  2.6667239665985107 4.602937698364258 4.602937698364258
Loss :  2.692145824432373 4.4160284996032715 4.4160284996032715
Loss :  2.7259533405303955 4.350125789642334 4.350125789642334
Loss :  2.7152490615844727 4.429623603820801 4.429623603820801
Loss :  2.5762088298797607 4.373952865600586 4.373952865600586
Loss :  2.791779041290283 4.36475133895874 4.36475133895874
Loss :  2.6402413845062256 4.610133171081543 4.610133171081543
  batch 40 loss: 2.6402413845062256, 4.610133171081543, 4.610133171081543
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3, 5], device='cuda:0')
Loss :  2.6395883560180664 4.483879089355469 4.483879089355469
Loss :  2.709643602371216 4.556869983673096 4.556869983673096
Loss :  2.624718427658081 4.498438358306885 4.498438358306885
Loss :  2.643176317214966 4.440685749053955 4.440685749053955
Loss :  2.6345691680908203 4.14067268371582 4.14067268371582
Loss :  2.641160726547241 4.365559101104736 4.365559101104736
Loss :  2.7570924758911133 4.457696914672852 4.457696914672852
Loss :  2.551236629486084 4.516305446624756 4.516305446624756
Loss :  2.468421697616577 4.434913158416748 4.434913158416748
Loss :  2.5958385467529297 4.4735941886901855 4.4735941886901855
Loss :  2.486124277114868 4.615793704986572 4.615793704986572
Loss :  2.603553295135498 4.29817533493042 4.29817533493042
Loss :  2.591707468032837 4.342957019805908 4.342957019805908
Loss :  2.7187910079956055 4.213858604431152 4.213858604431152
Loss :  2.529832601547241 4.3670759201049805 4.3670759201049805
Loss :  2.6827008724212646 4.33922815322876 4.33922815322876
Loss :  2.6179580688476562 4.5204176902771 4.5204176902771
Loss :  2.4902448654174805 4.521024703979492 4.521024703979492
Loss :  2.696378707885742 4.588170051574707 4.588170051574707
Loss :  2.535423517227173 4.479709148406982 4.479709148406982
  batch 60 loss: 2.535423517227173, 4.479709148406982, 4.479709148406982
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3, 4, 5], device='cuda:0')
Loss :  2.695913314819336 4.394439220428467 4.394439220428467
Loss :  2.5096631050109863 4.30842924118042 4.30842924118042
Loss :  2.621063470840454 4.268518447875977 4.268518447875977
Loss :  2.5397958755493164 4.258965492248535 4.258965492248535
Loss :  2.571298599243164 3.998265266418457 3.998265266418457
Loss :  6.403139591217041 4.39107608795166 4.39107608795166
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([5], device='cuda:0')
Loss :  6.20468282699585 4.422052383422852 4.422052383422852
Loss :  6.289947509765625 4.386663913726807 4.386663913726807
Loss :  6.254228115081787 4.291252136230469 4.291252136230469
Total LOSS train 4.450264644622803 valid 4.372761130332947
CE LOSS train 2.721038238818829 valid 1.5635570287704468
Contrastive LOSS train 4.450264644622803 valid 1.0728130340576172
EPOCH 23:
Loss :  2.5992846488952637 4.269223690032959 4.269223690032959
Loss :  2.617656707763672 4.31858491897583 4.31858491897583
Loss :  2.5941755771636963 4.12792444229126 4.12792444229126
Loss :  2.603937864303589 4.158754348754883 4.158754348754883
Loss :  2.499377727508545 4.315201282501221 4.315201282501221
Loss :  2.5931267738342285 4.344045639038086 4.344045639038086
Loss :  2.6005311012268066 4.340518474578857 4.340518474578857
Loss :  2.5041353702545166 4.3114190101623535 4.3114190101623535
Loss :  2.4593186378479004 4.349945545196533 4.349945545196533
Loss :  2.3518104553222656 4.270036697387695 4.270036697387695
Loss :  2.5277695655822754 4.435011386871338 4.435011386871338
Loss :  2.4760489463806152 4.329435348510742 4.329435348510742
Loss :  2.46500563621521 4.515313148498535 4.515313148498535
Loss :  2.361293077468872 4.445517539978027 4.445517539978027
Loss :  2.2007548809051514 4.260727882385254 4.260727882385254
Loss :  2.4426259994506836 4.391859531402588 4.391859531402588
Loss :  2.4691717624664307 4.550492286682129 4.550492286682129
Loss :  2.339999198913574 4.312345504760742 4.312345504760742
Loss :  2.281318187713623 4.458816051483154 4.458816051483154
Loss :  2.2525863647460938 4.380527019500732 4.380527019500732
  batch 20 loss: 2.2525863647460938, 4.380527019500732, 4.380527019500732
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3, 5], device='cuda:0')
Loss :  2.3038432598114014 4.608687877655029 4.608687877655029
Loss :  2.333146810531616 4.346543312072754 4.346543312072754
Loss :  2.3319296836853027 4.479579448699951 4.479579448699951
Loss :  2.341958522796631 4.4275898933410645 4.4275898933410645
Loss :  2.3257620334625244 4.502832412719727 4.502832412719727
Loss :  2.387124538421631 4.258371829986572 4.258371829986572
Loss :  2.4952027797698975 4.647595405578613 4.647595405578613
Loss :  2.305100440979004 4.420610427856445 4.420610427856445
Loss :  2.3432540893554688 4.511472702026367 4.511472702026367
Loss :  2.192976236343384 4.389946460723877 4.389946460723877
Loss :  2.3213722705841064 4.523200988769531 4.523200988769531
Loss :  2.1307759284973145 4.807972431182861 4.807972431182861
Loss :  2.2091939449310303 4.591089725494385 4.591089725494385
Loss :  2.2157061100006104 4.430371284484863 4.430371284484863
Loss :  2.2171640396118164 4.454679012298584 4.454679012298584
Loss :  2.214517831802368 4.45494270324707 4.45494270324707
Loss :  2.135425329208374 4.420435428619385 4.420435428619385
Loss :  2.0996999740600586 4.477108955383301 4.477108955383301
Loss :  2.1716468334198 4.565018653869629 4.565018653869629
Loss :  2.132718801498413 4.4444403648376465 4.4444403648376465
  batch 40 loss: 2.132718801498413, 4.4444403648376465, 4.4444403648376465
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  2.1991569995880127 4.399665355682373 4.399665355682373
Loss :  2.2442963123321533 4.29803466796875 4.29803466796875
Loss :  2.147176504135132 4.421109676361084 4.421109676361084
Loss :  2.080601692199707 4.532742977142334 4.532742977142334
Loss :  2.087310314178467 4.2948222160339355 4.2948222160339355
Loss :  2.1564419269561768 4.378036975860596 4.378036975860596
Loss :  2.2462551593780518 4.553511142730713 4.553511142730713
Loss :  2.059760093688965 4.611588954925537 4.611588954925537
Loss :  2.0910286903381348 4.412355899810791 4.412355899810791
Loss :  2.0888795852661133 4.503445148468018 4.503445148468018
Loss :  2.00150465965271 4.629727363586426 4.629727363586426
Loss :  2.111598014831543 4.398913383483887 4.398913383483887
Loss :  2.0851635932922363 4.333968639373779 4.333968639373779
Loss :  2.180601119995117 4.3571271896362305 4.3571271896362305
Loss :  2.032121181488037 4.493473529815674 4.493473529815674
Loss :  2.2295989990234375 4.454658508300781 4.454658508300781
Loss :  2.198225259780884 4.4977216720581055 4.4977216720581055
Loss :  2.1538357734680176 4.466241836547852 4.466241836547852
Loss :  2.2011539936065674 4.66950798034668 4.66950798034668
Loss :  2.140681743621826 4.670984745025635 4.670984745025635
  batch 60 loss: 2.140681743621826, 4.670984745025635, 4.670984745025635
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3, 5], device='cuda:0')
Loss :  2.165827989578247 4.546436786651611 4.546436786651611
Loss :  2.079911708831787 4.517525672912598 4.517525672912598
Loss :  2.1701042652130127 4.550779819488525 4.550779819488525
Loss :  2.105501413345337 4.412628650665283 4.412628650665283
Loss :  2.1553304195404053 4.132457733154297 4.132457733154297
Loss :  2.0081377029418945 4.438512802124023 4.438512802124023
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
Loss :  1.9936060905456543 4.463769435882568 4.463769435882568
Loss :  1.9604676961898804 4.3057379722595215 4.3057379722595215
Loss :  2.033564329147339 4.0726189613342285 4.0726189613342285
Total LOSS train 4.433625470674955 valid 4.3201597929000854
CE LOSS train 2.274777159324059 valid 0.5083910822868347
Contrastive LOSS train 4.433625470674955 valid 1.0181547403335571
EPOCH 24:
Loss :  2.164393663406372 4.333967208862305 4.333967208862305
Loss :  2.08605694770813 4.42362642288208 4.42362642288208
Loss :  2.1390533447265625 4.3892059326171875 4.3892059326171875
Loss :  2.1297640800476074 4.372978687286377 4.372978687286377
Loss :  2.126271963119507 4.3125786781311035 4.3125786781311035
Loss :  2.2390410900115967 4.355323791503906 4.355323791503906
Loss :  2.126823663711548 4.495521068572998 4.495521068572998
Loss :  2.112287759780884 4.37050724029541 4.37050724029541
Loss :  2.1025466918945312 4.3223958015441895 4.3223958015441895
Loss :  2.0978853702545166 4.484517574310303 4.484517574310303
Loss :  2.1408536434173584 4.468897819519043 4.468897819519043
Loss :  2.1657066345214844 4.441021919250488 4.441021919250488
Loss :  2.1931700706481934 4.518115043640137 4.518115043640137
Loss :  2.1143462657928467 4.48087739944458 4.48087739944458
Loss :  1.9976704120635986 4.466985702514648 4.466985702514648
Loss :  2.1546030044555664 4.4763641357421875 4.4763641357421875
Loss :  2.202308416366577 4.326909065246582 4.326909065246582
Loss :  2.1394479274749756 4.446656227111816 4.446656227111816
Loss :  2.1244571208953857 4.336791038513184 4.336791038513184
Loss :  2.0466175079345703 4.460254192352295 4.460254192352295
  batch 20 loss: 2.0466175079345703, 4.460254192352295, 4.460254192352295
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3, 5], device='cuda:0')
Loss :  2.1034467220306396 4.366145610809326 4.366145610809326
Loss :  2.1558609008789062 4.5308098793029785 4.5308098793029785
Loss :  2.136248826980591 4.512753009796143 4.512753009796143
Loss :  2.1446175575256348 4.500794887542725 4.500794887542725
Loss :  2.163541316986084 4.48836612701416 4.48836612701416
Loss :  2.1517233848571777 4.773299694061279 4.773299694061279
Loss :  2.2180161476135254 4.571673393249512 4.571673393249512
Loss :  2.061574697494507 4.326665878295898 4.326665878295898
Loss :  2.198817253112793 4.4495015144348145 4.4495015144348145
Loss :  2.0946481227874756 4.422280311584473 4.422280311584473
Loss :  2.168339490890503 4.472909927368164 4.472909927368164
Loss :  2.046308755874634 4.657158851623535 4.657158851623535
Loss :  2.096014976501465 4.5186896324157715 4.5186896324157715
Loss :  2.081934928894043 4.5133867263793945 4.5133867263793945
Loss :  2.0974597930908203 4.406421184539795 4.406421184539795
Loss :  2.137380599975586 4.390664100646973 4.390664100646973
Loss :  2.0883471965789795 4.3548173904418945 4.3548173904418945
Loss :  1.9994056224822998 4.412729740142822 4.412729740142822
Loss :  2.128602981567383 4.350491523742676 4.350491523742676
Loss :  2.047119140625 4.480759620666504 4.480759620666504
  batch 40 loss: 2.047119140625, 4.480759620666504, 4.480759620666504
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3, 4, 5], device='cuda:0')
Loss :  2.0536820888519287 4.40177583694458 4.40177583694458
Loss :  2.1126575469970703 4.250707626342773 4.250707626342773
Loss :  2.035266876220703 4.147680282592773 4.147680282592773
Loss :  2.0246806144714355 4.494961261749268 4.494961261749268
Loss :  2.065692901611328 4.3102922439575195 4.3102922439575195
Loss :  2.0595955848693848 4.151461601257324 4.151461601257324
Loss :  2.077038526535034 4.237009525299072 4.237009525299072
Loss :  2.038734197616577 4.223883152008057 4.223883152008057
Loss :  2.0297129154205322 4.2694091796875 4.2694091796875
Loss :  2.048447370529175 4.47031307220459 4.47031307220459
Loss :  1.9243886470794678 4.650347709655762 4.650347709655762
Loss :  2.0209033489227295 4.547402381896973 4.547402381896973
Loss :  2.0713956356048584 4.706794261932373 4.706794261932373
Loss :  2.073944568634033 4.335219860076904 4.335219860076904
Loss :  2.0123791694641113 4.410982131958008 4.410982131958008
Loss :  2.0838139057159424 4.405283451080322 4.405283451080322
Loss :  2.0941569805145264 4.430907726287842 4.430907726287842
Loss :  2.0889699459075928 4.533734321594238 4.533734321594238
Loss :  2.158508777618408 4.155622959136963 4.155622959136963
Loss :  2.010380744934082 4.135965824127197 4.135965824127197
  batch 60 loss: 2.010380744934082, 4.135965824127197, 4.135965824127197
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3, 5], device='cuda:0')
Loss :  2.1203267574310303 4.480267524719238 4.480267524719238
Loss :  2.0098838806152344 4.298939228057861 4.298939228057861
Loss :  2.020494222640991 4.107128620147705 4.107128620147705
Loss :  2.0508337020874023 4.091855525970459 4.091855525970459
Loss :  2.0907857418060303 3.9319021701812744 3.9319021701812744
Loss :  2.0348174571990967 4.391972064971924 4.391972064971924
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3, 5], device='cuda:0')
Loss :  2.0176334381103516 4.398625373840332 4.398625373840332
Loss :  2.0000550746917725 4.341029167175293 4.341029167175293
Loss :  2.045060396194458 4.14920711517334 4.14920711517334
Total LOSS train 4.399440945111788 valid 4.320208430290222
CE LOSS train 2.0969136714935304 valid 0.5112650990486145
Contrastive LOSS train 4.399440945111788 valid 1.037301778793335
EPOCH 25:
Loss :  2.039393901824951 4.154314994812012 4.154314994812012
Loss :  1.9818540811538696 4.48235559463501 4.48235559463501
Loss :  2.0655040740966797 4.198891639709473 4.198891639709473
Loss :  2.0470921993255615 4.294890403747559 4.294890403747559
Loss :  2.03017520904541 4.312328815460205 4.312328815460205
Loss :  2.1269283294677734 4.49471378326416 4.49471378326416
Loss :  2.0426037311553955 4.382252216339111 4.382252216339111
Loss :  2.0375475883483887 4.165921211242676 4.165921211242676
Loss :  2.0332906246185303 4.231098175048828 4.231098175048828
Loss :  2.0213160514831543 4.151573657989502 4.151573657989502
Loss :  2.1143150329589844 4.297633647918701 4.297633647918701
Loss :  2.079674005508423 4.291836261749268 4.291836261749268
Loss :  2.12548828125 4.272552013397217 4.272552013397217
Loss :  2.0884242057800293 4.2129950523376465 4.2129950523376465
Loss :  1.9770394563674927 3.785224676132202 3.785224676132202
Loss :  2.085845947265625 4.343347072601318 4.343347072601318
Loss :  2.0688397884368896 4.32065486907959 4.32065486907959
Loss :  2.074779748916626 4.7397027015686035 4.7397027015686035
Loss :  2.0594124794006348 4.41945219039917 4.41945219039917
Loss :  1.9939264059066772 4.462430953979492 4.462430953979492
  batch 20 loss: 1.9939264059066772, 4.462430953979492, 4.462430953979492
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3, 5], device='cuda:0')
Loss :  2.0475542545318604 4.287308216094971 4.287308216094971
Loss :  2.0538876056671143 4.434790134429932 4.434790134429932
Loss :  2.0259902477264404 4.52398157119751 4.52398157119751
Loss :  2.0673041343688965 4.451613426208496 4.451613426208496
Loss :  2.058688163757324 4.530797958374023 4.530797958374023
Loss :  2.053445816040039 4.3388471603393555 4.3388471603393555
Loss :  2.0848374366760254 4.5471954345703125 4.5471954345703125
Loss :  2.019874095916748 4.357847213745117 4.357847213745117
Loss :  2.1565215587615967 4.373805046081543 4.373805046081543
Loss :  2.004995346069336 4.4609832763671875 4.4609832763671875
Loss :  2.130688190460205 4.519623756408691 4.519623756408691
Loss :  2.0210394859313965 4.552856922149658 4.552856922149658
Loss :  2.037097454071045 4.485776901245117 4.485776901245117
Loss :  2.0969762802124023 4.472557544708252 4.472557544708252
Loss :  2.1259913444519043 4.437662124633789 4.437662124633789
Loss :  2.135915756225586 4.441038131713867 4.441038131713867
Loss :  2.124223470687866 4.352690696716309 4.352690696716309
Loss :  2.029850482940674 4.282286643981934 4.282286643981934
Loss :  2.0600318908691406 4.505868911743164 4.505868911743164
Loss :  2.0885140895843506 4.510410785675049 4.510410785675049
  batch 40 loss: 2.0885140895843506, 4.510410785675049, 4.510410785675049
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3, 5], device='cuda:0')
Loss :  2.108022928237915 4.408689498901367 4.408689498901367
Loss :  2.1040244102478027 4.446481227874756 4.446481227874756
Loss :  2.070220470428467 4.453187942504883 4.453187942504883
Loss :  2.0556654930114746 4.499200820922852 4.499200820922852
Loss :  2.109868288040161 4.327225208282471 4.327225208282471
Loss :  2.079228401184082 4.374351501464844 4.374351501464844
Loss :  2.083461284637451 4.404764652252197 4.404764652252197
Loss :  2.0643181800842285 4.546637535095215 4.546637535095215
Loss :  2.0320959091186523 4.624472141265869 4.624472141265869
Loss :  2.0408973693847656 4.481755256652832 4.481755256652832
Loss :  1.9479259252548218 4.5435638427734375 4.5435638427734375
Loss :  2.0527186393737793 4.437695026397705 4.437695026397705
Loss :  2.0991973876953125 4.384943008422852 4.384943008422852
Loss :  2.1144886016845703 4.58665657043457 4.58665657043457
Loss :  2.0373988151550293 4.507974624633789 4.507974624633789
Loss :  2.1461989879608154 4.46476936340332 4.46476936340332
Loss :  2.1564254760742188 4.510853290557861 4.510853290557861
Loss :  2.129979372024536 4.492042064666748 4.492042064666748
Loss :  2.149317502975464 4.605360507965088 4.605360507965088
Loss :  2.06674861907959 4.471904277801514 4.471904277801514
  batch 60 loss: 2.06674861907959, 4.471904277801514, 4.471904277801514
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3, 5], device='cuda:0')
Loss :  2.146949529647827 4.526400089263916 4.526400089263916
Loss :  2.0319466590881348 4.417701244354248 4.417701244354248
Loss :  2.070124864578247 4.380016803741455 4.380016803741455
Loss :  2.0948939323425293 4.371936321258545 4.371936321258545
Loss :  2.1401379108428955 4.284938812255859 4.284938812255859
Loss :  3.2574586868286133 4.452589988708496 4.452589988708496
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3, 5], device='cuda:0')
Loss :  3.332223892211914 4.376708984375 4.376708984375
Loss :  3.4548211097717285 4.444993019104004 4.444993019104004
Loss :  3.096572160720825 4.290716648101807 4.290716648101807
Total LOSS train 4.406702052629911 valid 4.391252160072327
CE LOSS train 2.0715251262371357 valid 0.7741430401802063
Contrastive LOSS train 4.406702052629911 valid 1.0726791620254517
EPOCH 26:
Loss :  2.0938000679016113 4.268674373626709 4.268674373626709
Loss :  2.0648109912872314 4.536782264709473 4.536782264709473
Loss :  2.1133666038513184 4.333282947540283 4.333282947540283
Loss :  2.1361327171325684 4.348336696624756 4.348336696624756
Loss :  2.1098265647888184 4.375356197357178 4.375356197357178
Loss :  2.1989614963531494 4.2970733642578125 4.2970733642578125
Loss :  2.106034278869629 4.646182060241699 4.646182060241699
Loss :  2.0834529399871826 4.600019454956055 4.600019454956055
Loss :  2.0984671115875244 4.4065470695495605 4.4065470695495605
Loss :  2.042451858520508 4.317622184753418 4.317622184753418
Loss :  2.095914363861084 4.448976516723633 4.448976516723633
Loss :  2.1356942653656006 4.446601390838623 4.446601390838623
Loss :  2.1172807216644287 4.553560256958008 4.553560256958008
Loss :  2.1040256023406982 4.508714199066162 4.508714199066162
Loss :  1.9839489459991455 4.40267276763916 4.40267276763916
Loss :  2.1175742149353027 4.512442588806152 4.512442588806152
Loss :  2.128148317337036 4.396712303161621 4.396712303161621
Loss :  2.120246410369873 4.3977556228637695 4.3977556228637695
Loss :  2.0286412239074707 4.457544326782227 4.457544326782227
Loss :  2.0436105728149414 4.3795084953308105 4.3795084953308105
  batch 20 loss: 2.0436105728149414, 4.3795084953308105, 4.3795084953308105
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3, 5], device='cuda:0')
Loss :  1.96603262424469 4.2888898849487305 4.2888898849487305
Loss :  2.1121227741241455 4.467584133148193 4.467584133148193
Loss :  2.071241617202759 4.515714645385742 4.515714645385742
Loss :  2.1017613410949707 4.46303653717041 4.46303653717041
Loss :  2.144705295562744 4.5667405128479 4.5667405128479
Loss :  2.134589433670044 4.460848331451416 4.460848331451416
Loss :  2.1533589363098145 4.609323501586914 4.609323501586914
Loss :  2.0858547687530518 4.3887553215026855 4.3887553215026855
Loss :  2.155271291732788 4.6244354248046875 4.6244354248046875
Loss :  2.050748348236084 4.451057434082031 4.451057434082031
Loss :  2.1280577182769775 4.576154708862305 4.576154708862305
Loss :  1.9926949739456177 4.527204990386963 4.527204990386963
Loss :  2.0801684856414795 4.516168117523193 4.516168117523193
Loss :  2.105954885482788 4.604886054992676 4.604886054992676
Loss :  2.136013984680176 4.456826686859131 4.456826686859131
Loss :  2.0931363105773926 4.5386810302734375 4.5386810302734375
Loss :  2.1088874340057373 4.473325252532959 4.473325252532959
Loss :  2.0015997886657715 4.461272716522217 4.461272716522217
Loss :  2.012094736099243 4.34841251373291 4.34841251373291
Loss :  2.045060634613037 4.461668014526367 4.461668014526367
  batch 40 loss: 2.045060634613037, 4.461668014526367, 4.461668014526367
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3, 5], device='cuda:0')
Loss :  2.09889817237854 4.421340465545654 4.421340465545654
Loss :  2.0719900131225586 4.2830963134765625 4.2830963134765625
Loss :  2.0798327922821045 4.442525863647461 4.442525863647461
Loss :  2.057823419570923 4.346877098083496 4.346877098083496
Loss :  2.083209276199341 4.449037551879883 4.449037551879883
Loss :  2.0646798610687256 4.337025165557861 4.337025165557861
Loss :  1.9580764770507812 4.465607166290283 4.465607166290283
Loss :  2.010101795196533 4.519764423370361 4.519764423370361
Loss :  1.903232455253601 4.48983097076416 4.48983097076416
Loss :  2.015094518661499 4.41469669342041 4.41469669342041
Loss :  1.9226502180099487 4.558875560760498 4.558875560760498
Loss :  2.0061471462249756 4.43241024017334 4.43241024017334
Loss :  2.0410244464874268 4.515615463256836 4.515615463256836
Loss :  2.006803512573242 4.507177352905273 4.507177352905273
Loss :  1.9742189645767212 4.424706935882568 4.424706935882568
Loss :  1.979103684425354 4.392030239105225 4.392030239105225
Loss :  2.021397590637207 4.495617389678955 4.495617389678955
Loss :  2.0594983100891113 4.451603889465332 4.451603889465332
Loss :  1.96262788772583 4.628129482269287 4.628129482269287
Loss :  1.8891279697418213 4.455174922943115 4.455174922943115
  batch 60 loss: 1.8891279697418213, 4.455174922943115, 4.455174922943115
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3, 5], device='cuda:0')
Loss :  2.0763766765594482 4.505519390106201 4.505519390106201
Loss :  1.9394339323043823 4.518026828765869 4.518026828765869
Loss :  1.9546748399734497 4.441357135772705 4.441357135772705
Loss :  2.0308971405029297 4.447025775909424 4.447025775909424
Loss :  1.989236831665039 4.133057594299316 4.133057594299316
Loss :  1.8312777280807495 4.4290666580200195 4.4290666580200195
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
Loss :  1.831637978553772 4.405409336090088 4.405409336090088
Loss :  1.8174200057983398 4.2999396324157715 4.2999396324157715
Loss :  1.8182215690612793 4.110408306121826 4.110408306121826
Total LOSS train 4.4540227816655085 valid 4.311205983161926
CE LOSS train 2.058429270524245 valid 0.4545553922653198
Contrastive LOSS train 4.4540227816655085 valid 1.0276020765304565
EPOCH 27:
Loss :  1.9926481246948242 4.268208980560303 4.268208980560303
Loss :  1.8624744415283203 4.540496826171875 4.540496826171875
Loss :  1.9616857767105103 4.395296096801758 4.395296096801758
Loss :  1.9597965478897095 4.515454292297363 4.515454292297363
Loss :  1.9749224185943604 4.3834147453308105 4.3834147453308105
Loss :  1.979642629623413 4.3631463050842285 4.3631463050842285
Loss :  1.886515736579895 4.410078525543213 4.410078525543213
Loss :  1.9186177253723145 4.419133186340332 4.419133186340332
Loss :  1.9884698390960693 4.420887470245361 4.420887470245361
Loss :  1.8681328296661377 4.369444370269775 4.369444370269775
Loss :  2.0280373096466064 4.504906177520752 4.504906177520752
Loss :  1.9096691608428955 4.483564376831055 4.483564376831055
Loss :  2.039299964904785 4.603172779083252 4.603172779083252
Loss :  1.9160736799240112 4.591428756713867 4.591428756713867
Loss :  1.8941327333450317 4.375336170196533 4.375336170196533
Loss :  1.8877992630004883 4.462540149688721 4.462540149688721
Loss :  2.0198752880096436 4.3749918937683105 4.3749918937683105
Loss :  1.987560749053955 4.42702054977417 4.42702054977417
Loss :  2.064237117767334 4.343155384063721 4.343155384063721
Loss :  1.9580414295196533 4.46913480758667 4.46913480758667
  batch 20 loss: 1.9580414295196533, 4.46913480758667, 4.46913480758667
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 3, 5], device='cuda:0')
Loss :  1.9272006750106812 4.333159446716309 4.333159446716309
Loss :  2.0436534881591797 4.55561637878418 4.55561637878418
Loss :  2.0427846908569336 4.602402687072754 4.602402687072754
Loss :  1.9298930168151855 4.312929630279541 4.312929630279541
Loss :  1.9323139190673828 4.486883163452148 4.486883163452148
Loss :  1.9886744022369385 4.4938788414001465 4.4938788414001465
Loss :  1.9690361022949219 4.626704216003418 4.626704216003418
Loss :  1.9987996816635132 4.334075450897217 4.334075450897217
Loss :  2.062892436981201 4.480124473571777 4.480124473571777
Loss :  1.8925172090530396 4.397240161895752 4.397240161895752
Loss :  2.078213691711426 4.419548034667969 4.419548034667969
Loss :  1.991213083267212 4.698897361755371 4.698897361755371
Loss :  1.9129616022109985 4.241919040679932 4.241919040679932
Loss :  1.9500442743301392 4.519323348999023 4.519323348999023
Loss :  2.0401668548583984 4.304542541503906 4.304542541503906
Loss :  2.014976739883423 4.375367164611816 4.375367164611816
Loss :  2.0634195804595947 4.485803127288818 4.485803127288818
Loss :  1.980462670326233 4.370841979980469 4.370841979980469
Loss :  1.891139268875122 4.298227310180664 4.298227310180664
Loss :  1.9263699054718018 4.551382064819336 4.551382064819336
  batch 40 loss: 1.9263699054718018, 4.551382064819336, 4.551382064819336
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 3, 5], device='cuda:0')
Loss :  2.0202105045318604 4.583927631378174 4.583927631378174
Loss :  2.044739007949829 4.389246940612793 4.389246940612793
Loss :  2.065524101257324 4.527292251586914 4.527292251586914
Loss :  2.0875942707061768 4.523471355438232 4.523471355438232
Loss :  2.0889499187469482 4.321808815002441 4.321808815002441
Loss :  1.9993525743484497 4.405052661895752 4.405052661895752
Loss :  1.955662488937378 4.512380599975586 4.512380599975586
Loss :  2.051046133041382 4.454259395599365 4.454259395599365
Loss :  1.9861308336257935 4.471059322357178 4.471059322357178
Loss :  2.0211341381073 4.496434211730957 4.496434211730957
Loss :  1.9739433526992798 4.5739641189575195 4.5739641189575195
Loss :  1.9940402507781982 4.418166637420654 4.418166637420654
Loss :  1.9768695831298828 4.410444736480713 4.410444736480713
Loss :  1.9610388278961182 4.424880027770996 4.424880027770996
Loss :  2.0868828296661377 4.451472282409668 4.451472282409668
Loss :  1.8889743089675903 4.3877763748168945 4.3877763748168945
Loss :  2.100975513458252 4.473998546600342 4.473998546600342
Loss :  2.136018753051758 4.376547336578369 4.376547336578369
Loss :  2.053790807723999 4.671389102935791 4.671389102935791
Loss :  1.912211298942566 4.405043601989746 4.405043601989746
  batch 60 loss: 1.912211298942566, 4.405043601989746, 4.405043601989746
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 3, 5], device='cuda:0')
Loss :  2.1117119789123535 4.5307207107543945 4.5307207107543945
Loss :  2.091277837753296 4.437200546264648 4.437200546264648
Loss :  2.151958703994751 4.423527240753174 4.423527240753174
Loss :  2.181648015975952 4.398188591003418 4.398188591003418
Loss :  2.1665101051330566 4.029517650604248 4.029517650604248
Loss :  3.5286974906921387 4.453776836395264 4.453776836395264
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 3], device='cuda:0')
Loss :  3.419508218765259 4.42294979095459 4.42294979095459
Loss :  3.595615863800049 4.255799293518066 4.255799293518066
Loss :  3.1620001792907715 4.2297468185424805 4.2297468185424805
Total LOSS train 4.441653060913086 valid 4.3405681848526
CE LOSS train 1.9975778799790602 valid 0.7905000448226929
Contrastive LOSS train 4.441653060913086 valid 1.0574367046356201
EPOCH 28:
Loss :  2.037320852279663 4.247178554534912 4.247178554534912
Loss :  1.9986052513122559 4.57421875 4.57421875
Loss :  2.076331615447998 4.4354987144470215 4.4354987144470215
Loss :  2.0734174251556396 4.4805097579956055 4.4805097579956055
Loss :  2.0392906665802 4.30637264251709 4.30637264251709
Loss :  2.011913299560547 4.4334001541137695 4.4334001541137695
Loss :  1.9767388105392456 4.505497932434082 4.505497932434082
Loss :  2.091853380203247 4.329916000366211 4.329916000366211
Loss :  2.1424646377563477 4.355147838592529 4.355147838592529
Loss :  2.0841481685638428 4.52670955657959 4.52670955657959
Loss :  2.22812819480896 4.443324565887451 4.443324565887451
Loss :  2.0593819618225098 4.587514400482178 4.587514400482178
Loss :  2.104423999786377 4.6677937507629395 4.6677937507629395
Loss :  2.098634719848633 4.860908031463623 4.860908031463623
Loss :  2.032015323638916 4.575518608093262 4.575518608093262
Loss :  1.9540565013885498 4.496165752410889 4.496165752410889
Loss :  2.0789663791656494 4.663919925689697 4.663919925689697
Loss :  2.108633279800415 4.6750874519348145 4.6750874519348145
Loss :  2.079495906829834 4.413156509399414 4.413156509399414
Loss :  1.9408414363861084 4.47055196762085 4.47055196762085
  batch 20 loss: 1.9408414363861084, 4.47055196762085, 4.47055196762085
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 3, 5], device='cuda:0')
Loss :  2.080108165740967 4.285236835479736 4.285236835479736
Loss :  2.129483222961426 4.410453796386719 4.410453796386719
Loss :  2.0410773754119873 4.597767353057861 4.597767353057861
Loss :  2.030198335647583 4.4236578941345215 4.4236578941345215
Loss :  1.9749282598495483 4.658056259155273 4.658056259155273
Loss :  2.0182907581329346 4.296087741851807 4.296087741851807
Loss :  2.034851551055908 4.661102294921875 4.661102294921875
Loss :  2.0147435665130615 4.352751731872559 4.352751731872559
Loss :  2.1366450786590576 4.481378555297852 4.481378555297852
Loss :  2.0383667945861816 4.43250036239624 4.43250036239624
Loss :  2.1352617740631104 4.5790629386901855 4.5790629386901855
Loss :  2.1580348014831543 4.573526859283447 4.573526859283447
Loss :  2.082411527633667 4.7183942794799805 4.7183942794799805
Loss :  2.077779769897461 4.478583812713623 4.478583812713623
Loss :  2.166653871536255 4.538354396820068 4.538354396820068
Loss :  2.052482843399048 4.453578472137451 4.453578472137451
Loss :  2.0579898357391357 4.516271591186523 4.516271591186523
Loss :  1.9949129819869995 4.614201545715332 4.614201545715332
Loss :  2.00441837310791 4.40402364730835 4.40402364730835
Loss :  2.0217933654785156 4.3954691886901855 4.3954691886901855
  batch 40 loss: 2.0217933654785156, 4.3954691886901855, 4.3954691886901855
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 3, 4, 5], device='cuda:0')
Loss :  2.050178050994873 4.665561199188232 4.665561199188232
Loss :  2.055438756942749 4.447522163391113 4.447522163391113
Loss :  2.118877410888672 4.402773857116699 4.402773857116699
Loss :  2.1664974689483643 4.62595272064209 4.62595272064209
Loss :  2.1007025241851807 4.443693161010742 4.443693161010742
Loss :  2.068291664123535 4.4802117347717285 4.4802117347717285
Loss :  2.0516088008880615 4.419687747955322 4.419687747955322
Loss :  2.03606915473938 4.359683036804199 4.359683036804199
Loss :  1.9955471754074097 4.386975288391113 4.386975288391113
Loss :  2.0098378658294678 4.507512092590332 4.507512092590332
Loss :  2.1120853424072266 4.502830982208252 4.502830982208252
Loss :  2.033001661300659 4.406188488006592 4.406188488006592
Loss :  2.0927529335021973 4.456582069396973 4.456582069396973
Loss :  2.07847261428833 4.41141939163208 4.41141939163208
Loss :  2.105661630630493 4.424394607543945 4.424394607543945
Loss :  2.0170845985412598 4.43453311920166 4.43453311920166
Loss :  2.048144817352295 4.567696571350098 4.567696571350098
Loss :  2.112229585647583 4.43051290512085 4.43051290512085
Loss :  2.025235891342163 4.673102378845215 4.673102378845215
Loss :  2.0113162994384766 4.379400253295898 4.379400253295898
  batch 60 loss: 2.0113162994384766, 4.379400253295898, 4.379400253295898
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 3, 4, 5], device='cuda:0')
Loss :  2.0767784118652344 4.485953330993652 4.485953330993652
Loss :  2.0144503116607666 3.9326388835906982 3.9326388835906982
Loss :  2.027177333831787 4.192172050476074 4.192172050476074
Loss :  2.0035653114318848 4.196207046508789 4.196207046508789
Loss :  2.0424764156341553 3.864187002182007 3.864187002182007
Loss :  2.3614208698272705 4.444239139556885 4.444239139556885
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([5], device='cuda:0')
Loss :  2.336780309677124 4.444457054138184 4.444457054138184
Loss :  2.353872776031494 4.370818138122559 4.370818138122559
Loss :  2.3942651748657227 4.296560764312744 4.296560764312744
Total LOSS train 4.4617883462172285 valid 4.389018774032593
CE LOSS train 2.0603165553166316 valid 0.5985662937164307
Contrastive LOSS train 4.4617883462172285 valid 1.074140191078186
EPOCH 29:
Loss :  1.94321608543396 3.493546485900879 3.493546485900879
Loss :  1.8893905878067017 3.688459634780884 3.688459634780884
Loss :  1.9423524141311646 3.989377021789551 3.989377021789551
Loss :  1.9518173933029175 3.554351806640625 3.554351806640625
Loss :  1.9096983671188354 4.152615070343018 4.152615070343018
Loss :  1.9410005807876587 3.9740562438964844 3.9740562438964844
Loss :  1.9910600185394287 4.172841548919678 4.172841548919678
Loss :  1.9280502796173096 3.3696234226226807 3.3696234226226807
Loss :  1.9555360078811646 3.4316678047180176 3.4316678047180176
Loss :  1.8898122310638428 3.305429220199585 3.305429220199585
Loss :  2.0041210651397705 3.910179615020752 3.910179615020752
Loss :  2.035215139389038 4.411910533905029 4.411910533905029
Loss :  1.9996284246444702 3.7426023483276367 3.7426023483276367
Loss :  2.0082340240478516 3.5079538822174072 3.5079538822174072
Loss :  1.9228265285491943 3.8439958095550537 3.8439958095550537
Loss :  1.9072036743164062 3.7274625301361084 3.7274625301361084
Loss :  1.996533989906311 3.4055259227752686 3.4055259227752686
Loss :  1.913248896598816 4.053906440734863 4.053906440734863
Loss :  1.9774930477142334 4.4543256759643555 4.4543256759643555
Loss :  1.8769067525863647 4.439668655395508 4.439668655395508
  batch 20 loss: 1.8769067525863647, 4.439668655395508, 4.439668655395508
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 5], device='cuda:0')
Loss :  1.9825034141540527 4.278451442718506 4.278451442718506
Loss :  1.9606080055236816 4.343048572540283 4.343048572540283
Loss :  1.904072880744934 4.720071315765381 4.720071315765381
Loss :  1.8355321884155273 4.425658226013184 4.425658226013184
Loss :  1.8078731298446655 4.490577220916748 4.490577220916748
Loss :  1.8293097019195557 3.896794080734253 3.896794080734253
Loss :  1.7892473936080933 4.2112298011779785 4.2112298011779785
Loss :  1.8031249046325684 3.6449716091156006 3.6449716091156006
Loss :  1.8381255865097046 3.65415358543396 3.65415358543396
Loss :  1.7815401554107666 4.439175605773926 4.439175605773926
Loss :  1.8220751285552979 4.520350456237793 4.520350456237793
Loss :  1.7676831483840942 4.536243915557861 4.536243915557861
Loss :  1.7839325666427612 4.335992813110352 4.335992813110352
Loss :  1.7909117937088013 4.4837870597839355 4.4837870597839355
Loss :  1.8151894807815552 4.416496753692627 4.416496753692627
Loss :  1.8321994543075562 4.434229373931885 4.434229373931885
Loss :  1.8205175399780273 4.489043712615967 4.489043712615967
Loss :  1.8352807760238647 4.25772762298584 4.25772762298584
Loss :  1.727396845817566 4.434366703033447 4.434366703033447
Loss :  1.8633248805999756 4.420176982879639 4.420176982879639
  batch 40 loss: 1.8633248805999756, 4.420176982879639, 4.420176982879639
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.886189579963684 4.619681358337402 4.619681358337402
Loss :  1.8649837970733643 4.484822750091553 4.484822750091553
Loss :  1.8078864812850952 4.518072605133057 4.518072605133057
Loss :  1.8048646450042725 4.6347126960754395 4.6347126960754395
Loss :  1.8309919834136963 4.0414862632751465 4.0414862632751465
Loss :  1.7857306003570557 4.388054370880127 4.388054370880127
Loss :  1.7628955841064453 4.119368076324463 4.119368076324463
Loss :  1.8065392971038818 4.325998783111572 4.325998783111572
Loss :  1.7668124437332153 4.074670314788818 4.074670314788818
Loss :  1.7873598337173462 4.038254261016846 4.038254261016846
Loss :  1.7994167804718018 4.157088279724121 4.157088279724121
Loss :  1.7790418863296509 4.2736077308654785 4.2736077308654785
Loss :  1.7544447183609009 4.346972942352295 4.346972942352295
Loss :  1.6935619115829468 4.535444736480713 4.535444736480713
Loss :  1.7560983896255493 4.408431529998779 4.408431529998779
Loss :  1.7662138938903809 4.4689507484436035 4.4689507484436035
Loss :  1.702945590019226 4.355816841125488 4.355816841125488
Loss :  1.7242971658706665 4.2804179191589355 4.2804179191589355
Loss :  1.7063454389572144 4.344864368438721 4.344864368438721
Loss :  1.6740496158599854 4.361446857452393 4.361446857452393
  batch 60 loss: 1.6740496158599854, 4.361446857452393, 4.361446857452393
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7174415588378906 3.9039816856384277 3.9039816856384277
Loss :  1.7512797117233276 4.230830192565918 4.230830192565918
Loss :  1.7761914730072021 4.2679762840271 4.2679762840271
Loss :  1.7601228952407837 4.185788631439209 4.185788631439209
Loss :  1.748687982559204 3.8122477531433105 3.8122477531433105
Loss :  6.576549053192139 4.346264839172363 4.346264839172363
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 5], device='cuda:0')
Loss :  6.353050708770752 4.445509433746338 4.445509433746338
Loss :  6.535887241363525 4.320114612579346 4.320114612579346
Loss :  6.1863837242126465 4.3958353996276855 4.3958353996276855
Total LOSS train 4.15755437704233 valid 4.376931071281433
CE LOSS train 1.842895196034358 valid 1.5465959310531616
Contrastive LOSS train 4.15755437704233 valid 1.0989588499069214
EPOCH 30:
Loss :  1.7609381675720215 3.63293194770813 3.63293194770813
Loss :  1.7565563917160034 5.025336742401123 5.025336742401123
Loss :  1.7489173412322998 4.349562168121338 4.349562168121338
Loss :  1.8215948343276978 4.77656888961792 4.77656888961792
Loss :  1.7668406963348389 4.4974822998046875 4.4974822998046875
Loss :  1.8085914850234985 4.409183025360107 4.409183025360107
Loss :  1.7754963636398315 4.593314170837402 4.593314170837402
Loss :  1.8107740879058838 4.370513439178467 4.370513439178467
Loss :  1.7939456701278687 4.316187858581543 4.316187858581543
Loss :  1.7700258493423462 3.9403717517852783 3.9403717517852783
Loss :  1.8037914037704468 4.036441326141357 4.036441326141357
Loss :  1.799594759941101 3.974957227706909 3.974957227706909
Loss :  1.8010939359664917 3.906873941421509 3.906873941421509
Loss :  1.8157988786697388 3.7265427112579346 3.7265427112579346
Loss :  1.79947829246521 3.8999814987182617 3.8999814987182617
Loss :  1.7637691497802734 4.35469913482666 4.35469913482666
Loss :  1.7479368448257446 3.957821846008301 3.957821846008301
Loss :  1.7239118814468384 4.232725620269775 4.232725620269775
Loss :  1.762734293937683 4.3532609939575195 4.3532609939575195
Loss :  1.7355670928955078 4.56120491027832 4.56120491027832
  batch 20 loss: 1.7355670928955078, 4.56120491027832, 4.56120491027832
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.798567295074463 4.325429439544678 4.325429439544678
Loss :  1.7547727823257446 4.542677402496338 4.542677402496338
Loss :  1.7494068145751953 4.473825931549072 4.473825931549072
Loss :  1.7594115734100342 4.466516971588135 4.466516971588135
Loss :  1.6936399936676025 4.631285667419434 4.631285667419434
Loss :  1.7575513124465942 4.4312424659729 4.4312424659729
Loss :  1.6833683252334595 4.524994373321533 4.524994373321533
Loss :  1.7284806966781616 4.396928310394287 4.396928310394287
Loss :  1.7247707843780518 4.275625228881836 4.275625228881836
Loss :  1.7517286539077759 4.48433780670166 4.48433780670166
Loss :  1.6812251806259155 4.520848274230957 4.520848274230957
Loss :  1.6955056190490723 4.783318996429443 4.783318996429443
Loss :  1.7360572814941406 4.447819232940674 4.447819232940674
Loss :  1.7291691303253174 4.334165573120117 4.334165573120117
Loss :  1.7022215127944946 4.319009304046631 4.319009304046631
Loss :  1.7092957496643066 4.526730537414551 4.526730537414551
Loss :  1.7190345525741577 4.515869140625 4.515869140625
Loss :  1.7046571969985962 4.389857292175293 4.389857292175293
Loss :  1.6930873394012451 4.46040678024292 4.46040678024292
Loss :  1.6400648355484009 4.320620059967041 4.320620059967041
  batch 40 loss: 1.6400648355484009, 4.320620059967041, 4.320620059967041
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 5], device='cuda:0')
Loss :  1.6821997165679932 4.134825229644775 4.134825229644775
Loss :  1.6925679445266724 4.362712383270264 4.362712383270264
Loss :  1.7105231285095215 4.358243942260742 4.358243942260742
Loss :  1.637560248374939 4.690464496612549 4.690464496612549
Loss :  1.6699957847595215 4.232240676879883 4.232240676879883
Loss :  1.6338914632797241 4.273431777954102 4.273431777954102
Loss :  1.6622982025146484 4.227624416351318 4.227624416351318
Loss :  1.6673094034194946 4.119594573974609 4.119594573974609
Loss :  1.6622875928878784 3.9425013065338135 3.9425013065338135
Loss :  1.6574591398239136 4.412927627563477 4.412927627563477
Loss :  1.6478323936462402 4.513792037963867 4.513792037963867
Loss :  1.6735236644744873 4.37923526763916 4.37923526763916
Loss :  1.6512879133224487 4.361771583557129 4.361771583557129
Loss :  1.6527354717254639 4.4342732429504395 4.4342732429504395
Loss :  1.6854642629623413 4.405674934387207 4.405674934387207
Loss :  1.6552186012268066 4.360185146331787 4.360185146331787
Loss :  1.5875086784362793 4.405722141265869 4.405722141265869
Loss :  1.6731103658676147 4.528092861175537 4.528092861175537
Loss :  1.6615678071975708 4.4292731285095215 4.4292731285095215
Loss :  1.64571213722229 4.310593128204346 4.310593128204346
  batch 60 loss: 1.64571213722229, 4.310593128204346, 4.310593128204346
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6170108318328857 4.362672328948975 4.362672328948975
Loss :  1.6524137258529663 4.190622329711914 4.190622329711914
Loss :  1.6875003576278687 4.20869779586792 4.20869779586792
Loss :  1.6572728157043457 4.388067722320557 4.388067722320557
Loss :  1.62368905544281 4.0992889404296875 4.0992889404296875
Loss :  2.4444947242736816 4.444429397583008 4.444429397583008
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3], device='cuda:0')
Loss :  2.4514222145080566 4.324615001678467 4.324615001678467
Loss :  2.52548885345459 4.453438758850098 4.453438758850098
Loss :  2.40914249420166 4.1768479347229 4.1768479347229
Total LOSS train 4.3418461432823765 valid 4.349832773208618
CE LOSS train 1.7142663808969352 valid 0.602285623550415
Contrastive LOSS train 4.3418461432823765 valid 1.044211983680725
EPOCH 31:
Loss :  1.6526161432266235 4.324858665466309 4.324858665466309
Loss :  1.664089560508728 4.345078468322754 4.345078468322754
Loss :  1.6525051593780518 4.644524574279785 4.644524574279785
Loss :  1.6810027360916138 4.587301254272461 4.587301254272461
Loss :  1.6606476306915283 4.3084797859191895 4.3084797859191895
Loss :  1.6704877614974976 4.3229265213012695 4.3229265213012695
Loss :  1.7125248908996582 4.521065711975098 4.521065711975098
Loss :  1.6810524463653564 4.412649631500244 4.412649631500244
Loss :  1.6826204061508179 4.415651798248291 4.415651798248291
Loss :  1.6608279943466187 4.2646284103393555 4.2646284103393555
Loss :  1.645333170890808 4.5425496101379395 4.5425496101379395
Loss :  1.6970919370651245 4.533459186553955 4.533459186553955
Loss :  1.6827353239059448 4.558487415313721 4.558487415313721
Loss :  1.786514401435852 4.5491790771484375 4.5491790771484375
Loss :  1.7299201488494873 4.4229559898376465 4.4229559898376465
Loss :  1.71670663356781 4.545454502105713 4.545454502105713
Loss :  1.7466211318969727 4.582099914550781 4.582099914550781
Loss :  1.735859751701355 4.387925624847412 4.387925624847412
Loss :  1.659895896911621 4.381479740142822 4.381479740142822
Loss :  1.736939787864685 4.487412452697754 4.487412452697754
  batch 20 loss: 1.736939787864685, 4.487412452697754, 4.487412452697754
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8142942190170288 4.660748481750488 4.660748481750488
Loss :  1.7005376815795898 4.453268527984619 4.453268527984619
Loss :  1.7594425678253174 4.4322686195373535 4.4322686195373535
Loss :  1.7961022853851318 4.713888645172119 4.713888645172119
Loss :  1.691663384437561 4.625936985015869 4.625936985015869
Loss :  1.7176810503005981 4.520791530609131 4.520791530609131
Loss :  1.6709356307983398 4.623350620269775 4.623350620269775
Loss :  1.6952818632125854 4.446683406829834 4.446683406829834
Loss :  1.6981605291366577 4.377540111541748 4.377540111541748
Loss :  1.711396336555481 4.541079998016357 4.541079998016357
Loss :  1.7084826231002808 4.800809860229492 4.800809860229492
Loss :  1.7241122722625732 4.62818717956543 4.62818717956543
Loss :  1.700473666191101 4.495162487030029 4.495162487030029
Loss :  1.7277108430862427 4.569382667541504 4.569382667541504
Loss :  1.745714545249939 4.555212020874023 4.555212020874023
Loss :  1.7475652694702148 4.565582752227783 4.565582752227783
Loss :  1.7335258722305298 4.4190592765808105 4.4190592765808105
Loss :  1.7743620872497559 4.357848167419434 4.357848167419434
Loss :  1.7614387273788452 4.397356033325195 4.397356033325195
Loss :  1.7093809843063354 4.535547733306885 4.535547733306885
  batch 40 loss: 1.7093809843063354, 4.535547733306885, 4.535547733306885
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4], device='cuda:0')
Loss :  1.7201586961746216 4.428192138671875 4.428192138671875
Loss :  1.726823091506958 4.422592639923096 4.422592639923096
Loss :  1.7650110721588135 4.448979377746582 4.448979377746582
Loss :  1.7017805576324463 4.537869930267334 4.537869930267334
Loss :  1.7429653406143188 4.474201679229736 4.474201679229736
Loss :  1.762524127960205 4.544079780578613 4.544079780578613
Loss :  1.6941087245941162 4.45561408996582 4.45561408996582
Loss :  1.7712903022766113 4.477845191955566 4.477845191955566
Loss :  1.728616714477539 4.394060134887695 4.394060134887695
Loss :  1.7293143272399902 4.674813270568848 4.674813270568848
Loss :  1.786969542503357 4.732841491699219 4.732841491699219
Loss :  1.7527868747711182 4.344564437866211 4.344564437866211
Loss :  1.7735975980758667 4.533638954162598 4.533638954162598
Loss :  1.751723289489746 4.376496315002441 4.376496315002441
Loss :  1.7759003639221191 4.428025722503662 4.428025722503662
Loss :  1.7881245613098145 4.481755256652832 4.481755256652832
Loss :  1.7563551664352417 4.438986778259277 4.438986778259277
Loss :  1.773248314857483 4.2811455726623535 4.2811455726623535
Loss :  1.7605282068252563 4.543661594390869 4.543661594390869
Loss :  1.7342100143432617 4.422018051147461 4.422018051147461
  batch 60 loss: 1.7342100143432617, 4.422018051147461, 4.422018051147461
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7419806718826294 4.533022403717041 4.533022403717041
Loss :  1.7899130582809448 4.428608417510986 4.428608417510986
Loss :  1.746087908744812 4.544195175170898 4.544195175170898
Loss :  1.742213249206543 4.469583511352539 4.469583511352539
Loss :  1.7208794355392456 4.206964492797852 4.206964492797852
Loss :  1.6058589220046997 4.3967790603637695 4.3967790603637695
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
Loss :  1.6160719394683838 4.462230205535889 4.462230205535889
Loss :  1.6013288497924805 4.290232181549072 4.290232181549072
Loss :  1.5554124116897583 4.243015766143799 4.243015766143799
Total LOSS train 4.484332766899696 valid 4.348064303398132
CE LOSS train 1.7258671778898973 valid 0.3888531029224396
Contrastive LOSS train 4.484332766899696 valid 1.0607539415359497
EPOCH 32:
Loss :  1.7406882047653198 4.230884075164795 4.230884075164795
Loss :  1.7235158681869507 4.597360134124756 4.597360134124756
Loss :  1.736728549003601 4.182956695556641 4.182956695556641
Loss :  1.7356947660446167 4.469603061676025 4.469603061676025
Loss :  1.7451754808425903 4.537049293518066 4.537049293518066
Loss :  1.7580825090408325 4.35370397567749 4.35370397567749
Loss :  1.7641202211380005 4.4945573806762695 4.4945573806762695
Loss :  1.721932291984558 4.167160511016846 4.167160511016846
Loss :  1.7344496250152588 4.563370704650879 4.563370704650879
Loss :  1.7340142726898193 4.2689056396484375 4.2689056396484375
Loss :  1.7608966827392578 4.5156474113464355 4.5156474113464355
Loss :  1.7568018436431885 4.409458637237549 4.409458637237549
Loss :  1.7801060676574707 4.508503437042236 4.508503437042236
Loss :  1.797957181930542 4.526370048522949 4.526370048522949
Loss :  1.767447829246521 4.379443645477295 4.379443645477295
Loss :  1.7672004699707031 4.478992938995361 4.478992938995361
Loss :  1.7908694744110107 4.360945701599121 4.360945701599121
Loss :  1.7422759532928467 4.496349811553955 4.496349811553955
Loss :  1.7608859539031982 4.420324802398682 4.420324802398682
Loss :  1.7499464750289917 4.545981407165527 4.545981407165527
  batch 20 loss: 1.7499464750289917, 4.545981407165527, 4.545981407165527
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3, 4], device='cuda:0')
Loss :  1.808976650238037 4.289198398590088 4.289198398590088
Loss :  1.7340481281280518 4.3962297439575195 4.3962297439575195
Loss :  1.7354882955551147 4.574881553649902 4.574881553649902
Loss :  1.7581219673156738 4.484616279602051 4.484616279602051
Loss :  1.7777906656265259 4.590811252593994 4.590811252593994
Loss :  1.7448930740356445 4.369663715362549 4.369663715362549
Loss :  1.7314094305038452 4.648759365081787 4.648759365081787
Loss :  1.7341516017913818 4.2821245193481445 4.2821245193481445
Loss :  1.753350853919983 4.2863264083862305 4.2863264083862305
Loss :  1.7466437816619873 4.481104373931885 4.481104373931885
Loss :  1.7664849758148193 4.46046257019043 4.46046257019043
Loss :  1.8306986093521118 4.829402446746826 4.829402446746826
Loss :  1.8129655122756958 4.433746814727783 4.433746814727783
Loss :  1.8603399991989136 4.4773335456848145 4.4773335456848145
Loss :  1.8247660398483276 4.405474662780762 4.405474662780762
Loss :  1.8469942808151245 4.465163230895996 4.465163230895996
Loss :  1.85625159740448 4.381970405578613 4.381970405578613
Loss :  1.8232614994049072 4.33971643447876 4.33971643447876
Loss :  1.851623296737671 4.447745323181152 4.447745323181152
Loss :  1.8545565605163574 4.478055953979492 4.478055953979492
  batch 40 loss: 1.8545565605163574, 4.478055953979492, 4.478055953979492
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3, 4], device='cuda:0')
Loss :  1.8358242511749268 4.42050313949585 4.42050313949585
Loss :  1.8415533304214478 4.427995681762695 4.427995681762695
Loss :  1.8434672355651855 4.489594459533691 4.489594459533691
Loss :  1.8131393194198608 4.46769380569458 4.46769380569458
Loss :  1.8543497323989868 4.331350803375244 4.331350803375244
Loss :  1.8472564220428467 4.533658027648926 4.533658027648926
Loss :  1.8465800285339355 4.538064956665039 4.538064956665039
Loss :  1.8241080045700073 4.509122371673584 4.509122371673584
Loss :  1.8486711978912354 4.36171293258667 4.36171293258667
Loss :  1.8473142385482788 4.622166156768799 4.622166156768799
Loss :  1.828460931777954 4.548744201660156 4.548744201660156
Loss :  1.8573265075683594 4.415933132171631 4.415933132171631
Loss :  1.8326530456542969 4.398446559906006 4.398446559906006
Loss :  1.8623868227005005 4.476743698120117 4.476743698120117
Loss :  1.850317358970642 4.557049751281738 4.557049751281738
Loss :  1.874475121498108 4.433349132537842 4.433349132537842
Loss :  1.8365806341171265 4.601560592651367 4.601560592651367
Loss :  1.8816776275634766 4.501784801483154 4.501784801483154
Loss :  1.8665781021118164 4.641149044036865 4.641149044036865
Loss :  1.8428828716278076 4.4073381423950195 4.4073381423950195
  batch 60 loss: 1.8428828716278076, 4.4073381423950195, 4.4073381423950195
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3, 4], device='cuda:0')
Loss :  1.8245090246200562 4.574573993682861 4.574573993682861
Loss :  1.76202392578125 4.281300067901611 4.281300067901611
Loss :  1.7689778804779053 4.260646343231201 4.260646343231201
Loss :  1.7369604110717773 4.38757848739624 4.38757848739624
Loss :  1.7600172758102417 4.1614670753479 4.1614670753479
Loss :  1.695909857749939 4.44971227645874 4.44971227645874
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1], device='cuda:0')
Loss :  1.7065900564193726 4.369822978973389 4.369822978973389
Loss :  1.71322762966156 4.379029273986816 4.379029273986816
Loss :  1.669323205947876 4.326629638671875 4.326629638671875
Total LOSS train 4.44584436416626 valid 4.381298542022705
CE LOSS train 1.7955338129630456 valid 0.417330801486969
Contrastive LOSS train 4.44584436416626 valid 1.0816574096679688
EPOCH 33:
Loss :  1.739017128944397 4.228058815002441 4.228058815002441
Loss :  1.7354421615600586 4.4672346115112305 4.4672346115112305
Loss :  1.8093035221099854 4.320303440093994 4.320303440093994
Loss :  1.7894856929779053 4.48953104019165 4.48953104019165
Loss :  1.8040379285812378 4.298605442047119 4.298605442047119
Loss :  1.8016048669815063 4.3460235595703125 4.3460235595703125
Loss :  1.7966352701187134 4.510115146636963 4.510115146636963
Loss :  1.8105571269989014 4.063865661621094 4.063865661621094
Loss :  1.7786502838134766 4.458800792694092 4.458800792694092
Loss :  1.7365487813949585 4.303981304168701 4.303981304168701
Loss :  1.7353614568710327 4.621414661407471 4.621414661407471
Loss :  1.7505770921707153 4.4340925216674805 4.4340925216674805
Loss :  1.7861427068710327 4.429282188415527 4.429282188415527
Loss :  1.7732608318328857 4.500500679016113 4.500500679016113
Loss :  1.750814437866211 4.4529709815979 4.4529709815979
Loss :  1.7802906036376953 4.54706335067749 4.54706335067749
Loss :  1.7179487943649292 4.521643161773682 4.521643161773682
Loss :  1.7681379318237305 4.243432998657227 4.243432998657227
Loss :  1.8097857236862183 3.9135663509368896 3.9135663509368896
Loss :  1.8328133821487427 4.203553676605225 4.203553676605225
  batch 20 loss: 1.8328133821487427, 4.203553676605225, 4.203553676605225
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
Loss :  1.8339173793792725 4.256507873535156 4.256507873535156
Loss :  1.7986007928848267 4.234856128692627 4.234856128692627
Loss :  1.7716608047485352 4.12442684173584 4.12442684173584
Loss :  1.7860521078109741 4.1057209968566895 4.1057209968566895
Loss :  1.7821404933929443 4.423916339874268 4.423916339874268
Loss :  1.762520432472229 4.33005428314209 4.33005428314209
Loss :  1.738128662109375 4.53926420211792 4.53926420211792
Loss :  1.8091330528259277 4.46245002746582 4.46245002746582
Loss :  1.7315489053726196 4.464211940765381 4.464211940765381
Loss :  1.7910051345825195 4.293192386627197 4.293192386627197
Loss :  1.7457674741744995 4.460028648376465 4.460028648376465
Loss :  1.822652816772461 4.594315528869629 4.594315528869629
Loss :  1.7468724250793457 4.413330554962158 4.413330554962158
Loss :  1.7077184915542603 4.550573348999023 4.550573348999023
Loss :  1.7285720109939575 4.46548318862915 4.46548318862915
Loss :  1.7011865377426147 4.511361122131348 4.511361122131348
Loss :  1.7348830699920654 4.417773723602295 4.417773723602295
Loss :  1.7485324144363403 4.49385404586792 4.49385404586792
Loss :  1.7533760070800781 4.567494869232178 4.567494869232178
Loss :  1.7573124170303345 4.483651638031006 4.483651638031006
  batch 40 loss: 1.7573124170303345, 4.483651638031006, 4.483651638031006
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
Loss :  1.7462517023086548 4.436003684997559 4.436003684997559
Loss :  1.757454514503479 4.45998477935791 4.45998477935791
Loss :  1.7615976333618164 4.411256313323975 4.411256313323975
Loss :  1.8371813297271729 4.5185041427612305 4.5185041427612305
Loss :  1.7601336240768433 4.385617256164551 4.385617256164551
Loss :  1.7583980560302734 4.34507417678833 4.34507417678833
Loss :  1.7929754257202148 4.587396621704102 4.587396621704102
Loss :  1.8111029863357544 4.483691692352295 4.483691692352295
Loss :  1.8409368991851807 4.386950492858887 4.386950492858887
Loss :  1.706722617149353 4.482644081115723 4.482644081115723
Loss :  1.8042817115783691 4.555168151855469 4.555168151855469
Loss :  1.7828017473220825 4.434097766876221 4.434097766876221
Loss :  1.7547130584716797 4.438772678375244 4.438772678375244
Loss :  1.714404582977295 4.452242374420166 4.452242374420166
Loss :  1.8054640293121338 4.4964823722839355 4.4964823722839355
Loss :  1.7106049060821533 4.446325302124023 4.446325302124023
Loss :  1.6674814224243164 4.525907039642334 4.525907039642334
Loss :  1.6737931966781616 4.444779872894287 4.444779872894287
Loss :  1.718276858329773 4.63015604019165 4.63015604019165
Loss :  1.8097813129425049 4.378902912139893 4.378902912139893
  batch 60 loss: 1.8097813129425049, 4.378902912139893, 4.378902912139893
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3], device='cuda:0')
Loss :  1.75101900100708 4.40460205078125 4.40460205078125
Loss :  1.7437559366226196 4.421574115753174 4.421574115753174
Loss :  1.7147197723388672 4.457249164581299 4.457249164581299
Loss :  1.7015968561172485 4.388829231262207 4.388829231262207
Loss :  1.7332894802093506 4.281899929046631 4.281899929046631
Loss :  1.7931182384490967 4.473357200622559 4.473357200622559
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3], device='cuda:0')
Loss :  1.7947008609771729 4.374589443206787 4.374589443206787
Loss :  1.781196117401123 4.331696510314941 4.331696510314941
Loss :  1.866670846939087 4.198003768920898 4.198003768920898
Total LOSS train 4.412317235653217 valid 4.344411730766296
CE LOSS train 1.7633343971692599 valid 0.46666771173477173
Contrastive LOSS train 4.412317235653217 valid 1.0495009422302246
EPOCH 34:
Loss :  1.7940818071365356 4.683091163635254 4.683091163635254
Loss :  1.755068302154541 4.495859622955322 4.495859622955322
Loss :  1.7176960706710815 4.361009120941162 4.361009120941162
Loss :  1.7177850008010864 4.513429641723633 4.513429641723633
Loss :  1.7057746648788452 4.328704833984375 4.328704833984375
Loss :  1.7438181638717651 4.318453311920166 4.318453311920166
Loss :  1.7735518217086792 4.458073616027832 4.458073616027832
Loss :  1.7316077947616577 4.369446754455566 4.369446754455566
Loss :  1.7854095697402954 4.341359615325928 4.341359615325928
Loss :  1.7291558980941772 4.286564350128174 4.286564350128174
Loss :  1.724593162536621 4.623066425323486 4.623066425323486
Loss :  1.6758369207382202 4.487867832183838 4.487867832183838
Loss :  1.6938694715499878 4.466344833374023 4.466344833374023
Loss :  1.7150834798812866 4.551191806793213 4.551191806793213
Loss :  1.779968023300171 4.314742565155029 4.314742565155029
Loss :  1.8245707750320435 4.429328441619873 4.429328441619873
Loss :  1.6605088710784912 4.446958065032959 4.446958065032959
Loss :  1.7385129928588867 4.485520839691162 4.485520839691162
Loss :  1.702681303024292 4.3762006759643555 4.3762006759643555
Loss :  1.7445704936981201 4.438113689422607 4.438113689422607
  batch 20 loss: 1.7445704936981201, 4.438113689422607, 4.438113689422607
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.6866340637207031 4.287599563598633 4.287599563598633
Loss :  1.68263578414917 4.586395740509033 4.586395740509033
Loss :  1.7662874460220337 4.698546409606934 4.698546409606934
Loss :  1.6616308689117432 4.436452865600586 4.436452865600586
Loss :  1.8706578016281128 4.6386237144470215 4.6386237144470215
Loss :  1.6856471300125122 4.394570827484131 4.394570827484131
Loss :  1.7383989095687866 4.564507484436035 4.564507484436035
Loss :  1.8178895711898804 4.484587669372559 4.484587669372559
Loss :  1.6846914291381836 4.408348083496094 4.408348083496094
Loss :  1.8404592275619507 4.431414604187012 4.431414604187012
Loss :  1.8188371658325195 4.562489986419678 4.562489986419678
Loss :  1.749890685081482 4.543964862823486 4.543964862823486
Loss :  1.8742555379867554 4.4286041259765625 4.4286041259765625
Loss :  1.726469874382019 4.561953544616699 4.561953544616699
Loss :  1.726879358291626 4.524189472198486 4.524189472198486
Loss :  1.7658473253250122 4.5174736976623535 4.5174736976623535
Loss :  1.7908892631530762 4.461030960083008 4.461030960083008
Loss :  1.9144505262374878 4.34366512298584 4.34366512298584
Loss :  1.8301537036895752 4.34301233291626 4.34301233291626
Loss :  1.9072588682174683 4.373402118682861 4.373402118682861
  batch 40 loss: 1.9072588682174683, 4.373402118682861, 4.373402118682861
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  1.9689538478851318 4.519110202789307 4.519110202789307
Loss :  1.9359328746795654 4.392374038696289 4.392374038696289
Loss :  1.8888828754425049 4.524125099182129 4.524125099182129
Loss :  1.8943783044815063 4.530843257904053 4.530843257904053
Loss :  1.8637367486953735 4.524377822875977 4.524377822875977
Loss :  1.8846790790557861 4.341035842895508 4.341035842895508
Loss :  1.8075188398361206 4.324125289916992 4.324125289916992
Loss :  1.8632718324661255 4.896306991577148 4.896306991577148
Loss :  1.9233741760253906 4.358677387237549 4.358677387237549
Loss :  2.0232484340667725 4.668481826782227 4.668481826782227
Loss :  1.9313055276870728 4.540961742401123 4.540961742401123
Loss :  2.0220961570739746 4.311731338500977 4.311731338500977
Loss :  2.0072286128997803 4.539873123168945 4.539873123168945
Loss :  1.861550211906433 4.550375461578369 4.550375461578369
Loss :  1.9182190895080566 4.607229232788086 4.607229232788086
Loss :  1.9407323598861694 4.366833209991455 4.366833209991455
Loss :  2.044226884841919 4.505755424499512 4.505755424499512
Loss :  1.9815343618392944 4.529080867767334 4.529080867767334
Loss :  1.9457350969314575 4.635699272155762 4.635699272155762
Loss :  2.031005620956421 4.490177631378174 4.490177631378174
  batch 60 loss: 2.031005620956421, 4.490177631378174, 4.490177631378174
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.9504421949386597 4.458726406097412 4.458726406097412
Loss :  2.0059173107147217 4.51177453994751 4.51177453994751
Loss :  2.007211923599243 4.365908622741699 4.365908622741699
Loss :  1.9152979850769043 4.43967342376709 4.43967342376709
Loss :  1.9547628164291382 4.2924885749816895 4.2924885749816895
Loss :  1.5843888521194458 4.39206600189209 4.39206600189209
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.678919792175293 4.466455459594727 4.466455459594727
Loss :  1.6376991271972656 4.3646321296691895 4.3646321296691895
Loss :  1.7822918891906738 4.144499778747559 4.144499778747559
Total LOSS train 4.470644723452055 valid 4.341913342475891
CE LOSS train 1.8280808045313908 valid 0.44557297229766846
Contrastive LOSS train 4.470644723452055 valid 1.0361249446868896
EPOCH 35:
Loss :  2.0430707931518555 4.324471950531006 4.324471950531006
Loss :  2.144432783126831 4.515593528747559 4.515593528747559
Loss :  1.899835228919983 4.389418125152588 4.389418125152588
Loss :  1.9013596773147583 4.638347148895264 4.638347148895264
Loss :  1.8918464183807373 4.725814342498779 4.725814342498779
Loss :  1.9320722818374634 4.4000725746154785 4.4000725746154785
Loss :  1.9438636302947998 4.713776588439941 4.713776588439941
Loss :  1.9166470766067505 4.430860996246338 4.430860996246338
Loss :  1.94511878490448 4.584833145141602 4.584833145141602
Loss :  2.013404130935669 4.622034072875977 4.622034072875977
Loss :  1.8950278759002686 4.471676826477051 4.471676826477051
Loss :  1.9745122194290161 4.550394058227539 4.550394058227539
Loss :  2.032636880874634 4.447659492492676 4.447659492492676
Loss :  1.9542118310928345 4.5807108879089355 4.5807108879089355
Loss :  1.8797070980072021 4.329599857330322 4.329599857330322
Loss :  1.9940204620361328 4.491705894470215 4.491705894470215
Loss :  1.8432905673980713 4.49708890914917 4.49708890914917
Loss :  1.9664225578308105 4.408470630645752 4.408470630645752
Loss :  1.9698864221572876 4.409195423126221 4.409195423126221
Loss :  2.0444912910461426 4.406792163848877 4.406792163848877
  batch 20 loss: 2.0444912910461426, 4.406792163848877, 4.406792163848877
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
Loss :  1.8785638809204102 4.358880519866943 4.358880519866943
Loss :  1.8695287704467773 4.606301784515381 4.606301784515381
Loss :  1.8698854446411133 4.6635026931762695 4.6635026931762695
Loss :  1.8845837116241455 4.483883380889893 4.483883380889893
Loss :  1.8783762454986572 4.663240432739258 4.663240432739258
Loss :  1.9936741590499878 4.367225646972656 4.367225646972656
Loss :  1.7951219081878662 4.713089942932129 4.713089942932129
Loss :  1.878290057182312 4.361085414886475 4.361085414886475
Loss :  1.9854754209518433 4.466552257537842 4.466552257537842
Loss :  1.9046112298965454 4.482147693634033 4.482147693634033
Loss :  1.988338828086853 4.510790824890137 4.510790824890137
Loss :  1.957155704498291 4.555605888366699 4.555605888366699
Loss :  1.9621130228042603 4.387290954589844 4.387290954589844
Loss :  2.0260517597198486 4.533881187438965 4.533881187438965
Loss :  1.9493621587753296 4.510519027709961 4.510519027709961
Loss :  1.9418461322784424 4.523626804351807 4.523626804351807
Loss :  2.009535312652588 4.490752220153809 4.490752220153809
Loss :  1.9918596744537354 4.6873602867126465 4.6873602867126465
Loss :  1.9874441623687744 4.4272027015686035 4.4272027015686035
Loss :  2.084709644317627 4.572146415710449 4.572146415710449
  batch 40 loss: 2.084709644317627, 4.572146415710449, 4.572146415710449
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  1.994946002960205 4.513683795928955 4.513683795928955
Loss :  1.9738348722457886 4.361336708068848 4.361336708068848
Loss :  1.9928737878799438 4.352271556854248 4.352271556854248
Loss :  1.9494600296020508 4.487410068511963 4.487410068511963
Loss :  2.0045783519744873 4.390439510345459 4.390439510345459
Loss :  2.028836250305176 4.408607482910156 4.408607482910156
Loss :  2.0714104175567627 4.46136999130249 4.46136999130249
Loss :  1.97616708278656 4.602713584899902 4.602713584899902
Loss :  1.9139899015426636 4.490455627441406 4.490455627441406
Loss :  1.985897421836853 4.546987533569336 4.546987533569336
Loss :  2.0190794467926025 4.556167125701904 4.556167125701904
Loss :  1.9979888200759888 4.4413862228393555 4.4413862228393555
Loss :  1.917905330657959 4.372887134552002 4.372887134552002
Loss :  2.003593683242798 4.494351863861084 4.494351863861084
Loss :  2.0298380851745605 4.672525405883789 4.672525405883789
Loss :  1.8355685472488403 4.3625335693359375 4.3625335693359375
Loss :  2.075345754623413 4.535128593444824 4.535128593444824
Loss :  2.0334415435791016 4.493949890136719 4.493949890136719
Loss :  2.033620595932007 4.881987571716309 4.881987571716309
Loss :  1.9483652114868164 4.66594934463501 4.66594934463501
  batch 60 loss: 1.9483652114868164, 4.66594934463501, 4.66594934463501
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
Loss :  1.9439893960952759 4.635684013366699 4.635684013366699
Loss :  1.9666392803192139 4.51586389541626 4.51586389541626
Loss :  2.0827300548553467 4.3618292808532715 4.3618292808532715
Loss :  2.06661319732666 4.5590362548828125 4.5590362548828125
Loss :  2.0219669342041016 4.148879528045654 4.148879528045654
Loss :  1.443528652191162 4.45308256149292 4.45308256149292
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.4860823154449463 4.407082557678223 4.407082557678223
Loss :  1.462032437324524 4.396080493927002 4.396080493927002
Loss :  1.5397541522979736 4.307516574859619 4.307516574859619
Total LOSS train 4.501800588461069 valid 4.390940546989441
CE LOSS train 1.9675548498447124 valid 0.3849385380744934
Contrastive LOSS train 4.501800588461069 valid 1.0768791437149048
EPOCH 36:
Loss :  2.112746238708496 4.459757328033447 4.459757328033447
Loss :  1.904214859008789 4.508466720581055 4.508466720581055
Loss :  2.0059516429901123 4.361741542816162 4.361741542816162
Loss :  2.033329725265503 4.695117473602295 4.695117473602295
Loss :  2.1096878051757812 4.39352560043335 4.39352560043335
Loss :  2.072101593017578 4.390717506408691 4.390717506408691
Loss :  1.9248214960098267 4.499766826629639 4.499766826629639
Loss :  1.9837852716445923 4.321562767028809 4.321562767028809
Loss :  2.0951220989227295 4.535573959350586 4.535573959350586
Loss :  1.969420075416565 4.590644359588623 4.590644359588623
Loss :  2.040726900100708 4.514061450958252 4.514061450958252
Loss :  2.0362110137939453 4.557748794555664 4.557748794555664
Loss :  2.0955491065979004 4.443894863128662 4.443894863128662
Loss :  2.130423069000244 4.7287116050720215 4.7287116050720215
Loss :  1.999361276626587 4.467973709106445 4.467973709106445
Loss :  1.9843677282333374 4.555150032043457 4.555150032043457
Loss :  2.075761318206787 4.462799072265625 4.462799072265625
Loss :  2.0319740772247314 4.303853988647461 4.303853988647461
Loss :  2.0716474056243896 4.581228256225586 4.581228256225586
Loss :  1.994718074798584 4.586246013641357 4.586246013641357
  batch 20 loss: 1.994718074798584, 4.586246013641357, 4.586246013641357
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  2.0577239990234375 4.426846027374268 4.426846027374268
Loss :  2.1098082065582275 4.576238632202148 4.576238632202148
Loss :  2.0880990028381348 4.620202541351318 4.620202541351318
Loss :  2.065669536590576 4.2866106033325195 4.2866106033325195
Loss :  2.0061097145080566 4.595273494720459 4.595273494720459
Loss :  2.000636100769043 4.226738929748535 4.226738929748535
Loss :  1.9616069793701172 4.409087181091309 4.409087181091309
Loss :  2.0247814655303955 4.048991680145264 4.048991680145264
Loss :  2.0934669971466064 4.450246334075928 4.450246334075928
Loss :  1.975785493850708 4.3951849937438965 4.3951849937438965
Loss :  2.1320323944091797 4.224537372589111 4.224537372589111
Loss :  2.0135746002197266 4.516241073608398 4.516241073608398
Loss :  1.9887349605560303 4.52640438079834 4.52640438079834
Loss :  2.0172407627105713 4.6423234939575195 4.6423234939575195
Loss :  2.0366594791412354 4.37253999710083 4.37253999710083
Loss :  1.9992390871047974 4.601806163787842 4.601806163787842
Loss :  2.055823564529419 4.586817264556885 4.586817264556885
Loss :  1.9867514371871948 4.56496524810791 4.56496524810791
Loss :  1.9576137065887451 4.40425968170166 4.40425968170166
Loss :  1.9531961679458618 4.284975051879883 4.284975051879883
  batch 40 loss: 1.9531961679458618, 4.284975051879883, 4.284975051879883
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.9985108375549316 4.470202445983887 4.470202445983887
Loss :  2.058018922805786 4.41612434387207 4.41612434387207
Loss :  2.100759983062744 4.381070613861084 4.381070613861084
Loss :  2.0562984943389893 4.483208656311035 4.483208656311035
Loss :  2.0729820728302 4.406047344207764 4.406047344207764
Loss :  2.0519838333129883 4.554333686828613 4.554333686828613
Loss :  1.904422402381897 4.415710926055908 4.415710926055908
Loss :  2.081505537033081 4.472373962402344 4.472373962402344
Loss :  1.9749733209609985 4.439474582672119 4.439474582672119
Loss :  2.018874168395996 4.50163459777832 4.50163459777832
Loss :  2.122152805328369 4.532281875610352 4.532281875610352
Loss :  1.9813696146011353 4.343827247619629 4.343827247619629
Loss :  2.0646982192993164 4.391366004943848 4.391366004943848
Loss :  1.9892139434814453 4.481241226196289 4.481241226196289
Loss :  2.1513373851776123 4.500943660736084 4.500943660736084
Loss :  1.9291926622390747 4.497255802154541 4.497255802154541
Loss :  2.0861306190490723 4.502691268920898 4.502691268920898
Loss :  2.136569023132324 4.515578269958496 4.515578269958496
Loss :  2.001490831375122 4.6322431564331055 4.6322431564331055
Loss :  1.9344232082366943 4.488968372344971 4.488968372344971
  batch 60 loss: 1.9344232082366943, 4.488968372344971, 4.488968372344971
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 4, 5], device='cuda:0')
Loss :  2.085946559906006 4.517096042633057 4.517096042633057
Loss :  2.10420823097229 4.406319618225098 4.406319618225098
Loss :  2.0069684982299805 4.467688083648682 4.467688083648682
Loss :  2.1080543994903564 4.420783519744873 4.420783519744873
Loss :  2.139491081237793 4.170988082885742 4.170988082885742
Loss :  1.994227409362793 4.416658401489258 4.416658401489258
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  2.074428081512451 4.42003870010376 4.42003870010376
Loss :  2.0395827293395996 4.333584308624268 4.333584308624268
Loss :  2.140927791595459 4.225998878479004 4.225998878479004
Total LOSS train 4.463512083200308 valid 4.349070072174072
CE LOSS train 2.036246939805838 valid 0.5352319478988647
Contrastive LOSS train 4.463512083200308 valid 1.056499719619751
EPOCH 37:
Loss :  2.127999782562256 4.341282844543457 4.341282844543457
Loss :  1.9978522062301636 4.540055751800537 4.540055751800537
Loss :  1.972462773323059 4.461433410644531 4.461433410644531
Loss :  2.0099968910217285 4.370999336242676 4.370999336242676
Loss :  2.161938190460205 4.3646697998046875 4.3646697998046875
Loss :  1.996934175491333 4.351315021514893 4.351315021514893
Loss :  2.107877254486084 4.554699420928955 4.554699420928955
Loss :  2.043287992477417 4.329628944396973 4.329628944396973
Loss :  2.133859157562256 4.292195796966553 4.292195796966553
Loss :  2.092214584350586 4.283049583435059 4.283049583435059
Loss :  2.08537220954895 4.54802131652832 4.54802131652832
Loss :  2.067901849746704 4.54601526260376 4.54601526260376
Loss :  2.0795397758483887 4.514223575592041 4.514223575592041
Loss :  2.0434556007385254 4.506841659545898 4.506841659545898
Loss :  1.996712565422058 4.239406585693359 4.239406585693359
Loss :  1.9173871278762817 4.349122524261475 4.349122524261475
Loss :  2.008148670196533 4.519813537597656 4.519813537597656
Loss :  1.9883273839950562 4.539820671081543 4.539820671081543
Loss :  2.103536605834961 4.456820964813232 4.456820964813232
Loss :  2.0201497077941895 4.488972187042236 4.488972187042236
  batch 20 loss: 2.0201497077941895, 4.488972187042236, 4.488972187042236
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  2.027123212814331 4.309072494506836 4.309072494506836
Loss :  2.1403329372406006 4.462325572967529 4.462325572967529
Loss :  2.0586459636688232 4.355061054229736 4.355061054229736
Loss :  2.0196852684020996 4.125628471374512 4.125628471374512
Loss :  1.9449806213378906 4.283552646636963 4.283552646636963
Loss :  2.0318450927734375 4.123411178588867 4.123411178588867
Loss :  1.9652413129806519 4.348451137542725 4.348451137542725
Loss :  2.114898443222046 4.354430198669434 4.354430198669434
Loss :  2.148780107498169 4.363326549530029 4.363326549530029
Loss :  2.08793044090271 4.302603721618652 4.302603721618652
Loss :  2.2253329753875732 4.561429500579834 4.561429500579834
Loss :  2.1673192977905273 4.5352044105529785 4.5352044105529785
Loss :  2.1725170612335205 4.494899272918701 4.494899272918701
Loss :  2.1700093746185303 4.38480281829834 4.38480281829834
Loss :  2.256047248840332 4.572592258453369 4.572592258453369
Loss :  2.2831063270568848 4.461366176605225 4.461366176605225
Loss :  2.2801833152770996 4.434767723083496 4.434767723083496
Loss :  2.152088165283203 4.394308090209961 4.394308090209961
Loss :  2.0853848457336426 4.424004077911377 4.424004077911377
Loss :  2.1011476516723633 4.458094120025635 4.458094120025635
  batch 40 loss: 2.1011476516723633, 4.458094120025635, 4.458094120025635
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 4, 5], device='cuda:0')
Loss :  2.2268271446228027 4.532464504241943 4.532464504241943
Loss :  2.2644448280334473 4.478548049926758 4.478548049926758
Loss :  2.2900009155273438 4.324367046356201 4.324367046356201
Loss :  2.2429258823394775 4.394742012023926 4.394742012023926
Loss :  2.255736827850342 4.3597002029418945 4.3597002029418945
Loss :  2.227682590484619 4.481342792510986 4.481342792510986
Loss :  2.1453869342803955 4.4877190589904785 4.4877190589904785
Loss :  2.25624942779541 4.435013771057129 4.435013771057129
Loss :  2.1207938194274902 4.436873912811279 4.436873912811279
Loss :  2.2072134017944336 4.542236328125 4.542236328125
Loss :  2.1701033115386963 4.572751998901367 4.572751998901367
Loss :  2.089872121810913 4.605705738067627 4.605705738067627
Loss :  2.1894547939300537 4.559341907501221 4.559341907501221
Loss :  2.0812249183654785 4.543696403503418 4.543696403503418
Loss :  2.2387630939483643 4.462120056152344 4.462120056152344
Loss :  1.978788137435913 4.410521507263184 4.410521507263184
Loss :  2.1269123554229736 4.376010417938232 4.376010417938232
Loss :  2.167898654937744 4.639665603637695 4.639665603637695
Loss :  2.1077985763549805 4.588604927062988 4.588604927062988
Loss :  1.972369909286499 4.392906665802002 4.392906665802002
  batch 60 loss: 1.972369909286499, 4.392906665802002, 4.392906665802002
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 5], device='cuda:0')
Loss :  2.1431937217712402 4.574920177459717 4.574920177459717
Loss :  2.1641476154327393 4.406611919403076 4.406611919403076
Loss :  2.1940670013427734 4.4265031814575195 4.4265031814575195
Loss :  2.2396905422210693 4.4146575927734375 4.4146575927734375
Loss :  2.2617390155792236 4.085688591003418 4.085688591003418
Loss :  1.9870166778564453 4.37469482421875 4.37469482421875
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1], device='cuda:0')
Loss :  1.9840648174285889 4.379270076751709 4.379270076751709
Loss :  2.0165231227874756 4.274327278137207 4.274327278137207
Loss :  1.8385049104690552 4.279818534851074 4.279818534851074
Total LOSS train 4.428929754403922 valid 4.327027678489685
CE LOSS train 2.1192436878497785 valid 0.4596262276172638
Contrastive LOSS train 4.428929754403922 valid 1.0699546337127686
EPOCH 38:
Loss :  2.114392042160034 4.260141849517822 4.260141849517822
Loss :  2.051494598388672 4.539017677307129 4.539017677307129
Loss :  2.0909717082977295 4.661680221557617 4.661680221557617
Loss :  2.1137609481811523 4.426011562347412 4.426011562347412
Loss :  2.014636993408203 4.290132522583008 4.290132522583008
Loss :  2.0606491565704346 4.416083812713623 4.416083812713623
Loss :  2.059887647628784 4.448710918426514 4.448710918426514
Loss :  2.116032361984253 4.3401312828063965 4.3401312828063965
Loss :  2.1582436561584473 4.452355861663818 4.452355861663818
Loss :  2.073093891143799 4.3987555503845215 4.3987555503845215
Loss :  2.108238458633423 4.530374526977539 4.530374526977539
Loss :  2.1596546173095703 4.485489368438721 4.485489368438721
Loss :  2.126926898956299 4.497235298156738 4.497235298156738
Loss :  2.1977627277374268 4.529407024383545 4.529407024383545
Loss :  2.080321788787842 4.5432353019714355 4.5432353019714355
Loss :  1.9366365671157837 4.701195240020752 4.701195240020752
Loss :  2.1054766178131104 4.589822292327881 4.589822292327881
Loss :  1.9993547201156616 4.875864505767822 4.875864505767822
Loss :  2.1455471515655518 4.678929328918457 4.678929328918457
Loss :  2.0337300300598145 4.5479326248168945 4.5479326248168945
  batch 20 loss: 2.0337300300598145, 4.5479326248168945, 4.5479326248168945
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 4, 5], device='cuda:0')
Loss :  2.1653599739074707 4.464291095733643 4.464291095733643
Loss :  2.161191463470459 4.44164514541626 4.44164514541626
Loss :  2.1583330631256104 4.529492378234863 4.529492378234863
Loss :  1.9654734134674072 4.475499629974365 4.475499629974365
Loss :  1.9602653980255127 4.619741916656494 4.619741916656494
Loss :  2.1174087524414062 4.3687424659729 4.3687424659729
Loss :  1.9909814596176147 4.740413188934326 4.740413188934326
Loss :  2.1505205631256104 4.3731889724731445 4.3731889724731445
Loss :  2.1667540073394775 4.422621726989746 4.422621726989746
Loss :  2.110370397567749 4.443251609802246 4.443251609802246
Loss :  2.1795830726623535 4.532601833343506 4.532601833343506
Loss :  2.1101810932159424 4.679960250854492 4.679960250854492
Loss :  2.1202261447906494 4.457430839538574 4.457430839538574
Loss :  2.152373790740967 4.4840779304504395 4.4840779304504395
Loss :  2.2019314765930176 4.507369041442871 4.507369041442871
Loss :  2.151930809020996 4.461178302764893 4.461178302764893
Loss :  2.161933422088623 4.422192573547363 4.422192573547363
Loss :  2.0514578819274902 4.398549556732178 4.398549556732178
Loss :  2.0140841007232666 4.435764789581299 4.435764789581299
Loss :  2.0244014263153076 4.37421178817749 4.37421178817749
  batch 40 loss: 2.0244014263153076, 4.37421178817749, 4.37421178817749
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 4], device='cuda:0')
Loss :  2.132827043533325 4.530754566192627 4.530754566192627
Loss :  2.1225080490112305 4.393194198608398 4.393194198608398
Loss :  2.1774678230285645 4.3848700523376465 4.3848700523376465
Loss :  2.0944149494171143 4.515851974487305 4.515851974487305
Loss :  2.1196303367614746 4.360921859741211 4.360921859741211
Loss :  1.9401882886886597 4.345372676849365 4.345372676849365
Loss :  1.9180289506912231 4.717343807220459 4.717343807220459
Loss :  2.086017370223999 4.450084209442139 4.450084209442139
Loss :  1.9201675653457642 4.735329627990723 4.735329627990723
Loss :  2.07722544670105 4.545485019683838 4.545485019683838
Loss :  2.050640344619751 4.608941555023193 4.608941555023193
Loss :  1.9485841989517212 4.465616703033447 4.465616703033447
Loss :  2.115619659423828 4.496906757354736 4.496906757354736
Loss :  1.9669075012207031 4.389843463897705 4.389843463897705
Loss :  2.0935065746307373 4.336857795715332 4.336857795715332
Loss :  1.8656576871871948 4.418395042419434 4.418395042419434
Loss :  1.9269635677337646 4.513866901397705 4.513866901397705
Loss :  2.037099599838257 4.6654253005981445 4.6654253005981445
Loss :  1.8614404201507568 4.5707807540893555 4.5707807540893555
Loss :  1.8454774618148804 4.386988162994385 4.386988162994385
  batch 60 loss: 1.8454774618148804, 4.386988162994385, 4.386988162994385
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.9801331758499146 4.4269328117370605 4.4269328117370605
Loss :  2.0672268867492676 4.527502059936523 4.527502059936523
Loss :  1.9950767755508423 4.355200290679932 4.355200290679932
Loss :  2.04841947555542 4.499060153961182 4.499060153961182
Loss :  2.015103578567505 4.022907257080078 4.022907257080078
Loss :  4.639438152313232 4.371213436126709 4.371213436126709
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 4], device='cuda:0')
Loss :  4.843132495880127 4.325125694274902 4.325125694274902
Loss :  4.654618740081787 4.290347099304199 4.290347099304199
Loss :  4.480931282043457 4.14634895324707 4.14634895324707
Total LOSS train 4.484756381695087 valid 4.28325879573822
CE LOSS train 2.0656601080527675 valid 1.1202328205108643
Contrastive LOSS train 4.484756381695087 valid 1.0365872383117676
Saved best model. Old loss 4.288440585136414 and new best loss 4.28325879573822
EPOCH 39:
Loss :  1.9627017974853516 4.274472713470459 4.274472713470459
Loss :  1.9654932022094727 4.596644878387451 4.596644878387451
Loss :  1.9666204452514648 4.398365497589111 4.398365497589111
Loss :  2.040205478668213 4.518646717071533 4.518646717071533
Loss :  1.867405652999878 4.36220121383667 4.36220121383667
Loss :  1.9574615955352783 4.334802150726318 4.334802150726318
Loss :  1.9375590085983276 4.445021152496338 4.445021152496338
Loss :  1.9431501626968384 4.652641773223877 4.652641773223877
Loss :  1.9849604368209839 4.375932693481445 4.375932693481445
Loss :  1.8902978897094727 4.382822036743164 4.382822036743164
Loss :  1.9669781923294067 4.451589107513428 4.451589107513428
Loss :  1.9704324007034302 4.462302207946777 4.462302207946777
Loss :  1.9638428688049316 4.476797103881836 4.476797103881836
Loss :  2.002614974975586 4.588846206665039 4.588846206665039
Loss :  2.059584617614746 4.4788899421691895 4.4788899421691895
Loss :  1.7809714078903198 4.3536295890808105 4.3536295890808105
Loss :  2.034827709197998 4.796945095062256 4.796945095062256
Loss :  1.927934169769287 5.158458232879639 5.158458232879639
Loss :  1.9798524379730225 4.729855537414551 4.729855537414551
Loss :  1.9390140771865845 4.786277770996094 4.786277770996094
  batch 20 loss: 1.9390140771865845, 4.786277770996094, 4.786277770996094
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.968259572982788 4.366261005401611 4.366261005401611
Loss :  1.9264835119247437 4.386176586151123 4.386176586151123
Loss :  1.9089469909667969 4.5549235343933105 4.5549235343933105
Loss :  1.901166558265686 4.396516799926758 4.396516799926758
Loss :  1.8296308517456055 4.422849178314209 4.422849178314209
Loss :  1.8728923797607422 4.666895866394043 4.666895866394043
Loss :  1.8909014463424683 4.603033065795898 4.603033065795898
Loss :  1.9635080099105835 4.294478416442871 4.294478416442871
Loss :  1.9030653238296509 4.3020219802856445 4.3020219802856445
Loss :  1.8437490463256836 4.383664608001709 4.383664608001709
Loss :  1.908413290977478 4.304141521453857 4.304141521453857
Loss :  1.884886622428894 4.680913925170898 4.680913925170898
Loss :  1.8460288047790527 4.567917346954346 4.567917346954346
Loss :  1.842410922050476 4.2600274085998535 4.2600274085998535
Loss :  1.910893440246582 4.763320446014404 4.763320446014404
Loss :  1.933504581451416 4.588014602661133 4.588014602661133
Loss :  1.95663321018219 4.503558158874512 4.503558158874512
Loss :  1.9260811805725098 4.354226589202881 4.354226589202881
Loss :  1.8995734453201294 4.399137496948242 4.399137496948242
Loss :  1.8526314496994019 4.577248573303223 4.577248573303223
  batch 40 loss: 1.8526314496994019, 4.577248573303223, 4.577248573303223
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.9403033256530762 4.541694164276123 4.541694164276123
Loss :  1.840307354927063 4.419142246246338 4.419142246246338
Loss :  2.02748966217041 4.401352882385254 4.401352882385254
Loss :  1.9155020713806152 4.459259510040283 4.459259510040283
Loss :  1.965173363685608 4.2601318359375 4.2601318359375
Loss :  1.8841331005096436 4.480200290679932 4.480200290679932
Loss :  1.8266535997390747 4.451735496520996 4.451735496520996
Loss :  1.926148533821106 4.499648094177246 4.499648094177246
Loss :  1.8109287023544312 4.4684224128723145 4.4684224128723145
Loss :  1.8351415395736694 4.595239639282227 4.595239639282227
Loss :  1.896440029144287 4.574514865875244 4.574514865875244
Loss :  1.898351788520813 4.4428558349609375 4.4428558349609375
Loss :  1.9074928760528564 4.557705402374268 4.557705402374268
Loss :  1.8355064392089844 4.387303829193115 4.387303829193115
Loss :  2.0003392696380615 4.4656982421875 4.4656982421875
Loss :  1.7835521697998047 4.35634708404541 4.35634708404541
Loss :  1.7765032052993774 4.564459323883057 4.564459323883057
Loss :  1.8792335987091064 4.533607482910156 4.533607482910156
Loss :  1.7183669805526733 4.5976643562316895 4.5976643562316895
Loss :  1.8168926239013672 4.362168788909912 4.362168788909912
  batch 60 loss: 1.8168926239013672, 4.362168788909912, 4.362168788909912
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.8161271810531616 4.537510871887207 4.537510871887207
Loss :  1.8952510356903076 4.43513822555542 4.43513822555542
Loss :  1.909957766532898 4.797464370727539 4.797464370727539
Loss :  1.85819673538208 4.364667892456055 4.364667892456055
Loss :  1.7965550422668457 4.152376174926758 4.152376174926758
Loss :  1.9169588088989258 4.3521318435668945 4.3521318435668945
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1], device='cuda:0')
Loss :  1.9079034328460693 4.512640953063965 4.512640953063965
Loss :  1.9307245016098022 4.376301288604736 4.376301288604736
Loss :  1.8167356252670288 4.23119592666626 4.23119592666626
Total LOSS train 4.487796585376446 valid 4.368067502975464
CE LOSS train 1.9057248794115507 valid 0.4541839063167572
Contrastive LOSS train 4.487796585376446 valid 1.057798981666565
EPOCH 40:
Loss :  1.8157325983047485 4.283488750457764 4.283488750457764
Loss :  1.7633490562438965 4.701823711395264 4.701823711395264
Loss :  1.756956934928894 4.38799524307251 4.38799524307251
Loss :  1.8222671747207642 4.683761119842529 4.683761119842529
Loss :  1.7672263383865356 4.3625383377075195 4.3625383377075195
Loss :  1.7869559526443481 4.423590183258057 4.423590183258057
Loss :  1.8280564546585083 4.642376899719238 4.642376899719238
Loss :  1.7698837518692017 4.413084983825684 4.413084983825684
Loss :  1.828879714012146 4.391509532928467 4.391509532928467
Loss :  1.763616919517517 4.312185287475586 4.312185287475586
Loss :  1.8323805332183838 4.401656627655029 4.401656627655029
Loss :  1.8181086778640747 4.481333255767822 4.481333255767822
Loss :  1.723539113998413 4.502903938293457 4.502903938293457
Loss :  1.77518630027771 4.60062313079834 4.60062313079834
Loss :  1.8004862070083618 4.466128349304199 4.466128349304199
Loss :  1.616166353225708 4.600564002990723 4.600564002990723
Loss :  1.7093372344970703 4.6607489585876465 4.6607489585876465
Loss :  1.717021107673645 4.4385666847229 4.4385666847229
Loss :  1.7441517114639282 4.3260345458984375 4.3260345458984375
Loss :  1.6832506656646729 4.367520809173584 4.367520809173584
  batch 20 loss: 1.6832506656646729, 4.367520809173584, 4.367520809173584
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.7516542673110962 4.293336391448975 4.293336391448975
Loss :  1.7581560611724854 4.491898536682129 4.491898536682129
Loss :  1.7974135875701904 4.577127933502197 4.577127933502197
Loss :  1.6841809749603271 4.445418834686279 4.445418834686279
Loss :  1.7212404012680054 4.541113376617432 4.541113376617432
Loss :  1.7809979915618896 4.376630783081055 4.376630783081055
Loss :  1.6427714824676514 4.731772422790527 4.731772422790527
Loss :  1.8009576797485352 4.20120906829834 4.20120906829834
Loss :  1.7830740213394165 4.341707229614258 4.341707229614258
Loss :  1.7915849685668945 4.491814136505127 4.491814136505127
Loss :  1.7735987901687622 4.6185455322265625 4.6185455322265625
Loss :  1.9054962396621704 4.661448001861572 4.661448001861572
Loss :  1.7678561210632324 4.3713297843933105 4.3713297843933105
Loss :  1.7884819507598877 4.4258880615234375 4.4258880615234375
Loss :  1.8313109874725342 4.72951602935791 4.72951602935791
Loss :  1.8202623128890991 4.50161075592041 4.50161075592041
Loss :  1.899506688117981 4.424612522125244 4.424612522125244
Loss :  1.8434171676635742 4.487792491912842 4.487792491912842
Loss :  1.7287688255310059 4.299543380737305 4.299543380737305
Loss :  1.9387856721878052 4.300803184509277 4.300803184509277
  batch 40 loss: 1.9387856721878052, 4.300803184509277, 4.300803184509277
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  2.0043087005615234 4.5482869148254395 4.5482869148254395
Loss :  1.9630519151687622 4.6894731521606445 4.6894731521606445
Loss :  1.9427510499954224 4.368714332580566 4.368714332580566
Loss :  1.8733632564544678 4.637268543243408 4.637268543243408
Loss :  1.7935357093811035 4.300240993499756 4.300240993499756
Loss :  1.7197579145431519 4.520627498626709 4.520627498626709
Loss :  1.8536527156829834 4.53642463684082 4.53642463684082
Loss :  1.9194767475128174 4.419289588928223 4.419289588928223
Loss :  1.9502465724945068 4.379263401031494 4.379263401031494
Loss :  1.8660160303115845 4.477447032928467 4.477447032928467
Loss :  1.8692359924316406 4.570387840270996 4.570387840270996
Loss :  1.8308569192886353 4.394436359405518 4.394436359405518
Loss :  1.763104796409607 4.573391914367676 4.573391914367676
Loss :  1.9379454851150513 4.47424840927124 4.47424840927124
Loss :  1.9076478481292725 4.52261209487915 4.52261209487915
Loss :  1.7481706142425537 4.457758903503418 4.457758903503418
Loss :  1.7633532285690308 4.549976825714111 4.549976825714111
Loss :  1.823913335800171 4.4484477043151855 4.4484477043151855
Loss :  1.7352068424224854 4.569842338562012 4.569842338562012
Loss :  1.7353992462158203 4.406581401824951 4.406581401824951
  batch 60 loss: 1.7353992462158203, 4.406581401824951, 4.406581401824951
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.8891555070877075 4.507861137390137 4.507861137390137
Loss :  1.8498859405517578 4.537909030914307 4.537909030914307
Loss :  1.722177505493164 4.584115982055664 4.584115982055664
Loss :  1.7333513498306274 4.649141788482666 4.649141788482666
Loss :  1.777943730354309 4.428808212280273 4.428808212280273
Loss :  1.9086109399795532 4.424534797668457 4.424534797668457
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.7011854648590088 4.457537651062012 4.457537651062012
Loss :  1.8577195405960083 4.4835205078125 4.4835205078125
Loss :  1.6290827989578247 4.42469596862793 4.42469596862793
Total LOSS train 4.481755520747258 valid 4.447572231292725
CE LOSS train 1.8016242760878343 valid 0.4072706997394562
Contrastive LOSS train 4.481755520747258 valid 1.1061739921569824
EPOCH 41:
Loss :  1.6645070314407349 4.338395595550537 4.338395595550537
Loss :  1.708553433418274 4.679769039154053 4.679769039154053
Loss :  1.6755112409591675 4.444664478302002 4.444664478302002
Loss :  1.7473196983337402 4.409696578979492 4.409696578979492
Loss :  1.8655015230178833 4.421437740325928 4.421437740325928
Loss :  1.7528048753738403 4.50806188583374 4.50806188583374
Loss :  1.876171350479126 4.742157936096191 4.742157936096191
Loss :  1.7115849256515503 4.243504047393799 4.243504047393799
Loss :  1.7596890926361084 4.634666442871094 4.634666442871094
Loss :  1.7880655527114868 4.331575393676758 4.331575393676758
Loss :  1.6981031894683838 4.477242469787598 4.477242469787598
Loss :  1.7236359119415283 4.7471160888671875 4.7471160888671875
Loss :  1.7258838415145874 4.537821292877197 4.537821292877197
Loss :  1.889135479927063 4.597776889801025 4.597776889801025
Loss :  1.7026628255844116 4.529202461242676 4.529202461242676
Loss :  1.6354557275772095 4.569370746612549 4.569370746612549
Loss :  1.7503690719604492 4.421747207641602 4.421747207641602
Loss :  1.7748520374298096 4.471733093261719 4.471733093261719
Loss :  1.8185346126556396 4.4361772537231445 4.4361772537231445
Loss :  1.7833530902862549 4.310601711273193 4.310601711273193
  batch 20 loss: 1.7833530902862549, 4.310601711273193, 4.310601711273193
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.7170722484588623 4.446404457092285 4.446404457092285
Loss :  1.7988568544387817 4.6541924476623535 4.6541924476623535
Loss :  1.817867398262024 4.472684860229492 4.472684860229492
Loss :  1.7937530279159546 4.4759202003479 4.4759202003479
Loss :  1.6142131090164185 4.616559028625488 4.616559028625488
Loss :  1.7662500143051147 4.545490741729736 4.545490741729736
Loss :  1.6319010257720947 4.676860332489014 4.676860332489014
Loss :  1.780008316040039 4.426384449005127 4.426384449005127
Loss :  1.7251105308532715 4.3978800773620605 4.3978800773620605
Loss :  1.8600261211395264 4.491665363311768 4.491665363311768
Loss :  1.7567188739776611 4.490072250366211 4.490072250366211
Loss :  1.9530171155929565 4.587308406829834 4.587308406829834
Loss :  1.8259968757629395 4.534326553344727 4.534326553344727
Loss :  1.8127280473709106 4.437471389770508 4.437471389770508
Loss :  1.7359956502914429 4.424096584320068 4.424096584320068
Loss :  1.756596565246582 4.454207897186279 4.454207897186279
Loss :  1.717589020729065 4.427425861358643 4.427425861358643
Loss :  1.8022136688232422 4.611356735229492 4.611356735229492
Loss :  1.7389358282089233 4.787961483001709 4.787961483001709
Loss :  1.6627098321914673 4.658544063568115 4.658544063568115
  batch 40 loss: 1.6627098321914673, 4.658544063568115, 4.658544063568115
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.834289312362671 4.604638576507568 4.604638576507568
Loss :  1.7194904088974 4.490328311920166 4.490328311920166
Loss :  1.777307391166687 4.403364181518555 4.403364181518555
Loss :  1.6963731050491333 4.49539041519165 4.49539041519165
Loss :  1.696663498878479 4.397461414337158 4.397461414337158
Loss :  1.7492324113845825 4.549201965332031 4.549201965332031
Loss :  1.6950355768203735 4.559220314025879 4.559220314025879
Loss :  1.858558177947998 4.437634468078613 4.437634468078613
Loss :  1.7802305221557617 4.361190319061279 4.361190319061279
Loss :  1.7179609537124634 4.467311382293701 4.467311382293701
Loss :  1.8107810020446777 4.5347490310668945 4.5347490310668945
Loss :  1.871254801750183 4.419641017913818 4.419641017913818
Loss :  1.714353322982788 4.376014709472656 4.376014709472656
Loss :  1.6870676279067993 4.446530342102051 4.446530342102051
Loss :  1.7702665328979492 4.470335483551025 4.470335483551025
Loss :  1.8888593912124634 4.559638977050781 4.559638977050781
Loss :  1.6206762790679932 4.473659038543701 4.473659038543701
Loss :  1.8449342250823975 4.535891532897949 4.535891532897949
Loss :  1.7171441316604614 4.469901084899902 4.469901084899902
Loss :  1.7483584880828857 4.404812812805176 4.404812812805176
  batch 60 loss: 1.7483584880828857, 4.404812812805176, 4.404812812805176
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.5738171339035034 4.3952202796936035 4.3952202796936035
Loss :  1.8000767230987549 4.533945560455322 4.533945560455322
Loss :  1.6901843547821045 4.269888877868652 4.269888877868652
Loss :  1.6965277194976807 4.475987434387207 4.475987434387207
Loss :  1.5834680795669556 4.098290920257568 4.098290920257568
Loss :  2.294205665588379 4.420709133148193 4.420709133148193
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 5], device='cuda:0')
Loss :  2.251443862915039 4.441662311553955 4.441662311553955
Loss :  2.2723164558410645 4.352436542510986 4.352436542510986
Loss :  2.232412338256836 4.28033447265625 4.28033447265625
Total LOSS train 4.48768892288208 valid 4.373785614967346
CE LOSS train 1.7517256278258104 valid 0.558103084564209
Contrastive LOSS train 4.48768892288208 valid 1.0700836181640625
EPOCH 42:
Loss :  1.6943429708480835 4.428941249847412 4.428941249847412
Loss :  1.6654491424560547 4.610321521759033 4.610321521759033
Loss :  1.7522183656692505 4.262614727020264 4.262614727020264
Loss :  1.7468475103378296 4.318617820739746 4.318617820739746
Loss :  1.6412895917892456 4.2927374839782715 4.2927374839782715
Loss :  1.5724482536315918 4.336450099945068 4.336450099945068
Loss :  1.7550815343856812 4.451717853546143 4.451717853546143
Loss :  1.7233268022537231 4.432210922241211 4.432210922241211
Loss :  1.700265645980835 4.478522777557373 4.478522777557373
Loss :  1.7785792350769043 4.542963027954102 4.542963027954102
Loss :  1.6276211738586426 4.743257999420166 4.743257999420166
Loss :  1.6655725240707397 4.464713096618652 4.464713096618652
Loss :  1.6291171312332153 4.457255840301514 4.457255840301514
Loss :  1.7024023532867432 4.56727409362793 4.56727409362793
Loss :  1.6633802652359009 4.472458362579346 4.472458362579346
Loss :  1.626570701599121 4.392404556274414 4.392404556274414
Loss :  1.63428795337677 4.381206035614014 4.381206035614014
Loss :  1.743849515914917 4.367842674255371 4.367842674255371
Loss :  1.7556500434875488 4.458670139312744 4.458670139312744
Loss :  1.7387967109680176 4.3859333992004395 4.3859333992004395
  batch 20 loss: 1.7387967109680176, 4.3859333992004395, 4.3859333992004395
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.7811930179595947 4.535668849945068 4.535668849945068
Loss :  1.7979459762573242 4.430425643920898 4.430425643920898
Loss :  1.736167311668396 4.540847301483154 4.540847301483154
Loss :  1.7060463428497314 4.47845983505249 4.47845983505249
Loss :  1.778611660003662 4.640349864959717 4.640349864959717
Loss :  1.7191439867019653 4.370093822479248 4.370093822479248
Loss :  1.7729690074920654 4.637598514556885 4.637598514556885
Loss :  1.7597236633300781 4.49512243270874 4.49512243270874
Loss :  1.6972825527191162 4.38169527053833 4.38169527053833
Loss :  1.732587218284607 4.524291515350342 4.524291515350342
Loss :  1.7310667037963867 4.522276878356934 4.522276878356934
Loss :  1.7775530815124512 4.69951057434082 4.69951057434082
Loss :  1.7754096984863281 4.417120933532715 4.417120933532715
Loss :  1.7798699140548706 4.557868480682373 4.557868480682373
Loss :  1.7777270078659058 4.610233783721924 4.610233783721924
Loss :  1.8148155212402344 4.509798526763916 4.509798526763916
Loss :  1.809981107711792 4.453486919403076 4.453486919403076
Loss :  1.7643381357192993 4.43247652053833 4.43247652053833
Loss :  1.737845778465271 4.4708123207092285 4.4708123207092285
Loss :  1.8133060932159424 4.536230564117432 4.536230564117432
  batch 40 loss: 1.8133060932159424, 4.536230564117432, 4.536230564117432
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.8867207765579224 4.508067607879639 4.508067607879639
Loss :  1.680458903312683 4.467643737792969 4.467643737792969
Loss :  1.7929189205169678 4.419627666473389 4.419627666473389
Loss :  1.7842708826065063 4.625872611999512 4.625872611999512
Loss :  1.8723286390304565 4.32571268081665 4.32571268081665
Loss :  1.8004252910614014 4.382262706756592 4.382262706756592
Loss :  1.712003469467163 4.4725141525268555 4.4725141525268555
Loss :  1.862889051437378 4.407207012176514 4.407207012176514
Loss :  1.8313311338424683 4.482829570770264 4.482829570770264
Loss :  1.7770527601242065 4.5114312171936035 4.5114312171936035
Loss :  1.8011631965637207 4.470954418182373 4.470954418182373
Loss :  1.8660165071487427 4.653769493103027 4.653769493103027
Loss :  1.840999722480774 4.443343162536621 4.443343162536621
Loss :  1.7411376237869263 4.440969944000244 4.440969944000244
Loss :  1.849735975265503 4.508413791656494 4.508413791656494
Loss :  1.7695382833480835 4.5278706550598145 4.5278706550598145
Loss :  1.7744297981262207 4.475654125213623 4.475654125213623
Loss :  1.7925961017608643 4.479525089263916 4.479525089263916
Loss :  1.7359263896942139 4.6645188331604 4.6645188331604
Loss :  1.7062243223190308 4.432051181793213 4.432051181793213
  batch 60 loss: 1.7062243223190308, 4.432051181793213, 4.432051181793213
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.7076890468597412 4.56315279006958 4.56315279006958
Loss :  1.8738356828689575 4.525111675262451 4.525111675262451
Loss :  1.857408046722412 4.372293472290039 4.372293472290039
Loss :  1.7954267263412476 4.446325778961182 4.446325778961182
Loss :  1.7934844493865967 4.124444484710693 4.124444484710693
Loss :  10.500370979309082 4.4323554039001465 4.4323554039001465
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  8.989784240722656 4.4229841232299805 4.4229841232299805
Loss :  13.979891777038574 4.360335350036621 4.360335350036621
Loss :  11.089177131652832 4.1621527671813965 4.1621527671813965
Total LOSS train 4.474185386070839 valid 4.344456911087036
CE LOSS train 1.7536414293142466 valid 2.772294282913208
Contrastive LOSS train 4.474185386070839 valid 1.0405381917953491
EPOCH 43:
Loss :  1.8294931650161743 4.315995216369629 4.315995216369629
Loss :  1.7279433012008667 4.445642948150635 4.445642948150635
Loss :  1.817696452140808 4.536913871765137 4.536913871765137
Loss :  1.7737797498703003 4.531493663787842 4.531493663787842
Loss :  1.72685968875885 4.2730817794799805 4.2730817794799805
Loss :  1.7737329006195068 4.441569805145264 4.441569805145264
Loss :  1.8003551959991455 4.5243239402771 4.5243239402771
Loss :  1.788372278213501 4.391228675842285 4.391228675842285
Loss :  1.8016942739486694 4.430307865142822 4.430307865142822
Loss :  1.768540859222412 4.4583635330200195 4.4583635330200195
Loss :  1.7686139345169067 4.482581615447998 4.482581615447998
Loss :  1.7186906337738037 4.481198310852051 4.481198310852051
Loss :  1.8045339584350586 4.680151462554932 4.680151462554932
Loss :  1.8241844177246094 4.474453449249268 4.474453449249268
Loss :  1.7942293882369995 4.453188419342041 4.453188419342041
Loss :  1.7593317031860352 4.755167007446289 4.755167007446289
Loss :  1.7714312076568604 4.686229228973389 4.686229228973389
Loss :  1.8045686483383179 4.583000659942627 4.583000659942627
Loss :  1.8265084028244019 4.297281265258789 4.297281265258789
Loss :  1.8399707078933716 4.424727439880371 4.424727439880371
  batch 20 loss: 1.8399707078933716, 4.424727439880371, 4.424727439880371
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.8427218198776245 4.275513648986816 4.275513648986816
Loss :  1.8086881637573242 4.375704765319824 4.375704765319824
Loss :  1.835116982460022 4.675586223602295 4.675586223602295
Loss :  1.7809135913848877 4.423629283905029 4.423629283905029
Loss :  1.7887964248657227 4.691987991333008 4.691987991333008
Loss :  1.8398772478103638 4.358571529388428 4.358571529388428
Loss :  1.73728346824646 4.546032428741455 4.546032428741455
Loss :  1.8489151000976562 4.363236427307129 4.363236427307129
Loss :  1.843894362449646 4.419183254241943 4.419183254241943
Loss :  1.7902350425720215 4.453991889953613 4.453991889953613
Loss :  1.829379677772522 4.554754734039307 4.554754734039307
Loss :  1.7785253524780273 4.605539798736572 4.605539798736572
Loss :  1.866024374961853 4.434998035430908 4.434998035430908
Loss :  1.8681719303131104 4.450431823730469 4.450431823730469
Loss :  1.8732578754425049 4.5694355964660645 4.5694355964660645
Loss :  1.9142134189605713 4.757081985473633 4.757081985473633
Loss :  1.8799874782562256 4.543912410736084 4.543912410736084
Loss :  1.7969590425491333 4.376877307891846 4.376877307891846
Loss :  1.8266688585281372 4.504762172698975 4.504762172698975
Loss :  1.8313449621200562 4.442174911499023 4.442174911499023
  batch 40 loss: 1.8313449621200562, 4.442174911499023, 4.442174911499023
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 3, 5], device='cuda:0')
Loss :  1.8988004922866821 4.742020606994629 4.742020606994629
Loss :  1.8654348850250244 4.517836570739746 4.517836570739746
Loss :  1.9190887212753296 4.497897148132324 4.497897148132324
Loss :  1.835880994796753 4.474362373352051 4.474362373352051
Loss :  1.8807408809661865 4.287353992462158 4.287353992462158
Loss :  1.8510724306106567 4.44091796875 4.44091796875
Loss :  1.807999849319458 4.604303359985352 4.604303359985352
Loss :  1.8970812559127808 4.4487409591674805 4.4487409591674805
Loss :  1.8701939582824707 4.439493179321289 4.439493179321289
Loss :  1.847040057182312 4.431295871734619 4.431295871734619
Loss :  1.908786654472351 4.829583168029785 4.829583168029785
Loss :  1.8387583494186401 4.346818923950195 4.346818923950195
Loss :  1.861641764640808 4.437924385070801 4.437924385070801
Loss :  1.877555251121521 4.458098888397217 4.458098888397217
Loss :  1.964731216430664 4.432671546936035 4.432671546936035
Loss :  1.77992844581604 4.432103157043457 4.432103157043457
Loss :  1.8642001152038574 4.632688045501709 4.632688045501709
Loss :  1.8943569660186768 4.5386433601379395 4.5386433601379395
Loss :  1.8927552700042725 4.520981311798096 4.520981311798096
Loss :  1.8304694890975952 4.47265100479126 4.47265100479126
  batch 60 loss: 1.8304694890975952, 4.47265100479126, 4.47265100479126
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.8595613241195679 4.529525279998779 4.529525279998779
Loss :  1.8068041801452637 4.486135482788086 4.486135482788086
Loss :  1.7914620637893677 4.363887310028076 4.363887310028076
Loss :  1.868330478668213 4.3605804443359375 4.3605804443359375
Loss :  1.8430509567260742 4.119612693786621 4.119612693786621
Loss :  2.0827643871307373 4.393331050872803 4.393331050872803
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 5], device='cuda:0')
Loss :  2.1137757301330566 4.4530510902404785 4.4530510902404785
Loss :  2.1189143657684326 4.3481621742248535 4.3481621742248535
Loss :  1.9798959493637085 4.316695690155029 4.316695690155029
Total LOSS train 4.482098975548378 valid 4.377810001373291
CE LOSS train 1.8285728014432467 valid 0.4949739873409271
Contrastive LOSS train 4.482098975548378 valid 1.0791739225387573
EPOCH 44:
Loss :  1.858292818069458 4.239767074584961 4.239767074584961
Loss :  1.839062213897705 4.451746463775635 4.451746463775635
Loss :  1.8387491703033447 4.353970050811768 4.353970050811768
Loss :  1.8142815828323364 4.412457466125488 4.412457466125488
Loss :  1.8906551599502563 4.5770769119262695 4.5770769119262695
Loss :  1.8829643726348877 4.323145389556885 4.323145389556885
Loss :  2.0037801265716553 4.524055480957031 4.524055480957031
Loss :  1.902923583984375 4.276816368103027 4.276816368103027
Loss :  1.9318145513534546 4.396070957183838 4.396070957183838
Loss :  1.880574345588684 4.3513078689575195 4.3513078689575195
Loss :  1.8331152200698853 4.422266483306885 4.422266483306885
Loss :  1.8687059879302979 4.470622539520264 4.470622539520264
Loss :  1.925108790397644 4.450930118560791 4.450930118560791
Loss :  1.920621633529663 4.4387006759643555 4.4387006759643555
Loss :  1.9398901462554932 4.31412410736084 4.31412410736084
Loss :  1.820894718170166 4.402681350708008 4.402681350708008
Loss :  1.9222670793533325 4.484807968139648 4.484807968139648
Loss :  1.8860198259353638 4.470681190490723 4.470681190490723
Loss :  1.870732069015503 4.471589088439941 4.471589088439941
Loss :  1.9197874069213867 4.521865367889404 4.521865367889404
  batch 20 loss: 1.9197874069213867, 4.521865367889404, 4.521865367889404
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.8282856941223145 4.346449851989746 4.346449851989746
Loss :  1.9006808996200562 4.556128025054932 4.556128025054932
Loss :  1.953887701034546 4.537356376647949 4.537356376647949
Loss :  1.8088977336883545 4.783624649047852 4.783624649047852
Loss :  1.9087849855422974 4.551076412200928 4.551076412200928
Loss :  1.9155484437942505 4.416511058807373 4.416511058807373
Loss :  1.9575557708740234 4.6476240158081055 4.6476240158081055
Loss :  1.9619630575180054 4.341525077819824 4.341525077819824
Loss :  1.9520093202590942 4.4158124923706055 4.4158124923706055
Loss :  1.8594900369644165 4.417017459869385 4.417017459869385
Loss :  1.912752628326416 4.446874141693115 4.446874141693115
Loss :  1.9892433881759644 4.532628536224365 4.532628536224365
Loss :  1.9042891263961792 4.525965213775635 4.525965213775635
Loss :  1.899671196937561 4.561009883880615 4.561009883880615
Loss :  1.9285097122192383 4.423954963684082 4.423954963684082
Loss :  1.9100730419158936 4.427582263946533 4.427582263946533
Loss :  1.9098035097122192 4.389235973358154 4.389235973358154
Loss :  1.8827762603759766 4.30469274520874 4.30469274520874
Loss :  1.904722809791565 4.447889804840088 4.447889804840088
Loss :  1.8614047765731812 4.417155742645264 4.417155742645264
  batch 40 loss: 1.8614047765731812, 4.417155742645264, 4.417155742645264
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.9331547021865845 4.4288434982299805 4.4288434982299805
Loss :  1.88331937789917 4.3728837966918945 4.3728837966918945
Loss :  1.9205312728881836 4.384673118591309 4.384673118591309
Loss :  1.8897138833999634 4.418010234832764 4.418010234832764
Loss :  1.9099171161651611 4.259485721588135 4.259485721588135
Loss :  1.9360713958740234 4.458444118499756 4.458444118499756
Loss :  1.8047014474868774 4.542078971862793 4.542078971862793
Loss :  1.8356873989105225 4.4449687004089355 4.4449687004089355
Loss :  1.7839715480804443 4.3137640953063965 4.3137640953063965
Loss :  1.8780241012573242 4.426519393920898 4.426519393920898
Loss :  1.883899450302124 4.447916030883789 4.447916030883789
Loss :  1.8276653289794922 4.268554210662842 4.268554210662842
Loss :  1.882897973060608 4.381716728210449 4.381716728210449
Loss :  1.8559592962265015 4.323891639709473 4.323891639709473
Loss :  1.799443244934082 4.415402412414551 4.415402412414551
Loss :  1.7347866296768188 4.387655735015869 4.387655735015869
Loss :  1.7293156385421753 4.652955055236816 4.652955055236816
Loss :  1.7374770641326904 4.475164413452148 4.475164413452148
Loss :  1.7224788665771484 4.704517841339111 4.704517841339111
Loss :  1.7055473327636719 4.291043281555176 4.291043281555176
  batch 60 loss: 1.7055473327636719, 4.291043281555176, 4.291043281555176
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 4], device='cuda:0')
Loss :  1.6875436305999756 4.287922382354736 4.287922382354736
Loss :  1.7080167531967163 4.017851829528809 4.017851829528809
Loss :  1.678564429283142 4.366519451141357 4.366519451141357
Loss :  1.6806910037994385 4.301244258880615 4.301244258880615
Loss :  1.6596179008483887 4.177022933959961 4.177022933959961
Loss :  5.071450233459473 4.375732898712158 4.375732898712158
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4], device='cuda:0')
Loss :  31.066110610961914 4.59443473815918 4.59443473815918
Loss :  3.195580244064331 4.3455963134765625 4.3455963134765625
Loss :  2.042663812637329 4.305390357971191 4.305390357971191
Total LOSS train 4.421443807161771 valid 4.405288577079773
CE LOSS train 1.8610705797488873 valid 0.5106659531593323
Contrastive LOSS train 4.421443807161771 valid 1.0763475894927979
EPOCH 45:
Loss :  1.784196376800537 4.3549699783325195 4.3549699783325195
Loss :  1.7164967060089111 4.5341291427612305 4.5341291427612305
Loss :  1.6599698066711426 4.353819370269775 4.353819370269775
Loss :  1.6750411987304688 4.491911888122559 4.491911888122559
Loss :  1.6618326902389526 4.200667858123779 4.200667858123779
Loss :  1.635189414024353 4.375864505767822 4.375864505767822
Loss :  1.6850534677505493 4.318243503570557 4.318243503570557
Loss :  1.656223177909851 4.005897521972656 4.005897521972656
Loss :  1.6566989421844482 3.9511587619781494 3.9511587619781494
Loss :  1.6592128276824951 4.170234203338623 4.170234203338623
Loss :  1.6064127683639526 4.312719821929932 4.312719821929932
Loss :  1.6125614643096924 4.238178730010986 4.238178730010986
Loss :  1.59401273727417 4.197829723358154 4.197829723358154
Loss :  1.6253178119659424 4.178697109222412 4.178697109222412
Loss :  1.6686457395553589 3.8363616466522217 3.8363616466522217
Loss :  1.5991268157958984 4.232639789581299 4.232639789581299
Loss :  1.573745608329773 4.354030132293701 4.354030132293701
Loss :  1.5822391510009766 4.3329010009765625 4.3329010009765625
Loss :  1.5794589519500732 4.062105655670166 4.062105655670166
Loss :  1.6264160871505737 3.922849178314209 3.922849178314209
  batch 20 loss: 1.6264160871505737, 3.922849178314209, 3.922849178314209
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.610275387763977 4.358095645904541 4.358095645904541
Loss :  1.5811872482299805 4.183187007904053 4.183187007904053
Loss :  1.591020107269287 4.368955135345459 4.368955135345459
Loss :  1.585252046585083 4.137869834899902 4.137869834899902
Loss :  1.5866681337356567 4.209343910217285 4.209343910217285
Loss :  1.55769681930542 4.1857147216796875 4.1857147216796875
Loss :  1.5344313383102417 4.403175354003906 4.403175354003906
Loss :  1.583365559577942 4.189720630645752 4.189720630645752
Loss :  1.5022839307785034 4.215684413909912 4.215684413909912
Loss :  1.6168971061706543 4.380507469177246 4.380507469177246
Loss :  1.5314208269119263 4.416413307189941 4.416413307189941
Loss :  1.625342845916748 4.402068138122559 4.402068138122559
Loss :  1.591566562652588 4.27331018447876 4.27331018447876
Loss :  1.59833824634552 4.320183753967285 4.320183753967285
Loss :  1.58254075050354 4.369901657104492 4.369901657104492
Loss :  1.5828862190246582 4.265542030334473 4.265542030334473
Loss :  1.5926295518875122 4.3582024574279785 4.3582024574279785
Loss :  1.6701525449752808 4.257351875305176 4.257351875305176
Loss :  1.6178464889526367 4.377102851867676 4.377102851867676
Loss :  1.6464195251464844 4.391117572784424 4.391117572784424
  batch 40 loss: 1.6464195251464844, 4.391117572784424, 4.391117572784424
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.625261664390564 4.318115234375 4.318115234375
Loss :  1.5968328714370728 4.360792636871338 4.360792636871338
Loss :  1.6282784938812256 4.278835773468018 4.278835773468018
Loss :  1.6270747184753418 4.318077087402344 4.318077087402344
Loss :  1.6149214506149292 4.341124057769775 4.341124057769775
Loss :  1.616777777671814 4.205392837524414 4.205392837524414
Loss :  1.6287851333618164 4.429101943969727 4.429101943969727
Loss :  1.605934739112854 4.4146904945373535 4.4146904945373535
Loss :  1.635294795036316 4.319665908813477 4.319665908813477
Loss :  1.5788614749908447 4.268369197845459 4.268369197845459
Loss :  1.6232653856277466 4.449921131134033 4.449921131134033
Loss :  1.59166419506073 4.219863414764404 4.219863414764404
Loss :  1.5708870887756348 4.071761131286621 4.071761131286621
Loss :  1.5862956047058105 4.098877429962158 4.098877429962158
Loss :  1.585955023765564 4.302915573120117 4.302915573120117
Loss :  1.6099519729614258 4.180645942687988 4.180645942687988
Loss :  1.5420763492584229 4.2331366539001465 4.2331366539001465
Loss :  1.542500376701355 4.180935382843018 4.180935382843018
Loss :  1.4675891399383545 4.5970330238342285 4.5970330238342285
Loss :  1.6000641584396362 4.046230792999268 4.046230792999268
  batch 60 loss: 1.6000641584396362, 4.046230792999268, 4.046230792999268
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5456268787384033 4.0750885009765625 4.0750885009765625
Loss :  1.569966197013855 4.061147212982178 4.061147212982178
Loss :  1.5374524593353271 3.882786750793457 3.882786750793457
Loss :  1.5384589433670044 3.86179256439209 3.86179256439209
Loss :  1.5099443197250366 3.7832345962524414 3.7832345962524414
Loss :  1.2736698389053345 4.519509792327881 4.519509792327881
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.328384518623352 4.5350213050842285 4.5350213050842285
Loss :  1.3275116682052612 4.3529157638549805 4.3529157638549805
Loss :  1.3420765399932861 4.144345760345459 4.144345760345459
Total LOSS train 4.242895273061899 valid 4.387948155403137
CE LOSS train 1.6034737568635207 valid 0.33551913499832153
Contrastive LOSS train 4.242895273061899 valid 1.0360864400863647
EPOCH 46:
Loss :  1.582472801208496 4.210736274719238 4.210736274719238
Loss :  1.6105846166610718 4.479465007781982 4.479465007781982
Loss :  1.5576395988464355 4.1788506507873535 4.1788506507873535
Loss :  1.577028751373291 4.392374515533447 4.392374515533447
Loss :  1.5846596956253052 3.973672389984131 3.973672389984131
Loss :  1.53229820728302 3.9950509071350098 3.9950509071350098
Loss :  1.590415120124817 4.384466171264648 4.384466171264648
Loss :  1.5595011711120605 4.147540092468262 4.147540092468262
Loss :  1.5531507730484009 4.2980475425720215 4.2980475425720215
Loss :  1.5898343324661255 4.378979682922363 4.378979682922363
Loss :  1.5116384029388428 4.216468334197998 4.216468334197998
Loss :  1.52045476436615 4.25278902053833 4.25278902053833
Loss :  1.5158401727676392 4.217606067657471 4.217606067657471
Loss :  1.5539216995239258 3.9922425746917725 3.9922425746917725
Loss :  1.6494719982147217 4.3282365798950195 4.3282365798950195
Loss :  1.5883913040161133 4.48104190826416 4.48104190826416
Loss :  1.53206205368042 4.233454704284668 4.233454704284668
Loss :  1.5678194761276245 4.286177158355713 4.286177158355713
Loss :  1.5520527362823486 4.221299171447754 4.221299171447754
Loss :  1.6308964490890503 4.475622653961182 4.475622653961182
  batch 20 loss: 1.6308964490890503, 4.475622653961182, 4.475622653961182
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
Loss :  1.6025967597961426 4.2570576667785645 4.2570576667785645
Loss :  1.5538076162338257 4.457767963409424 4.457767963409424
Loss :  1.588631272315979 4.359198570251465 4.359198570251465
Loss :  1.5989346504211426 3.905871629714966 3.905871629714966
Loss :  1.6294739246368408 3.9764387607574463 3.9764387607574463
Loss :  1.5909464359283447 3.8713860511779785 3.8713860511779785
Loss :  1.588187575340271 4.010277271270752 4.010277271270752
Loss :  1.6148794889450073 3.9988973140716553 3.9988973140716553
Loss :  1.52328622341156 4.395715236663818 4.395715236663818
Loss :  1.6507877111434937 4.47271203994751 4.47271203994751
Loss :  1.5320384502410889 4.490285396575928 4.490285396575928
Loss :  1.6340607404708862 4.48641300201416 4.48641300201416
Loss :  1.582837462425232 4.429910659790039 4.429910659790039
Loss :  1.585251808166504 4.431741714477539 4.431741714477539
Loss :  1.5267541408538818 4.630051612854004 4.630051612854004
Loss :  1.524013876914978 4.528016567230225 4.528016567230225
Loss :  1.534860372543335 4.340263366699219 4.340263366699219
Loss :  1.6281259059906006 4.426164627075195 4.426164627075195
Loss :  1.6027398109436035 4.506861209869385 4.506861209869385
Loss :  1.636295199394226 4.363285541534424 4.363285541534424
  batch 40 loss: 1.636295199394226, 4.363285541534424, 4.363285541534424
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.595041036605835 4.277464389801025 4.277464389801025
Loss :  1.564868688583374 4.275390625 4.275390625
Loss :  1.5801641941070557 4.215076446533203 4.215076446533203
Loss :  1.6028722524642944 4.317129611968994 4.317129611968994
Loss :  1.5868775844573975 3.9847631454467773 3.9847631454467773
Loss :  1.6243597269058228 3.8304247856140137 3.8304247856140137
Loss :  1.6796989440917969 3.815213203430176 3.815213203430176
Loss :  1.6466425657272339 4.08815860748291 4.08815860748291
Loss :  1.720975399017334 3.795889139175415 3.795889139175415
Loss :  1.6721450090408325 4.05687952041626 4.05687952041626
Loss :  1.726896047592163 4.166268348693848 4.166268348693848
Loss :  1.7022647857666016 4.204341411590576 4.204341411590576
Loss :  1.6855804920196533 4.055222034454346 4.055222034454346
Loss :  1.7222681045532227 3.9432733058929443 3.9432733058929443
Loss :  1.6799423694610596 4.0450544357299805 4.0450544357299805
Loss :  1.7224891185760498 3.8250439167022705 3.8250439167022705
Loss :  1.6625523567199707 3.8581106662750244 3.8581106662750244
Loss :  1.6395896673202515 3.8448126316070557 3.8448126316070557
Loss :  1.6495847702026367 3.8874638080596924 3.8874638080596924
Loss :  1.7323994636535645 3.7347044944763184 3.7347044944763184
  batch 60 loss: 1.7323994636535645, 3.7347044944763184, 3.7347044944763184
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.6450107097625732 4.1305952072143555 4.1305952072143555
Loss :  1.6923820972442627 4.360335350036621 4.360335350036621
Loss :  1.669959545135498 3.9617257118225098 3.9617257118225098
Loss :  1.6392024755477905 3.667706251144409 3.667706251144409
Loss :  1.600485920906067 3.8649580478668213 3.8649580478668213
Loss :  1.7453535795211792 4.335446357727051 4.335446357727051
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4, 5], device='cuda:0')
Loss :  1.7384923696517944 4.2412004470825195 4.2412004470825195
Loss :  1.7423094511032104 4.278061866760254 4.278061866760254
Loss :  1.7749663591384888 4.166588306427002 4.166588306427002
Total LOSS train 4.179822195493258 valid 4.2553242444992065
CE LOSS train 1.608198413482079 valid 0.4437415897846222
Contrastive LOSS train 4.179822195493258 valid 1.0416470766067505
Saved best model. Old loss 4.28325879573822 and new best loss 4.2553242444992065
EPOCH 47:
Loss :  1.6928914785385132 4.245197772979736 4.245197772979736
Loss :  1.7351030111312866 4.701958656311035 4.701958656311035
Loss :  1.6987520456314087 4.360494136810303 4.360494136810303
Loss :  1.7532422542572021 4.307000160217285 4.307000160217285
Loss :  1.7283567190170288 4.026701927185059 4.026701927185059
Loss :  1.7011780738830566 4.047636985778809 4.047636985778809
Loss :  1.7360966205596924 4.547662258148193 4.547662258148193
Loss :  1.754854440689087 4.5331220626831055 4.5331220626831055
Loss :  1.771873116493225 4.230467796325684 4.230467796325684
Loss :  1.7950187921524048 4.354029178619385 4.354029178619385
Loss :  1.7170120477676392 4.490503787994385 4.490503787994385
Loss :  1.7520545721054077 4.413414478302002 4.413414478302002
Loss :  1.7417923212051392 4.21357536315918 4.21357536315918
Loss :  1.762089729309082 4.840008735656738 4.840008735656738
Loss :  1.7824442386627197 4.50844144821167 4.50844144821167
Loss :  1.8121023178100586 4.603648662567139 4.603648662567139
Loss :  1.7774176597595215 4.65505838394165 4.65505838394165
Loss :  1.743370532989502 4.73113489151001 4.73113489151001
Loss :  1.7649040222167969 4.254103183746338 4.254103183746338
Loss :  1.8000130653381348 4.322127819061279 4.322127819061279
  batch 20 loss: 1.8000130653381348, 4.322127819061279, 4.322127819061279
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.7887308597564697 4.3414764404296875 4.3414764404296875
Loss :  1.7575340270996094 4.328982353210449 4.328982353210449
Loss :  1.7890630960464478 4.2174201011657715 4.2174201011657715
Loss :  1.763615369796753 4.280874729156494 4.280874729156494
Loss :  1.8157644271850586 4.517227649688721 4.517227649688721
Loss :  1.820728063583374 4.385595321655273 4.385595321655273
Loss :  1.8304312229156494 4.639875888824463 4.639875888824463
Loss :  1.8886754512786865 4.390693664550781 4.390693664550781
Loss :  1.7959916591644287 4.435601711273193 4.435601711273193
Loss :  1.9010311365127563 4.397923946380615 4.397923946380615
Loss :  1.8076986074447632 4.4051690101623535 4.4051690101623535
Loss :  1.8820626735687256 4.574386119842529 4.574386119842529
Loss :  1.8288629055023193 4.465603351593018 4.465603351593018
Loss :  1.8693114519119263 4.486739635467529 4.486739635467529
Loss :  1.8525457382202148 4.381574630737305 4.381574630737305
Loss :  1.8395028114318848 4.544137001037598 4.544137001037598
Loss :  1.8342344760894775 4.5213942527771 4.5213942527771
Loss :  1.9470460414886475 4.330658912658691 4.330658912658691
Loss :  1.8484512567520142 4.473033428192139 4.473033428192139
Loss :  1.9446361064910889 4.4579925537109375 4.4579925537109375
  batch 40 loss: 1.9446361064910889, 4.4579925537109375, 4.4579925537109375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.9454265832901 4.383815288543701 4.383815288543701
Loss :  1.8992851972579956 4.418457984924316 4.418457984924316
Loss :  1.8115065097808838 4.365348815917969 4.365348815917969
Loss :  1.9292391538619995 4.434885025024414 4.434885025024414
Loss :  1.917129397392273 4.604067325592041 4.604067325592041
Loss :  1.8974624872207642 4.670762062072754 4.670762062072754
Loss :  1.9625186920166016 4.41359806060791 4.41359806060791
Loss :  1.8751544952392578 4.6218342781066895 4.6218342781066895
Loss :  1.9067168235778809 4.255324840545654 4.255324840545654
Loss :  1.9810576438903809 4.5328826904296875 4.5328826904296875
Loss :  1.9407243728637695 4.690493583679199 4.690493583679199
Loss :  1.898921012878418 4.748586654663086 4.748586654663086
Loss :  1.9258183240890503 4.444260597229004 4.444260597229004
Loss :  1.952020287513733 4.455248832702637 4.455248832702637
Loss :  1.8773669004440308 4.787348747253418 4.787348747253418
Loss :  1.9574426412582397 4.404158115386963 4.404158115386963
Loss :  1.9377388954162598 4.624227046966553 4.624227046966553
Loss :  1.8540421724319458 4.478315353393555 4.478315353393555
Loss :  1.8143874406814575 4.653649806976318 4.653649806976318
Loss :  1.9390041828155518 4.402541160583496 4.402541160583496
  batch 60 loss: 1.9390041828155518, 4.402541160583496, 4.402541160583496
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.883995532989502 4.4179205894470215 4.4179205894470215
Loss :  1.946948766708374 4.525496959686279 4.525496959686279
Loss :  1.8934520483016968 4.440717697143555 4.440717697143555
Loss :  1.8926316499710083 4.424630641937256 4.424630641937256
Loss :  1.9107084274291992 4.139765739440918 4.139765739440918
Loss :  1.7910504341125488 4.470768928527832 4.470768928527832
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4, 5], device='cuda:0')
Loss :  1.7069774866104126 4.467843532562256 4.467843532562256
Loss :  1.7427983283996582 4.364624977111816 4.364624977111816
Loss :  1.7438437938690186 4.201067924499512 4.201067924499512
Total LOSS train 4.4507844044612 valid 4.376076340675354
CE LOSS train 1.8396489858627318 valid 0.43596094846725464
Contrastive LOSS train 4.4507844044612 valid 1.050266981124878
EPOCH 48:
Loss :  1.9077746868133545 4.311499118804932 4.311499118804932
Loss :  1.9649112224578857 4.480947017669678 4.480947017669678
Loss :  1.8660187721252441 4.516999244689941 4.516999244689941
Loss :  1.9307599067687988 4.423589706420898 4.423589706420898
Loss :  1.9536336660385132 4.37025260925293 4.37025260925293
Loss :  1.81204354763031 4.391140460968018 4.391140460968018
Loss :  1.856383204460144 4.455069541931152 4.455069541931152
Loss :  1.9273910522460938 4.3876824378967285 4.3876824378967285
Loss :  1.9270049333572388 4.357168197631836 4.357168197631836
Loss :  1.881889820098877 4.607377529144287 4.607377529144287
Loss :  1.8665357828140259 4.509127616882324 4.509127616882324
Loss :  1.867810845375061 4.807222843170166 4.807222843170166
Loss :  1.8589493036270142 4.504849433898926 4.504849433898926
Loss :  1.9134947061538696 4.537437438964844 4.537437438964844
Loss :  1.9186142683029175 4.3767828941345215 4.3767828941345215
Loss :  1.888950228691101 4.651806831359863 4.651806831359863
Loss :  1.834939956665039 4.379195213317871 4.379195213317871
Loss :  1.8397513628005981 4.484955787658691 4.484955787658691
Loss :  1.8816338777542114 4.505160331726074 4.505160331726074
Loss :  1.8883594274520874 4.337125778198242 4.337125778198242
  batch 20 loss: 1.8883594274520874, 4.337125778198242, 4.337125778198242
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 4, 5], device='cuda:0')
Loss :  1.8269450664520264 4.362584114074707 4.362584114074707
Loss :  1.825156807899475 4.462788105010986 4.462788105010986
Loss :  1.855760931968689 4.619157314300537 4.619157314300537
Loss :  1.8365589380264282 4.389176845550537 4.389176845550537
Loss :  1.8010199069976807 4.548764228820801 4.548764228820801
Loss :  1.8723275661468506 4.412191867828369 4.412191867828369
Loss :  1.884906530380249 4.681029796600342 4.681029796600342
Loss :  1.8560993671417236 4.498585224151611 4.498585224151611
Loss :  1.8574496507644653 5.114375591278076 5.114375591278076
Loss :  1.925441026687622 4.520331859588623 4.520331859588623
Loss :  1.851786494255066 4.523106098175049 4.523106098175049
Loss :  1.9028260707855225 4.583914279937744 4.583914279937744
Loss :  1.8693342208862305 4.433509826660156 4.433509826660156
Loss :  1.8596397638320923 4.4680891036987305 4.4680891036987305
Loss :  1.8398267030715942 4.43856954574585 4.43856954574585
Loss :  1.8661134243011475 4.714651107788086 4.714651107788086
Loss :  1.8226089477539062 4.569889545440674 4.569889545440674
Loss :  1.8553144931793213 4.524824142456055 4.524824142456055
Loss :  1.881767749786377 4.530143737792969 4.530143737792969
Loss :  1.9752253293991089 4.378527641296387 4.378527641296387
  batch 40 loss: 1.9752253293991089, 4.378527641296387, 4.378527641296387
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 4, 5], device='cuda:0')
Loss :  1.8830335140228271 4.497854709625244 4.497854709625244
Loss :  1.8648332357406616 4.288983345031738 4.288983345031738
Loss :  1.8889775276184082 4.396231651306152 4.396231651306152
Loss :  1.967983603477478 4.440185546875 4.440185546875
Loss :  1.9335575103759766 4.503770351409912 4.503770351409912
Loss :  1.9102281332015991 4.701638221740723 4.701638221740723
Loss :  1.9541443586349487 4.57888126373291 4.57888126373291
Loss :  1.9792156219482422 4.5240983963012695 4.5240983963012695
Loss :  1.9779249429702759 4.787985324859619 4.787985324859619
Loss :  1.8872249126434326 4.516010761260986 4.516010761260986
Loss :  2.025582790374756 4.612392902374268 4.612392902374268
Loss :  1.9780235290527344 4.508270263671875 4.508270263671875
Loss :  1.9183576107025146 4.432736396789551 4.432736396789551
Loss :  1.886648178100586 4.360250473022461 4.360250473022461
Loss :  1.9243721961975098 4.429295063018799 4.429295063018799
Loss :  2.007690668106079 4.555102825164795 4.555102825164795
Loss :  1.9562208652496338 4.564428806304932 4.564428806304932
Loss :  1.9339457750320435 4.520432949066162 4.520432949066162
Loss :  1.9243333339691162 4.580463886260986 4.580463886260986
Loss :  1.9781848192214966 4.301915168762207 4.301915168762207
  batch 60 loss: 1.9781848192214966, 4.301915168762207, 4.301915168762207
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.9500186443328857 4.659663200378418 4.659663200378418
Loss :  1.9917548894882202 4.432297229766846 4.432297229766846
Loss :  1.957331657409668 4.359860420227051 4.359860420227051
Loss :  1.9969426393508911 4.537367820739746 4.537367820739746
Loss :  1.943024754524231 4.370632648468018 4.370632648468018
Loss :  2.053074598312378 4.3922014236450195 4.3922014236450195
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.7349603176116943 4.535086154937744 4.535086154937744
Loss :  1.9859094619750977 4.483435153961182 4.483435153961182
Loss :  1.9144395589828491 4.273078441619873 4.273078441619873
Total LOSS train 4.502005379016583 valid 4.420950293540955
CE LOSS train 1.9026540811245258 valid 0.4786098897457123
Contrastive LOSS train 4.502005379016583 valid 1.0682696104049683
EPOCH 49:
Loss :  1.9233152866363525 4.511090278625488 4.511090278625488
Loss :  1.97452712059021 4.584576606750488 4.584576606750488
Loss :  1.9769765138626099 4.559381008148193 4.559381008148193
Loss :  1.9418869018554688 4.417302131652832 4.417302131652832
Loss :  1.958552360534668 4.53833532333374 4.53833532333374
Loss :  1.9555513858795166 4.311914920806885 4.311914920806885
Loss :  1.9817386865615845 4.5408244132995605 4.5408244132995605
Loss :  2.0076730251312256 4.640913486480713 4.640913486480713
Loss :  1.9923439025878906 4.4024434089660645 4.4024434089660645
Loss :  1.9658005237579346 4.504500865936279 4.504500865936279
Loss :  1.9623215198516846 4.785693645477295 4.785693645477295
Loss :  1.9287700653076172 4.620351791381836 4.620351791381836
Loss :  1.9292703866958618 4.558858871459961 4.558858871459961
Loss :  1.8445243835449219 4.460853099822998 4.460853099822998
Loss :  2.0250179767608643 4.383711338043213 4.383711338043213
Loss :  1.9072047472000122 4.461845397949219 4.461845397949219
Loss :  1.9267473220825195 4.534024238586426 4.534024238586426
Loss :  1.931003451347351 4.548874855041504 4.548874855041504
Loss :  1.9453837871551514 4.325663089752197 4.325663089752197
Loss :  1.9817930459976196 4.368536472320557 4.368536472320557
  batch 20 loss: 1.9817930459976196, 4.368536472320557, 4.368536472320557
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4, 5], device='cuda:0')
Loss :  1.860744833946228 4.300757884979248 4.300757884979248
Loss :  1.9119160175323486 4.395051956176758 4.395051956176758
Loss :  1.9163484573364258 4.608900547027588 4.608900547027588
Loss :  1.9020419120788574 4.460158348083496 4.460158348083496
Loss :  1.8791301250457764 4.781774997711182 4.781774997711182
Loss :  1.8960076570510864 4.358946800231934 4.358946800231934
Loss :  1.8619420528411865 4.840184688568115 4.840184688568115
Loss :  1.8951886892318726 4.4252610206604 4.4252610206604
Loss :  1.8033654689788818 4.3846235275268555 4.3846235275268555
Loss :  1.9268637895584106 4.446264266967773 4.446264266967773
Loss :  1.8472434282302856 4.49324369430542 4.49324369430542
Loss :  1.9008235931396484 4.664660930633545 4.664660930633545
Loss :  1.8295537233352661 4.413564205169678 4.413564205169678
Loss :  1.808868169784546 4.5677361488342285 4.5677361488342285
Loss :  1.8145201206207275 4.546474933624268 4.546474933624268
Loss :  1.8064863681793213 4.64961576461792 4.64961576461792
Loss :  1.8422513008117676 4.446305274963379 4.446305274963379
Loss :  1.8875083923339844 4.433391094207764 4.433391094207764
Loss :  1.82179594039917 4.282261848449707 4.282261848449707
Loss :  1.8861570358276367 4.507302761077881 4.507302761077881
  batch 40 loss: 1.8861570358276367, 4.507302761077881, 4.507302761077881
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4, 5], device='cuda:0')
Loss :  1.8565043210983276 4.4439592361450195 4.4439592361450195
Loss :  1.8206064701080322 4.361636638641357 4.361636638641357
Loss :  1.8235782384872437 4.402083873748779 4.402083873748779
Loss :  1.8501778841018677 4.5623321533203125 4.5623321533203125
Loss :  1.7906875610351562 4.342198848724365 4.342198848724365
Loss :  1.8176218271255493 4.402944564819336 4.402944564819336
Loss :  1.8607807159423828 4.493771076202393 4.493771076202393
Loss :  1.8373991250991821 4.49370813369751 4.49370813369751
Loss :  1.878524899482727 4.3950514793396 4.3950514793396
Loss :  1.8361603021621704 4.434807777404785 4.434807777404785
Loss :  1.873136043548584 4.536410331726074 4.536410331726074
Loss :  1.8187835216522217 4.458868026733398 4.458868026733398
Loss :  1.8481887578964233 4.376241207122803 4.376241207122803
Loss :  1.8354796171188354 4.537353515625 4.537353515625
Loss :  1.8941981792449951 4.462005138397217 4.462005138397217
Loss :  1.8594001531600952 4.345147609710693 4.345147609710693
Loss :  1.7928916215896606 4.35672664642334 4.35672664642334
Loss :  1.7692153453826904 4.51247501373291 4.51247501373291
Loss :  1.759440541267395 4.397949695587158 4.397949695587158
Loss :  1.8622214794158936 4.386343955993652 4.386343955993652
  batch 60 loss: 1.8622214794158936, 4.386343955993652, 4.386343955993652
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
Loss :  1.8525716066360474 4.396952152252197 4.396952152252197
Loss :  1.8966902494430542 4.439327716827393 4.439327716827393
Loss :  1.8922338485717773 4.368664264678955 4.368664264678955
Loss :  1.9130561351776123 4.383017539978027 4.383017539978027
Loss :  1.8881614208221436 4.4151387214660645 4.4151387214660645
Loss :  1.7201284170150757 4.356565952301025 4.356565952301025
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([5], device='cuda:0')
Loss :  1.7178394794464111 4.457368850708008 4.457368850708008
Loss :  1.724705457687378 4.399285793304443 4.399285793304443
Loss :  1.767102599143982 4.185932636260986 4.185932636260986
Total LOSS train 4.473404480860784 valid 4.349788308143616
CE LOSS train 1.8844133743873008 valid 0.4417756497859955
Contrastive LOSS train 4.473404480860784 valid 1.0464831590652466
EPOCH 50:
Loss :  1.9007874727249146 4.268314838409424 4.268314838409424
Loss :  1.9963356256484985 4.551324844360352 4.551324844360352
Loss :  1.881082534790039 4.501572132110596 4.501572132110596
Loss :  1.8937233686447144 4.55704927444458 4.55704927444458
Loss :  1.9530490636825562 4.242324352264404 4.242324352264404
Loss :  1.8865349292755127 4.381554126739502 4.381554126739502
Loss :  1.9711142778396606 4.5854949951171875 4.5854949951171875
Loss :  1.961100459098816 4.559791088104248 4.559791088104248
Loss :  1.9470951557159424 4.423190593719482 4.423190593719482
Loss :  1.9626319408416748 4.372047424316406 4.372047424316406
Loss :  1.9881083965301514 4.604783058166504 4.604783058166504
Loss :  1.9611958265304565 4.521207332611084 4.521207332611084
Loss :  1.9408622980117798 4.440073013305664 4.440073013305664
Loss :  1.9090991020202637 4.399018287658691 4.399018287658691
Loss :  2.0505893230438232 4.452476501464844 4.452476501464844
Loss :  2.0073132514953613 4.4615583419799805 4.4615583419799805
Loss :  2.098022937774658 4.398103713989258 4.398103713989258
Loss :  2.118046760559082 4.343058109283447 4.343058109283447
Loss :  2.0487349033355713 4.4146504402160645 4.4146504402160645
Loss :  2.139695405960083 4.492164611816406 4.492164611816406
  batch 20 loss: 2.139695405960083, 4.492164611816406, 4.492164611816406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 4, 5], device='cuda:0')
Loss :  2.081519365310669 4.294071674346924 4.294071674346924
Loss :  2.1058552265167236 4.42961311340332 4.42961311340332
Loss :  2.0492422580718994 4.552951335906982 4.552951335906982
Loss :  2.1558027267456055 4.473217487335205 4.473217487335205
Loss :  2.151273727416992 4.698819637298584 4.698819637298584
Loss :  2.1249775886535645 4.439076900482178 4.439076900482178
Loss :  2.2073795795440674 4.591549396514893 4.591549396514893
Loss :  2.1595964431762695 4.367292881011963 4.367292881011963
Loss :  2.092068910598755 4.251354694366455 4.251354694366455
Loss :  2.1727473735809326 4.461331844329834 4.461331844329834
Loss :  2.185408592224121 4.356194019317627 4.356194019317627
Loss :  2.192049741744995 4.3679890632629395 4.3679890632629395
Loss :  2.1812751293182373 4.432746410369873 4.432746410369873
Loss :  2.1035873889923096 4.483267307281494 4.483267307281494
Loss :  2.1823179721832275 4.396692276000977 4.396692276000977
Loss :  2.1741204261779785 4.3948259353637695 4.3948259353637695
Loss :  2.1559054851531982 4.458094120025635 4.458094120025635
Loss :  2.141627550125122 4.563481330871582 4.563481330871582
Loss :  2.135756492614746 4.512755870819092 4.512755870819092
Loss :  2.133307456970215 4.5399322509765625 4.5399322509765625
  batch 40 loss: 2.133307456970215, 4.5399322509765625, 4.5399322509765625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 4, 5], device='cuda:0')
Loss :  2.151836395263672 4.3770318031311035 4.3770318031311035
Loss :  2.2625339031219482 4.5266642570495605 4.5266642570495605
Loss :  2.0998055934906006 4.404959201812744 4.404959201812744
Loss :  2.153470277786255 4.544638156890869 4.544638156890869
Loss :  2.1661102771759033 4.382266044616699 4.382266044616699
Loss :  2.2285497188568115 4.470609188079834 4.470609188079834
Loss :  2.1642963886260986 4.581051826477051 4.581051826477051
Loss :  2.185107946395874 4.446382999420166 4.446382999420166
Loss :  2.1107988357543945 4.873814105987549 4.873814105987549
Loss :  2.128023862838745 4.535285949707031 4.535285949707031
Loss :  2.1613171100616455 4.517414569854736 4.517414569854736
Loss :  2.1136486530303955 4.419289588928223 4.419289588928223
Loss :  2.2538697719573975 4.3777875900268555 4.3777875900268555
Loss :  2.0966336727142334 4.305227756500244 4.305227756500244
Loss :  2.1552844047546387 4.449496269226074 4.449496269226074
Loss :  2.122495412826538 4.6645073890686035 4.6645073890686035
Loss :  2.2211618423461914 4.636819362640381 4.636819362640381
Loss :  2.1043903827667236 4.488749027252197 4.488749027252197
Loss :  2.1552014350891113 4.526636123657227 4.526636123657227
Loss :  2.213128089904785 4.322152137756348 4.322152137756348
  batch 60 loss: 2.213128089904785, 4.322152137756348, 4.322152137756348
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
Loss :  2.0954012870788574 4.567254543304443 4.567254543304443
Loss :  2.162108898162842 4.4277753829956055 4.4277753829956055
Loss :  2.1658730506896973 4.402082920074463 4.402082920074463
Loss :  2.1486246585845947 4.4484357833862305 4.4484357833862305
Loss :  2.1371498107910156 4.177150726318359 4.177150726318359
Loss :  2.326399564743042 4.4244208335876465 4.4244208335876465
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 4], device='cuda:0')
Loss :  2.3476920127868652 4.441629409790039 4.441629409790039
Loss :  2.5136878490448 4.338925838470459 4.338925838470459
Loss :  2.483224630355835 4.3681840896606445 4.3681840896606445
Total LOSS train 4.460130728208101 valid 4.393290042877197
CE LOSS train 2.0993502176724945 valid 0.6208061575889587
Contrastive LOSS train 4.460130728208101 valid 1.0920460224151611
EPOCH 51:
Loss :  2.2367842197418213 4.410470962524414 4.410470962524414
Loss :  2.2326996326446533 4.502694606781006 4.502694606781006
Loss :  2.143561601638794 4.3496551513671875 4.3496551513671875
Loss :  2.17574405670166 4.456691265106201 4.456691265106201
Loss :  2.2207095623016357 4.40311336517334 4.40311336517334
Loss :  2.151911497116089 4.408932685852051 4.408932685852051
Loss :  2.1058545112609863 4.544682502746582 4.544682502746582
Loss :  2.10813045501709 4.321876525878906 4.321876525878906
Loss :  2.194514751434326 4.597836017608643 4.597836017608643
Loss :  2.236574172973633 4.481051445007324 4.481051445007324
Loss :  2.2559821605682373 4.4783525466918945 4.4783525466918945
Loss :  2.2199671268463135 4.458157539367676 4.458157539367676
Loss :  2.2002460956573486 4.414186000823975 4.414186000823975
Loss :  2.134415864944458 4.491657733917236 4.491657733917236
Loss :  2.140048027038574 4.438811779022217 4.438811779022217
Loss :  2.1902592182159424 4.832062721252441 4.832062721252441
Loss :  2.2286159992218018 4.387156009674072 4.387156009674072
Loss :  2.2088820934295654 4.408729076385498 4.408729076385498
Loss :  2.1933107376098633 4.362993240356445 4.362993240356445
Loss :  2.2246816158294678 4.36618185043335 4.36618185043335
  batch 20 loss: 2.2246816158294678, 4.36618185043335, 4.36618185043335
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  2.1445908546447754 4.3388495445251465 4.3388495445251465
Loss :  2.2538325786590576 4.525489330291748 4.525489330291748
Loss :  2.2430403232574463 4.544014930725098 4.544014930725098
Loss :  2.1650338172912598 4.6772236824035645 4.6772236824035645
Loss :  2.251577377319336 4.622045040130615 4.622045040130615
Loss :  2.2723782062530518 4.51557731628418 4.51557731628418
Loss :  2.2059829235076904 4.680368900299072 4.680368900299072
Loss :  2.2084786891937256 4.370955944061279 4.370955944061279
Loss :  2.241689682006836 4.381297588348389 4.381297588348389
Loss :  2.166083812713623 4.497766494750977 4.497766494750977
Loss :  2.2720396518707275 4.489112377166748 4.489112377166748
Loss :  2.154926300048828 4.634424209594727 4.634424209594727
Loss :  2.1867856979370117 4.491837501525879 4.491837501525879
Loss :  2.1746559143066406 4.532066822052002 4.532066822052002
Loss :  2.2648353576660156 4.583784103393555 4.583784103393555
Loss :  2.2729620933532715 4.670758247375488 4.670758247375488
Loss :  2.1802427768707275 4.434520244598389 4.434520244598389
Loss :  2.175591230392456 4.437239646911621 4.437239646911621
Loss :  2.267007827758789 4.365606784820557 4.365606784820557
Loss :  2.2632293701171875 4.48168420791626 4.48168420791626
  batch 40 loss: 2.2632293701171875, 4.48168420791626, 4.48168420791626
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
Loss :  2.1349599361419678 4.381036281585693 4.381036281585693
Loss :  2.2050929069519043 4.5134782791137695 4.5134782791137695
Loss :  2.1904008388519287 4.464797019958496 4.464797019958496
Loss :  2.1985421180725098 4.610903739929199 4.610903739929199
Loss :  2.243903160095215 4.323005676269531 4.323005676269531
Loss :  2.267336130142212 4.4990715980529785 4.4990715980529785
Loss :  2.2497355937957764 4.57330322265625 4.57330322265625
Loss :  2.1892054080963135 4.822596073150635 4.822596073150635
Loss :  2.182328462600708 4.653660297393799 4.653660297393799
Loss :  2.253891944885254 4.593996524810791 4.593996524810791
Loss :  2.1890053749084473 4.4733805656433105 4.4733805656433105
Loss :  2.1876838207244873 4.41281795501709 4.41281795501709
Loss :  2.2315895557403564 4.3788161277771 4.3788161277771
Loss :  2.2322545051574707 4.526006698608398 4.526006698608398
Loss :  2.219850778579712 4.5541863441467285 4.5541863441467285
Loss :  2.1959433555603027 4.405512809753418 4.405512809753418
Loss :  2.257434129714966 4.572775363922119 4.572775363922119
Loss :  2.2625744342803955 4.539804458618164 4.539804458618164
Loss :  2.2446751594543457 4.678659439086914 4.678659439086914
Loss :  2.201552152633667 4.2929368019104 4.2929368019104
  batch 60 loss: 2.201552152633667, 4.2929368019104, 4.2929368019104
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3, 4, 5], device='cuda:0')
Loss :  2.3026678562164307 4.653092861175537 4.653092861175537
Loss :  2.1917126178741455 4.409433841705322 4.409433841705322
Loss :  2.291163682937622 4.421396255493164 4.421396255493164
Loss :  2.2907416820526123 4.427256107330322 4.427256107330322
Loss :  2.3231449127197266 4.201226234436035 4.201226234436035
Loss :  2.399482250213623 4.4438629150390625 4.4438629150390625
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3, 4, 5], device='cuda:0')
Loss :  2.566946029663086 4.388129711151123 4.388129711151123
Loss :  2.6658003330230713 4.392258644104004 4.392258644104004
Loss :  2.420619487762451 4.246403217315674 4.246403217315674
Total LOSS train 4.48881640801063 valid 4.367663621902466
CE LOSS train 2.2139268985161413 valid 0.6051548719406128
Contrastive LOSS train 4.48881640801063 valid 1.0616008043289185
EPOCH 52:
Loss :  2.278434991836548 4.34069299697876 4.34069299697876
Loss :  2.2615108489990234 4.551564693450928 4.551564693450928
Loss :  2.2875053882598877 4.375340938568115 4.375340938568115
Loss :  2.2484869956970215 4.3906168937683105 4.3906168937683105
Loss :  2.2810049057006836 4.39130973815918 4.39130973815918
Loss :  2.32771635055542 4.371370315551758 4.371370315551758
Loss :  2.287987232208252 4.733493328094482 4.733493328094482
Loss :  2.29901123046875 4.388672351837158 4.388672351837158
Loss :  2.3179771900177 4.41007137298584 4.41007137298584
Loss :  2.284058094024658 4.322357654571533 4.322357654571533
Loss :  2.3345611095428467 4.486972332000732 4.486972332000732
Loss :  2.327174425125122 4.576297283172607 4.576297283172607
Loss :  2.3514459133148193 4.22027587890625 4.22027587890625
Loss :  2.2915303707122803 4.337929725646973 4.337929725646973
Loss :  2.276477813720703 4.2150139808654785 4.2150139808654785
Loss :  2.2739760875701904 4.434324264526367 4.434324264526367
Loss :  2.3377277851104736 4.575904846191406 4.575904846191406
Loss :  2.315711498260498 4.423993110656738 4.423993110656738
Loss :  2.3143370151519775 4.232424259185791 4.232424259185791
Loss :  2.2864785194396973 4.386953830718994 4.386953830718994
  batch 20 loss: 2.2864785194396973, 4.386953830718994, 4.386953830718994
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3, 5], device='cuda:0')
Loss :  2.2907698154449463 4.4215474128723145 4.4215474128723145
Loss :  2.336989402770996 4.499722003936768 4.499722003936768
Loss :  2.3308093547821045 4.5728278160095215 4.5728278160095215
Loss :  2.308478832244873 4.481032848358154 4.481032848358154
Loss :  2.289177894592285 4.608687400817871 4.608687400817871
Loss :  2.338710069656372 4.378878593444824 4.378878593444824
Loss :  2.362610340118408 4.571019649505615 4.571019649505615
Loss :  2.304990291595459 4.26214599609375 4.26214599609375
Loss :  2.421527862548828 4.119155406951904 4.119155406951904
Loss :  2.275296926498413 4.193864822387695 4.193864822387695
Loss :  2.394052505493164 4.269135475158691 4.269135475158691
Loss :  2.259237289428711 4.2080183029174805 4.2080183029174805
Loss :  2.332412004470825 3.879034996032715 3.879034996032715
Loss :  2.34468412399292 4.445404052734375 4.445404052734375
Loss :  2.3862483501434326 3.8980607986450195 3.8980607986450195
Loss :  2.3603384494781494 4.333424091339111 4.333424091339111
Loss :  2.311032772064209 4.398226737976074 4.398226737976074
Loss :  2.24334454536438 4.446878433227539 4.446878433227539
Loss :  2.277803659439087 4.2465949058532715 4.2465949058532715
Loss :  2.2631969451904297 4.450534343719482 4.450534343719482
  batch 40 loss: 2.2631969451904297, 4.450534343719482, 4.450534343719482
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3, 4, 5], device='cuda:0')
Loss :  2.2765228748321533 4.3412909507751465 4.3412909507751465
Loss :  2.3275015354156494 4.484167098999023 4.484167098999023
Loss :  2.3130736351013184 4.341292858123779 4.341292858123779
Loss :  2.2157199382781982 4.106226444244385 4.106226444244385
Loss :  2.324591875076294 3.8009192943573 3.8009192943573
Loss :  2.3399710655212402 3.999941349029541 3.999941349029541
Loss :  2.2437214851379395 4.27425479888916 4.27425479888916
Loss :  2.2701058387756348 4.156184673309326 4.156184673309326
Loss :  2.260418176651001 4.4067063331604 4.4067063331604
Loss :  2.3163955211639404 4.09647274017334 4.09647274017334
Loss :  2.290217638015747 4.397070407867432 4.397070407867432
Loss :  2.3504836559295654 4.3531060218811035 4.3531060218811035
Loss :  2.2882368564605713 4.637665748596191 4.637665748596191
Loss :  2.324901819229126 4.569930553436279 4.569930553436279
Loss :  2.3295769691467285 4.612380027770996 4.612380027770996
Loss :  2.3188316822052 4.344977855682373 4.344977855682373
Loss :  2.361496925354004 4.42724084854126 4.42724084854126
Loss :  2.3582868576049805 4.368815898895264 4.368815898895264
Loss :  2.386605739593506 4.35605525970459 4.35605525970459
Loss :  2.221780300140381 4.31010627746582 4.31010627746582
  batch 60 loss: 2.221780300140381, 4.31010627746582, 4.31010627746582
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3, 5], device='cuda:0')
Loss :  2.4120635986328125 4.542904853820801 4.542904853820801
Loss :  2.3664097785949707 4.545213222503662 4.545213222503662
Loss :  2.423062801361084 4.317994117736816 4.317994117736816
Loss :  2.3876161575317383 4.350599765777588 4.350599765777588
Loss :  2.5065720081329346 4.12621545791626 4.12621545791626
Loss :  2.172701835632324 4.407251834869385 4.407251834869385
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3], device='cuda:0')
Loss :  2.161409378051758 4.431334495544434 4.431334495544434
Loss :  2.1410787105560303 4.367627143859863 4.367627143859863
Loss :  2.30255389213562 4.161833763122559 4.161833763122559
Total LOSS train 4.355653949884268 valid 4.34201180934906
CE LOSS train 2.3173690759218655 valid 0.575638473033905
Contrastive LOSS train 4.355653949884268 valid 1.0404584407806396
EPOCH 53:
Loss :  2.394430160522461 4.377825736999512 4.377825736999512
Loss :  2.342806100845337 4.374952793121338 4.374952793121338
Loss :  2.4816696643829346 4.261629104614258 4.261629104614258
Loss :  2.461716413497925 3.7483596801757812 3.7483596801757812
Loss :  2.460462808609009 4.1473517417907715 4.1473517417907715
Loss :  2.5473134517669678 4.043989181518555 4.043989181518555
Loss :  2.4621598720550537 4.2264909744262695 4.2264909744262695
Loss :  2.4618353843688965 3.8586297035217285 3.8586297035217285
Loss :  2.4632723331451416 3.763629198074341 3.763629198074341
Loss :  2.443031072616577 4.467926025390625 4.467926025390625
Loss :  2.491502523422241 4.57091760635376 4.57091760635376
Loss :  2.5081963539123535 4.5162034034729 4.5162034034729
Loss :  2.4606597423553467 4.5605692863464355 4.5605692863464355
Loss :  2.349292755126953 4.374405384063721 4.374405384063721
Loss :  2.353449583053589 4.415412425994873 4.415412425994873
Loss :  2.439575672149658 4.4925456047058105 4.4925456047058105
Loss :  2.4154884815216064 4.42617654800415 4.42617654800415
Loss :  2.4030842781066895 4.483858585357666 4.483858585357666
Loss :  2.4188077449798584 4.346317768096924 4.346317768096924
Loss :  2.293722629547119 4.553763389587402 4.553763389587402
  batch 20 loss: 2.293722629547119, 4.553763389587402, 4.553763389587402
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3, 4], device='cuda:0')
Loss :  2.3483340740203857 4.305648326873779 4.305648326873779
Loss :  2.466092348098755 4.332218647003174 4.332218647003174
Loss :  2.4109365940093994 4.5530900955200195 4.5530900955200195
Loss :  2.336064577102661 4.578193664550781 4.578193664550781
Loss :  2.22027850151062 4.672040939331055 4.672040939331055
Loss :  2.4281609058380127 4.3853349685668945 4.3853349685668945
Loss :  2.342742443084717 4.695947647094727 4.695947647094727
Loss :  2.4019877910614014 4.416743278503418 4.416743278503418
Loss :  2.5153160095214844 4.468656539916992 4.468656539916992
Loss :  2.2965118885040283 4.468843936920166 4.468843936920166
Loss :  2.5236856937408447 4.491808891296387 4.491808891296387
Loss :  2.3503785133361816 4.645725727081299 4.645725727081299
Loss :  2.3623580932617188 4.577904224395752 4.577904224395752
Loss :  2.4035675525665283 4.479659557342529 4.479659557342529
Loss :  2.5155365467071533 4.522221565246582 4.522221565246582
Loss :  2.445847272872925 4.468654155731201 4.468654155731201
Loss :  2.4460484981536865 4.564582824707031 4.564582824707031
Loss :  2.3205015659332275 4.449091911315918 4.449091911315918
Loss :  2.1974902153015137 4.329461097717285 4.329461097717285
Loss :  2.255695343017578 4.361569881439209 4.361569881439209
  batch 40 loss: 2.255695343017578, 4.361569881439209, 4.361569881439209
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  2.356407403945923 4.50941276550293 4.50941276550293
Loss :  2.393557071685791 4.409811973571777 4.409811973571777
Loss :  2.4507083892822266 4.347841739654541 4.347841739654541
Loss :  2.38077449798584 4.485252857208252 4.485252857208252
Loss :  2.4588229656219482 4.312716484069824 4.312716484069824
Loss :  2.4078683853149414 4.412646770477295 4.412646770477295
Loss :  2.344695568084717 4.714807033538818 4.714807033538818
Loss :  2.432506561279297 4.5620551109313965 4.5620551109313965
Loss :  2.2088916301727295 4.5170440673828125 4.5170440673828125
Loss :  2.389991044998169 4.543494701385498 4.543494701385498
Loss :  2.348370313644409 4.591056823730469 4.591056823730469
Loss :  2.239118814468384 4.378048419952393 4.378048419952393
Loss :  2.380516290664673 4.358556747436523 4.358556747436523
Loss :  2.2819366455078125 4.42337703704834 4.42337703704834
Loss :  2.4058473110198975 4.421335697174072 4.421335697174072
Loss :  2.1793053150177 4.339902400970459 4.339902400970459
Loss :  2.3983592987060547 4.35880708694458 4.35880708694458
Loss :  2.3727712631225586 4.464960098266602 4.464960098266602
Loss :  2.274102210998535 4.748611927032471 4.748611927032471
Loss :  2.1038978099823 4.4543328285217285 4.4543328285217285
  batch 60 loss: 2.1038978099823, 4.4543328285217285, 4.4543328285217285
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4], device='cuda:0')
Loss :  2.3621129989624023 4.5944108963012695 4.5944108963012695
Loss :  2.282822847366333 4.544557571411133 4.544557571411133
Loss :  2.3989310264587402 4.450984001159668 4.450984001159668
Loss :  2.405946731567383 4.4174981117248535 4.4174981117248535
Loss :  2.4229514598846436 4.064093112945557 4.064093112945557
Loss :  2.6778945922851562 4.444321155548096 4.444321155548096
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2], device='cuda:0')
Loss :  2.757349729537964 4.45871114730835 4.45871114730835
Loss :  2.703059434890747 4.351199626922607 4.351199626922607
Loss :  2.4175074100494385 4.304991245269775 4.304991245269775
Total LOSS train 4.418522589023297 valid 4.389805793762207
CE LOSS train 2.38340346629803 valid 0.6043768525123596
Contrastive LOSS train 4.418522589023297 valid 1.0762478113174438
EPOCH 54:
Loss :  2.3138041496276855 4.357722282409668 4.357722282409668
Loss :  2.211681842803955 4.738795280456543 4.738795280456543
Loss :  2.3850746154785156 4.333154678344727 4.333154678344727
Loss :  2.3000104427337646 4.454218864440918 4.454218864440918
Loss :  2.314652681350708 4.299509525299072 4.299509525299072
Loss :  2.3641152381896973 4.387527942657471 4.387527942657471
Loss :  2.330859661102295 4.458032131195068 4.458032131195068
Loss :  2.318852663040161 4.3279337882995605 4.3279337882995605
Loss :  2.3550539016723633 4.613865852355957 4.613865852355957
Loss :  2.1886744499206543 4.343634128570557 4.343634128570557
Loss :  2.327587366104126 4.529638767242432 4.529638767242432
Loss :  2.3599507808685303 4.4225616455078125 4.4225616455078125
Loss :  2.3156497478485107 4.529609680175781 4.529609680175781
Loss :  2.309934616088867 4.6569719314575195 4.6569719314575195
Loss :  2.275010108947754 4.415867805480957 4.415867805480957
Loss :  2.219935894012451 4.468570232391357 4.468570232391357
Loss :  2.339646577835083 4.541098594665527 4.541098594665527
Loss :  2.230903148651123 4.753091335296631 4.753091335296631
Loss :  2.342406988143921 4.369493007659912 4.369493007659912
Loss :  2.2090024948120117 4.346696853637695 4.346696853637695
  batch 20 loss: 2.2090024948120117, 4.346696853637695, 4.346696853637695
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  2.280780076980591 4.448634624481201 4.448634624481201
Loss :  2.3542823791503906 4.651423454284668 4.651423454284668
Loss :  2.2910449504852295 4.737220287322998 4.737220287322998
Loss :  2.289794683456421 4.444065570831299 4.444065570831299
Loss :  2.199049234390259 4.473696231842041 4.473696231842041
Loss :  2.290785789489746 4.358591079711914 4.358591079711914
Loss :  2.2218222618103027 4.598326206207275 4.598326206207275
Loss :  2.263686180114746 4.318242073059082 4.318242073059082
Loss :  2.382265329360962 4.517448902130127 4.517448902130127
Loss :  2.1636388301849365 4.476903438568115 4.476903438568115
Loss :  2.3918473720550537 4.592597007751465 4.592597007751465
Loss :  2.2430472373962402 4.72667932510376 4.72667932510376
Loss :  2.2554428577423096 4.407635688781738 4.407635688781738
Loss :  2.2810468673706055 4.55020809173584 4.55020809173584
Loss :  2.3518972396850586 4.476445198059082 4.476445198059082
Loss :  2.3801727294921875 4.6450958251953125 4.6450958251953125
Loss :  2.2857186794281006 4.383730411529541 4.383730411529541
Loss :  2.1418440341949463 4.355095386505127 4.355095386505127
Loss :  2.0574045181274414 4.3931169509887695 4.3931169509887695
Loss :  2.0959298610687256 4.450387001037598 4.450387001037598
  batch 40 loss: 2.0959298610687256, 4.450387001037598, 4.450387001037598
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 5], device='cuda:0')
Loss :  2.187382459640503 4.5692901611328125 4.5692901611328125
Loss :  2.2255592346191406 4.49778413772583 4.49778413772583
Loss :  2.301180839538574 4.388593673706055 4.388593673706055
Loss :  2.289536714553833 4.575753211975098 4.575753211975098
Loss :  2.32666015625 4.344134330749512 4.344134330749512
Loss :  2.206829309463501 4.448240280151367 4.448240280151367
Loss :  2.2521767616271973 4.557055950164795 4.557055950164795
Loss :  2.2459068298339844 4.613662242889404 4.613662242889404
Loss :  2.105647563934326 4.3472089767456055 4.3472089767456055
Loss :  2.272557020187378 4.518399238586426 4.518399238586426
Loss :  2.1959714889526367 4.5400471687316895 4.5400471687316895
Loss :  2.138120412826538 4.366164684295654 4.366164684295654
Loss :  2.2046940326690674 4.441051006317139 4.441051006317139
Loss :  2.2136683464050293 4.3896989822387695 4.3896989822387695
Loss :  2.288372039794922 4.448016166687012 4.448016166687012
Loss :  2.144691228866577 4.42521333694458 4.42521333694458
Loss :  2.2268078327178955 4.607832431793213 4.607832431793213
Loss :  2.2637906074523926 4.458016395568848 4.458016395568848
Loss :  2.2505292892456055 4.708110332489014 4.708110332489014
Loss :  2.2894437313079834 4.454347610473633 4.454347610473633
  batch 60 loss: 2.2894437313079834, 4.454347610473633, 4.454347610473633
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 3, 5], device='cuda:0')
Loss :  2.4310362339019775 4.570595741271973 4.570595741271973
Loss :  2.2597289085388184 4.438613414764404 4.438613414764404
Loss :  2.2445104122161865 4.5727715492248535 4.5727715492248535
Loss :  2.2942402362823486 4.484049320220947 4.484049320220947
Loss :  2.3029685020446777 4.100869178771973 4.100869178771973
Loss :  2.4532628059387207 4.434045791625977 4.434045791625977
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1], device='cuda:0')
Loss :  2.4308242797851562 4.4404826164245605 4.4404826164245605
Loss :  2.4646973609924316 4.36094331741333 4.36094331741333
Loss :  2.3382647037506104 4.41049337387085 4.41049337387085
Total LOSS train 4.48029317855835 valid 4.411491274833679
CE LOSS train 2.267635671909039 valid 0.5845661759376526
Contrastive LOSS train 4.48029317855835 valid 1.1026233434677124
EPOCH 55:
Loss :  2.2713677883148193 4.47016716003418 4.47016716003418
Loss :  2.178706407546997 4.575913429260254 4.575913429260254
Loss :  2.1755306720733643 4.479671001434326 4.479671001434326
Loss :  2.2866673469543457 4.483756065368652 4.483756065368652
Loss :  2.3188743591308594 4.466501235961914 4.466501235961914
Loss :  2.3086419105529785 4.3692803382873535 4.3692803382873535
Loss :  2.2879607677459717 4.553394794464111 4.553394794464111
Loss :  2.260812282562256 4.392266273498535 4.392266273498535
Loss :  2.3167953491210938 4.363020896911621 4.363020896911621
Loss :  2.2450308799743652 4.346101760864258 4.346101760864258
Loss :  2.2120814323425293 4.441009044647217 4.441009044647217
Loss :  2.1978771686553955 4.425989151000977 4.425989151000977
Loss :  2.2305774688720703 4.529202461242676 4.529202461242676
Loss :  2.220374584197998 4.541449546813965 4.541449546813965
Loss :  2.1437020301818848 4.599820137023926 4.599820137023926
Loss :  2.0675272941589355 4.4471611976623535 4.4471611976623535
Loss :  2.2744624614715576 4.445732116699219 4.445732116699219
Loss :  2.2158043384552 4.437227249145508 4.437227249145508
Loss :  2.279306173324585 4.478187561035156 4.478187561035156
Loss :  2.1759955883026123 4.612005710601807 4.612005710601807
  batch 20 loss: 2.1759955883026123, 4.612005710601807, 4.612005710601807
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 5], device='cuda:0')
Loss :  2.1574714183807373 4.325404644012451 4.325404644012451
Loss :  2.225003957748413 4.4707183837890625 4.4707183837890625
Loss :  2.1962454319000244 4.529755115509033 4.529755115509033
Loss :  2.2114076614379883 4.502699851989746 4.502699851989746
Loss :  2.0538086891174316 4.624477863311768 4.624477863311768
Loss :  2.197643280029297 4.448697566986084 4.448697566986084
Loss :  2.159423828125 4.581130027770996 4.581130027770996
Loss :  2.2077901363372803 4.4045281410217285 4.4045281410217285
Loss :  2.259958267211914 4.4309000968933105 4.4309000968933105
Loss :  2.136582136154175 4.434442043304443 4.434442043304443
Loss :  2.3104453086853027 4.555453777313232 4.555453777313232
Loss :  2.1913270950317383 4.5266642570495605 4.5266642570495605
Loss :  2.1926751136779785 4.472002983093262 4.472002983093262
Loss :  2.2386021614074707 4.428766250610352 4.428766250610352
Loss :  2.281780958175659 4.518564701080322 4.518564701080322
Loss :  2.311275005340576 4.430559158325195 4.430559158325195
Loss :  2.2304739952087402 4.426408767700195 4.426408767700195
Loss :  2.179192066192627 4.414243221282959 4.414243221282959
Loss :  2.123436450958252 4.549932956695557 4.549932956695557
Loss :  2.209473133087158 4.427908420562744 4.427908420562744
  batch 40 loss: 2.209473133087158, 4.427908420562744, 4.427908420562744
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 5], device='cuda:0')
Loss :  2.2115907669067383 4.389010906219482 4.389010906219482
Loss :  2.1552786827087402 4.381124973297119 4.381124973297119
Loss :  2.199683904647827 4.365433216094971 4.365433216094971
Loss :  2.220227003097534 4.375236511230469 4.375236511230469
Loss :  2.2375547885894775 4.274089336395264 4.274089336395264
Loss :  2.13924241065979 4.406550407409668 4.406550407409668
Loss :  2.1989798545837402 4.44572639465332 4.44572639465332
Loss :  2.1950464248657227 4.473254203796387 4.473254203796387
Loss :  2.0407536029815674 4.512085914611816 4.512085914611816
Loss :  2.1949732303619385 4.5580267906188965 4.5580267906188965
Loss :  2.1504294872283936 4.4933037757873535 4.4933037757873535
Loss :  2.0983235836029053 4.353889465332031 4.353889465332031
Loss :  2.145115852355957 4.489402770996094 4.489402770996094
Loss :  2.1773173809051514 4.365797996520996 4.365797996520996
Loss :  2.1384809017181396 4.477536201477051 4.477536201477051
Loss :  2.0370371341705322 4.349992752075195 4.349992752075195
Loss :  2.2979307174682617 4.553565502166748 4.553565502166748
Loss :  2.205115795135498 4.4872002601623535 4.4872002601623535
Loss :  2.1471445560455322 4.556488037109375 4.556488037109375
Loss :  2.1213324069976807 4.375387191772461 4.375387191772461
  batch 60 loss: 2.1213324069976807, 4.375387191772461, 4.375387191772461
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 5], device='cuda:0')
Loss :  2.1496152877807617 4.535515308380127 4.535515308380127
Loss :  2.2002511024475098 4.3873610496521 4.3873610496521
Loss :  2.133702278137207 4.395803928375244 4.395803928375244
Loss :  2.132611036300659 4.434930801391602 4.434930801391602
Loss :  2.220801591873169 4.140645980834961 4.140645980834961
Loss :  2.5827839374542236 4.436858177185059 4.436858177185059
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 5], device='cuda:0')
Loss :  2.8563594818115234 4.433393478393555 4.433393478393555
Loss :  3.2295613288879395 4.29451847076416 4.29451847076416
Loss :  2.453566551208496 4.13135290145874 4.13135290145874
Total LOSS train 4.454438077486478 valid 4.324030756950378
CE LOSS train 2.198317725841816 valid 0.613391637802124
Contrastive LOSS train 4.454438077486478 valid 1.032838225364685
EPOCH 56:
Loss :  2.0820271968841553 4.363512992858887 4.363512992858887
Loss :  2.09148907661438 4.536524772644043 4.536524772644043
Loss :  2.107473134994507 4.4538798332214355 4.4538798332214355
Loss :  2.0949249267578125 4.4711151123046875 4.4711151123046875
Loss :  2.106560468673706 4.320466995239258 4.320466995239258
Loss :  2.119816780090332 4.412564754486084 4.412564754486084
Loss :  2.0788962841033936 4.452733993530273 4.452733993530273
Loss :  2.0969791412353516 4.279839038848877 4.279839038848877
Loss :  2.0931544303894043 4.408480167388916 4.408480167388916
Loss :  2.059709072113037 4.218729019165039 4.218729019165039
Loss :  2.089055061340332 4.230466365814209 4.230466365814209
Loss :  2.0667307376861572 4.4891462326049805 4.4891462326049805
Loss :  2.067091941833496 4.394603252410889 4.394603252410889
Loss :  1.9897879362106323 4.519420146942139 4.519420146942139
Loss :  1.9926759004592896 4.417137145996094 4.417137145996094
Loss :  1.9680900573730469 4.468627452850342 4.468627452850342
Loss :  2.0105037689208984 4.253682613372803 4.253682613372803
Loss :  2.017274856567383 4.349917411804199 4.349917411804199
Loss :  2.005260467529297 4.377182483673096 4.377182483673096
Loss :  1.9666271209716797 4.451656818389893 4.451656818389893
  batch 20 loss: 1.9666271209716797, 4.451656818389893, 4.451656818389893
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([5], device='cuda:0')
Loss :  1.9514353275299072 4.297750949859619 4.297750949859619
Loss :  1.9600781202316284 4.464865684509277 4.464865684509277
Loss :  1.9727529287338257 4.542764186859131 4.542764186859131
Loss :  2.0023348331451416 4.5953145027160645 4.5953145027160645
Loss :  2.0123331546783447 4.642150402069092 4.642150402069092
Loss :  1.9893226623535156 4.668111324310303 4.668111324310303
Loss :  2.0401535034179688 4.702886581420898 4.702886581420898
Loss :  1.9790704250335693 4.3041791915893555 4.3041791915893555
Loss :  2.019865036010742 4.443456172943115 4.443456172943115
Loss :  2.001023292541504 4.4325385093688965 4.4325385093688965
Loss :  2.0123937129974365 4.534671306610107 4.534671306610107
Loss :  1.9720189571380615 4.652490139007568 4.652490139007568
Loss :  2.0025744438171387 4.581297397613525 4.581297397613525
Loss :  1.9785038232803345 4.364165782928467 4.364165782928467
Loss :  1.983129858970642 4.56686544418335 4.56686544418335
Loss :  2.0225515365600586 4.486020565032959 4.486020565032959
Loss :  1.9755812883377075 4.401275634765625 4.401275634765625
Loss :  1.96029531955719 4.3147969245910645 4.3147969245910645
Loss :  1.9365246295928955 4.526232719421387 4.526232719421387
Loss :  1.9803016185760498 4.418339729309082 4.418339729309082
  batch 40 loss: 1.9803016185760498, 4.418339729309082, 4.418339729309082
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3, 5], device='cuda:0')
Loss :  1.97704017162323 4.523698329925537 4.523698329925537
Loss :  1.9892350435256958 4.4400529861450195 4.4400529861450195
Loss :  1.937849998474121 4.360687732696533 4.360687732696533
Loss :  1.9730199575424194 4.473881721496582 4.473881721496582
Loss :  1.9527356624603271 4.298486232757568 4.298486232757568
Loss :  1.9714945554733276 4.419854640960693 4.419854640960693
Loss :  2.0247535705566406 4.495992660522461 4.495992660522461
Loss :  1.926683783531189 4.483001232147217 4.483001232147217
Loss :  1.9700615406036377 4.347027778625488 4.347027778625488
Loss :  1.9532957077026367 4.486945152282715 4.486945152282715
Loss :  1.936692476272583 4.491384029388428 4.491384029388428
Loss :  1.9516937732696533 4.393494129180908 4.393494129180908
Loss :  1.921421766281128 4.387631416320801 4.387631416320801
Loss :  2.0072176456451416 4.424880027770996 4.424880027770996
Loss :  1.9124373197555542 4.399963855743408 4.399963855743408
Loss :  1.9625056982040405 4.366803169250488 4.366803169250488
Loss :  1.9709073305130005 4.486924171447754 4.486924171447754
Loss :  1.9507778882980347 4.541700839996338 4.541700839996338
Loss :  2.000016927719116 4.414236545562744 4.414236545562744
Loss :  2.0437326431274414 4.570225715637207 4.570225715637207
  batch 60 loss: 2.0437326431274414, 4.570225715637207, 4.570225715637207
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3, 5], device='cuda:0')
Loss :  1.927986979484558 4.407118797302246 4.407118797302246
Loss :  1.942294716835022 4.478113651275635 4.478113651275635
Loss :  1.9962158203125 4.3609113693237305 4.3609113693237305
Loss :  1.931817650794983 4.3993682861328125 4.3993682861328125
Loss :  1.9956430196762085 4.198586463928223 4.198586463928223
Loss :  1.7229818105697632 4.4422173500061035 4.4422173500061035
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3, 5], device='cuda:0')
Loss :  1.73579740524292 4.402812480926514 4.402812480926514
Loss :  1.7341094017028809 4.282421588897705 4.282421588897705
Loss :  1.6788852214813232 4.197533130645752 4.197533130645752
Total LOSS train 4.4378589336688705 valid 4.3312461376190186
CE LOSS train 1.999752376629756 valid 0.4197213053703308
Contrastive LOSS train 4.4378589336688705 valid 1.049383282661438
EPOCH 57:
Loss :  1.9869147539138794 4.285491943359375 4.285491943359375
Loss :  1.9781320095062256 4.552546977996826 4.552546977996826
Loss :  1.959093689918518 4.355273246765137 4.355273246765137
Loss :  1.9439276456832886 4.410950183868408 4.410950183868408
Loss :  1.9572423696517944 4.299911975860596 4.299911975860596
Loss :  1.9519519805908203 4.322941303253174 4.322941303253174
Loss :  1.9690113067626953 4.453357219696045 4.453357219696045
Loss :  2.00347900390625 4.233156204223633 4.233156204223633
Loss :  1.9619499444961548 4.417619705200195 4.417619705200195
Loss :  1.9510090351104736 4.367303848266602 4.367303848266602
Loss :  1.9639979600906372 4.461655616760254 4.461655616760254
Loss :  1.9410521984100342 4.458977222442627 4.458977222442627
Loss :  1.9336925745010376 4.492995262145996 4.492995262145996
Loss :  1.927858829498291 4.509800910949707 4.509800910949707
Loss :  1.866400122642517 4.399729251861572 4.399729251861572
Loss :  2.053576707839966 4.463640213012695 4.463640213012695
Loss :  2.0379433631896973 4.4723968505859375 4.4723968505859375
Loss :  1.9453636407852173 4.370140075683594 4.370140075683594
Loss :  1.9142383337020874 4.321736812591553 4.321736812591553
Loss :  1.9815447330474854 4.358436584472656 4.358436584472656
  batch 20 loss: 1.9815447330474854, 4.358436584472656, 4.358436584472656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.9139050245285034 4.373768329620361 4.373768329620361
Loss :  1.8724132776260376 4.521181106567383 4.521181106567383
Loss :  1.920319676399231 4.583803176879883 4.583803176879883
Loss :  1.9066239595413208 4.430185794830322 4.430185794830322
Loss :  2.024116039276123 4.529832363128662 4.529832363128662
Loss :  1.8590753078460693 4.375014305114746 4.375014305114746
Loss :  1.833046793937683 4.5698652267456055 4.5698652267456055
Loss :  1.8503563404083252 4.399031639099121 4.399031639099121
Loss :  1.8993452787399292 4.485410690307617 4.485410690307617
Loss :  1.9838169813156128 4.455296039581299 4.455296039581299
Loss :  1.9274966716766357 4.5518798828125 4.5518798828125
Loss :  1.8505470752716064 4.403354167938232 4.403354167938232
Loss :  1.8610780239105225 4.409681797027588 4.409681797027588
Loss :  1.8052403926849365 4.458949565887451 4.458949565887451
Loss :  1.8568074703216553 4.570831298828125 4.570831298828125
Loss :  1.812080979347229 4.418493270874023 4.418493270874023
Loss :  1.8685834407806396 4.571408271789551 4.571408271789551
Loss :  1.9038631916046143 4.388352870941162 4.388352870941162
Loss :  1.8591210842132568 4.370262622833252 4.370262622833252
Loss :  1.8642083406448364 4.402703285217285 4.402703285217285
  batch 40 loss: 1.8642083406448364, 4.402703285217285, 4.402703285217285
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
Loss :  1.848152995109558 4.364701271057129 4.364701271057129
Loss :  1.9283870458602905 4.409039497375488 4.409039497375488
Loss :  1.8660130500793457 4.45531702041626 4.45531702041626
Loss :  1.9693267345428467 4.674862861633301 4.674862861633301
Loss :  1.8161756992340088 4.357948303222656 4.357948303222656
Loss :  1.8032441139221191 4.430703163146973 4.430703163146973
Loss :  1.8891522884368896 4.382406234741211 4.382406234741211
Loss :  1.7950360774993896 4.440251350402832 4.440251350402832
Loss :  1.895963191986084 4.328581809997559 4.328581809997559
Loss :  1.8106917142868042 4.427069664001465 4.427069664001465
Loss :  1.8109759092330933 4.57570219039917 4.57570219039917
Loss :  1.8326011896133423 4.289847373962402 4.289847373962402
Loss :  1.8422402143478394 4.187411308288574 4.187411308288574
Loss :  1.8072305917739868 4.501340866088867 4.501340866088867
Loss :  1.8042715787887573 4.854508399963379 4.854508399963379
Loss :  1.8459537029266357 4.156335830688477 4.156335830688477
Loss :  1.860823392868042 4.655961990356445 4.655961990356445
Loss :  1.7536691427230835 4.191153526306152 4.191153526306152
Loss :  1.8201395273208618 4.043292999267578 4.043292999267578
Loss :  1.8384819030761719 4.3599324226379395 4.3599324226379395
  batch 60 loss: 1.8384819030761719, 4.3599324226379395, 4.3599324226379395
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4, 5], device='cuda:0')
Loss :  1.8339779376983643 4.3122639656066895 4.3122639656066895
Loss :  1.837161898612976 4.171258926391602 4.171258926391602
Loss :  1.9426413774490356 4.45843505859375 4.45843505859375
Loss :  1.8241463899612427 4.613105773925781 4.613105773925781
Loss :  1.760056495666504 3.6158032417297363 3.6158032417297363
Loss :  4.551916122436523 4.4517903327941895 4.4517903327941895
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
Loss :  4.504385948181152 4.4867682456970215 4.4867682456970215
Loss :  4.424449443817139 4.259726524353027 4.259726524353027
Loss :  4.594183444976807 4.3633341789245605 4.3633341789245605
Total LOSS train 4.408224648695725 valid 4.3904048204422
CE LOSS train 1.8913375340975247 valid 1.1485458612442017
Contrastive LOSS train 4.408224648695725 valid 1.0908335447311401
EPOCH 58:
Loss :  1.7937297821044922 3.7729568481445312 3.7729568481445312
Loss :  1.8073641061782837 4.10360050201416 4.10360050201416
Loss :  1.8236321210861206 4.3593974113464355 4.3593974113464355
Loss :  1.7857697010040283 4.205257892608643 4.205257892608643
Loss :  1.8721792697906494 4.218411922454834 4.218411922454834
Loss :  1.7500786781311035 4.014835357666016 4.014835357666016
Loss :  1.7994180917739868 4.17954683303833 4.17954683303833
Loss :  1.6837081909179688 4.013349533081055 4.013349533081055
Loss :  1.7652660608291626 4.068638801574707 4.068638801574707
Loss :  1.8510935306549072 4.521103382110596 4.521103382110596
Loss :  1.7583729028701782 4.364723205566406 4.364723205566406
Loss :  1.756186842918396 4.39362907409668 4.39362907409668
Loss :  1.7990350723266602 4.244753837585449 4.244753837585449
Loss :  1.7839933633804321 4.265385150909424 4.265385150909424
Loss :  1.7651411294937134 4.13565731048584 4.13565731048584
Loss :  1.9156430959701538 4.297479152679443 4.297479152679443
Loss :  1.8482427597045898 4.047515392303467 4.047515392303467
Loss :  1.8221112489700317 3.990184783935547 3.990184783935547
Loss :  1.8007915019989014 3.9599757194519043 3.9599757194519043
Loss :  1.8823506832122803 3.916651725769043 3.916651725769043
  batch 20 loss: 1.8823506832122803, 3.916651725769043, 3.916651725769043
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.787462830543518 4.0498857498168945 4.0498857498168945
Loss :  1.7452853918075562 3.8536524772644043 3.8536524772644043
Loss :  1.851516604423523 3.9889705181121826 3.9889705181121826
Loss :  1.8099846839904785 4.171412467956543 4.171412467956543
Loss :  1.8517372608184814 4.19636344909668 4.19636344909668
Loss :  1.8112657070159912 4.093588829040527 4.093588829040527
Loss :  1.8971757888793945 4.4240336418151855 4.4240336418151855
Loss :  1.83417546749115 4.021899700164795 4.021899700164795
Loss :  1.8283799886703491 3.736278772354126 3.736278772354126
Loss :  1.9172608852386475 3.702692747116089 3.702692747116089
Loss :  1.8433034420013428 3.6131982803344727 3.6131982803344727
Loss :  1.7921538352966309 4.159196853637695 4.159196853637695
Loss :  1.8670146465301514 3.4818456172943115 3.4818456172943115
Loss :  1.8649017810821533 4.064238548278809 4.064238548278809
Loss :  1.8400918245315552 3.7149367332458496 3.7149367332458496
Loss :  1.848396897315979 4.105528354644775 4.105528354644775
Loss :  1.772179126739502 3.8982787132263184 3.8982787132263184
Loss :  1.8133890628814697 3.952103614807129 3.952103614807129
Loss :  1.8382817506790161 4.01882791519165 4.01882791519165
Loss :  1.7664700746536255 4.338499069213867 4.338499069213867
  batch 40 loss: 1.7664700746536255, 4.338499069213867, 4.338499069213867
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7597887516021729 3.82436203956604 3.82436203956604
Loss :  1.775577187538147 3.6274282932281494 3.6274282932281494
Loss :  1.754902720451355 3.746617555618286 3.746617555618286
Loss :  1.7771283388137817 3.6428067684173584 3.6428067684173584
Loss :  1.760115385055542 3.503282070159912 3.503282070159912
Loss :  1.8077908754348755 3.654083490371704 3.654083490371704
Loss :  1.8687478303909302 3.977099895477295 3.977099895477295
Loss :  1.7820295095443726 3.8103864192962646 3.8103864192962646
Loss :  1.849879264831543 3.676528215408325 3.676528215408325
Loss :  1.7722300291061401 3.6364896297454834 3.6364896297454834
Loss :  1.7980433702468872 3.909397602081299 3.909397602081299
Loss :  1.8423645496368408 3.6776657104492188 3.6776657104492188
Loss :  1.833357810974121 3.5100128650665283 3.5100128650665283
Loss :  1.8439605236053467 3.4492061138153076 3.4492061138153076
Loss :  1.7757951021194458 3.766611337661743 3.766611337661743
Loss :  1.9313513040542603 4.453459739685059 4.453459739685059
Loss :  1.8807897567749023 3.6305596828460693 3.6305596828460693
Loss :  1.812333106994629 3.714803457260132 3.714803457260132
Loss :  1.9315317869186401 3.8917696475982666 3.8917696475982666
Loss :  1.9429177045822144 3.532597303390503 3.532597303390503
  batch 60 loss: 1.9429177045822144, 3.532597303390503, 3.532597303390503
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8821626901626587 3.6161768436431885 3.6161768436431885
Loss :  1.8904949426651 3.4462807178497314 3.4462807178497314
Loss :  1.8657147884368896 3.5770487785339355 3.5770487785339355
Loss :  1.8711787462234497 3.8962318897247314 3.8962318897247314
Loss :  1.9143718481063843 3.839052438735962 3.839052438735962
Loss :  1.7933411598205566 4.100895881652832 4.100895881652832
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.754309058189392 3.97298526763916 3.97298526763916
Loss :  1.8508799076080322 4.189519882202148 4.189519882202148
Loss :  1.8922173976898193 3.8128035068511963 3.8128035068511963
Total LOSS train 3.9333606830010046 valid 4.019051134586334
CE LOSS train 1.8241398939719566 valid 0.47305434942245483
Contrastive LOSS train 3.9333606830010046 valid 0.9532008767127991
Saved best model. Old loss 4.2553242444992065 and new best loss 4.019051134586334
EPOCH 59:
Loss :  1.9435811042785645 4.402741432189941 4.402741432189941
Loss :  1.9687230587005615 4.114074230194092 4.114074230194092
Loss :  1.9667763710021973 4.197333812713623 4.197333812713623
Loss :  1.864107370376587 3.9661056995391846 3.9661056995391846
Loss :  1.8299702405929565 3.9881272315979004 3.9881272315979004
Loss :  1.8947075605392456 4.090853214263916 4.090853214263916
Loss :  1.8059735298156738 3.983961343765259 3.983961343765259
Loss :  1.810049295425415 3.6270601749420166 3.6270601749420166
Loss :  1.85747230052948 3.6335597038269043 3.6335597038269043
Loss :  1.872607946395874 4.2675018310546875 4.2675018310546875
Loss :  1.7891645431518555 4.203033924102783 4.203033924102783
Loss :  1.808774709701538 3.7197229862213135 3.7197229862213135
Loss :  1.7685490846633911 3.7915360927581787 3.7915360927581787
Loss :  1.8171420097351074 4.196484565734863 4.196484565734863
Loss :  1.8652230501174927 4.2828497886657715 4.2828497886657715
Loss :  1.8610824346542358 4.185445785522461 4.185445785522461
Loss :  1.813071608543396 4.0063276290893555 4.0063276290893555
Loss :  1.7923308610916138 3.6727335453033447 3.6727335453033447
Loss :  1.734358549118042 3.3524527549743652 3.3524527549743652
Loss :  1.8057283163070679 3.6752476692199707 3.6752476692199707
  batch 20 loss: 1.8057283163070679, 3.6752476692199707, 3.6752476692199707
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.7743470668792725 3.9378206729888916 3.9378206729888916
Loss :  1.8672091960906982 3.918039083480835 3.918039083480835
Loss :  1.8650727272033691 4.1767497062683105 4.1767497062683105
Loss :  1.8850603103637695 4.163116455078125 4.163116455078125
Loss :  1.9199286699295044 4.160392761230469 4.160392761230469
Loss :  1.918192744255066 4.3459553718566895 4.3459553718566895
Loss :  1.964715600013733 4.425558090209961 4.425558090209961
Loss :  1.9008373022079468 3.9886481761932373 3.9886481761932373
Loss :  1.8757376670837402 3.7254841327667236 3.7254841327667236
Loss :  1.8974136114120483 3.6800827980041504 3.6800827980041504
Loss :  1.775432825088501 3.898435354232788 3.898435354232788
Loss :  1.7710607051849365 4.289028644561768 4.289028644561768
Loss :  1.7503865957260132 3.8956141471862793 3.8956141471862793
Loss :  1.74166738986969 3.900416135787964 3.900416135787964
Loss :  1.7210301160812378 3.865523338317871 3.865523338317871
Loss :  1.7235028743743896 3.7179253101348877 3.7179253101348877
Loss :  1.69956636428833 4.22511100769043 4.22511100769043
Loss :  1.7667814493179321 4.273407459259033 4.273407459259033
Loss :  1.7759346961975098 4.082942485809326 4.082942485809326
Loss :  1.7890976667404175 4.030301570892334 4.030301570892334
  batch 40 loss: 1.7890976667404175, 4.030301570892334, 4.030301570892334
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.7558859586715698 3.886286497116089 3.886286497116089
Loss :  1.7411489486694336 3.4851396083831787 3.4851396083831787
Loss :  1.692765235900879 3.878601312637329 3.878601312637329
Loss :  1.7365424633026123 3.601710557937622 3.601710557937622
Loss :  1.6896259784698486 3.5552825927734375 3.5552825927734375
Loss :  1.7138878107070923 3.829514503479004 3.829514503479004
Loss :  1.798643946647644 4.109683036804199 4.109683036804199
Loss :  1.6716073751449585 3.7876510620117188 3.7876510620117188
Loss :  1.7822569608688354 3.9097955226898193 3.9097955226898193
Loss :  1.6982645988464355 3.6926369667053223 3.6926369667053223
Loss :  1.7038823366165161 3.6442480087280273 3.6442480087280273
Loss :  1.754733920097351 3.6008992195129395 3.6008992195129395
Loss :  1.7358371019363403 3.434213399887085 3.434213399887085
Loss :  1.7585854530334473 3.4695231914520264 3.4695231914520264
Loss :  1.642699122428894 3.550211191177368 3.550211191177368
Loss :  1.8171573877334595 3.7628793716430664 3.7628793716430664
Loss :  1.743147850036621 3.874009609222412 3.874009609222412
Loss :  1.6776540279388428 3.6242282390594482 3.6242282390594482
Loss :  1.7808562517166138 3.691295623779297 3.691295623779297
Loss :  1.850003719329834 3.9848501682281494 3.9848501682281494
  batch 60 loss: 1.850003719329834, 3.9848501682281494, 3.9848501682281494
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.788412094116211 4.104344844818115 4.104344844818115
Loss :  1.7874975204467773 3.819298505783081 3.819298505783081
Loss :  1.7927554845809937 3.7348616123199463 3.7348616123199463
Loss :  1.7210915088653564 3.868887424468994 3.868887424468994
Loss :  1.6875079870224 3.4271042346954346 3.4271042346954346
Loss :  1.7530397176742554 3.9877281188964844 3.9877281188964844
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.7304623126983643 4.162991523742676 4.162991523742676
Loss :  1.724216103553772 3.9693946838378906 3.9693946838378906
Loss :  1.8169130086898804 3.9072823524475098 3.9072823524475098
Total LOSS train 3.8982286526606633 valid 4.00684916973114
CE LOSS train 1.7965357010181133 valid 0.4542282521724701
Contrastive LOSS train 3.8982286526606633 valid 0.9768205881118774
Saved best model. Old loss 4.019051134586334 and new best loss 4.00684916973114
EPOCH 60:
Loss :  1.785470962524414 3.6594150066375732 3.6594150066375732
Loss :  1.8161053657531738 3.89467716217041 3.89467716217041
Loss :  1.789193034172058 3.5248095989227295 3.5248095989227295
Loss :  1.7872323989868164 3.4266514778137207 3.4266514778137207
Loss :  1.8199183940887451 3.766059398651123 3.766059398651123
Loss :  1.8216949701309204 3.50264310836792 3.50264310836792
Loss :  1.7901675701141357 3.7772927284240723 3.7772927284240723
Loss :  1.7558449506759644 3.5289390087127686 3.5289390087127686
Loss :  1.7277052402496338 3.8786911964416504 3.8786911964416504
Loss :  1.7765319347381592 3.376508951187134 3.376508951187134
Loss :  1.763381004333496 3.7802011966705322 3.7802011966705322
Loss :  1.7573137283325195 3.5717949867248535 3.5717949867248535
Loss :  1.754492163658142 3.5296027660369873 3.5296027660369873
Loss :  1.715362310409546 3.4375741481781006 3.4375741481781006
Loss :  1.7579265832901 3.914050340652466 3.914050340652466
Loss :  1.8196746110916138 4.007852077484131 4.007852077484131
Loss :  1.74062979221344 3.5973832607269287 3.5973832607269287
Loss :  1.7860100269317627 3.41582989692688 3.41582989692688
Loss :  1.7093358039855957 3.0515570640563965 3.0515570640563965
Loss :  1.8199135065078735 3.6476056575775146 3.6476056575775146
  batch 20 loss: 1.8199135065078735, 3.6476056575775146, 3.6476056575775146
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.7480047941207886 3.3840065002441406 3.3840065002441406
Loss :  1.7450112104415894 3.7273125648498535 3.7273125648498535
Loss :  1.7700268030166626 3.629962682723999 3.629962682723999
Loss :  1.7725375890731812 3.6294233798980713 3.6294233798980713
Loss :  1.8317747116088867 3.668613910675049 3.668613910675049
Loss :  1.7566723823547363 3.410613536834717 3.410613536834717
Loss :  1.8250716924667358 3.7414724826812744 3.7414724826812744
Loss :  1.7484872341156006 3.394535779953003 3.394535779953003
Loss :  1.7373552322387695 3.540309190750122 3.540309190750122
Loss :  1.7852439880371094 3.5276174545288086 3.5276174545288086
Loss :  1.71330726146698 3.575484275817871 3.575484275817871
Loss :  1.7487388849258423 3.6411402225494385 3.6411402225494385
Loss :  1.7385376691818237 3.364712953567505 3.364712953567505
Loss :  1.7448861598968506 3.5687899589538574 3.5687899589538574
Loss :  1.7095661163330078 3.6579675674438477 3.6579675674438477
Loss :  1.7200489044189453 3.4014902114868164 3.4014902114868164
Loss :  1.7094910144805908 3.498711109161377 3.498711109161377
Loss :  1.7752478122711182 3.7071995735168457 3.7071995735168457
Loss :  1.8007488250732422 3.940734386444092 3.940734386444092
Loss :  1.8166464567184448 3.9289636611938477 3.9289636611938477
  batch 40 loss: 1.8166464567184448, 3.9289636611938477, 3.9289636611938477
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.7745013236999512 3.4938228130340576 3.4938228130340576
Loss :  1.7566635608673096 3.6068508625030518 3.6068508625030518
Loss :  1.695612907409668 3.6660356521606445 3.6660356521606445
Loss :  1.7606887817382812 3.4547955989837646 3.4547955989837646
Loss :  1.7375985383987427 3.7930967807769775 3.7930967807769775
Loss :  1.7961729764938354 3.6849286556243896 3.6849286556243896
Loss :  1.8732599020004272 3.669062376022339 3.669062376022339
Loss :  1.7119232416152954 3.452199697494507 3.452199697494507
Loss :  1.8288358449935913 3.5090279579162598 3.5090279579162598
Loss :  1.7485183477401733 3.6002180576324463 3.6002180576324463
Loss :  1.740304708480835 3.554755926132202 3.554755926132202
Loss :  1.8037410974502563 3.588632345199585 3.588632345199585
Loss :  1.7784010171890259 3.3887085914611816 3.3887085914611816
Loss :  1.7995631694793701 3.5104730129241943 3.5104730129241943
Loss :  1.690250039100647 3.4923064708709717 3.4923064708709717
Loss :  1.8537144660949707 3.463935375213623 3.463935375213623
Loss :  1.7822009325027466 3.4309325218200684 3.4309325218200684
Loss :  1.7166190147399902 3.6883881092071533 3.6883881092071533
Loss :  1.793953776359558 3.9131548404693604 3.9131548404693604
Loss :  1.8445132970809937 3.487495183944702 3.487495183944702
  batch 60 loss: 1.8445132970809937, 3.487495183944702, 3.487495183944702
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.7701972723007202 3.443009376525879 3.443009376525879
Loss :  1.7789779901504517 3.932651996612549 3.932651996612549
Loss :  1.7910380363464355 3.2583625316619873 3.2583625316619873
Loss :  1.7329809665679932 3.677811622619629 3.677811622619629
Loss :  1.7144088745117188 3.0475659370422363 3.0475659370422363
Loss :  1.7201297283172607 3.605926513671875 3.605926513671875
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.7118721008300781 3.7607624530792236 3.7607624530792236
Loss :  1.7135624885559082 3.4685873985290527 3.4685873985290527
Loss :  1.7956757545471191 3.374319314956665 3.374319314956665
Total LOSS train 3.58517573429988 valid 3.552398920059204
CE LOSS train 1.7687069104268 valid 0.4489189386367798
Contrastive LOSS train 3.58517573429988 valid 0.8435798287391663
Saved best model. Old loss 4.00684916973114 and new best loss 3.552398920059204
EPOCH 61:
Loss :  1.7941341400146484 3.1569221019744873 3.1569221019744873
Loss :  1.8264939785003662 3.2880749702453613 3.2880749702453613
Loss :  1.7698503732681274 3.4193484783172607 3.4193484783172607
Loss :  1.7543247938156128 3.2848007678985596 3.2848007678985596
Loss :  1.7933803796768188 3.4301092624664307 3.4301092624664307
Loss :  1.7862175703048706 3.395488739013672 3.395488739013672
Loss :  1.7821673154830933 3.942720651626587 3.942720651626587
Loss :  1.7555944919586182 3.14313006401062 3.14313006401062
Loss :  1.737016201019287 3.0515992641448975 3.0515992641448975
Loss :  1.7915898561477661 3.2007899284362793 3.2007899284362793
Loss :  1.7816940546035767 3.531275510787964 3.531275510787964
Loss :  1.7564929723739624 3.441795825958252 3.441795825958252
Loss :  1.7626914978027344 3.282545804977417 3.282545804977417
Loss :  1.7200851440429688 3.651385545730591 3.651385545730591
Loss :  1.7881522178649902 3.7321128845214844 3.7321128845214844
Loss :  1.8545758724212646 3.6480090618133545 3.6480090618133545
Loss :  1.7449949979782104 3.802051305770874 3.802051305770874
Loss :  1.765669584274292 3.94649338722229 3.94649338722229
Loss :  1.693007230758667 3.58978271484375 3.58978271484375
Loss :  1.802358627319336 3.61342716217041 3.61342716217041
  batch 20 loss: 1.802358627319336, 3.61342716217041, 3.61342716217041
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.7116068601608276 3.381443500518799 3.381443500518799
Loss :  1.695597529411316 3.5866994857788086 3.5866994857788086
Loss :  1.73147451877594 4.103405952453613 4.103405952453613
Loss :  1.7364742755889893 3.541560649871826 3.541560649871826
Loss :  1.7908700704574585 4.1674275398254395 4.1674275398254395
Loss :  1.7194515466690063 3.9876222610473633 3.9876222610473633
Loss :  1.7727502584457397 3.9289157390594482 3.9289157390594482
Loss :  1.6885632276535034 3.405186891555786 3.405186891555786
Loss :  1.7044966220855713 4.238982677459717 4.238982677459717
Loss :  1.7678478956222534 4.468542575836182 4.468542575836182
Loss :  1.7395637035369873 4.402955532073975 4.402955532073975
Loss :  1.8308963775634766 4.428120136260986 4.428120136260986
Loss :  1.7952955961227417 3.887500524520874 3.887500524520874
Loss :  1.749544620513916 4.1492156982421875 4.1492156982421875
Loss :  1.7099969387054443 4.275015354156494 4.275015354156494
Loss :  1.7287993431091309 4.429335594177246 4.429335594177246
Loss :  1.728327989578247 4.000571250915527 4.000571250915527
Loss :  1.7538198232650757 4.49791145324707 4.49791145324707
Loss :  1.8192278146743774 4.339384078979492 4.339384078979492
Loss :  1.814477562904358 4.316324234008789 4.316324234008789
  batch 40 loss: 1.814477562904358, 4.316324234008789, 4.316324234008789
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.7945551872253418 4.399196147918701 4.399196147918701
Loss :  1.7589384317398071 4.36870813369751 4.36870813369751
Loss :  1.6909438371658325 4.411712646484375 4.411712646484375
Loss :  1.8966037034988403 4.334198474884033 4.334198474884033
Loss :  1.8054252862930298 4.178886890411377 4.178886890411377
Loss :  1.816888451576233 4.35556173324585 4.35556173324585
Loss :  1.92875075340271 4.597311496734619 4.597311496734619
Loss :  1.8168480396270752 4.393065929412842 4.393065929412842
Loss :  1.8798614740371704 4.353781223297119 4.353781223297119
Loss :  1.8337494134902954 4.596864223480225 4.596864223480225
Loss :  1.8362613916397095 4.38186502456665 4.38186502456665
Loss :  1.8644955158233643 4.238467693328857 4.238467693328857
Loss :  1.8297367095947266 4.141690731048584 4.141690731048584
Loss :  1.8311508893966675 4.218358516693115 4.218358516693115
Loss :  1.8104121685028076 4.398096561431885 4.398096561431885
Loss :  1.8472583293914795 4.578339099884033 4.578339099884033
Loss :  1.816510558128357 4.5197248458862305 4.5197248458862305
Loss :  1.767823338508606 4.530062675476074 4.530062675476074
Loss :  1.7281094789505005 4.6170854568481445 4.6170854568481445
Loss :  1.8641290664672852 4.478082656860352 4.478082656860352
  batch 60 loss: 1.8641290664672852, 4.478082656860352, 4.478082656860352
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.819661259651184 4.592087745666504 4.592087745666504
Loss :  1.9210200309753418 4.3730788230896 4.3730788230896
Loss :  1.788322925567627 4.494513034820557 4.494513034820557
Loss :  1.7714898586273193 4.568850040435791 4.568850040435791
Loss :  1.7423253059387207 4.190290451049805 4.190290451049805
Loss :  1.3462409973144531 4.452976703643799 4.452976703643799
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.345947504043579 4.510049819946289 4.510049819946289
Loss :  1.3712522983551025 4.306931972503662 4.306931972503662
Loss :  1.3365707397460938 4.311456680297852 4.311456680297852
Total LOSS train 4.0215056125934305 valid 4.3953537940979
CE LOSS train 1.7832437735337479 valid 0.33414268493652344
Contrastive LOSS train 4.0215056125934305 valid 1.077864170074463
EPOCH 62:
Loss :  1.8626765012741089 4.4186577796936035 4.4186577796936035
Loss :  1.8754366636276245 4.42974853515625 4.42974853515625
Loss :  1.8021858930587769 4.395212173461914 4.395212173461914
Loss :  1.8204501867294312 4.384485721588135 4.384485721588135
Loss :  1.9408326148986816 4.31207799911499 4.31207799911499
Loss :  1.8059176206588745 4.396727561950684 4.396727561950684
Loss :  1.8715649843215942 4.546803951263428 4.546803951263428
Loss :  1.7331002950668335 4.31049108505249 4.31049108505249
Loss :  1.7208131551742554 4.4683308601379395 4.4683308601379395
Loss :  1.737134575843811 4.311511516571045 4.311511516571045
Loss :  1.6591296195983887 4.6125030517578125 4.6125030517578125
Loss :  1.727539300918579 4.3910369873046875 4.3910369873046875
Loss :  1.7542455196380615 4.578610897064209 4.578610897064209
Loss :  1.6273072957992554 4.545504093170166 4.545504093170166
Loss :  1.7672373056411743 4.393701076507568 4.393701076507568
Loss :  1.8872480392456055 4.4759416580200195 4.4759416580200195
Loss :  1.7340993881225586 4.49517297744751 4.49517297744751
Loss :  1.7076665163040161 4.560873031616211 4.560873031616211
Loss :  1.6739132404327393 4.42344856262207 4.42344856262207
Loss :  1.7133351564407349 4.514449119567871 4.514449119567871
  batch 20 loss: 1.7133351564407349, 4.514449119567871, 4.514449119567871
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4, 5], device='cuda:0')
Loss :  1.7323393821716309 4.374205589294434 4.374205589294434
Loss :  1.6583664417266846 4.509461879730225 4.509461879730225
Loss :  1.733476996421814 4.5019731521606445 4.5019731521606445
Loss :  1.7654560804367065 4.33215856552124 4.33215856552124
Loss :  1.7548270225524902 4.678117275238037 4.678117275238037
Loss :  1.6806248426437378 4.410758972167969 4.410758972167969
Loss :  1.7450429201126099 4.5947394371032715 4.5947394371032715
Loss :  1.678946852684021 4.393491744995117 4.393491744995117
Loss :  1.6473324298858643 4.452162742614746 4.452162742614746
Loss :  1.7826334238052368 4.600117206573486 4.600117206573486
Loss :  1.653032660484314 4.342386245727539 4.342386245727539
Loss :  1.6396338939666748 4.465803146362305 4.465803146362305
Loss :  1.6359373331069946 4.2739787101745605 4.2739787101745605
Loss :  1.6385886669158936 4.399134159088135 4.399134159088135
Loss :  1.599251627922058 4.0465288162231445 4.0465288162231445
Loss :  1.6123381853103638 4.179664134979248 4.179664134979248
Loss :  1.5715912580490112 4.11118221282959 4.11118221282959
Loss :  1.6091138124465942 4.124046325683594 4.124046325683594
Loss :  1.6786197423934937 4.057481288909912 4.057481288909912
Loss :  1.6907166242599487 4.049870014190674 4.049870014190674
  batch 40 loss: 1.6907166242599487, 4.049870014190674, 4.049870014190674
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.5875921249389648 4.380626201629639 4.380626201629639
Loss :  1.645410418510437 4.103273391723633 4.103273391723633
Loss :  1.587360143661499 4.098835468292236 4.098835468292236
Loss :  1.6412632465362549 4.153448581695557 4.153448581695557
Loss :  1.5521126985549927 4.1557183265686035 4.1557183265686035
Loss :  1.6128674745559692 4.0588483810424805 4.0588483810424805
Loss :  1.7120085954666138 4.103318691253662 4.103318691253662
Loss :  1.5819345712661743 3.9279448986053467 3.9279448986053467
Loss :  1.7108433246612549 4.230950355529785 4.230950355529785
Loss :  1.6768170595169067 4.09963846206665 4.09963846206665
Loss :  1.696981430053711 4.737524509429932 4.737524509429932
Loss :  1.6971553564071655 4.4233479499816895 4.4233479499816895
Loss :  1.651461124420166 4.4036431312561035 4.4036431312561035
Loss :  1.6889922618865967 4.203598499298096 4.203598499298096
Loss :  1.5742591619491577 4.163515090942383 4.163515090942383
Loss :  1.805465817451477 4.318611145019531 4.318611145019531
Loss :  1.6644915342330933 4.538665294647217 4.538665294647217
Loss :  1.5610679388046265 4.394619941711426 4.394619941711426
Loss :  1.6655588150024414 4.455545902252197 4.455545902252197
Loss :  1.8247287273406982 4.372475624084473 4.372475624084473
  batch 60 loss: 1.8247287273406982, 4.372475624084473, 4.372475624084473
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.631844401359558 4.266366004943848 4.266366004943848
Loss :  1.601614236831665 4.125086307525635 4.125086307525635
Loss :  1.5554721355438232 4.454167366027832 4.454167366027832
Loss :  1.5091285705566406 4.385239124298096 4.385239124298096
Loss :  1.456661581993103 4.263866424560547 4.263866424560547
Loss :  1.677902340888977 4.407773494720459 4.407773494720459
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
Loss :  1.6673638820648193 4.526412487030029 4.526412487030029
Loss :  1.6311887502670288 4.251236915588379 4.251236915588379
Loss :  1.7746667861938477 4.2077226638793945 4.2077226638793945
Total LOSS train 4.348945005123432 valid 4.348286390304565
CE LOSS train 1.6895814895629884 valid 0.4436666965484619
Contrastive LOSS train 4.348945005123432 valid 1.0519306659698486
EPOCH 63:
Loss :  1.5520373582839966 4.325798988342285 4.325798988342285
Loss :  1.627705693244934 4.524788856506348 4.524788856506348
Loss :  1.590250849723816 4.262855052947998 4.262855052947998
Loss :  1.6008124351501465 4.314394950866699 4.314394950866699
Loss :  1.6649199724197388 4.360772132873535 4.360772132873535
Loss :  1.561751365661621 4.237894058227539 4.237894058227539
Loss :  1.6366950273513794 4.394381999969482 4.394381999969482
Loss :  1.582497239112854 4.419615268707275 4.419615268707275
Loss :  1.573532223701477 4.361692428588867 4.361692428588867
Loss :  1.6139016151428223 3.849083662033081 3.849083662033081
Loss :  1.5642870664596558 4.098077297210693 4.098077297210693
Loss :  1.5574229955673218 4.210425853729248 4.210425853729248
Loss :  1.5457146167755127 4.403622627258301 4.403622627258301
Loss :  1.5177491903305054 4.330505847930908 4.330505847930908
Loss :  1.6606569290161133 4.142901420593262 4.142901420593262
Loss :  1.6957451105117798 4.305238246917725 4.305238246917725
Loss :  1.5655776262283325 4.134560585021973 4.134560585021973
Loss :  1.6700698137283325 4.368330955505371 4.368330955505371
Loss :  1.544559121131897 3.9889492988586426 3.9889492988586426
Loss :  1.6620855331420898 4.107359409332275 4.107359409332275
  batch 20 loss: 1.6620855331420898, 4.107359409332275, 4.107359409332275
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.5498439073562622 4.024697780609131 4.024697780609131
Loss :  1.527310848236084 4.129049777984619 4.129049777984619
Loss :  1.5513132810592651 4.14176607131958 4.14176607131958
Loss :  1.5904651880264282 4.076812744140625 4.076812744140625
Loss :  1.650645136833191 4.310431957244873 4.310431957244873
Loss :  1.568812370300293 3.8844516277313232 3.8844516277313232
Loss :  1.6305043697357178 4.16612434387207 4.16612434387207
Loss :  1.578259825706482 3.9449427127838135 3.9449427127838135
Loss :  1.4998911619186401 4.1830549240112305 4.1830549240112305
Loss :  1.6578805446624756 3.91867733001709 3.91867733001709
Loss :  1.5351816415786743 4.185863494873047 4.185863494873047
Loss :  1.6617921590805054 4.378505229949951 4.378505229949951
Loss :  1.5746192932128906 4.113142490386963 4.113142490386963
Loss :  1.557151436805725 3.987967014312744 3.987967014312744
Loss :  1.525235891342163 3.870903253555298 3.870903253555298
Loss :  1.5373620986938477 4.18459939956665 4.18459939956665
Loss :  1.5234484672546387 3.9063613414764404 3.9063613414764404
Loss :  1.6748062372207642 3.8989405632019043 3.8989405632019043
Loss :  1.666251301765442 3.821585178375244 3.821585178375244
Loss :  1.6904022693634033 3.80782151222229 3.80782151222229
  batch 40 loss: 1.6904022693634033, 3.80782151222229, 3.80782151222229
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.6228277683258057 3.8763809204101562 3.8763809204101562
Loss :  1.6312497854232788 4.053554534912109 4.053554534912109
Loss :  1.5698753595352173 3.8176257610321045 3.8176257610321045
Loss :  1.6651521921157837 3.897425413131714 3.897425413131714
Loss :  1.619779109954834 3.734457492828369 3.734457492828369
Loss :  1.6694077253341675 3.666673421859741 3.666673421859741
Loss :  1.7558388710021973 3.742436647415161 3.742436647415161
Loss :  1.6468865871429443 3.6132192611694336 3.6132192611694336
Loss :  1.7614115476608276 3.731025218963623 3.731025218963623
Loss :  1.7760629653930664 4.585484027862549 4.585484027862549
Loss :  1.6890171766281128 3.7969586849212646 3.7969586849212646
Loss :  1.7580668926239014 3.9640614986419678 3.9640614986419678
Loss :  1.7363759279251099 3.9482572078704834 3.9482572078704834
Loss :  1.7837450504302979 3.748835325241089 3.748835325241089
Loss :  1.64219331741333 3.813775062561035 3.813775062561035
Loss :  1.812938928604126 3.89663028717041 3.89663028717041
Loss :  1.7137844562530518 3.8309593200683594 3.8309593200683594
Loss :  1.6370415687561035 4.01782751083374 4.01782751083374
Loss :  1.7209786176681519 4.239472389221191 4.239472389221191
Loss :  1.803176760673523 3.757934808731079 3.757934808731079
  batch 60 loss: 1.803176760673523, 3.757934808731079, 3.757934808731079
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.708026647567749 3.484217405319214 3.484217405319214
Loss :  1.7157853841781616 3.3639450073242188 3.3639450073242188
Loss :  1.7372957468032837 3.7620837688446045 3.7620837688446045
Loss :  1.6393903493881226 3.4165985584259033 3.4165985584259033
Loss :  1.6319366693496704 3.132209300994873 3.132209300994873
Loss :  1.7497345209121704 4.246037483215332 4.246037483215332
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.7179179191589355 4.202446460723877 4.202446460723877
Loss :  1.717570424079895 4.1123046875 4.1123046875
Loss :  1.851912498474121 4.24536657333374 4.24536657333374
Total LOSS train 4.014907638843243 valid 4.201538801193237
CE LOSS train 1.6336522249075083 valid 0.4629781246185303
Contrastive LOSS train 4.014907638843243 valid 1.061341643333435
EPOCH 64:
Loss :  1.7601786851882935 3.2270944118499756 3.2270944118499756
Loss :  1.7907240390777588 3.903209686279297 3.903209686279297
Loss :  1.7119758129119873 3.267544746398926 3.267544746398926
Loss :  1.7293165922164917 3.502223253250122 3.502223253250122
Loss :  1.7998583316802979 4.112983226776123 4.112983226776123
Loss :  1.7683783769607544 4.333007335662842 4.333007335662842
Loss :  1.7830520868301392 4.399824619293213 4.399824619293213
Loss :  1.768937349319458 3.791992425918579 3.791992425918579
Loss :  1.6711012125015259 4.431203365325928 4.431203365325928
Loss :  1.7092175483703613 4.095791339874268 4.095791339874268
Loss :  1.7246116399765015 3.8819756507873535 3.8819756507873535
Loss :  1.7175335884094238 3.723604679107666 3.723604679107666
Loss :  1.6916437149047852 4.0313944816589355 4.0313944816589355
Loss :  1.6400165557861328 3.7553935050964355 3.7553935050964355
Loss :  1.7039341926574707 3.7469308376312256 3.7469308376312256
Loss :  1.782601237297058 3.5779991149902344 3.5779991149902344
Loss :  1.6747151613235474 3.664761543273926 3.664761543273926
Loss :  1.7281972169876099 3.8633761405944824 3.8633761405944824
Loss :  1.646823763847351 3.64054536819458 3.64054536819458
Loss :  1.720162034034729 3.544869899749756 3.544869899749756
  batch 20 loss: 1.720162034034729, 3.544869899749756, 3.544869899749756
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.636385440826416 3.3622634410858154 3.3622634410858154
Loss :  1.6403900384902954 3.7362680435180664 3.7362680435180664
Loss :  1.671908974647522 3.7293686866760254 3.7293686866760254
Loss :  1.710729956626892 3.9850332736968994 3.9850332736968994
Loss :  1.7550654411315918 3.8349006175994873 3.8349006175994873
Loss :  1.6724259853363037 3.595606565475464 3.595606565475464
Loss :  1.7419542074203491 3.8702447414398193 3.8702447414398193
Loss :  1.6557074785232544 3.519174814224243 3.519174814224243
Loss :  1.6135460138320923 3.561598300933838 3.561598300933838
Loss :  1.717480182647705 4.036177158355713 4.036177158355713
Loss :  1.6354690790176392 3.767151117324829 3.767151117324829
Loss :  1.7052242755889893 4.222784519195557 4.222784519195557
Loss :  1.6627862453460693 3.658116340637207 3.658116340637207
Loss :  1.6461938619613647 3.515791654586792 3.515791654586792
Loss :  1.6238034963607788 3.83427095413208 3.83427095413208
Loss :  1.6432257890701294 3.7317347526550293 3.7317347526550293
Loss :  1.6288944482803345 3.7638769149780273 3.7638769149780273
Loss :  1.7294739484786987 3.741513967514038 3.741513967514038
Loss :  1.739362359046936 3.663372278213501 3.663372278213501
Loss :  1.744844913482666 3.697148084640503 3.697148084640503
  batch 40 loss: 1.744844913482666, 3.697148084640503, 3.697148084640503
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.676306128501892 3.590837001800537 3.590837001800537
Loss :  1.6904243230819702 3.5098371505737305 3.5098371505737305
Loss :  1.6248005628585815 3.762737989425659 3.762737989425659
Loss :  1.7004836797714233 3.6550419330596924 3.6550419330596924
Loss :  1.6528923511505127 3.5403478145599365 3.5403478145599365
Loss :  1.6949362754821777 3.548640012741089 3.548640012741089
Loss :  1.7631264925003052 3.856123447418213 3.856123447418213
Loss :  1.6477651596069336 3.7649953365325928 3.7649953365325928
Loss :  1.7470248937606812 3.4922051429748535 3.4922051429748535
Loss :  1.6738922595977783 3.8212971687316895 3.8212971687316895
Loss :  1.6783791780471802 3.910869598388672 3.910869598388672
Loss :  1.7170014381408691 3.7310445308685303 3.7310445308685303
Loss :  1.695093035697937 3.833536386489868 3.833536386489868
Loss :  1.7521692514419556 3.935727596282959 3.935727596282959
Loss :  1.6384016275405884 3.9024431705474854 3.9024431705474854
Loss :  1.7801002264022827 3.638139009475708 3.638139009475708
Loss :  1.6984245777130127 3.59976863861084 3.59976863861084
Loss :  1.6419401168823242 3.651394844055176 3.651394844055176
Loss :  1.7049490213394165 3.809911012649536 3.809911012649536
Loss :  1.7821117639541626 3.6346774101257324 3.6346774101257324
  batch 60 loss: 1.7821117639541626, 3.6346774101257324, 3.6346774101257324
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.6740518808364868 3.761214256286621 3.761214256286621
Loss :  1.6986552476882935 3.6422784328460693 3.6422784328460693
Loss :  1.6930696964263916 3.4819891452789307 3.4819891452789307
Loss :  1.64217209815979 3.542404890060425 3.542404890060425
Loss :  1.6408603191375732 3.3154919147491455 3.3154919147491455
Loss :  2.68381929397583 4.328497409820557 4.328497409820557
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  2.6444501876831055 4.308895587921143 4.308895587921143
Loss :  2.5904576778411865 4.272372245788574 4.272372245788574
Loss :  2.8125391006469727 4.104254722595215 4.104254722595215
Total LOSS train 3.7423857798943154 valid 4.253504991531372
CE LOSS train 1.698567428955665 valid 0.7031347751617432
Contrastive LOSS train 3.7423857798943154 valid 1.0260636806488037
EPOCH 65:
Loss :  1.748734951019287 3.3755526542663574 3.3755526542663574
Loss :  1.7788386344909668 4.267637252807617 4.267637252807617
Loss :  1.7233753204345703 3.7741878032684326 3.7741878032684326
Loss :  1.7237722873687744 3.7518935203552246 3.7518935203552246
Loss :  1.762807846069336 3.551172971725464 3.551172971725464
Loss :  1.7363111972808838 3.738851308822632 3.738851308822632
Loss :  1.7257015705108643 3.488692045211792 3.488692045211792
Loss :  1.6932592391967773 3.409813165664673 3.409813165664673
Loss :  1.6679025888442993 3.6823959350585938 3.6823959350585938
Loss :  1.7039445638656616 3.3642959594726562 3.3642959594726562
Loss :  1.7036492824554443 4.059426307678223 4.059426307678223
Loss :  1.7146191596984863 3.960867166519165 3.960867166519165
Loss :  1.6957365274429321 3.8646888732910156 3.8646888732910156
Loss :  1.6445322036743164 3.846895933151245 3.846895933151245
Loss :  1.720149040222168 3.5497095584869385 3.5497095584869385
Loss :  1.7729060649871826 3.7121708393096924 3.7121708393096924
Loss :  1.6932520866394043 3.912717342376709 3.912717342376709
Loss :  1.7437947988510132 3.395071506500244 3.395071506500244
Loss :  1.679345726966858 3.6097381114959717 3.6097381114959717
Loss :  1.7368767261505127 3.8017466068267822 3.8017466068267822
  batch 20 loss: 1.7368767261505127, 3.8017466068267822, 3.8017466068267822
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.6747918128967285 3.570255756378174 3.570255756378174
Loss :  1.6790679693222046 3.8957509994506836 3.8957509994506836
Loss :  1.6938531398773193 3.4566495418548584 3.4566495418548584
Loss :  1.7336968183517456 3.7806100845336914 3.7806100845336914
Loss :  1.7678316831588745 3.9281716346740723 3.9281716346740723
Loss :  1.7068877220153809 3.4901764392852783 3.4901764392852783
Loss :  1.7779783010482788 4.060714244842529 4.060714244842529
Loss :  1.7118034362792969 4.030751705169678 4.030751705169678
Loss :  1.6836705207824707 4.042667865753174 4.042667865753174
Loss :  1.7677059173583984 3.913510322570801 3.913510322570801
Loss :  1.6998399496078491 3.981509208679199 3.981509208679199
Loss :  1.7348355054855347 4.113259792327881 4.113259792327881
Loss :  1.707092523574829 3.7711145877838135 3.7711145877838135
Loss :  1.699819564819336 3.787680149078369 3.787680149078369
Loss :  1.671643853187561 3.975996494293213 3.975996494293213
Loss :  1.7365458011627197 4.370467662811279 4.370467662811279
Loss :  1.7414368391036987 3.78517484664917 3.78517484664917
Loss :  1.7736035585403442 3.9007532596588135 3.9007532596588135
Loss :  1.8167999982833862 4.409791469573975 4.409791469573975
Loss :  1.8181829452514648 4.04606819152832 4.04606819152832
  batch 40 loss: 1.8181829452514648, 4.04606819152832, 4.04606819152832
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.7581698894500732 3.5378096103668213 3.5378096103668213
Loss :  1.7776386737823486 3.451876640319824 3.451876640319824
Loss :  1.7436875104904175 4.303443908691406 4.303443908691406
Loss :  1.7851253747940063 4.259692192077637 4.259692192077637
Loss :  1.7501752376556396 4.310263633728027 4.310263633728027
Loss :  1.763519525527954 4.617188930511475 4.617188930511475
Loss :  1.817592740058899 4.302801609039307 4.302801609039307
Loss :  1.7277417182922363 4.425025939941406 4.425025939941406
Loss :  1.8398796319961548 3.535047769546509 3.535047769546509
Loss :  1.7614980936050415 3.7402963638305664 3.7402963638305664
Loss :  1.7651921510696411 3.7479755878448486 3.7479755878448486
Loss :  1.7738702297210693 3.943481922149658 3.943481922149658
Loss :  1.7351011037826538 3.8871779441833496 3.8871779441833496
Loss :  1.7820061445236206 3.7560536861419678 3.7560536861419678
Loss :  1.6621812582015991 3.7680375576019287 3.7680375576019287
Loss :  1.835998773574829 3.8975753784179688 3.8975753784179688
Loss :  1.7535440921783447 4.029524803161621 4.029524803161621
Loss :  1.723796010017395 3.9266629219055176 3.9266629219055176
Loss :  1.7899413108825684 3.972172260284424 3.972172260284424
Loss :  1.8917391300201416 4.009723663330078 4.009723663330078
  batch 60 loss: 1.8917391300201416, 4.009723663330078, 4.009723663330078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.7297545671463013 3.8368730545043945 3.8368730545043945
Loss :  1.7263100147247314 3.638864755630493 3.638864755630493
Loss :  1.740101933479309 3.6035215854644775 3.6035215854644775
Loss :  1.6434392929077148 3.599320650100708 3.599320650100708
Loss :  1.6601494550704956 3.3482320308685303 3.3482320308685303
Loss :  1.747184157371521 4.293385982513428 4.293385982513428
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.7378579378128052 4.154488563537598 4.154488563537598
Loss :  1.7322665452957153 4.063384532928467 4.063384532928467
Loss :  1.8470559120178223 4.221705436706543 4.221705436706543
Total LOSS train 3.8442652849050667 valid 4.183241128921509
CE LOSS train 1.7369961775266207 valid 0.46176397800445557
Contrastive LOSS train 3.8442652849050667 valid 1.0554263591766357
EPOCH 66:
Loss :  1.779962182044983 3.817596912384033 3.817596912384033
Loss :  1.8184387683868408 3.937774896621704 3.937774896621704
Loss :  1.7480159997940063 3.4021623134613037 3.4021623134613037
Loss :  1.747678518295288 3.7207536697387695 3.7207536697387695
Loss :  1.79871666431427 4.142059803009033 4.142059803009033
Loss :  1.7815330028533936 3.8256278038024902 3.8256278038024902
Loss :  1.7782913446426392 3.314342737197876 3.314342737197876
Loss :  1.7307902574539185 3.9973909854888916 3.9973909854888916
Loss :  1.7049111127853394 3.877284526824951 3.877284526824951
Loss :  1.724561095237732 3.7689082622528076 3.7689082622528076
Loss :  1.7329274415969849 4.156005382537842 4.156005382537842
Loss :  1.7353482246398926 3.8431124687194824 3.8431124687194824
Loss :  1.7114747762680054 4.116360664367676 4.116360664367676
Loss :  1.661901593208313 3.7954366207122803 3.7954366207122803
Loss :  1.7197502851486206 3.9927303791046143 3.9927303791046143
Loss :  1.7995240688323975 3.58888840675354 3.58888840675354
Loss :  1.691021203994751 3.6906027793884277 3.6906027793884277
Loss :  1.7469600439071655 3.896409749984741 3.896409749984741
Loss :  1.6760083436965942 3.588606357574463 3.588606357574463
Loss :  1.7742830514907837 3.9391093254089355 3.9391093254089355
  batch 20 loss: 1.7742830514907837, 3.9391093254089355, 3.9391093254089355
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7481704950332642 3.910248041152954 3.910248041152954
Loss :  1.6802318096160889 3.8555045127868652 3.8555045127868652
Loss :  1.6631923913955688 3.60640811920166 3.60640811920166
Loss :  1.7158418893814087 3.8819503784179688 3.8819503784179688
Loss :  1.8036596775054932 4.363298416137695 4.363298416137695
Loss :  1.710206389427185 4.414995193481445 4.414995193481445
Loss :  1.864272117614746 4.578683853149414 4.578683853149414
Loss :  1.7695609331130981 4.281983375549316 4.281983375549316
Loss :  1.6843756437301636 4.393733024597168 4.393733024597168
Loss :  1.7707641124725342 4.408365726470947 4.408365726470947
Loss :  1.6722111701965332 4.451016426086426 4.451016426086426
Loss :  1.784134864807129 4.623562812805176 4.623562812805176
Loss :  1.7904560565948486 4.3772969245910645 4.3772969245910645
Loss :  1.7405568361282349 4.608363628387451 4.608363628387451
Loss :  1.693019986152649 4.526647090911865 4.526647090911865
Loss :  1.720605731010437 4.324219703674316 4.324219703674316
Loss :  1.7424676418304443 4.341180324554443 4.341180324554443
Loss :  1.7743396759033203 4.255434513092041 4.255434513092041
Loss :  1.908697247505188 4.61135721206665 4.61135721206665
Loss :  1.8523703813552856 4.39739990234375 4.39739990234375
  batch 40 loss: 1.8523703813552856, 4.39739990234375, 4.39739990234375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
Loss :  1.8002259731292725 4.458611965179443 4.458611965179443
Loss :  1.799569010734558 4.533024787902832 4.533024787902832
Loss :  1.7422188520431519 4.532063961029053 4.532063961029053
Loss :  1.7760764360427856 4.670905113220215 4.670905113220215
Loss :  1.7329516410827637 4.471522808074951 4.471522808074951
Loss :  1.7823410034179688 4.720289707183838 4.720289707183838
Loss :  1.8638564348220825 4.536208629608154 4.536208629608154
Loss :  1.730954885482788 4.501094341278076 4.501094341278076
Loss :  1.872027039527893 4.443376541137695 4.443376541137695
Loss :  1.7506849765777588 4.459223747253418 4.459223747253418
Loss :  1.7905024290084839 4.487120628356934 4.487120628356934
Loss :  1.80764901638031 4.431610584259033 4.431610584259033
Loss :  1.7423148155212402 4.382209777832031 4.382209777832031
Loss :  1.8947592973709106 4.584709644317627 4.584709644317627
Loss :  1.6842796802520752 4.489840507507324 4.489840507507324
Loss :  1.9794937372207642 4.4738054275512695 4.4738054275512695
Loss :  1.8144381046295166 4.462730407714844 4.462730407714844
Loss :  1.726088047027588 4.378243923187256 4.378243923187256
Loss :  1.7956018447875977 4.677673816680908 4.677673816680908
Loss :  1.89032781124115 4.361565589904785 4.361565589904785
  batch 60 loss: 1.89032781124115, 4.361565589904785, 4.361565589904785
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.7240959405899048 4.102111339569092 4.102111339569092
Loss :  1.7349365949630737 4.150270938873291 4.150270938873291
Loss :  1.7115610837936401 4.368367671966553 4.368367671966553
Loss :  1.6832499504089355 4.445042610168457 4.445042610168457
Loss :  1.6445626020431519 4.249544143676758 4.249544143676758
Loss :  1.6559075117111206 4.549891948699951 4.549891948699951
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.7638946771621704 4.851631164550781 4.851631164550781
Loss :  1.5930808782577515 4.29243278503418 4.29243278503418
Loss :  1.7278571128845215 4.223847389221191 4.223847389221191
Total LOSS train 4.2152920282804045 valid 4.479450821876526
CE LOSS train 1.7604000036533063 valid 0.43196427822113037
Contrastive LOSS train 4.2152920282804045 valid 1.0559618473052979
EPOCH 67:
Loss :  1.7878082990646362 4.122920036315918 4.122920036315918
Loss :  1.7826226949691772 4.235895156860352 4.235895156860352
Loss :  1.7303519248962402 4.471223831176758 4.471223831176758
Loss :  1.7147212028503418 4.4551591873168945 4.4551591873168945
Loss :  1.8003926277160645 4.521684169769287 4.521684169769287
Loss :  1.7211557626724243 4.496124744415283 4.496124744415283
Loss :  1.7474497556686401 4.675267219543457 4.675267219543457
Loss :  1.709889531135559 4.254278182983398 4.254278182983398
Loss :  1.6607191562652588 4.451791763305664 4.451791763305664
Loss :  1.6985975503921509 4.1064558029174805 4.1064558029174805
Loss :  1.6732704639434814 4.230936527252197 4.230936527252197
Loss :  1.6808817386627197 4.409782886505127 4.409782886505127
Loss :  1.6778020858764648 4.441251277923584 4.441251277923584
Loss :  1.6364730596542358 4.394047737121582 4.394047737121582
Loss :  1.692097783088684 4.590510368347168 4.590510368347168
Loss :  1.7635719776153564 4.502054214477539 4.502054214477539
Loss :  1.6561334133148193 4.667142391204834 4.667142391204834
Loss :  1.709825038909912 4.388374328613281 4.388374328613281
Loss :  1.6169389486312866 4.046239376068115 4.046239376068115
Loss :  1.760329008102417 4.140050888061523 4.140050888061523
  batch 20 loss: 1.760329008102417, 4.140050888061523, 4.140050888061523
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.6474062204360962 4.029880046844482 4.029880046844482
Loss :  1.6212102174758911 4.224857807159424 4.224857807159424
Loss :  1.6755437850952148 4.071807384490967 4.071807384490967
Loss :  1.6883485317230225 4.177297115325928 4.177297115325928
Loss :  1.7345695495605469 4.482988357543945 4.482988357543945
Loss :  1.6738898754119873 4.1482343673706055 4.1482343673706055
Loss :  1.7360368967056274 4.375370979309082 4.375370979309082
Loss :  1.6538063287734985 4.176579475402832 4.176579475402832
Loss :  1.6103694438934326 4.2613701820373535 4.2613701820373535
Loss :  1.6866354942321777 4.119289875030518 4.119289875030518
Loss :  1.5628135204315186 3.9773852825164795 3.9773852825164795
Loss :  1.6836026906967163 4.215892791748047 4.215892791748047
Loss :  1.6287997961044312 3.6243069171905518 3.6243069171905518
Loss :  1.6329330205917358 3.6158640384674072 3.6158640384674072
Loss :  1.5972551107406616 4.060831069946289 4.060831069946289
Loss :  1.6123573780059814 3.9861748218536377 3.9861748218536377
Loss :  1.6054906845092773 4.138571262359619 4.138571262359619
Loss :  1.7126569747924805 3.8412318229675293 3.8412318229675293
Loss :  1.7208342552185059 3.9052574634552 3.9052574634552
Loss :  1.749137282371521 3.561993360519409 3.561993360519409
  batch 40 loss: 1.749137282371521, 3.561993360519409, 3.561993360519409
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.6703730821609497 3.9052019119262695 3.9052019119262695
Loss :  1.656313180923462 4.1084370613098145 4.1084370613098145
Loss :  1.5871808528900146 3.8871071338653564 3.8871071338653564
Loss :  1.6569287776947021 4.0383620262146 4.0383620262146
Loss :  1.6025983095169067 3.797602653503418 3.797602653503418
Loss :  1.6857575178146362 3.7958645820617676 3.7958645820617676
Loss :  1.7720661163330078 3.5982770919799805 3.5982770919799805
Loss :  1.606034755706787 3.6132168769836426 3.6132168769836426
Loss :  1.7736140489578247 3.5833611488342285 3.5833611488342285
Loss :  1.6371381282806396 3.7293362617492676 3.7293362617492676
Loss :  1.6693187952041626 4.267913341522217 4.267913341522217
Loss :  1.690440058708191 3.778406858444214 3.778406858444214
Loss :  1.664372205734253 3.7975151538848877 3.7975151538848877
Loss :  1.7146220207214355 3.9129092693328857 3.9129092693328857
Loss :  1.582485318183899 3.7276182174682617 3.7276182174682617
Loss :  1.7756913900375366 3.6297836303710938 3.6297836303710938
Loss :  1.6763007640838623 3.7447309494018555 3.7447309494018555
Loss :  1.6044764518737793 3.8995680809020996 3.8995680809020996
Loss :  1.6769102811813354 3.8283982276916504 3.8283982276916504
Loss :  1.7932361364364624 3.6225852966308594 3.6225852966308594
  batch 60 loss: 1.7932361364364624, 3.6225852966308594, 3.6225852966308594
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.669260859489441 3.9888007640838623 3.9888007640838623
Loss :  1.6950252056121826 3.9416160583496094 3.9416160583496094
Loss :  1.690287470817566 3.6284334659576416 3.6284334659576416
Loss :  1.619815707206726 3.802461862564087 3.802461862564087
Loss :  1.5972732305526733 3.2471981048583984 3.2471981048583984
Loss :  1.6493873596191406 4.037177085876465 4.037177085876465
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.634559988975525 4.124551773071289 4.124551773071289
Loss :  1.6372206211090088 3.7553091049194336 3.7553091049194336
Loss :  1.7425626516342163 4.07178258895874 4.07178258895874
Total LOSS train 4.053401239101703 valid 3.997205138206482
CE LOSS train 1.680342303789579 valid 0.4356406629085541
Contrastive LOSS train 4.053401239101703 valid 1.017945647239685
EPOCH 68:
Loss :  1.7022873163223267 3.434749126434326 3.434749126434326
Loss :  1.7460157871246338 3.598660707473755 3.598660707473755
Loss :  1.6924762725830078 3.4326791763305664 3.4326791763305664
Loss :  1.6802595853805542 3.7051994800567627 3.7051994800567627
Loss :  1.7527188062667847 3.8626861572265625 3.8626861572265625
Loss :  1.7096468210220337 3.9430503845214844 3.9430503845214844
Loss :  1.7357217073440552 3.7739410400390625 3.7739410400390625
Loss :  1.687301516532898 3.6312880516052246 3.6312880516052246
Loss :  1.6738930940628052 3.415386438369751 3.415386438369751
Loss :  1.707567811012268 3.422957420349121 3.422957420349121
Loss :  1.6723123788833618 3.446000814437866 3.446000814437866
Loss :  1.6481417417526245 3.7112605571746826 3.7112605571746826
Loss :  1.6465381383895874 3.6876699924468994 3.6876699924468994
Loss :  1.6044560670852661 3.7286698818206787 3.7286698818206787
Loss :  1.7019373178482056 3.9337687492370605 3.9337687492370605
Loss :  1.7412408590316772 3.5979809761047363 3.5979809761047363
Loss :  1.6538605690002441 3.5764970779418945 3.5764970779418945
Loss :  1.7031059265136719 3.4103405475616455 3.4103405475616455
Loss :  1.6136341094970703 3.8713064193725586 3.8713064193725586
Loss :  1.7300810813903809 3.5899057388305664 3.5899057388305664
  batch 20 loss: 1.7300810813903809, 3.5899057388305664, 3.5899057388305664
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.6345515251159668 3.9820923805236816 3.9820923805236816
Loss :  1.6301697492599487 3.625716209411621 3.625716209411621
Loss :  1.6705797910690308 3.400118589401245 3.400118589401245
Loss :  1.6826236248016357 3.5695559978485107 3.5695559978485107
Loss :  1.7476575374603271 3.812406063079834 3.812406063079834
Loss :  1.6862576007843018 3.3590517044067383 3.3590517044067383
Loss :  1.7555619478225708 3.827517032623291 3.827517032623291
Loss :  1.6906017065048218 3.77966570854187 3.77966570854187
Loss :  1.6476026773452759 3.6366186141967773 3.6366186141967773
Loss :  1.7195796966552734 3.6157076358795166 3.6157076358795166
Loss :  1.6363976001739502 3.6917836666107178 3.6917836666107178
Loss :  1.7098138332366943 4.102973937988281 4.102973937988281
Loss :  1.6722972393035889 3.6615989208221436 3.6615989208221436
Loss :  1.6670506000518799 3.524775266647339 3.524775266647339
Loss :  1.6344343423843384 3.5416007041931152 3.5416007041931152
Loss :  1.6512154340744019 3.5164804458618164 3.5164804458618164
Loss :  1.6481001377105713 3.712761402130127 3.712761402130127
Loss :  1.7235618829727173 3.715653896331787 3.715653896331787
Loss :  1.7394572496414185 3.319347381591797 3.319347381591797
Loss :  1.7529462575912476 3.4067223072052 3.4067223072052
  batch 40 loss: 1.7529462575912476, 3.4067223072052, 3.4067223072052
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.701098084449768 3.5920028686523438 3.5920028686523438
Loss :  1.6809126138687134 3.4605917930603027 3.4605917930603027
Loss :  1.6246424913406372 3.3862853050231934 3.3862853050231934
Loss :  1.673973560333252 3.998955011367798 3.998955011367798
Loss :  1.6498041152954102 3.405653953552246 3.405653953552246
Loss :  1.7254481315612793 3.537003993988037 3.537003993988037
Loss :  1.7939814329147339 3.226442575454712 3.226442575454712
Loss :  1.6471368074417114 3.6315948963165283 3.6315948963165283
Loss :  1.7724802494049072 3.620466709136963 3.620466709136963
Loss :  1.6626019477844238 3.3262243270874023 3.3262243270874023
Loss :  1.6855264902114868 3.7061233520507812 3.7061233520507812
Loss :  1.7287766933441162 3.601977825164795 3.601977825164795
Loss :  1.714339256286621 3.3697798252105713 3.3697798252105713
Loss :  1.7498122453689575 3.5224802494049072 3.5224802494049072
Loss :  1.631509780883789 3.636143445968628 3.636143445968628
Loss :  1.7982503175735474 3.3893332481384277 3.3893332481384277
Loss :  1.718540906906128 3.6291990280151367 3.6291990280151367
Loss :  1.6526010036468506 3.9698691368103027 3.9698691368103027
Loss :  1.7220726013183594 3.5686707496643066 3.5686707496643066
Loss :  1.7939023971557617 3.5463192462921143 3.5463192462921143
  batch 60 loss: 1.7939023971557617, 3.5463192462921143, 3.5463192462921143
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4, 5], device='cuda:0')
Loss :  1.7000281810760498 3.535161256790161 3.535161256790161
Loss :  1.7142874002456665 3.8482375144958496 3.8482375144958496
Loss :  1.719132423400879 3.6091737747192383 3.6091737747192383
Loss :  1.6700986623764038 3.7072036266326904 3.7072036266326904
Loss :  1.661341905593872 3.0805399417877197 3.0805399417877197
Loss :  1.8588430881500244 4.025825023651123 4.025825023651123
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4, 5], device='cuda:0')
Loss :  1.7807190418243408 4.134993076324463 4.134993076324463
Loss :  1.8251405954360962 4.006786823272705 4.006786823272705
Loss :  1.8763304948806763 3.960397243499756 3.960397243499756
Total LOSS train 3.6074089270371656 valid 4.032000541687012
CE LOSS train 1.6937839544736422 valid 0.46908262372016907
Contrastive LOSS train 3.6074089270371656 valid 0.990099310874939
EPOCH 69:
Loss :  1.750996470451355 3.712045431137085 3.712045431137085
Loss :  1.7882577180862427 3.6888742446899414 3.6888742446899414
Loss :  1.74062180519104 3.3677406311035156 3.3677406311035156
Loss :  1.7273000478744507 3.4410552978515625 3.4410552978515625
Loss :  1.76972234249115 3.618014335632324 3.618014335632324
Loss :  1.7415238618850708 3.6412832736968994 3.6412832736968994
Loss :  1.7549737691879272 3.4941279888153076 3.4941279888153076
Loss :  1.7191855907440186 2.942617654800415 2.942617654800415
Loss :  1.6985948085784912 3.2866005897521973 3.2866005897521973
Loss :  1.7355543375015259 3.4772536754608154 3.4772536754608154
Loss :  1.7139972448349 3.6025607585906982 3.6025607585906982
Loss :  1.6981443166732788 3.8689072132110596 3.8689072132110596
Loss :  1.7064050436019897 3.60573148727417 3.60573148727417
Loss :  1.6719458103179932 3.5044541358947754 3.5044541358947754
Loss :  1.7319122552871704 3.5254478454589844 3.5254478454589844
Loss :  1.7868413925170898 3.4543638229370117 3.4543638229370117
Loss :  1.707592487335205 3.6531035900115967 3.6531035900115967
Loss :  1.7480636835098267 3.7426483631134033 3.7426483631134033
Loss :  1.6710104942321777 3.332090139389038 3.332090139389038
Loss :  1.768633246421814 3.450507879257202 3.450507879257202
  batch 20 loss: 1.768633246421814, 3.450507879257202, 3.450507879257202
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4, 5], device='cuda:0')
Loss :  1.6937655210494995 3.2618114948272705 3.2618114948272705
Loss :  1.6896003484725952 3.4469010829925537 3.4469010829925537
Loss :  1.7103052139282227 3.777432918548584 3.777432918548584
Loss :  1.7228872776031494 3.8496880531311035 3.8496880531311035
Loss :  1.774444341659546 3.5141031742095947 3.5141031742095947
Loss :  1.706996202468872 3.2004427909851074 3.2004427909851074
Loss :  1.7664153575897217 3.381751775741577 3.381751775741577
Loss :  1.6967813968658447 3.5694708824157715 3.5694708824157715
Loss :  1.6708074808120728 3.1284518241882324 3.1284518241882324
Loss :  1.7433853149414062 3.4772121906280518 3.4772121906280518
Loss :  1.6654831171035767 3.3021857738494873 3.3021857738494873
Loss :  1.7277779579162598 3.8747358322143555 3.8747358322143555
Loss :  1.714377999305725 3.1961615085601807 3.1961615085601807
Loss :  1.7118726968765259 3.2329113483428955 3.2329113483428955
Loss :  1.6795101165771484 3.3842267990112305 3.3842267990112305
Loss :  1.6900564432144165 3.173821210861206 3.173821210861206
Loss :  1.6926010847091675 3.155251979827881 3.155251979827881
Loss :  1.770336389541626 3.0746285915374756 3.0746285915374756
Loss :  1.7887593507766724 3.15901780128479 3.15901780128479
Loss :  1.8120754957199097 3.072313070297241 3.072313070297241
  batch 40 loss: 1.8120754957199097, 3.072313070297241, 3.072313070297241
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4, 5], device='cuda:0')
Loss :  1.7747775316238403 3.0776665210723877 3.0776665210723877
Loss :  1.7548350095748901 2.9464380741119385 2.9464380741119385
Loss :  1.6989234685897827 3.3195183277130127 3.3195183277130127
Loss :  1.7552940845489502 3.153184413909912 3.153184413909912
Loss :  1.721610188484192 3.022512912750244 3.022512912750244
Loss :  1.779009222984314 3.007917642593384 3.007917642593384
Loss :  1.8513431549072266 2.984652280807495 2.984652280807495
Loss :  1.7231806516647339 3.0924224853515625 3.0924224853515625
Loss :  1.8261862993240356 3.00972580909729 3.00972580909729
Loss :  1.7366310358047485 3.718228578567505 3.718228578567505
Loss :  1.7449761629104614 3.118774890899658 3.118774890899658
Loss :  1.7870440483093262 3.187091588973999 3.187091588973999
Loss :  1.7773821353912354 2.8657734394073486 2.8657734394073486
Loss :  1.8099199533462524 3.20383882522583 3.20383882522583
Loss :  1.705039381980896 3.1388204097747803 3.1388204097747803
Loss :  1.8565467596054077 2.9263226985931396 2.9263226985931396
Loss :  1.7896535396575928 3.3617734909057617 3.3617734909057617
Loss :  1.722594976425171 3.306748867034912 3.306748867034912
Loss :  1.7918896675109863 3.0393154621124268 3.0393154621124268
Loss :  1.8566616773605347 3.1397225856781006 3.1397225856781006
  batch 60 loss: 1.8566616773605347, 3.1397225856781006, 3.1397225856781006
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4, 5], device='cuda:0')
Loss :  1.7754799127578735 3.1472105979919434 3.1472105979919434
Loss :  1.7844746112823486 3.5159685611724854 3.5159685611724854
Loss :  1.7958412170410156 2.8753225803375244 2.8753225803375244
Loss :  1.7340894937515259 3.2668187618255615 3.2668187618255615
Loss :  1.7150534391403198 2.6080360412597656 2.6080360412597656
Loss :  1.8443673849105835 4.092264175415039 4.092264175415039
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4, 5], device='cuda:0')
Loss :  1.8359287977218628 4.006475448608398 4.006475448608398
Loss :  1.8327536582946777 3.9186201095581055 3.9186201095581055
Loss :  1.9157854318618774 3.781181812286377 3.781181812286377
Total LOSS train 3.3180885278261627 valid 3.94963538646698
CE LOSS train 1.7435069762743436 valid 0.47894635796546936
Contrastive LOSS train 3.3180885278261627 valid 0.9452954530715942
EPOCH 70:
Loss :  1.7909016609191895 3.1056153774261475 3.1056153774261475
Loss :  1.8293529748916626 3.328608512878418 3.328608512878418
Loss :  1.7879326343536377 3.327342987060547 3.327342987060547
Loss :  1.7847936153411865 2.8068058490753174 2.8068058490753174
Loss :  1.8164031505584717 3.0131795406341553 3.0131795406341553
Loss :  1.8030258417129517 3.212376117706299 3.212376117706299
Loss :  1.8019424676895142 3.275529623031616 3.275529623031616
Loss :  1.7776838541030884 2.7476298809051514 2.7476298809051514
Loss :  1.7639609575271606 3.740164279937744 3.740164279937744
Loss :  1.7936651706695557 2.9622232913970947 2.9622232913970947
Loss :  1.779675006866455 3.048067331314087 3.048067331314087
Loss :  1.7543442249298096 3.108839988708496 3.108839988708496
Loss :  1.7547130584716797 3.39467453956604 3.39467453956604
Loss :  1.7211114168167114 3.2629077434539795 3.2629077434539795
Loss :  1.7743399143218994 3.3567426204681396 3.3567426204681396
Loss :  1.8333052396774292 3.170635938644409 3.170635938644409
Loss :  1.7588276863098145 3.00036883354187 3.00036883354187
Loss :  1.809888482093811 3.0842442512512207 3.0842442512512207
Loss :  1.7312862873077393 2.9542150497436523 2.9542150497436523
Loss :  1.8100594282150269 2.921905755996704 2.921905755996704
  batch 20 loss: 1.8100594282150269, 2.921905755996704, 2.921905755996704
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4, 5], device='cuda:0')
Loss :  1.7381150722503662 3.112149477005005 3.112149477005005
Loss :  1.734147310256958 3.542168140411377 3.542168140411377
Loss :  1.7527538537979126 3.1545112133026123 3.1545112133026123
Loss :  1.7697030305862427 3.1304051876068115 3.1304051876068115
Loss :  1.8342422246932983 3.2401978969573975 3.2401978969573975
Loss :  1.7782561779022217 4.127366542816162 4.127366542816162
Loss :  1.8390158414840698 3.5542712211608887 3.5542712211608887
Loss :  1.7646476030349731 3.5490148067474365 3.5490148067474365
Loss :  1.7311148643493652 3.916672706604004 3.916672706604004
Loss :  1.7825340032577515 3.533029794692993 3.533029794692993
Loss :  1.709999680519104 3.3628475666046143 3.3628475666046143
Loss :  1.777303695678711 3.2238991260528564 3.2238991260528564
Loss :  1.7579344511032104 3.170454978942871 3.170454978942871
Loss :  1.755725622177124 3.807563304901123 3.807563304901123
Loss :  1.717268705368042 3.4536144733428955 3.4536144733428955
Loss :  1.7292364835739136 3.1982147693634033 3.1982147693634033
Loss :  1.729207158088684 2.9539437294006348 2.9539437294006348
Loss :  1.798049807548523 3.224964141845703 3.224964141845703
Loss :  1.8089369535446167 3.054866313934326 3.054866313934326
Loss :  1.8159527778625488 3.360081911087036 3.360081911087036
  batch 40 loss: 1.8159527778625488, 3.360081911087036, 3.360081911087036
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4, 5], device='cuda:0')
Loss :  1.7760225534439087 3.3972387313842773 3.3972387313842773
Loss :  1.762886643409729 3.1975295543670654 3.1975295543670654
Loss :  1.7205005884170532 3.4598453044891357 3.4598453044891357
Loss :  1.752987265586853 3.6888623237609863 3.6888623237609863
Loss :  1.7484333515167236 3.3454465866088867 3.3454465866088867
Loss :  1.8039528131484985 3.8184499740600586 3.8184499740600586
Loss :  1.8868731260299683 4.046786785125732 4.046786785125732
Loss :  1.78975510597229 3.378509759902954 3.378509759902954
Loss :  1.87471342086792 3.365551710128784 3.365551710128784
Loss :  1.7881942987442017 4.1523027420043945 4.1523027420043945
Loss :  1.7850408554077148 4.0530314445495605 4.0530314445495605
Loss :  1.8119068145751953 3.4693922996520996 3.4693922996520996
Loss :  1.7904890775680542 3.664508104324341 3.664508104324341
Loss :  1.841046929359436 3.906461000442505 3.906461000442505
Loss :  1.8107408285140991 3.360509157180786 3.360509157180786
Loss :  1.922316074371338 3.513988733291626 3.513988733291626
Loss :  1.8548811674118042 4.0310163497924805 4.0310163497924805
Loss :  1.7706912755966187 4.387239456176758 4.387239456176758
Loss :  1.857527494430542 4.238702297210693 4.238702297210693
Loss :  1.882387399673462 4.439569473266602 4.439569473266602
  batch 60 loss: 1.882387399673462, 4.439569473266602, 4.439569473266602
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4, 5], device='cuda:0')
Loss :  1.8445675373077393 4.356902122497559 4.356902122497559
Loss :  1.832122564315796 4.383931636810303 4.383931636810303
Loss :  1.8397799730300903 4.5366339683532715 4.5366339683532715
Loss :  1.796674370765686 4.393300533294678 4.393300533294678
Loss :  1.8040590286254883 4.06074333190918 4.06074333190918
Loss :  2.489189624786377 4.544831275939941 4.544831275939941
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4, 5], device='cuda:0')
Loss :  2.7010695934295654 4.429807662963867 4.429807662963867
Loss :  2.4826886653900146 4.287627220153809 4.287627220153809
Loss :  2.453409194946289 4.479571342468262 4.479571342468262
Total LOSS train 3.4944425876323995 valid 4.43545937538147
CE LOSS train 1.7899985991991483 valid 0.6133522987365723
Contrastive LOSS train 3.4944425876323995 valid 1.1198928356170654
EPOCH 71:
Loss :  1.8490943908691406 4.295814514160156 4.295814514160156
Loss :  1.9100006818771362 4.575348377227783 4.575348377227783
Loss :  1.8355437517166138 4.6548237800598145 4.6548237800598145
Loss :  1.856242299079895 4.465000629425049 4.465000629425049
Loss :  1.8849775791168213 4.480661869049072 4.480661869049072
Loss :  1.8914517164230347 4.645050525665283 4.645050525665283
Loss :  1.899672508239746 4.432258129119873 4.432258129119873
Loss :  1.8328077793121338 4.442701816558838 4.442701816558838
Loss :  1.8502733707427979 4.962704658508301 4.962704658508301
Loss :  1.872889757156372 4.306144714355469 4.306144714355469
Loss :  1.8788282871246338 4.553841590881348 4.553841590881348
Loss :  1.8750890493392944 4.5261993408203125 4.5261993408203125
Loss :  1.8481931686401367 4.472261905670166 4.472261905670166
Loss :  1.8640432357788086 4.581883430480957 4.581883430480957
Loss :  1.8686256408691406 4.433602809906006 4.433602809906006
Loss :  1.937231183052063 4.44821834564209 4.44821834564209
Loss :  1.8503566980361938 4.370936393737793 4.370936393737793
Loss :  1.9031099081039429 4.444908142089844 4.444908142089844
Loss :  1.836643934249878 4.402285575866699 4.402285575866699
Loss :  1.886604905128479 4.321776866912842 4.321776866912842
  batch 20 loss: 1.886604905128479, 4.321776866912842, 4.321776866912842
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4, 5], device='cuda:0')
Loss :  1.8607432842254639 4.344752788543701 4.344752788543701
Loss :  1.887556552886963 4.407834053039551 4.407834053039551
Loss :  1.8818331956863403 4.493168830871582 4.493168830871582
Loss :  1.88344407081604 4.424034595489502 4.424034595489502
Loss :  1.9371700286865234 4.59513521194458 4.59513521194458
Loss :  1.8615002632141113 4.364343643188477 4.364343643188477
Loss :  1.960618257522583 4.644628524780273 4.644628524780273
Loss :  1.8344308137893677 4.337930679321289 4.337930679321289
Loss :  1.8485671281814575 4.42183256149292 4.42183256149292
Loss :  1.848131537437439 4.543150901794434 4.543150901794434
Loss :  1.8487762212753296 4.503565788269043 4.503565788269043
Loss :  1.822099208831787 4.582668781280518 4.582668781280518
Loss :  1.8310712575912476 4.313912868499756 4.313912868499756
Loss :  1.8133984804153442 4.338458061218262 4.338458061218262
Loss :  1.8207008838653564 4.477138042449951 4.477138042449951
Loss :  1.8401671648025513 4.469825744628906 4.469825744628906
Loss :  1.7873599529266357 4.517126083374023 4.517126083374023
Loss :  1.7875964641571045 4.24678897857666 4.24678897857666
Loss :  1.8367283344268799 4.215134143829346 4.215134143829346
Loss :  1.8429224491119385 4.080315113067627 4.080315113067627
  batch 40 loss: 1.8429224491119385, 4.080315113067627, 4.080315113067627
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4, 5], device='cuda:0')
Loss :  1.8212984800338745 4.196483135223389 4.196483135223389
Loss :  1.841731071472168 3.9393811225891113 3.9393811225891113
Loss :  1.8409736156463623 4.102813243865967 4.102813243865967
Loss :  1.8457603454589844 4.423391342163086 4.423391342163086
Loss :  1.8257802724838257 4.352838039398193 4.352838039398193
Loss :  1.8921562433242798 4.358269214630127 4.358269214630127
Loss :  1.9203745126724243 4.408131122589111 4.408131122589111
Loss :  1.8879151344299316 4.357882976531982 4.357882976531982
Loss :  1.8647123575210571 4.412072658538818 4.412072658538818
Loss :  1.909028172492981 4.527100086212158 4.527100086212158
Loss :  1.8819595575332642 4.497225284576416 4.497225284576416
Loss :  1.8912098407745361 4.369175434112549 4.369175434112549
Loss :  1.921061635017395 4.297456741333008 4.297456741333008
Loss :  1.8967617750167847 4.147891521453857 4.147891521453857
Loss :  1.8353859186172485 4.28745698928833 4.28745698928833
Loss :  1.9559818506240845 4.276767253875732 4.276767253875732
Loss :  1.9069371223449707 4.312127113342285 4.312127113342285
Loss :  1.860336422920227 4.279714584350586 4.279714584350586
Loss :  1.8978277444839478 4.3865885734558105 4.3865885734558105
Loss :  1.9445230960845947 4.305674076080322 4.305674076080322
  batch 60 loss: 1.9445230960845947, 4.305674076080322, 4.305674076080322
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4, 5], device='cuda:0')
Loss :  1.8882803916931152 3.8855063915252686 3.8855063915252686
Loss :  1.8934004306793213 3.925010919570923 3.925010919570923
Loss :  1.9099575281143188 4.227838039398193 4.227838039398193
Loss :  1.8775123357772827 4.215326309204102 4.215326309204102
Loss :  1.8733283281326294 3.5588700771331787 3.5588700771331787
Loss :  1.9389879703521729 4.266995429992676 4.266995429992676
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9066078662872314 4.341965675354004 4.341965675354004
Loss :  1.971304178237915 4.217315196990967 4.217315196990967
Loss :  1.951412320137024 4.229133129119873 4.229133129119873
Total LOSS train 4.372140939419086 valid 4.26385235786438
CE LOSS train 1.8700106088931745 valid 0.487853080034256
Contrastive LOSS train 4.372140939419086 valid 1.0572832822799683
EPOCH 72:
Loss :  1.9202766418457031 3.6187233924865723 3.6187233924865723
Loss :  1.9851183891296387 4.124665260314941 4.124665260314941
Loss :  1.876338005065918 4.460165977478027 4.460165977478027
Loss :  1.9082176685333252 4.4677581787109375 4.4677581787109375
Loss :  1.9318429231643677 4.290741443634033 4.290741443634033
Loss :  1.9144234657287598 4.551604270935059 4.551604270935059
Loss :  1.9319874048233032 4.573683738708496 4.573683738708496
Loss :  1.9695079326629639 4.097956657409668 4.097956657409668
Loss :  1.94829523563385 4.138663291931152 4.138663291931152
Loss :  1.9680187702178955 3.9624686241149902 3.9624686241149902
Loss :  1.9539906978607178 4.1183552742004395 4.1183552742004395
Loss :  1.8995449542999268 4.3709397315979 4.3709397315979
Loss :  1.9316455125808716 4.303990840911865 4.303990840911865
Loss :  1.9075696468353271 4.144738674163818 4.144738674163818
Loss :  1.9503726959228516 3.8165557384490967 3.8165557384490967
Loss :  1.9711045026779175 4.293709754943848 4.293709754943848
Loss :  1.9328378438949585 3.9599387645721436 3.9599387645721436
Loss :  1.9678609371185303 3.747807025909424 3.747807025909424
Loss :  1.8928656578063965 3.6248576641082764 3.6248576641082764
Loss :  2.0009918212890625 4.022739887237549 4.022739887237549
  batch 20 loss: 2.0009918212890625, 4.022739887237549, 4.022739887237549
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9197746515274048 3.6776936054229736 3.6776936054229736
Loss :  1.9283860921859741 4.03000020980835 4.03000020980835
Loss :  1.949602484703064 4.047435283660889 4.047435283660889
Loss :  1.9356664419174194 3.670815944671631 3.670815944671631
Loss :  1.9601811170578003 4.327956199645996 4.327956199645996
Loss :  1.9221522808074951 4.037507057189941 4.037507057189941
Loss :  1.953917145729065 4.0246052742004395 4.0246052742004395
Loss :  1.912259817123413 3.667099714279175 3.667099714279175
Loss :  1.8552420139312744 3.7373292446136475 3.7373292446136475
Loss :  1.906430959701538 3.912700653076172 3.912700653076172
Loss :  1.8498287200927734 4.498085021972656 4.498085021972656
Loss :  1.8810269832611084 4.595742702484131 4.595742702484131
Loss :  1.8999539613723755 4.562220096588135 4.562220096588135
Loss :  1.9027940034866333 4.393308162689209 4.393308162689209
Loss :  1.8653340339660645 4.213302135467529 4.213302135467529
Loss :  1.8635749816894531 3.9981582164764404 3.9981582164764404
Loss :  1.8853142261505127 3.8994228839874268 3.8994228839874268
Loss :  2.000744581222534 4.140501499176025 4.140501499176025
Loss :  1.978254795074463 4.153258323669434 4.153258323669434
Loss :  2.0024588108062744 4.153676986694336 4.153676986694336
  batch 40 loss: 2.0024588108062744, 4.153676986694336, 4.153676986694336
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9396721124649048 4.499647617340088 4.499647617340088
Loss :  1.8902525901794434 4.271499156951904 4.271499156951904
Loss :  1.8127776384353638 4.1457977294921875 4.1457977294921875
Loss :  1.8541197776794434 4.349362850189209 4.349362850189209
Loss :  1.9390851259231567 4.325328350067139 4.325328350067139
Loss :  2.004734992980957 4.520646095275879 4.520646095275879
Loss :  2.0139646530151367 4.519211292266846 4.519211292266846
Loss :  1.8598538637161255 4.476546764373779 4.476546764373779
Loss :  1.9997212886810303 4.564640522003174 4.564640522003174
Loss :  1.8455108404159546 4.540008544921875 4.540008544921875
Loss :  1.9281795024871826 4.506178855895996 4.506178855895996
Loss :  1.9227755069732666 4.470139026641846 4.470139026641846
Loss :  1.8589458465576172 4.516844272613525 4.516844272613525
Loss :  1.932515025138855 4.347633361816406 4.347633361816406
Loss :  1.8382351398468018 4.441580295562744 4.441580295562744
Loss :  1.8721449375152588 4.495547294616699 4.495547294616699
Loss :  1.8883291482925415 4.537835121154785 4.537835121154785
Loss :  1.8281617164611816 4.356415748596191 4.356415748596191
Loss :  1.8898733854293823 4.52972936630249 4.52972936630249
Loss :  1.8775635957717896 4.619170665740967 4.619170665740967
  batch 60 loss: 1.8775635957717896, 4.619170665740967, 4.619170665740967
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.8344035148620605 4.726202011108398 4.726202011108398
Loss :  1.8757773637771606 4.581318378448486 4.581318378448486
Loss :  1.8100636005401611 4.499656677246094 4.499656677246094
Loss :  1.7948473691940308 4.463029861450195 4.463029861450195
Loss :  1.7817802429199219 4.013143062591553 4.013143062591553
Loss :  1.8784692287445068 4.412791728973389 4.412791728973389
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2], device='cuda:0')
Loss :  1.8721911907196045 4.495789051055908 4.495789051055908
Loss :  1.865493655204773 4.333409786224365 4.333409786224365
Loss :  1.9305227994918823 4.244394302368164 4.244394302368164
Total LOSS train 4.242276866619403 valid 4.3715962171554565
CE LOSS train 1.9112153475101177 valid 0.4826306998729706
Contrastive LOSS train 4.242276866619403 valid 1.061098575592041
EPOCH 73:
Loss :  1.8410216569900513 4.265787601470947 4.265787601470947
Loss :  1.890803337097168 4.5825347900390625 4.5825347900390625
Loss :  1.8772772550582886 4.418545722961426 4.418545722961426
Loss :  1.8735673427581787 4.488569259643555 4.488569259643555
Loss :  1.8968968391418457 4.237832546234131 4.237832546234131
Loss :  1.8064265251159668 4.3514556884765625 4.3514556884765625
Loss :  1.803792119026184 4.647611141204834 4.647611141204834
Loss :  1.7670960426330566 4.617348670959473 4.617348670959473
Loss :  1.8176474571228027 4.466742038726807 4.466742038726807
Loss :  1.8629356622695923 4.311891555786133 4.311891555786133
Loss :  1.8087323904037476 4.527755260467529 4.527755260467529
Loss :  1.8904255628585815 4.4552812576293945 4.4552812576293945
Loss :  1.848954439163208 4.5395917892456055 4.5395917892456055
Loss :  1.809832215309143 4.536644458770752 4.536644458770752
Loss :  1.785699725151062 4.35923957824707 4.35923957824707
Loss :  1.8602564334869385 4.5099287033081055 4.5099287033081055
Loss :  1.864796757698059 4.5072197914123535 4.5072197914123535
Loss :  1.8234909772872925 4.4184370040893555 4.4184370040893555
Loss :  1.8288238048553467 4.4746856689453125 4.4746856689453125
Loss :  1.8756135702133179 4.4110188484191895 4.4110188484191895
  batch 20 loss: 1.8756135702133179, 4.4110188484191895, 4.4110188484191895
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.8599154949188232 4.318648815155029 4.318648815155029
Loss :  1.7732164859771729 4.553773403167725 4.553773403167725
Loss :  1.84706711769104 4.552328109741211 4.552328109741211
Loss :  1.7862430810928345 4.490361213684082 4.490361213684082
Loss :  1.8937666416168213 4.520599365234375 4.520599365234375
Loss :  1.7594220638275146 4.383813858032227 4.383813858032227
Loss :  1.7529900074005127 4.787328243255615 4.787328243255615
Loss :  1.9340815544128418 4.416589260101318 4.416589260101318
Loss :  1.8814256191253662 4.408391952514648 4.408391952514648
Loss :  1.7296222448349 4.508945941925049 4.508945941925049
Loss :  1.8081990480422974 4.5638628005981445 4.5638628005981445
Loss :  1.788908839225769 4.7174391746521 4.7174391746521
Loss :  1.7344722747802734 4.586130142211914 4.586130142211914
Loss :  1.8674854040145874 4.44880485534668 4.44880485534668
Loss :  1.9099549055099487 4.5015716552734375 4.5015716552734375
Loss :  1.8560243844985962 4.493886947631836 4.493886947631836
Loss :  1.8504201173782349 4.443676948547363 4.443676948547363
Loss :  1.8929351568222046 4.371377944946289 4.371377944946289
Loss :  1.9194749593734741 4.482598781585693 4.482598781585693
Loss :  2.1009068489074707 4.36746072769165 4.36746072769165
  batch 40 loss: 2.1009068489074707, 4.36746072769165, 4.36746072769165
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8563036918640137 4.412134647369385 4.412134647369385
Loss :  1.972809910774231 4.484193801879883 4.484193801879883
Loss :  1.823655366897583 4.3651814460754395 4.3651814460754395
Loss :  1.7743558883666992 4.437145709991455 4.437145709991455
Loss :  1.8452401161193848 4.289168357849121 4.289168357849121
Loss :  1.864833116531372 4.52864408493042 4.52864408493042
Loss :  1.840625524520874 4.535444259643555 4.535444259643555
Loss :  1.8432062864303589 4.445105075836182 4.445105075836182
Loss :  1.838642954826355 4.4182000160217285 4.4182000160217285
Loss :  1.8133206367492676 4.433990955352783 4.433990955352783
Loss :  1.8569685220718384 4.587352275848389 4.587352275848389
Loss :  1.8368552923202515 4.463735580444336 4.463735580444336
Loss :  1.8245158195495605 4.470082759857178 4.470082759857178
Loss :  1.8812534809112549 4.413736343383789 4.413736343383789
Loss :  1.8380587100982666 4.408350467681885 4.408350467681885
Loss :  1.8938369750976562 4.658074378967285 4.658074378967285
Loss :  1.8743942975997925 4.574508190155029 4.574508190155029
Loss :  1.8489019870758057 4.465882301330566 4.465882301330566
Loss :  1.8913555145263672 4.412078857421875 4.412078857421875
Loss :  1.8064792156219482 4.5959296226501465 4.5959296226501465
  batch 60 loss: 1.8064792156219482, 4.5959296226501465, 4.5959296226501465
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.8303688764572144 4.523545265197754 4.523545265197754
Loss :  1.8772331476211548 4.396416664123535 4.396416664123535
Loss :  1.8846884965896606 4.454360008239746 4.454360008239746
Loss :  1.8529301881790161 4.400170803070068 4.400170803070068
Loss :  1.8683396577835083 4.276323318481445 4.276323318481445
Loss :  10.104630470275879 4.442851543426514 4.442851543426514
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4], device='cuda:0')
Loss :  3.1278774738311768 4.377198696136475 4.377198696136475
Loss :  15.37390422821045 4.36972188949585 4.36972188949585
Loss :  5.417128086090088 4.266592502593994 4.266592502593994
Total LOSS train 4.469161026294415 valid 4.364091157913208
CE LOSS train 1.8479968621180607 valid 1.354282021522522
Contrastive LOSS train 4.469161026294415 valid 1.0666481256484985
EPOCH 74:
Loss :  1.880463719367981 4.2836174964904785 4.2836174964904785
Loss :  1.9332737922668457 4.629078388214111 4.629078388214111
Loss :  1.8874496221542358 4.392487049102783 4.392487049102783
Loss :  1.8767422437667847 4.502726078033447 4.502726078033447
Loss :  1.883617639541626 4.3547210693359375 4.3547210693359375
Loss :  1.8873995542526245 4.488577842712402 4.488577842712402
Loss :  1.8762073516845703 4.64250373840332 4.64250373840332
Loss :  1.8591054677963257 4.316728115081787 4.316728115081787
Loss :  1.8613438606262207 4.605886459350586 4.605886459350586
Loss :  1.878872275352478 4.2901153564453125 4.2901153564453125
Loss :  1.8793327808380127 4.598919868469238 4.598919868469238
Loss :  1.8634388446807861 4.571262836456299 4.571262836456299
Loss :  1.8663777112960815 4.4358110427856445 4.4358110427856445
Loss :  1.8528541326522827 4.618951320648193 4.618951320648193
Loss :  1.8808010816574097 4.472758769989014 4.472758769989014
Loss :  1.946695327758789 4.580147743225098 4.580147743225098
Loss :  1.9133163690567017 4.473480224609375 4.473480224609375
Loss :  1.9222835302352905 4.358666896820068 4.358666896820068
Loss :  1.8760849237442017 4.317078590393066 4.317078590393066
Loss :  1.9082664251327515 4.490812301635742 4.490812301635742
  batch 20 loss: 1.9082664251327515, 4.490812301635742, 4.490812301635742
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8839471340179443 4.349094867706299 4.349094867706299
Loss :  1.8538457155227661 4.447749614715576 4.447749614715576
Loss :  1.8606384992599487 4.76994514465332 4.76994514465332
Loss :  1.899938941001892 4.684916019439697 4.684916019439697
Loss :  1.9918161630630493 4.643397331237793 4.643397331237793
Loss :  1.9690608978271484 4.654668807983398 4.654668807983398
Loss :  1.9608932733535767 4.795949459075928 4.795949459075928
Loss :  1.9268758296966553 4.4050822257995605 4.4050822257995605
Loss :  1.8886111974716187 4.4906721115112305 4.4906721115112305
Loss :  1.826464056968689 4.50480842590332 4.50480842590332
Loss :  1.823900818824768 4.630411624908447 4.630411624908447
Loss :  1.850385069847107 4.641539096832275 4.641539096832275
Loss :  1.8577566146850586 4.492857456207275 4.492857456207275
Loss :  1.8160151243209839 4.418652534484863 4.418652534484863
Loss :  1.7886148691177368 4.440972328186035 4.440972328186035
Loss :  1.7915492057800293 4.5588788986206055 4.5588788986206055
Loss :  1.8443104028701782 4.381247520446777 4.381247520446777
Loss :  1.8036922216415405 4.444356918334961 4.444356918334961
Loss :  1.9010889530181885 4.4640703201293945 4.4640703201293945
Loss :  1.8757281303405762 4.549805164337158 4.549805164337158
  batch 40 loss: 1.8757281303405762, 4.549805164337158, 4.549805164337158
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 3, 4, 5], device='cuda:0')
Loss :  1.8261361122131348 4.549840927124023 4.549840927124023
Loss :  1.7966970205307007 4.46135139465332 4.46135139465332
Loss :  1.797986388206482 4.350489616394043 4.350489616394043
Loss :  1.8484748601913452 4.518383979797363 4.518383979797363
Loss :  1.7908765077590942 4.373296737670898 4.373296737670898
Loss :  1.8405187129974365 4.409531593322754 4.409531593322754
Loss :  1.839177131652832 4.299842834472656 4.299842834472656
Loss :  1.7890068292617798 4.1414079666137695 4.1414079666137695
Loss :  1.825234293937683 4.034024715423584 4.034024715423584
Loss :  1.8056319952011108 4.1204514503479 4.1204514503479
Loss :  1.7913895845413208 4.226596832275391 4.226596832275391
Loss :  1.8858981132507324 4.391842365264893 4.391842365264893
Loss :  1.8527320623397827 4.388189315795898 4.388189315795898
Loss :  1.8810176849365234 4.324660778045654 4.324660778045654
Loss :  1.8087728023529053 4.413133144378662 4.413133144378662
Loss :  1.888447642326355 4.427404403686523 4.427404403686523
Loss :  1.8966879844665527 4.557459354400635 4.557459354400635
Loss :  1.832862377166748 4.501803874969482 4.501803874969482
Loss :  1.8954826593399048 4.6279072761535645 4.6279072761535645
Loss :  1.8617982864379883 4.411128044128418 4.411128044128418
  batch 60 loss: 1.8617982864379883, 4.411128044128418, 4.411128044128418
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 3, 4, 5], device='cuda:0')
Loss :  1.803681492805481 4.507937431335449 4.507937431335449
Loss :  1.776808738708496 4.419894695281982 4.419894695281982
Loss :  1.8661543130874634 4.352906227111816 4.352906227111816
Loss :  1.8398022651672363 4.532666206359863 4.532666206359863
Loss :  1.8456196784973145 4.171506881713867 4.171506881713867
Loss :  1.771105170249939 4.4552412033081055 4.4552412033081055
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
Loss :  1.7756245136260986 4.436037540435791 4.436037540435791
Loss :  1.762557029724121 4.298070907592773 4.298070907592773
Loss :  1.8259609937667847 4.236634731292725 4.236634731292725
Total LOSS train 4.457031770852896 valid 4.356496095657349
CE LOSS train 1.8620916201518132 valid 0.45649024844169617
Contrastive LOSS train 4.457031770852896 valid 1.0591586828231812
EPOCH 75:
Loss :  1.876608967781067 4.248658657073975 4.248658657073975
Loss :  1.891475796699524 4.622161865234375 4.622161865234375
Loss :  1.9040230512619019 4.342625141143799 4.342625141143799
Loss :  1.858994483947754 4.346482753753662 4.346482753753662
Loss :  1.894382357597351 4.121394634246826 4.121394634246826
Loss :  1.9083508253097534 4.501510143280029 4.501510143280029
Loss :  1.8906476497650146 4.163609504699707 4.163609504699707
Loss :  1.8775460720062256 4.04771089553833 4.04771089553833
Loss :  1.8604809045791626 4.119043350219727 4.119043350219727
Loss :  1.8783146142959595 4.007701873779297 4.007701873779297
Loss :  1.8862203359603882 4.358187675476074 4.358187675476074
Loss :  1.863326072692871 4.660001277923584 4.660001277923584
Loss :  1.9827719926834106 4.453301906585693 4.453301906585693
Loss :  1.8949438333511353 4.6245269775390625 4.6245269775390625
Loss :  1.835546851158142 4.336218357086182 4.336218357086182
Loss :  1.8616585731506348 4.117061138153076 4.117061138153076
Loss :  1.85065758228302 4.481748104095459 4.481748104095459
Loss :  1.8916597366333008 4.8044281005859375 4.8044281005859375
Loss :  1.8928128480911255 4.582617282867432 4.582617282867432
Loss :  1.8780097961425781 4.418868064880371 4.418868064880371
  batch 20 loss: 1.8780097961425781, 4.418868064880371, 4.418868064880371
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  2.0208444595336914 4.544546604156494 4.544546604156494
Loss :  1.9153448343276978 4.528833389282227 4.528833389282227
Loss :  1.884221076965332 4.365212440490723 4.365212440490723
Loss :  1.9608875513076782 4.498876571655273 4.498876571655273
Loss :  1.9055566787719727 4.241693019866943 4.241693019866943
Loss :  1.9303785562515259 4.245759010314941 4.245759010314941
Loss :  1.9017412662506104 4.675105094909668 4.675105094909668
Loss :  1.9031115770339966 4.470499515533447 4.470499515533447
Loss :  1.9006985425949097 4.4866814613342285 4.4866814613342285
Loss :  1.8897799253463745 4.337584495544434 4.337584495544434
Loss :  1.8706400394439697 4.626022815704346 4.626022815704346
Loss :  1.888770580291748 4.6106462478637695 4.6106462478637695
Loss :  1.9031813144683838 4.3498711585998535 4.3498711585998535
Loss :  1.9230234622955322 4.540255069732666 4.540255069732666
Loss :  1.9122055768966675 4.565257549285889 4.565257549285889
Loss :  1.9226614236831665 4.52040433883667 4.52040433883667
Loss :  1.9431687593460083 4.498502254486084 4.498502254486084
Loss :  1.959442138671875 4.3489603996276855 4.3489603996276855
Loss :  1.9796981811523438 4.552378177642822 4.552378177642822
Loss :  1.8706308603286743 4.638448715209961 4.638448715209961
  batch 40 loss: 1.8706308603286743, 4.638448715209961, 4.638448715209961
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.9421926736831665 4.361686706542969 4.361686706542969
Loss :  1.958407998085022 4.3520026206970215 4.3520026206970215
Loss :  1.9237455129623413 4.377854824066162 4.377854824066162
Loss :  1.930177092552185 4.209299564361572 4.209299564361572
Loss :  1.9394259452819824 4.255528450012207 4.255528450012207
Loss :  1.9004313945770264 4.4093217849731445 4.4093217849731445
Loss :  1.972070336341858 4.608597755432129 4.608597755432129
Loss :  1.8734855651855469 4.584641456604004 4.584641456604004
Loss :  1.8670933246612549 4.4568352699279785 4.4568352699279785
Loss :  1.9186245203018188 4.624648094177246 4.624648094177246
Loss :  1.9161118268966675 4.475249290466309 4.475249290466309
Loss :  1.9207587242126465 4.383880138397217 4.383880138397217
Loss :  1.930448055267334 4.443118572235107 4.443118572235107
Loss :  1.9873631000518799 4.455028057098389 4.455028057098389
Loss :  1.9143770933151245 4.551757335662842 4.551757335662842
Loss :  2.0178592205047607 4.491560935974121 4.491560935974121
Loss :  1.9514292478561401 4.4685282707214355 4.4685282707214355
Loss :  1.9332010746002197 4.385644435882568 4.385644435882568
Loss :  1.9692401885986328 4.422605037689209 4.422605037689209
Loss :  1.9297547340393066 4.54349946975708 4.54349946975708
  batch 60 loss: 1.9297547340393066, 4.54349946975708, 4.54349946975708
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9942514896392822 4.524407863616943 4.524407863616943
Loss :  1.9224587678909302 4.624756336212158 4.624756336212158
Loss :  1.9192312955856323 4.587713241577148 4.587713241577148
Loss :  1.9095463752746582 4.541773319244385 4.541773319244385
Loss :  1.9692860841751099 4.085874557495117 4.085874557495117
Loss :  1.9182542562484741 4.501088619232178 4.501088619232178
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  2.0126490592956543 4.34885311126709 4.34885311126709
Loss :  1.9743924140930176 4.307710647583008 4.307710647583008
Loss :  1.8780782222747803 4.174666881561279 4.174666881561279
Total LOSS train 4.434295529585619 valid 4.333079814910889
CE LOSS train 1.9150060121829693 valid 0.46951955556869507
Contrastive LOSS train 4.434295529585619 valid 1.0436667203903198
EPOCH 76:
Loss :  2.0163536071777344 4.401669025421143 4.401669025421143
Loss :  1.9764344692230225 4.524521827697754 4.524521827697754
Loss :  1.9231892824172974 4.259135723114014 4.259135723114014
Loss :  1.9339289665222168 4.526010036468506 4.526010036468506
Loss :  1.9343301057815552 4.360666751861572 4.360666751861572
Loss :  1.9893929958343506 4.405786037445068 4.405786037445068
Loss :  1.9481722116470337 4.431403160095215 4.431403160095215
Loss :  1.974395990371704 4.321794033050537 4.321794033050537
Loss :  1.99431312084198 4.475022315979004 4.475022315979004
Loss :  1.9626001119613647 4.462246417999268 4.462246417999268
Loss :  2.023581027984619 4.763947486877441 4.763947486877441
Loss :  1.99263596534729 4.673551559448242 4.673551559448242
Loss :  1.9891905784606934 4.765935897827148 4.765935897827148
Loss :  2.0232434272766113 4.55919075012207 4.55919075012207
Loss :  1.9848181009292603 4.536195278167725 4.536195278167725
Loss :  2.002872943878174 4.671858310699463 4.671858310699463
Loss :  1.988505244255066 4.546234130859375 4.546234130859375
Loss :  1.980409860610962 4.804931640625 4.804931640625
Loss :  2.026482105255127 4.497220039367676 4.497220039367676
Loss :  1.9029107093811035 4.538344383239746 4.538344383239746
  batch 20 loss: 1.9029107093811035, 4.538344383239746, 4.538344383239746
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9801746606826782 4.531790733337402 4.531790733337402
Loss :  2.041322946548462 4.603097438812256 4.603097438812256
Loss :  1.9994583129882812 4.63584566116333 4.63584566116333
Loss :  2.0336508750915527 4.518515110015869 4.518515110015869
Loss :  1.9827207326889038 4.632213115692139 4.632213115692139
Loss :  2.0264039039611816 4.727282524108887 4.727282524108887
Loss :  1.9696159362792969 4.504770755767822 4.504770755767822
Loss :  2.028548002243042 4.314119815826416 4.314119815826416
Loss :  2.03511118888855 4.454180717468262 4.454180717468262
Loss :  2.0069122314453125 4.478798866271973 4.478798866271973
Loss :  2.0624382495880127 4.429487705230713 4.429487705230713
Loss :  2.0153212547302246 4.492985248565674 4.492985248565674
Loss :  2.014073133468628 4.447107315063477 4.447107315063477
Loss :  2.0451526641845703 4.533942222595215 4.533942222595215
Loss :  2.0630125999450684 4.505316734313965 4.505316734313965
Loss :  2.057615280151367 4.682730674743652 4.682730674743652
Loss :  2.0669236183166504 4.385329723358154 4.385329723358154
Loss :  1.9692960977554321 4.522343158721924 4.522343158721924
Loss :  1.9885281324386597 4.294163703918457 4.294163703918457
Loss :  1.9459739923477173 4.506669044494629 4.506669044494629
  batch 40 loss: 1.9459739923477173, 4.506669044494629, 4.506669044494629
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2], device='cuda:0')
Loss :  1.9701508283615112 4.492412090301514 4.492412090301514
Loss :  2.0359671115875244 4.403967380523682 4.403967380523682
Loss :  2.048797369003296 4.432981491088867 4.432981491088867
Loss :  2.019308567047119 4.50059175491333 4.50059175491333
Loss :  2.057706832885742 4.33432674407959 4.33432674407959
Loss :  1.992042064666748 4.40299129486084 4.40299129486084
Loss :  1.9205553531646729 4.413915634155273 4.413915634155273
Loss :  2.0320520401000977 4.400623798370361 4.400623798370361
Loss :  1.9462873935699463 4.269477844238281 4.269477844238281
Loss :  2.052832841873169 4.551168441772461 4.551168441772461
Loss :  2.020716667175293 4.526123046875 4.526123046875
Loss :  2.026616096496582 4.367821216583252 4.367821216583252
Loss :  2.0034050941467285 4.396069526672363 4.396069526672363
Loss :  2.0071752071380615 4.259407997131348 4.259407997131348
Loss :  2.0333521366119385 4.460896015167236 4.460896015167236
Loss :  2.038132905960083 4.4259161949157715 4.4259161949157715
Loss :  2.0298445224761963 4.615655422210693 4.615655422210693
Loss :  2.0305304527282715 4.573647499084473 4.573647499084473
Loss :  2.031623363494873 4.6792311668396 4.6792311668396
Loss :  2.0671398639678955 4.449888706207275 4.449888706207275
  batch 60 loss: 2.0671398639678955, 4.449888706207275, 4.449888706207275
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2], device='cuda:0')
Loss :  2.024853229522705 4.496873378753662 4.496873378753662
Loss :  2.0124027729034424 4.456737995147705 4.456737995147705
Loss :  2.0147387981414795 4.409201145172119 4.409201145172119
Loss :  2.059041976928711 4.45855188369751 4.45855188369751
Loss :  2.1445975303649902 4.232309818267822 4.232309818267822
Loss :  2.092550754547119 4.377997398376465 4.377997398376465
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2], device='cuda:0')
Loss :  2.086827516555786 4.410129070281982 4.410129070281982
Loss :  2.0809364318847656 4.327461242675781 4.327461242675781
Loss :  2.1660778522491455 4.280984878540039 4.280984878540039
Total LOSS train 4.488263731736403 valid 4.349143147468567
CE LOSS train 2.0079982409110437 valid 0.5415194630622864
Contrastive LOSS train 4.488263731736403 valid 1.0702462196350098
EPOCH 77:
Loss :  2.0366668701171875 4.33577823638916 4.33577823638916
Loss :  1.9587576389312744 4.546480178833008 4.546480178833008
Loss :  2.0612659454345703 4.259855270385742 4.259855270385742
Loss :  1.9699878692626953 4.4193925857543945 4.4193925857543945
Loss :  1.9692569971084595 4.296483039855957 4.296483039855957
Loss :  1.987717628479004 4.533812046051025 4.533812046051025
Loss :  1.9729878902435303 4.4605326652526855 4.4605326652526855
Loss :  2.004626750946045 4.456116676330566 4.456116676330566
Loss :  1.9998172521591187 4.321864604949951 4.321864604949951
Loss :  2.0255227088928223 4.270458698272705 4.270458698272705
Loss :  1.9389501810073853 4.637697696685791 4.637697696685791
Loss :  1.899134635925293 4.989178657531738 4.989178657531738
Loss :  2.076842784881592 4.583081245422363 4.583081245422363
Loss :  2.0690524578094482 4.618448257446289 4.618448257446289
Loss :  1.9413913488388062 4.508522987365723 4.508522987365723
Loss :  2.03784441947937 4.609685897827148 4.609685897827148
Loss :  2.075910806655884 4.460275173187256 4.460275173187256
Loss :  1.9873839616775513 4.443871021270752 4.443871021270752
Loss :  2.0611343383789062 4.6916184425354 4.6916184425354
Loss :  1.9391366243362427 4.616923809051514 4.616923809051514
  batch 20 loss: 1.9391366243362427, 4.616923809051514, 4.616923809051514
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4], device='cuda:0')
Loss :  1.9619790315628052 4.309852600097656 4.309852600097656
Loss :  2.058053970336914 4.61276388168335 4.61276388168335
Loss :  1.9774951934814453 4.410928726196289 4.410928726196289
Loss :  2.0688364505767822 4.733506679534912 4.733506679534912
Loss :  2.0698835849761963 4.653448104858398 4.653448104858398
Loss :  2.0285637378692627 4.514075756072998 4.514075756072998
Loss :  1.9977658987045288 4.381393909454346 4.381393909454346
Loss :  2.0025224685668945 4.325137138366699 4.325137138366699
Loss :  2.080794334411621 4.495777606964111 4.495777606964111
Loss :  1.9913192987442017 4.447715759277344 4.447715759277344
Loss :  2.054746389389038 4.467987537384033 4.467987537384033
Loss :  2.0593819618225098 4.670547962188721 4.670547962188721
Loss :  2.070338487625122 4.400620460510254 4.400620460510254
Loss :  2.0671489238739014 4.459532260894775 4.459532260894775
Loss :  2.0638511180877686 4.530856609344482 4.530856609344482
Loss :  2.078744649887085 4.5078253746032715 4.5078253746032715
Loss :  2.0524356365203857 4.355607032775879 4.355607032775879
Loss :  2.0093395709991455 4.372883319854736 4.372883319854736
Loss :  2.0367019176483154 4.39699649810791 4.39699649810791
Loss :  2.0248656272888184 4.465620040893555 4.465620040893555
  batch 40 loss: 2.0248656272888184, 4.465620040893555, 4.465620040893555
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3], device='cuda:0')
Loss :  2.0799458026885986 4.51173734664917 4.51173734664917
Loss :  2.020552635192871 4.361868858337402 4.361868858337402
Loss :  2.0184123516082764 4.72851037979126 4.72851037979126
Loss :  2.004927396774292 4.509573936462402 4.509573936462402
Loss :  2.0170559883117676 4.297129154205322 4.297129154205322
Loss :  2.0275073051452637 4.494771957397461 4.494771957397461
Loss :  2.0046653747558594 4.436648845672607 4.436648845672607
Loss :  2.034517526626587 4.432169437408447 4.432169437408447
Loss :  1.988811731338501 4.381405353546143 4.381405353546143
Loss :  2.053631544113159 4.397659778594971 4.397659778594971
Loss :  1.9476898908615112 4.495974540710449 4.495974540710449
Loss :  2.036783218383789 4.496382236480713 4.496382236480713
Loss :  1.9997811317443848 4.401233196258545 4.401233196258545
Loss :  1.9334712028503418 4.547015190124512 4.547015190124512
Loss :  2.020414352416992 4.506650924682617 4.506650924682617
Loss :  1.9703904390335083 4.456604957580566 4.456604957580566
Loss :  1.9608579874038696 4.347334861755371 4.347334861755371
Loss :  2.054267406463623 4.510826110839844 4.510826110839844
Loss :  2.0048553943634033 4.6560821533203125 4.6560821533203125
Loss :  1.989427089691162 4.5202460289001465 4.5202460289001465
  batch 60 loss: 1.989427089691162, 4.5202460289001465, 4.5202460289001465
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 3, 4, 5], device='cuda:0')
Loss :  2.0285909175872803 4.41268253326416 4.41268253326416
Loss :  1.9938337802886963 4.57401704788208 4.57401704788208
Loss :  1.9440504312515259 4.3925604820251465 4.3925604820251465
Loss :  1.9735311269760132 4.454620838165283 4.454620838165283
Loss :  2.014857053756714 4.157742023468018 4.157742023468018
Loss :  1.9398856163024902 4.4768900871276855 4.4768900871276855
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([4], device='cuda:0')
Loss :  1.9596983194351196 4.51385498046875 4.51385498046875
Loss :  1.955030083656311 4.354986667633057 4.354986667633057
Loss :  1.9230183362960815 4.196279525756836 4.196279525756836
Total LOSS train 4.477763117276705 valid 4.385502815246582
CE LOSS train 2.013707483731783 valid 0.4807545840740204
Contrastive LOSS train 4.477763117276705 valid 1.049069881439209
EPOCH 78:
Loss :  1.983507752418518 4.385798454284668 4.385798454284668
Loss :  1.9818227291107178 4.548285484313965 4.548285484313965
Loss :  1.9214938879013062 4.366079330444336 4.366079330444336
Loss :  1.9574531316757202 4.395297050476074 4.395297050476074
Loss :  1.9524739980697632 4.385785102844238 4.385785102844238
Loss :  1.9390289783477783 4.388847827911377 4.388847827911377
Loss :  1.8893836736679077 4.623660087585449 4.623660087585449
Loss :  1.9658344984054565 4.331759452819824 4.331759452819824
Loss :  1.9080071449279785 4.506571292877197 4.506571292877197
Loss :  1.9308247566223145 4.324790000915527 4.324790000915527
Loss :  2.01444935798645 4.456071376800537 4.456071376800537
Loss :  1.8912500143051147 4.58745002746582 4.58745002746582
Loss :  1.9860209226608276 4.580261707305908 4.580261707305908
Loss :  1.9239786863327026 4.476562976837158 4.476562976837158
Loss :  1.9470796585083008 4.472755432128906 4.472755432128906
Loss :  1.8994160890579224 4.45929479598999 4.45929479598999
Loss :  1.930439829826355 4.632801532745361 4.632801532745361
Loss :  1.9666391611099243 4.394711971282959 4.394711971282959
Loss :  1.9350050687789917 4.42338228225708 4.42338228225708
Loss :  1.9305750131607056 4.4303717613220215 4.4303717613220215
  batch 20 loss: 1.9305750131607056, 4.4303717613220215, 4.4303717613220215
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3, 4, 5], device='cuda:0')
Loss :  1.9459761381149292 4.308173656463623 4.308173656463623
Loss :  1.9395304918289185 4.354702949523926 4.354702949523926
Loss :  1.951141595840454 4.842217922210693 4.842217922210693
Loss :  1.9536746740341187 4.5564117431640625 4.5564117431640625
Loss :  1.942871332168579 4.57155704498291 4.57155704498291
Loss :  1.9610322713851929 4.367424011230469 4.367424011230469
Loss :  1.978824496269226 4.595590114593506 4.595590114593506
Loss :  1.9445579051971436 4.641476631164551 4.641476631164551
Loss :  1.9620039463043213 4.49915075302124 4.49915075302124
Loss :  1.9416011571884155 4.71004056930542 4.71004056930542
Loss :  1.9462485313415527 4.811775207519531 4.811775207519531
Loss :  1.9358702898025513 4.6995930671691895 4.6995930671691895
Loss :  1.9327958822250366 4.322890281677246 4.322890281677246
Loss :  1.9441685676574707 4.4194769859313965 4.4194769859313965
Loss :  1.9386156797409058 4.574219703674316 4.574219703674316
Loss :  1.9538973569869995 4.781275749206543 4.781275749206543
Loss :  1.9425325393676758 4.4658966064453125 4.4658966064453125
Loss :  1.9574717283248901 4.589871406555176 4.589871406555176
Loss :  1.8992791175842285 4.395236015319824 4.395236015319824
Loss :  1.8905824422836304 4.522823810577393 4.522823810577393
  batch 40 loss: 1.8905824422836304, 4.522823810577393, 4.522823810577393
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3, 4, 5], device='cuda:0')
Loss :  1.9198479652404785 5.004649639129639 5.004649639129639
Loss :  1.9408735036849976 4.468667507171631 4.468667507171631
Loss :  1.901977300643921 4.429268836975098 4.429268836975098
Loss :  1.9321727752685547 4.577934741973877 4.577934741973877
Loss :  1.925352692604065 4.247535228729248 4.247535228729248
Loss :  1.9346654415130615 4.382997512817383 4.382997512817383
Loss :  1.9325816631317139 4.446800231933594 4.446800231933594
Loss :  1.9114598035812378 4.462689399719238 4.462689399719238
Loss :  1.9036718606948853 4.6313557624816895 4.6313557624816895
Loss :  1.914139986038208 4.530186653137207 4.530186653137207
Loss :  1.8656067848205566 4.571926593780518 4.571926593780518
Loss :  1.9144164323806763 4.413626670837402 4.413626670837402
Loss :  1.91402268409729 4.486852645874023 4.486852645874023
Loss :  1.926899790763855 4.493097305297852 4.493097305297852
Loss :  1.8990236520767212 4.4025774002075195 4.4025774002075195
Loss :  1.903708815574646 4.616661071777344 4.616661071777344
Loss :  1.959059238433838 4.78672981262207 4.78672981262207
Loss :  1.9089505672454834 4.8393659591674805 4.8393659591674805
Loss :  1.9292303323745728 4.744382381439209 4.744382381439209
Loss :  1.9652957916259766 4.520602703094482 4.520602703094482
  batch 60 loss: 1.9652957916259766, 4.520602703094482, 4.520602703094482
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9466277360916138 4.457695960998535 4.457695960998535
Loss :  1.8896676301956177 4.579101085662842 4.579101085662842
Loss :  1.8951911926269531 4.392223834991455 4.392223834991455
Loss :  1.8985505104064941 4.458908557891846 4.458908557891846
Loss :  1.887975811958313 4.17516565322876 4.17516565322876
Loss :  1.8892498016357422 4.470583915710449 4.470583915710449
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([4], device='cuda:0')
Loss :  1.8968476057052612 4.474308490753174 4.474308490753174
Loss :  1.884211778640747 4.335701942443848 4.335701942443848
Loss :  1.8953776359558105 4.172673225402832 4.172673225402832
Total LOSS train 4.512636081988995 valid 4.363316893577576
CE LOSS train 1.9328973916860728 valid 0.47384440898895264
Contrastive LOSS train 4.512636081988995 valid 1.043168306350708
EPOCH 79:
Loss :  1.9143723249435425 4.34409236907959 4.34409236907959
Loss :  1.8996483087539673 4.44895601272583 4.44895601272583
Loss :  1.9410654306411743 4.388784885406494 4.388784885406494
Loss :  1.9104596376419067 4.412568092346191 4.412568092346191
Loss :  1.9381523132324219 4.304921627044678 4.304921627044678
Loss :  1.9062668085098267 4.3153910636901855 4.3153910636901855
Loss :  1.894951343536377 4.521176338195801 4.521176338195801
Loss :  1.919739007949829 4.412276268005371 4.412276268005371
Loss :  1.8915643692016602 4.461712837219238 4.461712837219238
Loss :  1.8903346061706543 4.405517101287842 4.405517101287842
Loss :  1.8960661888122559 4.450829982757568 4.450829982757568
Loss :  1.8804394006729126 4.406881809234619 4.406881809234619
Loss :  1.8936734199523926 4.409557819366455 4.409557819366455
Loss :  1.8777363300323486 4.649824142456055 4.649824142456055
Loss :  1.8687820434570312 4.3879475593566895 4.3879475593566895
Loss :  1.9300084114074707 4.356499671936035 4.356499671936035
Loss :  1.9166488647460938 4.688354969024658 4.688354969024658
Loss :  1.9630603790283203 4.493326187133789 4.493326187133789
Loss :  1.8761240243911743 4.369904518127441 4.369904518127441
Loss :  1.8629140853881836 4.42889404296875 4.42889404296875
  batch 20 loss: 1.8629140853881836, 4.42889404296875, 4.42889404296875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4, 5], device='cuda:0')
Loss :  1.858073353767395 4.311654567718506 4.311654567718506
Loss :  1.8626234531402588 4.476140022277832 4.476140022277832
Loss :  1.869922161102295 4.471554279327393 4.471554279327393
Loss :  1.8863216638565063 4.442486763000488 4.442486763000488
Loss :  1.9109224081039429 4.651949405670166 4.651949405670166
Loss :  1.9483935832977295 4.460675239562988 4.460675239562988
Loss :  1.9050182104110718 4.635502338409424 4.635502338409424
Loss :  1.8708921670913696 4.4080376625061035 4.4080376625061035
Loss :  1.9032403230667114 4.366618633270264 4.366618633270264
Loss :  1.9555848836898804 4.448564529418945 4.448564529418945
Loss :  1.8661168813705444 4.514231204986572 4.514231204986572
Loss :  1.874602198600769 4.537071704864502 4.537071704864502
Loss :  1.898398756980896 4.515624523162842 4.515624523162842
Loss :  1.8780689239501953 4.42435359954834 4.42435359954834
Loss :  1.8677425384521484 4.526392459869385 4.526392459869385
Loss :  1.8671648502349854 4.447475910186768 4.447475910186768
Loss :  1.861624836921692 4.443112373352051 4.443112373352051
Loss :  1.9159907102584839 4.459449291229248 4.459449291229248
Loss :  1.9263979196548462 4.319825649261475 4.319825649261475
Loss :  1.9034814834594727 4.461879253387451 4.461879253387451
  batch 40 loss: 1.9034814834594727, 4.461879253387451, 4.461879253387451
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4, 5], device='cuda:0')
Loss :  1.9052478075027466 4.612889289855957 4.612889289855957
Loss :  1.886968970298767 4.5690789222717285 4.5690789222717285
Loss :  1.8486671447753906 4.4031524658203125 4.4031524658203125
Loss :  1.8538615703582764 4.4607672691345215 4.4607672691345215
Loss :  1.8659087419509888 4.311150550842285 4.311150550842285
Loss :  1.9129163026809692 4.4647216796875 4.4647216796875
Loss :  1.933443307876587 4.468369007110596 4.468369007110596
Loss :  1.8498938083648682 4.4375081062316895 4.4375081062316895
Loss :  1.915071964263916 4.361036777496338 4.361036777496338
Loss :  1.917069673538208 4.437913417816162 4.437913417816162
Loss :  1.909527063369751 4.380002021789551 4.380002021789551
Loss :  1.9074045419692993 4.363185405731201 4.363185405731201
Loss :  1.9036097526550293 4.575901508331299 4.575901508331299
Loss :  1.946966528892517 4.504981994628906 4.504981994628906
Loss :  1.8654974699020386 4.44883394241333 4.44883394241333
Loss :  1.9801949262619019 4.4501776695251465 4.4501776695251465
Loss :  1.9610495567321777 4.472620010375977 4.472620010375977
Loss :  1.889605164527893 4.48488712310791 4.48488712310791
Loss :  1.9425771236419678 4.5228118896484375 4.5228118896484375
Loss :  1.916106104850769 4.451009273529053 4.451009273529053
  batch 60 loss: 1.916106104850769, 4.451009273529053, 4.451009273529053
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3, 4, 5], device='cuda:0')
Loss :  1.9379706382751465 4.669670104980469 4.669670104980469
Loss :  1.9603139162063599 4.4765143394470215 4.4765143394470215
Loss :  1.932451605796814 4.430171489715576 4.430171489715576
Loss :  1.9124047756195068 4.415665149688721 4.415665149688721
Loss :  1.915817379951477 4.113009929656982 4.113009929656982
Loss :  2.3342835903167725 4.423475742340088 4.423475742340088
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([5], device='cuda:0')
Loss :  2.2990827560424805 4.404077053070068 4.404077053070068
Loss :  2.2943313121795654 4.357460975646973 4.357460975646973
Loss :  2.3756351470947266 4.2623372077941895 4.2623372077941895
Total LOSS train 4.4502468622647795 valid 4.36183774471283
CE LOSS train 1.9026636068637555 valid 0.5939087867736816
Contrastive LOSS train 4.4502468622647795 valid 1.0655843019485474
EPOCH 80:
Loss :  1.93538498878479 4.49619197845459 4.49619197845459
Loss :  1.920186161994934 4.59072208404541 4.59072208404541
Loss :  1.96394944190979 4.557309150695801 4.557309150695801
Loss :  1.9082387685775757 4.338272571563721 4.338272571563721
Loss :  1.9542196989059448 4.34666109085083 4.34666109085083
Loss :  1.9885807037353516 4.350398063659668 4.350398063659668
Loss :  1.932179570198059 4.593499660491943 4.593499660491943
Loss :  1.9248563051223755 4.326977729797363 4.326977729797363
Loss :  1.943766713142395 4.46121072769165 4.46121072769165
Loss :  1.966623067855835 4.407961845397949 4.407961845397949
Loss :  1.9781403541564941 4.485832214355469 4.485832214355469
Loss :  1.96938157081604 4.563793659210205 4.563793659210205
Loss :  1.9656260013580322 4.595095634460449 4.595095634460449
Loss :  1.9485399723052979 4.573803901672363 4.573803901672363
Loss :  1.910840392112732 4.354140758514404 4.354140758514404
Loss :  1.9955531358718872 4.564265727996826 4.564265727996826
Loss :  1.9980767965316772 4.3978447914123535 4.3978447914123535
Loss :  1.979061484336853 4.476443290710449 4.476443290710449
Loss :  1.9342999458312988 4.4039506912231445 4.4039506912231445
Loss :  1.9454787969589233 4.493067741394043 4.493067741394043
  batch 20 loss: 1.9454787969589233, 4.493067741394043, 4.493067741394043
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3, 4, 5], device='cuda:0')
Loss :  1.9860705137252808 4.366570949554443 4.366570949554443
Loss :  1.9513410329818726 4.496509075164795 4.496509075164795
Loss :  1.9846761226654053 4.530090808868408 4.530090808868408
Loss :  2.016244411468506 4.383421897888184 4.383421897888184
Loss :  2.022500514984131 4.467066287994385 4.467066287994385
Loss :  2.0118000507354736 4.3512349128723145 4.3512349128723145
Loss :  2.0339348316192627 4.647305965423584 4.647305965423584
Loss :  1.961031198501587 4.4182047843933105 4.4182047843933105
Loss :  1.992419958114624 4.309887886047363 4.309887886047363
Loss :  1.9567424058914185 4.343357563018799 4.343357563018799
Loss :  1.9623206853866577 4.494108200073242 4.494108200073242
Loss :  1.9102301597595215 4.577817440032959 4.577817440032959
Loss :  1.9422553777694702 4.3856964111328125 4.3856964111328125
Loss :  1.9320099353790283 4.426591396331787 4.426591396331787
Loss :  1.9259246587753296 4.510126113891602 4.510126113891602
Loss :  1.9387649297714233 4.3524699211120605 4.3524699211120605
Loss :  1.9172230958938599 4.253163814544678 4.253163814544678
Loss :  1.9158294200897217 4.1777753829956055 4.1777753829956055
Loss :  1.9918432235717773 4.191673755645752 4.191673755645752
Loss :  1.9717034101486206 4.374380111694336 4.374380111694336
  batch 40 loss: 1.9717034101486206, 4.374380111694336, 4.374380111694336
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3, 4, 5], device='cuda:0')
Loss :  1.9872519969940186 4.4272589683532715 4.4272589683532715
Loss :  2.0079004764556885 4.517901420593262 4.517901420593262
Loss :  1.9673396348953247 4.42310094833374 4.42310094833374
Loss :  1.9803334474563599 4.429814338684082 4.429814338684082
Loss :  1.9726892709732056 4.5604681968688965 4.5604681968688965
Loss :  1.9881856441497803 4.47612190246582 4.47612190246582
Loss :  2.0321240425109863 4.385087490081787 4.385087490081787
Loss :  1.9959115982055664 4.483381748199463 4.483381748199463
Loss :  2.0520997047424316 4.530104637145996 4.530104637145996
Loss :  2.044827699661255 4.606384754180908 4.606384754180908
Loss :  1.957438588142395 4.491816997528076 4.491816997528076
Loss :  2.0475099086761475 4.525660037994385 4.525660037994385
Loss :  1.9523764848709106 4.384056091308594 4.384056091308594
Loss :  2.0827548503875732 4.408928394317627 4.408928394317627
Loss :  2.0379385948181152 4.520191669464111 4.520191669464111
Loss :  2.0659892559051514 4.453042507171631 4.453042507171631
Loss :  2.0413858890533447 4.413773059844971 4.413773059844971
Loss :  2.0449378490448 4.64648962020874 4.64648962020874
Loss :  2.094334602355957 4.51450777053833 4.51450777053833
Loss :  2.1013879776000977 4.56098747253418 4.56098747253418
  batch 60 loss: 2.1013879776000977, 4.56098747253418, 4.56098747253418
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3, 4, 5], device='cuda:0')
Loss :  2.0919485092163086 4.604798316955566 4.604798316955566
Loss :  2.1508190631866455 4.571705341339111 4.571705341339111
Loss :  2.033764123916626 4.513627529144287 4.513627529144287
Loss :  2.0018889904022217 4.6492600440979 4.6492600440979
Loss :  2.0389130115509033 4.406770706176758 4.406770706176758
Loss :  3.7757999897003174 4.410947799682617 4.410947799682617
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3, 5], device='cuda:0')
Loss :  3.763556718826294 4.519031047821045 4.519031047821045
Loss :  3.676461935043335 4.315437316894531 4.315437316894531
Loss :  3.698685884475708 4.251185417175293 4.251185417175293
Total LOSS train 4.46061747624324 valid 4.374150395393372
CE LOSS train 1.987075400352478 valid 0.924671471118927
Contrastive LOSS train 4.46061747624324 valid 1.0627963542938232
EPOCH 81:
Loss :  2.0684571266174316 4.423081398010254 4.423081398010254
Loss :  2.0769011974334717 4.5900092124938965 4.5900092124938965
Loss :  2.0633621215820312 4.295151233673096 4.295151233673096
Loss :  2.0540568828582764 4.461230278015137 4.461230278015137
Loss :  2.0925443172454834 4.2441301345825195 4.2441301345825195
Loss :  2.067854404449463 4.492184638977051 4.492184638977051
Loss :  2.0608112812042236 4.496546745300293 4.496546745300293
Loss :  2.0726442337036133 4.31700325012207 4.31700325012207
Loss :  2.042567253112793 4.472925186157227 4.472925186157227
Loss :  2.0313501358032227 4.277225971221924 4.277225971221924
Loss :  2.09906005859375 4.544549942016602 4.544549942016602
Loss :  2.0824527740478516 4.550203323364258 4.550203323364258
Loss :  2.0684022903442383 4.451407432556152 4.451407432556152
Loss :  2.085394859313965 4.5121588706970215 4.5121588706970215
Loss :  2.006551742553711 4.600000381469727 4.600000381469727
Loss :  2.0613512992858887 4.4783406257629395 4.4783406257629395
Loss :  2.0312466621398926 4.4479660987854 4.4479660987854
Loss :  2.083669900894165 4.435623645782471 4.435623645782471
Loss :  2.027135133743286 4.419367790222168 4.419367790222168
Loss :  2.014974594116211 4.547388553619385 4.547388553619385
  batch 20 loss: 2.014974594116211, 4.547388553619385, 4.547388553619385
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 4, 5], device='cuda:0')
Loss :  1.9903727769851685 4.341516017913818 4.341516017913818
Loss :  2.019030809402466 4.415491580963135 4.415491580963135
Loss :  2.056926727294922 4.723223686218262 4.723223686218262
Loss :  2.0558764934539795 4.435884475708008 4.435884475708008
Loss :  2.094021797180176 4.765355587005615 4.765355587005615
Loss :  2.061177968978882 4.496491432189941 4.496491432189941
Loss :  2.0869293212890625 4.735344886779785 4.735344886779785
Loss :  2.036456823348999 4.336609363555908 4.336609363555908
Loss :  2.057260036468506 4.891390800476074 4.891390800476074
Loss :  2.0207717418670654 4.391946792602539 4.391946792602539
Loss :  2.0170986652374268 4.556680679321289 4.556680679321289
Loss :  2.0064125061035156 4.665442943572998 4.665442943572998
Loss :  2.0187926292419434 4.415254592895508 4.415254592895508
Loss :  2.0220417976379395 4.590519428253174 4.590519428253174
Loss :  2.001537322998047 4.504965305328369 4.504965305328369
Loss :  2.0217432975769043 4.568912982940674 4.568912982940674
Loss :  2.0056841373443604 4.399885177612305 4.399885177612305
Loss :  2.012274742126465 4.400195598602295 4.400195598602295
Loss :  2.0308549404144287 4.402707576751709 4.402707576751709
Loss :  1.9883918762207031 4.584081649780273 4.584081649780273
  batch 40 loss: 1.9883918762207031, 4.584081649780273, 4.584081649780273
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 4, 5], device='cuda:0')
Loss :  2.0351603031158447 4.461821556091309 4.461821556091309
Loss :  1.9934006929397583 4.37566614151001 4.37566614151001
Loss :  1.9901411533355713 4.588198661804199 4.588198661804199
Loss :  2.002955675125122 4.595361709594727 4.595361709594727
Loss :  2.001509428024292 4.29742431640625 4.29742431640625
Loss :  2.055281639099121 4.414985656738281 4.414985656738281
Loss :  2.029904842376709 4.4556989669799805 4.4556989669799805
Loss :  1.9695672988891602 4.364577293395996 4.364577293395996
Loss :  2.021430730819702 4.572470664978027 4.572470664978027
Loss :  1.9959659576416016 4.6224517822265625 4.6224517822265625
Loss :  1.97226881980896 4.604708194732666 4.604708194732666
Loss :  2.0107970237731934 4.516199111938477 4.516199111938477
Loss :  2.0224955081939697 4.299383640289307 4.299383640289307
Loss :  2.013140916824341 4.3607497215271 4.3607497215271
Loss :  1.941636562347412 4.418976306915283 4.418976306915283
Loss :  2.0559957027435303 4.315153121948242 4.315153121948242
Loss :  2.062819004058838 4.467321872711182 4.467321872711182
Loss :  2.021878242492676 4.504605293273926 4.504605293273926
Loss :  2.0661003589630127 4.534998893737793 4.534998893737793
Loss :  2.034053087234497 4.435046672821045 4.435046672821045
  batch 60 loss: 2.034053087234497, 4.435046672821045, 4.435046672821045
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 5], device='cuda:0')
Loss :  2.0414910316467285 4.534249305725098 4.534249305725098
Loss :  1.9875359535217285 4.509702682495117 4.509702682495117
Loss :  2.0339879989624023 4.514945030212402 4.514945030212402
Loss :  1.9600882530212402 4.5991644859313965 4.5991644859313965
Loss :  1.9693657159805298 4.144600868225098 4.144600868225098
Loss :  1.8557177782058716 4.381323337554932 4.381323337554932
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 4], device='cuda:0')
Loss :  1.9057594537734985 4.437262535095215 4.437262535095215
Loss :  1.8300471305847168 4.325994491577148 4.325994491577148
Loss :  1.9285600185394287 4.169710636138916 4.169710636138916
Total LOSS train 4.479797803438627 valid 4.328572750091553
CE LOSS train 2.0320525627869825 valid 0.4821400046348572
Contrastive LOSS train 4.479797803438627 valid 1.042427659034729
EPOCH 82:
Loss :  2.0046043395996094 4.374570846557617 4.374570846557617
Loss :  2.0318689346313477 4.510279178619385 4.510279178619385
Loss :  2.025364398956299 4.609360694885254 4.609360694885254
Loss :  1.9835867881774902 4.3451924324035645 4.3451924324035645
Loss :  2.003556966781616 4.418125629425049 4.418125629425049
Loss :  2.0064003467559814 4.520516395568848 4.520516395568848
Loss :  1.96064031124115 4.554205417633057 4.554205417633057
Loss :  1.9855685234069824 4.3278913497924805 4.3278913497924805
Loss :  1.9538938999176025 4.411791801452637 4.411791801452637
Loss :  1.9936150312423706 4.336243629455566 4.336243629455566
Loss :  1.9790749549865723 4.383082866668701 4.383082866668701
Loss :  1.9453067779541016 4.327553749084473 4.327553749084473
Loss :  1.990893840789795 4.518828392028809 4.518828392028809
Loss :  1.9841324090957642 4.53250789642334 4.53250789642334
Loss :  1.9870902299880981 4.578446388244629 4.578446388244629
Loss :  2.000753164291382 4.539975166320801 4.539975166320801
Loss :  1.9648159742355347 4.583803176879883 4.583803176879883
Loss :  2.0370304584503174 4.873661518096924 4.873661518096924
Loss :  2.0067877769470215 4.427999973297119 4.427999973297119
Loss :  2.038048505783081 4.444698810577393 4.444698810577393
  batch 20 loss: 2.038048505783081, 4.444698810577393, 4.444698810577393
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.969156265258789 4.387274742126465 4.387274742126465
Loss :  2.087257146835327 4.488228797912598 4.488228797912598
Loss :  2.0378949642181396 4.687278747558594 4.687278747558594
Loss :  2.017061948776245 4.445945739746094 4.445945739746094
Loss :  2.0950868129730225 4.6461591720581055 4.6461591720581055
Loss :  2.1071557998657227 4.232344627380371 4.232344627380371
Loss :  2.0170772075653076 4.470113277435303 4.470113277435303
Loss :  2.0233516693115234 4.331988334655762 4.331988334655762
Loss :  2.04801082611084 4.3757405281066895 4.3757405281066895
Loss :  2.0149459838867188 4.46681022644043 4.46681022644043
Loss :  2.0102381706237793 4.472239971160889 4.472239971160889
Loss :  2.0825929641723633 4.784715175628662 4.784715175628662
Loss :  2.093463897705078 4.483552932739258 4.483552932739258
Loss :  2.0578453540802 4.422360897064209 4.422360897064209
Loss :  2.0149378776550293 4.504287242889404 4.504287242889404
Loss :  2.06211256980896 4.568188190460205 4.568188190460205
Loss :  2.0444371700286865 4.512662887573242 4.512662887573242
Loss :  2.0654869079589844 4.347069263458252 4.347069263458252
Loss :  2.1670165061950684 4.348602294921875 4.348602294921875
Loss :  2.1367080211639404 4.4122114181518555 4.4122114181518555
  batch 40 loss: 2.1367080211639404, 4.4122114181518555, 4.4122114181518555
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.9729459285736084 4.4677300453186035 4.4677300453186035
Loss :  2.058448076248169 4.516941547393799 4.516941547393799
Loss :  2.0785510540008545 4.3675994873046875 4.3675994873046875
Loss :  2.0225181579589844 4.421416282653809 4.421416282653809
Loss :  2.050089120864868 4.407539367675781 4.407539367675781
Loss :  2.0994458198547363 4.356362342834473 4.356362342834473
Loss :  2.1346817016601562 4.432515621185303 4.432515621185303
Loss :  2.0528697967529297 4.523246765136719 4.523246765136719
Loss :  2.050542116165161 4.366990089416504 4.366990089416504
Loss :  2.120272636413574 4.490163326263428 4.490163326263428
Loss :  2.0395095348358154 4.581228733062744 4.581228733062744
Loss :  2.0638139247894287 4.403199195861816 4.403199195861816
Loss :  1.995841145515442 4.455765724182129 4.455765724182129
Loss :  2.1033222675323486 4.339804649353027 4.339804649353027
Loss :  1.9694104194641113 4.45344352722168 4.45344352722168
Loss :  2.0603573322296143 4.366989612579346 4.366989612579346
Loss :  2.03790020942688 4.520874977111816 4.520874977111816
Loss :  2.0328803062438965 4.545147895812988 4.545147895812988
Loss :  2.0729928016662598 4.463122367858887 4.463122367858887
Loss :  2.075465202331543 4.391861438751221 4.391861438751221
  batch 60 loss: 2.075465202331543, 4.391861438751221, 4.391861438751221
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  2.000129461288452 4.452661514282227 4.452661514282227
Loss :  2.198538064956665 4.493303298950195 4.493303298950195
Loss :  2.151820659637451 4.478877544403076 4.478877544403076
Loss :  2.1314339637756348 4.480473518371582 4.480473518371582
Loss :  2.1392822265625 4.161628723144531 4.161628723144531
Loss :  2.0615060329437256 4.45487642288208 4.45487642288208
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 4], device='cuda:0')
Loss :  1.926534652709961 4.422136306762695 4.422136306762695
Loss :  1.9885263442993164 4.478608131408691 4.478608131408691
Loss :  1.7380921840667725 4.361304759979248 4.361304759979248
Total LOSS train 4.4606984211848335 valid 4.429231405258179
CE LOSS train 2.042275901941153 valid 0.4345230460166931
Contrastive LOSS train 4.4606984211848335 valid 1.090326189994812
EPOCH 83:
Loss :  2.048975706100464 4.3779730796813965 4.3779730796813965
Loss :  2.051306962966919 4.512960433959961 4.512960433959961
Loss :  2.0173909664154053 4.351714611053467 4.351714611053467
Loss :  1.966508388519287 4.743786811828613 4.743786811828613
Loss :  2.08262300491333 4.221108913421631 4.221108913421631
Loss :  2.016348361968994 5.168605327606201 5.168605327606201
Loss :  2.017408609390259 4.588524341583252 4.588524341583252
Loss :  2.1086483001708984 4.640004634857178 4.640004634857178
Loss :  2.0143866539001465 4.517858982086182 4.517858982086182
Loss :  2.0412943363189697 4.462464332580566 4.462464332580566
Loss :  2.0339791774749756 4.453398704528809 4.453398704528809
Loss :  1.9671075344085693 4.366580486297607 4.366580486297607
Loss :  2.0207149982452393 4.3995232582092285 4.3995232582092285
Loss :  1.9945834875106812 4.521018028259277 4.521018028259277
Loss :  1.9785349369049072 4.374862194061279 4.374862194061279
Loss :  2.028846263885498 4.472256183624268 4.472256183624268
Loss :  2.0272319316864014 4.388298988342285 4.388298988342285
Loss :  2.0003366470336914 4.341853618621826 4.341853618621826
Loss :  1.9903725385665894 4.354873180389404 4.354873180389404
Loss :  1.9627376794815063 4.519913196563721 4.519913196563721
  batch 20 loss: 1.9627376794815063, 4.519913196563721, 4.519913196563721
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
Loss :  2.004443407058716 4.333095073699951 4.333095073699951
Loss :  1.9700946807861328 4.378541946411133 4.378541946411133
Loss :  2.03105092048645 4.567115306854248 4.567115306854248
Loss :  2.042757272720337 4.389249324798584 4.389249324798584
Loss :  2.0206406116485596 4.6124982833862305 4.6124982833862305
Loss :  2.023256301879883 4.294821262359619 4.294821262359619
Loss :  2.02028226852417 4.6717658042907715 4.6717658042907715
Loss :  2.074490547180176 4.456329345703125 4.456329345703125
Loss :  2.0293726921081543 4.5311174392700195 4.5311174392700195
Loss :  2.0289554595947266 4.62357759475708 4.62357759475708
Loss :  2.0094614028930664 4.46685791015625 4.46685791015625
Loss :  1.97396981716156 4.771142959594727 4.771142959594727
Loss :  2.0002009868621826 4.459387302398682 4.459387302398682
Loss :  2.008173704147339 4.578911781311035 4.578911781311035
Loss :  1.979097843170166 4.456139087677002 4.456139087677002
Loss :  1.9119340181350708 4.684976577758789 4.684976577758789
Loss :  1.9412727355957031 4.551455974578857 4.551455974578857
Loss :  1.958052396774292 4.612060070037842 4.612060070037842
Loss :  1.9065849781036377 4.538236141204834 4.538236141204834
Loss :  1.8880871534347534 4.538726806640625 4.538726806640625
  batch 40 loss: 1.8880871534347534, 4.538726806640625, 4.538726806640625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9170924425125122 5.1201653480529785 5.1201653480529785
Loss :  1.9298800230026245 4.526803493499756 4.526803493499756
Loss :  1.881895899772644 4.74261999130249 4.74261999130249
Loss :  1.9059125185012817 4.620025634765625 4.620025634765625
Loss :  1.947209119796753 4.416907787322998 4.416907787322998
Loss :  1.9993340969085693 4.708374977111816 4.708374977111816
Loss :  1.9838790893554688 4.477837562561035 4.477837562561035
Loss :  1.9331082105636597 4.606786251068115 4.606786251068115
Loss :  1.9515759944915771 4.392305374145508 4.392305374145508
Loss :  1.9620798826217651 4.502864360809326 4.502864360809326
Loss :  1.9865878820419312 4.592415809631348 4.592415809631348
Loss :  2.0064048767089844 4.454844951629639 4.454844951629639
Loss :  1.935977578163147 4.68117094039917 4.68117094039917
Loss :  2.047767400741577 4.360649108886719 4.360649108886719
Loss :  1.9094241857528687 4.5290846824646 4.5290846824646
Loss :  2.044968366622925 4.385369300842285 4.385369300842285
Loss :  1.9006726741790771 4.48419713973999 4.48419713973999
Loss :  1.9059942960739136 4.727295875549316 4.727295875549316
Loss :  1.9131113290786743 4.627439022064209 4.627439022064209
Loss :  1.9044173955917358 4.370923042297363 4.370923042297363
  batch 60 loss: 1.9044173955917358, 4.370923042297363, 4.370923042297363
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
Loss :  2.018958806991577 4.4384002685546875 4.4384002685546875
Loss :  2.0490736961364746 4.500760555267334 4.500760555267334
Loss :  1.939656138420105 4.437047004699707 4.437047004699707
Loss :  1.9088128805160522 4.46175479888916 4.46175479888916
Loss :  1.9364347457885742 4.164948463439941 4.164948463439941
Loss :  1.8912657499313354 4.413637638092041 4.413637638092041
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.8545401096343994 4.397688865661621 4.397688865661621
Loss :  1.8042582273483276 4.268853187561035 4.268853187561035
Loss :  1.9647998809814453 4.307642459869385 4.307642459869385
Total LOSS train 4.517301185314472 valid 4.3469555377960205
CE LOSS train 1.9847960802224967 valid 0.49119997024536133
Contrastive LOSS train 4.517301185314472 valid 1.0769106149673462
EPOCH 84:
Loss :  1.975881576538086 4.23628568649292 4.23628568649292
Loss :  1.9459881782531738 4.503990173339844 4.503990173339844
Loss :  1.9551712274551392 4.4199042320251465 4.4199042320251465
Loss :  1.930802822113037 4.428188800811768 4.428188800811768
Loss :  1.8648033142089844 4.270840167999268 4.270840167999268
Loss :  1.90190589427948 4.351717948913574 4.351717948913574
Loss :  1.888792872428894 4.499290466308594 4.499290466308594
Loss :  1.8655139207839966 4.3196330070495605 4.3196330070495605
Loss :  1.8440853357315063 4.563784599304199 4.563784599304199
Loss :  1.9004360437393188 4.3697638511657715 4.3697638511657715
Loss :  1.8778382539749146 4.443916320800781 4.443916320800781
Loss :  1.8799238204956055 4.510572910308838 4.510572910308838
Loss :  1.8806664943695068 4.660696506500244 4.660696506500244
Loss :  1.8534151315689087 4.6631855964660645 4.6631855964660645
Loss :  1.9064470529556274 4.402283191680908 4.402283191680908
Loss :  1.984920859336853 4.460875034332275 4.460875034332275
Loss :  1.9413000345230103 4.40919828414917 4.40919828414917
Loss :  1.9236925840377808 4.489916801452637 4.489916801452637
Loss :  1.9150243997573853 4.386594295501709 4.386594295501709
Loss :  1.9790120124816895 4.399994373321533 4.399994373321533
  batch 20 loss: 1.9790120124816895, 4.399994373321533, 4.399994373321533
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9665777683258057 4.2315673828125 4.2315673828125
Loss :  1.948588490486145 4.4157395362854 4.4157395362854
Loss :  1.9315038919448853 4.459527969360352 4.459527969360352
Loss :  1.9485368728637695 4.294181823730469 4.294181823730469
Loss :  2.111551523208618 4.226978778839111 4.226978778839111
Loss :  1.9757614135742188 3.8072848320007324 3.8072848320007324
Loss :  1.9910322427749634 3.956012725830078 3.956012725830078
Loss :  1.9127813577651978 3.97430682182312 3.97430682182312
Loss :  1.9133508205413818 4.469281196594238 4.469281196594238
Loss :  2.0102217197418213 4.258184432983398 4.258184432983398
Loss :  1.9395698308944702 4.2051777839660645 4.2051777839660645
Loss :  1.942107081413269 4.450746059417725 4.450746059417725
Loss :  1.9274020195007324 4.454617500305176 4.454617500305176
Loss :  1.9426870346069336 4.49083948135376 4.49083948135376
Loss :  1.9189085960388184 4.097594738006592 4.097594738006592
Loss :  1.9251298904418945 4.0025529861450195 4.0025529861450195
Loss :  1.8934246301651 4.315744400024414 4.315744400024414
Loss :  1.9372321367263794 4.097579002380371 4.097579002380371
Loss :  1.9733388423919678 4.320915699005127 4.320915699005127
Loss :  1.9576025009155273 4.28461217880249 4.28461217880249
  batch 40 loss: 1.9576025009155273, 4.28461217880249, 4.28461217880249
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
Loss :  1.9014064073562622 4.455493450164795 4.455493450164795
Loss :  1.8822402954101562 4.35677433013916 4.35677433013916
Loss :  1.8332253694534302 4.417088508605957 4.417088508605957
Loss :  1.9334527254104614 4.4707489013671875 4.4707489013671875
Loss :  1.8576136827468872 4.301312446594238 4.301312446594238
Loss :  1.902710199356079 4.5132365226745605 4.5132365226745605
Loss :  1.9365440607070923 4.407260417938232 4.407260417938232
Loss :  1.8102607727050781 4.488033771514893 4.488033771514893
Loss :  1.8938930034637451 4.583713054656982 4.583713054656982
Loss :  1.839759349822998 4.660294532775879 4.660294532775879
Loss :  1.9012279510498047 4.677674293518066 4.677674293518066
Loss :  1.7881474494934082 4.460256576538086 4.460256576538086
Loss :  1.7914626598358154 4.472348690032959 4.472348690032959
Loss :  1.9116594791412354 4.497125148773193 4.497125148773193
Loss :  1.7993898391723633 4.427431583404541 4.427431583404541
Loss :  1.8957184553146362 4.374730110168457 4.374730110168457
Loss :  1.8523021936416626 4.466196537017822 4.466196537017822
Loss :  1.7481393814086914 4.442480087280273 4.442480087280273
Loss :  1.797731876373291 4.581736087799072 4.581736087799072
Loss :  1.8277652263641357 4.46306037902832 4.46306037902832
  batch 60 loss: 1.8277652263641357, 4.46306037902832, 4.46306037902832
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4, 5], device='cuda:0')
Loss :  1.856956958770752 4.546278953552246 4.546278953552246
Loss :  1.8701192140579224 4.3821210861206055 4.3821210861206055
Loss :  1.877510905265808 4.482235908508301 4.482235908508301
Loss :  1.8414866924285889 4.50717830657959 4.50717830657959
Loss :  1.8870121240615845 4.16237211227417 4.16237211227417
Loss :  1.6803555488586426 4.433578014373779 4.433578014373779
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4, 5], device='cuda:0')
Loss :  1.7887359857559204 4.349764823913574 4.349764823913574
Loss :  1.8144381046295166 4.4270405769348145 4.4270405769348145
Loss :  1.6675280332565308 4.328779220581055 4.328779220581055
Total LOSS train 4.387711682686439 valid 4.384790658950806
CE LOSS train 1.9033948733256414 valid 0.4168820083141327
Contrastive LOSS train 4.387711682686439 valid 1.0821948051452637
EPOCH 85:
Loss :  1.9598290920257568 4.272181034088135 4.272181034088135
Loss :  1.8418587446212769 4.407345771789551 4.407345771789551
Loss :  1.8715260028839111 4.427263259887695 4.427263259887695
Loss :  1.8237398862838745 4.330249309539795 4.330249309539795
Loss :  2.0121896266937256 4.248244285583496 4.248244285583496
Loss :  1.836040735244751 4.40655517578125 4.40655517578125
Loss :  1.8751542568206787 4.567640781402588 4.567640781402588
Loss :  1.9291890859603882 4.4159135818481445 4.4159135818481445
Loss :  1.873261570930481 4.384170055389404 4.384170055389404
Loss :  1.940908432006836 4.335972309112549 4.335972309112549
Loss :  1.900736927986145 4.489199161529541 4.489199161529541
Loss :  1.784748911857605 4.479511737823486 4.479511737823486
Loss :  1.91721510887146 4.508473873138428 4.508473873138428
Loss :  1.920982003211975 4.5650553703308105 4.5650553703308105
Loss :  1.8244129419326782 4.387093544006348 4.387093544006348
Loss :  1.8271795511245728 4.531951427459717 4.531951427459717
Loss :  1.8342171907424927 4.503982067108154 4.503982067108154
Loss :  1.82615327835083 4.418348789215088 4.418348789215088
Loss :  1.7696130275726318 4.365464210510254 4.365464210510254
Loss :  1.8092384338378906 4.459061622619629 4.459061622619629
  batch 20 loss: 1.8092384338378906, 4.459061622619629, 4.459061622619629
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4, 5], device='cuda:0')
Loss :  1.9334653615951538 4.315845012664795 4.315845012664795
Loss :  1.898031234741211 4.43783712387085 4.43783712387085
Loss :  1.8858001232147217 4.473076820373535 4.473076820373535
Loss :  1.8006185293197632 4.566923141479492 4.566923141479492
Loss :  1.8484255075454712 4.560498237609863 4.560498237609863
Loss :  1.9401088953018188 4.4083476066589355 4.4083476066589355
Loss :  1.844901442527771 4.658381462097168 4.658381462097168
Loss :  1.788142442703247 4.3674750328063965 4.3674750328063965
Loss :  1.7642849683761597 4.4523138999938965 4.4523138999938965
Loss :  1.8543875217437744 4.551949501037598 4.551949501037598
Loss :  1.9042314291000366 4.599212169647217 4.599212169647217
Loss :  1.9167028665542603 4.654090881347656 4.654090881347656
Loss :  1.8181231021881104 4.479909896850586 4.479909896850586
Loss :  1.9026224613189697 4.451079368591309 4.451079368591309
Loss :  1.8922114372253418 4.484768867492676 4.484768867492676
Loss :  1.8105432987213135 4.604008197784424 4.604008197784424
Loss :  1.8712905645370483 4.465279579162598 4.465279579162598
Loss :  1.8049542903900146 4.4921135902404785 4.4921135902404785
Loss :  1.8671865463256836 4.454771041870117 4.454771041870117
Loss :  1.8962206840515137 4.499278545379639 4.499278545379639
  batch 40 loss: 1.8962206840515137, 4.499278545379639, 4.499278545379639
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
Loss :  1.864457130432129 4.495049953460693 4.495049953460693
Loss :  1.7555376291275024 4.307546615600586 4.307546615600586
Loss :  1.800807237625122 4.4950432777404785 4.4950432777404785
Loss :  1.9268128871917725 4.4795451164245605 4.4795451164245605
Loss :  1.8551034927368164 4.238261699676514 4.238261699676514
Loss :  1.8413491249084473 4.425096035003662 4.425096035003662
Loss :  1.9991644620895386 4.479715824127197 4.479715824127197
Loss :  1.7852623462677002 4.521768569946289 4.521768569946289
Loss :  1.991633415222168 4.337463855743408 4.337463855743408
Loss :  1.8909319639205933 4.508426189422607 4.508426189422607
Loss :  1.8245683908462524 4.552423000335693 4.552423000335693
Loss :  1.9786523580551147 4.409644603729248 4.409644603729248
Loss :  1.8567477464675903 4.4296875 4.4296875
Loss :  1.7809131145477295 4.339860439300537 4.339860439300537
Loss :  1.8789472579956055 4.561808109283447 4.561808109283447
Loss :  1.8498952388763428 4.417171955108643 4.417171955108643
Loss :  1.8212143182754517 4.528274059295654 4.528274059295654
Loss :  1.8101977109909058 4.5176100730896 4.5176100730896
Loss :  1.7580888271331787 4.583418846130371 4.583418846130371
Loss :  1.8420289754867554 4.3474884033203125 4.3474884033203125
  batch 60 loss: 1.8420289754867554, 4.3474884033203125, 4.3474884033203125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4, 5], device='cuda:0')
Loss :  1.8611397743225098 4.636940002441406 4.636940002441406
Loss :  1.686545968055725 4.5253095626831055 4.5253095626831055
Loss :  1.7810359001159668 4.482451915740967 4.482451915740967
Loss :  1.9511888027191162 4.4736504554748535 4.4736504554748535
Loss :  1.9192932844161987 4.121003150939941 4.121003150939941
Loss :  1.292343258857727 4.381180763244629 4.381180763244629
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.3834291696548462 4.44036865234375 4.44036865234375
Loss :  1.347628116607666 4.316298961639404 4.316298961639404
Loss :  1.6627331972122192 4.257744312286377 4.257744312286377
Total LOSS train 4.456853793217586 valid 4.34889817237854
CE LOSS train 1.8604917672964243 valid 0.4156832993030548
Contrastive LOSS train 4.456853793217586 valid 1.0644360780715942
EPOCH 86:
Loss :  1.9398398399353027 4.34255838394165 4.34255838394165
Loss :  2.0689451694488525 4.647367477416992 4.647367477416992
Loss :  1.8987780809402466 4.365484237670898 4.365484237670898
Loss :  1.8265386819839478 4.454949855804443 4.454949855804443
Loss :  1.749650001525879 4.480082035064697 4.480082035064697
Loss :  1.9513354301452637 4.358694076538086 4.358694076538086
Loss :  1.8361276388168335 4.562227249145508 4.562227249145508
Loss :  1.8618072271347046 4.294684410095215 4.294684410095215
Loss :  1.9017343521118164 4.392651557922363 4.392651557922363
Loss :  1.9846701622009277 4.415787220001221 4.415787220001221
Loss :  1.895115852355957 4.643966197967529 4.643966197967529
Loss :  1.7206909656524658 4.444427013397217 4.444427013397217
Loss :  1.9234591722488403 4.817959785461426 4.817959785461426
Loss :  1.8528008460998535 4.461723804473877 4.461723804473877
Loss :  2.007098436355591 4.502651691436768 4.502651691436768
Loss :  1.9650506973266602 4.7243194580078125 4.7243194580078125
Loss :  1.8921515941619873 4.467196464538574 4.467196464538574
Loss :  1.9305154085159302 4.454797744750977 4.454797744750977
Loss :  1.7504947185516357 4.41007137298584 4.41007137298584
Loss :  1.8682092428207397 4.367669105529785 4.367669105529785
  batch 20 loss: 1.8682092428207397, 4.367669105529785, 4.367669105529785
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.7272390127182007 4.382009983062744 4.382009983062744
Loss :  1.803921103477478 4.3707780838012695 4.3707780838012695
Loss :  1.7207715511322021 4.620970249176025 4.620970249176025
Loss :  1.784865140914917 4.5073957443237305 4.5073957443237305
Loss :  1.759971022605896 4.656669616699219 4.656669616699219
Loss :  1.8021690845489502 4.4401750564575195 4.4401750564575195
Loss :  1.8895844221115112 4.605876445770264 4.605876445770264
Loss :  1.7626956701278687 4.404807090759277 4.404807090759277
Loss :  1.7724438905715942 4.440456867218018 4.440456867218018
Loss :  1.8532814979553223 4.465559005737305 4.465559005737305
Loss :  1.7139458656311035 4.537459373474121 4.537459373474121
Loss :  1.7469655275344849 4.590874195098877 4.590874195098877
Loss :  1.854447841644287 4.439506530761719 4.439506530761719
Loss :  1.8195888996124268 4.454766273498535 4.454766273498535
Loss :  1.7731027603149414 4.438581466674805 4.438581466674805
Loss :  1.8374290466308594 4.49069881439209 4.49069881439209
Loss :  1.8635106086730957 4.476212501525879 4.476212501525879
Loss :  1.9106311798095703 4.4273834228515625 4.4273834228515625
Loss :  1.976187825202942 4.332742214202881 4.332742214202881
Loss :  1.8816757202148438 4.364680290222168 4.364680290222168
  batch 40 loss: 1.8816757202148438, 4.364680290222168, 4.364680290222168
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.8939905166625977 4.463101387023926 4.463101387023926
Loss :  1.8216056823730469 4.429214954376221 4.429214954376221
Loss :  1.725472331047058 4.403311729431152 4.403311729431152
Loss :  1.763961911201477 4.512175559997559 4.512175559997559
Loss :  1.776962161064148 4.289556503295898 4.289556503295898
Loss :  1.9618967771530151 4.380951881408691 4.380951881408691
Loss :  1.83782958984375 4.445985317230225 4.445985317230225
Loss :  1.807888150215149 4.58566951751709 4.58566951751709
Loss :  1.8998264074325562 4.4435272216796875 4.4435272216796875
Loss :  1.91915762424469 4.493863105773926 4.493863105773926
Loss :  1.8388861417770386 4.533440113067627 4.533440113067627
Loss :  1.7866201400756836 4.432470798492432 4.432470798492432
Loss :  1.852115511894226 4.235743999481201 4.235743999481201
Loss :  1.9123603105545044 4.393950462341309 4.393950462341309
Loss :  1.8547159433364868 4.4399542808532715 4.4399542808532715
Loss :  1.843249797821045 4.335427761077881 4.335427761077881
Loss :  1.740703821182251 4.483835697174072 4.483835697174072
Loss :  1.7939702272415161 4.504941940307617 4.504941940307617
Loss :  1.7836167812347412 4.432877540588379 4.432877540588379
Loss :  1.7839034795761108 4.491168022155762 4.491168022155762
  batch 60 loss: 1.7839034795761108, 4.491168022155762, 4.491168022155762
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.8254051208496094 4.622363567352295 4.622363567352295
Loss :  1.9085297584533691 4.7120819091796875 4.7120819091796875
Loss :  1.7481918334960938 4.468328475952148 4.468328475952148
Loss :  1.6892213821411133 4.563075542449951 4.563075542449951
Loss :  1.807388424873352 4.145896911621094 4.145896911621094
Loss :  3.3139259815216064 4.466014385223389 4.466014385223389
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3, 4], device='cuda:0')
Loss :  3.1789608001708984 4.489211082458496 4.489211082458496
Loss :  2.9366424083709717 4.3418450355529785 4.3418450355529785
Loss :  3.041351795196533 4.390491962432861 4.390491962432861
Total LOSS train 4.4661505625798155 valid 4.421890616416931
CE LOSS train 1.8408755540847779 valid 0.7603379487991333
Contrastive LOSS train 4.4661505625798155 valid 1.0976229906082153
EPOCH 87:
Loss :  1.8103140592575073 4.480321407318115 4.480321407318115
Loss :  1.7923005819320679 4.462327003479004 4.462327003479004
Loss :  1.78010892868042 4.344848155975342 4.344848155975342
Loss :  1.8504583835601807 4.375240802764893 4.375240802764893
Loss :  1.9182345867156982 4.320215225219727 4.320215225219727
Loss :  1.7569756507873535 4.285597801208496 4.285597801208496
Loss :  1.8409647941589355 4.532412052154541 4.532412052154541
Loss :  1.8162672519683838 4.306352615356445 4.306352615356445
Loss :  1.7915767431259155 4.448721885681152 4.448721885681152
Loss :  1.8481833934783936 4.359527111053467 4.359527111053467
Loss :  1.8021200895309448 4.484451770782471 4.484451770782471
Loss :  1.8250340223312378 4.405027389526367 4.405027389526367
Loss :  1.8580322265625 4.705076217651367 4.705076217651367
Loss :  1.8243579864501953 4.5883073806762695 4.5883073806762695
Loss :  1.8689316511154175 4.399405002593994 4.399405002593994
Loss :  1.8933186531066895 4.617401123046875 4.617401123046875
Loss :  1.8502618074417114 4.510475158691406 4.510475158691406
Loss :  1.8152368068695068 4.370985507965088 4.370985507965088
Loss :  1.87172269821167 4.433484077453613 4.433484077453613
Loss :  1.8735159635543823 4.633584022521973 4.633584022521973
  batch 20 loss: 1.8735159635543823, 4.633584022521973, 4.633584022521973
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.860788106918335 4.5173516273498535 4.5173516273498535
Loss :  1.8120014667510986 4.58829927444458 4.58829927444458
Loss :  1.8217014074325562 4.540017604827881 4.540017604827881
Loss :  1.831497311592102 4.5757975578308105 4.5757975578308105
Loss :  1.8082420825958252 4.678027629852295 4.678027629852295
Loss :  1.8295643329620361 4.480567932128906 4.480567932128906
Loss :  1.9472081661224365 4.572588920593262 4.572588920593262
Loss :  1.8241252899169922 4.2870612144470215 4.2870612144470215
Loss :  1.8002034425735474 4.499715328216553 4.499715328216553
Loss :  1.891237497329712 4.466090202331543 4.466090202331543
Loss :  1.7803515195846558 4.505642414093018 4.505642414093018
Loss :  1.806535005569458 4.749351501464844 4.749351501464844
Loss :  1.8149614334106445 4.41529655456543 4.41529655456543
Loss :  1.9088808298110962 4.477933406829834 4.477933406829834
Loss :  1.8231391906738281 4.5831122398376465 4.5831122398376465
Loss :  1.8448628187179565 4.656567573547363 4.656567573547363
Loss :  1.8014849424362183 4.431175231933594 4.431175231933594
Loss :  1.937027096748352 4.413322925567627 4.413322925567627
Loss :  1.911838173866272 4.319194316864014 4.319194316864014
Loss :  1.863949179649353 4.438599586486816 4.438599586486816
  batch 40 loss: 1.863949179649353, 4.438599586486816, 4.438599586486816
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.9273018836975098 4.516471862792969 4.516471862792969
Loss :  1.8222246170043945 4.424025058746338 4.424025058746338
Loss :  1.8345247507095337 4.417119026184082 4.417119026184082
Loss :  1.857229232788086 4.475163459777832 4.475163459777832
Loss :  1.8251737356185913 4.491261959075928 4.491261959075928
Loss :  1.887158989906311 4.555257797241211 4.555257797241211
Loss :  1.834434151649475 4.356058120727539 4.356058120727539
Loss :  1.8066437244415283 4.511712074279785 4.511712074279785
Loss :  1.8739045858383179 4.498722553253174 4.498722553253174
Loss :  1.8799673318862915 4.733447074890137 4.733447074890137
Loss :  1.8097808361053467 4.475306510925293 4.475306510925293
Loss :  1.9068603515625 4.495225429534912 4.495225429534912
Loss :  1.9757543802261353 4.35753870010376 4.35753870010376
Loss :  1.8504414558410645 4.422118186950684 4.422118186950684
Loss :  1.8479435443878174 4.490187644958496 4.490187644958496
Loss :  2.003002405166626 4.429117679595947 4.429117679595947
Loss :  1.9699889421463013 4.395375728607178 4.395375728607178
Loss :  1.9212552309036255 4.517538070678711 4.517538070678711
Loss :  1.8392183780670166 4.470731735229492 4.470731735229492
Loss :  1.9969723224639893 4.404378890991211 4.404378890991211
  batch 60 loss: 1.9969723224639893, 4.404378890991211, 4.404378890991211
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
Loss :  1.9507380723953247 4.515133380889893 4.515133380889893
Loss :  2.032193899154663 4.391261100769043 4.391261100769043
Loss :  1.8792297840118408 4.673888206481934 4.673888206481934
Loss :  1.8963652849197388 4.347636699676514 4.347636699676514
Loss :  1.9878355264663696 4.1108222007751465 4.1108222007751465
Loss :  2.2325425148010254 4.413405418395996 4.413405418395996
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3, 4], device='cuda:0')
Loss :  2.1748383045196533 4.428900718688965 4.428900718688965
Loss :  2.2203524112701416 4.335650444030762 4.335650444030762
Loss :  2.206702947616577 4.2149834632873535 4.2149834632873535
Total LOSS train 4.472845752422626 valid 4.348235011100769
CE LOSS train 1.8619024460132305 valid 0.5516757369041443
Contrastive LOSS train 4.472845752422626 valid 1.0537458658218384
EPOCH 88:
Loss :  1.8589107990264893 4.342665195465088 4.342665195465088
Loss :  1.928863525390625 4.5683674812316895 4.5683674812316895
Loss :  1.9504085779190063 4.275729656219482 4.275729656219482
Loss :  1.9425164461135864 4.393765926361084 4.393765926361084
Loss :  1.9571069478988647 4.297384738922119 4.297384738922119
Loss :  2.010481119155884 4.354481220245361 4.354481220245361
Loss :  1.952784776687622 4.451876163482666 4.451876163482666
Loss :  1.955283284187317 4.4404401779174805 4.4404401779174805
Loss :  1.9464340209960938 4.5135321617126465 4.5135321617126465
Loss :  1.9986876249313354 4.372347354888916 4.372347354888916
Loss :  1.9441614151000977 4.542032718658447 4.542032718658447
Loss :  1.9970505237579346 4.464480400085449 4.464480400085449
Loss :  1.9255608320236206 4.630702018737793 4.630702018737793
Loss :  1.907631516456604 4.481053829193115 4.481053829193115
Loss :  1.9227006435394287 4.4452290534973145 4.4452290534973145
Loss :  1.9781352281570435 4.543548583984375 4.543548583984375
Loss :  1.9302018880844116 4.615583419799805 4.615583419799805
Loss :  1.885916829109192 4.735180377960205 4.735180377960205
Loss :  1.8667749166488647 4.383428573608398 4.383428573608398
Loss :  1.9292168617248535 4.459437370300293 4.459437370300293
  batch 20 loss: 1.9292168617248535, 4.459437370300293, 4.459437370300293
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.8866450786590576 4.4200873374938965 4.4200873374938965
Loss :  1.8664790391921997 4.6525959968566895 4.6525959968566895
Loss :  1.85914945602417 4.537337779998779 4.537337779998779
Loss :  1.8583722114562988 4.440005779266357 4.440005779266357
Loss :  1.9516780376434326 4.64554500579834 4.64554500579834
Loss :  1.8838832378387451 4.2886505126953125 4.2886505126953125
Loss :  1.9097931385040283 4.734145641326904 4.734145641326904
Loss :  1.920775294303894 4.408007621765137 4.408007621765137
Loss :  1.8697435855865479 4.551726818084717 4.551726818084717
Loss :  1.9658362865447998 4.461601734161377 4.461601734161377
Loss :  1.8458890914916992 4.497771739959717 4.497771739959717
Loss :  1.88498055934906 4.699739933013916 4.699739933013916
Loss :  1.9011633396148682 4.455862522125244 4.455862522125244
Loss :  1.8301379680633545 4.526305198669434 4.526305198669434
Loss :  1.7490216493606567 4.483649253845215 4.483649253845215
Loss :  1.7899866104125977 4.57711124420166 4.57711124420166
Loss :  1.820034146308899 4.381524562835693 4.381524562835693
Loss :  1.8717024326324463 4.443331718444824 4.443331718444824
Loss :  1.8568962812423706 4.34813117980957 4.34813117980957
Loss :  1.8765034675598145 4.349286079406738 4.349286079406738
  batch 40 loss: 1.8765034675598145, 4.349286079406738, 4.349286079406738
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4, 5], device='cuda:0')
Loss :  1.7983295917510986 4.513204574584961 4.513204574584961
Loss :  1.7967703342437744 4.43250846862793 4.43250846862793
Loss :  1.8229844570159912 4.375025749206543 4.375025749206543
Loss :  1.9198459386825562 4.545455455780029 4.545455455780029
Loss :  1.8114584684371948 4.359196662902832 4.359196662902832
Loss :  1.7814809083938599 4.6915106773376465 4.6915106773376465
Loss :  1.9086061716079712 4.512144565582275 4.512144565582275
Loss :  1.8696874380111694 4.610434532165527 4.610434532165527
Loss :  1.8244074583053589 4.4234538078308105 4.4234538078308105
Loss :  1.7885851860046387 4.5182085037231445 4.5182085037231445
Loss :  1.8814207315444946 4.542413234710693 4.542413234710693
Loss :  1.7488009929656982 4.379926681518555 4.379926681518555
Loss :  1.8768821954727173 4.633847236633301 4.633847236633301
Loss :  1.9444000720977783 4.520849704742432 4.520849704742432
Loss :  1.8922209739685059 4.510046482086182 4.510046482086182
Loss :  1.8382542133331299 4.371678352355957 4.371678352355957
Loss :  1.8925694227218628 4.861571788787842 4.861571788787842
Loss :  1.8643083572387695 4.526208400726318 4.526208400726318
Loss :  1.963489055633545 4.516646862030029 4.516646862030029
Loss :  1.8700170516967773 4.4325761795043945 4.4325761795043945
  batch 60 loss: 1.8700170516967773, 4.4325761795043945, 4.4325761795043945
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.914211630821228 4.585395812988281 4.585395812988281
Loss :  1.8813015222549438 4.4726738929748535 4.4726738929748535
Loss :  1.9111881256103516 4.350393295288086 4.350393295288086
Loss :  1.8613872528076172 4.4144744873046875 4.4144744873046875
Loss :  1.816391944885254 4.107369422912598 4.107369422912598
Loss :  3.8746347427368164 4.442678451538086 4.442678451538086
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  4.689344882965088 4.430222511291504 4.430222511291504
Loss :  1.8839527368545532 4.301506519317627 4.301506519317627
Loss :  4.380679130554199 4.610018253326416 4.610018253326416
Total LOSS train 4.483398444835956 valid 4.446106433868408
CE LOSS train 1.8871768951416015 valid 1.0951697826385498
Contrastive LOSS train 4.483398444835956 valid 1.152504563331604
EPOCH 89:
Loss :  1.9246717691421509 4.339120388031006 4.339120388031006
Loss :  1.9340462684631348 4.535164833068848 4.535164833068848
Loss :  1.8629409074783325 4.2927374839782715 4.2927374839782715
Loss :  1.8773481845855713 4.443780422210693 4.443780422210693
Loss :  1.9561817646026611 4.222731590270996 4.222731590270996
Loss :  1.9105948209762573 4.369049549102783 4.369049549102783
Loss :  2.0239198207855225 4.454706192016602 4.454706192016602
Loss :  1.9702037572860718 4.475905895233154 4.475905895233154
Loss :  1.8190488815307617 4.42882776260376 4.42882776260376
Loss :  1.9327068328857422 4.325570106506348 4.325570106506348
Loss :  1.8068909645080566 4.594629764556885 4.594629764556885
Loss :  1.914354681968689 4.443282127380371 4.443282127380371
Loss :  1.8471543788909912 4.705262184143066 4.705262184143066
Loss :  1.9396886825561523 4.554220676422119 4.554220676422119
Loss :  1.9063860177993774 4.328924179077148 4.328924179077148
Loss :  1.8783177137374878 4.493990898132324 4.493990898132324
Loss :  1.8643449544906616 4.449148178100586 4.449148178100586
Loss :  1.8259825706481934 4.446479797363281 4.446479797363281
Loss :  1.9006563425064087 4.357012748718262 4.357012748718262
Loss :  1.8378158807754517 4.411842346191406 4.411842346191406
  batch 20 loss: 1.8378158807754517, 4.411842346191406, 4.411842346191406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 4], device='cuda:0')
Loss :  1.8956270217895508 4.311172008514404 4.311172008514404
Loss :  1.9019320011138916 4.549166202545166 4.549166202545166
Loss :  1.852460265159607 4.586038112640381 4.586038112640381
Loss :  1.87484872341156 4.437085151672363 4.437085151672363
Loss :  1.8379147052764893 4.580273151397705 4.580273151397705
Loss :  1.8800455331802368 4.453084468841553 4.453084468841553
Loss :  1.865373134613037 4.630318641662598 4.630318641662598
Loss :  1.8953930139541626 4.4289727210998535 4.4289727210998535
Loss :  1.868045687675476 4.411940097808838 4.411940097808838
Loss :  1.8710099458694458 4.485269069671631 4.485269069671631
Loss :  1.8961704969406128 4.475239276885986 4.475239276885986
Loss :  1.8952524662017822 4.66524076461792 4.66524076461792
Loss :  1.933143138885498 4.478234767913818 4.478234767913818
Loss :  1.9009486436843872 4.484303951263428 4.484303951263428
Loss :  1.893413782119751 4.509081840515137 4.509081840515137
Loss :  1.894307255744934 4.607769966125488 4.607769966125488
Loss :  1.904374122619629 4.381784439086914 4.381784439086914
Loss :  1.895943522453308 4.440552234649658 4.440552234649658
Loss :  1.9123238325119019 4.493685722351074 4.493685722351074
Loss :  1.8806519508361816 4.400187969207764 4.400187969207764
  batch 40 loss: 1.8806519508361816, 4.400187969207764, 4.400187969207764
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.9099719524383545 4.530481815338135 4.530481815338135
Loss :  1.8656525611877441 4.464436054229736 4.464436054229736
Loss :  1.9032889604568481 4.451083660125732 4.451083660125732
Loss :  1.8816384077072144 4.494611740112305 4.494611740112305
Loss :  1.9169920682907104 4.597219944000244 4.597219944000244
Loss :  1.9092433452606201 4.407953262329102 4.407953262329102
Loss :  1.9014108180999756 4.51938009262085 4.51938009262085
Loss :  1.908260703086853 4.589762210845947 4.589762210845947
Loss :  1.8817212581634521 4.720078468322754 4.720078468322754
Loss :  1.9052464962005615 4.5475311279296875 4.5475311279296875
Loss :  1.9230296611785889 4.624773979187012 4.624773979187012
Loss :  1.915061116218567 4.411062717437744 4.411062717437744
Loss :  1.9273804426193237 4.418056011199951 4.418056011199951
Loss :  1.9186135530471802 4.354368209838867 4.354368209838867
Loss :  1.9120049476623535 4.46566915512085 4.46566915512085
Loss :  1.9107530117034912 4.351996898651123 4.351996898651123
Loss :  1.9112129211425781 4.53546667098999 4.53546667098999
Loss :  1.9219058752059937 4.500826835632324 4.500826835632324
Loss :  1.9123024940490723 4.577110290527344 4.577110290527344
Loss :  1.8760547637939453 4.422226905822754 4.422226905822754
  batch 60 loss: 1.8760547637939453, 4.422226905822754, 4.422226905822754
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.9434118270874023 4.471645832061768 4.471645832061768
Loss :  1.9034411907196045 4.436481952667236 4.436481952667236
Loss :  1.9073060750961304 4.454697132110596 4.454697132110596
Loss :  1.929824709892273 4.341191291809082 4.341191291809082
Loss :  1.915663719177246 4.433190822601318 4.433190822601318
Loss :  2.561572551727295 4.487054824829102 4.487054824829102
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([4], device='cuda:0')
Loss :  2.652089834213257 4.474357604980469 4.474357604980469
Loss :  2.6330225467681885 4.458690166473389 4.458690166473389
Loss :  2.4643521308898926 4.349263668060303 4.349263668060303
Total LOSS train 4.4708167809706465 valid 4.442341566085815
CE LOSS train 1.8983665741406954 valid 0.6160880327224731
Contrastive LOSS train 4.4708167809706465 valid 1.0873159170150757
EPOCH 90:
Loss :  1.8992135524749756 4.354124069213867 4.354124069213867
Loss :  1.8999279737472534 4.695599555969238 4.695599555969238
Loss :  1.8629276752471924 4.064558506011963 4.064558506011963
Loss :  1.8876534700393677 4.461609840393066 4.461609840393066
Loss :  1.8651350736618042 4.308969020843506 4.308969020843506
Loss :  1.9207327365875244 4.6442437171936035 4.6442437171936035
Loss :  1.899226188659668 4.482226371765137 4.482226371765137
Loss :  1.8853870630264282 4.277154922485352 4.277154922485352
Loss :  1.8990384340286255 4.558821678161621 4.558821678161621
Loss :  1.9212592840194702 4.323286533355713 4.323286533355713
Loss :  1.8835755586624146 4.645108699798584 4.645108699798584
Loss :  1.889673113822937 4.5757341384887695 4.5757341384887695
Loss :  1.8948148488998413 4.604918956756592 4.604918956756592
Loss :  1.9227192401885986 4.461796283721924 4.461796283721924
Loss :  1.9202574491500854 4.485876083374023 4.485876083374023
Loss :  1.8708416223526 4.839316368103027 4.839316368103027
Loss :  1.9063198566436768 4.643280506134033 4.643280506134033
Loss :  1.8837002515792847 4.5398359298706055 4.5398359298706055
Loss :  1.9044324159622192 4.4106059074401855 4.4106059074401855
Loss :  1.8124654293060303 4.478127956390381 4.478127956390381
  batch 20 loss: 1.8124654293060303, 4.478127956390381, 4.478127956390381
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.919969081878662 4.373659133911133 4.373659133911133
Loss :  1.9146053791046143 4.605903625488281 4.605903625488281
Loss :  1.8662045001983643 4.5978498458862305 4.5978498458862305
Loss :  1.8895057439804077 4.376821517944336 4.376821517944336
Loss :  1.884301781654358 4.608158588409424 4.608158588409424
Loss :  1.9024312496185303 4.410977840423584 4.410977840423584
Loss :  1.9233306646347046 4.727924823760986 4.727924823760986
Loss :  1.8561650514602661 4.432938575744629 4.432938575744629
Loss :  1.8760179281234741 4.520956993103027 4.520956993103027
Loss :  1.8497785329818726 4.507544994354248 4.507544994354248
Loss :  1.874127745628357 4.503637313842773 4.503637313842773
Loss :  1.91566002368927 4.765561580657959 4.765561580657959
Loss :  1.9011125564575195 4.514679908752441 4.514679908752441
Loss :  1.9046661853790283 4.378072261810303 4.378072261810303
Loss :  1.887330174446106 4.909425258636475 4.909425258636475
Loss :  1.866577386856079 4.470617771148682 4.470617771148682
Loss :  1.8994489908218384 4.574609756469727 4.574609756469727
Loss :  1.909001111984253 4.407644271850586 4.407644271850586
Loss :  1.9440975189208984 4.628845691680908 4.628845691680908
Loss :  1.9182852506637573 4.407626152038574 4.407626152038574
  batch 40 loss: 1.9182852506637573, 4.407626152038574, 4.407626152038574
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.8639661073684692 4.569351673126221 4.569351673126221
Loss :  1.8565038442611694 4.428092002868652 4.428092002868652
Loss :  1.8694071769714355 4.357947826385498 4.357947826385498
Loss :  1.8866138458251953 4.592105388641357 4.592105388641357
Loss :  1.8815237283706665 4.383282661437988 4.383282661437988
Loss :  1.8735840320587158 4.489255428314209 4.489255428314209
Loss :  1.8171660900115967 4.424588680267334 4.424588680267334
Loss :  1.9061118364334106 4.492814064025879 4.492814064025879
Loss :  1.8501516580581665 4.719639778137207 4.719639778137207
Loss :  1.866223931312561 4.646187782287598 4.646187782287598
Loss :  1.85456120967865 4.582431316375732 4.582431316375732
Loss :  1.846077799797058 4.392569541931152 4.392569541931152
Loss :  1.8642388582229614 4.369944095611572 4.369944095611572
Loss :  1.8551698923110962 4.4370036125183105 4.4370036125183105
Loss :  1.864970326423645 4.502797603607178 4.502797603607178
Loss :  1.8025296926498413 4.443856239318848 4.443856239318848
Loss :  1.8455122709274292 4.4863505363464355 4.4863505363464355
Loss :  1.8936723470687866 4.57131814956665 4.57131814956665
Loss :  1.8270562887191772 4.628983020782471 4.628983020782471
Loss :  1.8441630601882935 4.345982074737549 4.345982074737549
  batch 60 loss: 1.8441630601882935, 4.345982074737549, 4.345982074737549
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 4, 5], device='cuda:0')
Loss :  1.848054051399231 4.434167861938477 4.434167861938477
Loss :  1.8593924045562744 4.371850967407227 4.371850967407227
Loss :  1.8694640398025513 4.322309494018555 4.322309494018555
Loss :  1.8569965362548828 4.385848522186279 4.385848522186279
Loss :  1.9022670984268188 4.129113674163818 4.129113674163818
Loss :  1.9986766576766968 4.3877177238464355 4.3877177238464355
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 4, 5], device='cuda:0')
Loss :  1.9947890043258667 4.402781009674072 4.402781009674072
Loss :  1.9966959953308105 4.305474281311035 4.305474281311035
Loss :  2.0102591514587402 4.267278671264648 4.267278671264648
Total LOSS train 4.493637583805964 valid 4.340812921524048
CE LOSS train 1.8805737880560067 valid 0.5025647878646851
Contrastive LOSS train 4.493637583805964 valid 1.066819667816162
EPOCH 91:
Loss :  1.8262403011322021 4.307703018188477 4.307703018188477
Loss :  1.7986098527908325 4.633553504943848 4.633553504943848
Loss :  1.8449634313583374 4.299357891082764 4.299357891082764
Loss :  1.8424735069274902 4.481436252593994 4.481436252593994
Loss :  1.828608751296997 4.308486461639404 4.308486461639404
Loss :  1.8391528129577637 4.4267377853393555 4.4267377853393555
Loss :  1.848275065422058 4.671173572540283 4.671173572540283
Loss :  1.8292064666748047 4.284640789031982 4.284640789031982
Loss :  1.8612860441207886 4.468103885650635 4.468103885650635
Loss :  1.7955306768417358 4.766148090362549 4.766148090362549
Loss :  1.837064266204834 4.524998188018799 4.524998188018799
Loss :  1.8443164825439453 4.775494575500488 4.775494575500488
Loss :  1.8544296026229858 4.735748767852783 4.735748767852783
Loss :  1.9348673820495605 4.5771965980529785 4.5771965980529785
Loss :  1.8854845762252808 4.35216760635376 4.35216760635376
Loss :  1.878196120262146 4.4561848640441895 4.4561848640441895
Loss :  1.9083400964736938 4.495812892913818 4.495812892913818
Loss :  1.8807166814804077 4.417792320251465 4.417792320251465
Loss :  1.925862431526184 4.41843318939209 4.41843318939209
Loss :  1.8902114629745483 4.503920078277588 4.503920078277588
  batch 20 loss: 1.8902114629745483, 4.503920078277588, 4.503920078277588
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9104912281036377 4.43190860748291 4.43190860748291
Loss :  1.9306670427322388 4.7139129638671875 4.7139129638671875
Loss :  1.9003136157989502 4.600066661834717 4.600066661834717
Loss :  1.9189834594726562 4.48570442199707 4.48570442199707
Loss :  1.942378282546997 4.6542487144470215 4.6542487144470215
Loss :  1.9664063453674316 4.2445197105407715 4.2445197105407715
Loss :  1.9056053161621094 4.663567066192627 4.663567066192627
Loss :  1.888552188873291 4.390058517456055 4.390058517456055
Loss :  1.976011872291565 4.390619277954102 4.390619277954102
Loss :  1.9251585006713867 4.346185684204102 4.346185684204102
Loss :  1.9677648544311523 4.530956745147705 4.530956745147705
Loss :  1.9364231824874878 4.5357866287231445 4.5357866287231445
Loss :  1.953660011291504 4.423532485961914 4.423532485961914
Loss :  1.9760428667068481 4.488702774047852 4.488702774047852
Loss :  2.0080015659332275 4.486546516418457 4.486546516418457
Loss :  1.9535118341445923 4.39671516418457 4.39671516418457
Loss :  1.9668742418289185 4.421900749206543 4.421900749206543
Loss :  1.83686363697052 4.534595966339111 4.534595966339111
Loss :  1.8583195209503174 4.342301368713379 4.342301368713379
Loss :  1.8982244729995728 4.474562644958496 4.474562644958496
  batch 40 loss: 1.8982244729995728, 4.474562644958496, 4.474562644958496
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9139065742492676 4.467957973480225 4.467957973480225
Loss :  1.9312223196029663 4.4208173751831055 4.4208173751831055
Loss :  1.9532742500305176 4.532287120819092 4.532287120819092
Loss :  1.940307378768921 4.4340596199035645 4.4340596199035645
Loss :  1.9725934267044067 4.483701229095459 4.483701229095459
Loss :  1.9415963888168335 4.457926273345947 4.457926273345947
Loss :  1.856046199798584 4.395714282989502 4.395714282989502
Loss :  1.9974098205566406 4.49344539642334 4.49344539642334
Loss :  1.8708629608154297 4.52109432220459 4.52109432220459
Loss :  1.949136734008789 4.480615139007568 4.480615139007568
Loss :  1.9566115140914917 4.629919528961182 4.629919528961182
Loss :  1.8959331512451172 4.4283318519592285 4.4283318519592285
Loss :  1.9770350456237793 4.357280254364014 4.357280254364014
Loss :  1.938224196434021 4.4234209060668945 4.4234209060668945
Loss :  1.904189944267273 4.3702921867370605 4.3702921867370605
Loss :  1.908707857131958 4.477036476135254 4.477036476135254
Loss :  1.9021068811416626 4.477272033691406 4.477272033691406
Loss :  1.9551222324371338 4.532588481903076 4.532588481903076
Loss :  1.9787359237670898 4.578163146972656 4.578163146972656
Loss :  1.8646373748779297 4.552735805511475 4.552735805511475
  batch 60 loss: 1.8646373748779297, 4.552735805511475, 4.552735805511475
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.9459867477416992 4.546560287475586 4.546560287475586
Loss :  1.916910171508789 4.547643661499023 4.547643661499023
Loss :  1.9766029119491577 4.522497177124023 4.522497177124023
Loss :  1.9514563083648682 4.4531965255737305 4.4531965255737305
Loss :  2.0075371265411377 4.073829650878906 4.073829650878906
Loss :  1.4234000444412231 4.3911614418029785 4.3911614418029785
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
Loss :  1.5895440578460693 4.375304222106934 4.375304222106934
Loss :  1.613855242729187 4.305769443511963 4.305769443511963
Loss :  1.6453144550323486 4.232848167419434 4.232848167419434
Total LOSS train 4.478767226292537 valid 4.326270818710327
CE LOSS train 1.9104652844942533 valid 0.41132861375808716
Contrastive LOSS train 4.478767226292537 valid 1.0582120418548584
EPOCH 92:
Loss :  1.9268498420715332 4.3917951583862305 4.3917951583862305
Loss :  1.918636679649353 4.534337043762207 4.534337043762207
Loss :  2.000354051589966 4.451197624206543 4.451197624206543
Loss :  1.9310747385025024 4.4395270347595215 4.4395270347595215
Loss :  1.9892876148223877 4.364729881286621 4.364729881286621
Loss :  1.9144041538238525 4.355556011199951 4.355556011199951
Loss :  1.9000028371810913 4.584011554718018 4.584011554718018
Loss :  1.9156244993209839 4.3659348487854 4.3659348487854
Loss :  1.9380956888198853 4.340903282165527 4.340903282165527
Loss :  1.8956447839736938 4.344188213348389 4.344188213348389
Loss :  1.9354246854782104 4.618611812591553 4.618611812591553
Loss :  1.9259095191955566 4.529412269592285 4.529412269592285
Loss :  1.9191190004348755 4.529660224914551 4.529660224914551
Loss :  1.808073878288269 4.585176467895508 4.585176467895508
Loss :  1.8723663091659546 4.600539684295654 4.600539684295654
Loss :  1.9041807651519775 4.514406681060791 4.514406681060791
Loss :  1.8335994482040405 4.550744533538818 4.550744533538818
Loss :  1.8588953018188477 4.4388508796691895 4.4388508796691895
Loss :  1.8398183584213257 4.42155647277832 4.42155647277832
Loss :  1.8689254522323608 4.489474296569824 4.489474296569824
  batch 20 loss: 1.8689254522323608, 4.489474296569824, 4.489474296569824
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 3, 4, 5], device='cuda:0')
Loss :  1.8271223306655884 4.874989032745361 4.874989032745361
Loss :  1.8559482097625732 5.079133033752441 5.079133033752441
Loss :  1.8468132019042969 4.5671610832214355 4.5671610832214355
Loss :  1.8058724403381348 5.027608871459961 5.027608871459961
Loss :  1.8053148984909058 5.0682196617126465 5.0682196617126465
Loss :  1.832658290863037 4.3694233894348145 4.3694233894348145
Loss :  1.8900524377822876 4.63348388671875 4.63348388671875
Loss :  1.8636027574539185 4.555385112762451 4.555385112762451
Loss :  1.852803111076355 4.496997356414795 4.496997356414795
Loss :  1.8333516120910645 4.539566516876221 4.539566516876221
Loss :  1.8255175352096558 4.566502571105957 4.566502571105957
Loss :  1.8367682695388794 4.620509624481201 4.620509624481201
Loss :  1.8000752925872803 4.495392322540283 4.495392322540283
Loss :  1.8013092279434204 5.303999900817871 5.303999900817871
Loss :  1.834223985671997 4.4938459396362305 4.4938459396362305
Loss :  1.8098589181900024 4.995645523071289 4.995645523071289
Loss :  1.8198094367980957 4.409286022186279 4.409286022186279
Loss :  1.8275048732757568 4.43909215927124 4.43909215927124
Loss :  1.8655201196670532 4.352478504180908 4.352478504180908
Loss :  1.8781129121780396 4.660323619842529 4.660323619842529
  batch 40 loss: 1.8781129121780396, 4.660323619842529, 4.660323619842529
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 3, 4, 5], device='cuda:0')
Loss :  1.8660674095153809 4.404192924499512 4.404192924499512
Loss :  1.8628206253051758 4.700368881225586 4.700368881225586
Loss :  1.8069919347763062 4.416502952575684 4.416502952575684
Loss :  1.8262629508972168 4.5910325050354 4.5910325050354
Loss :  1.806230068206787 4.369304180145264 4.369304180145264
Loss :  1.8315426111221313 4.333603858947754 4.333603858947754
Loss :  1.795078992843628 4.517492771148682 4.517492771148682
Loss :  1.7888487577438354 4.565472602844238 4.565472602844238
Loss :  1.8008530139923096 4.5175347328186035 4.5175347328186035
Loss :  1.7750440835952759 4.58818244934082 4.58818244934082
Loss :  1.7660791873931885 4.560372829437256 4.560372829437256
Loss :  1.8425798416137695 4.436859130859375 4.436859130859375
Loss :  1.801591396331787 4.4785661697387695 4.4785661697387695
Loss :  1.8013746738433838 4.417913436889648 4.417913436889648
Loss :  1.7885167598724365 4.4284138679504395 4.4284138679504395
Loss :  1.8361371755599976 4.963317394256592 4.963317394256592
Loss :  1.852678894996643 4.51198673248291 4.51198673248291
Loss :  1.807589054107666 4.573167324066162 4.573167324066162
Loss :  1.7982161045074463 4.800559997558594 4.800559997558594
Loss :  1.8402916193008423 4.476866722106934 4.476866722106934
  batch 60 loss: 1.8402916193008423, 4.476866722106934, 4.476866722106934
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 3, 4, 5], device='cuda:0')
Loss :  1.7963460683822632 4.500138282775879 4.500138282775879
Loss :  1.811023473739624 4.479812145233154 4.479812145233154
Loss :  1.8572311401367188 4.5319061279296875 4.5319061279296875
Loss :  1.8358389139175415 4.518374443054199 4.518374443054199
Loss :  1.7933416366577148 4.125461578369141 4.125461578369141
Loss :  1.781843900680542 4.32771110534668 4.32771110534668
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([5], device='cuda:0')
Loss :  1.7748464345932007 4.425230503082275 4.425230503082275
Loss :  1.7661850452423096 4.421945571899414 4.421945571899414
Loss :  1.842859148979187 4.037408828735352 4.037408828735352
Total LOSS train 4.550877879216121 valid 4.30307400226593
CE LOSS train 1.8491862131999088 valid 0.46071478724479675
Contrastive LOSS train 4.550877879216121 valid 1.009352207183838
EPOCH 93:
Loss :  1.8400872945785522 4.619695663452148 4.619695663452148
Loss :  1.9011731147766113 4.741638660430908 4.741638660430908
Loss :  1.8799365758895874 4.441237926483154 4.441237926483154
Loss :  1.826887607574463 4.461099624633789 4.461099624633789
Loss :  1.7167662382125854 4.59960412979126 4.59960412979126
Loss :  1.9267795085906982 4.548555850982666 4.548555850982666
Loss :  1.8898475170135498 4.5688581466674805 4.5688581466674805
Loss :  1.8555958271026611 4.270770072937012 4.270770072937012
Loss :  1.870222806930542 4.376288890838623 4.376288890838623
Loss :  1.8336342573165894 4.27717399597168 4.27717399597168
Loss :  1.8383300304412842 4.539914131164551 4.539914131164551
Loss :  1.8161826133728027 4.593412399291992 4.593412399291992
Loss :  1.807003378868103 4.465710639953613 4.465710639953613
Loss :  1.8311517238616943 4.623289108276367 4.623289108276367
Loss :  1.816482663154602 4.576919078826904 4.576919078826904
Loss :  1.8913958072662354 4.563718795776367 4.563718795776367
Loss :  1.8446130752563477 4.417068004608154 4.417068004608154
Loss :  1.8387267589569092 4.386621952056885 4.386621952056885
Loss :  1.7926853895187378 4.57837438583374 4.57837438583374
Loss :  1.8023793697357178 4.635775089263916 4.635775089263916
  batch 20 loss: 1.8023793697357178, 4.635775089263916, 4.635775089263916
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 3, 4, 5], device='cuda:0')
Loss :  1.81198251247406 4.361215114593506 4.361215114593506
Loss :  1.8029295206069946 4.579426288604736 4.579426288604736
Loss :  1.8108218908309937 4.639434337615967 4.639434337615967
Loss :  1.7704929113388062 4.568164348602295 4.568164348602295
Loss :  1.8888256549835205 4.5763115882873535 4.5763115882873535
Loss :  1.8126661777496338 4.430202007293701 4.430202007293701
Loss :  1.9313859939575195 4.495462417602539 4.495462417602539
Loss :  1.7610336542129517 4.487305164337158 4.487305164337158
Loss :  1.8174874782562256 4.4212422370910645 4.4212422370910645
Loss :  1.8847832679748535 4.641848564147949 4.641848564147949
Loss :  1.7699891328811646 4.432602882385254 4.432602882385254
Loss :  1.8324377536773682 4.651007652282715 4.651007652282715
Loss :  1.8105837106704712 4.384734153747559 4.384734153747559
Loss :  1.7715320587158203 4.532246112823486 4.532246112823486
Loss :  1.8023254871368408 4.688106060028076 4.688106060028076
Loss :  1.793541431427002 4.469455242156982 4.469455242156982
Loss :  1.740994930267334 4.445361614227295 4.445361614227295
Loss :  1.8215302228927612 4.477545261383057 4.477545261383057
Loss :  1.7787635326385498 4.317417621612549 4.317417621612549
Loss :  1.8288825750350952 4.466169357299805 4.466169357299805
  batch 40 loss: 1.8288825750350952, 4.466169357299805, 4.466169357299805
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.8676902055740356 4.3839850425720215 4.3839850425720215
Loss :  1.833274245262146 4.4304680824279785 4.4304680824279785
Loss :  1.7652637958526611 4.401538372039795 4.401538372039795
Loss :  1.814836859703064 4.5358147621154785 4.5358147621154785
Loss :  1.7383038997650146 4.288542747497559 4.288542747497559
Loss :  1.8837323188781738 4.363462924957275 4.363462924957275
Loss :  1.933292031288147 4.504058837890625 4.504058837890625
Loss :  1.7553797960281372 4.495328903198242 4.495328903198242
Loss :  1.8383491039276123 4.51107120513916 4.51107120513916
Loss :  1.7062010765075684 4.539343357086182 4.539343357086182
Loss :  1.7251472473144531 4.529524803161621 4.529524803161621
Loss :  1.880423665046692 4.628364086151123 4.628364086151123
Loss :  1.7800308465957642 4.346590995788574 4.346590995788574
Loss :  1.8268043994903564 4.428529739379883 4.428529739379883
Loss :  1.7085230350494385 4.832565784454346 4.832565784454346
Loss :  1.835152268409729 4.439757347106934 4.439757347106934
Loss :  1.781355381011963 4.383764743804932 4.383764743804932
Loss :  1.7403781414031982 4.465506553649902 4.465506553649902
Loss :  1.784287691116333 4.677289009094238 4.677289009094238
Loss :  1.8090969324111938 4.54091739654541 4.54091739654541
  batch 60 loss: 1.8090969324111938, 4.54091739654541, 4.54091739654541
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.7882581949234009 4.469430446624756 4.469430446624756
Loss :  1.7969268560409546 4.414642333984375 4.414642333984375
Loss :  1.8261862993240356 4.47890043258667 4.47890043258667
Loss :  1.7445464134216309 4.436032772064209 4.436032772064209
Loss :  1.7454875707626343 4.137135982513428 4.137135982513428
Loss :  1.5789378881454468 4.455831050872803 4.455831050872803
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.5879524946212769 4.413061141967773 4.413061141967773
Loss :  1.5674045085906982 4.357411861419678 4.357411861419678
Loss :  1.6890082359313965 4.386521339416504 4.386521339416504
Total LOSS train 4.492516172849215 valid 4.4032063484191895
CE LOSS train 1.8149507650962242 valid 0.4222520589828491
Contrastive LOSS train 4.492516172849215 valid 1.096630334854126
EPOCH 94:
Loss :  1.7956020832061768 4.416120529174805 4.416120529174805
Loss :  1.9030896425247192 4.639793395996094 4.639793395996094
Loss :  1.8305726051330566 4.385181903839111 4.385181903839111
Loss :  1.8397947549819946 4.430349349975586 4.430349349975586
Loss :  1.9217815399169922 4.3654961585998535 4.3654961585998535
Loss :  1.778408408164978 4.528914928436279 4.528914928436279
Loss :  1.8191269636154175 4.470980644226074 4.470980644226074
Loss :  1.7374908924102783 4.415760517120361 4.415760517120361
Loss :  1.766096591949463 4.428999900817871 4.428999900817871
Loss :  1.8416162729263306 4.423757076263428 4.423757076263428
Loss :  1.7731850147247314 4.445267200469971 4.445267200469971
Loss :  1.7907253503799438 4.399458885192871 4.399458885192871
Loss :  1.815449595451355 4.664093971252441 4.664093971252441
Loss :  1.7556766271591187 4.584832668304443 4.584832668304443
Loss :  1.7646249532699585 4.6675567626953125 4.6675567626953125
Loss :  1.9139189720153809 4.6958112716674805 4.6958112716674805
Loss :  1.8523482084274292 4.535655498504639 4.535655498504639
Loss :  1.841396689414978 4.621251583099365 4.621251583099365
Loss :  1.7840611934661865 4.314567565917969 4.314567565917969
Loss :  1.8625938892364502 4.5085320472717285 4.5085320472717285
  batch 20 loss: 1.8625938892364502, 4.5085320472717285, 4.5085320472717285
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.8158847093582153 4.402981281280518 4.402981281280518
Loss :  1.8163769245147705 4.514131546020508 4.514131546020508
Loss :  1.8108599185943604 4.793524265289307 4.793524265289307
Loss :  1.8299126625061035 4.430768966674805 4.430768966674805
Loss :  1.9164550304412842 4.709833145141602 4.709833145141602
Loss :  1.803328514099121 4.37750768661499 4.37750768661499
Loss :  1.850825548171997 4.63206672668457 4.63206672668457
Loss :  1.8412271738052368 4.395683765411377 4.395683765411377
Loss :  1.8029658794403076 4.45730447769165 4.45730447769165
Loss :  1.8216578960418701 4.491591930389404 4.491591930389404
Loss :  1.7933838367462158 4.651207447052002 4.651207447052002
Loss :  1.8189231157302856 4.543732166290283 4.543732166290283
Loss :  1.811332106590271 4.529110908508301 4.529110908508301
Loss :  1.7970669269561768 4.485038757324219 4.485038757324219
Loss :  1.9094970226287842 4.555610179901123 4.555610179901123
Loss :  1.856126070022583 4.476731777191162 4.476731777191162
Loss :  1.8801966905593872 4.467453956604004 4.467453956604004
Loss :  1.8814901113510132 4.491453647613525 4.491453647613525
Loss :  1.9486644268035889 4.510983943939209 4.510983943939209
Loss :  1.9026092290878296 4.578433513641357 4.578433513641357
  batch 40 loss: 1.9026092290878296, 4.578433513641357, 4.578433513641357
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.9036705493927002 4.462707042694092 4.462707042694092
Loss :  1.8949816226959229 4.529093265533447 4.529093265533447
Loss :  1.81960928440094 4.478733539581299 4.478733539581299
Loss :  1.9175676107406616 4.6808390617370605 4.6808390617370605
Loss :  1.843718409538269 4.549376487731934 4.549376487731934
Loss :  1.9944078922271729 4.420028209686279 4.420028209686279
Loss :  1.9562721252441406 4.308135509490967 4.308135509490967
Loss :  1.8238890171051025 4.490594387054443 4.490594387054443
Loss :  1.8325223922729492 4.288003444671631 4.288003444671631
Loss :  1.8259748220443726 4.666842937469482 4.666842937469482
Loss :  1.8622804880142212 4.539290428161621 4.539290428161621
Loss :  1.9768741130828857 4.859823703765869 4.859823703765869
Loss :  1.9139788150787354 4.759132385253906 4.759132385253906
Loss :  1.9205806255340576 4.455291748046875 4.455291748046875
Loss :  1.9193779230117798 4.45669412612915 4.45669412612915
Loss :  1.9169509410858154 4.500430583953857 4.500430583953857
Loss :  1.8736169338226318 4.574456214904785 4.574456214904785
Loss :  1.9034924507141113 4.572412014007568 4.572412014007568
Loss :  1.9956122636795044 4.616383075714111 4.616383075714111
Loss :  1.8558310270309448 4.449410438537598 4.449410438537598
  batch 60 loss: 1.8558310270309448, 4.449410438537598, 4.449410438537598
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7182220220565796 4.47415828704834 4.47415828704834
Loss :  2.0368926525115967 4.668610572814941 4.668610572814941
Loss :  1.8769800662994385 4.555293560028076 4.555293560028076
Loss :  1.846828818321228 4.548745632171631 4.548745632171631
Loss :  1.910986065864563 4.0802106857299805 4.0802106857299805
Loss :  2.363083600997925 4.462100505828857 4.462100505828857
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
Loss :  2.0873162746429443 4.405385494232178 4.405385494232178
Loss :  2.721989393234253 4.39593505859375 4.39593505859375
Loss :  2.2535300254821777 4.260918140411377 4.260918140411377
Total LOSS train 4.514188142923208 valid 4.3810847997665405
CE LOSS train 1.8559609688245333 valid 0.5633825063705444
Contrastive LOSS train 4.514188142923208 valid 1.0652295351028442
EPOCH 95:
Loss :  1.986312985420227 4.38156270980835 4.38156270980835
Loss :  1.8504209518432617 4.806506633758545 4.806506633758545
Loss :  1.8314303159713745 4.3598151206970215 4.3598151206970215
Loss :  1.950196385383606 4.458226680755615 4.458226680755615
Loss :  2.064814567565918 4.250481128692627 4.250481128692627
Loss :  1.7706493139266968 4.327567100524902 4.327567100524902
Loss :  1.8747502565383911 4.482172012329102 4.482172012329102
Loss :  1.858377456665039 4.3518242835998535 4.3518242835998535
Loss :  1.8790980577468872 4.3559393882751465 4.3559393882751465
Loss :  1.887380838394165 4.655371189117432 4.655371189117432
Loss :  1.8771010637283325 4.614036560058594 4.614036560058594
Loss :  1.885977029800415 4.633572578430176 4.633572578430176
Loss :  1.9295856952667236 4.436238765716553 4.436238765716553
Loss :  1.876795768737793 4.673469066619873 4.673469066619873
Loss :  1.8151196241378784 4.366391181945801 4.366391181945801
Loss :  1.9031851291656494 4.6315717697143555 4.6315717697143555
Loss :  1.9487699270248413 4.464902877807617 4.464902877807617
Loss :  1.9004381895065308 4.435466289520264 4.435466289520264
Loss :  1.8564289808273315 4.3472185134887695 4.3472185134887695
Loss :  1.7932668924331665 4.5255818367004395 4.5255818367004395
  batch 20 loss: 1.7932668924331665, 4.5255818367004395, 4.5255818367004395
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
Loss :  1.8342158794403076 4.40180778503418 4.40180778503418
Loss :  1.8705974817276 4.409465312957764 4.409465312957764
Loss :  1.8621211051940918 4.6798176765441895 4.6798176765441895
Loss :  1.8441346883773804 4.490906238555908 4.490906238555908
Loss :  1.7369227409362793 4.566244602203369 4.566244602203369
Loss :  1.8501776456832886 4.505572319030762 4.505572319030762
Loss :  1.977041482925415 4.873039245605469 4.873039245605469
Loss :  1.8928732872009277 4.340446472167969 4.340446472167969
Loss :  1.8917016983032227 4.637465476989746 4.637465476989746
Loss :  1.9823907613754272 4.4713873863220215 4.4713873863220215
Loss :  1.883419156074524 4.515887260437012 4.515887260437012
Loss :  1.8637768030166626 4.667710304260254 4.667710304260254
Loss :  1.9331811666488647 4.475407123565674 4.475407123565674
Loss :  1.923406958580017 4.474369525909424 4.474369525909424
Loss :  1.9085149765014648 4.561365127563477 4.561365127563477
Loss :  1.8229905366897583 4.734731197357178 4.734731197357178
Loss :  1.9624134302139282 4.798001289367676 4.798001289367676
Loss :  1.9437938928604126 4.749011516571045 4.749011516571045
Loss :  1.9550126791000366 4.861391067504883 4.861391067504883
Loss :  1.9304052591323853 4.779471397399902 4.779471397399902
  batch 40 loss: 1.9304052591323853, 4.779471397399902, 4.779471397399902
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8609274625778198 4.473959922790527 4.473959922790527
Loss :  1.821721076965332 4.396156311035156 4.396156311035156
Loss :  1.8942339420318604 4.454278469085693 4.454278469085693
Loss :  1.824573278427124 4.5951690673828125 4.5951690673828125
Loss :  1.9169906377792358 4.481644630432129 4.481644630432129
Loss :  1.8504598140716553 4.5309906005859375 4.5309906005859375
Loss :  1.939497470855713 4.512368679046631 4.512368679046631
Loss :  1.9051576852798462 4.6863484382629395 4.6863484382629395
Loss :  1.9178329706192017 4.565157890319824 4.565157890319824
Loss :  1.8669912815093994 4.706476211547852 4.706476211547852
Loss :  1.8875290155410767 4.4571852684021 4.4571852684021
Loss :  1.9017075300216675 4.723637104034424 4.723637104034424
Loss :  1.9636554718017578 4.270993232727051 4.270993232727051
Loss :  1.9669972658157349 4.682531356811523 4.682531356811523
Loss :  1.9154289960861206 4.480296611785889 4.480296611785889
Loss :  1.9041391611099243 4.422957420349121 4.422957420349121
Loss :  1.9038779735565186 4.459136009216309 4.459136009216309
Loss :  1.8796751499176025 4.493136405944824 4.493136405944824
Loss :  1.8793879747390747 4.574898719787598 4.574898719787598
Loss :  1.8729442358016968 4.331402778625488 4.331402778625488
  batch 60 loss: 1.8729442358016968, 4.331402778625488, 4.331402778625488
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3], device='cuda:0')
Loss :  1.9271793365478516 4.745887279510498 4.745887279510498
Loss :  1.9319303035736084 4.60723876953125 4.60723876953125
Loss :  1.9481027126312256 4.5215888023376465 4.5215888023376465
Loss :  1.8941960334777832 4.41728401184082 4.41728401184082
Loss :  1.865203619003296 4.201622486114502 4.201622486114502
Loss :  2.61079740524292 4.396769046783447 4.396769046783447
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 4], device='cuda:0')
Loss :  2.5603742599487305 4.437438011169434 4.437438011169434
Loss :  2.5788633823394775 4.306357383728027 4.306357383728027
Loss :  2.5191750526428223 4.330021858215332 4.330021858215332
Total LOSS train 4.528365546006423 valid 4.36764657497406
CE LOSS train 1.8930697147662823 valid 0.6297937631607056
Contrastive LOSS train 4.528365546006423 valid 1.082505464553833
EPOCH 96:
Loss :  2.0111773014068604 4.780549049377441 4.780549049377441
Loss :  1.970927119255066 4.618788719177246 4.618788719177246
Loss :  1.9951330423355103 4.655431270599365 4.655431270599365
Loss :  1.9671000242233276 4.462658405303955 4.462658405303955
Loss :  1.843537449836731 4.415017604827881 4.415017604827881
Loss :  1.97120201587677 4.518987655639648 4.518987655639648
Loss :  1.9062285423278809 4.50480318069458 4.50480318069458
Loss :  1.9299860000610352 4.254887580871582 4.254887580871582
Loss :  1.9601943492889404 4.493689060211182 4.493689060211182
Loss :  1.86776864528656 4.39164400100708 4.39164400100708
Loss :  1.9246175289154053 4.515511512756348 4.515511512756348
Loss :  2.0128283500671387 4.433406352996826 4.433406352996826
Loss :  1.9146451950073242 4.468453884124756 4.468453884124756
Loss :  1.9346075057983398 4.452856063842773 4.452856063842773
Loss :  1.8677455186843872 4.4471049308776855 4.4471049308776855
Loss :  1.9644230604171753 4.4301557540893555 4.4301557540893555
Loss :  1.942859172821045 4.407046318054199 4.407046318054199
Loss :  1.9436601400375366 4.381632328033447 4.381632328033447
Loss :  1.9474624395370483 4.281196117401123 4.281196117401123
Loss :  1.9179589748382568 4.562514305114746 4.562514305114746
  batch 20 loss: 1.9179589748382568, 4.562514305114746, 4.562514305114746
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
Loss :  1.9129302501678467 4.378832817077637 4.378832817077637
Loss :  1.976590633392334 4.6465911865234375 4.6465911865234375
Loss :  1.9683336019515991 4.594368934631348 4.594368934631348
Loss :  1.9270051717758179 4.48262882232666 4.48262882232666
Loss :  1.9371036291122437 4.535853862762451 4.535853862762451
Loss :  1.9706181287765503 4.447984218597412 4.447984218597412
Loss :  2.027780532836914 4.50587797164917 4.50587797164917
Loss :  1.9733302593231201 4.500715732574463 4.500715732574463
Loss :  1.9965797662734985 4.482337951660156 4.482337951660156
Loss :  1.9963470697402954 4.433247089385986 4.433247089385986
Loss :  2.0368714332580566 4.559447765350342 4.559447765350342
Loss :  1.9230912923812866 4.6500115394592285 4.6500115394592285
Loss :  2.033423662185669 4.510883808135986 4.510883808135986
Loss :  1.9661084413528442 4.415121078491211 4.415121078491211
Loss :  2.0332119464874268 4.5361480712890625 4.5361480712890625
Loss :  2.0739831924438477 4.38995885848999 4.38995885848999
Loss :  1.9791324138641357 4.430944919586182 4.430944919586182
Loss :  1.8284602165222168 4.330876350402832 4.330876350402832
Loss :  1.8871524333953857 4.5650787353515625 4.5650787353515625
Loss :  1.9332194328308105 4.455400466918945 4.455400466918945
  batch 40 loss: 1.9332194328308105, 4.455400466918945, 4.455400466918945
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.9661962985992432 4.568991184234619 4.568991184234619
Loss :  1.992048740386963 4.438601970672607 4.438601970672607
Loss :  2.021388530731201 4.386958599090576 4.386958599090576
Loss :  1.9816776514053345 4.494136333465576 4.494136333465576
Loss :  1.9903721809387207 4.262538433074951 4.262538433074951
Loss :  1.957308053970337 4.398986339569092 4.398986339569092
Loss :  1.9280179738998413 4.537045955657959 4.537045955657959
Loss :  1.987926721572876 4.4545392990112305 4.4545392990112305
Loss :  1.898527979850769 4.4173359870910645 4.4173359870910645
Loss :  2.0081849098205566 4.549767017364502 4.549767017364502
Loss :  1.9289506673812866 4.697941780090332 4.697941780090332
Loss :  1.96450674533844 4.441947937011719 4.441947937011719
Loss :  2.0004684925079346 4.358154773712158 4.358154773712158
Loss :  1.9541555643081665 4.389023780822754 4.389023780822754
Loss :  1.982718586921692 4.511875629425049 4.511875629425049
Loss :  1.9286489486694336 4.448329448699951 4.448329448699951
Loss :  2.063123941421509 4.536777019500732 4.536777019500732
Loss :  1.9904943704605103 4.443881034851074 4.443881034851074
Loss :  1.9969223737716675 4.581503868103027 4.581503868103027
Loss :  1.9113835096359253 4.460248947143555 4.460248947143555
  batch 60 loss: 1.9113835096359253, 4.460248947143555, 4.460248947143555
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9718962907791138 4.67786979675293 4.67786979675293
Loss :  1.9580291509628296 4.363621234893799 4.363621234893799
Loss :  1.9690401554107666 4.320647716522217 4.320647716522217
Loss :  1.9977545738220215 4.292205333709717 4.292205333709717
Loss :  2.08573579788208 3.884962797164917 3.884962797164917
Loss :  1.672978401184082 4.396888732910156 4.396888732910156
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4], device='cuda:0')
Loss :  1.641217589378357 4.435952186584473 4.435952186584473
Loss :  1.6772160530090332 4.329482555389404 4.329482555389404
Loss :  1.6454616785049438 4.258956432342529 4.258956432342529
Total LOSS train 4.465331330666175 valid 4.355319976806641
CE LOSS train 1.963243293762207 valid 0.41136541962623596
Contrastive LOSS train 4.465331330666175 valid 1.0647391080856323
EPOCH 97:
Loss :  2.0054666996002197 3.9752697944641113 3.9752697944641113
Loss :  1.9923878908157349 4.310154438018799 4.310154438018799
Loss :  2.044844388961792 4.013559341430664 4.013559341430664
Loss :  2.036187171936035 4.0841474533081055 4.0841474533081055
Loss :  2.0063507556915283 3.912985324859619 3.912985324859619
Loss :  2.064387321472168 3.9870049953460693 3.9870049953460693
Loss :  1.9834920167922974 4.5280585289001465 4.5280585289001465
Loss :  2.0101447105407715 4.294995307922363 4.294995307922363
Loss :  2.0193049907684326 4.477968692779541 4.477968692779541
Loss :  1.9550120830535889 4.282045841217041 4.282045841217041
Loss :  2.0167760848999023 4.458723068237305 4.458723068237305
Loss :  2.028010845184326 4.36950159072876 4.36950159072876
Loss :  2.0283374786376953 4.409799575805664 4.409799575805664
Loss :  2.035602331161499 4.526268005371094 4.526268005371094
Loss :  1.9720335006713867 4.158881187438965 4.158881187438965
Loss :  1.904585599899292 4.4622955322265625 4.4622955322265625
Loss :  2.0004990100860596 4.31259298324585 4.31259298324585
Loss :  1.9247592687606812 4.434159278869629 4.434159278869629
Loss :  2.044677972793579 4.423973560333252 4.423973560333252
Loss :  1.8681483268737793 4.567325115203857 4.567325115203857
  batch 20 loss: 1.8681483268737793, 4.567325115203857, 4.567325115203857
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  2.019697904586792 4.513850688934326 4.513850688934326
Loss :  2.0144009590148926 4.294174671173096 4.294174671173096
Loss :  1.9456778764724731 4.510350704193115 4.510350704193115
Loss :  2.000854253768921 4.237592697143555 4.237592697143555
Loss :  1.963541865348816 4.192275524139404 4.192275524139404
Loss :  1.9585174322128296 3.996325969696045 3.996325969696045
Loss :  1.923348307609558 4.439314365386963 4.439314365386963
Loss :  1.9801276922225952 4.454583644866943 4.454583644866943
Loss :  2.089181423187256 4.382065773010254 4.382065773010254
Loss :  2.0392003059387207 4.481776237487793 4.481776237487793
Loss :  2.086911201477051 4.544366359710693 4.544366359710693
Loss :  1.9330931901931763 4.559382438659668 4.559382438659668
Loss :  1.9851958751678467 4.465333461761475 4.465333461761475
Loss :  2.0609750747680664 4.575377941131592 4.575377941131592
Loss :  2.097780227661133 4.552801132202148 4.552801132202148
Loss :  2.1192522048950195 4.703405857086182 4.703405857086182
Loss :  2.071697473526001 4.387429237365723 4.387429237365723
Loss :  1.9976541996002197 4.371715068817139 4.371715068817139
Loss :  2.0257551670074463 4.541504859924316 4.541504859924316
Loss :  1.9392231702804565 4.1519455909729 4.1519455909729
  batch 40 loss: 1.9392231702804565, 4.1519455909729, 4.1519455909729
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9356212615966797 4.1941022872924805 4.1941022872924805
Loss :  2.019704580307007 4.327506065368652 4.327506065368652
Loss :  2.020179033279419 4.684927463531494 4.684927463531494
Loss :  2.060210943222046 4.566967964172363 4.566967964172363
Loss :  2.01609206199646 4.3164143562316895 4.3164143562316895
Loss :  1.9797427654266357 4.6898193359375 4.6898193359375
Loss :  1.9012844562530518 4.64504861831665 4.64504861831665
Loss :  2.030186891555786 4.506194114685059 4.506194114685059
Loss :  1.8869482278823853 4.688783168792725 4.688783168792725
Loss :  2.010232448577881 4.780849933624268 4.780849933624268
Loss :  2.0765256881713867 4.475464820861816 4.475464820861816
Loss :  2.002440929412842 4.34516716003418 4.34516716003418
Loss :  2.0277953147888184 4.377903938293457 4.377903938293457
Loss :  1.9813849925994873 4.4028472900390625 4.4028472900390625
Loss :  2.014052152633667 4.529883861541748 4.529883861541748
Loss :  2.01591157913208 4.275352478027344 4.275352478027344
Loss :  1.968976378440857 4.470515727996826 4.470515727996826
Loss :  2.0317511558532715 4.5016303062438965 4.5016303062438965
Loss :  2.0741422176361084 4.600629806518555 4.600629806518555
Loss :  1.9369778633117676 4.417429447174072 4.417429447174072
  batch 60 loss: 1.9369778633117676, 4.417429447174072, 4.417429447174072
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
Loss :  1.909916639328003 4.501661777496338 4.501661777496338
Loss :  1.9278182983398438 4.459286689758301 4.459286689758301
Loss :  1.8853648900985718 4.5402021408081055 4.5402021408081055
Loss :  1.9843392372131348 4.423893451690674 4.423893451690674
Loss :  2.057076930999756 4.210112571716309 4.210112571716309
Loss :  2.2858798503875732 4.389453411102295 4.389453411102295
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 4], device='cuda:0')
Loss :  2.228687286376953 4.476435661315918 4.476435661315918
Loss :  2.3104827404022217 4.349536418914795 4.349536418914795
Loss :  2.088880777359009 4.278058052062988 4.278058052062988
Total LOSS train 4.40427502485422 valid 4.373370885848999
CE LOSS train 1.999196479870723 valid 0.5222201943397522
Contrastive LOSS train 4.40427502485422 valid 1.069514513015747
EPOCH 98:
Loss :  1.9734852313995361 4.559661388397217 4.559661388397217
Loss :  2.0254032611846924 4.492494583129883 4.492494583129883
Loss :  2.0215682983398438 4.375362873077393 4.375362873077393
Loss :  1.9445807933807373 4.686139106750488 4.686139106750488
Loss :  1.8648637533187866 4.41322135925293 4.41322135925293
Loss :  1.9827133417129517 4.430140972137451 4.430140972137451
Loss :  1.9241973161697388 4.386570930480957 4.386570930480957
Loss :  1.9671980142593384 4.5758585929870605 4.5758585929870605
Loss :  1.944383144378662 4.401461124420166 4.401461124420166
Loss :  1.8927041292190552 4.368651866912842 4.368651866912842
Loss :  1.9416463375091553 4.541364669799805 4.541364669799805
Loss :  1.9363576173782349 4.615509510040283 4.615509510040283
Loss :  1.984384298324585 4.624859809875488 4.624859809875488
Loss :  1.9741761684417725 4.43742561340332 4.43742561340332
Loss :  1.9815350770950317 4.294103145599365 4.294103145599365
Loss :  1.9525301456451416 4.4805145263671875 4.4805145263671875
Loss :  1.9588284492492676 4.6359663009643555 4.6359663009643555
Loss :  1.8940261602401733 4.350030422210693 4.350030422210693
Loss :  1.9328540563583374 4.45544958114624 4.45544958114624
Loss :  1.9283429384231567 4.474525451660156 4.474525451660156
  batch 20 loss: 1.9283429384231567, 4.474525451660156, 4.474525451660156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.9620682001113892 4.290054798126221 4.290054798126221
Loss :  1.9837079048156738 4.473933219909668 4.473933219909668
Loss :  1.9185956716537476 4.605805397033691 4.605805397033691
Loss :  1.971150517463684 4.439301490783691 4.439301490783691
Loss :  1.8586065769195557 4.510483741760254 4.510483741760254
Loss :  1.9001917839050293 4.326355934143066 4.326355934143066
Loss :  1.912480115890503 4.805044174194336 4.805044174194336
Loss :  1.9086307287216187 4.379594802856445 4.379594802856445
Loss :  1.9673506021499634 4.4632673263549805 4.4632673263549805
Loss :  1.864836573600769 4.431914329528809 4.431914329528809
Loss :  1.9072645902633667 4.431497097015381 4.431497097015381
Loss :  1.886538028717041 4.60403299331665 4.60403299331665
Loss :  1.9325401782989502 4.396479606628418 4.396479606628418
Loss :  1.9030561447143555 4.586825370788574 4.586825370788574
Loss :  1.9129823446273804 4.6899638175964355 4.6899638175964355
Loss :  1.8871344327926636 4.452245712280273 4.452245712280273
Loss :  1.9215633869171143 4.463175296783447 4.463175296783447
Loss :  1.8768517971038818 4.403872966766357 4.403872966766357
Loss :  1.858860731124878 4.410721302032471 4.410721302032471
Loss :  1.840606927871704 4.508631706237793 4.508631706237793
  batch 40 loss: 1.840606927871704, 4.508631706237793, 4.508631706237793
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3, 5], device='cuda:0')
Loss :  1.8721472024917603 4.553954124450684 4.553954124450684
Loss :  1.9580743312835693 4.598200798034668 4.598200798034668
Loss :  1.8989973068237305 4.47857141494751 4.47857141494751
Loss :  1.8841850757598877 4.622125625610352 4.622125625610352
Loss :  1.9055523872375488 4.561065673828125 4.561065673828125
Loss :  1.9120917320251465 4.510120868682861 4.510120868682861
Loss :  1.8520270586013794 4.421971321105957 4.421971321105957
Loss :  1.876617193222046 4.458451271057129 4.458451271057129
Loss :  1.8764338493347168 4.343203067779541 4.343203067779541
Loss :  1.8736658096313477 4.593338489532471 4.593338489532471
Loss :  1.8276546001434326 4.477694511413574 4.477694511413574
Loss :  1.9239917993545532 4.369675636291504 4.369675636291504
Loss :  1.8983356952667236 4.592817306518555 4.592817306518555
Loss :  1.853427529335022 4.3292365074157715 4.3292365074157715
Loss :  1.8638802766799927 4.4309401512146 4.4309401512146
Loss :  1.8708529472351074 4.542881965637207 4.542881965637207
Loss :  1.9008861780166626 4.479990005493164 4.479990005493164
Loss :  1.848979115486145 4.426047325134277 4.426047325134277
Loss :  1.8153108358383179 4.599510192871094 4.599510192871094
Loss :  1.8043663501739502 4.438342094421387 4.438342094421387
  batch 60 loss: 1.8043663501739502, 4.438342094421387, 4.438342094421387
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 5], device='cuda:0')
Loss :  1.8179060220718384 4.4447736740112305 4.4447736740112305
Loss :  1.7965456247329712 4.374814033508301 4.374814033508301
Loss :  1.8797216415405273 4.39930534362793 4.39930534362793
Loss :  1.8244668245315552 4.39152717590332 4.39152717590332
Loss :  1.8202847242355347 4.2842793464660645 4.2842793464660645
Loss :  1.9160411357879639 4.4471025466918945 4.4471025466918945
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2], device='cuda:0')
Loss :  1.9143482446670532 4.465033054351807 4.465033054351807
Loss :  1.918942928314209 4.319007396697998 4.319007396697998
Loss :  1.896593451499939 4.210631847381592 4.210631847381592
Total LOSS train 4.4768515513493465 valid 4.360443711280823
CE LOSS train 1.9054953520114606 valid 0.47414836287498474
Contrastive LOSS train 4.4768515513493465 valid 1.052657961845398
EPOCH 99:
Loss :  1.7969270944595337 4.430566310882568 4.430566310882568
Loss :  1.801749587059021 4.6046977043151855 4.6046977043151855
Loss :  1.7986420392990112 4.050327301025391 4.050327301025391
Loss :  1.8163424730300903 3.909165859222412 3.909165859222412
Loss :  1.7977159023284912 3.9377236366271973 3.9377236366271973
Loss :  1.8072936534881592 4.000329494476318 4.000329494476318
Loss :  1.8339619636535645 4.3238396644592285 4.3238396644592285
Loss :  1.8250672817230225 4.451286792755127 4.451286792755127
Loss :  1.8448772430419922 4.466554641723633 4.466554641723633
Loss :  1.889326810836792 4.383626461029053 4.383626461029053
Loss :  1.840859055519104 4.526558876037598 4.526558876037598
Loss :  1.8616918325424194 4.457420349121094 4.457420349121094
Loss :  1.8751389980316162 4.490827560424805 4.490827560424805
Loss :  1.893567442893982 4.550339698791504 4.550339698791504
Loss :  1.9075372219085693 4.49624490737915 4.49624490737915
Loss :  1.862251877784729 4.45582914352417 4.45582914352417
Loss :  1.893144130706787 4.537973403930664 4.537973403930664
Loss :  1.8861974477767944 4.4457621574401855 4.4457621574401855
Loss :  1.8792839050292969 4.410377025604248 4.410377025604248
Loss :  1.844165563583374 4.4263834953308105 4.4263834953308105
  batch 20 loss: 1.844165563583374, 4.4263834953308105, 4.4263834953308105
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.9623932838439941 4.431480407714844 4.431480407714844
Loss :  1.9152195453643799 4.512079238891602 4.512079238891602
Loss :  1.93795645236969 4.71051025390625 4.71051025390625
Loss :  1.8688373565673828 4.310433387756348 4.310433387756348
Loss :  1.8997139930725098 4.621288776397705 4.621288776397705
Loss :  1.9145103693008423 4.402824401855469 4.402824401855469
Loss :  1.833593487739563 4.524294853210449 4.524294853210449
Loss :  1.9534157514572144 4.387794017791748 4.387794017791748
Loss :  1.9441834688186646 4.480118751525879 4.480118751525879
Loss :  1.9465110301971436 4.449662685394287 4.449662685394287
Loss :  1.9920258522033691 4.432343482971191 4.432343482971191
Loss :  1.9206688404083252 4.319318771362305 4.319318771362305
Loss :  1.9293835163116455 4.196722030639648 4.196722030639648
Loss :  1.9797425270080566 4.431614398956299 4.431614398956299
Loss :  1.9970349073410034 4.30922269821167 4.30922269821167
Loss :  2.0074028968811035 4.216468334197998 4.216468334197998
Loss :  2.0209522247314453 3.885215997695923 3.885215997695923
Loss :  1.9674980640411377 4.358328819274902 4.358328819274902
Loss :  1.9468382596969604 4.076693534851074 4.076693534851074
Loss :  1.966347575187683 4.499661922454834 4.499661922454834
  batch 40 loss: 1.966347575187683, 4.499661922454834, 4.499661922454834
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  2.014045238494873 4.593282699584961 4.593282699584961
Loss :  2.025531768798828 4.517669200897217 4.517669200897217
Loss :  2.1156764030456543 4.494729518890381 4.494729518890381
Loss :  2.04231858253479 4.605221271514893 4.605221271514893
Loss :  2.0815677642822266 4.364933490753174 4.364933490753174
Loss :  2.0154528617858887 4.369000434875488 4.369000434875488
Loss :  1.9204176664352417 4.328839302062988 4.328839302062988
Loss :  2.135390281677246 4.4242939949035645 4.4242939949035645
Loss :  2.0060641765594482 4.245878219604492 4.245878219604492
Loss :  2.136066436767578 4.4634318351745605 4.4634318351745605
Loss :  2.0656113624572754 4.448799133300781 4.448799133300781
Loss :  2.0636672973632812 4.536499977111816 4.536499977111816
Loss :  2.028630495071411 4.515636920928955 4.515636920928955
Loss :  1.9955261945724487 4.485015392303467 4.485015392303467
Loss :  2.10073184967041 4.514966011047363 4.514966011047363
Loss :  1.9342509508132935 4.470688819885254 4.470688819885254
Loss :  1.9909849166870117 4.5906524658203125 4.5906524658203125
Loss :  2.091236114501953 4.607853412628174 4.607853412628174
Loss :  2.1570229530334473 4.388025760650635 4.388025760650635
Loss :  2.0322866439819336 4.420487403869629 4.420487403869629
  batch 60 loss: 2.0322866439819336, 4.420487403869629, 4.420487403869629
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 4, 5], device='cuda:0')
Loss :  2.039897918701172 4.526557922363281 4.526557922363281
Loss :  2.0565366744995117 4.463683605194092 4.463683605194092
Loss :  1.9798583984375 4.5010151863098145 4.5010151863098145
Loss :  2.0596091747283936 4.444577693939209 4.444577693939209
Loss :  2.1188547611236572 4.388820171356201 4.388820171356201
Loss :  2.3613100051879883 4.360297679901123 4.360297679901123
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1], device='cuda:0')
Loss :  2.3358583450317383 4.3658223152160645 4.3658223152160645
Loss :  2.286411762237549 4.393388271331787 4.393388271331787
Loss :  2.11968731880188 4.2679948806762695 4.2679948806762695
Total LOSS train 4.409576478371253 valid 4.346875786781311
CE LOSS train 1.9548801202040451 valid 0.52992182970047
Contrastive LOSS train 4.409576478371253 valid 1.0669987201690674
EPOCH 100:
Loss :  2.05412220954895 4.405905246734619 4.405905246734619
Loss :  2.0619053840637207 4.458045959472656 4.458045959472656
Loss :  2.0412349700927734 4.298896789550781 4.298896789550781
Loss :  2.1355388164520264 4.458390235900879 4.458390235900879
Loss :  2.090771198272705 4.393951892852783 4.393951892852783
Loss :  2.0806586742401123 4.4104413986206055 4.4104413986206055
Loss :  1.916160225868225 4.442550182342529 4.442550182342529
Loss :  2.0685384273529053 4.534153461456299 4.534153461456299
Loss :  2.065272331237793 4.622481822967529 4.622481822967529
Loss :  2.029073715209961 4.319970607757568 4.319970607757568
Loss :  2.0462520122528076 4.483945846557617 4.483945846557617
Loss :  2.1602280139923096 4.537408351898193 4.537408351898193
Loss :  2.0632967948913574 4.483434200286865 4.483434200286865
Loss :  2.140904188156128 4.460036277770996 4.460036277770996
Loss :  1.9838427305221558 4.426319599151611 4.426319599151611
Loss :  2.088531970977783 4.582590103149414 4.582590103149414
Loss :  2.169051170349121 4.425436019897461 4.425436019897461
Loss :  2.0000710487365723 4.388746738433838 4.388746738433838
Loss :  2.042029619216919 4.312239170074463 4.312239170074463
Loss :  2.014233112335205 4.347335338592529 4.347335338592529
  batch 20 loss: 2.014233112335205, 4.347335338592529, 4.347335338592529
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  2.0049803256988525 4.508399963378906 4.508399963378906
Loss :  2.1120121479034424 4.461898326873779 4.461898326873779
Loss :  2.023717164993286 4.541321277618408 4.541321277618408
Loss :  2.073740243911743 4.5085062980651855 4.5085062980651855
Loss :  1.997952938079834 4.624125003814697 4.624125003814697
Loss :  1.924979329109192 4.428243637084961 4.428243637084961
Loss :  1.9754117727279663 4.635256290435791 4.635256290435791
Loss :  1.9366122484207153 4.275234699249268 4.275234699249268
Loss :  2.0478150844573975 4.347963809967041 4.347963809967041
Loss :  2.0466794967651367 4.543970108032227 4.543970108032227
Loss :  2.0548105239868164 4.451716423034668 4.451716423034668
Loss :  1.9076194763183594 4.689950942993164 4.689950942993164
Loss :  2.1018171310424805 4.5724897384643555 4.5724897384643555
Loss :  2.030144453048706 4.418969631195068 4.418969631195068
Loss :  2.0951247215270996 4.581582546234131 4.581582546234131
Loss :  2.0154590606689453 4.56093692779541 4.56093692779541
Loss :  1.9977132081985474 4.3910231590271 4.3910231590271
Loss :  1.8603410720825195 4.603963851928711 4.603963851928711
Loss :  1.930228590965271 4.402723789215088 4.402723789215088
Loss :  1.883769154548645 4.465388298034668 4.465388298034668
  batch 40 loss: 1.883769154548645, 4.465388298034668, 4.465388298034668
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2], device='cuda:0')
Loss :  1.9295734167099 4.5315327644348145 4.5315327644348145
Loss :  2.10359263420105 4.505306720733643 4.505306720733643
Loss :  2.0976617336273193 4.319427013397217 4.319427013397217
Loss :  1.91533625125885 4.485722064971924 4.485722064971924
Loss :  2.024092435836792 4.242316246032715 4.242316246032715
Loss :  2.02955961227417 4.42241096496582 4.42241096496582
Loss :  1.860896110534668 4.7185540199279785 4.7185540199279785
Loss :  2.0128066539764404 4.510699272155762 4.510699272155762
Loss :  1.9430955648422241 4.436758041381836 4.436758041381836
Loss :  1.9325010776519775 4.28870964050293 4.28870964050293
Loss :  2.0340096950531006 4.525717735290527 4.525717735290527
Loss :  2.080228805541992 4.6059417724609375 4.6059417724609375
Loss :  2.0560176372528076 4.508433818817139 4.508433818817139
Loss :  1.943472981452942 4.4798054695129395 4.4798054695129395
Loss :  2.0363566875457764 4.5530524253845215 4.5530524253845215
Loss :  1.9053826332092285 4.452480792999268 4.452480792999268
Loss :  2.131223201751709 4.588439464569092 4.588439464569092
Loss :  1.9967678785324097 4.459410667419434 4.459410667419434
Loss :  1.9572392702102661 4.573908805847168 4.573908805847168
Loss :  1.8049232959747314 4.405089378356934 4.405089378356934
  batch 60 loss: 1.8049232959747314, 4.405089378356934, 4.405089378356934
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
Loss :  2.018261432647705 4.489175796508789 4.489175796508789
Loss :  1.8725342750549316 4.483240604400635 4.483240604400635
Loss :  1.9177789688110352 4.29998254776001 4.29998254776001
Loss :  2.0189287662506104 4.15838623046875 4.15838623046875
Loss :  1.9627114534378052 4.027773857116699 4.027773857116699
Loss :  4.137387752532959 4.3381218910217285 4.3381218910217285
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 3], device='cuda:0')
Loss :  4.009017467498779 4.389078140258789 4.389078140258789
Loss :  4.037111759185791 4.205685615539551 4.205685615539551
Loss :  3.945432186126709 4.156371116638184 4.156371116638184
Total LOSS train 4.459664924328144 valid 4.272314190864563
CE LOSS train 2.0131938035671526 valid 0.9863580465316772
Contrastive LOSS train 4.459664924328144 valid 1.039092779159546
EPOCH 101:
Loss :  1.962695598602295 3.9328415393829346 3.9328415393829346
Loss :  1.8690706491470337 4.174288749694824 4.174288749694824
Loss :  1.934342622756958 4.078526973724365 4.078526973724365
Loss :  2.000796318054199 4.0253825187683105 4.0253825187683105
Loss :  1.9064325094223022 4.101259231567383 4.101259231567383
Loss :  1.9239581823349 3.951488971710205 3.951488971710205
Loss :  1.91303551197052 4.039609432220459 4.039609432220459
Loss :  1.9394010305404663 3.872972249984741 3.872972249984741
Loss :  1.982751488685608 3.8817527294158936 3.8817527294158936
Loss :  1.9352869987487793 3.806175708770752 3.806175708770752
Loss :  1.962825894355774 4.105954647064209 4.105954647064209
Loss :  1.9588875770568848 4.207942008972168 4.207942008972168
Loss :  1.9686388969421387 4.208733081817627 4.208733081817627
Loss :  1.9762262105941772 4.121658802032471 4.121658802032471
Loss :  1.9439640045166016 4.089344024658203 4.089344024658203
Loss :  1.9182029962539673 4.234470367431641 4.234470367431641
Loss :  1.9600763320922852 4.077304840087891 4.077304840087891
Loss :  1.9259713888168335 4.106393337249756 4.106393337249756
Loss :  2.035609245300293 3.9683682918548584 3.9683682918548584
Loss :  1.9339232444763184 4.108044147491455 4.108044147491455
  batch 20 loss: 1.9339232444763184, 4.108044147491455, 4.108044147491455
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
Loss :  1.990404486656189 4.067065238952637 4.067065238952637
Loss :  1.983806848526001 4.072068691253662 4.072068691253662
Loss :  1.95980703830719 4.320462703704834 4.320462703704834
Loss :  1.9525648355484009 3.922261953353882 3.922261953353882
Loss :  1.9700403213500977 4.122664928436279 4.122664928436279
Loss :  1.9515180587768555 3.9263815879821777 3.9263815879821777
Loss :  1.907133936882019 4.101590156555176 4.101590156555176
Loss :  1.9446122646331787 3.8454947471618652 3.8454947471618652
Loss :  1.9832912683486938 4.080355167388916 4.080355167388916
Loss :  1.9561489820480347 4.071210861206055 4.071210861206055
Loss :  2.0253241062164307 4.246424674987793 4.246424674987793
Loss :  1.9446051120758057 4.194645404815674 4.194645404815674
Loss :  1.9774012565612793 4.047539234161377 4.047539234161377
Loss :  2.008424997329712 4.199223518371582 4.199223518371582
Loss :  2.0345420837402344 3.9296977519989014 3.9296977519989014
Loss :  2.0042099952697754 4.086880207061768 4.086880207061768
Loss :  1.9929722547531128 4.009118556976318 4.009118556976318
Loss :  1.9153518676757812 4.028217792510986 4.028217792510986
Loss :  1.9318135976791382 3.9237356185913086 3.9237356185913086
Loss :  1.9050109386444092 3.8694961071014404 3.8694961071014404
  batch 40 loss: 1.9050109386444092, 3.8694961071014404, 3.8694961071014404
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.922987461090088 4.091403007507324 4.091403007507324
Loss :  2.001523971557617 3.789482593536377 3.789482593536377
Loss :  2.0405185222625732 4.001267433166504 4.001267433166504
Loss :  1.9672887325286865 4.121822357177734 4.121822357177734
Loss :  2.022554397583008 3.9143805503845215 3.9143805503845215
Loss :  1.930930256843567 3.9547507762908936 3.9547507762908936
Loss :  1.875629186630249 4.155167579650879 4.155167579650879
Loss :  2.0360989570617676 3.87336802482605 3.87336802482605
Loss :  1.854885220527649 3.8014156818389893 3.8014156818389893
Loss :  2.0375795364379883 4.021261215209961 4.021261215209961
Loss :  1.99197518825531 3.8904223442077637 3.8904223442077637
Loss :  1.9368435144424438 3.8591153621673584 3.8591153621673584
Loss :  1.9198914766311646 4.113816738128662 4.113816738128662
Loss :  2.0099904537200928 3.841294527053833 3.841294527053833
Loss :  2.0320119857788086 4.138530731201172 4.138530731201172
Loss :  1.8392144441604614 3.90933895111084 3.90933895111084
Loss :  1.9242600202560425 4.105811595916748 4.105811595916748
Loss :  2.042569875717163 3.8309872150421143 3.8309872150421143
Loss :  1.9419008493423462 3.9918737411499023 3.9918737411499023
Loss :  1.8535921573638916 3.7892239093780518 3.7892239093780518
  batch 60 loss: 1.8535921573638916, 3.7892239093780518, 3.7892239093780518
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.9476022720336914 3.7006759643554688 3.7006759643554688
Loss :  1.9414349794387817 3.955397844314575 3.955397844314575
Loss :  1.9046767950057983 3.9528472423553467 3.9528472423553467
Loss :  1.9870693683624268 3.7501258850097656 3.7501258850097656
Loss :  2.034630298614502 3.500678777694702 3.500678777694702
Loss :  7.815012454986572 4.395009517669678 4.395009517669678
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3, 4], device='cuda:0')
Loss :  7.509702682495117 4.411867141723633 4.411867141723633
Loss :  7.499216079711914 4.364351749420166 4.364351749420166
Loss :  8.052382469177246 4.16928768157959 4.16928768157959
Total LOSS train 4.003253947771513 valid 4.335129022598267
CE LOSS train 1.9582883211282582 valid 2.0130956172943115
Contrastive LOSS train 4.003253947771513 valid 1.0423219203948975
EPOCH 102:
Loss :  1.97642982006073 3.5981223583221436 3.5981223583221436
Loss :  1.8635108470916748 4.04572868347168 4.04572868347168
Loss :  1.960768461227417 3.6152431964874268 3.6152431964874268
Loss :  1.974833369255066 3.6444714069366455 3.6444714069366455
Loss :  1.9188560247421265 3.902831554412842 3.902831554412842
Loss :  1.9377375841140747 4.046235084533691 4.046235084533691
Loss :  1.9080182313919067 3.745168924331665 3.745168924331665
Loss :  1.9137810468673706 3.9334635734558105 3.9334635734558105
Loss :  1.9794670343399048 3.606034994125366 3.606034994125366
Loss :  1.95222806930542 3.429898738861084 3.429898738861084
Loss :  1.9546180963516235 3.834775686264038 3.834775686264038
Loss :  1.924091100692749 3.9428229331970215 3.9428229331970215
Loss :  1.957451343536377 3.9639663696289062 3.9639663696289062
Loss :  1.949537754058838 3.7916722297668457 3.7916722297668457
Loss :  1.9234654903411865 3.6978542804718018 3.6978542804718018
Loss :  1.9311964511871338 3.705348253250122 3.705348253250122
Loss :  1.961516261100769 3.8762409687042236 3.8762409687042236
Loss :  1.9446576833724976 3.9051005840301514 3.9051005840301514
Loss :  2.00433349609375 4.0327229499816895 4.0327229499816895
Loss :  1.948377013206482 4.053897380828857 4.053897380828857
  batch 20 loss: 1.948377013206482, 4.053897380828857, 4.053897380828857
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.9572439193725586 3.99637770652771 3.99637770652771
Loss :  1.9528578519821167 4.405088901519775 4.405088901519775
Loss :  1.9339240789413452 4.204611778259277 4.204611778259277
Loss :  1.9081932306289673 4.013036251068115 4.013036251068115
Loss :  1.949117660522461 4.112819671630859 4.112819671630859
Loss :  1.9438339471817017 4.049190998077393 4.049190998077393
Loss :  1.9214191436767578 4.03261661529541 4.03261661529541
Loss :  1.9662538766860962 3.8265886306762695 3.8265886306762695
Loss :  1.9745259284973145 3.6951067447662354 3.6951067447662354
Loss :  1.9328688383102417 3.8989715576171875 3.8989715576171875
Loss :  1.9865955114364624 4.0095319747924805 4.0095319747924805
Loss :  1.964452862739563 3.8834779262542725 3.8834779262542725
Loss :  1.9581100940704346 3.8854548931121826 3.8854548931121826
Loss :  1.9993371963500977 3.798642635345459 3.798642635345459
Loss :  2.0122148990631104 3.8096632957458496 3.8096632957458496
Loss :  1.9888287782669067 3.7856318950653076 3.7856318950653076
Loss :  1.981038212776184 3.8064846992492676 3.8064846992492676
Loss :  1.9463589191436768 3.4406778812408447 3.4406778812408447
Loss :  1.9423367977142334 3.78430438041687 3.78430438041687
Loss :  1.9248567819595337 3.939880847930908 3.939880847930908
  batch 40 loss: 1.9248567819595337, 3.939880847930908, 3.939880847930908
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.9362326860427856 3.7460899353027344 3.7460899353027344
Loss :  2.007092237472534 3.4692695140838623 3.4692695140838623
Loss :  2.03128719329834 3.5629734992980957 3.5629734992980957
Loss :  1.9692996740341187 3.884418249130249 3.884418249130249
Loss :  2.0250308513641357 3.8691132068634033 3.8691132068634033
Loss :  1.9380037784576416 3.8637821674346924 3.8637821674346924
Loss :  1.9065561294555664 4.0170087814331055 4.0170087814331055
Loss :  2.0323100090026855 3.751535177230835 3.751535177230835
Loss :  1.8882935047149658 4.1513261795043945 4.1513261795043945
Loss :  2.025667667388916 4.11249303817749 4.11249303817749
Loss :  1.993122935295105 3.961987018585205 3.961987018585205
Loss :  1.9639496803283691 3.8357958793640137 3.8357958793640137
Loss :  1.924618124961853 3.8812835216522217 3.8812835216522217
Loss :  2.023249626159668 3.6825942993164062 3.6825942993164062
Loss :  2.002411365509033 3.8328864574432373 3.8328864574432373
Loss :  1.8735378980636597 3.8401310443878174 3.8401310443878174
Loss :  1.9401565790176392 3.6341586112976074 3.6341586112976074
Loss :  2.009098768234253 3.731369972229004 3.731369972229004
Loss :  1.9615740776062012 4.012916564941406 4.012916564941406
Loss :  1.8850584030151367 3.9146955013275146 3.9146955013275146
  batch 60 loss: 1.8850584030151367, 3.9146955013275146, 3.9146955013275146
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.9625918865203857 3.69352388381958 3.69352388381958
Loss :  1.9528698921203613 3.7082901000976562 3.7082901000976562
Loss :  1.908621072769165 3.626669406890869 3.626669406890869
Loss :  2.0003559589385986 3.7692086696624756 3.7692086696624756
Loss :  2.0434353351593018 3.3179147243499756 3.3179147243499756
Loss :  9.584925651550293 4.41644811630249 4.41644811630249
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 4], device='cuda:0')
Loss :  9.167587280273438 4.492650985717773 4.492650985717773
Loss :  9.317243576049805 4.337646961212158 4.337646961212158
Loss :  9.065690994262695 4.253872871398926 4.253872871398926
Total LOSS train 3.8403260744535004 valid 4.375154733657837
CE LOSS train 1.9574410621936504 valid 2.266422748565674
Contrastive LOSS train 3.8403260744535004 valid 1.0634682178497314
EPOCH 103:
Loss :  1.9795536994934082 3.8896424770355225 3.8896424770355225
Loss :  1.9160540103912354 3.9611127376556396 3.9611127376556396
Loss :  1.9901992082595825 3.742598056793213 3.742598056793213
Loss :  1.9791451692581177 3.477226972579956 3.477226972579956
Loss :  1.957163691520691 3.8470239639282227 3.8470239639282227
Loss :  1.9598435163497925 4.046622276306152 4.046622276306152
Loss :  1.962835431098938 3.95070481300354 3.95070481300354
Loss :  1.9581023454666138 3.835010290145874 3.835010290145874
Loss :  1.9898693561553955 3.8688924312591553 3.8688924312591553
Loss :  2.0031003952026367 3.3968794345855713 3.3968794345855713
Loss :  1.9930592775344849 3.7025887966156006 3.7025887966156006
Loss :  1.958203911781311 4.023159027099609 4.023159027099609
Loss :  1.9930644035339355 3.9555909633636475 3.9555909633636475
Loss :  1.9652456045150757 3.988926649093628 3.988926649093628
Loss :  1.945979118347168 3.948780059814453 3.948780059814453
Loss :  1.9872850179672241 3.8358047008514404 3.8358047008514404
Loss :  1.8000043630599976 4.235337257385254 4.235337257385254
Loss :  1.7487072944641113 4.325750350952148 4.325750350952148
Loss :  1.9434289932250977 4.570762634277344 4.570762634277344
Loss :  1.8840125799179077 4.707818508148193 4.707818508148193
  batch 20 loss: 1.8840125799179077, 4.707818508148193, 4.707818508148193
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.9224259853363037 4.456782817840576 4.456782817840576
Loss :  1.8938045501708984 4.435274600982666 4.435274600982666
Loss :  2.0099992752075195 4.43316650390625 4.43316650390625
Loss :  1.871241807937622 4.537772178649902 4.537772178649902
Loss :  1.899533748626709 4.73868989944458 4.73868989944458
Loss :  1.83689546585083 4.4488139152526855 4.4488139152526855
Loss :  1.9341473579406738 4.6833415031433105 4.6833415031433105
Loss :  1.9033325910568237 4.515208721160889 4.515208721160889
Loss :  1.9508349895477295 4.317041873931885 4.317041873931885
Loss :  2.069392681121826 4.3289594650268555 4.3289594650268555
Loss :  1.9954137802124023 4.5485758781433105 4.5485758781433105
Loss :  1.8206923007965088 4.577868461608887 4.577868461608887
Loss :  1.939829707145691 4.524401664733887 4.524401664733887
Loss :  1.9419254064559937 4.445231914520264 4.445231914520264
Loss :  1.927829384803772 4.539579391479492 4.539579391479492
Loss :  1.987442970275879 4.787883281707764 4.787883281707764
Loss :  1.9431639909744263 4.496147155761719 4.496147155761719
Loss :  1.8668298721313477 4.452274799346924 4.452274799346924
Loss :  1.866411566734314 4.613752365112305 4.613752365112305
Loss :  1.8479857444763184 4.489754676818848 4.489754676818848
  batch 40 loss: 1.8479857444763184, 4.489754676818848, 4.489754676818848
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.9716371297836304 4.461365699768066 4.461365699768066
Loss :  1.9914146661758423 4.511923789978027 4.511923789978027
Loss :  1.9775333404541016 4.417293071746826 4.417293071746826
Loss :  1.977603554725647 4.552096366882324 4.552096366882324
Loss :  1.9927455186843872 4.54321813583374 4.54321813583374
Loss :  2.0211029052734375 4.38860559463501 4.38860559463501
Loss :  1.9045302867889404 4.2378830909729 4.2378830909729
Loss :  1.989056944847107 4.203492641448975 4.203492641448975
Loss :  1.9157675504684448 4.481595993041992 4.481595993041992
Loss :  1.9778828620910645 4.463296413421631 4.463296413421631
Loss :  1.9520301818847656 4.507920265197754 4.507920265197754
Loss :  1.900287389755249 4.393497943878174 4.393497943878174
Loss :  1.8408782482147217 4.591475963592529 4.591475963592529
Loss :  1.89458429813385 4.501650810241699 4.501650810241699
Loss :  1.9953731298446655 4.243417263031006 4.243417263031006
Loss :  1.871419906616211 4.478704452514648 4.478704452514648
Loss :  1.897856593132019 4.617578029632568 4.617578029632568
Loss :  1.9048432111740112 4.536840915679932 4.536840915679932
Loss :  1.7997316122055054 4.570261478424072 4.570261478424072
Loss :  1.8197286128997803 4.378614902496338 4.378614902496338
  batch 60 loss: 1.8197286128997803, 4.378614902496338, 4.378614902496338
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8446913957595825 4.428922653198242 4.428922653198242
Loss :  1.9274426698684692 4.36466646194458 4.36466646194458
Loss :  1.8776381015777588 4.579930305480957 4.579930305480957
Loss :  1.9035450220108032 4.482203006744385 4.482203006744385
Loss :  2.0085859298706055 4.201408863067627 4.201408863067627
Loss :  4.855510711669922 4.433238506317139 4.433238506317139
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 4], device='cuda:0')
Loss :  4.736180782318115 4.4689435958862305 4.4689435958862305
Loss :  4.724775314331055 4.231159210205078 4.231159210205078
Loss :  4.572317600250244 4.322028636932373 4.322028636932373
Total LOSS train 4.3202864243434025 valid 4.363842487335205
CE LOSS train 1.9307984865628756 valid 1.143079400062561
Contrastive LOSS train 4.3202864243434025 valid 1.0805071592330933
EPOCH 104:
Loss :  1.9070073366165161 4.564816474914551 4.564816474914551
Loss :  1.8789695501327515 4.610079288482666 4.610079288482666
Loss :  1.862578272819519 4.414340019226074 4.414340019226074
Loss :  1.8926407098770142 4.6773529052734375 4.6773529052734375
Loss :  1.780871868133545 4.206097602844238 4.206097602844238
Loss :  1.996867060661316 4.413919448852539 4.413919448852539
Loss :  1.8838274478912354 4.39307975769043 4.39307975769043
Loss :  1.7989494800567627 4.48723840713501 4.48723840713501
Loss :  2.007143020629883 4.388792991638184 4.388792991638184
Loss :  2.052905321121216 4.255268096923828 4.255268096923828
Loss :  1.9702852964401245 4.5577826499938965 4.5577826499938965
Loss :  1.9237648248672485 4.399869441986084 4.399869441986084
Loss :  2.021332025527954 4.442967891693115 4.442967891693115
Loss :  2.0056471824645996 4.603271961212158 4.603271961212158
Loss :  1.9291973114013672 4.629004001617432 4.629004001617432
Loss :  1.9787391424179077 4.699850082397461 4.699850082397461
Loss :  2.0424206256866455 4.527985572814941 4.527985572814941
Loss :  1.9515712261199951 4.4585466384887695 4.4585466384887695
Loss :  1.9358105659484863 4.257498741149902 4.257498741149902
Loss :  1.9162226915359497 4.500056266784668 4.500056266784668
  batch 20 loss: 1.9162226915359497, 4.500056266784668, 4.500056266784668
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  2.0209991931915283 4.419267177581787 4.419267177581787
Loss :  1.990786075592041 4.556593894958496 4.556593894958496
Loss :  1.985675573348999 4.5735907554626465 4.5735907554626465
Loss :  1.9349502325057983 4.417428493499756 4.417428493499756
Loss :  1.8792496919631958 4.6460347175598145 4.6460347175598145
Loss :  1.9504196643829346 4.486531734466553 4.486531734466553
Loss :  2.0121939182281494 4.369568347930908 4.369568347930908
Loss :  2.0547034740448 4.427976608276367 4.427976608276367
Loss :  2.015127420425415 4.2800188064575195 4.2800188064575195
Loss :  1.9576210975646973 4.5507378578186035 4.5507378578186035
Loss :  2.002377510070801 4.349941730499268 4.349941730499268
Loss :  1.9634915590286255 4.5503435134887695 4.5503435134887695
Loss :  1.9725364446640015 4.33058500289917 4.33058500289917
Loss :  1.9178625345230103 4.565493106842041 4.565493106842041
Loss :  2.0331995487213135 4.897559642791748 4.897559642791748
Loss :  1.9276520013809204 4.3357157707214355 4.3357157707214355
Loss :  1.9940887689590454 4.466531276702881 4.466531276702881
Loss :  1.9644767045974731 4.349634647369385 4.349634647369385
Loss :  1.889884114265442 4.386934757232666 4.386934757232666
Loss :  1.8940930366516113 4.377394676208496 4.377394676208496
  batch 40 loss: 1.8940930366516113, 4.377394676208496, 4.377394676208496
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.9680581092834473 4.646960258483887 4.646960258483887
Loss :  1.9978989362716675 4.423815727233887 4.423815727233887
Loss :  2.0119411945343018 4.399125576019287 4.399125576019287
Loss :  1.9580752849578857 4.597234725952148 4.597234725952148
Loss :  1.9751495122909546 4.372580528259277 4.372580528259277
Loss :  2.082103967666626 4.439003944396973 4.439003944396973
Loss :  1.928328514099121 4.380008220672607 4.380008220672607
Loss :  1.9574190378189087 4.468687057495117 4.468687057495117
Loss :  1.9713406562805176 4.4812493324279785 4.4812493324279785
Loss :  1.8865705728530884 4.330294609069824 4.330294609069824
Loss :  1.9125542640686035 4.575928211212158 4.575928211212158
Loss :  2.0415873527526855 4.594199180603027 4.594199180603027
Loss :  1.9740675687789917 4.473570346832275 4.473570346832275
Loss :  1.9752933979034424 4.550745010375977 4.550745010375977
Loss :  1.9472472667694092 4.407776355743408 4.407776355743408
Loss :  1.9069502353668213 4.307943820953369 4.307943820953369
Loss :  1.9812217950820923 4.402049541473389 4.402049541473389
Loss :  2.0005640983581543 4.503258228302002 4.503258228302002
Loss :  1.9345024824142456 4.416781425476074 4.416781425476074
Loss :  2.0135908126831055 4.328338623046875 4.328338623046875
  batch 60 loss: 2.0135908126831055, 4.328338623046875, 4.328338623046875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  2.03517746925354 4.475374221801758 4.475374221801758
Loss :  1.925150752067566 4.433601379394531 4.433601379394531
Loss :  1.956114411354065 4.662729263305664 4.662729263305664
Loss :  2.027259349822998 4.49233341217041 4.49233341217041
Loss :  1.8647793531417847 4.039679527282715 4.039679527282715
Loss :  4.640170097351074 4.420827865600586 4.420827865600586
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2], device='cuda:0')
Loss :  4.540643215179443 4.52154016494751 4.52154016494751
Loss :  4.286256313323975 4.370596408843994 4.370596408843994
Loss :  4.578383922576904 4.3923258781433105 4.3923258781433105
Total LOSS train 4.461984142890343 valid 4.42632257938385
CE LOSS train 1.9589397833897517 valid 1.144595980644226
Contrastive LOSS train 4.461984142890343 valid 1.0980814695358276
EPOCH 105:
Loss :  1.837918996810913 4.542632579803467 4.542632579803467
Loss :  1.935064673423767 4.694516658782959 4.694516658782959
Loss :  1.8857297897338867 4.510910511016846 4.510910511016846
Loss :  1.8525111675262451 4.464193344116211 4.464193344116211
Loss :  1.9475117921829224 4.571295738220215 4.571295738220215
Loss :  1.9373618364334106 4.459318161010742 4.459318161010742
Loss :  1.872406005859375 4.676572322845459 4.676572322845459
Loss :  1.9051244258880615 4.346047401428223 4.346047401428223
Loss :  1.9745417833328247 4.422733783721924 4.422733783721924
Loss :  1.8265600204467773 4.2020134925842285 4.2020134925842285
Loss :  1.9933831691741943 4.289819240570068 4.289819240570068
Loss :  1.8154258728027344 4.595517635345459 4.595517635345459
Loss :  1.9364216327667236 4.6719207763671875 4.6719207763671875
Loss :  1.9402329921722412 4.500551223754883 4.500551223754883
Loss :  1.8793203830718994 4.366239547729492 4.366239547729492
Loss :  1.9303885698318481 4.48729944229126 4.48729944229126
Loss :  1.92169189453125 4.476425647735596 4.476425647735596
Loss :  1.907375454902649 4.590203762054443 4.590203762054443
Loss :  1.9807502031326294 4.442287921905518 4.442287921905518
Loss :  1.9212559461593628 4.3869709968566895 4.3869709968566895
  batch 20 loss: 1.9212559461593628, 4.3869709968566895, 4.3869709968566895
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.9736526012420654 4.47334623336792 4.47334623336792
Loss :  1.9636465311050415 4.493515968322754 4.493515968322754
Loss :  1.9523335695266724 4.216638088226318 4.216638088226318
Loss :  1.9317095279693604 3.7731239795684814 3.7731239795684814
Loss :  1.9799376726150513 3.9837043285369873 3.9837043285369873
Loss :  1.9574235677719116 3.8689181804656982 3.8689181804656982
Loss :  1.9071851968765259 4.06786584854126 4.06786584854126
Loss :  1.9217393398284912 3.853839635848999 3.853839635848999
Loss :  1.9746720790863037 3.9037628173828125 3.9037628173828125
Loss :  1.9737708568572998 3.941638946533203 3.941638946533203
Loss :  2.01835560798645 4.033984661102295 4.033984661102295
Loss :  1.9403656721115112 4.001944541931152 4.001944541931152
Loss :  1.9785759449005127 4.034908771514893 4.034908771514893
Loss :  2.0090787410736084 3.941333770751953 3.941333770751953
Loss :  2.007211208343506 4.066254615783691 4.066254615783691
Loss :  1.991388201713562 3.874701976776123 3.874701976776123
Loss :  1.9633151292800903 3.9523627758026123 3.9523627758026123
Loss :  1.878683090209961 3.8132448196411133 3.8132448196411133
Loss :  1.9144195318222046 3.7620797157287598 3.7620797157287598
Loss :  1.8804582357406616 3.6354715824127197 3.6354715824127197
  batch 40 loss: 1.8804582357406616, 3.6354715824127197, 3.6354715824127197
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.9005002975463867 3.775059223175049 3.775059223175049
Loss :  1.9695096015930176 3.9407660961151123 3.9407660961151123
Loss :  2.0033135414123535 3.6419029235839844 3.6419029235839844
Loss :  1.9499180316925049 3.914534568786621 3.914534568786621
Loss :  1.9798921346664429 4.013900279998779 4.013900279998779
Loss :  1.9015356302261353 3.860534906387329 3.860534906387329
Loss :  1.850924015045166 3.911593437194824 3.911593437194824
Loss :  1.982693076133728 4.0309367179870605 4.0309367179870605
Loss :  1.8428632020950317 3.844848155975342 3.844848155975342
Loss :  1.99403715133667 3.9881961345672607 3.9881961345672607
Loss :  1.9647947549819946 3.70853853225708 3.70853853225708
Loss :  1.9117172956466675 3.7422804832458496 3.7422804832458496
Loss :  1.8922791481018066 3.883680820465088 3.883680820465088
Loss :  1.9790897369384766 3.954268217086792 3.954268217086792
Loss :  2.0152063369750977 3.9429197311401367 3.9429197311401367
Loss :  1.8564754724502563 3.7650997638702393 3.7650997638702393
Loss :  1.9170827865600586 3.751075506210327 3.751075506210327
Loss :  1.9988678693771362 3.7776095867156982 3.7776095867156982
Loss :  1.9394031763076782 3.9740493297576904 3.9740493297576904
Loss :  1.8547229766845703 3.9692277908325195 3.9692277908325195
  batch 60 loss: 1.8547229766845703, 3.9692277908325195, 3.9692277908325195
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.9002249240875244 3.6193947792053223 3.6193947792053223
Loss :  1.8908882141113281 3.7078871726989746 3.7078871726989746
Loss :  1.8773257732391357 3.7087175846099854 3.7087175846099854
Loss :  1.917865514755249 3.7755143642425537 3.7755143642425537
Loss :  1.9674298763275146 3.3469929695129395 3.3469929695129395
Loss :  1.5844727754592896 4.274251937866211 4.274251937866211
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
Loss :  1.594581127166748 4.288694858551025 4.288694858551025
Loss :  1.5754859447479248 4.196998596191406 4.196998596191406
Loss :  1.541458010673523 3.924267292022705 3.924267292022705
Total LOSS train 4.07599446956928 valid 4.171053171157837
CE LOSS train 1.931960914685176 valid 0.38536450266838074
Contrastive LOSS train 4.07599446956928 valid 0.9810668230056763
EPOCH 106:
Loss :  1.9065097570419312 3.6750247478485107 3.6750247478485107
Loss :  1.813071846961975 3.8212225437164307 3.8212225437164307
Loss :  1.9054781198501587 3.4911258220672607 3.4911258220672607
Loss :  1.9509941339492798 3.4419567584991455 3.4419567584991455
Loss :  1.8731474876403809 3.688692331314087 3.688692331314087
Loss :  1.8835200071334839 3.572551727294922 3.572551727294922
Loss :  1.8455928564071655 3.9733662605285645 3.9733662605285645
Loss :  1.8333630561828613 3.972151041030884 3.972151041030884
Loss :  1.9139620065689087 3.677107334136963 3.677107334136963
Loss :  1.8664445877075195 3.663905620574951 3.663905620574951
Loss :  1.8684086799621582 3.658996343612671 3.658996343612671
Loss :  1.90522038936615 3.8608298301696777 3.8608298301696777
Loss :  1.8936527967453003 3.989790201187134 3.989790201187134
Loss :  1.8823927640914917 3.745229482650757 3.745229482650757
Loss :  1.901100516319275 3.9569551944732666 3.9569551944732666
Loss :  1.865100383758545 3.6470398902893066 3.6470398902893066
Loss :  1.879512071609497 3.8245511054992676 3.8245511054992676
Loss :  1.8622103929519653 3.5720794200897217 3.5720794200897217
Loss :  1.9611736536026 3.3906238079071045 3.3906238079071045
Loss :  1.8335516452789307 3.193812608718872 3.193812608718872
  batch 20 loss: 1.8335516452789307, 3.193812608718872, 3.193812608718872
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.91006338596344 3.9089040756225586 3.9089040756225586
Loss :  1.9085350036621094 3.6314475536346436 3.6314475536346436
Loss :  1.8731043338775635 3.6652657985687256 3.6652657985687256
Loss :  1.8873039484024048 3.7623469829559326 3.7623469829559326
Loss :  1.8872755765914917 4.082108020782471 4.082108020782471
Loss :  1.8599785566329956 3.7687571048736572 3.7687571048736572
Loss :  1.8280727863311768 4.147363662719727 4.147363662719727
Loss :  1.84321928024292 3.9796762466430664 3.9796762466430664
Loss :  1.851974606513977 3.8345627784729004 3.8345627784729004
Loss :  1.8702523708343506 3.83687686920166 3.83687686920166
Loss :  1.9335196018218994 3.854383945465088 3.854383945465088
Loss :  1.8293954133987427 3.5962648391723633 3.5962648391723633
Loss :  1.8897207975387573 3.5814104080200195 3.5814104080200195
Loss :  1.8725671768188477 3.661487340927124 3.661487340927124
Loss :  1.919649600982666 3.489630699157715 3.489630699157715
Loss :  1.883865475654602 3.7555177211761475 3.7555177211761475
Loss :  1.8993778228759766 3.787362575531006 3.787362575531006
Loss :  1.8053395748138428 3.5096864700317383 3.5096864700317383
Loss :  1.85993492603302 3.3389947414398193 3.3389947414398193
Loss :  1.8464515209197998 3.7881836891174316 3.7881836891174316
  batch 40 loss: 1.8464515209197998, 3.7881836891174316, 3.7881836891174316
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8497214317321777 3.641244411468506 3.641244411468506
Loss :  1.9362717866897583 3.537893056869507 3.537893056869507
Loss :  1.9684942960739136 3.4955172538757324 3.4955172538757324
Loss :  1.9316096305847168 3.8026018142700195 3.8026018142700195
Loss :  1.9306087493896484 3.597303867340088 3.597303867340088
Loss :  1.851972222328186 3.8734524250030518 3.8734524250030518
Loss :  1.8262978792190552 3.775940179824829 3.775940179824829
Loss :  1.933896541595459 3.6841583251953125 3.6841583251953125
Loss :  1.7949044704437256 3.6517300605773926 3.6517300605773926
Loss :  1.9587944746017456 3.743729591369629 3.743729591369629
Loss :  1.920320987701416 3.72831654548645 3.72831654548645
Loss :  1.8808120489120483 3.4705772399902344 3.4705772399902344
Loss :  1.8553858995437622 3.7348618507385254 3.7348618507385254
Loss :  1.925638198852539 3.281573534011841 3.281573534011841
Loss :  1.9579710960388184 3.4821391105651855 3.4821391105651855
Loss :  1.8177331686019897 3.7466471195220947 3.7466471195220947
Loss :  1.862887978553772 3.702418565750122 3.702418565750122
Loss :  1.9199882745742798 3.763157367706299 3.763157367706299
Loss :  1.8774968385696411 3.422445058822632 3.422445058822632
Loss :  1.8298882246017456 3.5640995502471924 3.5640995502471924
  batch 60 loss: 1.8298882246017456, 3.5640995502471924, 3.5640995502471924
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8404611349105835 3.538787603378296 3.538787603378296
Loss :  1.840035319328308 3.461210012435913 3.461210012435913
Loss :  1.8629924058914185 3.965273141860962 3.965273141860962
Loss :  1.8504177331924438 3.6526405811309814 3.6526405811309814
Loss :  1.8891164064407349 3.4773950576782227 3.4773950576782227
Loss :  1.5740196704864502 4.312510967254639 4.312510967254639
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
Loss :  1.6182104349136353 4.343701362609863 4.343701362609863
Loss :  1.5662031173706055 4.238898277282715 4.238898277282715
Loss :  1.5594873428344727 4.02683162689209 4.02683162689209
Total LOSS train 3.68603626031142 valid 4.230485558509827
CE LOSS train 1.8802727094063392 valid 0.38987183570861816
Contrastive LOSS train 3.68603626031142 valid 1.0067079067230225
EPOCH 107:
Loss :  1.8564872741699219 3.643869161605835 3.643869161605835
Loss :  1.7816836833953857 4.135855674743652 4.135855674743652
Loss :  1.8632299900054932 4.086525917053223 4.086525917053223
Loss :  1.9136215448379517 3.365648031234741 3.365648031234741
Loss :  1.8555059432983398 3.601672887802124 3.601672887802124
Loss :  1.8741005659103394 3.516003370285034 3.516003370285034
Loss :  1.8234418630599976 3.5423624515533447 3.5423624515533447
Loss :  1.8185348510742188 3.309211015701294 3.309211015701294
Loss :  1.9022237062454224 3.1705398559570312 3.1705398559570312
Loss :  1.837557315826416 3.533651113510132 3.533651113510132
Loss :  1.8611326217651367 3.7828664779663086 3.7828664779663086
Loss :  1.8897218704223633 3.676877021789551 3.676877021789551
Loss :  1.8689303398132324 3.6778922080993652 3.6778922080993652
Loss :  1.8472862243652344 3.5242233276367188 3.5242233276367188
Loss :  1.8631974458694458 3.546428680419922 3.546428680419922
Loss :  1.8075941801071167 3.6580135822296143 3.6580135822296143
Loss :  1.8402314186096191 3.697131633758545 3.697131633758545
Loss :  1.8345760107040405 3.7416038513183594 3.7416038513183594
Loss :  1.9149836301803589 3.304232120513916 3.304232120513916
Loss :  1.7944258451461792 3.4872806072235107 3.4872806072235107
  batch 20 loss: 1.7944258451461792, 3.4872806072235107, 3.4872806072235107
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8830034732818604 3.749117612838745 3.749117612838745
Loss :  1.882429599761963 3.5300800800323486 3.5300800800323486
Loss :  1.846567153930664 3.6081111431121826 3.6081111431121826
Loss :  1.8689428567886353 3.493034839630127 3.493034839630127
Loss :  1.8542417287826538 3.793898105621338 3.793898105621338
Loss :  1.8370338678359985 3.5793709754943848 3.5793709754943848
Loss :  1.8284099102020264 3.7142417430877686 3.7142417430877686
Loss :  1.8453127145767212 3.363828420639038 3.363828420639038
Loss :  1.8476731777191162 3.465291738510132 3.465291738510132
Loss :  1.8310542106628418 3.457875967025757 3.457875967025757
Loss :  1.9113852977752686 3.8183553218841553 3.8183553218841553
Loss :  1.8193343877792358 3.7365503311157227 3.7365503311157227
Loss :  1.8783365488052368 3.6095166206359863 3.6095166206359863
Loss :  1.828878402709961 3.5617103576660156 3.5617103576660156
Loss :  1.8795491456985474 3.27972674369812 3.27972674369812
Loss :  1.8530704975128174 3.543135643005371 3.543135643005371
Loss :  1.8624416589736938 3.7232301235198975 3.7232301235198975
Loss :  1.7886362075805664 3.3912436962127686 3.3912436962127686
Loss :  1.8167901039123535 3.725292921066284 3.725292921066284
Loss :  1.813050627708435 3.31225848197937 3.31225848197937
  batch 40 loss: 1.813050627708435, 3.31225848197937, 3.31225848197937
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.815490484237671 3.5229861736297607 3.5229861736297607
Loss :  1.8682781457901 3.2642405033111572 3.2642405033111572
Loss :  1.867404580116272 3.4299018383026123 3.4299018383026123
Loss :  1.8648340702056885 3.7550606727600098 3.7550606727600098
Loss :  1.8626253604888916 3.6378941535949707 3.6378941535949707
Loss :  1.8120938539505005 3.4412050247192383 3.4412050247192383
Loss :  1.8111425638198853 3.368314027786255 3.368314027786255
Loss :  1.870641827583313 3.590024709701538 3.590024709701538
Loss :  1.7890222072601318 3.4066152572631836 3.4066152572631836
Loss :  1.890672206878662 3.4821088314056396 3.4821088314056396
Loss :  1.8516616821289062 3.6583266258239746 3.6583266258239746
Loss :  1.8417603969573975 3.4664602279663086 3.4664602279663086
Loss :  1.832519769668579 3.630800485610962 3.630800485610962
Loss :  1.8681573867797852 3.5490925312042236 3.5490925312042236
Loss :  1.8911148309707642 3.594059467315674 3.594059467315674
Loss :  1.8054168224334717 3.2857754230499268 3.2857754230499268
Loss :  1.8407565355300903 3.6576685905456543 3.6576685905456543
Loss :  1.8872579336166382 3.3817710876464844 3.3817710876464844
Loss :  1.8453962802886963 3.795199394226074 3.795199394226074
Loss :  1.8036131858825684 3.4155313968658447 3.4155313968658447
  batch 60 loss: 1.8036131858825684, 3.4155313968658447, 3.4155313968658447
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8357570171356201 3.573730945587158 3.573730945587158
Loss :  1.8183170557022095 3.508951425552368 3.508951425552368
Loss :  1.8516340255737305 3.363921880722046 3.363921880722046
Loss :  1.8304083347320557 3.5408060550689697 3.5408060550689697
Loss :  1.8621139526367188 3.425962448120117 3.425962448120117
Loss :  4.537082672119141 4.335391044616699 4.335391044616699
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  4.527148246765137 4.388436794281006 4.388436794281006
Loss :  4.390041828155518 4.438342094421387 4.438342094421387
Loss :  4.588690757751465 4.356238842010498 4.356238842010498
Total LOSS train 3.556987215922429 valid 4.3796021938323975
CE LOSS train 1.8483492062642024 valid 1.1471726894378662
Contrastive LOSS train 3.556987215922429 valid 1.0890597105026245
EPOCH 108:
Loss :  1.8293696641921997 3.1764822006225586 3.1764822006225586
Loss :  1.786434292793274 3.6737442016601562 3.6737442016601562
Loss :  1.8378758430480957 3.4185914993286133 3.4185914993286133
Loss :  1.8686555624008179 3.3284857273101807 3.3284857273101807
Loss :  1.8415234088897705 3.5672366619110107 3.5672366619110107
Loss :  1.8614495992660522 3.5987584590911865 3.5987584590911865
Loss :  1.8127119541168213 3.581655740737915 3.581655740737915
Loss :  1.8117997646331787 3.3513143062591553 3.3513143062591553
Loss :  1.8694303035736084 3.351907968521118 3.351907968521118
Loss :  1.8157254457473755 3.003098487854004 3.003098487854004
Loss :  1.8475321531295776 3.6425116062164307 3.6425116062164307
Loss :  1.860210657119751 3.536792039871216 3.536792039871216
Loss :  1.8586375713348389 3.481956958770752 3.481956958770752
Loss :  1.8340920209884644 3.5686140060424805 3.5686140060424805
Loss :  1.8417617082595825 3.4688193798065186 3.4688193798065186
Loss :  1.7941579818725586 3.4610085487365723 3.4610085487365723
Loss :  1.8218295574188232 3.7599244117736816 3.7599244117736816
Loss :  1.8294423818588257 3.5418691635131836 3.5418691635131836
Loss :  1.8803671598434448 3.3137853145599365 3.3137853145599365
Loss :  1.7889679670333862 3.486684560775757 3.486684560775757
  batch 20 loss: 1.7889679670333862, 3.486684560775757, 3.486684560775757
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8750461339950562 3.3862991333007812 3.3862991333007812
Loss :  1.869741439819336 3.5314483642578125 3.5314483642578125
Loss :  1.836545467376709 3.4107022285461426 3.4107022285461426
Loss :  1.8709200620651245 3.641219139099121 3.641219139099121
Loss :  1.8440773487091064 3.6925089359283447 3.6925089359283447
Loss :  1.8482588529586792 3.6313276290893555 3.6313276290893555
Loss :  1.8459725379943848 3.4467244148254395 3.4467244148254395
Loss :  1.8511571884155273 3.3997650146484375 3.3997650146484375
Loss :  1.8532590866088867 3.368901252746582 3.368901252746582
Loss :  1.824080467224121 3.3795669078826904 3.3795669078826904
Loss :  1.886704444885254 3.6155710220336914 3.6155710220336914
Loss :  1.824916124343872 3.5301640033721924 3.5301640033721924
Loss :  1.8859912157058716 3.468710422515869 3.468710422515869
Loss :  1.828381896018982 3.638904571533203 3.638904571533203
Loss :  1.8757245540618896 3.3918516635894775 3.3918516635894775
Loss :  1.858551263809204 3.388472557067871 3.388472557067871
Loss :  1.8827298879623413 3.3838701248168945 3.3838701248168945
Loss :  1.8378300666809082 3.335283041000366 3.335283041000366
Loss :  1.8548860549926758 3.3202133178710938 3.3202133178710938
Loss :  1.847251296043396 3.65576171875 3.65576171875
  batch 40 loss: 1.847251296043396, 3.65576171875, 3.65576171875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8499826192855835 3.452897548675537 3.452897548675537
Loss :  1.897336483001709 3.1712872982025146 3.1712872982025146
Loss :  1.8886240720748901 3.3552279472351074 3.3552279472351074
Loss :  1.873472809791565 3.395423173904419 3.395423173904419
Loss :  1.8725974559783936 3.274005651473999 3.274005651473999
Loss :  1.8369466066360474 3.417433500289917 3.417433500289917
Loss :  1.8221222162246704 3.730323314666748 3.730323314666748
Loss :  1.8556787967681885 3.251617193222046 3.251617193222046
Loss :  1.8079298734664917 4.053582191467285 4.053582191467285
Loss :  1.877488136291504 3.5304410457611084 3.5304410457611084
Loss :  1.8396921157836914 3.4661898612976074 3.4661898612976074
Loss :  1.8428244590759277 3.6201605796813965 3.6201605796813965
Loss :  1.8422727584838867 3.834866523742676 3.834866523742676
Loss :  1.8660427331924438 3.3177614212036133 3.3177614212036133
Loss :  1.880324125289917 3.617973566055298 3.617973566055298
Loss :  1.8528019189834595 3.5430047512054443 3.5430047512054443
Loss :  1.8603789806365967 3.4824395179748535 3.4824395179748535
Loss :  1.8927727937698364 3.3445980548858643 3.3445980548858643
Loss :  1.8686578273773193 3.616731882095337 3.616731882095337
Loss :  1.8539999723434448 3.7016823291778564 3.7016823291778564
  batch 60 loss: 1.8539999723434448, 3.7016823291778564, 3.7016823291778564
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8659101724624634 3.3457531929016113 3.3457531929016113
Loss :  1.8373562097549438 3.2730016708374023 3.2730016708374023
Loss :  1.8715121746063232 3.147132396697998 3.147132396697998
Loss :  1.8441087007522583 3.3457272052764893 3.3457272052764893
Loss :  1.8675016164779663 3.2383599281311035 3.2383599281311035
Loss :  6.701319694519043 4.4009928703308105 4.4009928703308105
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2], device='cuda:0')
Loss :  6.549540996551514 4.374100685119629 4.374100685119629
Loss :  6.526981353759766 4.335517883300781 4.335517883300781
Loss :  6.852330684661865 4.224732875823975 4.224732875823975
Total LOSS train 3.468586591573862 valid 4.333836078643799
CE LOSS train 1.850189784856943 valid 1.7130826711654663
Contrastive LOSS train 3.468586591573862 valid 1.0561832189559937
EPOCH 109:
Loss :  1.8287769556045532 3.456352949142456 3.456352949142456
Loss :  1.8156431913375854 3.448560953140259 3.448560953140259
Loss :  1.8548163175582886 3.3691976070404053 3.3691976070404053
Loss :  1.8710968494415283 3.475165605545044 3.475165605545044
Loss :  1.8535213470458984 3.6668577194213867 3.6668577194213867
Loss :  1.8694627285003662 3.6582281589508057 3.6582281589508057
Loss :  1.8344305753707886 3.566707134246826 3.566707134246826
Loss :  1.834787368774414 3.679736375808716 3.679736375808716
Loss :  1.893716812133789 3.564878225326538 3.564878225326538
Loss :  1.8668447732925415 3.52679443359375 3.52679443359375
Loss :  1.8981807231903076 3.5584373474121094 3.5584373474121094
Loss :  1.8802306652069092 3.6186273097991943 3.6186273097991943
Loss :  1.9153327941894531 3.8054521083831787 3.8054521083831787
Loss :  1.880269169807434 3.394474506378174 3.394474506378174
Loss :  1.8762394189834595 3.126537561416626 3.126537561416626
Loss :  1.8448565006256104 3.3747944831848145 3.3747944831848145
Loss :  1.86333167552948 3.3628573417663574 3.3628573417663574
Loss :  1.8580424785614014 3.6153180599212646 3.6153180599212646
Loss :  1.9001258611679077 3.643007516860962 3.643007516860962
Loss :  1.8274154663085938 3.4755234718322754 3.4755234718322754
  batch 20 loss: 1.8274154663085938, 3.4755234718322754, 3.4755234718322754
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8905985355377197 3.506589412689209 3.506589412689209
Loss :  1.885012149810791 3.736274003982544 3.736274003982544
Loss :  1.8609017133712769 3.985799551010132 3.985799551010132
Loss :  1.868513584136963 3.703700304031372 3.703700304031372
Loss :  1.8456412553787231 3.7347922325134277 3.7347922325134277
Loss :  1.8544349670410156 3.724147081375122 3.724147081375122
Loss :  1.8635083436965942 3.8117120265960693 3.8117120265960693
Loss :  1.8878271579742432 3.3512773513793945 3.3512773513793945
Loss :  1.8971625566482544 3.514706611633301 3.514706611633301
Loss :  1.8532524108886719 3.5050787925720215 3.5050787925720215
Loss :  1.890638828277588 3.455549478530884 3.455549478530884
Loss :  1.8663350343704224 3.5678250789642334 3.5678250789642334
Loss :  1.910547137260437 3.6151938438415527 3.6151938438415527
Loss :  1.8719322681427002 3.6194136142730713 3.6194136142730713
Loss :  1.9002785682678223 3.703777551651001 3.703777551651001
Loss :  1.8917261362075806 3.4538819789886475 3.4538819789886475
Loss :  1.8952455520629883 3.840792179107666 3.840792179107666
Loss :  1.8887616395950317 3.3478541374206543 3.3478541374206543
Loss :  1.8698710203170776 3.4005696773529053 3.4005696773529053
Loss :  1.8602083921432495 3.3114068508148193 3.3114068508148193
  batch 40 loss: 1.8602083921432495, 3.3114068508148193, 3.3114068508148193
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8619335889816284 3.51377272605896 3.51377272605896
Loss :  1.9028587341308594 3.4836325645446777 3.4836325645446777
Loss :  1.906046986579895 3.5038979053497314 3.5038979053497314
Loss :  1.8792386054992676 3.5402297973632812 3.5402297973632812
Loss :  1.895951509475708 3.3827011585235596 3.3827011585235596
Loss :  1.857942819595337 3.536975622177124 3.536975622177124
Loss :  1.852457046508789 3.588324546813965 3.588324546813965
Loss :  1.8898437023162842 3.574022054672241 3.574022054672241
Loss :  1.842394471168518 3.85284686088562 3.85284686088562
Loss :  1.8951033353805542 3.795828342437744 3.795828342437744
Loss :  1.8630547523498535 3.675304651260376 3.675304651260376
Loss :  1.8830586671829224 3.517958879470825 3.517958879470825
Loss :  1.8777759075164795 3.736595630645752 3.736595630645752
Loss :  1.8712725639343262 3.595940351486206 3.595940351486206
Loss :  1.8704352378845215 3.6896142959594727 3.6896142959594727
Loss :  1.8634214401245117 3.5420150756835938 3.5420150756835938
Loss :  1.857466220855713 3.761936902999878 3.761936902999878
Loss :  1.885500192642212 3.613981008529663 3.613981008529663
Loss :  1.8581346273422241 3.719834327697754 3.719834327697754
Loss :  1.8314869403839111 3.8790104389190674 3.8790104389190674
  batch 60 loss: 1.8314869403839111, 3.8790104389190674, 3.8790104389190674
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8599165678024292 3.8368208408355713 3.8368208408355713
Loss :  1.8358385562896729 3.509331703186035 3.509331703186035
Loss :  1.8776811361312866 3.8198330402374268 3.8198330402374268
Loss :  1.8652814626693726 3.9600698947906494 3.9600698947906494
Loss :  1.9088106155395508 3.3763434886932373 3.3763434886932373
Loss :  4.661712169647217 4.4413957595825195 4.4413957595825195
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  4.434180736541748 4.573846340179443 4.573846340179443
Loss :  4.297708988189697 4.4103288650512695 4.4103288650512695
Loss :  4.572725772857666 4.404727458953857 4.404727458953857
Total LOSS train 3.588994965186486 valid 4.4575746059417725
CE LOSS train 1.8709603786468505 valid 1.1431814432144165
Contrastive LOSS train 3.588994965186486 valid 1.1011818647384644
EPOCH 110:
Loss :  1.8731799125671387 3.6369948387145996 3.6369948387145996
Loss :  1.8264997005462646 3.424264430999756 3.424264430999756
Loss :  1.8620553016662598 3.4474751949310303 3.4474751949310303
Loss :  1.8662889003753662 3.3938660621643066 3.3938660621643066
Loss :  1.8476120233535767 3.4939231872558594 3.4939231872558594
Loss :  1.84928560256958 3.456584930419922 3.456584930419922
Loss :  1.819916844367981 3.631470203399658 3.631470203399658
Loss :  1.829166293144226 3.672273874282837 3.672273874282837
Loss :  1.8725087642669678 3.1963188648223877 3.1963188648223877
Loss :  1.852432131767273 2.7637665271759033 2.7637665271759033
Loss :  1.8882660865783691 3.3367435932159424 3.3367435932159424
Loss :  1.8659594058990479 3.669919013977051 3.669919013977051
Loss :  1.9140822887420654 3.6224875450134277 3.6224875450134277
Loss :  1.8824306726455688 3.6017162799835205 3.6017162799835205
Loss :  1.85114324092865 3.4219579696655273 3.4219579696655273
Loss :  1.8378292322158813 3.736489772796631 3.736489772796631
Loss :  1.8502751588821411 3.6231112480163574 3.6231112480163574
Loss :  1.8483916521072388 3.496631383895874 3.496631383895874
Loss :  1.885519027709961 3.3860902786254883 3.3860902786254883
Loss :  1.8105533123016357 3.581592082977295 3.581592082977295
  batch 20 loss: 1.8105533123016357, 3.581592082977295, 3.581592082977295
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.880535364151001 3.338301658630371 3.338301658630371
Loss :  1.8715063333511353 3.6765899658203125 3.6765899658203125
Loss :  1.8461230993270874 3.4438107013702393 3.4438107013702393
Loss :  1.874174952507019 3.4725213050842285 3.4725213050842285
Loss :  1.8320242166519165 3.569160223007202 3.569160223007202
Loss :  1.8475396633148193 3.627584934234619 3.627584934234619
Loss :  1.8472546339035034 3.6148016452789307 3.6148016452789307
Loss :  1.8589046001434326 3.779242992401123 3.779242992401123
Loss :  1.8658902645111084 3.523616313934326 3.523616313934326
Loss :  1.8268994092941284 3.551149845123291 3.551149845123291
Loss :  1.8743515014648438 3.4851415157318115 3.4851415157318115
Loss :  1.8445504903793335 3.503540277481079 3.503540277481079
Loss :  1.8947094678878784 3.191960096359253 3.191960096359253
Loss :  1.8418933153152466 3.592970132827759 3.592970132827759
Loss :  1.878625512123108 3.6657395362854004 3.6657395362854004
Loss :  1.8665322065353394 3.4838786125183105 3.4838786125183105
Loss :  1.881889820098877 3.593174457550049 3.593174457550049
Loss :  1.8553423881530762 3.5834312438964844 3.5834312438964844
Loss :  1.8596071004867554 3.347620725631714 3.347620725631714
Loss :  1.8577933311462402 3.4294183254241943 3.4294183254241943
  batch 40 loss: 1.8577933311462402, 3.4294183254241943, 3.4294183254241943
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8569021224975586 3.663322925567627 3.663322925567627
Loss :  1.8960078954696655 3.6005120277404785 3.6005120277404785
Loss :  1.898266077041626 3.6596291065216064 3.6596291065216064
Loss :  1.8775869607925415 3.495361566543579 3.495361566543579
Loss :  1.9003630876541138 3.7059810161590576 3.7059810161590576
Loss :  1.8697694540023804 3.0000853538513184 3.0000853538513184
Loss :  1.870895266532898 3.6009621620178223 3.6009621620178223
Loss :  1.8933463096618652 3.5882134437561035 3.5882134437561035
Loss :  1.8616820573806763 3.5378952026367188 3.5378952026367188
Loss :  1.9031611680984497 3.6800742149353027 3.6800742149353027
Loss :  1.8771463632583618 3.654787302017212 3.654787302017212
Loss :  1.8857723474502563 3.7766523361206055 3.7766523361206055
Loss :  1.8668855428695679 3.3502538204193115 3.3502538204193115
Loss :  1.865816354751587 3.538344621658325 3.538344621658325
Loss :  1.8922603130340576 3.470987558364868 3.470987558364868
Loss :  1.8688933849334717 3.7138354778289795 3.7138354778289795
Loss :  1.8650823831558228 3.614241361618042 3.614241361618042
Loss :  1.9215017557144165 3.6382391452789307 3.6382391452789307
Loss :  1.8810789585113525 3.7982308864593506 3.7982308864593506
Loss :  1.8613632917404175 3.452042818069458 3.452042818069458
  batch 60 loss: 1.8613632917404175, 3.452042818069458, 3.452042818069458
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8774166107177734 3.583278179168701 3.583278179168701
Loss :  1.8566250801086426 3.4872756004333496 3.4872756004333496
Loss :  1.8908601999282837 3.522510528564453 3.522510528564453
Loss :  1.875828742980957 3.230168104171753 3.230168104171753
Loss :  1.9028149843215942 3.035201072692871 3.035201072692871
Loss :  2.460954189300537 4.355630397796631 4.355630397796631
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  2.4551451206207275 4.352715492248535 4.352715492248535
Loss :  2.4028944969177246 4.321797847747803 4.321797847747803
Loss :  2.533858060836792 4.057826042175293 4.057826042175293
Total LOSS train 3.514852578823383 valid 4.271992444992065
CE LOSS train 1.8670287682459905 valid 0.633464515209198
Contrastive LOSS train 3.514852578823383 valid 1.0144565105438232
EPOCH 111:
Loss :  1.8873261213302612 3.338170051574707 3.338170051574707
Loss :  1.847489356994629 3.6994524002075195 3.6994524002075195
Loss :  1.892634630203247 3.6575074195861816 3.6575074195861816
Loss :  1.9044468402862549 3.4823920726776123 3.4823920726776123
Loss :  1.8923838138580322 3.2054851055145264 3.2054851055145264
Loss :  1.8942015171051025 3.386425495147705 3.386425495147705
Loss :  1.8736945390701294 3.380422353744507 3.380422353744507
Loss :  1.8605968952178955 3.428591251373291 3.428591251373291
Loss :  1.905055046081543 3.403655529022217 3.403655529022217
Loss :  1.8873927593231201 3.137586832046509 3.137586832046509
Loss :  1.9096676111221313 3.256612539291382 3.256612539291382
Loss :  1.8733996152877808 3.7316927909851074 3.7316927909851074
Loss :  1.9321898221969604 3.704367160797119 3.704367160797119
Loss :  1.8872240781784058 3.4119386672973633 3.4119386672973633
Loss :  1.8713116645812988 3.392414093017578 3.392414093017578
Loss :  1.878627896308899 3.1685233116149902 3.1685233116149902
Loss :  1.8849105834960938 3.4618380069732666 3.4618380069732666
Loss :  1.8901546001434326 3.4482712745666504 3.4482712745666504
Loss :  1.9135358333587646 2.9931442737579346 2.9931442737579346
Loss :  1.879666805267334 3.10243558883667 3.10243558883667
  batch 20 loss: 1.879666805267334, 3.10243558883667, 3.10243558883667
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9214240312576294 3.2284867763519287 3.2284867763519287
Loss :  1.9173470735549927 3.248993396759033 3.248993396759033
Loss :  1.9003968238830566 3.078394651412964 3.078394651412964
Loss :  1.9008315801620483 3.2664237022399902 3.2664237022399902
Loss :  1.8916059732437134 3.788054943084717 3.788054943084717
Loss :  1.9112634658813477 3.301405906677246 3.301405906677246
Loss :  1.9038642644882202 3.518998622894287 3.518998622894287
Loss :  1.920493483543396 3.251108407974243 3.251108407974243
Loss :  1.925095558166504 3.212390899658203 3.212390899658203
Loss :  1.894095540046692 3.2301125526428223 3.2301125526428223
Loss :  1.9176006317138672 3.5114405155181885 3.5114405155181885
Loss :  1.8978949785232544 3.323413133621216 3.323413133621216
Loss :  1.9321637153625488 3.35642671585083 3.35642671585083
Loss :  1.8992775678634644 3.369379758834839 3.369379758834839
Loss :  1.9233675003051758 3.568814516067505 3.568814516067505
Loss :  1.9088510274887085 3.4470019340515137 3.4470019340515137
Loss :  1.9128831624984741 3.1575050354003906 3.1575050354003906
Loss :  1.898704171180725 3.4884226322174072 3.4884226322174072
Loss :  1.889119267463684 3.232713460922241 3.232713460922241
Loss :  1.8838331699371338 3.662745237350464 3.662745237350464
  batch 40 loss: 1.8838331699371338, 3.662745237350464, 3.662745237350464
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8759112358093262 3.614598274230957 3.614598274230957
Loss :  1.9135264158248901 3.311770439147949 3.311770439147949
Loss :  1.9209487438201904 3.3389058113098145 3.3389058113098145
Loss :  1.8864227533340454 3.5059409141540527 3.5059409141540527
Loss :  1.921115756034851 3.2509779930114746 3.2509779930114746
Loss :  1.889461874961853 3.3884437084198 3.3884437084198
Loss :  1.8841416835784912 3.6003098487854004 3.6003098487854004
Loss :  1.9177669286727905 3.4565622806549072 3.4565622806549072
Loss :  1.88070809841156 3.2248098850250244 3.2248098850250244
Loss :  1.9170970916748047 3.324927806854248 3.324927806854248
Loss :  1.894935965538025 3.4381704330444336 3.4381704330444336
Loss :  1.9006651639938354 3.4320459365844727 3.4320459365844727
Loss :  1.8838480710983276 3.3108956813812256 3.3108956813812256
Loss :  1.9160833358764648 3.2322685718536377 3.2322685718536377
Loss :  1.9137027263641357 3.3017380237579346 3.3017380237579346
Loss :  1.9017058610916138 3.1474390029907227 3.1474390029907227
Loss :  1.8983838558197021 3.1058506965637207 3.1058506965637207
Loss :  1.9372107982635498 3.292461395263672 3.292461395263672
Loss :  1.9068048000335693 3.474292039871216 3.474292039871216
Loss :  1.909353256225586 3.498934268951416 3.498934268951416
  batch 60 loss: 1.909353256225586, 3.498934268951416, 3.498934268951416
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.90773606300354 4.2963480949401855 4.2963480949401855
Loss :  1.8955270051956177 3.7296807765960693 3.7296807765960693
Loss :  1.922062873840332 3.4984171390533447 3.4984171390533447
Loss :  1.9167085886001587 3.474576711654663 3.474576711654663
Loss :  1.9406545162200928 3.217329502105713 3.217329502105713
Loss :  7.636855602264404 4.3456196784973145 4.3456196784973145
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  6.833216667175293 4.367114067077637 4.367114067077637
Loss :  6.999765872955322 4.458765983581543 4.458765983581543
Loss :  7.910308837890625 4.342216491699219 4.342216491699219
Total LOSS train 3.392320834673368 valid 4.378429055213928
CE LOSS train 1.9010539458348201 valid 1.9775772094726562
Contrastive LOSS train 3.392320834673368 valid 1.0855541229248047
EPOCH 112:
Loss :  1.941796898841858 3.604034423828125 3.604034423828125
Loss :  1.9098936319351196 3.6561784744262695 3.6561784744262695
Loss :  1.9531354904174805 3.952627658843994 3.952627658843994
Loss :  1.9521753787994385 3.4723000526428223 3.4723000526428223
Loss :  1.945749282836914 3.8734664916992188 3.8734664916992188
Loss :  1.959267497062683 3.557173252105713 3.557173252105713
Loss :  1.9465551376342773 3.5942184925079346 3.5942184925079346
Loss :  1.9450105428695679 3.50516414642334 3.50516414642334
Loss :  1.964739441871643 3.6000800132751465 3.6000800132751465
Loss :  1.95632803440094 3.1795411109924316 3.1795411109924316
Loss :  1.9755691289901733 3.2172420024871826 3.2172420024871826
Loss :  1.9384256601333618 3.3798582553863525 3.3798582553863525
Loss :  1.970414161682129 3.6185803413391113 3.6185803413391113
Loss :  1.9423036575317383 3.546875476837158 3.546875476837158
Loss :  1.9607871770858765 3.72475528717041 3.72475528717041
Loss :  1.9807822704315186 3.439152240753174 3.439152240753174
Loss :  1.961674690246582 3.630657911300659 3.630657911300659
Loss :  1.9811006784439087 3.654649257659912 3.654649257659912
Loss :  1.9632278680801392 4.055779457092285 4.055779457092285
Loss :  1.9604498147964478 4.059512138366699 4.059512138366699
  batch 20 loss: 1.9604498147964478, 4.059512138366699, 4.059512138366699
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9597538709640503 3.790299654006958 3.790299654006958
Loss :  1.9510232210159302 3.7508206367492676 3.7508206367492676
Loss :  1.9446842670440674 3.5320370197296143 3.5320370197296143
Loss :  1.9318749904632568 3.7809698581695557 3.7809698581695557
Loss :  1.9334596395492554 3.667358875274658 3.667358875274658
Loss :  1.9365543127059937 3.6532838344573975 3.6532838344573975
Loss :  1.9428786039352417 3.4058027267456055 3.4058027267456055
Loss :  1.930158257484436 3.402867317199707 3.402867317199707
Loss :  1.9481374025344849 3.3905980587005615 3.3905980587005615
Loss :  1.9099270105361938 3.5244336128234863 3.5244336128234863
Loss :  1.924456000328064 3.5722906589508057 3.5722906589508057
Loss :  1.9093413352966309 3.8016209602355957 3.8016209602355957
Loss :  1.9205747842788696 4.157397270202637 4.157397270202637
Loss :  1.9207223653793335 3.6032869815826416 3.6032869815826416
Loss :  1.920632004737854 3.7744340896606445 3.7744340896606445
Loss :  1.9212818145751953 3.9275500774383545 3.9275500774383545
Loss :  1.9113107919692993 3.507465124130249 3.507465124130249
Loss :  1.9118692874908447 3.6423730850219727 3.6423730850219727
Loss :  1.9153180122375488 3.4843356609344482 3.4843356609344482
Loss :  1.9117560386657715 3.5560462474823 3.5560462474823
  batch 40 loss: 1.9117560386657715, 3.5560462474823, 3.5560462474823
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8940883874893188 3.5782687664031982 3.5782687664031982
Loss :  1.9260321855545044 3.541027069091797 3.541027069091797
Loss :  1.9044114351272583 3.974623918533325 3.974623918533325
Loss :  1.893626093864441 3.862950086593628 3.862950086593628
Loss :  1.9087605476379395 3.5930051803588867 3.5930051803588867
Loss :  1.8966437578201294 3.820317029953003 3.820317029953003
Loss :  1.9000067710876465 3.591992139816284 3.591992139816284
Loss :  1.9075632095336914 3.7625839710235596 3.7625839710235596
Loss :  1.8839744329452515 3.7453720569610596 3.7453720569610596
Loss :  1.9015023708343506 3.5990827083587646 3.5990827083587646
Loss :  1.9080555438995361 3.4367077350616455 3.4367077350616455
Loss :  1.9280732870101929 3.342482566833496 3.342482566833496
Loss :  1.9164438247680664 3.598306179046631 3.598306179046631
Loss :  1.9462615251541138 3.6544952392578125 3.6544952392578125
Loss :  1.9209513664245605 3.685502052307129 3.685502052307129
Loss :  1.9255855083465576 3.493399143218994 3.493399143218994
Loss :  1.9167611598968506 3.358950138092041 3.358950138092041
Loss :  1.9440736770629883 3.9480602741241455 3.9480602741241455
Loss :  1.9143455028533936 3.896658182144165 3.896658182144165
Loss :  1.9136555194854736 3.7482075691223145 3.7482075691223145
  batch 60 loss: 1.9136555194854736, 3.7482075691223145, 3.7482075691223145
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9112845659255981 3.785893201828003 3.785893201828003
Loss :  1.900114893913269 3.950634479522705 3.950634479522705
Loss :  1.9250329732894897 4.117079734802246 4.117079734802246
Loss :  1.8770475387573242 4.326241493225098 4.326241493225098
Loss :  1.9065719842910767 3.989657163619995 3.989657163619995
Loss :  1.9985507726669312 4.33911657333374 4.33911657333374
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  2.0509860515594482 4.453763484954834 4.453763484954834
Loss :  2.0286736488342285 4.3827805519104 4.3827805519104
Loss :  1.995158314704895 4.047837734222412 4.047837734222412
Total LOSS train 3.671517174060528 valid 4.305874586105347
CE LOSS train 1.9297841310501098 valid 0.49878957867622375
Contrastive LOSS train 3.671517174060528 valid 1.011959433555603
EPOCH 113:
Loss :  1.8922775983810425 4.318676948547363 4.318676948547363
Loss :  1.8285977840423584 4.340466499328613 4.340466499328613
Loss :  1.8601715564727783 4.649354457855225 4.649354457855225
Loss :  1.9307291507720947 4.110015869140625 4.110015869140625
Loss :  1.8976081609725952 4.210083961486816 4.210083961486816
Loss :  1.9333951473236084 4.012115955352783 4.012115955352783
Loss :  1.870951533317566 4.051057815551758 4.051057815551758
Loss :  1.8781222105026245 3.948411464691162 3.948411464691162
Loss :  1.8895550966262817 4.101059436798096 4.101059436798096
Loss :  1.8905287981033325 4.052231788635254 4.052231788635254
Loss :  1.8906556367874146 4.31268310546875 4.31268310546875
Loss :  1.9063119888305664 4.25186014175415 4.25186014175415
Loss :  1.9040452241897583 4.267868995666504 4.267868995666504
Loss :  1.8951503038406372 4.204044342041016 4.204044342041016
Loss :  1.9207384586334229 4.092121601104736 4.092121601104736
Loss :  1.8015717267990112 4.552957534790039 4.552957534790039
Loss :  1.92999267578125 4.934189796447754 4.934189796447754
Loss :  1.954276442527771 4.401578903198242 4.401578903198242
Loss :  1.9278995990753174 4.407168388366699 4.407168388366699
Loss :  1.8776843547821045 4.4319963455200195 4.4319963455200195
  batch 20 loss: 1.8776843547821045, 4.4319963455200195, 4.4319963455200195
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9543544054031372 4.295683860778809 4.295683860778809
Loss :  1.8975393772125244 4.750277042388916 4.750277042388916
Loss :  1.8567997217178345 4.517102241516113 4.517102241516113
Loss :  1.9208877086639404 4.5648112297058105 4.5648112297058105
Loss :  1.8953241109848022 4.7030510902404785 4.7030510902404785
Loss :  1.9469586610794067 4.396452903747559 4.396452903747559
Loss :  1.8975292444229126 4.762282371520996 4.762282371520996
Loss :  1.9615774154663086 4.4521803855896 4.4521803855896
Loss :  1.954473614692688 4.410950183868408 4.410950183868408
Loss :  1.9447370767593384 4.581022262573242 4.581022262573242
Loss :  1.9297335147857666 4.659724235534668 4.659724235534668
Loss :  1.8744255304336548 4.541121482849121 4.541121482849121
Loss :  1.957949161529541 4.655361652374268 4.655361652374268
Loss :  1.947139024734497 4.468935966491699 4.468935966491699
Loss :  1.9223607778549194 4.653256416320801 4.653256416320801
Loss :  1.8928673267364502 4.377396583557129 4.377396583557129
Loss :  1.9015134572982788 4.4292826652526855 4.4292826652526855
Loss :  1.9440335035324097 4.456405162811279 4.456405162811279
Loss :  1.9315364360809326 4.554801940917969 4.554801940917969
Loss :  1.96571683883667 4.752444267272949 4.752444267272949
  batch 40 loss: 1.96571683883667, 4.752444267272949, 4.752444267272949
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8929316997528076 4.509800910949707 4.509800910949707
Loss :  1.9077892303466797 4.425309658050537 4.425309658050537
Loss :  1.8735748529434204 4.402052879333496 4.402052879333496
Loss :  1.9521315097808838 4.496678829193115 4.496678829193115
Loss :  1.93686044216156 4.548476696014404 4.548476696014404
Loss :  1.887929916381836 4.786465167999268 4.786465167999268
Loss :  1.8533376455307007 4.495266437530518 4.495266437530518
Loss :  1.9004572629928589 4.654099941253662 4.654099941253662
Loss :  1.8932441473007202 4.6079840660095215 4.6079840660095215
Loss :  1.861234188079834 4.456122398376465 4.456122398376465
Loss :  1.859997034072876 4.507266521453857 4.507266521453857
Loss :  1.8708117008209229 4.357861042022705 4.357861042022705
Loss :  1.7954376935958862 4.511286735534668 4.511286735534668
Loss :  1.9169789552688599 4.282141208648682 4.282141208648682
Loss :  1.8530373573303223 4.5818023681640625 4.5818023681640625
Loss :  1.9243566989898682 4.324441432952881 4.324441432952881
Loss :  1.8509998321533203 4.553929328918457 4.553929328918457
Loss :  1.8421688079833984 4.510787487030029 4.510787487030029
Loss :  1.9455864429473877 4.500442981719971 4.500442981719971
Loss :  1.8680936098098755 4.545781135559082 4.545781135559082
  batch 60 loss: 1.8680936098098755, 4.545781135559082, 4.545781135559082
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9343119859695435 4.422218322753906 4.422218322753906
Loss :  1.900280237197876 4.39794397354126 4.39794397354126
Loss :  1.9236149787902832 4.478891372680664 4.478891372680664
Loss :  1.90926194190979 4.341782569885254 4.341782569885254
Loss :  1.9154021739959717 4.148622512817383 4.148622512817383
Loss :  4.6808881759643555 4.348836898803711 4.348836898803711
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  5.201756000518799 4.261105537414551 4.261105537414551
Loss :  4.321678161621094 4.247866630554199 4.247866630554199
Loss :  4.458090782165527 4.453387260437012 4.453387260437012
Total LOSS train 4.438152973468487 valid 4.327799081802368
CE LOSS train 1.902270010801462 valid 1.1145226955413818
Contrastive LOSS train 4.438152973468487 valid 1.113346815109253
EPOCH 114:
Loss :  1.9349751472473145 4.308473587036133 4.308473587036133
Loss :  1.9038197994232178 4.694096088409424 4.694096088409424
Loss :  1.9114885330200195 4.412794589996338 4.412794589996338
Loss :  1.8940274715423584 4.4477434158325195 4.4477434158325195
Loss :  1.8539636135101318 4.345964431762695 4.345964431762695
Loss :  1.8796800374984741 4.508458137512207 4.508458137512207
Loss :  1.8251138925552368 4.616798400878906 4.616798400878906
Loss :  1.9298754930496216 4.411403179168701 4.411403179168701
Loss :  1.8749537467956543 4.498919486999512 4.498919486999512
Loss :  1.969443917274475 4.5600996017456055 4.5600996017456055
Loss :  1.9380247592926025 4.41809606552124 4.41809606552124
Loss :  1.8212437629699707 4.62011194229126 4.62011194229126
Loss :  1.8888524770736694 4.577171325683594 4.577171325683594
Loss :  1.9078580141067505 4.675626277923584 4.675626277923584
Loss :  1.8287177085876465 4.267334461212158 4.267334461212158
Loss :  1.8690495491027832 4.489427089691162 4.489427089691162
Loss :  1.8653160333633423 4.451852798461914 4.451852798461914
Loss :  1.8789565563201904 4.520766258239746 4.520766258239746
Loss :  1.9020166397094727 4.609992027282715 4.609992027282715
Loss :  1.8614892959594727 4.353855609893799 4.353855609893799
  batch 20 loss: 1.8614892959594727, 4.353855609893799, 4.353855609893799
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.912888526916504 4.545900344848633 4.545900344848633
Loss :  1.8667564392089844 4.54771614074707 4.54771614074707
Loss :  1.8907470703125 4.642943859100342 4.642943859100342
Loss :  1.883658528327942 4.470468044281006 4.470468044281006
Loss :  1.9628512859344482 4.667765140533447 4.667765140533447
Loss :  1.9249569177627563 4.575023174285889 4.575023174285889
Loss :  1.926696538925171 4.807896614074707 4.807896614074707
Loss :  1.9047640562057495 4.435369491577148 4.435369491577148
Loss :  1.9295903444290161 4.2628960609436035 4.2628960609436035
Loss :  1.8378490209579468 4.661602973937988 4.661602973937988
Loss :  1.9209561347961426 4.556413650512695 4.556413650512695
Loss :  1.9466118812561035 4.614657878875732 4.614657878875732
Loss :  1.8533052206039429 4.431927680969238 4.431927680969238
Loss :  1.9133005142211914 4.530758857727051 4.530758857727051
Loss :  1.9050824642181396 4.700952053070068 4.700952053070068
Loss :  1.891714334487915 4.617690563201904 4.617690563201904
Loss :  1.9071100950241089 4.579647541046143 4.579647541046143
Loss :  1.8335438966751099 4.457326889038086 4.457326889038086
Loss :  1.9059776067733765 4.41126012802124 4.41126012802124
Loss :  1.8735411167144775 4.4096479415893555 4.4096479415893555
  batch 40 loss: 1.8735411167144775, 4.4096479415893555, 4.4096479415893555
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8754653930664062 4.389646053314209 4.389646053314209
Loss :  1.9266583919525146 4.493474006652832 4.493474006652832
Loss :  1.9211323261260986 4.462339878082275 4.462339878082275
Loss :  1.9110790491104126 4.549251079559326 4.549251079559326
Loss :  1.9494644403457642 4.3935980796813965 4.3935980796813965
Loss :  1.9199976921081543 4.268295764923096 4.268295764923096
Loss :  1.8639675378799438 4.392400741577148 4.392400741577148
Loss :  1.9330626726150513 4.4697771072387695 4.4697771072387695
Loss :  1.9293462038040161 4.338062763214111 4.338062763214111
Loss :  1.9474091529846191 4.526062965393066 4.526062965393066
Loss :  1.9118447303771973 4.46556282043457 4.46556282043457
Loss :  1.9157356023788452 4.278554916381836 4.278554916381836
Loss :  1.9945688247680664 4.25092887878418 4.25092887878418
Loss :  1.968869924545288 4.1543288230896 4.1543288230896
Loss :  1.9327188730239868 4.489071369171143 4.489071369171143
Loss :  1.8906280994415283 4.454529762268066 4.454529762268066
Loss :  1.9008212089538574 4.464879512786865 4.464879512786865
Loss :  1.915047287940979 4.593491077423096 4.593491077423096
Loss :  1.8358008861541748 4.471196174621582 4.471196174621582
Loss :  1.8335713148117065 4.542661666870117 4.542661666870117
  batch 60 loss: 1.8335713148117065, 4.542661666870117, 4.542661666870117
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8686072826385498 4.860700607299805 4.860700607299805
Loss :  1.93674635887146 4.384335517883301 4.384335517883301
Loss :  1.9337916374206543 4.450921535491943 4.450921535491943
Loss :  1.963227391242981 4.564091682434082 4.564091682434082
Loss :  1.9258568286895752 4.237102031707764 4.237102031707764
Loss :  2.301504135131836 4.3929877281188965 4.3929877281188965
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 4], device='cuda:0')
Loss :  2.3370940685272217 4.406639099121094 4.406639099121094
Loss :  2.344943046569824 4.311063289642334 4.311063289642334
Loss :  2.2197861671447754 4.08119010925293 4.08119010925293
Total LOSS train 4.487109455695519 valid 4.2979700565338135
CE LOSS train 1.9020951931293195 valid 0.5549465417861938
Contrastive LOSS train 4.487109455695519 valid 1.0202975273132324
EPOCH 115:
Loss :  1.8480937480926514 4.438985347747803 4.438985347747803
Loss :  1.9023370742797852 4.60584831237793 4.60584831237793
Loss :  1.9414674043655396 4.28832483291626 4.28832483291626
Loss :  1.8892576694488525 4.450075149536133 4.450075149536133
Loss :  1.9078636169433594 4.471168041229248 4.471168041229248
Loss :  1.9074050188064575 4.682900428771973 4.682900428771973
Loss :  1.8292269706726074 4.407763957977295 4.407763957977295
Loss :  1.8756358623504639 4.468323707580566 4.468323707580566
Loss :  1.9134085178375244 4.451352119445801 4.451352119445801
Loss :  1.9201669692993164 4.356869697570801 4.356869697570801
Loss :  1.9295488595962524 4.466073989868164 4.466073989868164
Loss :  1.9196192026138306 4.4278645515441895 4.4278645515441895
Loss :  1.8445128202438354 4.4461188316345215 4.4461188316345215
Loss :  1.9233564138412476 4.605889320373535 4.605889320373535
Loss :  1.9025121927261353 4.529446125030518 4.529446125030518
Loss :  1.9023250341415405 4.426825046539307 4.426825046539307
Loss :  1.8162786960601807 4.458436489105225 4.458436489105225
Loss :  1.91847825050354 4.4320173263549805 4.4320173263549805
Loss :  1.9305977821350098 4.4459004402160645 4.4459004402160645
Loss :  1.9278713464736938 4.341988563537598 4.341988563537598
  batch 20 loss: 1.9278713464736938, 4.341988563537598, 4.341988563537598
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9302935600280762 4.413853168487549 4.413853168487549
Loss :  1.927101969718933 4.488236427307129 4.488236427307129
Loss :  1.9461629390716553 4.746355056762695 4.746355056762695
Loss :  1.8934053182601929 4.389954090118408 4.389954090118408
Loss :  1.8890266418457031 4.4721269607543945 4.4721269607543945
Loss :  1.9206891059875488 4.357382774353027 4.357382774353027
Loss :  1.9441537857055664 4.7242608070373535 4.7242608070373535
Loss :  1.8889840841293335 4.403960704803467 4.403960704803467
Loss :  1.9183160066604614 4.515989303588867 4.515989303588867
Loss :  1.9118648767471313 4.517087936401367 4.517087936401367
Loss :  1.9528648853302002 4.595707416534424 4.595707416534424
Loss :  1.8482873439788818 4.54374885559082 4.54374885559082
Loss :  1.9524199962615967 4.56399393081665 4.56399393081665
Loss :  1.9245034456253052 4.442831516265869 4.442831516265869
Loss :  1.951810359954834 4.6092705726623535 4.6092705726623535
Loss :  1.9776735305786133 4.60639762878418 4.60639762878418
Loss :  1.9347518682479858 4.557917594909668 4.557917594909668
Loss :  1.9682046175003052 4.3805084228515625 4.3805084228515625
Loss :  1.83476984500885 4.382715225219727 4.382715225219727
Loss :  1.910596251487732 4.510222434997559 4.510222434997559
  batch 40 loss: 1.910596251487732, 4.510222434997559, 4.510222434997559
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9230201244354248 4.620172500610352 4.620172500610352
Loss :  1.8772966861724854 4.4857401847839355 4.4857401847839355
Loss :  1.8780760765075684 4.368590354919434 4.368590354919434
Loss :  1.869579792022705 4.468897342681885 4.468897342681885
Loss :  1.8753467798233032 4.463467121124268 4.463467121124268
Loss :  1.8232860565185547 4.754936218261719 4.754936218261719
Loss :  1.9242092370986938 4.4579644203186035 4.4579644203186035
Loss :  1.8457037210464478 4.298184394836426 4.298184394836426
Loss :  1.902207374572754 4.488356590270996 4.488356590270996
Loss :  1.968024730682373 4.737274646759033 4.737274646759033
Loss :  1.891854166984558 4.549989223480225 4.549989223480225
Loss :  1.9532991647720337 4.317775249481201 4.317775249481201
Loss :  1.9337658882141113 4.3571319580078125 4.3571319580078125
Loss :  1.97091805934906 4.427685737609863 4.427685737609863
Loss :  1.961430311203003 4.5166497230529785 4.5166497230529785
Loss :  2.035370111465454 4.426836967468262 4.426836967468262
Loss :  1.8935998678207397 4.492777347564697 4.492777347564697
Loss :  1.9985984563827515 4.498586654663086 4.498586654663086
Loss :  2.0077388286590576 4.634546756744385 4.634546756744385
Loss :  1.9628595113754272 4.60499906539917 4.60499906539917
  batch 60 loss: 1.9628595113754272, 4.60499906539917, 4.60499906539917
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8657753467559814 4.48286247253418 4.48286247253418
Loss :  1.8395153284072876 4.456691265106201 4.456691265106201
Loss :  1.9323384761810303 4.536591529846191 4.536591529846191
Loss :  1.9273966550827026 4.432177543640137 4.432177543640137
Loss :  1.9661554098129272 4.126165866851807 4.126165866851807
Loss :  2.8397011756896973 4.314837455749512 4.314837455749512
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 4, 5], device='cuda:0')
Loss :  2.7823257446289062 4.331959247589111 4.331959247589111
Loss :  2.7506351470947266 4.32709264755249 4.32709264755249
Loss :  2.765937328338623 3.969613552093506 3.969613552093506
Total LOSS train 4.483534526824951 valid 4.235875725746155
CE LOSS train 1.9138955391370334 valid 0.6914843320846558
Contrastive LOSS train 4.483534526824951 valid 0.9924033880233765
EPOCH 116:
Loss :  1.9565216302871704 4.2635955810546875 4.2635955810546875
Loss :  1.9581413269042969 4.522146701812744 4.522146701812744
Loss :  1.8715897798538208 4.444566249847412 4.444566249847412
Loss :  1.9483243227005005 4.3697509765625 4.3697509765625
Loss :  1.955137014389038 4.422529697418213 4.422529697418213
Loss :  1.9521905183792114 4.435139179229736 4.435139179229736
Loss :  2.067631244659424 4.514184951782227 4.514184951782227
Loss :  1.9450386762619019 4.527879238128662 4.527879238128662
Loss :  1.93503737449646 4.452850818634033 4.452850818634033
Loss :  1.9456391334533691 4.608371257781982 4.608371257781982
Loss :  1.9378079175949097 4.182164669036865 4.182164669036865
Loss :  1.9666705131530762 4.49409818649292 4.49409818649292
Loss :  1.9462913274765015 4.621553421020508 4.621553421020508
Loss :  1.9148670434951782 4.58145809173584 4.58145809173584
Loss :  1.902295470237732 4.797017574310303 4.797017574310303
Loss :  1.9949322938919067 4.517172813415527 4.517172813415527
Loss :  1.9652906656265259 4.643957614898682 4.643957614898682
Loss :  1.9783555269241333 4.356747627258301 4.356747627258301
Loss :  1.9624323844909668 4.186572551727295 4.186572551727295
Loss :  1.9397552013397217 4.485998630523682 4.485998630523682
  batch 20 loss: 1.9397552013397217, 4.485998630523682, 4.485998630523682
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9252865314483643 4.237928867340088 4.237928867340088
Loss :  1.9672945737838745 4.589613914489746 4.589613914489746
Loss :  1.9372622966766357 4.551880359649658 4.551880359649658
Loss :  1.9863638877868652 4.4291510581970215 4.4291510581970215
Loss :  1.9681953191757202 4.714388847351074 4.714388847351074
Loss :  1.9291322231292725 4.280200481414795 4.280200481414795
Loss :  2.0704987049102783 4.446934700012207 4.446934700012207
Loss :  1.9039051532745361 4.42903470993042 4.42903470993042
Loss :  1.9133435487747192 4.52303409576416 4.52303409576416
Loss :  1.9320405721664429 4.4231181144714355 4.4231181144714355
Loss :  1.9589290618896484 4.357028484344482 4.357028484344482
Loss :  1.7720025777816772 4.522012710571289 4.522012710571289
Loss :  1.9748204946517944 4.3799967765808105 4.3799967765808105
Loss :  1.8896316289901733 4.3256754875183105 4.3256754875183105
Loss :  1.9193603992462158 4.339477062225342 4.339477062225342
Loss :  1.99239182472229 4.3687310218811035 4.3687310218811035
Loss :  1.9328736066818237 4.4406328201293945 4.4406328201293945
Loss :  1.9233418703079224 4.288069248199463 4.288069248199463
Loss :  1.7925604581832886 4.352949142456055 4.352949142456055
Loss :  1.9011422395706177 4.414701461791992 4.414701461791992
  batch 40 loss: 1.9011422395706177, 4.414701461791992, 4.414701461791992
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8105467557907104 4.431543350219727 4.431543350219727
Loss :  1.9507114887237549 4.3214921951293945 4.3214921951293945
Loss :  1.9332244396209717 4.192867755889893 4.192867755889893
Loss :  1.8715543746948242 4.289982795715332 4.289982795715332
Loss :  1.957458257675171 3.96864652633667 3.96864652633667
Loss :  1.9345126152038574 4.419151782989502 4.419151782989502
Loss :  1.9377256631851196 4.407290935516357 4.407290935516357
Loss :  1.9757872819900513 4.391582012176514 4.391582012176514
Loss :  1.9553852081298828 4.521899700164795 4.521899700164795
Loss :  1.979162335395813 4.536489486694336 4.536489486694336
Loss :  1.9610719680786133 4.26069450378418 4.26069450378418
Loss :  1.7534303665161133 4.1308207511901855 4.1308207511901855
Loss :  1.9710884094238281 4.558220863342285 4.558220863342285
Loss :  1.951959252357483 4.389443874359131 4.389443874359131
Loss :  1.849612832069397 4.518730640411377 4.518730640411377
Loss :  1.852349877357483 4.305256366729736 4.305256366729736
Loss :  1.8709557056427002 4.508902549743652 4.508902549743652
Loss :  1.940509557723999 4.492545127868652 4.492545127868652
Loss :  1.8465156555175781 4.4976115226745605 4.4976115226745605
Loss :  1.8188035488128662 4.449529647827148 4.449529647827148
  batch 60 loss: 1.8188035488128662, 4.449529647827148, 4.449529647827148
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9216370582580566 4.5258870124816895 4.5258870124816895
Loss :  1.880387544631958 4.396049976348877 4.396049976348877
Loss :  1.9773674011230469 4.389031410217285 4.389031410217285
Loss :  1.9404832124710083 4.657530307769775 4.657530307769775
Loss :  1.807206392288208 4.178290367126465 4.178290367126465
Loss :  1.8573050498962402 4.574735641479492 4.574735641479492
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.869185447692871 4.626961708068848 4.626961708068848
Loss :  2.0581462383270264 4.369755268096924 4.369755268096924
Loss :  1.8777374029159546 4.44614839553833 4.44614839553833
Total LOSS train 4.424335487072284 valid 4.504400253295898
CE LOSS train 1.9274426698684692 valid 0.46943435072898865
Contrastive LOSS train 4.424335487072284 valid 1.1115370988845825
EPOCH 117:
Loss :  1.8895771503448486 4.31614875793457 4.31614875793457
Loss :  1.8380765914916992 4.5383124351501465 4.5383124351501465
Loss :  1.8841255903244019 4.617080211639404 4.617080211639404
Loss :  1.9418576955795288 4.391342639923096 4.391342639923096
Loss :  1.9822267293930054 4.584590435028076 4.584590435028076
Loss :  1.8836305141448975 4.514167785644531 4.514167785644531
Loss :  1.8984640836715698 4.307342529296875 4.307342529296875
Loss :  1.9349400997161865 4.493983268737793 4.493983268737793
Loss :  1.9114887714385986 4.4921135902404785 4.4921135902404785
Loss :  1.935323715209961 4.628439903259277 4.628439903259277
Loss :  1.9677269458770752 4.493162155151367 4.493162155151367
Loss :  1.9487897157669067 4.3793864250183105 4.3793864250183105
Loss :  1.9283080101013184 4.497029781341553 4.497029781341553
Loss :  1.9283608198165894 4.3451714515686035 4.3451714515686035
Loss :  1.9143247604370117 4.381302833557129 4.381302833557129
Loss :  1.8790528774261475 4.4898600578308105 4.4898600578308105
Loss :  1.8641208410263062 4.504127025604248 4.504127025604248
Loss :  1.9125534296035767 4.663680553436279 4.663680553436279
Loss :  1.9851007461547852 4.404343605041504 4.404343605041504
Loss :  1.9212653636932373 4.591267108917236 4.591267108917236
  batch 20 loss: 1.9212653636932373, 4.591267108917236, 4.591267108917236
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9162254333496094 4.590308666229248 4.590308666229248
Loss :  1.9220744371414185 4.8846659660339355 4.8846659660339355
Loss :  1.9796953201293945 4.539531230926514 4.539531230926514
Loss :  1.9591745138168335 4.566792964935303 4.566792964935303
Loss :  1.8909186124801636 4.803921222686768 4.803921222686768
Loss :  1.9659124612808228 4.538929462432861 4.538929462432861
Loss :  1.8907445669174194 4.700507164001465 4.700507164001465
Loss :  1.9109755754470825 4.412438869476318 4.412438869476318
Loss :  1.9123185873031616 4.9072675704956055 4.9072675704956055
Loss :  1.9669829607009888 4.4403462409973145 4.4403462409973145
Loss :  1.8992170095443726 4.455705165863037 4.455705165863037
Loss :  1.8971343040466309 4.614062309265137 4.614062309265137
Loss :  1.9463493824005127 4.333521842956543 4.333521842956543
Loss :  1.984200119972229 4.609152317047119 4.609152317047119
Loss :  1.9224194288253784 4.591851711273193 4.591851711273193
Loss :  1.9292718172073364 4.6196112632751465 4.6196112632751465
Loss :  1.9260759353637695 4.530250549316406 4.530250549316406
Loss :  1.8360668420791626 4.585578918457031 4.585578918457031
Loss :  1.881988286972046 4.485272407531738 4.485272407531738
Loss :  1.7796496152877808 4.403629779815674 4.403629779815674
  batch 40 loss: 1.7796496152877808, 4.403629779815674, 4.403629779815674
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8240954875946045 4.501813888549805 4.501813888549805
Loss :  1.8402997255325317 4.484139919281006 4.484139919281006
Loss :  1.8827290534973145 4.477620601654053 4.477620601654053
Loss :  1.9085670709609985 4.422086238861084 4.422086238861084
Loss :  1.928664207458496 4.44708776473999 4.44708776473999
Loss :  1.832174301147461 4.373202323913574 4.373202323913574
Loss :  1.8887453079223633 4.972674369812012 4.972674369812012
Loss :  1.9113452434539795 4.644053936004639 4.644053936004639
Loss :  1.9523215293884277 4.618896961212158 4.618896961212158
Loss :  1.8273059129714966 4.5562262535095215 4.5562262535095215
Loss :  1.8657313585281372 4.4977641105651855 4.4977641105651855
Loss :  1.8401638269424438 4.403616428375244 4.403616428375244
Loss :  1.8737554550170898 4.467322826385498 4.467322826385498
Loss :  1.9241126775741577 4.663590908050537 4.663590908050537
Loss :  1.8902482986450195 4.508755683898926 4.508755683898926
Loss :  1.8468338251113892 4.325425148010254 4.325425148010254
Loss :  1.9024653434753418 4.688765048980713 4.688765048980713
Loss :  1.9007974863052368 4.642037868499756 4.642037868499756
Loss :  1.8849087953567505 4.460769176483154 4.460769176483154
Loss :  1.848946452140808 4.42680549621582 4.42680549621582
  batch 60 loss: 1.848946452140808, 4.42680549621582, 4.42680549621582
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.894620656967163 4.521219730377197 4.521219730377197
Loss :  1.8808822631835938 4.525537014007568 4.525537014007568
Loss :  1.8840488195419312 4.3855671882629395 4.3855671882629395
Loss :  1.9216407537460327 4.223714351654053 4.223714351654053
Loss :  1.8851287364959717 4.033958911895752 4.033958911895752
Loss :  2.2203972339630127 4.512114524841309 4.512114524841309
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  2.068725824356079 4.433411121368408 4.433411121368408
Loss :  2.22434663772583 4.26710319519043 4.26710319519043
Loss :  2.132598400115967 3.8329017162323 3.8329017162323
Total LOSS train 4.515674620408278 valid 4.261382639408112
CE LOSS train 1.9031883423145002 valid 0.5331496000289917
Contrastive LOSS train 4.515674620408278 valid 0.958225429058075
EPOCH 118:
Loss :  1.8455177545547485 4.6250457763671875 4.6250457763671875
Loss :  1.8744593858718872 4.673482418060303 4.673482418060303
Loss :  1.9149949550628662 4.248103618621826 4.248103618621826
Loss :  1.8353370428085327 4.4799418449401855 4.4799418449401855
Loss :  1.846174716949463 4.622313976287842 4.622313976287842
Loss :  1.8902473449707031 4.635370254516602 4.635370254516602
Loss :  1.8914040327072144 4.4992570877075195 4.4992570877075195
Loss :  1.8765695095062256 4.6877946853637695 4.6877946853637695
Loss :  1.8968583345413208 4.6828508377075195 4.6828508377075195
Loss :  1.8980395793914795 4.7169976234436035 4.7169976234436035
Loss :  1.928504467010498 4.5158371925354 4.5158371925354
Loss :  1.8861572742462158 4.515573024749756 4.515573024749756
Loss :  1.9274142980575562 4.378602027893066 4.378602027893066
Loss :  1.8964205980300903 4.517802715301514 4.517802715301514
Loss :  1.8809854984283447 4.65298318862915 4.65298318862915
Loss :  1.9050979614257812 4.575923442840576 4.575923442840576
Loss :  1.9094023704528809 4.496437072753906 4.496437072753906
Loss :  1.8922277688980103 4.473105430603027 4.473105430603027
Loss :  1.930951476097107 4.413384914398193 4.413384914398193
Loss :  1.8667229413986206 4.2696733474731445 4.2696733474731445
  batch 20 loss: 1.8667229413986206, 4.2696733474731445, 4.2696733474731445
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.925549030303955 4.539914131164551 4.539914131164551
Loss :  1.9457937479019165 4.29723596572876 4.29723596572876
Loss :  1.9057469367980957 4.953080654144287 4.953080654144287
Loss :  1.8543730974197388 4.485357284545898 4.485357284545898
Loss :  1.8133405447006226 4.5461530685424805 4.5461530685424805
Loss :  1.8411587476730347 4.646712779998779 4.646712779998779
Loss :  1.883415699005127 4.63713264465332 4.63713264465332
Loss :  1.9214626550674438 4.589365005493164 4.589365005493164
Loss :  1.9606519937515259 4.551773548126221 4.551773548126221
Loss :  1.9025144577026367 4.392698287963867 4.392698287963867
Loss :  1.9358022212982178 4.98805046081543 4.98805046081543
Loss :  1.9533860683441162 4.706763744354248 4.706763744354248
Loss :  1.9243186712265015 4.624269008636475 4.624269008636475
Loss :  1.983804702758789 4.518598556518555 4.518598556518555
Loss :  1.9369208812713623 4.621344089508057 4.621344089508057
Loss :  1.9449418783187866 4.551161289215088 4.551161289215088
Loss :  1.9204179048538208 4.64370584487915 4.64370584487915
Loss :  1.9154478311538696 4.423189163208008 4.423189163208008
Loss :  1.859379768371582 4.518477916717529 4.518477916717529
Loss :  1.880785584449768 4.573678493499756 4.573678493499756
  batch 40 loss: 1.880785584449768, 4.573678493499756, 4.573678493499756
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8660372495651245 4.761762619018555 4.761762619018555
Loss :  1.9737495183944702 4.64522123336792 4.64522123336792
Loss :  1.9220819473266602 4.872341632843018 4.872341632843018
Loss :  1.9106496572494507 4.553747653961182 4.553747653961182
Loss :  1.9305826425552368 4.516215801239014 4.516215801239014
Loss :  1.9061707258224487 4.529162406921387 4.529162406921387
Loss :  1.8999277353286743 4.583302021026611 4.583302021026611
Loss :  1.9761278629302979 4.618616580963135 4.618616580963135
Loss :  1.9304879903793335 4.5537309646606445 4.5537309646606445
Loss :  1.9546507596969604 4.533331394195557 4.533331394195557
Loss :  1.9445949792861938 4.625609874725342 4.625609874725342
Loss :  1.9725511074066162 4.391793727874756 4.391793727874756
Loss :  1.8979151248931885 4.574311256408691 4.574311256408691
Loss :  1.961819052696228 4.458471775054932 4.458471775054932
Loss :  1.8933144807815552 4.4672465324401855 4.4672465324401855
Loss :  1.9366477727890015 4.66973352432251 4.66973352432251
Loss :  1.8912779092788696 4.645678520202637 4.645678520202637
Loss :  1.8859575986862183 4.6123433113098145 4.6123433113098145
Loss :  1.9284453392028809 4.721001148223877 4.721001148223877
Loss :  1.9266347885131836 4.423299312591553 4.423299312591553
  batch 60 loss: 1.9266347885131836, 4.423299312591553, 4.423299312591553
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9229302406311035 4.555327415466309 4.555327415466309
Loss :  1.8343764543533325 4.568234443664551 4.568234443664551
Loss :  1.9467915296554565 4.731984615325928 4.731984615325928
Loss :  1.9302258491516113 4.664628982543945 4.664628982543945
Loss :  1.9276375770568848 4.3094258308410645 4.3094258308410645
Loss :  1.9481024742126465 4.241689205169678 4.241689205169678
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9773980379104614 4.462754726409912 4.462754726409912
Loss :  1.9078129529953003 4.313365459442139 4.313365459442139
Loss :  2.035454750061035 4.079174041748047 4.079174041748047
Total LOSS train 4.570394846109243 valid 4.274245858192444
CE LOSS train 1.908835163483253 valid 0.5088636875152588
Contrastive LOSS train 4.570394846109243 valid 1.0197935104370117
EPOCH 119:
Loss :  1.941691517829895 4.486910820007324 4.486910820007324
Loss :  1.8394540548324585 4.663695812225342 4.663695812225342
Loss :  1.9271595478057861 4.743440628051758 4.743440628051758
Loss :  1.930406928062439 4.481184482574463 4.481184482574463
Loss :  1.9792399406433105 4.467849254608154 4.467849254608154
Loss :  1.934519648551941 4.467033863067627 4.467033863067627
Loss :  1.8695759773254395 4.778233051300049 4.778233051300049
Loss :  1.846631646156311 4.6717705726623535 4.6717705726623535
Loss :  1.8701403141021729 4.705780982971191 4.705780982971191
Loss :  1.9684385061264038 4.828843116760254 4.828843116760254
Loss :  1.9319695234298706 4.276700496673584 4.276700496673584
Loss :  1.8958768844604492 4.917220592498779 4.917220592498779
Loss :  1.9297173023223877 4.463498592376709 4.463498592376709
Loss :  1.9335808753967285 4.367098331451416 4.367098331451416
Loss :  1.817831039428711 4.604300498962402 4.604300498962402
Loss :  1.8499946594238281 4.43218469619751 4.43218469619751
Loss :  1.9028823375701904 4.495228290557861 4.495228290557861
Loss :  1.8773701190948486 4.681422233581543 4.681422233581543
Loss :  1.913464069366455 4.370628356933594 4.370628356933594
Loss :  1.879219889640808 4.610013008117676 4.610013008117676
  batch 20 loss: 1.879219889640808, 4.610013008117676, 4.610013008117676
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8785886764526367 4.56094217300415 4.56094217300415
Loss :  1.9253170490264893 4.64949893951416 4.64949893951416
Loss :  1.8810865879058838 4.546815872192383 4.546815872192383
Loss :  1.9396286010742188 4.527220249176025 4.527220249176025
Loss :  1.8669886589050293 4.652891159057617 4.652891159057617
Loss :  1.8907760381698608 4.283915996551514 4.283915996551514
Loss :  1.8970491886138916 4.57777738571167 4.57777738571167
Loss :  1.9479037523269653 4.337500095367432 4.337500095367432
Loss :  1.933329701423645 4.376832962036133 4.376832962036133
Loss :  1.8434951305389404 4.528311729431152 4.528311729431152
Loss :  1.9167814254760742 4.433213710784912 4.433213710784912
Loss :  1.88226318359375 4.501168251037598 4.501168251037598
Loss :  1.8877277374267578 4.591812610626221 4.591812610626221
Loss :  1.8447682857513428 4.241082668304443 4.241082668304443
Loss :  1.880750060081482 4.22932243347168 4.22932243347168
Loss :  1.884631872177124 4.3012261390686035 4.3012261390686035
Loss :  1.884592890739441 4.395564556121826 4.395564556121826
Loss :  1.85812509059906 4.074765682220459 4.074765682220459
Loss :  1.881865382194519 4.272996425628662 4.272996425628662
Loss :  1.8760145902633667 4.3537445068359375 4.3537445068359375
  batch 40 loss: 1.8760145902633667, 4.3537445068359375, 4.3537445068359375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.880391001701355 4.431091785430908 4.431091785430908
Loss :  1.8579152822494507 4.329320430755615 4.329320430755615
Loss :  1.8359116315841675 4.274879455566406 4.274879455566406
Loss :  1.869196891784668 4.343216896057129 4.343216896057129
Loss :  1.8966957330703735 4.16776180267334 4.16776180267334
Loss :  1.8782813549041748 4.402927875518799 4.402927875518799
Loss :  1.892600417137146 4.710262775421143 4.710262775421143
Loss :  1.8771069049835205 4.321955680847168 4.321955680847168
Loss :  1.8799813985824585 4.304887771606445 4.304887771606445
Loss :  1.843154788017273 4.39826774597168 4.39826774597168
Loss :  1.846598744392395 4.198277473449707 4.198277473449707
Loss :  1.885944128036499 4.129449844360352 4.129449844360352
Loss :  1.912306785583496 4.164093494415283 4.164093494415283
Loss :  1.8660664558410645 3.9394078254699707 3.9394078254699707
Loss :  1.8556733131408691 3.9887726306915283 3.9887726306915283
Loss :  1.8810845613479614 3.9028408527374268 3.9028408527374268
Loss :  1.8981393575668335 3.829651117324829 3.829651117324829
Loss :  1.8720778226852417 3.8496954441070557 3.8496954441070557
Loss :  1.9087377786636353 4.289826393127441 4.289826393127441
Loss :  1.8817145824432373 4.120846748352051 4.120846748352051
  batch 60 loss: 1.8817145824432373, 4.120846748352051, 4.120846748352051
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9030461311340332 4.1403703689575195 4.1403703689575195
Loss :  1.8600306510925293 3.825244426727295 3.825244426727295
Loss :  1.8790276050567627 3.858513593673706 3.858513593673706
Loss :  1.881966471672058 3.85221266746521 3.85221266746521
Loss :  1.8705945014953613 3.61295747756958 3.61295747756958
Loss :  1.8025120496749878 4.307861328125 4.307861328125
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8159148693084717 4.374791145324707 4.374791145324707
Loss :  1.8173218965530396 4.025807857513428 4.025807857513428
Loss :  1.8168753385543823 4.0461812019348145 4.0461812019348145
Total LOSS train 4.35902113547692 valid 4.188660383224487
CE LOSS train 1.8885398919765766 valid 0.4542188346385956
Contrastive LOSS train 4.35902113547692 valid 1.0115453004837036
EPOCH 120:
Loss :  1.8640697002410889 3.4480538368225098 3.4480538368225098
Loss :  1.892105221748352 4.103419303894043 4.103419303894043
Loss :  1.8743456602096558 4.064235687255859 4.064235687255859
Loss :  1.8512365818023682 3.9125120639801025 3.9125120639801025
Loss :  1.8656954765319824 3.979304790496826 3.979304790496826
Loss :  1.875620722770691 4.062082290649414 4.062082290649414
Loss :  1.8753893375396729 3.5374515056610107 3.5374515056610107
Loss :  1.882448434829712 3.693228006362915 3.693228006362915
Loss :  1.8306598663330078 3.915555000305176 3.915555000305176
Loss :  1.8442268371582031 3.6946706771850586 3.6946706771850586
Loss :  1.871667742729187 3.5335161685943604 3.5335161685943604
Loss :  1.8844571113586426 3.7776315212249756 3.7776315212249756
Loss :  1.8744882345199585 3.7630701065063477 3.7630701065063477
Loss :  1.8797259330749512 3.6553335189819336 3.6553335189819336
Loss :  1.8312981128692627 3.8290421962738037 3.8290421962738037
Loss :  1.86917245388031 3.7717158794403076 3.7717158794403076
Loss :  1.8591996431350708 4.039679527282715 4.039679527282715
Loss :  1.8590586185455322 3.753844738006592 3.753844738006592
Loss :  1.8475072383880615 3.248572826385498 3.248572826385498
Loss :  1.83769953250885 3.8189666271209717 3.8189666271209717
  batch 20 loss: 1.83769953250885, 3.8189666271209717, 3.8189666271209717
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3, 4, 5], device='cuda:0')
Loss :  1.862703800201416 3.3901073932647705 3.3901073932647705
Loss :  1.8688827753067017 3.5400283336639404 3.5400283336639404
Loss :  1.8577454090118408 3.587246894836426 3.587246894836426
Loss :  1.900743842124939 3.88720440864563 3.88720440864563
Loss :  1.8497450351715088 4.041074275970459 4.041074275970459
Loss :  1.8639150857925415 3.4178390502929688 3.4178390502929688
Loss :  1.8855329751968384 3.7997794151306152 3.7997794151306152
Loss :  1.8553317785263062 3.8070614337921143 3.8070614337921143
Loss :  1.8988398313522339 3.7929062843322754 3.7929062843322754
Loss :  1.8314512968063354 3.8543903827667236 3.8543903827667236
Loss :  1.864789605140686 3.791133165359497 3.791133165359497
Loss :  1.838618516921997 3.7517025470733643 3.7517025470733643
Loss :  1.853957176208496 3.685717821121216 3.685717821121216
Loss :  1.8735240697860718 3.965203285217285 3.965203285217285
Loss :  1.8465240001678467 3.9694321155548096 3.9694321155548096
Loss :  1.871649146080017 4.153046131134033 4.153046131134033
Loss :  1.8637633323669434 4.347259998321533 4.347259998321533
Loss :  1.8446301221847534 3.9671199321746826 3.9671199321746826
Loss :  1.8672199249267578 3.887197256088257 3.887197256088257
Loss :  1.8510221242904663 3.7296955585479736 3.7296955585479736
  batch 40 loss: 1.8510221242904663, 3.7296955585479736, 3.7296955585479736
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8549436330795288 3.862342596054077 3.862342596054077
Loss :  1.8326927423477173 3.855851650238037 3.855851650238037
Loss :  1.832192301750183 3.73738956451416 3.73738956451416
Loss :  1.8372628688812256 3.729177236557007 3.729177236557007
Loss :  1.8439428806304932 3.9750826358795166 3.9750826358795166
Loss :  1.8494195938110352 3.9292471408843994 3.9292471408843994
Loss :  1.8395556211471558 3.7148325443267822 3.7148325443267822
Loss :  1.8301429748535156 3.946835994720459 3.946835994720459
Loss :  1.85185968875885 3.369529962539673 3.369529962539673
Loss :  1.82588791847229 3.6790060997009277 3.6790060997009277
Loss :  1.8246392011642456 3.775115728378296 3.775115728378296
Loss :  1.8733205795288086 3.679440975189209 3.679440975189209
Loss :  1.8795480728149414 3.5305025577545166 3.5305025577545166
Loss :  1.8488385677337646 3.3314974308013916 3.3314974308013916
Loss :  1.838283896446228 3.731431245803833 3.731431245803833
Loss :  1.86252760887146 3.709211587905884 3.709211587905884
Loss :  1.8680298328399658 3.790308713912964 3.790308713912964
Loss :  1.830057144165039 3.4800193309783936 3.4800193309783936
Loss :  1.8749808073043823 3.8314712047576904 3.8314712047576904
Loss :  1.8338590860366821 3.547116279602051 3.547116279602051
  batch 60 loss: 1.8338590860366821, 3.547116279602051, 3.547116279602051
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8612339496612549 3.5936052799224854 3.5936052799224854
Loss :  1.8193471431732178 3.5538275241851807 3.5538275241851807
Loss :  1.8428162336349487 3.3153295516967773 3.3153295516967773
Loss :  1.84431791305542 3.4841384887695312 3.4841384887695312
Loss :  1.8366055488586426 2.9967563152313232 2.9967563152313232
Loss :  1.7206569910049438 4.0103960037231445 4.0103960037231445
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4, 5], device='cuda:0')
Loss :  1.730016827583313 4.1883111000061035 4.1883111000061035
Loss :  1.7396985292434692 3.9540646076202393 3.9540646076202393
Loss :  1.731289029121399 3.865328311920166 3.865328311920166
Total LOSS train 3.7402476860926703 valid 4.004525005817413
CE LOSS train 1.8562610479501578 valid 0.43282225728034973
Contrastive LOSS train 3.7402476860926703 valid 0.9663320779800415
EPOCH 121:
Loss :  1.8308403491973877 3.542283058166504 3.542283058166504
Loss :  1.8563116788864136 3.693643808364868 3.693643808364868
Loss :  1.8382281064987183 3.866903066635132 3.866903066635132
Loss :  1.8290412425994873 3.7607645988464355 3.7607645988464355
Loss :  1.8354812860488892 3.60288143157959 3.60288143157959
Loss :  1.851287603378296 3.5934112071990967 3.5934112071990967
Loss :  1.845349907875061 3.5958938598632812 3.5958938598632812
Loss :  1.8564295768737793 3.47426438331604 3.47426438331604
Loss :  1.816774845123291 3.2944371700286865 3.2944371700286865
Loss :  1.8164154291152954 3.8478453159332275 3.8478453159332275
Loss :  1.8518133163452148 3.9782660007476807 3.9782660007476807
Loss :  1.8688369989395142 3.970395088195801 3.970395088195801
Loss :  1.8499724864959717 3.7043910026550293 3.7043910026550293
Loss :  1.8551329374313354 3.5777792930603027 3.5777792930603027
Loss :  1.8154834508895874 3.489091157913208 3.489091157913208
Loss :  1.8407708406448364 3.597252607345581 3.597252607345581
Loss :  1.8379632234573364 3.5849106311798096 3.5849106311798096
Loss :  1.8323553800582886 3.4180665016174316 3.4180665016174316
Loss :  1.824529767036438 3.240640163421631 3.240640163421631
Loss :  1.8216017484664917 3.484462261199951 3.484462261199951
  batch 20 loss: 1.8216017484664917, 3.484462261199951, 3.484462261199951
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8387749195098877 3.6285922527313232 3.6285922527313232
Loss :  1.8425228595733643 3.5270156860351562 3.5270156860351562
Loss :  1.8416106700897217 3.5277326107025146 3.5277326107025146
Loss :  1.8841900825500488 3.2701590061187744 3.2701590061187744
Loss :  1.835270643234253 3.6032633781433105 3.6032633781433105
Loss :  1.8409475088119507 3.227612257003784 3.227612257003784
Loss :  1.8536015748977661 3.541102647781372 3.541102647781372
Loss :  1.8280601501464844 3.697882652282715 3.697882652282715
Loss :  1.8707119226455688 3.778442144393921 3.778442144393921
Loss :  1.7997817993164062 3.437232494354248 3.437232494354248
Loss :  1.8449212312698364 3.4732136726379395 3.4732136726379395
Loss :  1.812027931213379 3.6220626831054688 3.6220626831054688
Loss :  1.8282216787338257 3.275864601135254 3.275864601135254
Loss :  1.852687120437622 3.442383050918579 3.442383050918579
Loss :  1.8334362506866455 3.7778141498565674 3.7778141498565674
Loss :  1.8545105457305908 3.757280111312866 3.757280111312866
Loss :  1.8434529304504395 3.342484951019287 3.342484951019287
Loss :  1.8128756284713745 3.1638524532318115 3.1638524532318115
Loss :  1.8434370756149292 3.469677448272705 3.469677448272705
Loss :  1.8264089822769165 3.212980031967163 3.212980031967163
  batch 40 loss: 1.8264089822769165, 3.212980031967163, 3.212980031967163
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8309593200683594 3.5935308933258057 3.5935308933258057
Loss :  1.8089205026626587 3.3516738414764404 3.3516738414764404
Loss :  1.7994821071624756 3.549783945083618 3.549783945083618
Loss :  1.8057628870010376 3.803053140640259 3.803053140640259
Loss :  1.8164266347885132 3.6755621433258057 3.6755621433258057
Loss :  1.8200373649597168 3.846066474914551 3.846066474914551
Loss :  1.8255411386489868 3.992385149002075 3.992385149002075
Loss :  1.8088183403015137 3.8495662212371826 3.8495662212371826
Loss :  1.8246991634368896 3.635986328125 3.635986328125
Loss :  1.7919552326202393 3.5624172687530518 3.5624172687530518
Loss :  1.7929655313491821 3.7402775287628174 3.7402775287628174
Loss :  1.83864164352417 3.4250447750091553 3.4250447750091553
Loss :  1.8494548797607422 3.4661686420440674 3.4661686420440674
Loss :  1.8217670917510986 3.312673330307007 3.312673330307007
Loss :  1.8141685724258423 3.3222782611846924 3.3222782611846924
Loss :  1.838403344154358 3.245288848876953 3.245288848876953
Loss :  1.8492717742919922 3.4617505073547363 3.4617505073547363
Loss :  1.8121410608291626 3.3780646324157715 3.3780646324157715
Loss :  1.8576925992965698 3.6539740562438965 3.6539740562438965
Loss :  1.836484670639038 3.892695188522339 3.892695188522339
  batch 60 loss: 1.836484670639038, 3.892695188522339, 3.892695188522339
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
Loss :  1.847198247909546 3.2656257152557373 3.2656257152557373
Loss :  1.815812349319458 3.351743221282959 3.351743221282959
Loss :  1.826106071472168 3.717465400695801 3.717465400695801
Loss :  1.832854986190796 3.374511241912842 3.374511241912842
Loss :  1.8179371356964111 3.0391995906829834 3.0391995906829834
Loss :  1.734011173248291 4.140308856964111 4.140308856964111
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.7489724159240723 4.125753879547119 4.125753879547119
Loss :  1.7587523460388184 3.9526047706604004 3.9526047706604004
Loss :  1.7569619417190552 3.8728573322296143 3.8728573322296143
Total LOSS train 3.54770795748784 valid 4.022881209850311
CE LOSS train 1.8330088358659011 valid 0.4392404854297638
Contrastive LOSS train 3.54770795748784 valid 0.9682143330574036
EPOCH 122:
Loss :  1.8148833513259888 3.15724778175354 3.15724778175354
Loss :  1.8428544998168945 3.6035425662994385 3.6035425662994385
Loss :  1.8248696327209473 3.2738635540008545 3.2738635540008545
Loss :  1.8138707876205444 3.651392698287964 3.651392698287964
Loss :  1.8233894109725952 3.6025538444519043 3.6025538444519043
Loss :  1.8404219150543213 3.6426947116851807 3.6426947116851807
Loss :  1.8380777835845947 3.5808768272399902 3.5808768272399902
Loss :  1.848111629486084 3.883060932159424 3.883060932159424
Loss :  1.8023414611816406 3.570000410079956 3.570000410079956
Loss :  1.8042596578598022 3.243021249771118 3.243021249771118
Loss :  1.8324335813522339 3.260559558868408 3.260559558868408
Loss :  1.8554372787475586 3.3265511989593506 3.3265511989593506
Loss :  1.8315744400024414 3.4673233032226562 3.4673233032226562
Loss :  1.837809681892395 3.4955403804779053 3.4955403804779053
Loss :  1.7995084524154663 3.688883066177368 3.688883066177368
Loss :  1.8341960906982422 3.658583879470825 3.658583879470825
Loss :  1.8288002014160156 3.684506416320801 3.684506416320801
Loss :  1.821650505065918 3.398876667022705 3.398876667022705
Loss :  1.8072805404663086 3.1873672008514404 3.1873672008514404
Loss :  1.8113949298858643 3.5137219429016113 3.5137219429016113
  batch 20 loss: 1.8113949298858643, 3.5137219429016113, 3.5137219429016113
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8155924081802368 3.124631404876709 3.124631404876709
Loss :  1.8320106267929077 3.3228368759155273 3.3228368759155273
Loss :  1.8255788087844849 3.3113174438476562 3.3113174438476562
Loss :  1.8553515672683716 3.2675156593322754 3.2675156593322754
Loss :  1.8165100812911987 3.4630002975463867 3.4630002975463867
Loss :  1.8293172121047974 3.3799822330474854 3.3799822330474854
Loss :  1.8504334688186646 3.3901853561401367 3.3901853561401367
Loss :  1.8185653686523438 3.5262856483459473 3.5262856483459473
Loss :  1.8635594844818115 3.512326955795288 3.512326955795288
Loss :  1.792300820350647 3.3197226524353027 3.3197226524353027
Loss :  1.8356759548187256 3.745069742202759 3.745069742202759
Loss :  1.797399640083313 3.467960834503174 3.467960834503174
Loss :  1.8113538026809692 3.5382561683654785 3.5382561683654785
Loss :  1.8312711715698242 3.2429983615875244 3.2429983615875244
Loss :  1.815291166305542 3.369337558746338 3.369337558746338
Loss :  1.8328152894973755 3.604205846786499 3.604205846786499
Loss :  1.8226889371871948 3.581617832183838 3.581617832183838
Loss :  1.7773463726043701 3.4293713569641113 3.4293713569641113
Loss :  1.8213857412338257 3.67525315284729 3.67525315284729
Loss :  1.8063887357711792 3.242960214614868 3.242960214614868
  batch 40 loss: 1.8063887357711792, 3.242960214614868, 3.242960214614868
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8138716220855713 3.3081283569335938 3.3081283569335938
Loss :  1.7959768772125244 3.194446325302124 3.194446325302124
Loss :  1.784518837928772 3.2206194400787354 3.2206194400787354
Loss :  1.799948811531067 3.4384732246398926 3.4384732246398926
Loss :  1.8000291585922241 3.5444397926330566 3.5444397926330566
Loss :  1.8094470500946045 3.5492286682128906 3.5492286682128906
Loss :  1.7994002103805542 3.92352557182312 3.92352557182312
Loss :  1.776774525642395 3.4848105907440186 3.4848105907440186
Loss :  1.798413872718811 3.3364434242248535 3.3364434242248535
Loss :  1.7876405715942383 3.186262607574463 3.186262607574463
Loss :  1.7827454805374146 3.363650321960449 3.363650321960449
Loss :  1.827699899673462 3.163909435272217 3.163909435272217
Loss :  1.8369572162628174 3.171071767807007 3.171071767807007
Loss :  1.8171024322509766 3.3460066318511963 3.3460066318511963
Loss :  1.8069318532943726 3.3132286071777344 3.3132286071777344
Loss :  1.8254433870315552 3.278083324432373 3.278083324432373
Loss :  1.8392478227615356 3.396271228790283 3.396271228790283
Loss :  1.804352045059204 3.346444845199585 3.346444845199585
Loss :  1.851904034614563 3.6978139877319336 3.6978139877319336
Loss :  1.82314932346344 3.3653321266174316 3.3653321266174316
  batch 60 loss: 1.82314932346344, 3.3653321266174316, 3.3653321266174316
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8349757194519043 3.603209972381592 3.603209972381592
Loss :  1.799182653427124 3.7186458110809326 3.7186458110809326
Loss :  1.8285856246948242 4.057030200958252 4.057030200958252
Loss :  1.8185222148895264 3.4227588176727295 3.4227588176727295
Loss :  1.8097341060638428 2.8173587322235107 2.8173587322235107
Loss :  1.6621989011764526 4.341116428375244 4.341116428375244
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.675950288772583 4.260879039764404 4.260879039764404
Loss :  1.672039270401001 4.091233730316162 4.091233730316162
Loss :  1.6757843494415283 3.880276918411255 3.880276918411255
Total LOSS train 3.440803039990939 valid 4.143376529216766
CE LOSS train 1.8190239667892456 valid 0.4189460873603821
Contrastive LOSS train 3.440803039990939 valid 0.9700692296028137
EPOCH 123:
Loss :  1.8021831512451172 3.2757697105407715 3.2757697105407715
Loss :  1.8316432237625122 3.653975248336792 3.653975248336792
Loss :  1.817065954208374 3.291802167892456 3.291802167892456
Loss :  1.8075127601623535 3.388653039932251 3.388653039932251
Loss :  1.810030460357666 3.55123233795166 3.55123233795166
Loss :  1.8322970867156982 3.5288052558898926 3.5288052558898926
Loss :  1.814538598060608 3.28000545501709 3.28000545501709
Loss :  1.8232498168945312 3.2403533458709717 3.2403533458709717
Loss :  1.7866019010543823 3.240168333053589 3.240168333053589
Loss :  1.7837400436401367 3.2808303833007812 3.2808303833007812
Loss :  1.8172130584716797 3.484812021255493 3.484812021255493
Loss :  1.8456319570541382 3.57299542427063 3.57299542427063
Loss :  1.8221732378005981 3.38580322265625 3.38580322265625
Loss :  1.8268767595291138 3.3554940223693848 3.3554940223693848
Loss :  1.7934041023254395 3.3394172191619873 3.3394172191619873
Loss :  1.806347370147705 3.459012746810913 3.459012746810913
Loss :  1.8146462440490723 3.3595073223114014 3.3595073223114014
Loss :  1.8076938390731812 3.4967150688171387 3.4967150688171387
Loss :  1.8016701936721802 3.1669726371765137 3.1669726371765137
Loss :  1.8002575635910034 3.2113749980926514 3.2113749980926514
  batch 20 loss: 1.8002575635910034, 3.2113749980926514, 3.2113749980926514
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8100167512893677 3.1804847717285156 3.1804847717285156
Loss :  1.8226696252822876 3.3843400478363037 3.3843400478363037
Loss :  1.8200451135635376 3.2161223888397217 3.2161223888397217
Loss :  1.858562707901001 3.1433441638946533 3.1433441638946533
Loss :  1.8224419355392456 3.9052176475524902 3.9052176475524902
Loss :  1.8352224826812744 3.532625913619995 3.532625913619995
Loss :  1.8428924083709717 3.4594130516052246 3.4594130516052246
Loss :  1.8139764070510864 3.4354498386383057 3.4354498386383057
Loss :  1.8454713821411133 3.4376068115234375 3.4376068115234375
Loss :  1.7920900583267212 3.581367015838623 3.581367015838623
Loss :  1.8333888053894043 3.4282302856445312 3.4282302856445312
Loss :  1.7967805862426758 3.356677293777466 3.356677293777466
Loss :  1.812263011932373 3.31482195854187 3.31482195854187
Loss :  1.8166203498840332 3.3786535263061523 3.3786535263061523
Loss :  1.813271403312683 3.5564687252044678 3.5564687252044678
Loss :  1.816084623336792 3.465117931365967 3.465117931365967
Loss :  1.8278380632400513 3.2776591777801514 3.2776591777801514
Loss :  1.7740980386734009 2.7556588649749756 2.7556588649749756
Loss :  1.8214006423950195 3.1092946529388428 3.1092946529388428
Loss :  1.8103195428848267 3.2142724990844727 3.2142724990844727
  batch 40 loss: 1.8103195428848267, 3.2142724990844727, 3.2142724990844727
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.81438148021698 3.2581605911254883 3.2581605911254883
Loss :  1.7912628650665283 3.3295786380767822 3.3295786380767822
Loss :  1.7932535409927368 3.288191795349121 3.288191795349121
Loss :  1.7978239059448242 3.5319018363952637 3.5319018363952637
Loss :  1.8020288944244385 3.740021228790283 3.740021228790283
Loss :  1.8053631782531738 3.4114606380462646 3.4114606380462646
Loss :  1.8050650358200073 3.984483242034912 3.984483242034912
Loss :  1.7868587970733643 3.6131439208984375 3.6131439208984375
Loss :  1.8038341999053955 3.521848201751709 3.521848201751709
Loss :  1.7916725873947144 3.219054937362671 3.219054937362671
Loss :  1.7850673198699951 3.4107487201690674 3.4107487201690674
Loss :  1.8325053453445435 3.121157169342041 3.121157169342041
Loss :  1.8368951082229614 3.723799467086792 3.723799467086792
Loss :  1.8223156929016113 3.238999843597412 3.238999843597412
Loss :  1.8026683330535889 3.277979850769043 3.277979850769043
Loss :  1.8289601802825928 3.544766902923584 3.544766902923584
Loss :  1.8478803634643555 3.6808407306671143 3.6808407306671143
Loss :  1.8132661581039429 3.7747557163238525 3.7747557163238525
Loss :  1.8521639108657837 3.8119473457336426 3.8119473457336426
Loss :  1.8241428136825562 4.001892566680908 4.001892566680908
  batch 60 loss: 1.8241428136825562, 4.001892566680908, 4.001892566680908
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.846390724182129 3.8448777198791504 3.8448777198791504
Loss :  1.8114043474197388 3.8839125633239746 3.8839125633239746
Loss :  1.8276289701461792 3.698219060897827 3.698219060897827
Loss :  1.8298217058181763 3.910182237625122 3.910182237625122
Loss :  1.8202242851257324 3.2092700004577637 3.2092700004577637
Loss :  1.7356951236724854 4.030428886413574 4.030428886413574
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7357559204101562 4.029122829437256 4.029122829437256
Loss :  1.7510567903518677 4.0640764236450195 4.0640764236450195
Loss :  1.735764741897583 3.822240114212036 3.822240114212036
Total LOSS train 3.441964945426354 valid 3.9864670634269714
CE LOSS train 1.8154324769973755 valid 0.43394118547439575
Contrastive LOSS train 3.441964945426354 valid 0.955560028553009
EPOCH 124:
Loss :  1.8085070848464966 3.0002455711364746 3.0002455711364746
Loss :  1.8438793420791626 3.4287500381469727 3.4287500381469727
Loss :  1.8241873979568481 3.410892963409424 3.410892963409424
Loss :  1.8133803606033325 3.259042263031006 3.259042263031006
Loss :  1.8180162906646729 3.2332541942596436 3.2332541942596436
Loss :  1.8409016132354736 3.403838872909546 3.403838872909546
Loss :  1.8311047554016113 3.485675573348999 3.485675573348999
Loss :  1.8358594179153442 3.2865030765533447 3.2865030765533447
Loss :  1.7930092811584473 3.31577730178833 3.31577730178833
Loss :  1.8001593351364136 2.8350026607513428 2.8350026607513428
Loss :  1.8256700038909912 3.3592824935913086 3.3592824935913086
Loss :  1.8548494577407837 3.475496768951416 3.475496768951416
Loss :  1.831368327140808 3.3005294799804688 3.3005294799804688
Loss :  1.8282561302185059 3.673518657684326 3.673518657684326
Loss :  1.786999225616455 3.512234926223755 3.512234926223755
Loss :  1.8270108699798584 3.277214765548706 3.277214765548706
Loss :  1.8265690803527832 3.303755283355713 3.303755283355713
Loss :  1.8212276697158813 3.256439208984375 3.256439208984375
Loss :  1.809233546257019 3.151817560195923 3.151817560195923
Loss :  1.8122197389602661 3.4770405292510986 3.4770405292510986
  batch 20 loss: 1.8122197389602661, 3.4770405292510986, 3.4770405292510986
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8154106140136719 3.364436388015747 3.364436388015747
Loss :  1.8343496322631836 3.3903543949127197 3.3903543949127197
Loss :  1.8337973356246948 3.3191094398498535 3.3191094398498535
Loss :  1.8620043992996216 3.1300857067108154 3.1300857067108154
Loss :  1.824653148651123 3.6510655879974365 3.6510655879974365
Loss :  1.8402438163757324 3.307668924331665 3.307668924331665
Loss :  1.8519667387008667 3.4812123775482178 3.4812123775482178
Loss :  1.82163405418396 3.3071579933166504 3.3071579933166504
Loss :  1.854863166809082 3.404174566268921 3.404174566268921
Loss :  1.8023746013641357 3.2804341316223145 3.2804341316223145
Loss :  1.8401271104812622 3.399327278137207 3.399327278137207
Loss :  1.8076809644699097 3.26214337348938 3.26214337348938
Loss :  1.8203532695770264 3.120250701904297 3.120250701904297
Loss :  1.8181381225585938 3.2401177883148193 3.2401177883148193
Loss :  1.819336175918579 3.2772152423858643 3.2772152423858643
Loss :  1.8187443017959595 3.317621946334839 3.317621946334839
Loss :  1.8350541591644287 3.3495218753814697 3.3495218753814697
Loss :  1.7776113748550415 3.1339800357818604 3.1339800357818604
Loss :  1.8268648386001587 3.28202748298645 3.28202748298645
Loss :  1.8212943077087402 3.1190643310546875 3.1190643310546875
  batch 40 loss: 1.8212943077087402, 3.1190643310546875, 3.1190643310546875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8219475746154785 3.390639066696167 3.390639066696167
Loss :  1.8045004606246948 3.160165548324585 3.160165548324585
Loss :  1.7945868968963623 3.256030321121216 3.256030321121216
Loss :  1.8138132095336914 3.3921144008636475 3.3921144008636475
Loss :  1.8050142526626587 3.25754976272583 3.25754976272583
Loss :  1.8197180032730103 3.4844446182250977 3.4844446182250977
Loss :  1.813763976097107 3.535386800765991 3.535386800765991
Loss :  1.7892590761184692 3.6514172554016113 3.6514172554016113
Loss :  1.8062011003494263 3.7104878425598145 3.7104878425598145
Loss :  1.8018782138824463 3.4095799922943115 3.4095799922943115
Loss :  1.7875864505767822 3.5648140907287598 3.5648140907287598
Loss :  1.819940209388733 3.118779420852661 3.118779420852661
Loss :  1.8364235162734985 3.426784038543701 3.426784038543701
Loss :  1.8345354795455933 3.2038514614105225 3.2038514614105225
Loss :  1.8139058351516724 3.2916061878204346 3.2916061878204346
Loss :  1.8356012105941772 3.217005491256714 3.217005491256714
Loss :  1.8502428531646729 3.5514674186706543 3.5514674186706543
Loss :  1.813905954360962 3.4159226417541504 3.4159226417541504
Loss :  1.8516255617141724 3.6065850257873535 3.6065850257873535
Loss :  1.8129950761795044 3.4177095890045166 3.4177095890045166
  batch 60 loss: 1.8129950761795044, 3.4177095890045166, 3.4177095890045166
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.83006751537323 3.0631513595581055 3.0631513595581055
Loss :  1.8037643432617188 3.300529956817627 3.300529956817627
Loss :  1.828179955482483 3.0947046279907227 3.0947046279907227
Loss :  1.8239575624465942 3.2156753540039062 3.2156753540039062
Loss :  1.8223350048065186 2.8498799800872803 2.8498799800872803
Loss :  1.6553038358688354 4.268827438354492 4.268827438354492
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 3, 4, 5], device='cuda:0')
Loss :  1.6658750772476196 4.338162422180176 4.338162422180176
Loss :  1.664535641670227 4.271939754486084 4.271939754486084
Loss :  1.6588541269302368 4.0158162117004395 4.0158162117004395
Total LOSS train 3.3256856001340425 valid 4.223686456680298
CE LOSS train 1.8214563131332397 valid 0.4147135317325592
Contrastive LOSS train 3.3256856001340425 valid 1.0039540529251099
EPOCH 125:
Loss :  1.8141223192214966 3.447756052017212 3.447756052017212
Loss :  1.8308380842208862 3.6652982234954834 3.6652982234954834
Loss :  1.8263112306594849 3.2540316581726074 3.2540316581726074
Loss :  1.8314851522445679 3.463000774383545 3.463000774383545
Loss :  1.8338584899902344 3.3995139598846436 3.3995139598846436
Loss :  1.8537858724594116 3.6059505939483643 3.6059505939483643
Loss :  1.8303440809249878 3.354104518890381 3.354104518890381
Loss :  1.843186855316162 3.6481993198394775 3.6481993198394775
Loss :  1.80777907371521 3.327408790588379 3.327408790588379
Loss :  1.8166464567184448 3.5676190853118896 3.5676190853118896
Loss :  1.835123896598816 3.6331331729888916 3.6331331729888916
Loss :  1.8554294109344482 3.2913148403167725 3.2913148403167725
Loss :  1.8374192714691162 3.4527132511138916 3.4527132511138916
Loss :  1.840484380722046 3.1249377727508545 3.1249377727508545
Loss :  1.8153384923934937 3.5336108207702637 3.5336108207702637
Loss :  1.836928367614746 3.5750324726104736 3.5750324726104736
Loss :  1.8308027982711792 3.65966534614563 3.65966534614563
Loss :  1.8289577960968018 3.266669273376465 3.266669273376465
Loss :  1.8175568580627441 3.323218584060669 3.323218584060669
Loss :  1.8168541193008423 3.365666151046753 3.365666151046753
  batch 20 loss: 1.8168541193008423, 3.365666151046753, 3.365666151046753
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.828837275505066 3.4166529178619385 3.4166529178619385
Loss :  1.8388659954071045 3.5080182552337646 3.5080182552337646
Loss :  1.8454093933105469 3.3972818851470947 3.3972818851470947
Loss :  1.878054141998291 3.3041841983795166 3.3041841983795166
Loss :  1.8348814249038696 3.389295816421509 3.389295816421509
Loss :  1.8446067571640015 3.3151955604553223 3.3151955604553223
Loss :  1.853410005569458 3.4185564517974854 3.4185564517974854
Loss :  1.82529616355896 3.1563963890075684 3.1563963890075684
Loss :  1.8791239261627197 3.4090282917022705 3.4090282917022705
Loss :  1.8147205114364624 3.3396594524383545 3.3396594524383545
Loss :  1.8446950912475586 3.351717710494995 3.351717710494995
Loss :  1.8179833889007568 3.4017374515533447 3.4017374515533447
Loss :  1.8345286846160889 3.2855169773101807 3.2855169773101807
Loss :  1.8777399063110352 3.29170298576355 3.29170298576355
Loss :  1.8426090478897095 3.22879695892334 3.22879695892334
Loss :  1.8733294010162354 3.495701551437378 3.495701551437378
Loss :  1.8538075685501099 3.2500815391540527 3.2500815391540527
Loss :  1.8625413179397583 3.512192726135254 3.512192726135254
Loss :  1.8660396337509155 3.25830078125 3.25830078125
Loss :  1.8465049266815186 3.963536024093628 3.963536024093628
  batch 40 loss: 1.8465049266815186, 3.963536024093628, 3.963536024093628
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8574919700622559 3.849449872970581 3.849449872970581
Loss :  1.8380318880081177 3.4251840114593506 3.4251840114593506
Loss :  1.8249132633209229 3.4249660968780518 3.4249660968780518
Loss :  1.8121238946914673 3.2205991744995117 3.2205991744995117
Loss :  1.8462449312210083 3.2695631980895996 3.2695631980895996
Loss :  1.8431633710861206 3.368708848953247 3.368708848953247
Loss :  1.8441779613494873 3.164790391921997 3.164790391921997
Loss :  1.8226039409637451 3.311347007751465 3.311347007751465
Loss :  1.833235740661621 3.0455517768859863 3.0455517768859863
Loss :  1.7967827320098877 3.405022382736206 3.405022382736206
Loss :  1.7960255146026611 3.480480432510376 3.480480432510376
Loss :  1.835860013961792 3.378384590148926 3.378384590148926
Loss :  1.8491525650024414 3.3145952224731445 3.3145952224731445
Loss :  1.8171820640563965 3.122300148010254 3.122300148010254
Loss :  1.8123853206634521 3.4819483757019043 3.4819483757019043
Loss :  1.8481606245040894 3.5173537731170654 3.5173537731170654
Loss :  1.8504215478897095 3.282411813735962 3.282411813735962
Loss :  1.8354215621948242 3.1090800762176514 3.1090800762176514
Loss :  1.864432454109192 3.4384984970092773 3.4384984970092773
Loss :  1.848919153213501 3.2845001220703125 3.2845001220703125
  batch 60 loss: 1.848919153213501, 3.2845001220703125, 3.2845001220703125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8521244525909424 3.036045789718628 3.036045789718628
Loss :  1.8327308893203735 3.442209005355835 3.442209005355835
Loss :  1.8253084421157837 3.5474085807800293 3.5474085807800293
Loss :  1.8290759325027466 3.3786816596984863 3.3786816596984863
Loss :  1.8230310678482056 3.146796703338623 3.146796703338623
Loss :  1.738364338874817 4.128874778747559 4.128874778747559
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.7417356967926025 4.132078647613525 4.132078647613525
Loss :  1.7548775672912598 4.088441848754883 4.088441848754883
Loss :  1.738527774810791 3.7991530895233154 3.7991530895233154
Total LOSS train 3.3861273252047024 valid 4.037137091159821
CE LOSS train 1.837003674873939 valid 0.43463194370269775
Contrastive LOSS train 3.3861273252047024 valid 0.9497882723808289
EPOCH 126:
Loss :  1.8244590759277344 3.191094398498535 3.191094398498535
Loss :  1.8311399221420288 3.9351398944854736 3.9351398944854736
Loss :  1.8178367614746094 3.3649985790252686 3.3649985790252686
Loss :  1.812070608139038 3.060314178466797 3.060314178466797
Loss :  1.8197531700134277 3.4964184761047363 3.4964184761047363
Loss :  1.8349472284317017 3.4580869674682617 3.4580869674682617
Loss :  1.8197212219238281 3.0341060161590576 3.0341060161590576
Loss :  1.8284283876419067 2.829719305038452 2.829719305038452
Loss :  1.7999686002731323 3.3255648612976074 3.3255648612976074
Loss :  1.7871527671813965 3.0465452671051025 3.0465452671051025
Loss :  1.8178937435150146 3.360142230987549 3.360142230987549
Loss :  1.8418214321136475 3.6722569465637207 3.6722569465637207
Loss :  1.8159358501434326 3.4690887928009033 3.4690887928009033
Loss :  1.8292248249053955 3.3670849800109863 3.3670849800109863
Loss :  1.8120322227478027 3.4970486164093018 3.4970486164093018
Loss :  1.8179422616958618 3.5859642028808594 3.5859642028808594
Loss :  1.8158535957336426 3.5531551837921143 3.5531551837921143
Loss :  1.800529956817627 3.3424713611602783 3.3424713611602783
Loss :  1.8003910779953003 2.8361198902130127 2.8361198902130127
Loss :  1.8021818399429321 3.295970916748047 3.295970916748047
  batch 20 loss: 1.8021818399429321, 3.295970916748047, 3.295970916748047
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.815261960029602 3.258500099182129 3.258500099182129
Loss :  1.825451374053955 3.282348394393921 3.282348394393921
Loss :  1.8272356986999512 3.208336114883423 3.208336114883423
Loss :  1.8624213933944702 3.10776424407959 3.10776424407959
Loss :  1.826018214225769 3.5134849548339844 3.5134849548339844
Loss :  1.8300683498382568 3.0263664722442627 3.0263664722442627
Loss :  1.837825059890747 3.2856879234313965 3.2856879234313965
Loss :  1.8109960556030273 3.3193788528442383 3.3193788528442383
Loss :  1.8419580459594727 3.4230411052703857 3.4230411052703857
Loss :  1.7975659370422363 3.2496750354766846 3.2496750354766846
Loss :  1.8332798480987549 3.559152126312256 3.559152126312256
Loss :  1.798054814338684 3.2083632946014404 3.2083632946014404
Loss :  1.8041454553604126 3.164724588394165 3.164724588394165
Loss :  1.8137807846069336 3.248584508895874 3.248584508895874
Loss :  1.813320279121399 3.303771734237671 3.303771734237671
Loss :  1.819454550743103 3.241955518722534 3.241955518722534
Loss :  1.8208528757095337 3.2587790489196777 3.2587790489196777
Loss :  1.7765116691589355 2.905290126800537 2.905290126800537
Loss :  1.819360613822937 3.468716859817505 3.468716859817505
Loss :  1.8079886436462402 2.8920979499816895 2.8920979499816895
  batch 40 loss: 1.8079886436462402, 2.8920979499816895, 2.8920979499816895
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8165732622146606 3.360023260116577 3.360023260116577
Loss :  1.7844091653823853 3.201907157897949 3.201907157897949
Loss :  1.7855204343795776 3.2364485263824463 3.2364485263824463
Loss :  1.794311285018921 3.3971593379974365 3.3971593379974365
Loss :  1.7981759309768677 3.148277759552002 3.148277759552002
Loss :  1.8061500787734985 3.1438539028167725 3.1438539028167725
Loss :  1.8017476797103882 3.333122730255127 3.333122730255127
Loss :  1.7767187356948853 3.3446977138519287 3.3446977138519287
Loss :  1.7969708442687988 3.429872989654541 3.429872989654541
Loss :  1.7794535160064697 3.186100959777832 3.186100959777832
Loss :  1.7750658988952637 3.2418630123138428 3.2418630123138428
Loss :  1.8215059041976929 3.002861976623535 3.002861976623535
Loss :  1.830140233039856 3.34639835357666 3.34639835357666
Loss :  1.8083338737487793 3.0391581058502197 3.0391581058502197
Loss :  1.8001110553741455 3.2931416034698486 3.2931416034698486
Loss :  1.831647276878357 3.270426034927368 3.270426034927368
Loss :  1.838795781135559 3.439079999923706 3.439079999923706
Loss :  1.7975019216537476 3.1907713413238525 3.1907713413238525
Loss :  1.8500062227249146 3.4851536750793457 3.4851536750793457
Loss :  1.8300211429595947 3.0675346851348877 3.0675346851348877
  batch 60 loss: 1.8300211429595947, 3.0675346851348877, 3.0675346851348877
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.831542730331421 3.2079966068267822 3.2079966068267822
Loss :  1.8064870834350586 3.3030200004577637 3.3030200004577637
Loss :  1.8212014436721802 3.144183397293091 3.144183397293091
Loss :  1.814221978187561 3.2653896808624268 3.2653896808624268
Loss :  1.8048946857452393 2.7928659915924072 2.7928659915924072
Loss :  1.7027937173843384 3.6944875717163086 3.6944875717163086
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7179014682769775 3.7086727619171143 3.7086727619171143
Loss :  1.709778904914856 3.487851619720459 3.487851619720459
Loss :  1.7298648357391357 3.6379523277282715 3.6379523277282715
Total LOSS train 3.269517212647658 valid 3.6322410702705383
CE LOSS train 1.814036066715534 valid 0.43246620893478394
Contrastive LOSS train 3.269517212647658 valid 0.9094880819320679
EPOCH 127:
Loss :  1.7985317707061768 3.3289270401000977 3.3289270401000977
Loss :  1.8256702423095703 3.6714272499084473 3.6714272499084473
Loss :  1.8101552724838257 3.436922311782837 3.436922311782837
Loss :  1.812493920326233 3.505774736404419 3.505774736404419
Loss :  1.8167802095413208 3.3681952953338623 3.3681952953338623
Loss :  1.8384641408920288 3.272890567779541 3.272890567779541
Loss :  1.818696141242981 3.465576648712158 3.465576648712158
Loss :  1.8254133462905884 3.1029510498046875 3.1029510498046875
Loss :  1.790594458580017 3.142582893371582 3.142582893371582
Loss :  1.7898298501968384 2.7242820262908936 2.7242820262908936
Loss :  1.8163264989852905 3.4489970207214355 3.4489970207214355
Loss :  1.8452833890914917 3.2987725734710693 3.2987725734710693
Loss :  1.8185667991638184 3.3180079460144043 3.3180079460144043
Loss :  1.8273202180862427 3.430891275405884 3.430891275405884
Loss :  1.8090715408325195 3.258814811706543 3.258814811706543
Loss :  1.8188438415527344 3.3152596950531006 3.3152596950531006
Loss :  1.820032000541687 3.2877402305603027 3.2877402305603027
Loss :  1.81353759765625 3.227959156036377 3.227959156036377
Loss :  1.8036575317382812 3.149749517440796 3.149749517440796
Loss :  1.8022984266281128 3.2880678176879883 3.2880678176879883
  batch 20 loss: 1.8022984266281128, 3.2880678176879883, 3.2880678176879883
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.813965916633606 3.0501208305358887 3.0501208305358887
Loss :  1.8238937854766846 3.3789398670196533 3.3789398670196533
Loss :  1.8276762962341309 3.2162153720855713 3.2162153720855713
Loss :  1.8626512289047241 3.0545461177825928 3.0545461177825928
Loss :  1.8201760053634644 3.587388038635254 3.587388038635254
Loss :  1.8310075998306274 3.1158289909362793 3.1158289909362793
Loss :  1.8417811393737793 3.4932913780212402 3.4932913780212402
Loss :  1.812045931816101 3.257639169692993 3.257639169692993
Loss :  1.8443760871887207 3.2295048236846924 3.2295048236846924
Loss :  1.796871304512024 3.21805739402771 3.21805739402771
Loss :  1.8315372467041016 3.5478978157043457 3.5478978157043457
Loss :  1.7961336374282837 3.4304275512695312 3.4304275512695312
Loss :  1.8102694749832153 3.0836493968963623 3.0836493968963623
Loss :  1.818360447883606 3.3769655227661133 3.3769655227661133
Loss :  1.8123372793197632 3.351233959197998 3.351233959197998
Loss :  1.8236507177352905 3.424811601638794 3.424811601638794
Loss :  1.8239498138427734 3.4055328369140625 3.4055328369140625
Loss :  1.7866177558898926 3.4446821212768555 3.4446821212768555
Loss :  1.8232172727584839 3.3793394565582275 3.3793394565582275
Loss :  1.812791347503662 4.380255222320557 4.380255222320557
  batch 40 loss: 1.812791347503662, 4.380255222320557, 4.380255222320557
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8244028091430664 4.543044090270996 4.543044090270996
Loss :  1.863282561302185 4.280676364898682 4.280676364898682
Loss :  1.8702179193496704 4.358389854431152 4.358389854431152
Loss :  1.8662291765213013 4.370173931121826 4.370173931121826
Loss :  1.8659002780914307 4.148230075836182 4.148230075836182
Loss :  1.8635414838790894 4.052460193634033 4.052460193634033
Loss :  1.8629510402679443 3.6041972637176514 3.6041972637176514
Loss :  1.8440922498703003 3.5857701301574707 3.5857701301574707
Loss :  1.861393690109253 3.7399206161499023 3.7399206161499023
Loss :  1.8526034355163574 3.807738780975342 3.807738780975342
Loss :  1.8465924263000488 3.9540505409240723 3.9540505409240723
Loss :  1.875901699066162 3.6775479316711426 3.6775479316711426
Loss :  1.881452202796936 3.965712070465088 3.965712070465088
Loss :  1.8939342498779297 3.9275732040405273 3.9275732040405273
Loss :  1.85869562625885 4.227010726928711 4.227010726928711
Loss :  1.8967516422271729 4.150071144104004 4.150071144104004
Loss :  1.838106393814087 4.329176425933838 4.329176425933838
Loss :  1.8496612310409546 3.7439193725585938 3.7439193725585938
Loss :  1.870308756828308 3.9789111614227295 3.9789111614227295
Loss :  1.8627480268478394 3.753359079360962 3.753359079360962
  batch 60 loss: 1.8627480268478394, 3.753359079360962, 3.753359079360962
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8707720041275024 3.432898759841919 3.432898759841919
Loss :  1.841210961341858 3.5888428688049316 3.5888428688049316
Loss :  1.8544508218765259 3.8191046714782715 3.8191046714782715
Loss :  1.8623056411743164 3.7172493934631348 3.7172493934631348
Loss :  1.865010380744934 3.193307638168335 3.193307638168335
Loss :  1.5911725759506226 4.418581008911133 4.418581008911133
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.612662434577942 4.367116928100586 4.367116928100586
Loss :  1.5860486030578613 4.297338962554932 4.297338962554932
Loss :  1.6996718645095825 4.179946422576904 4.179946422576904
Total LOSS train 3.5602992864755483 valid 4.315745830535889
CE LOSS train 1.8347291414554303 valid 0.42491796612739563
Contrastive LOSS train 3.5602992864755483 valid 1.044986605644226
EPOCH 128:
Loss :  1.8390088081359863 3.9055142402648926 3.9055142402648926
Loss :  1.8634238243103027 3.939257860183716 3.939257860183716
Loss :  1.8592770099639893 3.8469350337982178 3.8469350337982178
Loss :  1.8712337017059326 4.036828994750977 4.036828994750977
Loss :  1.8604683876037598 4.098138332366943 4.098138332366943
Loss :  1.8912099599838257 3.8204777240753174 3.8204777240753174
Loss :  1.8837528228759766 3.9408066272735596 3.9408066272735596
Loss :  1.872403860092163 4.029839515686035 4.029839515686035
Loss :  1.8665791749954224 4.12243127822876 4.12243127822876
Loss :  1.8768812417984009 3.813181161880493 3.813181161880493
Loss :  1.826113224029541 4.600209712982178 4.600209712982178
Loss :  1.8582353591918945 3.9038033485412598 3.9038033485412598
Loss :  1.8824012279510498 3.9618592262268066 3.9618592262268066
Loss :  1.8817089796066284 3.7667911052703857 3.7667911052703857
Loss :  1.8384804725646973 4.135436058044434 4.135436058044434
Loss :  1.9581677913665771 4.685857772827148 4.685857772827148
Loss :  1.8155401945114136 3.7295138835906982 3.7295138835906982
Loss :  1.808943271636963 3.8968138694763184 3.8968138694763184
Loss :  1.8050225973129272 3.3831474781036377 3.3831474781036377
Loss :  1.7964755296707153 3.6500840187072754 3.6500840187072754
  batch 20 loss: 1.7964755296707153, 3.6500840187072754, 3.6500840187072754
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8271443843841553 3.319286346435547 3.319286346435547
Loss :  1.8314870595932007 3.221233606338501 3.221233606338501
Loss :  1.8364447355270386 3.5016098022460938 3.5016098022460938
Loss :  1.870384693145752 3.309044361114502 3.309044361114502
Loss :  1.8275136947631836 3.5860753059387207 3.5860753059387207
Loss :  1.8383290767669678 3.597750663757324 3.597750663757324
Loss :  1.8437769412994385 3.7827060222625732 3.7827060222625732
Loss :  1.8213309049606323 3.4366636276245117 3.4366636276245117
Loss :  1.8421142101287842 3.205817461013794 3.205817461013794
Loss :  1.7985988855361938 3.3805887699127197 3.3805887699127197
Loss :  1.836902379989624 3.549281358718872 3.549281358718872
Loss :  1.8025050163269043 3.565248727798462 3.565248727798462
Loss :  1.815865159034729 3.425144910812378 3.425144910812378
Loss :  1.817604899406433 3.304919719696045 3.304919719696045
Loss :  1.818493366241455 3.471356153488159 3.471356153488159
Loss :  1.823201298713684 3.2978785037994385 3.2978785037994385
Loss :  1.8321053981781006 3.3415777683258057 3.3415777683258057
Loss :  1.784810185432434 3.41304349899292 3.41304349899292
Loss :  1.82374107837677 3.3767781257629395 3.3767781257629395
Loss :  1.8187201023101807 3.3013367652893066 3.3013367652893066
  batch 40 loss: 1.8187201023101807, 3.3013367652893066, 3.3013367652893066
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.825308918952942 3.539604902267456 3.539604902267456
Loss :  1.8070930242538452 3.493659734725952 3.493659734725952
Loss :  1.7977492809295654 3.3419206142425537 3.3419206142425537
Loss :  1.810097575187683 3.2950165271759033 3.2950165271759033
Loss :  1.8028771877288818 3.0151593685150146 3.0151593685150146
Loss :  1.8155995607376099 3.350605010986328 3.350605010986328
Loss :  1.811789870262146 3.241687059402466 3.241687059402466
Loss :  1.78394615650177 3.3700764179229736 3.3700764179229736
Loss :  1.8074129819869995 3.267965793609619 3.267965793609619
Loss :  1.7953298091888428 3.355645179748535 3.355645179748535
Loss :  1.7876996994018555 3.285496473312378 3.285496473312378
Loss :  1.8258211612701416 3.0691726207733154 3.0691726207733154
Loss :  1.8326419591903687 3.413954496383667 3.413954496383667
Loss :  1.8159922361373901 3.1845333576202393 3.1845333576202393
Loss :  1.8042330741882324 3.489180564880371 3.489180564880371
Loss :  1.8315074443817139 3.135685682296753 3.135685682296753
Loss :  1.83863365650177 3.364229917526245 3.364229917526245
Loss :  1.7985175848007202 3.350935220718384 3.350935220718384
Loss :  1.8503553867340088 3.5058705806732178 3.5058705806732178
Loss :  1.8295930624008179 3.487361431121826 3.487361431121826
  batch 60 loss: 1.8295930624008179, 3.487361431121826, 3.487361431121826
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8243409395217896 3.1128153800964355 3.1128153800964355
Loss :  1.7986607551574707 3.4270482063293457 3.4270482063293457
Loss :  1.8137627840042114 3.0855355262756348 3.0855355262756348
Loss :  1.8098289966583252 3.1835238933563232 3.1835238933563232
Loss :  1.802219271659851 2.873802900314331 2.873802900314331
Loss :  1.6736150979995728 4.174716472625732 4.174716472625732
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6879318952560425 4.264142990112305 4.264142990112305
Loss :  1.689791202545166 4.215817451477051 4.215817451477051
Loss :  1.6815098524093628 4.122734546661377 4.122734546661377
Total LOSS train 3.536842393875122 valid 4.194352865219116
CE LOSS train 1.8305755890332736 valid 0.4203774631023407
Contrastive LOSS train 3.536842393875122 valid 1.0306836366653442
EPOCH 129:
Loss :  1.7959933280944824 3.1119494438171387 3.1119494438171387
Loss :  1.8146635293960571 3.804309844970703 3.804309844970703
Loss :  1.800660252571106 3.1405630111694336 3.1405630111694336
Loss :  1.8068232536315918 3.2075347900390625 3.2075347900390625
Loss :  1.811026930809021 3.363137722015381 3.363137722015381
Loss :  1.8262354135513306 3.4770121574401855 3.4770121574401855
Loss :  1.805216908454895 3.3073925971984863 3.3073925971984863
Loss :  1.806304693222046 3.1892805099487305 3.1892805099487305
Loss :  1.7783794403076172 3.2549901008605957 3.2549901008605957
Loss :  1.7623465061187744 3.252394914627075 3.252394914627075
Loss :  1.789035439491272 3.7059428691864014 3.7059428691864014
Loss :  1.8108829259872437 3.7602148056030273 3.7602148056030273
Loss :  1.8054512739181519 3.7808685302734375 3.7808685302734375
Loss :  1.8195573091506958 4.175509452819824 4.175509452819824
Loss :  1.7722810506820679 4.728231906890869 4.728231906890869
Loss :  1.8692820072174072 4.38148832321167 4.38148832321167
Loss :  1.8561105728149414 3.786787748336792 3.786787748336792
Loss :  1.8661140203475952 3.508272409439087 3.508272409439087
Loss :  1.8502864837646484 3.4455502033233643 3.4455502033233643
Loss :  1.8551921844482422 3.5298402309417725 3.5298402309417725
  batch 20 loss: 1.8551921844482422, 3.5298402309417725, 3.5298402309417725
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8558409214019775 3.337883472442627 3.337883472442627
Loss :  1.8525726795196533 3.371070623397827 3.371070623397827
Loss :  1.859376311302185 3.346921443939209 3.346921443939209
Loss :  1.8803355693817139 3.407444953918457 3.407444953918457
Loss :  1.8488248586654663 3.7324743270874023 3.7324743270874023
Loss :  1.8515300750732422 3.1916353702545166 3.1916353702545166
Loss :  1.8601374626159668 3.7128427028656006 3.7128427028656006
Loss :  1.8413711786270142 3.966038465499878 3.966038465499878
Loss :  1.8550165891647339 3.5853497982025146 3.5853497982025146
Loss :  1.8408490419387817 4.071823596954346 4.071823596954346
Loss :  1.8392550945281982 3.352473020553589 3.352473020553589
Loss :  1.8426787853240967 3.580338954925537 3.580338954925537
Loss :  1.860469102859497 3.3269782066345215 3.3269782066345215
Loss :  1.8494902849197388 3.8192901611328125 3.8192901611328125
Loss :  1.870373249053955 3.5935046672821045 3.5935046672821045
Loss :  1.8497406244277954 3.3910114765167236 3.3910114765167236
Loss :  1.8676230907440186 3.233236074447632 3.233236074447632
Loss :  1.8531053066253662 3.2094476222991943 3.2094476222991943
Loss :  1.8622782230377197 3.5341427326202393 3.5341427326202393
Loss :  1.8590736389160156 3.4774935245513916 3.4774935245513916
  batch 40 loss: 1.8590736389160156, 3.4774935245513916, 3.4774935245513916
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8741083145141602 3.9970083236694336 3.9970083236694336
Loss :  1.8424152135849 3.3960468769073486 3.3960468769073486
Loss :  1.8528894186019897 3.289266347885132 3.289266347885132
Loss :  1.8333595991134644 3.3715622425079346 3.3715622425079346
Loss :  1.8418673276901245 3.705606698989868 3.705606698989868
Loss :  1.820865273475647 3.6651535034179688 3.6651535034179688
Loss :  1.8268078565597534 3.2227492332458496 3.2227492332458496
Loss :  1.820891261100769 3.3268120288848877 3.3268120288848877
Loss :  1.8145473003387451 3.2463719844818115 3.2463719844818115
Loss :  1.8256940841674805 3.5407211780548096 3.5407211780548096
Loss :  1.8084640502929688 3.7859318256378174 3.7859318256378174
Loss :  1.833208441734314 3.4618980884552 3.4618980884552
Loss :  1.8298003673553467 3.168581962585449 3.168581962585449
Loss :  1.8093398809432983 3.099642753601074 3.099642753601074
Loss :  1.824165940284729 3.082695484161377 3.082695484161377
Loss :  1.8456166982650757 2.9651637077331543 2.9651637077331543
Loss :  1.835945725440979 3.338658094406128 3.338658094406128
Loss :  1.8180100917816162 3.2469584941864014 3.2469584941864014
Loss :  1.8532981872558594 3.502613067626953 3.502613067626953
Loss :  1.843610167503357 3.0503361225128174 3.0503361225128174
  batch 60 loss: 1.843610167503357, 3.0503361225128174, 3.0503361225128174
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8342097997665405 3.1208336353302 3.1208336353302
Loss :  1.823487401008606 3.6321051120758057 3.6321051120758057
Loss :  1.8452945947647095 3.4540553092956543 3.4540553092956543
Loss :  1.820572853088379 3.5475552082061768 3.5475552082061768
Loss :  1.8217949867248535 2.732234001159668 2.732234001159668
Loss :  1.5978254079818726 4.4163055419921875 4.4163055419921875
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6159785985946655 4.399386882781982 4.399386882781982
Loss :  1.5889098644256592 4.1876654624938965 4.1876654624938965
Loss :  1.7035006284713745 4.299927711486816 4.299927711486816
Total LOSS train 3.478511293117817 valid 4.325821399688721
CE LOSS train 1.8338776991917536 valid 0.42587515711784363
Contrastive LOSS train 3.478511293117817 valid 1.074981927871704
EPOCH 130:
Loss :  1.8156853914260864 3.0451953411102295 3.0451953411102295
Loss :  1.826087474822998 3.5689070224761963 3.5689070224761963
Loss :  1.8187507390975952 3.5396666526794434 3.5396666526794434
Loss :  1.8254934549331665 3.6244957447052 3.6244957447052
Loss :  1.8300927877426147 3.322592258453369 3.322592258453369
Loss :  1.843309760093689 3.4672625064849854 3.4672625064849854
Loss :  1.8139299154281616 3.561810255050659 3.561810255050659
Loss :  1.8070616722106934 3.4010701179504395 3.4010701179504395
Loss :  1.8143216371536255 3.3030622005462646 3.3030622005462646
Loss :  1.8007739782333374 2.8121988773345947 2.8121988773345947
Loss :  1.8160687685012817 3.15901255607605 3.15901255607605
Loss :  1.8170311450958252 3.3526151180267334 3.3526151180267334
Loss :  1.8158226013183594 3.6061158180236816 3.6061158180236816
Loss :  1.8232917785644531 3.066251277923584 3.066251277923584
Loss :  1.804463267326355 3.1137399673461914 3.1137399673461914
Loss :  1.798472285270691 3.2654426097869873 3.2654426097869873
Loss :  1.8088245391845703 3.360673189163208 3.360673189163208
Loss :  1.784058928489685 3.234854221343994 3.234854221343994
Loss :  1.8040605783462524 3.5634043216705322 3.5634043216705322
Loss :  1.7824606895446777 3.0631022453308105 3.0631022453308105
  batch 20 loss: 1.7824606895446777, 3.0631022453308105, 3.0631022453308105
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8179326057434082 3.2490153312683105 3.2490153312683105
Loss :  1.8154983520507812 3.1032166481018066 3.1032166481018066
Loss :  1.826120376586914 3.237663984298706 3.237663984298706
Loss :  1.8526467084884644 3.4657070636749268 3.4657070636749268
Loss :  1.833177924156189 3.7619824409484863 3.7619824409484863
Loss :  1.8231403827667236 3.6456186771392822 3.6456186771392822
Loss :  1.831620693206787 3.3915460109710693 3.3915460109710693
Loss :  1.8269870281219482 3.190934658050537 3.190934658050537
Loss :  1.844982385635376 3.159916400909424 3.159916400909424
Loss :  1.8185515403747559 3.200028419494629 3.200028419494629
Loss :  1.8299636840820312 3.3960611820220947 3.3960611820220947
Loss :  1.826886773109436 3.4642722606658936 3.4642722606658936
Loss :  1.8460345268249512 3.5561676025390625 3.5561676025390625
Loss :  1.8173421621322632 3.4924521446228027 3.4924521446228027
Loss :  1.8383532762527466 3.5553786754608154 3.5553786754608154
Loss :  1.827188491821289 3.5823137760162354 3.5823137760162354
Loss :  1.8452680110931396 3.169375419616699 3.169375419616699
Loss :  1.8185068368911743 3.1805408000946045 3.1805408000946045
Loss :  1.8303725719451904 3.521892547607422 3.521892547607422
Loss :  1.8273930549621582 3.4179084300994873 3.4179084300994873
  batch 40 loss: 1.8273930549621582, 3.4179084300994873, 3.4179084300994873
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8401905298233032 3.9227421283721924 3.9227421283721924
Loss :  1.8316516876220703 4.135324954986572 4.135324954986572
Loss :  1.8358550071716309 3.662123441696167 3.662123441696167
Loss :  1.8677616119384766 3.640592098236084 3.640592098236084
Loss :  1.9032758474349976 4.437027454376221 4.437027454376221
Loss :  1.8454045057296753 4.069239616394043 4.069239616394043
Loss :  1.8326826095581055 3.539637804031372 3.539637804031372
Loss :  1.8015812635421753 3.337590456008911 3.337590456008911
Loss :  1.8233181238174438 3.2858574390411377 3.2858574390411377
Loss :  1.8086822032928467 3.375993013381958 3.375993013381958
Loss :  1.803133487701416 3.451375722885132 3.451375722885132
Loss :  1.8440756797790527 3.3611416816711426 3.3611416816711426
Loss :  1.8410744667053223 3.3295226097106934 3.3295226097106934
Loss :  1.8304314613342285 3.193208932876587 3.193208932876587
Loss :  1.8196474313735962 3.507272958755493 3.507272958755493
Loss :  1.8473402261734009 3.2157387733459473 3.2157387733459473
Loss :  1.8320642709732056 4.0874128341674805 4.0874128341674805
Loss :  1.8067708015441895 3.4695255756378174 3.4695255756378174
Loss :  1.8449878692626953 3.515259027481079 3.515259027481079
Loss :  1.8240158557891846 3.508064031600952 3.508064031600952
  batch 60 loss: 1.8240158557891846, 3.508064031600952, 3.508064031600952
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8293976783752441 3.4904885292053223 3.4904885292053223
Loss :  1.8056529760360718 3.36293888092041 3.36293888092041
Loss :  1.8200544118881226 3.595036745071411 3.595036745071411
Loss :  1.8131626844406128 3.373795747756958 3.373795747756958
Loss :  1.8021951913833618 3.2619948387145996 3.2619948387145996
Loss :  1.6264121532440186 4.221914768218994 4.221914768218994
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6454540491104126 4.155076503753662 4.155076503753662
Loss :  1.6413096189498901 4.091257572174072 4.091257572174072
Loss :  1.6334806680679321 3.960054636001587 3.960054636001587
Total LOSS train 3.43542107802171 valid 4.107075870037079
CE LOSS train 1.8246528717187736 valid 0.40837016701698303
Contrastive LOSS train 3.43542107802171 valid 0.9900136590003967
EPOCH 131:
Loss :  1.7890801429748535 2.984528064727783 2.984528064727783
Loss :  1.809008002281189 3.5446531772613525 3.5446531772613525
Loss :  1.7955827713012695 3.262408971786499 3.262408971786499
Loss :  1.8012940883636475 3.013486385345459 3.013486385345459
Loss :  1.800735354423523 3.12687087059021 3.12687087059021
Loss :  1.820750117301941 3.342477321624756 3.342477321624756
Loss :  1.8009227514266968 3.1684632301330566 3.1684632301330566
Loss :  1.8113553524017334 3.1213185787200928 3.1213185787200928
Loss :  1.7853107452392578 2.8726983070373535 2.8726983070373535
Loss :  1.7844059467315674 3.004488945007324 3.004488945007324
Loss :  1.7992466688156128 3.178556203842163 3.178556203842163
Loss :  1.8228039741516113 3.4240477085113525 3.4240477085113525
Loss :  1.8041995763778687 3.275399923324585 3.275399923324585
Loss :  1.8114920854568481 3.3742666244506836 3.3742666244506836
Loss :  1.8054195642471313 3.476628541946411 3.476628541946411
Loss :  1.796432614326477 3.3750216960906982 3.3750216960906982
Loss :  1.7993931770324707 3.2233657836914062 3.2233657836914062
Loss :  1.7970362901687622 3.1529948711395264 3.1529948711395264
Loss :  1.8025133609771729 3.301133155822754 3.301133155822754
Loss :  1.7897706031799316 3.2636172771453857 3.2636172771453857
  batch 20 loss: 1.7897706031799316, 3.2636172771453857, 3.2636172771453857
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8173044919967651 3.1859312057495117 3.1859312057495117
Loss :  1.8145718574523926 3.468794345855713 3.468794345855713
Loss :  1.8153791427612305 3.1986851692199707 3.1986851692199707
Loss :  1.8529657125473022 3.0336380004882812 3.0336380004882812
Loss :  1.8143595457077026 3.374265193939209 3.374265193939209
Loss :  1.8183791637420654 3.056861400604248 3.056861400604248
Loss :  1.822637915611267 3.371565580368042 3.371565580368042
Loss :  1.808891773223877 3.175295352935791 3.175295352935791
Loss :  1.8218333721160889 3.2110085487365723 3.2110085487365723
Loss :  1.7860558032989502 3.1350338459014893 3.1350338459014893
Loss :  1.8195056915283203 3.4483091831207275 3.4483091831207275
Loss :  1.7910351753234863 3.5775744915008545 3.5775744915008545
Loss :  1.8068788051605225 3.004340648651123 3.004340648651123
Loss :  1.802354335784912 3.3583521842956543 3.3583521842956543
Loss :  1.804613709449768 3.258742570877075 3.258742570877075
Loss :  1.809869408607483 3.149700880050659 3.149700880050659
Loss :  1.8188371658325195 3.321319580078125 3.321319580078125
Loss :  1.7753268480300903 3.033466100692749 3.033466100692749
Loss :  1.805101752281189 3.1888513565063477 3.1888513565063477
Loss :  1.8000408411026 3.166710376739502 3.166710376739502
  batch 40 loss: 1.8000408411026, 3.166710376739502, 3.166710376739502
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8062922954559326 3.086522102355957 3.086522102355957
Loss :  1.7915844917297363 2.9669289588928223 2.9669289588928223
Loss :  1.788587212562561 3.1800243854522705 3.1800243854522705
Loss :  1.801292896270752 3.259138584136963 3.259138584136963
Loss :  1.7913315296173096 3.0841867923736572 3.0841867923736572
Loss :  1.7923531532287598 3.1758458614349365 3.1758458614349365
Loss :  1.789928674697876 2.8668429851531982 2.8668429851531982
Loss :  1.7852954864501953 3.5776901245117188 3.5776901245117188
Loss :  1.783649206161499 3.460890531539917 3.460890531539917
Loss :  1.8053346872329712 3.3665316104888916 3.3665316104888916
Loss :  1.7927467823028564 3.0005171298980713 3.0005171298980713
Loss :  1.8210959434509277 3.37225604057312 3.37225604057312
Loss :  1.8132113218307495 3.543031930923462 3.543031930923462
Loss :  1.8191566467285156 3.470893144607544 3.470893144607544
Loss :  1.8146721124649048 3.408036947250366 3.408036947250366
Loss :  1.8171747922897339 3.4336118698120117 3.4336118698120117
Loss :  1.8277543783187866 3.596096992492676 3.596096992492676
Loss :  1.8095972537994385 3.2611243724823 3.2611243724823
Loss :  1.8423315286636353 3.438873529434204 3.438873529434204
Loss :  1.8273038864135742 3.1373727321624756 3.1373727321624756
  batch 60 loss: 1.8273038864135742, 3.1373727321624756, 3.1373727321624756
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8177396059036255 3.4153902530670166 3.4153902530670166
Loss :  1.8146957159042358 3.4065895080566406 3.4065895080566406
Loss :  1.8237366676330566 3.275683641433716 3.275683641433716
Loss :  1.8193981647491455 3.0186843872070312 3.0186843872070312
Loss :  1.8142496347427368 2.9538121223449707 2.9538121223449707
Loss :  1.7581924200057983 3.883272886276245 3.883272886276245
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.757035732269287 3.8930821418762207 3.8930821418762207
Loss :  1.7677795886993408 3.636476993560791 3.636476993560791
Loss :  1.7393699884414673 3.8482553958892822 3.8482553958892822
Total LOSS train 3.2455607414245606 valid 3.8152718544006348
CE LOSS train 1.8068186118052556 valid 0.4348424971103668
Contrastive LOSS train 3.2455607414245606 valid 0.9620638489723206
EPOCH 132:
Loss :  1.8061071634292603 3.1650729179382324 3.1650729179382324
Loss :  1.7998496294021606 3.387990713119507 3.387990713119507
Loss :  1.7954009771347046 3.145286798477173 3.145286798477173
Loss :  1.8115346431732178 3.216324806213379 3.216324806213379
Loss :  1.8155280351638794 3.1455228328704834 3.1455228328704834
Loss :  1.8220525979995728 3.261744260787964 3.261744260787964
Loss :  1.7952388525009155 3.3216588497161865 3.3216588497161865
Loss :  1.8034850358963013 3.082855463027954 3.082855463027954
Loss :  1.7844129800796509 3.0913283824920654 3.0913283824920654
Loss :  1.7829097509384155 3.1850922107696533 3.1850922107696533
Loss :  1.7970620393753052 3.41697359085083 3.41697359085083
Loss :  1.8166966438293457 3.1988914012908936 3.1988914012908936
Loss :  1.8022536039352417 3.026578426361084 3.026578426361084
Loss :  1.808958888053894 3.095768690109253 3.095768690109253
Loss :  1.7887966632843018 3.4770028591156006 3.4770028591156006
Loss :  1.788725733757019 3.363542318344116 3.363542318344116
Loss :  1.7970420122146606 3.185847759246826 3.185847759246826
Loss :  1.7880643606185913 3.1678152084350586 3.1678152084350586
Loss :  1.7961088418960571 3.069085121154785 3.069085121154785
Loss :  1.780264139175415 3.084352970123291 3.084352970123291
  batch 20 loss: 1.780264139175415, 3.084352970123291, 3.084352970123291
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8048532009124756 2.918034076690674 2.918034076690674
Loss :  1.8068686723709106 3.5811829566955566 3.5811829566955566
Loss :  1.8041152954101562 3.304997682571411 3.304997682571411
Loss :  1.8379720449447632 3.382838726043701 3.382838726043701
Loss :  1.8058768510818481 3.3884801864624023 3.3884801864624023
Loss :  1.8067283630371094 3.1298727989196777 3.1298727989196777
Loss :  1.8123247623443604 3.4554033279418945 3.4554033279418945
Loss :  1.7979860305786133 3.0952181816101074 3.0952181816101074
Loss :  1.8180580139160156 3.0703647136688232 3.0703647136688232
Loss :  1.7824710607528687 2.8590128421783447 2.8590128421783447
Loss :  1.8135173320770264 3.432542562484741 3.432542562484741
Loss :  1.7805688381195068 3.480055809020996 3.480055809020996
Loss :  1.7984428405761719 3.0623207092285156 3.0623207092285156
Loss :  1.7939451932907104 3.1971278190612793 3.1971278190612793
Loss :  1.7979846000671387 3.323795795440674 3.323795795440674
Loss :  1.8046352863311768 3.169847249984741 3.169847249984741
Loss :  1.8095324039459229 3.2125871181488037 3.2125871181488037
Loss :  1.7686964273452759 2.8865931034088135 2.8865931034088135
Loss :  1.7982860803604126 3.10721492767334 3.10721492767334
Loss :  1.7896835803985596 3.2407453060150146 3.2407453060150146
  batch 40 loss: 1.7896835803985596, 3.2407453060150146, 3.2407453060150146
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7994446754455566 3.342574119567871 3.342574119567871
Loss :  1.7910305261611938 3.6028873920440674 3.6028873920440674
Loss :  1.790413737297058 3.6230804920196533 3.6230804920196533
Loss :  1.8530939817428589 4.859434127807617 4.859434127807617
Loss :  1.8663779497146606 3.5538697242736816 3.5538697242736816
Loss :  1.8654627799987793 3.1476986408233643 3.1476986408233643
Loss :  1.8811784982681274 2.9706475734710693 2.9706475734710693
Loss :  1.8581805229187012 3.517821788787842 3.517821788787842
Loss :  1.8700168132781982 3.729275941848755 3.729275941848755
Loss :  1.85597562789917 3.3737733364105225 3.3737733364105225
Loss :  1.8330917358398438 3.9758195877075195 3.9758195877075195
Loss :  1.8516765832901 3.0900566577911377 3.0900566577911377
Loss :  1.8410520553588867 3.2123630046844482 3.2123630046844482
Loss :  1.829846739768982 3.572464942932129 3.572464942932129
Loss :  1.8371895551681519 3.449810743331909 3.449810743331909
Loss :  1.858336091041565 3.199324369430542 3.199324369430542
Loss :  1.843778371810913 3.3334829807281494 3.3334829807281494
Loss :  1.8317701816558838 3.2428083419799805 3.2428083419799805
Loss :  1.8521534204483032 3.5119805335998535 3.5119805335998535
Loss :  1.8362104892730713 3.4395663738250732 3.4395663738250732
  batch 60 loss: 1.8362104892730713, 3.4395663738250732, 3.4395663738250732
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8406200408935547 3.5033698081970215 3.5033698081970215
Loss :  1.8234128952026367 3.6025896072387695 3.6025896072387695
Loss :  1.858245611190796 3.3968565464019775 3.3968565464019775
Loss :  1.8646703958511353 3.8095784187316895 3.8095784187316895
Loss :  1.856105923652649 3.841914653778076 3.841914653778076
Loss :  1.9513070583343506 3.720730781555176 3.720730781555176
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.903324842453003 3.9320034980773926 3.9320034980773926
Loss :  1.9363291263580322 3.5009846687316895 3.5009846687316895
Loss :  1.9088163375854492 3.5640995502471924 3.5640995502471924
Total LOSS train 3.3275387874016396 valid 3.6794546246528625
CE LOSS train 1.8169596103521495 valid 0.4772040843963623
Contrastive LOSS train 3.3275387874016396 valid 0.8910248875617981
EPOCH 133:
Loss :  1.8669112920761108 4.276815414428711 4.276815414428711
Loss :  1.7963062524795532 4.157089710235596 4.157089710235596
Loss :  1.8564198017120361 4.223937034606934 4.223937034606934
Loss :  1.8466891050338745 4.288362979888916 4.288362979888916
Loss :  1.7937171459197998 4.134565353393555 4.134565353393555
Loss :  1.841373085975647 4.1722822189331055 4.1722822189331055
Loss :  1.8594943284988403 4.75319242477417 4.75319242477417
Loss :  1.8770256042480469 4.184020042419434 4.184020042419434
Loss :  1.8821269273757935 4.330481052398682 4.330481052398682
Loss :  1.855527639389038 4.174198150634766 4.174198150634766
Loss :  1.8495492935180664 4.18226432800293 4.18226432800293
Loss :  1.87595534324646 4.3243513107299805 4.3243513107299805
Loss :  1.8425039052963257 3.7598862648010254 3.7598862648010254
Loss :  1.848968744277954 3.70317006111145 3.70317006111145
Loss :  1.8536285161972046 3.5409955978393555 3.5409955978393555
Loss :  1.8290154933929443 3.4274404048919678 3.4274404048919678
Loss :  1.8248347043991089 3.463012456893921 3.463012456893921
Loss :  1.828081727027893 3.4732401371002197 3.4732401371002197
Loss :  1.8433340787887573 3.836648464202881 3.836648464202881
Loss :  1.799373745918274 3.9274537563323975 3.9274537563323975
  batch 20 loss: 1.799373745918274, 3.9274537563323975, 3.9274537563323975
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.859473705291748 3.4284143447875977 3.4284143447875977
Loss :  1.8423694372177124 3.565171241760254 3.565171241760254
Loss :  1.844473123550415 3.575861930847168 3.575861930847168
Loss :  1.8887630701065063 3.5129282474517822 3.5129282474517822
Loss :  1.8441715240478516 3.553285837173462 3.553285837173462
Loss :  1.8470770120620728 3.3338303565979004 3.3338303565979004
Loss :  1.8365371227264404 3.5081212520599365 3.5081212520599365
Loss :  1.8282148838043213 2.9757766723632812 2.9757766723632812
Loss :  1.8272204399108887 3.857651710510254 3.857651710510254
Loss :  1.7964445352554321 3.467799425125122 3.467799425125122
Loss :  1.8309502601623535 3.7012548446655273 3.7012548446655273
Loss :  1.80191171169281 3.588182210922241 3.588182210922241
Loss :  1.8156574964523315 3.236577272415161 3.236577272415161
Loss :  1.803574562072754 3.4895660877227783 3.4895660877227783
Loss :  1.815941333770752 3.4505136013031006 3.4505136013031006
Loss :  1.812442660331726 3.5597424507141113 3.5597424507141113
Loss :  1.831559658050537 3.379478931427002 3.379478931427002
Loss :  1.7822738885879517 3.4496777057647705 3.4496777057647705
Loss :  1.8144428730010986 3.6382100582122803 3.6382100582122803
Loss :  1.8090804815292358 3.3729262351989746 3.3729262351989746
  batch 40 loss: 1.8090804815292358, 3.3729262351989746, 3.3729262351989746
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8181803226470947 3.3978326320648193 3.3978326320648193
Loss :  1.8012388944625854 2.967822790145874 2.967822790145874
Loss :  1.804931402206421 3.1573843955993652 3.1573843955993652
Loss :  1.8061219453811646 3.1960701942443848 3.1960701942443848
Loss :  1.8043079376220703 2.905445098876953 2.905445098876953
Loss :  1.8018453121185303 3.019794464111328 3.019794464111328
Loss :  1.7974233627319336 3.1003177165985107 3.1003177165985107
Loss :  1.788561463356018 3.0175652503967285 3.0175652503967285
Loss :  1.79276442527771 3.4319188594818115 3.4319188594818115
Loss :  1.7960186004638672 3.384411573410034 3.384411573410034
Loss :  1.7835332155227661 3.2390670776367188 3.2390670776367188
Loss :  1.8134797811508179 3.0098395347595215 3.0098395347595215
Loss :  1.8196293115615845 3.286776065826416 3.286776065826416
Loss :  1.804879903793335 3.153125762939453 3.153125762939453
Loss :  1.8090888261795044 3.3698573112487793 3.3698573112487793
Loss :  1.8121953010559082 3.4029431343078613 3.4029431343078613
Loss :  1.8265236616134644 3.0819592475891113 3.0819592475891113
Loss :  1.8000940084457397 3.3404934406280518 3.3404934406280518
Loss :  1.83407461643219 3.532294750213623 3.532294750213623
Loss :  1.8104486465454102 3.3111460208892822 3.3111460208892822
  batch 60 loss: 1.8104486465454102, 3.3111460208892822, 3.3111460208892822
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8169660568237305 3.5330355167388916 3.5330355167388916
Loss :  1.801591157913208 3.2219924926757812 3.2219924926757812
Loss :  1.8145883083343506 3.1833794116973877 3.1833794116973877
Loss :  1.8111222982406616 3.2783477306365967 3.2783477306365967
Loss :  1.8034427165985107 2.685316801071167 2.685316801071167
Loss :  2.2086398601531982 4.341874599456787 4.341874599456787
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  2.222898483276367 4.244296073913574 4.244296073913574
Loss :  2.1984646320343018 4.200446605682373 4.200446605682373
Loss :  2.2164220809936523 4.206068515777588 4.206068515777588
Total LOSS train 3.5413309977604794 valid 4.248171448707581
CE LOSS train 1.824253353705773 valid 0.5541055202484131
Contrastive LOSS train 3.5413309977604794 valid 1.051517128944397
EPOCH 134:
Loss :  1.7948576211929321 2.734534740447998 2.734534740447998
Loss :  1.7999482154846191 3.4111673831939697 3.4111673831939697
Loss :  1.79537034034729 3.122178554534912 3.122178554534912
Loss :  1.8041678667068481 3.4123904705047607 3.4123904705047607
Loss :  1.8078123331069946 3.136599063873291 3.136599063873291
Loss :  1.8178952932357788 3.0732650756835938 3.0732650756835938
Loss :  1.7940911054611206 3.2476539611816406 3.2476539611816406
Loss :  1.8020274639129639 2.921769618988037 2.921769618988037
Loss :  1.787848949432373 3.0448315143585205 3.0448315143585205
Loss :  1.7823612689971924 2.8719382286071777 2.8719382286071777
Loss :  1.7959107160568237 3.045382022857666 3.045382022857666
Loss :  1.812695860862732 3.280116558074951 3.280116558074951
Loss :  1.8000859022140503 3.2770910263061523 3.2770910263061523
Loss :  1.8039753437042236 3.239445924758911 3.239445924758911
Loss :  1.7938488721847534 3.078529119491577 3.078529119491577
Loss :  1.7835747003555298 3.2086308002471924 3.2086308002471924
Loss :  1.791666030883789 3.5886778831481934 3.5886778831481934
Loss :  1.779988169670105 3.2748608589172363 3.2748608589172363
Loss :  1.7900978326797485 2.7776894569396973 2.7776894569396973
Loss :  1.7738573551177979 3.2046635150909424 3.2046635150909424
  batch 20 loss: 1.7738573551177979, 3.2046635150909424, 3.2046635150909424
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8053170442581177 3.265192985534668 3.265192985534668
Loss :  1.8004825115203857 3.371462106704712 3.371462106704712
Loss :  1.8028548955917358 3.2983272075653076 3.2983272075653076
Loss :  1.830822229385376 3.091001272201538 3.091001272201538
Loss :  1.8036960363388062 3.2187764644622803 3.2187764644622803
Loss :  1.8071564435958862 3.052833318710327 3.052833318710327
Loss :  1.8127140998840332 3.4268546104431152 3.4268546104431152
Loss :  1.8041595220565796 3.1332719326019287 3.1332719326019287
Loss :  1.8152847290039062 3.542152166366577 3.542152166366577
Loss :  1.7807320356369019 3.200139284133911 3.200139284133911
Loss :  1.813761591911316 3.6301064491271973 3.6301064491271973
Loss :  1.7850196361541748 3.493363618850708 3.493363618850708
Loss :  1.804021954536438 3.053567409515381 3.053567409515381
Loss :  1.7977439165115356 3.1485211849212646 3.1485211849212646
Loss :  1.7998489141464233 3.14608097076416 3.14608097076416
Loss :  1.8063074350357056 3.1537981033325195 3.1537981033325195
Loss :  1.810599446296692 3.421964406967163 3.421964406967163
Loss :  1.7687597274780273 2.945692777633667 2.945692777633667
Loss :  1.7990100383758545 3.222353458404541 3.222353458404541
Loss :  1.791426420211792 3.160844564437866 3.160844564437866
  batch 40 loss: 1.791426420211792, 3.160844564437866, 3.160844564437866
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7962391376495361 2.9962594509124756 2.9962594509124756
Loss :  1.7847483158111572 2.7512922286987305 2.7512922286987305
Loss :  1.7818078994750977 3.3321046829223633 3.3321046829223633
Loss :  1.7880442142486572 3.2835118770599365 3.2835118770599365
Loss :  1.7847399711608887 3.8013641834259033 3.8013641834259033
Loss :  1.78909432888031 3.148763418197632 3.148763418197632
Loss :  1.7868068218231201 3.4388110637664795 3.4388110637664795
Loss :  1.7734975814819336 3.524260997772217 3.524260997772217
Loss :  1.7825677394866943 3.262357234954834 3.262357234954834
Loss :  1.7817394733428955 3.442004680633545 3.442004680633545
Loss :  1.7724148035049438 3.5236332416534424 3.5236332416534424
Loss :  1.7990809679031372 3.4726853370666504 3.4726853370666504
Loss :  1.8043123483657837 3.125530242919922 3.125530242919922
Loss :  1.7946869134902954 2.9006621837615967 2.9006621837615967
Loss :  1.7969958782196045 3.0188798904418945 3.0188798904418945
Loss :  1.7972434759140015 2.874891996383667 2.874891996383667
Loss :  1.8102418184280396 3.0406248569488525 3.0406248569488525
Loss :  1.7861087322235107 3.0597100257873535 3.0597100257873535
Loss :  1.8200942277908325 3.4074270725250244 3.4074270725250244
Loss :  1.801627516746521 3.2304186820983887 3.2304186820983887
  batch 60 loss: 1.801627516746521, 3.2304186820983887, 3.2304186820983887
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.804516077041626 3.2381951808929443 3.2381951808929443
Loss :  1.7916533946990967 3.203845739364624 3.203845739364624
Loss :  1.8084853887557983 3.461787223815918 3.461787223815918
Loss :  1.8038080930709839 3.3820321559906006 3.3820321559906006
Loss :  1.7995831966400146 3.220590591430664 3.220590591430664
Loss :  1.9779449701309204 4.215985298156738 4.215985298156738
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4, 5], device='cuda:0')
Loss :  1.9762623310089111 4.288966178894043 4.288966178894043
Loss :  1.9786428213119507 4.133297443389893 4.133297443389893
Loss :  1.9621939659118652 3.9906539916992188 3.9906539916992188
Total LOSS train 3.216451758604783 valid 4.157225728034973
CE LOSS train 1.7967990490106436 valid 0.4905484914779663
Contrastive LOSS train 3.216451758604783 valid 0.9976634979248047
EPOCH 135:
Loss :  1.7940996885299683 3.223362445831299 3.223362445831299
Loss :  1.7972450256347656 3.469336986541748 3.469336986541748
Loss :  1.7976449728012085 3.2208399772644043 3.2208399772644043
Loss :  1.8071699142456055 2.9042277336120605 2.9042277336120605
Loss :  1.8097671270370483 3.0660817623138428 3.0660817623138428
Loss :  1.8179799318313599 3.4707417488098145 3.4707417488098145
Loss :  1.7977659702301025 3.223174810409546 3.223174810409546
Loss :  1.8058793544769287 2.993929624557495 2.993929624557495
Loss :  1.790228247642517 3.1977057456970215 3.1977057456970215
Loss :  1.790951132774353 2.734541416168213 2.734541416168213
Loss :  1.8044525384902954 3.1342573165893555 3.1342573165893555
Loss :  1.816083550453186 3.13936448097229 3.13936448097229
Loss :  1.8095289468765259 3.025391101837158 3.025391101837158
Loss :  1.8163174390792847 3.0514819622039795 3.0514819622039795
Loss :  1.795915126800537 3.271737813949585 3.271737813949585
Loss :  1.7981743812561035 3.1123452186584473 3.1123452186584473
Loss :  1.805037498474121 3.054399251937866 3.054399251937866
Loss :  1.7945860624313354 3.1053080558776855 3.1053080558776855
Loss :  1.7989203929901123 2.842120409011841 2.842120409011841
Loss :  1.7846424579620361 3.1048827171325684 3.1048827171325684
  batch 20 loss: 1.7846424579620361, 3.1048827171325684, 3.1048827171325684
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.81093430519104 3.323864221572876 3.323864221572876
Loss :  1.804057002067566 3.530918836593628 3.530918836593628
Loss :  1.804460048675537 3.6109092235565186 3.6109092235565186
Loss :  1.831278920173645 3.5652976036071777 3.5652976036071777
Loss :  1.8019574880599976 3.2676150798797607 3.2676150798797607
Loss :  1.802623987197876 3.169353485107422 3.169353485107422
Loss :  1.8107030391693115 3.4307589530944824 3.4307589530944824
Loss :  1.7975600957870483 3.2195494174957275 3.2195494174957275
Loss :  1.8165347576141357 3.2764291763305664 3.2764291763305664
Loss :  1.7786577939987183 3.3623056411743164 3.3623056411743164
Loss :  1.8099522590637207 3.4158523082733154 3.4158523082733154
Loss :  1.783843755722046 3.3213818073272705 3.3213818073272705
Loss :  1.8002349138259888 3.0053436756134033 3.0053436756134033
Loss :  1.79295814037323 3.195030689239502 3.195030689239502
Loss :  1.7959825992584229 3.255136728286743 3.255136728286743
Loss :  1.8005855083465576 3.1763625144958496 3.1763625144958496
Loss :  1.8081722259521484 3.019646644592285 3.019646644592285
Loss :  1.7668499946594238 2.8275372982025146 2.8275372982025146
Loss :  1.7929682731628418 3.2057676315307617 3.2057676315307617
Loss :  1.7853604555130005 3.4173943996429443 3.4173943996429443
  batch 40 loss: 1.7853604555130005, 3.4173943996429443, 3.4173943996429443
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7954754829406738 3.665534496307373 3.665534496307373
Loss :  1.7857270240783691 3.6931710243225098 3.6931710243225098
Loss :  1.8512159585952759 4.6518096923828125 4.6518096923828125
Loss :  1.8350327014923096 4.0555338859558105 4.0555338859558105
Loss :  1.8511735200881958 3.5554497241973877 3.5554497241973877
Loss :  1.8300809860229492 3.6335949897766113 3.6335949897766113
Loss :  1.8413742780685425 3.4382526874542236 3.4382526874542236
Loss :  1.8260881900787354 3.795400857925415 3.795400857925415
Loss :  1.812764048576355 3.3476674556732178 3.3476674556732178
Loss :  1.8190791606903076 3.468217372894287 3.468217372894287
Loss :  1.7865278720855713 3.5531983375549316 3.5531983375549316
Loss :  1.79918372631073 3.3918612003326416 3.3918612003326416
Loss :  1.7902476787567139 3.2304275035858154 3.2304275035858154
Loss :  1.787243366241455 3.0272343158721924 3.0272343158721924
Loss :  1.8001759052276611 3.2706949710845947 3.2706949710845947
Loss :  1.8073830604553223 3.140316963195801 3.140316963195801
Loss :  1.8039443492889404 3.511504650115967 3.511504650115967
Loss :  1.7994844913482666 3.697141408920288 3.697141408920288
Loss :  1.851941704750061 3.9472360610961914 3.9472360610961914
Loss :  1.8459432125091553 3.9865503311157227 3.9865503311157227
  batch 60 loss: 1.8459432125091553, 3.9865503311157227, 3.9865503311157227
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8018513917922974 4.090723991394043 4.090723991394043
Loss :  1.8072047233581543 3.9786911010742188 3.9786911010742188
Loss :  1.8300265073776245 3.4857163429260254 3.4857163429260254
Loss :  1.8083726167678833 3.243957281112671 3.243957281112671
Loss :  1.8130627870559692 2.955141305923462 2.955141305923462
Loss :  1.7494494915008545 4.193464756011963 4.193464756011963
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.717175841331482 4.184264659881592 4.184264659881592
Loss :  1.7340503931045532 4.100280284881592 4.100280284881592
Loss :  1.8144025802612305 3.8839428424835205 3.8839428424835205
Total LOSS train 3.350103290264423 valid 4.090488135814667
CE LOSS train 1.8062872318121104 valid 0.4536006450653076
Contrastive LOSS train 3.350103290264423 valid 0.9709857106208801
EPOCH 136:
Loss :  1.8125786781311035 3.7397232055664062 3.7397232055664062
Loss :  1.8185969591140747 3.6199147701263428 3.6199147701263428
Loss :  1.8225208520889282 3.2282145023345947 3.2282145023345947
Loss :  1.8231101036071777 3.550642251968384 3.550642251968384
Loss :  1.8262662887573242 3.2689270973205566 3.2689270973205566
Loss :  1.8314298391342163 3.4870200157165527 3.4870200157165527
Loss :  1.8089079856872559 3.4121878147125244 3.4121878147125244
Loss :  1.805810570716858 3.195509433746338 3.195509433746338
Loss :  1.816048264503479 3.2872204780578613 3.2872204780578613
Loss :  1.7960039377212524 2.89787220954895 2.89787220954895
Loss :  1.809328556060791 3.3394768238067627 3.3394768238067627
Loss :  1.8070917129516602 3.892458438873291 3.892458438873291
Loss :  1.825447916984558 3.718975782394409 3.718975782394409
Loss :  1.8270131349563599 3.634706735610962 3.634706735610962
Loss :  1.7998161315917969 3.934335470199585 3.934335470199585
Loss :  1.8525340557098389 4.195351600646973 4.195351600646973
Loss :  1.8538604974746704 4.258532524108887 4.258532524108887
Loss :  1.8164907693862915 3.744349479675293 3.744349479675293
Loss :  1.8031326532363892 3.4407989978790283 3.4407989978790283
Loss :  1.8006778955459595 3.3888964653015137 3.3888964653015137
  batch 20 loss: 1.8006778955459595, 3.3888964653015137, 3.3888964653015137
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8153868913650513 3.3411529064178467 3.3411529064178467
Loss :  1.8175253868103027 3.200762987136841 3.200762987136841
Loss :  1.8203885555267334 3.055595636367798 3.055595636367798
Loss :  1.8524712324142456 3.0307343006134033 3.0307343006134033
Loss :  1.8147474527359009 3.636638879776001 3.636638879776001
Loss :  1.8207662105560303 3.2337260246276855 3.2337260246276855
Loss :  1.8253278732299805 3.405062675476074 3.405062675476074
Loss :  1.8042019605636597 3.233125925064087 3.233125925064087
Loss :  1.8220698833465576 3.436448574066162 3.436448574066162
Loss :  1.7922438383102417 3.3953256607055664 3.3953256607055664
Loss :  1.8153821229934692 3.636653423309326 3.636653423309326
Loss :  1.792546272277832 3.4018709659576416 3.4018709659576416
Loss :  1.79972505569458 3.3213257789611816 3.3213257789611816
Loss :  1.811668872833252 3.403008460998535 3.403008460998535
Loss :  1.803068995475769 3.3288559913635254 3.3288559913635254
Loss :  1.812232494354248 3.1262667179107666 3.1262667179107666
Loss :  1.8121821880340576 3.4976325035095215 3.4976325035095215
Loss :  1.7845566272735596 3.5437023639678955 3.5437023639678955
Loss :  1.8138171434402466 3.2875165939331055 3.2875165939331055
Loss :  1.807936429977417 3.2103636264801025 3.2103636264801025
  batch 40 loss: 1.807936429977417, 3.2103636264801025, 3.2103636264801025
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8110816478729248 3.6473629474639893 3.6473629474639893
Loss :  1.7938019037246704 3.457235813140869 3.457235813140869
Loss :  1.7831649780273438 3.410928249359131 3.410928249359131
Loss :  1.7960158586502075 3.6088364124298096 3.6088364124298096
Loss :  1.7950875759124756 3.0025734901428223 3.0025734901428223
Loss :  1.804101824760437 3.363750696182251 3.363750696182251
Loss :  1.7998433113098145 2.937842607498169 2.937842607498169
Loss :  1.7781981229782104 2.9821372032165527 2.9821372032165527
Loss :  1.7989475727081299 3.4737560749053955 3.4737560749053955
Loss :  1.7719004154205322 3.4324257373809814 3.4324257373809814
Loss :  1.7718816995620728 3.5023186206817627 3.5023186206817627
Loss :  1.8000305891036987 3.203850507736206 3.203850507736206
Loss :  1.813336730003357 3.3025572299957275 3.3025572299957275
Loss :  1.786715030670166 3.165682315826416 3.165682315826416
Loss :  1.7828975915908813 3.3717615604400635 3.3717615604400635
Loss :  1.8038100004196167 3.4573652744293213 3.4573652744293213
Loss :  1.8212687969207764 3.3885891437530518 3.3885891437530518
Loss :  1.7986479997634888 3.393770933151245 3.393770933151245
Loss :  1.8338974714279175 3.6777756214141846 3.6777756214141846
Loss :  1.822100281715393 3.279109001159668 3.279109001159668
  batch 60 loss: 1.822100281715393, 3.279109001159668, 3.279109001159668
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8277006149291992 3.2479562759399414 3.2479562759399414
Loss :  1.8029083013534546 3.4913299083709717 3.4913299083709717
Loss :  1.8111646175384521 3.661998748779297 3.661998748779297
Loss :  1.8173162937164307 3.529482841491699 3.529482841491699
Loss :  1.807998538017273 3.2839694023132324 3.2839694023132324
Loss :  1.6964678764343262 4.113232612609863 4.113232612609863
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7115241289138794 4.12900447845459 4.12900447845459
Loss :  1.7161121368408203 3.936462879180908 3.936462879180908
Loss :  1.7007757425308228 3.6284878253936768 3.6284878253936768
Total LOSS train 3.419003857099093 valid 3.9517969489097595
CE LOSS train 1.8096727701333852 valid 0.4251939356327057
Contrastive LOSS train 3.419003857099093 valid 0.9071219563484192
EPOCH 137:
Loss :  1.8142588138580322 3.046088218688965 3.046088218688965
Loss :  1.8321188688278198 3.539891481399536 3.539891481399536
Loss :  1.8177733421325684 3.451988697052002 3.451988697052002
Loss :  1.804092526435852 3.317044258117676 3.317044258117676
Loss :  1.8120431900024414 3.473067045211792 3.473067045211792
Loss :  1.8243775367736816 3.440699815750122 3.440699815750122
Loss :  1.8307732343673706 3.2337687015533447 3.2337687015533447
Loss :  1.8350064754486084 3.19173264503479 3.19173264503479
Loss :  1.8004599809646606 3.2109642028808594 3.2109642028808594
Loss :  1.8069006204605103 2.974842071533203 2.974842071533203
Loss :  1.8327115774154663 3.445957660675049 3.445957660675049
Loss :  1.836978554725647 3.437772035598755 3.437772035598755
Loss :  1.8262516260147095 3.448049306869507 3.448049306869507
Loss :  1.8302520513534546 3.1974780559539795 3.1974780559539795
Loss :  1.7951668500900269 2.989511013031006 2.989511013031006
Loss :  1.831799864768982 3.339491844177246 3.339491844177246
Loss :  1.827017903327942 3.9417946338653564 3.9417946338653564
Loss :  1.832461953163147 3.3860924243927 3.3860924243927
Loss :  1.8098899126052856 3.404371976852417 3.404371976852417
Loss :  1.8271385431289673 3.2555997371673584 3.2555997371673584
  batch 20 loss: 1.8271385431289673, 3.2555997371673584, 3.2555997371673584
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.825872540473938 3.455874443054199 3.455874443054199
Loss :  1.8347315788269043 3.8482465744018555 3.8482465744018555
Loss :  1.828809142112732 3.704768657684326 3.704768657684326
Loss :  1.8385655879974365 3.7264811992645264 3.7264811992645264
Loss :  1.826309084892273 3.7108154296875 3.7108154296875
Loss :  1.848534107208252 3.1448702812194824 3.1448702812194824
Loss :  1.8679587841033936 3.484076499938965 3.484076499938965
Loss :  1.8396631479263306 3.132692813873291 3.132692813873291
Loss :  1.8733450174331665 3.1854376792907715 3.1854376792907715
Loss :  1.8411076068878174 3.0277726650238037 3.0277726650238037
Loss :  1.8444000482559204 3.4016852378845215 3.4016852378845215
Loss :  1.8463197946548462 3.6319375038146973 3.6319375038146973
Loss :  1.8503034114837646 3.3340418338775635 3.3340418338775635
Loss :  1.879797101020813 3.382927894592285 3.382927894592285
Loss :  1.8494881391525269 3.328504800796509 3.328504800796509
Loss :  1.8779996633529663 3.300997495651245 3.300997495651245
Loss :  1.8590790033340454 3.4433791637420654 3.4433791637420654
Loss :  1.8693768978118896 3.212352752685547 3.212352752685547
Loss :  1.8523346185684204 3.7947514057159424 3.7947514057159424
Loss :  1.8528462648391724 3.419487714767456 3.419487714767456
  batch 40 loss: 1.8528462648391724, 3.419487714767456, 3.419487714767456
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8439592123031616 3.496839761734009 3.496839761734009
Loss :  1.841883659362793 2.8365023136138916 2.8365023136138916
Loss :  1.8140619993209839 3.6372621059417725 3.6372621059417725
Loss :  1.8184802532196045 3.525364637374878 3.525364637374878
Loss :  1.8297350406646729 3.2703871726989746 3.2703871726989746
Loss :  1.8383450508117676 3.3589634895324707 3.3589634895324707
Loss :  1.8349701166152954 3.477034568786621 3.477034568786621
Loss :  1.8170453310012817 3.5331456661224365 3.5331456661224365
Loss :  1.8511457443237305 3.542344093322754 3.542344093322754
Loss :  1.8143516778945923 3.2368648052215576 3.2368648052215576
Loss :  1.807779312133789 3.5055947303771973 3.5055947303771973
Loss :  1.8428527116775513 3.2317404747009277 3.2317404747009277
Loss :  1.8593546152114868 3.3755414485931396 3.3755414485931396
Loss :  1.8419803380966187 3.185824155807495 3.185824155807495
Loss :  1.8066917657852173 3.518145799636841 3.518145799636841
Loss :  1.8486963510513306 3.0912835597991943 3.0912835597991943
Loss :  1.851435661315918 3.452608585357666 3.452608585357666
Loss :  1.8461946249008179 3.446136713027954 3.446136713027954
Loss :  1.8472875356674194 3.729266881942749 3.729266881942749
Loss :  1.8409368991851807 4.001262187957764 4.001262187957764
  batch 60 loss: 1.8409368991851807, 4.001262187957764, 4.001262187957764
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.861343264579773 4.023382186889648 4.023382186889648
Loss :  1.828156590461731 3.768939256668091 3.768939256668091
Loss :  1.8456543684005737 3.40869140625 3.40869140625
Loss :  1.8480795621871948 3.5213124752044678 3.5213124752044678
Loss :  1.8470293283462524 2.987210750579834 2.987210750579834
Loss :  1.7705459594726562 4.187938213348389 4.187938213348389
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7588430643081665 4.155768394470215 4.155768394470215
Loss :  1.7894049882888794 4.009289741516113 4.009289741516113
Loss :  1.7622253894805908 4.089895725250244 4.089895725250244
Total LOSS train 3.4085993399986854 valid 4.11072301864624
CE LOSS train 1.8363348612418542 valid 0.4405563473701477
Contrastive LOSS train 3.4085993399986854 valid 1.022473931312561
EPOCH 138:
Loss :  1.8520293235778809 2.8465168476104736 2.8465168476104736
Loss :  1.8587932586669922 3.655588388442993 3.655588388442993
Loss :  1.847856044769287 3.7679286003112793 3.7679286003112793
Loss :  1.8178905248641968 3.398049831390381 3.398049831390381
Loss :  1.8179794549942017 3.453601598739624 3.453601598739624
Loss :  1.8346678018569946 3.478999614715576 3.478999614715576
Loss :  1.8405824899673462 3.232360601425171 3.232360601425171
Loss :  1.845099925994873 3.338836431503296 3.338836431503296
Loss :  1.8050838708877563 3.3928956985473633 3.3928956985473633
Loss :  1.8075532913208008 3.3196041584014893 3.3196041584014893
Loss :  1.8329613208770752 3.2535486221313477 3.2535486221313477
Loss :  1.8375998735427856 3.835904359817505 3.835904359817505
Loss :  1.8326386213302612 3.3607211112976074 3.3607211112976074
Loss :  1.8250248432159424 3.237394094467163 3.237394094467163
Loss :  1.7889853715896606 3.4331536293029785 3.4331536293029785
Loss :  1.8288393020629883 3.4580166339874268 3.4580166339874268
Loss :  1.8248229026794434 3.426068067550659 3.426068067550659
Loss :  1.8200128078460693 3.3392629623413086 3.3392629623413086
Loss :  1.8025845289230347 3.115626573562622 3.115626573562622
Loss :  1.813338041305542 3.32025408744812 3.32025408744812
  batch 20 loss: 1.813338041305542, 3.32025408744812, 3.32025408744812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8026295900344849 3.2224600315093994 3.2224600315093994
Loss :  1.817230463027954 3.2879080772399902 3.2879080772399902
Loss :  1.8142579793930054 3.3005082607269287 3.3005082607269287
Loss :  1.8290555477142334 3.2742438316345215 3.2742438316345215
Loss :  1.8048689365386963 3.616935968399048 3.616935968399048
Loss :  1.8226882219314575 3.2482423782348633 3.2482423782348633
Loss :  1.8433698415756226 3.382622241973877 3.382622241973877
Loss :  1.8157427310943604 3.3135488033294678 3.3135488033294678
Loss :  1.8541431427001953 3.575014352798462 3.575014352798462
Loss :  1.8052289485931396 3.1796624660491943 3.1796624660491943
Loss :  1.822611689567566 3.419394016265869 3.419394016265869
Loss :  1.8069757223129272 3.397198438644409 3.397198438644409
Loss :  1.812783122062683 3.3821613788604736 3.3821613788604736
Loss :  1.8357244729995728 3.3885738849639893 3.3885738849639893
Loss :  1.8091920614242554 3.2477574348449707 3.2477574348449707
Loss :  1.8300366401672363 3.286550998687744 3.286550998687744
Loss :  1.815796136856079 3.2210276126861572 3.2210276126861572
Loss :  1.807599425315857 3.1529016494750977 3.1529016494750977
Loss :  1.8192427158355713 3.2733232975006104 3.2733232975006104
Loss :  1.8108903169631958 3.216752052307129 3.216752052307129
  batch 40 loss: 1.8108903169631958, 3.216752052307129, 3.216752052307129
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8157633543014526 3.479998826980591 3.479998826980591
Loss :  1.8039193153381348 3.1553144454956055 3.1553144454956055
Loss :  1.7898571491241455 3.277662992477417 3.277662992477417
Loss :  1.7970023155212402 3.178436279296875 3.178436279296875
Loss :  1.8015352487564087 3.027448892593384 3.027448892593384
Loss :  1.8124881982803345 3.247150421142578 3.247150421142578
Loss :  1.8123573064804077 3.1439411640167236 3.1439411640167236
Loss :  1.791698694229126 3.262774705886841 3.262774705886841
Loss :  1.8201022148132324 3.28122615814209 3.28122615814209
Loss :  1.7882381677627563 3.2512853145599365 3.2512853145599365
Loss :  1.7876914739608765 3.4083878993988037 3.4083878993988037
Loss :  1.8243380784988403 3.1140682697296143 3.1140682697296143
Loss :  1.835737943649292 3.3491947650909424 3.3491947650909424
Loss :  1.8214863538742065 3.0606627464294434 3.0606627464294434
Loss :  1.7918082475662231 3.3572804927825928 3.3572804927825928
Loss :  1.8266372680664062 3.223433256149292 3.223433256149292
Loss :  1.8370827436447144 3.264744997024536 3.264744997024536
Loss :  1.812113642692566 3.3635480403900146 3.3635480403900146
Loss :  1.8366159200668335 3.38769268989563 3.38769268989563
Loss :  1.8324919939041138 3.2929015159606934 3.2929015159606934
  batch 60 loss: 1.8324919939041138, 3.2929015159606934, 3.2929015159606934
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8343766927719116 3.3628311157226562 3.3628311157226562
Loss :  1.8118127584457397 3.3905744552612305 3.3905744552612305
Loss :  1.8139411211013794 3.219668388366699 3.219668388366699
Loss :  1.8174996376037598 3.2827985286712646 3.2827985286712646
Loss :  1.8084430694580078 2.8747644424438477 2.8747644424438477
Loss :  1.8435227870941162 3.784376382827759 3.784376382827759
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.8437119722366333 3.8827285766601562 3.8827285766601562
Loss :  1.81813383102417 3.6744182109832764 3.6744182109832764
Loss :  1.8529527187347412 3.656809091567993 3.656809091567993
Total LOSS train 3.3124447675851676 valid 3.749583065509796
CE LOSS train 1.8190684648660513 valid 0.4632381796836853
Contrastive LOSS train 3.3124447675851676 valid 0.9142022728919983
EPOCH 139:
Loss :  1.8119654655456543 3.103409767150879 3.103409767150879
Loss :  1.8265231847763062 3.689641237258911 3.689641237258911
Loss :  1.8124406337738037 3.360321521759033 3.360321521759033
Loss :  1.8054282665252686 3.38913893699646 3.38913893699646
Loss :  1.8099353313446045 3.3616340160369873 3.3616340160369873
Loss :  1.825624942779541 3.253699779510498 3.253699779510498
Loss :  1.8218153715133667 3.3441171646118164 3.3441171646118164
Loss :  1.827622890472412 3.1184229850769043 3.1184229850769043
Loss :  1.7880867719650269 3.402897596359253 3.402897596359253
Loss :  1.7933454513549805 3.359745979309082 3.359745979309082
Loss :  1.8072627782821655 3.5637166500091553 3.5637166500091553
Loss :  1.8305833339691162 3.6408724784851074 3.6408724784851074
Loss :  1.8061777353286743 3.5058207511901855 3.5058207511901855
Loss :  1.805935025215149 3.3307738304138184 3.3307738304138184
Loss :  1.7937047481536865 3.570209264755249 3.570209264755249
Loss :  1.8022081851959229 3.233905553817749 3.233905553817749
Loss :  1.7999578714370728 3.2373151779174805 3.2373151779174805
Loss :  1.795686960220337 3.2838363647460938 3.2838363647460938
Loss :  1.7921690940856934 3.421588659286499 3.421588659286499
Loss :  1.7924357652664185 3.132171154022217 3.132171154022217
  batch 20 loss: 1.7924357652664185, 3.132171154022217, 3.132171154022217
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7954908609390259 3.037574529647827 3.037574529647827
Loss :  1.8030805587768555 3.359835386276245 3.359835386276245
Loss :  1.8068996667861938 3.232102155685425 3.232102155685425
Loss :  1.8344683647155762 3.195578098297119 3.195578098297119
Loss :  1.8043444156646729 3.3917877674102783 3.3917877674102783
Loss :  1.8042125701904297 3.2717652320861816 3.2717652320861816
Loss :  1.8104009628295898 3.458249092102051 3.458249092102051
Loss :  1.8028697967529297 3.185997724533081 3.185997724533081
Loss :  1.8180153369903564 3.341499090194702 3.341499090194702
Loss :  1.7806161642074585 3.1975996494293213 3.1975996494293213
Loss :  1.8131296634674072 3.392608642578125 3.392608642578125
Loss :  1.7851935625076294 3.3420069217681885 3.3420069217681885
Loss :  1.792036771774292 3.075674295425415 3.075674295425415
Loss :  1.802232027053833 3.358407497406006 3.358407497406006
Loss :  1.789809226989746 3.477797508239746 3.477797508239746
Loss :  1.8074647188186646 3.391413450241089 3.391413450241089
Loss :  1.803525447845459 3.461871385574341 3.461871385574341
Loss :  1.7657034397125244 3.432915449142456 3.432915449142456
Loss :  1.7980051040649414 3.2785770893096924 3.2785770893096924
Loss :  1.7946887016296387 3.496737003326416 3.496737003326416
  batch 40 loss: 1.7946887016296387, 3.496737003326416, 3.496737003326416
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.792712926864624 3.2589609622955322 3.2589609622955322
Loss :  1.7841273546218872 3.322831869125366 3.322831869125366
Loss :  1.773836612701416 3.179873466491699 3.179873466491699
Loss :  1.7960785627365112 3.1743366718292236 3.1743366718292236
Loss :  1.7769931554794312 2.9871010780334473 2.9871010780334473
Loss :  1.7901124954223633 3.4113335609436035 3.4113335609436035
Loss :  1.7872259616851807 3.312591552734375 3.312591552734375
Loss :  1.7698085308074951 3.2947816848754883 3.2947816848754883
Loss :  1.7806260585784912 3.0547592639923096 3.0547592639923096
Loss :  1.7866660356521606 3.339695930480957 3.339695930480957
Loss :  1.7696313858032227 3.3873560428619385 3.3873560428619385
Loss :  1.7952028512954712 3.1071696281433105 3.1071696281433105
Loss :  1.8109253644943237 3.2350049018859863 3.2350049018859863
Loss :  1.8023490905761719 3.155026912689209 3.155026912689209
Loss :  1.7904179096221924 3.2556300163269043 3.2556300163269043
Loss :  1.8121373653411865 3.2444863319396973 3.2444863319396973
Loss :  1.81732976436615 3.325322389602661 3.325322389602661
Loss :  1.7826263904571533 3.058633327484131 3.058633327484131
Loss :  1.8300669193267822 3.38379168510437 3.38379168510437
Loss :  1.8152388334274292 3.1419413089752197 3.1419413089752197
  batch 60 loss: 1.8152388334274292, 3.1419413089752197, 3.1419413089752197
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8143770694732666 3.1280152797698975 3.1280152797698975
Loss :  1.7989271879196167 3.158118963241577 3.158118963241577
Loss :  1.8117403984069824 3.1176657676696777 3.1176657676696777
Loss :  1.8024187088012695 3.1232757568359375 3.1232757568359375
Loss :  1.79599928855896 2.9315009117126465 2.9315009117126465
Loss :  2.010474681854248 4.251215934753418 4.251215934753418
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9987815618515015 4.3813347816467285 4.3813347816467285
Loss :  1.9354182481765747 4.121764659881592 4.121764659881592
Loss :  2.0144035816192627 4.048386573791504 4.048386573791504
Total LOSS train 3.288806801575881 valid 4.2006754875183105
CE LOSS train 1.800748821405264 valid 0.5036008954048157
Contrastive LOSS train 3.288806801575881 valid 1.012096643447876
EPOCH 140:
Loss :  1.8036959171295166 3.3567824363708496 3.3567824363708496
Loss :  1.8122886419296265 3.654850482940674 3.654850482940674
Loss :  1.8009804487228394 3.51977276802063 3.51977276802063
Loss :  1.8063948154449463 3.121178388595581 3.121178388595581
Loss :  1.808300495147705 3.4951655864715576 3.4951655864715576
Loss :  1.8242865800857544 3.4967923164367676 3.4967923164367676
Loss :  1.810953974723816 3.3776564598083496 3.3776564598083496
Loss :  1.817081332206726 3.4621729850769043 3.4621729850769043
Loss :  1.7826038599014282 3.335676431655884 3.335676431655884
Loss :  1.7893315553665161 3.2287676334381104 3.2287676334381104
Loss :  1.798983097076416 3.479597806930542 3.479597806930542
Loss :  1.8212404251098633 3.26223087310791 3.26223087310791
Loss :  1.7984484434127808 3.4966835975646973 3.4966835975646973
Loss :  1.7971466779708862 3.2443645000457764 3.2443645000457764
Loss :  1.7790648937225342 3.3132176399230957 3.3132176399230957
Loss :  1.7853515148162842 3.4948108196258545 3.4948108196258545
Loss :  1.79092538356781 3.2888143062591553 3.2888143062591553
Loss :  1.782613754272461 3.1501076221466064 3.1501076221466064
Loss :  1.782882809638977 3.246436834335327 3.246436834335327
Loss :  1.782991647720337 3.238274335861206 3.238274335861206
  batch 20 loss: 1.782991647720337, 3.238274335861206, 3.238274335861206
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7940340042114258 3.585664987564087 3.585664987564087
Loss :  1.7963894605636597 3.3706436157226562 3.3706436157226562
Loss :  1.7980948686599731 3.253697395324707 3.253697395324707
Loss :  1.8269963264465332 3.2453267574310303 3.2453267574310303
Loss :  1.7979834079742432 3.449636697769165 3.449636697769165
Loss :  1.8041682243347168 3.05226469039917 3.05226469039917
Loss :  1.809866189956665 3.374849796295166 3.374849796295166
Loss :  1.8033447265625 3.0630383491516113 3.0630383491516113
Loss :  1.8197968006134033 3.1890761852264404 3.1890761852264404
Loss :  1.7841846942901611 3.132885456085205 3.132885456085205
Loss :  1.8125139474868774 3.3361237049102783 3.3361237049102783
Loss :  1.787575125694275 3.284264326095581 3.284264326095581
Loss :  1.7938028573989868 3.1961324214935303 3.1961324214935303
Loss :  1.8065218925476074 3.216726541519165 3.216726541519165
Loss :  1.7910491228103638 3.335709571838379 3.335709571838379
Loss :  1.8065353631973267 3.477069854736328 3.477069854736328
Loss :  1.8039697408676147 3.420231819152832 3.420231819152832
Loss :  1.7686567306518555 3.3367419242858887 3.3367419242858887
Loss :  1.7961055040359497 3.4587113857269287 3.4587113857269287
Loss :  1.797591209411621 3.1533541679382324 3.1533541679382324
  batch 40 loss: 1.797591209411621, 3.1533541679382324, 3.1533541679382324
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7932848930358887 3.275182008743286 3.275182008743286
Loss :  1.7924606800079346 2.9666686058044434 2.9666686058044434
Loss :  1.78778076171875 3.108227014541626 3.108227014541626
Loss :  1.8009456396102905 2.971698045730591 2.971698045730591
Loss :  1.7934935092926025 2.988128900527954 2.988128900527954
Loss :  1.7954483032226562 3.10661244392395 3.10661244392395
Loss :  1.7940306663513184 3.0538136959075928 3.0538136959075928
Loss :  1.7954875230789185 3.635284423828125 3.635284423828125
Loss :  1.7916260957717896 3.3532662391662598 3.3532662391662598
Loss :  1.8098622560501099 3.4400839805603027 3.4400839805603027
Loss :  1.7937872409820557 3.3853487968444824 3.3853487968444824
Loss :  1.8068867921829224 3.5312650203704834 3.5312650203704834
Loss :  1.8765335083007812 4.382348537445068 4.382348537445068
Loss :  1.7869760990142822 3.4153952598571777 3.4153952598571777
Loss :  1.800506591796875 4.197633743286133 4.197633743286133
Loss :  1.8650963306427002 3.886887550354004 3.886887550354004
Loss :  1.846766710281372 3.9029927253723145 3.9029927253723145
Loss :  1.8292185068130493 3.3171207904815674 3.3171207904815674
Loss :  1.8402149677276611 3.5917155742645264 3.5917155742645264
Loss :  1.833422064781189 3.2049062252044678 3.2049062252044678
  batch 60 loss: 1.833422064781189, 3.2049062252044678, 3.2049062252044678
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8202526569366455 3.3167879581451416 3.3167879581451416
Loss :  1.8143974542617798 3.217501401901245 3.217501401901245
Loss :  1.8281004428863525 3.199113368988037 3.199113368988037
Loss :  1.808149814605713 3.2493600845336914 3.2493600845336914
Loss :  1.8007551431655884 3.0733861923217773 3.0733861923217773
Loss :  1.8406726121902466 4.032485485076904 4.032485485076904
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.8041727542877197 4.213009357452393 4.213009357452393
Loss :  1.8339146375656128 4.063260555267334 4.063260555267334
Loss :  1.8333618640899658 3.8153226375579834 3.8153226375579834
Total LOSS train 3.353326647098248 valid 4.031019508838654
CE LOSS train 1.8043112479723418 valid 0.45834046602249146
Contrastive LOSS train 3.353326647098248 valid 0.9538306593894958
EPOCH 141:
Loss :  1.7824994325637817 3.114455223083496 3.114455223083496
Loss :  1.7986329793930054 3.4212677478790283 3.4212677478790283
Loss :  1.7798264026641846 3.4666240215301514 3.4666240215301514
Loss :  1.7829914093017578 3.366713762283325 3.366713762283325
Loss :  1.7880663871765137 3.4536805152893066 3.4536805152893066
Loss :  1.796025037765503 3.558361291885376 3.558361291885376
Loss :  1.7831560373306274 3.9203946590423584 3.9203946590423584
Loss :  1.7887299060821533 4.070893287658691 4.070893287658691
Loss :  1.7990666627883911 4.1065449714660645 4.1065449714660645
Loss :  1.7763028144836426 3.4539501667022705 3.4539501667022705
Loss :  1.79213547706604 3.694780111312866 3.694780111312866
Loss :  1.8160532712936401 3.667992353439331 3.667992353439331
Loss :  1.7962182760238647 3.395080089569092 3.395080089569092
Loss :  1.8084152936935425 3.326183795928955 3.326183795928955
Loss :  1.8006657361984253 3.306793212890625 3.306793212890625
Loss :  1.8135448694229126 3.2366604804992676 3.2366604804992676
Loss :  1.8067657947540283 3.2181007862091064 3.2181007862091064
Loss :  1.8120179176330566 3.4443843364715576 3.4443843364715576
Loss :  1.786359190940857 3.137951374053955 3.137951374053955
Loss :  1.7949506044387817 3.378373146057129 3.378373146057129
  batch 20 loss: 1.7949506044387817, 3.378373146057129, 3.378373146057129
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8025821447372437 3.4881622791290283 3.4881622791290283
Loss :  1.8065693378448486 3.4933736324310303 3.4933736324310303
Loss :  1.8106271028518677 3.4468376636505127 3.4468376636505127
Loss :  1.8357925415039062 3.5540759563446045 3.5540759563446045
Loss :  1.800567388534546 3.6062629222869873 3.6062629222869873
Loss :  1.8010622262954712 3.3482019901275635 3.3482019901275635
Loss :  1.8018076419830322 3.759401798248291 3.759401798248291
Loss :  1.7980173826217651 3.1314480304718018 3.1314480304718018
Loss :  1.8099313974380493 3.406468629837036 3.406468629837036
Loss :  1.782683253288269 3.079084873199463 3.079084873199463
Loss :  1.811185598373413 3.3363184928894043 3.3363184928894043
Loss :  1.7844221591949463 3.384866952896118 3.384866952896118
Loss :  1.7959461212158203 3.210329532623291 3.210329532623291
Loss :  1.808432698249817 3.4555020332336426 3.4555020332336426
Loss :  1.7917670011520386 3.2819817066192627 3.2819817066192627
Loss :  1.8080049753189087 3.2058310508728027 3.2058310508728027
Loss :  1.8055031299591064 3.2244207859039307 3.2244207859039307
Loss :  1.776296615600586 2.955420970916748 2.955420970916748
Loss :  1.799712896347046 3.084055185317993 3.084055185317993
Loss :  1.8040897846221924 3.005126953125 3.005126953125
  batch 40 loss: 1.8040897846221924, 3.005126953125, 3.005126953125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.798550009727478 3.3540749549865723 3.3540749549865723
Loss :  1.7934017181396484 3.2875049114227295 3.2875049114227295
Loss :  1.7725656032562256 3.3422694206237793 3.3422694206237793
Loss :  1.8026251792907715 3.3439300060272217 3.3439300060272217
Loss :  1.7825744152069092 2.813101053237915 2.813101053237915
Loss :  1.8030675649642944 3.1414635181427 3.1414635181427
Loss :  1.7988834381103516 3.0685768127441406 3.0685768127441406
Loss :  1.7669817209243774 3.2754390239715576 3.2754390239715576
Loss :  1.8007891178131104 3.0045664310455322 3.0045664310455322
Loss :  1.7827857732772827 3.222324848175049 3.222324848175049
Loss :  1.7746771574020386 3.2581191062927246 3.2581191062927246
Loss :  1.8009254932403564 3.0139846801757812 3.0139846801757812
Loss :  1.8203829526901245 3.1611862182617188 3.1611862182617188
Loss :  1.8162007331848145 3.0504214763641357 3.0504214763641357
Loss :  1.7828686237335205 3.105480670928955 3.105480670928955
Loss :  1.8077952861785889 3.0293986797332764 3.0293986797332764
Loss :  1.8261663913726807 3.1501591205596924 3.1501591205596924
Loss :  1.7935911417007446 3.3379054069519043 3.3379054069519043
Loss :  1.8289210796356201 3.498023748397827 3.498023748397827
Loss :  1.8075450658798218 3.2757482528686523 3.2757482528686523
  batch 60 loss: 1.8075450658798218, 3.2757482528686523, 3.2757482528686523
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8260717391967773 3.0333974361419678 3.0333974361419678
Loss :  1.7980273962020874 3.3355748653411865 3.3355748653411865
Loss :  1.8092681169509888 3.0668210983276367 3.0668210983276367
Loss :  1.8059056997299194 3.244910955429077 3.244910955429077
Loss :  1.7969293594360352 2.612422466278076 2.612422466278076
Loss :  1.6352756023406982 4.304934501647949 4.304934501647949
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.6427807807922363 4.369956970214844 4.369956970214844
Loss :  1.6369209289550781 4.181443214416504 4.181443214416504
Loss :  1.6577012538909912 4.223911762237549 4.223911762237549
Total LOSS train 3.3111255682431735 valid 4.270061612129211
CE LOSS train 1.7989988565444945 valid 0.4144253134727478
Contrastive LOSS train 3.3111255682431735 valid 1.0559779405593872
EPOCH 142:
Loss :  1.795006275177002 2.854997396469116 2.854997396469116
Loss :  1.8169372081756592 3.4823248386383057 3.4823248386383057
Loss :  1.8006209135055542 3.1709325313568115 3.1709325313568115
Loss :  1.8041363954544067 2.999291181564331 2.999291181564331
Loss :  1.8072116374969482 3.2946324348449707 3.2946324348449707
Loss :  1.8207767009735107 3.2827565670013428 3.2827565670013428
Loss :  1.8097485303878784 3.4534788131713867 3.4534788131713867
Loss :  1.8196684122085571 3.109668016433716 3.109668016433716
Loss :  1.7844212055206299 3.21589732170105 3.21589732170105
Loss :  1.791900873184204 3.8821280002593994 3.8821280002593994
Loss :  1.8010241985321045 3.4981422424316406 3.4981422424316406
Loss :  1.822832703590393 3.5230965614318848 3.5230965614318848
Loss :  1.8025940656661987 3.2290756702423096 3.2290756702423096
Loss :  1.8044542074203491 3.275470733642578 3.275470733642578
Loss :  1.7866339683532715 3.224717855453491 3.224717855453491
Loss :  1.804769515991211 3.163294792175293 3.163294792175293
Loss :  1.80129873752594 3.336277723312378 3.336277723312378
Loss :  1.8021094799041748 3.2622289657592773 3.2622289657592773
Loss :  1.7845817804336548 2.9897279739379883 2.9897279739379883
Loss :  1.7915894985198975 3.0294761657714844 3.0294761657714844
  batch 20 loss: 1.7915894985198975, 3.0294761657714844, 3.0294761657714844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7929949760437012 2.987579822540283 2.987579822540283
Loss :  1.8038268089294434 3.20646333694458 3.20646333694458
Loss :  1.8050031661987305 3.0962703227996826 3.0962703227996826
Loss :  1.825905203819275 3.1357133388519287 3.1357133388519287
Loss :  1.7992655038833618 3.354992151260376 3.354992151260376
Loss :  1.8080917596817017 3.088994026184082 3.088994026184082
Loss :  1.814564824104309 3.402871608734131 3.402871608734131
Loss :  1.8041698932647705 3.355909585952759 3.355909585952759
Loss :  1.8213908672332764 3.5743250846862793 3.5743250846862793
Loss :  1.7846766710281372 3.1973230838775635 3.1973230838775635
Loss :  1.8134208917617798 3.393129825592041 3.393129825592041
Loss :  1.7873672246932983 3.2648725509643555 3.2648725509643555
Loss :  1.7951445579528809 3.3559374809265137 3.3559374809265137
Loss :  1.8028558492660522 3.3589813709259033 3.3589813709259033
Loss :  1.7871882915496826 3.6349611282348633 3.6349611282348633
Loss :  1.8019869327545166 3.4365665912628174 3.4365665912628174
Loss :  1.8005437850952148 3.2099266052246094 3.2099266052246094
Loss :  1.7629014253616333 3.0809667110443115 3.0809667110443115
Loss :  1.7904480695724487 3.111269235610962 3.111269235610962
Loss :  1.7885370254516602 2.876614809036255 2.876614809036255
  batch 40 loss: 1.7885370254516602, 2.876614809036255, 2.876614809036255
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7842762470245361 3.1862032413482666 3.1862032413482666
Loss :  1.7837063074111938 3.1721506118774414 3.1721506118774414
Loss :  1.7797186374664307 3.0830702781677246 3.0830702781677246
Loss :  1.7882894277572632 3.2959535121917725 3.2959535121917725
Loss :  1.859113097190857 4.222619533538818 4.222619533538818
Loss :  1.792396903038025 3.7891457080841064 3.7891457080841064
Loss :  1.7794829607009888 3.33669376373291 3.33669376373291
Loss :  1.7639694213867188 3.053436040878296 3.053436040878296
Loss :  1.781915545463562 2.9444658756256104 2.9444658756256104
Loss :  1.7884712219238281 3.3592920303344727 3.3592920303344727
Loss :  1.7687840461730957 3.487091064453125 3.487091064453125
Loss :  1.7983109951019287 3.0301215648651123 3.0301215648651123
Loss :  1.8127119541168213 3.1090152263641357 3.1090152263641357
Loss :  1.809552788734436 3.0507938861846924 3.0507938861846924
Loss :  1.79169762134552 3.4748754501342773 3.4748754501342773
Loss :  1.811747670173645 3.280820369720459 3.280820369720459
Loss :  1.8221590518951416 3.3144309520721436 3.3144309520721436
Loss :  1.7899583578109741 3.3132376670837402 3.3132376670837402
Loss :  1.8265118598937988 3.397904872894287 3.397904872894287
Loss :  1.8055177927017212 3.3585305213928223 3.3585305213928223
  batch 60 loss: 1.8055177927017212, 3.3585305213928223, 3.3585305213928223
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.820396065711975 3.285475492477417 3.285475492477417
Loss :  1.7934988737106323 3.3689229488372803 3.3689229488372803
Loss :  1.803901195526123 3.1327619552612305 3.1327619552612305
Loss :  1.8020471334457397 3.62168550491333 3.62168550491333
Loss :  1.7943181991577148 3.129997491836548 3.129997491836548
Loss :  1.7788711786270142 4.237852573394775 4.237852573394775
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7923849821090698 4.390254497528076 4.390254497528076
Loss :  1.779292106628418 4.15536642074585 4.15536642074585
Loss :  1.7871837615966797 4.07355260848999 4.07355260848999
Total LOSS train 3.279999692623432 valid 4.214256525039673
CE LOSS train 1.7999238986235397 valid 0.4467959403991699
Contrastive LOSS train 3.279999692623432 valid 1.0183881521224976
EPOCH 143:
Loss :  1.803981900215149 4.48870325088501 4.48870325088501
Loss :  1.816911220550537 4.162442207336426 4.162442207336426
Loss :  1.8095794916152954 4.001904010772705 4.001904010772705
Loss :  1.810652256011963 3.4957306385040283 3.4957306385040283
Loss :  1.8180352449417114 3.2095096111297607 3.2095096111297607
Loss :  1.818081259727478 3.372138738632202 3.372138738632202
Loss :  1.8149384260177612 3.7008755207061768 3.7008755207061768
Loss :  1.8196293115615845 3.272449254989624 3.272449254989624
Loss :  1.83757746219635 3.335271120071411 3.335271120071411
Loss :  1.867801547050476 3.165126323699951 3.165126323699951
Loss :  1.861730933189392 3.8277926445007324 3.8277926445007324
Loss :  1.8370816707611084 3.8520452976226807 3.8520452976226807
Loss :  1.8752988576889038 3.527193784713745 3.527193784713745
Loss :  1.869646430015564 3.5390732288360596 3.5390732288360596
Loss :  1.8687491416931152 3.5624401569366455 3.5624401569366455
Loss :  1.8894124031066895 3.655677080154419 3.655677080154419
Loss :  1.8792510032653809 3.91032075881958 3.91032075881958
Loss :  1.8948209285736084 3.983367443084717 3.983367443084717
Loss :  1.8760002851486206 3.7400104999542236 3.7400104999542236
Loss :  1.8850266933441162 3.8632447719573975 3.8632447719573975
  batch 20 loss: 1.8850266933441162, 3.8632447719573975, 3.8632447719573975
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8839589357376099 3.535618543624878 3.535618543624878
Loss :  1.878343939781189 3.4836552143096924 3.4836552143096924
Loss :  1.8813194036483765 3.3790361881256104 3.3790361881256104
Loss :  1.881176471710205 3.369877815246582 3.369877815246582
Loss :  1.858141303062439 3.5164589881896973 3.5164589881896973
Loss :  1.8837982416152954 3.405818223953247 3.405818223953247
Loss :  1.8819060325622559 3.653820514678955 3.653820514678955
Loss :  1.8883110284805298 3.37386417388916 3.37386417388916
Loss :  1.8801686763763428 3.6662604808807373 3.6662604808807373
Loss :  1.854544758796692 3.3859975337982178 3.3859975337982178
Loss :  1.851607322692871 3.8397421836853027 3.8397421836853027
Loss :  1.8824894428253174 3.5215580463409424 3.5215580463409424
Loss :  1.8890371322631836 3.330984592437744 3.330984592437744
Loss :  1.8792020082473755 3.615145206451416 3.615145206451416
Loss :  1.8768928050994873 3.9361391067504883 3.9361391067504883
Loss :  1.8747695684432983 3.6739797592163086 3.6739797592163086
Loss :  1.8846436738967896 3.7549445629119873 3.7549445629119873
Loss :  1.905522108078003 3.5209944248199463 3.5209944248199463
Loss :  1.8755005598068237 3.731877326965332 3.731877326965332
Loss :  1.885229468345642 3.3990330696105957 3.3990330696105957
  batch 40 loss: 1.885229468345642, 3.3990330696105957, 3.3990330696105957
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8891066312789917 3.74056077003479 3.74056077003479
Loss :  1.876556396484375 3.3572707176208496 3.3572707176208496
Loss :  1.8733031749725342 3.5318124294281006 3.5318124294281006
Loss :  1.86809504032135 3.484999418258667 3.484999418258667
Loss :  1.874401330947876 3.153491735458374 3.153491735458374
Loss :  1.8986717462539673 3.322748899459839 3.322748899459839
Loss :  1.9003355503082275 3.426189422607422 3.426189422607422
Loss :  1.8759145736694336 3.799207925796509 3.799207925796509
Loss :  1.906574010848999 4.341421127319336 4.341421127319336
Loss :  1.8724786043167114 4.018060684204102 4.018060684204102
Loss :  1.8686457872390747 3.3488123416900635 3.3488123416900635
Loss :  1.8922570943832397 3.63955020904541 3.63955020904541
Loss :  1.863484263420105 3.535503625869751 3.535503625869751
Loss :  1.858412265777588 3.3040642738342285 3.3040642738342285
Loss :  1.85173499584198 3.6101231575012207 3.6101231575012207
Loss :  1.895277500152588 3.537348508834839 3.537348508834839
Loss :  1.8669829368591309 3.6119654178619385 3.6119654178619385
Loss :  1.8818691968917847 3.6637065410614014 3.6637065410614014
Loss :  1.8854880332946777 3.6639585494995117 3.6639585494995117
Loss :  1.878478765487671 3.688098907470703 3.688098907470703
  batch 60 loss: 1.878478765487671, 3.688098907470703, 3.688098907470703
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.87832510471344 3.3962581157684326 3.3962581157684326
Loss :  1.878318190574646 3.7199318408966064 3.7199318408966064
Loss :  1.888134479522705 3.8880834579467773 3.8880834579467773
Loss :  1.8791671991348267 3.602752208709717 3.602752208709717
Loss :  1.8812791109085083 2.9920856952667236 2.9920856952667236
Loss :  2.0355353355407715 3.9010977745056152 3.9010977745056152
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.982804536819458 4.0552544593811035 4.0552544593811035
Loss :  2.0469810962677 3.802459716796875 3.802459716796875
Loss :  1.9704416990280151 3.863269329071045 3.863269329071045
Total LOSS train 3.602125050471379 valid 3.9055203199386597
CE LOSS train 1.8694470974115225 valid 0.4926104247570038
Contrastive LOSS train 3.602125050471379 valid 0.9658173322677612
EPOCH 144:
Loss :  1.8872064352035522 3.3312246799468994 3.3312246799468994
Loss :  1.8740015029907227 3.805948495864868 3.805948495864868
Loss :  1.8831565380096436 3.3954808712005615 3.3954808712005615
Loss :  1.8794039487838745 3.2515885829925537 3.2515885829925537
Loss :  1.8793859481811523 3.4751243591308594 3.4751243591308594
Loss :  1.8810042142868042 3.679091691970825 3.679091691970825
Loss :  1.8843154907226562 3.3588979244232178 3.3588979244232178
Loss :  1.8809404373168945 3.0899713039398193 3.0899713039398193
Loss :  1.864197015762329 3.480041742324829 3.480041742324829
Loss :  1.903996467590332 3.622892141342163 3.622892141342163
Loss :  1.889134407043457 3.7578654289245605 3.7578654289245605
Loss :  1.8536150455474854 3.8473448753356934 3.8473448753356934
Loss :  1.8989721536636353 3.5191574096679688 3.5191574096679688
Loss :  1.867219090461731 3.5134494304656982 3.5134494304656982
Loss :  1.8559069633483887 3.434915065765381 3.434915065765381
Loss :  1.89112389087677 3.4702224731445312 3.4702224731445312
Loss :  1.8829227685928345 3.1332240104675293 3.1332240104675293
Loss :  1.9025230407714844 3.649709463119507 3.649709463119507
Loss :  1.8709596395492554 3.4774019718170166 3.4774019718170166
Loss :  1.881270408630371 3.529482126235962 3.529482126235962
  batch 20 loss: 1.881270408630371, 3.529482126235962, 3.529482126235962
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8722163438796997 3.823146343231201 3.823146343231201
Loss :  1.8729158639907837 3.6569528579711914 3.6569528579711914
Loss :  1.8753050565719604 3.570040225982666 3.570040225982666
Loss :  1.8645445108413696 3.7145159244537354 3.7145159244537354
Loss :  1.8369874954223633 3.6140334606170654 3.6140334606170654
Loss :  1.8585559129714966 3.346280813217163 3.346280813217163
Loss :  1.8446869850158691 3.535468578338623 3.535468578338623
Loss :  1.8545444011688232 3.3962438106536865 3.3962438106536865
Loss :  1.8520004749298096 3.665419816970825 3.665419816970825
Loss :  1.830459475517273 3.605456590652466 3.605456590652466
Loss :  1.8300137519836426 3.5271592140197754 3.5271592140197754
Loss :  1.8505151271820068 3.3136210441589355 3.3136210441589355
Loss :  1.8575226068496704 3.60886287689209 3.60886287689209
Loss :  1.8541630506515503 3.262777805328369 3.262777805328369
Loss :  1.858110785484314 3.530620813369751 3.530620813369751
Loss :  1.8455804586410522 3.4355785846710205 3.4355785846710205
Loss :  1.858383297920227 3.6866655349731445 3.6866655349731445
Loss :  1.8734567165374756 3.5631930828094482 3.5631930828094482
Loss :  1.8560487031936646 3.3900563716888428 3.3900563716888428
Loss :  1.8689199686050415 3.548191785812378 3.548191785812378
  batch 40 loss: 1.8689199686050415, 3.548191785812378, 3.548191785812378
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8695828914642334 3.4804775714874268 3.4804775714874268
Loss :  1.8573929071426392 3.217719793319702 3.217719793319702
Loss :  1.865071177482605 3.3601276874542236 3.3601276874542236
Loss :  1.8499327898025513 3.520916700363159 3.520916700363159
Loss :  1.8541842699050903 3.406202554702759 3.406202554702759
Loss :  1.8509280681610107 3.376772403717041 3.376772403717041
Loss :  1.8562675714492798 3.4194860458374023 3.4194860458374023
Loss :  1.8323618173599243 3.5717594623565674 3.5717594623565674
Loss :  1.8492882251739502 3.3560802936553955 3.3560802936553955
Loss :  1.8437837362289429 3.4355807304382324 3.4355807304382324
Loss :  1.8411561250686646 3.5771467685699463 3.5771467685699463
Loss :  1.854608178138733 3.565075397491455 3.565075397491455
Loss :  1.853507161140442 3.4549825191497803 3.4549825191497803
Loss :  1.8521136045455933 3.770296335220337 3.770296335220337
Loss :  1.830549955368042 3.3808796405792236 3.3808796405792236
Loss :  1.8601418733596802 3.2076480388641357 3.2076480388641357
Loss :  1.8461564779281616 3.426377058029175 3.426377058029175
Loss :  1.852570652961731 3.4275174140930176 3.4275174140930176
Loss :  1.8518922328948975 3.5014517307281494 3.5014517307281494
Loss :  1.856991171836853 3.8920490741729736 3.8920490741729736
  batch 60 loss: 1.856991171836853, 3.8920490741729736, 3.8920490741729736
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8578883409500122 3.7227399349212646 3.7227399349212646
Loss :  1.8452445268630981 3.2821686267852783 3.2821686267852783
Loss :  1.8639240264892578 3.2815005779266357 3.2815005779266357
Loss :  1.855762243270874 3.069011926651001 3.069011926651001
Loss :  1.8651785850524902 3.014735221862793 3.014735221862793
Loss :  1.9935946464538574 3.645454168319702 3.645454168319702
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9415875673294067 3.7615652084350586 3.7615652084350586
Loss :  1.9824039936065674 3.6321702003479004 3.6321702003479004
Loss :  1.98330819606781 3.617267370223999 3.617267370223999
Total LOSS train 3.4820926629579985 valid 3.664114236831665
CE LOSS train 1.862194846226619 valid 0.4958270490169525
Contrastive LOSS train 3.4820926629579985 valid 0.9043168425559998
EPOCH 145:
Loss :  1.863555908203125 3.1746678352355957 3.1746678352355957
Loss :  1.8650435209274292 3.497607707977295 3.497607707977295
Loss :  1.8805636167526245 3.5289981365203857 3.5289981365203857
Loss :  1.8656067848205566 3.355349063873291 3.355349063873291
Loss :  1.8553876876831055 3.3973846435546875 3.3973846435546875
Loss :  1.8662246465682983 3.354830503463745 3.354830503463745
Loss :  1.868516445159912 3.4642462730407715 3.4642462730407715
Loss :  1.8595019578933716 3.300250768661499 3.300250768661499
Loss :  1.8475401401519775 3.4840962886810303 3.4840962886810303
Loss :  1.8697861433029175 3.442456007003784 3.442456007003784
Loss :  1.8560972213745117 3.487950086593628 3.487950086593628
Loss :  1.8388479948043823 3.763296365737915 3.763296365737915
Loss :  1.8668063879013062 3.551959276199341 3.551959276199341
Loss :  1.8552072048187256 3.3486390113830566 3.3486390113830566
Loss :  1.8487014770507812 3.382660388946533 3.382660388946533
Loss :  1.8544397354125977 3.421891212463379 3.421891212463379
Loss :  1.8565773963928223 3.2646708488464355 3.2646708488464355
Loss :  1.865775227546692 3.590043306350708 3.590043306350708
Loss :  1.8511115312576294 3.5583810806274414 3.5583810806274414
Loss :  1.8627744913101196 3.264826536178589 3.264826536178589
  batch 20 loss: 1.8627744913101196, 3.264826536178589, 3.264826536178589
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8644897937774658 3.2083749771118164 3.2083749771118164
Loss :  1.864499807357788 3.1597025394439697 3.1597025394439697
Loss :  1.8726067543029785 3.2011208534240723 3.2011208534240723
Loss :  1.8703845739364624 3.1928024291992188 3.1928024291992188
Loss :  1.8590165376663208 3.494475841522217 3.494475841522217
Loss :  1.8610661029815674 3.236422538757324 3.236422538757324
Loss :  1.8582775592803955 3.692173480987549 3.692173480987549
Loss :  1.8519471883773804 3.478842258453369 3.478842258453369
Loss :  1.8529598712921143 3.6434903144836426 3.6434903144836426
Loss :  1.8313623666763306 3.4824752807617188 3.4824752807617188
Loss :  1.8288322687149048 3.6613917350769043 3.6613917350769043
Loss :  1.826399564743042 3.768798828125 3.768798828125
Loss :  1.8521658182144165 3.6166203022003174 3.6166203022003174
Loss :  1.8700648546218872 3.607506275177002 3.607506275177002
Loss :  1.8606268167495728 3.736837148666382 3.736837148666382
Loss :  1.8629273176193237 3.659092903137207 3.659092903137207
Loss :  1.8382883071899414 3.7098357677459717 3.7098357677459717
Loss :  1.8386356830596924 3.5431172847747803 3.5431172847747803
Loss :  1.8287376165390015 4.044659614562988 4.044659614562988
Loss :  1.8216404914855957 3.9008326530456543 3.9008326530456543
  batch 40 loss: 1.8216404914855957, 3.9008326530456543, 3.9008326530456543
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8295055627822876 3.702496290206909 3.702496290206909
Loss :  1.8124750852584839 3.7974841594696045 3.7974841594696045
Loss :  1.8126823902130127 3.85355281829834 3.85355281829834
Loss :  1.830507516860962 3.926408052444458 3.926408052444458
Loss :  1.8407880067825317 3.5950965881347656 3.5950965881347656
Loss :  1.8578319549560547 3.38486909866333 3.38486909866333
Loss :  1.8611475229263306 3.72208833694458 3.72208833694458
Loss :  1.8424534797668457 3.445514440536499 3.445514440536499
Loss :  1.857143521308899 3.544271945953369 3.544271945953369
Loss :  1.8297511339187622 3.5853490829467773 3.5853490829467773
Loss :  1.815963864326477 3.398449659347534 3.398449659347534
Loss :  1.8273663520812988 3.207463026046753 3.207463026046753
Loss :  1.8273674249649048 3.285839319229126 3.285839319229126
Loss :  1.810998797416687 3.2409489154815674 3.2409489154815674
Loss :  1.8071237802505493 3.6695191860198975 3.6695191860198975
Loss :  1.841718077659607 3.2100415229797363 3.2100415229797363
Loss :  1.8354218006134033 3.601332187652588 3.601332187652588
Loss :  1.8401892185211182 3.334782600402832 3.334782600402832
Loss :  1.8502109050750732 3.594059705734253 3.594059705734253
Loss :  1.8537931442260742 3.362964153289795 3.362964153289795
  batch 60 loss: 1.8537931442260742, 3.362964153289795, 3.362964153289795
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8597381114959717 3.6991186141967773 3.6991186141967773
Loss :  1.8527569770812988 3.184037446975708 3.184037446975708
Loss :  1.8718440532684326 3.479963541030884 3.479963541030884
Loss :  1.8605459928512573 3.48241925239563 3.48241925239563
Loss :  1.8730076551437378 2.8978166580200195 2.8978166580200195
Loss :  2.01584529876709 3.54095196723938 3.54095196723938
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9497685432434082 3.7389297485351562 3.7389297485351562
Loss :  2.013744592666626 3.344762086868286 3.344762086868286
Loss :  1.9634885787963867 3.4577064514160156 3.4577064514160156
Total LOSS train 3.490440999544584 valid 3.5205875635147095
CE LOSS train 1.8494665714410634 valid 0.4908721446990967
Contrastive LOSS train 3.490440999544584 valid 0.8644266128540039
Saved best model. Old loss 3.552398920059204 and new best loss 3.5205875635147095
EPOCH 146:
Loss :  1.8818451166152954 3.1554551124572754 3.1554551124572754
Loss :  1.8712105751037598 3.921674966812134 3.921674966812134
Loss :  1.884301781654358 3.5291097164154053 3.5291097164154053
Loss :  1.886460304260254 3.5081982612609863 3.5081982612609863
Loss :  1.8743739128112793 3.4945361614227295 3.4945361614227295
Loss :  1.8817460536956787 3.4612441062927246 3.4612441062927246
Loss :  1.8788721561431885 3.3343374729156494 3.3343374729156494
Loss :  1.8695605993270874 3.1927919387817383 3.1927919387817383
Loss :  1.856952428817749 3.3320424556732178 3.3320424556732178
Loss :  1.882352352142334 3.0589988231658936 3.0589988231658936
Loss :  1.859542965888977 3.453582286834717 3.453582286834717
Loss :  1.8429490327835083 3.680023670196533 3.680023670196533
Loss :  1.870958924293518 3.5562827587127686 3.5562827587127686
Loss :  1.8526506423950195 3.3719019889831543 3.3719019889831543
Loss :  1.859070897102356 3.431688070297241 3.431688070297241
Loss :  1.856955885887146 3.5002200603485107 3.5002200603485107
Loss :  1.85514235496521 3.1578609943389893 3.1578609943389893
Loss :  1.8537237644195557 3.2660293579101562 3.2660293579101562
Loss :  1.8375680446624756 3.2596380710601807 3.2596380710601807
Loss :  1.8479472398757935 3.7592735290527344 3.7592735290527344
  batch 20 loss: 1.8479472398757935, 3.7592735290527344, 3.7592735290527344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8418583869934082 3.6701037883758545 3.6701037883758545
Loss :  1.8390190601348877 3.542135715484619 3.542135715484619
Loss :  1.8367663621902466 3.3809497356414795 3.3809497356414795
Loss :  1.8435606956481934 3.229030132293701 3.229030132293701
Loss :  1.8331379890441895 3.6137378215789795 3.6137378215789795
Loss :  1.8436552286148071 3.0894665718078613 3.0894665718078613
Loss :  1.8477587699890137 3.459691286087036 3.459691286087036
Loss :  1.855676531791687 3.1781299114227295 3.1781299114227295
Loss :  1.8601990938186646 3.237678289413452 3.237678289413452
Loss :  1.8511815071105957 3.2400803565979004 3.2400803565979004
Loss :  1.838336706161499 3.5761420726776123 3.5761420726776123
Loss :  1.8568474054336548 3.6178371906280518 3.6178371906280518
Loss :  1.8665506839752197 3.681631565093994 3.681631565093994
Loss :  1.8627396821975708 3.352581262588501 3.352581262588501
Loss :  1.8601603507995605 3.4124958515167236 3.4124958515167236
Loss :  1.8508474826812744 3.4902453422546387 3.4902453422546387
Loss :  1.854345679283142 3.4836742877960205 3.4836742877960205
Loss :  1.8362740278244019 3.753082275390625 3.753082275390625
Loss :  1.797299861907959 4.027132034301758 4.027132034301758
Loss :  1.7896931171417236 4.045679569244385 4.045679569244385
  batch 40 loss: 1.7896931171417236, 4.045679569244385, 4.045679569244385
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8082163333892822 4.222558498382568 4.222558498382568
Loss :  1.8127695322036743 3.5914549827575684 3.5914549827575684
Loss :  1.8356255292892456 3.625460386276245 3.625460386276245
Loss :  1.7999811172485352 3.468142032623291 3.468142032623291
Loss :  1.835351586341858 3.327354669570923 3.327354669570923
Loss :  1.827453374862671 3.876896858215332 3.876896858215332
Loss :  1.8128876686096191 3.7875726222991943 3.7875726222991943
Loss :  1.8237054347991943 4.11892557144165 4.11892557144165
Loss :  1.7904661893844604 3.939066171646118 3.939066171646118
Loss :  1.8694709539413452 3.925633192062378 3.925633192062378
Loss :  1.8639159202575684 4.059274196624756 4.059274196624756
Loss :  1.8383119106292725 3.5990676879882812 3.5990676879882812
Loss :  1.8155765533447266 3.7758634090423584 3.7758634090423584
Loss :  1.8436675071716309 4.115592956542969 4.115592956542969
Loss :  1.861688256263733 3.475947141647339 3.475947141647339
Loss :  1.810567855834961 3.3462679386138916 3.3462679386138916
Loss :  1.8387596607208252 3.905538320541382 3.905538320541382
Loss :  1.8691495656967163 3.738617420196533 3.738617420196533
Loss :  1.8430110216140747 3.694662570953369 3.694662570953369
Loss :  1.830444097518921 3.3884284496307373 3.3884284496307373
  batch 60 loss: 1.830444097518921, 3.3884284496307373, 3.3884284496307373
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8461933135986328 3.5270228385925293 3.5270228385925293
Loss :  1.8339754343032837 3.373389482498169 3.373389482498169
Loss :  1.8246619701385498 3.309999704360962 3.309999704360962
Loss :  1.8307732343673706 3.1620333194732666 3.1620333194732666
Loss :  1.8521355390548706 2.9388322830200195 2.9388322830200195
Loss :  1.8142192363739014 4.228424549102783 4.228424549102783
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.8232786655426025 4.239608287811279 4.239608287811279
Loss :  1.788657307624817 4.130273818969727 4.130273818969727
Loss :  1.7997381687164307 3.877397060394287 3.877397060394287
Total LOSS train 3.5353845779712385 valid 4.118925929069519
CE LOSS train 1.8459823571718657 valid 0.44993454217910767
Contrastive LOSS train 3.5353845779712385 valid 0.9693492650985718
EPOCH 147:
Loss :  1.835286259651184 3.533646821975708 3.533646821975708
Loss :  1.7877370119094849 3.4475362300872803 3.4475362300872803
Loss :  1.8285571336746216 3.076246738433838 3.076246738433838
Loss :  1.842850685119629 3.0327537059783936 3.0327537059783936
Loss :  1.8244248628616333 2.908829689025879 2.908829689025879
Loss :  1.831182599067688 3.2271645069122314 3.2271645069122314
Loss :  1.806422472000122 2.9979593753814697 2.9979593753814697
Loss :  1.807543396949768 3.279309034347534 3.279309034347534
Loss :  1.837026596069336 3.3006374835968018 3.3006374835968018
Loss :  1.8178529739379883 3.125946283340454 3.125946283340454
Loss :  1.8260347843170166 3.5828118324279785 3.5828118324279785
Loss :  1.8295320272445679 3.4658122062683105 3.4658122062683105
Loss :  1.8272480964660645 3.4709908962249756 3.4709908962249756
Loss :  1.8191616535186768 3.2860450744628906 3.2860450744628906
Loss :  1.8143128156661987 3.4799959659576416 3.4799959659576416
Loss :  1.8163728713989258 3.5855817794799805 3.5855817794799805
Loss :  1.8115531206130981 3.388690948486328 3.388690948486328
Loss :  1.807997703552246 3.490199089050293 3.490199089050293
Loss :  1.8303720951080322 2.86727237701416 2.86727237701416
Loss :  1.7777070999145508 3.173828363418579 3.173828363418579
  batch 20 loss: 1.7777070999145508, 3.173828363418579, 3.173828363418579
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8146337270736694 3.212318181991577 3.212318181991577
Loss :  1.8137109279632568 3.575875759124756 3.575875759124756
Loss :  1.8064769506454468 3.339561939239502 3.339561939239502
Loss :  1.8106532096862793 3.5450592041015625 3.5450592041015625
Loss :  1.8226275444030762 3.637791633605957 3.637791633605957
Loss :  1.8112411499023438 3.1348729133605957 3.1348729133605957
Loss :  1.8132367134094238 3.632178783416748 3.632178783416748
Loss :  1.7963975667953491 3.5194826126098633 3.5194826126098633
Loss :  1.8161752223968506 3.3933181762695312 3.3933181762695312
Loss :  1.8262784481048584 3.5069119930267334 3.5069119930267334
Loss :  1.85013747215271 3.5287632942199707 3.5287632942199707
Loss :  1.8165283203125 3.5364696979522705 3.5364696979522705
Loss :  1.8364015817642212 3.317708730697632 3.317708730697632
Loss :  1.8388532400131226 3.369950771331787 3.369950771331787
Loss :  1.8470287322998047 3.461245536804199 3.461245536804199
Loss :  1.8418667316436768 3.454479694366455 3.454479694366455
Loss :  1.836659550666809 3.486494302749634 3.486494302749634
Loss :  1.8112596273422241 3.4214463233947754 3.4214463233947754
Loss :  1.8340098857879639 3.6897096633911133 3.6897096633911133
Loss :  1.8223460912704468 3.7191107273101807 3.7191107273101807
  batch 40 loss: 1.8223460912704468, 3.7191107273101807, 3.7191107273101807
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8295516967773438 3.3445229530334473 3.3445229530334473
Loss :  1.8526427745819092 3.279994487762451 3.279994487762451
Loss :  1.8584120273590088 3.368089437484741 3.368089437484741
Loss :  1.8483225107192993 3.315714120864868 3.315714120864868
Loss :  1.8517063856124878 3.3259403705596924 3.3259403705596924
Loss :  1.8311353921890259 3.7398552894592285 3.7398552894592285
Loss :  1.8234363794326782 3.3082661628723145 3.3082661628723145
Loss :  1.8580615520477295 3.2917487621307373 3.2917487621307373
Loss :  1.809220552444458 3.3027544021606445 3.3027544021606445
Loss :  1.8633226156234741 3.929795026779175 3.929795026779175
Loss :  1.85233473777771 3.1590421199798584 3.1590421199798584
Loss :  1.834982991218567 2.982213258743286 2.982213258743286
Loss :  1.8303837776184082 3.125749349594116 3.125749349594116
Loss :  1.873070240020752 3.1053097248077393 3.1053097248077393
Loss :  1.8655047416687012 3.712228536605835 3.712228536605835
Loss :  1.8313838243484497 3.214827537536621 3.214827537536621
Loss :  1.8440618515014648 3.555255174636841 3.555255174636841
Loss :  1.8591340780258179 3.271010637283325 3.271010637283325
Loss :  1.8466583490371704 3.4696755409240723 3.4696755409240723
Loss :  1.8260891437530518 3.2657878398895264 3.2657878398895264
  batch 60 loss: 1.8260891437530518, 3.2657878398895264, 3.2657878398895264
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8347853422164917 3.439678430557251 3.439678430557251
Loss :  1.8381584882736206 3.1915464401245117 3.1915464401245117
Loss :  1.8276346921920776 3.137099504470825 3.137099504470825
Loss :  1.8378620147705078 3.4379613399505615 3.4379613399505615
Loss :  1.85997474193573 3.0157346725463867 3.0157346725463867
Loss :  1.9284242391586304 4.189881801605225 4.189881801605225
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.9361099004745483 4.329794883728027 4.329794883728027
Loss :  1.8793134689331055 4.198228359222412 4.198228359222412
Loss :  1.9080355167388916 4.029630661010742 4.029630661010742
Total LOSS train 3.3614432224860558 valid 4.186883926391602
CE LOSS train 1.8297466131357045 valid 0.4770088791847229
Contrastive LOSS train 3.3614432224860558 valid 1.0074076652526855
EPOCH 148:
Loss :  1.8479472398757935 3.2910687923431396 3.2910687923431396
Loss :  1.8024171590805054 3.337754011154175 3.337754011154175
Loss :  1.8470314741134644 3.013232469558716 3.013232469558716
Loss :  1.8595434427261353 3.256503105163574 3.256503105163574
Loss :  1.8364075422286987 3.130422830581665 3.130422830581665
Loss :  1.8444044589996338 3.1855854988098145 3.1855854988098145
Loss :  1.823649287223816 3.21270489692688 3.21270489692688
Loss :  1.8211456537246704 2.868821859359741 2.868821859359741
Loss :  1.84190034866333 3.3827261924743652 3.3827261924743652
Loss :  1.8361676931381226 2.824169874191284 2.824169874191284
Loss :  1.8364436626434326 3.0483832359313965 3.0483832359313965
Loss :  1.8417024612426758 3.6391212940216064 3.6391212940216064
Loss :  1.8385183811187744 3.389726161956787 3.389726161956787
Loss :  1.8369663953781128 3.5107407569885254 3.5107407569885254
Loss :  1.8307886123657227 3.4144461154937744 3.4144461154937744
Loss :  1.8435231447219849 2.966670513153076 2.966670513153076
Loss :  1.8391722440719604 2.924217700958252 2.924217700958252
Loss :  1.8421382904052734 3.2094013690948486 3.2094013690948486
Loss :  1.8627465963363647 3.0170984268188477 3.0170984268188477
Loss :  1.815411925315857 3.008039712905884 3.008039712905884
  batch 20 loss: 1.815411925315857, 3.008039712905884, 3.008039712905884
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8375353813171387 3.1910383701324463 3.1910383701324463
Loss :  1.8356257677078247 3.2665419578552246 3.2665419578552246
Loss :  1.8337756395339966 3.0139260292053223 3.0139260292053223
Loss :  1.8296886682510376 3.257582187652588 3.257582187652588
Loss :  1.85573148727417 3.3247344493865967 3.3247344493865967
Loss :  1.8361307382583618 2.963332414627075 2.963332414627075
Loss :  1.8273966312408447 3.5368216037750244 3.5368216037750244
Loss :  1.8162647485733032 3.1284561157226562 3.1284561157226562
Loss :  1.8280142545700073 3.128596544265747 3.128596544265747
Loss :  1.8422037363052368 3.180737018585205 3.180737018585205
Loss :  1.8523215055465698 3.3156731128692627 3.3156731128692627
Loss :  1.8211296796798706 3.451308012008667 3.451308012008667
Loss :  1.8370455503463745 3.0845468044281006 3.0845468044281006
Loss :  1.842844009399414 3.8213727474212646 3.8213727474212646
Loss :  1.8486210107803345 3.1316347122192383 3.1316347122192383
Loss :  1.8384509086608887 3.1605448722839355 3.1605448722839355
Loss :  1.8382638692855835 3.105903387069702 3.105903387069702
Loss :  1.8073453903198242 2.922846794128418 2.922846794128418
Loss :  1.82870614528656 2.9363787174224854 2.9363787174224854
Loss :  1.8181636333465576 2.8319976329803467 2.8319976329803467
  batch 40 loss: 1.8181636333465576, 2.8319976329803467, 2.8319976329803467
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.825695514678955 3.4642417430877686 3.4642417430877686
Loss :  1.8505511283874512 3.1996285915374756 3.1996285915374756
Loss :  1.862674355506897 3.1063129901885986 3.1063129901885986
Loss :  1.8430767059326172 3.321204900741577 3.321204900741577
Loss :  1.8529722690582275 3.079190731048584 3.079190731048584
Loss :  1.8210396766662598 3.0655906200408936 3.0655906200408936
Loss :  1.8155862092971802 2.997309446334839 2.997309446334839
Loss :  1.8505077362060547 3.0763700008392334 3.0763700008392334
Loss :  1.8003575801849365 3.335273265838623 3.335273265838623
Loss :  1.8564544916152954 3.171879768371582 3.171879768371582
Loss :  1.8456069231033325 3.391279458999634 3.391279458999634
Loss :  1.8332898616790771 3.084749460220337 3.084749460220337
Loss :  1.8237613439559937 3.0421760082244873 3.0421760082244873
Loss :  1.8691848516464233 3.1918368339538574 3.1918368339538574
Loss :  1.8616158962249756 3.367995023727417 3.367995023727417
Loss :  1.8258029222488403 3.1629676818847656 3.1629676818847656
Loss :  1.8345719575881958 3.643723726272583 3.643723726272583
Loss :  1.8523378372192383 3.4940881729125977 3.4940881729125977
Loss :  1.8332778215408325 3.436652660369873 3.436652660369873
Loss :  1.821203589439392 3.5395700931549072 3.5395700931549072
  batch 60 loss: 1.821203589439392, 3.5395700931549072, 3.5395700931549072
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8239109516143799 3.565380096435547 3.565380096435547
Loss :  1.8239964246749878 2.930525064468384 2.930525064468384
Loss :  1.8184363842010498 3.0045621395111084 3.0045621395111084
Loss :  1.822866678237915 2.8129172325134277 2.8129172325134277
Loss :  1.8485028743743896 2.956505537033081 2.956505537033081
Loss :  1.7523798942565918 4.1763386726379395 4.1763386726379395
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7665979862213135 4.259268283843994 4.259268283843994
Loss :  1.745609998703003 3.8916966915130615 3.8916966915130615
Loss :  1.7611143589019775 4.087234973907471 4.087234973907471
Total LOSS train 3.1973344546097975 valid 4.1036346554756165
CE LOSS train 1.8360087192975558 valid 0.4402785897254944
Contrastive LOSS train 3.1973344546097975 valid 1.0218087434768677
EPOCH 149:
Loss :  1.8456116914749146 3.593029260635376 3.593029260635376
Loss :  1.8542107343673706 3.58211350440979 3.58211350440979
Loss :  1.8700573444366455 3.393885374069214 3.393885374069214
Loss :  1.8664581775665283 3.1599843502044678 3.1599843502044678
Loss :  1.869070291519165 3.315805196762085 3.315805196762085
Loss :  1.8679054975509644 3.4796693325042725 3.4796693325042725
Loss :  1.8735542297363281 3.4081060886383057 3.4081060886383057
Loss :  1.8727422952651978 3.361853837966919 3.361853837966919
Loss :  1.8641551733016968 3.8124871253967285 3.8124871253967285
Loss :  1.877427339553833 3.5242905616760254 3.5242905616760254
Loss :  1.8636671304702759 3.694164514541626 3.694164514541626
Loss :  1.838113784790039 3.66438627243042 3.66438627243042
Loss :  1.8637785911560059 3.5591881275177 3.5591881275177
Loss :  1.8508727550506592 3.367546796798706 3.367546796798706
Loss :  1.860550880432129 3.7543578147888184 3.7543578147888184
Loss :  1.8635387420654297 3.7799694538116455 3.7799694538116455
Loss :  1.8542433977127075 3.549724578857422 3.549724578857422
Loss :  1.8689146041870117 3.783722400665283 3.783722400665283
Loss :  1.857317328453064 3.260645866394043 3.260645866394043
Loss :  1.8726317882537842 3.519841432571411 3.519841432571411
  batch 20 loss: 1.8726317882537842, 3.519841432571411, 3.519841432571411
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8658652305603027 3.432347536087036 3.432347536087036
Loss :  1.8760257959365845 3.3786733150482178 3.3786733150482178
Loss :  1.871907114982605 3.2324507236480713 3.2324507236480713
Loss :  1.872672438621521 3.2280240058898926 3.2280240058898926
Loss :  1.8675709962844849 3.632795810699463 3.632795810699463
Loss :  1.8690185546875 3.4496309757232666 3.4496309757232666
Loss :  1.8664698600769043 3.390294075012207 3.390294075012207
Loss :  1.870717167854309 3.5328991413116455 3.5328991413116455
Loss :  1.8598121404647827 3.2200844287872314 3.2200844287872314
Loss :  1.8598058223724365 3.1267073154449463 3.1267073154449463
Loss :  1.8467155694961548 3.6562001705169678 3.6562001705169678
Loss :  1.867203950881958 3.4140729904174805 3.4140729904174805
Loss :  1.8670626878738403 3.172696828842163 3.172696828842163
Loss :  1.871038556098938 3.2967042922973633 3.2967042922973633
Loss :  1.861743688583374 3.6567885875701904 3.6567885875701904
Loss :  1.8529694080352783 3.553126335144043 3.553126335144043
Loss :  1.8515198230743408 3.5080487728118896 3.5080487728118896
Loss :  1.8555150032043457 3.4360721111297607 3.4360721111297607
Loss :  1.8468958139419556 3.5401244163513184 3.5401244163513184
Loss :  1.8511003255844116 3.416088819503784 3.416088819503784
  batch 40 loss: 1.8511003255844116, 3.416088819503784, 3.416088819503784
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.844620704650879 3.540876626968384 3.540876626968384
Loss :  1.8558833599090576 3.4027299880981445 3.4027299880981445
Loss :  1.852290391921997 3.158690929412842 3.158690929412842
Loss :  1.8583793640136719 3.5229568481445312 3.5229568481445312
Loss :  1.8589054346084595 3.57551646232605 3.57551646232605
Loss :  1.8720735311508179 3.8650400638580322 3.8650400638580322
Loss :  1.873071551322937 3.5827038288116455 3.5827038288116455
Loss :  1.8609379529953003 3.8442094326019287 3.8442094326019287
Loss :  1.8762130737304688 3.404313087463379 3.404313087463379
Loss :  1.8677419424057007 3.7397634983062744 3.7397634983062744
Loss :  1.8655064105987549 3.5506346225738525 3.5506346225738525
Loss :  1.875712513923645 3.2367935180664062 3.2367935180664062
Loss :  1.864607810974121 3.095745801925659 3.095745801925659
Loss :  1.8668701648712158 3.542104482650757 3.542104482650757
Loss :  1.840124249458313 3.3319461345672607 3.3319461345672607
Loss :  1.8587297201156616 3.4343340396881104 3.4343340396881104
Loss :  1.8535926342010498 3.2659060955047607 3.2659060955047607
Loss :  1.8532485961914062 3.522064447402954 3.522064447402954
Loss :  1.8539210557937622 3.8967697620391846 3.8967697620391846
Loss :  1.8359436988830566 3.4707136154174805 3.4707136154174805
  batch 60 loss: 1.8359436988830566, 3.4707136154174805, 3.4707136154174805
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8461309671401978 3.5780911445617676 3.5780911445617676
Loss :  1.8210368156433105 3.809455394744873 3.809455394744873
Loss :  1.8393315076828003 3.7417185306549072 3.7417185306549072
Loss :  1.8408279418945312 3.4305331707000732 3.4305331707000732
Loss :  1.847210168838501 2.871115207672119 2.871115207672119
Loss :  1.741486668586731 4.431511402130127 4.431511402130127
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7286032438278198 4.329058647155762 4.329058647155762
Loss :  1.73931086063385 4.31536340713501 4.31536340713501
Loss :  1.738839030265808 4.35388708114624 4.35388708114624
Total LOSS train 3.480820450415978 valid 4.357455134391785
CE LOSS train 1.8598363582904522 valid 0.434709757566452
Contrastive LOSS train 3.480820450415978 valid 1.08847177028656
EPOCH 150:
Loss :  1.8418413400650024 3.469794273376465 3.469794273376465
Loss :  1.8520028591156006 3.553283452987671 3.553283452987671
Loss :  1.8522775173187256 3.4684057235717773 3.4684057235717773
Loss :  1.854184865951538 3.4140512943267822 3.4140512943267822
Loss :  1.8504310846328735 3.2593581676483154 3.2593581676483154
Loss :  1.854821801185608 3.3952527046203613 3.3952527046203613
Loss :  1.8594695329666138 3.6889548301696777 3.6889548301696777
Loss :  1.8585717678070068 2.8648555278778076 2.8648555278778076
Loss :  1.8474466800689697 3.089730739593506 3.089730739593506
Loss :  1.863540768623352 2.8241114616394043 2.8241114616394043
Loss :  1.8586229085922241 3.2264559268951416 3.2264559268951416
Loss :  1.8451206684112549 3.865230083465576 3.865230083465576
Loss :  1.8588758707046509 3.5883734226226807 3.5883734226226807
Loss :  1.8517513275146484 3.3622665405273438 3.3622665405273438
Loss :  1.8530043363571167 3.3662309646606445 3.3662309646606445
Loss :  1.8624887466430664 3.7492756843566895 3.7492756843566895
Loss :  1.8485219478607178 3.547560214996338 3.547560214996338
Loss :  1.8596808910369873 3.426229953765869 3.426229953765869
Loss :  1.838221788406372 3.0942888259887695 3.0942888259887695
Loss :  1.847916603088379 2.7940471172332764 2.7940471172332764
  batch 20 loss: 1.847916603088379, 2.7940471172332764, 2.7940471172332764
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.841318964958191 3.5707762241363525 3.5707762241363525
Loss :  1.8453181982040405 3.37426495552063 3.37426495552063
Loss :  1.847571849822998 3.295830726623535 3.295830726623535
Loss :  1.8501814603805542 3.45036244392395 3.45036244392395
Loss :  1.8423265218734741 3.665583372116089 3.665583372116089
Loss :  1.8478806018829346 3.2235522270202637 3.2235522270202637
Loss :  1.8511959314346313 3.3667311668395996 3.3667311668395996
Loss :  1.8566139936447144 3.2339186668395996 3.2339186668395996
Loss :  1.8609845638275146 3.311055898666382 3.311055898666382
Loss :  1.8369325399398804 3.2773828506469727 3.2773828506469727
Loss :  1.8393410444259644 3.3398115634918213 3.3398115634918213
Loss :  1.848567247390747 3.6992106437683105 3.6992106437683105
Loss :  1.850704550743103 3.68876576423645 3.68876576423645
Loss :  1.861129879951477 3.467085838317871 3.467085838317871
Loss :  1.8579862117767334 3.444293260574341 3.444293260574341
Loss :  1.8483704328536987 3.5809600353240967 3.5809600353240967
Loss :  1.8561102151870728 3.037506580352783 3.037506580352783
Loss :  1.85956871509552 2.894225597381592 2.894225597381592
Loss :  1.8583943843841553 3.2062830924987793 3.2062830924987793
Loss :  1.8587640523910522 3.4421069622039795 3.4421069622039795
  batch 40 loss: 1.8587640523910522, 3.4421069622039795, 3.4421069622039795
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8602416515350342 3.353834390640259 3.353834390640259
Loss :  1.856151819229126 3.5429511070251465 3.5429511070251465
Loss :  1.8594416379928589 3.6021692752838135 3.6021692752838135
Loss :  1.8445385694503784 3.6747756004333496 3.6747756004333496
Loss :  1.8603743314743042 3.258814573287964 3.258814573287964
Loss :  1.8618137836456299 3.2929024696350098 3.2929024696350098
Loss :  1.8671296834945679 3.5665364265441895 3.5665364265441895
Loss :  1.8581186532974243 3.4653592109680176 3.4653592109680176
Loss :  1.8650965690612793 3.6551804542541504 3.6551804542541504
Loss :  1.86083984375 3.457484722137451 3.457484722137451
Loss :  1.8608965873718262 3.383242607116699 3.383242607116699
Loss :  1.868737816810608 3.529567241668701 3.529567241668701
Loss :  1.851572871208191 3.6198570728302 3.6198570728302
Loss :  1.866398811340332 3.1878809928894043 3.1878809928894043
Loss :  1.8468263149261475 3.5616464614868164 3.5616464614868164
Loss :  1.852812647819519 3.232494354248047 3.232494354248047
Loss :  1.8501447439193726 3.6211516857147217 3.6211516857147217
Loss :  1.8415369987487793 3.218120574951172 3.218120574951172
Loss :  1.8589662313461304 3.2653145790100098 3.2653145790100098
Loss :  1.8458434343338013 3.0870463848114014 3.0870463848114014
  batch 60 loss: 1.8458434343338013, 3.0870463848114014, 3.0870463848114014
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8446977138519287 3.4823548793792725 3.4823548793792725
Loss :  1.839717149734497 3.2910561561584473 3.2910561561584473
Loss :  1.8371297121047974 3.3122074604034424 3.3122074604034424
Loss :  1.8472161293029785 3.2131879329681396 3.2131879329681396
Loss :  1.8503614664077759 3.351104974746704 3.351104974746704
Loss :  1.7732294797897339 3.73238205909729 3.73238205909729
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7761725187301636 3.6757402420043945 3.6757402420043945
Loss :  1.76999032497406 3.5387232303619385 3.5387232303619385
Loss :  1.779835820198059 3.793210744857788 3.793210744857788
Total LOSS train 3.3822108672215387 valid 3.685014069080353
CE LOSS train 1.8528409205950223 valid 0.44495895504951477
Contrastive LOSS train 3.3822108672215387 valid 0.948302686214447
EPOCH 151:
Loss :  1.8502660989761353 3.8103950023651123 3.8103950023651123
Loss :  1.830528736114502 3.4578235149383545 3.4578235149383545
Loss :  1.8437854051589966 3.612255573272705 3.612255573272705
Loss :  1.8510624170303345 3.6381962299346924 3.6381962299346924
Loss :  1.8390251398086548 3.702324390411377 3.702324390411377
Loss :  1.8453021049499512 3.3202850818634033 3.3202850818634033
Loss :  1.835750699043274 3.076859951019287 3.076859951019287
Loss :  1.834438443183899 3.0021355152130127 3.0021355152130127
Loss :  1.8349272012710571 3.088141679763794 3.088141679763794
Loss :  1.8364191055297852 3.2849581241607666 3.2849581241607666
Loss :  1.8380160331726074 3.422266721725464 3.422266721725464
Loss :  1.837296962738037 3.3988184928894043 3.3988184928894043
Loss :  1.8355096578598022 3.361827850341797 3.361827850341797
Loss :  1.8413708209991455 3.06569242477417 3.06569242477417
Loss :  1.8418827056884766 3.00960111618042 3.00960111618042
Loss :  1.8548235893249512 3.2142412662506104 3.2142412662506104
Loss :  1.8398921489715576 3.283555030822754 3.283555030822754
Loss :  1.8433921337127686 3.219294548034668 3.219294548034668
Loss :  1.8426629304885864 3.0615999698638916 3.0615999698638916
Loss :  1.8440771102905273 3.053279399871826 3.053279399871826
  batch 20 loss: 1.8440771102905273, 3.053279399871826, 3.053279399871826
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8429157733917236 3.489276885986328 3.489276885986328
Loss :  1.84451425075531 3.4992406368255615 3.4992406368255615
Loss :  1.8471667766571045 3.343596935272217 3.343596935272217
Loss :  1.854776382446289 3.3065268993377686 3.3065268993377686
Loss :  1.8570152521133423 3.6560261249542236 3.6560261249542236
Loss :  1.8463656902313232 3.217522144317627 3.217522144317627
Loss :  1.8451262712478638 3.4948012828826904 3.4948012828826904
Loss :  1.8368258476257324 3.2279891967773438 3.2279891967773438
Loss :  1.8429803848266602 3.185925006866455 3.185925006866455
Loss :  1.8394876718521118 3.2585482597351074 3.2585482597351074
Loss :  1.839489459991455 3.240109920501709 3.240109920501709
Loss :  1.834985613822937 3.128754138946533 3.128754138946533
Loss :  1.8370598554611206 3.194880247116089 3.194880247116089
Loss :  1.8486921787261963 3.2437856197357178 3.2437856197357178
Loss :  1.847274899482727 3.35376238822937 3.35376238822937
Loss :  1.8382196426391602 3.3253889083862305 3.3253889083862305
Loss :  1.8422884941101074 3.174187183380127 3.174187183380127
Loss :  1.8331670761108398 3.3044326305389404 3.3044326305389404
Loss :  1.8461639881134033 2.981001138687134 2.981001138687134
Loss :  1.8375117778778076 3.2933194637298584 3.2933194637298584
  batch 40 loss: 1.8375117778778076, 3.2933194637298584, 3.2933194637298584
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8417763710021973 3.40258526802063 3.40258526802063
Loss :  1.8409219980239868 3.4867749214172363 3.4867749214172363
Loss :  1.8458497524261475 3.3033173084259033 3.3033173084259033
Loss :  1.834513545036316 3.229564666748047 3.229564666748047
Loss :  1.846826434135437 2.8347012996673584 2.8347012996673584
Loss :  1.8421326875686646 3.5049850940704346 3.5049850940704346
Loss :  1.8391873836517334 3.425076484680176 3.425076484680176
Loss :  1.8417807817459106 3.3918468952178955 3.3918468952178955
Loss :  1.8300457000732422 3.171781301498413 3.171781301498413
Loss :  1.8425209522247314 3.3247501850128174 3.3247501850128174
Loss :  1.8434460163116455 3.5822691917419434 3.5822691917419434
Loss :  1.855823040008545 3.5428123474121094 3.5428123474121094
Loss :  1.843355417251587 3.152010679244995 3.152010679244995
Loss :  1.858067274093628 3.406325101852417 3.406325101852417
Loss :  1.843356728553772 3.257763624191284 3.257763624191284
Loss :  1.8527272939682007 3.527613639831543 3.527613639831543
Loss :  1.849098801612854 3.2989609241485596 3.2989609241485596
Loss :  1.8465101718902588 3.161574125289917 3.161574125289917
Loss :  1.862346887588501 3.4634008407592773 3.4634008407592773
Loss :  1.8517786264419556 3.2627673149108887 3.2627673149108887
  batch 60 loss: 1.8517786264419556, 3.2627673149108887, 3.2627673149108887
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8434981107711792 3.1403090953826904 3.1403090953826904
Loss :  1.8387428522109985 3.3790392875671387 3.3790392875671387
Loss :  1.8370733261108398 2.9497690200805664 2.9497690200805664
Loss :  1.8428736925125122 3.4696505069732666 3.4696505069732666
Loss :  1.845068097114563 2.9700162410736084 2.9700162410736084
Loss :  1.7529525756835938 3.7547147274017334 3.7547147274017334
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.754489779472351 3.7387444972991943 3.7387444972991943
Loss :  1.750535011291504 3.801577568054199 3.801577568054199
Loss :  1.7645411491394043 3.661407470703125 3.661407470703125
Total LOSS train 3.3021891117095947 valid 3.739111065864563
CE LOSS train 1.8431042872942411 valid 0.4411352872848511
Contrastive LOSS train 3.3021891117095947 valid 0.9153518676757812
EPOCH 152:
Loss :  1.8491981029510498 3.173117160797119 3.173117160797119
Loss :  1.8330156803131104 3.3756675720214844 3.3756675720214844
Loss :  1.8436622619628906 3.3348727226257324 3.3348727226257324
Loss :  1.8500616550445557 3.18853759765625 3.18853759765625
Loss :  1.8410146236419678 3.2053182125091553 3.2053182125091553
Loss :  1.8467037677764893 3.4344820976257324 3.4344820976257324
Loss :  1.8391422033309937 3.380687952041626 3.380687952041626
Loss :  1.837933897972107 3.3323276042938232 3.3323276042938232
Loss :  1.8369066715240479 3.1243062019348145 3.1243062019348145
Loss :  1.838519811630249 2.9937949180603027 2.9937949180603027
Loss :  1.8387738466262817 3.2049787044525146 3.2049787044525146
Loss :  1.835827350616455 3.533947706222534 3.533947706222534
Loss :  1.8348153829574585 3.4052820205688477 3.4052820205688477
Loss :  1.8393563032150269 3.403029680252075 3.403029680252075
Loss :  1.8390576839447021 3.036444902420044 3.036444902420044
Loss :  1.850369930267334 3.3703384399414062 3.3703384399414062
Loss :  1.8368637561798096 3.269185781478882 3.269185781478882
Loss :  1.8390676975250244 3.2452712059020996 3.2452712059020996
Loss :  1.836906909942627 3.42899489402771 3.42899489402771
Loss :  1.8389357328414917 3.5342047214508057 3.5342047214508057
  batch 20 loss: 1.8389357328414917, 3.5342047214508057, 3.5342047214508057
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8367273807525635 3.215261936187744 3.215261936187744
Loss :  1.839578628540039 3.3062961101531982 3.3062961101531982
Loss :  1.840821385383606 3.0922160148620605 3.0922160148620605
Loss :  1.849536657333374 3.594815254211426 3.594815254211426
Loss :  1.849631428718567 3.7911384105682373 3.7911384105682373
Loss :  1.8391287326812744 3.371872901916504 3.371872901916504
Loss :  1.8399585485458374 3.4272677898406982 3.4272677898406982
Loss :  1.830005168914795 3.3750922679901123 3.3750922679901123
Loss :  1.83964204788208 3.7618069648742676 3.7618069648742676
Loss :  1.83327317237854 3.4857118129730225 3.4857118129730225
Loss :  1.835482120513916 3.3078982830047607 3.3078982830047607
Loss :  1.8269672393798828 3.335447072982788 3.335447072982788
Loss :  1.829792857170105 3.3885977268218994 3.3885977268218994
Loss :  1.8427155017852783 3.514620542526245 3.514620542526245
Loss :  1.8402358293533325 3.579012155532837 3.579012155532837
Loss :  1.834594964981079 3.644242525100708 3.644242525100708
Loss :  1.8340609073638916 3.5366334915161133 3.5366334915161133
Loss :  1.8215049505233765 3.276991844177246 3.276991844177246
Loss :  1.8374708890914917 3.7043004035949707 3.7043004035949707
Loss :  1.825575828552246 3.8718466758728027 3.8718466758728027
  batch 40 loss: 1.825575828552246, 3.8718466758728027, 3.8718466758728027
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8302781581878662 3.745014190673828 3.745014190673828
Loss :  1.8286405801773071 3.2612502574920654 3.2612502574920654
Loss :  1.8307806253433228 3.5556538105010986 3.5556538105010986
Loss :  1.8237602710723877 3.4841206073760986 3.4841206073760986
Loss :  1.834588885307312 3.349226474761963 3.349226474761963
Loss :  1.8302271366119385 3.202704668045044 3.202704668045044
Loss :  1.8241841793060303 3.2229270935058594 3.2229270935058594
Loss :  1.8319333791732788 3.4364748001098633 3.4364748001098633
Loss :  1.8153622150421143 3.195768117904663 3.195768117904663
Loss :  1.8330681324005127 3.338550090789795 3.338550090789795
Loss :  1.8333746194839478 3.2608234882354736 3.2608234882354736
Loss :  1.8431499004364014 3.383354663848877 3.383354663848877
Loss :  1.8343009948730469 3.1540110111236572 3.1540110111236572
Loss :  1.851799488067627 3.230748414993286 3.230748414993286
Loss :  1.8394094705581665 3.580397129058838 3.580397129058838
Loss :  1.8401012420654297 3.092177629470825 3.092177629470825
Loss :  1.8441829681396484 3.2483718395233154 3.2483718395233154
Loss :  1.8415361642837524 3.2194314002990723 3.2194314002990723
Loss :  1.859349250793457 3.595053195953369 3.595053195953369
Loss :  1.8447200059890747 3.4010589122772217 3.4010589122772217
  batch 60 loss: 1.8447200059890747, 3.4010589122772217, 3.4010589122772217
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8377599716186523 3.0328023433685303 3.0328023433685303
Loss :  1.8352563381195068 3.462628126144409 3.462628126144409
Loss :  1.8294696807861328 3.5355162620544434 3.5355162620544434
Loss :  1.8387471437454224 3.0748543739318848 3.0748543739318848
Loss :  1.8392176628112793 3.2778310775756836 3.2778310775756836
Loss :  1.754131555557251 3.831882953643799 3.831882953643799
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.75608229637146 3.8127646446228027 3.8127646446228027
Loss :  1.7515857219696045 3.6805825233459473 3.6805825233459473
Loss :  1.7648532390594482 3.6511881351470947 3.6511881351470947
Total LOSS train 3.367640157846304 valid 3.744104564189911
CE LOSS train 1.8373543996077317 valid 0.44121330976486206
Contrastive LOSS train 3.367640157846304 valid 0.9127970337867737
EPOCH 153:
Loss :  1.842239499092102 3.499274253845215 3.499274253845215
Loss :  1.825407862663269 3.5275537967681885 3.5275537967681885
Loss :  1.8353404998779297 3.2193784713745117 3.2193784713745117
Loss :  1.8415189981460571 3.4198708534240723 3.4198708534240723
Loss :  1.8322020769119263 3.3181822299957275 3.3181822299957275
Loss :  1.838020920753479 3.361597776412964 3.361597776412964
Loss :  1.8282935619354248 3.091395139694214 3.091395139694214
Loss :  1.828057885169983 3.30472469329834 3.30472469329834
Loss :  1.8245302438735962 3.1569297313690186 3.1569297313690186
Loss :  1.8242639303207397 3.409576416015625 3.409576416015625
Loss :  1.8297007083892822 3.497837781906128 3.497837781906128
Loss :  1.8325501680374146 3.3038432598114014 3.3038432598114014
Loss :  1.8261287212371826 3.3445849418640137 3.3445849418640137
Loss :  1.834533452987671 3.1831908226013184 3.1831908226013184
Loss :  1.830944538116455 3.4458091259002686 3.4458091259002686
Loss :  1.8475114107131958 3.416337251663208 3.416337251663208
Loss :  1.8326226472854614 3.450641393661499 3.450641393661499
Loss :  1.831907868385315 3.73455548286438 3.73455548286438
Loss :  1.8335014581680298 3.504756212234497 3.504756212234497
Loss :  1.8316847085952759 3.2717888355255127 3.2717888355255127
  batch 20 loss: 1.8316847085952759, 3.2717888355255127, 3.2717888355255127
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8317756652832031 3.315478563308716 3.315478563308716
Loss :  1.8351789712905884 3.3095619678497314 3.3095619678497314
Loss :  1.8346445560455322 3.7230172157287598 3.7230172157287598
Loss :  1.8457484245300293 3.6024768352508545 3.6024768352508545
Loss :  1.8481686115264893 3.4305033683776855 3.4305033683776855
Loss :  1.83562171459198 3.300396680831909 3.300396680831909
Loss :  1.8368854522705078 3.6376633644104004 3.6376633644104004
Loss :  1.8244277238845825 3.4954445362091064 3.4954445362091064
Loss :  1.836957335472107 3.6018996238708496 3.6018996238708496
Loss :  1.8328351974487305 3.673598527908325 3.673598527908325
Loss :  1.835469365119934 3.548337697982788 3.548337697982788
Loss :  1.824269413948059 3.4509308338165283 3.4509308338165283
Loss :  1.827857494354248 3.536912441253662 3.536912441253662
Loss :  1.8400287628173828 3.414820432662964 3.414820432662964
Loss :  1.837427020072937 3.6048872470855713 3.6048872470855713
Loss :  1.8330278396606445 3.5983476638793945 3.5983476638793945
Loss :  1.8323312997817993 3.355872631072998 3.355872631072998
Loss :  1.8179160356521606 3.219536542892456 3.219536542892456
Loss :  1.835304856300354 3.311126947402954 3.311126947402954
Loss :  1.8234456777572632 3.5014727115631104 3.5014727115631104
  batch 40 loss: 1.8234456777572632, 3.5014727115631104, 3.5014727115631104
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.829699158668518 3.453928232192993 3.453928232192993
Loss :  1.82810640335083 3.320305585861206 3.320305585861206
Loss :  1.8308831453323364 3.2207906246185303 3.2207906246185303
Loss :  1.8227922916412354 3.341151714324951 3.341151714324951
Loss :  1.8330539464950562 3.433180332183838 3.433180332183838
Loss :  1.8289529085159302 3.583315134048462 3.583315134048462
Loss :  1.8226927518844604 3.562840223312378 3.562840223312378
Loss :  1.828263759613037 3.2015092372894287 3.2015092372894287
Loss :  1.8142783641815186 3.1862845420837402 3.1862845420837402
Loss :  1.8292104005813599 3.5218253135681152 3.5218253135681152
Loss :  1.8282414674758911 3.7032430171966553 3.7032430171966553
Loss :  1.8392047882080078 3.595860242843628 3.595860242843628
Loss :  1.8313910961151123 3.1174886226654053 3.1174886226654053
Loss :  1.844741702079773 3.21112322807312 3.21112322807312
Loss :  1.8305224180221558 3.578617572784424 3.578617572784424
Loss :  1.8337076902389526 3.2570126056671143 3.2570126056671143
Loss :  1.8372689485549927 3.4963347911834717 3.4963347911834717
Loss :  1.8299946784973145 3.3030874729156494 3.3030874729156494
Loss :  1.8503278493881226 3.249603271484375 3.249603271484375
Loss :  1.8346036672592163 3.1096513271331787 3.1096513271331787
  batch 60 loss: 1.8346036672592163, 3.1096513271331787, 3.1096513271331787
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8318214416503906 3.1371703147888184 3.1371703147888184
Loss :  1.8254425525665283 3.4232070446014404 3.4232070446014404
Loss :  1.8231710195541382 3.334979295730591 3.334979295730591
Loss :  1.8299994468688965 3.3667261600494385 3.3667261600494385
Loss :  1.8284775018692017 2.730321168899536 2.730321168899536
Loss :  1.7480857372283936 3.9402949810028076 3.9402949810028076
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.7501859664916992 3.7428946495056152 3.7428946495056152
Loss :  1.745234489440918 3.7099449634552 3.7099449634552
Loss :  1.7605454921722412 3.7554590702056885 3.7554590702056885
Total LOSS train 3.3928257135244517 valid 3.787148416042328
CE LOSS train 1.8321097227243277 valid 0.4401363730430603
Contrastive LOSS train 3.3928257135244517 valid 0.9388647675514221
EPOCH 154:
Loss :  1.8311983346939087 3.4735913276672363 3.4735913276672363
Loss :  1.82358717918396 3.546107530593872 3.546107530593872
Loss :  1.8298360109329224 2.924614906311035 2.924614906311035
Loss :  1.836063265800476 3.0989832878112793 3.0989832878112793
Loss :  1.829429268836975 3.207789897918701 3.207789897918701
Loss :  1.8370919227600098 3.228027105331421 3.228027105331421
Loss :  1.8281875848770142 3.284404993057251 3.284404993057251
Loss :  1.8283897638320923 3.508105516433716 3.508105516433716
Loss :  1.8273966312408447 3.137603998184204 3.137603998184204
Loss :  1.8291515111923218 3.0070571899414062 3.0070571899414062
Loss :  1.8332988023757935 3.331308126449585 3.331308126449585
Loss :  1.8346112966537476 3.409266710281372 3.409266710281372
Loss :  1.8301693201065063 3.4668900966644287 3.4668900966644287
Loss :  1.8379451036453247 3.199063539505005 3.199063539505005
Loss :  1.8336408138275146 3.174748659133911 3.174748659133911
Loss :  1.8493109941482544 3.1492068767547607 3.1492068767547607
Loss :  1.8341255187988281 3.1293699741363525 3.1293699741363525
Loss :  1.8335413932800293 3.2548680305480957 3.2548680305480957
Loss :  1.8353960514068604 3.0577688217163086 3.0577688217163086
Loss :  1.8334736824035645 3.529212236404419 3.529212236404419
  batch 20 loss: 1.8334736824035645, 3.529212236404419, 3.529212236404419
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8348100185394287 3.140981435775757 3.140981435775757
Loss :  1.8371883630752563 3.3275749683380127 3.3275749683380127
Loss :  1.8361653089523315 3.395371675491333 3.395371675491333
Loss :  1.8463271856307983 3.2878456115722656 3.2878456115722656
Loss :  1.8498311042785645 3.6283233165740967 3.6283233165740967
Loss :  1.8370267152786255 3.3964927196502686 3.3964927196502686
Loss :  1.8375846147537231 3.566136360168457 3.566136360168457
Loss :  1.8256621360778809 3.412008047103882 3.412008047103882
Loss :  1.8370052576065063 3.392136335372925 3.392136335372925
Loss :  1.8323851823806763 3.4346539974212646 3.4346539974212646
Loss :  1.8351385593414307 3.6019279956817627 3.6019279956817627
Loss :  1.8254481554031372 3.4564549922943115 3.4564549922943115
Loss :  1.8286402225494385 3.574307441711426 3.574307441711426
Loss :  1.8393003940582275 3.3302161693573 3.3302161693573
Loss :  1.8378428220748901 3.3587090969085693 3.3587090969085693
Loss :  1.8325179815292358 3.431689977645874 3.431689977645874
Loss :  1.8329341411590576 3.581228017807007 3.581228017807007
Loss :  1.8203380107879639 3.303415060043335 3.303415060043335
Loss :  1.836637258529663 3.95434308052063 3.95434308052063
Loss :  1.8257062435150146 3.2312841415405273 3.2312841415405273
  batch 40 loss: 1.8257062435150146, 3.2312841415405273, 3.2312841415405273
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.83110511302948 3.5533339977264404 3.5533339977264404
Loss :  1.8280856609344482 3.3344945907592773 3.3344945907592773
Loss :  1.830060362815857 3.3260951042175293 3.3260951042175293
Loss :  1.822258472442627 3.4242167472839355 3.4242167472839355
Loss :  1.8323673009872437 3.3581297397613525 3.3581297397613525
Loss :  1.8287904262542725 3.414431095123291 3.414431095123291
Loss :  1.8240876197814941 3.214898109436035 3.214898109436035
Loss :  1.8276938199996948 3.5910959243774414 3.5910959243774414
Loss :  1.815155267715454 3.0230343341827393 3.0230343341827393
Loss :  1.8288077116012573 3.355605363845825 3.355605363845825
Loss :  1.8277513980865479 3.6119139194488525 3.6119139194488525
Loss :  1.8395403623580933 3.290890693664551 3.290890693664551
Loss :  1.8316688537597656 3.2136070728302 3.2136070728302
Loss :  1.844362497329712 3.4155328273773193 3.4155328273773193
Loss :  1.8322921991348267 3.6563737392425537 3.6563737392425537
Loss :  1.8363580703735352 3.3024394512176514 3.3024394512176514
Loss :  1.8376178741455078 3.655147075653076 3.655147075653076
Loss :  1.8329919576644897 3.3374311923980713 3.3374311923980713
Loss :  1.8523305654525757 3.5409772396087646 3.5409772396087646
Loss :  1.8384747505187988 3.4596986770629883 3.4596986770629883
  batch 60 loss: 1.8384747505187988, 3.4596986770629883, 3.4596986770629883
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.83307683467865 3.378225803375244 3.378225803375244
Loss :  1.8285000324249268 3.5831246376037598 3.5831246376037598
Loss :  1.8263726234436035 3.287498950958252 3.287498950958252
Loss :  1.8320574760437012 3.280924081802368 3.280924081802368
Loss :  1.8310858011245728 2.6935038566589355 2.6935038566589355
Loss :  1.7526044845581055 3.9056801795959473 3.9056801795959473
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.7514350414276123 3.8862991333007812 3.8862991333007812
Loss :  1.7493208646774292 3.8496758937835693 3.8496758937835693
Loss :  1.7638285160064697 3.6557774543762207 3.6557774543762207
Total LOSS train 3.357318669099074 valid 3.8243581652641296
CE LOSS train 1.8328804181172298 valid 0.44095712900161743
Contrastive LOSS train 3.357318669099074 valid 0.9139443635940552
EPOCH 155:
Loss :  1.8340049982070923 3.293058156967163 3.293058156967163
Loss :  1.8231804370880127 3.5888428688049316 3.5888428688049316
Loss :  1.8308911323547363 3.1561813354492188 3.1561813354492188
Loss :  1.8369600772857666 3.214226722717285 3.214226722717285
Loss :  1.829964280128479 3.6158382892608643 3.6158382892608643
Loss :  1.835680365562439 3.5833895206451416 3.5833895206451416
Loss :  1.8260126113891602 3.8187508583068848 3.8187508583068848
Loss :  1.8255555629730225 3.44435977935791 3.44435977935791
Loss :  1.8219494819641113 3.404648542404175 3.404648542404175
Loss :  1.8208290338516235 3.345750331878662 3.345750331878662
Loss :  1.827197790145874 3.4595093727111816 3.4595093727111816
Loss :  1.8298182487487793 3.398056983947754 3.398056983947754
Loss :  1.8236572742462158 3.4929277896881104 3.4929277896881104
Loss :  1.831233263015747 3.3997092247009277 3.3997092247009277
Loss :  1.8244484663009644 3.006225109100342 3.006225109100342
Loss :  1.8411579132080078 3.3515727519989014 3.3515727519989014
Loss :  1.8285260200500488 3.106933116912842 3.106933116912842
Loss :  1.8261488676071167 3.2415268421173096 3.2415268421173096
Loss :  1.8289415836334229 3.329646587371826 3.329646587371826
Loss :  1.8272532224655151 3.3985917568206787 3.3985917568206787
  batch 20 loss: 1.8272532224655151, 3.3985917568206787, 3.3985917568206787
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.829613208770752 3.3406405448913574 3.3406405448913574
Loss :  1.8328999280929565 3.355337381362915 3.355337381362915
Loss :  1.831742763519287 3.3635406494140625 3.3635406494140625
Loss :  1.8436434268951416 3.482656717300415 3.482656717300415
Loss :  1.8474873304367065 3.7219431400299072 3.7219431400299072
Loss :  1.8342033624649048 3.546820878982544 3.546820878982544
Loss :  1.8343336582183838 3.5517418384552 3.5517418384552
Loss :  1.8221603631973267 3.3382976055145264 3.3382976055145264
Loss :  1.834886074066162 3.37558913230896 3.37558913230896
Loss :  1.8312371969223022 3.3403375148773193 3.3403375148773193
Loss :  1.8350193500518799 3.6526482105255127 3.6526482105255127
Loss :  1.8220628499984741 3.610430955886841 3.610430955886841
Loss :  1.8265515565872192 3.429673194885254 3.429673194885254
Loss :  1.8369287252426147 3.207996368408203 3.207996368408203
Loss :  1.8356473445892334 3.4389500617980957 3.4389500617980957
Loss :  1.8315882682800293 3.4534690380096436 3.4534690380096436
Loss :  1.831680417060852 2.9173643589019775 2.9173643589019775
Loss :  1.815912127494812 2.8321709632873535 2.8321709632873535
Loss :  1.8346500396728516 3.3113107681274414 3.3113107681274414
Loss :  1.8232403993606567 3.4524896144866943 3.4524896144866943
  batch 40 loss: 1.8232403993606567, 3.4524896144866943, 3.4524896144866943
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8303489685058594 3.514387845993042 3.514387845993042
Loss :  1.8308711051940918 3.5341999530792236 3.5341999530792236
Loss :  1.8343777656555176 3.3684473037719727 3.3684473037719727
Loss :  1.8238484859466553 3.34676456451416 3.34676456451416
Loss :  1.834650993347168 3.2273333072662354 3.2273333072662354
Loss :  1.828718662261963 3.2603046894073486 3.2603046894073486
Loss :  1.8217617273330688 3.259899139404297 3.259899139404297
Loss :  1.8293647766113281 3.171074628829956 3.171074628829956
Loss :  1.8125896453857422 2.95241641998291 2.95241641998291
Loss :  1.8300187587738037 3.1277718544006348 3.1277718544006348
Loss :  1.8290237188339233 3.2087185382843018 3.2087185382843018
Loss :  1.840612530708313 3.441678047180176 3.441678047180176
Loss :  1.8304495811462402 3.3701250553131104 3.3701250553131104
Loss :  1.844046711921692 3.38822865486145 3.38822865486145
Loss :  1.834496021270752 3.3850643634796143 3.3850643634796143
Loss :  1.8360416889190674 3.0662779808044434 3.0662779808044434
Loss :  1.836478352546692 3.3094711303710938 3.3094711303710938
Loss :  1.8338158130645752 3.4410958290100098 3.4410958290100098
Loss :  1.8525903224945068 3.476010322570801 3.476010322570801
Loss :  1.838532567024231 3.7003889083862305 3.7003889083862305
  batch 60 loss: 1.838532567024231, 3.7003889083862305, 3.7003889083862305
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8317610025405884 3.3296918869018555 3.3296918869018555
Loss :  1.8284064531326294 3.480160713195801 3.480160713195801
Loss :  1.8259505033493042 3.2874369621276855 3.2874369621276855
Loss :  1.8313517570495605 3.5320067405700684 3.5320067405700684
Loss :  1.8304816484451294 2.7368221282958984 2.7368221282958984
Loss :  1.764704942703247 3.894244432449341 3.894244432449341
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.7613153457641602 3.8902904987335205 3.8902904987335205
Loss :  1.7584787607192993 3.7241106033325195 3.7241106033325195
Loss :  1.7715485095977783 3.780592441558838 3.780592441558838
Total LOSS train 3.3582912591787486 valid 3.8223094940185547
CE LOSS train 1.8309152089632474 valid 0.4428871273994446
Contrastive LOSS train 3.3582912591787486 valid 0.9451481103897095
EPOCH 156:
Loss :  1.8325340747833252 3.151595115661621 3.151595115661621
Loss :  1.8208585977554321 3.345909833908081 3.345909833908081
Loss :  1.8288558721542358 3.244518756866455 3.244518756866455
Loss :  1.8351337909698486 3.1853394508361816 3.1853394508361816
Loss :  1.828577995300293 3.0767602920532227 3.0767602920532227
Loss :  1.8335063457489014 3.101797580718994 3.101797580718994
Loss :  1.823730230331421 3.002505302429199 3.002505302429199
Loss :  1.8241009712219238 2.944007635116577 2.944007635116577
Loss :  1.8227906227111816 3.4928507804870605 3.4928507804870605
Loss :  1.8227955102920532 3.100207567214966 3.100207567214966
Loss :  1.8279802799224854 3.4174554347991943 3.4174554347991943
Loss :  1.8299307823181152 3.513259172439575 3.513259172439575
Loss :  1.826204538345337 3.475519895553589 3.475519895553589
Loss :  1.833702564239502 3.366763114929199 3.366763114929199
Loss :  1.8299700021743774 3.3975090980529785 3.3975090980529785
Loss :  1.8427425622940063 3.4434354305267334 3.4434354305267334
Loss :  1.8302682638168335 3.3695180416107178 3.3695180416107178
Loss :  1.8282970190048218 3.409721612930298 3.409721612930298
Loss :  1.8312745094299316 3.5572478771209717 3.5572478771209717
Loss :  1.8284225463867188 3.2728681564331055 3.2728681564331055
  batch 20 loss: 1.8284225463867188, 3.2728681564331055, 3.2728681564331055
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8327395915985107 3.4732019901275635 3.4732019901275635
Loss :  1.8342351913452148 3.2952640056610107 3.2952640056610107
Loss :  1.832048773765564 3.2452986240386963 3.2452986240386963
Loss :  1.84384286403656 3.2223854064941406 3.2223854064941406
Loss :  1.8458470106124878 3.5381200313568115 3.5381200313568115
Loss :  1.832343578338623 3.281154155731201 3.281154155731201
Loss :  1.832781195640564 3.520871162414551 3.520871162414551
Loss :  1.8206219673156738 3.284632444381714 3.284632444381714
Loss :  1.832406997680664 3.213819742202759 3.213819742202759
Loss :  1.8268710374832153 3.3498361110687256 3.3498361110687256
Loss :  1.8316137790679932 3.396608829498291 3.396608829498291
Loss :  1.8189679384231567 3.563199758529663 3.563199758529663
Loss :  1.8230950832366943 3.491560220718384 3.491560220718384
Loss :  1.831959843635559 3.1112022399902344 3.1112022399902344
Loss :  1.8308607339859009 3.0445945262908936 3.0445945262908936
Loss :  1.8274005651474 3.1545209884643555 3.1545209884643555
Loss :  1.8268028497695923 2.914433240890503 2.914433240890503
Loss :  1.8094556331634521 2.9220998287200928 2.9220998287200928
Loss :  1.8287495374679565 3.534564256668091 3.534564256668091
Loss :  1.816841721534729 2.972827672958374 2.972827672958374
  batch 40 loss: 1.816841721534729, 2.972827672958374, 2.972827672958374
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8250517845153809 3.1365115642547607 3.1365115642547607
Loss :  1.8233050107955933 3.061142921447754 3.061142921447754
Loss :  1.8279192447662354 3.2368476390838623 3.2368476390838623
Loss :  1.8198059797286987 3.3003127574920654 3.3003127574920654
Loss :  1.831192970275879 3.0047593116760254 3.0047593116760254
Loss :  1.8266093730926514 3.3660728931427 3.3660728931427
Loss :  1.8209649324417114 3.0182180404663086 3.0182180404663086
Loss :  1.8299493789672852 3.1646716594696045 3.1646716594696045
Loss :  1.8130903244018555 3.2245278358459473 3.2245278358459473
Loss :  1.8315088748931885 3.539530038833618 3.539530038833618
Loss :  1.8319429159164429 3.331596851348877 3.331596851348877
Loss :  1.8433756828308105 3.30816912651062 3.30816912651062
Loss :  1.8315321207046509 3.15228533744812 3.15228533744812
Loss :  1.8460760116577148 3.128506660461426 3.128506660461426
Loss :  1.839849829673767 3.2050352096557617 3.2050352096557617
Loss :  1.841099739074707 3.2410521507263184 3.2410521507263184
Loss :  1.8395031690597534 3.2624142169952393 3.2624142169952393
Loss :  1.8406202793121338 3.3819780349731445 3.3819780349731445
Loss :  1.8573057651519775 3.6163084506988525 3.6163084506988525
Loss :  1.8449686765670776 3.4289746284484863 3.4289746284484863
  batch 60 loss: 1.8449686765670776, 3.4289746284484863, 3.4289746284484863
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8349895477294922 3.18391489982605 3.18391489982605
Loss :  1.8351807594299316 3.3693647384643555 3.3693647384643555
Loss :  1.8307621479034424 3.4005839824676514 3.4005839824676514
Loss :  1.8367630243301392 3.638012647628784 3.638012647628784
Loss :  1.83781898021698 3.019040822982788 3.019040822982788
Loss :  1.755359172821045 3.979546308517456 3.979546308517456
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7537832260131836 3.8781898021698 3.8781898021698
Loss :  1.749376893043518 3.8208136558532715 3.8208136558532715
Loss :  1.7632896900177002 3.756175994873047 3.756175994873047
Total LOSS train 3.278751043172983 valid 3.8586814403533936
CE LOSS train 1.8308053768598116 valid 0.44082242250442505
Contrastive LOSS train 3.278751043172983 valid 0.9390439987182617
EPOCH 157:
Loss :  1.8413043022155762 3.6632282733917236 3.6632282733917236
Loss :  1.8220279216766357 3.3557000160217285 3.3557000160217285
Loss :  1.8336341381072998 3.4555015563964844 3.4555015563964844
Loss :  1.8403878211975098 3.318138360977173 3.318138360977173
Loss :  1.8317151069641113 3.4178013801574707 3.4178013801574707
Loss :  1.8358081579208374 3.2539970874786377 3.2539970874786377
Loss :  1.8245373964309692 3.647952079772949 3.647952079772949
Loss :  1.8244715929031372 3.3195033073425293 3.3195033073425293
Loss :  1.821547508239746 3.1385202407836914 3.1385202407836914
Loss :  1.820475697517395 3.1718547344207764 3.1718547344207764
Loss :  1.8263522386550903 3.4300990104675293 3.4300990104675293
Loss :  1.8287229537963867 3.716515064239502 3.716515064239502
Loss :  1.8232280015945435 3.51467227935791 3.51467227935791
Loss :  1.829599380493164 3.160284996032715 3.160284996032715
Loss :  1.823050618171692 3.2565395832061768 3.2565395832061768
Loss :  1.8376046419143677 3.4856324195861816 3.4856324195861816
Loss :  1.8264565467834473 3.28930926322937 3.28930926322937
Loss :  1.8232511281967163 3.3988254070281982 3.3988254070281982
Loss :  1.8240392208099365 3.2884721755981445 3.2884721755981445
Loss :  1.8236116170883179 3.472921133041382 3.472921133041382
  batch 20 loss: 1.8236116170883179, 3.472921133041382, 3.472921133041382
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8265050649642944 3.1473300457000732 3.1473300457000732
Loss :  1.8301620483398438 3.427330732345581 3.427330732345581
Loss :  1.8282268047332764 3.4751133918762207 3.4751133918762207
Loss :  1.8415778875350952 3.5057272911071777 3.5057272911071777
Loss :  1.8411539793014526 3.6807162761688232 3.6807162761688232
Loss :  1.8298758268356323 3.3917899131774902 3.3917899131774902
Loss :  1.8323570489883423 3.557523012161255 3.557523012161255
Loss :  1.82120943069458 3.3928418159484863 3.3928418159484863
Loss :  1.8323438167572021 3.460571765899658 3.460571765899658
Loss :  1.826744556427002 3.5928144454956055 3.5928144454956055
Loss :  1.8312320709228516 3.4016542434692383 3.4016542434692383
Loss :  1.8214900493621826 3.331265687942505 3.331265687942505
Loss :  1.825081706047058 3.3892624378204346 3.3892624378204346
Loss :  1.8337785005569458 3.4341676235198975 3.4341676235198975
Loss :  1.8335756063461304 3.4165148735046387 3.4165148735046387
Loss :  1.8286508321762085 3.5218634605407715 3.5218634605407715
Loss :  1.830414891242981 3.3505566120147705 3.3505566120147705
Loss :  1.8150520324707031 3.0777900218963623 3.0777900218963623
Loss :  1.8331139087677002 3.00138783454895 3.00138783454895
Loss :  1.8218775987625122 2.8369927406311035 2.8369927406311035
  batch 40 loss: 1.8218775987625122, 2.8369927406311035, 2.8369927406311035
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8292531967163086 3.450779914855957 3.450779914855957
Loss :  1.8280484676361084 3.4678611755371094 3.4678611755371094
Loss :  1.8327686786651611 3.3325178623199463 3.3325178623199463
Loss :  1.8221158981323242 3.2232472896575928 3.2232472896575928
Loss :  1.8336726427078247 3.338040351867676 3.338040351867676
Loss :  1.8280502557754517 3.2250263690948486 3.2250263690948486
Loss :  1.8219053745269775 3.1680357456207275 3.1680357456207275
Loss :  1.8286209106445312 3.4444022178649902 3.4444022178649902
Loss :  1.812601923942566 3.257053852081299 3.257053852081299
Loss :  1.8282787799835205 3.4815709590911865 3.4815709590911865
Loss :  1.8272186517715454 3.520887851715088 3.520887851715088
Loss :  1.840174913406372 3.7354025840759277 3.7354025840759277
Loss :  1.8297455310821533 3.4249684810638428 3.4249684810638428
Loss :  1.8416554927825928 3.307457208633423 3.307457208633423
Loss :  1.832370400428772 3.107536554336548 3.107536554336548
Loss :  1.8357505798339844 3.169299840927124 3.169299840927124
Loss :  1.835599422454834 3.6921496391296387 3.6921496391296387
Loss :  1.8327351808547974 3.3804638385772705 3.3804638385772705
Loss :  1.8524041175842285 3.297525644302368 3.297525644302368
Loss :  1.8392598628997803 3.1718459129333496 3.1718459129333496
  batch 60 loss: 1.8392598628997803, 3.1718459129333496, 3.1718459129333496
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8317642211914062 3.3732213973999023 3.3732213973999023
Loss :  1.8303674459457397 3.3842785358428955 3.3842785358428955
Loss :  1.827208161354065 3.557391881942749 3.557391881942749
Loss :  1.8336684703826904 3.459085702896118 3.459085702896118
Loss :  1.8345983028411865 3.063197135925293 3.063197135925293
Loss :  1.7483736276626587 3.9122438430786133 3.9122438430786133
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.749279499053955 3.908202886581421 3.908202886581421
Loss :  1.7436057329177856 3.817237377166748 3.817237377166748
Loss :  1.7577745914459229 3.7632343769073486 3.7632343769073486
Total LOSS train 3.3720297006460336 valid 3.8502296209335327
CE LOSS train 1.8297859466992892 valid 0.4394436478614807
Contrastive LOSS train 3.3720297006460336 valid 0.9408085942268372
EPOCH 158:
Loss :  1.838869571685791 3.266627550125122 3.266627550125122
Loss :  1.8216310739517212 3.518427610397339 3.518427610397339
Loss :  1.833581805229187 3.464639186859131 3.464639186859131
Loss :  1.8407776355743408 3.1765222549438477 3.1765222549438477
Loss :  1.8325459957122803 3.559471607208252 3.559471607208252
Loss :  1.838142991065979 3.261789560317993 3.261789560317993
Loss :  1.8261849880218506 3.286874294281006 3.286874294281006
Loss :  1.8268574476242065 3.2761409282684326 3.2761409282684326
Loss :  1.827321171760559 3.0266683101654053 3.0266683101654053
Loss :  1.8283804655075073 2.9916296005249023 2.9916296005249023
Loss :  1.8313692808151245 3.4944746494293213 3.4944746494293213
Loss :  1.8327298164367676 3.5868492126464844 3.5868492126464844
Loss :  1.829447865486145 3.5038325786590576 3.5038325786590576
Loss :  1.836774468421936 3.3891496658325195 3.3891496658325195
Loss :  1.834499478340149 3.3406131267547607 3.3406131267547607
Loss :  1.847480297088623 3.4156899452209473 3.4156899452209473
Loss :  1.8328989744186401 3.652574300765991 3.652574300765991
Loss :  1.8318842649459839 3.3366153240203857 3.3366153240203857
Loss :  1.835097074508667 2.9347195625305176 2.9347195625305176
Loss :  1.8313031196594238 2.8112833499908447 2.8112833499908447
  batch 20 loss: 1.8313031196594238, 2.8112833499908447, 2.8112833499908447
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8352878093719482 3.1605865955352783 3.1605865955352783
Loss :  1.836356282234192 3.505627393722534 3.505627393722534
Loss :  1.8341008424758911 3.2541770935058594 3.2541770935058594
Loss :  1.8455679416656494 3.1604490280151367 3.1604490280151367
Loss :  1.8479406833648682 3.441601037979126 3.441601037979126
Loss :  1.8344335556030273 3.100132703781128 3.100132703781128
Loss :  1.834797739982605 3.6043546199798584 3.6043546199798584
Loss :  1.8235125541687012 3.3059051036834717 3.3059051036834717
Loss :  1.8341636657714844 3.5244410037994385 3.5244410037994385
Loss :  1.8288464546203613 3.490591526031494 3.490591526031494
Loss :  1.8322460651397705 3.547351360321045 3.547351360321045
Loss :  1.8225315809249878 3.4316673278808594 3.4316673278808594
Loss :  1.8262991905212402 3.411198616027832 3.411198616027832
Loss :  1.8354496955871582 3.1668972969055176 3.1668972969055176
Loss :  1.8345012664794922 3.281944751739502 3.281944751739502
Loss :  1.8297427892684937 3.612071990966797 3.612071990966797
Loss :  1.830291509628296 3.23633074760437 3.23633074760437
Loss :  1.8158681392669678 3.1647887229919434 3.1647887229919434
Loss :  1.8329975605010986 3.2822840213775635 3.2822840213775635
Loss :  1.8216047286987305 3.558570146560669 3.558570146560669
  batch 40 loss: 1.8216047286987305, 3.558570146560669, 3.558570146560669
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8291327953338623 3.2030556201934814 3.2030556201934814
Loss :  1.8283287286758423 2.868386745452881 2.868386745452881
Loss :  1.8321036100387573 3.746328830718994 3.746328830718994
Loss :  1.8218581676483154 3.283658504486084 3.283658504486084
Loss :  1.8333936929702759 3.1680426597595215 3.1680426597595215
Loss :  1.828476071357727 3.3179423809051514 3.3179423809051514
Loss :  1.8224431276321411 3.393728494644165 3.393728494644165
Loss :  1.8292584419250488 3.417938709259033 3.417938709259033
Loss :  1.8131579160690308 3.4802136421203613 3.4802136421203613
Loss :  1.8280097246170044 3.587714433670044 3.587714433670044
Loss :  1.8258843421936035 3.5966837406158447 3.5966837406158447
Loss :  1.838118314743042 3.5127222537994385 3.5127222537994385
Loss :  1.828639268875122 3.359912872314453 3.359912872314453
Loss :  1.8391069173812866 3.472106456756592 3.472106456756592
Loss :  1.828381896018982 3.422975778579712 3.422975778579712
Loss :  1.8320978879928589 3.1153228282928467 3.1153228282928467
Loss :  1.832836389541626 3.4486680030822754 3.4486680030822754
Loss :  1.829310417175293 3.2653276920318604 3.2653276920318604
Loss :  1.8490744829177856 3.3349602222442627 3.3349602222442627
Loss :  1.8355194330215454 3.47579026222229 3.47579026222229
  batch 60 loss: 1.8355194330215454, 3.47579026222229, 3.47579026222229
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8302162885665894 3.2611215114593506 3.2611215114593506
Loss :  1.827273964881897 2.971531629562378 2.971531629562378
Loss :  1.825047492980957 3.1943299770355225 3.1943299770355225
Loss :  1.8310884237289429 3.4724740982055664 3.4724740982055664
Loss :  1.8311963081359863 2.981382369995117 2.981382369995117
Loss :  1.7410842180252075 3.9507744312286377 3.9507744312286377
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4, 5], device='cuda:0')
Loss :  1.7425785064697266 3.8886172771453857 3.8886172771453857
Loss :  1.7371206283569336 3.783717155456543 3.783717155456543
Loss :  1.7516138553619385 3.9009411334991455 3.9009411334991455
Total LOSS train 3.3367366680732142 valid 3.881012499332428
CE LOSS train 1.831450337630052 valid 0.4379034638404846
Contrastive LOSS train 3.3367366680732142 valid 0.9752352833747864
EPOCH 159:
Loss :  1.8345011472702026 3.0272257328033447 3.0272257328033447
Loss :  1.820393681526184 3.4511523246765137 3.4511523246765137
Loss :  1.8307849168777466 2.9802262783050537 2.9802262783050537
Loss :  1.837760329246521 3.1426565647125244 3.1426565647125244
Loss :  1.8307123184204102 3.4639947414398193 3.4639947414398193
Loss :  1.8363591432571411 3.4368791580200195 3.4368791580200195
Loss :  1.824096441268921 3.1700403690338135 3.1700403690338135
Loss :  1.8248231410980225 3.2070975303649902 3.2070975303649902
Loss :  1.8252533674240112 3.243553876876831 3.243553876876831
Loss :  1.8256009817123413 3.1459944248199463 3.1459944248199463
Loss :  1.8282768726348877 3.3696701526641846 3.3696701526641846
Loss :  1.8301723003387451 3.472527265548706 3.472527265548706
Loss :  1.8252884149551392 3.402055025100708 3.402055025100708
Loss :  1.8326787948608398 3.0339910984039307 3.0339910984039307
Loss :  1.8275835514068604 3.299713373184204 3.299713373184204
Loss :  1.8408700227737427 3.4824559688568115 3.4824559688568115
Loss :  1.8274741172790527 3.201910972595215 3.201910972595215
Loss :  1.8242888450622559 3.402869701385498 3.402869701385498
Loss :  1.8267545700073242 2.9651310443878174 2.9651310443878174
Loss :  1.822737693786621 2.9661827087402344 2.9661827087402344
  batch 20 loss: 1.822737693786621, 2.9661827087402344, 2.9661827087402344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8278099298477173 3.1725540161132812 3.1725540161132812
Loss :  1.828856348991394 3.259244203567505 3.259244203567505
Loss :  1.8253072500228882 3.0257983207702637 3.0257983207702637
Loss :  1.8387614488601685 3.247121572494507 3.247121572494507
Loss :  1.837839126586914 3.4952914714813232 3.4952914714813232
Loss :  1.8246906995773315 3.1776206493377686 3.1776206493377686
Loss :  1.82784903049469 3.252207040786743 3.252207040786743
Loss :  1.815216302871704 3.0542640686035156 3.0542640686035156
Loss :  1.8284757137298584 3.089200019836426 3.089200019836426
Loss :  1.8195961713790894 3.2858786582946777 3.2858786582946777
Loss :  1.8265891075134277 3.380194902420044 3.380194902420044
Loss :  1.814409613609314 3.1856167316436768 3.1856167316436768
Loss :  1.8197568655014038 3.421085834503174 3.421085834503174
Loss :  1.8275161981582642 3.3432507514953613 3.3432507514953613
Loss :  1.8276457786560059 3.6749866008758545 3.6749866008758545
Loss :  1.8249473571777344 3.3071064949035645 3.3071064949035645
Loss :  1.8255765438079834 3.52280592918396 3.52280592918396
Loss :  1.8086376190185547 3.4428458213806152 3.4428458213806152
Loss :  1.827528476715088 3.218646287918091 3.218646287918091
Loss :  1.8158546686172485 3.000373601913452 3.000373601913452
  batch 40 loss: 1.8158546686172485, 3.000373601913452, 3.000373601913452
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.824276328086853 3.2705564498901367 3.2705564498901367
Loss :  1.8229029178619385 3.120249032974243 3.120249032974243
Loss :  1.8266265392303467 3.361347198486328 3.361347198486328
Loss :  1.8179090023040771 3.214473009109497 3.214473009109497
Loss :  1.8283065557479858 3.1615700721740723 3.1615700721740723
Loss :  1.8238701820373535 3.572026014328003 3.572026014328003
Loss :  1.81778085231781 3.4580447673797607 3.4580447673797607
Loss :  1.824279546737671 3.26759934425354 3.26759934425354
Loss :  1.8088301420211792 3.347153663635254 3.347153663635254
Loss :  1.824257254600525 3.402888536453247 3.402888536453247
Loss :  1.821099877357483 3.6374082565307617 3.6374082565307617
Loss :  1.8347831964492798 3.562641143798828 3.562641143798828
Loss :  1.8260873556137085 3.7611806392669678 3.7611806392669678
Loss :  1.834943175315857 3.4252970218658447 3.4252970218658447
Loss :  1.827501654624939 3.276766300201416 3.276766300201416
Loss :  1.8306323289871216 3.6105170249938965 3.6105170249938965
Loss :  1.8291997909545898 3.4441778659820557 3.4441778659820557
Loss :  1.8261442184448242 3.5763766765594482 3.5763766765594482
Loss :  1.8458501100540161 3.617788553237915 3.617788553237915
Loss :  1.8321157693862915 3.361515522003174 3.361515522003174
  batch 60 loss: 1.8321157693862915, 3.361515522003174, 3.361515522003174
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8257304430007935 3.4492881298065186 3.4492881298065186
Loss :  1.822534441947937 3.1465258598327637 3.1465258598327637
Loss :  1.8214240074157715 3.320357084274292 3.320357084274292
Loss :  1.8249150514602661 3.4245309829711914 3.4245309829711914
Loss :  1.8232911825180054 2.898669958114624 2.898669958114624
Loss :  1.7355847358703613 4.026725769042969 4.026725769042969
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.7377842664718628 3.9593429565429688 3.9593429565429688
Loss :  1.7320175170898438 3.9141643047332764 3.9141643047332764
Loss :  1.7494193315505981 3.8563218116760254 3.8563218116760254
Total LOSS train 3.3093903138087346 valid 3.93913871049881
CE LOSS train 1.8263779511818519 valid 0.43735483288764954
Contrastive LOSS train 3.3093903138087346 valid 0.9640804529190063
EPOCH 160:
Loss :  1.8261975049972534 3.3725850582122803 3.3725850582122803
Loss :  1.815319538116455 3.5413644313812256 3.5413644313812256
Loss :  1.8235061168670654 3.456597328186035 3.456597328186035
Loss :  1.830411672592163 3.3666067123413086 3.3666067123413086
Loss :  1.8249082565307617 3.4036335945129395 3.4036335945129395
Loss :  1.8306167125701904 3.585263252258301 3.585263252258301
Loss :  1.8162437677383423 3.37626314163208 3.37626314163208
Loss :  1.8174278736114502 3.0749895572662354 3.0749895572662354
Loss :  1.8164342641830444 3.283473491668701 3.283473491668701
Loss :  1.8154082298278809 3.0579988956451416 3.0579988956451416
Loss :  1.8212674856185913 3.3083455562591553 3.3083455562591553
Loss :  1.8261805772781372 3.480107307434082 3.480107307434082
Loss :  1.8189729452133179 3.587146282196045 3.587146282196045
Loss :  1.8268638849258423 3.579847812652588 3.579847812652588
Loss :  1.8223001956939697 3.5602118968963623 3.5602118968963623
Loss :  1.834210991859436 3.157916307449341 3.157916307449341
Loss :  1.822935938835144 3.112093925476074 3.112093925476074
Loss :  1.82014000415802 3.092355251312256 3.092355251312256
Loss :  1.8252346515655518 3.312424421310425 3.312424421310425
Loss :  1.8195050954818726 3.6073973178863525 3.6073973178863525
  batch 20 loss: 1.8195050954818726, 3.6073973178863525, 3.6073973178863525
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8284521102905273 3.3119351863861084 3.3119351863861084
Loss :  1.8283579349517822 3.333069086074829 3.333069086074829
Loss :  1.825093388557434 2.9850287437438965 2.9850287437438965
Loss :  1.8422338962554932 2.935131311416626 2.935131311416626
Loss :  1.8410800695419312 3.4627318382263184 3.4627318382263184
Loss :  1.8264905214309692 3.0383667945861816 3.0383667945861816
Loss :  1.8287348747253418 3.464097023010254 3.464097023010254
Loss :  1.8181931972503662 3.644146203994751 3.644146203994751
Loss :  1.825865387916565 3.4271552562713623 3.4271552562713623
Loss :  1.819473147392273 3.3694941997528076 3.3694941997528076
Loss :  1.8257696628570557 3.3580634593963623 3.3580634593963623
Loss :  1.8167190551757812 3.298128843307495 3.298128843307495
Loss :  1.8203668594360352 3.244281530380249 3.244281530380249
Loss :  1.8244467973709106 3.5019052028656006 3.5019052028656006
Loss :  1.8272656202316284 3.452113628387451 3.452113628387451
Loss :  1.8213590383529663 3.361762046813965 3.361762046813965
Loss :  1.824213981628418 3.3343567848205566 3.3343567848205566
Loss :  1.8078069686889648 3.2962653636932373 3.2962653636932373
Loss :  1.8252503871917725 3.425421714782715 3.425421714782715
Loss :  1.813110113143921 3.0176262855529785 3.0176262855529785
  batch 40 loss: 1.813110113143921, 3.0176262855529785, 3.0176262855529785
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.821189522743225 3.3170812129974365 3.3170812129974365
Loss :  1.8173120021820068 3.3209967613220215 3.3209967613220215
Loss :  1.820607304573059 3.2058022022247314 3.2058022022247314
Loss :  1.8130121231079102 3.712033987045288 3.712033987045288
Loss :  1.822556495666504 3.2121825218200684 3.2121825218200684
Loss :  1.81854248046875 3.5587737560272217 3.5587737560272217
Loss :  1.8133277893066406 3.1866867542266846 3.1866867542266846
Loss :  1.8179126977920532 3.387016773223877 3.387016773223877
Loss :  1.8044214248657227 3.5135786533355713 3.5135786533355713
Loss :  1.8176443576812744 3.4525229930877686 3.4525229930877686
Loss :  1.8129677772521973 3.3352887630462646 3.3352887630462646
Loss :  1.8284562826156616 3.37432599067688 3.37432599067688
Loss :  1.821579933166504 3.842865228652954 3.842865228652954
Loss :  1.8280829191207886 3.228429079055786 3.228429079055786
Loss :  1.8213852643966675 3.286705493927002 3.286705493927002
Loss :  1.8257777690887451 3.1116433143615723 3.1116433143615723
Loss :  1.823748230934143 3.528933048248291 3.528933048248291
Loss :  1.8205676078796387 3.396672248840332 3.396672248840332
Loss :  1.841260552406311 3.470062494277954 3.470062494277954
Loss :  1.8288718461990356 3.313682794570923 3.313682794570923
  batch 60 loss: 1.8288718461990356, 3.313682794570923, 3.313682794570923
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.821610689163208 3.412702798843384 3.412702798843384
Loss :  1.8197910785675049 3.3952760696411133 3.3952760696411133
Loss :  1.8199549913406372 3.430790901184082 3.430790901184082
Loss :  1.8212610483169556 3.2263078689575195 3.2263078689575195
Loss :  1.8206909894943237 2.7527716159820557 2.7527716159820557
Loss :  1.7277368307113647 3.891249895095825 3.891249895095825
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.7313976287841797 3.7068748474121094 3.7068748474121094
Loss :  1.7245430946350098 3.6699342727661133 3.6699342727661133
Loss :  1.7462252378463745 3.7517812252044678 3.7517812252044678
Total LOSS train 3.346935928784884 valid 3.754960060119629
CE LOSS train 1.8225677215136014 valid 0.43655630946159363
Contrastive LOSS train 3.346935928784884 valid 0.9379453063011169
EPOCH 161:
Loss :  1.824799656867981 2.9572861194610596 2.9572861194610596
Loss :  1.811710238456726 3.6967902183532715 3.6967902183532715
Loss :  1.820504903793335 3.3887827396392822 3.3887827396392822
Loss :  1.827972412109375 3.2807247638702393 3.2807247638702393
Loss :  1.8221696615219116 3.1548080444335938 3.1548080444335938
Loss :  1.8285256624221802 3.4242866039276123 3.4242866039276123
Loss :  1.811128854751587 3.3367793560028076 3.3367793560028076
Loss :  1.8128241300582886 3.0759637355804443 3.0759637355804443
Loss :  1.8130786418914795 3.3388168811798096 3.3388168811798096
Loss :  1.8099991083145142 3.275080680847168 3.275080680847168
Loss :  1.8168880939483643 3.3973190784454346 3.3973190784454346
Loss :  1.8227078914642334 3.51446795463562 3.51446795463562
Loss :  1.8145560026168823 3.1389505863189697 3.1389505863189697
Loss :  1.8224588632583618 2.9224953651428223 2.9224953651428223
Loss :  1.81570303440094 3.034440279006958 3.034440279006958
Loss :  1.8273553848266602 3.510409116744995 3.510409116744995
Loss :  1.817684531211853 3.144578695297241 3.144578695297241
Loss :  1.8135631084442139 3.3682806491851807 3.3682806491851807
Loss :  1.818819284439087 3.385615825653076 3.385615825653076
Loss :  1.8124254941940308 3.3438940048217773 3.3438940048217773
  batch 20 loss: 1.8124254941940308, 3.3438940048217773, 3.3438940048217773
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8226234912872314 3.3253862857818604 3.3253862857818604
Loss :  1.822847604751587 3.260796308517456 3.260796308517456
Loss :  1.819015383720398 3.3790972232818604 3.3790972232818604
Loss :  1.837666630744934 3.318882703781128 3.318882703781128
Loss :  1.8351110219955444 3.414149045944214 3.414149045944214
Loss :  1.8202190399169922 2.760378837585449 2.760378837585449
Loss :  1.8241543769836426 3.062546491622925 3.062546491622925
Loss :  1.813259482383728 2.8223445415496826 2.8223445415496826
Loss :  1.8221445083618164 3.368271827697754 3.368271827697754
Loss :  1.814342975616455 3.3329286575317383 3.3329286575317383
Loss :  1.8218892812728882 3.1168720722198486 3.1168720722198486
Loss :  1.8119702339172363 3.278585195541382 3.278585195541382
Loss :  1.817243218421936 3.270859718322754 3.270859718322754
Loss :  1.8202300071716309 3.6261746883392334 3.6261746883392334
Loss :  1.8239643573760986 3.587578296661377 3.587578296661377
Loss :  1.819303274154663 3.4469175338745117 3.4469175338745117
Loss :  1.8224074840545654 3.1280415058135986 3.1280415058135986
Loss :  1.8061702251434326 3.262382984161377 3.262382984161377
Loss :  1.823630452156067 3.2633323669433594 3.2633323669433594
Loss :  1.8118444681167603 3.2928435802459717 3.2928435802459717
  batch 40 loss: 1.8118444681167603, 3.2928435802459717, 3.2928435802459717
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8199563026428223 3.6044859886169434 3.6044859886169434
Loss :  1.8174583911895752 3.468921661376953 3.468921661376953
Loss :  1.8200771808624268 3.4115970134735107 3.4115970134735107
Loss :  1.811976671218872 3.4143316745758057 3.4143316745758057
Loss :  1.8215054273605347 3.325408697128296 3.325408697128296
Loss :  1.8170636892318726 3.242999315261841 3.242999315261841
Loss :  1.8118720054626465 3.0280351638793945 3.0280351638793945
Loss :  1.8165993690490723 3.145461082458496 3.145461082458496
Loss :  1.8029141426086426 3.260174512863159 3.260174512863159
Loss :  1.8165416717529297 3.5924601554870605 3.5924601554870605
Loss :  1.8107454776763916 3.401111602783203 3.401111602783203
Loss :  1.8256617784500122 3.5689897537231445 3.5689897537231445
Loss :  1.8200747966766357 3.2270255088806152 3.2270255088806152
Loss :  1.8247004747390747 3.271177053451538 3.271177053451538
Loss :  1.8185772895812988 3.0771148204803467 3.0771148204803467
Loss :  1.823629379272461 2.7695114612579346 2.7695114612579346
Loss :  1.8211500644683838 3.1138203144073486 3.1138203144073486
Loss :  1.8187288045883179 3.5374271869659424 3.5374271869659424
Loss :  1.8383910655975342 3.418804883956909 3.418804883956909
Loss :  1.8263351917266846 3.354982852935791 3.354982852935791
  batch 60 loss: 1.8263351917266846, 3.354982852935791, 3.354982852935791
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8203130960464478 3.101424217224121 3.101424217224121
Loss :  1.817461609840393 3.1826770305633545 3.1826770305633545
Loss :  1.8185672760009766 3.361663579940796 3.361663579940796
Loss :  1.8194462060928345 3.4133870601654053 3.4133870601654053
Loss :  1.8188244104385376 2.9131929874420166 2.9131929874420166
Loss :  1.723648190498352 4.121838092803955 4.121838092803955
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4, 5], device='cuda:0')
Loss :  1.726448655128479 4.042540550231934 4.042540550231934
Loss :  1.7206813097000122 3.938681125640869 3.938681125640869
Loss :  1.740234375 4.0684309005737305 4.0684309005737305
Total LOSS train 3.2802204022040735 valid 4.042872667312622
CE LOSS train 1.8192843730633075 valid 0.43505859375
Contrastive LOSS train 3.2802204022040735 valid 1.0171077251434326
EPOCH 162:
Loss :  1.8217217922210693 3.6478943824768066 3.6478943824768066
Loss :  1.8118711709976196 3.72721529006958 3.72721529006958
Loss :  1.8192858695983887 3.2858386039733887 3.2858386039733887
Loss :  1.8249013423919678 3.2654449939727783 3.2654449939727783
Loss :  1.8211148977279663 3.1743695735931396 3.1743695735931396
Loss :  1.826426386833191 3.3005807399749756 3.3005807399749756
Loss :  1.812026858329773 3.0057625770568848 3.0057625770568848
Loss :  1.8138818740844727 2.9636220932006836 2.9636220932006836
Loss :  1.8135930299758911 3.1626346111297607 3.1626346111297607
Loss :  1.8120675086975098 3.027427911758423 3.027427911758423
Loss :  1.8188220262527466 3.2947733402252197 3.2947733402252197
Loss :  1.8215690851211548 3.4521992206573486 3.4521992206573486
Loss :  1.8175113201141357 3.2704174518585205 3.2704174518585205
Loss :  1.8235434293746948 3.2444512844085693 3.2444512844085693
Loss :  1.8174235820770264 3.1047990322113037 3.1047990322113037
Loss :  1.8283876180648804 3.3023428916931152 3.3023428916931152
Loss :  1.8197576999664307 3.2740683555603027 3.2740683555603027
Loss :  1.8164918422698975 3.197138786315918 3.197138786315918
Loss :  1.8187419176101685 3.2051384449005127 3.2051384449005127
Loss :  1.8141649961471558 3.2492287158966064 3.2492287158966064
  batch 20 loss: 1.8141649961471558, 3.2492287158966064, 3.2492287158966064
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8237757682800293 3.4426088333129883 3.4426088333129883
Loss :  1.82368004322052 3.509139060974121 3.509139060974121
Loss :  1.8194667100906372 2.8714029788970947 2.8714029788970947
Loss :  1.8369346857070923 3.1204047203063965 3.1204047203063965
Loss :  1.8331660032272339 3.7172136306762695 3.7172136306762695
Loss :  1.8200653791427612 3.245068073272705 3.245068073272705
Loss :  1.8250387907028198 3.6746633052825928 3.6746633052825928
Loss :  1.81516432762146 3.6250832080841064 3.6250832080841064
Loss :  1.824114441871643 3.637888193130493 3.637888193130493
Loss :  1.8128353357315063 3.099653959274292 3.099653959274292
Loss :  1.8201484680175781 3.2037088871002197 3.2037088871002197
Loss :  1.8114300966262817 3.4510552883148193 3.4510552883148193
Loss :  1.8157178163528442 3.6911213397979736 3.6911213397979736
Loss :  1.8200494050979614 3.5391042232513428 3.5391042232513428
Loss :  1.8225219249725342 3.388164520263672 3.388164520263672
Loss :  1.8193665742874146 3.397113800048828 3.397113800048828
Loss :  1.821714997291565 2.9480156898498535 2.9480156898498535
Loss :  1.8072452545166016 3.008216142654419 3.008216142654419
Loss :  1.824434757232666 3.2707502841949463 3.2707502841949463
Loss :  1.813218355178833 3.054992914199829 3.054992914199829
  batch 40 loss: 1.813218355178833, 3.054992914199829, 3.054992914199829
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8214911222457886 3.325139045715332 3.325139045715332
Loss :  1.8203952312469482 3.066551446914673 3.066551446914673
Loss :  1.8238368034362793 3.2619729042053223 3.2619729042053223
Loss :  1.8148412704467773 3.3471784591674805 3.3471784591674805
Loss :  1.8260314464569092 3.284147024154663 3.284147024154663
Loss :  1.8214210271835327 3.49639630317688 3.49639630317688
Loss :  1.8162752389907837 3.475024938583374 3.475024938583374
Loss :  1.8225048780441284 3.1111645698547363 3.1111645698547363
Loss :  1.8071367740631104 3.269075632095337 3.269075632095337
Loss :  1.8217984437942505 3.2560856342315674 3.2560856342315674
Loss :  1.8172920942306519 3.321488618850708 3.321488618850708
Loss :  1.8308106660842896 3.247591972351074 3.247591972351074
Loss :  1.8235137462615967 3.339759349822998 3.339759349822998
Loss :  1.8298157453536987 2.804983377456665 2.804983377456665
Loss :  1.823296070098877 2.9894001483917236 2.9894001483917236
Loss :  1.8277267217636108 2.9756534099578857 2.9756534099578857
Loss :  1.8250013589859009 2.8340654373168945 2.8340654373168945
Loss :  1.8251187801361084 3.22941255569458 3.22941255569458
Loss :  1.8409779071807861 3.4403278827667236 3.4403278827667236
Loss :  1.8289754390716553 3.1761300563812256 3.1761300563812256
  batch 60 loss: 1.8289754390716553, 3.1761300563812256, 3.1761300563812256
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8246983289718628 3.338676929473877 3.338676929473877
Loss :  1.8211231231689453 3.2195067405700684 3.2195067405700684
Loss :  1.8218096494674683 3.2199747562408447 3.2199747562408447
Loss :  1.8240139484405518 2.9844143390655518 2.9844143390655518
Loss :  1.8249175548553467 2.9039831161499023 2.9039831161499023
Loss :  1.7354073524475098 4.223689556121826 4.223689556121826
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.7375319004058838 4.113050937652588 4.113050937652588
Loss :  1.7308019399642944 3.9862401485443115 3.9862401485443115
Loss :  1.7510560750961304 4.088581562042236 4.088581562042236
Total LOSS train 3.2610588000370906 valid 4.1028905510902405
CE LOSS train 1.8210494885077844 valid 0.4377640187740326
Contrastive LOSS train 3.2610588000370906 valid 1.022145390510559
EPOCH 163:
Loss :  1.8258814811706543 3.058075428009033 3.058075428009033
Loss :  1.8163920640945435 3.5575878620147705 3.5575878620147705
Loss :  1.8244155645370483 3.3448469638824463 3.3448469638824463
Loss :  1.8268526792526245 3.4774973392486572 3.4774973392486572
Loss :  1.8231667280197144 3.345517158508301 3.345517158508301
Loss :  1.8266640901565552 3.0226919651031494 3.0226919651031494
Loss :  1.816314458847046 2.8351595401763916 2.8351595401763916
Loss :  1.8169000148773193 2.9545071125030518 2.9545071125030518
Loss :  1.8150054216384888 3.268597364425659 3.268597364425659
Loss :  1.8143434524536133 3.122634172439575 3.122634172439575
Loss :  1.8221181631088257 3.2202868461608887 3.2202868461608887
Loss :  1.8217785358428955 3.5849602222442627 3.5849602222442627
Loss :  1.8206584453582764 3.31461238861084 3.31461238861084
Loss :  1.8255645036697388 3.2360026836395264 3.2360026836395264
Loss :  1.8188064098358154 3.297882556915283 3.297882556915283
Loss :  1.8311506509780884 3.550039529800415 3.550039529800415
Loss :  1.8218451738357544 3.2925469875335693 3.2925469875335693
Loss :  1.8181309700012207 3.300091028213501 3.300091028213501
Loss :  1.8189888000488281 3.268542766571045 3.268542766571045
Loss :  1.8153489828109741 3.3512048721313477 3.3512048721313477
  batch 20 loss: 1.8153489828109741, 3.3512048721313477, 3.3512048721313477
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8236322402954102 3.5261166095733643 3.5261166095733643
Loss :  1.823318362236023 3.4534482955932617 3.4534482955932617
Loss :  1.818407654762268 3.468766212463379 3.468766212463379
Loss :  1.8345396518707275 3.4080162048339844 3.4080162048339844
Loss :  1.8306785821914673 3.427323341369629 3.427323341369629
Loss :  1.817769169807434 3.0225934982299805 3.0225934982299805
Loss :  1.8232759237289429 3.2172188758850098 3.2172188758850098
Loss :  1.8125903606414795 3.1467556953430176 3.1467556953430176
Loss :  1.8225072622299194 3.1485671997070312 3.1485671997070312
Loss :  1.8110970258712769 3.20695424079895 3.20695424079895
Loss :  1.818955421447754 3.329958438873291 3.329958438873291
Loss :  1.809493064880371 3.4042279720306396 3.4042279720306396
Loss :  1.8141663074493408 3.377169609069824 3.377169609069824
Loss :  1.8182413578033447 3.3671560287475586 3.3671560287475586
Loss :  1.8202006816864014 3.4253599643707275 3.4253599643707275
Loss :  1.817294955253601 3.4407243728637695 3.4407243728637695
Loss :  1.8181838989257812 3.208383083343506 3.208383083343506
Loss :  1.802384853363037 2.801997423171997 2.801997423171997
Loss :  1.8199658393859863 2.9907004833221436 2.9907004833221436
Loss :  1.8077974319458008 2.6938552856445312 2.6938552856445312
  batch 40 loss: 1.8077974319458008, 2.6938552856445312, 2.6938552856445312
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.816265344619751 3.0283539295196533 3.0283539295196533
Loss :  1.8134249448776245 2.8861100673675537 2.8861100673675537
Loss :  1.8154481649398804 2.9626760482788086 2.9626760482788086
Loss :  1.8098417520523071 2.90396785736084 2.90396785736084
Loss :  1.8195782899856567 2.8038668632507324 2.8038668632507324
Loss :  1.8162842988967896 2.9962990283966064 2.9962990283966064
Loss :  1.8121581077575684 2.9970905780792236 2.9970905780792236
Loss :  1.8198624849319458 3.0987730026245117 3.0987730026245117
Loss :  1.8047986030578613 3.0541484355926514 3.0541484355926514
Loss :  1.821407675743103 3.3032147884368896 3.3032147884368896
Loss :  1.8185441493988037 3.266240358352661 3.266240358352661
Loss :  1.832918405532837 3.5049026012420654 3.5049026012420654
Loss :  1.8235703706741333 3.306492567062378 3.306492567062378
Loss :  1.8318101167678833 3.164771795272827 3.164771795272827
Loss :  1.8267698287963867 3.2472658157348633 3.2472658157348633
Loss :  1.8299429416656494 3.4362235069274902 3.4362235069274902
Loss :  1.8259037733078003 3.2479028701782227 3.2479028701782227
Loss :  1.8265472650527954 3.333220958709717 3.333220958709717
Loss :  1.8427995443344116 3.459632158279419 3.459632158279419
Loss :  1.8314212560653687 3.144620656967163 3.144620656967163
  batch 60 loss: 1.8314212560653687, 3.144620656967163, 3.144620656967163
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8244593143463135 3.056211233139038 3.056211233139038
Loss :  1.8227031230926514 3.2644455432891846 3.2644455432891846
Loss :  1.822253704071045 3.282130479812622 3.282130479812622
Loss :  1.8244463205337524 3.6022210121154785 3.6022210121154785
Loss :  1.8252538442611694 2.971987247467041 2.971987247467041
Loss :  1.7136850357055664 4.116827964782715 4.116827964782715
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.717788815498352 4.137609958648682 4.137609958648682
Loss :  1.7100801467895508 4.0812087059021 4.0812087059021
Loss :  1.7305620908737183 3.769404172897339 3.769404172897339
Total LOSS train 3.2275591850280763 valid 4.026262700557709
CE LOSS train 1.8206652347858137 valid 0.43264052271842957
Contrastive LOSS train 3.2275591850280763 valid 0.9423510432243347
EPOCH 164:
Loss :  1.8271301984786987 3.5286521911621094 3.5286521911621094
Loss :  1.8156729936599731 3.650796413421631 3.650796413421631
Loss :  1.8245106935501099 3.6063406467437744 3.6063406467437744
Loss :  1.8271458148956299 3.426514148712158 3.426514148712158
Loss :  1.823216438293457 3.2366111278533936 3.2366111278533936
Loss :  1.8269239664077759 3.399662971496582 3.399662971496582
Loss :  1.815156102180481 3.186732769012451 3.186732769012451
Loss :  1.8159549236297607 3.1336185932159424 3.1336185932159424
Loss :  1.8143975734710693 3.2401037216186523 3.2401037216186523
Loss :  1.8135679960250854 3.528489589691162 3.528489589691162
Loss :  1.820293664932251 3.3781044483184814 3.3781044483184814
Loss :  1.8213368654251099 3.6894595623016357 3.6894595623016357
Loss :  1.819264531135559 3.5588223934173584 3.5588223934173584
Loss :  1.824023723602295 3.2041070461273193 3.2041070461273193
Loss :  1.8170722723007202 3.2425553798675537 3.2425553798675537
Loss :  1.8288160562515259 3.0866332054138184 3.0866332054138184
Loss :  1.8199352025985718 2.9180169105529785 2.9180169105529785
Loss :  1.8162544965744019 3.1734368801116943 3.1734368801116943
Loss :  1.8174664974212646 3.1271777153015137 3.1271777153015137
Loss :  1.814261555671692 3.149914026260376 3.149914026260376
  batch 20 loss: 1.814261555671692, 3.149914026260376, 3.149914026260376
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8232848644256592 2.9512524604797363 2.9512524604797363
Loss :  1.8235641717910767 3.0724282264709473 3.0724282264709473
Loss :  1.8197288513183594 3.0573887825012207 3.0573887825012207
Loss :  1.836368441581726 3.5304548740386963 3.5304548740386963
Loss :  1.8335562944412231 3.3574512004852295 3.3574512004852295
Loss :  1.8211702108383179 3.1374619007110596 3.1374619007110596
Loss :  1.8257970809936523 3.4270758628845215 3.4270758628845215
Loss :  1.8166999816894531 3.218494176864624 3.218494176864624
Loss :  1.8247603178024292 3.286930561065674 3.286930561065674
Loss :  1.8165767192840576 2.928966999053955 2.928966999053955
Loss :  1.822660207748413 3.5290379524230957 3.5290379524230957
Loss :  1.81583571434021 3.453000068664551 3.453000068664551
Loss :  1.8206214904785156 3.1638734340667725 3.1638734340667725
Loss :  1.8245160579681396 3.1514573097229004 3.1514573097229004
Loss :  1.8277431726455688 3.294156789779663 3.294156789779663
Loss :  1.823190689086914 3.2701523303985596 3.2701523303985596
Loss :  1.8261638879776 3.315448045730591 3.315448045730591
Loss :  1.8121492862701416 3.424110174179077 3.424110174179077
Loss :  1.8279154300689697 3.290273427963257 3.290273427963257
Loss :  1.8157063722610474 2.927032947540283 2.927032947540283
  batch 40 loss: 1.8157063722610474, 2.927032947540283, 2.927032947540283
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8228784799575806 3.1009881496429443 3.1009881496429443
Loss :  1.8217214345932007 3.1192758083343506 3.1192758083343506
Loss :  1.8232150077819824 3.0808260440826416 3.0808260440826416
Loss :  1.814302682876587 3.121697425842285 3.121697425842285
Loss :  1.8253835439682007 3.4907524585723877 3.4907524585723877
Loss :  1.8211220502853394 3.2562875747680664 3.2562875747680664
Loss :  1.8160630464553833 3.1474690437316895 3.1474690437316895
Loss :  1.822555661201477 2.973029851913452 2.973029851913452
Loss :  1.8075388669967651 3.314974069595337 3.314974069595337
Loss :  1.8197314739227295 3.409489154815674 3.409489154815674
Loss :  1.8157403469085693 3.2851107120513916 3.2851107120513916
Loss :  1.829663872718811 3.1113550662994385 3.1113550662994385
Loss :  1.8232662677764893 3.1336827278137207 3.1336827278137207
Loss :  1.8274738788604736 3.2799158096313477 3.2799158096313477
Loss :  1.8202683925628662 3.3520171642303467 3.3520171642303467
Loss :  1.8261299133300781 3.128152370452881 3.128152370452881
Loss :  1.8241260051727295 3.5104780197143555 3.5104780197143555
Loss :  1.8239644765853882 3.2551121711730957 3.2551121711730957
Loss :  1.8392306566238403 3.714529514312744 3.714529514312744
Loss :  1.8272370100021362 3.4282803535461426 3.4282803535461426
  batch 60 loss: 1.8272370100021362, 3.4282803535461426, 3.4282803535461426
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8242614269256592 3.257354736328125 3.257354736328125
Loss :  1.8196929693222046 3.4780924320220947 3.4780924320220947
Loss :  1.8197273015975952 3.2905220985412598 3.2905220985412598
Loss :  1.8226261138916016 3.110135555267334 3.110135555267334
Loss :  1.823390007019043 2.7261033058166504 2.7261033058166504
Loss :  1.7073948383331299 4.249488830566406 4.249488830566406
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.7111177444458008 4.120213031768799 4.120213031768799
Loss :  1.7037452459335327 4.085305213928223 4.085305213928223
Loss :  1.7257468700408936 3.7742233276367188 3.7742233276367188
Total LOSS train 3.2665819828326885 valid 4.057307600975037
CE LOSS train 1.821872641490056 valid 0.4314367175102234
Contrastive LOSS train 3.2665819828326885 valid 0.9435558319091797
EPOCH 165:
Loss :  1.8240609169006348 3.316972017288208 3.316972017288208
Loss :  1.817529559135437 3.5535800457000732 3.5535800457000732
Loss :  1.824250340461731 3.1683332920074463 3.1683332920074463
Loss :  1.8256616592407227 3.003739833831787 3.003739833831787
Loss :  1.822945237159729 2.7531702518463135 2.7531702518463135
Loss :  1.8278028964996338 3.1173996925354004 3.1173996925354004
Loss :  1.8181453943252563 3.0703999996185303 3.0703999996185303
Loss :  1.8186453580856323 3.2586171627044678 3.2586171627044678
Loss :  1.817610740661621 3.26841139793396 3.26841139793396
Loss :  1.8189328908920288 3.1597189903259277 3.1597189903259277
Loss :  1.8249847888946533 3.036250114440918 3.036250114440918
Loss :  1.8235195875167847 3.2854909896850586 3.2854909896850586
Loss :  1.824112892150879 3.004887819290161 3.004887819290161
Loss :  1.8282052278518677 3.211371421813965 3.211371421813965
Loss :  1.820984125137329 3.278810739517212 3.278810739517212
Loss :  1.8358286619186401 3.41105318069458 3.41105318069458
Loss :  1.8255566358566284 3.470241069793701 3.470241069793701
Loss :  1.8229337930679321 3.1017186641693115 3.1017186641693115
Loss :  1.823825478553772 3.254408836364746 3.254408836364746
Loss :  1.8203176259994507 3.3448832035064697 3.3448832035064697
  batch 20 loss: 1.8203176259994507, 3.3448832035064697, 3.3448832035064697
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8282355070114136 3.1057510375976562 3.1057510375976562
Loss :  1.8273359537124634 3.316709041595459 3.316709041595459
Loss :  1.8228567838668823 3.1702182292938232 3.1702182292938232
Loss :  1.8363239765167236 3.2714967727661133 3.2714967727661133
Loss :  1.835330843925476 3.583204746246338 3.583204746246338
Loss :  1.8231977224349976 3.0170931816101074 3.0170931816101074
Loss :  1.8276959657669067 3.3327505588531494 3.3327505588531494
Loss :  1.818400263786316 3.0361361503601074 3.0361361503601074
Loss :  1.827677845954895 3.0794177055358887 3.0794177055358887
Loss :  1.8187521696090698 2.9224133491516113 2.9224133491516113
Loss :  1.8235087394714355 3.214543104171753 3.214543104171753
Loss :  1.816828966140747 3.331223726272583 3.331223726272583
Loss :  1.8217366933822632 3.290586233139038 3.290586233139038
Loss :  1.8261528015136719 3.261145830154419 3.261145830154419
Loss :  1.8269054889678955 3.3231263160705566 3.3231263160705566
Loss :  1.8241537809371948 3.4049603939056396 3.4049603939056396
Loss :  1.8243333101272583 3.338230609893799 3.338230609893799
Loss :  1.812877893447876 2.9627487659454346 2.9627487659454346
Loss :  1.8266770839691162 3.5572350025177 3.5572350025177
Loss :  1.8146659135818481 3.225929021835327 3.225929021835327
  batch 40 loss: 1.8146659135818481, 3.225929021835327, 3.225929021835327
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.821985125541687 3.5333445072174072 3.5333445072174072
Loss :  1.8212623596191406 3.396634578704834 3.396634578704834
Loss :  1.821394920349121 3.2133843898773193 3.2133843898773193
Loss :  1.8138844966888428 3.3019871711730957 3.3019871711730957
Loss :  1.8238956928253174 3.4373600482940674 3.4373600482940674
Loss :  1.8198424577713013 3.336278200149536 3.336278200149536
Loss :  1.8149068355560303 3.591913938522339 3.591913938522339
Loss :  1.8206934928894043 3.1885077953338623 3.1885077953338623
Loss :  1.8058531284332275 3.437640905380249 3.437640905380249
Loss :  1.8191239833831787 3.5716516971588135 3.5716516971588135
Loss :  1.8138844966888428 3.3835151195526123 3.3835151195526123
Loss :  1.8279283046722412 3.4917166233062744 3.4917166233062744
Loss :  1.8206779956817627 3.446617364883423 3.446617364883423
Loss :  1.8249458074569702 3.2393300533294678 3.2393300533294678
Loss :  1.815955638885498 3.385390520095825 3.385390520095825
Loss :  1.8225020170211792 3.1954798698425293 3.1954798698425293
Loss :  1.8196892738342285 3.3726260662078857 3.3726260662078857
Loss :  1.815954566001892 3.2764594554901123 3.2764594554901123
Loss :  1.83517587184906 3.7699458599090576 3.7699458599090576
Loss :  1.8240442276000977 3.6723029613494873 3.6723029613494873
  batch 60 loss: 1.8240442276000977, 3.6723029613494873, 3.6723029613494873
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8191546201705933 3.4733457565307617 3.4733457565307617
Loss :  1.8157881498336792 3.324397087097168 3.324397087097168
Loss :  1.8177146911621094 3.26212215423584 3.26212215423584
Loss :  1.8174623250961304 3.1858103275299072 3.1858103275299072
Loss :  1.817621111869812 2.690804958343506 2.690804958343506
Loss :  1.7969459295272827 3.7998616695404053 3.7998616695404053
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7869257926940918 3.774336814880371 3.774336814880371
Loss :  1.7923953533172607 3.6097865104675293 3.6097865104675293
Loss :  1.80216383934021 3.7163350582122803 3.7163350582122803
Total LOSS train 3.276814552453848 valid 3.7250800132751465
CE LOSS train 1.822166817004864 valid 0.4505409598350525
Contrastive LOSS train 3.276814552453848 valid 0.9290837645530701
EPOCH 166:
Loss :  1.8219393491744995 3.0843453407287598 3.0843453407287598
Loss :  1.8112770318984985 3.289677143096924 3.289677143096924
Loss :  1.8188072443008423 3.3532187938690186 3.3532187938690186
Loss :  1.82449209690094 3.418205976486206 3.418205976486206
Loss :  1.8198955059051514 3.2819700241088867 3.2819700241088867
Loss :  1.8264338970184326 3.229048013687134 3.229048013687134
Loss :  1.8115547895431519 3.2327582836151123 3.2327582836151123
Loss :  1.8133569955825806 2.9945571422576904 2.9945571422576904
Loss :  1.8132548332214355 3.034895420074463 3.034895420074463
Loss :  1.8115583658218384 3.0108344554901123 3.0108344554901123
Loss :  1.81936514377594 3.2768285274505615 3.2768285274505615
Loss :  1.8205554485321045 3.5993478298187256 3.5993478298187256
Loss :  1.8173524141311646 3.354081869125366 3.354081869125366
Loss :  1.8230726718902588 3.5644328594207764 3.5644328594207764
Loss :  1.818034291267395 3.224163055419922 3.224163055419922
Loss :  1.8290921449661255 3.366555690765381 3.366555690765381
Loss :  1.8193057775497437 3.3573086261749268 3.3573086261749268
Loss :  1.8172861337661743 3.372246742248535 3.372246742248535
Loss :  1.8192024230957031 3.1336238384246826 3.1336238384246826
Loss :  1.815604567527771 3.317019462585449 3.317019462585449
  batch 20 loss: 1.815604567527771, 3.317019462585449, 3.317019462585449
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8251750469207764 3.231111764907837 3.231111764907837
Loss :  1.8246315717697144 3.5221428871154785 3.5221428871154785
Loss :  1.8192542791366577 2.844120979309082 2.844120979309082
Loss :  1.8346011638641357 3.079995632171631 3.079995632171631
Loss :  1.830223560333252 3.3068935871124268 3.3068935871124268
Loss :  1.8191903829574585 3.221381425857544 3.221381425857544
Loss :  1.825883388519287 3.2500836849212646 3.2500836849212646
Loss :  1.8164689540863037 3.1371397972106934 3.1371397972106934
Loss :  1.826332688331604 3.2912757396698 3.2912757396698
Loss :  1.8152368068695068 3.408480405807495 3.408480405807495
Loss :  1.8214969635009766 3.2118961811065674 3.2118961811065674
Loss :  1.8161838054656982 3.229217767715454 3.229217767715454
Loss :  1.8218222856521606 3.2210605144500732 3.2210605144500732
Loss :  1.82656991481781 3.4835972785949707 3.4835972785949707
Loss :  1.827795386314392 3.723066568374634 3.723066568374634
Loss :  1.8248380422592163 3.3337767124176025 3.3337767124176025
Loss :  1.8252174854278564 3.4783823490142822 3.4783823490142822
Loss :  1.814895749092102 3.467642068862915 3.467642068862915
Loss :  1.8259594440460205 3.5683846473693848 3.5683846473693848
Loss :  1.8141812086105347 3.8303487300872803 3.8303487300872803
  batch 40 loss: 1.8141812086105347, 3.8303487300872803, 3.8303487300872803
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8213112354278564 3.5728275775909424 3.5728275775909424
Loss :  1.8194363117218018 3.048152446746826 3.048152446746826
Loss :  1.8175004720687866 3.3219645023345947 3.3219645023345947
Loss :  1.8122613430023193 3.4811360836029053 3.4811360836029053
Loss :  1.8206809759140015 3.304760694503784 3.304760694503784
Loss :  1.817475438117981 3.495389699935913 3.495389699935913
Loss :  1.813048243522644 3.2868173122406006 3.2868173122406006
Loss :  1.8160983324050903 3.094074249267578 3.094074249267578
Loss :  1.8036837577819824 3.2891852855682373 3.2891852855682373
Loss :  1.8147610425949097 3.285383939743042 3.285383939743042
Loss :  1.808038592338562 3.410756826400757 3.410756826400757
Loss :  1.8247216939926147 3.704624652862549 3.704624652862549
Loss :  1.818363070487976 3.396944284439087 3.396944284439087
Loss :  1.8231569528579712 3.1825551986694336 3.1825551986694336
Loss :  1.8134586811065674 3.1723105907440186 3.1723105907440186
Loss :  1.8212369680404663 3.2806849479675293 3.2806849479675293
Loss :  1.8194061517715454 3.4156992435455322 3.4156992435455322
Loss :  1.8149771690368652 3.4363529682159424 3.4363529682159424
Loss :  1.8339112997055054 3.539743661880493 3.539743661880493
Loss :  1.8225946426391602 3.1985228061676025 3.1985228061676025
  batch 60 loss: 1.8225946426391602, 3.1985228061676025, 3.1985228061676025
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8187097311019897 3.164395332336426 3.164395332336426
Loss :  1.8154584169387817 3.3545188903808594 3.3545188903808594
Loss :  1.817792534828186 3.3126273155212402 3.3126273155212402
Loss :  1.8177183866500854 3.3709027767181396 3.3709027767181396
Loss :  1.8190374374389648 2.913259267807007 2.913259267807007
Loss :  1.7481749057769775 4.002652645111084 4.002652645111084
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.7488040924072266 3.9058640003204346 3.9058640003204346
Loss :  1.7449463605880737 3.879483461380005 3.879483461380005
Loss :  1.7619715929031372 3.872955560684204 3.872955560684204
Total LOSS train 3.3133647441864014 valid 3.915238916873932
CE LOSS train 1.8195728943898128 valid 0.4404928982257843
Contrastive LOSS train 3.3133647441864014 valid 0.968238890171051
EPOCH 167:
Loss :  1.8228907585144043 3.3099429607391357 3.3099429607391357
Loss :  1.8107540607452393 3.261774778366089 3.261774778366089
Loss :  1.8191792964935303 3.0790066719055176 3.0790066719055176
Loss :  1.8252060413360596 3.3684141635894775 3.3684141635894775
Loss :  1.819295048713684 3.5011510848999023 3.5011510848999023
Loss :  1.825869083404541 3.2758188247680664 3.2758188247680664
Loss :  1.8082835674285889 3.0496575832366943 3.0496575832366943
Loss :  1.8097652196884155 3.2541074752807617 3.2541074752807617
Loss :  1.8114787340164185 3.455416440963745 3.455416440963745
Loss :  1.807767391204834 3.140070676803589 3.140070676803589
Loss :  1.816056251525879 3.348837375640869 3.348837375640869
Loss :  1.8185040950775146 3.4494714736938477 3.4494714736938477
Loss :  1.8139514923095703 3.1996145248413086 3.1996145248413086
Loss :  1.8190134763717651 3.453624725341797 3.453624725341797
Loss :  1.8106955289840698 3.4879822731018066 3.4879822731018066
Loss :  1.8237426280975342 3.3225746154785156 3.3225746154785156
Loss :  1.814997911453247 3.2406692504882812 3.2406692504882812
Loss :  1.8098160028457642 3.496838092803955 3.496838092803955
Loss :  1.8135156631469727 3.383755683898926 3.383755683898926
Loss :  1.8070778846740723 3.054013252258301 3.054013252258301
  batch 20 loss: 1.8070778846740723, 3.054013252258301, 3.054013252258301
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.817892074584961 3.3052308559417725 3.3052308559417725
Loss :  1.8182615041732788 2.9869208335876465 2.9869208335876465
Loss :  1.8133171796798706 2.7942590713500977 2.7942590713500977
Loss :  1.8330007791519165 2.912621021270752 2.912621021270752
Loss :  1.8293344974517822 3.1828131675720215 3.1828131675720215
Loss :  1.816968321800232 2.7999110221862793 2.7999110221862793
Loss :  1.824163794517517 3.051774501800537 3.051774501800537
Loss :  1.8158948421478271 2.917325019836426 2.917325019836426
Loss :  1.824536681175232 3.2028324604034424 3.2028324604034424
Loss :  1.8206133842468262 3.219086170196533 3.219086170196533
Loss :  1.8249202966690063 3.462191343307495 3.462191343307495
Loss :  1.8201746940612793 3.4829530715942383 3.4829530715942383
Loss :  1.827312707901001 3.203634738922119 3.203634738922119
Loss :  1.8293339014053345 2.9197230339050293 2.9197230339050293
Loss :  1.8330817222595215 3.088759183883667 3.088759183883667
Loss :  1.8279222249984741 2.9333643913269043 2.9333643913269043
Loss :  1.831998348236084 2.8955562114715576 2.8955562114715576
Loss :  1.821174144744873 2.729558229446411 2.729558229446411
Loss :  1.8346139192581177 2.9176766872406006 2.9176766872406006
Loss :  1.8237181901931763 2.8891069889068604 2.8891069889068604
  batch 40 loss: 1.8237181901931763, 2.8891069889068604, 2.8891069889068604
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8313219547271729 3.189931631088257 3.189931631088257
Loss :  1.8343517780303955 3.0799479484558105 3.0799479484558105
Loss :  1.8360464572906494 3.1677427291870117 3.1677427291870117
Loss :  1.824052333831787 3.256100654602051 3.256100654602051
Loss :  1.837992787361145 3.4562745094299316 3.4562745094299316
Loss :  1.831559419631958 3.5700125694274902 3.5700125694274902
Loss :  1.8270280361175537 3.2221951484680176 3.2221951484680176
Loss :  1.8368866443634033 3.075838804244995 3.075838804244995
Loss :  1.817873477935791 3.488196611404419 3.488196611404419
Loss :  1.8340070247650146 3.3171675205230713 3.3171675205230713
Loss :  1.8327323198318481 3.0194289684295654 3.0194289684295654
Loss :  1.8441139459609985 2.949065685272217 2.949065685272217
Loss :  1.8301703929901123 3.285407543182373 3.285407543182373
Loss :  1.841188907623291 3.396183967590332 3.396183967590332
Loss :  1.8340991735458374 3.5017154216766357 3.5017154216766357
Loss :  1.837435245513916 3.365669012069702 3.365669012069702
Loss :  1.8313162326812744 3.069542169570923 3.069542169570923
Loss :  1.8320883512496948 3.1693480014801025 3.1693480014801025
Loss :  1.845591425895691 3.458672285079956 3.458672285079956
Loss :  1.8357387781143188 3.25811767578125 3.25811767578125
  batch 60 loss: 1.8357387781143188, 3.25811767578125, 3.25811767578125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.827285885810852 3.3522279262542725 3.3522279262542725
Loss :  1.8257086277008057 3.2143731117248535 3.2143731117248535
Loss :  1.8261504173278809 3.2703371047973633 3.2703371047973633
Loss :  1.826611876487732 3.330634593963623 3.330634593963623
Loss :  1.8295680284500122 3.004483461380005 3.004483461380005
Loss :  1.7363439798355103 4.1004204750061035 4.1004204750061035
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.737229347229004 4.078545570373535 4.078545570373535
Loss :  1.7320067882537842 3.976679801940918 3.976679801940918
Loss :  1.754396677017212 3.991716146469116 3.991716146469116
Total LOSS train 3.2076408459590033 valid 4.036840498447418
CE LOSS train 1.824723274891193 valid 0.438599169254303
Contrastive LOSS train 3.2076408459590033 valid 0.997929036617279
EPOCH 168:
Loss :  1.8332749605178833 3.1197714805603027 3.1197714805603027
Loss :  1.8165146112442017 3.524019956588745 3.524019956588745
Loss :  1.8284865617752075 3.0422093868255615 3.0422093868255615
Loss :  1.8341913223266602 3.235807180404663 3.235807180404663
Loss :  1.8275789022445679 3.1601555347442627 3.1601555347442627
Loss :  1.8336008787155151 3.31524395942688 3.31524395942688
Loss :  1.8180701732635498 3.6848015785217285 3.6848015785217285
Loss :  1.81886887550354 2.876840353012085 2.876840353012085
Loss :  1.8228504657745361 2.745347499847412 2.745347499847412
Loss :  1.8231827020645142 2.7126221656799316 2.7126221656799316
Loss :  1.8278870582580566 2.9629034996032715 2.9629034996032715
Loss :  1.823836088180542 3.275371551513672 3.275371551513672
Loss :  1.8266273736953735 3.3640315532684326 3.3640315532684326
Loss :  1.830568552017212 2.950990915298462 2.950990915298462
Loss :  1.8257768154144287 3.2445154190063477 3.2445154190063477
Loss :  1.8392654657363892 3.2585525512695312 3.2585525512695312
Loss :  1.8275783061981201 3.325556516647339 3.325556516647339
Loss :  1.826063871383667 3.317854642868042 3.317854642868042
Loss :  1.824995517730713 3.1325454711914062 3.1325454711914062
Loss :  1.8213701248168945 2.970860004425049 2.970860004425049
  batch 20 loss: 1.8213701248168945, 2.970860004425049, 2.970860004425049
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8291785717010498 2.9925637245178223 2.9925637245178223
Loss :  1.8274792432785034 3.103416919708252 3.103416919708252
Loss :  1.8225531578063965 3.2526371479034424 3.2526371479034424
Loss :  1.835466980934143 3.5239226818084717 3.5239226818084717
Loss :  1.8329687118530273 3.3840057849884033 3.3840057849884033
Loss :  1.8232142925262451 2.7463345527648926 2.7463345527648926
Loss :  1.829169750213623 3.533581256866455 3.533581256866455
Loss :  1.8204227685928345 3.2898781299591064 3.2898781299591064
Loss :  1.8295748233795166 2.9864959716796875 2.9864959716796875
Loss :  1.8185114860534668 2.92362380027771 2.92362380027771
Loss :  1.8230479955673218 3.2297065258026123 3.2297065258026123
Loss :  1.8187123537063599 3.3695225715637207 3.3695225715637207
Loss :  1.8237522840499878 2.9841883182525635 2.9841883182525635
Loss :  1.8281065225601196 3.1527085304260254 3.1527085304260254
Loss :  1.8278900384902954 3.5197103023529053 3.5197103023529053
Loss :  1.8262109756469727 3.371124744415283 3.371124744415283
Loss :  1.8261083364486694 3.0274717807769775 3.0274717807769775
Loss :  1.8180032968521118 3.6202452182769775 3.6202452182769775
Loss :  1.8277747631072998 3.1387741565704346 3.1387741565704346
Loss :  1.8171885013580322 3.388324499130249 3.388324499130249
  batch 40 loss: 1.8171885013580322, 3.388324499130249, 3.388324499130249
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.82369065284729 3.2857236862182617 3.2857236862182617
Loss :  1.823350429534912 3.214752435684204 3.214752435684204
Loss :  1.8207565546035767 3.1353495121002197 3.1353495121002197
Loss :  1.8146790266036987 3.455171823501587 3.455171823501587
Loss :  1.8239272832870483 3.2641537189483643 3.2641537189483643
Loss :  1.8204443454742432 3.4948346614837646 3.4948346614837646
Loss :  1.8161003589630127 3.4336748123168945 3.4336748123168945
Loss :  1.8207831382751465 3.0909483432769775 3.0909483432769775
Loss :  1.807753562927246 3.3137476444244385 3.3137476444244385
Loss :  1.8199279308319092 3.5390710830688477 3.5390710830688477
Loss :  1.8146451711654663 3.7079241275787354 3.7079241275787354
Loss :  1.829686164855957 3.410738229751587 3.410738229751587
Loss :  1.8218096494674683 3.276529550552368 3.276529550552368
Loss :  1.8302383422851562 3.120619535446167 3.120619535446167
Loss :  1.8200985193252563 3.6378324031829834 3.6378324031829834
Loss :  1.8264240026474 3.711827278137207 3.711827278137207
Loss :  1.8235975503921509 3.2301576137542725 3.2301576137542725
Loss :  1.8198930025100708 3.3461287021636963 3.3461287021636963
Loss :  1.8361804485321045 3.65993332862854 3.65993332862854
Loss :  1.82393217086792 3.180788993835449 3.180788993835449
  batch 60 loss: 1.82393217086792, 3.180788993835449, 3.180788993835449
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8205931186676025 3.2679715156555176 3.2679715156555176
Loss :  1.8148679733276367 3.2613463401794434 3.2613463401794434
Loss :  1.8179254531860352 3.3704171180725098 3.3704171180725098
Loss :  1.8155176639556885 3.083441734313965 3.083441734313965
Loss :  1.8155282735824585 2.726287841796875 2.726287841796875
Loss :  1.7235612869262695 4.16043758392334 4.16043758392334
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.7254964113235474 4.097662925720215 4.097662925720215
Loss :  1.7199848890304565 4.052450180053711 4.052450180053711
Loss :  1.7424814701080322 4.024282455444336 4.024282455444336
Total LOSS train 3.2458093826587384 valid 4.0837082862854
CE LOSS train 1.8239734502939078 valid 0.43562036752700806
Contrastive LOSS train 3.2458093826587384 valid 1.006070613861084
EPOCH 169:
Loss :  1.8179316520690918 3.1773996353149414 3.1773996353149414
Loss :  1.8128137588500977 3.4894816875457764 3.4894816875457764
Loss :  1.8169326782226562 3.491576910018921 3.491576910018921
Loss :  1.81989586353302 3.314019203186035 3.314019203186035
Loss :  1.8160854578018188 3.4942195415496826 3.4942195415496826
Loss :  1.82335364818573 3.0881006717681885 3.0881006717681885
Loss :  1.8087657690048218 2.974637269973755 2.974637269973755
Loss :  1.8099308013916016 3.106856346130371 3.106856346130371
Loss :  1.8074482679367065 3.077420234680176 3.077420234680176
Loss :  1.806104063987732 3.0092694759368896 3.0092694759368896
Loss :  1.8173441886901855 3.2775933742523193 3.2775933742523193
Loss :  1.8177117109298706 3.3983078002929688 3.3983078002929688
Loss :  1.8159953355789185 3.352729082107544 3.352729082107544
Loss :  1.8200201988220215 3.266162395477295 3.266162395477295
Loss :  1.8104101419448853 3.5294382572174072 3.5294382572174072
Loss :  1.8270169496536255 3.4105064868927 3.4105064868927
Loss :  1.817652702331543 3.1114306449890137 3.1114306449890137
Loss :  1.8129537105560303 3.130882978439331 3.130882978439331
Loss :  1.8150060176849365 3.4645562171936035 3.4645562171936035
Loss :  1.8093754053115845 3.5203616619110107 3.5203616619110107
  batch 20 loss: 1.8093754053115845, 3.5203616619110107, 3.5203616619110107
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8188048601150513 3.197873592376709 3.197873592376709
Loss :  1.8180025815963745 3.2096192836761475 3.2096192836761475
Loss :  1.8125333786010742 3.375119924545288 3.375119924545288
Loss :  1.8301914930343628 3.4458422660827637 3.4458422660827637
Loss :  1.82588529586792 3.4760525226593018 3.4760525226593018
Loss :  1.8142330646514893 3.0802855491638184 3.0802855491638184
Loss :  1.8217312097549438 3.1164889335632324 3.1164889335632324
Loss :  1.8107770681381226 3.069434881210327 3.069434881210327
Loss :  1.8206230401992798 2.929054021835327 2.929054021835327
Loss :  1.8113267421722412 3.027829647064209 3.027829647064209
Loss :  1.8181772232055664 3.4541208744049072 3.4541208744049072
Loss :  1.808796763420105 3.3369836807250977 3.3369836807250977
Loss :  1.8170534372329712 3.2392449378967285 3.2392449378967285
Loss :  1.817978024482727 3.4481425285339355 3.4481425285339355
Loss :  1.8199586868286133 3.555889129638672 3.555889129638672
Loss :  1.818669080734253 3.3484225273132324 3.3484225273132324
Loss :  1.8182904720306396 3.2051827907562256 3.2051827907562256
Loss :  1.8053442239761353 3.0336289405822754 3.0336289405822754
Loss :  1.8197027444839478 3.1279380321502686 3.1279380321502686
Loss :  1.8081672191619873 3.0487639904022217 3.0487639904022217
  batch 40 loss: 1.8081672191619873, 3.0487639904022217, 3.0487639904022217
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8166345357894897 3.4348182678222656 3.4348182678222656
Loss :  1.8180413246154785 3.234844446182251 3.234844446182251
Loss :  1.8153550624847412 3.2490525245666504 3.2490525245666504
Loss :  1.811777949333191 3.322922706604004 3.322922706604004
Loss :  1.8184596300125122 2.9991769790649414 2.9991769790649414
Loss :  1.814052939414978 3.0309269428253174 3.0309269428253174
Loss :  1.8100481033325195 2.8189077377319336 2.8189077377319336
Loss :  1.814955472946167 3.1844563484191895 3.1844563484191895
Loss :  1.801202416419983 2.9186506271362305 2.9186506271362305
Loss :  1.815355896949768 2.992718458175659 2.992718458175659
Loss :  1.8078964948654175 3.091987133026123 3.091987133026123
Loss :  1.8255691528320312 3.0698814392089844 3.0698814392089844
Loss :  1.8187288045883179 3.1002650260925293 3.1002650260925293
Loss :  1.825171947479248 3.2769274711608887 3.2769274711608887
Loss :  1.8143649101257324 3.599388599395752 3.599388599395752
Loss :  1.825134515762329 2.704920530319214 2.704920530319214
Loss :  1.822651982307434 3.4666736125946045 3.4666736125946045
Loss :  1.8194811344146729 3.1689116954803467 3.1689116954803467
Loss :  1.8373618125915527 3.3579888343811035 3.3579888343811035
Loss :  1.827009916305542 3.0183379650115967 3.0183379650115967
  batch 60 loss: 1.827009916305542, 3.0183379650115967, 3.0183379650115967
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8249709606170654 2.925931215286255 2.925931215286255
Loss :  1.8207314014434814 2.9164047241210938 2.9164047241210938
Loss :  1.823386311531067 2.9285285472869873 2.9285285472869873
Loss :  1.824081540107727 3.2754969596862793 3.2754969596862793
Loss :  1.8269275426864624 2.7963361740112305 2.7963361740112305
Loss :  1.7444517612457275 3.972276210784912 3.972276210784912
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.7443063259124756 3.913949728012085 3.913949728012085
Loss :  1.739140272140503 3.832946300506592 3.832946300506592
Loss :  1.760664701461792 3.8828136920928955 3.8828136920928955
Total LOSS train 3.204543429154616 valid 3.900496482849121
CE LOSS train 1.8172053337097167 valid 0.440166175365448
Contrastive LOSS train 3.204543429154616 valid 0.9707034230232239
EPOCH 170:
Loss :  1.831632137298584 3.0348358154296875 3.0348358154296875
Loss :  1.819466233253479 3.2440011501312256 3.2440011501312256
Loss :  1.8291385173797607 3.321061372756958 3.321061372756958
Loss :  1.832305669784546 3.4315927028656006 3.4315927028656006
Loss :  1.8268263339996338 3.3637266159057617 3.3637266159057617
Loss :  1.8327901363372803 3.2311501502990723 3.2311501502990723
Loss :  1.8201662302017212 3.0824697017669678 3.0824697017669678
Loss :  1.8209569454193115 3.0004703998565674 3.0004703998565674
Loss :  1.8201099634170532 2.797781467437744 2.797781467437744
Loss :  1.8223347663879395 2.9098010063171387 2.9098010063171387
Loss :  1.8285138607025146 3.523573875427246 3.523573875427246
Loss :  1.8231838941574097 3.302631378173828 3.302631378173828
Loss :  1.8249166011810303 3.344966411590576 3.344966411590576
Loss :  1.82778000831604 3.3239991664886475 3.3239991664886475
Loss :  1.8190703392028809 3.311167001724243 3.311167001724243
Loss :  1.8366063833236694 3.300342082977295 3.300342082977295
Loss :  1.8245683908462524 3.2017393112182617 3.2017393112182617
Loss :  1.823225498199463 3.267711877822876 3.267711877822876
Loss :  1.8220288753509521 3.084412097930908 3.084412097930908
Loss :  1.8201693296432495 3.172079563140869 3.172079563140869
  batch 20 loss: 1.8201693296432495, 3.172079563140869, 3.172079563140869
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8272582292556763 2.9935624599456787 2.9935624599456787
Loss :  1.8255661725997925 3.1436800956726074 3.1436800956726074
Loss :  1.8211596012115479 2.943509578704834 2.943509578704834
Loss :  1.8337533473968506 3.1032023429870605 3.1032023429870605
Loss :  1.832382082939148 3.203522205352783 3.203522205352783
Loss :  1.8228118419647217 3.110059976577759 3.110059976577759
Loss :  1.8285727500915527 3.114885091781616 3.114885091781616
Loss :  1.8191640377044678 2.9564123153686523 2.9564123153686523
Loss :  1.8286365270614624 3.338127374649048 3.338127374649048
Loss :  1.8200911283493042 3.4540646076202393 3.4540646076202393
Loss :  1.823250412940979 3.3855443000793457 3.3855443000793457
Loss :  1.817814826965332 3.4756388664245605 3.4756388664245605
Loss :  1.8244190216064453 3.381237268447876 3.381237268447876
Loss :  1.8283809423446655 3.04449725151062 3.04449725151062
Loss :  1.827010989189148 3.2089600563049316 3.2089600563049316
Loss :  1.8267219066619873 3.138129472732544 3.138129472732544
Loss :  1.8259832859039307 3.333364248275757 3.333364248275757
Loss :  1.8186887502670288 3.275521755218506 3.275521755218506
Loss :  1.8286842107772827 3.2094032764434814 3.2094032764434814
Loss :  1.81805419921875 3.179537057876587 3.179537057876587
  batch 40 loss: 1.81805419921875, 3.179537057876587, 3.179537057876587
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8243749141693115 3.3910903930664062 3.3910903930664062
Loss :  1.8256559371948242 3.2675392627716064 3.2675392627716064
Loss :  1.8205771446228027 3.460838556289673 3.460838556289673
Loss :  1.815716028213501 3.5070059299468994 3.5070059299468994
Loss :  1.8236737251281738 3.594888210296631 3.594888210296631
Loss :  1.819950819015503 3.2996602058410645 3.2996602058410645
Loss :  1.8158115148544312 3.2008488178253174 3.2008488178253174
Loss :  1.819006323814392 3.5652456283569336 3.5652456283569336
Loss :  1.8059065341949463 3.2173562049865723 3.2173562049865723
Loss :  1.8163610696792603 3.1748809814453125 3.1748809814453125
Loss :  1.8089689016342163 3.177870750427246 3.177870750427246
Loss :  1.8258289098739624 3.397813558578491 3.397813558578491
Loss :  1.818542718887329 3.4912161827087402 3.4912161827087402
Loss :  1.8249058723449707 3.4983673095703125 3.4983673095703125
Loss :  1.8133624792099 3.2518277168273926 3.2518277168273926
Loss :  1.8229280710220337 3.415982961654663 3.415982961654663
Loss :  1.8203845024108887 3.2980141639709473 3.2980141639709473
Loss :  1.8143473863601685 3.3146955966949463 3.3146955966949463
Loss :  1.833864450454712 3.4385108947753906 3.4385108947753906
Loss :  1.8228256702423096 3.1997721195220947 3.1997721195220947
  batch 60 loss: 1.8228256702423096, 3.1997721195220947, 3.1997721195220947
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8189011812210083 3.124572992324829 3.124572992324829
Loss :  1.814297080039978 3.157747983932495 3.157747983932495
Loss :  1.8182021379470825 3.164433240890503 3.164433240890503
Loss :  1.8144707679748535 3.1969990730285645 3.1969990730285645
Loss :  1.8157645463943481 3.0193698406219482 3.0193698406219482
Loss :  1.7392559051513672 4.001139163970947 4.001139163970947
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.7399247884750366 4.011942386627197 4.011942386627197
Loss :  1.732917308807373 3.9982733726501465 3.9982733726501465
Loss :  1.759338617324829 3.905089855194092 3.905089855194092
Total LOSS train 3.24721417427063 valid 3.9791111946105957
CE LOSS train 1.8227511240885808 valid 0.4398346543312073
Contrastive LOSS train 3.24721417427063 valid 0.976272463798523
EPOCH 171:
Loss :  1.8208938837051392 3.3415186405181885 3.3415186405181885
Loss :  1.8087407350540161 3.4490647315979004 3.4490647315979004
Loss :  1.8157227039337158 3.1212191581726074 3.1212191581726074
Loss :  1.822669506072998 3.287431478500366 3.287431478500366
Loss :  1.816759705543518 3.345104217529297 3.345104217529297
Loss :  1.8263248205184937 3.288316011428833 3.288316011428833
Loss :  1.804085612297058 3.3045618534088135 3.3045618534088135
Loss :  1.8058446645736694 3.086249589920044 3.086249589920044
Loss :  1.8104138374328613 3.0634195804595947 3.0634195804595947
Loss :  1.8063462972640991 2.9894285202026367 2.9894285202026367
Loss :  1.8165671825408936 3.2837629318237305 3.2837629318237305
Loss :  1.8177158832550049 3.393468141555786 3.393468141555786
Loss :  1.8140363693237305 3.3737149238586426 3.3737149238586426
Loss :  1.8189207315444946 3.360041856765747 3.360041856765747
Loss :  1.8107064962387085 3.31943941116333 3.31943941116333
Loss :  1.8251841068267822 3.1219239234924316 3.1219239234924316
Loss :  1.8143370151519775 3.159120798110962 3.159120798110962
Loss :  1.8096840381622314 3.3559117317199707 3.3559117317199707
Loss :  1.8164994716644287 3.1079647541046143 3.1079647541046143
Loss :  1.8038314580917358 3.2029225826263428 3.2029225826263428
  batch 20 loss: 1.8038314580917358, 3.2029225826263428, 3.2029225826263428
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8173208236694336 3.0505995750427246 3.0505995750427246
Loss :  1.8154795169830322 3.2317018508911133 3.2317018508911133
Loss :  1.8101495504379272 3.222337484359741 3.222337484359741
Loss :  1.8308677673339844 2.9283270835876465 2.9283270835876465
Loss :  1.8292747735977173 3.3135576248168945 3.3135576248168945
Loss :  1.813516616821289 3.100642204284668 3.100642204284668
Loss :  1.8198776245117188 3.174506187438965 3.174506187438965
Loss :  1.8090314865112305 3.0423169136047363 3.0423169136047363
Loss :  1.8164857625961304 3.2134788036346436 3.2134788036346436
Loss :  1.8108067512512207 3.039980888366699 3.039980888366699
Loss :  1.8162258863449097 3.091134786605835 3.091134786605835
Loss :  1.8082222938537598 3.272637367248535 3.272637367248535
Loss :  1.815725564956665 3.2054648399353027 3.2054648399353027
Loss :  1.8171679973602295 3.1397511959075928 3.1397511959075928
Loss :  1.8175854682922363 3.2424933910369873 3.2424933910369873
Loss :  1.8179075717926025 3.0872902870178223 3.0872902870178223
Loss :  1.8177512884140015 3.3316426277160645 3.3316426277160645
Loss :  1.8112269639968872 3.2575478553771973 3.2575478553771973
Loss :  1.8219544887542725 3.2921950817108154 3.2921950817108154
Loss :  1.8119696378707886 2.93935227394104 2.93935227394104
  batch 40 loss: 1.8119696378707886, 2.93935227394104, 2.93935227394104
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8194350004196167 3.5010108947753906 3.5010108947753906
Loss :  1.8187204599380493 3.2539355754852295 3.2539355754852295
Loss :  1.8146915435791016 3.2520976066589355 3.2520976066589355
Loss :  1.8116254806518555 3.291240930557251 3.291240930557251
Loss :  1.8206665515899658 3.026811361312866 3.026811361312866
Loss :  1.8176438808441162 2.9704198837280273 2.9704198837280273
Loss :  1.814752459526062 3.053339958190918 3.053339958190918
Loss :  1.817909836769104 3.4110894203186035 3.4110894203186035
Loss :  1.806954264640808 3.165274143218994 3.165274143218994
Loss :  1.8153183460235596 2.9854211807250977 2.9854211807250977
Loss :  1.8087753057479858 3.306654691696167 3.306654691696167
Loss :  1.8263767957687378 3.1654751300811768 3.1654751300811768
Loss :  1.8199399709701538 3.2372851371765137 3.2372851371765137
Loss :  1.8239643573760986 2.9076952934265137 2.9076952934265137
Loss :  1.813188910484314 3.2316625118255615 3.2316625118255615
Loss :  1.8249810934066772 3.2703893184661865 3.2703893184661865
Loss :  1.8214350938796997 3.7284977436065674 3.7284977436065674
Loss :  1.817467212677002 3.3308188915252686 3.3308188915252686
Loss :  1.8346505165100098 3.3527603149414062 3.3527603149414062
Loss :  1.8245104551315308 3.2516520023345947 3.2516520023345947
  batch 60 loss: 1.8245104551315308, 3.2516520023345947, 3.2516520023345947
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8219683170318604 3.3892905712127686 3.3892905712127686
Loss :  1.8166943788528442 3.092064619064331 3.092064619064331
Loss :  1.8200677633285522 2.965226888656616 2.965226888656616
Loss :  1.8183447122573853 3.1781535148620605 3.1781535148620605
Loss :  1.820216178894043 3.152235984802246 3.152235984802246
Loss :  1.7291443347930908 4.132795333862305 4.132795333862305
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.7297905683517456 3.90921688079834 3.90921688079834
Loss :  1.7229220867156982 3.9776713848114014 3.9776713848114014
Loss :  1.7498034238815308 3.993410348892212 3.993410348892212
Total LOSS train 3.20926225735591 valid 4.0032734870910645
CE LOSS train 1.816678942166842 valid 0.4374508559703827
Contrastive LOSS train 3.20926225735591 valid 0.998352587223053
EPOCH 172:
Loss :  1.8249772787094116 2.9289352893829346 2.9289352893829346
Loss :  1.8156609535217285 3.4246232509613037 3.4246232509613037
Loss :  1.8221826553344727 2.870307683944702 2.870307683944702
Loss :  1.8254395723342896 2.978804111480713 2.978804111480713
Loss :  1.82075035572052 3.302640438079834 3.302640438079834
Loss :  1.8276609182357788 3.0783846378326416 3.0783846378326416
Loss :  1.8111333847045898 3.2512118816375732 3.2512118816375732
Loss :  1.8113391399383545 3.2914319038391113 3.2914319038391113
Loss :  1.812027931213379 3.520317316055298 3.520317316055298
Loss :  1.8081250190734863 2.901488780975342 2.901488780975342
Loss :  1.8170355558395386 3.2079741954803467 3.2079741954803467
Loss :  1.816654920578003 3.5268776416778564 3.5268776416778564
Loss :  1.8148281574249268 3.5963733196258545 3.5963733196258545
Loss :  1.8180701732635498 3.1326253414154053 3.1326253414154053
Loss :  1.8076117038726807 3.295227289199829 3.295227289199829
Loss :  1.8242591619491577 3.276275396347046 3.276275396347046
Loss :  1.815443515777588 3.245530605316162 3.245530605316162
Loss :  1.8100656270980835 3.44978928565979 3.44978928565979
Loss :  1.8130595684051514 3.3453423976898193 3.3453423976898193
Loss :  1.8046529293060303 3.390475273132324 3.390475273132324
  batch 20 loss: 1.8046529293060303, 3.390475273132324, 3.390475273132324
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8157563209533691 3.3946897983551025 3.3946897983551025
Loss :  1.8141376972198486 3.4362716674804688 3.4362716674804688
Loss :  1.807877779006958 3.428870439529419 3.428870439529419
Loss :  1.8265130519866943 3.181976079940796 3.181976079940796
Loss :  1.821842074394226 3.447969913482666 3.447969913482666
Loss :  1.8092150688171387 3.120816707611084 3.120816707611084
Loss :  1.817988634109497 3.2396507263183594 3.2396507263183594
Loss :  1.805700421333313 3.155968427658081 3.155968427658081
Loss :  1.815127968788147 3.1033806800842285 3.1033806800842285
Loss :  1.8055177927017212 2.865384578704834 2.865384578704834
Loss :  1.8140121698379517 3.0489425659179688 3.0489425659179688
Loss :  1.8038283586502075 3.227477550506592 3.227477550506592
Loss :  1.8131794929504395 3.152388572692871 3.152388572692871
Loss :  1.8114663362503052 3.1829185485839844 3.1829185485839844
Loss :  1.8146051168441772 3.2769243717193604 3.2769243717193604
Loss :  1.8142399787902832 3.1470792293548584 3.1470792293548584
Loss :  1.8135274648666382 3.1533877849578857 3.1533877849578857
Loss :  1.8036521673202515 3.051461935043335 3.051461935043335
Loss :  1.8169573545455933 3.088408946990967 3.088408946990967
Loss :  1.8063138723373413 2.966003894805908 2.966003894805908
  batch 40 loss: 1.8063138723373413, 2.966003894805908, 2.966003894805908
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8151862621307373 3.041205406188965 3.041205406188965
Loss :  1.8163615465164185 3.2163584232330322 3.2163584232330322
Loss :  1.812021255493164 3.3637499809265137 3.3637499809265137
Loss :  1.80908203125 3.0040788650512695 3.0040788650512695
Loss :  1.8174281120300293 2.977851629257202 2.977851629257202
Loss :  1.8145337104797363 3.162792205810547 3.162792205810547
Loss :  1.8128423690795898 3.0412204265594482 3.0412204265594482
Loss :  1.816240668296814 3.43880033493042 3.43880033493042
Loss :  1.8065435886383057 2.9483866691589355 2.9483866691589355
Loss :  1.817702054977417 3.0416407585144043 3.0416407585144043
Loss :  1.8127957582473755 3.340405225753784 3.340405225753784
Loss :  1.8299973011016846 3.2838428020477295 3.2838428020477295
Loss :  1.8216809034347534 3.32326602935791 3.32326602935791
Loss :  1.8280092477798462 3.125237464904785 3.125237464904785
Loss :  1.8165920972824097 3.463143825531006 3.463143825531006
Loss :  1.8274112939834595 3.0929808616638184 3.0929808616638184
Loss :  1.8222665786743164 3.5511014461517334 3.5511014461517334
Loss :  1.8190492391586304 3.3022379875183105 3.3022379875183105
Loss :  1.8349195718765259 3.362013101577759 3.362013101577759
Loss :  1.8251373767852783 3.201321840286255 3.201321840286255
  batch 60 loss: 1.8251373767852783, 3.201321840286255, 3.201321840286255
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.822277307510376 3.1190192699432373 3.1190192699432373
Loss :  1.8165030479431152 3.311255931854248 3.311255931854248
Loss :  1.8201425075531006 3.358816385269165 3.358816385269165
Loss :  1.8180015087127686 2.965517997741699 2.965517997741699
Loss :  1.8196682929992676 2.689457416534424 2.689457416534424
Loss :  1.7274432182312012 4.080336093902588 4.080336093902588
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.7279521226882935 4.03848934173584 4.03848934173584
Loss :  1.7223572731018066 4.054632186889648 4.054632186889648
Loss :  1.7479223012924194 3.90885853767395 3.90885853767395
Total LOSS train 3.2063124730036807 valid 4.020579040050507
CE LOSS train 1.816074327322153 valid 0.43698057532310486
Contrastive LOSS train 3.2063124730036807 valid 0.9772146344184875
EPOCH 173:
Loss :  1.824157476425171 3.2929046154022217 3.2929046154022217
Loss :  1.8160514831542969 3.3139803409576416 3.3139803409576416
Loss :  1.8220808506011963 3.036747694015503 3.036747694015503
Loss :  1.8242480754852295 3.5675976276397705 3.5675976276397705
Loss :  1.8204306364059448 3.1072347164154053 3.1072347164154053
Loss :  1.8265286684036255 3.243441581726074 3.243441581726074
Loss :  1.8139698505401611 3.043323278427124 3.043323278427124
Loss :  1.8141056299209595 3.2601826190948486 3.2601826190948486
Loss :  1.8123031854629517 3.4711577892303467 3.4711577892303467
Loss :  1.810624122619629 3.0757405757904053 3.0757405757904053
Loss :  1.820375919342041 3.492253303527832 3.492253303527832
Loss :  1.817353367805481 3.6787309646606445 3.6787309646606445
Loss :  1.8188549280166626 3.2473597526550293 3.2473597526550293
Loss :  1.8209147453308105 3.2412445545196533 3.2412445545196533
Loss :  1.8094431161880493 3.501746892929077 3.501746892929077
Loss :  1.8279211521148682 3.495650053024292 3.495650053024292
Loss :  1.8192261457443237 3.14304256439209 3.14304256439209
Loss :  1.8162404298782349 2.981555461883545 2.981555461883545
Loss :  1.8165088891983032 3.1239688396453857 3.1239688396453857
Loss :  1.8137294054031372 3.2565741539001465 3.2565741539001465
  batch 20 loss: 1.8137294054031372, 3.2565741539001465, 3.2565741539001465
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8226200342178345 2.9082977771759033 2.9082977771759033
Loss :  1.8204500675201416 3.1923277378082275 3.1923277378082275
Loss :  1.815053939819336 3.350490093231201 3.350490093231201
Loss :  1.8294858932495117 3.153096914291382 3.153096914291382
Loss :  1.826379656791687 3.326033115386963 3.326033115386963
Loss :  1.8174067735671997 2.8978662490844727 2.8978662490844727
Loss :  1.824907898902893 3.086970567703247 3.086970567703247
Loss :  1.815184473991394 3.112840175628662 3.112840175628662
Loss :  1.823030710220337 3.2938222885131836 3.2938222885131836
Loss :  1.8123714923858643 3.1734507083892822 3.1734507083892822
Loss :  1.8169338703155518 3.3732831478118896 3.3732831478118896
Loss :  1.8119175434112549 3.3125202655792236 3.3125202655792236
Loss :  1.8190606832504272 3.5935404300689697 3.5935404300689697
Loss :  1.8190784454345703 3.189054250717163 3.189054250717163
Loss :  1.819063425064087 3.3433144092559814 3.3433144092559814
Loss :  1.8177952766418457 3.362731456756592 3.362731456756592
Loss :  1.8178341388702393 3.2937545776367188 3.2937545776367188
Loss :  1.8091082572937012 3.0799403190612793 3.0799403190612793
Loss :  1.8192360401153564 3.1979596614837646 3.1979596614837646
Loss :  1.8081035614013672 3.0969836711883545 3.0969836711883545
  batch 40 loss: 1.8081035614013672, 3.0969836711883545, 3.0969836711883545
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8161578178405762 3.469080924987793 3.469080924987793
Loss :  1.8171403408050537 3.1038007736206055 3.1038007736206055
Loss :  1.8147602081298828 3.2114601135253906 3.2114601135253906
Loss :  1.8103607892990112 3.335308313369751 3.335308313369751
Loss :  1.818398118019104 3.077932596206665 3.077932596206665
Loss :  1.8118188381195068 3.162733793258667 3.162733793258667
Loss :  1.8104232549667358 3.2828965187072754 3.2828965187072754
Loss :  1.815303087234497 3.34858775138855 3.34858775138855
Loss :  1.7996670007705688 2.9645297527313232 2.9645297527313232
Loss :  1.8176333904266357 2.7875027656555176 2.7875027656555176
Loss :  1.8096709251403809 3.0649664402008057 3.0649664402008057
Loss :  1.8248310089111328 3.015336036682129 3.015336036682129
Loss :  1.814919352531433 2.9906160831451416 2.9906160831451416
Loss :  1.825142741203308 3.087775945663452 3.087775945663452
Loss :  1.8167364597320557 3.0337767601013184 3.0337767601013184
Loss :  1.8258649110794067 3.22773814201355 3.22773814201355
Loss :  1.820517897605896 3.2662193775177 3.2662193775177
Loss :  1.8167750835418701 3.2592263221740723 3.2592263221740723
Loss :  1.8333213329315186 3.3972222805023193 3.3972222805023193
Loss :  1.8258163928985596 3.2635586261749268 3.2635586261749268
  batch 60 loss: 1.8258163928985596, 3.2635586261749268, 3.2635586261749268
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8185923099517822 3.3127777576446533 3.3127777576446533
Loss :  1.8145673274993896 3.3655104637145996 3.3655104637145996
Loss :  1.819248080253601 3.3944027423858643 3.3944027423858643
Loss :  1.8129630088806152 3.1911230087280273 3.1911230087280273
Loss :  1.8137784004211426 2.9770073890686035 2.9770073890686035
Loss :  1.751071572303772 4.038107872009277 4.038107872009277
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.7474470138549805 3.8276102542877197 3.8276102542877197
Loss :  1.7471033334732056 3.912872076034546 3.912872076034546
Loss :  1.7692838907241821 3.666858673095703 3.666858673095703
Total LOSS train 3.223135475011972 valid 3.8613622188568115
CE LOSS train 1.8177615129030669 valid 0.44232097268104553
Contrastive LOSS train 3.223135475011972 valid 0.9167146682739258
EPOCH 174:
Loss :  1.8207656145095825 3.600966453552246 3.600966453552246
Loss :  1.8100186586380005 3.2429745197296143 3.2429745197296143
Loss :  1.8154937028884888 2.971602201461792 2.971602201461792
Loss :  1.8197983503341675 3.2531394958496094 3.2531394958496094
Loss :  1.8162215948104858 3.0351481437683105 3.0351481437683105
Loss :  1.8234971761703491 3.1598615646362305 3.1598615646362305
Loss :  1.8056286573410034 3.1363565921783447 3.1363565921783447
Loss :  1.8060085773468018 3.231980800628662 3.231980800628662
Loss :  1.8077282905578613 2.9827487468719482 2.9827487468719482
Loss :  1.8021432161331177 3.2513315677642822 3.2513315677642822
Loss :  1.8146785497665405 3.060276985168457 3.060276985168457
Loss :  1.8137764930725098 3.2765023708343506 3.2765023708343506
Loss :  1.8133479356765747 3.4974746704101562 3.4974746704101562
Loss :  1.8162909746170044 3.188091993331909 3.188091993331909
Loss :  1.8067412376403809 3.408749580383301 3.408749580383301
Loss :  1.824506402015686 3.5281853675842285 3.5281853675842285
Loss :  1.8155632019042969 3.310460090637207 3.310460090637207
Loss :  1.8109681606292725 3.679736614227295 3.679736614227295
Loss :  1.8120371103286743 2.849080801010132 2.849080801010132
Loss :  1.8048663139343262 3.2058446407318115 3.2058446407318115
  batch 20 loss: 1.8048663139343262, 3.2058446407318115, 3.2058446407318115
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8163607120513916 2.8017425537109375 2.8017425537109375
Loss :  1.815157413482666 2.9629454612731934 2.9629454612731934
Loss :  1.810960292816162 2.8576650619506836 2.8576650619506836
Loss :  1.8268048763275146 2.9807097911834717 2.9807097911834717
Loss :  1.8240528106689453 3.2331809997558594 3.2331809997558594
Loss :  1.8160622119903564 2.8015177249908447 2.8015177249908447
Loss :  1.8242188692092896 3.0572924613952637 3.0572924613952637
Loss :  1.8155314922332764 2.8736860752105713 2.8736860752105713
Loss :  1.8223646879196167 3.2089805603027344 3.2089805603027344
Loss :  1.8171454668045044 2.9141652584075928 2.9141652584075928
Loss :  1.8191267251968384 3.1536529064178467 3.1536529064178467
Loss :  1.8168151378631592 3.1998441219329834 3.1998441219329834
Loss :  1.823785662651062 3.038356065750122 3.038356065750122
Loss :  1.8241238594055176 2.9725751876831055 2.9725751876831055
Loss :  1.8243412971496582 3.1622750759124756 3.1622750759124756
Loss :  1.8230860233306885 3.1594674587249756 3.1594674587249756
Loss :  1.8240729570388794 3.308377504348755 3.308377504348755
Loss :  1.8187731504440308 3.4099245071411133 3.4099245071411133
Loss :  1.8262484073638916 3.1797778606414795 3.1797778606414795
Loss :  1.815877079963684 3.0077273845672607 3.0077273845672607
  batch 40 loss: 1.815877079963684, 3.0077273845672607, 3.0077273845672607
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8223103284835815 3.3005154132843018 3.3005154132843018
Loss :  1.8241758346557617 3.033752918243408 3.033752918243408
Loss :  1.8212381601333618 3.024568796157837 3.024568796157837
Loss :  1.8151277303695679 2.9796090126037598 2.9796090126037598
Loss :  1.8254042863845825 2.880707025527954 2.880707025527954
Loss :  1.8198658227920532 3.0599255561828613 3.0599255561828613
Loss :  1.8183811902999878 3.066662073135376 3.066662073135376
Loss :  1.821924090385437 3.0521833896636963 3.0521833896636963
Loss :  1.8093880414962769 3.126697301864624 3.126697301864624
Loss :  1.820613980293274 2.9148855209350586 2.9148855209350586
Loss :  1.814347267150879 3.527066469192505 3.527066469192505
Loss :  1.8306797742843628 3.3543479442596436 3.3543479442596436
Loss :  1.8224221467971802 3.266531467437744 3.266531467437744
Loss :  1.826773762702942 3.4464752674102783 3.4464752674102783
Loss :  1.8166155815124512 3.543705701828003 3.543705701828003
Loss :  1.830106496810913 3.228807210922241 3.228807210922241
Loss :  1.824824571609497 3.3296847343444824 3.3296847343444824
Loss :  1.8223599195480347 3.4163691997528076 3.4163691997528076
Loss :  1.836897611618042 3.199737310409546 3.199737310409546
Loss :  1.8280863761901855 3.0961344242095947 3.0961344242095947
  batch 60 loss: 1.8280863761901855, 3.0961344242095947, 3.0961344242095947
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8249943256378174 3.313415288925171 3.313415288925171
Loss :  1.8200891017913818 3.2021615505218506 3.2021615505218506
Loss :  1.8217613697052002 3.1642205715179443 3.1642205715179443
Loss :  1.8217567205429077 3.1566383838653564 3.1566383838653564
Loss :  1.8239524364471436 2.8572089672088623 2.8572089672088623
Loss :  1.7377201318740845 4.1908392906188965 4.1908392906188965
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.7367236614227295 4.0590057373046875 4.0590057373046875
Loss :  1.73078191280365 3.945624351501465 3.945624351501465
Loss :  1.7568358182907104 3.9657578468322754 3.9657578468322754
Total LOSS train 3.1645598264840933 valid 4.040306806564331
CE LOSS train 1.8189090196902935 valid 0.4392089545726776
Contrastive LOSS train 3.1645598264840933 valid 0.9914394617080688
EPOCH 175:
Loss :  1.8280909061431885 2.7572288513183594 2.7572288513183594
Loss :  1.8178627490997314 3.1484107971191406 3.1484107971191406
Loss :  1.8248119354248047 2.8039064407348633 2.8039064407348633
Loss :  1.826459288597107 2.8384361267089844 2.8384361267089844
Loss :  1.8223340511322021 2.8118937015533447 2.8118937015533447
Loss :  1.8288850784301758 2.957864999771118 2.957864999771118
Loss :  1.8158981800079346 3.041405439376831 3.041405439376831
Loss :  1.816225528717041 3.1208887100219727 3.1208887100219727
Loss :  1.817549705505371 3.2618532180786133 3.2618532180786133
Loss :  1.8173768520355225 3.032860040664673 3.032860040664673
Loss :  1.8250555992126465 3.3427515029907227 3.3427515029907227
Loss :  1.8195960521697998 3.565352439880371 3.565352439880371
Loss :  1.8216127157211304 3.1377952098846436 3.1377952098846436
Loss :  1.8243094682693481 3.0874340534210205 3.0874340534210205
Loss :  1.8141556978225708 3.184556484222412 3.184556484222412
Loss :  1.8312692642211914 3.3858766555786133 3.3858766555786133
Loss :  1.8202433586120605 3.2012815475463867 3.2012815475463867
Loss :  1.816645622253418 3.311176300048828 3.311176300048828
Loss :  1.817513108253479 3.293769598007202 3.293769598007202
Loss :  1.8100515604019165 3.179161787033081 3.179161787033081
  batch 20 loss: 1.8100515604019165, 3.179161787033081, 3.179161787033081
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.820204734802246 3.048443555831909 3.048443555831909
Loss :  1.8181594610214233 3.129268169403076 3.129268169403076
Loss :  1.812356948852539 3.2862274646759033 3.2862274646759033
Loss :  1.8258594274520874 2.9857277870178223 2.9857277870178223
Loss :  1.8243170976638794 3.276618719100952 3.276618719100952
Loss :  1.8154277801513672 3.180126905441284 3.180126905441284
Loss :  1.8235596418380737 3.49630069732666 3.49630069732666
Loss :  1.8119738101959229 3.3086228370666504 3.3086228370666504
Loss :  1.8216159343719482 3.0604209899902344 3.0604209899902344
Loss :  1.8121459484100342 3.6422207355499268 3.6422207355499268
Loss :  1.817276954650879 3.4529502391815186 3.4529502391815186
Loss :  1.8075053691864014 3.46867036819458 3.46867036819458
Loss :  1.8161426782608032 2.9788832664489746 2.9788832664489746
Loss :  1.8135794401168823 3.045680522918701 3.045680522918701
Loss :  1.8163213729858398 3.367766857147217 3.367766857147217
Loss :  1.814659595489502 3.1894099712371826 3.1894099712371826
Loss :  1.8123811483383179 3.517009973526001 3.517009973526001
Loss :  1.7979954481124878 3.0446090698242188 3.0446090698242188
Loss :  1.8127176761627197 3.1707770824432373 3.1707770824432373
Loss :  1.7993443012237549 2.950183391571045 2.950183391571045
  batch 40 loss: 1.7993443012237549, 2.950183391571045, 2.950183391571045
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8095695972442627 3.169875383377075 3.169875383377075
Loss :  1.8153026103973389 3.0061116218566895 3.0061116218566895
Loss :  1.8122308254241943 3.3377721309661865 3.3377721309661865
Loss :  1.807577133178711 3.3280270099639893 3.3280270099639893
Loss :  1.8157100677490234 3.4452476501464844 3.4452476501464844
Loss :  1.8052552938461304 3.049325466156006 3.049325466156006
Loss :  1.8020622730255127 3.012640953063965 3.012640953063965
Loss :  1.8120191097259521 3.0633621215820312 3.0633621215820312
Loss :  1.7906745672225952 3.0677781105041504 3.0677781105041504
Loss :  1.8144551515579224 3.2615621089935303 3.2615621089935303
Loss :  1.8046599626541138 3.369676351547241 3.369676351547241
Loss :  1.8174678087234497 3.248964548110962 3.248964548110962
Loss :  1.8085278272628784 3.5372824668884277 3.5372824668884277
Loss :  1.8197904825210571 3.127054452896118 3.127054452896118
Loss :  1.8102384805679321 3.4391369819641113 3.4391369819641113
Loss :  1.8181755542755127 3.042039155960083 3.042039155960083
Loss :  1.8161543607711792 3.661024332046509 3.661024332046509
Loss :  1.8117116689682007 3.495421886444092 3.495421886444092
Loss :  1.8308613300323486 3.3704674243927 3.3704674243927
Loss :  1.8224855661392212 3.118516445159912 3.118516445159912
  batch 60 loss: 1.8224855661392212, 3.118516445159912, 3.118516445159912
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8206114768981934 2.9609386920928955 2.9609386920928955
Loss :  1.8164551258087158 2.949899911880493 2.949899911880493
Loss :  1.819411277770996 3.050215005874634 3.050215005874634
Loss :  1.8208690881729126 2.84555721282959 2.84555721282959
Loss :  1.8238452672958374 2.59792160987854 2.59792160987854
Loss :  1.7479348182678223 4.108086585998535 4.108086585998535
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.7456021308898926 3.9312963485717773 3.9312963485717773
Loss :  1.743726134300232 3.9316203594207764 3.9316203594207764
Loss :  1.7648018598556519 4.011578559875488 4.011578559875488
Total LOSS train 3.178763716037457 valid 3.9956454634666443
CE LOSS train 1.8162402061315683 valid 0.44120046496391296
Contrastive LOSS train 3.178763716037457 valid 1.002894639968872
EPOCH 176:
Loss :  1.832653284072876 2.690387010574341 2.690387010574341
Loss :  1.8217557668685913 3.10376238822937 3.10376238822937
Loss :  1.8315757513046265 2.9453327655792236 2.9453327655792236
Loss :  1.83412504196167 3.242550849914551 3.242550849914551
Loss :  1.8299357891082764 3.22902774810791 3.22902774810791
Loss :  1.835307002067566 3.1914241313934326 3.1914241313934326
Loss :  1.8237463235855103 3.0220253467559814 3.0220253467559814
Loss :  1.8237078189849854 2.936474561691284 2.936474561691284
Loss :  1.824752688407898 3.0107264518737793 3.0107264518737793
Loss :  1.8276304006576538 3.123061180114746 3.123061180114746
Loss :  1.831257700920105 3.3524839878082275 3.3524839878082275
Loss :  1.8212990760803223 3.4101812839508057 3.4101812839508057
Loss :  1.8273998498916626 3.599876880645752 3.599876880645752
Loss :  1.8294216394424438 3.2254385948181152 3.2254385948181152
Loss :  1.822351336479187 3.109833002090454 3.109833002090454
Loss :  1.8368220329284668 2.9432482719421387 2.9432482719421387
Loss :  1.8248727321624756 3.095010280609131 3.095010280609131
Loss :  1.8253227472305298 3.115741729736328 3.115741729736328
Loss :  1.8225983381271362 3.1538889408111572 3.1538889408111572
Loss :  1.8181538581848145 3.081278085708618 3.081278085708618
  batch 20 loss: 1.8181538581848145, 3.081278085708618, 3.081278085708618
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.827297568321228 3.2931580543518066 3.2931580543518066
Loss :  1.8234111070632935 3.230727195739746 3.230727195739746
Loss :  1.8191388845443726 2.892747640609741 2.892747640609741
Loss :  1.8299967050552368 3.082796573638916 3.082796573638916
Loss :  1.8265364170074463 3.2229931354522705 3.2229931354522705
Loss :  1.8190057277679443 2.9310171604156494 2.9310171604156494
Loss :  1.8255821466445923 3.149709939956665 3.149709939956665
Loss :  1.8159139156341553 3.304192543029785 3.304192543029785
Loss :  1.8249558210372925 3.4421374797821045 3.4421374797821045
Loss :  1.8121662139892578 3.323155641555786 3.323155641555786
Loss :  1.8181966543197632 3.42887282371521 3.42887282371521
Loss :  1.813158631324768 3.4168121814727783 3.4168121814727783
Loss :  1.8207117319107056 2.9935240745544434 2.9935240745544434
Loss :  1.8225420713424683 2.967468023300171 2.967468023300171
Loss :  1.8223663568496704 2.9890267848968506 2.9890267848968506
Loss :  1.8231455087661743 2.931083917617798 2.931083917617798
Loss :  1.822657585144043 2.97072434425354 2.97072434425354
Loss :  1.8185930252075195 3.2344560623168945 3.2344560623168945
Loss :  1.826391577720642 3.2262909412384033 3.2262909412384033
Loss :  1.817557692527771 3.410324811935425 3.410324811935425
  batch 40 loss: 1.817557692527771, 3.410324811935425, 3.410324811935425
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8233286142349243 3.2824838161468506 3.2824838161468506
Loss :  1.8252757787704468 2.9340896606445312 2.9340896606445312
Loss :  1.8217264413833618 2.856064558029175 2.856064558029175
Loss :  1.8147799968719482 2.9859132766723633 2.9859132766723633
Loss :  1.826266884803772 3.0439791679382324 3.0439791679382324
Loss :  1.822805643081665 3.074431896209717 3.074431896209717
Loss :  1.8203407526016235 3.2112162113189697 3.2112162113189697
Loss :  1.8230607509613037 3.294914722442627 3.294914722442627
Loss :  1.8137718439102173 3.249694347381592 3.249694347381592
Loss :  1.8198082447052002 3.3178904056549072 3.3178904056549072
Loss :  1.814602017402649 3.2937092781066895 3.2937092781066895
Loss :  1.8300584554672241 3.2593774795532227 3.2593774795532227
Loss :  1.8206349611282349 3.427854061126709 3.427854061126709
Loss :  1.825201153755188 3.506046772003174 3.506046772003174
Loss :  1.8144588470458984 3.4920990467071533 3.4920990467071533
Loss :  1.826341986656189 3.3556742668151855 3.3556742668151855
Loss :  1.822199821472168 3.4570741653442383 3.4570741653442383
Loss :  1.8173915147781372 3.2452356815338135 3.2452356815338135
Loss :  1.8339751958847046 3.44169020652771 3.44169020652771
Loss :  1.8206424713134766 3.6597776412963867 3.6597776412963867
  batch 60 loss: 1.8206424713134766, 3.6597776412963867, 3.6597776412963867
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8186700344085693 3.1321260929107666 3.1321260929107666
Loss :  1.8114714622497559 3.0571069717407227 3.0571069717407227
Loss :  1.8164947032928467 3.3833138942718506 3.3833138942718506
Loss :  1.8132281303405762 3.239657163619995 3.239657163619995
Loss :  1.8134981393814087 2.9769551753997803 2.9769551753997803
Loss :  1.8135900497436523 3.8484232425689697 3.8484232425689697
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7982511520385742 3.715299129486084 3.715299129486084
Loss :  1.807731032371521 3.7573492527008057 3.7573492527008057
Loss :  1.818893551826477 3.7668967247009277 3.7668967247009277
Total LOSS train 3.1877130581782414 valid 3.7719920873641968
CE LOSS train 1.8228315133314865 valid 0.45472338795661926
Contrastive LOSS train 3.1877130581782414 valid 0.9417241811752319
EPOCH 177:
Loss :  1.8191965818405151 3.2711730003356934 3.2711730003356934
Loss :  1.8143677711486816 3.509608030319214 3.509608030319214
Loss :  1.816049337387085 3.245495319366455 3.245495319366455
Loss :  1.8166043758392334 3.120185613632202 3.120185613632202
Loss :  1.815653681755066 3.164179563522339 3.164179563522339
Loss :  1.821775197982788 3.0522267818450928 3.0522267818450928
Loss :  1.8098101615905762 3.117384195327759 3.117384195327759
Loss :  1.8094888925552368 3.022385835647583 3.022385835647583
Loss :  1.8112783432006836 3.206840753555298 3.206840753555298
Loss :  1.8107823133468628 3.289599657058716 3.289599657058716
Loss :  1.819447636604309 3.192850351333618 3.192850351333618
Loss :  1.816597580909729 3.3744919300079346 3.3744919300079346
Loss :  1.819165825843811 3.449201822280884 3.449201822280884
Loss :  1.8229924440383911 3.1903560161590576 3.1903560161590576
Loss :  1.8101081848144531 3.0570693016052246 3.0570693016052246
Loss :  1.829780101776123 3.061676502227783 3.061676502227783
Loss :  1.8196280002593994 3.1062049865722656 3.1062049865722656
Loss :  1.8165547847747803 3.2273199558258057 3.2273199558258057
Loss :  1.8118457794189453 3.106856107711792 3.106856107711792
Loss :  1.813922643661499 3.1911134719848633 3.1911134719848633
  batch 20 loss: 1.813922643661499, 3.1911134719848633, 3.1911134719848633
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8202852010726929 3.1275811195373535 3.1275811195373535
Loss :  1.8189687728881836 3.21661376953125 3.21661376953125
Loss :  1.8159087896347046 3.1170389652252197 3.1170389652252197
Loss :  1.8271386623382568 3.0782952308654785 3.0782952308654785
Loss :  1.8244773149490356 3.3959553241729736 3.3959553241729736
Loss :  1.8176672458648682 3.106475830078125 3.106475830078125
Loss :  1.8233418464660645 3.258497714996338 3.258497714996338
Loss :  1.8121730089187622 3.5308187007904053 3.5308187007904053
Loss :  1.8194516897201538 3.42868971824646 3.42868971824646
Loss :  1.8107179403305054 3.31557559967041 3.31557559967041
Loss :  1.8147974014282227 3.3806540966033936 3.3806540966033936
Loss :  1.8060702085494995 3.1580023765563965 3.1580023765563965
Loss :  1.815553903579712 3.0304131507873535 3.0304131507873535
Loss :  1.8121284246444702 3.068408489227295 3.068408489227295
Loss :  1.8160759210586548 3.2110936641693115 3.2110936641693115
Loss :  1.8133748769760132 3.2965636253356934 3.2965636253356934
Loss :  1.813590407371521 3.2245569229125977 3.2245569229125977
Loss :  1.8002886772155762 3.1142725944519043 3.1142725944519043
Loss :  1.8138395547866821 3.1529183387756348 3.1529183387756348
Loss :  1.8003616333007812 3.0653555393218994 3.0653555393218994
  batch 40 loss: 1.8003616333007812, 3.0653555393218994, 3.0653555393218994
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8084386587142944 3.3532705307006836 3.3532705307006836
Loss :  1.8085993528366089 3.0464348793029785 3.0464348793029785
Loss :  1.8028813600540161 3.355973720550537 3.355973720550537
Loss :  1.8007811307907104 3.358804941177368 3.358804941177368
Loss :  1.8046663999557495 3.157484769821167 3.157484769821167
Loss :  1.7975295782089233 3.1144518852233887 3.1144518852233887
Loss :  1.7962218523025513 3.2551639080047607 3.2551639080047607
Loss :  1.7983076572418213 3.3870432376861572 3.3870432376861572
Loss :  1.7860803604125977 3.1866791248321533 3.1866791248321533
Loss :  1.7994329929351807 3.2830824851989746 3.2830824851989746
Loss :  1.7877051830291748 3.3492727279663086 3.3492727279663086
Loss :  1.8082385063171387 3.280116319656372 3.280116319656372
Loss :  1.803411602973938 3.276554822921753 3.276554822921753
Loss :  1.804711937904358 3.3442208766937256 3.3442208766937256
Loss :  1.7950634956359863 3.5068652629852295 3.5068652629852295
Loss :  1.809657096862793 3.123871326446533 3.123871326446533
Loss :  1.8067480325698853 3.249480724334717 3.249480724334717
Loss :  1.7944267988204956 3.1977221965789795 3.1977221965789795
Loss :  1.8195699453353882 3.4230246543884277 3.4230246543884277
Loss :  1.8101630210876465 3.218456268310547 3.218456268310547
  batch 60 loss: 1.8101630210876465, 3.218456268310547, 3.218456268310547
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8056565523147583 3.459315776824951 3.459315776824951
Loss :  1.798801302909851 3.36588191986084 3.36588191986084
Loss :  1.8078975677490234 3.0793967247009277 3.0793967247009277
Loss :  1.7995095252990723 3.2563326358795166 3.2563326358795166
Loss :  1.7973663806915283 2.859421730041504 2.859421730041504
Loss :  1.7353943586349487 4.26272439956665 4.26272439956665
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.736660361289978 4.172738552093506 4.172738552093506
Loss :  1.7294111251831055 4.1707682609558105 4.1707682609558105
Loss :  1.7600882053375244 4.07243013381958 4.07243013381958
Total LOSS train 3.2264972833486705 valid 4.169665336608887
CE LOSS train 1.810355806350708 valid 0.4400220513343811
Contrastive LOSS train 3.2264972833486705 valid 1.018107533454895
EPOCH 178:
Loss :  1.809067964553833 2.8605520725250244 2.8605520725250244
Loss :  1.8028370141983032 3.164870262145996 3.164870262145996
Loss :  1.8044111728668213 3.157505512237549 3.157505512237549
Loss :  1.8116074800491333 3.2415831089019775 3.2415831089019775
Loss :  1.8100028038024902 3.06663179397583 3.06663179397583
Loss :  1.819138765335083 2.9739830493927 2.9739830493927
Loss :  1.8021994829177856 2.9722189903259277 2.9722189903259277
Loss :  1.8046824932098389 2.92885160446167 2.92885160446167
Loss :  1.8068925142288208 2.7880117893218994 2.7880117893218994
Loss :  1.8086247444152832 2.6676440238952637 2.6676440238952637
Loss :  1.818482756614685 2.902454137802124 2.902454137802124
Loss :  1.817384958267212 3.1080427169799805 3.1080427169799805
Loss :  1.8187044858932495 2.947978973388672 2.947978973388672
Loss :  1.8252668380737305 3.1909756660461426 3.1909756660461426
Loss :  1.8219516277313232 3.2706809043884277 3.2706809043884277
Loss :  1.8406271934509277 3.1990203857421875 3.1990203857421875
Loss :  1.8249865770339966 3.274583578109741 3.274583578109741
Loss :  1.829436182975769 3.187373399734497 3.187373399734497
Loss :  1.8246185779571533 3.0360913276672363 3.0360913276672363
Loss :  1.82430899143219 3.216034412384033 3.216034412384033
  batch 20 loss: 1.82430899143219, 3.216034412384033, 3.216034412384033
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8304338455200195 2.873210906982422 2.873210906982422
Loss :  1.8255188465118408 3.1063008308410645 3.1063008308410645
Loss :  1.8218061923980713 3.0011398792266846 3.0011398792266846
Loss :  1.8329590559005737 3.345184803009033 3.345184803009033
Loss :  1.8312395811080933 3.9146673679351807 3.9146673679351807
Loss :  1.8210104703903198 2.8537256717681885 2.8537256717681885
Loss :  1.825238585472107 3.119715929031372 3.119715929031372
Loss :  1.8135480880737305 3.111950159072876 3.111950159072876
Loss :  1.8220113515853882 3.355189561843872 3.355189561843872
Loss :  1.812726378440857 3.1795289516448975 3.1795289516448975
Loss :  1.815985083580017 3.1901636123657227 3.1901636123657227
Loss :  1.8078951835632324 3.2494986057281494 3.2494986057281494
Loss :  1.8153871297836304 3.1170601844787598 3.1170601844787598
Loss :  1.8147519826889038 3.239185094833374 3.239185094833374
Loss :  1.8153738975524902 3.3975796699523926 3.3975796699523926
Loss :  1.815035104751587 3.3909223079681396 3.3909223079681396
Loss :  1.813270092010498 3.3522303104400635 3.3522303104400635
Loss :  1.8060060739517212 3.395310401916504 3.395310401916504
Loss :  1.817231297492981 3.0497453212738037 3.0497453212738037
Loss :  1.8068543672561646 3.049161434173584 3.049161434173584
  batch 40 loss: 1.8068543672561646, 3.049161434173584, 3.049161434173584
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8147640228271484 3.1359236240386963 3.1359236240386963
Loss :  1.816723346710205 2.8178703784942627 2.8178703784942627
Loss :  1.8150088787078857 2.8840742111206055 2.8840742111206055
Loss :  1.8095630407333374 3.0174853801727295 3.0174853801727295
Loss :  1.819593071937561 2.8552980422973633 2.8552980422973633
Loss :  1.8148140907287598 3.174593687057495 3.174593687057495
Loss :  1.8142038583755493 2.971118927001953 2.971118927001953
Loss :  1.8194035291671753 3.071930170059204 3.071930170059204
Loss :  1.8074272871017456 3.120962381362915 3.120962381362915
Loss :  1.8221668004989624 3.094792604446411 3.094792604446411
Loss :  1.8177019357681274 3.178412675857544 3.178412675857544
Loss :  1.8322092294692993 3.3554301261901855 3.3554301261901855
Loss :  1.8207831382751465 3.200820207595825 3.200820207595825
Loss :  1.8299638032913208 3.4681999683380127 3.4681999683380127
Loss :  1.820749282836914 3.260136365890503 3.260136365890503
Loss :  1.8313566446304321 2.859037160873413 2.859037160873413
Loss :  1.824862003326416 3.2006924152374268 3.2006924152374268
Loss :  1.822694182395935 3.2344319820404053 3.2344319820404053
Loss :  1.835094690322876 3.342310667037964 3.342310667037964
Loss :  1.8264061212539673 3.433011531829834 3.433011531829834
  batch 60 loss: 1.8264061212539673, 3.433011531829834, 3.433011531829834
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8238552808761597 3.0530097484588623 3.0530097484588623
Loss :  1.8181272745132446 3.017892360687256 3.017892360687256
Loss :  1.8199602365493774 3.0611050128936768 3.0611050128936768
Loss :  1.819251537322998 3.105600595474243 3.105600595474243
Loss :  1.8195582628250122 2.8664541244506836 2.8664541244506836
Loss :  1.7177671194076538 4.157182216644287 4.157182216644287
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.7180761098861694 4.050748348236084 4.050748348236084
Loss :  1.7131770849227905 4.129934787750244 4.129934787750244
Loss :  1.7413525581359863 4.086434841156006 4.086434841156006
Total LOSS train 3.126571585581853 valid 4.106075048446655
CE LOSS train 1.8185501043613141 valid 0.4353381395339966
Contrastive LOSS train 3.126571585581853 valid 1.0216087102890015
EPOCH 179:
Loss :  1.825453519821167 3.2629995346069336 3.2629995346069336
Loss :  1.8177669048309326 3.4080381393432617 3.4080381393432617
Loss :  1.8213499784469604 3.082275629043579 3.082275629043579
Loss :  1.820980191230774 3.453223705291748 3.453223705291748
Loss :  1.8206110000610352 2.984869956970215 2.984869956970215
Loss :  1.826326847076416 3.546325445175171 3.546325445175171
Loss :  1.8166207075119019 3.4347784519195557 3.4347784519195557
Loss :  1.8156743049621582 3.083219051361084 3.083219051361084
Loss :  1.8180245161056519 3.261965751647949 3.261965751647949
Loss :  1.8189365863800049 2.9444546699523926 2.9444546699523926
Loss :  1.8242056369781494 3.3778421878814697 3.3778421878814697
Loss :  1.818354845046997 3.550549030303955 3.550549030303955
Loss :  1.820963740348816 3.2337286472320557 3.2337286472320557
Loss :  1.8239339590072632 3.363335132598877 3.363335132598877
Loss :  1.8152564764022827 3.2764947414398193 3.2764947414398193
Loss :  1.8297137022018433 3.3228721618652344 3.3228721618652344
Loss :  1.8188095092773438 3.1311349868774414 3.1311349868774414
Loss :  1.8183140754699707 2.9650065898895264 2.9650065898895264
Loss :  1.816377878189087 3.1429522037506104 3.1429522037506104
Loss :  1.8123639822006226 3.094719648361206 3.094719648361206
  batch 20 loss: 1.8123639822006226, 3.094719648361206, 3.094719648361206
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8231024742126465 3.193923234939575 3.193923234939575
Loss :  1.8192559480667114 3.452484607696533 3.452484607696533
Loss :  1.8161258697509766 3.0692028999328613 3.0692028999328613
Loss :  1.8270183801651 3.1876678466796875 3.1876678466796875
Loss :  1.8233370780944824 3.4324514865875244 3.4324514865875244
Loss :  1.816467523574829 3.031484842300415 3.031484842300415
Loss :  1.822290062904358 3.2332494258880615 3.2332494258880615
Loss :  1.812021017074585 3.1509056091308594 3.1509056091308594
Loss :  1.8184765577316284 3.231572151184082 3.231572151184082
Loss :  1.8080949783325195 3.655600070953369 3.655600070953369
Loss :  1.8124568462371826 3.4232664108276367 3.4232664108276367
Loss :  1.8060756921768188 3.4818060398101807 3.4818060398101807
Loss :  1.8158725500106812 3.4291656017303467 3.4291656017303467
Loss :  1.8130741119384766 3.6629443168640137 3.6629443168640137
Loss :  1.8162295818328857 3.0306684970855713 3.0306684970855713
Loss :  1.8158670663833618 3.212306261062622 3.212306261062622
Loss :  1.818503975868225 3.474666118621826 3.474666118621826
Loss :  1.814170002937317 3.1331562995910645 3.1331562995910645
Loss :  1.8250812292099 3.0474166870117188 3.0474166870117188
Loss :  1.8159399032592773 3.471435308456421 3.471435308456421
  batch 40 loss: 1.8159399032592773, 3.471435308456421, 3.471435308456421
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.823244571685791 3.3856403827667236 3.3856403827667236
Loss :  1.8247838020324707 3.454162120819092 3.454162120819092
Loss :  1.8232483863830566 3.255054473876953 3.255054473876953
Loss :  1.8143160343170166 3.3131911754608154 3.3131911754608154
Loss :  1.8266496658325195 3.065735340118408 3.065735340118408
Loss :  1.8216723203659058 2.91141939163208 2.91141939163208
Loss :  1.8208630084991455 2.9477670192718506 2.9477670192718506
Loss :  1.8244880437850952 3.437330961227417 3.437330961227417
Loss :  1.8152656555175781 3.2445385456085205 3.2445385456085205
Loss :  1.8234604597091675 3.203742742538452 3.203742742538452
Loss :  1.8203767538070679 3.374446153640747 3.374446153640747
Loss :  1.8371891975402832 3.3640952110290527 3.3640952110290527
Loss :  1.825365662574768 3.22574782371521 3.22574782371521
Loss :  1.8317606449127197 3.4232230186462402 3.4232230186462402
Loss :  1.8222746849060059 3.5741050243377686 3.5741050243377686
Loss :  1.8366212844848633 3.4211955070495605 3.4211955070495605
Loss :  1.8273694515228271 3.3044395446777344 3.3044395446777344
Loss :  1.825321912765503 3.635296583175659 3.635296583175659
Loss :  1.8368898630142212 3.430037498474121 3.430037498474121
Loss :  1.8308082818984985 3.7314224243164062 3.7314224243164062
  batch 60 loss: 1.8308082818984985, 3.7314224243164062, 3.7314224243164062
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8236104249954224 3.418924570083618 3.418924570083618
Loss :  1.8193747997283936 3.6702675819396973 3.6702675819396973
Loss :  1.820520043373108 3.5008270740509033 3.5008270740509033
Loss :  1.816452145576477 3.404670238494873 3.404670238494873
Loss :  1.8158577680587769 3.1776602268218994 3.1776602268218994
Loss :  1.725282073020935 4.246114730834961 4.246114730834961
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.7281427383422852 4.225584506988525 4.225584506988525
Loss :  1.7205175161361694 4.240263938903809 4.240263938903809
Loss :  1.7494487762451172 4.054266452789307 4.054266452789307
Total LOSS train 3.304632307932927 valid 4.19155740737915
CE LOSS train 1.8207274473630466 valid 0.4373621940612793
Contrastive LOSS train 3.304632307932927 valid 1.0135666131973267
EPOCH 180:
Loss :  1.8240149021148682 3.004903554916382 3.004903554916382
Loss :  1.8111076354980469 3.1957716941833496 3.1957716941833496
Loss :  1.8157403469085693 3.1538166999816895 3.1538166999816895
Loss :  1.818095088005066 3.355947256088257 3.355947256088257
Loss :  1.8146417140960693 2.9907498359680176 2.9907498359680176
Loss :  1.8211231231689453 3.623657703399658 3.623657703399658
Loss :  1.810579538345337 3.3502609729766846 3.3502609729766846
Loss :  1.8119877576828003 3.0272939205169678 3.0272939205169678
Loss :  1.810989260673523 3.2217917442321777 3.2217917442321777
Loss :  1.811303734779358 3.147881031036377 3.147881031036377
Loss :  1.8231910467147827 3.269257068634033 3.269257068634033
Loss :  1.8158856630325317 3.3981568813323975 3.3981568813323975
Loss :  1.821509838104248 3.1315765380859375 3.1315765380859375
Loss :  1.8262584209442139 3.4195539951324463 3.4195539951324463
Loss :  1.8192671537399292 3.2029974460601807 3.2029974460601807
Loss :  1.8348015546798706 2.885716676712036 2.885716676712036
Loss :  1.8231148719787598 3.1369285583496094 3.1369285583496094
Loss :  1.8264199495315552 3.1128087043762207 3.1128087043762207
Loss :  1.8183845281600952 2.856147050857544 2.856147050857544
Loss :  1.8196548223495483 2.9750008583068848 2.9750008583068848
  batch 20 loss: 1.8196548223495483, 2.9750008583068848, 2.9750008583068848
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8307877779006958 3.141263484954834 3.141263484954834
Loss :  1.8261768817901611 2.99554181098938 2.99554181098938
Loss :  1.82725989818573 2.732130527496338 2.732130527496338
Loss :  1.8352103233337402 2.7871694564819336 2.7871694564819336
Loss :  1.8339343070983887 3.02475643157959 3.02475643157959
Loss :  1.8290081024169922 3.2642757892608643 3.2642757892608643
Loss :  1.8308340311050415 3.012272596359253 3.012272596359253
Loss :  1.8203213214874268 3.387141466140747 3.387141466140747
Loss :  1.822614073753357 3.4637668132781982 3.4637668132781982
Loss :  1.8157826662063599 3.5056166648864746 3.5056166648864746
Loss :  1.817075252532959 3.3824384212493896 3.3824384212493896
Loss :  1.814130425453186 3.8027069568634033 3.8027069568634033
Loss :  1.8240787982940674 3.5648820400238037 3.5648820400238037
Loss :  1.8201459646224976 3.558809518814087 3.558809518814087
Loss :  1.820136547088623 3.4282875061035156 3.4282875061035156
Loss :  1.8176617622375488 3.533400058746338 3.533400058746338
Loss :  1.819374918937683 3.6212668418884277 3.6212668418884277
Loss :  1.8149515390396118 3.552908182144165 3.552908182144165
Loss :  1.8242530822753906 3.464020013809204 3.464020013809204
Loss :  1.8150361776351929 3.3472483158111572 3.3472483158111572
  batch 40 loss: 1.8150361776351929, 3.3472483158111572, 3.3472483158111572
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.821415901184082 3.4823384284973145 3.4823384284973145
Loss :  1.8208128213882446 3.5665502548217773 3.5665502548217773
Loss :  1.8199902772903442 3.0330872535705566 3.0330872535705566
Loss :  1.8118255138397217 3.1469411849975586 3.1469411849975586
Loss :  1.8232219219207764 3.0670931339263916 3.0670931339263916
Loss :  1.8198895454406738 3.214405059814453 3.214405059814453
Loss :  1.8186185359954834 3.2761662006378174 3.2761662006378174
Loss :  1.8194931745529175 3.0408220291137695 3.0408220291137695
Loss :  1.8122063875198364 3.497843027114868 3.497843027114868
Loss :  1.8194522857666016 3.3382251262664795 3.3382251262664795
Loss :  1.8143792152404785 3.380991220474243 3.380991220474243
Loss :  1.8302180767059326 3.1934657096862793 3.1934657096862793
Loss :  1.8195043802261353 3.197272300720215 3.197272300720215
Loss :  1.8245316743850708 3.1653895378112793 3.1653895378112793
Loss :  1.8161324262619019 3.3011257648468018 3.3011257648468018
Loss :  1.8312170505523682 3.0450873374938965 3.0450873374938965
Loss :  1.82442045211792 3.5663247108459473 3.5663247108459473
Loss :  1.8221721649169922 3.021904468536377 3.021904468536377
Loss :  1.8350584506988525 3.4056854248046875 3.4056854248046875
Loss :  1.83004891872406 3.449026584625244 3.449026584625244
  batch 60 loss: 1.83004891872406, 3.449026584625244, 3.449026584625244
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8221230506896973 3.7620673179626465 3.7620673179626465
Loss :  1.818603515625 3.0822949409484863 3.0822949409484863
Loss :  1.8206052780151367 3.044699192047119 3.044699192047119
Loss :  1.8174482583999634 3.4413833618164062 3.4413833618164062
Loss :  1.8170552253723145 3.134462356567383 3.134462356567383
Loss :  1.732153058052063 4.295297622680664 4.295297622680664
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.732533574104309 4.216708660125732 4.216708660125732
Loss :  1.728222131729126 4.188521862030029 4.188521862030029
Loss :  1.7532585859298706 4.070474147796631 4.070474147796631
Total LOSS train 3.2597657387073222 valid 4.192750573158264
CE LOSS train 1.8210352200728197 valid 0.43831464648246765
Contrastive LOSS train 3.2597657387073222 valid 1.0176185369491577
EPOCH 181:
Loss :  1.8246268033981323 3.267688751220703 3.267688751220703
Loss :  1.813829779624939 3.426077365875244 3.426077365875244
Loss :  1.8196481466293335 3.1540210247039795 3.1540210247039795
Loss :  1.821943759918213 3.674823760986328 3.674823760986328
Loss :  1.8189555406570435 3.29211688041687 3.29211688041687
Loss :  1.8238506317138672 3.106715202331543 3.106715202331543
Loss :  1.8154160976409912 3.051781177520752 3.051781177520752
Loss :  1.8160607814788818 3.080518960952759 3.080518960952759
Loss :  1.8160051107406616 3.209160566329956 3.209160566329956
Loss :  1.8172717094421387 3.22377347946167 3.22377347946167
Loss :  1.8253377676010132 3.1624197959899902 3.1624197959899902
Loss :  1.816522479057312 3.213982582092285 3.213982582092285
Loss :  1.822664737701416 2.9974827766418457 2.9974827766418457
Loss :  1.826655626296997 2.934570074081421 2.934570074081421
Loss :  1.8200117349624634 3.200087785720825 3.200087785720825
Loss :  1.833274245262146 3.353154182434082 3.353154182434082
Loss :  1.8233603239059448 3.1736862659454346 3.1736862659454346
Loss :  1.8230867385864258 3.278508424758911 3.278508424758911
Loss :  1.8211157321929932 3.629194498062134 3.629194498062134
Loss :  1.8193942308425903 3.5250940322875977 3.5250940322875977
  batch 20 loss: 1.8193942308425903, 3.5250940322875977, 3.5250940322875977
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8300048112869263 3.3630459308624268 3.3630459308624268
Loss :  1.8247416019439697 2.990338087081909 2.990338087081909
Loss :  1.822795033454895 2.9975507259368896 2.9975507259368896
Loss :  1.8318699598312378 2.895568609237671 2.895568609237671
Loss :  1.8322488069534302 3.3414013385772705 3.3414013385772705
Loss :  1.8254910707473755 2.886744260787964 2.886744260787964
Loss :  1.8286577463150024 3.2544033527374268 3.2544033527374268
Loss :  1.8214561939239502 3.0448336601257324 3.0448336601257324
Loss :  1.8250049352645874 3.410445213317871 3.410445213317871
Loss :  1.8203431367874146 3.3274447917938232 3.3274447917938232
Loss :  1.8192954063415527 3.471663475036621 3.471663475036621
Loss :  1.8181262016296387 3.388756036758423 3.388756036758423
Loss :  1.8259738683700562 3.1719162464141846 3.1719162464141846
Loss :  1.8240492343902588 3.0890438556671143 3.0890438556671143
Loss :  1.8251765966415405 3.1397886276245117 3.1397886276245117
Loss :  1.8220456838607788 3.386479377746582 3.386479377746582
Loss :  1.8243452310562134 3.4184041023254395 3.4184041023254395
Loss :  1.8218417167663574 2.9583709239959717 2.9583709239959717
Loss :  1.8291904926300049 2.8336360454559326 2.8336360454559326
Loss :  1.819807767868042 2.8970727920532227 2.8970727920532227
  batch 40 loss: 1.819807767868042, 2.8970727920532227, 2.8970727920532227
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.825001835823059 2.9635775089263916 2.9635775089263916
Loss :  1.8237162828445435 3.084158182144165 3.084158182144165
Loss :  1.8230602741241455 3.0775415897369385 3.0775415897369385
Loss :  1.8122451305389404 2.8256680965423584 2.8256680965423584
Loss :  1.8257023096084595 3.0152876377105713 3.0152876377105713
Loss :  1.8214694261550903 3.0197439193725586 3.0197439193725586
Loss :  1.8205525875091553 3.0646121501922607 3.0646121501922607
Loss :  1.8230884075164795 3.152080535888672 3.152080535888672
Loss :  1.813305139541626 3.2959299087524414 3.2959299087524414
Loss :  1.8201510906219482 3.22023868560791 3.22023868560791
Loss :  1.8145619630813599 3.5533387660980225 3.5533387660980225
Loss :  1.829365849494934 3.614893913269043 3.614893913269043
Loss :  1.8193724155426025 3.2625961303710938 3.2625961303710938
Loss :  1.8237239122390747 2.9838449954986572 2.9838449954986572
Loss :  1.8178213834762573 3.1798996925354004 3.1798996925354004
Loss :  1.8309448957443237 2.6177916526794434 2.6177916526794434
Loss :  1.822818398475647 3.270537853240967 3.270537853240967
Loss :  1.8221993446350098 3.2026190757751465 3.2026190757751465
Loss :  1.8325406312942505 3.301515817642212 3.301515817642212
Loss :  1.8280515670776367 3.240081548690796 3.240081548690796
  batch 60 loss: 1.8280515670776367, 3.240081548690796, 3.240081548690796
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8237085342407227 3.548529624938965 3.548529624938965
Loss :  1.8202086687088013 3.8498950004577637 3.8498950004577637
Loss :  1.8210810422897339 3.6725807189941406 3.6725807189941406
Loss :  1.81995689868927 3.683793544769287 3.683793544769287
Loss :  1.8206921815872192 3.4716479778289795 3.4716479778289795
Loss :  1.711693286895752 4.289151191711426 4.289151191711426
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.7158597707748413 4.360981464385986 4.360981464385986
Loss :  1.7090420722961426 4.110123157501221 4.110123157501221
Loss :  1.736608862876892 4.1114068031311035 4.1114068031311035
Total LOSS train 3.2214487626002386 valid 4.217915654182434
CE LOSS train 1.8225667329934927 valid 0.434152215719223
Contrastive LOSS train 3.2214487626002386 valid 1.0278517007827759
EPOCH 182:
Loss :  1.8277227878570557 3.2344772815704346 3.2344772815704346
Loss :  1.8167555332183838 3.3953044414520264 3.3953044414520264
Loss :  1.82318913936615 3.1602816581726074 3.1602816581726074
Loss :  1.8242982625961304 3.4642906188964844 3.4642906188964844
Loss :  1.8214532136917114 2.808072328567505 2.808072328567505
Loss :  1.826899528503418 3.4697866439819336 3.4697866439819336
Loss :  1.815606713294983 3.7854762077331543 3.7854762077331543
Loss :  1.8145101070404053 3.389822244644165 3.389822244644165
Loss :  1.817428708076477 3.3918685913085938 3.3918685913085938
Loss :  1.8143539428710938 3.1276097297668457 3.1276097297668457
Loss :  1.8210561275482178 3.2632827758789062 3.2632827758789062
Loss :  1.8156344890594482 3.4016199111938477 3.4016199111938477
Loss :  1.8179463148117065 3.4090330600738525 3.4090330600738525
Loss :  1.8215692043304443 3.2269399166107178 3.2269399166107178
Loss :  1.8153735399246216 3.4045398235321045 3.4045398235321045
Loss :  1.8271080255508423 3.396108865737915 3.396108865737915
Loss :  1.8190419673919678 3.1565258502960205 3.1565258502960205
Loss :  1.8201369047164917 3.241046667098999 3.241046667098999
Loss :  1.818143606185913 2.784548282623291 2.784548282623291
Loss :  1.8169797658920288 3.1302554607391357 3.1302554607391357
  batch 20 loss: 1.8169797658920288, 3.1302554607391357, 3.1302554607391357
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.827837347984314 3.0059406757354736 3.0059406757354736
Loss :  1.8231871128082275 2.829801082611084 2.829801082611084
Loss :  1.8217597007751465 2.9048867225646973 2.9048867225646973
Loss :  1.8300586938858032 3.019693613052368 3.019693613052368
Loss :  1.829866886138916 3.4543893337249756 3.4543893337249756
Loss :  1.8245989084243774 3.234421730041504 3.234421730041504
Loss :  1.827742099761963 3.6287038326263428 3.6287038326263428
Loss :  1.8199944496154785 3.275374412536621 3.275374412536621
Loss :  1.8243378400802612 3.385511875152588 3.385511875152588
Loss :  1.8178092241287231 3.4975576400756836 3.4975576400756836
Loss :  1.8175623416900635 3.375580310821533 3.375580310821533
Loss :  1.8169331550598145 3.629099130630493 3.629099130630493
Loss :  1.8237930536270142 3.6630966663360596 3.6630966663360596
Loss :  1.8228795528411865 3.10146164894104 3.10146164894104
Loss :  1.822450876235962 3.027316093444824 3.027316093444824
Loss :  1.8202012777328491 3.199861764907837 3.199861764907837
Loss :  1.821183443069458 2.9218735694885254 2.9218735694885254
Loss :  1.8195078372955322 3.0003647804260254 3.0003647804260254
Loss :  1.8252434730529785 3.3751511573791504 3.3751511573791504
Loss :  1.816721796989441 3.3993680477142334 3.3993680477142334
  batch 40 loss: 1.816721796989441, 3.3993680477142334, 3.3993680477142334
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.821530818939209 3.744182586669922 3.744182586669922
Loss :  1.8183417320251465 3.4528961181640625 3.4528961181640625
Loss :  1.8178621530532837 3.532827138900757 3.532827138900757
Loss :  1.8077491521835327 3.2149221897125244 3.2149221897125244
Loss :  1.8187772035598755 2.9756898880004883 2.9756898880004883
Loss :  1.8140900135040283 2.9790029525756836 2.9790029525756836
Loss :  1.8144134283065796 2.519260883331299 2.519260883331299
Loss :  1.8144952058792114 2.7331953048706055 2.7331953048706055
Loss :  1.8097037076950073 3.301975965499878 3.301975965499878
Loss :  1.8153518438339233 3.315595865249634 3.315595865249634
Loss :  1.811071515083313 3.1683847904205322 3.1683847904205322
Loss :  1.8278087377548218 2.940606117248535 2.940606117248535
Loss :  1.8189235925674438 2.630448818206787 2.630448818206787
Loss :  1.8225518465042114 2.8746821880340576 2.8746821880340576
Loss :  1.8165174722671509 3.067359447479248 3.067359447479248
Loss :  1.8308309316635132 2.8807055950164795 2.8807055950164795
Loss :  1.8226979970932007 3.3626341819763184 3.3626341819763184
Loss :  1.8229299783706665 3.5698161125183105 3.5698161125183105
Loss :  1.832727313041687 3.613103151321411 3.613103151321411
Loss :  1.8282146453857422 3.5914347171783447 3.5914347171783447
  batch 60 loss: 1.8282146453857422, 3.5914347171783447, 3.5914347171783447
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8221521377563477 3.4423131942749023 3.4423131942749023
Loss :  1.8184151649475098 3.025029420852661 3.025029420852661
Loss :  1.8198025226593018 3.3600728511810303 3.3600728511810303
Loss :  1.8184272050857544 3.4803085327148438 3.4803085327148438
Loss :  1.8192555904388428 3.021489143371582 3.021489143371582
Loss :  1.7312629222869873 4.3034844398498535 4.3034844398498535
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.7322906255722046 4.238685607910156 4.238685607910156
Loss :  1.727882981300354 4.2261576652526855 4.2261576652526855
Loss :  1.7540695667266846 4.186426162719727 4.186426162719727
Total LOSS train 3.2364351015824537 valid 4.2386884689331055
CE LOSS train 1.820515674811143 valid 0.43851739168167114
Contrastive LOSS train 3.2364351015824537 valid 1.0466065406799316
EPOCH 183:
Loss :  1.8267667293548584 3.221578359603882 3.221578359603882
Loss :  1.8154881000518799 3.618536949157715 3.618536949157715
Loss :  1.8215144872665405 3.17981219291687 3.17981219291687
Loss :  1.8214614391326904 3.3955163955688477 3.3955163955688477
Loss :  1.8177335262298584 3.1286375522613525 3.1286375522613525
Loss :  1.8216373920440674 3.3475847244262695 3.3475847244262695
Loss :  1.8118253946304321 3.0878140926361084 3.0878140926361084
Loss :  1.811188817024231 3.5150632858276367 3.5150632858276367
Loss :  1.8113038539886475 3.3088021278381348 3.3088021278381348
Loss :  1.8090956211090088 3.144012212753296 3.144012212753296
Loss :  1.8186635971069336 3.4147887229919434 3.4147887229919434
Loss :  1.8139573335647583 3.4058713912963867 3.4058713912963867
Loss :  1.8169556856155396 3.3129348754882812 3.3129348754882812
Loss :  1.8207478523254395 3.1759417057037354 3.1759417057037354
Loss :  1.8153470754623413 3.1924257278442383 3.1924257278442383
Loss :  1.8273158073425293 3.2776083946228027 3.2776083946228027
Loss :  1.8186472654342651 3.3349900245666504 3.3349900245666504
Loss :  1.8203558921813965 2.8020594120025635 2.8020594120025635
Loss :  1.8175073862075806 3.1602540016174316 3.1602540016174316
Loss :  1.8173058032989502 3.0384597778320312 3.0384597778320312
  batch 20 loss: 1.8173058032989502, 3.0384597778320312, 3.0384597778320312
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.826749563217163 2.9550468921661377 2.9550468921661377
Loss :  1.8222018480300903 3.3316988945007324 3.3316988945007324
Loss :  1.8202263116836548 3.5419435501098633 3.5419435501098633
Loss :  1.8288367986679077 2.972271203994751 2.972271203994751
Loss :  1.828174352645874 3.5002503395080566 3.5002503395080566
Loss :  1.8215287923812866 3.3555614948272705 3.3555614948272705
Loss :  1.8256776332855225 3.1999971866607666 3.1999971866607666
Loss :  1.8182640075683594 3.3614633083343506 3.3614633083343506
Loss :  1.8225260972976685 3.453336000442505 3.453336000442505
Loss :  1.8158074617385864 3.2897253036499023 3.2897253036499023
Loss :  1.816015362739563 3.4273383617401123 3.4273383617401123
Loss :  1.8152378797531128 3.287734270095825 3.287734270095825
Loss :  1.822870135307312 3.28945255279541 3.28945255279541
Loss :  1.821410894393921 3.2381372451782227 3.2381372451782227
Loss :  1.8233251571655273 3.0586905479431152 3.0586905479431152
Loss :  1.821021556854248 3.122650384902954 3.122650384902954
Loss :  1.8250007629394531 3.059004068374634 3.059004068374634
Loss :  1.8242326974868774 3.017620801925659 3.017620801925659
Loss :  1.8318146467208862 3.7563066482543945 3.7563066482543945
Loss :  1.8240869045257568 3.1890816688537598 3.1890816688537598
  batch 40 loss: 1.8240869045257568, 3.1890816688537598, 3.1890816688537598
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8277671337127686 3.583972454071045 3.583972454071045
Loss :  1.8253921270370483 3.682403802871704 3.682403802871704
Loss :  1.8259878158569336 3.6064069271087646 3.6064069271087646
Loss :  1.8141868114471436 3.3415651321411133 3.3415651321411133
Loss :  1.8275119066238403 3.7019104957580566 3.7019104957580566
Loss :  1.82465398311615 3.3833274841308594 3.3833274841308594
Loss :  1.8241279125213623 3.4504237174987793 3.4504237174987793
Loss :  1.8248872756958008 3.1893935203552246 3.1893935203552246
Loss :  1.8188824653625488 2.9829230308532715 2.9829230308532715
Loss :  1.821742296218872 2.997249126434326 2.997249126434326
Loss :  1.8185005187988281 3.1621017456054688 3.1621017456054688
Loss :  1.8331577777862549 3.4235305786132812 3.4235305786132812
Loss :  1.8228429555892944 3.0588765144348145 3.0588765144348145
Loss :  1.8250926733016968 3.3266050815582275 3.3266050815582275
Loss :  1.8157296180725098 3.2888975143432617 3.2888975143432617
Loss :  1.829732060432434 3.209381103515625 3.209381103515625
Loss :  1.8215773105621338 3.673236608505249 3.673236608505249
Loss :  1.8208495378494263 3.2662320137023926 3.2662320137023926
Loss :  1.8315104246139526 3.403949499130249 3.403949499130249
Loss :  1.8271085023880005 3.0891919136047363 3.0891919136047363
  batch 60 loss: 1.8271085023880005, 3.0891919136047363, 3.0891919136047363
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8223626613616943 2.727524518966675 2.727524518966675
Loss :  1.8191423416137695 2.6324691772460938 2.6324691772460938
Loss :  1.8205080032348633 2.687717914581299 2.687717914581299
Loss :  1.819076418876648 2.886765241622925 2.886765241622925
Loss :  1.8202447891235352 2.6515066623687744 2.6515066623687744
Loss :  1.7406202554702759 4.364326000213623 4.364326000213623
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.7412301301956177 4.235030651092529 4.235030651092529
Loss :  1.7386713027954102 4.1542277336120605 4.1542277336120605
Loss :  1.7619709968566895 3.998326539993286 3.998326539993286
Total LOSS train 3.2442702220036432 valid 4.187977731227875
CE LOSS train 1.8214215883841882 valid 0.44049274921417236
Contrastive LOSS train 3.2442702220036432 valid 0.9995816349983215
EPOCH 184:
Loss :  1.8301814794540405 2.6334993839263916 2.6334993839263916
Loss :  1.8174519538879395 3.2502143383026123 3.2502143383026123
Loss :  1.8261027336120605 3.378427743911743 3.378427743911743
Loss :  1.828305721282959 3.3119306564331055 3.3119306564331055
Loss :  1.823990821838379 2.889188051223755 2.889188051223755
Loss :  1.8292497396469116 3.031064510345459 3.031064510345459
Loss :  1.8188087940216064 2.667661190032959 2.667661190032959
Loss :  1.8173725605010986 3.4192452430725098 3.4192452430725098
Loss :  1.8208415508270264 3.6425583362579346 3.6425583362579346
Loss :  1.8210785388946533 3.429673194885254 3.429673194885254
Loss :  1.824833869934082 3.230189085006714 3.230189085006714
Loss :  1.8179932832717896 3.589657783508301 3.589657783508301
Loss :  1.8224142789840698 3.0539638996124268 3.0539638996124268
Loss :  1.8258169889450073 2.876281261444092 2.876281261444092
Loss :  1.820014476776123 3.03145432472229 3.03145432472229
Loss :  1.829532265663147 3.2796032428741455 3.2796032428741455
Loss :  1.821250557899475 3.33178973197937 3.33178973197937
Loss :  1.821366548538208 2.863166332244873 2.863166332244873
Loss :  1.820332407951355 2.8824639320373535 2.8824639320373535
Loss :  1.8169881105422974 3.1447548866271973 3.1447548866271973
  batch 20 loss: 1.8169881105422974, 3.1447548866271973, 3.1447548866271973
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8265330791473389 3.187288522720337 3.187288522720337
Loss :  1.8226968050003052 3.493696451187134 3.493696451187134
Loss :  1.8195918798446655 3.1630873680114746 3.1630873680114746
Loss :  1.8289070129394531 3.242520332336426 3.242520332336426
Loss :  1.829541563987732 3.474663019180298 3.474663019180298
Loss :  1.822176218032837 3.067898988723755 3.067898988723755
Loss :  1.8259849548339844 3.3839757442474365 3.3839757442474365
Loss :  1.818991780281067 3.3058385848999023 3.3058385848999023
Loss :  1.8226503133773804 3.3151934146881104 3.3151934146881104
Loss :  1.8173633813858032 3.145495891571045 3.145495891571045
Loss :  1.8154964447021484 3.257174015045166 3.257174015045166
Loss :  1.816341757774353 2.949106454849243 2.949106454849243
Loss :  1.8216763734817505 2.934929370880127 2.934929370880127
Loss :  1.8213789463043213 2.9289679527282715 2.9289679527282715
Loss :  1.8215423822402954 2.7520623207092285 2.7520623207092285
Loss :  1.819117546081543 2.823605537414551 2.823605537414551
Loss :  1.8212249279022217 2.947493076324463 2.947493076324463
Loss :  1.821036696434021 3.041987419128418 3.041987419128418
Loss :  1.82651686668396 2.756530284881592 2.756530284881592
Loss :  1.818110466003418 3.401726245880127 3.401726245880127
  batch 40 loss: 1.818110466003418, 3.401726245880127, 3.401726245880127
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8225677013397217 3.590710163116455 3.590710163116455
Loss :  1.8195725679397583 3.220811605453491 3.220811605453491
Loss :  1.8199260234832764 3.4477615356445312 3.4477615356445312
Loss :  1.8101657629013062 3.2521345615386963 3.2521345615386963
Loss :  1.8219066858291626 2.727274179458618 2.727274179458618
Loss :  1.8181273937225342 2.772320508956909 2.772320508956909
Loss :  1.8187233209609985 3.2920870780944824 3.2920870780944824
Loss :  1.8205100297927856 3.1814141273498535 3.1814141273498535
Loss :  1.8141494989395142 3.191150188446045 3.191150188446045
Loss :  1.820963978767395 3.7177658081054688 3.7177658081054688
Loss :  1.8176445960998535 2.9696686267852783 2.9696686267852783
Loss :  1.8333959579467773 2.843273639678955 2.843273639678955
Loss :  1.8223845958709717 2.963008165359497 2.963008165359497
Loss :  1.827876091003418 3.3148438930511475 3.3148438930511475
Loss :  1.8210163116455078 3.093921184539795 3.093921184539795
Loss :  1.8362908363342285 2.742933750152588 2.742933750152588
Loss :  1.8257359266281128 3.3555352687835693 3.3555352687835693
Loss :  1.8264964818954468 3.060933828353882 3.060933828353882
Loss :  1.8351075649261475 3.903301954269409 3.903301954269409
Loss :  1.8318008184432983 3.740023374557495 3.740023374557495
  batch 60 loss: 1.8318008184432983, 3.740023374557495, 3.740023374557495
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8255469799041748 3.7481656074523926 3.7481656074523926
Loss :  1.8216842412948608 3.329906463623047 3.329906463623047
Loss :  1.8213136196136475 3.0878286361694336 3.0878286361694336
Loss :  1.8187495470046997 3.153003215789795 3.153003215789795
Loss :  1.819869041442871 3.2074193954467773 3.2074193954467773
Loss :  1.7998974323272705 3.704436779022217 3.704436779022217
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7937390804290771 3.640101194381714 3.640101194381714
Loss :  1.798029899597168 3.5669748783111572 3.5669748783111572
Loss :  1.809767246246338 3.472121238708496 3.472121238708496
Total LOSS train 3.175649613600511 valid 3.595908522605896
CE LOSS train 1.8224974100406353 valid 0.4524418115615845
Contrastive LOSS train 3.175649613600511 valid 0.868030309677124
EPOCH 185:
Loss :  1.8279197216033936 3.3761332035064697 3.3761332035064697
Loss :  1.8168021440505981 3.8304812908172607 3.8304812908172607
Loss :  1.8238314390182495 3.788085699081421 3.788085699081421
Loss :  1.8242815732955933 3.8582277297973633 3.8582277297973633
Loss :  1.8200711011886597 3.741065263748169 3.741065263748169
Loss :  1.8243424892425537 3.239241123199463 3.239241123199463
Loss :  1.8151285648345947 3.3041720390319824 3.3041720390319824
Loss :  1.814085602760315 2.869638681411743 2.869638681411743
Loss :  1.8129920959472656 2.961233377456665 2.961233377456665
Loss :  1.813598871231079 2.924548387527466 2.924548387527466
Loss :  1.8209294080734253 3.164454936981201 3.164454936981201
Loss :  1.8152481317520142 3.0548126697540283 3.0548126697540283
Loss :  1.818108081817627 2.7480275630950928 2.7480275630950928
Loss :  1.8217394351959229 3.383358955383301 3.383358955383301
Loss :  1.8161815404891968 3.1621973514556885 3.1621973514556885
Loss :  1.8262114524841309 3.2470762729644775 3.2470762729644775
Loss :  1.8188354969024658 3.493422031402588 3.493422031402588
Loss :  1.822770357131958 3.928574800491333 3.928574800491333
Loss :  1.8195366859436035 3.5111262798309326 3.5111262798309326
Loss :  1.8187001943588257 3.6431307792663574 3.6431307792663574
  batch 20 loss: 1.8187001943588257, 3.6431307792663574, 3.6431307792663574
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8273252248764038 3.9240705966949463 3.9240705966949463
Loss :  1.8224016427993774 3.466174602508545 3.466174602508545
Loss :  1.8193845748901367 3.7482004165649414 3.7482004165649414
Loss :  1.8280102014541626 3.7269082069396973 3.7269082069396973
Loss :  1.8259193897247314 4.006089210510254 4.006089210510254
Loss :  1.8166996240615845 3.19049334526062 3.19049334526062
Loss :  1.8200658559799194 3.73832368850708 3.73832368850708
Loss :  1.8114163875579834 2.935218095779419 2.935218095779419
Loss :  1.8159403800964355 3.2742457389831543 3.2742457389831543
Loss :  1.8095015287399292 3.2417168617248535 3.2417168617248535
Loss :  1.8110724687576294 3.2916347980499268 3.2916347980499268
Loss :  1.8084384202957153 3.324923038482666 3.324923038482666
Loss :  1.8163151741027832 3.1312787532806396 3.1312787532806396
Loss :  1.813551425933838 3.6056530475616455 3.6056530475616455
Loss :  1.8162587881088257 3.3193585872650146 3.3193585872650146
Loss :  1.8138474225997925 2.803346872329712 2.803346872329712
Loss :  1.817110538482666 2.9171507358551025 2.9171507358551025
Loss :  1.8147187232971191 3.638577461242676 3.638577461242676
Loss :  1.8215776681900024 2.9081528186798096 2.9081528186798096
Loss :  1.8134758472442627 3.4567666053771973 3.4567666053771973
  batch 40 loss: 1.8134758472442627, 3.4567666053771973, 3.4567666053771973
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8186261653900146 3.8264706134796143 3.8264706134796143
Loss :  1.815431833267212 3.2539687156677246 3.2539687156677246
Loss :  1.815678596496582 3.2063701152801514 3.2063701152801514
Loss :  1.8071749210357666 2.9504401683807373 2.9504401683807373
Loss :  1.817415475845337 3.1779074668884277 3.1779074668884277
Loss :  1.8142037391662598 2.8067591190338135 2.8067591190338135
Loss :  1.8150818347930908 2.87003755569458 2.87003755569458
Loss :  1.8155626058578491 2.6745686531066895 2.6745686531066895
Loss :  1.8115193843841553 3.026369333267212 3.026369333267212
Loss :  1.8167213201522827 3.1690821647644043 3.1690821647644043
Loss :  1.8132827281951904 2.8893206119537354 2.8893206119537354
Loss :  1.8284976482391357 2.970203161239624 2.970203161239624
Loss :  1.818711280822754 2.694627046585083 2.694627046585083
Loss :  1.822141170501709 3.074272394180298 3.074272394180298
Loss :  1.8163491487503052 3.533759593963623 3.533759593963623
Loss :  1.8320691585540771 3.5946640968322754 3.5946640968322754
Loss :  1.8219960927963257 3.1931350231170654 3.1931350231170654
Loss :  1.8219629526138306 3.0698094367980957 3.0698094367980957
Loss :  1.8311651945114136 3.7182846069335938 3.7182846069335938
Loss :  1.8276889324188232 3.4880452156066895 3.4880452156066895
  batch 60 loss: 1.8276889324188232, 3.4880452156066895, 3.4880452156066895
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8214316368103027 3.934983491897583 3.934983491897583
Loss :  1.8188350200653076 3.246788740158081 3.246788740158081
Loss :  1.8204342126846313 3.1096014976501465 3.1096014976501465
Loss :  1.817557454109192 2.9439358711242676 2.9439358711242676
Loss :  1.818587303161621 2.8784101009368896 2.8784101009368896
Loss :  1.7715636491775513 4.231307029724121 4.231307029724121
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7698360681533813 4.178422451019287 4.178422451019287
Loss :  1.7712419033050537 4.061054229736328 4.061054229736328
Loss :  1.7880786657333374 3.9848315715789795 3.9848315715789795
Total LOSS train 3.2950631801898664 valid 4.113903820514679
CE LOSS train 1.8186534074636607 valid 0.44701966643333435
Contrastive LOSS train 3.2950631801898664 valid 0.9962078928947449
EPOCH 186:
Loss :  1.8271867036819458 2.9857048988342285 2.9857048988342285
Loss :  1.8152269124984741 3.296964168548584 3.296964168548584
Loss :  1.8224389553070068 3.4960644245147705 3.4960644245147705
Loss :  1.8235677480697632 3.398245096206665 3.398245096206665
Loss :  1.8197507858276367 3.387392997741699 3.387392997741699
Loss :  1.8241028785705566 3.7812588214874268 3.7812588214874268
Loss :  1.814741611480713 3.2633087635040283 3.2633087635040283
Loss :  1.813643455505371 3.6255013942718506 3.6255013942718506
Loss :  1.8141624927520752 3.5197107791900635 3.5197107791900635
Loss :  1.8128559589385986 3.1173768043518066 3.1173768043518066
Loss :  1.8203084468841553 3.0740301609039307 3.0740301609039307
Loss :  1.8143577575683594 3.2848756313323975 3.2848756313323975
Loss :  1.818584680557251 3.4810266494750977 3.4810266494750977
Loss :  1.821324348449707 3.1899139881134033 3.1899139881134033
Loss :  1.8158833980560303 3.250500202178955 3.250500202178955
Loss :  1.8250576257705688 2.9190001487731934 2.9190001487731934
Loss :  1.817510962486267 3.206763744354248 3.206763744354248
Loss :  1.8198189735412598 3.1128251552581787 3.1128251552581787
Loss :  1.8159372806549072 3.2653708457946777 3.2653708457946777
Loss :  1.814527153968811 3.5578691959381104 3.5578691959381104
  batch 20 loss: 1.814527153968811, 3.5578691959381104, 3.5578691959381104
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.823422908782959 3.238179922103882 3.238179922103882
Loss :  1.8180800676345825 3.3776981830596924 3.3776981830596924
Loss :  1.8150826692581177 3.4386823177337646 3.4386823177337646
Loss :  1.8248729705810547 3.0302693843841553 3.0302693843841553
Loss :  1.8216978311538696 3.355523109436035 3.355523109436035
Loss :  1.8157620429992676 2.7721381187438965 2.7721381187438965
Loss :  1.8210784196853638 3.2270901203155518 3.2270901203155518
Loss :  1.8137956857681274 3.1139042377471924 3.1139042377471924
Loss :  1.8176827430725098 3.7613437175750732 3.7613437175750732
Loss :  1.8115837574005127 2.693610191345215 2.693610191345215
Loss :  1.8116626739501953 2.6947591304779053 2.6947591304779053
Loss :  1.8122265338897705 3.1437249183654785 3.1437249183654785
Loss :  1.8188252449035645 3.188227653503418 3.188227653503418
Loss :  1.815441608428955 3.2159390449523926 3.2159390449523926
Loss :  1.817945957183838 3.122915744781494 3.122915744781494
Loss :  1.8140475749969482 3.5668699741363525 3.5668699741363525
Loss :  1.8181196451187134 3.742687702178955 3.742687702178955
Loss :  1.8161298036575317 3.4759445190429688 3.4759445190429688
Loss :  1.8209903240203857 3.670078754425049 3.670078754425049
Loss :  1.8126226663589478 3.6477038860321045 3.6477038860321045
  batch 40 loss: 1.8126226663589478, 3.6477038860321045, 3.6477038860321045
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8182899951934814 4.279054164886475 4.279054164886475
Loss :  1.8146262168884277 3.5136709213256836 3.5136709213256836
Loss :  1.8136979341506958 3.0210061073303223 3.0210061073303223
Loss :  1.806893229484558 3.5134549140930176 3.5134549140930176
Loss :  1.8161214590072632 2.997572660446167 2.997572660446167
Loss :  1.8127031326293945 3.065845012664795 3.065845012664795
Loss :  1.8131569623947144 3.594094753265381 3.594094753265381
Loss :  1.8126170635223389 3.0810511112213135 3.0810511112213135
Loss :  1.8076874017715454 3.331455945968628 3.331455945968628
Loss :  1.8141165971755981 3.31760573387146 3.31760573387146
Loss :  1.8095369338989258 2.7898166179656982 2.7898166179656982
Loss :  1.8265774250030518 2.710845947265625 2.710845947265625
Loss :  1.8162624835968018 2.8506991863250732 2.8506991863250732
Loss :  1.8212350606918335 2.855229377746582 2.855229377746582
Loss :  1.8145263195037842 3.021401882171631 3.021401882171631
Loss :  1.8309639692306519 2.9497768878936768 2.9497768878936768
Loss :  1.821117877960205 3.1366236209869385 3.1366236209869385
Loss :  1.8208855390548706 3.0615146160125732 3.0615146160125732
Loss :  1.8312920331954956 3.2349092960357666 3.2349092960357666
Loss :  1.8286802768707275 3.142268180847168 3.142268180847168
  batch 60 loss: 1.8286802768707275, 3.142268180847168, 3.142268180847168
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8206799030303955 3.0623533725738525 3.0623533725738525
Loss :  1.8188401460647583 3.1125473976135254 3.1125473976135254
Loss :  1.8194801807403564 2.857146978378296 2.857146978378296
Loss :  1.8160614967346191 3.1687517166137695 3.1687517166137695
Loss :  1.8168472051620483 2.8547983169555664 2.8547983169555664
Loss :  1.7943311929702759 4.1243085861206055 4.1243085861206055
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7909961938858032 4.20045804977417 4.20045804977417
Loss :  1.793188452720642 4.038649082183838 4.038649082183838
Loss :  1.8093258142471313 4.2426910400390625 4.2426910400390625
Total LOSS train 3.234069064947275 valid 4.151526689529419
CE LOSS train 1.8177685554210956 valid 0.45233145356178284
Contrastive LOSS train 3.234069064947275 valid 1.0606727600097656
EPOCH 187:
Loss :  1.8256332874298096 2.7635257244110107 2.7635257244110107
Loss :  1.8133188486099243 2.961331605911255 2.961331605911255
Loss :  1.8211876153945923 2.9931387901306152 2.9931387901306152
Loss :  1.823110580444336 2.7926406860351562 2.7926406860351562
Loss :  1.8205668926239014 3.0982978343963623 3.0982978343963623
Loss :  1.8251851797103882 3.493558406829834 3.493558406829834
Loss :  1.8158482313156128 3.078763246536255 3.078763246536255
Loss :  1.8150862455368042 3.678335428237915 3.678335428237915
Loss :  1.8176268339157104 3.288882255554199 3.288882255554199
Loss :  1.819238543510437 2.9601049423217773 2.9601049423217773
Loss :  1.8235594034194946 3.0868005752563477 3.0868005752563477
Loss :  1.8168494701385498 3.5153744220733643 3.5153744220733643
Loss :  1.8221856355667114 3.159991979598999 3.159991979598999
Loss :  1.8265368938446045 2.866586208343506 2.866586208343506
Loss :  1.8231350183486938 2.9767041206359863 2.9767041206359863
Loss :  1.8319246768951416 2.913126230239868 2.913126230239868
Loss :  1.8236826658248901 3.0741117000579834 3.0741117000579834
Loss :  1.8270660638809204 3.0979042053222656 3.0979042053222656
Loss :  1.8245906829833984 2.9555695056915283 2.9555695056915283
Loss :  1.821935772895813 2.934520959854126 2.934520959854126
  batch 20 loss: 1.821935772895813, 2.934520959854126, 2.934520959854126
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8310359716415405 3.581272840499878 3.581272840499878
Loss :  1.8251205682754517 3.4985909461975098 3.4985909461975098
Loss :  1.8223971128463745 2.4429757595062256 2.4429757595062256
Loss :  1.8314169645309448 3.506765842437744 3.506765842437744
Loss :  1.8309341669082642 3.8592002391815186 3.8592002391815186
Loss :  1.822282314300537 3.2964375019073486 3.2964375019073486
Loss :  1.825023889541626 3.407886266708374 3.407886266708374
Loss :  1.8179727792739868 3.014267921447754 3.014267921447754
Loss :  1.8200665712356567 2.972062587738037 2.972062587738037
Loss :  1.8173015117645264 3.2172768115997314 3.2172768115997314
Loss :  1.815834879875183 3.0995559692382812 3.0995559692382812
Loss :  1.8170078992843628 3.4277775287628174 3.4277775287628174
Loss :  1.8235026597976685 3.498131275177002 3.498131275177002
Loss :  1.8193367719650269 3.293179750442505 3.293179750442505
Loss :  1.8229509592056274 3.441725969314575 3.441725969314575
Loss :  1.8173633813858032 3.3442394733428955 3.3442394733428955
Loss :  1.82265305519104 3.934967279434204 3.934967279434204
Loss :  1.8179998397827148 3.560676097869873 3.560676097869873
Loss :  1.824548363685608 3.7227301597595215 3.7227301597595215
Loss :  1.814310908317566 3.7684478759765625 3.7684478759765625
  batch 40 loss: 1.814310908317566, 3.7684478759765625, 3.7684478759765625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8201271295547485 3.9722867012023926 3.9722867012023926
Loss :  1.8175787925720215 3.5406594276428223 3.5406594276428223
Loss :  1.820717215538025 3.8246607780456543 3.8246607780456543
Loss :  1.80934739112854 3.1241533756256104 3.1241533756256104
Loss :  1.8214528560638428 3.1293833255767822 3.1293833255767822
Loss :  1.8147523403167725 3.510972738265991 3.510972738265991
Loss :  1.8140207529067993 3.9776899814605713 3.9776899814605713
Loss :  1.820516586303711 4.293884754180908 4.293884754180908
Loss :  1.8085793256759644 3.8729043006896973 3.8729043006896973
Loss :  1.820878028869629 3.99967622756958 3.99967622756958
Loss :  1.8165098428726196 3.9652950763702393 3.9652950763702393
Loss :  1.8277667760849 3.7673449516296387 3.7673449516296387
Loss :  1.8145519495010376 3.6709043979644775 3.6709043979644775
Loss :  1.8201334476470947 3.7091739177703857 3.7091739177703857
Loss :  1.8162164688110352 3.394524335861206 3.394524335861206
Loss :  1.8251582384109497 3.3837945461273193 3.3837945461273193
Loss :  1.8146799802780151 3.3656198978424072 3.3656198978424072
Loss :  1.8173357248306274 3.8145742416381836 3.8145742416381836
Loss :  1.8256021738052368 3.5164029598236084 3.5164029598236084
Loss :  1.8227914571762085 3.7224247455596924 3.7224247455596924
  batch 60 loss: 1.8227914571762085, 3.7224247455596924, 3.7224247455596924
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8138489723205566 3.0231802463531494 3.0231802463531494
Loss :  1.8151224851608276 2.96703839302063 2.96703839302063
Loss :  1.8134764432907104 2.5885167121887207 2.5885167121887207
Loss :  1.8106178045272827 2.5404515266418457 2.5404515266418457
Loss :  1.8127013444900513 2.248044490814209 2.248044490814209
Loss :  1.8079559803009033 4.071455478668213 4.071455478668213
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.8026245832443237 4.087200164794922 4.087200164794922
Loss :  1.8036562204360962 3.946302890777588 3.946302890777588
Loss :  1.8201929330825806 4.039439678192139 4.039439678192139
Total LOSS train 3.3307845995976373 valid 4.036099553108215
CE LOSS train 1.82018173291133 valid 0.45504823327064514
Contrastive LOSS train 3.3307845995976373 valid 1.0098599195480347
EPOCH 188:
Loss :  1.8228156566619873 2.6512112617492676 2.6512112617492676
Loss :  1.8077526092529297 3.0672154426574707 3.0672154426574707
Loss :  1.8182097673416138 2.831328868865967 2.831328868865967
Loss :  1.8223838806152344 2.971714735031128 2.971714735031128
Loss :  1.8185425996780396 2.8779916763305664 2.8779916763305664
Loss :  1.8231045007705688 3.4881529808044434 3.4881529808044434
Loss :  1.8121888637542725 3.1811065673828125 3.1811065673828125
Loss :  1.8126842975616455 2.961982250213623 2.961982250213623
Loss :  1.8150413036346436 2.9051551818847656 2.9051551818847656
Loss :  1.8152709007263184 2.736154794692993 2.736154794692993
Loss :  1.8197251558303833 3.171597480773926 3.171597480773926
Loss :  1.8137928247451782 3.2175803184509277 3.2175803184509277
Loss :  1.8169506788253784 3.248112916946411 3.248112916946411
Loss :  1.8205227851867676 2.940185785293579 2.940185785293579
Loss :  1.81496262550354 2.8883020877838135 2.8883020877838135
Loss :  1.8221052885055542 2.59975266456604 2.59975266456604
Loss :  1.815038800239563 2.8290278911590576 2.8290278911590576
Loss :  1.817106008529663 2.79205584526062 2.79205584526062
Loss :  1.816178321838379 2.875211238861084 2.875211238861084
Loss :  1.8108497858047485 3.1052443981170654 3.1052443981170654
  batch 20 loss: 1.8108497858047485, 3.1052443981170654, 3.1052443981170654
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8237072229385376 3.3289835453033447 3.3289835453033447
Loss :  1.81948721408844 2.882650136947632 2.882650136947632
Loss :  1.816667914390564 2.783234119415283 2.783234119415283
Loss :  1.8268492221832275 3.0784099102020264 3.0784099102020264
Loss :  1.8270628452301025 3.5053985118865967 3.5053985118865967
Loss :  1.8184945583343506 3.8417937755584717 3.8417937755584717
Loss :  1.8221440315246582 3.520526170730591 3.520526170730591
Loss :  1.8144334554672241 3.720520257949829 3.720520257949829
Loss :  1.8174206018447876 2.727728843688965 2.727728843688965
Loss :  1.812790870666504 2.8911585807800293 2.8911585807800293
Loss :  1.8135719299316406 3.2095913887023926 3.2095913887023926
Loss :  1.8127673864364624 3.2892518043518066 3.2892518043518066
Loss :  1.8182625770568848 2.9876179695129395 2.9876179695129395
Loss :  1.814632534980774 3.0478265285491943 3.0478265285491943
Loss :  1.8186447620391846 3.406379222869873 3.406379222869873
Loss :  1.813884973526001 3.227245807647705 3.227245807647705
Loss :  1.8187004327774048 3.5004007816314697 3.5004007816314697
Loss :  1.814367413520813 3.6859290599823 3.6859290599823
Loss :  1.8212445974349976 3.309319257736206 3.309319257736206
Loss :  1.8114920854568481 3.1173086166381836 3.1173086166381836
  batch 40 loss: 1.8114920854568481, 3.1173086166381836, 3.1173086166381836
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.817008376121521 3.273362159729004 3.273362159729004
Loss :  1.8139220476150513 2.915524482727051 2.915524482727051
Loss :  1.8169094324111938 3.05483341217041 3.05483341217041
Loss :  1.8059179782867432 3.0272812843322754 3.0272812843322754
Loss :  1.8181480169296265 3.195328712463379 3.195328712463379
Loss :  1.8095130920410156 3.8668017387390137 3.8668017387390137
Loss :  1.8089408874511719 3.0006706714630127 3.0006706714630127
Loss :  1.8168365955352783 2.96773362159729 2.96773362159729
Loss :  1.8008931875228882 3.116849184036255 3.116849184036255
Loss :  1.8153092861175537 3.0346856117248535 3.0346856117248535
Loss :  1.811230182647705 2.6803765296936035 2.6803765296936035
Loss :  1.8242415189743042 2.76408052444458 2.76408052444458
Loss :  1.8135744333267212 2.5205304622650146 2.5205304622650146
Loss :  1.817946434020996 2.788334608078003 2.788334608078003
Loss :  1.8133947849273682 2.8935155868530273 2.8935155868530273
Loss :  1.8251721858978271 3.0560052394866943 3.0560052394866943
Loss :  1.8154199123382568 3.5281431674957275 3.5281431674957275
Loss :  1.816658616065979 3.0165109634399414 3.0165109634399414
Loss :  1.8250319957733154 2.945298433303833 2.945298433303833
Loss :  1.8231474161148071 2.795325756072998 2.795325756072998
  batch 60 loss: 1.8231474161148071, 2.795325756072998, 2.795325756072998
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8151016235351562 2.945894479751587 2.945894479751587
Loss :  1.8140240907669067 2.69089937210083 2.69089937210083
Loss :  1.8146857023239136 2.6579182147979736 2.6579182147979736
Loss :  1.8102798461914062 2.7393181324005127 2.7393181324005127
Loss :  1.8122514486312866 2.54376220703125 2.54376220703125
Loss :  1.8010002374649048 4.316145420074463 4.316145420074463
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3, 4], device='cuda:0')
Loss :  1.7983373403549194 4.251169681549072 4.251169681549072
Loss :  1.7956254482269287 4.268778324127197 4.268778324127197
Loss :  1.81669282913208 4.173675060272217 4.173675060272217
Total LOSS train 3.0526052805093618 valid 4.252442121505737
CE LOSS train 1.8164217288677509 valid 0.45417320728302
Contrastive LOSS train 3.0526052805093618 valid 1.0434187650680542
EPOCH 189:
Loss :  1.8202357292175293 3.4605369567871094 3.4605369567871094
Loss :  1.804832100868225 3.6742913722991943 3.6742913722991943
Loss :  1.8156458139419556 3.7760958671569824 3.7760958671569824
Loss :  1.8196219205856323 3.6360390186309814 3.6360390186309814
Loss :  1.8141247034072876 3.9648959636688232 3.9648959636688232
Loss :  1.8188897371292114 3.663869619369507 3.663869619369507
Loss :  1.8057290315628052 3.48667311668396 3.48667311668396
Loss :  1.8077861070632935 4.230504989624023 4.230504989624023
Loss :  1.8141474723815918 3.215265989303589 3.215265989303589
Loss :  1.812093734741211 3.5962741374969482 3.5962741374969482
Loss :  1.8172541856765747 3.6641957759857178 3.6641957759857178
Loss :  1.8137723207473755 3.6447153091430664 3.6447153091430664
Loss :  1.8200064897537231 3.8042328357696533 3.8042328357696533
Loss :  1.823379635810852 2.839012384414673 2.839012384414673
Loss :  1.8203258514404297 2.849881649017334 2.849881649017334
Loss :  1.8254992961883545 3.202077627182007 3.202077627182007
Loss :  1.8182258605957031 3.235593557357788 3.235593557357788
Loss :  1.8211450576782227 3.319659471511841 3.319659471511841
Loss :  1.8197681903839111 2.7477643489837646 2.7477643489837646
Loss :  1.813069224357605 3.7877044677734375 3.7877044677734375
  batch 20 loss: 1.813069224357605, 3.7877044677734375, 3.7877044677734375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8242738246917725 3.3547143936157227 3.3547143936157227
Loss :  1.8186392784118652 3.5644705295562744 3.5644705295562744
Loss :  1.8146456480026245 3.5704293251037598 3.5704293251037598
Loss :  1.8247348070144653 3.111521005630493 3.111521005630493
Loss :  1.8226664066314697 3.4480693340301514 3.4480693340301514
Loss :  1.8145549297332764 3.139463186264038 3.139463186264038
Loss :  1.819224238395691 3.5401690006256104 3.5401690006256104
Loss :  1.810585618019104 3.314483642578125 3.314483642578125
Loss :  1.8157562017440796 2.8922293186187744 2.8922293186187744
Loss :  1.8089739084243774 2.9813151359558105 2.9813151359558105
Loss :  1.8117432594299316 3.220388889312744 3.220388889312744
Loss :  1.8115965127944946 2.890617847442627 2.890617847442627
Loss :  1.8174327611923218 3.3139986991882324 3.3139986991882324
Loss :  1.8130651712417603 3.364224672317505 3.364224672317505
Loss :  1.8171831369400024 3.5251259803771973 3.5251259803771973
Loss :  1.8130685091018677 3.540748119354248 3.540748119354248
Loss :  1.8159483671188354 3.3495075702667236 3.3495075702667236
Loss :  1.8099383115768433 3.1130263805389404 3.1130263805389404
Loss :  1.8199408054351807 3.296294689178467 3.296294689178467
Loss :  1.809906244277954 3.574608087539673 3.574608087539673
  batch 40 loss: 1.809906244277954, 3.574608087539673, 3.574608087539673
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8145166635513306 3.7806243896484375 3.7806243896484375
Loss :  1.813501238822937 3.872175693511963 3.872175693511963
Loss :  1.818191409111023 3.226871967315674 3.226871967315674
Loss :  1.8068681955337524 3.1260948181152344 3.1260948181152344
Loss :  1.8195055723190308 3.6644856929779053 3.6644856929779053
Loss :  1.8096176385879517 2.948385000228882 2.948385000228882
Loss :  1.809956431388855 2.6650874614715576 2.6650874614715576
Loss :  1.8180088996887207 2.9388234615325928 2.9388234615325928
Loss :  1.8021178245544434 3.121265172958374 3.121265172958374
Loss :  1.8149793148040771 2.9735000133514404 2.9735000133514404
Loss :  1.810517430305481 3.218977689743042 3.218977689743042
Loss :  1.8222507238388062 3.0607998371124268 3.0607998371124268
Loss :  1.8126118183135986 2.8354861736297607 2.8354861736297607
Loss :  1.8161687850952148 3.2448840141296387 3.2448840141296387
Loss :  1.8109411001205444 3.799624443054199 3.799624443054199
Loss :  1.8234015703201294 3.037415027618408 3.037415027618408
Loss :  1.8150495290756226 3.048285961151123 3.048285961151123
Loss :  1.8176413774490356 2.6081159114837646 2.6081159114837646
Loss :  1.8257602453231812 3.1126198768615723 3.1126198768615723
Loss :  1.8240859508514404 2.7021853923797607 2.7021853923797607
  batch 60 loss: 1.8240859508514404, 2.7021853923797607, 2.7021853923797607
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8155517578125 2.781660556793213 2.781660556793213
Loss :  1.8153886795043945 2.900601863861084 2.900601863861084
Loss :  1.8151003122329712 3.2014822959899902 3.2014822959899902
Loss :  1.8111833333969116 2.7617619037628174 2.7617619037628174
Loss :  1.812699317932129 2.7896523475646973 2.7896523475646973
Loss :  1.7706652879714966 4.1574201583862305 4.1574201583862305
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.7716021537780762 4.146168231964111 4.146168231964111
Loss :  1.7657098770141602 4.018482208251953 4.018482208251953
Loss :  1.792569875717163 4.0487589836120605 4.0487589836120605
Total LOSS train 3.2819316497215856 valid 4.092707395553589
CE LOSS train 1.8156160849791307 valid 0.44814246892929077
Contrastive LOSS train 3.2819316497215856 valid 1.0121897459030151
EPOCH 190:
Loss :  1.8226995468139648 2.9309961795806885 2.9309961795806885
Loss :  1.808052659034729 3.1916956901550293 3.1916956901550293
Loss :  1.816131591796875 2.769392251968384 2.769392251968384
Loss :  1.8186734914779663 2.457700490951538 2.457700490951538
Loss :  1.8158094882965088 3.1060452461242676 3.1060452461242676
Loss :  1.8196340799331665 2.937856435775757 2.937856435775757
Loss :  1.807692050933838 2.6556169986724854 2.6556169986724854
Loss :  1.808879017829895 2.3711836338043213 2.3711836338043213
Loss :  1.8102376461029053 3.098437786102295 3.098437786102295
Loss :  1.8078316450119019 2.9849791526794434 2.9849791526794434
Loss :  1.8157458305358887 3.322723150253296 3.322723150253296
Loss :  1.8125252723693848 3.199540138244629 3.199540138244629
Loss :  1.812958836555481 3.2375824451446533 3.2375824451446533
Loss :  1.81690514087677 3.428234577178955 3.428234577178955
Loss :  1.8113653659820557 3.1635425090789795 3.1635425090789795
Loss :  1.8181123733520508 2.835228443145752 2.835228443145752
Loss :  1.8119274377822876 2.872323513031006 2.872323513031006
Loss :  1.8150255680084229 3.252171516418457 3.252171516418457
Loss :  1.8140335083007812 3.3394720554351807 3.3394720554351807
Loss :  1.8080060482025146 3.048419237136841 3.048419237136841
  batch 20 loss: 1.8080060482025146, 3.048419237136841, 3.048419237136841
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8208622932434082 3.390204429626465 3.390204429626465
Loss :  1.8185697793960571 3.250521183013916 3.250521183013916
Loss :  1.8171712160110474 3.8790297508239746 3.8790297508239746
Loss :  1.8272135257720947 3.7787513732910156 3.7787513732910156
Loss :  1.8256303071975708 3.493079900741577 3.493079900741577
Loss :  1.8166204690933228 2.4499266147613525 2.4499266147613525
Loss :  1.820357322692871 3.250258445739746 3.250258445739746
Loss :  1.810445785522461 2.8603217601776123 2.8603217601776123
Loss :  1.8138741254806519 2.7676444053649902 2.7676444053649902
Loss :  1.8088830709457397 2.8516006469726562 2.8516006469726562
Loss :  1.8110953569412231 2.849076271057129 2.849076271057129
Loss :  1.8095028400421143 2.8438427448272705 2.8438427448272705
Loss :  1.8142609596252441 3.17820405960083 3.17820405960083
Loss :  1.8099431991577148 3.117419719696045 3.117419719696045
Loss :  1.8141509294509888 2.8556525707244873 2.8556525707244873
Loss :  1.810121774673462 2.962111711502075 2.962111711502075
Loss :  1.8138494491577148 3.0384933948516846 3.0384933948516846
Loss :  1.81133234500885 3.2384955883026123 3.2384955883026123
Loss :  1.81692636013031 3.283951759338379 3.283951759338379
Loss :  1.8098657131195068 2.740764856338501 2.740764856338501
  batch 40 loss: 1.8098657131195068, 2.740764856338501, 2.740764856338501
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8169530630111694 3.093851327896118 3.093851327896118
Loss :  1.8145619630813599 3.8078386783599854 3.8078386783599854
Loss :  1.8175028562545776 3.1948561668395996 3.1948561668395996
Loss :  1.8070755004882812 3.281694173812866 3.281694173812866
Loss :  1.8218179941177368 3.3448987007141113 3.3448987007141113
Loss :  1.8143302202224731 3.503103494644165 3.503103494644165
Loss :  1.8143310546875 3.101728916168213 3.101728916168213
Loss :  1.8204492330551147 2.8711471557617188 2.8711471557617188
Loss :  1.8080164194107056 3.7619221210479736 3.7619221210479736
Loss :  1.8164749145507812 3.461789846420288 3.461789846420288
Loss :  1.8146867752075195 3.3619308471679688 3.3619308471679688
Loss :  1.8260297775268555 2.6843225955963135 2.6843225955963135
Loss :  1.8140703439712524 2.489849328994751 2.489849328994751
Loss :  1.8183186054229736 2.480255126953125 2.480255126953125
Loss :  1.8169093132019043 2.891225576400757 2.891225576400757
Loss :  1.8254563808441162 2.906872034072876 2.906872034072876
Loss :  1.815190076828003 3.045440435409546 3.045440435409546
Loss :  1.8206526041030884 2.7850935459136963 2.7850935459136963
Loss :  1.8249702453613281 3.586108446121216 3.586108446121216
Loss :  1.8242605924606323 3.027676820755005 3.027676820755005
  batch 60 loss: 1.8242605924606323, 3.027676820755005, 3.027676820755005
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8149912357330322 2.7154176235198975 2.7154176235198975
Loss :  1.8151321411132812 3.0456511974334717 3.0456511974334717
Loss :  1.8151665925979614 3.0496277809143066 3.0496277809143066
Loss :  1.8097965717315674 3.036071538925171 3.036071538925171
Loss :  1.8108856678009033 2.5309948921203613 2.5309948921203613
Loss :  1.795469880104065 4.135808944702148 4.135808944702148
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3, 4], device='cuda:0')
Loss :  1.7955853939056396 4.213229656219482 4.213229656219482
Loss :  1.7909902334213257 4.0224385261535645 4.0224385261535645
Loss :  1.8137296438217163 3.8608880043029785 3.8608880043029785
Total LOSS train 3.0667978616861196 valid 4.0580912828445435
CE LOSS train 1.8152454394560593 valid 0.4534324109554291
Contrastive LOSS train 3.0667978616861196 valid 0.9652220010757446
EPOCH 191:
Loss :  1.8182369470596313 2.868974447250366 2.868974447250366
Loss :  1.8062410354614258 3.566220283508301 3.566220283508301
Loss :  1.8123420476913452 3.1237967014312744 3.1237967014312744
Loss :  1.8152503967285156 3.5264196395874023 3.5264196395874023
Loss :  1.812164306640625 2.9667320251464844 2.9667320251464844
Loss :  1.8174065351486206 3.419781446456909 3.419781446456909
Loss :  1.8048511743545532 3.485589027404785 3.485589027404785
Loss :  1.8071430921554565 3.112990617752075 3.112990617752075
Loss :  1.8095457553863525 3.171893835067749 3.171893835067749
Loss :  1.8079644441604614 3.189784526824951 3.189784526824951
Loss :  1.8158726692199707 3.595024347305298 3.595024347305298
Loss :  1.8114923238754272 3.7659904956817627 3.7659904956817627
Loss :  1.8157657384872437 3.43674898147583 3.43674898147583
Loss :  1.8197180032730103 2.7429873943328857 2.7429873943328857
Loss :  1.8153951168060303 3.132850170135498 3.132850170135498
Loss :  1.8215341567993164 3.6139402389526367 3.6139402389526367
Loss :  1.8151053190231323 3.324662208557129 3.324662208557129
Loss :  1.8194540739059448 2.8334224224090576 2.8334224224090576
Loss :  1.8175041675567627 2.892333745956421 2.892333745956421
Loss :  1.8126959800720215 3.5268266201019287 3.5268266201019287
  batch 20 loss: 1.8126959800720215, 3.5268266201019287, 3.5268266201019287
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8246432542800903 3.8691353797912598 3.8691353797912598
Loss :  1.8194940090179443 3.4937641620635986 3.4937641620635986
Loss :  1.816426396369934 3.340388536453247 3.340388536453247
Loss :  1.826108694076538 3.4789681434631348 3.4789681434631348
Loss :  1.824539303779602 3.535177707672119 3.535177707672119
Loss :  1.815571904182434 2.688593864440918 2.688593864440918
Loss :  1.8198113441467285 2.8972198963165283 2.8972198963165283
Loss :  1.8107600212097168 2.9387314319610596 2.9387314319610596
Loss :  1.8135496377944946 2.8230178356170654 2.8230178356170654
Loss :  1.8085665702819824 2.7923896312713623 2.7923896312713623
Loss :  1.8120800256729126 2.9132773876190186 2.9132773876190186
Loss :  1.8099983930587769 2.9612784385681152 2.9612784385681152
Loss :  1.8139002323150635 2.916313648223877 2.916313648223877
Loss :  1.8106247186660767 3.6078579425811768 3.6078579425811768
Loss :  1.8147661685943604 2.941584825515747 2.941584825515747
Loss :  1.81101655960083 3.0910346508026123 3.0910346508026123
Loss :  1.8158024549484253 2.792649745941162 2.792649745941162
Loss :  1.8131775856018066 2.627525806427002 2.627525806427002
Loss :  1.8186349868774414 3.014831066131592 3.014831066131592
Loss :  1.8114731311798096 2.564274311065674 2.564274311065674
  batch 40 loss: 1.8114731311798096, 2.564274311065674, 2.564274311065674
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8162775039672852 3.1082749366760254 3.1082749366760254
Loss :  1.812579870223999 2.861487865447998 2.861487865447998
Loss :  1.813912034034729 3.4409637451171875 3.4409637451171875
Loss :  1.8051921129226685 3.3040406703948975 3.3040406703948975
Loss :  1.8162128925323486 2.774818181991577 2.774818181991577
Loss :  1.8097741603851318 2.5849130153656006 2.5849130153656006
Loss :  1.80975341796875 2.561466932296753 2.561466932296753
Loss :  1.8128175735473633 2.7024714946746826 2.7024714946746826
Loss :  1.802978754043579 3.4223217964172363 3.4223217964172363
Loss :  1.8096667528152466 3.4731011390686035 3.4731011390686035
Loss :  1.8054722547531128 3.2090604305267334 3.2090604305267334
Loss :  1.818424940109253 2.787834644317627 2.787834644317627
Loss :  1.8099719285964966 3.5083444118499756 3.5083444118499756
Loss :  1.812661051750183 3.1211729049682617 3.1211729049682617
Loss :  1.8085769414901733 3.0537192821502686 3.0537192821502686
Loss :  1.8189811706542969 3.660055637359619 3.660055637359619
Loss :  1.8116573095321655 3.630951166152954 3.630951166152954
Loss :  1.8158258199691772 3.2657628059387207 3.2657628059387207
Loss :  1.820580005645752 3.8775153160095215 3.8775153160095215
Loss :  1.819229006767273 3.2182912826538086 3.2182912826538086
  batch 60 loss: 1.819229006767273, 3.2182912826538086, 3.2182912826538086
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8127567768096924 3.957958221435547 3.957958221435547
Loss :  1.813001036643982 3.6623759269714355 3.6623759269714355
Loss :  1.8132301568984985 4.033334255218506 4.033334255218506
Loss :  1.8086048364639282 3.651529550552368 3.651529550552368
Loss :  1.8116272687911987 2.7911906242370605 2.7911906242370605
Loss :  1.788940668106079 4.2402849197387695 4.2402849197387695
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3, 4], device='cuda:0')
Loss :  1.7890809774398804 4.19281530380249 4.19281530380249
Loss :  1.782633662223816 4.0311079025268555 4.0311079025268555
Loss :  1.8105437755584717 3.9980454444885254 3.9980454444885254
Total LOSS train 3.203814458847046 valid 4.11556339263916
CE LOSS train 1.8137906808119553 valid 0.4526359438896179
Contrastive LOSS train 3.203814458847046 valid 0.9995113611221313
EPOCH 192:
Loss :  1.8189550638198853 3.56793212890625 3.56793212890625
Loss :  1.8061610460281372 3.9280431270599365 3.9280431270599365
Loss :  1.813685417175293 3.884188652038574 3.884188652038574
Loss :  1.8176980018615723 2.9279775619506836 2.9279775619506836
Loss :  1.8139899969100952 2.6355412006378174 2.6355412006378174
Loss :  1.8195745944976807 2.796358585357666 2.796358585357666
Loss :  1.8066052198410034 3.2313883304595947 3.2313883304595947
Loss :  1.8089466094970703 3.3966314792633057 3.3966314792633057
Loss :  1.8109447956085205 2.875973701477051 2.875973701477051
Loss :  1.8098421096801758 2.698681116104126 2.698681116104126
Loss :  1.8166468143463135 3.0419390201568604 3.0419390201568604
Loss :  1.8130114078521729 2.9889726638793945 2.9889726638793945
Loss :  1.8148938417434692 3.1468656063079834 3.1468656063079834
Loss :  1.8190449476242065 2.529784679412842 2.529784679412842
Loss :  1.8144806623458862 3.0927751064300537 3.0927751064300537
Loss :  1.8221725225448608 3.091334819793701 3.091334819793701
Loss :  1.8140783309936523 3.0328001976013184 3.0328001976013184
Loss :  1.8179352283477783 3.248446464538574 3.248446464538574
Loss :  1.816419005393982 3.0307066440582275 3.0307066440582275
Loss :  1.8094033002853394 3.2562718391418457 3.2562718391418457
  batch 20 loss: 1.8094033002853394, 3.2562718391418457, 3.2562718391418457
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.820709466934204 2.835520029067993 2.835520029067993
Loss :  1.8161604404449463 3.0244626998901367 3.0244626998901367
Loss :  1.8114651441574097 2.792412042617798 2.792412042617798
Loss :  1.8226736783981323 2.771254777908325 2.771254777908325
Loss :  1.819461703300476 3.0972516536712646 3.0972516536712646
Loss :  1.810375452041626 2.3701424598693848 2.3701424598693848
Loss :  1.8157709836959839 3.229804754257202 3.229804754257202
Loss :  1.8050861358642578 2.8915176391601562 2.8915176391601562
Loss :  1.8110486268997192 3.029083728790283 3.029083728790283
Loss :  1.805350661277771 3.2921526432037354 3.2921526432037354
Loss :  1.809977412223816 3.243088483810425 3.243088483810425
Loss :  1.806872844696045 3.0944364070892334 3.0944364070892334
Loss :  1.8119391202926636 3.3311731815338135 3.3311731815338135
Loss :  1.8078100681304932 3.528201103210449 3.528201103210449
Loss :  1.8131965398788452 3.4366109371185303 3.4366109371185303
Loss :  1.8086693286895752 2.9225800037384033 2.9225800037384033
Loss :  1.8130481243133545 3.128403902053833 3.128403902053833
Loss :  1.809273600578308 3.211509943008423 3.211509943008423
Loss :  1.8141696453094482 3.1892106533050537 3.1892106533050537
Loss :  1.8056907653808594 3.2049643993377686 3.2049643993377686
  batch 40 loss: 1.8056907653808594, 3.2049643993377686, 3.2049643993377686
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8118802309036255 3.281825065612793 3.281825065612793
Loss :  1.808484435081482 2.928260326385498 2.928260326385498
Loss :  1.8100831508636475 3.1153769493103027 3.1153769493103027
Loss :  1.8014686107635498 3.8524961471557617 3.8524961471557617
Loss :  1.8124749660491943 3.1167750358581543 3.1167750358581543
Loss :  1.8061412572860718 2.8593664169311523 2.8593664169311523
Loss :  1.8047808408737183 3.158531904220581 3.158531904220581
Loss :  1.810788631439209 3.384707450866699 3.384707450866699
Loss :  1.7998168468475342 3.112766981124878 3.112766981124878
Loss :  1.8079791069030762 2.995004892349243 2.995004892349243
Loss :  1.8048309087753296 3.4023072719573975 3.4023072719573975
Loss :  1.8184901475906372 2.7615087032318115 2.7615087032318115
Loss :  1.8092422485351562 2.741664409637451 2.741664409637451
Loss :  1.810266137123108 2.9873154163360596 2.9873154163360596
Loss :  1.808684229850769 3.3572261333465576 3.3572261333465576
Loss :  1.8167774677276611 3.177933931350708 3.177933931350708
Loss :  1.8098037242889404 3.241957902908325 3.241957902908325
Loss :  1.8155272006988525 3.0289433002471924 3.0289433002471924
Loss :  1.8199793100357056 3.494884729385376 3.494884729385376
Loss :  1.8172026872634888 3.2803430557250977 3.2803430557250977
  batch 60 loss: 1.8172026872634888, 3.2803430557250977, 3.2803430557250977
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8110178709030151 3.110778570175171 3.110778570175171
Loss :  1.8119874000549316 2.8509814739227295 2.8509814739227295
Loss :  1.8109490871429443 3.4602363109588623 3.4602363109588623
Loss :  1.8081507682800293 3.0533828735351562 3.0533828735351562
Loss :  1.8131020069122314 3.172792911529541 3.172792911529541
Loss :  1.8129106760025024 4.079491138458252 4.079491138458252
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3, 4], device='cuda:0')
Loss :  1.81035578250885 4.10787296295166 4.10787296295166
Loss :  1.805921196937561 4.060152530670166 4.060152530670166
Loss :  1.8285001516342163 3.9983606338500977 3.9983606338500977
Total LOSS train 3.1223655774043158 valid 4.061469316482544
CE LOSS train 1.8120484297092145 valid 0.4571250379085541
Contrastive LOSS train 3.1223655774043158 valid 0.9995901584625244
EPOCH 193:
Loss :  1.8192541599273682 3.0666656494140625 3.0666656494140625
Loss :  1.800296664237976 3.461080312728882 3.461080312728882
Loss :  1.8127466440200806 3.0600883960723877 3.0600883960723877
Loss :  1.8168994188308716 3.446751594543457 3.446751594543457
Loss :  1.813478708267212 3.2186005115509033 3.2186005115509033
Loss :  1.8162425756454468 3.3181278705596924 3.3181278705596924
Loss :  1.8026409149169922 3.69767689704895 3.69767689704895
Loss :  1.8058780431747437 3.382375478744507 3.382375478744507
Loss :  1.8116649389266968 2.9479124546051025 2.9479124546051025
Loss :  1.8074822425842285 2.8554611206054688 2.8554611206054688
Loss :  1.814585566520691 3.068145275115967 3.068145275115967
Loss :  1.8101171255111694 3.0647809505462646 3.0647809505462646
Loss :  1.8127976655960083 3.3965442180633545 3.3965442180633545
Loss :  1.8167718648910522 2.5191004276275635 2.5191004276275635
Loss :  1.8113434314727783 2.727839469909668 2.727839469909668
Loss :  1.8169453144073486 2.65859317779541 2.65859317779541
Loss :  1.81035578250885 2.993779420852661 2.993779420852661
Loss :  1.8118935823440552 2.8473598957061768 2.8473598957061768
Loss :  1.8099232912063599 3.4121289253234863 3.4121289253234863
Loss :  1.803727626800537 3.613466501235962 3.613466501235962
  batch 20 loss: 1.803727626800537, 3.613466501235962, 3.613466501235962
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8141164779663086 3.0982792377471924 3.0982792377471924
Loss :  1.811167597770691 3.468336820602417 3.468336820602417
Loss :  1.8060884475708008 3.215679168701172 3.215679168701172
Loss :  1.8156590461730957 3.285050868988037 3.285050868988037
Loss :  1.8128257989883423 3.8240585327148438 3.8240585327148438
Loss :  1.802514672279358 3.6160972118377686 3.6160972118377686
Loss :  1.8095145225524902 3.548848867416382 3.548848867416382
Loss :  1.7980115413665771 3.169253349304199 3.169253349304199
Loss :  1.8074793815612793 3.507092237472534 3.507092237472534
Loss :  1.7975356578826904 3.2834181785583496 3.2834181785583496
Loss :  1.8054730892181396 2.7951130867004395 2.7951130867004395
Loss :  1.797994613647461 3.453864574432373 3.453864574432373
Loss :  1.8037618398666382 3.0403671264648438 3.0403671264648438
Loss :  1.8027253150939941 3.1723852157592773 3.1723852157592773
Loss :  1.807034969329834 3.375441551208496 3.375441551208496
Loss :  1.8045713901519775 3.251568078994751 3.251568078994751
Loss :  1.808225393295288 3.2327630519866943 3.2327630519866943
Loss :  1.802634358406067 2.7864174842834473 2.7864174842834473
Loss :  1.8098737001419067 3.0552072525024414 3.0552072525024414
Loss :  1.80286705493927 2.643603563308716 2.643603563308716
  batch 40 loss: 1.80286705493927, 2.643603563308716, 2.643603563308716
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8079224824905396 2.862563371658325 2.862563371658325
Loss :  1.8052226305007935 2.683047294616699 2.683047294616699
Loss :  1.8101441860198975 2.8504908084869385 2.8504908084869385
Loss :  1.8013718128204346 2.874706983566284 2.874706983566284
Loss :  1.8124202489852905 3.6491847038269043 3.6491847038269043
Loss :  1.8034296035766602 3.3460946083068848 3.3460946083068848
Loss :  1.8029496669769287 3.490933418273926 3.490933418273926
Loss :  1.8098094463348389 2.8637068271636963 2.8637068271636963
Loss :  1.7955831289291382 2.9920477867126465 2.9920477867126465
Loss :  1.8065794706344604 3.001657009124756 3.001657009124756
Loss :  1.8030246496200562 3.2018942832946777 3.2018942832946777
Loss :  1.8136080503463745 3.3511743545532227 3.3511743545532227
Loss :  1.8065760135650635 2.952716827392578 2.952716827392578
Loss :  1.809346079826355 3.27181077003479 3.27181077003479
Loss :  1.808493733406067 3.0567636489868164 3.0567636489868164
Loss :  1.812908411026001 3.0108790397644043 3.0108790397644043
Loss :  1.8074376583099365 3.0600123405456543 3.0600123405456543
Loss :  1.8117899894714355 3.076568603515625 3.076568603515625
Loss :  1.816625952720642 3.020045042037964 3.020045042037964
Loss :  1.8135312795639038 2.631366729736328 2.631366729736328
  batch 60 loss: 1.8135312795639038, 2.631366729736328, 2.631366729736328
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8076006174087524 2.6403188705444336 2.6403188705444336
Loss :  1.8073465824127197 2.64119291305542 2.64119291305542
Loss :  1.8073724508285522 3.0122954845428467 3.0122954845428467
Loss :  1.8031505346298218 3.0889384746551514 3.0889384746551514
Loss :  1.8060859441757202 2.7570040225982666 2.7570040225982666
Loss :  1.80391526222229 4.3968706130981445 4.3968706130981445
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3, 4], device='cuda:0')
Loss :  1.8014569282531738 4.420538425445557 4.420538425445557
Loss :  1.7975387573242188 4.2891645431518555 4.2891645431518555
Loss :  1.8176679611206055 4.219566345214844 4.219566345214844
Total LOSS train 3.1225652034466083 valid 4.3315349817276
CE LOSS train 1.8082377085318933 valid 0.45441699028015137
Contrastive LOSS train 3.1225652034466083 valid 1.054891586303711
EPOCH 194:
Loss :  1.8102165460586548 3.1046149730682373 3.1046149730682373
Loss :  1.8010512590408325 2.956054925918579 2.956054925918579
Loss :  1.807355284690857 3.3022077083587646 3.3022077083587646
Loss :  1.8103519678115845 3.1499104499816895 3.1499104499816895
Loss :  1.8085169792175293 2.574012279510498 2.574012279510498
Loss :  1.8125602006912231 3.1345393657684326 3.1345393657684326
Loss :  1.8029603958129883 3.2967686653137207 3.2967686653137207
Loss :  1.8047285079956055 3.204268217086792 3.204268217086792
Loss :  1.8056358098983765 3.200880289077759 3.200880289077759
Loss :  1.803314208984375 2.8869433403015137 2.8869433403015137
Loss :  1.8108608722686768 3.4507431983947754 3.4507431983947754
Loss :  1.806841254234314 3.235241413116455 3.235241413116455
Loss :  1.8101439476013184 2.8482327461242676 2.8482327461242676
Loss :  1.8126662969589233 2.5162572860717773 2.5162572860717773
Loss :  1.804858922958374 2.7649154663085938 2.7649154663085938
Loss :  1.8109896183013916 2.7425224781036377 2.7425224781036377
Loss :  1.80880868434906 2.6780025959014893 2.6780025959014893
Loss :  1.8087364435195923 2.710186719894409 2.710186719894409
Loss :  1.8092056512832642 2.9629480838775635 2.9629480838775635
Loss :  1.80197012424469 2.6677849292755127 2.6677849292755127
  batch 20 loss: 1.80197012424469, 2.6677849292755127, 2.6677849292755127
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8147376775741577 2.8044393062591553 2.8044393062591553
Loss :  1.811971664428711 2.984100818634033 2.984100818634033
Loss :  1.8080241680145264 2.861755609512329 2.861755609512329
Loss :  1.8163061141967773 2.96876859664917 2.96876859664917
Loss :  1.8136992454528809 3.102459192276001 3.102459192276001
Loss :  1.806767225265503 2.8836753368377686 2.8836753368377686
Loss :  1.812657356262207 3.395202159881592 3.395202159881592
Loss :  1.8032630681991577 3.138909339904785 3.138909339904785
Loss :  1.8104947805404663 2.9222960472106934 2.9222960472106934
Loss :  1.8006715774536133 3.110198974609375 3.110198974609375
Loss :  1.8076636791229248 2.7711079120635986 2.7711079120635986
Loss :  1.8033583164215088 2.8998680114746094 2.8998680114746094
Loss :  1.808085560798645 2.763568162918091 2.763568162918091
Loss :  1.8061600923538208 2.8707568645477295 2.8707568645477295
Loss :  1.810647964477539 2.938941717147827 2.938941717147827
Loss :  1.8068259954452515 3.1903369426727295 3.1903369426727295
Loss :  1.8110451698303223 2.7444255352020264 2.7444255352020264
Loss :  1.806081771850586 2.8595242500305176 2.8595242500305176
Loss :  1.8120700120925903 3.014838457107544 3.014838457107544
Loss :  1.8045103549957275 3.053203582763672 3.053203582763672
  batch 40 loss: 1.8045103549957275, 3.053203582763672, 3.053203582763672
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8099243640899658 3.3447043895721436 3.3447043895721436
Loss :  1.8060364723205566 3.9586105346679688 3.9586105346679688
Loss :  1.8111599683761597 3.2640533447265625 3.2640533447265625
Loss :  1.8031915426254272 3.2816147804260254 3.2816147804260254
Loss :  1.8136216402053833 3.5938611030578613 3.5938611030578613
Loss :  1.805180311203003 3.0869250297546387 3.0869250297546387
Loss :  1.8046374320983887 3.3775861263275146 3.3775861263275146
Loss :  1.810127854347229 3.490481376647949 3.490481376647949
Loss :  1.798803448677063 3.3870224952697754 3.3870224952697754
Loss :  1.8075374364852905 3.32153582572937 3.32153582572937
Loss :  1.8035413026809692 3.3173558712005615 3.3173558712005615
Loss :  1.8162988424301147 3.3845489025115967 3.3845489025115967
Loss :  1.8080329895019531 2.8018689155578613 2.8018689155578613
Loss :  1.8094254732131958 2.8568406105041504 2.8568406105041504
Loss :  1.806960940361023 3.0585217475891113 3.0585217475891113
Loss :  1.8136529922485352 2.9236879348754883 2.9236879348754883
Loss :  1.8080799579620361 3.336907148361206 3.336907148361206
Loss :  1.8118791580200195 3.2434113025665283 3.2434113025665283
Loss :  1.8160381317138672 3.26124906539917 3.26124906539917
Loss :  1.8118482828140259 3.2290937900543213 3.2290937900543213
  batch 60 loss: 1.8118482828140259, 3.2290937900543213, 3.2290937900543213
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8081543445587158 3.0806894302368164 3.0806894302368164
Loss :  1.807431936264038 3.07509708404541 3.07509708404541
Loss :  1.8070237636566162 3.610356569290161 3.610356569290161
Loss :  1.8032097816467285 3.5789806842803955 3.5789806842803955
Loss :  1.806293249130249 3.2710680961608887 3.2710680961608887
Loss :  1.8037551641464233 4.321583271026611 4.321583271026611
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3, 4], device='cuda:0')
Loss :  1.8013241291046143 4.368523597717285 4.368523597717285
Loss :  1.7987412214279175 4.273064136505127 4.273064136505127
Loss :  1.8183447122573853 4.105764389038086 4.105764389038086
Total LOSS train 3.0892536016610954 valid 4.267233848571777
CE LOSS train 1.8082293290358322 valid 0.4545861780643463
Contrastive LOSS train 3.0892536016610954 valid 1.0264410972595215
EPOCH 195:
Loss :  1.8094559907913208 3.254734992980957 3.254734992980957
Loss :  1.7995002269744873 3.6584370136260986 3.6584370136260986
Loss :  1.807100534439087 4.179581165313721 4.179581165313721
Loss :  1.8106988668441772 3.073512554168701 3.073512554168701
Loss :  1.8072147369384766 2.9849209785461426 2.9849209785461426
Loss :  1.811570167541504 3.287867546081543 3.287867546081543
Loss :  1.8001060485839844 2.832073211669922 2.832073211669922
Loss :  1.8025908470153809 2.674501657485962 2.674501657485962
Loss :  1.8019990921020508 2.517629384994507 2.517629384994507
Loss :  1.7996469736099243 2.626542329788208 2.626542329788208
Loss :  1.808861255645752 2.966069221496582 2.966069221496582
Loss :  1.805290699005127 2.9944491386413574 2.9944491386413574
Loss :  1.8082083463668823 3.0941662788391113 3.0941662788391113
Loss :  1.8111649751663208 3.250023365020752 3.250023365020752
Loss :  1.805023431777954 2.9594335556030273 2.9594335556030273
Loss :  1.8118557929992676 2.584454298019409 2.584454298019409
Loss :  1.808789849281311 2.541593313217163 2.541593313217163
Loss :  1.809524655342102 2.600177764892578 2.600177764892578
Loss :  1.810357928276062 2.597388744354248 2.597388744354248
Loss :  1.8026381731033325 2.615325927734375 2.615325927734375
  batch 20 loss: 1.8026381731033325, 2.615325927734375, 2.615325927734375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8158526420593262 2.8265960216522217 2.8265960216522217
Loss :  1.8131790161132812 2.8087546825408936 2.8087546825408936
Loss :  1.8079800605773926 2.569936513900757 2.569936513900757
Loss :  1.81629478931427 3.1587934494018555 3.1587934494018555
Loss :  1.8156112432479858 3.288154363632202 3.288154363632202
Loss :  1.8079783916473389 3.3478238582611084 3.3478238582611084
Loss :  1.8128681182861328 3.0739586353302 3.0739586353302
Loss :  1.8053056001663208 3.110107898712158 3.110107898712158
Loss :  1.8114209175109863 2.9573357105255127 2.9573357105255127
Loss :  1.8034120798110962 3.233808994293213 3.233808994293213
Loss :  1.8087860345840454 3.141618013381958 3.141618013381958
Loss :  1.8053689002990723 3.272287607192993 3.272287607192993
Loss :  1.8110324144363403 3.2529594898223877 3.2529594898223877
Loss :  1.806815266609192 3.53309965133667 3.53309965133667
Loss :  1.8125674724578857 3.201573371887207 3.201573371887207
Loss :  1.8072682619094849 3.332937002182007 3.332937002182007
Loss :  1.8119972944259644 3.7156713008880615 3.7156713008880615
Loss :  1.8050549030303955 2.7493255138397217 2.7493255138397217
Loss :  1.8111463785171509 2.724238872528076 2.724238872528076
Loss :  1.802727222442627 2.7917983531951904 2.7917983531951904
  batch 40 loss: 1.802727222442627, 2.7917983531951904, 2.7917983531951904
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8075021505355835 3.9529500007629395 3.9529500007629395
Loss :  1.805281400680542 3.3434741497039795 3.3434741497039795
Loss :  1.8105639219284058 3.1299097537994385 3.1299097537994385
Loss :  1.8007774353027344 3.250515937805176 3.250515937805176
Loss :  1.811091423034668 3.077777147293091 3.077777147293091
Loss :  1.8020390272140503 3.138263702392578 3.138263702392578
Loss :  1.800611138343811 2.6491150856018066 2.6491150856018066
Loss :  1.8073309659957886 2.6025280952453613 2.6025280952453613
Loss :  1.7911475896835327 2.7295851707458496 2.7295851707458496
Loss :  1.80502450466156 2.926162004470825 2.926162004470825
Loss :  1.798074722290039 3.2670085430145264 3.2670085430145264
Loss :  1.8077975511550903 3.1533455848693848 3.1533455848693848
Loss :  1.80197274684906 2.9288527965545654 2.9288527965545654
Loss :  1.8057256937026978 2.8461735248565674 2.8461735248565674
Loss :  1.8044580221176147 3.1737523078918457 3.1737523078918457
Loss :  1.8096911907196045 3.1590538024902344 3.1590538024902344
Loss :  1.8050400018692017 3.0291950702667236 3.0291950702667236
Loss :  1.8107640743255615 2.956925868988037 2.956925868988037
Loss :  1.8155903816223145 2.952397108078003 2.952397108078003
Loss :  1.81361985206604 2.9556992053985596 2.9556992053985596
  batch 60 loss: 1.81361985206604, 2.9556992053985596, 2.9556992053985596
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8086408376693726 3.18326473236084 3.18326473236084
Loss :  1.8077641725540161 3.7286946773529053 3.7286946773529053
Loss :  1.8091645240783691 3.0391006469726562 3.0391006469726562
Loss :  1.8050732612609863 3.5977425575256348 3.5977425575256348
Loss :  1.8065177202224731 2.9278066158294678 2.9278066158294678
Loss :  1.773914098739624 4.260056495666504 4.260056495666504
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.7738741636276245 4.320730209350586 4.320730209350586
Loss :  1.7690972089767456 4.122687816619873 4.122687816619873
Loss :  1.7921631336212158 4.129679203033447 4.129679203033447
Total LOSS train 3.0628147051884578 valid 4.2082884311676025
CE LOSS train 1.8073158447559063 valid 0.44804078340530396
Contrastive LOSS train 3.0628147051884578 valid 1.0324198007583618
EPOCH 196:
Loss :  1.81194007396698 3.1982922554016113 3.1982922554016113
Loss :  1.802229881286621 3.1398305892944336 3.1398305892944336
Loss :  1.8077958822250366 2.7525322437286377 2.7525322437286377
Loss :  1.8104419708251953 3.038313627243042 3.038313627243042
Loss :  1.8066872358322144 3.3474671840667725 3.3474671840667725
Loss :  1.8101580142974854 3.6546096801757812 3.6546096801757812
Loss :  1.7997115850448608 3.396113395690918 3.396113395690918
Loss :  1.801498293876648 2.7381749153137207 2.7381749153137207
Loss :  1.803340196609497 2.6916022300720215 2.6916022300720215
Loss :  1.7989453077316284 3.143402099609375 3.143402099609375
Loss :  1.8090980052947998 3.0949015617370605 3.0949015617370605
Loss :  1.804753303527832 3.003281593322754 3.003281593322754
Loss :  1.8083792924880981 3.175398111343384 3.175398111343384
Loss :  1.8112868070602417 3.293888807296753 3.293888807296753
Loss :  1.8042484521865845 3.1104960441589355 3.1104960441589355
Loss :  1.8092939853668213 3.2184221744537354 3.2184221744537354
Loss :  1.8078296184539795 3.6250622272491455 3.6250622272491455
Loss :  1.8077024221420288 3.635075330734253 3.635075330734253
Loss :  1.8081458806991577 3.519853353500366 3.519853353500366
Loss :  1.8009251356124878 2.6295762062072754 2.6295762062072754
  batch 20 loss: 1.8009251356124878, 2.6295762062072754, 2.6295762062072754
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8135461807250977 2.8776791095733643 2.8776791095733643
Loss :  1.810403823852539 3.7348272800445557 3.7348272800445557
Loss :  1.8050867319107056 3.4939539432525635 3.4939539432525635
Loss :  1.8135316371917725 3.385941743850708 3.385941743850708
Loss :  1.8108000755310059 3.117556571960449 3.117556571960449
Loss :  1.8047364950180054 3.310863494873047 3.310863494873047
Loss :  1.8114725351333618 3.389174461364746 3.389174461364746
Loss :  1.8023120164871216 3.6441969871520996 3.6441969871520996
Loss :  1.810421109199524 3.2170259952545166 3.2170259952545166
Loss :  1.799918532371521 3.372194766998291 3.372194766998291
Loss :  1.8072971105575562 2.9758758544921875 2.9758758544921875
Loss :  1.8032135963439941 3.1819491386413574 3.1819491386413574
Loss :  1.8074116706848145 3.2716753482818604 3.2716753482818604
Loss :  1.8049012422561646 3.4583323001861572 3.4583323001861572
Loss :  1.8105813264846802 3.539478063583374 3.539478063583374
Loss :  1.8055874109268188 3.753384590148926 3.753384590148926
Loss :  1.809629201889038 3.4347872734069824 3.4347872734069824
Loss :  1.8036580085754395 2.9900214672088623 2.9900214672088623
Loss :  1.8095370531082153 2.50014066696167 2.50014066696167
Loss :  1.803059458732605 2.540741443634033 2.540741443634033
  batch 40 loss: 1.803059458732605, 2.540741443634033, 2.540741443634033
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8081932067871094 3.0517303943634033 3.0517303943634033
Loss :  1.8044054508209229 2.883225440979004 2.883225440979004
Loss :  1.8097456693649292 3.0161919593811035 3.0161919593811035
Loss :  1.7999886274337769 2.904921770095825 2.904921770095825
Loss :  1.8113664388656616 2.4139959812164307 2.4139959812164307
Loss :  1.8031911849975586 2.815363645553589 2.815363645553589
Loss :  1.8028199672698975 2.5392801761627197 2.5392801761627197
Loss :  1.8074731826782227 3.0585238933563232 3.0585238933563232
Loss :  1.796548843383789 2.9617488384246826 2.9617488384246826
Loss :  1.8034465312957764 3.1501028537750244 3.1501028537750244
Loss :  1.7995223999023438 3.070812702178955 3.070812702178955
Loss :  1.809890627861023 3.116398572921753 3.116398572921753
Loss :  1.8037800788879395 3.2579896450042725 3.2579896450042725
Loss :  1.8036686182022095 2.935091972351074 2.935091972351074
Loss :  1.8032487630844116 2.9200944900512695 2.9200944900512695
Loss :  1.8076956272125244 2.9667258262634277 2.9667258262634277
Loss :  1.8040437698364258 3.1109817028045654 3.1109817028045654
Loss :  1.8092714548110962 3.60420298576355 3.60420298576355
Loss :  1.8125137090682983 3.7290420532226562 3.7290420532226562
Loss :  1.8099297285079956 3.196150779724121 3.196150779724121
  batch 60 loss: 1.8099297285079956, 3.196150779724121, 3.196150779724121
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8060897588729858 2.987759590148926 2.987759590148926
Loss :  1.8055685758590698 2.693573474884033 2.693573474884033
Loss :  1.8079230785369873 2.421889543533325 2.421889543533325
Loss :  1.8035144805908203 3.00736665725708 3.00736665725708
Loss :  1.806162714958191 2.288275718688965 2.288275718688965
Loss :  1.7814582586288452 4.308773994445801 4.308773994445801
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3, 4], device='cuda:0')
Loss :  1.780022144317627 4.403939247131348 4.403939247131348
Loss :  1.7757234573364258 4.242814540863037 4.242814540863037
Loss :  1.7956945896148682 4.277015686035156 4.277015686035156
Total LOSS train 3.118423612301166 valid 4.3081358671188354
CE LOSS train 1.8063310623168944 valid 0.44892364740371704
Contrastive LOSS train 3.118423612301166 valid 1.069253921508789
EPOCH 197:
Loss :  1.8081055879592896 2.7127685546875 2.7127685546875
Loss :  1.7972959280014038 3.136934995651245 3.136934995651245
Loss :  1.8043415546417236 3.0308754444122314 3.0308754444122314
Loss :  1.8072260618209839 3.318523645401001 3.318523645401001
Loss :  1.8043745756149292 3.2998626232147217 3.2998626232147217
Loss :  1.8081077337265015 3.386131763458252 3.386131763458252
Loss :  1.7970720529556274 3.022216558456421 3.022216558456421
Loss :  1.7985901832580566 3.4879045486450195 3.4879045486450195
Loss :  1.8031773567199707 2.906679391860962 2.906679391860962
Loss :  1.7969963550567627 2.7928500175476074 2.7928500175476074
Loss :  1.807443618774414 3.4784865379333496 3.4784865379333496
Loss :  1.803086280822754 3.3458988666534424 3.3458988666534424
Loss :  1.8077642917633057 3.480487585067749 3.480487585067749
Loss :  1.8098095655441284 3.6455557346343994 3.6455557346343994
Loss :  1.8041949272155762 3.3626363277435303 3.3626363277435303
Loss :  1.8074935674667358 2.767401933670044 2.767401933670044
Loss :  1.806065320968628 2.591752767562866 2.591752767562866
Loss :  1.8055692911148071 2.421699047088623 2.421699047088623
Loss :  1.8067634105682373 2.4561750888824463 2.4561750888824463
Loss :  1.7992552518844604 2.408426523208618 2.408426523208618
  batch 20 loss: 1.7992552518844604, 2.408426523208618, 2.408426523208618
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8114840984344482 2.303382396697998 2.303382396697998
Loss :  1.8087854385375977 2.4838902950286865 2.4838902950286865
Loss :  1.803235411643982 2.357896327972412 2.357896327972412
Loss :  1.8127429485321045 2.489142417907715 2.489142417907715
Loss :  1.8086944818496704 2.695530652999878 2.695530652999878
Loss :  1.8017934560775757 2.7218263149261475 2.7218263149261475
Loss :  1.8090388774871826 2.747581958770752 2.747581958770752
Loss :  1.799484133720398 2.4155590534210205 2.4155590534210205
Loss :  1.8100700378417969 3.4567408561706543 3.4567408561706543
Loss :  1.7960007190704346 3.5431036949157715 3.5431036949157715
Loss :  1.80509352684021 3.107166290283203 3.107166290283203
Loss :  1.7983025312423706 3.004406213760376 3.004406213760376
Loss :  1.8042142391204834 2.843796730041504 2.843796730041504
Loss :  1.8028168678283691 3.1783647537231445 3.1783647537231445
Loss :  1.8070781230926514 3.8070802688598633 3.8070802688598633
Loss :  1.8050116300582886 3.6931445598602295 3.6931445598602295
Loss :  1.8078685998916626 3.27466082572937 3.27466082572937
Loss :  1.801212191581726 3.4379875659942627 3.4379875659942627
Loss :  1.8073554039001465 3.4841058254241943 3.4841058254241943
Loss :  1.801167607307434 2.83842134475708 2.83842134475708
  batch 40 loss: 1.801167607307434, 2.83842134475708, 2.83842134475708
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8068578243255615 2.990762233734131 2.990762233734131
Loss :  1.802910327911377 3.586158275604248 3.586158275604248
Loss :  1.806105613708496 3.435765504837036 3.435765504837036
Loss :  1.7986562252044678 3.7104971408843994 3.7104971408843994
Loss :  1.8085827827453613 3.4405581951141357 3.4405581951141357
Loss :  1.8028196096420288 3.550733804702759 3.550733804702759
Loss :  1.8019989728927612 3.03008770942688 3.03008770942688
Loss :  1.8069010972976685 3.218867301940918 3.218867301940918
Loss :  1.7970654964447021 3.01271390914917 3.01271390914917
Loss :  1.8038828372955322 3.0565950870513916 3.0565950870513916
Loss :  1.8007007837295532 3.017564535140991 3.017564535140991
Loss :  1.8105417490005493 3.1080009937286377 3.1080009937286377
Loss :  1.8054604530334473 2.6384103298187256 2.6384103298187256
Loss :  1.8054972887039185 2.6840641498565674 2.6840641498565674
Loss :  1.8036936521530151 3.1903345584869385 3.1903345584869385
Loss :  1.8056720495224 3.811103343963623 3.811103343963623
Loss :  1.8034336566925049 3.8892972469329834 3.8892972469329834
Loss :  1.8080613613128662 3.5830650329589844 3.5830650329589844
Loss :  1.8113713264465332 3.5441110134124756 3.5441110134124756
Loss :  1.806402564048767 3.088674306869507 3.088674306869507
  batch 60 loss: 1.806402564048767, 3.088674306869507, 3.088674306869507
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8033757209777832 3.2643332481384277 3.2643332481384277
Loss :  1.8012710809707642 2.73311448097229 2.73311448097229
Loss :  1.8039624691009521 2.5965936183929443 2.5965936183929443
Loss :  1.7989575862884521 2.6245219707489014 2.6245219707489014
Loss :  1.802871584892273 2.5663881301879883 2.5663881301879883
Loss :  1.7892162799835205 4.32450008392334 4.32450008392334
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3, 4], device='cuda:0')
Loss :  1.786697268486023 4.369293689727783 4.369293689727783
Loss :  1.7846156358718872 4.365828990936279 4.365828990936279
Loss :  1.7976843118667603 4.319134712219238 4.319134712219238
Total LOSS train 3.0816826526935284 valid 4.34468936920166
CE LOSS train 1.8044805746812087 valid 0.44942107796669006
Contrastive LOSS train 3.0816826526935284 valid 1.0797836780548096
EPOCH 198:
Loss :  1.805749535560608 2.8394501209259033 2.8394501209259033
Loss :  1.7970975637435913 2.885653495788574 2.885653495788574
Loss :  1.8033711910247803 2.7087018489837646 2.7087018489837646
Loss :  1.8068909645080566 2.7265734672546387 2.7265734672546387
Loss :  1.804092526435852 3.1646692752838135 3.1646692752838135
Loss :  1.8071556091308594 3.447572946548462 3.447572946548462
Loss :  1.7965118885040283 3.3613877296447754 3.3613877296447754
Loss :  1.7983647584915161 3.769709587097168 3.769709587097168
Loss :  1.8003119230270386 3.8263652324676514 3.8263652324676514
Loss :  1.7962608337402344 3.225334405899048 3.225334405899048
Loss :  1.805590271949768 3.4434499740600586 3.4434499740600586
Loss :  1.803556203842163 4.032784938812256 4.032784938812256
Loss :  1.8075062036514282 3.7281925678253174 3.7281925678253174
Loss :  1.8109114170074463 3.3347647190093994 3.3347647190093994
Loss :  1.8046159744262695 2.712570905685425 2.712570905685425
Loss :  1.8083640336990356 2.5056350231170654 2.5056350231170654
Loss :  1.8075677156448364 2.6613383293151855 2.6613383293151855
Loss :  1.8085194826126099 3.1870546340942383 3.1870546340942383
Loss :  1.8084169626235962 2.4496726989746094 2.4496726989746094
Loss :  1.802045226097107 2.4330039024353027 2.4330039024353027
  batch 20 loss: 1.802045226097107, 2.4330039024353027, 2.4330039024353027
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.815361499786377 2.383502244949341 2.383502244949341
Loss :  1.8121956586837769 2.6511025428771973 2.6511025428771973
Loss :  1.8072260618209839 2.6614086627960205 2.6614086627960205
Loss :  1.81450355052948 3.0101940631866455 3.0101940631866455
Loss :  1.813335657119751 3.1572611331939697 3.1572611331939697
Loss :  1.805703043937683 2.6509780883789062 2.6509780883789062
Loss :  1.8107222318649292 2.8677663803100586 2.8677663803100586
Loss :  1.802614450454712 2.854278087615967 2.854278087615967
Loss :  1.8103655576705933 3.4071550369262695 3.4071550369262695
Loss :  1.8015285730361938 3.0039613246917725 3.0039613246917725
Loss :  1.807822346687317 3.069596767425537 3.069596767425537
Loss :  1.8032708168029785 2.9186670780181885 2.9186670780181885
Loss :  1.8085781335830688 2.5009853839874268 2.5009853839874268
Loss :  1.8057152032852173 2.592092275619507 2.592092275619507
Loss :  1.809427261352539 2.7334861755371094 2.7334861755371094
Loss :  1.8059101104736328 2.713796377182007 2.713796377182007
Loss :  1.8088070154190063 3.0007452964782715 3.0007452964782715
Loss :  1.8015586137771606 2.5655014514923096 2.5655014514923096
Loss :  1.807408332824707 2.924344778060913 2.924344778060913
Loss :  1.800279974937439 2.902941942214966 2.902941942214966
  batch 40 loss: 1.800279974937439, 2.902941942214966, 2.902941942214966
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8054264783859253 2.8796229362487793 2.8796229362487793
Loss :  1.802039623260498 3.277693033218384 3.277693033218384
Loss :  1.805509090423584 3.2105581760406494 3.2105581760406494
Loss :  1.7985765933990479 3.5220835208892822 3.5220835208892822
Loss :  1.8078593015670776 3.2185473442077637 3.2185473442077637
Loss :  1.8009172677993774 3.161255359649658 3.161255359649658
Loss :  1.8005784749984741 2.9764750003814697 2.9764750003814697
Loss :  1.8051629066467285 2.976142168045044 2.976142168045044
Loss :  1.7948046922683716 2.9364211559295654 2.9364211559295654
Loss :  1.8035461902618408 2.724533796310425 2.724533796310425
Loss :  1.7996975183486938 3.1135330200195312 3.1135330200195312
Loss :  1.8099499940872192 3.080054521560669 3.080054521560669
Loss :  1.8050297498703003 2.889507293701172 2.889507293701172
Loss :  1.8065239191055298 2.8893654346466064 2.8893654346466064
Loss :  1.805709958076477 2.5725698471069336 2.5725698471069336
Loss :  1.811043620109558 2.2546610832214355 2.2546610832214355
Loss :  1.8066304922103882 2.5501928329467773 2.5501928329467773
Loss :  1.8107705116271973 2.4874823093414307 2.4874823093414307
Loss :  1.8148128986358643 3.200096368789673 3.200096368789673
Loss :  1.81158447265625 2.715892791748047 2.715892791748047
  batch 60 loss: 1.81158447265625, 2.715892791748047, 2.715892791748047
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8071707487106323 2.5562756061553955 2.5562756061553955
Loss :  1.8062344789505005 2.5253257751464844 2.5253257751464844
Loss :  1.8076298236846924 2.490290880203247 2.490290880203247
Loss :  1.8036160469055176 2.8330905437469482 2.8330905437469482
Loss :  1.807195782661438 2.308753490447998 2.308753490447998
Loss :  1.7761210203170776 4.357179164886475 4.357179164886475
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3, 4], device='cuda:0')
Loss :  1.775998592376709 4.357124328613281 4.357124328613281
Loss :  1.7716792821884155 4.1943817138671875 4.1943817138671875
Loss :  1.7905950546264648 4.123194694519043 4.123194694519043
Total LOSS train 2.9286780797518217 valid 4.257969975471497
CE LOSS train 1.8057423848372238 valid 0.4476487636566162
Contrastive LOSS train 2.9286780797518217 valid 1.0307986736297607
EPOCH 199:
Loss :  1.8090182542800903 2.769779920578003 2.769779920578003
Loss :  1.7996265888214111 2.930304765701294 2.930304765701294
Loss :  1.8062962293624878 2.775059223175049 2.775059223175049
Loss :  1.809467077255249 3.1365063190460205 3.1365063190460205
Loss :  1.8065370321273804 2.5873727798461914 2.5873727798461914
Loss :  1.8104571104049683 2.733063220977783 2.733063220977783
Loss :  1.8008782863616943 2.566668748855591 2.566668748855591
Loss :  1.8021906614303589 2.6496171951293945 2.6496171951293945
Loss :  1.80393648147583 3.0489425659179688 3.0489425659179688
Loss :  1.800473690032959 2.723879098892212 2.723879098892212
Loss :  1.8085963726043701 2.8874778747558594 2.8874778747558594
Loss :  1.8050057888031006 3.098189353942871 3.098189353942871
Loss :  1.8083906173706055 3.143562078475952 3.143562078475952
Loss :  1.8114566802978516 2.9107041358947754 2.9107041358947754
Loss :  1.8045464754104614 3.0005064010620117 3.0005064010620117
Loss :  1.8090426921844482 2.907803535461426 2.907803535461426
Loss :  1.8070948123931885 2.8021223545074463 2.8021223545074463
Loss :  1.8063157796859741 2.7562665939331055 2.7562665939331055
Loss :  1.8067569732666016 2.916472911834717 2.916472911834717
Loss :  1.7993792295455933 2.8327794075012207 2.8327794075012207
  batch 20 loss: 1.7993792295455933, 2.8327794075012207, 2.8327794075012207
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8115042448043823 3.224074363708496 3.224074363708496
Loss :  1.8092068433761597 3.0983567237854004 3.0983567237854004
Loss :  1.805134892463684 3.0993926525115967 3.0993926525115967
Loss :  1.8141459226608276 3.1697423458099365 3.1697423458099365
Loss :  1.8109965324401855 2.903245210647583 2.903245210647583
Loss :  1.8062188625335693 3.029320001602173 3.029320001602173
Loss :  1.8112777471542358 3.6745758056640625 3.6745758056640625
Loss :  1.8033137321472168 3.4514663219451904 3.4514663219451904
Loss :  1.8103196620941162 3.3241779804229736 3.3241779804229736
Loss :  1.8021522760391235 2.726154088973999 2.726154088973999
Loss :  1.807409405708313 3.137383460998535 3.137383460998535
Loss :  1.8039788007736206 3.3992791175842285 3.3992791175842285
Loss :  1.8084934949874878 3.512592077255249 3.512592077255249
Loss :  1.8069911003112793 3.645625114440918 3.645625114440918
Loss :  1.811521291732788 3.252100706100464 3.252100706100464
Loss :  1.8065602779388428 3.3373937606811523 3.3373937606811523
Loss :  1.8105686902999878 3.7130346298217773 3.7130346298217773
Loss :  1.805974006652832 3.167128801345825 3.167128801345825
Loss :  1.8114583492279053 3.367802858352661 3.367802858352661
Loss :  1.8044816255569458 3.11859393119812 3.11859393119812
  batch 40 loss: 1.8044816255569458, 3.11859393119812, 3.11859393119812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.810072422027588 3.038339376449585 3.038339376449585
Loss :  1.8067547082901 2.859678030014038 2.859678030014038
Loss :  1.8108985424041748 2.6277151107788086 2.6277151107788086
Loss :  1.8023923635482788 3.5068583488464355 3.5068583488464355
Loss :  1.8135613203048706 3.354383945465088 3.354383945465088
Loss :  1.8056190013885498 2.973599672317505 2.973599672317505
Loss :  1.8051294088363647 3.0313024520874023 3.0313024520874023
Loss :  1.8093817234039307 2.850958824157715 2.850958824157715
Loss :  1.8005423545837402 2.714186429977417 2.714186429977417
Loss :  1.8065576553344727 2.9571735858917236 2.9571735858917236
Loss :  1.803031325340271 2.96380877494812 2.96380877494812
Loss :  1.8143949508666992 3.0534110069274902 3.0534110069274902
Loss :  1.8077561855316162 3.1296422481536865 3.1296422481536865
Loss :  1.8100299835205078 2.6792094707489014 2.6792094707489014
Loss :  1.8049572706222534 2.919768810272217 2.919768810272217
Loss :  1.8110061883926392 2.9608521461486816 2.9608521461486816
Loss :  1.8069279193878174 3.200230836868286 3.200230836868286
Loss :  1.809518814086914 3.228095769882202 3.228095769882202
Loss :  1.8136314153671265 3.049118995666504 3.049118995666504
Loss :  1.8098933696746826 2.9311740398406982 2.9311740398406982
  batch 60 loss: 1.8098933696746826, 2.9311740398406982, 2.9311740398406982
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8061621189117432 2.916334867477417 2.916334867477417
Loss :  1.8048852682113647 3.058417558670044 3.058417558670044
Loss :  1.8066285848617554 3.3204262256622314 3.3204262256622314
Loss :  1.8027342557907104 2.6280357837677 2.6280357837677
Loss :  1.80565345287323 2.3328206539154053 2.3328206539154053
Loss :  1.7757796049118042 4.3773698806762695 4.3773698806762695
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3, 4], device='cuda:0')
Loss :  1.775179386138916 4.389246940612793 4.389246940612793
Loss :  1.7716947793960571 4.272565841674805 4.272565841674805
Loss :  1.788365125656128 4.234570026397705 4.234570026397705
Total LOSS train 3.0279086369734545 valid 4.318438172340393
CE LOSS train 1.8071583876243005 valid 0.447091281414032
Contrastive LOSS train 3.0279086369734545 valid 1.0586425065994263
EPOCH 200:
Loss :  1.808171272277832 2.9825081825256348 2.9825081825256348
Loss :  1.7999118566513062 2.9810104370117188 2.9810104370117188
Loss :  1.8053323030471802 2.696352481842041 2.696352481842041
Loss :  1.8083233833312988 2.681513786315918 2.681513786315918
Loss :  1.8062584400177002 2.3850364685058594 2.3850364685058594
Loss :  1.8104016780853271 2.8293404579162598 2.8293404579162598
Loss :  1.8021169900894165 2.866813898086548 2.866813898086548
Loss :  1.8028830289840698 2.7053213119506836 2.7053213119506836
Loss :  1.8039700984954834 2.7214531898498535 2.7214531898498535
Loss :  1.8013343811035156 3.3764567375183105 3.3764567375183105
Loss :  1.8094487190246582 3.3171956539154053 3.3171956539154053
Loss :  1.805140733718872 3.4175074100494385 3.4175074100494385
Loss :  1.80875825881958 3.8859152793884277 3.8859152793884277
Loss :  1.8125971555709839 2.9682393074035645 2.9682393074035645
Loss :  1.8068697452545166 3.606184482574463 3.606184482574463
Loss :  1.8108679056167603 3.442049980163574 3.442049980163574
Loss :  1.809456706047058 3.084645986557007 3.084645986557007
Loss :  1.8094055652618408 3.342628002166748 3.342628002166748
Loss :  1.810646414756775 2.9748153686523438 2.9748153686523438
Loss :  1.8049736022949219 3.193448781967163 3.193448781967163
  batch 20 loss: 1.8049736022949219, 3.193448781967163, 3.193448781967163
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.816097378730774 4.140262126922607 4.140262126922607
Loss :  1.8126834630966187 3.4468917846679688 3.4468917846679688
Loss :  1.8091483116149902 3.484086036682129 3.484086036682129
Loss :  1.8155848979949951 3.3935930728912354 3.3935930728912354
Loss :  1.8150922060012817 3.5405421257019043 3.5405421257019043
Loss :  1.8093534708023071 2.96500563621521 2.96500563621521
Loss :  1.8136762380599976 3.1326591968536377 3.1326591968536377
Loss :  1.805113434791565 3.446284532546997 3.446284532546997
Loss :  1.8118592500686646 3.6156342029571533 3.6156342029571533
Loss :  1.8024626970291138 3.035048484802246 3.035048484802246
Loss :  1.807210087776184 3.133500576019287 3.133500576019287
Loss :  1.8023998737335205 3.1328885555267334 3.1328885555267334
Loss :  1.8056848049163818 2.722667694091797 2.722667694091797
Loss :  1.8038579225540161 3.051165819168091 3.051165819168091
Loss :  1.8073629140853882 3.0839219093322754 3.0839219093322754
Loss :  1.8043322563171387 3.1065561771392822 3.1065561771392822
Loss :  1.8060591220855713 3.223862648010254 3.223862648010254
Loss :  1.7999920845031738 3.2319345474243164 3.2319345474243164
Loss :  1.8058136701583862 3.1013693809509277 3.1013693809509277
Loss :  1.8004510402679443 2.4905688762664795 2.4905688762664795
  batch 40 loss: 1.8004510402679443, 2.4905688762664795, 2.4905688762664795
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8050143718719482 2.7343924045562744 2.7343924045562744
Loss :  1.8013410568237305 2.5915451049804688 2.5915451049804688
Loss :  1.805607795715332 2.7271811962127686 2.7271811962127686
Loss :  1.7978157997131348 2.4489781856536865 2.4489781856536865
Loss :  1.8072044849395752 2.6947720050811768 2.6947720050811768
Loss :  1.8004142045974731 2.9502058029174805 2.9502058029174805
Loss :  1.8000749349594116 2.6588544845581055 2.6588544845581055
Loss :  1.8037497997283936 2.87892746925354 2.87892746925354
Loss :  1.7948236465454102 2.7602319717407227 2.7602319717407227
Loss :  1.8015074729919434 2.5536599159240723 2.5536599159240723
Loss :  1.7973939180374146 2.7068841457366943 2.7068841457366943
Loss :  1.8063011169433594 2.5306355953216553 2.5306355953216553
Loss :  1.802907943725586 2.2876460552215576 2.2876460552215576
Loss :  1.8025778532028198 2.65671968460083 2.65671968460083
Loss :  1.8023267984390259 2.8871328830718994 2.8871328830718994
Loss :  1.8061156272888184 2.716618537902832 2.716618537902832
Loss :  1.8035818338394165 2.848217010498047 2.848217010498047
Loss :  1.8069747686386108 2.4286048412323 2.4286048412323
Loss :  1.8103258609771729 3.2368357181549072 3.2368357181549072
Loss :  1.8069162368774414 2.916853904724121 2.916853904724121
  batch 60 loss: 1.8069162368774414, 2.916853904724121, 2.916853904724121
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8046152591705322 2.8879759311676025 2.8879759311676025
Loss :  1.8020538091659546 3.0548133850097656 3.0548133850097656
Loss :  1.8044941425323486 2.730557918548584 2.730557918548584
Loss :  1.801521897315979 2.5117905139923096 2.5117905139923096
Loss :  1.8045247793197632 2.3908631801605225 2.3908631801605225
Loss :  1.7694916725158691 4.397997856140137 4.397997856140137
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.7684221267700195 4.391870975494385 4.391870975494385
Loss :  1.7649428844451904 4.319252967834473 4.319252967834473
Loss :  1.7814059257507324 4.2071213722229 4.2071213722229
Total LOSS train 2.980427360534668 valid 4.329060792922974
CE LOSS train 1.805712135021503 valid 0.4453514814376831
Contrastive LOSS train 2.980427360534668 valid 1.051780343055725
EPOCH 201:
Loss :  1.805271863937378 2.6803090572357178 2.6803090572357178
Loss :  1.797896385192871 2.954662799835205 2.954662799835205
Loss :  1.8033770322799683 2.5326011180877686 2.5326011180877686
Loss :  1.8061834573745728 2.8328981399536133 2.8328981399536133
Loss :  1.8034238815307617 2.7319579124450684 2.7319579124450684
Loss :  1.8065736293792725 2.934103488922119 2.934103488922119
Loss :  1.799414038658142 3.310551881790161 3.310551881790161
Loss :  1.8005456924438477 3.254880428314209 3.254880428314209
Loss :  1.8013942241668701 2.4803030490875244 2.4803030490875244
Loss :  1.797843337059021 2.333125591278076 2.333125591278076
Loss :  1.806640386581421 3.0198099613189697 3.0198099613189697
Loss :  1.8036824464797974 2.7804887294769287 2.7804887294769287
Loss :  1.806026816368103 2.7872583866119385 2.7872583866119385
Loss :  1.8094584941864014 2.4678914546966553 2.4678914546966553
Loss :  1.8015036582946777 2.505185842514038 2.505185842514038
Loss :  1.806388258934021 2.3773012161254883 2.3773012161254883
Loss :  1.8057695627212524 2.3630902767181396 2.3630902767181396
Loss :  1.8038164377212524 2.431265354156494 2.431265354156494
Loss :  1.80463707447052 2.1623787879943848 2.1623787879943848
Loss :  1.7984997034072876 2.46486759185791 2.46486759185791
  batch 20 loss: 1.7984997034072876, 2.46486759185791, 2.46486759185791
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.809589147567749 2.3198893070220947 2.3198893070220947
Loss :  1.8078912496566772 2.744133949279785 2.744133949279785
Loss :  1.802943229675293 2.234531879425049 2.234531879425049
Loss :  1.8093687295913696 2.3850624561309814 2.3850624561309814
Loss :  1.8083325624465942 2.5236270427703857 2.5236270427703857
Loss :  1.803316593170166 2.244737148284912 2.244737148284912
Loss :  1.808593511581421 2.5106024742126465 2.5106024742126465
Loss :  1.8002182245254517 2.313565969467163 2.313565969467163
Loss :  1.8089596033096313 2.747755289077759 2.747755289077759
Loss :  1.798445463180542 2.6675736904144287 2.6675736904144287
Loss :  1.805472731590271 2.743455648422241 2.743455648422241
Loss :  1.7976149320602417 2.697192430496216 2.697192430496216
Loss :  1.8039172887802124 2.7473814487457275 2.7473814487457275
Loss :  1.8029781579971313 2.6086857318878174 2.6086857318878174
Loss :  1.806262493133545 2.9150543212890625 2.9150543212890625
Loss :  1.803826928138733 2.601344108581543 2.601344108581543
Loss :  1.8053946495056152 2.796283006668091 2.796283006668091
Loss :  1.7980140447616577 3.792571544647217 3.792571544647217
Loss :  1.8050857782363892 3.2969863414764404 3.2969863414764404
Loss :  1.798900842666626 3.059441566467285 3.059441566467285
  batch 40 loss: 1.798900842666626, 3.059441566467285, 3.059441566467285
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8037152290344238 3.2706594467163086 3.2706594467163086
Loss :  1.800028681755066 2.548517942428589 2.548517942428589
Loss :  1.803722620010376 2.7083323001861572 2.7083323001861572
Loss :  1.7971320152282715 2.4173967838287354 2.4173967838287354
Loss :  1.8059022426605225 2.443804979324341 2.443804979324341
Loss :  1.8004674911499023 2.5883564949035645 2.5883564949035645
Loss :  1.80044686794281 2.4550869464874268 2.4550869464874268
Loss :  1.8028616905212402 2.5681309700012207 2.5681309700012207
Loss :  1.7952158451080322 2.7653403282165527 2.7653403282165527
Loss :  1.8018877506256104 2.6247265338897705 2.6247265338897705
Loss :  1.798208236694336 2.6121270656585693 2.6121270656585693
Loss :  1.8074390888214111 2.4126269817352295 2.4126269817352295
Loss :  1.8047287464141846 2.628725290298462 2.628725290298462
Loss :  1.8055740594863892 2.6987550258636475 2.6987550258636475
Loss :  1.8041290044784546 2.563774585723877 2.563774585723877
Loss :  1.8081307411193848 2.9390761852264404 2.9390761852264404
Loss :  1.8057928085327148 2.85807466506958 2.85807466506958
Loss :  1.808994174003601 3.287250518798828 3.287250518798828
Loss :  1.81204354763031 3.3306961059570312 3.3306961059570312
Loss :  1.8084412813186646 3.3682007789611816 3.3682007789611816
  batch 60 loss: 1.8084412813186646, 3.3682007789611816, 3.3682007789611816
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8063651323318481 3.0828704833984375 3.0828704833984375
Loss :  1.8038904666900635 3.1591684818267822 3.1591684818267822
Loss :  1.805531620979309 2.6485865116119385 2.6485865116119385
Loss :  1.8028289079666138 2.838716983795166 2.838716983795166
Loss :  1.8058805465698242 2.241501808166504 2.241501808166504
Loss :  1.7634352445602417 4.425611972808838 4.425611972808838
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3, 4], device='cuda:0')
Loss :  1.7635583877563477 4.40923547744751 4.40923547744751
Loss :  1.7595475912094116 4.242061138153076 4.242061138153076
Loss :  1.7773085832595825 4.233110427856445 4.233110427856445
Total LOSS train 2.714081701865563 valid 4.327504754066467
CE LOSS train 1.8038897129205558 valid 0.44432714581489563
Contrastive LOSS train 2.714081701865563 valid 1.0582776069641113
EPOCH 202:
Loss :  1.8058151006698608 2.7606468200683594 2.7606468200683594
Loss :  1.7994192838668823 2.8358500003814697 2.8358500003814697
Loss :  1.804457664489746 2.893009901046753 2.893009901046753
Loss :  1.8065718412399292 3.0243096351623535 3.0243096351623535
Loss :  1.8036384582519531 2.758150339126587 2.758150339126587
Loss :  1.8076167106628418 2.479329824447632 2.479329824447632
Loss :  1.8004995584487915 2.679448127746582 2.679448127746582
Loss :  1.8013768196105957 2.676319122314453 2.676319122314453
Loss :  1.8016279935836792 2.6817290782928467 2.6817290782928467
Loss :  1.7987738847732544 2.3902578353881836 2.3902578353881836
Loss :  1.807234287261963 2.9298970699310303 2.9298970699310303
Loss :  1.8039122819900513 3.03874135017395 3.03874135017395
Loss :  1.8064693212509155 2.7642173767089844 2.7642173767089844
Loss :  1.8098105192184448 2.879356861114502 2.879356861114502
Loss :  1.8017034530639648 2.533390522003174 2.533390522003174
Loss :  1.80778968334198 2.7511560916900635 2.7511560916900635
Loss :  1.806315541267395 3.0182032585144043 3.0182032585144043
Loss :  1.8044390678405762 3.302056074142456 3.302056074142456
Loss :  1.8047415018081665 2.734778881072998 2.734778881072998
Loss :  1.799888253211975 2.8895792961120605 2.8895792961120605
  batch 20 loss: 1.799888253211975, 2.8895792961120605, 2.8895792961120605
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8100244998931885 2.7775845527648926 2.7775845527648926
Loss :  1.8083758354187012 2.9316394329071045 2.9316394329071045
Loss :  1.8038928508758545 2.76074481010437 2.76074481010437
Loss :  1.8096283674240112 2.8063321113586426 2.8063321113586426
Loss :  1.8091803789138794 2.7585513591766357 2.7585513591766357
Loss :  1.8048770427703857 2.4960052967071533 2.4960052967071533
Loss :  1.8097033500671387 2.8942620754241943 2.8942620754241943
Loss :  1.801781177520752 2.975379467010498 2.975379467010498
Loss :  1.810205101966858 3.157463550567627 3.157463550567627
Loss :  1.8002521991729736 2.7424721717834473 2.7424721717834473
Loss :  1.806289553642273 2.856872081756592 2.856872081756592
Loss :  1.7991260290145874 3.0021541118621826 3.0021541118621826
Loss :  1.8054403066635132 2.9933979511260986 2.9933979511260986
Loss :  1.8049813508987427 2.9218029975891113 2.9218029975891113
Loss :  1.807275414466858 2.679522752761841 2.679522752761841
Loss :  1.8053995370864868 3.0053791999816895 3.0053791999816895
Loss :  1.806955099105835 2.8719570636749268 2.8719570636749268
Loss :  1.8005156517028809 2.6569578647613525 2.6569578647613525
Loss :  1.8069727420806885 2.9622466564178467 2.9622466564178467
Loss :  1.8015278577804565 2.952479600906372 2.952479600906372
  batch 40 loss: 1.8015278577804565, 2.952479600906372, 2.952479600906372
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8058507442474365 2.9452974796295166 2.9452974796295166
Loss :  1.8020943403244019 2.708815336227417 2.708815336227417
Loss :  1.8051131963729858 2.948716163635254 2.948716163635254
Loss :  1.7985948324203491 2.812486171722412 2.812486171722412
Loss :  1.8071480989456177 2.8157286643981934 2.8157286643981934
Loss :  1.8023536205291748 2.8934404850006104 2.8934404850006104
Loss :  1.8020957708358765 2.847466230392456 2.847466230392456
Loss :  1.803786039352417 2.9157919883728027 2.9157919883728027
Loss :  1.7969900369644165 2.7308499813079834 2.7308499813079834
Loss :  1.8024927377700806 2.600090265274048 2.600090265274048
Loss :  1.7991877794265747 3.1917927265167236 3.1917927265167236
Loss :  1.8085606098175049 2.760899066925049 2.760899066925049
Loss :  1.8054579496383667 3.2433290481567383 3.2433290481567383
Loss :  1.8059730529785156 3.0270655155181885 3.0270655155181885
Loss :  1.8040533065795898 3.0055477619171143 3.0055477619171143
Loss :  1.808664321899414 3.0220437049865723 3.0220437049865723
Loss :  1.8064179420471191 2.725583553314209 2.725583553314209
Loss :  1.8089793920516968 2.508439064025879 2.508439064025879
Loss :  1.8122906684875488 2.7516942024230957 2.7516942024230957
Loss :  1.8085073232650757 2.6130666732788086 2.6130666732788086
  batch 60 loss: 1.8085073232650757, 2.6130666732788086, 2.6130666732788086
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.806723713874817 2.6676783561706543 2.6676783561706543
Loss :  1.8038307428359985 2.475353240966797 2.475353240966797
Loss :  1.805774450302124 2.8756911754608154 2.8756911754608154
Loss :  1.8027012348175049 3.080090284347534 3.080090284347534
Loss :  1.8057544231414795 2.978602647781372 2.978602647781372
Loss :  1.7617475986480713 4.4051666259765625 4.4051666259765625
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3, 4], device='cuda:0')
Loss :  1.762162208557129 4.504457950592041 4.504457950592041
Loss :  1.7578160762786865 4.3283257484436035 4.3283257484436035
Loss :  1.7763140201568604 4.227593421936035 4.227593421936035
Total LOSS train 2.836449113258949 valid 4.3663859367370605
CE LOSS train 1.8048292600191556 valid 0.4440785050392151
Contrastive LOSS train 2.836449113258949 valid 1.0568983554840088
EPOCH 203:
Loss :  1.8059941530227661 2.966123342514038 2.966123342514038
Loss :  1.7999894618988037 2.853130340576172 2.853130340576172
Loss :  1.804545521736145 2.523200035095215 2.523200035095215
Loss :  1.8063678741455078 2.613316059112549 2.613316059112549
Loss :  1.8036766052246094 2.632622718811035 2.632622718811035
Loss :  1.8080065250396729 2.8994529247283936 2.8994529247283936
Loss :  1.8011022806167603 2.8947019577026367 2.8947019577026367
Loss :  1.8017826080322266 2.993746280670166 2.993746280670166
Loss :  1.8019160032272339 2.911006450653076 2.911006450653076
Loss :  1.799565076828003 2.9128401279449463 2.9128401279449463
Loss :  1.8076108694076538 2.8767149448394775 2.8767149448394775
Loss :  1.803787112236023 3.077928066253662 3.077928066253662
Loss :  1.8069907426834106 2.961024522781372 2.961024522781372
Loss :  1.8103042840957642 2.9836018085479736 2.9836018085479736
Loss :  1.8025381565093994 3.16337513923645 3.16337513923645
Loss :  1.8087520599365234 2.8417234420776367 2.8417234420776367
Loss :  1.8067808151245117 2.923926830291748 2.923926830291748
Loss :  1.8050322532653809 3.1964993476867676 3.1964993476867676
Loss :  1.805030345916748 2.9491803646087646 2.9491803646087646
Loss :  1.8006871938705444 3.2852180004119873 3.2852180004119873
  batch 20 loss: 1.8006871938705444, 3.2852180004119873, 3.2852180004119873
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8103456497192383 2.828251361846924 2.828251361846924
Loss :  1.8084251880645752 3.2103285789489746 3.2103285789489746
Loss :  1.8040423393249512 3.1738228797912598 3.1738228797912598
Loss :  1.8092756271362305 2.6667520999908447 2.6667520999908447
Loss :  1.8086645603179932 2.8838613033294678 2.8838613033294678
Loss :  1.8047938346862793 3.1491506099700928 3.1491506099700928
Loss :  1.8095790147781372 2.897757053375244 2.897757053375244
Loss :  1.801543116569519 2.9231183528900146 2.9231183528900146
Loss :  1.8103517293930054 3.0609359741210938 3.0609359741210938
Loss :  1.799760103225708 3.2786099910736084 3.2786099910736084
Loss :  1.8060765266418457 3.1462509632110596 3.1462509632110596
Loss :  1.7986620664596558 2.822366952896118 2.822366952896118
Loss :  1.8052514791488647 2.520430564880371 2.520430564880371
Loss :  1.8043484687805176 2.339756965637207 2.339756965637207
Loss :  1.8067437410354614 2.3484432697296143 2.3484432697296143
Loss :  1.8049455881118774 2.4328954219818115 2.4328954219818115
Loss :  1.80625319480896 2.574943780899048 2.574943780899048
Loss :  1.7999192476272583 2.5929291248321533 2.5929291248321533
Loss :  1.8060988187789917 2.995910167694092 2.995910167694092
Loss :  1.8007405996322632 2.8146913051605225 2.8146913051605225
  batch 40 loss: 1.8007405996322632, 2.8146913051605225, 2.8146913051605225
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8048548698425293 3.3000142574310303 3.3000142574310303
Loss :  1.8012194633483887 2.5681769847869873 2.5681769847869873
Loss :  1.8040965795516968 2.5979502201080322 2.5979502201080322
Loss :  1.7979754209518433 2.772272825241089 2.772272825241089
Loss :  1.8063490390777588 2.1620194911956787 2.1620194911956787
Loss :  1.8017390966415405 2.3384206295013428 2.3384206295013428
Loss :  1.8016233444213867 2.7095210552215576 2.7095210552215576
Loss :  1.8030508756637573 2.987868070602417 2.987868070602417
Loss :  1.7968130111694336 3.0266528129577637 3.0266528129577637
Loss :  1.801949143409729 2.895604372024536 2.895604372024536
Loss :  1.7986286878585815 2.6470839977264404 2.6470839977264404
Loss :  1.8083215951919556 2.3870747089385986 2.3870747089385986
Loss :  1.805312991142273 2.404738426208496 2.404738426208496
Loss :  1.8056814670562744 2.7543540000915527 2.7543540000915527
Loss :  1.8037580251693726 2.8448071479797363 2.8448071479797363
Loss :  1.8086732625961304 2.644782543182373 2.644782543182373
Loss :  1.8065881729125977 3.301842451095581 3.301842451095581
Loss :  1.8089776039123535 2.510977029800415 2.510977029800415
Loss :  1.8125091791152954 2.8048713207244873 2.8048713207244873
Loss :  1.8085695505142212 2.7455203533172607 2.7455203533172607
  batch 60 loss: 1.8085695505142212, 2.7455203533172607, 2.7455203533172607
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8071438074111938 2.5991294384002686 2.5991294384002686
Loss :  1.8040026426315308 2.46549654006958 2.46549654006958
Loss :  1.806027889251709 2.508200168609619 2.508200168609619
Loss :  1.8030040264129639 2.5419156551361084 2.5419156551361084
Loss :  1.805990219116211 2.5599148273468018 2.5599148273468018
Loss :  1.7593803405761719 4.450666427612305 4.450666427612305
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3, 4], device='cuda:0')
Loss :  1.7600607872009277 4.458559989929199 4.458559989929199
Loss :  1.7554768323898315 4.273778438568115 4.273778438568115
Loss :  1.7743288278579712 4.170839309692383 4.170839309692383
Total LOSS train 2.803073057761559 valid 4.3384610414505005
CE LOSS train 1.8047560123296884 valid 0.4435822069644928
Contrastive LOSS train 2.803073057761559 valid 1.0427098274230957
EPOCH 204:
Loss :  1.8065149784088135 2.9075968265533447 2.9075968265533447
Loss :  1.8005303144454956 2.980154514312744 2.980154514312744
Loss :  1.804841160774231 2.815851926803589 2.815851926803589
Loss :  1.8065576553344727 2.9050605297088623 2.9050605297088623
Loss :  1.8040049076080322 2.8540730476379395 2.8540730476379395
Loss :  1.8081896305084229 2.8911499977111816 2.8911499977111816
Loss :  1.801316499710083 2.728419780731201 2.728419780731201
Loss :  1.802034854888916 2.4571449756622314 2.4571449756622314
Loss :  1.8018032312393188 2.322667360305786 2.322667360305786
Loss :  1.7994484901428223 2.5802528858184814 2.5802528858184814
Loss :  1.8076356649398804 2.567188024520874 2.567188024520874
Loss :  1.803706169128418 2.6421306133270264 2.6421306133270264
Loss :  1.8068033456802368 2.5987675189971924 2.5987675189971924
Loss :  1.8100242614746094 2.490076780319214 2.490076780319214
Loss :  1.801782250404358 2.690316677093506 2.690316677093506
Loss :  1.8082003593444824 2.834007978439331 2.834007978439331
Loss :  1.806391716003418 3.0434985160827637 3.0434985160827637
Loss :  1.804663062095642 2.779120683670044 2.779120683670044
Loss :  1.8046886920928955 2.781822443008423 2.781822443008423
Loss :  1.8001381158828735 2.8562185764312744 2.8562185764312744
  batch 20 loss: 1.8001381158828735, 2.8562185764312744, 2.8562185764312744
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.810062050819397 3.0302305221557617 3.0302305221557617
Loss :  1.8081958293914795 3.432744264602661 3.432744264602661
Loss :  1.803788185119629 2.9209542274475098 2.9209542274475098
Loss :  1.8091672658920288 2.6851298809051514 2.6851298809051514
Loss :  1.8086326122283936 2.7531895637512207 2.7531895637512207
Loss :  1.8048269748687744 2.5010485649108887 2.5010485649108887
Loss :  1.8096766471862793 3.1683290004730225 3.1683290004730225
Loss :  1.801546573638916 2.7568955421447754 2.7568955421447754
Loss :  1.810416579246521 2.7361652851104736 2.7361652851104736
Loss :  1.7999851703643799 2.477027416229248 2.477027416229248
Loss :  1.8062987327575684 2.9224870204925537 2.9224870204925537
Loss :  1.7987794876098633 2.7080225944519043 2.7080225944519043
Loss :  1.805443286895752 3.1556901931762695 3.1556901931762695
Loss :  1.804549217224121 3.2120754718780518 3.2120754718780518
Loss :  1.806741714477539 3.0408873558044434 3.0408873558044434
Loss :  1.8052196502685547 3.0973894596099854 3.0973894596099854
Loss :  1.806404948234558 3.1922035217285156 3.1922035217285156
Loss :  1.800035834312439 2.872687339782715 2.872687339782715
Loss :  1.8061230182647705 3.3545544147491455 3.3545544147491455
Loss :  1.8007780313491821 2.8802876472473145 2.8802876472473145
  batch 40 loss: 1.8007780313491821, 2.8802876472473145, 2.8802876472473145
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8049113750457764 2.9653446674346924 2.9653446674346924
Loss :  1.801269292831421 3.282273292541504 3.282273292541504
Loss :  1.8040467500686646 3.274364471435547 3.274364471435547
Loss :  1.7979774475097656 2.847548007965088 2.847548007965088
Loss :  1.8063230514526367 2.7475240230560303 2.7475240230560303
Loss :  1.8016911745071411 2.8174057006835938 2.8174057006835938
Loss :  1.8013375997543335 3.060446262359619 3.060446262359619
Loss :  1.8028770685195923 2.8545479774475098 2.8545479774475098
Loss :  1.7964587211608887 2.6169772148132324 2.6169772148132324
Loss :  1.8016531467437744 3.2692599296569824 3.2692599296569824
Loss :  1.7982370853424072 3.2870934009552 3.2870934009552
Loss :  1.8078546524047852 2.9850475788116455 2.9850475788116455
Loss :  1.8050538301467896 3.2416679859161377 3.2416679859161377
Loss :  1.805517554283142 2.8731114864349365 2.8731114864349365
Loss :  1.8034578561782837 3.1565656661987305 3.1565656661987305
Loss :  1.8079744577407837 3.042102098464966 3.042102098464966
Loss :  1.8062642812728882 2.879016637802124 2.879016637802124
Loss :  1.8085441589355469 2.7538485527038574 2.7538485527038574
Loss :  1.81201171875 3.259742259979248 3.259742259979248
Loss :  1.807651162147522 2.904900312423706 2.904900312423706
  batch 60 loss: 1.807651162147522, 2.904900312423706, 2.904900312423706
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.806596040725708 3.018038034439087 3.018038034439087
Loss :  1.8031527996063232 3.2744879722595215 3.2744879722595215
Loss :  1.8054810762405396 3.3879525661468506 3.3879525661468506
Loss :  1.8020408153533936 3.2838447093963623 3.2838447093963623
Loss :  1.8049582242965698 2.372257947921753 2.372257947921753
Loss :  1.7625365257263184 4.384328365325928 4.384328365325928
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3, 4], device='cuda:0')
Loss :  1.7628692388534546 4.389133930206299 4.389133930206299
Loss :  1.7584306001663208 4.265969276428223 4.265969276428223
Loss :  1.7771492004394531 4.352720260620117 4.352720260620117
Total LOSS train 2.908905964631301 valid 4.348037958145142
CE LOSS train 1.804604438635019 valid 0.4442873001098633
Contrastive LOSS train 2.908905964631301 valid 1.0881800651550293
EPOCH 205:
Loss :  1.8054033517837524 2.2155611515045166 2.2155611515045166
Loss :  1.7997169494628906 2.8470230102539062 2.8470230102539062
Loss :  1.8037527799606323 2.440199613571167 2.440199613571167
Loss :  1.805557131767273 2.5205600261688232 2.5205600261688232
Loss :  1.8032313585281372 2.352921485900879 2.352921485900879
Loss :  1.8074893951416016 2.5211668014526367 2.5211668014526367
Loss :  1.8004369735717773 2.2299647331237793 2.2299647331237793
Loss :  1.8014552593231201 2.1231067180633545 2.1231067180633545
Loss :  1.801431655883789 2.2922861576080322 2.2922861576080322
Loss :  1.7991878986358643 2.447244167327881 2.447244167327881
Loss :  1.8077479600906372 2.766716718673706 2.766716718673706
Loss :  1.8037152290344238 3.1253247261047363 3.1253247261047363
Loss :  1.8072125911712646 2.8125016689300537 2.8125016689300537
Loss :  1.8106435537338257 2.5548157691955566 2.5548157691955566
Loss :  1.8022602796554565 2.8767893314361572 2.8767893314361572
Loss :  1.8095389604568481 2.8003437519073486 2.8003437519073486
Loss :  1.807342529296875 2.804316997528076 2.804316997528076
Loss :  1.805688500404358 2.657320737838745 2.657320737838745
Loss :  1.8056038618087769 2.3332180976867676 2.3332180976867676
Loss :  1.8009467124938965 2.5048673152923584 2.5048673152923584
  batch 20 loss: 1.8009467124938965, 2.5048673152923584, 2.5048673152923584
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8106520175933838 2.535254716873169 2.535254716873169
Loss :  1.8087128400802612 2.6991629600524902 2.6991629600524902
Loss :  1.803768515586853 2.933088779449463 2.933088779449463
Loss :  1.8094242811203003 3.372661828994751 3.372661828994751
Loss :  1.8088343143463135 3.3373286724090576 3.3373286724090576
Loss :  1.8048954010009766 3.0318233966827393 3.0318233966827393
Loss :  1.8102447986602783 3.4014945030212402 3.4014945030212402
Loss :  1.8018420934677124 2.687626361846924 2.687626361846924
Loss :  1.8112248182296753 2.635791301727295 2.635791301727295
Loss :  1.800425410270691 2.77644944190979 2.77644944190979
Loss :  1.8068361282348633 2.768977403640747 2.768977403640747
Loss :  1.7991784811019897 2.6786882877349854 2.6786882877349854
Loss :  1.805959939956665 3.067882537841797 3.067882537841797
Loss :  1.8049722909927368 2.674757957458496 2.674757957458496
Loss :  1.8070292472839355 2.8321025371551514 2.8321025371551514
Loss :  1.8057039976119995 2.7837953567504883 2.7837953567504883
Loss :  1.8065762519836426 2.911393642425537 2.911393642425537
Loss :  1.8002588748931885 2.906291961669922 2.906291961669922
Loss :  1.806142807006836 2.9664204120635986 2.9664204120635986
Loss :  1.8009254932403564 3.049967050552368 3.049967050552368
  batch 40 loss: 1.8009254932403564, 3.049967050552368, 3.049967050552368
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8050748109817505 2.974285364151001 2.974285364151001
Loss :  1.8016231060028076 2.865116596221924 2.865116596221924
Loss :  1.8045634031295776 2.6745855808258057 2.6745855808258057
Loss :  1.7981866598129272 2.527107000350952 2.527107000350952
Loss :  1.8066149950027466 2.674851894378662 2.674851894378662
Loss :  1.8017284870147705 2.966217517852783 2.966217517852783
Loss :  1.8014531135559082 2.686919689178467 2.686919689178467
Loss :  1.8032032251358032 2.5365688800811768 2.5365688800811768
Loss :  1.7966041564941406 2.570439577102661 2.570439577102661
Loss :  1.8017991781234741 3.0164871215820312 3.0164871215820312
Loss :  1.7984247207641602 3.098479747772217 3.098479747772217
Loss :  1.8078458309173584 2.9604885578155518 2.9604885578155518
Loss :  1.805098056793213 2.723707914352417 2.723707914352417
Loss :  1.8054895401000977 2.7315497398376465 2.7315497398376465
Loss :  1.8033835887908936 2.856534719467163 2.856534719467163
Loss :  1.8077348470687866 3.7088725566864014 3.7088725566864014
Loss :  1.8064987659454346 3.437953472137451 3.437953472137451
Loss :  1.8086659908294678 3.0132460594177246 3.0132460594177246
Loss :  1.812075138092041 3.339210033416748 3.339210033416748
Loss :  1.807124376296997 2.811079978942871 2.811079978942871
  batch 60 loss: 1.807124376296997, 2.811079978942871, 2.811079978942871
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.80695641040802 2.846165418624878 2.846165418624878
Loss :  1.8030736446380615 3.0451223850250244 3.0451223850250244
Loss :  1.8057619333267212 3.3043198585510254 3.3043198585510254
Loss :  1.8023052215576172 3.469789981842041 3.469789981842041
Loss :  1.8052924871444702 2.6543073654174805 2.6543073654174805
Loss :  1.7593637704849243 4.431135177612305 4.431135177612305
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3, 4], device='cuda:0')
Loss :  1.760032296180725 4.357754707336426 4.357754707336426
Loss :  1.7555371522903442 4.325626850128174 4.325626850128174
Loss :  1.774150013923645 4.0417656898498535 4.0417656898498535
Total LOSS train 2.8118556169363167 valid 4.2890706062316895
CE LOSS train 1.804746963427617 valid 0.44353750348091125
Contrastive LOSS train 2.8118556169363167 valid 1.0104414224624634
EPOCH 206:
Loss :  1.8052542209625244 3.204749345779419 3.204749345779419
Loss :  1.8004326820373535 3.190654993057251 3.190654993057251
Loss :  1.8041876554489136 3.330348014831543 3.330348014831543
Loss :  1.8056269884109497 2.86623215675354 2.86623215675354
Loss :  1.8033099174499512 2.9424431324005127 2.9424431324005127
Loss :  1.8077397346496582 2.7505407333374023 2.7505407333374023
Loss :  1.8008123636245728 3.1105639934539795 3.1105639934539795
Loss :  1.8015292882919312 3.470902681350708 3.470902681350708
Loss :  1.8010355234146118 2.792827606201172 2.792827606201172
Loss :  1.7989219427108765 2.709984064102173 2.709984064102173
Loss :  1.8073844909667969 3.0271501541137695 3.0271501541137695
Loss :  1.8033138513565063 3.5642096996307373 3.5642096996307373
Loss :  1.8069570064544678 3.5600736141204834 3.5600736141204834
Loss :  1.8099720478057861 3.2710089683532715 3.2710089683532715
Loss :  1.800993800163269 3.088855028152466 3.088855028152466
Loss :  1.808942198753357 2.6759865283966064 2.6759865283966064
Loss :  1.806754231452942 2.751483201980591 2.751483201980591
Loss :  1.8046342134475708 2.571406602859497 2.571406602859497
Loss :  1.8045345544815063 2.419642448425293 2.419642448425293
Loss :  1.8001347780227661 2.534431219100952 2.534431219100952
  batch 20 loss: 1.8001347780227661, 2.534431219100952, 2.534431219100952
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8096340894699097 2.5 2.5
Loss :  1.808158040046692 2.5389161109924316 2.5389161109924316
Loss :  1.8031646013259888 2.5221030712127686 2.5221030712127686
Loss :  1.8092427253723145 2.5012004375457764 2.5012004375457764
Loss :  1.8085694313049316 2.9397590160369873 2.9397590160369873
Loss :  1.8049367666244507 2.163771867752075 2.163771867752075
Loss :  1.8104125261306763 2.425490140914917 2.425490140914917
Loss :  1.8020844459533691 2.6454641819000244 2.6454641819000244
Loss :  1.811650276184082 2.6165738105773926 2.6165738105773926
Loss :  1.8010082244873047 3.5281755924224854 3.5281755924224854
Loss :  1.807276725769043 3.2591326236724854 3.2591326236724854
Loss :  1.799709677696228 2.7394649982452393 2.7394649982452393
Loss :  1.8066215515136719 2.8194398880004883 2.8194398880004883
Loss :  1.8060579299926758 3.070068836212158 3.070068836212158
Loss :  1.8074232339859009 3.3140628337860107 3.3140628337860107
Loss :  1.8067594766616821 3.5646049976348877 3.5646049976348877
Loss :  1.8072794675827026 3.0516350269317627 3.0516350269317627
Loss :  1.8012943267822266 2.933143377304077 2.933143377304077
Loss :  1.8069214820861816 3.1417832374572754 3.1417832374572754
Loss :  1.8020691871643066 3.0030860900878906 3.0030860900878906
  batch 40 loss: 1.8020691871643066, 3.0030860900878906, 3.0030860900878906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8062703609466553 2.966277599334717 2.966277599334717
Loss :  1.8029412031173706 2.9038708209991455 2.9038708209991455
Loss :  1.805305004119873 3.013941764831543 3.013941764831543
Loss :  1.799259066581726 2.996366262435913 2.996366262435913
Loss :  1.8073920011520386 2.8540234565734863 2.8540234565734863
Loss :  1.8029758930206299 2.771864652633667 2.771864652633667
Loss :  1.80213463306427 2.6578221321105957 2.6578221321105957
Loss :  1.8040289878845215 3.166614055633545 3.166614055633545
Loss :  1.7971946001052856 3.443519115447998 3.443519115447998
Loss :  1.802310585975647 3.1583075523376465 3.1583075523376465
Loss :  1.799261450767517 3.1657028198242188 3.1657028198242188
Loss :  1.8083373308181763 2.7957496643066406 2.7957496643066406
Loss :  1.8055330514907837 2.4453999996185303 2.4453999996185303
Loss :  1.8062167167663574 2.7901289463043213 2.7901289463043213
Loss :  1.804125428199768 2.8071582317352295 2.8071582317352295
Loss :  1.8080973625183105 2.6483771800994873 2.6483771800994873
Loss :  1.8069531917572021 2.673114538192749 2.673114538192749
Loss :  1.809266448020935 2.5410871505737305 2.5410871505737305
Loss :  1.8122848272323608 2.8535544872283936 2.8535544872283936
Loss :  1.8076066970825195 2.552008867263794 2.552008867263794
  batch 60 loss: 1.8076066970825195, 2.552008867263794, 2.552008867263794
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8073090314865112 2.77241587638855 2.77241587638855
Loss :  1.8035365343093872 3.120133638381958 3.120133638381958
Loss :  1.8060452938079834 3.2148633003234863 3.2148633003234863
Loss :  1.8028345108032227 3.7563793659210205 3.7563793659210205
Loss :  1.8061671257019043 3.806900978088379 3.806900978088379
Loss :  1.7546429634094238 4.447533130645752 4.447533130645752
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3, 4], device='cuda:0')
Loss :  1.7557612657546997 4.417244911193848 4.417244911193848
Loss :  1.7509450912475586 4.309671401977539 4.309671401977539
Loss :  1.769990086555481 4.074463844299316 4.074463844299316
Total LOSS train 2.9382608120258036 valid 4.312228322029114
CE LOSS train 1.805017430965717 valid 0.44249752163887024
Contrastive LOSS train 2.9382608120258036 valid 1.018615961074829
EPOCH 207:
Loss :  1.8056048154830933 3.8486175537109375 3.8486175537109375
Loss :  1.8006465435028076 3.338109016418457 3.338109016418457
Loss :  1.8045871257781982 3.6942975521087646 3.6942975521087646
Loss :  1.8057805299758911 3.6336922645568848 3.6336922645568848
Loss :  1.8035887479782104 4.033899307250977 4.033899307250977
Loss :  1.8079547882080078 3.331171751022339 3.331171751022339
Loss :  1.8011815547943115 3.0287396907806396 3.0287396907806396
Loss :  1.8019763231277466 2.5603420734405518 2.5603420734405518
Loss :  1.8015403747558594 2.9165115356445312 2.9165115356445312
Loss :  1.7997443675994873 2.899555206298828 2.899555206298828
Loss :  1.8079707622528076 3.088944435119629 3.088944435119629
Loss :  1.8034594058990479 2.694072723388672 2.694072723388672
Loss :  1.8075438737869263 2.9396491050720215 2.9396491050720215
Loss :  1.8106504678726196 2.8403422832489014 2.8403422832489014
Loss :  1.8014174699783325 2.676772356033325 2.676772356033325
Loss :  1.8097658157348633 2.884699583053589 2.884699583053589
Loss :  1.8073469400405884 2.372668981552124 2.372668981552124
Loss :  1.8052618503570557 2.6608428955078125 2.6608428955078125
Loss :  1.8050466775894165 2.670593738555908 2.670593738555908
Loss :  1.8007726669311523 2.5749380588531494 2.5749380588531494
  batch 20 loss: 1.8007726669311523, 2.5749380588531494, 2.5749380588531494
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8101301193237305 2.3247878551483154 2.3247878551483154
Loss :  1.8084936141967773 2.9067142009735107 2.9067142009735107
Loss :  1.8034489154815674 2.6875126361846924 2.6875126361846924
Loss :  1.809142827987671 2.7311103343963623 2.7311103343963623
Loss :  1.808666467666626 2.8664021492004395 2.8664021492004395
Loss :  1.8050239086151123 2.651761770248413 2.651761770248413
Loss :  1.810394525527954 2.911583423614502 2.911583423614502
Loss :  1.8020257949829102 2.8445801734924316 2.8445801734924316
Loss :  1.8116569519042969 3.0088918209075928 3.0088918209075928
Loss :  1.8009108304977417 3.2550673484802246 3.2550673484802246
Loss :  1.8072702884674072 3.6069986820220947 3.6069986820220947
Loss :  1.7994513511657715 3.7940523624420166 3.7940523624420166
Loss :  1.8064872026443481 2.8016045093536377 2.8016045093536377
Loss :  1.8056697845458984 3.110703945159912 3.110703945159912
Loss :  1.807248830795288 3.6369378566741943 3.6369378566741943
Loss :  1.8064525127410889 3.5328173637390137 3.5328173637390137
Loss :  1.806943416595459 3.3431894779205322 3.3431894779205322
Loss :  1.8007572889328003 3.616960287094116 3.616960287094116
Loss :  1.8065444231033325 3.4277429580688477 3.4277429580688477
Loss :  1.8016836643218994 3.1652348041534424 3.1652348041534424
  batch 40 loss: 1.8016836643218994, 3.1652348041534424, 3.1652348041534424
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8057763576507568 3.8919730186462402 3.8919730186462402
Loss :  1.8024499416351318 3.438098192214966 3.438098192214966
Loss :  1.8049790859222412 3.4712471961975098 3.4712471961975098
Loss :  1.7989591360092163 2.5443637371063232 2.5443637371063232
Loss :  1.8070390224456787 2.6018803119659424 2.6018803119659424
Loss :  1.802628517150879 3.0816080570220947 3.0816080570220947
Loss :  1.801624059677124 3.42501163482666 3.42501163482666
Loss :  1.8037697076797485 3.8621268272399902 3.8621268272399902
Loss :  1.7967795133590698 3.5839834213256836 3.5839834213256836
Loss :  1.8021477460861206 3.264238119125366 3.264238119125366
Loss :  1.7994072437286377 3.563328504562378 3.563328504562378
Loss :  1.8083118200302124 3.104372262954712 3.104372262954712
Loss :  1.805599570274353 3.0964574813842773 3.0964574813842773
Loss :  1.806687593460083 2.8590664863586426 2.8590664863586426
Loss :  1.8043545484542847 2.4642159938812256 2.4642159938812256
Loss :  1.8081622123718262 2.351384162902832 2.351384162902832
Loss :  1.8073735237121582 2.7484681606292725 2.7484681606292725
Loss :  1.8094871044158936 2.725541353225708 2.725541353225708
Loss :  1.8128955364227295 3.5767877101898193 3.5767877101898193
Loss :  1.8079150915145874 3.5558664798736572 3.5558664798736572
  batch 60 loss: 1.8079150915145874, 3.5558664798736572, 3.5558664798736572
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8076261281967163 2.8366177082061768 2.8366177082061768
Loss :  1.8042080402374268 2.6934869289398193 2.6934869289398193
Loss :  1.8060200214385986 3.1011126041412354 3.1011126041412354
Loss :  1.8030214309692383 3.238940715789795 3.238940715789795
Loss :  1.806497573852539 2.5521740913391113 2.5521740913391113
Loss :  1.7559890747070312 4.436017036437988 4.436017036437988
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3, 4], device='cuda:0')
Loss :  1.756978154182434 4.429934024810791 4.429934024810791
Loss :  1.7520651817321777 4.388823986053467 4.388823986053467
Loss :  1.7712008953094482 4.251091003417969 4.251091003417969
Total LOSS train 3.0853148497067964 valid 4.376466512680054
CE LOSS train 1.8051379130436824 valid 0.44280022382736206
Contrastive LOSS train 3.0853148497067964 valid 1.0627727508544922
EPOCH 208:
Loss :  1.8066017627716064 2.4331648349761963 2.4331648349761963
Loss :  1.8004355430603027 2.84136700630188 2.84136700630188
Loss :  1.8049026727676392 2.4906258583068848 2.4906258583068848
Loss :  1.8063435554504395 2.941601514816284 2.941601514816284
Loss :  1.803828477859497 3.051002264022827 3.051002264022827
Loss :  1.8081752061843872 3.1411068439483643 3.1411068439483643
Loss :  1.8007453680038452 3.037175416946411 3.037175416946411
Loss :  1.8018059730529785 2.676360845565796 2.676360845565796
Loss :  1.8016325235366821 2.4660539627075195 2.4660539627075195
Loss :  1.799228549003601 2.388360023498535 2.388360023498535
Loss :  1.8078892230987549 2.9694340229034424 2.9694340229034424
Loss :  1.803483486175537 3.5692384243011475 3.5692384243011475
Loss :  1.8069154024124146 3.4757089614868164 3.4757089614868164
Loss :  1.8100827932357788 3.0490031242370605 3.0490031242370605
Loss :  1.800884485244751 2.621295928955078 2.621295928955078
Loss :  1.8090413808822632 2.879432439804077 2.879432439804077
Loss :  1.8066960573196411 3.528463125228882 3.528463125228882
Loss :  1.804304838180542 3.4513769149780273 3.4513769149780273
Loss :  1.804699420928955 3.1090261936187744 3.1090261936187744
Loss :  1.7994487285614014 2.8661558628082275 2.8661558628082275
  batch 20 loss: 1.7994487285614014, 2.8661558628082275, 2.8661558628082275
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.809281826019287 3.349571943283081 3.349571943283081
Loss :  1.8078267574310303 2.646698474884033 2.646698474884033
Loss :  1.802322268486023 2.966625213623047 2.966625213623047
Loss :  1.808548927307129 2.588762044906616 2.588762044906616
Loss :  1.808143973350525 3.151731252670288 3.151731252670288
Loss :  1.804059624671936 2.93098783493042 2.93098783493042
Loss :  1.809827446937561 2.825119733810425 2.825119733810425
Loss :  1.8010413646697998 3.373459577560425 3.373459577560425
Loss :  1.8109492063522339 3.6020801067352295 3.6020801067352295
Loss :  1.8002573251724243 3.424405813217163 3.424405813217163
Loss :  1.8069697618484497 3.485473394393921 3.485473394393921
Loss :  1.7988170385360718 2.796266794204712 2.796266794204712
Loss :  1.8056687116622925 2.7935585975646973 2.7935585975646973
Loss :  1.8048412799835205 2.7506930828094482 2.7506930828094482
Loss :  1.8065564632415771 2.6752920150756836 2.6752920150756836
Loss :  1.8058665990829468 2.5198490619659424 2.5198490619659424
Loss :  1.8062677383422852 2.3631269931793213 2.3631269931793213
Loss :  1.8001493215560913 2.2781784534454346 2.2781784534454346
Loss :  1.8060518503189087 2.3117995262145996 2.3117995262145996
Loss :  1.8010061979293823 2.244520902633667 2.244520902633667
  batch 40 loss: 1.8010061979293823, 2.244520902633667, 2.244520902633667
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8052763938903809 2.589179754257202 2.589179754257202
Loss :  1.8019747734069824 2.321124792098999 2.321124792098999
Loss :  1.8043649196624756 2.465885877609253 2.465885877609253
Loss :  1.7984102964401245 2.383268117904663 2.383268117904663
Loss :  1.8066844940185547 2.2937517166137695 2.2937517166137695
Loss :  1.8022491931915283 3.354069232940674 3.354069232940674
Loss :  1.801069974899292 2.953819751739502 2.953819751739502
Loss :  1.803432583808899 3.0644164085388184 3.0644164085388184
Loss :  1.7962557077407837 2.8688278198242188 2.8688278198242188
Loss :  1.8015836477279663 2.5793099403381348 2.5793099403381348
Loss :  1.7986220121383667 2.5473532676696777 2.5473532676696777
Loss :  1.8077303171157837 2.867908477783203 2.867908477783203
Loss :  1.8050466775894165 2.640709400177002 2.640709400177002
Loss :  1.805616021156311 2.9506161212921143 2.9506161212921143
Loss :  1.8038268089294434 2.7491629123687744 2.7491629123687744
Loss :  1.8073066473007202 2.965684413909912 2.965684413909912
Loss :  1.8067365884780884 2.925764322280884 2.925764322280884
Loss :  1.8092142343521118 2.823805570602417 2.823805570602417
Loss :  1.8124287128448486 2.9341094493865967 2.9341094493865967
Loss :  1.807347297668457 2.8740570545196533 2.8740570545196533
  batch 60 loss: 1.807347297668457, 2.8740570545196533, 2.8740570545196533
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8072009086608887 2.650212049484253 2.650212049484253
Loss :  1.8037015199661255 2.5206010341644287 2.5206010341644287
Loss :  1.8059545755386353 2.57015323638916 2.57015323638916
Loss :  1.8025842905044556 2.886648416519165 2.886648416519165
Loss :  1.8060493469238281 2.311075210571289 2.311075210571289
Loss :  1.7537434101104736 4.396171569824219 4.396171569824219
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3, 4], device='cuda:0')
Loss :  1.7549406290054321 4.457086086273193 4.457086086273193
Loss :  1.749938726425171 4.2662787437438965 4.2662787437438965
Loss :  1.7690261602401733 4.2549357414245605 4.2549357414245605
Total LOSS train 2.8331641343923715 valid 4.343618035316467
CE LOSS train 1.8046502626859224 valid 0.44225654006004333
Contrastive LOSS train 2.8331641343923715 valid 1.0637339353561401
EPOCH 209:
Loss :  1.8059440851211548 3.2401018142700195 3.2401018142700195
Loss :  1.7999932765960693 2.9868850708007812 2.9868850708007812
Loss :  1.8042576313018799 2.668677806854248 2.668677806854248
Loss :  1.8056540489196777 2.8285398483276367 2.8285398483276367
Loss :  1.803345799446106 2.772655963897705 2.772655963897705
Loss :  1.8076473474502563 2.7971649169921875 2.7971649169921875
Loss :  1.8003157377243042 2.9960358142852783 2.9960358142852783
Loss :  1.8012843132019043 2.6802475452423096 2.6802475452423096
Loss :  1.8010797500610352 2.8533620834350586 2.8533620834350586
Loss :  1.7987556457519531 2.8514137268066406 2.8514137268066406
Loss :  1.8074324131011963 3.530991315841675 3.530991315841675
Loss :  1.8030141592025757 3.08703875541687 3.08703875541687
Loss :  1.806899905204773 2.6038668155670166 2.6038668155670166
Loss :  1.810025691986084 2.6418850421905518 2.6418850421905518
Loss :  1.8004910945892334 2.6768620014190674 2.6768620014190674
Loss :  1.8087619543075562 3.7360520362854004 3.7360520362854004
Loss :  1.8066105842590332 2.816958427429199 2.816958427429199
Loss :  1.8038984537124634 2.757502555847168 2.757502555847168
Loss :  1.8041987419128418 2.482952117919922 2.482952117919922
Loss :  1.7991278171539307 2.6539883613586426 2.6539883613586426
  batch 20 loss: 1.7991278171539307, 2.6539883613586426, 2.6539883613586426
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8090234994888306 2.560555934906006 2.560555934906006
Loss :  1.8074783086776733 2.657644033432007 2.657644033432007
Loss :  1.8020803928375244 2.628957509994507 2.628957509994507
Loss :  1.807963490486145 3.070599317550659 3.070599317550659
Loss :  1.8071945905685425 3.3123068809509277 3.3123068809509277
Loss :  1.8036547899246216 2.8635716438293457 2.8635716438293457
Loss :  1.8092923164367676 2.526747941970825 2.526747941970825
Loss :  1.8006960153579712 2.415682792663574 2.415682792663574
Loss :  1.8108521699905396 2.8961451053619385 2.8961451053619385
Loss :  1.7991819381713867 2.7165679931640625 2.7165679931640625
Loss :  1.8064193725585938 2.8374457359313965 2.8374457359313965
Loss :  1.7980743646621704 3.0403707027435303 3.0403707027435303
Loss :  1.8053897619247437 3.322251081466675 3.322251081466675
Loss :  1.8036043643951416 2.736055612564087 2.736055612564087
Loss :  1.8059399127960205 2.7965009212493896 2.7965009212493896
Loss :  1.8051575422286987 2.7858352661132812 2.7858352661132812
Loss :  1.805599570274353 2.8564116954803467 2.8564116954803467
Loss :  1.7990785837173462 2.683295249938965 2.683295249938965
Loss :  1.804857850074768 2.703138589859009 2.703138589859009
Loss :  1.799825668334961 2.9186851978302 2.9186851978302
  batch 40 loss: 1.799825668334961, 2.9186851978302, 2.9186851978302
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8042070865631104 2.9611880779266357 2.9611880779266357
Loss :  1.8011215925216675 2.494074821472168 2.494074821472168
Loss :  1.8039331436157227 2.501842737197876 2.501842737197876
Loss :  1.7977465391159058 2.3133370876312256 2.3133370876312256
Loss :  1.8062273263931274 2.3371682167053223 2.3371682167053223
Loss :  1.801835536956787 2.8990867137908936 2.8990867137908936
Loss :  1.8009452819824219 2.6110403537750244 2.6110403537750244
Loss :  1.8032662868499756 2.6076207160949707 2.6076207160949707
Loss :  1.7962948083877563 2.5272583961486816 2.5272583961486816
Loss :  1.801637053489685 2.7532615661621094 2.7532615661621094
Loss :  1.798675298690796 3.1293587684631348 3.1293587684631348
Loss :  1.8077837228775024 3.396176815032959 3.396176815032959
Loss :  1.8051989078521729 2.626343011856079 2.626343011856079
Loss :  1.805640697479248 2.781097650527954 2.781097650527954
Loss :  1.8039699792861938 3.269160509109497 3.269160509109497
Loss :  1.8074891567230225 3.0936477184295654 3.0936477184295654
Loss :  1.806900143623352 3.7626469135284424 3.7626469135284424
Loss :  1.8092948198318481 3.0197360515594482 3.0197360515594482
Loss :  1.812526822090149 3.086665153503418 3.086665153503418
Loss :  1.8073785305023193 2.5667052268981934 2.5667052268981934
  batch 60 loss: 1.8073785305023193, 2.5667052268981934, 2.5667052268981934
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8073036670684814 2.9274938106536865 2.9274938106536865
Loss :  1.8037081956863403 3.071709632873535 3.071709632873535
Loss :  1.8060253858566284 2.913619041442871 2.913619041442871
Loss :  1.8029837608337402 3.2953410148620605 3.2953410148620605
Loss :  1.806351661682129 2.702364921569824 2.702364921569824
Loss :  1.7522821426391602 4.402703762054443 4.402703762054443
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3, 4], device='cuda:0')
Loss :  1.7535388469696045 4.456550598144531 4.456550598144531
Loss :  1.7486412525177002 4.29884147644043 4.29884147644043
Loss :  1.7671302556991577 4.249082088470459 4.249082088470459
Total LOSS train 2.8559984023754414 valid 4.351794481277466
CE LOSS train 1.8042853593826294 valid 0.44178256392478943
Contrastive LOSS train 2.8559984023754414 valid 1.0622705221176147
EPOCH 210:
Loss :  1.805564522743225 2.736973762512207 2.736973762512207
Loss :  1.8002524375915527 3.2570888996124268 3.2570888996124268
Loss :  1.8044792413711548 2.8857269287109375 2.8857269287109375
Loss :  1.805607795715332 3.062159538269043 3.062159538269043
Loss :  1.8035273551940918 2.655586004257202 2.655586004257202
Loss :  1.8076797723770142 2.687830924987793 2.687830924987793
Loss :  1.8006260395050049 2.8902571201324463 2.8902571201324463
Loss :  1.801466941833496 2.9708216190338135 2.9708216190338135
Loss :  1.8014283180236816 2.843625783920288 2.843625783920288
Loss :  1.7991007566452026 2.8946661949157715 2.8946661949157715
Loss :  1.8074462413787842 3.268474817276001 3.268474817276001
Loss :  1.803206205368042 3.6737961769104004 3.6737961769104004
Loss :  1.8068244457244873 3.074077606201172 3.074077606201172
Loss :  1.8099689483642578 2.87983775138855 2.87983775138855
Loss :  1.801200032234192 2.7730600833892822 2.7730600833892822
Loss :  1.8088274002075195 3.119330406188965 3.119330406188965
Loss :  1.806884765625 2.9602859020233154 2.9602859020233154
Loss :  1.804520845413208 2.874049425125122 2.874049425125122
Loss :  1.8047242164611816 2.6967978477478027 2.6967978477478027
Loss :  1.8000634908676147 2.699242115020752 2.699242115020752
  batch 20 loss: 1.8000634908676147, 2.699242115020752, 2.699242115020752
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8097213506698608 2.8918330669403076 2.8918330669403076
Loss :  1.8080506324768066 2.7768938541412354 2.7768938541412354
Loss :  1.8028228282928467 2.449496269226074 2.449496269226074
Loss :  1.8084290027618408 2.7576749324798584 2.7576749324798584
Loss :  1.807775616645813 3.5340728759765625 3.5340728759765625
Loss :  1.8043564558029175 3.610243320465088 3.610243320465088
Loss :  1.8097140789031982 3.432638645172119 3.432638645172119
Loss :  1.8014158010482788 3.7110841274261475 3.7110841274261475
Loss :  1.810986876487732 3.763002634048462 3.763002634048462
Loss :  1.8000906705856323 3.4697422981262207 3.4697422981262207
Loss :  1.8066742420196533 3.8578286170959473 3.8578286170959473
Loss :  1.7986669540405273 3.534795045852661 3.534795045852661
Loss :  1.8057193756103516 3.0307841300964355 3.0307841300964355
Loss :  1.8048222064971924 2.7299890518188477 2.7299890518188477
Loss :  1.8065788745880127 3.0806498527526855 3.0806498527526855
Loss :  1.805872917175293 3.2555317878723145 3.2555317878723145
Loss :  1.8061782121658325 2.723341226577759 2.723341226577759
Loss :  1.8000264167785645 3.0149807929992676 3.0149807929992676
Loss :  1.8057422637939453 3.4324214458465576 3.4324214458465576
Loss :  1.8009594678878784 3.0185930728912354 3.0185930728912354
  batch 40 loss: 1.8009594678878784, 3.0185930728912354, 3.0185930728912354
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8051310777664185 2.83475661277771 2.83475661277771
Loss :  1.8018743991851807 2.7335307598114014 2.7335307598114014
Loss :  1.8042185306549072 2.735341787338257 2.735341787338257
Loss :  1.7984405755996704 3.2273523807525635 3.2273523807525635
Loss :  1.806436538696289 2.743748664855957 2.743748664855957
Loss :  1.8025672435760498 2.7495720386505127 2.7495720386505127
Loss :  1.8014284372329712 2.5186526775360107 2.5186526775360107
Loss :  1.8033075332641602 2.794132947921753 2.794132947921753
Loss :  1.7966939210891724 2.5478227138519287 2.5478227138519287
Loss :  1.801697850227356 2.8869125843048096 2.8869125843048096
Loss :  1.798872470855713 3.0619115829467773 3.0619115829467773
Loss :  1.808141827583313 2.8002843856811523 2.8002843856811523
Loss :  1.8054440021514893 2.5032196044921875 2.5032196044921875
Loss :  1.8061606884002686 2.40598726272583 2.40598726272583
Loss :  1.8039555549621582 2.9147558212280273 2.9147558212280273
Loss :  1.807950496673584 3.000486135482788 3.000486135482788
Loss :  1.8072166442871094 3.253998279571533 3.253998279571533
Loss :  1.8093029260635376 3.055210828781128 3.055210828781128
Loss :  1.8129611015319824 3.7798779010772705 3.7798779010772705
Loss :  1.8075584173202515 3.2749078273773193 3.2749078273773193
  batch 60 loss: 1.8075584173202515, 3.2749078273773193, 3.2749078273773193
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8074976205825806 2.8215155601501465 2.8215155601501465
Loss :  1.8038069009780884 2.641883373260498 2.641883373260498
Loss :  1.8060873746871948 2.9462668895721436 2.9462668895721436
Loss :  1.8028829097747803 3.2250683307647705 3.2250683307647705
Loss :  1.8061543703079224 2.7949111461639404 2.7949111461639404
Loss :  1.7517036199569702 4.4035515785217285 4.4035515785217285
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3, 4], device='cuda:0')
Loss :  1.7530393600463867 4.455606937408447 4.455606937408447
Loss :  1.748024821281433 4.368535041809082 4.368535041809082
Loss :  1.7667858600616455 4.158207416534424 4.158207416534424
Total LOSS train 3.0035599085000846 valid 4.34647524356842
CE LOSS train 1.804673745081975 valid 0.4416964650154114
Contrastive LOSS train 3.0035599085000846 valid 1.039551854133606
EPOCH 211:
Loss :  1.8060023784637451 2.756587505340576 2.756587505340576
Loss :  1.8004462718963623 2.8816778659820557 2.8816778659820557
Loss :  1.8046177625656128 2.9029433727264404 2.9029433727264404
Loss :  1.805877447128296 2.6648027896881104 2.6648027896881104
Loss :  1.8037880659103394 2.4502673149108887 2.4502673149108887
Loss :  1.8080260753631592 2.319610834121704 2.319610834121704
Loss :  1.8007477521896362 2.606645107269287 2.606645107269287
Loss :  1.801885962486267 2.9173314571380615 2.9173314571380615
Loss :  1.8015685081481934 2.854790210723877 2.854790210723877
Loss :  1.7997374534606934 2.859586000442505 2.859586000442505
Loss :  1.8081581592559814 3.338151216506958 3.338151216506958
Loss :  1.8036104440689087 3.0681838989257812 3.0681838989257812
Loss :  1.8075065612792969 3.08381986618042 3.08381986618042
Loss :  1.8107802867889404 2.6263153553009033 2.6263153553009033
Loss :  1.8013032674789429 2.6008071899414062 2.6008071899414062
Loss :  1.8101462125778198 2.8212568759918213 2.8212568759918213
Loss :  1.8075838088989258 2.729367733001709 2.729367733001709
Loss :  1.805179476737976 3.0916247367858887 3.0916247367858887
Loss :  1.8054745197296143 2.827815294265747 2.827815294265747
Loss :  1.8003487586975098 3.275380849838257 3.275380849838257
  batch 20 loss: 1.8003487586975098, 3.275380849838257, 3.275380849838257
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8100509643554688 3.249464273452759 3.249464273452759
Loss :  1.8084450960159302 3.2481770515441895 3.2481770515441895
Loss :  1.8028206825256348 3.069413185119629 3.069413185119629
Loss :  1.8089485168457031 3.5370755195617676 3.5370755195617676
Loss :  1.8086824417114258 3.539982557296753 3.539982557296753
Loss :  1.804679274559021 2.943214178085327 2.943214178085327
Loss :  1.8103079795837402 3.148627281188965 3.148627281188965
Loss :  1.8015551567077637 2.7626073360443115 2.7626073360443115
Loss :  1.8114402294158936 2.6221799850463867 2.6221799850463867
Loss :  1.8006058931350708 2.8746421337127686 2.8746421337127686
Loss :  1.8072459697723389 3.1441192626953125 3.1441192626953125
Loss :  1.7990251779556274 3.358020305633545 3.358020305633545
Loss :  1.806038498878479 3.179121971130371 3.179121971130371
Loss :  1.804889440536499 3.462597370147705 3.462597370147705
Loss :  1.8066602945327759 3.3320279121398926 3.3320279121398926
Loss :  1.8060319423675537 3.1966850757598877 3.1966850757598877
Loss :  1.8061401844024658 2.6157114505767822 2.6157114505767822
Loss :  1.799963355064392 2.7047410011291504 2.7047410011291504
Loss :  1.8054195642471313 2.7230541706085205 2.7230541706085205
Loss :  1.8006447553634644 2.7102513313293457 2.7102513313293457
  batch 40 loss: 1.8006447553634644, 2.7102513313293457, 2.7102513313293457
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8048819303512573 3.2366623878479004 3.2366623878479004
Loss :  1.8017441034317017 3.0565919876098633 3.0565919876098633
Loss :  1.8040271997451782 3.014227867126465 3.014227867126465
Loss :  1.798272728919983 3.0432188510894775 3.0432188510894775
Loss :  1.8062410354614258 2.885470151901245 2.885470151901245
Loss :  1.8020081520080566 3.36206316947937 3.36206316947937
Loss :  1.8006972074508667 3.3777921199798584 3.3777921199798584
Loss :  1.8029754161834717 2.7998156547546387 2.7998156547546387
Loss :  1.7957820892333984 3.291653633117676 3.291653633117676
Loss :  1.8010660409927368 3.3106696605682373 3.3106696605682373
Loss :  1.798032522201538 3.303379535675049 3.303379535675049
Loss :  1.8069359064102173 3.100346088409424 3.100346088409424
Loss :  1.8046289682388306 3.098877191543579 3.098877191543579
Loss :  1.8051551580429077 3.0387520790100098 3.0387520790100098
Loss :  1.8030697107315063 3.019177198410034 3.019177198410034
Loss :  1.8066221475601196 2.404845714569092 2.404845714569092
Loss :  1.8064695596694946 3.1251251697540283 3.1251251697540283
Loss :  1.8083515167236328 2.8796629905700684 2.8796629905700684
Loss :  1.8117812871932983 3.697976589202881 3.697976589202881
Loss :  1.805895209312439 4.021268367767334 4.021268367767334
  batch 60 loss: 1.805895209312439, 4.021268367767334, 4.021268367767334
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8067138195037842 3.1984620094299316 3.1984620094299316
Loss :  1.8027408123016357 2.8795318603515625 2.8795318603515625
Loss :  1.8052856922149658 2.851890802383423 2.851890802383423
Loss :  1.8016893863677979 2.9752423763275146 2.9752423763275146
Loss :  1.8051931858062744 2.7275805473327637 2.7275805473327637
Loss :  1.7539341449737549 4.418127536773682 4.418127536773682
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3, 4], device='cuda:0')
Loss :  1.7550674676895142 4.415429592132568 4.415429592132568
Loss :  1.7501026391983032 4.389756679534912 4.389756679534912
Loss :  1.769316554069519 4.14122200012207 4.14122200012207
Total LOSS train 3.0122917358691876 valid 4.341133952140808
CE LOSS train 1.8045945442639864 valid 0.44232913851737976
Contrastive LOSS train 3.0122917358691876 valid 1.0353055000305176
EPOCH 212:
Loss :  1.8043668270111084 3.2507376670837402 3.2507376670837402
Loss :  1.7996225357055664 3.6394946575164795 3.6394946575164795
Loss :  1.8033430576324463 3.3646655082702637 3.3646655082702637
Loss :  1.804403305053711 3.4077799320220947 3.4077799320220947
Loss :  1.802504301071167 3.1868655681610107 3.1868655681610107
Loss :  1.8070875406265259 2.786724328994751 2.786724328994751
Loss :  1.7994475364685059 2.3717315196990967 2.3717315196990967
Loss :  1.800721287727356 2.038245439529419 2.038245439529419
Loss :  1.8002632856369019 2.4132349491119385 2.4132349491119385
Loss :  1.798100233078003 2.3871891498565674 2.3871891498565674
Loss :  1.8073455095291138 2.5889322757720947 2.5889322757720947
Loss :  1.802963376045227 2.748908758163452 2.748908758163452
Loss :  1.8068206310272217 2.3617348670959473 2.3617348670959473
Loss :  1.8098928928375244 2.3863863945007324 2.3863863945007324
Loss :  1.7997889518737793 2.671297788619995 2.671297788619995
Loss :  1.8093819618225098 2.5420565605163574 2.5420565605163574
Loss :  1.8068571090698242 3.17480206489563 3.17480206489563
Loss :  1.8039840459823608 2.9904770851135254 2.9904770851135254
Loss :  1.8044710159301758 2.8036091327667236 2.8036091327667236
Loss :  1.799190878868103 2.7342324256896973 2.7342324256896973
  batch 20 loss: 1.799190878868103, 2.7342324256896973, 2.7342324256896973
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8088946342468262 2.78951096534729 2.78951096534729
Loss :  1.8075224161148071 3.2400870323181152 3.2400870323181152
Loss :  1.8015342950820923 3.3062732219696045 3.3062732219696045
Loss :  1.8079274892807007 2.7934064865112305 2.7934064865112305
Loss :  1.8072535991668701 2.665225028991699 2.665225028991699
Loss :  1.8037536144256592 2.6407244205474854 2.6407244205474854
Loss :  1.80974543094635 3.6554954051971436 3.6554954051971436
Loss :  1.8009178638458252 2.9346134662628174 2.9346134662628174
Loss :  1.8115028142929077 3.230635404586792 3.230635404586792
Loss :  1.799903392791748 3.078064441680908 3.078064441680908
Loss :  1.8069621324539185 3.5534491539001465 3.5534491539001465
Loss :  1.7986806631088257 3.0669734477996826 3.0669734477996826
Loss :  1.8058950901031494 3.124800443649292 3.124800443649292
Loss :  1.8044925928115845 2.501594305038452 2.501594305038452
Loss :  1.8061662912368774 2.5201547145843506 2.5201547145843506
Loss :  1.806074857711792 2.2512474060058594 2.2512474060058594
Loss :  1.805885672569275 2.682284116744995 2.682284116744995
Loss :  1.8000401258468628 2.8122928142547607 2.8122928142547607
Loss :  1.8051942586898804 2.7875547409057617 2.7875547409057617
Loss :  1.800564169883728 2.883859395980835 2.883859395980835
  batch 40 loss: 1.800564169883728, 2.883859395980835, 2.883859395980835
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8049569129943848 2.9704840183258057 2.9704840183258057
Loss :  1.8020079135894775 3.285599708557129 3.285599708557129
Loss :  1.803933024406433 3.209406614303589 3.209406614303589
Loss :  1.7984033823013306 2.587202548980713 2.587202548980713
Loss :  1.806408166885376 2.372361898422241 2.372361898422241
Loss :  1.8024696111679077 2.6289031505584717 2.6289031505584717
Loss :  1.8008413314819336 2.8557145595550537 2.8557145595550537
Loss :  1.8034212589263916 2.929013967514038 2.929013967514038
Loss :  1.7962708473205566 2.601179599761963 2.601179599761963
Loss :  1.801347255706787 2.6284074783325195 2.6284074783325195
Loss :  1.798616647720337 2.82327938079834 2.82327938079834
Loss :  1.807509183883667 2.3605473041534424 2.3605473041534424
Loss :  1.8050259351730347 3.119032621383667 3.119032621383667
Loss :  1.8057774305343628 2.9290592670440674 2.9290592670440674
Loss :  1.8034964799880981 2.98317551612854 2.98317551612854
Loss :  1.8070156574249268 2.9705798625946045 2.9705798625946045
Loss :  1.807033658027649 3.386725425720215 3.386725425720215
Loss :  1.8089526891708374 3.41666579246521 3.41666579246521
Loss :  1.8123003244400024 3.6604037284851074 3.6604037284851074
Loss :  1.805969476699829 3.3245222568511963 3.3245222568511963
  batch 60 loss: 1.805969476699829, 3.3245222568511963, 3.3245222568511963
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8071413040161133 3.340210437774658 3.340210437774658
Loss :  1.8030306100845337 3.022547960281372 3.022547960281372
Loss :  1.8056389093399048 2.7321557998657227 2.7321557998657227
Loss :  1.8020374774932861 2.97013783454895 2.97013783454895
Loss :  1.8056904077529907 2.999892234802246 2.999892234802246
Loss :  1.7492973804473877 4.464510440826416 4.464510440826416
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3, 4], device='cuda:0')
Loss :  1.7508490085601807 4.430890083312988 4.430890083312988
Loss :  1.7456090450286865 4.264319896697998 4.264319896697998
Loss :  1.7654426097869873 4.330583572387695 4.330583572387695
Total LOSS train 2.8996086377363937 valid 4.372575998306274
CE LOSS train 1.8042271320636456 valid 0.4413606524467468
Contrastive LOSS train 2.8996086377363937 valid 1.0826458930969238
EPOCH 213:
Loss :  1.8046289682388306 3.4106650352478027 3.4106650352478027
Loss :  1.7998921871185303 3.570493459701538 3.570493459701538
Loss :  1.803492546081543 2.6517231464385986 2.6517231464385986
Loss :  1.8044662475585938 2.9036929607391357 2.9036929607391357
Loss :  1.8026292324066162 2.8098437786102295 2.8098437786102295
Loss :  1.8072001934051514 2.858020067214966 2.858020067214966
Loss :  1.7996352910995483 3.1853017807006836 3.1853017807006836
Loss :  1.8007813692092896 2.8393032550811768 2.8393032550811768
Loss :  1.800398349761963 2.6902730464935303 2.6902730464935303
Loss :  1.7981362342834473 2.893427610397339 2.893427610397339
Loss :  1.8071527481079102 3.1106820106506348 3.1106820106506348
Loss :  1.8027781248092651 3.4727683067321777 3.4727683067321777
Loss :  1.8064221143722534 3.4861817359924316 3.4861817359924316
Loss :  1.8093901872634888 2.8306524753570557 2.8306524753570557
Loss :  1.7998850345611572 2.700597047805786 2.700597047805786
Loss :  1.8086588382720947 2.7194583415985107 2.7194583415985107
Loss :  1.8063207864761353 2.890249013900757 2.890249013900757
Loss :  1.8035577535629272 2.736633777618408 2.736633777618408
Loss :  1.8040595054626465 2.6225802898406982 2.6225802898406982
Loss :  1.7990434169769287 2.876413583755493 2.876413583755493
  batch 20 loss: 1.7990434169769287, 2.876413583755493, 2.876413583755493
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8087637424468994 2.376295328140259 2.376295328140259
Loss :  1.807267665863037 2.7531898021698 2.7531898021698
Loss :  1.8014764785766602 2.8759336471557617 2.8759336471557617
Loss :  1.8073008060455322 2.7978341579437256 2.7978341579437256
Loss :  1.8066678047180176 3.1880481243133545 3.1880481243133545
Loss :  1.8035064935684204 2.565664052963257 2.565664052963257
Loss :  1.809702754020691 3.379244565963745 3.379244565963745
Loss :  1.801123023033142 3.3931660652160645 3.3931660652160645
Loss :  1.8119031190872192 2.6812849044799805 2.6812849044799805
Loss :  1.7996652126312256 3.507084846496582 3.507084846496582
Loss :  1.8068585395812988 2.927809000015259 2.927809000015259
Loss :  1.7985260486602783 3.5610079765319824 3.5610079765319824
Loss :  1.8060157299041748 3.390415668487549 3.390415668487549
Loss :  1.804707407951355 3.270413398742676 3.270413398742676
Loss :  1.8063149452209473 3.4823713302612305 3.4823713302612305
Loss :  1.8062353134155273 3.375805616378784 3.375805616378784
Loss :  1.8059430122375488 3.31683087348938 3.31683087348938
Loss :  1.8002705574035645 3.090654134750366 3.090654134750366
Loss :  1.805267333984375 3.242668390274048 3.242668390274048
Loss :  1.8008092641830444 3.4190003871917725 3.4190003871917725
  batch 40 loss: 1.8008092641830444, 3.4190003871917725, 3.4190003871917725
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8051337003707886 3.4457952976226807 3.4457952976226807
Loss :  1.8021247386932373 3.2418720722198486 3.2418720722198486
Loss :  1.8036985397338867 3.3588550090789795 3.3588550090789795
Loss :  1.798539161682129 3.2827484607696533 3.2827484607696533
Loss :  1.8060225248336792 3.509805202484131 3.509805202484131
Loss :  1.8027464151382446 3.4468607902526855 3.4468607902526855
Loss :  1.8011095523834229 3.470180034637451 3.470180034637451
Loss :  1.8029998540878296 3.93768048286438 3.93768048286438
Loss :  1.7963109016418457 2.9619741439819336 2.9619741439819336
Loss :  1.8009835481643677 3.2163171768188477 3.2163171768188477
Loss :  1.7982022762298584 3.784808397293091 3.784808397293091
Loss :  1.8072277307510376 3.110903024673462 3.110903024673462
Loss :  1.8049085140228271 3.2381696701049805 3.2381696701049805
Loss :  1.8055098056793213 3.2856369018554688 3.2856369018554688
Loss :  1.803018569946289 2.9967761039733887 2.9967761039733887
Loss :  1.8069857358932495 2.762019634246826 2.762019634246826
Loss :  1.8069345951080322 2.976325035095215 2.976325035095215
Loss :  1.808497428894043 2.547184705734253 2.547184705734253
Loss :  1.812225103378296 3.3181092739105225 3.3181092739105225
Loss :  1.8058279752731323 3.5380403995513916 3.5380403995513916
  batch 60 loss: 1.8058279752731323, 3.5380403995513916, 3.5380403995513916
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8069982528686523 3.1270570755004883 3.1270570755004883
Loss :  1.802852749824524 3.1068291664123535 3.1068291664123535
Loss :  1.8056068420410156 2.6781492233276367 2.6781492233276367
Loss :  1.8019400835037231 2.7058374881744385 2.7058374881744385
Loss :  1.805404543876648 2.723637580871582 2.723637580871582
Loss :  1.750578761100769 4.435662269592285 4.435662269592285
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3, 4], device='cuda:0')
Loss :  1.7519649267196655 4.451452732086182 4.451452732086182
Loss :  1.7468938827514648 4.285616874694824 4.285616874694824
Loss :  1.7661755084991455 4.313445568084717 4.313445568084717
Total LOSS train 3.101927005327665 valid 4.371544361114502
CE LOSS train 1.804133285008944 valid 0.4415438771247864
Contrastive LOSS train 3.101927005327665 valid 1.0783613920211792
EPOCH 214:
Loss :  1.8045037984848022 2.8946080207824707 2.8946080207824707
Loss :  1.799971103668213 2.724726676940918 2.724726676940918
Loss :  1.8036162853240967 2.6326613426208496 2.6326613426208496
Loss :  1.8046132326126099 2.88696026802063 2.88696026802063
Loss :  1.8028024435043335 2.5987608432769775 2.5987608432769775
Loss :  1.807279348373413 3.0946238040924072 3.0946238040924072
Loss :  1.8001339435577393 3.2095372676849365 3.2095372676849365
Loss :  1.8013054132461548 2.9094698429107666 2.9094698429107666
Loss :  1.8004534244537354 2.7688002586364746 2.7688002586364746
Loss :  1.7989336252212524 2.5922446250915527 2.5922446250915527
Loss :  1.8076975345611572 2.697516918182373 2.697516918182373
Loss :  1.8030155897140503 2.729536771774292 2.729536771774292
Loss :  1.8072609901428223 2.830780267715454 2.830780267715454
Loss :  1.8102256059646606 2.650866985321045 2.650866985321045
Loss :  1.8003768920898438 2.6215133666992188 2.6215133666992188
Loss :  1.8096593618392944 2.66367769241333 2.66367769241333
Loss :  1.8071160316467285 2.659999370574951 2.659999370574951
Loss :  1.8042610883712769 2.794996976852417 2.794996976852417
Loss :  1.8047096729278564 2.5397138595581055 2.5397138595581055
Loss :  1.7996724843978882 2.754392385482788 2.754392385482788
  batch 20 loss: 1.7996724843978882, 2.754392385482788, 2.754392385482788
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.809301733970642 2.592393159866333 2.592393159866333
Loss :  1.8077985048294067 2.794968366622925 2.794968366622925
Loss :  1.8018043041229248 2.8666605949401855 2.8666605949401855
Loss :  1.8079612255096436 2.8249142169952393 2.8249142169952393
Loss :  1.8074489831924438 3.2706189155578613 3.2706189155578613
Loss :  1.804131031036377 3.5770139694213867 3.5770139694213867
Loss :  1.809869408607483 3.577223062515259 3.577223062515259
Loss :  1.8013019561767578 3.258747100830078 3.258747100830078
Loss :  1.8115304708480835 3.416356325149536 3.416356325149536
Loss :  1.8002204895019531 3.5254974365234375 3.5254974365234375
Loss :  1.8070003986358643 3.263197183609009 3.263197183609009
Loss :  1.7989603281021118 2.9983346462249756 2.9983346462249756
Loss :  1.8061784505844116 2.942359685897827 2.942359685897827
Loss :  1.8048672676086426 2.469987392425537 2.469987392425537
Loss :  1.8063358068466187 3.146888494491577 3.146888494491577
Loss :  1.8063267469406128 2.7854042053222656 2.7854042053222656
Loss :  1.8058092594146729 2.9898552894592285 2.9898552894592285
Loss :  1.8002140522003174 2.962921142578125 2.962921142578125
Loss :  1.8051131963729858 3.057279348373413 3.057279348373413
Loss :  1.8005541563034058 2.926405906677246 2.926405906677246
  batch 40 loss: 1.8005541563034058, 2.926405906677246, 2.926405906677246
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8049620389938354 3.434474229812622 3.434474229812622
Loss :  1.8019129037857056 3.2648873329162598 3.2648873329162598
Loss :  1.8035095930099487 2.8951408863067627 2.8951408863067627
Loss :  1.7982851266860962 2.7520644664764404 2.7520644664764404
Loss :  1.805864930152893 3.0525763034820557 3.0525763034820557
Loss :  1.8023393154144287 3.14481520652771 3.14481520652771
Loss :  1.8006218671798706 3.136253833770752 3.136253833770752
Loss :  1.8026888370513916 2.8783531188964844 2.8783531188964844
Loss :  1.7957550287246704 2.7740530967712402 2.7740530967712402
Loss :  1.8006013631820679 2.456254482269287 2.456254482269287
Loss :  1.7978463172912598 2.822554349899292 2.822554349899292
Loss :  1.8067504167556763 2.598212718963623 2.598212718963623
Loss :  1.8044201135635376 2.484903573989868 2.484903573989868
Loss :  1.80500328540802 2.456376791000366 2.456376791000366
Loss :  1.802841305732727 2.5981040000915527 2.5981040000915527
Loss :  1.8068426847457886 2.872511625289917 2.872511625289917
Loss :  1.8067355155944824 2.574589490890503 2.574589490890503
Loss :  1.808410882949829 2.9240899085998535 2.9240899085998535
Loss :  1.812055230140686 3.204052686691284 3.204052686691284
Loss :  1.8057646751403809 2.8077051639556885 2.8077051639556885
  batch 60 loss: 1.8057646751403809, 2.8077051639556885, 2.8077051639556885
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.806949257850647 3.304203510284424 3.304203510284424
Loss :  1.8026970624923706 3.0252137184143066 3.0252137184143066
Loss :  1.8056269884109497 2.746551275253296 2.746551275253296
Loss :  1.8020159006118774 3.395699977874756 3.395699977874756
Loss :  1.8055322170257568 3.1196775436401367 3.1196775436401367
Loss :  1.7460336685180664 4.434078693389893 4.434078693389893
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.7478070259094238 4.471484661102295 4.471484661102295
Loss :  1.7425566911697388 4.369786262512207 4.369786262512207
Loss :  1.7620545625686646 4.182499885559082 4.182499885559082
Total LOSS train 2.9111651273874135 valid 4.364462375640869
CE LOSS train 1.8042518230584952 valid 0.44051364064216614
Contrastive LOSS train 2.9111651273874135 valid 1.0456249713897705
EPOCH 215:
Loss :  1.8041077852249146 2.9164016246795654 2.9164016246795654
Loss :  1.7999755144119263 3.187372922897339 3.187372922897339
Loss :  1.8033299446105957 2.9344496726989746 2.9344496726989746
Loss :  1.804150104522705 2.8347737789154053 2.8347737789154053
Loss :  1.8024367094039917 3.054094076156616 3.054094076156616
Loss :  1.8069422245025635 3.4323384761810303 3.4323384761810303
Loss :  1.7995344400405884 3.601294994354248 3.601294994354248
Loss :  1.8006243705749512 2.8225343227386475 2.8225343227386475
Loss :  1.800048589706421 3.2561917304992676 3.2561917304992676
Loss :  1.797896385192871 3.1308717727661133 3.1308717727661133
Loss :  1.8070894479751587 3.0283608436584473 3.0283608436584473
Loss :  1.8026626110076904 3.319826364517212 3.319826364517212
Loss :  1.8066396713256836 2.8823540210723877 2.8823540210723877
Loss :  1.8094456195831299 2.8654329776763916 2.8654329776763916
Loss :  1.7989997863769531 3.309006452560425 3.309006452560425
Loss :  1.8085167407989502 3.383573055267334 3.383573055267334
Loss :  1.8062494993209839 3.281731605529785 3.281731605529785
Loss :  1.8031377792358398 2.836498498916626 2.836498498916626
Loss :  1.8039683103561401 2.4444096088409424 2.4444096088409424
Loss :  1.7983773946762085 2.9435389041900635 2.9435389041900635
  batch 20 loss: 1.7983773946762085, 2.9435389041900635, 2.9435389041900635
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.80826735496521 2.0659031867980957 2.0659031867980957
Loss :  1.8069446086883545 2.609588146209717 2.609588146209717
Loss :  1.8006714582443237 2.5889499187469482 2.5889499187469482
Loss :  1.8069409132003784 2.9345555305480957 2.9345555305480957
Loss :  1.806596279144287 3.335887908935547 3.335887908935547
Loss :  1.8031160831451416 3.141709804534912 3.141709804534912
Loss :  1.8092052936553955 2.957956075668335 2.957956075668335
Loss :  1.8004690408706665 2.781473398208618 2.781473398208618
Loss :  1.8109487295150757 2.5465822219848633 2.5465822219848633
Loss :  1.7992231845855713 2.7443649768829346 2.7443649768829346
Loss :  1.8066143989562988 2.498382806777954 2.498382806777954
Loss :  1.7982540130615234 2.5220999717712402 2.5220999717712402
Loss :  1.8056533336639404 2.6267306804656982 2.6267306804656982
Loss :  1.8041269779205322 2.594937801361084 2.594937801361084
Loss :  1.8061069250106812 2.809722661972046 2.809722661972046
Loss :  1.8058550357818604 2.599853038787842 2.599853038787842
Loss :  1.8056707382202148 2.6728286743164062 2.6728286743164062
Loss :  1.7997137308120728 2.492828845977783 2.492828845977783
Loss :  1.805018424987793 3.406334161758423 3.406334161758423
Loss :  1.8003466129302979 3.1276092529296875 3.1276092529296875
  batch 40 loss: 1.8003466129302979, 3.1276092529296875, 3.1276092529296875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.804861307144165 3.315704584121704 3.315704584121704
Loss :  1.8019472360610962 3.246197462081909 3.246197462081909
Loss :  1.804047703742981 3.187051296234131 3.187051296234131
Loss :  1.798343539237976 2.9864137172698975 2.9864137172698975
Loss :  1.806258201599121 2.803067207336426 2.803067207336426
Loss :  1.8025891780853271 3.3971946239471436 3.3971946239471436
Loss :  1.8007210493087769 3.3774216175079346 3.3774216175079346
Loss :  1.8033878803253174 2.888977527618408 2.888977527618408
Loss :  1.795982837677002 3.106369972229004 3.106369972229004
Loss :  1.8010356426239014 2.7613685131073 2.7613685131073
Loss :  1.7986036539077759 3.3592560291290283 3.3592560291290283
Loss :  1.8073257207870483 2.457291603088379 2.457291603088379
Loss :  1.8048694133758545 2.728257894515991 2.728257894515991
Loss :  1.8059303760528564 2.499434471130371 2.499434471130371
Loss :  1.8034327030181885 2.5477535724639893 2.5477535724639893
Loss :  1.8074363470077515 2.411219596862793 2.411219596862793
Loss :  1.807539939880371 2.603581666946411 2.603581666946411
Loss :  1.809065580368042 2.721604108810425 2.721604108810425
Loss :  1.8130402565002441 3.263179063796997 3.263179063796997
Loss :  1.8068714141845703 3.3044402599334717 3.3044402599334717
  batch 60 loss: 1.8068714141845703, 3.3044402599334717, 3.3044402599334717
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.807612419128418 3.4674012660980225 3.4674012660980225
Loss :  1.8037039041519165 2.8553309440612793 2.8553309440612793
Loss :  1.8061959743499756 2.8768272399902344 2.8768272399902344
Loss :  1.8026131391525269 2.9432804584503174 2.9432804584503174
Loss :  1.8063390254974365 2.627790689468384 2.627790689468384
Loss :  1.743320107460022 4.388315200805664 4.388315200805664
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.7453787326812744 4.413285255432129 4.413285255432129
Loss :  1.7397658824920654 4.322075843811035 4.322075843811035
Loss :  1.7606655359268188 4.327476978302002 4.327476978302002
Total LOSS train 2.927073386999277 valid 4.3627883195877075
CE LOSS train 1.8040558539904081 valid 0.4401663839817047
Contrastive LOSS train 2.927073386999277 valid 1.0818692445755005
EPOCH 216:
Loss :  1.8057472705841064 2.6050760746002197 2.6050760746002197
Loss :  1.800453782081604 2.8934524059295654 2.8934524059295654
Loss :  1.804189920425415 2.4618821144104004 2.4618821144104004
Loss :  1.8051565885543823 2.9419779777526855 2.9419779777526855
Loss :  1.8032625913619995 2.2542216777801514 2.2542216777801514
Loss :  1.8077939748764038 2.3241610527038574 2.3241610527038574
Loss :  1.8004730939865112 2.6627132892608643 2.6627132892608643
Loss :  1.8015856742858887 2.3306884765625 2.3306884765625
Loss :  1.8007445335388184 2.665034294128418 2.665034294128418
Loss :  1.7990437746047974 3.3081133365631104 3.3081133365631104
Loss :  1.8077473640441895 3.688016414642334 3.688016414642334
Loss :  1.8030238151550293 3.9958913326263428 3.9958913326263428
Loss :  1.8069782257080078 3.6818156242370605 3.6818156242370605
Loss :  1.8098971843719482 2.9347665309906006 2.9347665309906006
Loss :  1.8001601696014404 3.676551342010498 3.676551342010498
Loss :  1.8098397254943848 3.706636905670166 3.706636905670166
Loss :  1.806743860244751 3.617666482925415 3.617666482925415
Loss :  1.8039052486419678 2.8791956901550293 2.8791956901550293
Loss :  1.8045175075531006 2.6453561782836914 2.6453561782836914
Loss :  1.7991621494293213 2.7745134830474854 2.7745134830474854
  batch 20 loss: 1.7991621494293213, 2.7745134830474854, 2.7745134830474854
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8087105751037598 2.4632644653320312 2.4632644653320312
Loss :  1.8071751594543457 2.4754459857940674 2.4754459857940674
Loss :  1.801180124282837 2.6733152866363525 2.6733152866363525
Loss :  1.8074698448181152 2.768953561782837 2.768953561782837
Loss :  1.8068814277648926 3.1260640621185303 3.1260640621185303
Loss :  1.8034467697143555 2.1885883808135986 2.1885883808135986
Loss :  1.8093576431274414 2.720101833343506 2.720101833343506
Loss :  1.800437569618225 3.1019723415374756 3.1019723415374756
Loss :  1.8114771842956543 3.8509469032287598 3.8509469032287598
Loss :  1.799185872077942 3.673527956008911 3.673527956008911
Loss :  1.8066344261169434 3.0533699989318848 3.0533699989318848
Loss :  1.7980949878692627 3.195019245147705 3.195019245147705
Loss :  1.8055015802383423 3.3442349433898926 3.3442349433898926
Loss :  1.8037998676300049 3.378345251083374 3.378345251083374
Loss :  1.8056741952896118 3.2812063694000244 3.2812063694000244
Loss :  1.8056466579437256 3.6634912490844727 3.6634912490844727
Loss :  1.8051999807357788 3.0842435359954834 3.0842435359954834
Loss :  1.7994707822799683 2.878842353820801 2.878842353820801
Loss :  1.8043419122695923 3.420206308364868 3.420206308364868
Loss :  1.7998473644256592 3.3640711307525635 3.3640711307525635
  batch 40 loss: 1.7998473644256592, 3.3640711307525635, 3.3640711307525635
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8042253255844116 3.013374090194702 3.013374090194702
Loss :  1.8012782335281372 2.608626127243042 2.608626127243042
Loss :  1.803027868270874 2.759211778640747 2.759211778640747
Loss :  1.7976757287979126 3.227112293243408 3.227112293243408
Loss :  1.8053139448165894 3.0921387672424316 3.0921387672424316
Loss :  1.8018122911453247 3.2120578289031982 3.2120578289031982
Loss :  1.8001512289047241 2.8435633182525635 2.8435633182525635
Loss :  1.8021584749221802 2.986208438873291 2.986208438873291
Loss :  1.7953404188156128 2.6520588397979736 2.6520588397979736
Loss :  1.800025463104248 3.076349973678589 3.076349973678589
Loss :  1.7971004247665405 3.2671353816986084 3.2671353816986084
Loss :  1.8057963848114014 3.3075811862945557 3.3075811862945557
Loss :  1.8035945892333984 2.9392411708831787 2.9392411708831787
Loss :  1.8038065433502197 2.9604806900024414 2.9604806900024414
Loss :  1.8021440505981445 3.5036122798919678 3.5036122798919678
Loss :  1.8055872917175293 3.3846230506896973 3.3846230506896973
Loss :  1.80595064163208 3.1842992305755615 3.1842992305755615
Loss :  1.807847261428833 3.2947921752929688 3.2947921752929688
Loss :  1.8111385107040405 3.5257842540740967 3.5257842540740967
Loss :  1.8044568300247192 3.3471972942352295 3.3471972942352295
  batch 60 loss: 1.8044568300247192, 3.3471972942352295, 3.3471972942352295
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.806006908416748 3.6216201782226562 3.6216201782226562
Loss :  1.801714539527893 3.02498197555542 3.02498197555542
Loss :  1.8048925399780273 2.7637760639190674 2.7637760639190674
Loss :  1.8008724451065063 3.068878650665283 3.068878650665283
Loss :  1.8043341636657715 2.7911252975463867 2.7911252975463867
Loss :  1.7493252754211426 4.420769691467285 4.420769691467285
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3, 4], device='cuda:0')
Loss :  1.7508213520050049 4.516977787017822 4.516977787017822
Loss :  1.745715618133545 4.261592864990234 4.261592864990234
Loss :  1.7651734352111816 4.17426872253418 4.17426872253418
Total LOSS train 3.0647503412686863 valid 4.34340226650238
CE LOSS train 1.8037878843454214 valid 0.4412933588027954
Contrastive LOSS train 3.0647503412686863 valid 1.043567180633545
EPOCH 217:
Loss :  1.8028265237808228 3.5733156204223633 3.5733156204223633
Loss :  1.798689842224121 4.141073226928711 4.141073226928711
Loss :  1.8019791841506958 3.2879106998443604 3.2879106998443604
Loss :  1.8030747175216675 3.2317914962768555 3.2317914962768555
Loss :  1.8013474941253662 3.0365610122680664 3.0365610122680664
Loss :  1.8059656620025635 2.553663492202759 2.553663492202759
Loss :  1.7983497381210327 2.654421806335449 2.654421806335449
Loss :  1.799404501914978 2.865086555480957 2.865086555480957
Loss :  1.7989931106567383 3.0807015895843506 3.0807015895843506
Loss :  1.796274185180664 2.4984850883483887 2.4984850883483887
Loss :  1.8059797286987305 2.815443515777588 2.815443515777588
Loss :  1.8018395900726318 2.7500810623168945 2.7500810623168945
Loss :  1.8057063817977905 3.7512595653533936 3.7512595653533936
Loss :  1.8082751035690308 3.26831316947937 3.26831316947937
Loss :  1.7981616258621216 3.0444653034210205 3.0444653034210205
Loss :  1.8072423934936523 2.718639612197876 2.718639612197876
Loss :  1.805193305015564 2.693862199783325 2.693862199783325
Loss :  1.8019269704818726 3.1841418743133545 3.1841418743133545
Loss :  1.8029356002807617 3.048273801803589 3.048273801803589
Loss :  1.797257900238037 2.587510108947754 2.587510108947754
  batch 20 loss: 1.797257900238037, 2.587510108947754, 2.587510108947754
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.807134747505188 3.267061710357666 3.267061710357666
Loss :  1.8060091733932495 3.1313397884368896 3.1313397884368896
Loss :  1.7995333671569824 2.2911415100097656 2.2911415100097656
Loss :  1.8059276342391968 3.023810625076294 3.023810625076294
Loss :  1.8050916194915771 2.9921412467956543 2.9921412467956543
Loss :  1.801992654800415 2.5432469844818115 2.5432469844818115
Loss :  1.8083363771438599 3.2315266132354736 3.2315266132354736
Loss :  1.799487590789795 2.594144582748413 2.594144582748413
Loss :  1.8104500770568848 2.549422264099121 2.549422264099121
Loss :  1.7981986999511719 2.9234490394592285 2.9234490394592285
Loss :  1.8059054613113403 2.5078468322753906 2.5078468322753906
Loss :  1.7972763776779175 3.081514596939087 3.081514596939087
Loss :  1.804706335067749 3.2770469188690186 3.2770469188690186
Loss :  1.8029097318649292 3.13784122467041 3.13784122467041
Loss :  1.8050512075424194 3.050135850906372 3.050135850906372
Loss :  1.8050613403320312 2.9648420810699463 2.9648420810699463
Loss :  1.8045259714126587 2.9824907779693604 2.9824907779693604
Loss :  1.7986959218978882 2.969625949859619 2.969625949859619
Loss :  1.8037575483322144 2.629998207092285 2.629998207092285
Loss :  1.7991410493850708 2.8029749393463135 2.8029749393463135
  batch 40 loss: 1.7991410493850708, 2.8029749393463135, 2.8029749393463135
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8037084341049194 2.9221997261047363 2.9221997261047363
Loss :  1.800665020942688 2.9100046157836914 2.9100046157836914
Loss :  1.802462100982666 2.7456488609313965 2.7456488609313965
Loss :  1.7972643375396729 2.588613748550415 2.588613748550415
Loss :  1.80489981174469 3.167799472808838 3.167799472808838
Loss :  1.8013335466384888 2.857325315475464 2.857325315475464
Loss :  1.799453854560852 3.14459490776062 3.14459490776062
Loss :  1.8017959594726562 3.19834303855896 3.19834303855896
Loss :  1.7945469617843628 2.5230917930603027 2.5230917930603027
Loss :  1.7995787858963013 2.669158935546875 2.669158935546875
Loss :  1.7966554164886475 3.016667127609253 3.016667127609253
Loss :  1.8054277896881104 2.970036506652832 2.970036506652832
Loss :  1.8033500909805298 2.4733076095581055 2.4733076095581055
Loss :  1.8037413358688354 2.394428253173828 2.394428253173828
Loss :  1.8020492792129517 2.4551451206207275 2.4551451206207275
Loss :  1.8054983615875244 2.5106425285339355 2.5106425285339355
Loss :  1.8058900833129883 2.7589762210845947 2.7589762210845947
Loss :  1.807853102684021 2.960812568664551 2.960812568664551
Loss :  1.811434030532837 3.447406530380249 3.447406530380249
Loss :  1.804916501045227 3.1002869606018066 3.1002869606018066
  batch 60 loss: 1.804916501045227, 3.1002869606018066, 3.1002869606018066
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8062998056411743 2.933429718017578 2.933429718017578
Loss :  1.8022929430007935 3.216766357421875 3.216766357421875
Loss :  1.8052294254302979 3.463183879852295 3.463183879852295
Loss :  1.8016051054000854 3.487929582595825 3.487929582595825
Loss :  1.8052340745925903 2.184781789779663 2.184781789779663
Loss :  1.7456618547439575 4.3666839599609375 4.3666839599609375
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.7474726438522339 4.463265419006348 4.463265419006348
Loss :  1.7421635389328003 4.29600191116333 4.29600191116333
Loss :  1.7620700597763062 4.2400221824646 4.2400221824646
Total LOSS train 2.935956672521738 valid 4.341493368148804
CE LOSS train 1.8028277323796198 valid 0.44051751494407654
Contrastive LOSS train 2.935956672521738 valid 1.06000554561615
EPOCH 218:
Loss :  1.8037018775939941 3.4522078037261963 3.4522078037261963
Loss :  1.7993110418319702 2.90916109085083 2.90916109085083
Loss :  1.8029916286468506 2.664886236190796 2.664886236190796
Loss :  1.803947925567627 2.8449995517730713 2.8449995517730713
Loss :  1.802190899848938 3.1370599269866943 3.1370599269866943
Loss :  1.8066291809082031 2.8613152503967285 2.8613152503967285
Loss :  1.7993892431259155 3.1009743213653564 3.1009743213653564
Loss :  1.8005369901657104 2.5802865028381348 2.5802865028381348
Loss :  1.7997456789016724 2.760111093521118 2.760111093521118
Loss :  1.7977807521820068 2.6095855236053467 2.6095855236053467
Loss :  1.8069846630096436 2.7523751258850098 2.7523751258850098
Loss :  1.8024756908416748 3.5403034687042236 3.5403034687042236
Loss :  1.806636929512024 2.737464427947998 2.737464427947998
Loss :  1.8092790842056274 2.57080078125 2.57080078125
Loss :  1.7989486455917358 2.6459076404571533 2.6459076404571533
Loss :  1.8084616661071777 2.894029378890991 2.894029378890991
Loss :  1.806219458580017 3.1270220279693604 3.1270220279693604
Loss :  1.8029029369354248 2.9488775730133057 2.9488775730133057
Loss :  1.8035629987716675 2.7997162342071533 2.7997162342071533
Loss :  1.7982021570205688 2.9538826942443848 2.9538826942443848
  batch 20 loss: 1.7982021570205688, 2.9538826942443848, 2.9538826942443848
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8078277111053467 2.814302921295166 2.814302921295166
Loss :  1.8065807819366455 3.189622163772583 3.189622163772583
Loss :  1.8001534938812256 2.5294198989868164 2.5294198989868164
Loss :  1.8063476085662842 2.459886312484741 2.459886312484741
Loss :  1.8055411577224731 2.478684663772583 2.478684663772583
Loss :  1.8027102947235107 2.33372163772583 2.33372163772583
Loss :  1.8088016510009766 2.7020034790039062 2.7020034790039062
Loss :  1.800106167793274 3.070887327194214 3.070887327194214
Loss :  1.8110123872756958 3.2890408039093018 3.2890408039093018
Loss :  1.798545241355896 2.73711895942688 2.73711895942688
Loss :  1.8061633110046387 2.9057276248931885 2.9057276248931885
Loss :  1.7976669073104858 3.2066400051116943 3.2066400051116943
Loss :  1.8051338195800781 2.5467283725738525 2.5467283725738525
Loss :  1.8033355474472046 2.956312417984009 2.956312417984009
Loss :  1.805395245552063 2.73551869392395 2.73551869392395
Loss :  1.805414080619812 3.185403347015381 3.185403347015381
Loss :  1.8048830032348633 3.245410442352295 3.245410442352295
Loss :  1.7990423440933228 2.588783025741577 2.588783025741577
Loss :  1.80386483669281 2.792620897293091 2.792620897293091
Loss :  1.7992275953292847 2.7057745456695557 2.7057745456695557
  batch 40 loss: 1.7992275953292847, 2.7057745456695557, 2.7057745456695557
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8038328886032104 3.225074291229248 3.225074291229248
Loss :  1.8008623123168945 2.9984164237976074 2.9984164237976074
Loss :  1.802708625793457 2.9048092365264893 2.9048092365264893
Loss :  1.7973374128341675 3.584890604019165 3.584890604019165
Loss :  1.8051079511642456 2.880946159362793 2.880946159362793
Loss :  1.801527738571167 3.3422110080718994 3.3422110080718994
Loss :  1.7996692657470703 2.9788358211517334 2.9788358211517334
Loss :  1.8020426034927368 3.8375062942504883 3.8375062942504883
Loss :  1.7947834730148315 2.9982850551605225 2.9982850551605225
Loss :  1.7997338771820068 2.618260622024536 2.618260622024536
Loss :  1.796811580657959 2.6204543113708496 2.6204543113708496
Loss :  1.8055297136306763 2.4224114418029785 2.4224114418029785
Loss :  1.803352952003479 2.341576099395752 2.341576099395752
Loss :  1.8037993907928467 2.711256504058838 2.711256504058838
Loss :  1.8019475936889648 2.5513978004455566 2.5513978004455566
Loss :  1.8053390979766846 2.317103624343872 2.317103624343872
Loss :  1.8059006929397583 2.512411594390869 2.512411594390869
Loss :  1.807650089263916 2.638817071914673 2.638817071914673
Loss :  1.8112789392471313 3.3727293014526367 3.3727293014526367
Loss :  1.8046751022338867 2.961669683456421 2.961669683456421
  batch 60 loss: 1.8046751022338867, 2.961669683456421, 2.961669683456421
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8061033487319946 2.7858290672302246 2.7858290672302246
Loss :  1.801985502243042 2.6325278282165527 2.6325278282165527
Loss :  1.8051246404647827 2.70676326751709 2.70676326751709
Loss :  1.8013266324996948 2.591984510421753 2.591984510421753
Loss :  1.8048309087753296 2.209367036819458 2.209367036819458
Loss :  1.7459182739257812 4.4343976974487305 4.4343976974487305
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.7477396726608276 4.420348644256592 4.420348644256592
Loss :  1.7423821687698364 4.334927082061768 4.334927082061768
Loss :  1.7628508806228638 4.185838222503662 4.185838222503662
Total LOSS train 2.8478478284982534 valid 4.343877911567688
CE LOSS train 1.803244845683758 valid 0.44071272015571594
Contrastive LOSS train 2.8478478284982534 valid 1.0464595556259155
EPOCH 219:
Loss :  1.8037956953048706 2.6934428215026855 2.6934428215026855
Loss :  1.7992677688598633 2.74481463432312 2.74481463432312
Loss :  1.8029485940933228 2.4164044857025146 2.4164044857025146
Loss :  1.804218053817749 2.8358023166656494 2.8358023166656494
Loss :  1.8024170398712158 2.400141954421997 2.400141954421997
Loss :  1.8069136142730713 2.690066337585449 2.690066337585449
Loss :  1.8000510931015015 2.84529185295105 2.84529185295105
Loss :  1.8010166883468628 2.715578317642212 2.715578317642212
Loss :  1.8003984689712524 3.023568630218506 3.023568630218506
Loss :  1.7989481687545776 3.239149808883667 3.239149808883667
Loss :  1.8074820041656494 2.9463791847229004 2.9463791847229004
Loss :  1.8027732372283936 2.9785141944885254 2.9785141944885254
Loss :  1.8070518970489502 2.9034552574157715 2.9034552574157715
Loss :  1.809878945350647 2.788607597351074 2.788607597351074
Loss :  1.8003363609313965 2.90301775932312 2.90301775932312
Loss :  1.8095896244049072 3.237260341644287 3.237260341644287
Loss :  1.8068664073944092 2.6523215770721436 2.6523215770721436
Loss :  1.804121494293213 3.1870827674865723 3.1870827674865723
Loss :  1.8043426275253296 3.4283509254455566 3.4283509254455566
Loss :  1.7996348142623901 3.071913957595825 3.071913957595825
  batch 20 loss: 1.7996348142623901, 3.071913957595825, 3.071913957595825
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8088771104812622 2.803947925567627 2.803947925567627
Loss :  1.807249903678894 2.818060874938965 2.818060874938965
Loss :  1.8011515140533447 2.197965383529663 2.197965383529663
Loss :  1.8068649768829346 3.1092071533203125 3.1092071533203125
Loss :  1.8059520721435547 2.8248276710510254 2.8248276710510254
Loss :  1.803341269493103 2.5992579460144043 2.5992579460144043
Loss :  1.809304118156433 3.017263174057007 3.017263174057007
Loss :  1.800751805305481 3.123420238494873 3.123420238494873
Loss :  1.8115178346633911 3.0775680541992188 3.0775680541992188
Loss :  1.7991727590560913 2.9297680854797363 2.9297680854797363
Loss :  1.8063626289367676 3.1288747787475586 3.1288747787475586
Loss :  1.7980512380599976 3.490201473236084 3.490201473236084
Loss :  1.805593729019165 3.765427827835083 3.765427827835083
Loss :  1.803857684135437 3.2353744506835938 3.2353744506835938
Loss :  1.805816411972046 3.6820015907287598 3.6820015907287598
Loss :  1.8057934045791626 3.606881856918335 3.606881856918335
Loss :  1.8053686618804932 3.7619550228118896 3.7619550228118896
Loss :  1.7996480464935303 3.676161527633667 3.676161527633667
Loss :  1.804453730583191 3.359065532684326 3.359065532684326
Loss :  1.8000084161758423 3.0981099605560303 3.0981099605560303
  batch 40 loss: 1.8000084161758423, 3.0981099605560303, 3.0981099605560303
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8044899702072144 3.259157657623291 3.259157657623291
Loss :  1.801404595375061 3.415971040725708 3.415971040725708
Loss :  1.8031398057937622 3.583339214324951 3.583339214324951
Loss :  1.797821283340454 3.715876340866089 3.715876340866089
Loss :  1.8054349422454834 2.9050309658050537 2.9050309658050537
Loss :  1.8020864725112915 3.1814615726470947 3.1814615726470947
Loss :  1.8003698587417603 2.5299322605133057 2.5299322605133057
Loss :  1.802293062210083 2.715820074081421 2.715820074081421
Loss :  1.7953678369522095 2.9097988605499268 2.9097988605499268
Loss :  1.800155520439148 2.7876055240631104 2.7876055240631104
Loss :  1.797400951385498 2.8866755962371826 2.8866755962371826
Loss :  1.8060333728790283 3.4183926582336426 3.4183926582336426
Loss :  1.803770661354065 2.5788931846618652 2.5788931846618652
Loss :  1.8044341802597046 2.1963350772857666 2.1963350772857666
Loss :  1.8023693561553955 2.7524445056915283 2.7524445056915283
Loss :  1.8059346675872803 2.4135539531707764 2.4135539531707764
Loss :  1.8062772750854492 3.1732094287872314 3.1732094287872314
Loss :  1.8079626560211182 3.0347378253936768 3.0347378253936768
Loss :  1.811623454093933 3.0404295921325684 3.0404295921325684
Loss :  1.8051146268844604 2.7938392162323 2.7938392162323
  batch 60 loss: 1.8051146268844604, 2.7938392162323, 2.7938392162323
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8063576221466064 2.9747231006622314 2.9747231006622314
Loss :  1.8023734092712402 2.398552656173706 2.398552656173706
Loss :  1.8052870035171509 2.484926462173462 2.484926462173462
Loss :  1.8016307353973389 2.744196653366089 2.744196653366089
Loss :  1.8051695823669434 2.272022247314453 2.272022247314453
Loss :  1.7462010383605957 4.409378528594971 4.409378528594971
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3, 4], device='cuda:0')
Loss :  1.748011827468872 4.453924179077148 4.453924179077148
Loss :  1.7426867485046387 4.261605739593506 4.261605739593506
Loss :  1.7628370523452759 4.09853982925415 4.09853982925415
Total LOSS train 2.9718989372253417 valid 4.305862069129944
CE LOSS train 1.8037768125534057 valid 0.44070926308631897
Contrastive LOSS train 2.9718989372253417 valid 1.0246349573135376
EPOCH 220:
Loss :  1.803802728652954 2.6526403427124023 2.6526403427124023
Loss :  1.7993115186691284 2.9401965141296387 2.9401965141296387
Loss :  1.803072452545166 2.8948018550872803 2.8948018550872803
Loss :  1.8042123317718506 2.9304425716400146 2.9304425716400146
Loss :  1.8023432493209839 2.606196880340576 2.606196880340576
Loss :  1.806859016418457 2.6314914226531982 2.6314914226531982
Loss :  1.7993935346603394 2.4979939460754395 2.4979939460754395
Loss :  1.800634741783142 2.871570110321045 2.871570110321045
Loss :  1.8000802993774414 2.6975557804107666 2.6975557804107666
Loss :  1.7980531454086304 2.586423635482788 2.586423635482788
Loss :  1.8072729110717773 3.0803682804107666 3.0803682804107666
Loss :  1.802790880203247 3.248983144760132 3.248983144760132
Loss :  1.8066532611846924 3.5547494888305664 3.5547494888305664
Loss :  1.8093299865722656 3.1331658363342285 3.1331658363342285
Loss :  1.799177885055542 3.278782367706299 3.278782367706299
Loss :  1.808823823928833 3.1694068908691406 3.1694068908691406
Loss :  1.8063297271728516 3.0371739864349365 3.0371739864349365
Loss :  1.8031431436538696 3.1176459789276123 3.1176459789276123
Loss :  1.8040282726287842 2.9141011238098145 2.9141011238098145
Loss :  1.7983564138412476 2.7946088314056396 2.7946088314056396
  batch 20 loss: 1.7983564138412476, 2.7946088314056396, 2.7946088314056396
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.808025598526001 2.443084239959717 2.443084239959717
Loss :  1.806791067123413 3.0423855781555176 3.0423855781555176
Loss :  1.8002476692199707 3.0778284072875977 3.0778284072875977
Loss :  1.8067015409469604 3.0675814151763916 3.0675814151763916
Loss :  1.8062770366668701 2.7355093955993652 2.7355093955993652
Loss :  1.8031123876571655 2.6091439723968506 2.6091439723968506
Loss :  1.8090569972991943 2.6416585445404053 2.6416585445404053
Loss :  1.8002351522445679 2.634840965270996 2.634840965270996
Loss :  1.8110491037368774 3.14577054977417 3.14577054977417
Loss :  1.7992881536483765 3.1931192874908447 3.1931192874908447
Loss :  1.8066023588180542 3.3523783683776855 3.3523783683776855
Loss :  1.7981106042861938 2.9683585166931152 2.9683585166931152
Loss :  1.8055261373519897 2.6390538215637207 2.6390538215637207
Loss :  1.8040512800216675 2.5296883583068848 2.5296883583068848
Loss :  1.805743932723999 2.9232771396636963 2.9232771396636963
Loss :  1.8058842420578003 3.193354606628418 3.193354606628418
Loss :  1.8053449392318726 2.7897627353668213 2.7897627353668213
Loss :  1.7997767925262451 2.7832493782043457 2.7832493782043457
Loss :  1.8046714067459106 3.037001371383667 3.037001371383667
Loss :  1.8002657890319824 2.588679313659668 2.588679313659668
  batch 40 loss: 1.8002657890319824, 2.588679313659668, 2.588679313659668
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8048648834228516 2.5033936500549316 2.5033936500549316
Loss :  1.801863670349121 2.5951342582702637 2.5951342582702637
Loss :  1.8037574291229248 2.5461642742156982 2.5461642742156982
Loss :  1.7982231378555298 2.2983310222625732 2.2983310222625732
Loss :  1.806208848953247 2.4135775566101074 2.4135775566101074
Loss :  1.8028638362884521 2.605422258377075 2.605422258377075
Loss :  1.801183819770813 2.5907199382781982 2.5907199382781982
Loss :  1.8032867908477783 2.2170774936676025 2.2170774936676025
Loss :  1.7968392372131348 2.5834977626800537 2.5834977626800537
Loss :  1.8011752367019653 2.6203291416168213 2.6203291416168213
Loss :  1.7987072467803955 3.1848225593566895 3.1848225593566895
Loss :  1.8073549270629883 3.232012987136841 3.232012987136841
Loss :  1.8049765825271606 2.8310418128967285 2.8310418128967285
Loss :  1.8058075904846191 3.0414083003997803 3.0414083003997803
Loss :  1.8032633066177368 3.3546109199523926 3.3546109199523926
Loss :  1.8072123527526855 2.869703769683838 2.869703769683838
Loss :  1.8077689409255981 3.1852405071258545 3.1852405071258545
Loss :  1.809195637702942 3.0353384017944336 3.0353384017944336
Loss :  1.8129920959472656 3.235927104949951 3.235927104949951
Loss :  1.8060028553009033 3.546090602874756 3.546090602874756
  batch 60 loss: 1.8060028553009033, 3.546090602874756, 3.546090602874756
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8076565265655518 3.4850354194641113 3.4850354194641113
Loss :  1.8033478260040283 3.3316190242767334 3.3316190242767334
Loss :  1.8061603307724 3.2766640186309814 3.2766640186309814
Loss :  1.8027546405792236 2.5979764461517334 2.5979764461517334
Loss :  1.8062870502471924 2.0239927768707275 2.0239927768707275
Loss :  1.7395241260528564 4.395541191101074 4.395541191101074
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.7418466806411743 4.3722147941589355 4.3722147941589355
Loss :  1.7361868619918823 4.2698845863342285 4.2698845863342285
Loss :  1.757218837738037 4.189422607421875 4.189422607421875
Total LOSS train 2.8806024147914004 valid 4.306765794754028
CE LOSS train 1.8040028663781973 valid 0.4393047094345093
Contrastive LOSS train 2.8806024147914004 valid 1.0473556518554688
EPOCH 221:
Loss :  1.8049527406692505 2.5727949142456055 2.5727949142456055
Loss :  1.800277829170227 2.921797513961792 2.921797513961792
Loss :  1.8039194345474243 2.6227188110351562 2.6227188110351562
Loss :  1.8047281503677368 3.2481393814086914 3.2481393814086914
Loss :  1.802830696105957 3.4680697917938232 3.4680697917938232
Loss :  1.8072961568832397 2.908127784729004 2.908127784729004
Loss :  1.8003824949264526 2.969984769821167 2.969984769821167
Loss :  1.801396131515503 3.2016870975494385 3.2016870975494385
Loss :  1.8003846406936646 3.376661777496338 3.376661777496338
Loss :  1.7990248203277588 2.8368184566497803 2.8368184566497803
Loss :  1.8075666427612305 2.8513097763061523 2.8513097763061523
Loss :  1.8028075695037842 2.848134756088257 2.848134756088257
Loss :  1.806989312171936 2.7565977573394775 2.7565977573394775
Loss :  1.8096944093704224 2.622225761413574 2.622225761413574
Loss :  1.799666166305542 2.8021867275238037 2.8021867275238037
Loss :  1.8094037771224976 2.7702815532684326 2.7702815532684326
Loss :  1.8067246675491333 3.22597074508667 3.22597074508667
Loss :  1.8035789728164673 3.073448657989502 3.073448657989502
Loss :  1.8040435314178467 2.834127902984619 2.834127902984619
Loss :  1.799216628074646 2.8428800106048584 2.8428800106048584
  batch 20 loss: 1.799216628074646, 2.8428800106048584, 2.8428800106048584
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8085490465164185 3.0439631938934326 3.0439631938934326
Loss :  1.8070240020751953 2.9481828212738037 2.9481828212738037
Loss :  1.8008860349655151 2.6947875022888184 2.6947875022888184
Loss :  1.8070244789123535 2.98378586769104 2.98378586769104
Loss :  1.8062447309494019 2.9248273372650146 2.9248273372650146
Loss :  1.8037189245224 2.7442328929901123 2.7442328929901123
Loss :  1.8094009160995483 2.9277377128601074 2.9277377128601074
Loss :  1.8008460998535156 2.778103828430176 2.778103828430176
Loss :  1.8115071058273315 2.60903000831604 2.60903000831604
Loss :  1.7996920347213745 2.537543296813965 2.537543296813965
Loss :  1.8067046403884888 2.911606550216675 2.911606550216675
Loss :  1.7985540628433228 2.6526496410369873 2.6526496410369873
Loss :  1.8058918714523315 2.490612030029297 2.490612030029297
Loss :  1.8045991659164429 2.615964412689209 2.615964412689209
Loss :  1.8060922622680664 3.0486931800842285 3.0486931800842285
Loss :  1.806238055229187 2.675461769104004 2.675461769104004
Loss :  1.805753231048584 2.558777332305908 2.558777332305908
Loss :  1.8003392219543457 2.7702224254608154 2.7702224254608154
Loss :  1.8050287961959839 2.9284913539886475 2.9284913539886475
Loss :  1.8007514476776123 3.0440690517425537 3.0440690517425537
  batch 40 loss: 1.8007514476776123, 3.0440690517425537, 3.0440690517425537
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8052263259887695 3.109286069869995 3.109286069869995
Loss :  1.802202582359314 2.9246246814727783 2.9246246814727783
Loss :  1.80391526222229 3.174198865890503 3.174198865890503
Loss :  1.7984662055969238 3.110752582550049 3.110752582550049
Loss :  1.8062063455581665 3.041438579559326 3.041438579559326
Loss :  1.8030222654342651 3.4172370433807373 3.4172370433807373
Loss :  1.8009734153747559 2.8701212406158447 2.8701212406158447
Loss :  1.8031065464019775 2.989821195602417 2.989821195602417
Loss :  1.7963889837265015 3.0654947757720947 3.0654947757720947
Loss :  1.8008590936660767 3.3385934829711914 3.3385934829711914
Loss :  1.798323392868042 2.833385467529297 2.833385467529297
Loss :  1.806836724281311 3.1706137657165527 3.1706137657165527
Loss :  1.8044623136520386 2.961649179458618 2.961649179458618
Loss :  1.8052741289138794 3.375762939453125 3.375762939453125
Loss :  1.802915334701538 3.064257860183716 3.064257860183716
Loss :  1.8066210746765137 2.643899440765381 2.643899440765381
Loss :  1.8071383237838745 3.082962989807129 3.082962989807129
Loss :  1.808569312095642 2.8110318183898926 2.8110318183898926
Loss :  1.8123759031295776 3.1300692558288574 3.1300692558288574
Loss :  1.805814266204834 2.683375597000122 2.683375597000122
  batch 60 loss: 1.805814266204834, 2.683375597000122, 2.683375597000122
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8069801330566406 2.7481586933135986 2.7481586933135986
Loss :  1.8028675317764282 2.632171630859375 2.632171630859375
Loss :  1.8058520555496216 2.534775733947754 2.534775733947754
Loss :  1.8021934032440186 3.161879539489746 3.161879539489746
Loss :  1.8056046962738037 2.988734245300293 2.988734245300293
Loss :  1.7414695024490356 4.406006813049316 4.406006813049316
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.7436907291412354 4.4495744705200195 4.4495744705200195
Loss :  1.7380344867706299 4.375387191772461 4.375387191772461
Loss :  1.7593681812286377 4.293458938598633 4.293458938598633
Total LOSS train 2.915492351238544 valid 4.381106853485107
CE LOSS train 1.8041834849577683 valid 0.4398420453071594
Contrastive LOSS train 2.915492351238544 valid 1.0733647346496582
EPOCH 222:
Loss :  1.804608702659607 2.5753798484802246 2.5753798484802246
Loss :  1.799978494644165 2.57266902923584 2.57266902923584
Loss :  1.8035290241241455 2.481290578842163 2.481290578842163
Loss :  1.8045495748519897 2.8161191940307617 2.8161191940307617
Loss :  1.8027620315551758 3.1027896404266357 3.1027896404266357
Loss :  1.8072590827941895 2.840254783630371 2.840254783630371
Loss :  1.800270676612854 3.080970048904419 3.080970048904419
Loss :  1.8012579679489136 3.4568228721618652 3.4568228721618652
Loss :  1.8002495765686035 3.0343871116638184 3.0343871116638184
Loss :  1.7988160848617554 3.439429998397827 3.439429998397827
Loss :  1.8074098825454712 3.0626931190490723 3.0626931190490723
Loss :  1.8027453422546387 2.974142551422119 2.974142551422119
Loss :  1.8069062232971191 2.7640039920806885 2.7640039920806885
Loss :  1.8096057176589966 2.8051061630249023 2.8051061630249023
Loss :  1.799707293510437 3.3098413944244385 3.3098413944244385
Loss :  1.8093523979187012 3.5313689708709717 3.5313689708709717
Loss :  1.8065991401672363 3.395159959793091 3.395159959793091
Loss :  1.803359866142273 3.410038948059082 3.410038948059082
Loss :  1.8038443326950073 3.2079970836639404 3.2079970836639404
Loss :  1.7987273931503296 3.1291117668151855 3.1291117668151855
  batch 20 loss: 1.7987273931503296, 3.1291117668151855, 3.1291117668151855
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8080732822418213 2.8634495735168457 2.8634495735168457
Loss :  1.8066569566726685 3.2504043579101562 3.2504043579101562
Loss :  1.8001521825790405 3.3649353981018066 3.3649353981018066
Loss :  1.8065552711486816 3.6051063537597656 3.6051063537597656
Loss :  1.8055728673934937 3.5741701126098633 3.5741701126098633
Loss :  1.8029296398162842 3.632641077041626 3.632641077041626
Loss :  1.8088585138320923 3.804309368133545 3.804309368133545
Loss :  1.8001619577407837 3.838179349899292 3.838179349899292
Loss :  1.8108851909637451 3.8125431537628174 3.8125431537628174
Loss :  1.7990678548812866 3.6441376209259033 3.6441376209259033
Loss :  1.8063874244689941 3.538743019104004 3.538743019104004
Loss :  1.797789216041565 2.7958219051361084 2.7958219051361084
Loss :  1.8052728176116943 3.2541821002960205 3.2541821002960205
Loss :  1.8035104274749756 2.5817723274230957 2.5817723274230957
Loss :  1.805446743965149 3.1310272216796875 3.1310272216796875
Loss :  1.805334448814392 2.7195277214050293 2.7195277214050293
Loss :  1.8049479722976685 2.525620222091675 2.525620222091675
Loss :  1.7991527318954468 2.315887928009033 2.315887928009033
Loss :  1.8042994737625122 2.344404935836792 2.344404935836792
Loss :  1.7998336553573608 2.419003963470459 2.419003963470459
  batch 40 loss: 1.7998336553573608, 2.419003963470459, 2.419003963470459
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8043594360351562 2.4455130100250244 2.4455130100250244
Loss :  1.801198124885559 2.8102223873138428 2.8102223873138428
Loss :  1.8033134937286377 2.695793390274048 2.695793390274048
Loss :  1.7977077960968018 2.585158109664917 2.585158109664917
Loss :  1.8055860996246338 2.6597654819488525 2.6597654819488525
Loss :  1.8021034002304077 2.957392454147339 2.957392454147339
Loss :  1.8006501197814941 2.35012149810791 2.35012149810791
Loss :  1.8025492429733276 2.951802968978882 2.951802968978882
Loss :  1.795730471611023 2.8729255199432373 2.8729255199432373
Loss :  1.8005236387252808 2.7442705631256104 2.7442705631256104
Loss :  1.7978129386901855 3.1743323802948 3.1743323802948
Loss :  1.8062664270401 2.8935608863830566 2.8935608863830566
Loss :  1.803821086883545 3.0919981002807617 3.0919981002807617
Loss :  1.804451584815979 2.7369654178619385 2.7369654178619385
Loss :  1.802675724029541 3.1509554386138916 3.1509554386138916
Loss :  1.8058149814605713 3.69553804397583 3.69553804397583
Loss :  1.8062158823013306 4.047961711883545 4.047961711883545
Loss :  1.8083763122558594 3.573507070541382 3.573507070541382
Loss :  1.8116194009780884 3.4423718452453613 3.4423718452453613
Loss :  1.8049713373184204 3.278346061706543 3.278346061706543
  batch 60 loss: 1.8049713373184204, 3.278346061706543, 3.278346061706543
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8062341213226318 3.6203110218048096 3.6203110218048096
Loss :  1.8022851943969727 4.043789386749268 4.043789386749268
Loss :  1.805108666419983 3.6468398571014404 3.6468398571014404
Loss :  1.8016277551651 3.5513458251953125 3.5513458251953125
Loss :  1.8051226139068604 3.1870226860046387 3.1870226860046387
Loss :  1.744110345840454 4.371638774871826 4.371638774871826
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.7460390329360962 4.395240306854248 4.395240306854248
Loss :  1.740705966949463 4.3484344482421875 4.3484344482421875
Loss :  1.7608743906021118 4.282010555267334 4.282010555267334
Total LOSS train 3.1109731674194334 valid 4.349331021308899
CE LOSS train 1.803670050547673 valid 0.44021859765052795
Contrastive LOSS train 3.1109731674194334 valid 1.0705026388168335
EPOCH 223:
Loss :  1.8033649921417236 3.542813539505005 3.542813539505005
Loss :  1.7989325523376465 3.371236562728882 3.371236562728882
Loss :  1.8028303384780884 2.989269495010376 2.989269495010376
Loss :  1.803841233253479 3.083717107772827 3.083717107772827
Loss :  1.8019858598709106 3.084798574447632 3.084798574447632
Loss :  1.806323766708374 3.239435911178589 3.239435911178589
Loss :  1.7989615201950073 3.5230562686920166 3.5230562686920166
Loss :  1.8001744747161865 3.1559572219848633 3.1559572219848633
Loss :  1.7997349500656128 3.1920173168182373 3.1920173168182373
Loss :  1.7975008487701416 3.354560375213623 3.354560375213623
Loss :  1.8066847324371338 3.280156373977661 3.280156373977661
Loss :  1.8024629354476929 2.766238212585449 2.766238212585449
Loss :  1.8061308860778809 2.794055938720703 2.794055938720703
Loss :  1.808747410774231 3.0533671379089355 3.0533671379089355
Loss :  1.7987736463546753 2.8634073734283447 2.8634073734283447
Loss :  1.8077880144119263 3.1105167865753174 3.1105167865753174
Loss :  1.8058382272720337 3.0130834579467773 3.0130834579467773
Loss :  1.80239999294281 3.3151443004608154 3.3151443004608154
Loss :  1.8036421537399292 3.417778491973877 3.417778491973877
Loss :  1.7976999282836914 3.4459385871887207 3.4459385871887207
  batch 20 loss: 1.7976999282836914, 3.4459385871887207, 3.4459385871887207
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.807605504989624 3.427819013595581 3.427819013595581
Loss :  1.8065329790115356 3.070582389831543 3.070582389831543
Loss :  1.7998170852661133 2.530923366546631 2.530923366546631
Loss :  1.806261658668518 3.120896339416504 3.120896339416504
Loss :  1.8061543703079224 3.475158929824829 3.475158929824829
Loss :  1.8023531436920166 3.360888719558716 3.360888719558716
Loss :  1.8083620071411133 3.2329630851745605 3.2329630851745605
Loss :  1.7997243404388428 2.7666807174682617 2.7666807174682617
Loss :  1.8101102113723755 2.8028817176818848 2.8028817176818848
Loss :  1.7986317873001099 2.6709187030792236 2.6709187030792236
Loss :  1.8061178922653198 2.8786191940307617 2.8786191940307617
Loss :  1.7974860668182373 2.6225290298461914 2.6225290298461914
Loss :  1.8046926259994507 2.6210687160491943 2.6210687160491943
Loss :  1.8031421899795532 2.853956937789917 2.853956937789917
Loss :  1.805363655090332 3.1652135848999023 3.1652135848999023
Loss :  1.8050532341003418 3.2738757133483887 3.2738757133483887
Loss :  1.8046064376831055 3.348632335662842 3.348632335662842
Loss :  1.7985308170318604 3.0341153144836426 3.0341153144836426
Loss :  1.803693413734436 3.5900495052337646 3.5900495052337646
Loss :  1.799133062362671 2.8011155128479004 2.8011155128479004
  batch 40 loss: 1.799133062362671, 2.8011155128479004, 2.8011155128479004
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8038361072540283 2.9783036708831787 2.9783036708831787
Loss :  1.8007972240447998 2.6931190490722656 2.6931190490722656
Loss :  1.8028068542480469 2.5034937858581543 2.5034937858581543
Loss :  1.7974698543548584 2.4182403087615967 2.4182403087615967
Loss :  1.80520761013031 2.73164701461792 2.73164701461792
Loss :  1.8015799522399902 2.565669298171997 2.565669298171997
Loss :  1.8000843524932861 2.45218563079834 2.45218563079834
Loss :  1.802298903465271 2.4857325553894043 2.4857325553894043
Loss :  1.7951903343200684 2.4453628063201904 2.4453628063201904
Loss :  1.8002216815948486 2.889521598815918 2.889521598815918
Loss :  1.7974212169647217 2.751039981842041 2.751039981842041
Loss :  1.8059577941894531 3.043853521347046 3.043853521347046
Loss :  1.8037941455841064 2.8965561389923096 2.8965561389923096
Loss :  1.804391622543335 3.117786169052124 3.117786169052124
Loss :  1.8024643659591675 3.140042304992676 3.140042304992676
Loss :  1.805649995803833 3.1463677883148193 3.1463677883148193
Loss :  1.8062207698822021 3.3207736015319824 3.3207736015319824
Loss :  1.8081923723220825 3.3158318996429443 3.3158318996429443
Loss :  1.8117122650146484 3.512521505355835 3.512521505355835
Loss :  1.8048200607299805 3.327558755874634 3.327558755874634
  batch 60 loss: 1.8048200607299805, 3.327558755874634, 3.327558755874634
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8063608407974243 3.663243293762207 3.663243293762207
Loss :  1.80221688747406 3.802825450897217 3.802825450897217
Loss :  1.8051280975341797 3.5042943954467773 3.5042943954467773
Loss :  1.801613450050354 3.49650239944458 3.49650239944458
Loss :  1.8050755262374878 3.367713451385498 3.367713451385498
Loss :  1.742629885673523 4.468340873718262 4.468340873718262
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.7446255683898926 4.395517826080322 4.395517826080322
Loss :  1.7392466068267822 4.373730659484863 4.373730659484863
Loss :  1.7597761154174805 4.207361698150635 4.207361698150635
Total LOSS train 3.0740865267240083 valid 4.3612377643585205
CE LOSS train 1.8031954343502339 valid 0.4399440288543701
Contrastive LOSS train 3.0740865267240083 valid 1.0518404245376587
EPOCH 224:
Loss :  1.8035260438919067 3.6130332946777344 3.6130332946777344
Loss :  1.7989721298217773 3.6543984413146973 3.6543984413146973
Loss :  1.8027139902114868 2.904311418533325 2.904311418533325
Loss :  1.803721308708191 3.3906614780426025 3.3906614780426025
Loss :  1.8018897771835327 2.5679705142974854 2.5679705142974854
Loss :  1.8061784505844116 2.8303725719451904 2.8303725719451904
Loss :  1.7989479303359985 2.7821919918060303 2.7821919918060303
Loss :  1.8000929355621338 2.663832187652588 2.663832187652588
Loss :  1.7993882894515991 2.486189842224121 2.486189842224121
Loss :  1.797308087348938 3.386812448501587 3.386812448501587
Loss :  1.806479573249817 3.3910491466522217 3.3910491466522217
Loss :  1.8022445440292358 3.656195640563965 3.656195640563965
Loss :  1.8059852123260498 3.1137936115264893 3.1137936115264893
Loss :  1.8085918426513672 3.081958055496216 3.081958055496216
Loss :  1.7985762357711792 2.8462939262390137 2.8462939262390137
Loss :  1.8078560829162598 2.995750904083252 2.995750904083252
Loss :  1.8058267831802368 2.8076183795928955 2.8076183795928955
Loss :  1.8025282621383667 2.9957640171051025 2.9957640171051025
Loss :  1.8034274578094482 2.788240671157837 2.788240671157837
Loss :  1.7980401515960693 2.7591443061828613 2.7591443061828613
  batch 20 loss: 1.7980401515960693, 2.7591443061828613, 2.7591443061828613
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8075826168060303 2.5190367698669434 2.5190367698669434
Loss :  1.806575894355774 2.7358131408691406 2.7358131408691406
Loss :  1.8000221252441406 2.2523341178894043 2.2523341178894043
Loss :  1.8064237833023071 2.894608497619629 2.894608497619629
Loss :  1.806226134300232 2.6500020027160645 2.6500020027160645
Loss :  1.8026880025863647 2.379671335220337 2.379671335220337
Loss :  1.808752179145813 2.597102642059326 2.597102642059326
Loss :  1.800058364868164 2.2437522411346436 2.2437522411346436
Loss :  1.8105850219726562 2.370427370071411 2.370427370071411
Loss :  1.799392580986023 3.021986246109009 3.021986246109009
Loss :  1.8066110610961914 3.3866872787475586 3.3866872787475586
Loss :  1.798201084136963 3.0929572582244873 3.0929572582244873
Loss :  1.8052711486816406 2.793933629989624 2.793933629989624
Loss :  1.8039660453796387 2.969672679901123 2.969672679901123
Loss :  1.805834174156189 3.052546977996826 3.052546977996826
Loss :  1.8055917024612427 2.9453442096710205 2.9453442096710205
Loss :  1.805407166481018 3.102160930633545 3.102160930633545
Loss :  1.799338459968567 2.281918525695801 2.281918525695801
Loss :  1.8045969009399414 2.726285696029663 2.726285696029663
Loss :  1.80020010471344 2.907261610031128 2.907261610031128
  batch 40 loss: 1.80020010471344, 2.907261610031128, 2.907261610031128
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8047139644622803 2.842925786972046 2.842925786972046
Loss :  1.8017032146453857 2.7902657985687256 2.7902657985687256
Loss :  1.8039919137954712 3.3706214427948 3.3706214427948
Loss :  1.79816734790802 3.3009579181671143 3.3009579181671143
Loss :  1.8059437274932861 3.8609137535095215 3.8609137535095215
Loss :  1.8024709224700928 3.649217367172241 3.649217367172241
Loss :  1.8006848096847534 2.837064504623413 2.837064504623413
Loss :  1.8030411005020142 3.152848482131958 3.152848482131958
Loss :  1.7959623336791992 2.4000391960144043 2.4000391960144043
Loss :  1.800606369972229 2.319640636444092 2.319640636444092
Loss :  1.7984884977340698 2.9466755390167236 2.9466755390167236
Loss :  1.8067588806152344 2.595228433609009 2.595228433609009
Loss :  1.8041967153549194 2.522456645965576 2.522456645965576
Loss :  1.805317759513855 3.460200548171997 3.460200548171997
Loss :  1.803205966949463 3.23195743560791 3.23195743560791
Loss :  1.806676983833313 3.5458481311798096 3.5458481311798096
Loss :  1.8071837425231934 3.1958281993865967 3.1958281993865967
Loss :  1.808629035949707 3.0375428199768066 3.0375428199768066
Loss :  1.8125277757644653 3.2695844173431396 3.2695844173431396
Loss :  1.8059886693954468 2.6339030265808105 2.6339030265808105
  batch 60 loss: 1.8059886693954468, 2.6339030265808105, 2.6339030265808105
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8069159984588623 2.484252452850342 2.484252452850342
Loss :  1.8029769659042358 2.4325406551361084 2.4325406551361084
Loss :  1.8056590557098389 2.6161768436431885 2.6161768436431885
Loss :  1.8020925521850586 3.1662659645080566 3.1662659645080566
Loss :  1.8056055307388306 2.130958080291748 2.130958080291748
Loss :  1.7435438632965088 4.429162979125977 4.429162979125977
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.745581030845642 4.395025730133057 4.395025730133057
Loss :  1.7400717735290527 4.301181316375732 4.301181316375732
Loss :  1.7610479593276978 4.366846084594727 4.366846084594727
Total LOSS train 2.914353847503662 valid 4.373054027557373
CE LOSS train 1.8036173765475934 valid 0.44026198983192444
Contrastive LOSS train 2.914353847503662 valid 1.0917115211486816
EPOCH 225:
Loss :  1.8044896125793457 3.1405017375946045 3.1405017375946045
Loss :  1.799792766571045 3.410139322280884 3.410139322280884
Loss :  1.8033276796340942 3.0597727298736572 3.0597727298736572
Loss :  1.8045705556869507 3.1809332370758057 3.1809332370758057
Loss :  1.8027094602584839 2.9838662147521973 2.9838662147521973
Loss :  1.8071879148483276 3.045973300933838 3.045973300933838
Loss :  1.7999699115753174 3.36861252784729 3.36861252784729
Loss :  1.8010040521621704 3.516925096511841 3.516925096511841
Loss :  1.8002395629882812 3.2855937480926514 3.2855937480926514
Loss :  1.7984668016433716 2.745448112487793 2.745448112487793
Loss :  1.8071261644363403 2.8695778846740723 2.8695778846740723
Loss :  1.8027572631835938 3.160892963409424 3.160892963409424
Loss :  1.8061829805374146 2.9591429233551025 2.9591429233551025
Loss :  1.808962106704712 2.835963726043701 2.835963726043701
Loss :  1.7996469736099243 2.7608723640441895 2.7608723640441895
Loss :  1.8089144229888916 3.029680013656616 3.029680013656616
Loss :  1.806188702583313 3.446655750274658 3.446655750274658
Loss :  1.803049921989441 3.3629560470581055 3.3629560470581055
Loss :  1.803844928741455 3.1502130031585693 3.1502130031585693
Loss :  1.798516869544983 2.9642112255096436 2.9642112255096436
  batch 20 loss: 1.798516869544983, 2.9642112255096436, 2.9642112255096436
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.807849407196045 3.2939155101776123 3.2939155101776123
Loss :  1.8064665794372559 3.30718994140625 3.30718994140625
Loss :  1.7999868392944336 3.3034238815307617 3.3034238815307617
Loss :  1.8060274124145508 3.3429293632507324 3.3429293632507324
Loss :  1.805448293685913 3.386127233505249 3.386127233505249
Loss :  1.8023934364318848 3.0922300815582275 3.0922300815582275
Loss :  1.8083667755126953 3.603318691253662 3.603318691253662
Loss :  1.799556851387024 3.6165177822113037 3.6165177822113037
Loss :  1.8103420734405518 2.9396188259124756 2.9396188259124756
Loss :  1.798514485359192 3.360712766647339 3.360712766647339
Loss :  1.806077480316162 3.1967239379882812 3.1967239379882812
Loss :  1.7973073720932007 3.3242363929748535 3.3242363929748535
Loss :  1.804457664489746 2.9962196350097656 2.9962196350097656
Loss :  1.8027979135513306 3.238154888153076 3.238154888153076
Loss :  1.8051520586013794 3.594499349594116 3.594499349594116
Loss :  1.8046828508377075 3.1754796504974365 3.1754796504974365
Loss :  1.804348349571228 2.7536957263946533 2.7536957263946533
Loss :  1.798296332359314 2.864563465118408 2.864563465118408
Loss :  1.8034428358078003 2.505037784576416 2.505037784576416
Loss :  1.7990020513534546 2.9382591247558594 2.9382591247558594
  batch 40 loss: 1.7990020513534546, 2.9382591247558594, 2.9382591247558594
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.803628921508789 3.4400365352630615 3.4400365352630615
Loss :  1.8005313873291016 3.2052698135375977 3.2052698135375977
Loss :  1.8024513721466064 2.7394442558288574 2.7394442558288574
Loss :  1.7973015308380127 3.19952392578125 3.19952392578125
Loss :  1.8048515319824219 2.60585880279541 2.60585880279541
Loss :  1.8013828992843628 3.128723382949829 3.128723382949829
Loss :  1.7994894981384277 2.9811675548553467 2.9811675548553467
Loss :  1.8016844987869263 2.62191104888916 2.62191104888916
Loss :  1.7945890426635742 3.482846260070801 3.482846260070801
Loss :  1.7995893955230713 2.6137356758117676 2.6137356758117676
Loss :  1.7968336343765259 2.741297483444214 2.741297483444214
Loss :  1.8051289319992065 2.4604928493499756 2.4604928493499756
Loss :  1.8031795024871826 2.4511401653289795 2.4511401653289795
Loss :  1.8038434982299805 2.7354254722595215 2.7354254722595215
Loss :  1.801993489265442 2.7199599742889404 2.7199599742889404
Loss :  1.8050602674484253 2.59948992729187 2.59948992729187
Loss :  1.8059322834014893 3.502795457839966 3.502795457839966
Loss :  1.8076694011688232 2.571765899658203 2.571765899658203
Loss :  1.811287522315979 2.8372788429260254 2.8372788429260254
Loss :  1.8044085502624512 2.9849748611450195 2.9849748611450195
  batch 60 loss: 1.8044085502624512, 2.9849748611450195, 2.9849748611450195
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8060529232025146 3.067833423614502 3.067833423614502
Loss :  1.8020517826080322 3.0552597045898438 3.0552597045898438
Loss :  1.8049286603927612 3.702908992767334 3.702908992767334
Loss :  1.801269292831421 3.475440502166748 3.475440502166748
Loss :  1.804882287979126 2.9071884155273438 2.9071884155273438
Loss :  1.743515133857727 4.380777835845947 4.380777835845947
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.745475172996521 4.377156734466553 4.377156734466553
Loss :  1.740090250968933 4.395305156707764 4.395305156707764
Loss :  1.7606747150421143 4.43683385848999 4.43683385848999
Total LOSS train 3.0760393106020416 valid 4.3975183963775635
CE LOSS train 1.8031921203319843 valid 0.44016867876052856
Contrastive LOSS train 3.0760393106020416 valid 1.1092084646224976
EPOCH 226:
Loss :  1.8031381368637085 3.555591583251953 3.555591583251953
Loss :  1.7990620136260986 3.5923306941986084 3.5923306941986084
Loss :  1.8025251626968384 3.233558177947998 3.233558177947998
Loss :  1.8035149574279785 3.4103195667266846 3.4103195667266846
Loss :  1.8017467260360718 3.074838876724243 3.074838876724243
Loss :  1.806259036064148 2.8648247718811035 2.8648247718811035
Loss :  1.7989946603775024 2.8115079402923584 2.8115079402923584
Loss :  1.8001633882522583 3.2655816078186035 3.2655816078186035
Loss :  1.7991286516189575 3.1755716800689697 3.1755716800689697
Loss :  1.7972389459609985 2.813739538192749 2.813739538192749
Loss :  1.8065110445022583 3.2742083072662354 3.2742083072662354
Loss :  1.8021475076675415 3.153169870376587 3.153169870376587
Loss :  1.8060863018035889 3.293771982192993 3.293771982192993
Loss :  1.80864679813385 2.938930034637451 2.938930034637451
Loss :  1.798064947128296 2.625441074371338 2.625441074371338
Loss :  1.808098316192627 2.669949531555176 2.669949531555176
Loss :  1.8058574199676514 2.658862352371216 2.658862352371216
Loss :  1.8021595478057861 3.023193597793579 3.023193597793579
Loss :  1.8031421899795532 2.678969144821167 2.678969144821167
Loss :  1.7976514101028442 3.099724054336548 3.099724054336548
  batch 20 loss: 1.7976514101028442, 3.099724054336548, 3.099724054336548
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8070993423461914 2.8257675170898438 2.8257675170898438
Loss :  1.8061320781707764 2.8537862300872803 2.8537862300872803
Loss :  1.7994130849838257 3.0956575870513916 3.0956575870513916
Loss :  1.8059172630310059 3.260746717453003 3.260746717453003
Loss :  1.8053478002548218 3.0655245780944824 3.0655245780944824
Loss :  1.8023027181625366 2.9381754398345947 2.9381754398345947
Loss :  1.8084019422531128 3.374279022216797 3.374279022216797
Loss :  1.799612283706665 3.0940134525299072 3.0940134525299072
Loss :  1.8107595443725586 3.4405553340911865 3.4405553340911865
Loss :  1.798492193222046 3.139509677886963 3.139509677886963
Loss :  1.8062208890914917 3.578301191329956 3.578301191329956
Loss :  1.7973792552947998 3.5378801822662354 3.5378801822662354
Loss :  1.8047820329666138 3.812476396560669 3.812476396560669
Loss :  1.8032058477401733 3.2508654594421387 3.2508654594421387
Loss :  1.8050919771194458 3.1101927757263184 3.1101927757263184
Loss :  1.8052973747253418 2.5340158939361572 2.5340158939361572
Loss :  1.8044482469558716 2.832235097885132 2.832235097885132
Loss :  1.7986884117126465 3.243819236755371 3.243819236755371
Loss :  1.8034517765045166 2.7076597213745117 2.7076597213745117
Loss :  1.799156665802002 2.932032585144043 2.932032585144043
  batch 40 loss: 1.799156665802002, 2.932032585144043, 2.932032585144043
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.803946614265442 2.764404535293579 2.764404535293579
Loss :  1.8008052110671997 2.6293082237243652 2.6293082237243652
Loss :  1.8025065660476685 2.3644022941589355 2.3644022941589355
Loss :  1.7975115776062012 2.758134126663208 2.758134126663208
Loss :  1.8050482273101807 2.3521339893341064 2.3521339893341064
Loss :  1.801781177520752 2.8311877250671387 2.8311877250671387
Loss :  1.7994636297225952 2.8160247802734375 2.8160247802734375
Loss :  1.8019285202026367 2.5114259719848633 2.5114259719848633
Loss :  1.7947344779968262 3.1548514366149902 3.1548514366149902
Loss :  1.7996286153793335 2.660721778869629 2.660721778869629
Loss :  1.7970186471939087 2.6785354614257812 2.6785354614257812
Loss :  1.805300235748291 2.4829819202423096 2.4829819202423096
Loss :  1.8033419847488403 2.983548164367676 2.983548164367676
Loss :  1.8041807413101196 3.095482349395752 3.095482349395752
Loss :  1.8021148443222046 3.368908405303955 3.368908405303955
Loss :  1.8050386905670166 2.706031322479248 2.706031322479248
Loss :  1.8060699701309204 2.8827617168426514 2.8827617168426514
Loss :  1.8076287508010864 2.7262609004974365 2.7262609004974365
Loss :  1.81135094165802 3.325209140777588 3.325209140777588
Loss :  1.8043018579483032 3.119459867477417 3.119459867477417
  batch 60 loss: 1.8043018579483032, 3.119459867477417, 3.119459867477417
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8058727979660034 3.468944787979126 3.468944787979126
Loss :  1.8020014762878418 2.937915802001953 2.937915802001953
Loss :  1.8048675060272217 3.146993398666382 3.146993398666382
Loss :  1.8010199069976807 2.9441964626312256 2.9441964626312256
Loss :  1.8046566247940063 2.7638771533966064 2.7638771533966064
Loss :  1.7428110837936401 4.409222602844238 4.409222602844238
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.7447088956832886 4.464360237121582 4.464360237121582
Loss :  1.7392845153808594 4.315218448638916 4.315218448638916
Loss :  1.7600597143173218 4.384061813354492 4.384061813354492
Total LOSS train 3.004850464600783 valid 4.393215775489807
CE LOSS train 1.8029762689883893 valid 0.44001492857933044
Contrastive LOSS train 3.004850464600783 valid 1.096015453338623
EPOCH 227:
Loss :  1.803123116493225 2.860184669494629 2.860184669494629
Loss :  1.7985234260559082 3.2680134773254395 3.2680134773254395
Loss :  1.802111268043518 3.69313907623291 3.69313907623291
Loss :  1.803252100944519 3.1153833866119385 3.1153833866119385
Loss :  1.8014668226242065 2.9618279933929443 2.9618279933929443
Loss :  1.8059180974960327 2.7035908699035645 2.7035908699035645
Loss :  1.7985800504684448 2.9351205825805664 2.9351205825805664
Loss :  1.7997242212295532 2.562190294265747 2.562190294265747
Loss :  1.7989426851272583 2.5655107498168945 2.5655107498168945
Loss :  1.7969030141830444 3.19035005569458 3.19035005569458
Loss :  1.8062459230422974 2.9297444820404053 2.9297444820404053
Loss :  1.8020398616790771 2.879870891571045 2.879870891571045
Loss :  1.8058167695999146 2.553112030029297 2.553112030029297
Loss :  1.8084992170333862 2.4283883571624756 2.4283883571624756
Loss :  1.7979899644851685 2.6639411449432373 2.6639411449432373
Loss :  1.8080064058303833 2.5113348960876465 2.5113348960876465
Loss :  1.8057615756988525 3.006091833114624 3.006091833114624
Loss :  1.8019776344299316 2.7641146183013916 2.7641146183013916
Loss :  1.8032104969024658 2.5829756259918213 2.5829756259918213
Loss :  1.7971481084823608 2.9228546619415283 2.9228546619415283
  batch 20 loss: 1.7971481084823608, 2.9228546619415283, 2.9228546619415283
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8070478439331055 2.900635004043579 2.900635004043579
Loss :  1.8059189319610596 3.082972288131714 3.082972288131714
Loss :  1.7989733219146729 2.819176435470581 2.819176435470581
Loss :  1.805813193321228 3.429337501525879 3.429337501525879
Loss :  1.8052434921264648 2.860394239425659 2.860394239425659
Loss :  1.8021520376205444 2.7740745544433594 2.7740745544433594
Loss :  1.8080438375473022 2.7312378883361816 2.7312378883361816
Loss :  1.7992149591445923 3.060713529586792 3.060713529586792
Loss :  1.8103176355361938 3.0402166843414307 3.0402166843414307
Loss :  1.7980287075042725 2.358079671859741 2.358079671859741
Loss :  1.8059947490692139 2.5532853603363037 2.5532853603363037
Loss :  1.7971069812774658 2.5891873836517334 2.5891873836517334
Loss :  1.8045095205307007 3.2636773586273193 3.2636773586273193
Loss :  1.8026763200759888 2.701019525527954 2.701019525527954
Loss :  1.8048162460327148 3.0602498054504395 3.0602498054504395
Loss :  1.8049408197402954 3.9332730770111084 3.9332730770111084
Loss :  1.804152488708496 3.287048101425171 3.287048101425171
Loss :  1.7982887029647827 3.3754429817199707 3.3754429817199707
Loss :  1.8030728101730347 3.139601230621338 3.139601230621338
Loss :  1.798677682876587 3.091547966003418 3.091547966003418
  batch 40 loss: 1.798677682876587, 3.091547966003418, 3.091547966003418
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.803605079650879 3.1748223304748535 3.1748223304748535
Loss :  1.8005414009094238 2.7099862098693848 2.7099862098693848
Loss :  1.8021512031555176 3.0086557865142822 3.0086557865142822
Loss :  1.7972153425216675 2.795142412185669 2.795142412185669
Loss :  1.8047906160354614 2.426254987716675 2.426254987716675
Loss :  1.8016817569732666 3.1468896865844727 3.1468896865844727
Loss :  1.799100637435913 2.6068832874298096 2.6068832874298096
Loss :  1.801715612411499 2.546325922012329 2.546325922012329
Loss :  1.7947828769683838 2.3605072498321533 2.3605072498321533
Loss :  1.799458384513855 2.3297860622406006 2.3297860622406006
Loss :  1.796915054321289 2.506202220916748 2.506202220916748
Loss :  1.8053277730941772 2.3992602825164795 2.3992602825164795
Loss :  1.8034722805023193 2.5439326763153076 2.5439326763153076
Loss :  1.8046094179153442 2.3148906230926514 2.3148906230926514
Loss :  1.8022319078445435 2.3804380893707275 2.3804380893707275
Loss :  1.8053069114685059 2.1351420879364014 2.1351420879364014
Loss :  1.8065619468688965 2.53822922706604 2.53822922706604
Loss :  1.8083134889602661 2.5342440605163574 2.5342440605163574
Loss :  1.8119059801101685 3.3389923572540283 3.3389923572540283
Loss :  1.8044620752334595 2.7146034240722656 2.7146034240722656
  batch 60 loss: 1.8044620752334595, 2.7146034240722656, 2.7146034240722656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8064619302749634 2.9236862659454346 2.9236862659454346
Loss :  1.8024591207504272 2.9418630599975586 2.9418630599975586
Loss :  1.8052465915679932 2.9586703777313232 2.9586703777313232
Loss :  1.8015800714492798 3.4212329387664795 3.4212329387664795
Loss :  1.8056055307388306 3.5040123462677 3.5040123462677
Loss :  1.7385233640670776 4.41841983795166 4.41841983795166
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.7408472299575806 4.437975883483887 4.437975883483887
Loss :  1.7350178956985474 4.349023342132568 4.349023342132568
Loss :  1.7570523023605347 4.043336868286133 4.043336868286133
Total LOSS train 2.8524548347179706 valid 4.312188982963562
CE LOSS train 1.8028574466705323 valid 0.43926307559013367
Contrastive LOSS train 2.8524548347179706 valid 1.0108342170715332
EPOCH 228:
Loss :  1.804107904434204 3.339042901992798 3.339042901992798
Loss :  1.798987865447998 3.166717052459717 3.166717052459717
Loss :  1.802891731262207 3.0253140926361084 3.0253140926361084
Loss :  1.8040730953216553 2.7744956016540527 2.7744956016540527
Loss :  1.8022483587265015 2.7025511264801025 2.7025511264801025
Loss :  1.8066070079803467 2.44804048538208 2.44804048538208
Loss :  1.7995301485061646 2.8283915519714355 2.8283915519714355
Loss :  1.8007384538650513 3.03835391998291 3.03835391998291
Loss :  1.7995277643203735 3.056473970413208 3.056473970413208
Loss :  1.7982271909713745 2.8021199703216553 2.8021199703216553
Loss :  1.8070138692855835 2.942042112350464 2.942042112350464
Loss :  1.802223563194275 3.1355154514312744 3.1355154514312744
Loss :  1.8063600063323975 3.0849015712738037 3.0849015712738037
Loss :  1.8090568780899048 3.448124885559082 3.448124885559082
Loss :  1.7982860803604126 3.089123487472534 3.089123487472534
Loss :  1.8089408874511719 3.2470998764038086 3.2470998764038086
Loss :  1.8062448501586914 2.832676649093628 2.832676649093628
Loss :  1.8025113344192505 3.717247247695923 3.717247247695923
Loss :  1.8034406900405884 2.7898824214935303 2.7898824214935303
Loss :  1.7977485656738281 2.584085702896118 2.584085702896118
  batch 20 loss: 1.7977485656738281, 2.584085702896118, 2.584085702896118
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8073700666427612 2.503509044647217 2.503509044647217
Loss :  1.8060271739959717 2.9590342044830322 2.9590342044830322
Loss :  1.799233317375183 2.7755556106567383 2.7755556106567383
Loss :  1.8059957027435303 2.822894811630249 2.822894811630249
Loss :  1.8052794933319092 2.9502649307250977 2.9502649307250977
Loss :  1.8026427030563354 3.1325526237487793 3.1325526237487793
Loss :  1.8085932731628418 3.0142998695373535 3.0142998695373535
Loss :  1.7996397018432617 2.7513835430145264 2.7513835430145264
Loss :  1.810943841934204 3.2305915355682373 3.2305915355682373
Loss :  1.7989908456802368 3.0207748413085938 3.0207748413085938
Loss :  1.8066232204437256 2.8239495754241943 2.8239495754241943
Loss :  1.7977951765060425 2.824946165084839 2.824946165084839
Loss :  1.8052080869674683 2.448746681213379 2.448746681213379
Loss :  1.8037642240524292 2.3618903160095215 2.3618903160095215
Loss :  1.8053598403930664 2.7873332500457764 2.7873332500457764
Loss :  1.8057301044464111 2.4143056869506836 2.4143056869506836
Loss :  1.8049030303955078 2.6718757152557373 2.6718757152557373
Loss :  1.7996100187301636 2.5941615104675293 2.5941615104675293
Loss :  1.804213285446167 3.0545501708984375 3.0545501708984375
Loss :  1.8001455068588257 2.796408176422119 2.796408176422119
  batch 40 loss: 1.8001455068588257, 2.796408176422119, 2.796408176422119
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8048418760299683 2.9695510864257812 2.9695510864257812
Loss :  1.801672101020813 2.927647352218628 2.927647352218628
Loss :  1.8037269115447998 3.0766921043395996 3.0766921043395996
Loss :  1.7983101606369019 2.567540168762207 2.567540168762207
Loss :  1.8059186935424805 2.8297183513641357 2.8297183513641357
Loss :  1.8029707670211792 2.935757637023926 2.935757637023926
Loss :  1.8007638454437256 2.713913917541504 2.713913917541504
Loss :  1.8026939630508423 2.8183836936950684 2.8183836936950684
Loss :  1.7964906692504883 3.005920886993408 3.005920886993408
Loss :  1.8006504774093628 2.916860818862915 2.916860818862915
Loss :  1.7977479696273804 2.9228062629699707 2.9228062629699707
Loss :  1.806115746498108 3.3287315368652344 3.3287315368652344
Loss :  1.803730845451355 3.415381908416748 3.415381908416748
Loss :  1.804402232170105 3.4381096363067627 3.4381096363067627
Loss :  1.8023741245269775 3.319193124771118 3.319193124771118
Loss :  1.8053169250488281 3.5965511798858643 3.5965511798858643
Loss :  1.806827425956726 3.2091782093048096 3.2091782093048096
Loss :  1.808443546295166 3.5566632747650146 3.5566632747650146
Loss :  1.8119477033615112 3.5313804149627686 3.5313804149627686
Loss :  1.8048603534698486 2.7864928245544434 2.7864928245544434
  batch 60 loss: 1.8048603534698486, 2.7864928245544434, 2.7864928245544434
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8063411712646484 3.4140918254852295 3.4140918254852295
Loss :  1.8023278713226318 2.8779916763305664 2.8779916763305664
Loss :  1.8051897287368774 2.807382345199585 2.807382345199585
Loss :  1.8014073371887207 3.1164865493774414 3.1164865493774414
Loss :  1.805069088935852 2.2813310623168945 2.2813310623168945
Loss :  1.738836407661438 4.414207935333252 4.414207935333252
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.741133213043213 4.373723030090332 4.373723030090332
Loss :  1.7354350090026855 4.266016960144043 4.266016960144043
Loss :  1.75731360912323 4.247848987579346 4.247848987579346
Total LOSS train 2.9593074798583983 valid 4.325449228286743
CE LOSS train 1.803491944533128 valid 0.4393284022808075
Contrastive LOSS train 2.9593074798583983 valid 1.0619622468948364
EPOCH 229:
Loss :  1.8032923936843872 3.2495648860931396 3.2495648860931396
Loss :  1.7990418672561646 3.0858185291290283 3.0858185291290283
Loss :  1.8023220300674438 2.3744513988494873 2.3744513988494873
Loss :  1.80320143699646 2.6604840755462646 2.6604840755462646
Loss :  1.801393747329712 2.7568235397338867 2.7568235397338867
Loss :  1.8058686256408691 3.2306225299835205 3.2306225299835205
Loss :  1.79889714717865 3.4468367099761963 3.4468367099761963
Loss :  1.799678087234497 3.6376419067382812 3.6376419067382812
Loss :  1.7988673448562622 3.5044147968292236 3.5044147968292236
Loss :  1.7968754768371582 3.3721487522125244 3.3721487522125244
Loss :  1.80593740940094 3.37650990486145 3.37650990486145
Loss :  1.8016610145568848 3.4502182006835938 3.4502182006835938
Loss :  1.805674433708191 3.192131996154785 3.192131996154785
Loss :  1.8081148862838745 2.6795332431793213 2.6795332431793213
Loss :  1.7978789806365967 3.1890060901641846 3.1890060901641846
Loss :  1.807550311088562 3.093818187713623 3.093818187713623
Loss :  1.805482029914856 2.8004565238952637 2.8004565238952637
Loss :  1.8013626337051392 3.0465617179870605 3.0465617179870605
Loss :  1.8026119470596313 2.6202452182769775 2.6202452182769775
Loss :  1.7968827486038208 3.415168285369873 3.415168285369873
  batch 20 loss: 1.7968827486038208, 3.415168285369873, 3.415168285369873
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8067013025283813 3.435861825942993 3.435861825942993
Loss :  1.8054531812667847 3.19539475440979 3.19539475440979
Loss :  1.7986476421356201 3.3725922107696533 3.3725922107696533
Loss :  1.8052599430084229 4.0350165367126465 4.0350165367126465
Loss :  1.8045440912246704 3.8654110431671143 3.8654110431671143
Loss :  1.8017629384994507 3.1412951946258545 3.1412951946258545
Loss :  1.807493805885315 3.5298428535461426 3.5298428535461426
Loss :  1.7988624572753906 3.0593252182006836 3.0593252182006836
Loss :  1.8098639249801636 3.6179912090301514 3.6179912090301514
Loss :  1.7978794574737549 2.8684802055358887 2.8684802055358887
Loss :  1.805745244026184 2.99059796333313 2.99059796333313
Loss :  1.7968378067016602 2.7311019897460938 2.7311019897460938
Loss :  1.8044178485870361 2.6767492294311523 2.6767492294311523
Loss :  1.8026584386825562 2.9349939823150635 2.9349939823150635
Loss :  1.8046715259552002 3.0153818130493164 3.0153818130493164
Loss :  1.8049869537353516 2.9423775672912598 2.9423775672912598
Loss :  1.8041099309921265 3.2508249282836914 3.2508249282836914
Loss :  1.7983025312423706 2.7727317810058594 2.7727317810058594
Loss :  1.8033111095428467 3.0711233615875244 3.0711233615875244
Loss :  1.7990204095840454 3.0866944789886475 3.0866944789886475
  batch 40 loss: 1.7990204095840454, 3.0866944789886475, 3.0866944789886475
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.803850769996643 2.623274326324463 2.623274326324463
Loss :  1.8007404804229736 2.583224058151245 2.583224058151245
Loss :  1.802675724029541 2.530435085296631 2.530435085296631
Loss :  1.7976621389389038 2.4306247234344482 2.4306247234344482
Loss :  1.8052786588668823 3.1513938903808594 3.1513938903808594
Loss :  1.8020085096359253 2.6056153774261475 2.6056153774261475
Loss :  1.799914002418518 3.0665671825408936 3.0665671825408936
Loss :  1.8022924661636353 3.358185291290283 3.358185291290283
Loss :  1.7954093217849731 3.2585577964782715 3.2585577964782715
Loss :  1.8002485036849976 2.780029058456421 2.780029058456421
Loss :  1.7975568771362305 2.9946541786193848 2.9946541786193848
Loss :  1.8056583404541016 3.1130075454711914 3.1130075454711914
Loss :  1.803609013557434 2.9644570350646973 2.9644570350646973
Loss :  1.8046826124191284 2.9004435539245605 2.9004435539245605
Loss :  1.80245840549469 2.945451259613037 2.945451259613037
Loss :  1.8054496049880981 2.7484419345855713 2.7484419345855713
Loss :  1.8064956665039062 3.3214480876922607 3.3214480876922607
Loss :  1.8083184957504272 3.1167590618133545 3.1167590618133545
Loss :  1.8118116855621338 3.2594003677368164 3.2594003677368164
Loss :  1.8043522834777832 3.358879804611206 3.358879804611206
  batch 60 loss: 1.8043522834777832, 3.358879804611206, 3.358879804611206
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8062034845352173 3.4410271644592285 3.4410271644592285
Loss :  1.8024262189865112 2.916001081466675 2.916001081466675
Loss :  1.80507493019104 2.8610129356384277 2.8610129356384277
Loss :  1.801518201828003 3.3563485145568848 3.3563485145568848
Loss :  1.8052849769592285 3.0700883865356445 3.0700883865356445
Loss :  1.7382458448410034 4.421148300170898 4.421148300170898
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.7404682636260986 4.457109451293945 4.457109451293945
Loss :  1.7348213195800781 4.323339462280273 4.323339462280273
Loss :  1.7565048933029175 4.294579029083252 4.294579029083252
Total LOSS train 3.085101112952599 valid 4.374044060707092
CE LOSS train 1.8028324072177593 valid 0.43912622332572937
Contrastive LOSS train 3.085101112952599 valid 1.073644757270813
EPOCH 230:
Loss :  1.8035378456115723 3.3036837577819824 3.3036837577819824
Loss :  1.7988550662994385 3.21844220161438 3.21844220161438
Loss :  1.8025339841842651 3.437574625015259 3.437574625015259
Loss :  1.8034406900405884 3.393606424331665 3.393606424331665
Loss :  1.8015598058700562 3.4868998527526855 3.4868998527526855
Loss :  1.8059916496276855 3.122810125350952 3.122810125350952
Loss :  1.798915982246399 2.881800889968872 2.881800889968872
Loss :  1.800025463104248 2.508408784866333 2.508408784866333
Loss :  1.7989263534545898 3.3429338932037354 3.3429338932037354
Loss :  1.797223448753357 3.2410330772399902 3.2410330772399902
Loss :  1.8063883781433105 3.429260492324829 3.429260492324829
Loss :  1.801990032196045 3.45635986328125 3.45635986328125
Loss :  1.8058369159698486 3.4679486751556396 3.4679486751556396
Loss :  1.8084216117858887 3.863264322280884 3.863264322280884
Loss :  1.7978670597076416 3.2348480224609375 3.2348480224609375
Loss :  1.8079783916473389 3.7374470233917236 3.7374470233917236
Loss :  1.8057128190994263 2.6604225635528564 2.6604225635528564
Loss :  1.801729679107666 2.6918513774871826 2.6918513774871826
Loss :  1.8030253648757935 2.5735325813293457 2.5735325813293457
Loss :  1.7973190546035767 2.6049811840057373 2.6049811840057373
  batch 20 loss: 1.7973190546035767, 2.6049811840057373, 2.6049811840057373
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8071048259735107 2.785567045211792 2.785567045211792
Loss :  1.805891513824463 2.698840379714966 2.698840379714966
Loss :  1.7993065118789673 2.4687654972076416 2.4687654972076416
Loss :  1.805843710899353 2.5358712673187256 2.5358712673187256
Loss :  1.805631399154663 2.7541089057922363 2.7541089057922363
Loss :  1.8026305437088013 2.4234819412231445 2.4234819412231445
Loss :  1.8082069158554077 2.5653412342071533 2.5653412342071533
Loss :  1.7998067140579224 2.5953209400177 2.5953209400177
Loss :  1.8103508949279785 2.643543243408203 2.643543243408203
Loss :  1.798636555671692 2.398881435394287 2.398881435394287
Loss :  1.8062639236450195 2.7646608352661133 2.7646608352661133
Loss :  1.7977224588394165 2.393230676651001 2.393230676651001
Loss :  1.8049745559692383 2.8775014877319336 2.8775014877319336
Loss :  1.8036322593688965 2.7967984676361084 2.7967984676361084
Loss :  1.8054580688476562 2.8441836833953857 2.8441836833953857
Loss :  1.805711269378662 2.7939271926879883 2.7939271926879883
Loss :  1.8047515153884888 2.6651740074157715 2.6651740074157715
Loss :  1.7989531755447388 2.954843759536743 2.954843759536743
Loss :  1.8034791946411133 3.0679421424865723 3.0679421424865723
Loss :  1.7992278337478638 3.1076629161834717 3.1076629161834717
  batch 40 loss: 1.7992278337478638, 3.1076629161834717, 3.1076629161834717
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8042256832122803 3.456803560256958 3.456803560256958
Loss :  1.8013309240341187 3.294952154159546 3.294952154159546
Loss :  1.8032788038253784 2.963710308074951 2.963710308074951
Loss :  1.7979716062545776 3.2019028663635254 3.2019028663635254
Loss :  1.8054896593093872 3.209973096847534 3.209973096847534
Loss :  1.8021074533462524 2.496030569076538 2.496030569076538
Loss :  1.7997170686721802 2.4971320629119873 2.4971320629119873
Loss :  1.80226731300354 2.3916923999786377 2.3916923999786377
Loss :  1.795245885848999 2.9674527645111084 2.9674527645111084
Loss :  1.7998614311218262 2.9880945682525635 2.9880945682525635
Loss :  1.7968968152999878 3.1906955242156982 3.1906955242156982
Loss :  1.8050236701965332 3.699392080307007 3.699392080307007
Loss :  1.803139328956604 3.3370769023895264 3.3370769023895264
Loss :  1.8038266897201538 3.2732913494110107 3.2732913494110107
Loss :  1.8018780946731567 3.178335666656494 3.178335666656494
Loss :  1.804668664932251 3.453141450881958 3.453141450881958
Loss :  1.8059660196304321 3.3332149982452393 3.3332149982452393
Loss :  1.8078361749649048 2.987450361251831 2.987450361251831
Loss :  1.8113021850585938 2.7587239742279053 2.7587239742279053
Loss :  1.8038547039031982 3.1541903018951416 3.1541903018951416
  batch 60 loss: 1.8038547039031982, 3.1541903018951416, 3.1541903018951416
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8057422637939453 3.1563949584960938 3.1563949584960938
Loss :  1.8017116785049438 3.1412477493286133 3.1412477493286133
Loss :  1.8048038482666016 2.8286194801330566 2.8286194801330566
Loss :  1.801047682762146 2.4803647994995117 2.4803647994995117
Loss :  1.804601788520813 2.183180093765259 2.183180093765259
Loss :  1.7365232706069946 4.4265313148498535 4.4265313148498535
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.7388505935668945 4.433707237243652 4.433707237243652
Loss :  1.7331615686416626 4.344931602478027 4.344931602478027
Loss :  1.7554898262023926 4.3213605880737305 4.3213605880737305
Total LOSS train 2.975628012877244 valid 4.381632685661316
CE LOSS train 1.8029947519302367 valid 0.43887245655059814
Contrastive LOSS train 2.975628012877244 valid 1.0803401470184326
EPOCH 231:
Loss :  1.8031375408172607 2.734410524368286 2.734410524368286
Loss :  1.7989002466201782 2.777824878692627 2.777824878692627
Loss :  1.8022494316101074 2.9276556968688965 2.9276556968688965
Loss :  1.8031389713287354 2.9715325832366943 2.9715325832366943
Loss :  1.8013241291046143 2.7498090267181396 2.7498090267181396
Loss :  1.8057647943496704 2.9475386142730713 2.9475386142730713
Loss :  1.7991958856582642 3.020880937576294 3.020880937576294
Loss :  1.8000667095184326 3.081132650375366 3.081132650375366
Loss :  1.7984933853149414 3.188812255859375 3.188812255859375
Loss :  1.7973204851150513 2.6713056564331055 2.6713056564331055
Loss :  1.806235671043396 3.0956616401672363 3.0956616401672363
Loss :  1.8017722368240356 2.8061459064483643 2.8061459064483643
Loss :  1.8058897256851196 2.876894235610962 2.876894235610962
Loss :  1.8083235025405884 2.672333002090454 2.672333002090454
Loss :  1.7980176210403442 2.9568557739257812 2.9568557739257812
Loss :  1.8083117008209229 3.009047508239746 3.009047508239746
Loss :  1.8058438301086426 3.322862386703491 3.322862386703491
Loss :  1.802034616470337 3.5187904834747314 3.5187904834747314
Loss :  1.8029847145080566 2.9539871215820312 2.9539871215820312
Loss :  1.7977077960968018 2.789489984512329 2.789489984512329
  batch 20 loss: 1.7977077960968018, 2.789489984512329, 2.789489984512329
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8072311878204346 3.162323474884033 3.162323474884033
Loss :  1.8058559894561768 2.940056085586548 2.940056085586548
Loss :  1.7991890907287598 2.5046958923339844 2.5046958923339844
Loss :  1.8057407140731812 2.9935877323150635 2.9935877323150635
Loss :  1.8053523302078247 3.131382465362549 3.131382465362549
Loss :  1.802738904953003 2.915433406829834 2.915433406829834
Loss :  1.808346152305603 3.032421350479126 3.032421350479126
Loss :  1.7997009754180908 3.3971965312957764 3.3971965312957764
Loss :  1.810550332069397 3.6538989543914795 3.6538989543914795
Loss :  1.7988907098770142 3.787771701812744 3.787771701812744
Loss :  1.8065437078475952 3.5643553733825684 3.5643553733825684
Loss :  1.7976826429367065 3.888502597808838 3.888502597808838
Loss :  1.8049671649932861 3.0163369178771973 3.0163369178771973
Loss :  1.8039482831954956 3.1787819862365723 3.1787819862365723
Loss :  1.8053154945373535 3.1926302909851074 3.1926302909851074
Loss :  1.8055369853973389 2.835266351699829 2.835266351699829
Loss :  1.8044408559799194 3.195375919342041 3.195375919342041
Loss :  1.7986118793487549 2.9328854084014893 2.9328854084014893
Loss :  1.8032572269439697 3.232252836227417 3.232252836227417
Loss :  1.7990138530731201 2.8470866680145264 2.8470866680145264
  batch 40 loss: 1.7990138530731201, 2.8470866680145264, 2.8470866680145264
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.803818941116333 3.4982173442840576 3.4982173442840576
Loss :  1.8007862567901611 3.4431111812591553 3.4431111812591553
Loss :  1.8023498058319092 3.8880906105041504 3.8880906105041504
Loss :  1.7975728511810303 3.1626038551330566 3.1626038551330566
Loss :  1.8047747611999512 2.9536678791046143 2.9536678791046143
Loss :  1.8013335466384888 3.4550905227661133 3.4550905227661133
Loss :  1.7987817525863647 3.291417360305786 3.291417360305786
Loss :  1.801392674446106 3.419430732727051 3.419430732727051
Loss :  1.7940279245376587 3.421851873397827 3.421851873397827
Loss :  1.799196481704712 2.7973508834838867 2.7973508834838867
Loss :  1.7963151931762695 2.981447458267212 2.981447458267212
Loss :  1.80413818359375 2.619910717010498 2.619910717010498
Loss :  1.8023185729980469 2.6180872917175293 2.6180872917175293
Loss :  1.8034592866897583 2.5250022411346436 2.5250022411346436
Loss :  1.8013761043548584 2.4568898677825928 2.4568898677825928
Loss :  1.8042933940887451 2.557861566543579 2.557861566543579
Loss :  1.8053629398345947 2.9617016315460205 2.9617016315460205
Loss :  1.806742548942566 2.8621509075164795 2.8621509075164795
Loss :  1.8107836246490479 3.2757954597473145 3.2757954597473145
Loss :  1.8036693334579468 3.001023530960083 3.001023530960083
  batch 60 loss: 1.8036693334579468, 3.001023530960083, 3.001023530960083
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8050774335861206 2.7975428104400635 2.7975428104400635
Loss :  1.8015819787979126 2.3203580379486084 2.3203580379486084
Loss :  1.8044042587280273 2.6474125385284424 2.6474125385284424
Loss :  1.8005567789077759 3.205139398574829 3.205139398574829
Loss :  1.8041973114013672 2.701526403427124 2.701526403427124
Loss :  1.7380692958831787 4.391592025756836 4.391592025756836
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.740217685699463 4.488065719604492 4.488065719604492
Loss :  1.7345703840255737 4.335336685180664 4.335336685180664
Loss :  1.7567087411880493 4.220243453979492 4.220243453979492
Total LOSS train 3.0359676141005294 valid 4.358809471130371
CE LOSS train 1.8027375294612005 valid 0.43917718529701233
Contrastive LOSS train 3.0359676141005294 valid 1.055060863494873
EPOCH 232:
Loss :  1.802930474281311 2.7826507091522217 2.7826507091522217
Loss :  1.798082709312439 3.376063108444214 3.376063108444214
Loss :  1.8017863035202026 3.1179609298706055 3.1179609298706055
Loss :  1.8032293319702148 3.0648770332336426 3.0648770332336426
Loss :  1.8013644218444824 3.17275333404541 3.17275333404541
Loss :  1.8058606386184692 3.383643865585327 3.383643865585327
Loss :  1.7987300157546997 2.986449956893921 2.986449956893921
Loss :  1.7997336387634277 2.5306997299194336 2.5306997299194336
Loss :  1.7991260290145874 2.8027570247650146 2.8027570247650146
Loss :  1.7973588705062866 2.609915018081665 2.609915018081665
Loss :  1.8062400817871094 2.943370819091797 2.943370819091797
Loss :  1.8020936250686646 2.577610731124878 2.577610731124878
Loss :  1.8056929111480713 2.319763660430908 2.319763660430908
Loss :  1.8083688020706177 2.5373120307922363 2.5373120307922363
Loss :  1.7986525297164917 2.4255480766296387 2.4255480766296387
Loss :  1.8086187839508057 2.5816550254821777 2.5816550254821777
Loss :  1.8061763048171997 2.6585500240325928 2.6585500240325928
Loss :  1.8023179769515991 2.821622133255005 2.821622133255005
Loss :  1.8033336400985718 2.575505018234253 2.575505018234253
Loss :  1.7978391647338867 3.2149527072906494 3.2149527072906494
  batch 20 loss: 1.7978391647338867, 3.2149527072906494, 3.2149527072906494
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8074661493301392 3.4603681564331055 3.4603681564331055
Loss :  1.806054711341858 3.4269375801086426 3.4269375801086426
Loss :  1.799562931060791 3.137193202972412 3.137193202972412
Loss :  1.8060613870620728 3.4269425868988037 3.4269425868988037
Loss :  1.805451512336731 3.2440757751464844 3.2440757751464844
Loss :  1.8027688264846802 3.014346122741699 3.014346122741699
Loss :  1.8081320524215698 3.187251567840576 3.187251567840576
Loss :  1.7995609045028687 2.803351879119873 2.803351879119873
Loss :  1.8102363348007202 3.348604917526245 3.348604917526245
Loss :  1.798835277557373 2.9490256309509277 2.9490256309509277
Loss :  1.8062747716903687 2.9994661808013916 2.9994661808013916
Loss :  1.7974985837936401 3.04833722114563 3.04833722114563
Loss :  1.804563045501709 2.919522285461426 2.919522285461426
Loss :  1.8031485080718994 2.906715154647827 2.906715154647827
Loss :  1.8051190376281738 3.0911290645599365 3.0911290645599365
Loss :  1.8050081729888916 3.707501173019409 3.707501173019409
Loss :  1.8042219877243042 3.9106500148773193 3.9106500148773193
Loss :  1.7982722520828247 3.7866766452789307 3.7866766452789307
Loss :  1.8029793500900269 3.827393054962158 3.827393054962158
Loss :  1.7986482381820679 3.5912201404571533 3.5912201404571533
  batch 40 loss: 1.7986482381820679, 3.5912201404571533, 3.5912201404571533
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8036538362503052 3.363417863845825 3.363417863845825
Loss :  1.8005013465881348 3.4491140842437744 3.4491140842437744
Loss :  1.802483081817627 3.512542247772217 3.512542247772217
Loss :  1.7974315881729126 3.3880574703216553 3.3880574703216553
Loss :  1.8048405647277832 2.9160916805267334 2.9160916805267334
Loss :  1.801133155822754 3.273426055908203 3.273426055908203
Loss :  1.7989290952682495 2.8580548763275146 2.8580548763275146
Loss :  1.801654577255249 3.410879611968994 3.410879611968994
Loss :  1.7942878007888794 3.564600706100464 3.564600706100464
Loss :  1.7996277809143066 3.4780383110046387 3.4780383110046387
Loss :  1.796741247177124 2.901082754135132 2.901082754135132
Loss :  1.804488182067871 2.7858328819274902 2.7858328819274902
Loss :  1.8026163578033447 3.0717742443084717 3.0717742443084717
Loss :  1.8037446737289429 3.027906656265259 3.027906656265259
Loss :  1.8016955852508545 2.806966543197632 2.806966543197632
Loss :  1.8042875528335571 2.9506261348724365 2.9506261348724365
Loss :  1.8055596351623535 3.2201685905456543 3.2201685905456543
Loss :  1.8072526454925537 2.6978297233581543 2.6978297233581543
Loss :  1.810912013053894 3.0205185413360596 3.0205185413360596
Loss :  1.803398609161377 3.4549150466918945 3.4549150466918945
  batch 60 loss: 1.803398609161377, 3.4549150466918945, 3.4549150466918945
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8054999113082886 3.4510719776153564 3.4510719776153564
Loss :  1.8018906116485596 3.2679531574249268 3.2679531574249268
Loss :  1.8045214414596558 3.269739866256714 3.269739866256714
Loss :  1.8008414506912231 3.2290244102478027 3.2290244102478027
Loss :  1.8048118352890015 3.8253631591796875 3.8253631591796875
Loss :  1.7398067712783813 4.392467021942139 4.392467021942139
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.7417842149734497 4.460042476654053 4.460042476654053
Loss :  1.7362565994262695 4.37898063659668 4.37898063659668
Loss :  1.7584283351898193 4.227632522583008 4.227632522583008
Total LOSS train 3.1148517828721265 valid 4.36478066444397
CE LOSS train 1.802772382589487 valid 0.43960708379745483
Contrastive LOSS train 3.1148517828721265 valid 1.056908130645752
EPOCH 233:
Loss :  1.8036390542984009 3.609433174133301 3.609433174133301
Loss :  1.79777193069458 3.4168553352355957 3.4168553352355957
Loss :  1.8023930788040161 3.007087469100952 3.007087469100952
Loss :  1.8039426803588867 3.2025511264801025 3.2025511264801025
Loss :  1.8014384508132935 3.0287554264068604 3.0287554264068604
Loss :  1.8056704998016357 3.6077687740325928 3.6077687740325928
Loss :  1.7985650300979614 3.335721492767334 3.335721492767334
Loss :  1.7998265027999878 2.5122971534729004 2.5122971534729004
Loss :  1.7983708381652832 2.3932929039001465 2.3932929039001465
Loss :  1.7968343496322632 3.661054849624634 3.661054849624634
Loss :  1.8063769340515137 3.3391716480255127 3.3391716480255127
Loss :  1.8019522428512573 3.227175235748291 3.227175235748291
Loss :  1.8054996728897095 3.4594876766204834 3.4594876766204834
Loss :  1.8079338073730469 3.4826607704162598 3.4826607704162598
Loss :  1.797297716140747 3.3323581218719482 3.3323581218719482
Loss :  1.8080050945281982 3.2549853324890137 3.2549853324890137
Loss :  1.8054622411727905 3.8152666091918945 3.8152666091918945
Loss :  1.8012125492095947 3.2333314418792725 3.2333314418792725
Loss :  1.8025755882263184 3.0793206691741943 3.0793206691741943
Loss :  1.7965986728668213 2.8616220951080322 2.8616220951080322
  batch 20 loss: 1.7965986728668213, 2.8616220951080322, 2.8616220951080322
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8061517477035522 2.6119353771209717 2.6119353771209717
Loss :  1.8052717447280884 2.920184373855591 2.920184373855591
Loss :  1.7981963157653809 2.692582845687866 2.692582845687866
Loss :  1.8053234815597534 2.7161285877227783 2.7161285877227783
Loss :  1.804634690284729 2.572521924972534 2.572521924972534
Loss :  1.8019388914108276 2.2401041984558105 2.2401041984558105
Loss :  1.8079044818878174 2.911769390106201 2.911769390106201
Loss :  1.7989776134490967 2.3680379390716553 2.3680379390716553
Loss :  1.810372233390808 2.5647008419036865 2.5647008419036865
Loss :  1.798532247543335 2.905383825302124 2.905383825302124
Loss :  1.8061988353729248 2.4031927585601807 2.4031927585601807
Loss :  1.7972042560577393 2.636033773422241 2.636033773422241
Loss :  1.8046270608901978 2.5419678688049316 2.5419678688049316
Loss :  1.8035932779312134 2.526994466781616 2.526994466781616
Loss :  1.80496084690094 2.62463641166687 2.62463641166687
Loss :  1.8055408000946045 2.6367862224578857 2.6367862224578857
Loss :  1.8043419122695923 2.5515341758728027 2.5515341758728027
Loss :  1.7988144159317017 2.376603603363037 2.376603603363037
Loss :  1.8032857179641724 2.881340503692627 2.881340503692627
Loss :  1.7991939783096313 2.4440650939941406 2.4440650939941406
  batch 40 loss: 1.7991939783096313, 2.4440650939941406, 2.4440650939941406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.80411696434021 2.687781572341919 2.687781572341919
Loss :  1.801249623298645 2.3374059200286865 2.3374059200286865
Loss :  1.8029334545135498 2.6532392501831055 2.6532392501831055
Loss :  1.7980754375457764 2.5036022663116455 2.5036022663116455
Loss :  1.8052986860275269 3.0282413959503174 3.0282413959503174
Loss :  1.8022831678390503 3.059875965118408 3.059875965118408
Loss :  1.7992620468139648 3.1682348251342773 3.1682348251342773
Loss :  1.8021882772445679 2.6378276348114014 2.6378276348114014
Loss :  1.795283317565918 2.6223671436309814 2.6223671436309814
Loss :  1.7999482154846191 2.932662010192871 2.932662010192871
Loss :  1.7971076965332031 2.9484734535217285 2.9484734535217285
Loss :  1.8049594163894653 2.990664005279541 2.990664005279541
Loss :  1.8032969236373901 2.858597755432129 2.858597755432129
Loss :  1.8048771619796753 2.597299814224243 2.597299814224243
Loss :  1.8020812273025513 2.7119803428649902 2.7119803428649902
Loss :  1.805188536643982 2.82589054107666 2.82589054107666
Loss :  1.806746006011963 3.0917389392852783 3.0917389392852783
Loss :  1.8080501556396484 2.673882007598877 2.673882007598877
Loss :  1.8118345737457275 2.7857213020324707 2.7857213020324707
Loss :  1.8043231964111328 2.6001856327056885 2.6001856327056885
  batch 60 loss: 1.8043231964111328, 2.6001856327056885, 2.6001856327056885
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8064554929733276 3.0191245079040527 3.0191245079040527
Loss :  1.802313208580017 3.077580690383911 3.077580690383911
Loss :  1.8052231073379517 2.5376250743865967 2.5376250743865967
Loss :  1.8015097379684448 2.690316915512085 2.690316915512085
Loss :  1.8055150508880615 2.6399972438812256 2.6399972438812256
Loss :  1.7346575260162354 4.423285484313965 4.423285484313965
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.737037181854248 4.42902946472168 4.42902946472168
Loss :  1.7310045957565308 4.328241348266602 4.328241348266602
Loss :  1.755070447921753 4.172647476196289 4.172647476196289
Total LOSS train 2.8718002722813534 valid 4.338300943374634
CE LOSS train 1.8028700333375198 valid 0.43876761198043823
Contrastive LOSS train 2.8718002722813534 valid 1.0431618690490723
EPOCH 234:
Loss :  1.803746223449707 2.5836565494537354 2.5836565494537354
Loss :  1.7991300821304321 2.902132511138916 2.902132511138916
Loss :  1.8025712966918945 3.084113359451294 3.084113359451294
Loss :  1.8033806085586548 3.5560200214385986 3.5560200214385986
Loss :  1.8014808893203735 3.803027868270874 3.803027868270874
Loss :  1.8060574531555176 3.11863374710083 3.11863374710083
Loss :  1.7992360591888428 3.0030100345611572 3.0030100345611572
Loss :  1.8004721403121948 3.153024435043335 3.153024435043335
Loss :  1.7983853816986084 3.1324827671051025 3.1324827671051025
Loss :  1.7975406646728516 3.0887019634246826 3.0887019634246826
Loss :  1.806742548942566 3.3890562057495117 3.3890562057495117
Loss :  1.8019837141036987 3.59985613822937 3.59985613822937
Loss :  1.8058538436889648 2.9716684818267822 2.9716684818267822
Loss :  1.8082698583602905 3.1279664039611816 3.1279664039611816
Loss :  1.7979539632797241 2.901031017303467 2.901031017303467
Loss :  1.8088258504867554 3.137861490249634 3.137861490249634
Loss :  1.8058699369430542 2.951618194580078 2.951618194580078
Loss :  1.8022559881210327 2.785139322280884 2.785139322280884
Loss :  1.802876591682434 2.807119131088257 2.807119131088257
Loss :  1.7979295253753662 3.116431474685669 3.116431474685669
  batch 20 loss: 1.7979295253753662, 3.116431474685669, 3.116431474685669
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8072030544281006 3.0699994564056396 3.0699994564056396
Loss :  1.805694818496704 3.3662807941436768 3.3662807941436768
Loss :  1.79941987991333 3.1726443767547607 3.1726443767547607
Loss :  1.805773377418518 2.993666648864746 2.993666648864746
Loss :  1.8053417205810547 3.307061195373535 3.307061195373535
Loss :  1.8030704259872437 2.816520929336548 2.816520929336548
Loss :  1.8082019090652466 2.4241015911102295 2.4241015911102295
Loss :  1.7997008562088013 2.750344753265381 2.750344753265381
Loss :  1.8107922077178955 2.8403780460357666 2.8403780460357666
Loss :  1.7984185218811035 3.0533065795898438 3.0533065795898438
Loss :  1.8061929941177368 3.1174964904785156 3.1174964904785156
Loss :  1.797131896018982 3.3152589797973633 3.3152589797973633
Loss :  1.8050552606582642 3.372570276260376 3.372570276260376
Loss :  1.803505539894104 2.9530234336853027 2.9530234336853027
Loss :  1.805066466331482 3.2742831707000732 3.2742831707000732
Loss :  1.8053828477859497 2.8765671253204346 2.8765671253204346
Loss :  1.8042553663253784 3.621103525161743 3.621103525161743
Loss :  1.7983620166778564 3.1811487674713135 3.1811487674713135
Loss :  1.8028640747070312 2.9130351543426514 2.9130351543426514
Loss :  1.7985973358154297 2.3399617671966553 2.3399617671966553
  batch 40 loss: 1.7985973358154297, 2.3399617671966553, 2.3399617671966553
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8034740686416626 2.9578583240509033 2.9578583240509033
Loss :  1.8006128072738647 3.3452041149139404 3.3452041149139404
Loss :  1.802349328994751 3.770134210586548 3.770134210586548
Loss :  1.7975503206253052 3.5181989669799805 3.5181989669799805
Loss :  1.8045258522033691 3.1804373264312744 3.1804373264312744
Loss :  1.8012230396270752 3.8590307235717773 3.8590307235717773
Loss :  1.7982755899429321 3.469019651412964 3.469019651412964
Loss :  1.8010927438735962 4.051314830780029 4.051314830780029
Loss :  1.7940276861190796 3.599332332611084 3.599332332611084
Loss :  1.7990082502365112 3.5125186443328857 3.5125186443328857
Loss :  1.7957210540771484 3.5373361110687256 3.5373361110687256
Loss :  1.8035105466842651 3.6448988914489746 3.6448988914489746
Loss :  1.8019307851791382 2.9360547065734863 2.9360547065734863
Loss :  1.8030740022659302 3.4928982257843018 3.4928982257843018
Loss :  1.8010425567626953 2.763974905014038 2.763974905014038
Loss :  1.804106593132019 3.5108561515808105 3.5108561515808105
Loss :  1.8052908182144165 3.429185152053833 3.429185152053833
Loss :  1.8064781427383423 2.539179801940918 2.539179801940918
Loss :  1.8105711936950684 2.7588589191436768 2.7588589191436768
Loss :  1.8032896518707275 3.158188819885254 3.158188819885254
  batch 60 loss: 1.8032896518707275, 3.158188819885254, 3.158188819885254
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8052635192871094 2.838510751724243 2.838510751724243
Loss :  1.8013942241668701 2.763925790786743 2.763925790786743
Loss :  1.8045096397399902 2.5845465660095215 2.5845465660095215
Loss :  1.8007224798202515 3.0517921447753906 3.0517921447753906
Loss :  1.80458664894104 2.309976816177368 2.309976816177368
Loss :  1.7353942394256592 4.418781757354736 4.418781757354736
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.7376188039779663 4.393006801605225 4.393006801605225
Loss :  1.7317838668823242 4.250816345214844 4.250816345214844
Loss :  1.7552555799484253 4.063433647155762 4.063433647155762
Total LOSS train 3.1316082624288706 valid 4.281509637832642
CE LOSS train 1.8026803805277898 valid 0.4388138949871063
Contrastive LOSS train 3.1316082624288706 valid 1.0158584117889404
EPOCH 235:
Loss :  1.802985668182373 2.804274559020996 2.804274559020996
Loss :  1.7984317541122437 3.2626378536224365 3.2626378536224365
Loss :  1.8021162748336792 2.798811674118042 2.798811674118042
Loss :  1.8033950328826904 3.464012384414673 3.464012384414673
Loss :  1.8014787435531616 2.2802014350891113 2.2802014350891113
Loss :  1.8060611486434937 2.452214002609253 2.452214002609253
Loss :  1.7990385293960571 2.722181797027588 2.722181797027588
Loss :  1.8003411293029785 3.0415101051330566 3.0415101051330566
Loss :  1.7985280752182007 3.0565261840820312 3.0565261840820312
Loss :  1.7975122928619385 3.360158681869507 3.360158681869507
Loss :  1.8070974349975586 3.453063488006592 3.453063488006592
Loss :  1.802514672279358 3.1988656520843506 3.1988656520843506
Loss :  1.8060755729675293 3.7011618614196777 3.7011618614196777
Loss :  1.8085978031158447 4.021224021911621 4.021224021911621
Loss :  1.797859787940979 2.8071274757385254 2.8071274757385254
Loss :  1.8099027872085571 3.302395820617676 3.302395820617676
Loss :  1.8066036701202393 3.2918155193328857 3.2918155193328857
Loss :  1.8027594089508057 2.9568989276885986 2.9568989276885986
Loss :  1.8038361072540283 3.853060722351074 3.853060722351074
Loss :  1.7980456352233887 3.8507885932922363 3.8507885932922363
  batch 20 loss: 1.7980456352233887, 3.8507885932922363, 3.8507885932922363
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.807355284690857 3.4263405799865723 3.4263405799865723
Loss :  1.8062260150909424 2.6845316886901855 2.6845316886901855
Loss :  1.7993361949920654 2.3497519493103027 2.3497519493103027
Loss :  1.8063621520996094 2.7459018230438232 2.7459018230438232
Loss :  1.80655038356781 3.1226518154144287 3.1226518154144287
Loss :  1.803787112236023 2.4221460819244385 2.4221460819244385
Loss :  1.809061050415039 2.8259992599487305 2.8259992599487305
Loss :  1.800444483757019 2.623849630355835 2.623849630355835
Loss :  1.8111592531204224 3.105563163757324 3.105563163757324
Loss :  1.7999889850616455 2.9169223308563232 2.9169223308563232
Loss :  1.8069751262664795 3.4572057723999023 3.4572057723999023
Loss :  1.797930121421814 3.2811598777770996 3.2811598777770996
Loss :  1.8054862022399902 2.8266441822052 2.8266441822052
Loss :  1.804531216621399 2.7120935916900635 2.7120935916900635
Loss :  1.8053251504898071 2.7682814598083496 2.7682814598083496
Loss :  1.8060197830200195 3.2035365104675293 3.2035365104675293
Loss :  1.804426908493042 2.9824860095977783 2.9824860095977783
Loss :  1.79903244972229 2.920426368713379 2.920426368713379
Loss :  1.8030370473861694 3.0632851123809814 3.0632851123809814
Loss :  1.7990394830703735 2.8827195167541504 2.8827195167541504
  batch 40 loss: 1.7990394830703735, 2.8827195167541504, 2.8827195167541504
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8039090633392334 3.233386278152466 3.233386278152466
Loss :  1.801103115081787 3.029945135116577 3.029945135116577
Loss :  1.8021352291107178 3.4935035705566406 3.4935035705566406
Loss :  1.7978336811065674 3.4751226902008057 3.4751226902008057
Loss :  1.8046966791152954 3.2821218967437744 3.2821218967437744
Loss :  1.801887035369873 3.9683279991149902 3.9683279991149902
Loss :  1.798798680305481 3.4194869995117188 3.4194869995117188
Loss :  1.8013068437576294 3.3237340450286865 3.3237340450286865
Loss :  1.7947657108306885 3.172726631164551 3.172726631164551
Loss :  1.7991437911987305 2.9543802738189697 2.9543802738189697
Loss :  1.7959787845611572 3.3855504989624023 3.3855504989624023
Loss :  1.803978443145752 3.1937167644500732 3.1937167644500732
Loss :  1.8025462627410889 3.405799388885498 3.405799388885498
Loss :  1.8041683435440063 3.190525531768799 3.190525531768799
Loss :  1.8011887073516846 2.9455244541168213 2.9455244541168213
Loss :  1.8042573928833008 3.5616137981414795 3.5616137981414795
Loss :  1.8061151504516602 3.9496207237243652 3.9496207237243652
Loss :  1.806954026222229 2.9265992641448975 2.9265992641448975
Loss :  1.8111435174942017 3.1595256328582764 3.1595256328582764
Loss :  1.8032450675964355 2.832568407058716 2.832568407058716
  batch 60 loss: 1.8032450675964355, 2.832568407058716, 2.832568407058716
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.805883526802063 3.158362865447998 3.158362865447998
Loss :  1.8015717267990112 2.986132860183716 2.986132860183716
Loss :  1.8048200607299805 2.5162150859832764 2.5162150859832764
Loss :  1.8009124994277954 2.9168107509613037 2.9168107509613037
Loss :  1.8049834966659546 2.9749810695648193 2.9749810695648193
Loss :  1.7358200550079346 4.388224124908447 4.388224124908447
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.7379934787750244 4.371031284332275 4.371031284332275
Loss :  1.7321089506149292 4.345332622528076 4.345332622528076
Loss :  1.7561323642730713 4.345531940460205 4.345531940460205
Total LOSS train 3.114718616925753 valid 4.362529993057251
CE LOSS train 1.8030551195144653 valid 0.4390330910682678
Contrastive LOSS train 3.114718616925753 valid 1.0863829851150513
EPOCH 236:
Loss :  1.8032279014587402 3.0139148235321045 3.0139148235321045
Loss :  1.7989001274108887 3.147353172302246 3.147353172302246
Loss :  1.8024239540100098 2.8381545543670654 2.8381545543670654
Loss :  1.803524136543274 3.0446696281433105 3.0446696281433105
Loss :  1.8018165826797485 2.47727370262146 2.47727370262146
Loss :  1.8063428401947021 2.655627489089966 2.655627489089966
Loss :  1.7993956804275513 2.7094151973724365 2.7094151973724365
Loss :  1.8008109331130981 2.67205548286438 2.67205548286438
Loss :  1.7989064455032349 2.5274863243103027 2.5274863243103027
Loss :  1.798340916633606 2.3137948513031006 2.3137948513031006
Loss :  1.8077936172485352 2.536914348602295 2.536914348602295
Loss :  1.8025517463684082 2.9846746921539307 2.9846746921539307
Loss :  1.8067328929901123 2.7631173133850098 2.7631173133850098
Loss :  1.809073567390442 2.7547807693481445 2.7547807693481445
Loss :  1.7977831363677979 3.043346405029297 3.043346405029297
Loss :  1.8102344274520874 3.4010846614837646 3.4010846614837646
Loss :  1.8068503141403198 3.0646543502807617 3.0646543502807617
Loss :  1.803181529045105 2.9697494506835938 2.9697494506835938
Loss :  1.8038989305496216 2.94490909576416 2.94490909576416
Loss :  1.7981208562850952 3.004096508026123 3.004096508026123
  batch 20 loss: 1.7981208562850952, 3.004096508026123, 3.004096508026123
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8073041439056396 2.8657350540161133 2.8657350540161133
Loss :  1.8058949708938599 2.8583824634552 2.8583824634552
Loss :  1.7987271547317505 2.8737971782684326 2.8737971782684326
Loss :  1.8054473400115967 2.76480770111084 2.76480770111084
Loss :  1.8050445318222046 3.0650484561920166 3.0650484561920166
Loss :  1.8035995960235596 2.890744686126709 2.890744686126709
Loss :  1.809017539024353 3.733935832977295 3.733935832977295
Loss :  1.8001933097839355 2.960956573486328 2.960956573486328
Loss :  1.8119959831237793 2.8832430839538574 2.8832430839538574
Loss :  1.7995517253875732 2.962616205215454 2.962616205215454
Loss :  1.807133674621582 3.0298800468444824 3.0298800468444824
Loss :  1.7977869510650635 2.747209072113037 2.747209072113037
Loss :  1.8059853315353394 2.72375750541687 2.72375750541687
Loss :  1.8047869205474854 2.8395848274230957 2.8395848274230957
Loss :  1.8052434921264648 2.967721700668335 2.967721700668335
Loss :  1.806740164756775 3.020751476287842 3.020751476287842
Loss :  1.8048168420791626 3.291768789291382 3.291768789291382
Loss :  1.7997868061065674 2.7116787433624268 2.7116787433624268
Loss :  1.8033338785171509 2.8265750408172607 2.8265750408172607
Loss :  1.7998408079147339 2.3788576126098633 2.3788576126098633
  batch 40 loss: 1.7998408079147339, 2.3788576126098633, 2.3788576126098633
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8046914339065552 3.035677671432495 3.035677671432495
Loss :  1.8022652864456177 2.604661703109741 2.604661703109741
Loss :  1.803285837173462 3.0402848720550537 3.0402848720550537
Loss :  1.7989767789840698 2.687404155731201 2.687404155731201
Loss :  1.8055713176727295 2.76132869720459 2.76132869720459
Loss :  1.8033088445663452 2.540463924407959 2.540463924407959
Loss :  1.8002263307571411 2.8492579460144043 2.8492579460144043
Loss :  1.8023581504821777 3.192453145980835 3.192453145980835
Loss :  1.7970542907714844 3.1490464210510254 3.1490464210510254
Loss :  1.8005905151367188 3.4280450344085693 3.4280450344085693
Loss :  1.797139286994934 3.1688265800476074 3.1688265800476074
Loss :  1.8051949739456177 3.305237054824829 3.305237054824829
Loss :  1.8034569025039673 3.38692307472229 3.38692307472229
Loss :  1.8053350448608398 3.0974416732788086 3.0974416732788086
Loss :  1.8019176721572876 2.9261209964752197 2.9261209964752197
Loss :  1.805460810661316 2.5750463008880615 2.5750463008880615
Loss :  1.8071438074111938 2.9389495849609375 2.9389495849609375
Loss :  1.8078320026397705 2.733449697494507 2.733449697494507
Loss :  1.811464548110962 3.518550395965576 3.518550395965576
Loss :  1.8032299280166626 3.7472963333129883 3.7472963333129883
  batch 60 loss: 1.8032299280166626, 3.7472963333129883, 3.7472963333129883
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.806364893913269 3.4088058471679688 3.4088058471679688
Loss :  1.8016009330749512 3.410339117050171 3.410339117050171
Loss :  1.805003046989441 3.584890604019165 3.584890604019165
Loss :  1.8008757829666138 3.720041513442993 3.720041513442993
Loss :  1.8048882484436035 2.4599571228027344 2.4599571228027344
Loss :  1.7331243753433228 4.462940692901611 4.462940692901611
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.7351657152175903 4.450055122375488 4.450055122375488
Loss :  1.7293193340301514 4.289588451385498 4.289588451385498
Loss :  1.754992127418518 4.3706560134887695 4.3706560134887695
Total LOSS train 2.9620711436638465 valid 4.393310070037842
CE LOSS train 1.8034981287442722 valid 0.4387480318546295
Contrastive LOSS train 2.9620711436638465 valid 1.0926640033721924
EPOCH 237:
Loss :  1.8027864694595337 3.2506906986236572 3.2506906986236572
Loss :  1.798475980758667 3.419512987136841 3.419512987136841
Loss :  1.8017688989639282 3.446855306625366 3.446855306625366
Loss :  1.8030463457107544 3.6069884300231934 3.6069884300231934
Loss :  1.8012531995773315 2.812302589416504 2.812302589416504
Loss :  1.8056528568267822 3.192641496658325 3.192641496658325
Loss :  1.7988280057907104 2.8517184257507324 2.8517184257507324
Loss :  1.7995754480361938 2.4805943965911865 2.4805943965911865
Loss :  1.7980455160140991 2.846432685852051 2.846432685852051
Loss :  1.7968164682388306 3.0814661979675293 3.0814661979675293
Loss :  1.806494116783142 2.651994466781616 2.651994466781616
Loss :  1.801574468612671 2.7559854984283447 2.7559854984283447
Loss :  1.8060581684112549 2.742971181869507 2.742971181869507
Loss :  1.8081011772155762 2.9965999126434326 2.9965999126434326
Loss :  1.7970858812332153 2.824760913848877 2.824760913848877
Loss :  1.8088295459747314 2.9585492610931396 2.9585492610931396
Loss :  1.8058737516403198 2.866820812225342 2.866820812225342
Loss :  1.8017690181732178 3.4200661182403564 3.4200661182403564
Loss :  1.8028544187545776 2.788496255874634 2.788496255874634
Loss :  1.79722261428833 3.2259175777435303 3.2259175777435303
  batch 20 loss: 1.79722261428833, 3.2259175777435303, 3.2259175777435303
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.80667245388031 2.8108904361724854 2.8108904361724854
Loss :  1.8052356243133545 3.3878896236419678 3.3878896236419678
Loss :  1.7982828617095947 3.423795461654663 3.423795461654663
Loss :  1.8049050569534302 2.902575731277466 2.902575731277466
Loss :  1.8045735359191895 3.0807337760925293 3.0807337760925293
Loss :  1.8029507398605347 2.765543222427368 2.765543222427368
Loss :  1.8081411123275757 2.9520671367645264 2.9520671367645264
Loss :  1.7993810176849365 2.8407812118530273 2.8407812118530273
Loss :  1.8113176822662354 2.7073628902435303 2.7073628902435303
Loss :  1.7990018129348755 3.054135322570801 3.054135322570801
Loss :  1.8068807125091553 3.147834300994873 3.147834300994873
Loss :  1.7970175743103027 2.843815803527832 2.843815803527832
Loss :  1.8053747415542603 3.2499725818634033 3.2499725818634033
Loss :  1.8039436340332031 3.912384033203125 3.912384033203125
Loss :  1.8048832416534424 3.2814102172851562 3.2814102172851562
Loss :  1.8060767650604248 2.7682456970214844 2.7682456970214844
Loss :  1.8042504787445068 2.793909788131714 2.793909788131714
Loss :  1.798986554145813 2.534594774246216 2.534594774246216
Loss :  1.802811622619629 2.6400935649871826 2.6400935649871826
Loss :  1.7991482019424438 2.816915273666382 2.816915273666382
  batch 40 loss: 1.7991482019424438, 2.816915273666382, 2.816915273666382
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.803950309753418 3.4803574085235596 3.4803574085235596
Loss :  1.8016434907913208 3.3426053524017334 3.3426053524017334
Loss :  1.8024308681488037 3.4456517696380615 3.4456517696380615
Loss :  1.798388957977295 3.6613223552703857 3.6613223552703857
Loss :  1.8049999475479126 2.921341896057129 2.921341896057129
Loss :  1.802523136138916 2.5166537761688232 2.5166537761688232
Loss :  1.799098253250122 2.5704569816589355 2.5704569816589355
Loss :  1.8016281127929688 2.802455425262451 2.802455425262451
Loss :  1.795676827430725 2.4796829223632812 2.4796829223632812
Loss :  1.799676537513733 3.45044207572937 3.45044207572937
Loss :  1.7960681915283203 3.349351167678833 3.349351167678833
Loss :  1.8036433458328247 3.344679594039917 3.344679594039917
Loss :  1.8025445938110352 3.1013524532318115 3.1013524532318115
Loss :  1.8040357828140259 3.203599214553833 3.203599214553833
Loss :  1.8008583784103394 3.4692740440368652 3.4692740440368652
Loss :  1.8038849830627441 2.6100542545318604 2.6100542545318604
Loss :  1.8063263893127441 2.5333077907562256 2.5333077907562256
Loss :  1.8070520162582397 2.5921332836151123 2.5921332836151123
Loss :  1.810716152191162 2.666226863861084 2.666226863861084
Loss :  1.802603840827942 2.7625648975372314 2.7625648975372314
  batch 60 loss: 1.802603840827942, 2.7625648975372314, 2.7625648975372314
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8060123920440674 2.6570141315460205 2.6570141315460205
Loss :  1.8013722896575928 2.5668821334838867 2.5668821334838867
Loss :  1.804897665977478 2.4295661449432373 2.4295661449432373
Loss :  1.8009660243988037 2.5742690563201904 2.5742690563201904
Loss :  1.805167317390442 2.282059669494629 2.282059669494629
Loss :  1.7300891876220703 4.460721492767334 4.460721492767334
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.7321267127990723 4.326517581939697 4.326517581939697
Loss :  1.7263644933700562 4.268935680389404 4.268935680389404
Loss :  1.752444863319397 4.242075443267822 4.242075443267822
Total LOSS train 2.9684557034419132 valid 4.3245625495910645
CE LOSS train 1.8027402089192317 valid 0.43811121582984924
Contrastive LOSS train 2.9684557034419132 valid 1.0605188608169556
EPOCH 238:
Loss :  1.8030955791473389 2.86030912399292 2.86030912399292
Loss :  1.7990167140960693 2.8057336807250977 2.8057336807250977
Loss :  1.802355170249939 2.4590656757354736 2.4590656757354736
Loss :  1.8032151460647583 2.5400657653808594 2.5400657653808594
Loss :  1.801431655883789 3.160515308380127 3.160515308380127
Loss :  1.8058017492294312 2.7559287548065186 2.7559287548065186
Loss :  1.7994651794433594 2.9476401805877686 2.9476401805877686
Loss :  1.8005527257919312 3.162320375442505 3.162320375442505
Loss :  1.7979165315628052 2.7207703590393066 2.7207703590393066
Loss :  1.797550082206726 2.9347870349884033 2.9347870349884033
Loss :  1.8071668148040771 3.3327932357788086 3.3327932357788086
Loss :  1.8019018173217773 3.0518808364868164 3.0518808364868164
Loss :  1.8062161207199097 3.416625499725342 3.416625499725342
Loss :  1.8079720735549927 2.719355583190918 2.719355583190918
Loss :  1.7970035076141357 3.1263210773468018 3.1263210773468018
Loss :  1.8089991807937622 2.803378105163574 2.803378105163574
Loss :  1.8060113191604614 3.1019606590270996 3.1019606590270996
Loss :  1.8018656969070435 3.5793423652648926 3.5793423652648926
Loss :  1.8028029203414917 3.5135488510131836 3.5135488510131836
Loss :  1.7970870733261108 3.1970160007476807 3.1970160007476807
  batch 20 loss: 1.7970870733261108, 3.1970160007476807, 3.1970160007476807
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8065003156661987 3.669318199157715 3.669318199157715
Loss :  1.8051046133041382 2.9948570728302 2.9948570728302
Loss :  1.7981187105178833 2.8536322116851807 2.8536322116851807
Loss :  1.804711937904358 2.7217507362365723 2.7217507362365723
Loss :  1.8047051429748535 2.725170850753784 2.725170850753784
Loss :  1.8029584884643555 2.3577141761779785 2.3577141761779785
Loss :  1.8079676628112793 2.863560914993286 2.863560914993286
Loss :  1.799603819847107 3.022014856338501 3.022014856338501
Loss :  1.8107292652130127 2.812788963317871 2.812788963317871
Loss :  1.7991139888763428 3.1942250728607178 3.1942250728607178
Loss :  1.8064303398132324 3.6202147006988525 3.6202147006988525
Loss :  1.7970030307769775 2.9050207138061523 2.9050207138061523
Loss :  1.805247187614441 2.5425024032592773 2.5425024032592773
Loss :  1.8038853406906128 2.6081767082214355 2.6081767082214355
Loss :  1.804954171180725 2.793567180633545 2.793567180633545
Loss :  1.8061026334762573 2.807559013366699 2.807559013366699
Loss :  1.8043582439422607 2.877863883972168 2.877863883972168
Loss :  1.7988612651824951 2.7067339420318604 2.7067339420318604
Loss :  1.8026443719863892 3.207228422164917 3.207228422164917
Loss :  1.7988930940628052 3.6867897510528564 3.6867897510528564
  batch 40 loss: 1.7988930940628052, 3.6867897510528564, 3.6867897510528564
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8038804531097412 3.6736934185028076 3.6736934185028076
Loss :  1.8013170957565308 3.494764804840088 3.494764804840088
Loss :  1.8027018308639526 3.1808583736419678 3.1808583736419678
Loss :  1.7983722686767578 3.1649391651153564 3.1649391651153564
Loss :  1.80504310131073 3.266094207763672 3.266094207763672
Loss :  1.8023573160171509 4.070271015167236 4.070271015167236
Loss :  1.7991948127746582 3.6464052200317383 3.6464052200317383
Loss :  1.8016619682312012 3.437736749649048 3.437736749649048
Loss :  1.7957828044891357 3.577481746673584 3.577481746673584
Loss :  1.7999778985977173 3.4364545345306396 3.4364545345306396
Loss :  1.7962403297424316 3.210422992706299 3.210422992706299
Loss :  1.8039835691452026 3.1698732376098633 3.1698732376098633
Loss :  1.8027068376541138 3.061652898788452 3.061652898788452
Loss :  1.804358959197998 3.2921254634857178 3.2921254634857178
Loss :  1.8010706901550293 3.0721354484558105 3.0721354484558105
Loss :  1.8043253421783447 2.531461238861084 2.531461238861084
Loss :  1.8065012693405151 3.1020777225494385 3.1020777225494385
Loss :  1.8072080612182617 2.764739751815796 2.764739751815796
Loss :  1.811023473739624 3.228344678878784 3.228344678878784
Loss :  1.8022167682647705 3.487401247024536 3.487401247024536
  batch 60 loss: 1.8022167682647705, 3.487401247024536, 3.487401247024536
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8064011335372925 3.4637720584869385 3.4637720584869385
Loss :  1.8011542558670044 2.9314510822296143 2.9314510822296143
Loss :  1.8049899339675903 2.5283970832824707 2.5283970832824707
Loss :  1.8010716438293457 2.6259195804595947 2.6259195804595947
Loss :  1.8049657344818115 2.2706334590911865 2.2706334590911865
Loss :  1.7295219898223877 4.42919397354126 4.42919397354126
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.731533408164978 4.4609880447387695 4.4609880447387695
Loss :  1.725888729095459 4.423845291137695 4.423845291137695
Loss :  1.7516837120056152 4.233733654022217 4.233733654022217
Total LOSS train 3.059217775785006 valid 4.386940240859985
CE LOSS train 1.8028281266872699 valid 0.4379209280014038
Contrastive LOSS train 3.059217775785006 valid 1.0584334135055542
EPOCH 239:
Loss :  1.8027162551879883 3.430645704269409 3.430645704269409
Loss :  1.799089789390564 3.437147617340088 3.437147617340088
Loss :  1.8020007610321045 2.9072821140289307 2.9072821140289307
Loss :  1.803091287612915 3.1038095951080322 3.1038095951080322
Loss :  1.8014628887176514 2.5208816528320312 2.5208816528320312
Loss :  1.8058497905731201 2.8935649394989014 2.8935649394989014
Loss :  1.7995796203613281 2.7001187801361084 2.7001187801361084
Loss :  1.800549030303955 2.9579129219055176 2.9579129219055176
Loss :  1.7982932329177856 2.9760310649871826 2.9760310649871826
Loss :  1.7976261377334595 2.7742722034454346 2.7742722034454346
Loss :  1.8072609901428223 2.798962116241455 2.798962116241455
Loss :  1.8021596670150757 3.0919411182403564 3.0919411182403564
Loss :  1.8064640760421753 2.6884007453918457 2.6884007453918457
Loss :  1.808365821838379 2.486433267593384 2.486433267593384
Loss :  1.7969756126403809 2.7152278423309326 2.7152278423309326
Loss :  1.809716820716858 2.667743682861328 2.667743682861328
Loss :  1.8062987327575684 2.9892609119415283 2.9892609119415283
Loss :  1.8023439645767212 2.7100794315338135 2.7100794315338135
Loss :  1.8029478788375854 3.059893846511841 3.059893846511841
Loss :  1.7975658178329468 2.5578420162200928 2.5578420162200928
  batch 20 loss: 1.7975658178329468, 2.5578420162200928, 2.5578420162200928
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8067923784255981 3.197155714035034 3.197155714035034
Loss :  1.8051449060440063 3.183903217315674 3.183903217315674
Loss :  1.798343539237976 3.153679847717285 3.153679847717285
Loss :  1.804963231086731 2.9151923656463623 2.9151923656463623
Loss :  1.8049432039260864 3.027661085128784 3.027661085128784
Loss :  1.8032691478729248 2.858125925064087 2.858125925064087
Loss :  1.8079912662506104 3.0676445960998535 3.0676445960998535
Loss :  1.7993714809417725 3.0410845279693604 3.0410845279693604
Loss :  1.8109931945800781 3.4711787700653076 3.4711787700653076
Loss :  1.798824429512024 3.756568431854248 3.756568431854248
Loss :  1.8065210580825806 3.425718069076538 3.425718069076538
Loss :  1.7965725660324097 3.391578197479248 3.391578197479248
Loss :  1.8050650358200073 3.0950474739074707 3.0950474739074707
Loss :  1.8034944534301758 3.297740936279297 3.297740936279297
Loss :  1.8045109510421753 3.3068926334381104 3.3068926334381104
Loss :  1.8058682680130005 2.974652051925659 2.974652051925659
Loss :  1.8039623498916626 3.107913017272949 3.107913017272949
Loss :  1.7987533807754517 2.49883770942688 2.49883770942688
Loss :  1.8023532629013062 3.156625986099243 3.156625986099243
Loss :  1.7986596822738647 3.1767709255218506 3.1767709255218506
  batch 40 loss: 1.7986596822738647, 3.1767709255218506, 3.1767709255218506
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8037097454071045 3.3855175971984863 3.3855175971984863
Loss :  1.801534652709961 2.490814685821533 2.490814685821533
Loss :  1.802251935005188 3.13942551612854 3.13942551612854
Loss :  1.7985813617706299 3.6665637493133545 3.6665637493133545
Loss :  1.8049862384796143 3.3108091354370117 3.3108091354370117
Loss :  1.8024613857269287 3.644233465194702 3.644233465194702
Loss :  1.799078345298767 3.3606297969818115 3.3606297969818115
Loss :  1.80172598361969 3.1784210205078125 3.1784210205078125
Loss :  1.7958335876464844 2.8653063774108887 2.8653063774108887
Loss :  1.8002573251724243 3.785348415374756 3.785348415374756
Loss :  1.796458125114441 3.537226676940918 3.537226676940918
Loss :  1.8039063215255737 3.5263378620147705 3.5263378620147705
Loss :  1.8030221462249756 3.3434817790985107 3.3434817790985107
Loss :  1.805501937866211 3.1523778438568115 3.1523778438568115
Loss :  1.8012733459472656 2.7555758953094482 2.7555758953094482
Loss :  1.8044441938400269 2.231431245803833 2.231431245803833
Loss :  1.8069597482681274 2.8492696285247803 2.8492696285247803
Loss :  1.8073766231536865 3.2152700424194336 3.2152700424194336
Loss :  1.8112645149230957 3.1021199226379395 3.1021199226379395
Loss :  1.8022620677947998 3.1982738971710205 3.1982738971710205
  batch 60 loss: 1.8022620677947998, 3.1982738971710205, 3.1982738971710205
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8065857887268066 3.390465497970581 3.390465497970581
Loss :  1.8014250993728638 3.3746085166931152 3.3746085166931152
Loss :  1.8049845695495605 3.097586154937744 3.097586154937744
Loss :  1.8011091947555542 2.6558148860931396 2.6558148860931396
Loss :  1.8054473400115967 2.2472991943359375 2.2472991943359375
Loss :  1.7287330627441406 4.4205708503723145 4.4205708503723145
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.7305666208267212 4.358518123626709 4.358518123626709
Loss :  1.7251026630401611 4.259491920471191 4.259491920471191
Loss :  1.7517513036727905 4.149350643157959 4.149350643157959
Total LOSS train 3.0627020285679745 valid 4.2969828844070435
CE LOSS train 1.8029107313889723 valid 0.43793782591819763
Contrastive LOSS train 3.0627020285679745 valid 1.0373376607894897
EPOCH 240:
Loss :  1.802812099456787 2.667397975921631 2.667397975921631
Loss :  1.7989517450332642 3.451399326324463 3.451399326324463
Loss :  1.8021844625473022 2.9643633365631104 2.9643633365631104
Loss :  1.8030256032943726 3.1556668281555176 3.1556668281555176
Loss :  1.801375150680542 3.15411114692688 3.15411114692688
Loss :  1.8058382272720337 3.160794734954834 3.160794734954834
Loss :  1.7995820045471191 2.7575771808624268 2.7575771808624268
Loss :  1.8006671667099 2.8411056995391846 2.8411056995391846
Loss :  1.797916293144226 2.67000150680542 2.67000150680542
Loss :  1.7977583408355713 2.5089125633239746 2.5089125633239746
Loss :  1.8075250387191772 2.801384449005127 2.801384449005127
Loss :  1.8022087812423706 2.855609893798828 2.855609893798828
Loss :  1.8063151836395264 2.6487767696380615 2.6487767696380615
Loss :  1.8078335523605347 2.7022464275360107 2.7022464275360107
Loss :  1.7971988916397095 2.596717119216919 2.596717119216919
Loss :  1.809794545173645 2.6728785037994385 2.6728785037994385
Loss :  1.8063304424285889 2.62747859954834 2.62747859954834
Loss :  1.8027437925338745 2.9605400562286377 2.9605400562286377
Loss :  1.802950382232666 2.5120608806610107 2.5120608806610107
Loss :  1.7978525161743164 3.0135598182678223 3.0135598182678223
  batch 20 loss: 1.7978525161743164, 3.0135598182678223, 3.0135598182678223
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8067796230316162 3.0139474868774414 3.0139474868774414
Loss :  1.8050695657730103 3.205873727798462 3.205873727798462
Loss :  1.7984645366668701 3.2003824710845947 3.2003824710845947
Loss :  1.8047653436660767 3.2800331115722656 3.2800331115722656
Loss :  1.8046998977661133 3.695063591003418 3.695063591003418
Loss :  1.8033757209777832 3.513944149017334 3.513944149017334
Loss :  1.808103322982788 3.536818504333496 3.536818504333496
Loss :  1.7994880676269531 3.3947741985321045 3.3947741985321045
Loss :  1.8110626935958862 3.3782029151916504 3.3782029151916504
Loss :  1.7988640069961548 2.849257230758667 2.849257230758667
Loss :  1.8062692880630493 3.1977152824401855 3.1977152824401855
Loss :  1.7963835000991821 2.754305601119995 2.754305601119995
Loss :  1.8050205707550049 3.1029717922210693 3.1029717922210693
Loss :  1.803153395652771 3.215261220932007 3.215261220932007
Loss :  1.804189920425415 3.0391435623168945 3.0391435623168945
Loss :  1.8057715892791748 3.2601890563964844 3.2601890563964844
Loss :  1.8036085367202759 3.3303487300872803 3.3303487300872803
Loss :  1.7982679605484009 3.524500846862793 3.524500846862793
Loss :  1.801853060722351 2.758697271347046 2.758697271347046
Loss :  1.7980788946151733 2.715035915374756 2.715035915374756
  batch 40 loss: 1.7980788946151733, 2.715035915374756, 2.715035915374756
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.803088665008545 3.1524851322174072 3.1524851322174072
Loss :  1.8006283044815063 2.4882380962371826 2.4882380962371826
Loss :  1.8017090559005737 2.735706329345703 2.735706329345703
Loss :  1.7977831363677979 2.7241218090057373 2.7241218090057373
Loss :  1.804471492767334 3.2274861335754395 3.2274861335754395
Loss :  1.8019976615905762 3.409641981124878 3.409641981124878
Loss :  1.798754334449768 3.468161106109619 3.468161106109619
Loss :  1.8011243343353271 2.6642372608184814 2.6642372608184814
Loss :  1.7958040237426758 3.029581308364868 3.029581308364868
Loss :  1.7995853424072266 3.305170774459839 3.305170774459839
Loss :  1.7956500053405762 3.482166051864624 3.482166051864624
Loss :  1.8036280870437622 2.914510488510132 2.914510488510132
Loss :  1.802363634109497 2.342823028564453 2.342823028564453
Loss :  1.8046088218688965 2.7364556789398193 2.7364556789398193
Loss :  1.8010423183441162 2.7091498374938965 2.7091498374938965
Loss :  1.8047490119934082 2.77329158782959 2.77329158782959
Loss :  1.806388258934021 3.0122063159942627 3.0122063159942627
Loss :  1.8069320917129517 3.1436753273010254 3.1436753273010254
Loss :  1.8107880353927612 3.3579213619232178 3.3579213619232178
Loss :  1.8030574321746826 2.98183012008667 2.98183012008667
  batch 60 loss: 1.8030574321746826, 2.98183012008667, 2.98183012008667
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8062182664871216 2.7398951053619385 2.7398951053619385
Loss :  1.8013498783111572 3.268409013748169 3.268409013748169
Loss :  1.80521821975708 3.1111154556274414 3.1111154556274414
Loss :  1.8013207912445068 3.1110193729400635 3.1110193729400635
Loss :  1.8056007623672485 2.647630214691162 2.647630214691162
Loss :  1.7289676666259766 4.395938396453857 4.395938396453857
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.730919361114502 4.40488862991333 4.40488862991333
Loss :  1.7252854108810425 4.3248162269592285 4.3248162269592285
Loss :  1.7519938945770264 4.342726230621338 4.342726230621338
Total LOSS train 3.003476590376634 valid 4.3670923709869385
CE LOSS train 1.8027383951040414 valid 0.4379984736442566
Contrastive LOSS train 3.003476590376634 valid 1.0856815576553345
EPOCH 241:
Loss :  1.8030484914779663 2.9490535259246826 2.9490535259246826
Loss :  1.7992053031921387 3.123772144317627 3.123772144317627
Loss :  1.8023242950439453 3.0513038635253906 3.0513038635253906
Loss :  1.8032352924346924 3.2871387004852295 3.2871387004852295
Loss :  1.801612377166748 2.848459005355835 2.848459005355835
Loss :  1.8059157133102417 2.6244471073150635 2.6244471073150635
Loss :  1.7999202013015747 2.786159038543701 2.786159038543701
Loss :  1.8008476495742798 2.4842917919158936 2.4842917919158936
Loss :  1.7979586124420166 2.8074774742126465 2.8074774742126465
Loss :  1.7977795600891113 3.3010246753692627 3.3010246753692627
Loss :  1.8076592683792114 3.0019936561584473 3.0019936561584473
Loss :  1.8022218942642212 3.1199090480804443 3.1199090480804443
Loss :  1.806552529335022 3.4081180095672607 3.4081180095672607
Loss :  1.8079150915145874 2.7842066287994385 2.7842066287994385
Loss :  1.7965149879455566 3.095162868499756 3.095162868499756
Loss :  1.8093851804733276 3.472195625305176 3.472195625305176
Loss :  1.8061102628707886 2.9733235836029053 2.9733235836029053
Loss :  1.8021907806396484 2.918851137161255 2.918851137161255
Loss :  1.8028714656829834 3.3730509281158447 3.3730509281158447
Loss :  1.7972266674041748 3.4753575325012207 3.4753575325012207
  batch 20 loss: 1.7972266674041748, 3.4753575325012207, 3.4753575325012207
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.806269645690918 3.041478395462036 3.041478395462036
Loss :  1.8049118518829346 3.461923837661743 3.461923837661743
Loss :  1.797827124595642 3.3296186923980713 3.3296186923980713
Loss :  1.8046092987060547 3.3623948097229004 3.3623948097229004
Loss :  1.8041949272155762 3.1896398067474365 3.1896398067474365
Loss :  1.8032093048095703 3.724649667739868 3.724649667739868
Loss :  1.8082727193832397 3.4050703048706055 3.4050703048706055
Loss :  1.7994295358657837 3.7976882457733154 3.7976882457733154
Loss :  1.811702847480774 3.8334298133850098 3.8334298133850098
Loss :  1.7991807460784912 3.784170389175415 3.784170389175415
Loss :  1.8065283298492432 3.127784252166748 3.127784252166748
Loss :  1.7967751026153564 3.2285516262054443 3.2285516262054443
Loss :  1.8053674697875977 3.1618552207946777 3.1618552207946777
Loss :  1.8040512800216675 3.2173736095428467 3.2173736095428467
Loss :  1.8043898344039917 3.124558210372925 3.124558210372925
Loss :  1.8063240051269531 3.0517821311950684 3.0517821311950684
Loss :  1.803928256034851 3.068394422531128 3.068394422531128
Loss :  1.7991234064102173 2.6681809425354004 2.6681809425354004
Loss :  1.8023185729980469 2.992567777633667 2.992567777633667
Loss :  1.7987698316574097 2.708123207092285 2.708123207092285
  batch 40 loss: 1.7987698316574097, 2.708123207092285, 2.708123207092285
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8038201332092285 3.164684295654297 3.164684295654297
Loss :  1.8015636205673218 2.6189472675323486 2.6189472675323486
Loss :  1.8019707202911377 2.8085520267486572 2.8085520267486572
Loss :  1.7986763715744019 3.1476149559020996 3.1476149559020996
Loss :  1.8048921823501587 2.7792201042175293 2.7792201042175293
Loss :  1.802348017692566 3.070770263671875 3.070770263671875
Loss :  1.799098014831543 3.4916462898254395 3.4916462898254395
Loss :  1.8015271425247192 3.161235809326172 3.161235809326172
Loss :  1.7956377267837524 2.925489664077759 2.925489664077759
Loss :  1.8000662326812744 3.4828031063079834 3.4828031063079834
Loss :  1.7961819171905518 2.930929660797119 2.930929660797119
Loss :  1.8033509254455566 2.6974587440490723 2.6974587440490723
Loss :  1.8025330305099487 2.6515533924102783 2.6515533924102783
Loss :  1.8049943447113037 3.0017242431640625 3.0017242431640625
Loss :  1.8009427785873413 3.067505359649658 3.067505359649658
Loss :  1.803512692451477 2.574969530105591 2.574969530105591
Loss :  1.8062940835952759 2.9477250576019287 2.9477250576019287
Loss :  1.8068442344665527 2.5748307704925537 2.5748307704925537
Loss :  1.8104476928710938 2.9204134941101074 2.9204134941101074
Loss :  1.80154287815094 2.772254705429077 2.772254705429077
  batch 60 loss: 1.80154287815094, 2.772254705429077, 2.772254705429077
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.806185007095337 2.880122184753418 2.880122184753418
Loss :  1.8008620738983154 3.400074005126953 3.400074005126953
Loss :  1.8048655986785889 3.147264003753662 3.147264003753662
Loss :  1.800764560699463 2.976165533065796 2.976165533065796
Loss :  1.8049585819244385 2.9996116161346436 2.9996116161346436
Loss :  1.732587456703186 4.384982585906982 4.384982585906982
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.7341773509979248 4.3409624099731445 4.3409624099731445
Loss :  1.7286405563354492 4.319251537322998 4.319251537322998
Loss :  1.7549266815185547 4.2364935874938965 4.2364935874938965
Total LOSS train 3.0824322737180267 valid 4.320422530174255
CE LOSS train 1.802793235045213 valid 0.43873167037963867
Contrastive LOSS train 3.0824322737180267 valid 1.0591233968734741
EPOCH 242:
Loss :  1.802392601966858 2.8534231185913086 2.8534231185913086
Loss :  1.7988241910934448 3.9468483924865723 3.9468483924865723
Loss :  1.8020915985107422 2.991535186767578 2.991535186767578
Loss :  1.8029934167861938 2.918266534805298 2.918266534805298
Loss :  1.8013182878494263 2.9013819694519043 2.9013819694519043
Loss :  1.8056554794311523 2.6558523178100586 2.6558523178100586
Loss :  1.7995177507400513 2.9037156105041504 2.9037156105041504
Loss :  1.8004459142684937 2.4527201652526855 2.4527201652526855
Loss :  1.7977213859558105 2.688594102859497 2.688594102859497
Loss :  1.797420620918274 2.776109218597412 2.776109218597412
Loss :  1.8076419830322266 3.0923588275909424 3.0923588275909424
Loss :  1.8021173477172852 2.7873008251190186 2.7873008251190186
Loss :  1.806920051574707 2.8731534481048584 2.8731534481048584
Loss :  1.8080723285675049 3.017803430557251 3.017803430557251
Loss :  1.7957210540771484 2.8151166439056396 2.8151166439056396
Loss :  1.8095241785049438 3.521091938018799 3.521091938018799
Loss :  1.806479573249817 3.7364821434020996 3.7364821434020996
Loss :  1.802293300628662 3.321920156478882 3.321920156478882
Loss :  1.8030436038970947 3.7477211952209473 3.7477211952209473
Loss :  1.797426462173462 2.9822535514831543 2.9822535514831543
  batch 20 loss: 1.797426462173462, 2.9822535514831543, 2.9822535514831543
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.806234359741211 3.378491163253784 3.378491163253784
Loss :  1.8050830364227295 3.2518303394317627 3.2518303394317627
Loss :  1.797821044921875 3.368779420852661 3.368779420852661
Loss :  1.8041936159133911 4.047491550445557 4.047491550445557
Loss :  1.8037300109863281 3.4790499210357666 3.4790499210357666
Loss :  1.8031237125396729 3.460987091064453 3.460987091064453
Loss :  1.8083956241607666 3.1266560554504395 3.1266560554504395
Loss :  1.7998652458190918 3.611144542694092 3.611144542694092
Loss :  1.8126829862594604 2.9412641525268555 2.9412641525268555
Loss :  1.798985242843628 3.3248355388641357 3.3248355388641357
Loss :  1.806774616241455 3.5646138191223145 3.5646138191223145
Loss :  1.79680597782135 3.0120537281036377 3.0120537281036377
Loss :  1.8060646057128906 3.603940010070801 3.603940010070801
Loss :  1.804844617843628 3.1604700088500977 3.1604700088500977
Loss :  1.8048070669174194 3.03231143951416 3.03231143951416
Loss :  1.807316780090332 3.133859157562256 3.133859157562256
Loss :  1.8042128086090088 3.6268599033355713 3.6268599033355713
Loss :  1.7998104095458984 2.551365613937378 2.551365613937378
Loss :  1.802395224571228 2.63629150390625 2.63629150390625
Loss :  1.7993428707122803 2.7414653301239014 2.7414653301239014
  batch 40 loss: 1.7993428707122803, 2.7414653301239014, 2.7414653301239014
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8043200969696045 3.290459632873535 3.290459632873535
Loss :  1.8026882410049438 2.7825303077697754 2.7825303077697754
Loss :  1.8027799129486084 2.7553505897521973 2.7553505897521973
Loss :  1.7998231649398804 3.48301100730896 3.48301100730896
Loss :  1.805266261100769 2.467759609222412 2.467759609222412
Loss :  1.803452730178833 3.0111730098724365 3.0111730098724365
Loss :  1.7998135089874268 3.196580410003662 3.196580410003662
Loss :  1.8019813299179077 3.743807315826416 3.743807315826416
Loss :  1.7968865633010864 3.405672550201416 3.405672550201416
Loss :  1.8005999326705933 3.0192153453826904 3.0192153453826904
Loss :  1.7963602542877197 3.5560920238494873 3.5560920238494873
Loss :  1.8037348985671997 3.0386362075805664 3.0386362075805664
Loss :  1.8030056953430176 2.7618823051452637 2.7618823051452637
Loss :  1.805999517440796 2.994168996810913 2.994168996810913
Loss :  1.8009566068649292 3.013969898223877 3.013969898223877
Loss :  1.8045114278793335 3.4658381938934326 3.4658381938934326
Loss :  1.8073290586471558 3.110239267349243 3.110239267349243
Loss :  1.8071634769439697 2.9199256896972656 2.9199256896972656
Loss :  1.8108088970184326 3.0073282718658447 3.0073282718658447
Loss :  1.8015720844268799 3.332514524459839 3.332514524459839
  batch 60 loss: 1.8015720844268799, 3.332514524459839, 3.332514524459839
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8069913387298584 3.0581562519073486 3.0581562519073486
Loss :  1.8008732795715332 3.07745623588562 3.07745623588562
Loss :  1.8052198886871338 2.956721067428589 2.956721067428589
Loss :  1.8010704517364502 2.3331832885742188 2.3331832885742188
Loss :  1.8054628372192383 2.3538689613342285 2.3538689613342285
Loss :  1.7248454093933105 4.412353038787842 4.412353038787842
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.7267431020736694 4.462347984313965 4.462347984313965
Loss :  1.7213280200958252 4.307401180267334 4.307401180267334
Loss :  1.748925805091858 4.179091453552246 4.179091453552246
Total LOSS train 3.1103530773749717 valid 4.340298414230347
CE LOSS train 1.8030581914461576 valid 0.4372314512729645
Contrastive LOSS train 3.1103530773749717 valid 1.0447728633880615
EPOCH 243:
Loss :  1.8023275136947632 2.9859819412231445 2.9859819412231445
Loss :  1.7992663383483887 3.087948799133301 3.087948799133301
Loss :  1.8020110130310059 3.2425644397735596 3.2425644397735596
Loss :  1.8025838136672974 2.874166250228882 2.874166250228882
Loss :  1.8012360334396362 2.8816962242126465 2.8816962242126465
Loss :  1.8055860996246338 2.8347859382629395 2.8347859382629395
Loss :  1.799710750579834 2.925525665283203 2.925525665283203
Loss :  1.8004268407821655 2.0710833072662354 2.0710833072662354
Loss :  1.7973411083221436 2.558983325958252 2.558983325958252
Loss :  1.7973523139953613 3.0850822925567627 3.0850822925567627
Loss :  1.8076558113098145 2.8489277362823486 2.8489277362823486
Loss :  1.8019521236419678 3.2831623554229736 3.2831623554229736
Loss :  1.8069565296173096 3.0302858352661133 3.0302858352661133
Loss :  1.8076521158218384 3.328474521636963 3.328474521636963
Loss :  1.7953670024871826 3.1859958171844482 3.1859958171844482
Loss :  1.8095788955688477 2.964580535888672 2.964580535888672
Loss :  1.8063523769378662 3.0677363872528076 3.0677363872528076
Loss :  1.802106499671936 3.1046910285949707 3.1046910285949707
Loss :  1.8028358221054077 2.2043960094451904 2.2043960094451904
Loss :  1.7972840070724487 2.796828269958496 2.796828269958496
  batch 20 loss: 1.7972840070724487, 2.796828269958496, 2.796828269958496
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8059309720993042 2.922332525253296 2.922332525253296
Loss :  1.804782509803772 2.7992608547210693 2.7992608547210693
Loss :  1.7975571155548096 2.8216042518615723 2.8216042518615723
Loss :  1.803977608680725 3.2422845363616943 3.2422845363616943
Loss :  1.8036534786224365 3.353271245956421 3.353271245956421
Loss :  1.8036465644836426 2.6695213317871094 2.6695213317871094
Loss :  1.8087458610534668 3.29641056060791 3.29641056060791
Loss :  1.8000980615615845 3.5678491592407227 3.5678491592407227
Loss :  1.8130648136138916 3.6796457767486572 3.6796457767486572
Loss :  1.7998716831207275 4.254161834716797 4.254161834716797
Loss :  1.807278037071228 3.665740966796875 3.665740966796875
Loss :  1.7972681522369385 3.590094566345215 3.590094566345215
Loss :  1.806357979774475 3.467221736907959 3.467221736907959
Loss :  1.8055126667022705 3.6751809120178223 3.6751809120178223
Loss :  1.8051584959030151 3.2809629440307617 3.2809629440307617
Loss :  1.8076765537261963 3.407916307449341 3.407916307449341
Loss :  1.8046120405197144 3.271656036376953 3.271656036376953
Loss :  1.8002707958221436 2.631319284439087 2.631319284439087
Loss :  1.8024215698242188 3.037158966064453 3.037158966064453
Loss :  1.799585223197937 2.549370765686035 2.549370765686035
  batch 40 loss: 1.799585223197937, 2.549370765686035, 2.549370765686035
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8043471574783325 3.457026481628418 3.457026481628418
Loss :  1.8028292655944824 3.2829599380493164 3.2829599380493164
Loss :  1.802749514579773 2.96561598777771 2.96561598777771
Loss :  1.7996183633804321 2.8718082904815674 2.8718082904815674
Loss :  1.8050874471664429 3.0816166400909424 3.0816166400909424
Loss :  1.8032361268997192 3.2858054637908936 3.2858054637908936
Loss :  1.7996987104415894 3.103576421737671 3.103576421737671
Loss :  1.8013666868209839 2.9609673023223877 2.9609673023223877
Loss :  1.7968766689300537 3.5419719219207764 3.5419719219207764
Loss :  1.8000413179397583 3.8083407878875732 3.8083407878875732
Loss :  1.795190453529358 3.7802042961120605 3.7802042961120605
Loss :  1.802680492401123 3.451530933380127 3.451530933380127
Loss :  1.8021512031555176 3.225637197494507 3.225637197494507
Loss :  1.8046947717666626 2.7148375511169434 2.7148375511169434
Loss :  1.799880027770996 2.769697427749634 2.769697427749634
Loss :  1.803546667098999 2.40767240524292 2.40767240524292
Loss :  1.8064876794815063 3.027956962585449 3.027956962585449
Loss :  1.806060791015625 3.194114923477173 3.194114923477173
Loss :  1.8102413415908813 3.4135444164276123 3.4135444164276123
Loss :  1.8014721870422363 2.9618749618530273 2.9618749618530273
  batch 60 loss: 1.8014721870422363, 2.9618749618530273, 2.9618749618530273
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8063050508499146 2.8774590492248535 2.8774590492248535
Loss :  1.8004454374313354 3.0164568424224854 3.0164568424224854
Loss :  1.8049323558807373 3.015047550201416 3.015047550201416
Loss :  1.8008023500442505 3.1759262084960938 3.1759262084960938
Loss :  1.8052949905395508 2.4056293964385986 2.4056293964385986
Loss :  1.7179319858551025 4.38701868057251 4.38701868057251
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7193658351898193 4.3441243171691895 4.3441243171691895
Loss :  1.7149772644042969 4.2330427169799805 4.2330427169799805
Loss :  1.7441422939300537 4.234437942504883 4.234437942504883
Total LOSS train 3.097586778494028 valid 4.299655914306641
CE LOSS train 1.8029090807988093 valid 0.4360355734825134
Contrastive LOSS train 3.097586778494028 valid 1.0586094856262207
EPOCH 244:
Loss :  1.8027065992355347 3.7341575622558594 3.7341575622558594
Loss :  1.7994945049285889 3.3700263500213623 3.3700263500213623
Loss :  1.802262783050537 3.1263270378112793 3.1263270378112793
Loss :  1.8031119108200073 3.519463300704956 3.519463300704956
Loss :  1.8020317554473877 3.5401604175567627 3.5401604175567627
Loss :  1.8058600425720215 3.6091980934143066 3.6091980934143066
Loss :  1.7998595237731934 3.9285295009613037 3.9285295009613037
Loss :  1.8006891012191772 3.923142194747925 3.923142194747925
Loss :  1.7980707883834839 4.180593013763428 4.180593013763428
Loss :  1.7976778745651245 4.196418285369873 4.196418285369873
Loss :  1.8080217838287354 4.006258487701416 4.006258487701416
Loss :  1.8021818399429321 4.189422130584717 4.189422130584717
Loss :  1.8067786693572998 3.825756072998047 3.825756072998047
Loss :  1.807013988494873 3.9361746311187744 3.9361746311187744
Loss :  1.7956194877624512 3.773141622543335 3.773141622543335
Loss :  1.8097399473190308 3.78464674949646 3.78464674949646
Loss :  1.806196689605713 2.947073221206665 2.947073221206665
Loss :  1.8021591901779175 3.5924668312072754 3.5924668312072754
Loss :  1.802612066268921 3.0321242809295654 3.0321242809295654
Loss :  1.7966408729553223 3.130470037460327 3.130470037460327
  batch 20 loss: 1.7966408729553223, 3.130470037460327, 3.130470037460327
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8052902221679688 3.0500833988189697 3.0500833988189697
Loss :  1.8043711185455322 2.8878066539764404 2.8878066539764404
Loss :  1.796863079071045 2.9008634090423584 2.9008634090423584
Loss :  1.803241491317749 3.5268731117248535 3.5268731117248535
Loss :  1.802823781967163 3.4455435276031494 3.4455435276031494
Loss :  1.8028258085250854 3.6429083347320557 3.6429083347320557
Loss :  1.8081865310668945 3.402897596359253 3.402897596359253
Loss :  1.7996524572372437 3.4400250911712646 3.4400250911712646
Loss :  1.8123403787612915 3.5530753135681152 3.5530753135681152
Loss :  1.799450159072876 3.0243453979492188 3.0243453979492188
Loss :  1.806625247001648 3.121994972229004 3.121994972229004
Loss :  1.796657681465149 3.213880777359009 3.213880777359009
Loss :  1.8054481744766235 2.8281607627868652 2.8281607627868652
Loss :  1.8046799898147583 2.979391574859619 2.979391574859619
Loss :  1.8045910596847534 2.9276084899902344 2.9276084899902344
Loss :  1.8069123029708862 2.9980051517486572 2.9980051517486572
Loss :  1.8036022186279297 3.1461293697357178 3.1461293697357178
Loss :  1.7991857528686523 2.7591004371643066 2.7591004371643066
Loss :  1.801835298538208 2.981417179107666 2.981417179107666
Loss :  1.7986786365509033 2.9212558269500732 2.9212558269500732
  batch 40 loss: 1.7986786365509033, 2.9212558269500732, 2.9212558269500732
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8036892414093018 2.8367345333099365 2.8367345333099365
Loss :  1.8025039434432983 2.7508318424224854 2.7508318424224854
Loss :  1.8019484281539917 2.6379714012145996 2.6379714012145996
Loss :  1.7996042966842651 2.8439009189605713 2.8439009189605713
Loss :  1.8046016693115234 2.68925142288208 2.68925142288208
Loss :  1.802685022354126 3.183946371078491 3.183946371078491
Loss :  1.799085021018982 2.8914031982421875 2.8914031982421875
Loss :  1.8012194633483887 2.7018864154815674 2.7018864154815674
Loss :  1.7961597442626953 2.6876699924468994 2.6876699924468994
Loss :  1.8001331090927124 3.0620789527893066 3.0620789527893066
Loss :  1.7953954935073853 3.751474618911743 3.751474618911743
Loss :  1.8025596141815186 2.9598538875579834 2.9598538875579834
Loss :  1.802381157875061 2.9159364700317383 2.9159364700317383
Loss :  1.8058509826660156 2.562228202819824 2.562228202819824
Loss :  1.8002886772155762 2.78865909576416 2.78865909576416
Loss :  1.8038629293441772 2.506391763687134 2.506391763687134
Loss :  1.8071794509887695 2.9754059314727783 2.9754059314727783
Loss :  1.8064788579940796 2.505359172821045 2.505359172821045
Loss :  1.8104946613311768 2.7502048015594482 2.7502048015594482
Loss :  1.8008822202682495 2.8033270835876465 2.8033270835876465
  batch 60 loss: 1.8008822202682495, 2.8033270835876465, 2.8033270835876465
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8071147203445435 2.9098265171051025 2.9098265171051025
Loss :  1.800533413887024 2.9586825370788574 2.9586825370788574
Loss :  1.805137038230896 3.1846694946289062 3.1846694946289062
Loss :  1.8009792566299438 3.385859966278076 3.385859966278076
Loss :  1.8057771921157837 2.703763961791992 2.703763961791992
Loss :  1.7179327011108398 4.448496341705322 4.448496341705322
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.7196317911148071 4.323469161987305 4.323469161987305
Loss :  1.7148364782333374 4.401640892028809 4.401640892028809
Loss :  1.74344003200531 4.380187034606934 4.380187034606934
Total LOSS train 3.200680534656231 valid 4.388448357582092
CE LOSS train 1.802746714078463 valid 0.4358600080013275
Contrastive LOSS train 3.200680534656231 valid 1.0950467586517334
EPOCH 245:
Loss :  1.8022758960723877 2.873535633087158 2.873535633087158
Loss :  1.7993042469024658 3.108806848526001 3.108806848526001
Loss :  1.8020912408828735 3.1860342025756836 3.1860342025756836
Loss :  1.802836537361145 3.01310396194458 3.01310396194458
Loss :  1.8013354539871216 2.939016103744507 2.939016103744507
Loss :  1.8055565357208252 2.6599671840667725 2.6599671840667725
Loss :  1.7995911836624146 3.16127872467041 3.16127872467041
Loss :  1.8002240657806396 2.502530097961426 2.502530097961426
Loss :  1.7969462871551514 2.8949928283691406 2.8949928283691406
Loss :  1.7969721555709839 2.7933948040008545 2.7933948040008545
Loss :  1.807354211807251 2.6739141941070557 2.6739141941070557
Loss :  1.8018993139266968 2.651296377182007 2.651296377182007
Loss :  1.8063691854476929 2.7182204723358154 2.7182204723358154
Loss :  1.8067771196365356 2.6171176433563232 2.6171176433563232
Loss :  1.794772982597351 2.6344656944274902 2.6344656944274902
Loss :  1.8089025020599365 2.8066022396087646 2.8066022396087646
Loss :  1.8056570291519165 2.3818368911743164 2.3818368911743164
Loss :  1.8016208410263062 3.0646865367889404 3.0646865367889404
Loss :  1.8021979331970215 2.7487072944641113 2.7487072944641113
Loss :  1.7962315082550049 3.0161960124969482 3.0161960124969482
  batch 20 loss: 1.7962315082550049, 3.0161960124969482, 3.0161960124969482
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.805114507675171 3.156921863555908 3.156921863555908
Loss :  1.8040236234664917 3.397772789001465 3.397772789001465
Loss :  1.7967486381530762 3.1302788257598877 3.1302788257598877
Loss :  1.803357720375061 3.560023069381714 3.560023069381714
Loss :  1.803479552268982 3.2452967166900635 3.2452967166900635
Loss :  1.8027222156524658 2.9650795459747314 2.9650795459747314
Loss :  1.807558536529541 3.098782539367676 3.098782539367676
Loss :  1.7988256216049194 2.77763295173645 2.77763295173645
Loss :  1.810788869857788 2.507664918899536 2.507664918899536
Loss :  1.7992085218429565 2.7338345050811768 2.7338345050811768
Loss :  1.8063987493515015 3.3150172233581543 3.3150172233581543
Loss :  1.7958799600601196 2.948911428451538 2.948911428451538
Loss :  1.8048230409622192 2.3092150688171387 2.3092150688171387
Loss :  1.8035039901733398 2.2181591987609863 2.2181591987609863
Loss :  1.804247498512268 2.857334852218628 2.857334852218628
Loss :  1.806061863899231 2.3414201736450195 2.3414201736450195
Loss :  1.803369164466858 2.3888087272644043 2.3888087272644043
Loss :  1.7985320091247559 2.345123529434204 2.345123529434204
Loss :  1.8011471033096313 2.7639315128326416 2.7639315128326416
Loss :  1.7979681491851807 3.1057047843933105 3.1057047843933105
  batch 40 loss: 1.7979681491851807, 3.1057047843933105, 3.1057047843933105
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.802918553352356 3.042039394378662 3.042039394378662
Loss :  1.801477074623108 2.869983434677124 2.869983434677124
Loss :  1.8016046285629272 3.3720180988311768 3.3720180988311768
Loss :  1.798585295677185 3.3472447395324707 3.3472447395324707
Loss :  1.804368257522583 3.5113298892974854 3.5113298892974854
Loss :  1.8021255731582642 3.3525023460388184 3.3525023460388184
Loss :  1.7988332509994507 3.158935546875 3.158935546875
Loss :  1.8007159233093262 3.08339262008667 3.08339262008667
Loss :  1.795885682106018 3.2344133853912354 3.2344133853912354
Loss :  1.7995857000350952 3.263640880584717 3.263640880584717
Loss :  1.7945716381072998 3.2942376136779785 3.2942376136779785
Loss :  1.8021575212478638 3.070401906967163 3.070401906967163
Loss :  1.8018380403518677 3.391134738922119 3.391134738922119
Loss :  1.8042826652526855 3.4304897785186768 3.4304897785186768
Loss :  1.799631953239441 3.053739070892334 3.053739070892334
Loss :  1.802778959274292 2.403420925140381 2.403420925140381
Loss :  1.8065134286880493 3.2100942134857178 3.2100942134857178
Loss :  1.806361436843872 3.029679298400879 3.029679298400879
Loss :  1.8099793195724487 3.4084579944610596 3.4084579944610596
Loss :  1.8006501197814941 3.6318182945251465 3.6318182945251465
  batch 60 loss: 1.8006501197814941, 3.6318182945251465, 3.6318182945251465
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8065056800842285 3.793088674545288 3.793088674545288
Loss :  1.800203561782837 3.827385902404785 3.827385902404785
Loss :  1.8049129247665405 3.4065101146698 3.4065101146698
Loss :  1.8006937503814697 3.638360023498535 3.638360023498535
Loss :  1.80528724193573 3.0263872146606445 3.0263872146606445
Loss :  1.7207081317901611 4.4222517013549805 4.4222517013549805
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7221814393997192 4.389571189880371 4.389571189880371
Loss :  1.7173688411712646 4.250705242156982 4.250705242156982
Loss :  1.7460086345672607 4.122631549835205 4.122631549835205
Total LOSS train 3.0071280626150276 valid 4.296289920806885
CE LOSS train 1.8022329807281494 valid 0.4365021586418152
Contrastive LOSS train 3.0071280626150276 valid 1.0306578874588013
EPOCH 246:
Loss :  1.8023453950881958 3.232229471206665 3.232229471206665
Loss :  1.7990000247955322 3.0564868450164795 3.0564868450164795
Loss :  1.8019224405288696 2.763963460922241 2.763963460922241
Loss :  1.8031138181686401 2.5872533321380615 2.5872533321380615
Loss :  1.801490306854248 2.9173996448516846 2.9173996448516846
Loss :  1.805761694908142 2.8247873783111572 2.8247873783111572
Loss :  1.8000332117080688 3.488624095916748 3.488624095916748
Loss :  1.8005268573760986 3.4997079372406006 3.4997079372406006
Loss :  1.797487735748291 2.819516181945801 2.819516181945801
Loss :  1.7977226972579956 2.4041872024536133 2.4041872024536133
Loss :  1.8074921369552612 2.605410575866699 2.605410575866699
Loss :  1.8022902011871338 2.5521228313446045 2.5521228313446045
Loss :  1.8065537214279175 2.4712789058685303 2.4712789058685303
Loss :  1.8068832159042358 2.576561212539673 2.576561212539673
Loss :  1.794674038887024 3.1294169425964355 3.1294169425964355
Loss :  1.8089715242385864 3.0172877311706543 3.0172877311706543
Loss :  1.8056968450546265 2.984585762023926 2.984585762023926
Loss :  1.8016119003295898 3.2205119132995605 3.2205119132995605
Loss :  1.801926612854004 3.1028547286987305 3.1028547286987305
Loss :  1.796109676361084 3.120375871658325 3.120375871658325
  batch 20 loss: 1.796109676361084, 3.120375871658325, 3.120375871658325
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8049124479293823 2.7225770950317383 2.7225770950317383
Loss :  1.8036872148513794 3.1701176166534424 3.1701176166534424
Loss :  1.7966123819351196 3.16310453414917 3.16310453414917
Loss :  1.8030439615249634 3.3115944862365723 3.3115944862365723
Loss :  1.8026471138000488 3.061211109161377 3.061211109161377
Loss :  1.8027031421661377 2.808476686477661 2.808476686477661
Loss :  1.8075909614562988 2.887644052505493 2.887644052505493
Loss :  1.7988405227661133 2.508388042449951 2.508388042449951
Loss :  1.8118395805358887 2.4460840225219727 2.4460840225219727
Loss :  1.7985339164733887 2.7354135513305664 2.7354135513305664
Loss :  1.8064759969711304 3.4427592754364014 3.4427592754364014
Loss :  1.795836329460144 3.1405248641967773 3.1405248641967773
Loss :  1.8053035736083984 3.3339719772338867 3.3339719772338867
Loss :  1.8039671182632446 3.0360097885131836 3.0360097885131836
Loss :  1.80420982837677 2.9378397464752197 2.9378397464752197
Loss :  1.8066563606262207 3.138303279876709 3.138303279876709
Loss :  1.8036147356033325 2.7625083923339844 2.7625083923339844
Loss :  1.7986539602279663 2.7690234184265137 2.7690234184265137
Loss :  1.801296591758728 3.0082156658172607 3.0082156658172607
Loss :  1.7982323169708252 3.0834341049194336 3.0834341049194336
  batch 40 loss: 1.7982323169708252, 3.0834341049194336, 3.0834341049194336
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8030542135238647 3.016615390777588 3.016615390777588
Loss :  1.8020899295806885 2.507368564605713 2.507368564605713
Loss :  1.8019306659698486 2.304417848587036 2.304417848587036
Loss :  1.7990703582763672 2.9288084506988525 2.9288084506988525
Loss :  1.8043588399887085 2.6666975021362305 2.6666975021362305
Loss :  1.8022383451461792 2.704073667526245 2.704073667526245
Loss :  1.7986265420913696 2.747521162033081 2.747521162033081
Loss :  1.8006186485290527 3.920459270477295 3.920459270477295
Loss :  1.7957847118377686 3.45548939704895 3.45548939704895
Loss :  1.7998440265655518 3.4146010875701904 3.4146010875701904
Loss :  1.794486403465271 3.4890785217285156 3.4890785217285156
Loss :  1.8017603158950806 2.7801284790039062 2.7801284790039062
Loss :  1.8014472723007202 2.337780237197876 2.337780237197876
Loss :  1.8044661283493042 2.509679079055786 2.509679079055786
Loss :  1.799760341644287 2.573275327682495 2.573275327682495
Loss :  1.8029289245605469 2.3469960689544678 2.3469960689544678
Loss :  1.8063766956329346 2.683698892593384 2.683698892593384
Loss :  1.8063054084777832 2.9272801876068115 2.9272801876068115
Loss :  1.8097227811813354 2.875614881515503 2.875614881515503
Loss :  1.8011784553527832 2.7068064212799072 2.7068064212799072
  batch 60 loss: 1.8011784553527832, 2.7068064212799072, 2.7068064212799072
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8061667680740356 2.964317798614502 2.964317798614502
Loss :  1.8002068996429443 3.27104115486145 3.27104115486145
Loss :  1.8046406507492065 2.8201277256011963 2.8201277256011963
Loss :  1.8006384372711182 3.2737956047058105 3.2737956047058105
Loss :  1.8052235841751099 2.9285995960235596 2.9285995960235596
Loss :  1.71678626537323 4.4544501304626465 4.4544501304626465
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7184242010116577 4.44228458404541 4.44228458404541
Loss :  1.7137343883514404 4.341996192932129 4.341996192932129
Loss :  1.7424204349517822 4.331766605377197 4.331766605377197
Total LOSS train 2.923015939272367 valid 4.392624378204346
CE LOSS train 1.8022953455264752 valid 0.43560510873794556
Contrastive LOSS train 2.923015939272367 valid 1.0829416513442993
EPOCH 247:
Loss :  1.8021411895751953 3.3003387451171875 3.3003387451171875
Loss :  1.7988519668579102 3.191467046737671 3.191467046737671
Loss :  1.8018699884414673 2.4405741691589355 2.4405741691589355
Loss :  1.8026429414749146 2.5658860206604004 2.5658860206604004
Loss :  1.800838589668274 2.7918965816497803 2.7918965816497803
Loss :  1.8050850629806519 2.9434962272644043 2.9434962272644043
Loss :  1.7996852397918701 3.3622426986694336 3.3622426986694336
Loss :  1.8001424074172974 3.2561330795288086 3.2561330795288086
Loss :  1.7967947721481323 2.9994912147521973 2.9994912147521973
Loss :  1.7968878746032715 2.826449394226074 2.826449394226074
Loss :  1.8072253465652466 2.927736520767212 2.927736520767212
Loss :  1.80195951461792 3.230224847793579 3.230224847793579
Loss :  1.8066010475158691 3.267025947570801 3.267025947570801
Loss :  1.8066952228546143 3.062155246734619 3.062155246734619
Loss :  1.7941309213638306 3.2067837715148926 3.2067837715148926
Loss :  1.8084505796432495 3.161757707595825 3.161757707595825
Loss :  1.8056522607803345 3.144395112991333 3.144395112991333
Loss :  1.8015588521957397 2.890279531478882 2.890279531478882
Loss :  1.8021317720413208 3.020720958709717 3.020720958709717
Loss :  1.796431303024292 2.5314550399780273 2.5314550399780273
  batch 20 loss: 1.796431303024292, 2.5314550399780273, 2.5314550399780273
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8052399158477783 2.5831894874572754 2.5831894874572754
Loss :  1.8040233850479126 2.463113307952881 2.463113307952881
Loss :  1.7969251871109009 2.7110955715179443 2.7110955715179443
Loss :  1.803026556968689 3.107551336288452 3.107551336288452
Loss :  1.8027454614639282 3.0958809852600098 3.0958809852600098
Loss :  1.802946925163269 3.3561062812805176 3.3561062812805176
Loss :  1.8079184293746948 3.519815683364868 3.519815683364868
Loss :  1.7994312047958374 3.391075611114502 3.391075611114502
Loss :  1.8123420476913452 2.717939615249634 2.717939615249634
Loss :  1.7990167140960693 2.670499086380005 2.670499086380005
Loss :  1.8065571784973145 2.67834734916687 2.67834734916687
Loss :  1.7962181568145752 3.1583659648895264 3.1583659648895264
Loss :  1.8056087493896484 2.9628489017486572 2.9628489017486572
Loss :  1.8047336339950562 2.865139961242676 2.865139961242676
Loss :  1.8047001361846924 3.1874964237213135 3.1874964237213135
Loss :  1.8068342208862305 2.651174306869507 2.651174306869507
Loss :  1.803981900215149 2.75858736038208 2.75858736038208
Loss :  1.7995474338531494 2.8923499584198 2.8923499584198
Loss :  1.801987648010254 3.2158782482147217 3.2158782482147217
Loss :  1.7989232540130615 3.0602152347564697 3.0602152347564697
  batch 40 loss: 1.7989232540130615, 3.0602152347564697, 3.0602152347564697
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.803589105606079 3.8597772121429443 3.8597772121429443
Loss :  1.8029305934906006 3.110199451446533 3.110199451446533
Loss :  1.8021432161331177 3.2815942764282227 3.2815942764282227
Loss :  1.7994873523712158 2.7964024543762207 2.7964024543762207
Loss :  1.8047535419464111 2.694277763366699 2.694277763366699
Loss :  1.8032065629959106 2.922247886657715 2.922247886657715
Loss :  1.7998052835464478 3.228562355041504 3.228562355041504
Loss :  1.8011107444763184 3.4766433238983154 3.4766433238983154
Loss :  1.7971813678741455 3.580451250076294 3.580451250076294
Loss :  1.8004214763641357 3.612455129623413 3.612455129623413
Loss :  1.7951849699020386 3.727017879486084 3.727017879486084
Loss :  1.8026167154312134 3.0224313735961914 3.0224313735961914
Loss :  1.8022024631500244 3.3097617626190186 3.3097617626190186
Loss :  1.8055397272109985 3.3583953380584717 3.3583953380584717
Loss :  1.8002269268035889 2.8751115798950195 2.8751115798950195
Loss :  1.8039172887802124 2.9954171180725098 2.9954171180725098
Loss :  1.8069393634796143 3.0585691928863525 3.0585691928863525
Loss :  1.8062773942947388 3.5871822834014893 3.5871822834014893
Loss :  1.8098843097686768 3.4290049076080322 3.4290049076080322
Loss :  1.8008625507354736 3.092294454574585 3.092294454574585
  batch 60 loss: 1.8008625507354736, 3.092294454574585, 3.092294454574585
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8067083358764648 3.0981838703155518 3.0981838703155518
Loss :  1.7998329401016235 3.187037467956543 3.187037467956543
Loss :  1.8049092292785645 2.8934338092803955 2.8934338092803955
Loss :  1.8008238077163696 2.957200527191162 2.957200527191162
Loss :  1.8050287961959839 2.917098045349121 2.917098045349121
Loss :  1.7176493406295776 4.375442981719971 4.375442981719971
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7191914319992065 4.342626094818115 4.342626094818115
Loss :  1.7147166728973389 4.318612098693848 4.318612098693848
Loss :  1.743436574935913 4.093481063842773 4.093481063842773
Total LOSS train 3.0651989423311674 valid 4.282540559768677
CE LOSS train 1.8025241393309372 valid 0.43585914373397827
Contrastive LOSS train 3.0651989423311674 valid 1.0233702659606934
EPOCH 248:
Loss :  1.8017146587371826 3.3154845237731934 3.3154845237731934
Loss :  1.799410343170166 3.4488611221313477 3.4488611221313477
Loss :  1.8017802238464355 2.875213861465454 2.875213861465454
Loss :  1.8024601936340332 3.163278102874756 3.163278102874756
Loss :  1.8009883165359497 2.800177574157715 2.800177574157715
Loss :  1.8050175905227661 2.8153865337371826 2.8153865337371826
Loss :  1.8000794649124146 3.175816059112549 3.175816059112549
Loss :  1.8002104759216309 3.092689037322998 3.092689037322998
Loss :  1.7964662313461304 3.724729299545288 3.724729299545288
Loss :  1.796862006187439 3.443462610244751 3.443462610244751
Loss :  1.807263731956482 3.2536890506744385 3.2536890506744385
Loss :  1.801700472831726 3.6396148204803467 3.6396148204803467
Loss :  1.806767225265503 3.200582981109619 3.200582981109619
Loss :  1.8064016103744507 3.1148972511291504 3.1148972511291504
Loss :  1.7942423820495605 3.051931142807007 3.051931142807007
Loss :  1.808485984802246 2.775479793548584 2.775479793548584
Loss :  1.8056195974349976 2.796762228012085 2.796762228012085
Loss :  1.80153226852417 2.3411219120025635 2.3411219120025635
Loss :  1.8018081188201904 2.9390058517456055 2.9390058517456055
Loss :  1.7963473796844482 2.673828125 2.673828125
  batch 20 loss: 1.7963473796844482, 2.673828125, 2.673828125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8049299716949463 2.658426523208618 2.658426523208618
Loss :  1.803774118423462 3.2032742500305176 3.2032742500305176
Loss :  1.796813726425171 3.627504825592041 3.627504825592041
Loss :  1.8028087615966797 3.4476304054260254 3.4476304054260254
Loss :  1.8030271530151367 3.8638060092926025 3.8638060092926025
Loss :  1.8029224872589111 3.672799825668335 3.672799825668335
Loss :  1.8076927661895752 3.4279491901397705 3.4279491901397705
Loss :  1.7993820905685425 3.1879661083221436 3.1879661083221436
Loss :  1.811926007270813 3.3173277378082275 3.3173277378082275
Loss :  1.7991058826446533 3.1703989505767822 3.1703989505767822
Loss :  1.8063768148422241 3.4244093894958496 3.4244093894958496
Loss :  1.795689344406128 3.576871395111084 3.576871395111084
Loss :  1.805364727973938 3.7563111782073975 3.7563111782073975
Loss :  1.8046735525131226 3.5625038146972656 3.5625038146972656
Loss :  1.804446816444397 3.615863800048828 3.615863800048828
Loss :  1.807080864906311 4.167135238647461 4.167135238647461
Loss :  1.8034327030181885 3.9690003395080566 3.9690003395080566
Loss :  1.7989389896392822 3.8524866104125977 3.8524866104125977
Loss :  1.8015092611312866 3.3569977283477783 3.3569977283477783
Loss :  1.7983990907669067 3.409496784210205 3.409496784210205
  batch 40 loss: 1.7983990907669067, 3.409496784210205, 3.409496784210205
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8032798767089844 3.3512868881225586 3.3512868881225586
Loss :  1.8025723695755005 2.9484975337982178 2.9484975337982178
Loss :  1.802038311958313 2.842501640319824 2.842501640319824
Loss :  1.799769401550293 3.220136880874634 3.220136880874634
Loss :  1.8047012090682983 3.6540932655334473 3.6540932655334473
Loss :  1.8030096292495728 3.2890079021453857 3.2890079021453857
Loss :  1.7997170686721802 3.4030330181121826 3.4030330181121826
Loss :  1.8013803958892822 3.8405942916870117 3.8405942916870117
Loss :  1.7973450422286987 3.401463747024536 3.401463747024536
Loss :  1.8009002208709717 3.5214967727661133 3.5214967727661133
Loss :  1.795250415802002 3.4980063438415527 3.4980063438415527
Loss :  1.8025585412979126 3.263068199157715 3.263068199157715
Loss :  1.8027360439300537 3.0207433700561523 3.0207433700561523
Loss :  1.8063172101974487 3.1615638732910156 3.1615638732910156
Loss :  1.7997642755508423 3.6188433170318604 3.6188433170318604
Loss :  1.8039796352386475 3.239757776260376 3.239757776260376
Loss :  1.8077126741409302 3.329190254211426 3.329190254211426
Loss :  1.8067023754119873 3.668105363845825 3.668105363845825
Loss :  1.8100389242172241 3.4139091968536377 3.4139091968536377
Loss :  1.8001446723937988 3.3067805767059326 3.3067805767059326
  batch 60 loss: 1.8001446723937988, 3.3067805767059326, 3.3067805767059326
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8078222274780273 3.2880542278289795 3.2880542278289795
Loss :  1.8002341985702515 3.2354955673217773 3.2354955673217773
Loss :  1.8054499626159668 2.9471981525421143 2.9471981525421143
Loss :  1.8015401363372803 3.00100040435791 3.00100040435791
Loss :  1.8064236640930176 2.40151047706604 2.40151047706604
Loss :  1.716198444366455 4.421513080596924 4.421513080596924
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.717479944229126 4.4045610427856445 4.4045610427856445
Loss :  1.713213324546814 4.339936256408691 4.339936256408691
Loss :  1.7428600788116455 4.200952529907227 4.200952529907227
Total LOSS train 3.2888540157904993 valid 4.341740727424622
CE LOSS train 1.8025360290820782 valid 0.4357150197029114
Contrastive LOSS train 3.2888540157904993 valid 1.0502381324768066
EPOCH 249:
Loss :  1.8020051717758179 3.0759735107421875 3.0759735107421875
Loss :  1.7998335361480713 2.874830722808838 2.874830722808838
Loss :  1.8026071786880493 2.7053635120391846 2.7053635120391846
Loss :  1.8027164936065674 3.082514762878418 3.082514762878418
Loss :  1.8012995719909668 3.1793293952941895 3.1793293952941895
Loss :  1.8056011199951172 3.0847432613372803 3.0847432613372803
Loss :  1.8006150722503662 3.5579872131347656 3.5579872131347656
Loss :  1.8013222217559814 3.257521390914917 3.257521390914917
Loss :  1.7969938516616821 3.589963674545288 3.589963674545288
Loss :  1.7982474565505981 2.881624937057495 2.881624937057495
Loss :  1.808726191520691 2.89227032661438 2.89227032661438
Loss :  1.802685022354126 3.225248098373413 3.225248098373413
Loss :  1.8076180219650269 2.9303529262542725 2.9303529262542725
Loss :  1.8071211576461792 3.549102544784546 3.549102544784546
Loss :  1.7939953804016113 3.470578908920288 3.470578908920288
Loss :  1.8096824884414673 3.358459711074829 3.358459711074829
Loss :  1.8064348697662354 3.097438097000122 3.097438097000122
Loss :  1.802242398262024 2.7670018672943115 2.7670018672943115
Loss :  1.8025017976760864 3.0740880966186523 3.0740880966186523
Loss :  1.7967499494552612 3.1258063316345215 3.1258063316345215
  batch 20 loss: 1.7967499494552612, 3.1258063316345215, 3.1258063316345215
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8048408031463623 2.4946165084838867 2.4946165084838867
Loss :  1.8039451837539673 3.300440788269043 3.300440788269043
Loss :  1.7965964078903198 3.132540464401245 3.132540464401245
Loss :  1.8028178215026855 3.2619292736053467 3.2619292736053467
Loss :  1.8029834032058716 3.172447919845581 3.172447919845581
Loss :  1.8031513690948486 2.9717206954956055 2.9717206954956055
Loss :  1.8080517053604126 2.8924546241760254 2.8924546241760254
Loss :  1.7995353937149048 2.8940577507019043 2.8940577507019043
Loss :  1.812309741973877 2.4870517253875732 2.4870517253875732
Loss :  1.7994083166122437 2.691946506500244 2.691946506500244
Loss :  1.8067235946655273 2.7584147453308105 2.7584147453308105
Loss :  1.795762538909912 2.3935065269470215 2.3935065269470215
Loss :  1.8055301904678345 2.305506467819214 2.305506467819214
Loss :  1.8046027421951294 2.4034647941589355 2.4034647941589355
Loss :  1.8043289184570312 2.4695963859558105 2.4695963859558105
Loss :  1.8069757223129272 2.9538276195526123 2.9538276195526123
Loss :  1.8033959865570068 3.1176881790161133 3.1176881790161133
Loss :  1.798576831817627 2.7925400733947754 2.7925400733947754
Loss :  1.800912857055664 2.9952685832977295 2.9952685832977295
Loss :  1.7979100942611694 3.1776363849639893 3.1776363849639893
  batch 40 loss: 1.7979100942611694, 3.1776363849639893, 3.1776363849639893
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8026584386825562 3.2398407459259033 3.2398407459259033
Loss :  1.802291989326477 2.841905355453491 2.841905355453491
Loss :  1.8014185428619385 3.036060333251953 3.036060333251953
Loss :  1.7991527318954468 3.0783395767211914 3.0783395767211914
Loss :  1.8040589094161987 3.215202808380127 3.215202808380127
Loss :  1.802144169807434 2.4514319896698 2.4514319896698
Loss :  1.7988591194152832 2.417289972305298 2.417289972305298
Loss :  1.80046808719635 2.1815686225891113 2.1815686225891113
Loss :  1.7961418628692627 2.387030601501465 2.387030601501465
Loss :  1.8000664710998535 2.759599208831787 2.759599208831787
Loss :  1.7945564985275269 3.0644543170928955 3.0644543170928955
Loss :  1.8015896081924438 2.502601146697998 2.502601146697998
Loss :  1.8014702796936035 2.584643840789795 2.584643840789795
Loss :  1.8047709465026855 2.6063625812530518 2.6063625812530518
Loss :  1.7994194030761719 2.9039926528930664 2.9039926528930664
Loss :  1.8022074699401855 3.075637102127075 3.075637102127075
Loss :  1.8062589168548584 3.045020341873169 3.045020341873169
Loss :  1.8058266639709473 2.782130241394043 2.782130241394043
Loss :  1.809049367904663 2.733975887298584 2.733975887298584
Loss :  1.7994611263275146 2.7201454639434814 2.7201454639434814
  batch 60 loss: 1.7994611263275146, 2.7201454639434814, 2.7201454639434814
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8060076236724854 2.970350742340088 2.970350742340088
Loss :  1.7992244958877563 3.141453504562378 3.141453504562378
Loss :  1.8042821884155273 3.8382551670074463 3.8382551670074463
Loss :  1.8001434803009033 3.2250924110412598 3.2250924110412598
Loss :  1.8049014806747437 3.4475691318511963 3.4475691318511963
Loss :  1.7140400409698486 4.423224925994873 4.423224925994873
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.71555757522583 4.398251056671143 4.398251056671143
Loss :  1.7113721370697021 4.292914390563965 4.292914390563965
Loss :  1.7400213479995728 4.335897922515869 4.335897922515869
Total LOSS train 2.9491816777449387 valid 4.362572073936462
CE LOSS train 1.8023659761135395 valid 0.4350053369998932
Contrastive LOSS train 2.9491816777449387 valid 1.0839744806289673
EPOCH 250:
Loss :  1.8011927604675293 2.7090485095977783 2.7090485095977783
Loss :  1.7982548475265503 2.748581647872925 2.748581647872925
Loss :  1.8012069463729858 2.361675500869751 2.361675500869751
Loss :  1.802189588546753 2.404311180114746 2.404311180114746
Loss :  1.8005857467651367 2.432643413543701 2.432643413543701
Loss :  1.8048597574234009 2.7626419067382812 2.7626419067382812
Loss :  1.7992652654647827 2.694282293319702 2.694282293319702
Loss :  1.7997630834579468 2.4264683723449707 2.4264683723449707
Loss :  1.7965072393417358 2.502744197845459 2.502744197845459
Loss :  1.79694664478302 2.50443434715271 2.50443434715271
Loss :  1.8070474863052368 2.8316609859466553 2.8316609859466553
Loss :  1.8018251657485962 2.6126291751861572 2.6126291751861572
Loss :  1.8063126802444458 2.8022620677948 2.8022620677948
Loss :  1.8063651323318481 3.1247949600219727 3.1247949600219727
Loss :  1.7934257984161377 3.6506173610687256 3.6506173610687256
Loss :  1.807969570159912 3.2898571491241455 3.2898571491241455
Loss :  1.805103063583374 3.5988245010375977 3.5988245010375977
Loss :  1.8010432720184326 2.712829351425171 2.712829351425171
Loss :  1.8016444444656372 2.5408449172973633 2.5408449172973633
Loss :  1.795755386352539 2.310277223587036 2.310277223587036
  batch 20 loss: 1.795755386352539, 2.310277223587036, 2.310277223587036
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.804667592048645 2.302943706512451 2.302943706512451
Loss :  1.8034052848815918 2.426072120666504 2.426072120666504
Loss :  1.7968921661376953 2.440335988998413 2.440335988998413
Loss :  1.8028861284255981 2.586937189102173 2.586937189102173
Loss :  1.8032164573669434 2.690917730331421 2.690917730331421
Loss :  1.8027610778808594 2.3757803440093994 2.3757803440093994
Loss :  1.8072742223739624 2.940178155899048 2.940178155899048
Loss :  1.7990436553955078 2.681269645690918 2.681269645690918
Loss :  1.8113970756530762 3.5636661052703857 3.5636661052703857
Loss :  1.7988853454589844 3.230987548828125 3.230987548828125
Loss :  1.8061188459396362 3.082036256790161 3.082036256790161
Loss :  1.7957477569580078 3.0729763507843018 3.0729763507843018
Loss :  1.805377721786499 3.770314931869507 3.770314931869507
Loss :  1.8043714761734009 3.0897419452667236 3.0897419452667236
Loss :  1.804385781288147 3.567885637283325 3.567885637283325
Loss :  1.8065159320831299 3.1335909366607666 3.1335909366607666
Loss :  1.803588628768921 2.3492980003356934 2.3492980003356934
Loss :  1.7986366748809814 2.304711103439331 2.304711103439331
Loss :  1.8016568422317505 2.5065252780914307 2.5065252780914307
Loss :  1.7983981370925903 2.813098907470703 2.813098907470703
  batch 40 loss: 1.7983981370925903, 2.813098907470703, 2.813098907470703
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8031551837921143 2.9749937057495117 2.9749937057495117
Loss :  1.8021444082260132 3.239996910095215 3.239996910095215
Loss :  1.8019311428070068 3.218139410018921 3.218139410018921
Loss :  1.7990766763687134 3.19087290763855 3.19087290763855
Loss :  1.804282546043396 3.520679473876953 3.520679473876953
Loss :  1.8021374940872192 3.306936740875244 3.306936740875244
Loss :  1.799203872680664 3.020538806915283 3.020538806915283
Loss :  1.8005050420761108 3.4464027881622314 3.4464027881622314
Loss :  1.7960525751113892 2.794672727584839 2.794672727584839
Loss :  1.7997506856918335 2.9600441455841064 2.9600441455841064
Loss :  1.7944605350494385 2.598053216934204 2.598053216934204
Loss :  1.8017879724502563 2.5765507221221924 2.5765507221221924
Loss :  1.8014531135559082 2.3305013179779053 2.3305013179779053
Loss :  1.8041094541549683 2.566263437271118 2.566263437271118
Loss :  1.799312710762024 2.6121437549591064 2.6121437549591064
Loss :  1.8026467561721802 2.81965708732605 2.81965708732605
Loss :  1.8060328960418701 3.1288647651672363 3.1288647651672363
Loss :  1.805772066116333 2.9656312465667725 2.9656312465667725
Loss :  1.8092195987701416 3.4419708251953125 3.4419708251953125
Loss :  1.800106406211853 3.171105146408081 3.171105146408081
  batch 60 loss: 1.800106406211853, 3.171105146408081, 3.171105146408081
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8061673641204834 3.4648776054382324 3.4648776054382324
Loss :  1.79935884475708 2.8420958518981934 2.8420958518981934
Loss :  1.8045302629470825 2.719223976135254 2.719223976135254
Loss :  1.8005104064941406 3.0281596183776855 3.0281596183776855
Loss :  1.8046810626983643 2.2748594284057617 2.2748594284057617
Loss :  1.7171095609664917 4.405649662017822 4.405649662017822
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7188667058944702 4.384728908538818 4.384728908538818
Loss :  1.7142505645751953 4.291205406188965 4.291205406188965
Loss :  1.742525577545166 4.343743801116943 4.343743801116943
Total LOSS train 2.8640605009519136 valid 4.356331944465637
CE LOSS train 1.8020135347659771 valid 0.4356313943862915
Contrastive LOSS train 2.8640605009519136 valid 1.0859359502792358
EPOCH 251:
Loss :  1.8012880086898804 3.089339017868042 3.089339017868042
Loss :  1.799108862876892 3.1434783935546875 3.1434783935546875
Loss :  1.8014286756515503 2.668175220489502 2.668175220489502
Loss :  1.8020548820495605 3.2867214679718018 3.2867214679718018
Loss :  1.800639033317566 3.229905843734741 3.229905843734741
Loss :  1.8047269582748413 2.872328996658325 2.872328996658325
Loss :  1.799815058708191 3.112816333770752 3.112816333770752
Loss :  1.7997623682022095 3.0344467163085938 3.0344467163085938
Loss :  1.796404242515564 2.9358723163604736 2.9358723163604736
Loss :  1.7967582941055298 3.10683012008667 3.10683012008667
Loss :  1.8069146871566772 2.9695520401000977 2.9695520401000977
Loss :  1.8015742301940918 2.725250005722046 2.725250005722046
Loss :  1.8065253496170044 3.0073583126068115 3.0073583126068115
Loss :  1.8062379360198975 2.53778076171875 2.53778076171875
Loss :  1.7940438985824585 3.1099536418914795 3.1099536418914795
Loss :  1.8079038858413696 2.5800845623016357 2.5800845623016357
Loss :  1.8052854537963867 3.089017152786255 3.089017152786255
Loss :  1.8013287782669067 2.640307664871216 2.640307664871216
Loss :  1.8016345500946045 2.6728367805480957 2.6728367805480957
Loss :  1.7963615655899048 2.5731585025787354 2.5731585025787354
  batch 20 loss: 1.7963615655899048, 2.5731585025787354, 2.5731585025787354
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8049471378326416 2.2472076416015625 2.2472076416015625
Loss :  1.8036020994186401 3.393951892852783 3.393951892852783
Loss :  1.7970242500305176 2.6665990352630615 2.6665990352630615
Loss :  1.802778959274292 2.9379897117614746 2.9379897117614746
Loss :  1.8024643659591675 3.081676483154297 3.081676483154297
Loss :  1.8025951385498047 2.8866541385650635 2.8866541385650635
Loss :  1.807421326637268 3.0834505558013916 3.0834505558013916
Loss :  1.7989927530288696 2.892061471939087 2.892061471939087
Loss :  1.81175696849823 2.7223808765411377 2.7223808765411377
Loss :  1.7982099056243896 2.946279764175415 2.946279764175415
Loss :  1.8060359954833984 3.1489195823669434 3.1489195823669434
Loss :  1.7953852415084839 2.8421926498413086 2.8421926498413086
Loss :  1.8050265312194824 3.0327470302581787 3.0327470302581787
Loss :  1.803755760192871 3.0605483055114746 3.0605483055114746
Loss :  1.8040858507156372 3.3180124759674072 3.3180124759674072
Loss :  1.8064391613006592 3.324847936630249 3.324847936630249
Loss :  1.803297996520996 3.4929678440093994 3.4929678440093994
Loss :  1.7984424829483032 3.124168634414673 3.124168634414673
Loss :  1.8010133504867554 3.469780921936035 3.469780921936035
Loss :  1.7977511882781982 3.18450927734375 3.18450927734375
  batch 40 loss: 1.7977511882781982, 3.18450927734375, 3.18450927734375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.802840232849121 3.5042974948883057 3.5042974948883057
Loss :  1.8019742965698242 2.9343056678771973 2.9343056678771973
Loss :  1.8016036748886108 3.039361000061035 3.039361000061035
Loss :  1.7989578247070312 2.963407278060913 2.963407278060913
Loss :  1.8042056560516357 3.014392375946045 3.014392375946045
Loss :  1.8022691011428833 3.2969069480895996 3.2969069480895996
Loss :  1.7992126941680908 3.331864833831787 3.331864833831787
Loss :  1.8007055521011353 3.2675209045410156 3.2675209045410156
Loss :  1.7964212894439697 3.6914987564086914 3.6914987564086914
Loss :  1.8000539541244507 3.5366058349609375 3.5366058349609375
Loss :  1.7946981191635132 3.467024564743042 3.467024564743042
Loss :  1.801991581916809 3.0813167095184326 3.0813167095184326
Loss :  1.8019194602966309 3.0305399894714355 3.0305399894714355
Loss :  1.804834246635437 3.367797613143921 3.367797613143921
Loss :  1.7996177673339844 3.1512506008148193 3.1512506008148193
Loss :  1.8029875755310059 2.7274587154388428 2.7274587154388428
Loss :  1.8065608739852905 3.0041441917419434 3.0041441917419434
Loss :  1.8060426712036133 2.4849963188171387 2.4849963188171387
Loss :  1.8094656467437744 2.984761953353882 2.984761953353882
Loss :  1.800394058227539 2.6688497066497803 2.6688497066497803
  batch 60 loss: 1.800394058227539, 2.6688497066497803, 2.6688497066497803
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.806485891342163 3.094390392303467 3.094390392303467
Loss :  1.799771785736084 2.963531970977783 2.963531970977783
Loss :  1.804714322090149 2.8634979724884033 2.8634979724884033
Loss :  1.800789713859558 3.229015588760376 3.229015588760376
Loss :  1.8052059412002563 2.3925974369049072 2.3925974369049072
Loss :  1.7158184051513672 4.434683322906494 4.434683322906494
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.717471957206726 4.389566421508789 4.389566421508789
Loss :  1.7129631042480469 4.2349772453308105 4.2349772453308105
Loss :  1.7415037155151367 4.2203264236450195 4.2203264236450195
Total LOSS train 3.02051530617934 valid 4.319888353347778
CE LOSS train 1.802069955605727 valid 0.4353759288787842
Contrastive LOSS train 3.02051530617934 valid 1.0550816059112549
EPOCH 252:
Loss :  1.801479697227478 2.9679243564605713 2.9679243564605713
Loss :  1.7991927862167358 3.299851894378662 3.299851894378662
Loss :  1.8018244504928589 2.9489352703094482 2.9489352703094482
Loss :  1.8023818731307983 2.907461643218994 2.907461643218994
Loss :  1.8008345365524292 3.1386449337005615 3.1386449337005615
Loss :  1.8050683736801147 3.270747423171997 3.270747423171997
Loss :  1.7999814748764038 3.4374547004699707 3.4374547004699707
Loss :  1.8001792430877686 3.416944980621338 3.416944980621338
Loss :  1.7965232133865356 3.2219595909118652 3.2219595909118652
Loss :  1.7971161603927612 3.488083839416504 3.488083839416504
Loss :  1.8073664903640747 3.153775453567505 3.153775453567505
Loss :  1.8018872737884521 3.3194777965545654 3.3194777965545654
Loss :  1.8068426847457886 3.4747228622436523 3.4747228622436523
Loss :  1.8064770698547363 3.0668880939483643 3.0668880939483643
Loss :  1.7940011024475098 3.636025905609131 3.636025905609131
Loss :  1.808284044265747 3.677229166030884 3.677229166030884
Loss :  1.8056031465530396 3.1274726390838623 3.1274726390838623
Loss :  1.8015758991241455 3.6608970165252686 3.6608970165252686
Loss :  1.8019309043884277 3.5491509437561035 3.5491509437561035
Loss :  1.7964850664138794 3.625213146209717 3.625213146209717
  batch 20 loss: 1.7964850664138794, 3.625213146209717, 3.625213146209717
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.804988980293274 3.425048828125 3.425048828125
Loss :  1.803720235824585 3.354827880859375 3.354827880859375
Loss :  1.7970207929611206 3.3670756816864014 3.3670756816864014
Loss :  1.802818775177002 3.4281091690063477 3.4281091690063477
Loss :  1.8026041984558105 4.086587905883789 4.086587905883789
Loss :  1.8027598857879639 2.723463296890259 2.723463296890259
Loss :  1.8075151443481445 3.4003236293792725 3.4003236293792725
Loss :  1.799173355102539 3.0619935989379883 3.0619935989379883
Loss :  1.8119240999221802 2.7185347080230713 2.7185347080230713
Loss :  1.7985517978668213 2.9405863285064697 2.9405863285064697
Loss :  1.8061811923980713 2.991644859313965 2.991644859313965
Loss :  1.7955900430679321 3.061277151107788 3.061277151107788
Loss :  1.805179476737976 2.8613436222076416 2.8613436222076416
Loss :  1.8041237592697144 2.84014630317688 2.84014630317688
Loss :  1.8042550086975098 3.1447343826293945 3.1447343826293945
Loss :  1.8066262006759644 3.013115882873535 3.013115882873535
Loss :  1.8034090995788574 2.8525278568267822 2.8525278568267822
Loss :  1.7986700534820557 2.678050994873047 2.678050994873047
Loss :  1.8012266159057617 3.0031650066375732 3.0031650066375732
Loss :  1.798043966293335 2.8472187519073486 2.8472187519073486
  batch 40 loss: 1.798043966293335, 2.8472187519073486, 2.8472187519073486
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8030701875686646 3.166294813156128 3.166294813156128
Loss :  1.802245855331421 2.690810203552246 2.690810203552246
Loss :  1.801855444908142 2.8589231967926025 2.8589231967926025
Loss :  1.7992475032806396 2.601224660873413 2.601224660873413
Loss :  1.8043991327285767 2.6765596866607666 2.6765596866607666
Loss :  1.802450180053711 2.9888501167297363 2.9888501167297363
Loss :  1.7994135618209839 3.3794660568237305 3.3794660568237305
Loss :  1.8009237051010132 2.7261977195739746 2.7261977195739746
Loss :  1.796581506729126 3.1133322715759277 3.1133322715759277
Loss :  1.8002625703811646 3.0391149520874023 3.0391149520874023
Loss :  1.7949036359786987 3.191408395767212 3.191408395767212
Loss :  1.8020724058151245 3.223106622695923 3.223106622695923
Loss :  1.802049160003662 3.525428295135498 3.525428295135498
Loss :  1.8051832914352417 3.304979085922241 3.304979085922241
Loss :  1.799737572669983 3.554453134536743 3.554453134536743
Loss :  1.8030054569244385 3.2234628200531006 3.2234628200531006
Loss :  1.8067244291305542 3.546875476837158 3.546875476837158
Loss :  1.8061209917068481 3.6757543087005615 3.6757543087005615
Loss :  1.8095968961715698 3.8873484134674072 3.8873484134674072
Loss :  1.8003746271133423 3.393385648727417 3.393385648727417
  batch 60 loss: 1.8003746271133423, 3.393385648727417, 3.393385648727417
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8066153526306152 3.7105977535247803 3.7105977535247803
Loss :  1.799853801727295 3.4237277507781982 3.4237277507781982
Loss :  1.804741382598877 2.7620584964752197 2.7620584964752197
Loss :  1.800874948501587 2.8792312145233154 2.8792312145233154
Loss :  1.805317997932434 3.540471076965332 3.540471076965332
Loss :  1.7157223224639893 4.421889305114746 4.421889305114746
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7173446416854858 4.398599147796631 4.398599147796631
Loss :  1.7128679752349854 4.366183280944824 4.366183280944824
Loss :  1.74152410030365 4.368921756744385 4.368921756744385
Total LOSS train 3.204179994876568 valid 4.3888983726501465
CE LOSS train 1.802262150324308 valid 0.4353810250759125
Contrastive LOSS train 3.204179994876568 valid 1.0922304391860962
EPOCH 253:
Loss :  1.8015973567962646 2.733933210372925 2.733933210372925
Loss :  1.7992304563522339 3.0264713764190674 3.0264713764190674
Loss :  1.8019391298294067 2.1838035583496094 2.1838035583496094
Loss :  1.802514910697937 2.460171699523926 2.460171699523926
Loss :  1.8009333610534668 2.1862924098968506 2.1862924098968506
Loss :  1.805164098739624 2.380911111831665 2.380911111831665
Loss :  1.8000568151474 2.6276228427886963 2.6276228427886963
Loss :  1.8002480268478394 2.487213373184204 2.487213373184204
Loss :  1.7966320514678955 2.5734970569610596 2.5734970569610596
Loss :  1.7972221374511719 2.6588294506073 2.6588294506073
Loss :  1.8074569702148438 2.718545436859131 2.718545436859131
Loss :  1.8019731044769287 3.4434564113616943 3.4434564113616943
Loss :  1.8068925142288208 3.3476388454437256 3.3476388454437256
Loss :  1.8065422773361206 2.966580629348755 2.966580629348755
Loss :  1.7941254377365112 3.3730437755584717 3.3730437755584717
Loss :  1.808411717414856 2.9989547729492188 2.9989547729492188
Loss :  1.8056981563568115 3.1864876747131348 3.1864876747131348
Loss :  1.8016659021377563 3.2110936641693115 3.2110936641693115
Loss :  1.8020780086517334 3.110295295715332 3.110295295715332
Loss :  1.7966004610061646 2.712829113006592 2.712829113006592
  batch 20 loss: 1.7966004610061646, 2.712829113006592, 2.712829113006592
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8050576448440552 2.7692553997039795 2.7692553997039795
Loss :  1.8038290739059448 2.794064521789551 2.794064521789551
Loss :  1.7971168756484985 3.538684368133545 3.538684368133545
Loss :  1.8029168844223022 3.250525712966919 3.250525712966919
Loss :  1.8027933835983276 3.1731278896331787 3.1731278896331787
Loss :  1.802836537361145 2.9757118225097656 2.9757118225097656
Loss :  1.8075532913208008 3.2741777896881104 3.2741777896881104
Loss :  1.7992417812347412 3.131861925125122 3.131861925125122
Loss :  1.8119393587112427 2.840827703475952 2.840827703475952
Loss :  1.7986830472946167 3.0590062141418457 3.0590062141418457
Loss :  1.8062782287597656 3.269991397857666 3.269991397857666
Loss :  1.7956300973892212 3.5845401287078857 3.5845401287078857
Loss :  1.8052324056625366 3.367506504058838 3.367506504058838
Loss :  1.804254174232483 3.2346253395080566 3.2346253395080566
Loss :  1.8043303489685059 3.224417209625244 3.224417209625244
Loss :  1.8067171573638916 4.007002353668213 4.007002353668213
Loss :  1.8034398555755615 3.66648268699646 3.66648268699646
Loss :  1.7986457347869873 3.9746553897857666 3.9746553897857666
Loss :  1.801218032836914 3.7146382331848145 3.7146382331848145
Loss :  1.798028826713562 3.6250643730163574 3.6250643730163574
  batch 40 loss: 1.798028826713562, 3.6250643730163574, 3.6250643730163574
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8030505180358887 3.588428497314453 3.588428497314453
Loss :  1.8023102283477783 3.691016674041748 3.691016674041748
Loss :  1.801863193511963 3.274808406829834 3.274808406829834
Loss :  1.799294352531433 3.2240383625030518 3.2240383625030518
Loss :  1.8044006824493408 3.0724666118621826 3.0724666118621826
Loss :  1.8024568557739258 3.5396947860717773 3.5396947860717773
Loss :  1.7993525266647339 3.29630970954895 3.29630970954895
Loss :  1.800923228263855 3.2663023471832275 3.2663023471832275
Loss :  1.7965582609176636 3.1637766361236572 3.1637766361236572
Loss :  1.8002488613128662 3.2628135681152344 3.2628135681152344
Loss :  1.7948665618896484 3.559654474258423 3.559654474258423
Loss :  1.8020342588424683 3.471359968185425 3.471359968185425
Loss :  1.8020482063293457 3.5162103176116943 3.5162103176116943
Loss :  1.8051941394805908 3.2512407302856445 3.2512407302856445
Loss :  1.7996658086776733 3.831611156463623 3.831611156463623
Loss :  1.802956223487854 3.977741241455078 3.977741241455078
Loss :  1.8067744970321655 3.782043695449829 3.782043695449829
Loss :  1.806105375289917 3.9783942699432373 3.9783942699432373
Loss :  1.80959951877594 4.020663738250732 4.020663738250732
Loss :  1.8002641201019287 3.71639084815979 3.71639084815979
  batch 60 loss: 1.8002641201019287, 3.71639084815979, 3.71639084815979
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8067021369934082 3.4489240646362305 3.4489240646362305
Loss :  1.7997982501983643 3.8560163974761963 3.8560163974761963
Loss :  1.8047916889190674 2.9859306812286377 2.9859306812286377
Loss :  1.800892949104309 3.1878645420074463 3.1878645420074463
Loss :  1.8053370714187622 3.0180578231811523 3.0180578231811523
Loss :  1.7153105735778809 4.420722961425781 4.420722961425781
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7169121503829956 4.4429216384887695 4.4429216384887695
Loss :  1.7124930620193481 4.287626266479492 4.287626266479492
Loss :  1.7411606311798096 4.182610034942627 4.182610034942627
Total LOSS train 3.228393818781926 valid 4.3334702253341675
CE LOSS train 1.802311002291166 valid 0.4352901577949524
Contrastive LOSS train 3.228393818781926 valid 1.0456525087356567
EPOCH 254:
Loss :  1.8015432357788086 2.7820138931274414 2.7820138931274414
Loss :  1.7993440628051758 3.053382158279419 3.053382158279419
Loss :  1.8019251823425293 3.4525701999664307 3.4525701999664307
Loss :  1.8024327754974365 2.9379706382751465 2.9379706382751465
Loss :  1.8009260892868042 3.300328493118286 3.300328493118286
Loss :  1.8051635026931763 3.164792776107788 3.164792776107788
Loss :  1.8001179695129395 3.526376247406006 3.526376247406006
Loss :  1.8003391027450562 3.3486859798431396 3.3486859798431396
Loss :  1.7965067625045776 3.4707729816436768 3.4707729816436768
Loss :  1.7972195148468018 2.972506046295166 2.972506046295166
Loss :  1.8075484037399292 3.552124261856079 3.552124261856079
Loss :  1.801973581314087 3.767707109451294 3.767707109451294
Loss :  1.8070012331008911 3.959130048751831 3.959130048751831
Loss :  1.8064955472946167 3.1017346382141113 3.1017346382141113
Loss :  1.7938892841339111 3.2536561489105225 3.2536561489105225
Loss :  1.8084522485733032 3.155399799346924 3.155399799346924
Loss :  1.8057113885879517 3.024493455886841 3.024493455886841
Loss :  1.8016942739486694 2.8052220344543457 2.8052220344543457
Loss :  1.8019202947616577 2.76261305809021 2.76261305809021
Loss :  1.796575665473938 2.9454963207244873 2.9454963207244873
  batch 20 loss: 1.796575665473938, 2.9454963207244873, 2.9454963207244873
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8049304485321045 2.636138916015625 2.636138916015625
Loss :  1.8037159442901611 2.9002206325531006 2.9002206325531006
Loss :  1.7970209121704102 2.6254560947418213 2.6254560947418213
Loss :  1.802834153175354 2.825298309326172 2.825298309326172
Loss :  1.802615761756897 3.0438385009765625 3.0438385009765625
Loss :  1.802901029586792 2.700228691101074 2.700228691101074
Loss :  1.8076304197311401 3.0010175704956055 3.0010175704956055
Loss :  1.7993241548538208 2.9010329246520996 2.9010329246520996
Loss :  1.8120654821395874 2.722614288330078 2.722614288330078
Loss :  1.7987126111984253 2.711151599884033 2.711151599884033
Loss :  1.8062447309494019 2.7151715755462646 2.7151715755462646
Loss :  1.795689344406128 2.5584194660186768 2.5584194660186768
Loss :  1.8053042888641357 2.510221004486084 2.510221004486084
Loss :  1.8043291568756104 3.025355100631714 3.025355100631714
Loss :  1.8043183088302612 2.9969732761383057 2.9969732761383057
Loss :  1.8068069219589233 3.1780359745025635 3.1780359745025635
Loss :  1.8034440279006958 3.1757090091705322 3.1757090091705322
Loss :  1.7988001108169556 3.306919813156128 3.306919813156128
Loss :  1.801216721534729 3.0899109840393066 3.0899109840393066
Loss :  1.798080563545227 2.600158452987671 2.600158452987671
  batch 40 loss: 1.798080563545227, 2.600158452987671, 2.600158452987671
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8030924797058105 3.210873603820801 3.210873603820801
Loss :  1.8023958206176758 3.0067999362945557 3.0067999362945557
Loss :  1.801821231842041 2.8409790992736816 2.8409790992736816
Loss :  1.7993727922439575 2.6638097763061523 2.6638097763061523
Loss :  1.8044322729110718 2.9139583110809326 2.9139583110809326
Loss :  1.8025435209274292 3.108384370803833 3.108384370803833
Loss :  1.7994444370269775 3.382855176925659 3.382855176925659
Loss :  1.8009692430496216 3.2663655281066895 3.2663655281066895
Loss :  1.7966809272766113 3.4140613079071045 3.4140613079071045
Loss :  1.8003134727478027 3.499600648880005 3.499600648880005
Loss :  1.7949070930480957 3.581709384918213 3.581709384918213
Loss :  1.8020901679992676 3.61946177482605 3.61946177482605
Loss :  1.8021349906921387 3.6119489669799805 3.6119489669799805
Loss :  1.805371880531311 3.006100654602051 3.006100654602051
Loss :  1.7997037172317505 3.3005623817443848 3.3005623817443848
Loss :  1.8030829429626465 2.4608659744262695 2.4608659744262695
Loss :  1.8068792819976807 3.488431692123413 3.488431692123413
Loss :  1.8061294555664062 2.7524850368499756 2.7524850368499756
Loss :  1.809691309928894 2.773940086364746 2.773940086364746
Loss :  1.8004517555236816 2.4896903038024902 2.4896903038024902
  batch 60 loss: 1.8004517555236816, 2.4896903038024902, 2.4896903038024902
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8067868947982788 2.806880474090576 2.806880474090576
Loss :  1.7999472618103027 2.6193974018096924 2.6193974018096924
Loss :  1.804848074913025 2.4273338317871094 2.4273338317871094
Loss :  1.8010058403015137 2.876446485519409 2.876446485519409
Loss :  1.8054780960083008 2.220233678817749 2.220233678817749
Loss :  1.715110182762146 4.408167839050293 4.408167839050293
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7166976928710938 4.338377475738525 4.338377475738525
Loss :  1.7122782468795776 4.371552467346191 4.371552467346191
Loss :  1.7410920858383179 4.30254602432251 4.30254602432251
Total LOSS train 3.029292682500986 valid 4.35516095161438
CE LOSS train 1.802343694980328 valid 0.43527302145957947
Contrastive LOSS train 3.029292682500986 valid 1.0756365060806274
EPOCH 255:
Loss :  1.8018105030059814 2.427018404006958 2.427018404006958
Loss :  1.7994012832641602 2.6458349227905273 2.6458349227905273
Loss :  1.8021703958511353 2.593491315841675 2.593491315841675
Loss :  1.802738904953003 2.6711013317108154 2.6711013317108154
Loss :  1.8011239767074585 2.489248752593994 2.489248752593994
Loss :  1.805346131324768 2.254228115081787 2.254228115081787
Loss :  1.8002688884735107 2.9487364292144775 2.9487364292144775
Loss :  1.800429105758667 2.991459369659424 2.991459369659424
Loss :  1.796769142150879 2.8831238746643066 2.8831238746643066
Loss :  1.7974315881729126 2.5743563175201416 2.5743563175201416
Loss :  1.8076207637786865 2.7152998447418213 2.7152998447418213
Loss :  1.802112102508545 2.876753807067871 2.876753807067871
Loss :  1.8069556951522827 3.0964303016662598 3.0964303016662598
Loss :  1.8065158128738403 2.7162389755249023 2.7162389755249023
Loss :  1.7941704988479614 3.260519504547119 3.260519504547119
Loss :  1.8085182905197144 2.897488832473755 2.897488832473755
Loss :  1.8057427406311035 2.76162052154541 2.76162052154541
Loss :  1.8017648458480835 3.3113255500793457 3.3113255500793457
Loss :  1.802126169204712 2.720761299133301 2.720761299133301
Loss :  1.7966285943984985 3.0744171142578125 3.0744171142578125
  batch 20 loss: 1.7966285943984985, 3.0744171142578125, 3.0744171142578125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.805040717124939 2.932028293609619 2.932028293609619
Loss :  1.8038443326950073 2.7474098205566406 2.7474098205566406
Loss :  1.7971642017364502 2.9322705268859863 2.9322705268859863
Loss :  1.8029534816741943 3.3653433322906494 3.3653433322906494
Loss :  1.802927017211914 3.5718328952789307 3.5718328952789307
Loss :  1.8029578924179077 3.5692360401153564 3.5692360401153564
Loss :  1.8075833320617676 3.4531755447387695 3.4531755447387695
Loss :  1.7992832660675049 3.2345728874206543 3.2345728874206543
Loss :  1.8118724822998047 2.931480646133423 2.931480646133423
Loss :  1.7988392114639282 3.3859057426452637 3.3859057426452637
Loss :  1.8063184022903442 3.2162060737609863 3.2162060737609863
Loss :  1.7956621646881104 3.246366262435913 3.246366262435913
Loss :  1.8052136898040771 3.3593032360076904 3.3593032360076904
Loss :  1.8043551445007324 3.1976318359375 3.1976318359375
Loss :  1.804405689239502 3.1454224586486816 3.1454224586486816
Loss :  1.8067233562469482 3.2787973880767822 3.2787973880767822
Loss :  1.8034268617630005 3.9909169673919678 3.9909169673919678
Loss :  1.7986502647399902 3.17567777633667 3.17567777633667
Loss :  1.8012614250183105 3.5821797847747803 3.5821797847747803
Loss :  1.798082709312439 3.2447471618652344 3.2447471618652344
  batch 40 loss: 1.798082709312439, 3.2447471618652344, 3.2447471618652344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8030904531478882 3.421665668487549 3.421665668487549
Loss :  1.8023784160614014 3.152249813079834 3.152249813079834
Loss :  1.8019874095916748 3.1393258571624756 3.1393258571624756
Loss :  1.799429178237915 2.6005640029907227 2.6005640029907227
Loss :  1.8045063018798828 2.7470598220825195 2.7470598220825195
Loss :  1.802438497543335 2.5001165866851807 2.5001165866851807
Loss :  1.7994695901870728 2.888089656829834 2.888089656829834
Loss :  1.8010894060134888 2.8345463275909424 2.8345463275909424
Loss :  1.7965795993804932 3.224418878555298 3.224418878555298
Loss :  1.8004509210586548 3.086845874786377 3.086845874786377
Loss :  1.7950727939605713 2.8380143642425537 2.8380143642425537
Loss :  1.8021135330200195 3.1323018074035645 3.1323018074035645
Loss :  1.8021245002746582 2.823678731918335 2.823678731918335
Loss :  1.8055387735366821 3.0488576889038086 3.0488576889038086
Loss :  1.799866795539856 3.42771577835083 3.42771577835083
Loss :  1.8031247854232788 3.594594717025757 3.594594717025757
Loss :  1.8068557977676392 3.404799699783325 3.404799699783325
Loss :  1.806107759475708 3.213397264480591 3.213397264480591
Loss :  1.8096845149993896 3.5074915885925293 3.5074915885925293
Loss :  1.800412654876709 3.320582151412964 3.320582151412964
  batch 60 loss: 1.800412654876709, 3.320582151412964, 3.320582151412964
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8067132234573364 3.2879059314727783 3.2879059314727783
Loss :  1.7999625205993652 3.268855571746826 3.268855571746826
Loss :  1.804751992225647 3.096652030944824 3.096652030944824
Loss :  1.8009517192840576 3.373530626296997 3.373530626296997
Loss :  1.805476188659668 2.960300922393799 2.960300922393799
Loss :  1.7147109508514404 4.3929924964904785 4.3929924964904785
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7162671089172363 4.387115478515625 4.387115478515625
Loss :  1.71193265914917 4.386727809906006 4.386727809906006
Loss :  1.7407503128051758 4.117658615112305 4.117658615112305
Total LOSS train 3.0671310864962065 valid 4.3211236000061035
CE LOSS train 1.8024059754151565 valid 0.43518757820129395
Contrastive LOSS train 3.0671310864962065 valid 1.0294146537780762
EPOCH 256:
Loss :  1.8017488718032837 3.276097059249878 3.276097059249878
Loss :  1.7992088794708252 3.247908353805542 3.247908353805542
Loss :  1.8020696640014648 2.5278971195220947 2.5278971195220947
Loss :  1.8026751279830933 3.014505624771118 3.014505624771118
Loss :  1.8010265827178955 2.696730375289917 2.696730375289917
Loss :  1.8052613735198975 2.8268067836761475 2.8268067836761475
Loss :  1.8000901937484741 3.178921699523926 3.178921699523926
Loss :  1.8003053665161133 3.1495447158813477 3.1495447158813477
Loss :  1.7966774702072144 3.5160715579986572 3.5160715579986572
Loss :  1.7972744703292847 2.993028402328491 2.993028402328491
Loss :  1.8075491189956665 3.428513288497925 3.428513288497925
Loss :  1.8021140098571777 3.60988450050354 3.60988450050354
Loss :  1.8068839311599731 3.584772825241089 3.584772825241089
Loss :  1.8064285516738892 3.1764068603515625 3.1764068603515625
Loss :  1.7939882278442383 3.349109649658203 3.349109649658203
Loss :  1.8084110021591187 3.49763560295105 3.49763560295105
Loss :  1.805686354637146 3.0564162731170654 3.0564162731170654
Loss :  1.8016778230667114 3.398221492767334 3.398221492767334
Loss :  1.8020578622817993 3.0948116779327393 3.0948116779327393
Loss :  1.796525478363037 3.4835474491119385 3.4835474491119385
  batch 20 loss: 1.796525478363037, 3.4835474491119385, 3.4835474491119385
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8049378395080566 3.5522878170013428 3.5522878170013428
Loss :  1.8037751913070679 3.298445701599121 3.298445701599121
Loss :  1.7970863580703735 3.285499095916748 3.285499095916748
Loss :  1.8029086589813232 3.1564900875091553 3.1564900875091553
Loss :  1.802829384803772 3.490537166595459 3.490537166595459
Loss :  1.80294668674469 2.6738474369049072 2.6738474369049072
Loss :  1.8075644969940186 3.124624490737915 3.124624490737915
Loss :  1.799280047416687 2.7113912105560303 2.7113912105560303
Loss :  1.8119006156921387 2.7660582065582275 2.7660582065582275
Loss :  1.7988172769546509 3.152247905731201 3.152247905731201
Loss :  1.8062723875045776 3.058217763900757 3.058217763900757
Loss :  1.7956571578979492 3.012352228164673 3.012352228164673
Loss :  1.805245041847229 3.0313732624053955 3.0313732624053955
Loss :  1.8043456077575684 3.0895466804504395 3.0895466804504395
Loss :  1.8043709993362427 3.3975729942321777 3.3975729942321777
Loss :  1.8067649602890015 3.176590919494629 3.176590919494629
Loss :  1.8034203052520752 3.088188409805298 3.088188409805298
Loss :  1.798734426498413 2.6954925060272217 2.6954925060272217
Loss :  1.8011996746063232 3.0407068729400635 3.0407068729400635
Loss :  1.798060655593872 3.3280906677246094 3.3280906677246094
  batch 40 loss: 1.798060655593872, 3.3280906677246094, 3.3280906677246094
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8030762672424316 3.272765636444092 3.272765636444092
Loss :  1.8024253845214844 2.971313953399658 2.971313953399658
Loss :  1.8018854856491089 3.4164576530456543 3.4164576530456543
Loss :  1.7994415760040283 3.497413158416748 3.497413158416748
Loss :  1.8044744729995728 3.584162712097168 3.584162712097168
Loss :  1.8024970293045044 3.827416181564331 3.827416181564331
Loss :  1.7994945049285889 3.7165212631225586 3.7165212631225586
Loss :  1.8010209798812866 3.622805595397949 3.622805595397949
Loss :  1.7966880798339844 3.0904324054718018 3.0904324054718018
Loss :  1.800417423248291 3.1658034324645996 3.1658034324645996
Loss :  1.7949533462524414 2.9426093101501465 2.9426093101501465
Loss :  1.8021187782287598 2.6761066913604736 2.6761066913604736
Loss :  1.8021605014801025 2.458916425704956 2.458916425704956
Loss :  1.8055484294891357 2.4697630405426025 2.4697630405426025
Loss :  1.7997466325759888 2.7483983039855957 2.7483983039855957
Loss :  1.8032000064849854 2.6915807723999023 2.6915807723999023
Loss :  1.8069348335266113 2.624210834503174 2.624210834503174
Loss :  1.8060435056686401 2.594456434249878 2.594456434249878
Loss :  1.8097254037857056 2.7785258293151855 2.7785258293151855
Loss :  1.800484538078308 2.5342462062835693 2.5342462062835693
  batch 60 loss: 1.800484538078308, 2.5342462062835693, 2.5342462062835693
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8068050146102905 3.2085046768188477 3.2085046768188477
Loss :  1.7999556064605713 2.6347241401672363 2.6347241401672363
Loss :  1.8048365116119385 3.644361734390259 3.644361734390259
Loss :  1.8009928464889526 3.009831666946411 3.009831666946411
Loss :  1.8054884672164917 2.937600612640381 2.937600612640381
Loss :  1.7145057916641235 4.380511283874512 4.380511283874512
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7160276174545288 4.397434711456299 4.397434711456299
Loss :  1.7117143869400024 4.404463768005371 4.404463768005371
Loss :  1.7406222820281982 4.251262664794922 4.251262664794922
Total LOSS train 3.1131587908818172 valid 4.358418107032776
CE LOSS train 1.8023722116763776 valid 0.43515557050704956
Contrastive LOSS train 3.1131587908818172 valid 1.0628156661987305
EPOCH 257:
Loss :  1.8018156290054321 3.0004630088806152 3.0004630088806152
Loss :  1.799353003501892 3.545243978500366 3.545243978500366
Loss :  1.8021272420883179 2.835273027420044 2.835273027420044
Loss :  1.8027381896972656 3.4567348957061768 3.4567348957061768
Loss :  1.8011053800582886 2.981330633163452 2.981330633163452
Loss :  1.805330514907837 3.120518207550049 3.120518207550049
Loss :  1.8002214431762695 3.2346582412719727 3.2346582412719727
Loss :  1.8003780841827393 3.233048677444458 3.233048677444458
Loss :  1.796721339225769 3.631962537765503 3.631962537765503
Loss :  1.7973477840423584 3.1507527828216553 3.1507527828216553
Loss :  1.80759859085083 3.0892837047576904 3.0892837047576904
Loss :  1.8021608591079712 2.941520929336548 2.941520929336548
Loss :  1.8069132566452026 3.102879047393799 3.102879047393799
Loss :  1.806414008140564 3.1204514503479004 3.1204514503479004
Loss :  1.7940346002578735 3.452599287033081 3.452599287033081
Loss :  1.8084442615509033 3.3693761825561523 3.3693761825561523
Loss :  1.8056950569152832 3.086268424987793 3.086268424987793
Loss :  1.8017184734344482 2.822899580001831 2.822899580001831
Loss :  1.8020623922348022 3.3490936756134033 3.3490936756134033
Loss :  1.7965317964553833 3.174022912979126 3.174022912979126
  batch 20 loss: 1.7965317964553833, 3.174022912979126, 3.174022912979126
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.804904580116272 2.7073705196380615 2.7073705196380615
Loss :  1.803766131401062 2.8089025020599365 2.8089025020599365
Loss :  1.7970592975616455 3.12640118598938 3.12640118598938
Loss :  1.8029158115386963 3.2639272212982178 3.2639272212982178
Loss :  1.8028292655944824 3.3362953662872314 3.3362953662872314
Loss :  1.8029338121414185 3.0277671813964844 3.0277671813964844
Loss :  1.8075430393218994 2.9929919242858887 2.9929919242858887
Loss :  1.7992594242095947 2.940251350402832 2.940251350402832
Loss :  1.811871886253357 3.3991663455963135 3.3991663455963135
Loss :  1.7988172769546509 3.055016040802002 3.055016040802002
Loss :  1.8062816858291626 3.2590560913085938 3.2590560913085938
Loss :  1.79562509059906 3.638059139251709 3.638059139251709
Loss :  1.8052119016647339 3.197138547897339 3.197138547897339
Loss :  1.8043686151504517 3.644486427307129 3.644486427307129
Loss :  1.8043665885925293 3.5097172260284424 3.5097172260284424
Loss :  1.8067665100097656 3.2490696907043457 3.2490696907043457
Loss :  1.8033767938613892 3.7095580101013184 3.7095580101013184
Loss :  1.798658013343811 3.5024948120117188 3.5024948120117188
Loss :  1.8011635541915894 3.360349178314209 3.360349178314209
Loss :  1.7980185747146606 3.187431573867798 3.187431573867798
  batch 40 loss: 1.7980185747146606, 3.187431573867798, 3.187431573867798
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8030306100845337 3.6344664096832275 3.6344664096832275
Loss :  1.8024488687515259 3.7376632690429688 3.7376632690429688
Loss :  1.801888346672058 3.0289690494537354 3.0289690494537354
Loss :  1.799482822418213 2.8493075370788574 2.8493075370788574
Loss :  1.8044358491897583 3.3647942543029785 3.3647942543029785
Loss :  1.8024383783340454 3.535266876220703 3.535266876220703
Loss :  1.7994464635849 4.1365275382995605 4.1365275382995605
Loss :  1.8009870052337646 3.3670129776000977 3.3670129776000977
Loss :  1.7966214418411255 2.8712759017944336 2.8712759017944336
Loss :  1.8003889322280884 3.6802642345428467 3.6802642345428467
Loss :  1.7948601245880127 2.9723191261291504 2.9723191261291504
Loss :  1.8020302057266235 2.915473222732544 2.915473222732544
Loss :  1.8021084070205688 2.8716354370117188 2.8716354370117188
Loss :  1.8055157661437988 2.5622146129608154 2.5622146129608154
Loss :  1.7996500730514526 2.8373968601226807 2.8373968601226807
Loss :  1.8031302690505981 2.570490837097168 2.570490837097168
Loss :  1.806897521018982 2.7717435359954834 2.7717435359954834
Loss :  1.8059722185134888 2.5853888988494873 2.5853888988494873
Loss :  1.8096693754196167 2.6957480907440186 2.6957480907440186
Loss :  1.8003672361373901 2.7119154930114746 2.7119154930114746
  batch 60 loss: 1.8003672361373901, 2.7119154930114746, 2.7119154930114746
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.806774616241455 2.9046406745910645 2.9046406745910645
Loss :  1.7998816967010498 2.9076998233795166 2.9076998233795166
Loss :  1.8047800064086914 2.814495801925659 2.814495801925659
Loss :  1.8009291887283325 3.7306549549102783 3.7306549549102783
Loss :  1.8054437637329102 3.3295445442199707 3.3295445442199707
Loss :  1.7146698236465454 4.438355445861816 4.438355445861816
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7161791324615479 4.3878679275512695 4.3878679275512695
Loss :  1.7118672132492065 4.370509624481201 4.370509624481201
Loss :  1.7408214807510376 4.3481268882751465 4.3481268882751465
Total LOSS train 3.1692421766427845 valid 4.386214971542358
CE LOSS train 1.8023635222361638 valid 0.4352053701877594
Contrastive LOSS train 3.1692421766427845 valid 1.0870317220687866
EPOCH 258:
Loss :  1.801709532737732 3.1793177127838135 3.1793177127838135
Loss :  1.799298644065857 3.089789628982544 3.089789628982544
Loss :  1.802060604095459 3.5938713550567627 3.5938713550567627
Loss :  1.8026292324066162 3.4034767150878906 3.4034767150878906
Loss :  1.8010181188583374 3.3537306785583496 3.3537306785583496
Loss :  1.8052541017532349 3.002027750015259 3.002027750015259
Loss :  1.800149917602539 3.632472515106201 3.632472515106201
Loss :  1.8003250360488892 2.614302635192871 2.614302635192871
Loss :  1.7966511249542236 2.833977222442627 2.833977222442627
Loss :  1.7972960472106934 2.5676119327545166 2.5676119327545166
Loss :  1.8075544834136963 3.1343321800231934 3.1343321800231934
Loss :  1.8021456003189087 2.8853325843811035 2.8853325843811035
Loss :  1.8068606853485107 2.9300789833068848 2.9300789833068848
Loss :  1.8063466548919678 3.068782329559326 3.068782329559326
Loss :  1.7940146923065186 3.122420072555542 3.122420072555542
Loss :  1.8083845376968384 3.2757508754730225 3.2757508754730225
Loss :  1.8056318759918213 3.2399563789367676 3.2399563789367676
Loss :  1.8017250299453735 2.9608006477355957 2.9608006477355957
Loss :  1.8020280599594116 3.160761833190918 3.160761833190918
Loss :  1.7965283393859863 3.1336119174957275 3.1336119174957275
  batch 20 loss: 1.7965283393859863, 3.1336119174957275, 3.1336119174957275
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8049273490905762 3.214291572570801 3.214291572570801
Loss :  1.8037469387054443 3.1362762451171875 3.1362762451171875
Loss :  1.7971292734146118 2.8192007541656494 2.8192007541656494
Loss :  1.802912950515747 2.837763547897339 2.837763547897339
Loss :  1.8028680086135864 3.4857890605926514 3.4857890605926514
Loss :  1.8029694557189941 3.749525308609009 3.749525308609009
Loss :  1.8075237274169922 3.066250801086426 3.066250801086426
Loss :  1.799259901046753 3.0210211277008057 3.0210211277008057
Loss :  1.8117773532867432 2.8979337215423584 2.8979337215423584
Loss :  1.7987630367279053 3.07279896736145 3.07279896736145
Loss :  1.8062019348144531 2.7515082359313965 2.7515082359313965
Loss :  1.7955749034881592 2.8244524002075195 2.8244524002075195
Loss :  1.8051422834396362 2.5528225898742676 2.5528225898742676
Loss :  1.8042707443237305 3.4211630821228027 3.4211630821228027
Loss :  1.8043408393859863 3.393944025039673 3.393944025039673
Loss :  1.8066335916519165 3.90305757522583 3.90305757522583
Loss :  1.8033167123794556 3.7767088413238525 3.7767088413238525
Loss :  1.7985836267471313 2.9756381511688232 2.9756381511688232
Loss :  1.8011219501495361 2.7923901081085205 2.7923901081085205
Loss :  1.7979519367218018 2.3880093097686768 2.3880093097686768
  batch 40 loss: 1.7979519367218018, 2.3880093097686768, 2.3880093097686768
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8029568195343018 3.418025255203247 3.418025255203247
Loss :  1.80229651927948 2.969205141067505 2.969205141067505
Loss :  1.801855206489563 2.8991408348083496 2.8991408348083496
Loss :  1.79937744140625 2.266294002532959 2.266294002532959
Loss :  1.8043898344039917 2.476267099380493 2.476267099380493
Loss :  1.802294135093689 2.83876371383667 2.83876371383667
Loss :  1.7994310855865479 2.87732195854187 2.87732195854187
Loss :  1.8009464740753174 3.318272590637207 3.318272590637207
Loss :  1.796518325805664 3.0308804512023926 3.0308804512023926
Loss :  1.8003684282302856 3.263570547103882 3.263570547103882
Loss :  1.7948498725891113 3.2514281272888184 3.2514281272888184
Loss :  1.8019860982894897 3.0835516452789307 3.0835516452789307
Loss :  1.8020182847976685 3.253035306930542 3.253035306930542
Loss :  1.805406093597412 3.3500866889953613 3.3500866889953613
Loss :  1.7996629476547241 3.1828298568725586 3.1828298568725586
Loss :  1.803099513053894 2.5235650539398193 2.5235650539398193
Loss :  1.8067281246185303 3.5724599361419678 3.5724599361419678
Loss :  1.8058559894561768 3.1211955547332764 3.1211955547332764
Loss :  1.8095252513885498 2.8573532104492188 2.8573532104492188
Loss :  1.8003756999969482 3.100856065750122 3.100856065750122
  batch 60 loss: 1.8003756999969482, 3.100856065750122, 3.100856065750122
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8065521717071533 2.9639089107513428 2.9639089107513428
Loss :  1.7998117208480835 2.6478240489959717 2.6478240489959717
Loss :  1.8046561479568481 2.9938597679138184 2.9938597679138184
Loss :  1.800790548324585 2.8895809650421143 2.8895809650421143
Loss :  1.8052868843078613 2.7891759872436523 2.7891759872436523
Loss :  1.715237021446228 4.4284162521362305 4.4284162521362305
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7167338132858276 4.420107841491699 4.420107841491699
Loss :  1.7123966217041016 4.287133693695068 4.287133693695068
Loss :  1.7412019968032837 4.354197025299072 4.354197025299072
Total LOSS train 3.0646369860722467 valid 4.372463703155518
CE LOSS train 1.8023025916172908 valid 0.4353004992008209
Contrastive LOSS train 3.0646369860722467 valid 1.088549256324768
EPOCH 259:
Loss :  1.8016455173492432 2.9123730659484863 2.9123730659484863
Loss :  1.7991102933883667 3.1453399658203125 3.1453399658203125
Loss :  1.8019543886184692 2.4865219593048096 2.4865219593048096
Loss :  1.802600383758545 2.7915492057800293 2.7915492057800293
Loss :  1.8009527921676636 3.42693829536438 3.42693829536438
Loss :  1.8051749467849731 2.6610000133514404 2.6610000133514404
Loss :  1.80003821849823 2.8335890769958496 2.8335890769958496
Loss :  1.8001585006713867 2.472456216812134 2.472456216812134
Loss :  1.7966972589492798 2.3282997608184814 2.3282997608184814
Loss :  1.7971913814544678 2.5602164268493652 2.5602164268493652
Loss :  1.8074028491973877 2.6204674243927 2.6204674243927
Loss :  1.8021081686019897 2.7921974658966064 2.7921974658966064
Loss :  1.8067063093185425 2.790557384490967 2.790557384490967
Loss :  1.806258201599121 3.0401649475097656 3.0401649475097656
Loss :  1.7940845489501953 3.3045854568481445 3.3045854568481445
Loss :  1.8081870079040527 3.118481159210205 3.118481159210205
Loss :  1.8055113554000854 2.806488037109375 2.806488037109375
Loss :  1.8016443252563477 3.2003583908081055 3.2003583908081055
Loss :  1.8020336627960205 3.2632648944854736 3.2632648944854736
Loss :  1.7964187860488892 3.234219551086426 3.234219551086426
  batch 20 loss: 1.7964187860488892, 3.234219551086426, 3.234219551086426
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8049393892288208 2.8416662216186523 2.8416662216186523
Loss :  1.8037409782409668 2.842247247695923 2.842247247695923
Loss :  1.797141194343567 3.0134339332580566 3.0134339332580566
Loss :  1.8028908967971802 2.9915249347686768 2.9915249347686768
Loss :  1.802893042564392 3.202317476272583 3.202317476272583
Loss :  1.802897572517395 2.722743272781372 2.722743272781372
Loss :  1.8073972463607788 3.246212959289551 3.246212959289551
Loss :  1.799142837524414 2.940579414367676 2.940579414367676
Loss :  1.8116265535354614 2.963731288909912 2.963731288909912
Loss :  1.7986830472946167 3.0007925033569336 3.0007925033569336
Loss :  1.806208848953247 3.1266252994537354 3.1266252994537354
Loss :  1.7955056428909302 3.5440726280212402 3.5440726280212402
Loss :  1.8050473928451538 3.0859315395355225 3.0859315395355225
Loss :  1.8041962385177612 2.8151087760925293 2.8151087760925293
Loss :  1.8043551445007324 3.2051174640655518 3.2051174640655518
Loss :  1.8065825700759888 3.9483845233917236 3.9483845233917236
Loss :  1.8033028841018677 3.7332770824432373 3.7332770824432373
Loss :  1.7984788417816162 4.020918846130371 4.020918846130371
Loss :  1.8010804653167725 3.9693188667297363 3.9693188667297363
Loss :  1.7978899478912354 3.4268252849578857 3.4268252849578857
  batch 40 loss: 1.7978899478912354, 3.4268252849578857, 3.4268252849578857
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8029118776321411 3.5675015449523926 3.5675015449523926
Loss :  1.8022894859313965 3.3398449420928955 3.3398449420928955
Loss :  1.8018807172775269 3.537031888961792 3.537031888961792
Loss :  1.7993688583374023 3.3662502765655518 3.3662502765655518
Loss :  1.8043829202651978 2.8898074626922607 2.8898074626922607
Loss :  1.802255392074585 3.513957977294922 3.513957977294922
Loss :  1.7993632555007935 4.160849571228027 4.160849571228027
Loss :  1.8009204864501953 3.4861838817596436 3.4861838817596436
Loss :  1.796471118927002 3.8008391857147217 3.8008391857147217
Loss :  1.8003445863723755 3.7713491916656494 3.7713491916656494
Loss :  1.7947661876678467 3.1687116622924805 3.1687116622924805
Loss :  1.8019222021102905 3.418635368347168 3.418635368347168
Loss :  1.8019702434539795 3.6835219860076904 3.6835219860076904
Loss :  1.8053193092346191 3.4734408855438232 3.4734408855438232
Loss :  1.7995808124542236 2.9253499507904053 2.9253499507904053
Loss :  1.8029893636703491 2.619976282119751 2.619976282119751
Loss :  1.8067129850387573 3.375302791595459 3.375302791595459
Loss :  1.8058286905288696 2.795607328414917 2.795607328414917
Loss :  1.8094760179519653 3.2909533977508545 3.2909533977508545
Loss :  1.8002086877822876 3.1829023361206055 3.1829023361206055
  batch 60 loss: 1.8002086877822876, 3.1829023361206055, 3.1829023361206055
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8065801858901978 3.368237257003784 3.368237257003784
Loss :  1.799695611000061 3.2466695308685303 3.2466695308685303
Loss :  1.8046561479568481 3.367337465286255 3.367337465286255
Loss :  1.8007606267929077 3.359215021133423 3.359215021133423
Loss :  1.8052533864974976 3.7122561931610107 3.7122561931610107
Loss :  1.7141926288604736 4.408532619476318 4.408532619476318
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7156661748886108 4.4095635414123535 4.4095635414123535
Loss :  1.7114441394805908 4.35841178894043 4.35841178894043
Loss :  1.740229845046997 4.216575622558594 4.216575622558594
Total LOSS train 3.182332794482891 valid 4.348270893096924
CE LOSS train 1.8022429044430073 valid 0.43505746126174927
Contrastive LOSS train 3.182332794482891 valid 1.0541439056396484
EPOCH 260:
Loss :  1.801506519317627 3.224426507949829 3.224426507949829
Loss :  1.7991552352905273 3.6529853343963623 3.6529853343963623
Loss :  1.8018361330032349 3.495408296585083 3.495408296585083
Loss :  1.802424430847168 3.5089669227600098 3.5089669227600098
Loss :  1.8008557558059692 3.5357954502105713 3.5357954502105713
Loss :  1.8050788640975952 3.724578857421875 3.724578857421875
Loss :  1.8000200986862183 3.8229856491088867 3.8229856491088867
Loss :  1.8001651763916016 3.4898979663848877 3.4898979663848877
Loss :  1.796496033668518 3.7197394371032715 3.7197394371032715
Loss :  1.7970857620239258 2.7236433029174805 2.7236433029174805
Loss :  1.8074185848236084 3.3476791381835938 3.3476791381835938
Loss :  1.8020703792572021 3.2052762508392334 3.2052762508392334
Loss :  1.8067967891693115 3.1673476696014404 3.1673476696014404
Loss :  1.8062106370925903 3.13395357131958 3.13395357131958
Loss :  1.7937532663345337 3.2602686882019043 3.2602686882019043
Loss :  1.8081839084625244 3.3052890300750732 3.3052890300750732
Loss :  1.8055164813995361 3.399334669113159 3.399334669113159
Loss :  1.8016245365142822 3.506666660308838 3.506666660308838
Loss :  1.8018656969070435 3.3894011974334717 3.3894011974334717
Loss :  1.7963496446609497 3.2628557682037354 3.2628557682037354
  batch 20 loss: 1.7963496446609497, 3.2628557682037354, 3.2628557682037354
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8047823905944824 3.490138530731201 3.490138530731201
Loss :  1.8035986423492432 2.9567389488220215 2.9567389488220215
Loss :  1.7969619035720825 3.1047637462615967 3.1047637462615967
Loss :  1.802736759185791 3.3483083248138428 3.3483083248138428
Loss :  1.8025598526000977 3.601066827774048 3.601066827774048
Loss :  1.8028652667999268 3.3918607234954834 3.3918607234954834
Loss :  1.8074406385421753 3.1784913539886475 3.1784913539886475
Loss :  1.799148440361023 3.2185628414154053 3.2185628414154053
Loss :  1.811837077140808 3.524991750717163 3.524991750717163
Loss :  1.7985094785690308 4.156324863433838 4.156324863433838
Loss :  1.806114673614502 3.6084675788879395 3.6084675788879395
Loss :  1.7954490184783936 4.075763702392578 4.075763702392578
Loss :  1.8051042556762695 3.6765449047088623 3.6765449047088623
Loss :  1.804141879081726 3.5923309326171875 3.5923309326171875
Loss :  1.804183006286621 3.339498281478882 3.339498281478882
Loss :  1.8066604137420654 3.681551218032837 3.681551218032837
Loss :  1.8032206296920776 4.2241435050964355 4.2241435050964355
Loss :  1.79860520362854 3.8380966186523438 3.8380966186523438
Loss :  1.8009140491485596 3.7332403659820557 3.7332403659820557
Loss :  1.7977901697158813 3.2901344299316406 3.2901344299316406
  batch 40 loss: 1.7977901697158813, 3.2901344299316406, 3.2901344299316406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.802809238433838 3.5933353900909424 3.5933353900909424
Loss :  1.8023046255111694 3.6062211990356445 3.6062211990356445
Loss :  1.8015910387039185 3.5114147663116455 3.5114147663116455
Loss :  1.7993247509002686 3.453700542449951 3.453700542449951
Loss :  1.804236888885498 3.5797066688537598 3.5797066688537598
Loss :  1.8023173809051514 3.6438751220703125 3.6438751220703125
Loss :  1.7993674278259277 3.319242238998413 3.319242238998413
Loss :  1.8007322549819946 3.5686073303222656 3.5686073303222656
Loss :  1.7966262102127075 3.6209890842437744 3.6209890842437744
Loss :  1.8002228736877441 3.4258155822753906 3.4258155822753906
Loss :  1.794437289237976 2.8648219108581543 2.8648219108581543
Loss :  1.8018046617507935 3.606865882873535 3.606865882873535
Loss :  1.8019657135009766 2.7917306423187256 2.7917306423187256
Loss :  1.8051986694335938 2.9696648120880127 2.9696648120880127
Loss :  1.799241065979004 3.0884504318237305 3.0884504318237305
Loss :  1.8028937578201294 3.1926934719085693 3.1926934719085693
Loss :  1.8067628145217896 3.4594368934631348 3.4594368934631348
Loss :  1.8056977987289429 3.679797649383545 3.679797649383545
Loss :  1.8094277381896973 3.3532629013061523 3.3532629013061523
Loss :  1.7999860048294067 3.763868808746338 3.763868808746338
  batch 60 loss: 1.7999860048294067, 3.763868808746338, 3.763868808746338
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8066939115524292 3.2832796573638916 3.2832796573638916
Loss :  1.799513578414917 3.4631729125976562 3.4631729125976562
Loss :  1.8047031164169312 3.1364142894744873 3.1364142894744873
Loss :  1.8007129430770874 3.1961724758148193 3.1961724758148193
Loss :  1.8051533699035645 3.403075933456421 3.403075933456421
Loss :  1.7139019966125488 4.46525239944458 4.46525239944458
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7153486013412476 4.486098289489746 4.486098289489746
Loss :  1.711164951324463 4.341032028198242 4.341032028198242
Loss :  1.740071177482605 4.2830071449279785 4.2830071449279785
Total LOSS train 3.4382020986997164 valid 4.393847465515137
CE LOSS train 1.8021655816298265 valid 0.43501779437065125
Contrastive LOSS train 3.4382020986997164 valid 1.0707517862319946
EPOCH 261:
Loss :  1.8013246059417725 3.1867735385894775 3.1867735385894775
Loss :  1.7993404865264893 3.271491289138794 3.271491289138794
Loss :  1.8017245531082153 3.16764235496521 3.16764235496521
Loss :  1.8022090196609497 3.140174150466919 3.140174150466919
Loss :  1.8007935285568237 3.2437195777893066 3.2437195777893066
Loss :  1.805020809173584 3.390481472015381 3.390481472015381
Loss :  1.8001108169555664 3.5268356800079346 3.5268356800079346
Loss :  1.8002488613128662 3.2618701457977295 3.2618701457977295
Loss :  1.796340823173523 3.2091562747955322 3.2091562747955322
Loss :  1.797104835510254 3.3403561115264893 3.3403561115264893
Loss :  1.807528018951416 3.437849521636963 3.437849521636963
Loss :  1.8020812273025513 2.9085428714752197 2.9085428714752197
Loss :  1.8069456815719604 3.5692288875579834 3.5692288875579834
Loss :  1.8061765432357788 3.2384486198425293 3.2384486198425293
Loss :  1.7935197353363037 3.142221689224243 3.142221689224243
Loss :  1.808241367340088 3.365367889404297 3.365367889404297
Loss :  1.8055304288864136 3.1858110427856445 3.1858110427856445
Loss :  1.8017102479934692 3.316593885421753 3.316593885421753
Loss :  1.8017237186431885 3.631894826889038 3.631894826889038
Loss :  1.7963446378707886 3.3082380294799805 3.3082380294799805
  batch 20 loss: 1.7963446378707886, 3.3082380294799805, 3.3082380294799805
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.804689884185791 3.3631160259246826 3.3631160259246826
Loss :  1.8034712076187134 3.1151034832000732 3.1151034832000732
Loss :  1.7968685626983643 3.521223783493042 3.521223783493042
Loss :  1.8025844097137451 2.8442420959472656 2.8442420959472656
Loss :  1.8022680282592773 3.193620443344116 3.193620443344116
Loss :  1.80288565158844 2.7762463092803955 2.7762463092803955
Loss :  1.8074753284454346 2.9266483783721924 2.9266483783721924
Loss :  1.799175500869751 3.099674701690674 3.099674701690674
Loss :  1.8120089769363403 3.33384370803833 3.33384370803833
Loss :  1.7983368635177612 3.3938722610473633 3.3938722610473633
Loss :  1.8060357570648193 3.4198241233825684 3.4198241233825684
Loss :  1.7954069375991821 3.20918607711792 3.20918607711792
Loss :  1.8051488399505615 2.947235345840454 2.947235345840454
Loss :  1.8041026592254639 3.0425894260406494 3.0425894260406494
Loss :  1.804078221321106 2.7654943466186523 2.7654943466186523
Loss :  1.8067378997802734 3.208942174911499 3.208942174911499
Loss :  1.8031952381134033 3.012356758117676 3.012356758117676
Loss :  1.798749327659607 2.744387149810791 2.744387149810791
Loss :  1.800827145576477 3.0053818225860596 3.0053818225860596
Loss :  1.7977772951126099 3.1500351428985596 3.1500351428985596
  batch 40 loss: 1.7977772951126099, 3.1500351428985596, 3.1500351428985596
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8027737140655518 3.213308572769165 3.213308572769165
Loss :  1.8023689985275269 3.681701898574829 3.681701898574829
Loss :  1.8014636039733887 3.294203996658325 3.294203996658325
Loss :  1.7993347644805908 2.7260093688964844 2.7260093688964844
Loss :  1.8041801452636719 3.1781718730926514 3.1781718730926514
Loss :  1.802417278289795 3.4952425956726074 3.4952425956726074
Loss :  1.799420714378357 3.543092966079712 3.543092966079712
Loss :  1.8006539344787598 3.537588596343994 3.537588596343994
Loss :  1.7967842817306519 3.23329496383667 3.23329496383667
Loss :  1.8001991510391235 3.2063210010528564 3.2063210010528564
Loss :  1.7943005561828613 2.9796462059020996 2.9796462059020996
Loss :  1.8017808198928833 3.0256428718566895 3.0256428718566895
Loss :  1.8019859790802002 2.8975636959075928 2.8975636959075928
Loss :  1.8051538467407227 2.9447271823883057 2.9447271823883057
Loss :  1.7991091012954712 3.2970223426818848 3.2970223426818848
Loss :  1.8028755187988281 2.6349880695343018 2.6349880695343018
Loss :  1.8067905902862549 3.3039212226867676 3.3039212226867676
Loss :  1.8056718111038208 3.1543891429901123 3.1543891429901123
Loss :  1.8093966245651245 3.2269294261932373 3.2269294261932373
Loss :  1.7999155521392822 2.920701742172241 2.920701742172241
  batch 60 loss: 1.7999155521392822, 2.920701742172241, 2.920701742172241
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8067163228988647 3.7174599170684814 3.7174599170684814
Loss :  1.799472689628601 3.354417562484741 3.354417562484741
Loss :  1.80470609664917 3.2942302227020264 3.2942302227020264
Loss :  1.8007183074951172 3.0130627155303955 3.0130627155303955
Loss :  1.8051940202713013 3.5059289932250977 3.5059289932250977
Loss :  1.713619589805603 4.3900065422058105 4.3900065422058105
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7150646448135376 4.4203782081604 4.4203782081604
Loss :  1.7108920812606812 4.312018871307373 4.312018871307373
Loss :  1.7398499250411987 4.141589164733887 4.141589164733887
Total LOSS train 3.2045429009657638 valid 4.315998196601868
CE LOSS train 1.8021419708545392 valid 0.4349624812602997
Contrastive LOSS train 3.2045429009657638 valid 1.0353972911834717
EPOCH 262:
Loss :  1.801339864730835 2.96126127243042 2.96126127243042
Loss :  1.7993375062942505 3.042381763458252 3.042381763458252
Loss :  1.8017672300338745 3.0429883003234863 3.0429883003234863
Loss :  1.8022257089614868 3.19423246383667 3.19423246383667
Loss :  1.8007971048355103 3.106900930404663 3.106900930404663
Loss :  1.8050178289413452 2.88700795173645 2.88700795173645
Loss :  1.8001227378845215 3.153238534927368 3.153238534927368
Loss :  1.8002448081970215 2.9048755168914795 2.9048755168914795
Loss :  1.7963969707489014 2.938962459564209 2.938962459564209
Loss :  1.7971546649932861 3.2279767990112305 3.2279767990112305
Loss :  1.8075093030929565 2.63280987739563 2.63280987739563
Loss :  1.8021180629730225 3.12333607673645 3.12333607673645
Loss :  1.8068746328353882 2.8457441329956055 2.8457441329956055
Loss :  1.8061394691467285 2.9846208095550537 2.9846208095550537
Loss :  1.7935868501663208 3.375247001647949 3.375247001647949
Loss :  1.8082058429718018 3.0399093627929688 3.0399093627929688
Loss :  1.8054983615875244 3.049358606338501 3.049358606338501
Loss :  1.8017417192459106 2.973470449447632 2.973470449447632
Loss :  1.8017958402633667 2.849998712539673 2.849998712539673
Loss :  1.7963322401046753 2.8776676654815674 2.8776676654815674
  batch 20 loss: 1.7963322401046753, 2.8776676654815674, 2.8776676654815674
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8047537803649902 2.728895902633667 2.728895902633667
Loss :  1.8035218715667725 2.810567617416382 2.810567617416382
Loss :  1.7969635725021362 2.9213614463806152 2.9213614463806152
Loss :  1.802632212638855 3.2255361080169678 3.2255361080169678
Loss :  1.8024606704711914 3.3087103366851807 3.3087103366851807
Loss :  1.8029556274414062 2.8131604194641113 2.8131604194641113
Loss :  1.8074684143066406 3.0799973011016846 3.0799973011016846
Loss :  1.799198031425476 2.9397850036621094 2.9397850036621094
Loss :  1.8118728399276733 2.8749144077301025 2.8749144077301025
Loss :  1.7984758615493774 3.2490222454071045 3.2490222454071045
Loss :  1.806103229522705 3.276827812194824 3.276827812194824
Loss :  1.795419692993164 3.431579113006592 3.431579113006592
Loss :  1.8051167726516724 2.742417335510254 2.742417335510254
Loss :  1.8042250871658325 3.0965561866760254 3.0965561866760254
Loss :  1.8042045831680298 3.153512477874756 3.153512477874756
Loss :  1.8067148923873901 3.57773756980896 3.57773756980896
Loss :  1.803213119506836 3.5011565685272217 3.5011565685272217
Loss :  1.7986173629760742 3.317619562149048 3.317619562149048
Loss :  1.8008850812911987 3.3098340034484863 3.3098340034484863
Loss :  1.797792911529541 3.4662511348724365 3.4662511348724365
  batch 40 loss: 1.797792911529541, 3.4662511348724365, 3.4662511348724365
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8027782440185547 3.8649837970733643 3.8649837970733643
Loss :  1.8024011850357056 3.6546385288238525 3.6546385288238525
Loss :  1.801615595817566 3.1669867038726807 3.1669867038726807
Loss :  1.7994163036346436 2.8142030239105225 2.8142030239105225
Loss :  1.8042316436767578 2.6489880084991455 2.6489880084991455
Loss :  1.802310824394226 3.024474620819092 3.024474620819092
Loss :  1.7993451356887817 3.099078893661499 3.099078893661499
Loss :  1.8007482290267944 3.1306538581848145 3.1306538581848145
Loss :  1.7966089248657227 3.570159435272217 3.570159435272217
Loss :  1.8002783060073853 3.5793328285217285 3.5793328285217285
Loss :  1.7944161891937256 3.2215540409088135 3.2215540409088135
Loss :  1.8017374277114868 3.2269368171691895 3.2269368171691895
Loss :  1.8019503355026245 3.6321396827697754 3.6321396827697754
Loss :  1.8052866458892822 3.363878011703491 3.363878011703491
Loss :  1.7992416620254517 3.294386863708496 3.294386863708496
Loss :  1.8028513193130493 2.607461452484131 2.607461452484131
Loss :  1.8067792654037476 3.1942360401153564 3.1942360401153564
Loss :  1.805674433708191 3.2585463523864746 3.2585463523864746
Loss :  1.8094017505645752 3.4331512451171875 3.4331512451171875
Loss :  1.7998723983764648 3.3471219539642334 3.3471219539642334
  batch 60 loss: 1.7998723983764648, 3.3471219539642334, 3.3471219539642334
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8066916465759277 3.625962972640991 3.625962972640991
Loss :  1.7994948625564575 3.68279767036438 3.68279767036438
Loss :  1.8046541213989258 3.9278671741485596 3.9278671741485596
Loss :  1.80070161819458 3.5091919898986816 3.5091919898986816
Loss :  1.8052040338516235 3.626556873321533 3.626556873321533
Loss :  1.7128881216049194 4.408775806427002 4.408775806427002
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.714288592338562 4.434595108032227 4.434595108032227
Loss :  1.7102385759353638 4.350194454193115 4.350194454193115
Loss :  1.7391345500946045 4.327000617980957 4.327000617980957
Total LOSS train 3.1775803089141847 valid 4.380141496658325
CE LOSS train 1.80216145332043 valid 0.4347836375236511
Contrastive LOSS train 3.1775803089141847 valid 1.0817501544952393
EPOCH 263:
Loss :  1.801310658454895 2.8183114528656006 2.8183114528656006
Loss :  1.7992478609085083 3.473517656326294 3.473517656326294
Loss :  1.8017253875732422 3.6861369609832764 3.6861369609832764
Loss :  1.8021990060806274 3.3029556274414062 3.3029556274414062
Loss :  1.800749659538269 3.152088165283203 3.152088165283203
Loss :  1.8049827814102173 2.9816932678222656 2.9816932678222656
Loss :  1.8000246286392212 3.80255126953125 3.80255126953125
Loss :  1.8001927137374878 3.3357083797454834 3.3357083797454834
Loss :  1.7962762117385864 3.038034200668335 3.038034200668335
Loss :  1.7970296144485474 3.4941585063934326 3.4941585063934326
Loss :  1.8074796199798584 3.412083387374878 3.412083387374878
Loss :  1.8021044731140137 3.474653482437134 3.474653482437134
Loss :  1.8068530559539795 3.571366310119629 3.571366310119629
Loss :  1.8060582876205444 3.8523383140563965 3.8523383140563965
Loss :  1.7933586835861206 3.721994638442993 3.721994638442993
Loss :  1.8081254959106445 4.156007766723633 4.156007766723633
Loss :  1.8054362535476685 3.3565714359283447 3.3565714359283447
Loss :  1.8016430139541626 3.5675668716430664 3.5675668716430664
Loss :  1.8016791343688965 3.3231613636016846 3.3231613636016846
Loss :  1.7962104082107544 3.0708296298980713 3.0708296298980713
  batch 20 loss: 1.7962104082107544, 3.0708296298980713, 3.0708296298980713
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8046026229858398 2.659341335296631 2.659341335296631
Loss :  1.803403377532959 2.4626879692077637 2.4626879692077637
Loss :  1.7968099117279053 3.0441300868988037 3.0441300868988037
Loss :  1.8025344610214233 2.8992249965667725 2.8992249965667725
Loss :  1.8022677898406982 3.0582926273345947 3.0582926273345947
Loss :  1.8028980493545532 2.8989646434783936 2.8989646434783936
Loss :  1.8074525594711304 3.137759208679199 3.137759208679199
Loss :  1.7991530895233154 2.9613237380981445 2.9613237380981445
Loss :  1.8119843006134033 3.3328452110290527 3.3328452110290527
Loss :  1.798375129699707 3.148853302001953 3.148853302001953
Loss :  1.8060925006866455 3.2645211219787598 3.2645211219787598
Loss :  1.795381784439087 3.05796217918396 3.05796217918396
Loss :  1.805155634880066 3.0478835105895996 3.0478835105895996
Loss :  1.8041973114013672 2.7998363971710205 2.7998363971710205
Loss :  1.8041112422943115 2.652447462081909 2.652447462081909
Loss :  1.8068017959594727 2.716327428817749 2.716327428817749
Loss :  1.8032121658325195 2.9372689723968506 2.9372689723968506
Loss :  1.7987289428710938 2.6335127353668213 2.6335127353668213
Loss :  1.8008360862731934 2.4864401817321777 2.4864401817321777
Loss :  1.7978097200393677 2.825251579284668 2.825251579284668
  batch 40 loss: 1.7978097200393677, 2.825251579284668, 2.825251579284668
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8027775287628174 2.830460786819458 2.830460786819458
Loss :  1.8025178909301758 2.68753719329834 2.68753719329834
Loss :  1.8015375137329102 2.668266534805298 2.668266534805298
Loss :  1.79947030544281 2.579082489013672 2.579082489013672
Loss :  1.8042124509811401 2.650961399078369 2.650961399078369
Loss :  1.8024591207504272 3.0717580318450928 3.0717580318450928
Loss :  1.7994409799575806 3.0133509635925293 3.0133509635925293
Loss :  1.8007055521011353 3.414590358734131 3.414590358734131
Loss :  1.7968308925628662 3.0170223712921143 3.0170223712921143
Loss :  1.80032479763031 3.0356719493865967 3.0356719493865967
Loss :  1.7943665981292725 2.4879727363586426 2.4879727363586426
Loss :  1.8017996549606323 2.361732244491577 2.361732244491577
Loss :  1.8020285367965698 2.4124276638031006 2.4124276638031006
Loss :  1.8053535223007202 2.3018887042999268 2.3018887042999268
Loss :  1.799180030822754 2.7836406230926514 2.7836406230926514
Loss :  1.8028935194015503 2.5471949577331543 2.5471949577331543
Loss :  1.8069130182266235 3.205343008041382 3.205343008041382
Loss :  1.805753231048584 3.053534984588623 3.053534984588623
Loss :  1.8094457387924194 3.6598598957061768 3.6598598957061768
Loss :  1.799809455871582 3.523855447769165 3.523855447769165
  batch 60 loss: 1.799809455871582, 3.523855447769165, 3.523855447769165
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8068045377731323 3.101398468017578 3.101398468017578
Loss :  1.7994835376739502 2.7034032344818115 2.7034032344818115
Loss :  1.8047064542770386 3.0557913780212402 3.0557913780212402
Loss :  1.8007732629776 2.89180850982666 2.89180850982666
Loss :  1.805336833000183 2.8406825065612793 2.8406825065612793
Loss :  1.71176016330719 4.339560031890869 4.339560031890869
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7131627798080444 4.417078971862793 4.417078971862793
Loss :  1.7091926336288452 4.34069299697876 4.34069299697876
Loss :  1.7381280660629272 4.334471702575684 4.334471702575684
Total LOSS train 3.0540898433098427 valid 4.357950925827026
CE LOSS train 1.802144929078909 valid 0.4345320165157318
Contrastive LOSS train 3.0540898433098427 valid 1.083617925643921
EPOCH 264:
Loss :  1.8013449907302856 3.2678568363189697 3.2678568363189697
Loss :  1.7993173599243164 3.5175371170043945 3.5175371170043945
Loss :  1.8018079996109009 2.702895402908325 2.702895402908325
Loss :  1.802243709564209 2.9851112365722656 2.9851112365722656
Loss :  1.800797700881958 3.0422325134277344 3.0422325134277344
Loss :  1.8050472736358643 2.686429262161255 2.686429262161255
Loss :  1.8001402616500854 3.1445834636688232 3.1445834636688232
Loss :  1.8003426790237427 2.97019362449646 2.97019362449646
Loss :  1.7963361740112305 2.9703056812286377 2.9703056812286377
Loss :  1.7972655296325684 2.646712303161621 2.646712303161621
Loss :  1.8076343536376953 2.79872465133667 2.79872465133667
Loss :  1.802190899848938 2.5780646800994873 2.5780646800994873
Loss :  1.8069870471954346 2.831136465072632 2.831136465072632
Loss :  1.806201696395874 2.6540985107421875 2.6540985107421875
Loss :  1.7934916019439697 2.907841205596924 2.907841205596924
Loss :  1.808369517326355 3.3846652507781982 3.3846652507781982
Loss :  1.8056237697601318 3.2381887435913086 3.2381887435913086
Loss :  1.8018827438354492 3.4518885612487793 3.4518885612487793
Loss :  1.801933765411377 3.0067317485809326 3.0067317485809326
Loss :  1.7964065074920654 3.3765816688537598 3.3765816688537598
  batch 20 loss: 1.7964065074920654, 3.3765816688537598, 3.3765816688537598
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8047932386398315 3.6836421489715576 3.6836421489715576
Loss :  1.8035637140274048 3.2273333072662354 3.2273333072662354
Loss :  1.79696524143219 3.4978532791137695 3.4978532791137695
Loss :  1.8026256561279297 3.1759603023529053 3.1759603023529053
Loss :  1.8024731874465942 3.2877323627471924 3.2877323627471924
Loss :  1.8030667304992676 2.7294087409973145 2.7294087409973145
Loss :  1.80758535861969 3.31957745552063 3.31957745552063
Loss :  1.7993143796920776 2.611644983291626 2.611644983291626
Loss :  1.8121370077133179 3.2774505615234375 3.2774505615234375
Loss :  1.798587679862976 3.4431333541870117 3.4431333541870117
Loss :  1.8062944412231445 3.2009117603302 3.2009117603302
Loss :  1.795527458190918 2.906834602355957 2.906834602355957
Loss :  1.8053100109100342 3.1070237159729004 3.1070237159729004
Loss :  1.8045401573181152 3.1134424209594727 3.1134424209594727
Loss :  1.8043094873428345 3.006897211074829 3.006897211074829
Loss :  1.8070392608642578 3.194959878921509 3.194959878921509
Loss :  1.8033843040466309 3.271846055984497 3.271846055984497
Loss :  1.7988529205322266 3.2233102321624756 3.2233102321624756
Loss :  1.8010022640228271 3.5974671840667725 3.5974671840667725
Loss :  1.7980098724365234 3.260392189025879 3.260392189025879
  batch 40 loss: 1.7980098724365234, 3.260392189025879, 3.260392189025879
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.802945852279663 3.3592164516448975 3.3592164516448975
Loss :  1.8028193712234497 3.346264123916626 3.346264123916626
Loss :  1.8017948865890503 2.825270175933838 2.825270175933838
Loss :  1.7997276782989502 3.0816609859466553 3.0816609859466553
Loss :  1.8043720722198486 3.21032452583313 3.21032452583313
Loss :  1.8026515245437622 3.6592657566070557 3.6592657566070557
Loss :  1.7995498180389404 3.5513663291931152 3.5513663291931152
Loss :  1.8008968830108643 3.7264273166656494 3.7264273166656494
Loss :  1.7969480752944946 3.499821186065674 3.499821186065674
Loss :  1.8005040884017944 3.674334764480591 3.674334764480591
Loss :  1.794551134109497 2.9628591537475586 2.9628591537475586
Loss :  1.8018860816955566 2.914156913757324 2.914156913757324
Loss :  1.8021553754806519 2.7177624702453613 2.7177624702453613
Loss :  1.8055952787399292 2.506211280822754 2.506211280822754
Loss :  1.7993119955062866 3.099858522415161 3.099858522415161
Loss :  1.8029208183288574 2.475046396255493 2.475046396255493
Loss :  1.8070818185806274 3.1300132274627686 3.1300132274627686
Loss :  1.8059600591659546 2.95798921585083 2.95798921585083
Loss :  1.809586524963379 3.01875901222229 3.01875901222229
Loss :  1.7997626066207886 2.9093992710113525 2.9093992710113525
  batch 60 loss: 1.7997626066207886, 2.9093992710113525, 2.9093992710113525
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8070106506347656 2.9758849143981934 2.9758849143981934
Loss :  1.799613118171692 2.8638722896575928 2.8638722896575928
Loss :  1.8047937154769897 3.1807045936584473 3.1807045936584473
Loss :  1.8009519577026367 3.2216036319732666 3.2216036319732666
Loss :  1.8056175708770752 2.5258450508117676 2.5258450508117676
Loss :  1.7102737426757812 4.415901184082031 4.415901184082031
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.711671233177185 4.379038333892822 4.379038333892822
Loss :  1.7078258991241455 4.350222110748291 4.350222110748291
Loss :  1.7368220090866089 4.047617435455322 4.047617435455322
Total LOSS train 3.1029618189885064 valid 4.298194766044617
CE LOSS train 1.8023039524371807 valid 0.4342055022716522
Contrastive LOSS train 3.1029618189885064 valid 1.0119043588638306
EPOCH 265:
Loss :  1.8014819622039795 3.0527098178863525 3.0527098178863525
Loss :  1.7994320392608643 2.905155897140503 2.905155897140503
Loss :  1.8020405769348145 3.3254551887512207 3.3254551887512207
Loss :  1.8023937940597534 3.115734577178955 3.115734577178955
Loss :  1.8009064197540283 2.5882935523986816 2.5882935523986816
Loss :  1.8051706552505493 2.6376354694366455 2.6376354694366455
Loss :  1.800284504890442 2.9637513160705566 2.9637513160705566
Loss :  1.8005504608154297 2.969306468963623 2.969306468963623
Loss :  1.796403408050537 2.954176187515259 2.954176187515259
Loss :  1.7975153923034668 2.8768973350524902 2.8768973350524902
Loss :  1.807845950126648 3.323629140853882 3.323629140853882
Loss :  1.8023065328598022 3.108689546585083 3.108689546585083
Loss :  1.8071500062942505 3.2486982345581055 3.2486982345581055
Loss :  1.806294560432434 3.177952766418457 3.177952766418457
Loss :  1.793439269065857 3.359830379486084 3.359830379486084
Loss :  1.808521032333374 3.5409536361694336 3.5409536361694336
Loss :  1.805738925933838 3.149632453918457 3.149632453918457
Loss :  1.8020051717758179 3.1996805667877197 3.1996805667877197
Loss :  1.8020117282867432 3.0693538188934326 3.0693538188934326
Loss :  1.796472191810608 3.5056405067443848 3.5056405067443848
  batch 20 loss: 1.796472191810608, 3.5056405067443848, 3.5056405067443848
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8048219680786133 3.604879856109619 3.604879856109619
Loss :  1.8035869598388672 3.843458414077759 3.843458414077759
Loss :  1.796975016593933 3.162397623062134 3.162397623062134
Loss :  1.802593469619751 3.3986237049102783 3.3986237049102783
Loss :  1.8024451732635498 3.620126485824585 3.620126485824585
Loss :  1.803161382675171 3.052757740020752 3.052757740020752
Loss :  1.8076833486557007 3.02164888381958 3.02164888381958
Loss :  1.7994312047958374 2.574233055114746 2.574233055114746
Loss :  1.8123019933700562 2.938267469406128 2.938267469406128
Loss :  1.7986619472503662 3.205493927001953 3.205493927001953
Loss :  1.8063610792160034 3.4013397693634033 3.4013397693634033
Loss :  1.7955999374389648 2.896304130554199 2.896304130554199
Loss :  1.8054322004318237 2.7265660762786865 2.7265660762786865
Loss :  1.8047040700912476 3.157837390899658 3.157837390899658
Loss :  1.8043659925460815 3.1013023853302 3.1013023853302
Loss :  1.8071691989898682 3.3307645320892334 3.3307645320892334
Loss :  1.8034663200378418 3.2806129455566406 3.2806129455566406
Loss :  1.7989956140518188 3.3412084579467773 3.3412084579467773
Loss :  1.8010799884796143 3.339259147644043 3.339259147644043
Loss :  1.7981228828430176 2.9039695262908936 2.9039695262908936
  batch 40 loss: 1.7981228828430176, 2.9039695262908936, 2.9039695262908936
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8030239343643188 3.1161298751831055 3.1161298751831055
Loss :  1.802954077720642 3.4078431129455566 3.4078431129455566
Loss :  1.8018683195114136 3.0453197956085205 3.0453197956085205
Loss :  1.799832820892334 2.9633796215057373 2.9633796215057373
Loss :  1.8044513463974 3.114046335220337 3.114046335220337
Loss :  1.8027615547180176 3.374290704727173 3.374290704727173
Loss :  1.7996915578842163 3.330901622772217 3.330901622772217
Loss :  1.8010011911392212 3.5974044799804688 3.5974044799804688
Loss :  1.7970997095108032 3.2239315509796143 3.2239315509796143
Loss :  1.8006397485733032 3.520184278488159 3.520184278488159
Loss :  1.7947276830673218 2.7860045433044434 2.7860045433044434
Loss :  1.8020362854003906 2.8435921669006348 2.8435921669006348
Loss :  1.8022632598876953 3.1344804763793945 3.1344804763793945
Loss :  1.8057990074157715 3.2259762287139893 3.2259762287139893
Loss :  1.7994619607925415 3.4773941040039062 3.4773941040039062
Loss :  1.8031666278839111 2.8697991371154785 2.8697991371154785
Loss :  1.8071845769882202 3.4973297119140625 3.4973297119140625
Loss :  1.8060091733932495 3.1474387645721436 3.1474387645721436
Loss :  1.8096619844436646 2.902255058288574 2.902255058288574
Loss :  1.8000586032867432 2.8231022357940674 2.8231022357940674
  batch 60 loss: 1.8000586032867432, 2.8231022357940674, 2.8231022357940674
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.807050347328186 3.3840296268463135 3.3840296268463135
Loss :  1.7997702360153198 2.8463058471679688 2.8463058471679688
Loss :  1.8048701286315918 2.5865097045898438 2.5865097045898438
Loss :  1.8010470867156982 2.826549530029297 2.826549530029297
Loss :  1.8057276010513306 3.2972195148468018 3.2972195148468018
Loss :  1.710242748260498 4.430692672729492 4.430692672729492
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7115955352783203 4.406378746032715 4.406378746032715
Loss :  1.707771897315979 4.331899642944336 4.331899642944336
Loss :  1.7369158267974854 4.248886585235596 4.248886585235596
Total LOSS train 3.143317637076745 valid 4.354464411735535
CE LOSS train 1.8024166639034565 valid 0.43422895669937134
Contrastive LOSS train 3.143317637076745 valid 1.062221646308899
EPOCH 266:
Loss :  1.8017354011535645 2.796320915222168 2.796320915222168
Loss :  1.799487829208374 2.887739896774292 2.887739896774292
Loss :  1.8022445440292358 3.236487865447998 3.236487865447998
Loss :  1.802661657333374 2.9407477378845215 2.9407477378845215
Loss :  1.80108642578125 3.356668710708618 3.356668710708618
Loss :  1.8053388595581055 2.5907397270202637 2.5907397270202637
Loss :  1.8004058599472046 2.743856191635132 2.743856191635132
Loss :  1.800594687461853 2.6447181701660156 2.6447181701660156
Loss :  1.7966609001159668 2.8955557346343994 2.8955557346343994
Loss :  1.7976754903793335 2.738345146179199 2.738345146179199
Loss :  1.8078886270523071 3.0508177280426025 3.0508177280426025
Loss :  1.8023887872695923 3.099052906036377 3.099052906036377
Loss :  1.8071280717849731 2.8330039978027344 2.8330039978027344
Loss :  1.806315541267395 2.735422372817993 2.735422372817993
Loss :  1.7937134504318237 2.641373872756958 2.641373872756958
Loss :  1.8085336685180664 2.7730324268341064 2.7730324268341064
Loss :  1.8057337999343872 2.8639535903930664 2.8639535903930664
Loss :  1.8020758628845215 3.4209096431732178 3.4209096431732178
Loss :  1.8021299839019775 3.286144495010376 3.286144495010376
Loss :  1.7965763807296753 3.6411502361297607 3.6411502361297607
  batch 20 loss: 1.7965763807296753, 3.6411502361297607, 3.6411502361297607
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8049447536468506 3.0565757751464844 3.0565757751464844
Loss :  1.8036797046661377 2.8042612075805664 2.8042612075805664
Loss :  1.7971782684326172 2.822075843811035 2.822075843811035
Loss :  1.8027222156524658 2.8477771282196045 2.8477771282196045
Loss :  1.8027446269989014 3.1725127696990967 3.1725127696990967
Loss :  1.8032315969467163 2.383039712905884 2.383039712905884
Loss :  1.807627558708191 2.9659292697906494 2.9659292697906494
Loss :  1.7994325160980225 2.4500973224639893 2.4500973224639893
Loss :  1.8120214939117432 2.6107912063598633 2.6107912063598633
Loss :  1.7987887859344482 2.8220503330230713 2.8220503330230713
Loss :  1.80629563331604 2.9235727787017822 2.9235727787017822
Loss :  1.7956119775772095 2.958256959915161 2.958256959915161
Loss :  1.8053457736968994 2.599407911300659 2.599407911300659
Loss :  1.804595947265625 3.133464813232422 3.133464813232422
Loss :  1.8044394254684448 2.993476152420044 2.993476152420044
Loss :  1.8069562911987305 2.9579050540924072 2.9579050540924072
Loss :  1.8034495115280151 2.971972942352295 2.971972942352295
Loss :  1.7988643646240234 2.766373634338379 2.766373634338379
Loss :  1.8011255264282227 2.9199516773223877 2.9199516773223877
Loss :  1.7981071472167969 3.053180694580078 3.053180694580078
  batch 40 loss: 1.7981071472167969, 3.053180694580078, 3.053180694580078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.802978754043579 3.1407132148742676 3.1407132148742676
Loss :  1.8027844429016113 3.0422463417053223 3.0422463417053223
Loss :  1.8019434213638306 2.5430328845977783 2.5430328845977783
Loss :  1.799730896949768 2.6084988117218018 2.6084988117218018
Loss :  1.8044521808624268 2.8389363288879395 2.8389363288879395
Loss :  1.802559494972229 3.043246269226074 3.043246269226074
Loss :  1.79960036277771 3.1041581630706787 3.1041581630706787
Loss :  1.8009933233261108 3.0400004386901855 3.0400004386901855
Loss :  1.7968531847000122 2.7475552558898926 2.7475552558898926
Loss :  1.800599217414856 3.0237278938293457 3.0237278938293457
Loss :  1.7947278022766113 3.0527567863464355 3.0527567863464355
Loss :  1.8019132614135742 3.1704623699188232 3.1704623699188232
Loss :  1.8021118640899658 3.0767664909362793 3.0767664909362793
Loss :  1.805646538734436 2.9008214473724365 2.9008214473724365
Loss :  1.7995171546936035 2.8695626258850098 2.8695626258850098
Loss :  1.802978754043579 2.8622896671295166 2.8622896671295166
Loss :  1.8069981336593628 3.125640392303467 3.125640392303467
Loss :  1.805968165397644 2.7389063835144043 2.7389063835144043
Loss :  1.8095309734344482 3.2246453762054443 3.2246453762054443
Loss :  1.7999422550201416 3.2186427116394043 3.2186427116394043
  batch 60 loss: 1.7999422550201416, 3.2186427116394043, 3.2186427116394043
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.806875228881836 3.1604561805725098 3.1604561805725098
Loss :  1.799694538116455 2.9812381267547607 2.9812381267547607
Loss :  1.8047497272491455 3.5627176761627197 3.5627176761627197
Loss :  1.8009288311004639 3.871981620788574 3.871981620788574
Loss :  1.8056011199951172 3.5682477951049805 3.5682477951049805
Loss :  1.7102090120315552 4.384636402130127 4.384636402130127
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7115633487701416 4.340120792388916 4.340120792388916
Loss :  1.707784652709961 4.344500541687012 4.344500541687012
Loss :  1.7367517948150635 4.240006923675537 4.240006923675537
Total LOSS train 2.967784089308519 valid 4.327316164970398
CE LOSS train 1.80241874731504 valid 0.43418794870376587
Contrastive LOSS train 2.967784089308519 valid 1.0600017309188843
EPOCH 267:
Loss :  1.8015755414962769 3.5734968185424805 3.5734968185424805
Loss :  1.7992912530899048 3.9486773014068604 3.9486773014068604
Loss :  1.802060842514038 3.305715322494507 3.305715322494507
Loss :  1.8025352954864502 3.6903252601623535 3.6903252601623535
Loss :  1.8009449243545532 3.7873692512512207 3.7873692512512207
Loss :  1.8052064180374146 3.47314190864563 3.47314190864563
Loss :  1.8001937866210938 3.5304176807403564 3.5304176807403564
Loss :  1.8004231452941895 3.588477849960327 3.588477849960327
Loss :  1.7965087890625 3.7116241455078125 3.7116241455078125
Loss :  1.7974305152893066 3.530853509902954 3.530853509902954
Loss :  1.807750940322876 3.1123571395874023 3.1123571395874023
Loss :  1.8022629022598267 3.703430414199829 3.703430414199829
Loss :  1.807051420211792 3.5394012928009033 3.5394012928009033
Loss :  1.8062562942504883 3.3210268020629883 3.3210268020629883
Loss :  1.7935590744018555 3.787694215774536 3.787694215774536
Loss :  1.8084113597869873 2.904142141342163 2.904142141342163
Loss :  1.8056869506835938 2.7663726806640625 2.7663726806640625
Loss :  1.8018912076950073 2.9280710220336914 2.9280710220336914
Loss :  1.8020405769348145 2.928739070892334 2.928739070892334
Loss :  1.796426773071289 3.1503348350524902 3.1503348350524902
  batch 20 loss: 1.796426773071289, 3.1503348350524902, 3.1503348350524902
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8047798871994019 2.9011013507843018 2.9011013507843018
Loss :  1.803601861000061 2.6212852001190186 2.6212852001190186
Loss :  1.7969677448272705 3.247645616531372 3.247645616531372
Loss :  1.8026520013809204 3.4428658485412598 3.4428658485412598
Loss :  1.802557110786438 3.710808753967285 3.710808753967285
Loss :  1.8030511140823364 3.5501811504364014 3.5501811504364014
Loss :  1.8075627088546753 3.7140631675720215 3.7140631675720215
Loss :  1.799304485321045 3.228203058242798 3.228203058242798
Loss :  1.81210458278656 2.9312851428985596 2.9312851428985596
Loss :  1.7986836433410645 3.8186895847320557 3.8186895847320557
Loss :  1.806315541267395 3.4628920555114746 3.4628920555114746
Loss :  1.7955470085144043 3.4840426445007324 3.4840426445007324
Loss :  1.8053100109100342 2.6004528999328613 2.6004528999328613
Loss :  1.8044960498809814 2.964704751968384 2.964704751968384
Loss :  1.8043409585952759 3.123804807662964 3.123804807662964
Loss :  1.8070030212402344 3.0631682872772217 3.0631682872772217
Loss :  1.8033758401870728 3.2452445030212402 3.2452445030212402
Loss :  1.798841118812561 3.0675370693206787 3.0675370693206787
Loss :  1.8009812831878662 3.2245450019836426 3.2245450019836426
Loss :  1.7979785203933716 3.1893889904022217 3.1893889904022217
  batch 40 loss: 1.7979785203933716, 3.1893889904022217, 3.1893889904022217
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8029236793518066 3.88217830657959 3.88217830657959
Loss :  1.8027822971343994 3.272318124771118 3.272318124771118
Loss :  1.8017815351486206 3.4128501415252686 3.4128501415252686
Loss :  1.7997015714645386 3.459563732147217 3.459563732147217
Loss :  1.8044074773788452 2.962611198425293 2.962611198425293
Loss :  1.8026069402694702 3.2162840366363525 3.2162840366363525
Loss :  1.799554467201233 3.6554765701293945 3.6554765701293945
Loss :  1.8009330034255981 3.142665147781372 3.142665147781372
Loss :  1.7969021797180176 3.406308174133301 3.406308174133301
Loss :  1.800520896911621 3.6908304691314697 3.6908304691314697
Loss :  1.7946091890335083 2.5490024089813232 2.5490024089813232
Loss :  1.8018765449523926 2.9535062313079834 2.9535062313079834
Loss :  1.8021130561828613 2.8091084957122803 2.8091084957122803
Loss :  1.8056343793869019 2.5498194694519043 2.5498194694519043
Loss :  1.799405813217163 2.613783597946167 2.613783597946167
Loss :  1.8029474020004272 2.109079360961914 2.109079360961914
Loss :  1.8070464134216309 2.6216719150543213 2.6216719150543213
Loss :  1.8059402704238892 2.586649179458618 2.586649179458618
Loss :  1.8095805644989014 2.950606346130371 2.950606346130371
Loss :  1.799915075302124 3.1644232273101807 3.1644232273101807
  batch 60 loss: 1.799915075302124, 3.1644232273101807, 3.1644232273101807
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.806972861289978 3.0087504386901855 3.0087504386901855
Loss :  1.7997007369995117 2.622406005859375 2.622406005859375
Loss :  1.8048158884048462 3.0386157035827637 3.0386157035827637
Loss :  1.8009802103042603 3.3207831382751465 3.3207831382751465
Loss :  1.8056509494781494 2.7945306301116943 2.7945306301116943
Loss :  1.7096658945083618 4.3799967765808105 4.3799967765808105
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7109850645065308 4.4296979904174805 4.4296979904174805
Loss :  1.7072865962982178 4.353492259979248 4.353492259979248
Loss :  1.7362720966339111 4.33941650390625 4.33941650390625
Total LOSS train 3.2102677015157846 valid 4.375650882720947
CE LOSS train 1.8023425524051373 valid 0.4340680241584778
Contrastive LOSS train 3.2102677015157846 valid 1.0848541259765625
EPOCH 268:
Loss :  1.8016165494918823 2.731995105743408 2.731995105743408
Loss :  1.7994073629379272 2.5869483947753906 2.5869483947753906
Loss :  1.8021072149276733 2.9468603134155273 2.9468603134155273
Loss :  1.80256986618042 2.860083818435669 2.860083818435669
Loss :  1.8010039329528809 2.8017396926879883 2.8017396926879883
Loss :  1.8052647113800049 3.1459245681762695 3.1459245681762695
Loss :  1.8002796173095703 3.2979695796966553 3.2979695796966553
Loss :  1.8005259037017822 2.9212725162506104 2.9212725162506104
Loss :  1.7965048551559448 3.058626890182495 3.058626890182495
Loss :  1.797529697418213 2.8689332008361816 2.8689332008361816
Loss :  1.8078683614730835 3.3944828510284424 3.3944828510284424
Loss :  1.8023375272750854 3.004772663116455 3.004772663116455
Loss :  1.8071428537368774 3.9716460704803467 3.9716460704803467
Loss :  1.8062846660614014 3.304844379425049 3.304844379425049
Loss :  1.7935434579849243 3.31321382522583 3.31321382522583
Loss :  1.8085196018218994 3.724886178970337 3.724886178970337
Loss :  1.8057466745376587 3.1857833862304688 3.1857833862304688
Loss :  1.801992416381836 3.154484748840332 3.154484748840332
Loss :  1.8020806312561035 3.400493621826172 3.400493621826172
Loss :  1.7964717149734497 3.4986379146575928 3.4986379146575928
  batch 20 loss: 1.7964717149734497, 3.4986379146575928, 3.4986379146575928
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.804789662361145 3.4903457164764404 3.4903457164764404
Loss :  1.803606390953064 3.376344919204712 3.376344919204712
Loss :  1.7969892024993896 3.2945282459259033 3.2945282459259033
Loss :  1.8026496171951294 3.1774401664733887 3.1774401664733887
Loss :  1.8025639057159424 3.8525116443634033 3.8525116443634033
Loss :  1.8031288385391235 3.8718149662017822 3.8718149662017822
Loss :  1.8076190948486328 3.9202964305877686 3.9202964305877686
Loss :  1.7993617057800293 3.867872714996338 3.867872714996338
Loss :  1.8121806383132935 3.3320820331573486 3.3320820331573486
Loss :  1.7987163066864014 3.7095022201538086 3.7095022201538086
Loss :  1.806357502937317 3.4393420219421387 3.4393420219421387
Loss :  1.795580267906189 3.125662088394165 3.125662088394165
Loss :  1.8053611516952515 3.0160155296325684 3.0160155296325684
Loss :  1.8045809268951416 2.883014440536499 2.883014440536499
Loss :  1.8043527603149414 3.022817373275757 3.022817373275757
Loss :  1.8070911169052124 3.096275806427002 3.096275806427002
Loss :  1.8034080266952515 3.161205291748047 3.161205291748047
Loss :  1.798901915550232 3.2320616245269775 3.2320616245269775
Loss :  1.8009916543960571 3.5670981407165527 3.5670981407165527
Loss :  1.7980159521102905 3.2315688133239746 3.2315688133239746
  batch 40 loss: 1.7980159521102905, 3.2315688133239746, 3.2315688133239746
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8029506206512451 3.3971455097198486 3.3971455097198486
Loss :  1.8028820753097534 3.251391887664795 3.251391887664795
Loss :  1.801778793334961 3.3230481147766113 3.3230481147766113
Loss :  1.7997658252716064 3.0648996829986572 3.0648996829986572
Loss :  1.8044193983078003 2.880110502243042 2.880110502243042
Loss :  1.8026981353759766 3.8438234329223633 3.8438234329223633
Loss :  1.799561619758606 3.534449815750122 3.534449815750122
Loss :  1.8009411096572876 3.3563692569732666 3.3563692569732666
Loss :  1.7969642877578735 3.4127519130706787 3.4127519130706787
Loss :  1.8005168437957764 3.6815602779388428 3.6815602779388428
Loss :  1.7945798635482788 3.1856720447540283 3.1856720447540283
Loss :  1.801856517791748 3.5605039596557617 3.5605039596557617
Loss :  1.802146553993225 3.1247270107269287 3.1247270107269287
Loss :  1.80564546585083 3.457080125808716 3.457080125808716
Loss :  1.7993271350860596 3.6004397869110107 3.6004397869110107
Loss :  1.8028589487075806 3.7462046146392822 3.7462046146392822
Loss :  1.8070948123931885 3.5921831130981445 3.5921831130981445
Loss :  1.8059355020523071 3.3856310844421387 3.3856310844421387
Loss :  1.8095754384994507 3.3783469200134277 3.3783469200134277
Loss :  1.7997885942459106 3.422492504119873 3.422492504119873
  batch 60 loss: 1.7997885942459106, 3.422492504119873, 3.422492504119873
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8070281744003296 3.401928424835205 3.401928424835205
Loss :  1.7996094226837158 3.293713331222534 3.293713331222534
Loss :  1.8048253059387207 3.2318332195281982 3.2318332195281982
Loss :  1.8009610176086426 3.0965960025787354 3.0965960025787354
Loss :  1.8056026697158813 3.640778064727783 3.640778064727783
Loss :  1.7094117403030396 4.409736633300781 4.409736633300781
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7107245922088623 4.3701701164245605 4.3701701164245605
Loss :  1.707060694694519 4.361479759216309 4.361479759216309
Loss :  1.7360368967056274 4.380866527557373 4.380866527557373
Total LOSS train 3.31853970014132 valid 4.380563259124756
CE LOSS train 1.8023747444152831 valid 0.43400922417640686
Contrastive LOSS train 3.31853970014132 valid 1.0952166318893433
EPOCH 269:
Loss :  1.8015161752700806 2.908817768096924 2.908817768096924
Loss :  1.7994904518127441 3.232346296310425 3.232346296310425
Loss :  1.80205500125885 2.715897798538208 2.715897798538208
Loss :  1.8024646043777466 3.058340311050415 3.058340311050415
Loss :  1.8009724617004395 3.1808371543884277 3.1808371543884277
Loss :  1.805242657661438 2.674574613571167 2.674574613571167
Loss :  1.8003407716751099 3.024115562438965 3.024115562438965
Loss :  1.80057954788208 3.5039517879486084 3.5039517879486084
Loss :  1.7964171171188354 3.165447235107422 3.165447235107422
Loss :  1.797523021697998 2.878448486328125 2.878448486328125
Loss :  1.8079131841659546 3.089677333831787 3.089677333831787
Loss :  1.8023244142532349 3.1791322231292725 3.1791322231292725
Loss :  1.8072173595428467 3.181293249130249 3.181293249130249
Loss :  1.8062560558319092 3.1183393001556396 3.1183393001556396
Loss :  1.7933430671691895 3.3483214378356934 3.3483214378356934
Loss :  1.8085336685180664 3.363966226577759 3.363966226577759
Loss :  1.805754542350769 2.8426339626312256 2.8426339626312256
Loss :  1.801999568939209 3.126682758331299 3.126682758331299
Loss :  1.801969051361084 3.1871159076690674 3.1871159076690674
Loss :  1.7964613437652588 3.1723341941833496 3.1723341941833496
  batch 20 loss: 1.7964613437652588, 3.1723341941833496, 3.1723341941833496
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8047069311141968 3.0641465187072754 3.0641465187072754
Loss :  1.803518295288086 3.3887746334075928 3.3887746334075928
Loss :  1.7969039678573608 3.818469285964966 3.818469285964966
Loss :  1.8025281429290771 3.28739857673645 3.28739857673645
Loss :  1.8023356199264526 4.296976089477539 4.296976089477539
Loss :  1.8031384944915771 3.544212579727173 3.544212579727173
Loss :  1.8076568841934204 3.7019848823547363 3.7019848823547363
Loss :  1.7993879318237305 2.973254680633545 2.973254680633545
Loss :  1.81234872341156 3.1500160694122314 3.1500160694122314
Loss :  1.7986394166946411 3.7350854873657227 3.7350854873657227
Loss :  1.8063488006591797 3.280022621154785 3.280022621154785
Loss :  1.795574426651001 4.079662799835205 4.079662799835205
Loss :  1.8054453134536743 3.3468246459960938 3.3468246459960938
Loss :  1.8046327829360962 3.4381942749023438 3.4381942749023438
Loss :  1.804312825202942 3.033557176589966 3.033557176589966
Loss :  1.8071986436843872 3.4300107955932617 3.4300107955932617
Loss :  1.8034257888793945 3.3062891960144043 3.3062891960144043
Loss :  1.7990306615829468 3.2475926876068115 3.2475926876068115
Loss :  1.8009719848632812 2.776827096939087 2.776827096939087
Loss :  1.798057198524475 2.9893620014190674 2.9893620014190674
  batch 40 loss: 1.798057198524475, 2.9893620014190674, 2.9893620014190674
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.802959680557251 3.3182897567749023 3.3182897567749023
Loss :  1.8029887676239014 3.1748318672180176 3.1748318672180176
Loss :  1.801737666130066 3.229311943054199 3.229311943054199
Loss :  1.7998312711715698 2.8715896606445312 2.8715896606445312
Loss :  1.8044244050979614 2.376244306564331 2.376244306564331
Loss :  1.802803635597229 3.1405441761016846 3.1405441761016846
Loss :  1.799652338027954 3.2853844165802 3.2853844165802
Loss :  1.80094313621521 2.918745756149292 2.918745756149292
Loss :  1.797128438949585 3.0503690242767334 3.0503690242767334
Loss :  1.800580382347107 3.0305116176605225 3.0305116176605225
Loss :  1.7945746183395386 3.244658946990967 3.244658946990967
Loss :  1.8018953800201416 2.879847764968872 2.879847764968872
Loss :  1.8022065162658691 3.000267267227173 3.000267267227173
Loss :  1.8057327270507812 2.5731682777404785 2.5731682777404785
Loss :  1.79931640625 2.763718605041504 2.763718605041504
Loss :  1.8029284477233887 2.5212152004241943 2.5212152004241943
Loss :  1.807186484336853 2.7403500080108643 2.7403500080108643
Loss :  1.8059604167938232 3.009432792663574 3.009432792663574
Loss :  1.8096076250076294 3.29972767829895 3.29972767829895
Loss :  1.7997868061065674 3.2021596431732178 3.2021596431732178
  batch 60 loss: 1.7997868061065674, 3.2021596431732178, 3.2021596431732178
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8070882558822632 2.990013599395752 2.990013599395752
Loss :  1.7996344566345215 3.1086652278900146 3.1086652278900146
Loss :  1.804853916168213 3.078906774520874 3.078906774520874
Loss :  1.80100417137146 2.8050484657287598 2.8050484657287598
Loss :  1.8056821823120117 2.4091899394989014 2.4091899394989014
Loss :  1.7086961269378662 4.4295430183410645 4.4295430183410645
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7099831104278564 4.41630744934082 4.41630744934082
Loss :  1.7064074277877808 4.411613941192627 4.411613941192627
Loss :  1.7354801893234253 4.210084438323975 4.210084438323975
Total LOSS train 3.1363558219029355 valid 4.366887211799622
CE LOSS train 1.8023853081923265 valid 0.4338700473308563
Contrastive LOSS train 3.1363558219029355 valid 1.0525211095809937
EPOCH 270:
Loss :  1.801556944847107 3.2577855587005615 3.2577855587005615
Loss :  1.799479603767395 3.41487193107605 3.41487193107605
Loss :  1.8021000623703003 3.4423611164093018 3.4423611164093018
Loss :  1.8025023937225342 3.1269145011901855 3.1269145011901855
Loss :  1.8009917736053467 3.127902030944824 3.127902030944824
Loss :  1.805266261100769 2.778055429458618 2.778055429458618
Loss :  1.8003283739089966 3.224531412124634 3.224531412124634
Loss :  1.8005675077438354 2.674198627471924 2.674198627471924
Loss :  1.7964715957641602 2.5591835975646973 2.5591835975646973
Loss :  1.7975690364837646 2.5674891471862793 2.5674891471862793
Loss :  1.8079073429107666 2.935021162033081 2.935021162033081
Loss :  1.8023394346237183 3.1084492206573486 3.1084492206573486
Loss :  1.8071702718734741 3.1308434009552 3.1308434009552
Loss :  1.8062490224838257 2.971334934234619 2.971334934234619
Loss :  1.793429970741272 3.728508472442627 3.728508472442627
Loss :  1.80851149559021 3.836853504180908 3.836853504180908
Loss :  1.805728554725647 3.2063724994659424 3.2063724994659424
Loss :  1.801985502243042 2.8367438316345215 2.8367438316345215
Loss :  1.8020496368408203 2.8133738040924072 2.8133738040924072
Loss :  1.7964136600494385 3.3958332538604736 3.3958332538604736
  batch 20 loss: 1.7964136600494385, 3.3958332538604736, 3.3958332538604736
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8047361373901367 3.1498374938964844 3.1498374938964844
Loss :  1.8035553693771362 2.6762213706970215 2.6762213706970215
Loss :  1.79694402217865 2.961658000946045 2.961658000946045
Loss :  1.8025906085968018 3.2449429035186768 3.2449429035186768
Loss :  1.8025034666061401 3.314558982849121 3.314558982849121
Loss :  1.803104281425476 3.1735119819641113 3.1735119819641113
Loss :  1.807582974433899 3.1273484230041504 3.1273484230041504
Loss :  1.7993251085281372 3.0602879524230957 3.0602879524230957
Loss :  1.812142252922058 2.599578380584717 2.599578380584717
Loss :  1.7987022399902344 3.1967217922210693 3.1967217922210693
Loss :  1.8063327074050903 3.1881215572357178 3.1881215572357178
Loss :  1.7955554723739624 2.7910361289978027 2.7910361289978027
Loss :  1.80534029006958 2.702662944793701 2.702662944793701
Loss :  1.8045395612716675 2.791224718093872 2.791224718093872
Loss :  1.8043473958969116 3.0749905109405518 3.0749905109405518
Loss :  1.8070406913757324 3.0203444957733154 3.0203444957733154
Loss :  1.8033751249313354 3.4516289234161377 3.4516289234161377
Loss :  1.7988413572311401 3.0974795818328857 3.0974795818328857
Loss :  1.8009661436080933 3.1100122928619385 3.1100122928619385
Loss :  1.797985553741455 2.8720703125 2.8720703125
  batch 40 loss: 1.797985553741455, 2.8720703125, 2.8720703125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8028916120529175 3.7437903881073 3.7437903881073
Loss :  1.8028347492218018 3.122321367263794 3.122321367263794
Loss :  1.8017839193344116 2.783900022506714 2.783900022506714
Loss :  1.79974365234375 3.123255968093872 3.123255968093872
Loss :  1.8043980598449707 2.942692518234253 2.942692518234253
Loss :  1.8025826215744019 3.2083194255828857 3.2083194255828857
Loss :  1.7995084524154663 3.3044822216033936 3.3044822216033936
Loss :  1.8009328842163086 3.2033987045288086 3.2033987045288086
Loss :  1.7968409061431885 3.2462658882141113 3.2462658882141113
Loss :  1.8005201816558838 3.186743974685669 3.186743974685669
Loss :  1.7946109771728516 2.848384141921997 2.848384141921997
Loss :  1.801806926727295 3.196746349334717 3.196746349334717
Loss :  1.80205500125885 3.082810401916504 3.082810401916504
Loss :  1.8056327104568481 3.033015727996826 3.033015727996826
Loss :  1.7994208335876465 3.1656858921051025 3.1656858921051025
Loss :  1.8028634786605835 2.7802517414093018 2.7802517414093018
Loss :  1.806994080543518 3.2592103481292725 3.2592103481292725
Loss :  1.8058722019195557 2.836498498916626 2.836498498916626
Loss :  1.8094921112060547 2.9337522983551025 2.9337522983551025
Loss :  1.7998454570770264 3.2166171073913574 3.2166171073913574
  batch 60 loss: 1.7998454570770264, 3.2166171073913574, 3.2166171073913574
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8068631887435913 2.929403066635132 2.929403066635132
Loss :  1.7996269464492798 2.7904953956604004 2.7904953956604004
Loss :  1.8047453165054321 2.4597601890563965 2.4597601890563965
Loss :  1.8008968830108643 2.7132222652435303 2.7132222652435303
Loss :  1.8055837154388428 2.650961399078369 2.650961399078369
Loss :  1.7091037034988403 4.341122150421143 4.341122150421143
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.710375189781189 4.413094520568848 4.413094520568848
Loss :  1.7067980766296387 4.318094253540039 4.318094253540039
Loss :  1.735688328742981 4.280271530151367 4.280271530151367
Total LOSS train 3.0538900228647083 valid 4.338145613670349
CE LOSS train 1.8023457857278677 valid 0.43392208218574524
Contrastive LOSS train 3.0538900228647083 valid 1.0700678825378418
EPOCH 271:
Loss :  1.8015562295913696 2.5452611446380615 2.5452611446380615
Loss :  1.7992768287658691 2.956265926361084 2.956265926361084
Loss :  1.8020355701446533 2.6143651008605957 2.6143651008605957
Loss :  1.802533507347107 2.6623194217681885 2.6623194217681885
Loss :  1.8009541034698486 2.960362434387207 2.960362434387207
Loss :  1.805212140083313 3.0722267627716064 3.0722267627716064
Loss :  1.8001538515090942 3.2540595531463623 3.2540595531463623
Loss :  1.800370454788208 2.8753576278686523 2.8753576278686523
Loss :  1.7964928150177002 3.1324408054351807 3.1324408054351807
Loss :  1.797408103942871 3.383702516555786 3.383702516555786
Loss :  1.8077232837677002 3.6396539211273193 3.6396539211273193
Loss :  1.802268385887146 3.1758694648742676 3.1758694648742676
Loss :  1.8069795370101929 3.380483865737915 3.380483865737915
Loss :  1.8061590194702148 2.9392404556274414 2.9392404556274414
Loss :  1.793534278869629 2.7719690799713135 2.7719690799713135
Loss :  1.8083187341690063 3.0853989124298096 3.0853989124298096
Loss :  1.805594801902771 3.212489366531372 3.212489366531372
Loss :  1.801834225654602 2.8492062091827393 2.8492062091827393
Loss :  1.8020426034927368 3.0210232734680176 3.0210232734680176
Loss :  1.7962970733642578 3.232583522796631 3.232583522796631
  batch 20 loss: 1.7962970733642578, 3.232583522796631, 3.232583522796631
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8047218322753906 2.710095167160034 2.710095167160034
Loss :  1.8035459518432617 3.0911812782287598 3.0911812782287598
Loss :  1.796946406364441 3.1503119468688965 3.1503119468688965
Loss :  1.8026024103164673 2.4749767780303955 2.4749767780303955
Loss :  1.8025944232940674 2.751694440841675 2.751694440841675
Loss :  1.8030064105987549 2.50758695602417 2.50758695602417
Loss :  1.807454228401184 2.7345364093780518 2.7345364093780518
Loss :  1.7992141246795654 3.0516750812530518 3.0516750812530518
Loss :  1.811946988105774 3.1227076053619385 3.1227076053619385
Loss :  1.7986656427383423 3.2347495555877686 3.2347495555877686
Loss :  1.8063098192214966 3.0893476009368896 3.0893476009368896
Loss :  1.7954851388931274 3.457185983657837 3.457185983657837
Loss :  1.8052186965942383 3.626823902130127 3.626823902130127
Loss :  1.8043959140777588 3.063666343688965 3.063666343688965
Loss :  1.8043396472930908 3.088775873184204 3.088775873184204
Loss :  1.8069193363189697 3.2758219242095947 3.2758219242095947
Loss :  1.803309679031372 3.434953451156616 3.434953451156616
Loss :  1.7986594438552856 3.2638955116271973 3.2638955116271973
Loss :  1.8009083271026611 3.837462902069092 3.837462902069092
Loss :  1.7978744506835938 2.639415740966797 2.639415740966797
  batch 40 loss: 1.7978744506835938, 2.639415740966797, 2.639415740966797
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.802822232246399 3.5940780639648438 3.5940780639648438
Loss :  1.8027149438858032 2.6770124435424805 2.6770124435424805
Loss :  1.801796793937683 2.770615816116333 2.770615816116333
Loss :  1.7996644973754883 2.600057363510132 2.600057363510132
Loss :  1.804374098777771 2.82582950592041 2.82582950592041
Loss :  1.8024636507034302 3.2787857055664062 3.2787857055664062
Loss :  1.7993988990783691 3.5068166255950928 3.5068166255950928
Loss :  1.8009339570999146 3.448631763458252 3.448631763458252
Loss :  1.7966740131378174 3.871495485305786 3.871495485305786
Loss :  1.8004610538482666 3.647838592529297 3.647838592529297
Loss :  1.7945833206176758 3.8850677013397217 3.8850677013397217
Loss :  1.8017033338546753 3.577843427658081 3.577843427658081
Loss :  1.801975965499878 3.4971702098846436 3.4971702098846436
Loss :  1.8055371046066284 3.1190149784088135 3.1190149784088135
Loss :  1.7994143962860107 3.3040685653686523 3.3040685653686523
Loss :  1.8027585744857788 3.0357120037078857 3.0357120037078857
Loss :  1.80690336227417 3.553478479385376 3.553478479385376
Loss :  1.8058393001556396 3.818387031555176 3.818387031555176
Loss :  1.8094183206558228 3.825204610824585 3.825204610824585
Loss :  1.799724817276001 3.9499330520629883 3.9499330520629883
  batch 60 loss: 1.799724817276001, 3.9499330520629883, 3.9499330520629883
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8068119287490845 3.7681069374084473 3.7681069374084473
Loss :  1.799561619758606 3.651278495788574 3.651278495788574
Loss :  1.8046796321868896 3.6242785453796387 3.6242785453796387
Loss :  1.800842046737671 3.203331708908081 3.203331708908081
Loss :  1.8055000305175781 3.0770263671875 3.0770263671875
Loss :  1.7090781927108765 4.4708476066589355 4.4708476066589355
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7103458642959595 4.437844753265381 4.437844753265381
Loss :  1.706800937652588 4.383871078491211 4.383871078491211
Loss :  1.7356584072113037 4.2045087814331055 4.2045087814331055
Total LOSS train 3.1920339584350588 valid 4.374268054962158
CE LOSS train 1.8022680355952336 valid 0.4339146018028259
Contrastive LOSS train 3.1920339584350588 valid 1.0511271953582764
EPOCH 272:
Loss :  1.80140221118927 3.034958600997925 3.034958600997925
Loss :  1.7992160320281982 3.476922035217285 3.476922035217285
Loss :  1.8019187450408936 2.777320146560669 2.777320146560669
Loss :  1.8023782968521118 2.9777536392211914 2.9777536392211914
Loss :  1.8008432388305664 3.111236095428467 3.111236095428467
Loss :  1.8050960302352905 2.6638453006744385 2.6638453006744385
Loss :  1.8000730276107788 3.2398011684417725 3.2398011684417725
Loss :  1.8003036975860596 3.6242258548736572 3.6242258548736572
Loss :  1.7963546514511108 2.9088521003723145 2.9088521003723145
Loss :  1.7972760200500488 3.228656768798828 3.228656768798828
Loss :  1.8077043294906616 3.108867645263672 3.108867645263672
Loss :  1.8022212982177734 3.199615478515625 3.199615478515625
Loss :  1.8070162534713745 3.350144624710083 3.350144624710083
Loss :  1.8061081171035767 3.229462146759033 3.229462146759033
Loss :  1.7933247089385986 3.0496292114257812 3.0496292114257812
Loss :  1.8082544803619385 3.285921573638916 3.285921573638916
Loss :  1.805586338043213 3.2304723262786865 3.2304723262786865
Loss :  1.8018109798431396 3.0760562419891357 3.0760562419891357
Loss :  1.8019452095031738 3.219468355178833 3.219468355178833
Loss :  1.7962795495986938 3.007227659225464 3.007227659225464
  batch 20 loss: 1.7962795495986938, 3.007227659225464, 3.007227659225464
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8046536445617676 3.321031332015991 3.321031332015991
Loss :  1.803488850593567 3.192143678665161 3.192143678665161
Loss :  1.7968642711639404 3.2788000106811523 3.2788000106811523
Loss :  1.8024990558624268 3.1085636615753174 3.1085636615753174
Loss :  1.8023791313171387 3.2633137702941895 3.2633137702941895
Loss :  1.8029870986938477 3.424895763397217 3.424895763397217
Loss :  1.8075008392333984 3.3127028942108154 3.3127028942108154
Loss :  1.7992534637451172 3.1207351684570312 3.1207351684570312
Loss :  1.8121237754821777 3.225794553756714 3.225794553756714
Loss :  1.7985663414001465 2.996990203857422 2.996990203857422
Loss :  1.806261658668518 3.111664295196533 3.111664295196533
Loss :  1.7954776287078857 2.954533815383911 2.954533815383911
Loss :  1.8052886724472046 3.049121856689453 3.049121856689453
Loss :  1.8044211864471436 3.0240166187286377 3.0240166187286377
Loss :  1.8042761087417603 3.1391687393188477 3.1391687393188477
Loss :  1.8070192337036133 2.7982089519500732 2.7982089519500732
Loss :  1.8033044338226318 3.3423068523406982 3.3423068523406982
Loss :  1.7988091707229614 2.773920774459839 2.773920774459839
Loss :  1.800844430923462 3.0890660285949707 3.0890660285949707
Loss :  1.7978569269180298 3.102614164352417 3.102614164352417
  batch 40 loss: 1.7978569269180298, 3.102614164352417, 3.102614164352417
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8028026819229126 3.874969482421875 3.874969482421875
Loss :  1.8027993440628052 3.312774181365967 3.312774181365967
Loss :  1.8016839027404785 2.9718568325042725 2.9718568325042725
Loss :  1.7996865510940552 2.937601327896118 2.937601327896118
Loss :  1.8043378591537476 3.0975964069366455 3.0975964069366455
Loss :  1.802582859992981 3.5866200923919678 3.5866200923919678
Loss :  1.7994823455810547 3.6753768920898438 3.6753768920898438
Loss :  1.800883412361145 3.3269200325012207 3.3269200325012207
Loss :  1.7968591451644897 3.379863977432251 3.379863977432251
Loss :  1.8004443645477295 3.3007726669311523 3.3007726669311523
Loss :  1.7944567203521729 3.145012617111206 3.145012617111206
Loss :  1.801695466041565 3.196532726287842 3.196532726287842
Loss :  1.8020402193069458 3.0775957107543945 3.0775957107543945
Loss :  1.8055212497711182 3.2857754230499268 3.2857754230499268
Loss :  1.7992693185806274 3.586787700653076 3.586787700653076
Loss :  1.802767276763916 2.616955280303955 2.616955280303955
Loss :  1.8069958686828613 2.945281505584717 2.945281505584717
Loss :  1.8058775663375854 2.8391385078430176 2.8391385078430176
Loss :  1.8094532489776611 2.6569607257843018 2.6569607257843018
Loss :  1.7996827363967896 2.6548080444335938 2.6548080444335938
  batch 60 loss: 1.7996827363967896, 2.6548080444335938, 2.6548080444335938
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8069771528244019 2.77254056930542 2.77254056930542
Loss :  1.7995725870132446 2.3133482933044434 2.3133482933044434
Loss :  1.8047908544540405 2.393550395965576 2.393550395965576
Loss :  1.8009757995605469 2.5725319385528564 2.5725319385528564
Loss :  1.805680274963379 2.109140634536743 2.109140634536743
Loss :  1.7082558870315552 4.409408092498779 4.409408092498779
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7095247507095337 4.4125285148620605 4.4125285148620605
Loss :  1.706034779548645 4.216375350952148 4.216375350952148
Loss :  1.7349168062210083 4.022506237030029 4.022506237030029
Total LOSS train 3.0932360318990852 valid 4.265204548835754
CE LOSS train 1.8022508914654072 valid 0.4337292015552521
Contrastive LOSS train 3.0932360318990852 valid 1.0056265592575073
EPOCH 273:
Loss :  1.801477074623108 2.3871350288391113 2.3871350288391113
Loss :  1.799422025680542 2.769979238510132 2.769979238510132
Loss :  1.802128553390503 2.886000394821167 2.886000394821167
Loss :  1.8024845123291016 2.8685014247894287 2.8685014247894287
Loss :  1.8009719848632812 3.0764129161834717 3.0764129161834717
Loss :  1.805235743522644 2.982078790664673 2.982078790664673
Loss :  1.8002973794937134 2.8701014518737793 2.8701014518737793
Loss :  1.8005419969558716 2.8147616386413574 2.8147616386413574
Loss :  1.7964909076690674 2.651994466781616 2.651994466781616
Loss :  1.7976107597351074 3.0105373859405518 3.0105373859405518
Loss :  1.8079140186309814 3.444838047027588 3.444838047027588
Loss :  1.8023228645324707 3.223140239715576 3.223140239715576
Loss :  1.8071892261505127 3.192528009414673 3.192528009414673
Loss :  1.8062529563903809 3.0020499229431152 3.0020499229431152
Loss :  1.793421745300293 2.7414956092834473 2.7414956092834473
Loss :  1.808444619178772 3.7911791801452637 3.7911791801452637
Loss :  1.8057291507720947 3.577470541000366 3.577470541000366
Loss :  1.8019815683364868 3.541496753692627 3.541496753692627
Loss :  1.802077293395996 3.6194140911102295 3.6194140911102295
Loss :  1.7963930368423462 3.6979331970214844 3.6979331970214844
  batch 20 loss: 1.7963930368423462, 3.6979331970214844, 3.6979331970214844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.804755687713623 3.349515199661255 3.349515199661255
Loss :  1.803555965423584 3.362544059753418 3.362544059753418
Loss :  1.7969520092010498 3.359534740447998 3.359534740447998
Loss :  1.8025225400924683 2.661007881164551 2.661007881164551
Loss :  1.802449345588684 3.190317392349243 3.190317392349243
Loss :  1.8030917644500732 2.7272789478302 2.7272789478302
Loss :  1.8075557947158813 2.873337507247925 2.873337507247925
Loss :  1.7993327379226685 2.7520649433135986 2.7520649433135986
Loss :  1.8121840953826904 2.677006959915161 2.677006959915161
Loss :  1.798643708229065 2.8218607902526855 2.8218607902526855
Loss :  1.8063546419143677 3.264892339706421 3.264892339706421
Loss :  1.7955235242843628 3.2350573539733887 3.2350573539733887
Loss :  1.8053427934646606 2.9085533618927 2.9085533618927
Loss :  1.804547905921936 3.72336483001709 3.72336483001709
Loss :  1.8043615818023682 3.3248164653778076 3.3248164653778076
Loss :  1.807067632675171 3.5230705738067627 3.5230705738067627
Loss :  1.8033736944198608 3.4315505027770996 3.4315505027770996
Loss :  1.7987903356552124 2.6039412021636963 2.6039412021636963
Loss :  1.800925374031067 3.1150994300842285 3.1150994300842285
Loss :  1.7979400157928467 2.4083092212677 2.4083092212677
  batch 40 loss: 1.7979400157928467, 2.4083092212677, 2.4083092212677
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8028379678726196 3.323495626449585 3.323495626449585
Loss :  1.8028780221939087 2.9643819332122803 2.9643819332122803
Loss :  1.801847219467163 3.3511364459991455 3.3511364459991455
Loss :  1.7997621297836304 2.9194350242614746 2.9194350242614746
Loss :  1.8044078350067139 3.1716482639312744 3.1716482639312744
Loss :  1.8025718927383423 3.3405611515045166 3.3405611515045166
Loss :  1.7994768619537354 3.01545786857605 3.01545786857605
Loss :  1.8009722232818604 3.1474881172180176 3.1474881172180176
Loss :  1.796805500984192 3.3954319953918457 3.3954319953918457
Loss :  1.8005377054214478 3.148775339126587 3.148775339126587
Loss :  1.794621467590332 2.93727445602417 2.93727445602417
Loss :  1.8017421960830688 2.8097524642944336 2.8097524642944336
Loss :  1.8020226955413818 3.0272679328918457 3.0272679328918457
Loss :  1.805590033531189 2.61922025680542 2.61922025680542
Loss :  1.7994492053985596 2.952469825744629 2.952469825744629
Loss :  1.8027812242507935 3.4685919284820557 3.4685919284820557
Loss :  1.8069677352905273 3.651427745819092 3.651427745819092
Loss :  1.805940866470337 3.395195245742798 3.395195245742798
Loss :  1.8094298839569092 3.4737298488616943 3.4737298488616943
Loss :  1.7997069358825684 2.815664052963257 2.815664052963257
  batch 60 loss: 1.7997069358825684, 2.815664052963257, 2.815664052963257
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8068686723709106 3.37320613861084 3.37320613861084
Loss :  1.7996021509170532 2.9351911544799805 2.9351911544799805
Loss :  1.804715633392334 2.56252121925354 2.56252121925354
Loss :  1.800927996635437 2.7312211990356445 2.7312211990356445
Loss :  1.8056755065917969 2.014988422393799 2.014988422393799
Loss :  1.7080345153808594 4.4383673667907715 4.4383673667907715
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7093098163604736 4.377278804779053 4.377278804779053
Loss :  1.7058446407318115 4.2964630126953125 4.2964630126953125
Loss :  1.7347033023834229 3.9904134273529053 3.9904134273529053
Total LOSS train 3.0770416259765625 valid 4.2756306529045105
CE LOSS train 1.8023353558320265 valid 0.4336758255958557
Contrastive LOSS train 3.0770416259765625 valid 0.9976033568382263
EPOCH 274:
Loss :  1.8014713525772095 2.4544081687927246 2.4544081687927246
Loss :  1.799225926399231 2.7316787242889404 2.7316787242889404
Loss :  1.80205500125885 2.695190668106079 2.695190668106079
Loss :  1.8024624586105347 2.8584835529327393 2.8584835529327393
Loss :  1.8008958101272583 2.5382776260375977 2.5382776260375977
Loss :  1.8051453828811646 2.7991111278533936 2.7991111278533936
Loss :  1.8001070022583008 3.45889949798584 3.45889949798584
Loss :  1.8003541231155396 4.186959743499756 4.186959743499756
Loss :  1.7964361906051636 3.6453921794891357 3.6453921794891357
Loss :  1.7974140644073486 3.4306752681732178 3.4306752681732178
Loss :  1.8077328205108643 3.1720869541168213 3.1720869541168213
Loss :  1.8022513389587402 3.4427618980407715 3.4427618980407715
Loss :  1.807004451751709 3.3927812576293945 3.3927812576293945
Loss :  1.8061590194702148 3.6476728916168213 3.6476728916168213
Loss :  1.7933965921401978 3.424323558807373 3.424323558807373
Loss :  1.8082629442214966 3.747035026550293 3.747035026550293
Loss :  1.8056089878082275 3.2072017192840576 3.2072017192840576
Loss :  1.8018423318862915 3.3877463340759277 3.3877463340759277
Loss :  1.8020638227462769 2.889049530029297 2.889049530029297
Loss :  1.7962647676467896 3.173884391784668 3.173884391784668
  batch 20 loss: 1.7962647676467896, 3.173884391784668, 3.173884391784668
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.80472731590271 2.847492218017578 2.847492218017578
Loss :  1.8035379648208618 2.5570154190063477 2.5570154190063477
Loss :  1.7969228029251099 2.7743172645568848 2.7743172645568848
Loss :  1.8025197982788086 2.686983823776245 2.686983823776245
Loss :  1.8024996519088745 3.0175604820251465 3.0175604820251465
Loss :  1.803005337715149 2.347198247909546 2.347198247909546
Loss :  1.8074647188186646 2.7619333267211914 2.7619333267211914
Loss :  1.7992308139801025 2.3880815505981445 2.3880815505981445
Loss :  1.8120620250701904 2.6531975269317627 2.6531975269317627
Loss :  1.7986376285552979 3.054948091506958 3.054948091506958
Loss :  1.8063732385635376 3.113600730895996 3.113600730895996
Loss :  1.7955037355422974 3.2457971572875977 3.2457971572875977
Loss :  1.8052849769592285 2.9895989894866943 2.9895989894866943
Loss :  1.8045138120651245 2.7886769771575928 2.7886769771575928
Loss :  1.8043758869171143 2.9998037815093994 2.9998037815093994
Loss :  1.8070534467697144 2.8618645668029785 2.8618645668029785
Loss :  1.8033740520477295 3.309401512145996 3.309401512145996
Loss :  1.7987414598464966 2.6201469898223877 2.6201469898223877
Loss :  1.800931692123413 2.928532361984253 2.928532361984253
Loss :  1.7979429960250854 3.0278143882751465 3.0278143882751465
  batch 40 loss: 1.7979429960250854, 3.0278143882751465, 3.0278143882751465
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8028545379638672 3.5931029319763184 3.5931029319763184
Loss :  1.802913784980774 3.3503406047821045 3.3503406047821045
Loss :  1.8018947839736938 3.1996960639953613 3.1996960639953613
Loss :  1.7998021841049194 3.6916592121124268 3.6916592121124268
Loss :  1.804439663887024 3.289938449859619 3.289938449859619
Loss :  1.8026070594787598 3.379251718521118 3.379251718521118
Loss :  1.7994827032089233 3.6858084201812744 3.6858084201812744
Loss :  1.8010039329528809 3.6507444381713867 3.6507444381713867
Loss :  1.796829104423523 3.5401899814605713 3.5401899814605713
Loss :  1.800586462020874 3.322847843170166 3.322847843170166
Loss :  1.7946631908416748 3.0855536460876465 3.0855536460876465
Loss :  1.8017725944519043 3.6926140785217285 3.6926140785217285
Loss :  1.8020700216293335 3.956892967224121 3.956892967224121
Loss :  1.8056856393814087 3.6054704189300537 3.6054704189300537
Loss :  1.7994940280914307 2.8918070793151855 2.8918070793151855
Loss :  1.8028151988983154 2.643110513687134 2.643110513687134
Loss :  1.8070614337921143 3.2826919555664062 3.2826919555664062
Loss :  1.8060320615768433 3.161090135574341 3.161090135574341
Loss :  1.809510588645935 2.9672796726226807 2.9672796726226807
Loss :  1.7997164726257324 3.076723575592041 3.076723575592041
  batch 60 loss: 1.7997164726257324, 3.076723575592041, 3.076723575592041
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8069744110107422 2.8933770656585693 2.8933770656585693
Loss :  1.7996628284454346 2.794980764389038 2.794980764389038
Loss :  1.8047735691070557 2.8415918350219727 2.8415918350219727
Loss :  1.8010200262069702 2.87910795211792 2.87910795211792
Loss :  1.8058234453201294 2.933881998062134 2.933881998062134
Loss :  1.707053780555725 4.394345283508301 4.394345283508301
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7083113193511963 4.3640241622924805 4.3640241622924805
Loss :  1.7049480676651 4.236172199249268 4.236172199249268
Loss :  1.7338933944702148 4.301308631896973 4.301308631896973
Total LOSS train 3.117959059201754 valid 4.323962569236755
CE LOSS train 1.8023130380190335 valid 0.4334733486175537
Contrastive LOSS train 3.117959059201754 valid 1.0753271579742432
EPOCH 275:
Loss :  1.801547646522522 3.0886998176574707 3.0886998176574707
Loss :  1.7992990016937256 3.125656843185425 3.125656843185425
Loss :  1.8021546602249146 2.907794237136841 2.907794237136841
Loss :  1.802539587020874 2.9665634632110596 2.9665634632110596
Loss :  1.8009690046310425 2.802342176437378 2.802342176437378
Loss :  1.8052451610565186 3.11012601852417 3.11012601852417
Loss :  1.8002382516860962 3.5270659923553467 3.5270659923553467
Loss :  1.8005343675613403 3.3957676887512207 3.3957676887512207
Loss :  1.7965009212493896 3.05549693107605 3.05549693107605
Loss :  1.79764986038208 2.909334659576416 2.909334659576416
Loss :  1.8079217672348022 2.9201860427856445 2.9201860427856445
Loss :  1.8023548126220703 2.8628077507019043 2.8628077507019043
Loss :  1.8071770668029785 3.5205800533294678 3.5205800533294678
Loss :  1.8062684535980225 3.3212101459503174 3.3212101459503174
Loss :  1.7934385538101196 3.4024083614349365 3.4024083614349365
Loss :  1.8084772825241089 3.7686729431152344 3.7686729431152344
Loss :  1.80575430393219 2.73471999168396 2.73471999168396
Loss :  1.8020241260528564 3.2671430110931396 3.2671430110931396
Loss :  1.8021793365478516 2.5411946773529053 2.5411946773529053
Loss :  1.796397089958191 2.8151609897613525 2.8151609897613525
  batch 20 loss: 1.796397089958191, 2.8151609897613525, 2.8151609897613525
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8047986030578613 2.6762869358062744 2.6762869358062744
Loss :  1.8035968542099 3.125509738922119 3.125509738922119
Loss :  1.7969805002212524 3.202679395675659 3.202679395675659
Loss :  1.8025356531143188 2.502835988998413 2.502835988998413
Loss :  1.8025203943252563 3.4120030403137207 3.4120030403137207
Loss :  1.8031240701675415 2.5138423442840576 2.5138423442840576
Loss :  1.807569146156311 2.7275426387786865 2.7275426387786865
Loss :  1.7993448972702026 3.073146104812622 3.073146104812622
Loss :  1.8122214078903198 3.0135791301727295 3.0135791301727295
Loss :  1.7987139225006104 3.0883378982543945 3.0883378982543945
Loss :  1.8064680099487305 3.2665421962738037 3.2665421962738037
Loss :  1.7955830097198486 3.2433130741119385 3.2433130741119385
Loss :  1.8054229021072388 3.209918737411499 3.209918737411499
Loss :  1.8047047853469849 3.000359535217285 3.000359535217285
Loss :  1.8044573068618774 3.184420347213745 3.184420347213745
Loss :  1.8072212934494019 3.22371768951416 3.22371768951416
Loss :  1.8034790754318237 3.4345011711120605 3.4345011711120605
Loss :  1.7988883256912231 3.2270455360412598 3.2270455360412598
Loss :  1.801016092300415 3.2783257961273193 3.2783257961273193
Loss :  1.7980687618255615 3.3457679748535156 3.3457679748535156
  batch 40 loss: 1.7980687618255615, 3.3457679748535156, 3.3457679748535156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8029581308364868 4.184922695159912 4.184922695159912
Loss :  1.8031376600265503 3.860992670059204 3.860992670059204
Loss :  1.8020081520080566 3.474276542663574 3.474276542663574
Loss :  1.7999827861785889 3.163308620452881 3.163308620452881
Loss :  1.8045341968536377 3.1102373600006104 3.1102373600006104
Loss :  1.8027907609939575 3.4018023014068604 3.4018023014068604
Loss :  1.7996275424957275 3.3362386226654053 3.3362386226654053
Loss :  1.801135778427124 2.9371376037597656 2.9371376037597656
Loss :  1.797008752822876 3.300558567047119 3.300558567047119
Loss :  1.8007221221923828 3.0056703090667725 3.0056703090667725
Loss :  1.7947821617126465 3.020005702972412 3.020005702972412
Loss :  1.8018685579299927 3.251570224761963 3.251570224761963
Loss :  1.8021841049194336 3.550391674041748 3.550391674041748
Loss :  1.8058418035507202 2.5894277095794678 2.5894277095794678
Loss :  1.7995309829711914 3.1096627712249756 3.1096627712249756
Loss :  1.8028557300567627 2.477065324783325 2.477065324783325
Loss :  1.8071701526641846 3.2411043643951416 3.2411043643951416
Loss :  1.8061248064041138 3.019545555114746 3.019545555114746
Loss :  1.8095742464065552 3.372828960418701 3.372828960418701
Loss :  1.799667239189148 3.0333800315856934 3.0333800315856934
  batch 60 loss: 1.799667239189148, 3.0333800315856934, 3.0333800315856934
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8071006536483765 2.9129011631011963 2.9129011631011963
Loss :  1.799687385559082 3.3080637454986572 3.3080637454986572
Loss :  1.8048362731933594 3.148239850997925 3.148239850997925
Loss :  1.801090955734253 3.117039918899536 3.117039918899536
Loss :  1.8059062957763672 3.3144991397857666 3.3144991397857666
Loss :  1.706525444984436 4.42750883102417 4.42750883102417
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7077840566635132 4.41053581237793 4.41053581237793
Loss :  1.7044655084609985 4.331634044647217 4.331634044647217
Loss :  1.7332980632781982 4.272729873657227 4.272729873657227
Total LOSS train 3.1389765922839823 valid 4.360602140426636
CE LOSS train 1.8024232846039991 valid 0.43332451581954956
Contrastive LOSS train 3.1389765922839823 valid 1.0681824684143066
EPOCH 276:
Loss :  1.8015629053115845 3.2012503147125244 3.2012503147125244
Loss :  1.7994039058685303 3.8601510524749756 3.8601510524749756
Loss :  1.8022334575653076 3.3065075874328613 3.3065075874328613
Loss :  1.802549123764038 2.7216298580169678 2.7216298580169678
Loss :  1.801004409790039 2.67851185798645 2.67851185798645
Loss :  1.8052773475646973 2.9957127571105957 2.9957127571105957
Loss :  1.800310730934143 3.008383274078369 3.008383274078369
Loss :  1.8006136417388916 3.3224711418151855 3.3224711418151855
Loss :  1.7964946031570435 3.231646776199341 3.231646776199341
Loss :  1.7977358102798462 2.9205052852630615 2.9205052852630615
Loss :  1.8080201148986816 3.235335350036621 3.235335350036621
Loss :  1.8023790121078491 2.9878692626953125 2.9878692626953125
Loss :  1.8072926998138428 2.74041748046875 2.74041748046875
Loss :  1.8063416481018066 2.9078786373138428 2.9078786373138428
Loss :  1.7934101819992065 3.913254499435425 3.913254499435425
Loss :  1.8085854053497314 3.7289326190948486 3.7289326190948486
Loss :  1.8058557510375977 3.1050524711608887 3.1050524711608887
Loss :  1.8021376132965088 3.2509024143218994 3.2509024143218994
Loss :  1.802220106124878 2.8422343730926514 2.8422343730926514
Loss :  1.7964956760406494 2.9420251846313477 2.9420251846313477
  batch 20 loss: 1.7964956760406494, 2.9420251846313477, 2.9420251846313477
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8048396110534668 3.1303019523620605 3.1303019523620605
Loss :  1.803628921508789 3.506033420562744 3.506033420562744
Loss :  1.797003149986267 2.842503070831299 2.842503070831299
Loss :  1.8025492429733276 2.725158929824829 2.725158929824829
Loss :  1.8024786710739136 3.2542521953582764 3.2542521953582764
Loss :  1.803181529045105 2.4246175289154053 2.4246175289154053
Loss :  1.8076571226119995 2.847759246826172 2.847759246826172
Loss :  1.7994420528411865 3.6311545372009277 3.6311545372009277
Loss :  1.8123774528503418 3.794732093811035 3.794732093811035
Loss :  1.7986998558044434 3.6689465045928955 3.6689465045928955
Loss :  1.8064745664596558 3.30580997467041 3.30580997467041
Loss :  1.7956424951553345 2.9508371353149414 2.9508371353149414
Loss :  1.8054850101470947 2.5651628971099854 2.5651628971099854
Loss :  1.8047528266906738 2.800910711288452 2.800910711288452
Loss :  1.8044549226760864 3.1442933082580566 3.1442933082580566
Loss :  1.8073031902313232 3.0028419494628906 3.0028419494628906
Loss :  1.8035141229629517 3.0128965377807617 3.0128965377807617
Loss :  1.7990169525146484 2.7626280784606934 2.7626280784606934
Loss :  1.801027536392212 3.2596185207366943 3.2596185207366943
Loss :  1.7981213331222534 3.3734545707702637 3.3734545707702637
  batch 40 loss: 1.7981213331222534, 3.3734545707702637, 3.3734545707702637
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8030136823654175 3.454632520675659 3.454632520675659
Loss :  1.8031870126724243 2.9209606647491455 2.9209606647491455
Loss :  1.8019938468933105 2.6726088523864746 2.6726088523864746
Loss :  1.800000548362732 2.4787564277648926 2.4787564277648926
Loss :  1.8045521974563599 2.4773318767547607 2.4773318767547607
Loss :  1.8028748035430908 2.8456246852874756 2.8456246852874756
Loss :  1.7996854782104492 2.6899123191833496 2.6899123191833496
Loss :  1.8011332750320435 2.6261229515075684 2.6261229515075684
Loss :  1.7971030473709106 2.860142946243286 2.860142946243286
Loss :  1.800696849822998 2.72859525680542 2.72859525680542
Loss :  1.7947561740875244 2.8606414794921875 2.8606414794921875
Loss :  1.80190110206604 2.743507146835327 2.743507146835327
Loss :  1.8022122383117676 2.663010597229004 2.663010597229004
Loss :  1.805794358253479 2.8019564151763916 2.8019564151763916
Loss :  1.7995010614395142 3.04370379447937 3.04370379447937
Loss :  1.8028488159179688 2.9528005123138428 2.9528005123138428
Loss :  1.8071619272232056 3.3624703884124756 3.3624703884124756
Loss :  1.806139588356018 3.281964063644409 3.281964063644409
Loss :  1.8095735311508179 3.3877687454223633 3.3877687454223633
Loss :  1.7996619939804077 3.074859857559204 3.074859857559204
  batch 60 loss: 1.7996619939804077, 3.074859857559204, 3.074859857559204
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8070818185806274 3.366137742996216 3.366137742996216
Loss :  1.7996668815612793 3.0374386310577393 3.0374386310577393
Loss :  1.8048362731933594 3.1710829734802246 3.1710829734802246
Loss :  1.8010691404342651 2.7846381664276123 2.7846381664276123
Loss :  1.8058644533157349 3.1391239166259766 3.1391239166259766
Loss :  1.7068545818328857 4.40054988861084 4.40054988861084
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7081373929977417 4.467990398406982 4.467990398406982
Loss :  1.7047717571258545 4.329205513000488 4.329205513000488
Loss :  1.733521819114685 4.110808372497559 4.110808372497559
Total LOSS train 3.0512365891383246 valid 4.327138543128967
CE LOSS train 1.8024597663145798 valid 0.43338045477867126
Contrastive LOSS train 3.0512365891383246 valid 1.0277020931243896
EPOCH 277:
Loss :  1.8015239238739014 2.756587505340576 2.756587505340576
Loss :  1.7993916273117065 2.748110294342041 2.748110294342041
Loss :  1.8021926879882812 2.886174201965332 2.886174201965332
Loss :  1.8025059700012207 3.097151041030884 3.097151041030884
Loss :  1.8009732961654663 2.8932044506073 2.8932044506073
Loss :  1.8052414655685425 2.9941983222961426 2.9941983222961426
Loss :  1.800266146659851 3.177057981491089 3.177057981491089
Loss :  1.800584077835083 2.871570348739624 2.871570348739624
Loss :  1.796435832977295 3.2475922107696533 3.2475922107696533
Loss :  1.7976667881011963 3.314265251159668 3.314265251159668
Loss :  1.8079744577407837 3.628432273864746 3.628432273864746
Loss :  1.8023213148117065 3.33534836769104 3.33534836769104
Loss :  1.8072562217712402 3.5247292518615723 3.5247292518615723
Loss :  1.8063230514526367 3.368807792663574 3.368807792663574
Loss :  1.7933388948440552 3.441192626953125 3.441192626953125
Loss :  1.808514952659607 3.693891763687134 3.693891763687134
Loss :  1.8058156967163086 3.3318569660186768 3.3318569660186768
Loss :  1.8020358085632324 3.1495511531829834 3.1495511531829834
Loss :  1.8021533489227295 3.3742551803588867 3.3742551803588867
Loss :  1.796406865119934 3.350520372390747 3.350520372390747
  batch 20 loss: 1.796406865119934, 3.350520372390747, 3.350520372390747
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.804764986038208 3.293823480606079 3.293823480606079
Loss :  1.803574800491333 3.2488348484039307 3.2488348484039307
Loss :  1.7969064712524414 3.5171287059783936 3.5171287059783936
Loss :  1.8024924993515015 3.5589816570281982 3.5589816570281982
Loss :  1.8023782968521118 3.251316785812378 3.251316785812378
Loss :  1.8030952215194702 3.0413758754730225 3.0413758754730225
Loss :  1.8076251745224 3.1528966426849365 3.1528966426849365
Loss :  1.7994030714035034 2.9157419204711914 2.9157419204711914
Loss :  1.8123588562011719 2.6268434524536133 2.6268434524536133
Loss :  1.7986747026443481 2.9551291465759277 2.9551291465759277
Loss :  1.806419849395752 3.367058038711548 3.367058038711548
Loss :  1.7956231832504272 2.6680872440338135 2.6680872440338135
Loss :  1.805460810661316 2.623427629470825 2.623427629470825
Loss :  1.8047230243682861 2.704179525375366 2.704179525375366
Loss :  1.8044229745864868 2.8294641971588135 2.8294641971588135
Loss :  1.807225227355957 2.887181043624878 2.887181043624878
Loss :  1.8034677505493164 2.9369304180145264 2.9369304180145264
Loss :  1.7989702224731445 2.8244335651397705 2.8244335651397705
Loss :  1.8010081052780151 3.140082597732544 3.140082597732544
Loss :  1.79806387424469 2.5630061626434326 2.5630061626434326
  batch 40 loss: 1.79806387424469, 2.5630061626434326, 2.5630061626434326
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8029381036758423 2.6893157958984375 2.6893157958984375
Loss :  1.803064227104187 2.626838445663452 2.626838445663452
Loss :  1.8018878698349 2.6272616386413574 2.6272616386413574
Loss :  1.7998805046081543 2.363858938217163 2.363858938217163
Loss :  1.804448127746582 2.829664945602417 2.829664945602417
Loss :  1.8027458190917969 3.2981889247894287 3.2981889247894287
Loss :  1.7995661497116089 3.0527186393737793 3.0527186393737793
Loss :  1.8010151386260986 2.8872084617614746 2.8872084617614746
Loss :  1.796947956085205 2.8170788288116455 2.8170788288116455
Loss :  1.8005707263946533 3.2542097568511963 3.2542097568511963
Loss :  1.7946239709854126 3.2935643196105957 3.2935643196105957
Loss :  1.8017717599868774 3.6470494270324707 3.6470494270324707
Loss :  1.8020938634872437 2.5985894203186035 2.5985894203186035
Loss :  1.8056344985961914 2.578580141067505 2.578580141067505
Loss :  1.799422264099121 2.7765753269195557 2.7765753269195557
Loss :  1.802794098854065 2.4417569637298584 2.4417569637298584
Loss :  1.8070482015609741 2.8259570598602295 2.8259570598602295
Loss :  1.806026577949524 2.785275936126709 2.785275936126709
Loss :  1.8094879388809204 2.733818769454956 2.733818769454956
Loss :  1.799697756767273 2.746995210647583 2.746995210647583
  batch 60 loss: 1.799697756767273, 2.746995210647583, 2.746995210647583
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.806980848312378 2.6561410427093506 2.6561410427093506
Loss :  1.799622893333435 2.6637425422668457 2.6637425422668457
Loss :  1.8047820329666138 2.290051221847534 2.290051221847534
Loss :  1.8010015487670898 2.6694071292877197 2.6694071292877197
Loss :  1.8057737350463867 2.485962152481079 2.485962152481079
Loss :  1.7073001861572266 4.405462265014648 4.405462265014648
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7085816860198975 4.379831790924072 4.379831790924072
Loss :  1.7051846981048584 4.423170566558838 4.423170566558838
Loss :  1.7340117692947388 4.144996643066406 4.144996643066406
Total LOSS train 2.983542020504291 valid 4.338365316390991
CE LOSS train 1.8023908945230338 valid 0.4335029423236847
Contrastive LOSS train 2.983542020504291 valid 1.0362491607666016
EPOCH 278:
Loss :  1.8014627695083618 2.809272289276123 2.809272289276123
Loss :  1.7993210554122925 3.3973429203033447 3.3973429203033447
Loss :  1.8021183013916016 2.668712615966797 2.668712615966797
Loss :  1.8024420738220215 3.104832172393799 3.104832172393799
Loss :  1.8009033203125 2.73966121673584 2.73966121673584
Loss :  1.805172324180603 2.7338626384735107 2.7338626384735107
Loss :  1.8001656532287598 2.6727347373962402 2.6727347373962402
Loss :  1.800439715385437 2.965224027633667 2.965224027633667
Loss :  1.7963919639587402 3.0985519886016846 3.0985519886016846
Loss :  1.797508716583252 3.015669107437134 3.015669107437134
Loss :  1.8078274726867676 2.9873762130737305 2.9873762130737305
Loss :  1.802243709564209 2.9945931434631348 2.9945931434631348
Loss :  1.8071404695510864 3.249814748764038 3.249814748764038
Loss :  1.8062134981155396 3.3720314502716064 3.3720314502716064
Loss :  1.7933276891708374 3.5421547889709473 3.5421547889709473
Loss :  1.8083336353302002 3.295290231704712 3.295290231704712
Loss :  1.8056833744049072 2.928889513015747 2.928889513015747
Loss :  1.8019051551818848 2.659592866897583 2.659592866897583
Loss :  1.8020702600479126 2.6734378337860107 2.6734378337860107
Loss :  1.7963286638259888 2.8164522647857666 2.8164522647857666
  batch 20 loss: 1.7963286638259888, 2.8164522647857666, 2.8164522647857666
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8047257661819458 2.5688867568969727 2.5688867568969727
Loss :  1.803529977798462 2.7572357654571533 2.7572357654571533
Loss :  1.7969214916229248 2.551548719406128 2.551548719406128
Loss :  1.8025137186050415 2.926295042037964 2.926295042037964
Loss :  1.802433967590332 3.000027894973755 3.000027894973755
Loss :  1.8030308485031128 2.4447104930877686 2.4447104930877686
Loss :  1.8075230121612549 2.7384157180786133 2.7384157180786133
Loss :  1.7993227243423462 2.611598014831543 2.611598014831543
Loss :  1.8121501207351685 2.555816650390625 2.555816650390625
Loss :  1.7986345291137695 2.430845022201538 2.430845022201538
Loss :  1.8063161373138428 2.5729119777679443 2.5729119777679443
Loss :  1.7955597639083862 2.494532823562622 2.494532823562622
Loss :  1.8053452968597412 2.3490993976593018 2.3490993976593018
Loss :  1.8045357465744019 2.4765939712524414 2.4765939712524414
Loss :  1.8043687343597412 2.659416675567627 2.659416675567627
Loss :  1.8070441484451294 2.8983776569366455 2.8983776569366455
Loss :  1.8033970594406128 3.080787420272827 3.080787420272827
Loss :  1.7987948656082153 2.584075689315796 2.584075689315796
Loss :  1.8009405136108398 2.724992275238037 2.724992275238037
Loss :  1.7979329824447632 2.546985626220703 2.546985626220703
  batch 40 loss: 1.7979329824447632, 2.546985626220703, 2.546985626220703
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8028327226638794 3.3180689811706543 3.3180689811706543
Loss :  1.8028725385665894 3.526092767715454 3.526092767715454
Loss :  1.8018262386322021 3.065080165863037 3.065080165863037
Loss :  1.799734354019165 2.608538866043091 2.608538866043091
Loss :  1.804377555847168 2.6952579021453857 2.6952579021453857
Loss :  1.802565574645996 3.108248472213745 3.108248472213745
Loss :  1.7994135618209839 2.6898629665374756 2.6898629665374756
Loss :  1.800954818725586 3.0777227878570557 3.0777227878570557
Loss :  1.7967307567596436 3.0617315769195557 3.0617315769195557
Loss :  1.800478458404541 3.191882848739624 3.191882848739624
Loss :  1.7945994138717651 2.9821324348449707 2.9821324348449707
Loss :  1.8017007112503052 3.792428731918335 3.792428731918335
Loss :  1.8019899129867554 3.4790806770324707 3.4790806770324707
Loss :  1.805524468421936 3.253425121307373 3.253425121307373
Loss :  1.7994519472122192 3.429705858230591 3.429705858230591
Loss :  1.8026888370513916 2.8646509647369385 2.8646509647369385
Loss :  1.8069264888763428 3.1065902709960938 3.1065902709960938
Loss :  1.8059831857681274 3.2396633625030518 3.2396633625030518
Loss :  1.8094338178634644 3.1611053943634033 3.1611053943634033
Loss :  1.7996447086334229 2.839931011199951 2.839931011199951
  batch 60 loss: 1.7996447086334229, 2.839931011199951, 2.839931011199951
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8068729639053345 3.4149105548858643 3.4149105548858643
Loss :  1.7995824813842773 3.2841873168945312 3.2841873168945312
Loss :  1.8047126531600952 4.029542446136475 4.029542446136475
Loss :  1.800929069519043 3.5616917610168457 3.5616917610168457
Loss :  1.8056833744049072 3.0706140995025635 3.0706140995025635
Loss :  1.70767080783844 4.476103782653809 4.476103782653809
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.708959698677063 4.350249290466309 4.350249290466309
Loss :  1.7055509090423584 4.4043498039245605 4.4043498039245605
Loss :  1.7341381311416626 4.287516117095947 4.287516117095947
Total LOSS train 2.962319964628953 valid 4.379554748535156
CE LOSS train 1.8023004898658166 valid 0.43353453278541565
Contrastive LOSS train 2.962319964628953 valid 1.0718790292739868
EPOCH 279:
Loss :  1.801377534866333 3.2782914638519287 3.2782914638519287
Loss :  1.7992475032806396 3.81939697265625 3.81939697265625
Loss :  1.8020343780517578 3.2380220890045166 3.2380220890045166
Loss :  1.802394151687622 3.311102867126465 3.311102867126465
Loss :  1.8008607625961304 3.4917094707489014 3.4917094707489014
Loss :  1.8051347732543945 3.2653303146362305 3.2653303146362305
Loss :  1.8001163005828857 3.3621931076049805 3.3621931076049805
Loss :  1.8003896474838257 3.0499942302703857 3.0499942302703857
Loss :  1.7963448762893677 3.1191446781158447 3.1191446781158447
Loss :  1.7974375486373901 3.0990943908691406 3.0990943908691406
Loss :  1.8077750205993652 3.295459032058716 3.295459032058716
Loss :  1.8021973371505737 3.183698892593384 3.183698892593384
Loss :  1.8071173429489136 2.8491313457489014 2.8491313457489014
Loss :  1.8061919212341309 3.012056827545166 3.012056827545166
Loss :  1.7933274507522583 2.77996826171875 2.77996826171875
Loss :  1.8083178997039795 2.4712142944335938 2.4712142944335938
Loss :  1.8056679964065552 2.6105382442474365 2.6105382442474365
Loss :  1.8018851280212402 2.737116813659668 2.737116813659668
Loss :  1.802039623260498 3.760141134262085 3.760141134262085
Loss :  1.7963457107543945 2.9960153102874756 2.9960153102874756
  batch 20 loss: 1.7963457107543945, 2.9960153102874756, 2.9960153102874756
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8047107458114624 3.1824867725372314 3.1824867725372314
Loss :  1.8035259246826172 2.841740846633911 2.841740846633911
Loss :  1.7969321012496948 2.5245132446289062 2.5245132446289062
Loss :  1.8025423288345337 2.783661365509033 2.783661365509033
Loss :  1.8024412393569946 2.8591737747192383 2.8591737747192383
Loss :  1.8030331134796143 2.3806281089782715 2.3806281089782715
Loss :  1.8075164556503296 3.027697801589966 3.027697801589966
Loss :  1.7992937564849854 3.4304776191711426 3.4304776191711426
Loss :  1.8121446371078491 3.1305699348449707 3.1305699348449707
Loss :  1.7986204624176025 3.465972900390625 3.465972900390625
Loss :  1.806327223777771 3.5828490257263184 3.5828490257263184
Loss :  1.7955290079116821 2.883284330368042 2.883284330368042
Loss :  1.805311679840088 2.882455348968506 2.882455348968506
Loss :  1.804487943649292 2.974630832672119 2.974630832672119
Loss :  1.8043420314788818 2.933302402496338 2.933302402496338
Loss :  1.807020902633667 3.0752503871917725 3.0752503871917725
Loss :  1.803370475769043 3.132716655731201 3.132716655731201
Loss :  1.7987661361694336 3.042051076889038 3.042051076889038
Loss :  1.800929307937622 2.5340139865875244 2.5340139865875244
Loss :  1.7979240417480469 2.7522292137145996 2.7522292137145996
  batch 40 loss: 1.7979240417480469, 2.7522292137145996, 2.7522292137145996
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8028311729431152 3.242018222808838 3.242018222808838
Loss :  1.8028502464294434 3.0291874408721924 3.0291874408721924
Loss :  1.801820158958435 3.032125949859619 3.032125949859619
Loss :  1.7997373342514038 2.7013251781463623 2.7013251781463623
Loss :  1.8043917417526245 2.4392035007476807 2.4392035007476807
Loss :  1.8025712966918945 3.300478935241699 3.300478935241699
Loss :  1.799504280090332 3.0370583534240723 3.0370583534240723
Loss :  1.8009718656539917 3.135021924972534 3.135021924972534
Loss :  1.796797275543213 3.2357068061828613 3.2357068061828613
Loss :  1.8005317449569702 3.1715359687805176 3.1715359687805176
Loss :  1.7946668863296509 2.752120018005371 2.752120018005371
Loss :  1.8017795085906982 3.0789222717285156 3.0789222717285156
Loss :  1.8020071983337402 3.034123182296753 3.034123182296753
Loss :  1.8055942058563232 2.8430392742156982 2.8430392742156982
Loss :  1.7994768619537354 2.7647552490234375 2.7647552490234375
Loss :  1.8028061389923096 2.2778260707855225 2.2778260707855225
Loss :  1.8069463968276978 2.8006982803344727 2.8006982803344727
Loss :  1.805925965309143 2.784491777420044 2.784491777420044
Loss :  1.8094457387924194 3.207481622695923 3.207481622695923
Loss :  1.7997578382492065 2.927279233932495 2.927279233932495
  batch 60 loss: 1.7997578382492065, 2.927279233932495, 2.927279233932495
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8068358898162842 3.0555307865142822 3.0555307865142822
Loss :  1.7995917797088623 2.786728620529175 2.786728620529175
Loss :  1.8046964406967163 2.8865933418273926 2.8865933418273926
Loss :  1.8009076118469238 2.7608606815338135 2.7608606815338135
Loss :  1.8056453466415405 2.618722677230835 2.618722677230835
Loss :  1.7075369358062744 4.402574062347412 4.402574062347412
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7087841033935547 4.4246931076049805 4.4246931076049805
Loss :  1.7054229974746704 4.2826642990112305 4.2826642990112305
Loss :  1.7341135740280151 4.146973133087158 4.146973133087158
Total LOSS train 3.000802472921518 valid 4.314226150512695
CE LOSS train 1.8022932822887714 valid 0.4335283935070038
Contrastive LOSS train 3.000802472921518 valid 1.0367432832717896
EPOCH 280:
Loss :  1.8014172315597534 2.922131299972534 2.922131299972534
Loss :  1.7992075681686401 3.1268935203552246 3.1268935203552246
Loss :  1.802026629447937 2.7126877307891846 2.7126877307891846
Loss :  1.802446961402893 3.2020442485809326 3.2020442485809326
Loss :  1.8008716106414795 3.1123058795928955 3.1123058795928955
Loss :  1.8051478862762451 3.621922492980957 3.621922492980957
Loss :  1.8001011610031128 3.6249542236328125 3.6249542236328125
Loss :  1.8003431558609009 3.5993149280548096 3.5993149280548096
Loss :  1.7964041233062744 3.0936758518218994 3.0936758518218994
Loss :  1.7974237203598022 3.0688982009887695 3.0688982009887695
Loss :  1.8077200651168823 3.264193296432495 3.264193296432495
Loss :  1.8022136688232422 3.2319462299346924 3.2319462299346924
Loss :  1.8070482015609741 3.1339669227600098 3.1339669227600098
Loss :  1.806158423423767 2.8159139156341553 2.8159139156341553
Loss :  1.7934260368347168 2.9127912521362305 2.9127912521362305
Loss :  1.8082689046859741 3.0085673332214355 3.0085673332214355
Loss :  1.8056341409683228 2.7400567531585693 2.7400567531585693
Loss :  1.8018572330474854 3.4120683670043945 3.4120683670043945
Loss :  1.8020719289779663 3.6316330432891846 3.6316330432891846
Loss :  1.796332836151123 3.4328715801239014 3.4328715801239014
  batch 20 loss: 1.796332836151123, 3.4328715801239014, 3.4328715801239014
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8047064542770386 3.060788869857788 3.060788869857788
Loss :  1.8035221099853516 2.8441624641418457 2.8441624641418457
Loss :  1.796915888786316 3.162301778793335 3.162301778793335
Loss :  1.8025259971618652 3.085895538330078 3.085895538330078
Loss :  1.8024052381515503 3.5721991062164307 3.5721991062164307
Loss :  1.8029829263687134 3.2263429164886475 3.2263429164886475
Loss :  1.807456374168396 3.4599616527557373 3.4599616527557373
Loss :  1.7992334365844727 2.7522177696228027 2.7522177696228027
Loss :  1.8121215105056763 2.828552484512329 2.828552484512329
Loss :  1.7985947132110596 3.656785488128662 3.656785488128662
Loss :  1.8063111305236816 3.515347719192505 3.515347719192505
Loss :  1.7955206632614136 3.6275575160980225 3.6275575160980225
Loss :  1.805293321609497 3.352231502532959 3.352231502532959
Loss :  1.8044276237487793 3.189194440841675 3.189194440841675
Loss :  1.8043235540390015 3.5984556674957275 3.5984556674957275
Loss :  1.8069958686828613 3.331825017929077 3.331825017929077
Loss :  1.8033696413040161 3.4266674518585205 3.4266674518585205
Loss :  1.7987645864486694 2.9303667545318604 2.9303667545318604
Loss :  1.8008954524993896 2.976083278656006 2.976083278656006
Loss :  1.7978888750076294 3.4702727794647217 3.4702727794647217
  batch 40 loss: 1.7978888750076294, 3.4702727794647217, 3.4702727794647217
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8027986288070679 3.9115078449249268 3.9115078449249268
Loss :  1.8028113842010498 3.150029420852661 3.150029420852661
Loss :  1.8017590045928955 2.5880630016326904 2.5880630016326904
Loss :  1.799664855003357 2.5595593452453613 2.5595593452453613
Loss :  1.8043631315231323 2.3758108615875244 2.3758108615875244
Loss :  1.8025611639022827 2.885906934738159 2.885906934738159
Loss :  1.7994825839996338 2.7758677005767822 2.7758677005767822
Loss :  1.8008966445922852 2.5172157287597656 2.5172157287597656
Loss :  1.7968244552612305 2.5127055644989014 2.5127055644989014
Loss :  1.800464153289795 2.5926971435546875 2.5926971435546875
Loss :  1.7945623397827148 3.1514830589294434 3.1514830589294434
Loss :  1.8017535209655762 3.1101737022399902 3.1101737022399902
Loss :  1.801969051361084 2.799280881881714 2.799280881881714
Loss :  1.8054438829421997 3.0614500045776367 3.0614500045776367
Loss :  1.7993884086608887 3.0395307540893555 3.0395307540893555
Loss :  1.8027429580688477 2.9991447925567627 2.9991447925567627
Loss :  1.8069018125534058 3.66965913772583 3.66965913772583
Loss :  1.8058810234069824 3.3145828247070312 3.3145828247070312
Loss :  1.80937922000885 2.78470778465271 2.78470778465271
Loss :  1.799738883972168 2.700916290283203 2.700916290283203
  batch 60 loss: 1.799738883972168, 2.700916290283203, 2.700916290283203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.80677330493927 3.223883628845215 3.223883628845215
Loss :  1.7995003461837769 3.2134220600128174 3.2134220600128174
Loss :  1.804685115814209 3.0317208766937256 3.0317208766937256
Loss :  1.8008546829223633 2.9317448139190674 2.9317448139190674
Loss :  1.8055391311645508 3.048672914505005 3.048672914505005
Loss :  1.707399606704712 4.426057815551758 4.426057815551758
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7086538076400757 4.368010520935059 4.368010520935059
Loss :  1.7053041458129883 4.37200403213501 4.37200403213501
Loss :  1.7338932752609253 4.108859062194824 4.108859062194824
Total LOSS train 3.1182428359985352 valid 4.318732857704163
CE LOSS train 1.802263331413269 valid 0.4334733188152313
Contrastive LOSS train 3.1182428359985352 valid 1.027214765548706
EPOCH 281:
Loss :  1.8013300895690918 2.7034711837768555 2.7034711837768555
Loss :  1.7992180585861206 3.21905779838562 3.21905779838562
Loss :  1.8018977642059326 2.830348253250122 2.830348253250122
Loss :  1.8023329973220825 2.888380289077759 2.888380289077759
Loss :  1.800818920135498 3.0128562450408936 3.0128562450408936
Loss :  1.8050569295883179 2.7023348808288574 2.7023348808288574
Loss :  1.8001254796981812 3.0809414386749268 3.0809414386749268
Loss :  1.8002684116363525 2.4110124111175537 2.4110124111175537
Loss :  1.7963558435440063 2.6634650230407715 2.6634650230407715
Loss :  1.7973443269729614 2.782304048538208 2.782304048538208
Loss :  1.8075865507125854 3.1848788261413574 3.1848788261413574
Loss :  1.8021091222763062 3.1242806911468506 3.1242806911468506
Loss :  1.8069664239883423 3.612581968307495 3.612581968307495
Loss :  1.8060601949691772 3.3801119327545166 3.3801119327545166
Loss :  1.7933975458145142 3.218463182449341 3.218463182449341
Loss :  1.8080826997756958 3.473548412322998 3.473548412322998
Loss :  1.8054864406585693 3.556863307952881 3.556863307952881
Loss :  1.8017643690109253 3.4286255836486816 3.4286255836486816
Loss :  1.801916480064392 3.371189832687378 3.371189832687378
Loss :  1.7962539196014404 3.020228385925293 3.020228385925293
  batch 20 loss: 1.7962539196014404, 3.020228385925293, 3.020228385925293
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8047069311141968 3.0900557041168213 3.0900557041168213
Loss :  1.8034403324127197 3.0248098373413086 3.0248098373413086
Loss :  1.7969967126846313 2.8900399208068848 2.8900399208068848
Loss :  1.8024826049804688 3.0841028690338135 3.0841028690338135
Loss :  1.8024101257324219 3.2424979209899902 3.2424979209899902
Loss :  1.8029885292053223 3.3124353885650635 3.3124353885650635
Loss :  1.8073526620864868 3.346097707748413 3.346097707748413
Loss :  1.799179196357727 3.5127036571502686 3.5127036571502686
Loss :  1.811875820159912 2.6074395179748535 2.6074395179748535
Loss :  1.7985057830810547 3.398266315460205 3.398266315460205
Loss :  1.8061573505401611 3.124255418777466 3.124255418777466
Loss :  1.795448899269104 2.951000452041626 2.951000452041626
Loss :  1.8051458597183228 2.8313491344451904 2.8313491344451904
Loss :  1.804249882698059 3.1434266567230225 3.1434266567230225
Loss :  1.8043066263198853 3.2358224391937256 3.2358224391937256
Loss :  1.8067495822906494 2.8366153240203857 2.8366153240203857
Loss :  1.803316354751587 3.0563418865203857 3.0563418865203857
Loss :  1.798629641532898 2.8613157272338867 2.8613157272338867
Loss :  1.800890326499939 2.842745304107666 2.842745304107666
Loss :  1.797837257385254 2.4726722240448 2.4726722240448
  batch 40 loss: 1.797837257385254, 2.4726722240448, 2.4726722240448
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8027091026306152 2.7998571395874023 2.7998571395874023
Loss :  1.8026211261749268 2.4223523139953613 2.4223523139953613
Loss :  1.8018029928207397 3.117123603820801 3.117123603820801
Loss :  1.7995495796203613 3.036935567855835 3.036935567855835
Loss :  1.8043243885040283 2.6825976371765137 2.6825976371765137
Loss :  1.8023483753204346 3.056182384490967 3.056182384490967
Loss :  1.799411654472351 3.178478956222534 3.178478956222534
Loss :  1.800857663154602 2.7803220748901367 2.7803220748901367
Loss :  1.7966327667236328 2.548626661300659 2.548626661300659
Loss :  1.800451636314392 2.5747714042663574 2.5747714042663574
Loss :  1.7945473194122314 2.82572603225708 2.82572603225708
Loss :  1.8016891479492188 2.9261577129364014 2.9261577129364014
Loss :  1.8018519878387451 2.8428428173065186 2.8428428173065186
Loss :  1.8053525686264038 3.0802481174468994 3.0802481174468994
Loss :  1.7994568347930908 3.0410075187683105 3.0410075187683105
Loss :  1.802727222442627 2.4878780841827393 2.4878780841827393
Loss :  1.806762933731079 2.9510130882263184 2.9510130882263184
Loss :  1.8058100938796997 3.0600621700286865 3.0600621700286865
Loss :  1.809297800064087 3.2965686321258545 3.2965686321258545
Loss :  1.7997896671295166 3.2618601322174072 3.2618601322174072
  batch 60 loss: 1.7997896671295166, 3.2618601322174072, 3.2618601322174072
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8066024780273438 3.7446062564849854 3.7446062564849854
Loss :  1.799511432647705 2.911508321762085 2.911508321762085
Loss :  1.8045953512191772 2.7954599857330322 2.7954599857330322
Loss :  1.8007984161376953 3.2272753715515137 3.2272753715515137
Loss :  1.8054919242858887 3.158329725265503 3.158329725265503
Loss :  1.7072333097457886 4.4447174072265625 4.4447174072265625
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.708471655845642 4.406641960144043 4.406641960144043
Loss :  1.7051581144332886 4.382007122039795 4.382007122039795
Loss :  1.7336235046386719 4.138765335083008 4.138765335083008
Total LOSS train 3.020565058634831 valid 4.343032956123352
CE LOSS train 1.8021851924749521 valid 0.43340587615966797
Contrastive LOSS train 3.020565058634831 valid 1.034691333770752
EPOCH 282:
Loss :  1.801348090171814 3.28564715385437 3.28564715385437
Loss :  1.7990436553955078 2.907999277114868 2.907999277114868
Loss :  1.8018382787704468 3.009655475616455 3.009655475616455
Loss :  1.802346110343933 3.1917686462402344 3.1917686462402344
Loss :  1.8007607460021973 3.122979164123535 3.122979164123535
Loss :  1.8049812316894531 3.1470234394073486 3.1470234394073486
Loss :  1.7999622821807861 3.6288435459136963 3.6288435459136963
Loss :  1.8001160621643066 3.38411283493042 3.38411283493042
Loss :  1.7963148355484009 3.3524906635284424 3.3524906635284424
Loss :  1.7971652746200562 3.4148590564727783 3.4148590564727783
Loss :  1.8074536323547363 3.362901449203491 3.362901449203491
Loss :  1.8020769357681274 3.0480852127075195 3.0480852127075195
Loss :  1.8068339824676514 2.881080389022827 2.881080389022827
Loss :  1.8059943914413452 3.332494020462036 3.332494020462036
Loss :  1.793382167816162 2.981205701828003 2.981205701828003
Loss :  1.8079521656036377 3.003432035446167 3.003432035446167
Loss :  1.8054126501083374 2.6932966709136963 2.6932966709136963
Loss :  1.8016471862792969 3.274960517883301 3.274960517883301
Loss :  1.8019144535064697 2.868114471435547 2.868114471435547
Loss :  1.796157717704773 3.601388454437256 3.601388454437256
  batch 20 loss: 1.796157717704773, 3.601388454437256, 3.601388454437256
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.804653286933899 3.3077893257141113 3.3077893257141113
Loss :  1.803419828414917 3.5825304985046387 3.5825304985046387
Loss :  1.7969355583190918 3.6541693210601807 3.6541693210601807
Loss :  1.8024628162384033 3.5647478103637695 3.5647478103637695
Loss :  1.8023884296417236 3.5645079612731934 3.5645079612731934
Loss :  1.802901029586792 3.1936991214752197 3.1936991214752197
Loss :  1.8072658777236938 3.666017770767212 3.666017770767212
Loss :  1.799066424369812 3.1033132076263428 3.1033132076263428
Loss :  1.811824917793274 2.418550968170166 2.418550968170166
Loss :  1.7984225749969482 3.2404470443725586 3.2404470443725586
Loss :  1.806158185005188 2.8193352222442627 2.8193352222442627
Loss :  1.795370101928711 2.828845977783203 2.828845977783203
Loss :  1.8050758838653564 3.063270092010498 3.063270092010498
Loss :  1.8041733503341675 2.996582269668579 2.996582269668579
Loss :  1.8042405843734741 3.083987236022949 3.083987236022949
Loss :  1.8067632913589478 3.089254140853882 3.089254140853882
Loss :  1.8032563924789429 3.2993316650390625 3.2993316650390625
Loss :  1.798534631729126 2.577296257019043 2.577296257019043
Loss :  1.8007644414901733 3.1527936458587646 3.1527936458587646
Loss :  1.7976981401443481 2.9260458946228027 2.9260458946228027
  batch 40 loss: 1.7976981401443481, 2.9260458946228027, 2.9260458946228027
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8026301860809326 3.084296703338623 3.084296703338623
Loss :  1.8026127815246582 3.1244239807128906 3.1244239807128906
Loss :  1.8017016649246216 2.8646631240844727 2.8646631240844727
Loss :  1.799511194229126 3.130347967147827 3.130347967147827
Loss :  1.8042834997177124 3.2942206859588623 3.2942206859588623
Loss :  1.802372694015503 3.0349104404449463 3.0349104404449463
Loss :  1.7993617057800293 2.611626148223877 2.611626148223877
Loss :  1.8008209466934204 2.519695997238159 2.519695997238159
Loss :  1.7966639995574951 2.9555342197418213 2.9555342197418213
Loss :  1.8004456758499146 3.067467451095581 3.067467451095581
Loss :  1.7945255041122437 2.9762399196624756 2.9762399196624756
Loss :  1.8016945123672485 3.5040953159332275 3.5040953159332275
Loss :  1.8018882274627686 3.23362135887146 3.23362135887146
Loss :  1.8054262399673462 3.1478240489959717 3.1478240489959717
Loss :  1.7994285821914673 3.098188638687134 3.098188638687134
Loss :  1.802735447883606 2.767543077468872 2.767543077468872
Loss :  1.8068643808364868 3.53277325630188 3.53277325630188
Loss :  1.8058669567108154 3.3059966564178467 3.3059966564178467
Loss :  1.8093640804290771 3.0990164279937744 3.0990164279937744
Loss :  1.7997692823410034 3.1281824111938477 3.1281824111938477
  batch 60 loss: 1.7997692823410034, 3.1281824111938477, 3.1281824111938477
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8067104816436768 3.202122926712036 3.202122926712036
Loss :  1.7995234727859497 2.6658935546875 2.6658935546875
Loss :  1.8046283721923828 3.1818695068359375 3.1818695068359375
Loss :  1.8008683919906616 2.61584734916687 2.61584734916687
Loss :  1.8055909872055054 3.2935492992401123 3.2935492992401123
Loss :  1.7061527967453003 4.398745059967041 4.398745059967041
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7073813676834106 4.356563091278076 4.356563091278076
Loss :  1.7041666507720947 4.3749284744262695 4.3749284744262695
Loss :  1.7327370643615723 4.222102642059326 4.222102642059326
Total LOSS train 3.1240739088792067 valid 4.338084816932678
CE LOSS train 1.8021441056178167 valid 0.43318426609039307
Contrastive LOSS train 3.1240739088792067 valid 1.0555256605148315
EPOCH 283:
Loss :  1.8013761043548584 2.680525779724121 2.680525779724121
Loss :  1.7991257905960083 3.0485711097717285 3.0485711097717285
Loss :  1.8019107580184937 3.167677402496338 3.167677402496338
Loss :  1.802367925643921 2.733367681503296 2.733367681503296
Loss :  1.8007898330688477 3.4391939640045166 3.4391939640045166
Loss :  1.8050214052200317 3.0292344093322754 3.0292344093322754
Loss :  1.800088882446289 2.9899685382843018 2.9899685382843018
Loss :  1.8002561330795288 3.0698347091674805 3.0698347091674805
Loss :  1.7963199615478516 2.818901538848877 2.818901538848877
Loss :  1.7973278760910034 2.9397366046905518 2.9397366046905518
Loss :  1.8075551986694336 3.193275213241577 3.193275213241577
Loss :  1.802131175994873 3.486205816268921 3.486205816268921
Loss :  1.806933879852295 3.303388833999634 3.303388833999634
Loss :  1.8060414791107178 3.8412859439849854 3.8412859439849854
Loss :  1.7933781147003174 3.165508270263672 3.165508270263672
Loss :  1.8080674409866333 3.632648468017578 3.632648468017578
Loss :  1.805493950843811 2.768517255783081 2.768517255783081
Loss :  1.801772117614746 3.367570638656616 3.367570638656616
Loss :  1.8019360303878784 3.6682584285736084 3.6682584285736084
Loss :  1.796250581741333 4.046202659606934 4.046202659606934
  batch 20 loss: 1.796250581741333, 4.046202659606934, 4.046202659606934
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8046950101852417 3.858083486557007 3.858083486557007
Loss :  1.8034253120422363 3.3102285861968994 3.3102285861968994
Loss :  1.7969611883163452 3.726274013519287 3.726274013519287
Loss :  1.80242919921875 3.0782113075256348 3.0782113075256348
Loss :  1.8023006916046143 3.5490705966949463 3.5490705966949463
Loss :  1.8029707670211792 3.3383467197418213 3.3383467197418213
Loss :  1.8073623180389404 2.8968594074249268 2.8968594074249268
Loss :  1.7991586923599243 3.3227150440216064 3.3227150440216064
Loss :  1.8120323419570923 2.5190346240997314 2.5190346240997314
Loss :  1.7984142303466797 2.8621981143951416 2.8621981143951416
Loss :  1.8062020540237427 3.281891345977783 3.281891345977783
Loss :  1.7954219579696655 3.6178691387176514 3.6178691387176514
Loss :  1.805168628692627 3.148542642593384 3.148542642593384
Loss :  1.8042694330215454 3.4789347648620605 3.4789347648620605
Loss :  1.8042174577713013 3.811743974685669 3.811743974685669
Loss :  1.8068912029266357 3.9383459091186523 3.9383459091186523
Loss :  1.8032927513122559 3.7707135677337646 3.7707135677337646
Loss :  1.7987014055252075 3.1733665466308594 3.1733665466308594
Loss :  1.8007657527923584 2.9429779052734375 2.9429779052734375
Loss :  1.7977455854415894 2.9889609813690186 2.9889609813690186
  batch 40 loss: 1.7977455854415894, 2.9889609813690186, 2.9889609813690186
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8026716709136963 3.221975803375244 3.221975803375244
Loss :  1.8026783466339111 3.0137133598327637 3.0137133598327637
Loss :  1.801599383354187 3.5990281105041504 3.5990281105041504
Loss :  1.7995059490203857 3.1228463649749756 3.1228463649749756
Loss :  1.8042607307434082 3.4487011432647705 3.4487011432647705
Loss :  1.8025009632110596 3.2096896171569824 3.2096896171569824
Loss :  1.7994130849838257 3.1414921283721924 3.1414921283721924
Loss :  1.8007351160049438 2.7613728046417236 2.7613728046417236
Loss :  1.7968212366104126 3.5618081092834473 3.5618081092834473
Loss :  1.8003506660461426 3.4367690086364746 3.4367690086364746
Loss :  1.7943470478057861 3.4980051517486572 3.4980051517486572
Loss :  1.8016610145568848 2.959409475326538 2.959409475326538
Loss :  1.8018909692764282 3.7161545753479004 3.7161545753479004
Loss :  1.8052419424057007 2.5827724933624268 2.5827724933624268
Loss :  1.7992024421691895 2.9895246028900146 2.9895246028900146
Loss :  1.8026525974273682 2.5408506393432617 2.5408506393432617
Loss :  1.8068461418151855 2.734994649887085 2.734994649887085
Loss :  1.8057852983474731 2.2102417945861816 2.2102417945861816
Loss :  1.809309959411621 2.5139167308807373 2.5139167308807373
Loss :  1.799668550491333 2.788015604019165 2.788015604019165
  batch 60 loss: 1.799668550491333, 2.788015604019165, 2.788015604019165
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8067512512207031 3.305354356765747 3.305354356765747
Loss :  1.799407958984375 2.8243324756622314 2.8243324756622314
Loss :  1.8046760559082031 3.0825767517089844 3.0825767517089844
Loss :  1.8008570671081543 3.541205644607544 3.541205644607544
Loss :  1.8055238723754883 2.41888165473938 2.41888165473938
Loss :  1.705966591835022 4.425310134887695 4.425310134887695
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7071930170059204 4.457911014556885 4.457911014556885
Loss :  1.7040061950683594 4.264899253845215 4.264899253845215
Loss :  1.732486605644226 4.270188331604004 4.270188331604004
Total LOSS train 3.1881211537581224 valid 4.35457718372345
CE LOSS train 1.8021681528825026 valid 0.4331216514110565
Contrastive LOSS train 3.1881211537581224 valid 1.067547082901001
EPOCH 284:
Loss :  1.8012830018997192 2.7584474086761475 2.7584474086761475
Loss :  1.7992581129074097 2.6977670192718506 2.6977670192718506
Loss :  1.8018461465835571 2.3198401927948 2.3198401927948
Loss :  1.802254319190979 2.6335577964782715 2.6335577964782715
Loss :  1.8007622957229614 2.524181604385376 2.524181604385376
Loss :  1.8049933910369873 2.436021566390991 2.436021566390991
Loss :  1.8001748323440552 2.7246203422546387 2.7246203422546387
Loss :  1.8002978563308716 3.003540515899658 3.003540515899658
Loss :  1.7962371110916138 3.130784273147583 3.130784273147583
Loss :  1.7973326444625854 2.892756938934326 2.892756938934326
Loss :  1.8075801134109497 3.098468542098999 3.098468542098999
Loss :  1.8020914793014526 2.873772382736206 2.873772382736206
Loss :  1.8070166110992432 3.036729335784912 3.036729335784912
Loss :  1.8060495853424072 3.074007511138916 3.074007511138916
Loss :  1.7932758331298828 3.4787144660949707 3.4787144660949707
Loss :  1.8081079721450806 3.276628017425537 3.276628017425537
Loss :  1.805526614189148 3.218043327331543 3.218043327331543
Loss :  1.8018245697021484 3.297266721725464 3.297266721725464
Loss :  1.8018832206726074 3.309283494949341 3.309283494949341
Loss :  1.796298861503601 3.2906951904296875 3.2906951904296875
  batch 20 loss: 1.796298861503601, 3.2906951904296875, 3.2906951904296875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8046989440917969 3.2239911556243896 3.2239911556243896
Loss :  1.8033993244171143 3.7789742946624756 3.7789742946624756
Loss :  1.7969671487808228 3.825425624847412 3.825425624847412
Loss :  1.8023957014083862 3.659426689147949 3.659426689147949
Loss :  1.802201271057129 3.4573190212249756 3.4573190212249756
Loss :  1.8030234575271606 3.18064284324646 3.18064284324646
Loss :  1.8074151277542114 3.209240436553955 3.209240436553955
Loss :  1.799221396446228 2.5392441749572754 2.5392441749572754
Loss :  1.8121479749679565 2.241997718811035 2.241997718811035
Loss :  1.7983824014663696 2.640223264694214 2.640223264694214
Loss :  1.806212306022644 3.139289379119873 3.139289379119873
Loss :  1.795446753501892 3.530951976776123 3.530951976776123
Loss :  1.805232286453247 3.703874349594116 3.703874349594116
Loss :  1.8043212890625 3.097679376602173 3.097679376602173
Loss :  1.8042104244232178 3.245605945587158 3.245605945587158
Loss :  1.8069771528244019 3.0967652797698975 3.0967652797698975
Loss :  1.8033442497253418 3.340139865875244 3.340139865875244
Loss :  1.79878568649292 3.161986827850342 3.161986827850342
Loss :  1.8007993698120117 3.4642720222473145 3.4642720222473145
Loss :  1.7978136539459229 3.4104135036468506 3.4104135036468506
  batch 40 loss: 1.7978136539459229, 3.4104135036468506, 3.4104135036468506
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8027313947677612 3.3169631958007812 3.3169631958007812
Loss :  1.802788257598877 3.191019296646118 3.191019296646118
Loss :  1.8016351461410522 2.821859121322632 2.821859121322632
Loss :  1.7995712757110596 2.7781736850738525 2.7781736850738525
Loss :  1.804275631904602 3.6844770908355713 3.6844770908355713
Loss :  1.8026193380355835 3.7532660961151123 3.7532660961151123
Loss :  1.7994611263275146 3.853635549545288 3.853635549545288
Loss :  1.8007436990737915 3.671739101409912 3.671739101409912
Loss :  1.796923041343689 3.0300424098968506 3.0300424098968506
Loss :  1.8003640174865723 3.002065420150757 3.002065420150757
Loss :  1.7943562269210815 2.74858021736145 2.74858021736145
Loss :  1.8016798496246338 3.310173988342285 3.310173988342285
Loss :  1.8019418716430664 3.3437397480010986 3.3437397480010986
Loss :  1.805238962173462 2.7768867015838623 2.7768867015838623
Loss :  1.799189567565918 3.0946879386901855 3.0946879386901855
Loss :  1.8026280403137207 2.7467892169952393 2.7467892169952393
Loss :  1.8069037199020386 3.058816432952881 3.058816432952881
Loss :  1.8058758974075317 2.7025554180145264 2.7025554180145264
Loss :  1.8093394041061401 2.873577356338501 2.873577356338501
Loss :  1.7996275424957275 2.881395101547241 2.881395101547241
  batch 60 loss: 1.7996275424957275, 2.881395101547241, 2.881395101547241
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8068232536315918 3.114633321762085 3.114633321762085
Loss :  1.7994340658187866 3.188915967941284 3.188915967941284
Loss :  1.804710030555725 2.790066957473755 2.790066957473755
Loss :  1.8009263277053833 2.929286479949951 2.929286479949951
Loss :  1.805628776550293 2.777466297149658 2.777466297149658
Loss :  1.7053755521774292 4.394120216369629 4.394120216369629
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7066211700439453 4.39082670211792 4.39082670211792
Loss :  1.703467845916748 4.325414657592773 4.325414657592773
Loss :  1.7319915294647217 4.311522483825684 4.311522483825684
Total LOSS train 3.099436961687528 valid 4.3554710149765015
CE LOSS train 1.80219241472391 valid 0.4329978823661804
Contrastive LOSS train 3.099436961687528 valid 1.077880620956421
EPOCH 285:
Loss :  1.8013406991958618 3.0421454906463623 3.0421454906463623
Loss :  1.7993184328079224 3.080942392349243 3.080942392349243
Loss :  1.8019901514053345 2.6356492042541504 2.6356492042541504
Loss :  1.8023459911346436 2.9972352981567383 2.9972352981567383
Loss :  1.8008382320404053 3.0560150146484375 3.0560150146484375
Loss :  1.8050739765167236 2.3691275119781494 2.3691275119781494
Loss :  1.800269603729248 2.9659130573272705 2.9659130573272705
Loss :  1.8004002571105957 2.781346082687378 2.781346082687378
Loss :  1.7963391542434692 3.097564458847046 3.097564458847046
Loss :  1.7975013256072998 3.2937538623809814 3.2937538623809814
Loss :  1.8076525926589966 3.0477476119995117 3.0477476119995117
Loss :  1.8021454811096191 2.953678607940674 2.953678607940674
Loss :  1.8070461750030518 3.0166237354278564 3.0166237354278564
Loss :  1.806103229522705 2.8763649463653564 2.8763649463653564
Loss :  1.793375849723816 2.976283073425293 2.976283073425293
Loss :  1.8081557750701904 3.062506675720215 3.062506675720215
Loss :  1.8055641651153564 2.9062445163726807 2.9062445163726807
Loss :  1.8018947839736938 2.9987924098968506 2.9987924098968506
Loss :  1.801977276802063 3.2525973320007324 3.2525973320007324
Loss :  1.7963401079177856 3.1041572093963623 3.1041572093963623
  batch 20 loss: 1.7963401079177856, 3.1041572093963623, 3.1041572093963623
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8047826290130615 2.8416049480438232 2.8416049480438232
Loss :  1.8034708499908447 3.369368314743042 3.369368314743042
Loss :  1.797065258026123 3.3757715225219727 3.3757715225219727
Loss :  1.8024635314941406 3.0820226669311523 3.0820226669311523
Loss :  1.8023792505264282 3.132981777191162 3.132981777191162
Loss :  1.8030734062194824 2.677567720413208 2.677567720413208
Loss :  1.8074203729629517 3.2714388370513916 3.2714388370513916
Loss :  1.7992624044418335 3.045954704284668 3.045954704284668
Loss :  1.8120473623275757 2.728904962539673 2.728904962539673
Loss :  1.7984981536865234 3.0544323921203613 3.0544323921203613
Loss :  1.806254267692566 3.203176259994507 3.203176259994507
Loss :  1.7954907417297363 2.8072102069854736 2.8072102069854736
Loss :  1.8051990270614624 2.5567612648010254 2.5567612648010254
Loss :  1.8044037818908691 3.007768154144287 3.007768154144287
Loss :  1.804337739944458 3.4052388668060303 3.4052388668060303
Loss :  1.8069283962249756 3.225271701812744 3.225271701812744
Loss :  1.8033684492111206 2.9863662719726562 2.9863662719726562
Loss :  1.7986667156219482 2.7894129753112793 2.7894129753112793
Loss :  1.8008671998977661 2.7677431106567383 2.7677431106567383
Loss :  1.7978368997573853 2.71479868888855 2.71479868888855
  batch 40 loss: 1.7978368997573853, 2.71479868888855, 2.71479868888855
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.802744746208191 3.2418532371520996 3.2418532371520996
Loss :  1.802785873413086 3.0323684215545654 3.0323684215545654
Loss :  1.8018379211425781 3.2814459800720215 3.2814459800720215
Loss :  1.7996549606323242 2.929082155227661 2.929082155227661
Loss :  1.8043626546859741 2.7696070671081543 2.7696070671081543
Loss :  1.8025000095367432 3.1073241233825684 3.1073241233825684
Loss :  1.7994139194488525 3.024012327194214 3.024012327194214
Loss :  1.8009272813796997 3.5996687412261963 3.5996687412261963
Loss :  1.7967249155044556 3.710787773132324 3.710787773132324
Loss :  1.8005151748657227 3.3525280952453613 3.3525280952453613
Loss :  1.7946465015411377 2.9140424728393555 2.9140424728393555
Loss :  1.80173659324646 2.7807869911193848 2.7807869911193848
Loss :  1.8019546270370483 3.005033254623413 3.005033254623413
Loss :  1.8055131435394287 3.142423152923584 3.142423152923584
Loss :  1.7995294332504272 3.1953485012054443 3.1953485012054443
Loss :  1.8027263879776 3.0481584072113037 3.0481584072113037
Loss :  1.806901454925537 3.3577628135681152 3.3577628135681152
Loss :  1.8059691190719604 3.1032373905181885 3.1032373905181885
Loss :  1.8094104528427124 3.0398941040039062 3.0398941040039062
Loss :  1.7997207641601562 3.005124568939209 3.005124568939209
  batch 60 loss: 1.7997207641601562, 3.005124568939209, 3.005124568939209
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.806772232055664 3.319399833679199 3.319399833679199
Loss :  1.7995946407318115 3.1050233840942383 3.1050233840942383
Loss :  1.8046449422836304 3.148131847381592 3.148131847381592
Loss :  1.8009536266326904 3.035230875015259 3.035230875015259
Loss :  1.8057242631912231 2.7074618339538574 2.7074618339538574
Loss :  1.705178141593933 4.397799968719482 4.397799968719482
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7064026594161987 4.382497310638428 4.382497310638428
Loss :  1.7033073902130127 4.341986656188965 4.341986656188965
Loss :  1.7317514419555664 4.177319526672363 4.177319526672363
Total LOSS train 3.0386803260216344 valid 4.32490086555481
CE LOSS train 1.802257774426387 valid 0.4329378604888916
Contrastive LOSS train 3.0386803260216344 valid 1.0443298816680908
EPOCH 286:
Loss :  1.8013988733291626 2.783665657043457 2.783665657043457
Loss :  1.79912269115448 2.8879733085632324 2.8879733085632324
Loss :  1.8020081520080566 2.8438222408294678 2.8438222408294678
Loss :  1.8024059534072876 2.8490653038024902 2.8490653038024902
Loss :  1.8008109331130981 2.678466558456421 2.678466558456421
Loss :  1.8050531148910522 2.691769599914551 2.691769599914551
Loss :  1.8000863790512085 2.840698480606079 2.840698480606079
Loss :  1.8003023862838745 2.439239740371704 2.439239740371704
Loss :  1.7963263988494873 2.68212890625 2.68212890625
Loss :  1.7974050045013428 2.9640822410583496 2.9640822410583496
Loss :  1.8076066970825195 3.352219581604004 3.352219581604004
Loss :  1.8021284341812134 3.204333543777466 3.204333543777466
Loss :  1.8069708347320557 3.312080144882202 3.312080144882202
Loss :  1.8060981035232544 3.185959815979004 3.185959815979004
Loss :  1.7933545112609863 3.555927038192749 3.555927038192749
Loss :  1.808092474937439 3.6028735637664795 3.6028735637664795
Loss :  1.805541753768921 3.6281187534332275 3.6281187534332275
Loss :  1.8017810583114624 3.2799808979034424 3.2799808979034424
Loss :  1.8019980192184448 3.4986941814422607 3.4986941814422607
Loss :  1.7962595224380493 2.987757921218872 2.987757921218872
  batch 20 loss: 1.7962595224380493, 2.987757921218872, 2.987757921218872
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.804740071296692 3.2961232662200928 3.2961232662200928
Loss :  1.8034635782241821 2.7126786708831787 2.7126786708831787
Loss :  1.7969802618026733 2.951443672180176 2.951443672180176
Loss :  1.802407145500183 2.8775391578674316 2.8775391578674316
Loss :  1.802325963973999 3.202847719192505 3.202847719192505
Loss :  1.8029850721359253 2.930973529815674 2.930973529815674
Loss :  1.8073670864105225 3.3677549362182617 3.3677549362182617
Loss :  1.7991831302642822 3.729551076889038 3.729551076889038
Loss :  1.8120510578155518 3.5354230403900146 3.5354230403900146
Loss :  1.7984333038330078 3.5881731510162354 3.5881731510162354
Loss :  1.806277871131897 3.650057315826416 3.650057315826416
Loss :  1.7954347133636475 3.4410030841827393 3.4410030841827393
Loss :  1.8051780462265015 2.854236602783203 2.854236602783203
Loss :  1.8043824434280396 3.132561683654785 3.132561683654785
Loss :  1.8042948246002197 2.9363582134246826 2.9363582134246826
Loss :  1.8069393634796143 3.375509738922119 3.375509738922119
Loss :  1.8033430576324463 2.999748706817627 2.999748706817627
Loss :  1.7986356019973755 2.6047701835632324 2.6047701835632324
Loss :  1.800829291343689 3.2005772590637207 3.2005772590637207
Loss :  1.7977980375289917 3.1243276596069336 3.1243276596069336
  batch 40 loss: 1.7977980375289917, 3.1243276596069336, 3.1243276596069336
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8027129173278809 3.3233816623687744 3.3233816623687744
Loss :  1.8027914762496948 3.0085198879241943 3.0085198879241943
Loss :  1.8018038272857666 2.7927558422088623 2.7927558422088623
Loss :  1.7996251583099365 2.820469617843628 2.820469617843628
Loss :  1.8043341636657715 2.7846221923828125 2.7846221923828125
Loss :  1.8025259971618652 3.146674394607544 3.146674394607544
Loss :  1.7993907928466797 3.4101099967956543 3.4101099967956543
Loss :  1.8008882999420166 3.4483954906463623 3.4483954906463623
Loss :  1.79674232006073 3.2856099605560303 3.2856099605560303
Loss :  1.8004677295684814 3.0319502353668213 3.0319502353668213
Loss :  1.7945871353149414 2.4875805377960205 2.4875805377960205
Loss :  1.801684856414795 2.83782958984375 2.83782958984375
Loss :  1.8019317388534546 3.641248941421509 3.641248941421509
Loss :  1.8054202795028687 3.305262327194214 3.305262327194214
Loss :  1.799454927444458 3.222027540206909 3.222027540206909
Loss :  1.8026341199874878 3.020020008087158 3.020020008087158
Loss :  1.8068867921829224 3.5036535263061523 3.5036535263061523
Loss :  1.8059748411178589 2.8209266662597656 2.8209266662597656
Loss :  1.8093680143356323 2.902801275253296 2.902801275253296
Loss :  1.799607515335083 2.9573469161987305 2.9573469161987305
  batch 60 loss: 1.799607515335083, 2.9573469161987305, 2.9573469161987305
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8067783117294312 3.203500986099243 3.203500986099243
Loss :  1.799531102180481 2.8622095584869385 2.8622095584869385
Loss :  1.8046314716339111 2.993694305419922 2.993694305419922
Loss :  1.8009496927261353 2.832585334777832 2.832585334777832
Loss :  1.8057290315628052 3.94034743309021 3.94034743309021
Loss :  1.7045363187789917 4.409040451049805 4.409040451049805
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.705764651298523 4.374819755554199 4.374819755554199
Loss :  1.7027349472045898 4.2885847091674805 4.2885847091674805
Loss :  1.7311593294143677 4.175802707672119 4.175802707672119
Total LOSS train 3.113292928842398 valid 4.312061905860901
CE LOSS train 1.8022192881657526 valid 0.4327898323535919
Contrastive LOSS train 3.113292928842398 valid 1.0439506769180298
EPOCH 287:
Loss :  1.8013404607772827 3.5080668926239014 3.5080668926239014
Loss :  1.799146056175232 3.0341145992279053 3.0341145992279053
Loss :  1.8020211458206177 2.9227919578552246 2.9227919578552246
Loss :  1.802371621131897 2.60392165184021 2.60392165184021
Loss :  1.8008077144622803 2.3232457637786865 2.3232457637786865
Loss :  1.8050572872161865 2.3547918796539307 2.3547918796539307
Loss :  1.800146222114563 3.049170732498169 3.049170732498169
Loss :  1.8003610372543335 2.5418002605438232 2.5418002605438232
Loss :  1.7963453531265259 2.916436195373535 2.916436195373535
Loss :  1.797515630722046 2.61734676361084 2.61734676361084
Loss :  1.8076534271240234 2.9963085651397705 2.9963085651397705
Loss :  1.8021395206451416 2.757685899734497 2.757685899734497
Loss :  1.8070170879364014 2.9581925868988037 2.9581925868988037
Loss :  1.8061318397521973 3.149183750152588 3.149183750152588
Loss :  1.7934274673461914 2.881802797317505 2.881802797317505
Loss :  1.8081741333007812 2.751430034637451 2.751430034637451
Loss :  1.805586576461792 2.5339043140411377 2.5339043140411377
Loss :  1.8018453121185303 2.615898370742798 2.615898370742798
Loss :  1.8020378351211548 2.957292079925537 2.957292079925537
Loss :  1.7963275909423828 2.809351921081543 2.809351921081543
  batch 20 loss: 1.7963275909423828, 2.809351921081543, 2.809351921081543
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8047682046890259 2.7688610553741455 2.7688610553741455
Loss :  1.8034875392913818 2.687878131866455 2.687878131866455
Loss :  1.7970117330551147 3.2884936332702637 3.2884936332702637
Loss :  1.8024405241012573 3.107792615890503 3.107792615890503
Loss :  1.802364468574524 3.7181107997894287 3.7181107997894287
Loss :  1.802996039390564 3.6338839530944824 3.6338839530944824
Loss :  1.8073816299438477 3.868929147720337 3.868929147720337
Loss :  1.7992002964019775 2.8937838077545166 2.8937838077545166
Loss :  1.8120296001434326 2.5371973514556885 2.5371973514556885
Loss :  1.7984840869903564 3.0807998180389404 3.0807998180389404
Loss :  1.8062646389007568 3.396512746810913 3.396512746810913
Loss :  1.7954472303390503 3.2840020656585693 3.2840020656585693
Loss :  1.805175542831421 3.0834193229675293 3.0834193229675293
Loss :  1.8043771982192993 3.3632595539093018 3.3632595539093018
Loss :  1.804301381111145 3.3831613063812256 3.3831613063812256
Loss :  1.8068841695785522 3.1466689109802246 3.1466689109802246
Loss :  1.803340196609497 2.820500373840332 2.820500373840332
Loss :  1.7985994815826416 2.501457929611206 2.501457929611206
Loss :  1.8008759021759033 2.6558446884155273 2.6558446884155273
Loss :  1.797824501991272 2.541250467300415 2.541250467300415
  batch 40 loss: 1.797824501991272, 2.541250467300415, 2.541250467300415
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8027147054672241 3.368964433670044 3.368964433670044
Loss :  1.8027273416519165 3.00856351852417 3.00856351852417
Loss :  1.8018076419830322 3.434892177581787 3.434892177581787
Loss :  1.7995861768722534 2.8316619396209717 2.8316619396209717
Loss :  1.8042882680892944 3.1016123294830322 3.1016123294830322
Loss :  1.8024184703826904 3.49526309967041 3.49526309967041
Loss :  1.7993799448013306 3.2505345344543457 3.2505345344543457
Loss :  1.8008389472961426 3.176952600479126 3.176952600479126
Loss :  1.796640396118164 2.9533700942993164 2.9533700942993164
Loss :  1.800442099571228 2.812346935272217 2.812346935272217
Loss :  1.7946038246154785 2.706796884536743 2.706796884536743
Loss :  1.801676869392395 2.6486778259277344 2.6486778259277344
Loss :  1.8018549680709839 2.681569814682007 2.681569814682007
Loss :  1.8053865432739258 2.8119938373565674 2.8119938373565674
Loss :  1.7994874715805054 2.9860215187072754 2.9860215187072754
Loss :  1.802729606628418 2.752903699874878 2.752903699874878
Loss :  1.806791067123413 2.9584035873413086 2.9584035873413086
Loss :  1.8058396577835083 2.6387553215026855 2.6387553215026855
Loss :  1.8093055486679077 3.007401704788208 3.007401704788208
Loss :  1.7997580766677856 2.7592618465423584 2.7592618465423584
  batch 60 loss: 1.7997580766677856, 2.7592618465423584, 2.7592618465423584
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8066110610961914 2.546617269515991 2.546617269515991
Loss :  1.7995139360427856 2.954777479171753 2.954777479171753
Loss :  1.80454421043396 3.372274875640869 3.372274875640869
Loss :  1.8008407354354858 2.782100200653076 2.782100200653076
Loss :  1.8055683374404907 3.1166765689849854 3.1166765689849854
Loss :  1.7053369283676147 4.411141872406006 4.411141872406006
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.706547737121582 4.436545372009277 4.436545372009277
Loss :  1.7034645080566406 4.463582515716553 4.463582515716553
Loss :  1.731913447380066 4.329617500305176 4.329617500305176
Total LOSS train 2.9569682891552267 valid 4.410221815109253
CE LOSS train 1.802216362953186 valid 0.4329783618450165
Contrastive LOSS train 2.9569682891552267 valid 1.082404375076294
EPOCH 288:
Loss :  1.8012758493423462 2.895580768585205 2.895580768585205
Loss :  1.7989921569824219 3.1603240966796875 3.1603240966796875
Loss :  1.8018993139266968 2.8828694820404053 2.8828694820404053
Loss :  1.8023195266723633 2.8538851737976074 2.8538851737976074
Loss :  1.8007142543792725 2.8513600826263428 2.8513600826263428
Loss :  1.8049612045288086 2.722205638885498 2.722205638885498
Loss :  1.7999681234359741 3.4990994930267334 3.4990994930267334
Loss :  1.8001238107681274 3.4315617084503174 3.4315617084503174
Loss :  1.7962861061096191 3.296376943588257 3.296376943588257
Loss :  1.7972298860549927 3.3042385578155518 3.3042385578155518
Loss :  1.8074216842651367 3.5049171447753906 3.5049171447753906
Loss :  1.8020472526550293 3.7238264083862305 3.7238264083862305
Loss :  1.806807518005371 3.8572099208831787 3.8572099208831787
Loss :  1.8059779405593872 3.1838934421539307 3.1838934421539307
Loss :  1.793379545211792 2.993237257003784 2.993237257003784
Loss :  1.8078912496566772 3.4733786582946777 3.4733786582946777
Loss :  1.8053998947143555 3.3082146644592285 3.3082146644592285
Loss :  1.801600456237793 2.9716567993164062 2.9716567993164062
Loss :  1.801919937133789 3.32653546333313 3.32653546333313
Loss :  1.7961345911026 3.200591564178467 3.200591564178467
  batch 20 loss: 1.7961345911026, 3.200591564178467, 3.200591564178467
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8046451807022095 2.9872472286224365 2.9872472286224365
Loss :  1.8034111261367798 2.837996244430542 2.837996244430542
Loss :  1.79690420627594 3.1778476238250732 3.1778476238250732
Loss :  1.8024040460586548 2.7658309936523438 2.7658309936523438
Loss :  1.8022948503494263 3.0965418815612793 3.0965418815612793
Loss :  1.8028297424316406 3.246774911880493 3.246774911880493
Loss :  1.8072434663772583 3.241745948791504 3.241745948791504
Loss :  1.7990632057189941 3.3890655040740967 3.3890655040740967
Loss :  1.8118765354156494 3.0862362384796143 3.0862362384796143
Loss :  1.7983862161636353 3.4390742778778076 3.4390742778778076
Loss :  1.8061829805374146 3.408147096633911 3.408147096633911
Loss :  1.7953565120697021 3.267117977142334 3.267117977142334
Loss :  1.805053949356079 3.6022098064422607 3.6022098064422607
Loss :  1.804227352142334 3.5874686241149902 3.5874686241149902
Loss :  1.804226279258728 3.5573532581329346 3.5573532581329346
Loss :  1.806777834892273 3.7395236492156982 3.7395236492156982
Loss :  1.8032257556915283 3.5324437618255615 3.5324437618255615
Loss :  1.7984544038772583 2.891127109527588 2.891127109527588
Loss :  1.8007538318634033 3.116595983505249 3.116595983505249
Loss :  1.7976607084274292 3.063669443130493 3.063669443130493
  batch 40 loss: 1.7976607084274292, 3.063669443130493, 3.063669443130493
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8026090860366821 3.5195870399475098 3.5195870399475098
Loss :  1.802595615386963 3.456990957260132 3.456990957260132
Loss :  1.8016722202301025 3.456264019012451 3.456264019012451
Loss :  1.7994744777679443 3.5744729042053223 3.5744729042053223
Loss :  1.8042018413543701 3.491220712661743 3.491220712661743
Loss :  1.8023320436477661 3.125767469406128 3.125767469406128
Loss :  1.7992674112319946 3.058957815170288 3.058957815170288
Loss :  1.8007413148880005 3.765843629837036 3.765843629837036
Loss :  1.7965441942214966 3.1771671772003174 3.1771671772003174
Loss :  1.8003047704696655 3.438473701477051 3.438473701477051
Loss :  1.7944345474243164 2.977010488510132 2.977010488510132
Loss :  1.8015496730804443 3.4164633750915527 3.4164633750915527
Loss :  1.8017891645431519 3.2685372829437256 3.2685372829437256
Loss :  1.8052235841751099 2.9262375831604004 2.9262375831604004
Loss :  1.799316644668579 2.8098950386047363 2.8098950386047363
Loss :  1.8025833368301392 2.416172981262207 2.416172981262207
Loss :  1.8067286014556885 2.6275742053985596 2.6275742053985596
Loss :  1.805740475654602 2.47998046875 2.47998046875
Loss :  1.8092619180679321 2.824744462966919 2.824744462966919
Loss :  1.7996829748153687 2.3380179405212402 2.3380179405212402
  batch 60 loss: 1.7996829748153687, 2.3380179405212402, 2.3380179405212402
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8065972328186035 2.5726065635681152 2.5726065635681152
Loss :  1.7994639873504639 2.7688546180725098 2.7688546180725098
Loss :  1.8045271635055542 2.3454461097717285 2.3454461097717285
Loss :  1.8008145093917847 2.8707082271575928 2.8707082271575928
Loss :  1.805507779121399 2.0810842514038086 2.0810842514038086
Loss :  1.7054059505462646 4.4225969314575195 4.4225969314575195
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7066041231155396 4.422529697418213 4.422529697418213
Loss :  1.7035315036773682 4.255627155303955 4.255627155303955
Loss :  1.7319356203079224 4.094521999359131 4.094521999359131
Total LOSS train 3.1425086131462683 valid 4.298818945884705
CE LOSS train 1.8020968162096465 valid 0.4329839050769806
Contrastive LOSS train 3.1425086131462683 valid 1.0236304998397827
EPOCH 289:
Loss :  1.8012863397598267 2.417055368423462 2.417055368423462
Loss :  1.7990450859069824 2.8732943534851074 2.8732943534851074
Loss :  1.801919937133789 2.7042200565338135 2.7042200565338135
Loss :  1.8023372888565063 2.493792772293091 2.493792772293091
Loss :  1.8007409572601318 2.7004058361053467 2.7004058361053467
Loss :  1.8049753904342651 3.0085175037384033 3.0085175037384033
Loss :  1.800016164779663 2.8632280826568604 2.8632280826568604
Loss :  1.8001474142074585 2.6806254386901855 2.6806254386901855
Loss :  1.796288013458252 2.6102499961853027 2.6102499961853027
Loss :  1.797245740890503 2.8780648708343506 2.8780648708343506
Loss :  1.8074250221252441 2.9574995040893555 2.9574995040893555
Loss :  1.802061915397644 2.7802534103393555 2.7802534103393555
Loss :  1.8068065643310547 2.9929254055023193 2.9929254055023193
Loss :  1.805955171585083 3.2572970390319824 3.2572970390319824
Loss :  1.7933491468429565 2.7309725284576416 2.7309725284576416
Loss :  1.807873010635376 2.766361713409424 2.766361713409424
Loss :  1.8053855895996094 2.830899238586426 2.830899238586426
Loss :  1.8015997409820557 2.7581021785736084 2.7581021785736084
Loss :  1.8018790483474731 2.806548595428467 2.806548595428467
Loss :  1.79612135887146 2.7116634845733643 2.7116634845733643
  batch 20 loss: 1.79612135887146, 2.7116634845733643, 2.7116634845733643
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8046196699142456 2.701482057571411 2.701482057571411
Loss :  1.8033851385116577 2.6229984760284424 2.6229984760284424
Loss :  1.7968889474868774 3.518990993499756 3.518990993499756
Loss :  1.8023930788040161 3.589378833770752 3.589378833770752
Loss :  1.8022691011428833 3.476907253265381 3.476907253265381
Loss :  1.8028074502944946 3.296699285507202 3.296699285507202
Loss :  1.8072221279144287 3.3877041339874268 3.3877041339874268
Loss :  1.7990316152572632 3.090257167816162 3.090257167816162
Loss :  1.8118215799331665 2.905979633331299 2.905979633331299
Loss :  1.7983477115631104 2.8222544193267822 2.8222544193267822
Loss :  1.8061344623565674 2.8945629596710205 2.8945629596710205
Loss :  1.7953274250030518 2.9279987812042236 2.9279987812042236
Loss :  1.8049969673156738 2.8904459476470947 2.8904459476470947
Loss :  1.8041658401489258 3.0591561794281006 3.0591561794281006
Loss :  1.80418860912323 3.274630308151245 3.274630308151245
Loss :  1.806717038154602 2.9957475662231445 2.9957475662231445
Loss :  1.8031814098358154 3.2992005348205566 3.2992005348205566
Loss :  1.798409104347229 3.035367488861084 3.035367488861084
Loss :  1.8007266521453857 3.7825915813446045 3.7825915813446045
Loss :  1.7976096868515015 3.2049107551574707 3.2049107551574707
  batch 40 loss: 1.7976096868515015, 3.2049107551574707, 3.2049107551574707
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8025784492492676 3.162677764892578 3.162677764892578
Loss :  1.8025234937667847 3.2546069622039795 3.2546069622039795
Loss :  1.8016337156295776 3.1465373039245605 3.1465373039245605
Loss :  1.799424648284912 3.134434461593628 3.134434461593628
Loss :  1.8041869401931763 3.494358539581299 3.494358539581299
Loss :  1.8022810220718384 3.694143772125244 3.694143772125244
Loss :  1.799251675605774 3.357999086380005 3.357999086380005
Loss :  1.8007304668426514 3.529080629348755 3.529080629348755
Loss :  1.7965089082717896 3.1983251571655273 3.1983251571655273
Loss :  1.8002904653549194 2.923417568206787 2.923417568206787
Loss :  1.7944493293762207 2.846311330795288 2.846311330795288
Loss :  1.8015573024749756 2.7286484241485596 2.7286484241485596
Loss :  1.8017760515213013 3.209550380706787 3.209550380706787
Loss :  1.8052277565002441 2.851504325866699 2.851504325866699
Loss :  1.7993336915969849 3.198920488357544 3.198920488357544
Loss :  1.802572250366211 2.896780490875244 2.896780490875244
Loss :  1.8066959381103516 3.439985513687134 3.439985513687134
Loss :  1.8057141304016113 2.7505648136138916 2.7505648136138916
Loss :  1.8092575073242188 3.0085628032684326 3.0085628032684326
Loss :  1.7996463775634766 2.901242256164551 2.901242256164551
  batch 60 loss: 1.7996463775634766, 2.901242256164551, 2.901242256164551
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.806564211845398 3.2489805221557617 3.2489805221557617
Loss :  1.7994329929351807 3.211570978164673 3.211570978164673
Loss :  1.8044893741607666 3.6916191577911377 3.6916191577911377
Loss :  1.8007725477218628 3.4754183292388916 3.4754183292388916
Loss :  1.8054358959197998 3.3718690872192383 3.3718690872192383
Loss :  1.7055529356002808 4.3674774169921875 4.3674774169921875
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7067480087280273 4.484500408172607 4.484500408172607
Loss :  1.70368492603302 4.246400356292725 4.246400356292725
Loss :  1.7318956851959229 4.161595821380615 4.161595821380615
Total LOSS train 3.0511746443234955 valid 4.314993500709534
CE LOSS train 1.80207750247075 valid 0.4329739212989807
Contrastive LOSS train 3.0511746443234955 valid 1.0403989553451538
EPOCH 290:
Loss :  1.8011860847473145 2.7333030700683594 2.7333030700683594
Loss :  1.7990034818649292 2.8347787857055664 2.8347787857055664
Loss :  1.8018031120300293 2.7197585105895996 2.7197585105895996
Loss :  1.8022255897521973 2.630640745162964 2.630640745162964
Loss :  1.800667405128479 2.8865625858306885 2.8865625858306885
Loss :  1.8049119710922241 2.703735828399658 2.703735828399658
Loss :  1.7999638319015503 2.831800699234009 2.831800699234009
Loss :  1.8001062870025635 2.9312055110931396 2.9312055110931396
Loss :  1.7962000370025635 3.3009157180786133 3.3009157180786133
Loss :  1.79715895652771 3.160407304763794 3.160407304763794
Loss :  1.807386040687561 3.0045204162597656 3.0045204162597656
Loss :  1.8020161390304565 3.2964024543762207 3.2964024543762207
Loss :  1.8067996501922607 3.015481472015381 3.015481472015381
Loss :  1.8059524297714233 3.0173704624176025 3.0173704624176025
Loss :  1.7932968139648438 3.1877591609954834 3.1877591609954834
Loss :  1.8078640699386597 3.2021420001983643 3.2021420001983643
Loss :  1.8053874969482422 2.9968960285186768 2.9968960285186768
Loss :  1.801586389541626 3.438903331756592 3.438903331756592
Loss :  1.8018373250961304 3.2397751808166504 3.2397751808166504
Loss :  1.7961479425430298 2.9967122077941895 2.9967122077941895
  batch 20 loss: 1.7961479425430298, 2.9967122077941895, 2.9967122077941895
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8046199083328247 3.2560322284698486 3.2560322284698486
Loss :  1.8033802509307861 3.5263922214508057 3.5263922214508057
Loss :  1.796918511390686 2.9649298191070557 2.9649298191070557
Loss :  1.8024085760116577 2.906486988067627 2.906486988067627
Loss :  1.8022615909576416 2.928954839706421 2.928954839706421
Loss :  1.8028485774993896 2.6652700901031494 2.6652700901031494
Loss :  1.8072539567947388 3.1932058334350586 3.1932058334350586
Loss :  1.799074411392212 3.1658775806427 3.1658775806427
Loss :  1.811884880065918 2.9291768074035645 2.9291768074035645
Loss :  1.7983289957046509 3.3403773307800293 3.3403773307800293
Loss :  1.8061288595199585 3.2722513675689697 3.2722513675689697
Loss :  1.7953521013259888 3.5514938831329346 3.5514938831329346
Loss :  1.8050127029418945 2.852402687072754 2.852402687072754
Loss :  1.8041694164276123 2.99849271774292 2.99849271774292
Loss :  1.804174780845642 3.077611207962036 3.077611207962036
Loss :  1.8067437410354614 3.161121129989624 3.161121129989624
Loss :  1.8032019138336182 3.186522960662842 3.186522960662842
Loss :  1.798459529876709 3.3957369327545166 3.3957369327545166
Loss :  1.8007147312164307 3.33839750289917 3.33839750289917
Loss :  1.7976104021072388 3.3640220165252686 3.3640220165252686
  batch 40 loss: 1.7976104021072388, 3.3640220165252686, 3.3640220165252686
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8025755882263184 3.3467721939086914 3.3467721939086914
Loss :  1.8025116920471191 2.6685972213745117 2.6685972213745117
Loss :  1.8015810251235962 2.5062355995178223 2.5062355995178223
Loss :  1.7993792295455933 2.766036033630371 2.766036033630371
Loss :  1.8041489124298096 2.855848550796509 2.855848550796509
Loss :  1.8023020029067993 3.3261663913726807 3.3261663913726807
Loss :  1.7992331981658936 3.644514560699463 3.644514560699463
Loss :  1.800665020942688 3.5816385746002197 3.5816385746002197
Loss :  1.7965307235717773 3.6286966800689697 3.6286966800689697
Loss :  1.8002088069915771 3.504777431488037 3.504777431488037
Loss :  1.7943235635757446 3.358668565750122 3.358668565750122
Loss :  1.8015097379684448 3.5236382484436035 3.5236382484436035
Loss :  1.801758050918579 3.544377565383911 3.544377565383911
Loss :  1.8050874471664429 3.606459379196167 3.606459379196167
Loss :  1.7992161512374878 3.7254159450531006 3.7254159450531006
Loss :  1.802518606185913 2.656344413757324 2.656344413757324
Loss :  1.8066706657409668 3.1882424354553223 3.1882424354553223
Loss :  1.8056936264038086 3.1916043758392334 3.1916043758392334
Loss :  1.809207558631897 2.8121752738952637 2.8121752738952637
Loss :  1.7995682954788208 2.948711633682251 2.948711633682251
  batch 60 loss: 1.7995682954788208, 2.948711633682251, 2.948711633682251
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8065719604492188 3.1988110542297363 3.1988110542297363
Loss :  1.7993407249450684 3.1809096336364746 3.1809096336364746
Loss :  1.804496169090271 3.0746681690216064 3.0746681690216064
Loss :  1.8007524013519287 3.0597550868988037 3.0597550868988037
Loss :  1.8054062128067017 3.495957374572754 3.495957374572754
Loss :  1.7054952383041382 4.415111064910889 4.415111064910889
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7067065238952637 4.420467853546143 4.420467853546143
Loss :  1.7036372423171997 4.384071350097656 4.384071350097656
Loss :  1.73189377784729 4.157991409301758 4.157991409301758
Total LOSS train 3.1322900001819316 valid 4.344410419464111
CE LOSS train 1.8020508656134973 valid 0.4329734444618225
Contrastive LOSS train 3.1322900001819316 valid 1.0394978523254395
EPOCH 291:
Loss :  1.8010565042495728 2.7097482681274414 2.7097482681274414
Loss :  1.799059271812439 3.157142400741577 3.157142400741577
Loss :  1.8017336130142212 2.662627935409546 2.662627935409546
Loss :  1.8020648956298828 2.9184153079986572 2.9184153079986572
Loss :  1.8005805015563965 2.802487850189209 2.802487850189209
Loss :  1.8048293590545654 3.0314371585845947 3.0314371585845947
Loss :  1.7999846935272217 2.960477590560913 2.960477590560913
Loss :  1.8001384735107422 2.9622724056243896 2.9622724056243896
Loss :  1.7960741519927979 2.7641232013702393 2.7641232013702393
Loss :  1.7971493005752563 3.5934252738952637 3.5934252738952637
Loss :  1.8073875904083252 3.6613590717315674 3.6613590717315674
Loss :  1.801963210105896 3.663011074066162 3.663011074066162
Loss :  1.8068525791168213 3.261509656906128 3.261509656906128
Loss :  1.8059171438217163 2.935152530670166 2.935152530670166
Loss :  1.7931276559829712 3.1912331581115723 3.1912331581115723
Loss :  1.8078184127807617 3.092963457107544 3.092963457107544
Loss :  1.8053512573242188 2.7884230613708496 2.7884230613708496
Loss :  1.8015921115875244 3.2235796451568604 3.2235796451568604
Loss :  1.801692247390747 3.9060580730438232 3.9060580730438232
Loss :  1.796095609664917 3.4086415767669678 3.4086415767669678
  batch 20 loss: 1.796095609664917, 3.4086415767669678, 3.4086415767669678
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.804559350013733 2.718256711959839 2.718256711959839
Loss :  1.8032540082931519 3.210862159729004 3.210862159729004
Loss :  1.7968202829360962 3.1889798641204834 3.1889798641204834
Loss :  1.802217721939087 2.774948835372925 2.774948835372925
Loss :  1.801937460899353 2.8974292278289795 2.8974292278289795
Loss :  1.8028056621551514 2.7808899879455566 2.7808899879455566
Loss :  1.807247281074524 2.6771512031555176 2.6771512031555176
Loss :  1.7990466356277466 2.65846586227417 2.65846586227417
Loss :  1.8120341300964355 2.7483739852905273 2.7483739852905273
Loss :  1.7981090545654297 3.185969114303589 3.185969114303589
Loss :  1.8060762882232666 3.0844380855560303 3.0844380855560303
Loss :  1.7952767610549927 3.6215875148773193 3.6215875148773193
Loss :  1.8050310611724854 3.968204975128174 3.968204975128174
Loss :  1.8041578531265259 3.4868881702423096 3.4868881702423096
Loss :  1.8040794134140015 3.366408109664917 3.366408109664917
Loss :  1.8068064451217651 3.6261098384857178 3.6261098384857178
Loss :  1.803186297416687 3.4856746196746826 3.4856746196746826
Loss :  1.798526406288147 2.859349250793457 2.859349250793457
Loss :  1.8006539344787598 2.9112398624420166 2.9112398624420166
Loss :  1.7975996732711792 2.6455237865448 2.6455237865448
  batch 40 loss: 1.7975996732711792, 2.6455237865448, 2.6455237865448
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8025364875793457 3.012493848800659 3.012493848800659
Loss :  1.802586317062378 3.5644800662994385 3.5644800662994385
Loss :  1.8015168905258179 2.8655691146850586 2.8655691146850586
Loss :  1.799391508102417 2.725687265396118 2.725687265396118
Loss :  1.804118037223816 2.793949604034424 2.793949604034424
Loss :  1.8023988008499146 2.8860156536102295 2.8860156536102295
Loss :  1.7992842197418213 2.63191294670105 2.63191294670105
Loss :  1.800622582435608 2.8785200119018555 2.8785200119018555
Loss :  1.7966755628585815 2.988593101501465 2.988593101501465
Loss :  1.800248384475708 3.273401975631714 3.273401975631714
Loss :  1.7942883968353271 2.917370557785034 2.917370557785034
Loss :  1.8015320301055908 3.062976121902466 3.062976121902466
Loss :  1.8017969131469727 2.8105318546295166 2.8105318546295166
Loss :  1.8051214218139648 2.9587326049804688 2.9587326049804688
Loss :  1.7991876602172852 3.3360328674316406 3.3360328674316406
Loss :  1.8025224208831787 2.9037938117980957 2.9037938117980957
Loss :  1.8067231178283691 2.968050479888916 2.968050479888916
Loss :  1.8057377338409424 3.5735890865325928 3.5735890865325928
Loss :  1.8091905117034912 3.348194122314453 3.348194122314453
Loss :  1.7995156049728394 2.9324913024902344 2.9324913024902344
  batch 60 loss: 1.7995156049728394, 2.9324913024902344, 2.9324913024902344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8065749406814575 2.9072682857513428 2.9072682857513428
Loss :  1.79930579662323 3.2295196056365967 3.2295196056365967
Loss :  1.804481029510498 2.7205159664154053 2.7205159664154053
Loss :  1.8007413148880005 2.8304028511047363 2.8304028511047363
Loss :  1.805423617362976 3.038668632507324 3.038668632507324
Loss :  1.7046746015548706 4.349279880523682 4.349279880523682
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7058827877044678 4.3663129806518555 4.3663129806518555
Loss :  1.7028831243515015 4.330874443054199 4.330874443054199
Loss :  1.7311655282974243 4.287698745727539 4.287698745727539
Total LOSS train 3.073070793885451 valid 4.333541512489319
CE LOSS train 1.8020217785468469 valid 0.4327913820743561
Contrastive LOSS train 3.073070793885451 valid 1.0719246864318848
EPOCH 292:
Loss :  1.801058292388916 2.8663759231567383 2.8663759231567383
Loss :  1.7990046739578247 2.859253406524658 2.859253406524658
Loss :  1.8017216920852661 2.221035957336426 2.221035957336426
Loss :  1.8020707368850708 2.5655531883239746 2.5655531883239746
Loss :  1.8005629777908325 2.642671823501587 2.642671823501587
Loss :  1.8048126697540283 2.6325039863586426 2.6325039863586426
Loss :  1.799940586090088 3.1526827812194824 3.1526827812194824
Loss :  1.8001104593276978 3.096165895462036 3.096165895462036
Loss :  1.7960882186889648 2.9427709579467773 2.9427709579467773
Loss :  1.7971553802490234 2.944976806640625 2.944976806640625
Loss :  1.8073784112930298 3.1423351764678955 3.1423351764678955
Loss :  1.8019994497299194 3.181979179382324 3.181979179382324
Loss :  1.806807041168213 3.7085466384887695 3.7085466384887695
Loss :  1.80591881275177 3.4238922595977783 3.4238922595977783
Loss :  1.7931784391403198 3.826812744140625 3.826812744140625
Loss :  1.8078292608261108 3.4372169971466064 3.4372169971466064
Loss :  1.8053628206253052 3.0242578983306885 3.0242578983306885
Loss :  1.801596999168396 3.1310555934906006 3.1310555934906006
Loss :  1.8018053770065308 3.234480619430542 3.234480619430542
Loss :  1.7960851192474365 3.589426279067993 3.589426279067993
  batch 20 loss: 1.7960851192474365, 3.589426279067993, 3.589426279067993
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8045841455459595 3.3678390979766846 3.3678390979766846
Loss :  1.8033074140548706 3.1140036582946777 3.1140036582946777
Loss :  1.7968451976776123 3.7337920665740967 3.7337920665740967
Loss :  1.8022689819335938 3.2826333045959473 3.2826333045959473
Loss :  1.802095890045166 3.5830845832824707 3.5830845832824707
Loss :  1.8028192520141602 3.5753753185272217 3.5753753185272217
Loss :  1.8072352409362793 3.8052878379821777 3.8052878379821777
Loss :  1.7990387678146362 3.4436421394348145 3.4436421394348145
Loss :  1.811951756477356 2.9852724075317383 2.9852724075317383
Loss :  1.798230767250061 2.9489715099334717 2.9489715099334717
Loss :  1.8061254024505615 3.6037168502807617 3.6037168502807617
Loss :  1.7953059673309326 3.2269744873046875 3.2269744873046875
Loss :  1.8050146102905273 2.7226204872131348 2.7226204872131348
Loss :  1.804194450378418 2.815507411956787 2.815507411956787
Loss :  1.8041380643844604 3.3192942142486572 3.3192942142486572
Loss :  1.8067814111709595 3.512909173965454 3.512909173965454
Loss :  1.8031827211380005 3.4953677654266357 3.4953677654266357
Loss :  1.798466444015503 2.8008856773376465 2.8008856773376465
Loss :  1.8006699085235596 2.948723554611206 2.948723554611206
Loss :  1.7975966930389404 2.8847696781158447 2.8847696781158447
  batch 40 loss: 1.7975966930389404, 2.8847696781158447, 2.8847696781158447
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8025305271148682 3.46836519241333 3.46836519241333
Loss :  1.8025728464126587 3.019362688064575 3.019362688064575
Loss :  1.8015719652175903 3.446267604827881 3.446267604827881
Loss :  1.799415111541748 2.927440643310547 2.927440643310547
Loss :  1.80412757396698 2.824896812438965 2.824896812438965
Loss :  1.8022958040237427 3.2554986476898193 3.2554986476898193
Loss :  1.799211025238037 2.9381537437438965 2.9381537437438965
Loss :  1.8006376028060913 3.1740760803222656 3.1740760803222656
Loss :  1.7965131998062134 3.4222848415374756 3.4222848415374756
Loss :  1.8002187013626099 2.9466230869293213 2.9466230869293213
Loss :  1.7942771911621094 3.1524546146392822 3.1524546146392822
Loss :  1.8014436960220337 3.088158369064331 3.088158369064331
Loss :  1.801711082458496 2.951460599899292 2.951460599899292
Loss :  1.8050973415374756 2.6509878635406494 2.6509878635406494
Loss :  1.7992132902145386 3.0408740043640137 3.0408740043640137
Loss :  1.802490472793579 2.806603193283081 2.806603193283081
Loss :  1.8066489696502686 2.9458107948303223 2.9458107948303223
Loss :  1.8056750297546387 2.802542209625244 2.802542209625244
Loss :  1.8091782331466675 2.9256134033203125 2.9256134033203125
Loss :  1.799576759338379 2.9080684185028076 2.9080684185028076
  batch 60 loss: 1.799576759338379, 2.9080684185028076, 2.9080684185028076
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8065145015716553 2.893933057785034 2.893933057785034
Loss :  1.7993746995925903 3.0896835327148438 3.0896835327148438
Loss :  1.804451584815979 3.1303703784942627 3.1303703784942627
Loss :  1.8007408380508423 2.7425308227539062 2.7425308227539062
Loss :  1.805436134338379 2.467139482498169 2.467139482498169
Loss :  1.7046184539794922 4.4457688331604 4.4457688331604
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7057826519012451 4.455435752868652 4.455435752868652
Loss :  1.7028305530548096 4.224119186401367 4.224119186401367
Loss :  1.731176733970642 4.146152019500732 4.146152019500732
Total LOSS train 3.1048286988185003 valid 4.317868947982788
CE LOSS train 1.8020194567166843 valid 0.4327941834926605
Contrastive LOSS train 3.1048286988185003 valid 1.036538004875183
EPOCH 293:
Loss :  1.8011833429336548 2.5204379558563232 2.5204379558563232
Loss :  1.798933982849121 2.752960443496704 2.752960443496704
Loss :  1.801805853843689 2.782435178756714 2.782435178756714
Loss :  1.8022518157958984 2.760925769805908 2.760925769805908
Loss :  1.800659418106079 2.5936176776885986 2.5936176776885986
Loss :  1.8048890829086304 2.671788215637207 2.671788215637207
Loss :  1.7999541759490967 2.939775228500366 2.939775228500366
Loss :  1.800060510635376 2.7904000282287598 2.7904000282287598
Loss :  1.7962439060211182 2.9820446968078613 2.9820446968078613
Loss :  1.7972017526626587 2.7910749912261963 2.7910749912261963
Loss :  1.8073132038116455 2.9920802116394043 2.9920802116394043
Loss :  1.8020334243774414 3.2421529293060303 3.2421529293060303
Loss :  1.8066860437393188 2.7789416313171387 2.7789416313171387
Loss :  1.8058574199676514 3.0367000102996826 3.0367000102996826
Loss :  1.7933495044708252 2.9618306159973145 2.9618306159973145
Loss :  1.807737112045288 2.9404690265655518 2.9404690265655518
Loss :  1.8052533864974976 3.187932252883911 3.187932252883911
Loss :  1.8015365600585938 2.949453830718994 2.949453830718994
Loss :  1.8018194437026978 2.958514928817749 2.958514928817749
Loss :  1.7960535287857056 2.95638370513916 2.95638370513916
  batch 20 loss: 1.7960535287857056, 2.95638370513916, 2.95638370513916
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8046157360076904 2.6514899730682373 2.6514899730682373
Loss :  1.803314447402954 2.788490056991577 2.788490056991577
Loss :  1.796973705291748 2.535071611404419 2.535071611404419
Loss :  1.8023735284805298 2.6364500522613525 2.6364500522613525
Loss :  1.8024098873138428 2.9140849113464355 2.9140849113464355
Loss :  1.8028253316879272 2.686903953552246 2.686903953552246
Loss :  1.8070993423461914 2.6382927894592285 2.6382927894592285
Loss :  1.798943042755127 2.5588104724884033 2.5588104724884033
Loss :  1.8115230798721313 2.4545931816101074 2.4545931816101074
Loss :  1.7983415126800537 2.717606782913208 2.717606782913208
Loss :  1.8060276508331299 2.753300905227661 2.753300905227661
Loss :  1.7952232360839844 2.4905266761779785 2.4905266761779785
Loss :  1.8048032522201538 2.5562541484832764 2.5562541484832764
Loss :  1.804057240486145 2.4986519813537598 2.4986519813537598
Loss :  1.804179310798645 2.6275079250335693 2.6275079250335693
Loss :  1.8065071105957031 2.695751905441284 2.695751905441284
Loss :  1.8031398057937622 2.8649942874908447 2.8649942874908447
Loss :  1.7982467412948608 2.9656295776367188 2.9656295776367188
Loss :  1.8007209300994873 2.829044818878174 2.829044818878174
Loss :  1.7975836992263794 2.9999773502349854 2.9999773502349854
  batch 40 loss: 1.7975836992263794, 2.9999773502349854, 2.9999773502349854
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8024855852127075 3.184194803237915 3.184194803237915
Loss :  1.8024179935455322 2.9568750858306885 2.9568750858306885
Loss :  1.801681399345398 2.9716248512268066 2.9716248512268066
Loss :  1.7993463277816772 2.9131762981414795 2.9131762981414795
Loss :  1.8041077852249146 2.8908746242523193 2.8908746242523193
Loss :  1.8020951747894287 3.2335174083709717 3.2335174083709717
Loss :  1.7991849184036255 3.0694475173950195 3.0694475173950195
Loss :  1.800621509552002 3.472884178161621 3.472884178161621
Loss :  1.7963619232177734 3.117241382598877 3.117241382598877
Loss :  1.800276517868042 3.266998291015625 3.266998291015625
Loss :  1.794411540031433 3.284111738204956 3.284111738204956
Loss :  1.801508903503418 3.2117152214050293 3.2117152214050293
Loss :  1.8016247749328613 2.8039915561676025 2.8039915561676025
Loss :  1.8051129579544067 2.742244005203247 2.742244005203247
Loss :  1.799364447593689 3.021606922149658 3.021606922149658
Loss :  1.8026150465011597 2.893134355545044 2.893134355545044
Loss :  1.8065013885498047 3.006680727005005 3.006680727005005
Loss :  1.8055475950241089 2.977118968963623 2.977118968963623
Loss :  1.8090564012527466 3.2503457069396973 3.2503457069396973
Loss :  1.7996972799301147 3.2769126892089844 3.2769126892089844
  batch 60 loss: 1.7996972799301147, 3.2769126892089844, 3.2769126892089844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8062211275100708 2.9904513359069824 2.9904513359069824
Loss :  1.7993131875991821 3.154658555984497 3.154658555984497
Loss :  1.8042608499526978 3.067995548248291 3.067995548248291
Loss :  1.8005563020706177 3.4380128383636475 3.4380128383636475
Loss :  1.8052231073379517 3.4242334365844727 3.4242334365844727
Loss :  1.7046337127685547 4.4508490562438965 4.4508490562438965
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7058162689208984 4.362131118774414 4.362131118774414
Loss :  1.702828288078308 4.35184907913208 4.35184907913208
Loss :  1.7311742305755615 4.3460869789123535 4.3460869789123535
Total LOSS train 2.9088215497823864 valid 4.377729058265686
CE LOSS train 1.8019896323864277 valid 0.4327935576438904
Contrastive LOSS train 2.9088215497823864 valid 1.0865217447280884
EPOCH 294:
Loss :  1.8010238409042358 3.4150967597961426 3.4150967597961426
Loss :  1.7986385822296143 3.1480133533477783 3.1480133533477783
Loss :  1.8015530109405518 3.2393529415130615 3.2393529415130615
Loss :  1.8020635843276978 3.491473913192749 3.491473913192749
Loss :  1.8004390001296997 3.5827794075012207 3.5827794075012207
Loss :  1.804660439491272 3.3638103008270264 3.3638103008270264
Loss :  1.7996464967727661 3.1029276847839355 3.1029276847839355
Loss :  1.7997336387634277 3.492946147918701 3.492946147918701
Loss :  1.7960362434387207 3.4631991386413574 3.4631991386413574
Loss :  1.7967725992202759 2.7961747646331787 2.7961747646331787
Loss :  1.8069679737091064 3.1485116481781006 3.1485116481781006
Loss :  1.8018454313278198 3.4036216735839844 3.4036216735839844
Loss :  1.8064064979553223 3.505641222000122 3.505641222000122
Loss :  1.805651068687439 3.2750353813171387 3.2750353813171387
Loss :  1.7931731939315796 3.508427143096924 3.508427143096924
Loss :  1.8073675632476807 3.4436941146850586 3.4436941146850586
Loss :  1.8050336837768555 3.1820685863494873 3.1820685863494873
Loss :  1.8012242317199707 3.0755112171173096 3.0755112171173096
Loss :  1.8016365766525269 3.0804803371429443 3.0804803371429443
Loss :  1.7957724332809448 2.725139617919922 2.725139617919922
  batch 20 loss: 1.7957724332809448, 2.725139617919922, 2.725139617919922
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8044376373291016 2.6825289726257324 2.6825289726257324
Loss :  1.8031715154647827 2.756675958633423 2.756675958633423
Loss :  1.7967424392700195 2.660696506500244 2.660696506500244
Loss :  1.80219304561615 2.758297920227051 2.758297920227051
Loss :  1.8020793199539185 2.903522491455078 2.903522491455078
Loss :  1.8025585412979126 2.745069742202759 2.745069742202759
Loss :  1.8069192171096802 2.995769500732422 2.995769500732422
Loss :  1.7986931800842285 3.2210397720336914 3.2210397720336914
Loss :  1.8114932775497437 3.4850785732269287 3.4850785732269287
Loss :  1.7980226278305054 3.973217487335205 3.973217487335205
Loss :  1.8059569597244263 3.2654788494110107 3.2654788494110107
Loss :  1.7950304746627808 3.6338040828704834 3.6338040828704834
Loss :  1.8046525716781616 3.2101593017578125 3.2101593017578125
Loss :  1.8037735223770142 3.3638150691986084 3.3638150691986084
Loss :  1.8039789199829102 3.6829538345336914 3.6829538345336914
Loss :  1.8063844442367554 3.8390378952026367 3.8390378952026367
Loss :  1.8029457330703735 3.6526243686676025 3.6526243686676025
Loss :  1.7979865074157715 3.6777546405792236 3.6777546405792236
Loss :  1.8004536628723145 4.117148399353027 4.117148399353027
Loss :  1.7972770929336548 3.360865592956543 3.360865592956543
  batch 40 loss: 1.7972770929336548, 3.360865592956543, 3.360865592956543
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8022336959838867 3.395909070968628 3.395909070968628
Loss :  1.8021571636199951 2.9679510593414307 2.9679510593414307
Loss :  1.801412582397461 2.976764678955078 2.976764678955078
Loss :  1.7990964651107788 3.132829189300537 3.132829189300537
Loss :  1.8039329051971436 3.3301632404327393 3.3301632404327393
Loss :  1.8018842935562134 3.8657240867614746 3.8657240867614746
Loss :  1.7989109754562378 3.7406938076019287 3.7406938076019287
Loss :  1.8004240989685059 3.7839837074279785 3.7839837074279785
Loss :  1.796130657196045 3.480820894241333 3.480820894241333
Loss :  1.8000540733337402 3.369572162628174 3.369572162628174
Loss :  1.7941277027130127 3.086057424545288 3.086057424545288
Loss :  1.8012601137161255 3.2157294750213623 3.2157294750213623
Loss :  1.80147385597229 3.217574119567871 3.217574119567871
Loss :  1.804856300354004 2.537968397140503 2.537968397140503
Loss :  1.7991479635238647 3.1276421546936035 3.1276421546936035
Loss :  1.8022996187210083 2.8316240310668945 2.8316240310668945
Loss :  1.8064138889312744 3.4329333305358887 3.4329333305358887
Loss :  1.805477499961853 2.9594030380249023 2.9594030380249023
Loss :  1.8089710474014282 3.4137818813323975 3.4137818813323975
Loss :  1.7994247674942017 3.2500040531158447 3.2500040531158447
  batch 60 loss: 1.7994247674942017, 3.2500040531158447, 3.2500040531158447
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8062556982040405 3.171813488006592 3.171813488006592
Loss :  1.7991762161254883 3.3058559894561768 3.3058559894561768
Loss :  1.8042523860931396 2.9600822925567627 2.9600822925567627
Loss :  1.8005199432373047 3.3807032108306885 3.3807032108306885
Loss :  1.8051834106445312 2.8318071365356445 2.8318071365356445
Loss :  1.7047518491744995 4.406168460845947 4.406168460845947
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7058953046798706 4.4246320724487305 4.4246320724487305
Loss :  1.7029850482940674 4.246755599975586 4.246755599975586
Loss :  1.7311903238296509 4.397186279296875 4.397186279296875
Total LOSS train 3.264566711279062 valid 4.368685603141785
CE LOSS train 1.8017765246904813 valid 0.4327975809574127
Contrastive LOSS train 3.264566711279062 valid 1.0992965698242188
EPOCH 295:
Loss :  1.8008886575698853 3.332228422164917 3.332228422164917
Loss :  1.7987139225006104 3.494274854660034 3.494274854660034
Loss :  1.8014880418777466 3.355342388153076 3.355342388153076
Loss :  1.8019357919692993 3.059605121612549 3.059605121612549
Loss :  1.8003933429718018 3.1251220703125 3.1251220703125
Loss :  1.8046519756317139 3.390822649002075 3.390822649002075
Loss :  1.7997092008590698 3.4433178901672363 3.4433178901672363
Loss :  1.7998403310775757 3.637941598892212 3.637941598892212
Loss :  1.7959632873535156 3.216574192047119 3.216574192047119
Loss :  1.7968285083770752 3.0016746520996094 3.0016746520996094
Loss :  1.8071074485778809 3.4187521934509277 3.4187521934509277
Loss :  1.8018710613250732 3.8091721534729004 3.8091721534729004
Loss :  1.8065598011016846 3.6316192150115967 3.6316192150115967
Loss :  1.8056868314743042 3.562283515930176 3.562283515930176
Loss :  1.7931121587753296 3.1774749755859375 3.1774749755859375
Loss :  1.8075305223464966 2.9958302974700928 2.9958302974700928
Loss :  1.8051364421844482 3.021029472351074 3.021029472351074
Loss :  1.8013627529144287 3.160857915878296 3.160857915878296
Loss :  1.8016504049301147 2.981220245361328 2.981220245361328
Loss :  1.795927882194519 2.757206439971924 2.757206439971924
  batch 20 loss: 1.795927882194519, 2.757206439971924, 2.757206439971924
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8044795989990234 2.461007595062256 2.461007595062256
Loss :  1.8032252788543701 2.7353405952453613 2.7353405952453613
Loss :  1.7968355417251587 2.377149820327759 2.377149820327759
Loss :  1.8022416830062866 2.5278661251068115 2.5278661251068115
Loss :  1.80219304561615 2.6734840869903564 2.6734840869903564
Loss :  1.8027549982070923 2.8659257888793945 2.8659257888793945
Loss :  1.807097315788269 2.474733591079712 2.474733591079712
Loss :  1.7989227771759033 2.945369243621826 2.945369243621826
Loss :  1.8116697072982788 2.870227575302124 2.870227575302124
Loss :  1.7982451915740967 3.1605043411254883 3.1605043411254883
Loss :  1.8060890436172485 2.9569432735443115 2.9569432735443115
Loss :  1.7952244281768799 3.0824198722839355 3.0824198722839355
Loss :  1.8048620223999023 3.0630924701690674 3.0630924701690674
Loss :  1.8041225671768188 3.0198287963867188 3.0198287963867188
Loss :  1.8041656017303467 3.278135061264038 3.278135061264038
Loss :  1.8066271543502808 3.0470733642578125 3.0470733642578125
Loss :  1.803110957145691 3.1520204544067383 3.1520204544067383
Loss :  1.7982412576675415 2.917473077774048 2.917473077774048
Loss :  1.8006477355957031 3.241363286972046 3.241363286972046
Loss :  1.7974987030029297 3.2677392959594727 3.2677392959594727
  batch 40 loss: 1.7974987030029297, 3.2677392959594727, 3.2677392959594727
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8024429082870483 3.1786930561065674 3.1786930561065674
Loss :  1.8024648427963257 3.0802881717681885 3.0802881717681885
Loss :  1.8016170263290405 3.542501926422119 3.542501926422119
Loss :  1.7993769645690918 3.3944294452667236 3.3944294452667236
Loss :  1.8041192293167114 3.186530590057373 3.186530590057373
Loss :  1.802125096321106 3.544816255569458 3.544816255569458
Loss :  1.7991039752960205 3.293142557144165 3.293142557144165
Loss :  1.8006647825241089 3.8008086681365967 3.8008086681365967
Loss :  1.7963249683380127 3.6048009395599365 3.6048009395599365
Loss :  1.8002506494522095 3.856419801712036 3.856419801712036
Loss :  1.7943613529205322 3.02390456199646 3.02390456199646
Loss :  1.8014003038406372 2.8552937507629395 2.8552937507629395
Loss :  1.8016606569290161 3.1574695110321045 3.1574695110321045
Loss :  1.8051972389221191 2.845548391342163 2.845548391342163
Loss :  1.799306035041809 3.1594316959381104 3.1594316959381104
Loss :  1.8024537563323975 2.8824667930603027 2.8824667930603027
Loss :  1.8066067695617676 3.085782527923584 3.085782527923584
Loss :  1.8056371212005615 3.355351448059082 3.355351448059082
Loss :  1.8091412782669067 3.516108751296997 3.516108751296997
Loss :  1.7995213270187378 3.5244295597076416 3.5244295597076416
  batch 60 loss: 1.7995213270187378, 3.5244295597076416, 3.5244295597076416
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.806464672088623 4.174397945404053 4.174397945404053
Loss :  1.7993606328964233 3.4123177528381348 3.4123177528381348
Loss :  1.8043562173843384 3.541856527328491 3.541856527328491
Loss :  1.8006908893585205 2.911390781402588 2.911390781402588
Loss :  1.8053969144821167 2.7095797061920166 2.7095797061920166
Loss :  1.704086422920227 4.461811542510986 4.461811542510986
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7051856517791748 4.385375022888184 4.385375022888184
Loss :  1.702376127243042 4.282403469085693 4.282403469085693
Loss :  1.7306849956512451 4.085439205169678 4.085439205169678
Total LOSS train 3.1742739860828104 valid 4.303757309913635
CE LOSS train 1.8019178243783804 valid 0.4326712489128113
Contrastive LOSS train 3.1742739860828104 valid 1.0213598012924194
EPOCH 296:
Loss :  1.8010869026184082 2.9263830184936523 2.9263830184936523
Loss :  1.798838496208191 3.3564612865448 3.3564612865448
Loss :  1.801728367805481 2.8824357986450195 2.8824357986450195
Loss :  1.8021576404571533 3.725066900253296 3.725066900253296
Loss :  1.8005632162094116 3.2811670303344727 3.2811670303344727
Loss :  1.8048241138458252 3.655954122543335 3.655954122543335
Loss :  1.79984712600708 3.7242114543914795 3.7242114543914795
Loss :  1.800004005432129 3.380955696105957 3.380955696105957
Loss :  1.7960991859436035 3.3377366065979004 3.3377366065979004
Loss :  1.7970296144485474 3.6874032020568848 3.6874032020568848
Loss :  1.8073116540908813 3.180377244949341 3.180377244949341
Loss :  1.802014946937561 3.221560001373291 3.221560001373291
Loss :  1.8066890239715576 3.445441722869873 3.445441722869873
Loss :  1.8057717084884644 3.43669056892395 3.43669056892395
Loss :  1.7931444644927979 3.0561060905456543 3.0561060905456543
Loss :  1.8076852560043335 2.792407989501953 2.792407989501953
Loss :  1.8052650690078735 3.005068063735962 3.005068063735962
Loss :  1.8014456033706665 3.1329760551452637 3.1329760551452637
Loss :  1.8017634153366089 3.1363635063171387 3.1363635063171387
Loss :  1.7959539890289307 3.192215919494629 3.192215919494629
  batch 20 loss: 1.7959539890289307, 3.192215919494629, 3.192215919494629
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8044416904449463 2.9139320850372314 2.9139320850372314
Loss :  1.8032574653625488 2.9165475368499756 2.9165475368499756
Loss :  1.796755313873291 3.294947385787964 3.294947385787964
Loss :  1.8022340536117554 2.9060473442077637 2.9060473442077637
Loss :  1.8021240234375 3.286795139312744 3.286795139312744
Loss :  1.802714467048645 3.7442498207092285 3.7442498207092285
Loss :  1.8071115016937256 3.332617998123169 3.332617998123169
Loss :  1.798893690109253 3.6372439861297607 3.6372439861297607
Loss :  1.811747431755066 3.8636529445648193 3.8636529445648193
Loss :  1.7982046604156494 3.9487223625183105 3.9487223625183105
Loss :  1.806057333946228 3.592181444168091 3.592181444168091
Loss :  1.7951864004135132 2.973665475845337 2.973665475845337
Loss :  1.8048537969589233 3.396754741668701 3.396754741668701
Loss :  1.8040544986724854 3.2363038063049316 3.2363038063049316
Loss :  1.8040661811828613 3.4916768074035645 3.4916768074035645
Loss :  1.8066446781158447 4.025051116943359 4.025051116943359
Loss :  1.8030288219451904 3.475592851638794 3.475592851638794
Loss :  1.7982735633850098 3.3265204429626465 3.3265204429626465
Loss :  1.8005326986312866 2.847961902618408 2.847961902618408
Loss :  1.7974011898040771 2.8861453533172607 2.8861453533172607
  batch 40 loss: 1.7974011898040771, 2.8861453533172607, 2.8861453533172607
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.802365779876709 3.3110761642456055 3.3110761642456055
Loss :  1.8024064302444458 3.077228307723999 3.077228307723999
Loss :  1.8013927936553955 2.892239570617676 2.892239570617676
Loss :  1.7992808818817139 3.32116436958313 3.32116436958313
Loss :  1.8040133714675903 3.4706175327301025 3.4706175327301025
Loss :  1.802115797996521 3.435840368270874 3.435840368270874
Loss :  1.7990801334381104 3.3204050064086914 3.3204050064086914
Loss :  1.8005130290985107 3.36702036857605 3.36702036857605
Loss :  1.796376347541809 3.5464327335357666 3.5464327335357666
Loss :  1.8001168966293335 3.293875217437744 3.293875217437744
Loss :  1.7940996885299683 3.1795530319213867 3.1795530319213867
Loss :  1.8012702465057373 3.205756425857544 3.205756425857544
Loss :  1.801599144935608 3.5630686283111572 3.5630686283111572
Loss :  1.8050166368484497 3.4869985580444336 3.4869985580444336
Loss :  1.7990283966064453 3.394162178039551 3.394162178039551
Loss :  1.8023221492767334 3.6364431381225586 3.6364431381225586
Loss :  1.8065605163574219 2.9990174770355225 2.9990174770355225
Loss :  1.8054786920547485 2.8645288944244385 2.8645288944244385
Loss :  1.8090451955795288 2.8506081104278564 2.8506081104278564
Loss :  1.7994046211242676 2.808331251144409 2.808331251144409
  batch 60 loss: 1.7994046211242676, 2.808331251144409, 2.808331251144409
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8064453601837158 2.812565803527832 2.812565803527832
Loss :  1.799206018447876 2.731154680252075 2.731154680252075
Loss :  1.8043551445007324 2.485930919647217 2.485930919647217
Loss :  1.8006128072738647 2.7769792079925537 2.7769792079925537
Loss :  1.805263638496399 2.7132887840270996 2.7132887840270996
Loss :  1.70407235622406 4.4364142417907715 4.4364142417907715
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7051595449447632 4.4653544425964355 4.4653544425964355
Loss :  1.7023704051971436 4.3266730308532715 4.3266730308532715
Loss :  1.7306841611862183 4.171065330505371 4.171065330505371
Total LOSS train 3.2491981469667874 valid 4.349876761436462
CE LOSS train 1.8019104150625376 valid 0.43267104029655457
Contrastive LOSS train 3.2491981469667874 valid 1.0427663326263428
EPOCH 297:
Loss :  1.8010118007659912 2.836158037185669 2.836158037185669
Loss :  1.7989248037338257 2.7875096797943115 2.7875096797943115
Loss :  1.8016823530197144 3.0944488048553467 3.0944488048553467
Loss :  1.8021034002304077 2.959221124649048 2.959221124649048
Loss :  1.8005555868148804 3.088362455368042 3.088362455368042
Loss :  1.8048046827316284 2.8508574962615967 2.8508574962615967
Loss :  1.7999188899993896 3.473487615585327 3.473487615585327
Loss :  1.8000035285949707 3.1085121631622314 3.1085121631622314
Loss :  1.7960878610610962 2.892831563949585 2.892831563949585
Loss :  1.7970362901687622 3.24816632270813 3.24816632270813
Loss :  1.8072861433029175 3.1366536617279053 3.1366536617279053
Loss :  1.8019983768463135 3.4097609519958496 3.4097609519958496
Loss :  1.8066723346710205 3.282407522201538 3.282407522201538
Loss :  1.8057076930999756 3.180626630783081 3.180626630783081
Loss :  1.7932008504867554 2.955498218536377 2.955498218536377
Loss :  1.8076896667480469 2.6641311645507812 2.6641311645507812
Loss :  1.805229663848877 3.0438008308410645 3.0438008308410645
Loss :  1.801533818244934 2.384129047393799 2.384129047393799
Loss :  1.8017573356628418 2.6019279956817627 2.6019279956817627
Loss :  1.796098232269287 2.681842803955078 2.681842803955078
  batch 20 loss: 1.796098232269287, 2.681842803955078, 2.681842803955078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8045402765274048 2.5971715450286865 2.5971715450286865
Loss :  1.8033068180084229 2.604982614517212 2.604982614517212
Loss :  1.7969605922698975 2.451399087905884 2.451399087905884
Loss :  1.8023347854614258 2.846240997314453 2.846240997314453
Loss :  1.8023265600204468 3.087080955505371 3.087080955505371
Loss :  1.8028843402862549 3.0699706077575684 3.0699706077575684
Loss :  1.8071722984313965 3.3166344165802 3.3166344165802
Loss :  1.7990134954452515 3.1067864894866943 3.1067864894866943
Loss :  1.811710000038147 3.1182684898376465 3.1182684898376465
Loss :  1.7983371019363403 3.2251062393188477 3.2251062393188477
Loss :  1.8060740232467651 3.298961877822876 3.298961877822876
Loss :  1.7952680587768555 3.257201910018921 3.257201910018921
Loss :  1.8049067258834839 3.0531246662139893 3.0531246662139893
Loss :  1.8042014837265015 3.1178090572357178 3.1178090572357178
Loss :  1.804200530052185 3.140996217727661 3.140996217727661
Loss :  1.806657314300537 3.4834418296813965 3.4834418296813965
Loss :  1.803130865097046 2.964740037918091 2.964740037918091
Loss :  1.7983325719833374 2.957411289215088 2.957411289215088
Loss :  1.8006818294525146 3.3513286113739014 3.3513286113739014
Loss :  1.797564148902893 3.0253753662109375 3.0253753662109375
  batch 40 loss: 1.797564148902893, 3.0253753662109375, 3.0253753662109375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8024691343307495 3.1378533840179443 3.1378533840179443
Loss :  1.8025349378585815 3.2493503093719482 3.2493503093719482
Loss :  1.8016538619995117 3.206507682800293 3.206507682800293
Loss :  1.7994613647460938 3.3590452671051025 3.3590452671051025
Loss :  1.8041452169418335 3.537957191467285 3.537957191467285
Loss :  1.8021290302276611 3.659954309463501 3.659954309463501
Loss :  1.799246072769165 3.60884428024292 3.60884428024292
Loss :  1.8007014989852905 3.373913049697876 3.373913049697876
Loss :  1.7964338064193726 3.0160574913024902 3.0160574913024902
Loss :  1.8003596067428589 3.4248666763305664 3.4248666763305664
Loss :  1.794395923614502 3.0543107986450195 3.0543107986450195
Loss :  1.8014683723449707 3.039180040359497 3.039180040359497
Loss :  1.8016902208328247 3.0173885822296143 3.0173885822296143
Loss :  1.8053202629089355 3.131206512451172 3.131206512451172
Loss :  1.7993245124816895 3.2427589893341064 3.2427589893341064
Loss :  1.8026427030563354 3.078399419784546 3.078399419784546
Loss :  1.8066209554672241 2.9420764446258545 2.9420764446258545
Loss :  1.8055509328842163 2.989898443222046 2.989898443222046
Loss :  1.809141755104065 3.5594935417175293 3.5594935417175293
Loss :  1.799713373184204 2.896899938583374 2.896899938583374
  batch 60 loss: 1.799713373184204, 2.896899938583374, 2.896899938583374
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8064348697662354 3.121124029159546 3.121124029159546
Loss :  1.7994208335876465 2.6409568786621094 2.6409568786621094
Loss :  1.8043842315673828 3.1524505615234375 3.1524505615234375
Loss :  1.800709843635559 2.716804027557373 2.716804027557373
Loss :  1.8054066896438599 2.674234390258789 2.674234390258789
Loss :  1.7041443586349487 4.4370856285095215 4.4370856285095215
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7051581144332886 4.416167736053467 4.416167736053467
Loss :  1.7024002075195312 4.245650291442871 4.245650291442871
Loss :  1.7308359146118164 4.343081474304199 4.343081474304199
Total LOSS train 3.070552286734948 valid 4.360496282577515
CE LOSS train 1.8020041098961463 valid 0.4327089786529541
Contrastive LOSS train 3.070552286734948 valid 1.0857703685760498
EPOCH 298:
Loss :  1.8013075590133667 2.430047035217285 2.430047035217285
Loss :  1.798883080482483 2.7197694778442383 2.7197694778442383
Loss :  1.8018851280212402 2.9229843616485596 2.9229843616485596
Loss :  1.8024181127548218 2.5850234031677246 2.5850234031677246
Loss :  1.8007279634475708 2.741034507751465 2.741034507751465
Loss :  1.8049182891845703 2.7753396034240723 2.7753396034240723
Loss :  1.8000298738479614 3.048053026199341 3.048053026199341
Loss :  1.7999951839447021 2.634141683578491 2.634141683578491
Loss :  1.7963697910308838 2.6172704696655273 2.6172704696655273
Loss :  1.797234058380127 2.6121275424957275 2.6121275424957275
Loss :  1.8073009252548218 2.797366142272949 2.797366142272949
Loss :  1.8020951747894287 2.732013702392578 2.732013702392578
Loss :  1.806660532951355 2.654843807220459 2.654843807220459
Loss :  1.8057513236999512 2.941938638687134 2.941938638687134
Loss :  1.793440580368042 3.0303428173065186 3.0303428173065186
Loss :  1.8077011108398438 2.819309711456299 2.819309711456299
Loss :  1.8052297830581665 2.818840503692627 2.818840503692627
Loss :  1.8016278743743896 2.874455690383911 2.874455690383911
Loss :  1.801896095275879 2.785151720046997 2.785151720046997
Loss :  1.7960984706878662 2.9027016162872314 2.9027016162872314
  batch 20 loss: 1.7960984706878662, 2.9027016162872314, 2.9027016162872314
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8046221733093262 3.0167393684387207 3.0167393684387207
Loss :  1.8033676147460938 3.086188793182373 3.086188793182373
Loss :  1.7970614433288574 3.0243849754333496 3.0243849754333496
Loss :  1.8024014234542847 3.225494623184204 3.225494623184204
Loss :  1.802486538887024 3.4725561141967773 3.4725561141967773
Loss :  1.8028678894042969 3.7880735397338867 3.7880735397338867
Loss :  1.8070707321166992 3.0717549324035645 3.0717549324035645
Loss :  1.7989475727081299 3.5722172260284424 3.5722172260284424
Loss :  1.811437726020813 3.815246820449829 3.815246820449829
Loss :  1.7983428239822388 3.9112603664398193 3.9112603664398193
Loss :  1.8060015439987183 3.9150872230529785 3.9150872230529785
Loss :  1.7952065467834473 3.4188077449798584 3.4188077449798584
Loss :  1.8047538995742798 2.910433769226074 2.910433769226074
Loss :  1.804062008857727 3.363696575164795 3.363696575164795
Loss :  1.8042035102844238 3.2121834754943848 3.2121834754943848
Loss :  1.8064719438552856 3.586933135986328 3.586933135986328
Loss :  1.8030637502670288 3.3370401859283447 3.3370401859283447
Loss :  1.798191785812378 2.697239637374878 2.697239637374878
Loss :  1.8006277084350586 2.7883713245391846 2.7883713245391846
Loss :  1.7974584102630615 2.797865390777588 2.797865390777588
  batch 40 loss: 1.7974584102630615, 2.797865390777588, 2.797865390777588
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8023700714111328 3.301997184753418 3.301997184753418
Loss :  1.8023791313171387 3.1390597820281982 3.1390597820281982
Loss :  1.801624059677124 2.7106263637542725 2.7106263637542725
Loss :  1.7993584871292114 2.926020860671997 2.926020860671997
Loss :  1.8040858507156372 3.4008266925811768 3.4008266925811768
Loss :  1.8019323348999023 3.5728847980499268 3.5728847980499268
Loss :  1.7991214990615845 3.2405550479888916 3.2405550479888916
Loss :  1.800614833831787 3.2515082359313965 3.2515082359313965
Loss :  1.7962135076522827 3.7484066486358643 3.7484066486358643
Loss :  1.8002480268478394 3.3272788524627686 3.3272788524627686
Loss :  1.7942222356796265 3.306534767150879 3.306534767150879
Loss :  1.8013116121292114 3.3591721057891846 3.3591721057891846
Loss :  1.8015426397323608 3.5465006828308105 3.5465006828308105
Loss :  1.8050678968429565 3.253092050552368 3.253092050552368
Loss :  1.7991865873336792 3.31531023979187 3.31531023979187
Loss :  1.8023678064346313 3.1433768272399902 3.1433768272399902
Loss :  1.8064241409301758 3.8237357139587402 3.8237357139587402
Loss :  1.805428385734558 3.5445077419281006 3.5445077419281006
Loss :  1.808973789215088 3.2157037258148193 3.2157037258148193
Loss :  1.7994534969329834 3.7592711448669434 3.7592711448669434
  batch 60 loss: 1.7994534969329834, 3.7592711448669434, 3.7592711448669434
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8062686920166016 3.8727049827575684 3.8727049827575684
Loss :  1.799218773841858 3.932499647140503 3.932499647140503
Loss :  1.8042523860931396 3.4128644466400146 3.4128644466400146
Loss :  1.800521969795227 2.9153153896331787 2.9153153896331787
Loss :  1.805207371711731 2.435821533203125 2.435821533203125
Loss :  1.7044198513031006 4.455569744110107 4.455569744110107
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.705482006072998 4.402152061462402 4.402152061462402
Loss :  1.7027132511138916 4.321393013000488 4.321393013000488
Loss :  1.7308967113494873 4.248266696929932 4.248266696929932
Total LOSS train 3.1524600945986236 valid 4.356845378875732
CE LOSS train 1.8019632852994478 valid 0.4327241778373718
Contrastive LOSS train 3.1524600945986236 valid 1.062066674232483
EPOCH 299:
Loss :  1.8009594678878784 2.737940788269043 2.737940788269043
Loss :  1.7987024784088135 3.399350881576538 3.399350881576538
Loss :  1.8015422821044922 2.8431994915008545 2.8431994915008545
Loss :  1.8020414113998413 3.3741796016693115 3.3741796016693115
Loss :  1.8004322052001953 2.7858643531799316 2.7858643531799316
Loss :  1.8046671152114868 3.0363118648529053 3.0363118648529053
Loss :  1.7996872663497925 3.3209357261657715 3.3209357261657715
Loss :  1.7997899055480957 3.34216046333313 3.34216046333313
Loss :  1.79598867893219 3.2486112117767334 3.2486112117767334
Loss :  1.7967915534973145 3.0548367500305176 3.0548367500305176
Loss :  1.8071377277374268 3.296905279159546 3.296905279159546
Loss :  1.8019758462905884 2.9615859985351562 2.9615859985351562
Loss :  1.806531310081482 3.3539698123931885 3.3539698123931885
Loss :  1.8055751323699951 3.1178534030914307 3.1178534030914307
Loss :  1.793012261390686 3.8210692405700684 3.8210692405700684
Loss :  1.8073954582214355 3.5379562377929688 3.5379562377929688
Loss :  1.805066704750061 3.594424247741699 3.594424247741699
Loss :  1.8013657331466675 3.4330358505249023 3.4330358505249023
Loss :  1.8016626834869385 3.368297576904297 3.368297576904297
Loss :  1.795811653137207 3.6605024337768555 3.6605024337768555
  batch 20 loss: 1.795811653137207, 3.6605024337768555, 3.6605024337768555
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8043755292892456 4.095607757568359 4.095607757568359
Loss :  1.8031740188598633 3.433438301086426 3.433438301086426
Loss :  1.7967853546142578 3.360971689224243 3.360971689224243
Loss :  1.8022199869155884 3.585421323776245 3.585421323776245
Loss :  1.8020718097686768 3.18387508392334 3.18387508392334
Loss :  1.8026865720748901 3.3279318809509277 3.3279318809509277
Loss :  1.806984782218933 3.171792507171631 3.171792507171631
Loss :  1.7988057136535645 3.15579891204834 3.15579891204834
Loss :  1.8116015195846558 2.959435224533081 2.959435224533081
Loss :  1.7980128526687622 3.39900279045105 3.39900279045105
Loss :  1.8059086799621582 3.2313714027404785 3.2313714027404785
Loss :  1.7950979471206665 3.592475414276123 3.592475414276123
Loss :  1.804742693901062 2.827491521835327 2.827491521835327
Loss :  1.8038840293884277 3.3360512256622314 3.3360512256622314
Loss :  1.8039904832839966 3.265354871749878 3.265354871749878
Loss :  1.8065052032470703 3.094477653503418 3.094477653503418
Loss :  1.8029485940933228 3.3027708530426025 3.3027708530426025
Loss :  1.7982208728790283 3.1590094566345215 3.1590094566345215
Loss :  1.8004037141799927 3.52992582321167 3.52992582321167
Loss :  1.7972644567489624 3.3447928428649902 3.3447928428649902
  batch 40 loss: 1.7972644567489624, 3.3447928428649902, 3.3447928428649902
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8022161722183228 3.77508282661438 3.77508282661438
Loss :  1.8023269176483154 3.486969470977783 3.486969470977783
Loss :  1.801287055015564 3.487539768218994 3.487539768218994
Loss :  1.799220085144043 3.495741367340088 3.495741367340088
Loss :  1.8039262294769287 3.813598155975342 3.813598155975342
Loss :  1.8019717931747437 3.245140314102173 3.245140314102173
Loss :  1.799082636833191 3.0380358695983887 3.0380358695983887
Loss :  1.8003991842269897 3.165339708328247 3.165339708328247
Loss :  1.796371340751648 2.872899293899536 2.872899293899536
Loss :  1.8001055717468262 2.8148245811462402 2.8148245811462402
Loss :  1.7939276695251465 2.8315932750701904 2.8315932750701904
Loss :  1.8012478351593018 2.914801597595215 2.914801597595215
Loss :  1.8015284538269043 2.60683274269104 2.60683274269104
Loss :  1.8049391508102417 2.6891298294067383 2.6891298294067383
Loss :  1.7989078760147095 3.033273935317993 3.033273935317993
Loss :  1.8023871183395386 3.0526013374328613 3.0526013374328613
Loss :  1.80648934841156 3.2584645748138428 3.2584645748138428
Loss :  1.80533766746521 3.2532780170440674 3.2532780170440674
Loss :  1.8089290857315063 3.8159494400024414 3.8159494400024414
Loss :  1.7993170022964478 3.679311513900757 3.679311513900757
  batch 60 loss: 1.7993170022964478, 3.679311513900757, 3.679311513900757
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8063712120056152 3.365920305252075 3.365920305252075
Loss :  1.7990556955337524 3.134669303894043 3.134669303894043
Loss :  1.804287075996399 3.321784734725952 3.321784734725952
Loss :  1.8005099296569824 3.4584696292877197 3.4584696292877197
Loss :  1.8051656484603882 3.3530821800231934 3.3530821800231934
Loss :  1.7034873962402344 4.427205562591553 4.427205562591553
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7045167684555054 4.361900329589844 4.361900329589844
Loss :  1.701856255531311 4.272733688354492 4.272733688354492
Loss :  1.7300721406936646 4.291793346405029 4.291793346405029
Total LOSS train 3.2709280234116775 valid 4.3384082317352295
CE LOSS train 1.8018019914627075 valid 0.43251803517341614
Contrastive LOSS train 3.2709280234116775 valid 1.0729483366012573
EPOCH 300:
Loss :  1.8008447885513306 3.395470380783081 3.395470380783081
Loss :  1.7988567352294922 3.2200043201446533 3.2200043201446533
Loss :  1.8015133142471313 2.9008259773254395 2.9008259773254395
Loss :  1.8019301891326904 2.934507131576538 2.934507131576538
Loss :  1.8004118204116821 2.759967803955078 2.759967803955078
Loss :  1.8046631813049316 2.9032278060913086 2.9032278060913086
Loss :  1.7998336553573608 3.128131866455078 3.128131866455078
Loss :  1.7998722791671753 2.901301860809326 2.901301860809326
Loss :  1.7959667444229126 2.75831937789917 2.75831937789917
Loss :  1.7968952655792236 3.1588799953460693 3.1588799953460693
Loss :  1.8071824312210083 3.143789768218994 3.143789768218994
Loss :  1.8019596338272095 3.164973735809326 3.164973735809326
Loss :  1.8066351413726807 3.451132297515869 3.451132297515869
Loss :  1.8055675029754639 3.147369384765625 3.147369384765625
Loss :  1.7930432558059692 2.7933523654937744 2.7933523654937744
Loss :  1.8074913024902344 3.0639519691467285 3.0639519691467285
Loss :  1.8051142692565918 2.8350412845611572 2.8350412845611572
Loss :  1.8014978170394897 3.0670270919799805 3.0670270919799805
Loss :  1.801653504371643 2.4579668045043945 2.4579668045043945
Loss :  1.7959645986557007 2.7807469367980957 2.7807469367980957
  batch 20 loss: 1.7959645986557007, 2.7807469367980957, 2.7807469367980957
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8044400215148926 2.287745714187622 2.287745714187622
Loss :  1.803187608718872 2.4523825645446777 2.4523825645446777
Loss :  1.7968984842300415 2.279167652130127 2.279167652130127
Loss :  1.8022286891937256 2.8171775341033936 2.8171775341033936
Loss :  1.8021575212478638 2.842941999435425 2.842941999435425
Loss :  1.8028377294540405 2.850904703140259 2.850904703140259
Loss :  1.807079792022705 2.7958195209503174 2.7958195209503174
Loss :  1.7989602088928223 3.117138624191284 3.117138624191284
Loss :  1.811644434928894 3.0495827198028564 3.0495827198028564
Loss :  1.7981624603271484 2.8571317195892334 2.8571317195892334
Loss :  1.8059916496276855 2.9854278564453125 2.9854278564453125
Loss :  1.7951691150665283 3.011596918106079 3.011596918106079
Loss :  1.8048590421676636 3.255664348602295 3.255664348602295
Loss :  1.8040989637374878 3.318570137023926 3.318570137023926
Loss :  1.80412757396698 3.2606141567230225 3.2606141567230225
Loss :  1.806632161140442 3.188390016555786 3.188390016555786
Loss :  1.80306077003479 3.5345535278320312 3.5345535278320312
Loss :  1.798296332359314 3.214773416519165 3.214773416519165
Loss :  1.8005210161209106 3.116748332977295 3.116748332977295
Loss :  1.7973982095718384 2.7371420860290527 2.7371420860290527
  batch 40 loss: 1.7973982095718384, 2.7371420860290527, 2.7371420860290527
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8023258447647095 3.2272040843963623 3.2272040843963623
Loss :  1.802507996559143 3.215160608291626 3.215160608291626
Loss :  1.8014910221099854 3.1471316814422607 3.1471316814422607
Loss :  1.7993857860565186 3.2239761352539062 3.2239761352539062
Loss :  1.8040486574172974 3.3013927936553955 3.3013927936553955
Loss :  1.8020912408828735 3.3950319290161133 3.3950319290161133
Loss :  1.799171805381775 3.4378392696380615 3.4378392696380615
Loss :  1.8005502223968506 3.350446939468384 3.350446939468384
Loss :  1.7964471578598022 3.1580770015716553 3.1580770015716553
Loss :  1.8002755641937256 3.442542552947998 3.442542552947998
Loss :  1.7940819263458252 2.8167500495910645 2.8167500495910645
Loss :  1.8012984991073608 2.703265905380249 2.703265905380249
Loss :  1.8016108274459839 3.4495742321014404 3.4495742321014404
Loss :  1.8050923347473145 3.505056858062744 3.505056858062744
Loss :  1.7990117073059082 3.1314990520477295 3.1314990520477295
Loss :  1.8023676872253418 3.006546974182129 3.006546974182129
Loss :  1.806589126586914 3.585753917694092 3.585753917694092
Loss :  1.80549955368042 3.501131534576416 3.501131534576416
Loss :  1.8089816570281982 3.4559195041656494 3.4559195041656494
Loss :  1.7992761135101318 3.509765625 3.509765625
  batch 60 loss: 1.7992761135101318, 3.509765625, 3.509765625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8065065145492554 3.4615097045898438 3.4615097045898438
Loss :  1.7991076707839966 3.8456473350524902 3.8456473350524902
Loss :  1.8043692111968994 3.7188000679016113 3.7188000679016113
Loss :  1.800632119178772 3.4169762134552 3.4169762134552
Loss :  1.8053175210952759 2.9121506214141846 2.9121506214141846
Loss :  1.7024215459823608 4.394210338592529 4.394210338592529
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.7033979892730713 4.439358234405518 4.439358234405518
Loss :  1.7009165287017822 4.325594902038574 4.325594902038574
Loss :  1.7291783094406128 4.182339668273926 4.182339668273926
Total LOSS train 3.105517112291776 valid 4.335375785827637
CE LOSS train 1.8018874920331516 valid 0.4322945773601532
Contrastive LOSS train 3.105517112291776 valid 1.0455849170684814
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: | 0.136 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 0.345 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 0.345 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: \ 0.345 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: | 0.345 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 0.439 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 0.634 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: \ 0.634 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: | 0.743 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 0.821 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 0.993 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: \ 1.071 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: | 1.235 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 1.329 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 1.509 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: \ 1.538 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: | 1.538 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 1.538 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 1.538 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: \ 1.538 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: | 1.538 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 1.538 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: - 1.538 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: \ 1.538 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: | 1.538 MB of 1.538 MB uploaded (0.000 MB deduped)wandb: / 1.538 MB of 1.538 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                    epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:                       lr ‚ñà‚ñà‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   train contrastive loss ‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñÖ‚ñá‚ñà‚ñá‚ñà‚ñà‚ñÑ‚ñá‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb: train cross-entropy loss ‚ñà‚ñÖ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:               train loss ‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñÖ‚ñá‚ñà‚ñá‚ñà‚ñà‚ñÑ‚ñá‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:     val contrastive loss ‚ñÜ‚ñá‚ñà‚ñÜ‚ñá‚ñà‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñÖ‚ñÇ‚ñÉ‚ñÖ‚ñÇ‚ñÉ‚ñÖ‚ñÉ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñÜ‚ñá‚ñÜ‚ñÜ
wandb:   val cross-entropy loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                 val loss ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñà‚ñÜ‚ñÇ‚ñÑ‚ñÖ‚ñÇ‚ñÇ‚ñÖ‚ñÇ‚ñÅ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá
wandb: 
wandb: Run summary:
wandb:            best_val_loss 3.52059
wandb:                    epoch 300
wandb:                       lr 0.0
wandb:   train contrastive loss 3.10552
wandb: train cross-entropy loss 1.80189
wandb:               train loss 3.10552
wandb:     val contrastive loss 1.04558
wandb:   val cross-entropy loss 0.43229
wandb:                 val loss 4.33538
wandb: 
wandb: Synced winter-bush-28: https://wandb.ai/harsh21122/part_segmentation/runs/16u9ivv6
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230206_105659-16u9ivv6/logs

Length of Train dataset : 646 and Test dataset : 35
Length of Train loader : 65 and Test loader : 4
self.partnames :  ['background', 'head', 'neck', 'torso', 'tail', 'legs']
prompts :  [' the background of the cat.', ' the head of the cat.', ' the neck of the cat.', ' the torso of the cat.', ' the tail of the cat.', ' the legs of the cat.']
model device :  cuda
self.prompts : torch.Size([6, 77])
self.prompts : torch.Size([6, 1024])
self.prompts rquires grad :  <built-in method requires_grad_ of Tensor object at 0x7efeed7e2590>
layer: relu3_2,relu5_4
weighs: [0.33, 1.0]
/home/harsh21122/.conda/envs/part/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
/home/harsh21122/.conda/envs/part/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Printing parameters and their gradient
gamma True
image_encoder.0.weight False
image_encoder.1.weight False
image_encoder.1.bias False
image_encoder.3.weight False
image_encoder.4.weight False
image_encoder.4.bias False
image_encoder.6.weight False
image_encoder.7.weight False
image_encoder.7.bias False
image_encoder.10.0.conv1.weight False
image_encoder.10.0.bn1.weight False
image_encoder.10.0.bn1.bias False
image_encoder.10.0.conv2.weight False
image_encoder.10.0.bn2.weight False
image_encoder.10.0.bn2.bias False
image_encoder.10.0.conv3.weight False
image_encoder.10.0.bn3.weight False
image_encoder.10.0.bn3.bias False
image_encoder.10.0.downsample.0.weight False
image_encoder.10.0.downsample.1.weight False
image_encoder.10.0.downsample.1.bias False
image_encoder.10.1.conv1.weight False
image_encoder.10.1.bn1.weight False
image_encoder.10.1.bn1.bias False
image_encoder.10.1.conv2.weight False
image_encoder.10.1.bn2.weight False
image_encoder.10.1.bn2.bias False
image_encoder.10.1.conv3.weight False
image_encoder.10.1.bn3.weight False
image_encoder.10.1.bn3.bias False
image_encoder.10.2.conv1.weight False
image_encoder.10.2.bn1.weight False
image_encoder.10.2.bn1.bias False
image_encoder.10.2.conv2.weight False
image_encoder.10.2.bn2.weight False
image_encoder.10.2.bn2.bias False
image_encoder.10.2.conv3.weight False
image_encoder.10.2.bn3.weight False
image_encoder.10.2.bn3.bias False
image_encoder.11.0.conv1.weight False
image_encoder.11.0.bn1.weight False
image_encoder.11.0.bn1.bias False
image_encoder.11.0.conv2.weight False
image_encoder.11.0.bn2.weight False
image_encoder.11.0.bn2.bias False
image_encoder.11.0.conv3.weight False
image_encoder.11.0.bn3.weight False
image_encoder.11.0.bn3.bias False
image_encoder.11.0.downsample.0.weight False
image_encoder.11.0.downsample.1.weight False
image_encoder.11.0.downsample.1.bias False
image_encoder.11.1.conv1.weight False
image_encoder.11.1.bn1.weight False
image_encoder.11.1.bn1.bias False
image_encoder.11.1.conv2.weight False
image_encoder.11.1.bn2.weight False
image_encoder.11.1.bn2.bias False
image_encoder.11.1.conv3.weight False
image_encoder.11.1.bn3.weight False
image_encoder.11.1.bn3.bias False
image_encoder.11.2.conv1.weight False
image_encoder.11.2.bn1.weight False
image_encoder.11.2.bn1.bias False
image_encoder.11.2.conv2.weight False
image_encoder.11.2.bn2.weight False
image_encoder.11.2.bn2.bias False
image_encoder.11.2.conv3.weight False
image_encoder.11.2.bn3.weight False
image_encoder.11.2.bn3.bias False
image_encoder.11.3.conv1.weight False
image_encoder.11.3.bn1.weight False
image_encoder.11.3.bn1.bias False
image_encoder.11.3.conv2.weight False
image_encoder.11.3.bn2.weight False
image_encoder.11.3.bn2.bias False
image_encoder.11.3.conv3.weight False
image_encoder.11.3.bn3.weight False
image_encoder.11.3.bn3.bias False
image_encoder.12.0.conv1.weight False
image_encoder.12.0.bn1.weight False
image_encoder.12.0.bn1.bias False
image_encoder.12.0.conv2.weight False
image_encoder.12.0.bn2.weight False
image_encoder.12.0.bn2.bias False
image_encoder.12.0.conv3.weight False
image_encoder.12.0.bn3.weight False
image_encoder.12.0.bn3.bias False
image_encoder.12.0.downsample.0.weight False
image_encoder.12.0.downsample.1.weight False
image_encoder.12.0.downsample.1.bias False
image_encoder.12.1.conv1.weight False
image_encoder.12.1.bn1.weight False
image_encoder.12.1.bn1.bias False
image_encoder.12.1.conv2.weight False
image_encoder.12.1.bn2.weight False
image_encoder.12.1.bn2.bias False
image_encoder.12.1.conv3.weight False
image_encoder.12.1.bn3.weight False
image_encoder.12.1.bn3.bias False
image_encoder.12.2.conv1.weight False
image_encoder.12.2.bn1.weight False
image_encoder.12.2.bn1.bias False
image_encoder.12.2.conv2.weight False
image_encoder.12.2.bn2.weight False
image_encoder.12.2.bn2.bias False
image_encoder.12.2.conv3.weight False
image_encoder.12.2.bn3.weight False
image_encoder.12.2.bn3.bias False
image_encoder.12.3.conv1.weight False
image_encoder.12.3.bn1.weight False
image_encoder.12.3.bn1.bias False
image_encoder.12.3.conv2.weight False
image_encoder.12.3.bn2.weight False
image_encoder.12.3.bn2.bias False
image_encoder.12.3.conv3.weight False
image_encoder.12.3.bn3.weight False
image_encoder.12.3.bn3.bias False
image_encoder.12.4.conv1.weight False
image_encoder.12.4.bn1.weight False
image_encoder.12.4.bn1.bias False
image_encoder.12.4.conv2.weight False
image_encoder.12.4.bn2.weight False
image_encoder.12.4.bn2.bias False
image_encoder.12.4.conv3.weight False
image_encoder.12.4.bn3.weight False
image_encoder.12.4.bn3.bias False
image_encoder.12.5.conv1.weight False
image_encoder.12.5.bn1.weight False
image_encoder.12.5.bn1.bias False
image_encoder.12.5.conv2.weight False
image_encoder.12.5.bn2.weight False
image_encoder.12.5.bn2.bias False
image_encoder.12.5.conv3.weight False
image_encoder.12.5.bn3.weight False
image_encoder.12.5.bn3.bias False
image_encoder.13.0.conv1.weight False
image_encoder.13.0.bn1.weight False
image_encoder.13.0.bn1.bias False
image_encoder.13.0.conv2.weight False
image_encoder.13.0.bn2.weight False
image_encoder.13.0.bn2.bias False
image_encoder.13.0.conv3.weight False
image_encoder.13.0.bn3.weight False
image_encoder.13.0.bn3.bias False
image_encoder.13.0.downsample.0.weight False
image_encoder.13.0.downsample.1.weight False
image_encoder.13.0.downsample.1.bias False
image_encoder.13.1.conv1.weight False
image_encoder.13.1.bn1.weight False
image_encoder.13.1.bn1.bias False
image_encoder.13.1.conv2.weight False
image_encoder.13.1.bn2.weight False
image_encoder.13.1.bn2.bias False
image_encoder.13.1.conv3.weight False
image_encoder.13.1.bn3.weight False
image_encoder.13.1.bn3.bias False
image_encoder.13.2.conv1.weight False
image_encoder.13.2.bn1.weight False
image_encoder.13.2.bn1.bias False
image_encoder.13.2.conv2.weight False
image_encoder.13.2.bn2.weight False
image_encoder.13.2.bn2.bias False
image_encoder.13.2.conv3.weight False
image_encoder.13.2.bn3.weight False
image_encoder.13.2.bn3.bias False
attnpool.positional_embedding True
attnpool.k_proj.weight True
attnpool.k_proj.bias True
attnpool.q_proj.weight True
attnpool.q_proj.bias True
attnpool.v_proj.weight True
attnpool.v_proj.bias True
attnpool.c_proj.weight True
attnpool.c_proj.bias True
align_context.memory_proj.0.weight True
align_context.memory_proj.0.bias True
align_context.memory_proj.1.weight True
align_context.memory_proj.1.bias True
align_context.memory_proj.2.weight True
align_context.memory_proj.2.bias True
align_context.text_proj.0.weight True
align_context.text_proj.0.bias True
align_context.text_proj.1.weight True
align_context.text_proj.1.bias True
align_context.decoder.0.self_attn.q_proj.weight True
align_context.decoder.0.self_attn.k_proj.weight True
align_context.decoder.0.self_attn.v_proj.weight True
align_context.decoder.0.self_attn.proj.weight True
align_context.decoder.0.self_attn.proj.bias True
align_context.decoder.0.cross_attn.q_proj.weight True
align_context.decoder.0.cross_attn.k_proj.weight True
align_context.decoder.0.cross_attn.v_proj.weight True
align_context.decoder.0.cross_attn.proj.weight True
align_context.decoder.0.cross_attn.proj.bias True
align_context.decoder.0.norm1.weight True
align_context.decoder.0.norm1.bias True
align_context.decoder.0.norm2.weight True
align_context.decoder.0.norm2.bias True
align_context.decoder.0.norm3.weight True
align_context.decoder.0.norm3.bias True
align_context.decoder.0.mlp.0.weight True
align_context.decoder.0.mlp.0.bias True
align_context.decoder.0.mlp.3.weight True
align_context.decoder.0.mlp.3.bias True
align_context.decoder.1.self_attn.q_proj.weight True
align_context.decoder.1.self_attn.k_proj.weight True
align_context.decoder.1.self_attn.v_proj.weight True
align_context.decoder.1.self_attn.proj.weight True
align_context.decoder.1.self_attn.proj.bias True
align_context.decoder.1.cross_attn.q_proj.weight True
align_context.decoder.1.cross_attn.k_proj.weight True
align_context.decoder.1.cross_attn.v_proj.weight True
align_context.decoder.1.cross_attn.proj.weight True
align_context.decoder.1.cross_attn.proj.bias True
align_context.decoder.1.norm1.weight True
align_context.decoder.1.norm1.bias True
align_context.decoder.1.norm2.weight True
align_context.decoder.1.norm2.bias True
align_context.decoder.1.norm3.weight True
align_context.decoder.1.norm3.bias True
align_context.decoder.1.mlp.0.weight True
align_context.decoder.1.mlp.0.bias True
align_context.decoder.1.mlp.3.weight True
align_context.decoder.1.mlp.3.bias True
align_context.decoder.2.self_attn.q_proj.weight True
align_context.decoder.2.self_attn.k_proj.weight True
align_context.decoder.2.self_attn.v_proj.weight True
align_context.decoder.2.self_attn.proj.weight True
align_context.decoder.2.self_attn.proj.bias True
align_context.decoder.2.cross_attn.q_proj.weight True
align_context.decoder.2.cross_attn.k_proj.weight True
align_context.decoder.2.cross_attn.v_proj.weight True
align_context.decoder.2.cross_attn.proj.weight True
align_context.decoder.2.cross_attn.proj.bias True
align_context.decoder.2.norm1.weight True
align_context.decoder.2.norm1.bias True
align_context.decoder.2.norm2.weight True
align_context.decoder.2.norm2.bias True
align_context.decoder.2.norm3.weight True
align_context.decoder.2.norm3.bias True
align_context.decoder.2.mlp.0.weight True
align_context.decoder.2.mlp.0.bias True
align_context.decoder.2.mlp.3.weight True
align_context.decoder.2.mlp.3.bias True
align_context.decoder.3.self_attn.q_proj.weight True
align_context.decoder.3.self_attn.k_proj.weight True
align_context.decoder.3.self_attn.v_proj.weight True
align_context.decoder.3.self_attn.proj.weight True
align_context.decoder.3.self_attn.proj.bias True
align_context.decoder.3.cross_attn.q_proj.weight True
align_context.decoder.3.cross_attn.k_proj.weight True
align_context.decoder.3.cross_attn.v_proj.weight True
align_context.decoder.3.cross_attn.proj.weight True
align_context.decoder.3.cross_attn.proj.bias True
align_context.decoder.3.norm1.weight True
align_context.decoder.3.norm1.bias True
align_context.decoder.3.norm2.weight True
align_context.decoder.3.norm2.bias True
align_context.decoder.3.norm3.weight True
align_context.decoder.3.norm3.bias True
align_context.decoder.3.mlp.0.weight True
align_context.decoder.3.mlp.0.bias True
align_context.decoder.3.mlp.3.weight True
align_context.decoder.3.mlp.3.bias True
align_context.decoder.4.self_attn.q_proj.weight True
align_context.decoder.4.self_attn.k_proj.weight True
align_context.decoder.4.self_attn.v_proj.weight True
align_context.decoder.4.self_attn.proj.weight True
align_context.decoder.4.self_attn.proj.bias True
align_context.decoder.4.cross_attn.q_proj.weight True
align_context.decoder.4.cross_attn.k_proj.weight True
align_context.decoder.4.cross_attn.v_proj.weight True
align_context.decoder.4.cross_attn.proj.weight True
align_context.decoder.4.cross_attn.proj.bias True
align_context.decoder.4.norm1.weight True
align_context.decoder.4.norm1.bias True
align_context.decoder.4.norm2.weight True
align_context.decoder.4.norm2.bias True
align_context.decoder.4.norm3.weight True
align_context.decoder.4.norm3.bias True
align_context.decoder.4.mlp.0.weight True
align_context.decoder.4.mlp.0.bias True
align_context.decoder.4.mlp.3.weight True
align_context.decoder.4.mlp.3.bias True
align_context.decoder.5.self_attn.q_proj.weight True
align_context.decoder.5.self_attn.k_proj.weight True
align_context.decoder.5.self_attn.v_proj.weight True
align_context.decoder.5.self_attn.proj.weight True
align_context.decoder.5.self_attn.proj.bias True
align_context.decoder.5.cross_attn.q_proj.weight True
align_context.decoder.5.cross_attn.k_proj.weight True
align_context.decoder.5.cross_attn.v_proj.weight True
align_context.decoder.5.cross_attn.proj.weight True
align_context.decoder.5.cross_attn.proj.bias True
align_context.decoder.5.norm1.weight True
align_context.decoder.5.norm1.bias True
align_context.decoder.5.norm2.weight True
align_context.decoder.5.norm2.bias True
align_context.decoder.5.norm3.weight True
align_context.decoder.5.norm3.bias True
align_context.decoder.5.mlp.0.weight True
align_context.decoder.5.mlp.0.bias True
align_context.decoder.5.mlp.3.weight True
align_context.decoder.5.mlp.3.bias True
align_context.out_proj.0.weight True
align_context.out_proj.0.bias True
align_context.out_proj.1.weight True
align_context.out_proj.1.bias True
decoder.conv_layers.0.weight True
decoder.conv_layers.0.bias True
decoder.conv_layers.2.weight True
decoder.conv_layers.2.bias True
decoder.conv_layers.3.weight True
decoder.conv_layers.3.bias True
decoder.conv_layers.5.weight True
decoder.conv_layers.5.bias True
Total epochs to be executed :  300
EPOCH 1:
Loss :  2.3226675987243652 4.4048380851745605 6.727505683898926
Loss :  2.2364747524261475 4.731750965118408 6.968225479125977
Loss :  2.199382781982422 4.386478900909424 6.585861682891846
Loss :  2.286421775817871 4.6081366539001465 6.894558429718018
Loss :  2.36600923538208 4.530268669128418 6.896277904510498
Loss :  2.3084943294525146 5.0122504234313965 7.320744514465332
Loss :  2.3117001056671143 4.708780288696289 7.020480155944824
Loss :  2.231208086013794 4.620814800262451 6.852023124694824
Loss :  2.1850967407226562 4.744645118713379 6.929741859436035
Loss :  2.2065608501434326 4.459043502807617 6.665604591369629
Loss :  2.228583335876465 4.466963291168213 6.695546627044678
Loss :  2.1945855617523193 5.118858814239502 7.313444137573242
Loss :  2.1784801483154297 4.690219402313232 6.868699550628662
Loss :  2.267791509628296 4.552931308746338 6.820722579956055
Loss :  2.2892329692840576 4.595666885375977 6.884900093078613
Loss :  2.2382726669311523 4.702712535858154 6.940985202789307
Loss :  2.2193350791931152 4.520110130310059 6.739445209503174
Loss :  2.2088358402252197 4.669825553894043 6.878661155700684
Loss :  2.227154016494751 4.493598461151123 6.720752716064453
Loss :  2.2401652336120605 4.647703647613525 6.887868881225586
  batch 20 loss: 2.2401652336120605, 4.647703647613525, 6.887868881225586
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  2.294579029083252 4.6960344314575195 6.9906134605407715
Loss :  2.3212952613830566 4.774768352508545 7.096063613891602
Loss :  2.2156715393066406 4.627964973449707 6.843636512756348
Loss :  2.2415170669555664 4.788264751434326 7.029781818389893
Loss :  2.367122173309326 4.79742431640625 7.164546489715576
Loss :  2.322537899017334 4.578660488128662 6.901198387145996
Loss :  2.329580783843994 4.967879295349121 7.297460079193115
Loss :  2.3385009765625 4.601809024810791 6.940310001373291
Loss :  2.30715274810791 4.535536289215088 6.842689037322998
Loss :  2.341708183288574 4.599843978881836 6.94155216217041
Loss :  2.195122241973877 4.774156093597412 6.969278335571289
Loss :  2.2821784019470215 4.776863098144531 7.059041500091553
Loss :  2.1924028396606445 4.598001480102539 6.790404319763184
Loss :  2.2501416206359863 4.44429874420166 6.6944403648376465
Loss :  2.263972282409668 4.336316108703613 6.600288391113281
Loss :  2.2955985069274902 4.455822944641113 6.7514214515686035
Loss :  2.262434959411621 4.615671634674072 6.878106594085693
Loss :  2.323721170425415 4.635319709777832 6.959040641784668
Loss :  2.2617897987365723 4.43225622177124 6.6940460205078125
Loss :  2.21551251411438 4.265969753265381 6.48148250579834
  batch 40 loss: 2.21551251411438, 4.265969753265381, 6.48148250579834
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  2.244691848754883 4.484655380249023 6.729347229003906
Loss :  2.1945176124572754 4.365474224090576 6.559991836547852
Loss :  2.1968307495117188 4.454446792602539 6.651277542114258
Loss :  2.303429365158081 4.258214950561523 6.561644554138184
Loss :  2.207371234893799 4.38991117477417 6.597282409667969
Loss :  2.2942535877227783 4.474517822265625 6.768771171569824
Loss :  2.3079848289489746 4.340452671051025 6.6484375
Loss :  2.1927316188812256 4.5868048667907715 6.779536247253418
Loss :  2.250384569168091 4.315476894378662 6.565861701965332
Loss :  2.1735527515411377 4.423220634460449 6.596773147583008
Loss :  2.1731162071228027 4.555377006530762 6.7284932136535645
Loss :  2.195253849029541 4.239751815795898 6.4350056648254395
Loss :  2.2543933391571045 4.56133508682251 6.815728187561035
Loss :  2.2596590518951416 4.326554775238037 6.586214065551758
Loss :  2.1773176193237305 4.364845275878906 6.542162895202637
Loss :  2.242401599884033 4.537018775939941 6.779420375823975
Loss :  2.2865493297576904 4.63559103012085 6.922140121459961
Loss :  2.2205982208251953 4.469207286834717 6.689805507659912
Loss :  2.252877950668335 4.334109306335449 6.586987495422363
Loss :  2.2435359954833984 4.4280548095703125 6.671590805053711
  batch 60 loss: 2.2435359954833984, 4.4280548095703125, 6.671590805053711
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  2.243537425994873 4.623682975769043 6.867220401763916
Loss :  2.158198595046997 4.403600215911865 6.561799049377441
Loss :  2.2497150897979736 4.48260498046875 6.7323198318481445
Loss :  2.195600748062134 4.224049091339111 6.419650077819824
Loss :  2.207864284515381 3.8948466777801514 6.102710723876953
Loss :  4.3302507400512695 4.3997602462768555 8.730010986328125
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3], device='cuda:0')
Loss :  4.4003095626831055 4.3163886070251465 8.716697692871094
Loss :  4.519657135009766 4.343069076538086 8.862726211547852
Loss :  4.379485607147217 4.130575656890869 8.510061264038086
Total LOSS train 6.791348046522874 valid 8.704874038696289
CE LOSS train 2.2506978475130524 valid 1.0948714017868042
Contrastive LOSS train 4.540650210013756 valid 1.0326439142227173
Saved best model. Old loss 1000000.0 and new best loss 8.704874038696289
EPOCH 2:
Loss :  2.2008216381073 4.332912921905518 6.533734321594238
Loss :  2.27264404296875 4.430624008178711 6.703268051147461
Loss :  2.2309789657592773 4.290442943572998 6.521421909332275
Loss :  2.1492066383361816 4.206786155700684 6.355992794036865
Loss :  2.2048821449279785 4.409945487976074 6.614827632904053
Loss :  2.1854958534240723 4.295323848724365 6.4808197021484375
Loss :  2.095137119293213 4.3091559410095215 6.404293060302734
Loss :  2.1314070224761963 4.616444110870361 6.747851371765137
Loss :  2.178682804107666 4.639633655548096 6.818316459655762
Loss :  2.091317892074585 4.578026294708252 6.669343948364258
Loss :  2.0756545066833496 4.503660202026367 6.579314708709717
Loss :  2.1338067054748535 4.636269569396973 6.770076274871826
Loss :  2.149808883666992 4.575797080993652 6.7256059646606445
Loss :  2.1989681720733643 4.503153324127197 6.702121734619141
Loss :  2.1804962158203125 4.320868015289307 6.501364231109619
Loss :  2.1595370769500732 4.555938720703125 6.715476036071777
Loss :  2.130080461502075 4.526176929473877 6.656257629394531
Loss :  2.110980749130249 4.143695831298828 6.254676818847656
Loss :  2.084376335144043 4.4175004959106445 6.5018768310546875
Loss :  2.14449143409729 4.504714012145996 6.649205207824707
  batch 20 loss: 2.14449143409729, 4.504714012145996, 6.649205207824707
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  2.1095924377441406 4.568234443664551 6.677826881408691
Loss :  2.1696009635925293 4.431084632873535 6.6006855964660645
Loss :  2.1256446838378906 4.626572132110596 6.752216815948486
Loss :  2.1265857219696045 4.280007839202881 6.406593322753906
Loss :  2.1487746238708496 4.403267860412598 6.552042484283447
Loss :  2.14294695854187 4.592390537261963 6.735337257385254
Loss :  2.129371404647827 4.307051181793213 6.436422348022461
Loss :  2.1605067253112793 4.450145244598389 6.610651969909668
Loss :  2.165152072906494 4.311884880065918 6.477036952972412
Loss :  2.099679470062256 4.158519268035889 6.2581987380981445
Loss :  2.105724334716797 4.315809726715088 6.421534061431885
Loss :  2.0911803245544434 4.467541694641113 6.558722019195557
Loss :  2.077937126159668 4.336129188537598 6.414066314697266
Loss :  2.0527145862579346 4.459169864654541 6.511884689331055
Loss :  2.1410679817199707 4.182607173919678 6.323675155639648
Loss :  2.1155569553375244 4.268364906311035 6.3839216232299805
Loss :  2.1174533367156982 4.240316867828369 6.357769966125488
Loss :  2.105926752090454 4.1866326332092285 6.292559623718262
Loss :  2.1343464851379395 4.261224746704102 6.395571231842041
Loss :  2.078888416290283 4.410590171813965 6.489478588104248
  batch 40 loss: 2.078888416290283, 4.410590171813965, 6.489478588104248
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  2.092850923538208 4.354862689971924 6.447713851928711
Loss :  2.0914347171783447 4.450240612030029 6.541675567626953
Loss :  2.090785026550293 4.248751163482666 6.339536190032959
Loss :  2.093611478805542 4.107552528381348 6.201164245605469
Loss :  2.097775936126709 4.191410541534424 6.289186477661133
Loss :  2.093073844909668 4.2686448097229 6.361718654632568
Loss :  2.1181118488311768 4.172696590423584 6.29080867767334
Loss :  2.0622684955596924 4.253837585449219 6.316105842590332
Loss :  2.042870283126831 4.224267959594727 6.267138481140137
Loss :  2.0960800647735596 4.200719356536865 6.296799659729004
Loss :  2.035581350326538 4.325505256652832 6.361086845397949
Loss :  2.060145854949951 4.261466026306152 6.3216118812561035
Loss :  2.0409529209136963 4.459921360015869 6.5008745193481445
Loss :  2.0457396507263184 4.327662944793701 6.3734025955200195
Loss :  2.047063112258911 4.2432475090026855 6.290310859680176
Loss :  2.0324645042419434 4.491388320922852 6.523852825164795
Loss :  2.0561254024505615 4.443450450897217 6.499575614929199
Loss :  2.052943706512451 4.264449596405029 6.3173933029174805
Loss :  2.0855424404144287 4.454946517944336 6.540489196777344
Loss :  1.9998739957809448 4.127182483673096 6.12705659866333
  batch 60 loss: 1.9998739957809448, 4.127182483673096, 6.12705659866333
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  2.048492193222046 4.163347244262695 6.21183967590332
Loss :  2.000145196914673 4.206158638000488 6.206303596496582
Loss :  2.0557079315185547 4.040716648101807 6.096424579620361
Loss :  2.047210931777954 4.190189361572266 6.237400054931641
Loss :  2.01650333404541 3.9040937423706055 5.920597076416016
Loss :  1.968957781791687 4.284080505371094 6.25303840637207
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2, 3, 4, 5], device='cuda:0')
Loss :  2.009078025817871 4.287651538848877 6.296729564666748
Loss :  1.9731721878051758 4.052384376525879 6.025556564331055
Loss :  2.071117877960205 4.025062561035156 6.096180438995361
Total LOSS train 6.452955495394193 valid 6.167876243591309
CE LOSS train 2.1078581718298106 valid 0.5177794694900513
Contrastive LOSS train 4.345097299722525 valid 1.006265640258789
Saved best model. Old loss 8.704874038696289 and new best loss 6.167876243591309
EPOCH 3:
Loss :  2.0550618171691895 4.204942226409912 6.260004043579102
Loss :  2.0826385021209717 4.62562370300293 6.7082624435424805
Loss :  2.0206222534179688 4.240175724029541 6.26079797744751
Loss :  2.0359957218170166 4.303687572479248 6.339683532714844
Loss :  2.038968324661255 4.08485746383667 6.123826026916504
Loss :  2.0170044898986816 4.344675064086914 6.361679553985596
Loss :  2.019352436065674 4.1882123947143555 6.207564830780029
Loss :  2.04703426361084 4.059605598449707 6.106639862060547
Loss :  2.03847336769104 4.033820629119873 6.072294235229492
Loss :  2.0080008506774902 4.209364414215088 6.217365264892578
Loss :  2.0355193614959717 4.24194860458374 6.277467727661133
Loss :  2.033411741256714 4.2802414894104 6.313652992248535
Loss :  2.0228748321533203 4.068265914916992 6.0911407470703125
Loss :  1.9747201204299927 4.217965126037598 6.192685127258301
Loss :  2.024054527282715 4.521095275878906 6.545149803161621
Loss :  2.0014584064483643 4.2713165283203125 6.272774696350098
Loss :  2.026799201965332 4.395886421203613 6.422685623168945
Loss :  2.0194034576416016 4.352042198181152 6.371445655822754
Loss :  1.999504566192627 3.634321451187134 5.63382625579834
Loss :  1.9768105745315552 4.141637802124023 6.118448257446289
  batch 20 loss: 1.9768105745315552, 4.141637802124023, 6.118448257446289
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9902154207229614 3.915170192718506 5.905385494232178
Loss :  2.022859811782837 4.046021461486816 6.068881034851074
Loss :  2.0161242485046387 4.19421911239624 6.210343360900879
Loss :  1.9754091501235962 3.9857285022735596 5.961137771606445
Loss :  1.9860198497772217 4.274008750915527 6.260028839111328
Loss :  2.0153326988220215 4.054110050201416 6.0694427490234375
Loss :  1.992767095565796 4.117292881011963 6.11005973815918
Loss :  2.008425235748291 4.107363700866699 6.11578893661499
Loss :  2.066983222961426 4.124491214752197 6.191474437713623
Loss :  2.0293118953704834 4.22034215927124 6.2496538162231445
Loss :  2.016648530960083 4.402966499328613 6.419614791870117
Loss :  1.9693204164505005 4.464999675750732 6.434319972991943
Loss :  1.9775481224060059 4.244436264038086 6.221984386444092
Loss :  1.9776830673217773 4.227573871612549 6.205256938934326
Loss :  1.9791489839553833 4.0651655197143555 6.044314384460449
Loss :  2.021989583969116 4.110917568206787 6.132906913757324
Loss :  1.993573784828186 3.9200263023376465 5.913599967956543
Loss :  2.0233614444732666 4.182003498077393 6.205365180969238
Loss :  2.0434799194335938 4.130948066711426 6.1744279861450195
Loss :  2.0523571968078613 3.9487335681915283 6.001091003417969
  batch 40 loss: 2.0523571968078613, 3.9487335681915283, 6.001091003417969
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  2.0052456855773926 4.105303764343262 6.110549449920654
Loss :  2.0220727920532227 3.9695494174957275 5.991621971130371
Loss :  2.0381550788879395 4.046283721923828 6.084438800811768
Loss :  2.0837044715881348 4.019182205200195 6.10288667678833
Loss :  2.001164674758911 4.01641321182251 6.017578125
Loss :  2.005039691925049 4.244946479797363 6.249986171722412
Loss :  2.0410397052764893 4.125887870788574 6.166927337646484
Loss :  2.0058958530426025 4.093571662902832 6.0994672775268555
Loss :  2.032162666320801 4.1999077796936035 6.232070446014404
Loss :  2.020906686782837 4.086488723754883 6.107395172119141
Loss :  1.963207483291626 4.250513076782227 6.213720321655273
Loss :  2.0218141078948975 3.9615590572357178 5.983373165130615
Loss :  1.9978344440460205 4.075968265533447 6.073802947998047
Loss :  2.0060160160064697 3.991786479949951 5.997802734375
Loss :  2.0074009895324707 4.158938884735107 6.166339874267578
Loss :  2.005382537841797 4.1780195236206055 6.183402061462402
Loss :  2.0039496421813965 4.015033721923828 6.018983364105225
Loss :  2.0294172763824463 4.007992267608643 6.037409782409668
Loss :  2.0197298526763916 4.053550720214844 6.073280334472656
Loss :  1.9460666179656982 4.1913957595825195 6.137462615966797
  batch 60 loss: 1.9460666179656982, 4.1913957595825195, 6.137462615966797
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  2.0057625770568848 3.969973087310791 5.975735664367676
Loss :  1.94597589969635 4.178989887237549 6.124965667724609
Loss :  1.9734907150268555 3.8106863498687744 5.784176826477051
Loss :  1.9898102283477783 3.9533944129943848 5.943204879760742
Loss :  1.946609616279602 3.6650705337524414 5.611680030822754
Loss :  1.8041472434997559 4.366952419281006 6.171099662780762
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8220150470733643 4.3033528327941895 6.125368118286133
Loss :  1.8078809976577759 4.2450456619262695 6.052926540374756
Loss :  1.900032877922058 4.292714595794678 6.192747592926025
Total LOSS train 6.142780524033767 valid 6.135535478591919
CE LOSS train 2.0116019047223603 valid 0.4750082194805145
Contrastive LOSS train 4.1311786358173075 valid 1.0731786489486694
Saved best model. Old loss 6.167876243591309 and new best loss 6.135535478591919
EPOCH 4:
Loss :  1.9847140312194824 4.039242744445801 6.023956775665283
Loss :  2.0190725326538086 4.108046054840088 6.1271185874938965
Loss :  1.9865660667419434 4.03021240234375 6.016778469085693
Loss :  1.9775173664093018 3.85246205329895 5.829979419708252
Loss :  1.9975956678390503 3.946725606918335 5.944321155548096
Loss :  1.9839766025543213 4.00347375869751 5.98745059967041
Loss :  1.9322292804718018 4.240856647491455 6.173086166381836
Loss :  1.9872596263885498 3.990509033203125 5.977768898010254
Loss :  1.9963037967681885 3.8451788425445557 5.841482639312744
Loss :  2.0008769035339355 4.1378936767578125 6.138770580291748
Loss :  1.989181399345398 4.214740753173828 6.203922271728516
Loss :  2.021303415298462 3.953056812286377 5.974360466003418
Loss :  2.040904998779297 3.960082530975342 6.000987529754639
Loss :  2.003589630126953 3.9532666206359863 5.9568562507629395
Loss :  1.976440668106079 3.8363494873046875 5.8127899169921875
Loss :  2.0026302337646484 4.331045627593994 6.333675861358643
Loss :  1.9884508848190308 4.250687122344971 6.239138126373291
Loss :  1.9779999256134033 3.9276418685913086 5.905641555786133
Loss :  1.9985148906707764 4.17883825302124 6.1773529052734375
Loss :  1.9684332609176636 4.100179672241211 6.068613052368164
  batch 20 loss: 1.9684332609176636, 4.100179672241211, 6.068613052368164
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9839235544204712 3.807835578918457 5.791759014129639
Loss :  1.9942368268966675 3.9316139221191406 5.925850868225098
Loss :  1.9462417364120483 4.181591033935547 6.127832889556885
Loss :  1.9361516237258911 3.9137122631073 5.8498640060424805
Loss :  1.9430254697799683 4.075817108154297 6.018842697143555
Loss :  1.9728895425796509 3.9223310947418213 5.895220756530762
Loss :  1.9618967771530151 4.243463516235352 6.205360412597656
Loss :  1.9747906923294067 4.011122703552246 5.985913276672363
Loss :  2.0181829929351807 4.180366516113281 6.198549270629883
Loss :  1.9732232093811035 4.193435192108154 6.166658401489258
Loss :  1.9776198863983154 4.052294731140137 6.029914855957031
Loss :  1.9233582019805908 4.314419746398926 6.2377777099609375
Loss :  1.983691692352295 3.877042531967163 5.860733985900879
Loss :  1.9711878299713135 3.9533424377441406 5.924530029296875
Loss :  1.9525625705718994 4.070695877075195 6.023258209228516
Loss :  1.9936150312423706 4.0580153465271 6.05163049697876
Loss :  1.939840316772461 4.088593006134033 6.028433322906494
Loss :  1.9923251867294312 4.0236029624938965 6.015928268432617
Loss :  1.9835125207901 4.1612229347229 6.144735336303711
Loss :  1.9835602045059204 3.979475259780884 5.963035583496094
  batch 40 loss: 1.9835602045059204, 3.979475259780884, 5.963035583496094
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9403859376907349 3.8269460201263428 5.767332077026367
Loss :  1.989066243171692 3.946547746658325 5.935614109039307
Loss :  1.9915771484375 3.974778652191162 5.966355800628662
Loss :  2.0110220909118652 4.329318523406982 6.340340614318848
Loss :  1.994119644165039 4.099080562591553 6.093200206756592
Loss :  1.9859082698822021 3.949826955795288 5.93573522567749
Loss :  1.9770199060440063 3.7908689975738525 5.767889022827148
Loss :  1.9878511428833008 3.9391489028930664 5.927000045776367
Loss :  1.9671870470046997 3.8454880714416504 5.8126749992370605
Loss :  1.9771523475646973 4.064313888549805 6.041466236114502
Loss :  1.896249771118164 4.066490650177002 5.962740421295166
Loss :  1.964077115058899 3.951571226119995 5.915648460388184
Loss :  1.9537924528121948 4.2146687507629395 6.168461322784424
Loss :  1.941156268119812 3.9051411151885986 5.846297264099121
Loss :  1.9563143253326416 4.095890998840332 6.0522050857543945
Loss :  1.9288185834884644 4.006857395172119 5.935676097869873
Loss :  1.9502469301223755 4.132987022399902 6.083233833312988
Loss :  1.9560221433639526 3.964402198791504 5.920424461364746
Loss :  1.9666721820831299 4.060351371765137 6.0270233154296875
Loss :  1.9236598014831543 4.079710483551025 6.00337028503418
  batch 60 loss: 1.9236598014831543, 4.079710483551025, 6.00337028503418
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9770259857177734 3.9484498500823975 5.92547607421875
Loss :  1.9718360900878906 3.9415290355682373 5.913365364074707
Loss :  1.9679033756256104 3.692485809326172 5.660388946533203
Loss :  1.9559178352355957 3.7387611865997314 5.694679260253906
Loss :  1.9759373664855957 3.498532772064209 5.474470138549805
Loss :  1.8286181688308716 4.332951545715332 6.161569595336914
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8187410831451416 4.359485149383545 6.178226470947266
Loss :  1.8166561126708984 4.201528549194336 6.018184661865234
Loss :  1.8914227485656738 4.23152494430542 6.122947692871094
Total LOSS train 5.990014604421762 valid 6.120232105255127
CE LOSS train 1.9745587238898643 valid 0.47285568714141846
Contrastive LOSS train 4.015455869527964 valid 1.057881236076355
Saved best model. Old loss 6.135535478591919 and new best loss 6.120232105255127
EPOCH 5:
Loss :  1.9926364421844482 3.7452151775360107 5.737851619720459
Loss :  1.9530839920043945 4.186379432678223 6.139463424682617
Loss :  1.9612388610839844 4.042538642883301 6.003777503967285
Loss :  1.9524712562561035 4.190455436706543 6.1429266929626465
Loss :  1.962950348854065 3.8496296405792236 5.812580108642578
Loss :  1.9280798435211182 3.939547538757324 5.867627143859863
Loss :  1.9321449995040894 4.192592144012451 6.12473726272583
Loss :  1.9592831134796143 4.016427040100098 5.975709915161133
Loss :  1.9338098764419556 3.862767457962036 5.796577453613281
Loss :  1.9187380075454712 3.7545392513275146 5.673277378082275
Loss :  1.9356321096420288 4.048220157623291 5.983852386474609
Loss :  1.9657998085021973 4.111474990844727 6.077274799346924
Loss :  1.946440577507019 4.2568793296813965 6.203320026397705
Loss :  1.9376606941223145 4.015976428985596 5.95363712310791
Loss :  1.8867547512054443 4.118007183074951 6.004761695861816
Loss :  1.9664448499679565 4.032911777496338 5.999356746673584
Loss :  1.93684983253479 4.2049055099487305 6.141755104064941
Loss :  1.9489573240280151 4.122635364532471 6.071592807769775
Loss :  1.943498969078064 3.6998884677886963 5.643387317657471
Loss :  1.9480446577072144 4.28572940826416 6.233774185180664
  batch 20 loss: 1.9480446577072144, 4.28572940826416, 6.233774185180664
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9631121158599854 4.023026466369629 5.986138343811035
Loss :  1.95546555519104 3.898582696914673 5.854048252105713
Loss :  1.9500759840011597 4.063937664031982 6.014013767242432
Loss :  1.974753975868225 4.207461357116699 6.182215213775635
Loss :  1.9496921300888062 4.429923057556152 6.379615306854248
Loss :  1.9839445352554321 4.235902309417725 6.219846725463867
Loss :  1.9414156675338745 4.362091541290283 6.303507328033447
Loss :  1.9668779373168945 4.431544780731201 6.398422718048096
Loss :  1.9759002923965454 4.267000675201416 6.242900848388672
Loss :  1.9267100095748901 4.13872766494751 6.0654377937316895
Loss :  1.977213978767395 4.087562084197998 6.0647759437561035
Loss :  1.925250768661499 4.15920352935791 6.084454536437988
Loss :  1.9419140815734863 4.123899459838867 6.0658135414123535
Loss :  1.9477999210357666 3.9583704471588135 5.90617036819458
Loss :  1.9646530151367188 3.80405855178833 5.768711566925049
Loss :  1.9755711555480957 3.90860915184021 5.884180068969727
Loss :  1.9481940269470215 4.056326389312744 6.004520416259766
Loss :  1.9310647249221802 4.065761089324951 5.996825695037842
Loss :  1.9176961183547974 3.9956676959991455 5.913363933563232
Loss :  1.9408107995986938 3.940711736679077 5.8815226554870605
  batch 40 loss: 1.9408107995986938, 3.940711736679077, 5.8815226554870605
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9582772254943848 4.243479251861572 6.201756477355957
Loss :  1.917930006980896 3.8392210006713867 5.757151126861572
Loss :  1.932222843170166 3.962926149368286 5.895149230957031
Loss :  1.9198583364486694 4.152375221252441 6.0722336769104
Loss :  1.920419692993164 4.163370609283447 6.083790302276611
Loss :  1.9306457042694092 4.124588966369629 6.055234909057617
Loss :  1.9059066772460938 3.930750846862793 5.836657524108887
Loss :  1.9106818437576294 4.216187000274658 6.126868724822998
Loss :  1.926633358001709 3.7129738330841064 5.6396074295043945
Loss :  1.9398012161254883 4.144165992736816 6.083967208862305
Loss :  1.8677080869674683 3.9874119758605957 5.8551201820373535
Loss :  1.9074599742889404 3.7910971641540527 5.698556900024414
Loss :  1.9270682334899902 4.109086036682129 6.036154270172119
Loss :  1.8760054111480713 3.9258692264556885 5.80187463760376
Loss :  1.93998384475708 4.156378746032715 6.096362590789795
Loss :  1.8996927738189697 3.794726610183716 5.6944193840026855
Loss :  1.888503074645996 3.9149293899536133 5.803432464599609
Loss :  1.9197940826416016 3.770372152328491 5.690166473388672
Loss :  1.9220560789108276 3.971977710723877 5.894033908843994
Loss :  1.857592225074768 4.116260528564453 5.973852634429932
  batch 60 loss: 1.857592225074768, 4.116260528564453, 5.973852634429932
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9516934156417847 4.1970038414001465 6.148697376251221
Loss :  1.9085313081741333 4.38062047958374 6.289151668548584
Loss :  1.9153422117233276 4.395663261413574 6.311005592346191
Loss :  1.9277763366699219 4.052611827850342 5.980388164520264
Loss :  1.9213125705718994 3.6032497882843018 5.524562358856201
Loss :  1.8165675401687622 4.3655219078063965 6.182089328765869
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.792107105255127 4.215646266937256 6.007753372192383
Loss :  1.8153135776519775 4.16953182220459 5.984845161437988
Loss :  1.8594958782196045 4.219653606414795 6.07914924621582
Total LOSS train 5.9900603221013 valid 6.063459277153015
CE LOSS train 1.9363312867971567 valid 0.4648739695549011
Contrastive LOSS train 4.053729020632231 valid 1.0549134016036987
Saved best model. Old loss 6.120232105255127 and new best loss 6.063459277153015
EPOCH 6:
Loss :  1.9119075536727905 3.910905361175537 5.822813034057617
Loss :  1.9118125438690186 4.171548843383789 6.083361625671387
Loss :  1.8856157064437866 4.0628252029418945 5.948441028594971
Loss :  1.8738195896148682 4.09827184677124 5.9720916748046875
Loss :  1.9254162311553955 4.242499351501465 6.167915344238281
Loss :  1.8896020650863647 4.181265830993652 6.070868015289307
Loss :  1.8765732049942017 4.309253215789795 6.185826301574707
Loss :  1.9012060165405273 4.017958641052246 5.919164657592773
Loss :  1.9048653841018677 4.102499485015869 6.007364749908447
Loss :  1.90446138381958 3.9795753955841064 5.884037017822266
Loss :  1.8761842250823975 4.220038414001465 6.096222877502441
Loss :  1.939152479171753 4.1394572257995605 6.078609466552734
Loss :  1.9033856391906738 4.260796546936035 6.164182186126709
Loss :  1.925171136856079 4.253213405609131 6.178384780883789
Loss :  1.8963171243667603 3.6338984966278076 5.530215740203857
Loss :  1.9102423191070557 4.008269786834717 5.918512344360352
Loss :  1.9067468643188477 4.164035320281982 6.07078218460083
Loss :  1.899940848350525 4.1919264793396 6.091867446899414
Loss :  1.886893391609192 3.944770097732544 5.831663608551025
Loss :  1.8894643783569336 3.9068641662597656 5.796328544616699
  batch 20 loss: 1.8894643783569336, 3.9068641662597656, 5.796328544616699
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9083243608474731 4.072980880737305 5.981305122375488
Loss :  1.9157449007034302 4.0440192222595215 5.959764003753662
Loss :  1.9115393161773682 3.871141195297241 5.782680511474609
Loss :  1.8990161418914795 3.766266107559204 5.665282249450684
Loss :  1.9022767543792725 3.8200621604919434 5.722338676452637
Loss :  1.916292428970337 3.8487298488616943 5.765022277832031
Loss :  1.9205756187438965 3.8630621433258057 5.783638000488281
Loss :  1.8938852548599243 3.804553747177124 5.698439121246338
Loss :  1.944105625152588 3.911437749862671 5.85554313659668
Loss :  1.9229720830917358 3.8509671688079834 5.77393913269043
Loss :  1.9227352142333984 3.9821345806121826 5.90487003326416
Loss :  1.880321741104126 4.026168346405029 5.906490325927734
Loss :  1.897368311882019 3.774970054626465 5.672338485717773
Loss :  1.9226751327514648 4.166749000549316 6.089424133300781
Loss :  1.892549753189087 3.9415457248687744 5.834095478057861
Loss :  1.9297415018081665 4.1314191818237305 6.061160564422607
Loss :  1.8780211210250854 4.16814661026001 6.046167850494385
Loss :  1.8852461576461792 4.050843238830566 5.936089515686035
Loss :  1.8748809099197388 3.8928418159484863 5.7677226066589355
Loss :  1.8905432224273682 3.7057509422302246 5.596294403076172
  batch 40 loss: 1.8905432224273682, 3.7057509422302246, 5.596294403076172
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8772133588790894 3.930777072906494 5.807990550994873
Loss :  1.9150168895721436 4.002490997314453 5.917508125305176
Loss :  1.892762303352356 3.970719337463379 5.863481521606445
Loss :  1.9147956371307373 3.9790446758270264 5.893840312957764
Loss :  1.8980463743209839 3.768277645111084 5.666324138641357
Loss :  1.9159448146820068 3.8388426303863525 5.754787445068359
Loss :  1.8833032846450806 3.7678678035736084 5.6511712074279785
Loss :  1.8756691217422485 4.160403728485107 6.036072731018066
Loss :  1.8867125511169434 3.8272013664245605 5.713913917541504
Loss :  1.8903135061264038 3.8600447177886963 5.7503581047058105
Loss :  1.8543648719787598 4.104806900024414 5.959171772003174
Loss :  1.8796921968460083 3.816376209259033 5.696068286895752
Loss :  1.8991694450378418 4.123507976531982 6.022677421569824
Loss :  1.8566014766693115 3.9344029426574707 5.791004180908203
Loss :  1.8435511589050293 3.845416784286499 5.688967704772949
Loss :  1.8200502395629883 3.9928884506225586 5.812938690185547
Loss :  1.890440821647644 3.9100747108459473 5.800515651702881
Loss :  1.8830188512802124 3.9248805046081543 5.807899475097656
Loss :  1.9068424701690674 3.9073164463043213 5.814158916473389
Loss :  1.8538248538970947 3.9149909019470215 5.768815994262695
  batch 60 loss: 1.8538248538970947, 3.9149909019470215, 5.768815994262695
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8797897100448608 4.0417866706848145 5.921576499938965
Loss :  1.8513998985290527 4.00242280960083 5.853822708129883
Loss :  1.877699375152588 3.838744640350342 5.71644401550293
Loss :  1.8979512453079224 3.8407540321350098 5.738705158233643
Loss :  1.896902084350586 3.2652082443237305 5.162110328674316
Loss :  1.811460256576538 4.40231466293335 6.213774681091309
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.786759614944458 4.2663750648498535 6.053134918212891
Loss :  1.796051025390625 4.181600093841553 5.977651119232178
Loss :  1.847922682762146 4.085023403167725 5.93294620513916
Total LOSS train 5.865101278745211 valid 6.044376730918884
CE LOSS train 1.8949026181147648 valid 0.4619806706905365
Contrastive LOSS train 3.970198631286621 valid 1.0212558507919312
Saved best model. Old loss 6.063459277153015 and new best loss 6.044376730918884
EPOCH 7:
Loss :  1.9048761129379272 3.960303783416748 5.865180015563965
Loss :  1.9035197496414185 4.101518630981445 6.005038261413574
Loss :  1.8498610258102417 4.52364444732666 6.373505592346191
Loss :  1.8596206903457642 4.020183563232422 5.8798041343688965
Loss :  1.9138990640640259 4.084702014923096 5.998600959777832
Loss :  1.8481475114822388 4.185797691345215 6.033945083618164
Loss :  1.9070100784301758 4.122488021850586 6.029498100280762
Loss :  1.8876171112060547 4.050529956817627 5.938147068023682
Loss :  1.8995869159698486 3.9698517322540283 5.869438648223877
Loss :  1.9022650718688965 4.095682621002197 5.997947692871094
Loss :  1.8918508291244507 4.013007164001465 5.904858112335205
Loss :  1.9282325506210327 3.9182655811309814 5.846498012542725
Loss :  1.9047142267227173 4.1802825927734375 6.084996700286865
Loss :  1.8940891027450562 4.06008768081665 5.954176902770996
Loss :  1.856972575187683 3.8814682960510254 5.738440990447998
Loss :  1.9045239686965942 4.270658493041992 6.175182342529297
Loss :  1.8956124782562256 3.835219621658325 5.730832099914551
Loss :  1.8734074831008911 3.959012746810913 5.832420349121094
Loss :  1.8794374465942383 3.9276373386383057 5.807074546813965
Loss :  1.856855869293213 4.278255939483643 6.1351118087768555
  batch 20 loss: 1.856855869293213, 4.278255939483643, 6.1351118087768555
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.879040241241455 3.821108102798462 5.700148582458496
Loss :  1.858852505683899 3.8510854244232178 5.709938049316406
Loss :  1.8952953815460205 4.211642742156982 6.106938362121582
Loss :  1.8534737825393677 3.9351449012756348 5.788618564605713
Loss :  1.898269772529602 3.9551806449890137 5.853450298309326
Loss :  1.8813883066177368 3.761821746826172 5.643209934234619
Loss :  1.9036650657653809 3.9941256046295166 5.897790908813477
Loss :  1.868625521659851 3.862922430038452 5.731547832489014
Loss :  1.9106884002685547 3.782116174697876 5.692804336547852
Loss :  1.8494504690170288 3.822377920150757 5.671828269958496
Loss :  1.9060454368591309 3.7487833499908447 5.654829025268555
Loss :  1.864446759223938 4.259713172912598 6.124159812927246
Loss :  1.8827906847000122 4.121250629425049 6.0040411949157715
Loss :  1.8734140396118164 3.9009034633636475 5.774317741394043
Loss :  1.861251950263977 3.765228509902954 5.626480579376221
Loss :  1.9246430397033691 3.8908603191375732 5.815503120422363
Loss :  1.8670933246612549 3.8714702129364014 5.738563537597656
Loss :  1.8787612915039062 3.71696400642395 5.595725059509277
Loss :  1.8870599269866943 3.987187623977661 5.8742475509643555
Loss :  1.8994594812393188 3.8892457485198975 5.788705348968506
  batch 40 loss: 1.8994594812393188, 3.8892457485198975, 5.788705348968506
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8779252767562866 4.183727264404297 6.061652660369873
Loss :  1.9051852226257324 4.313096046447754 6.218281269073486
Loss :  1.8919445276260376 4.02874231338501 5.920686721801758
Loss :  1.8782317638397217 4.1550421714782715 6.033273696899414
Loss :  1.897928237915039 4.335840225219727 6.233768463134766
Loss :  1.9065775871276855 4.253592491149902 6.160170078277588
Loss :  1.889309287071228 4.027592182159424 5.916901588439941
Loss :  1.8493096828460693 3.99760103225708 5.84691047668457
Loss :  1.8718196153640747 3.911956787109375 5.78377628326416
Loss :  1.8889970779418945 3.983227491378784 5.872224807739258
Loss :  1.8760464191436768 3.6804237365722656 5.556469917297363
Loss :  1.8391927480697632 3.9156010150909424 5.754793643951416
Loss :  1.8698936700820923 4.153534889221191 6.023428440093994
Loss :  1.8621896505355835 3.6189332008361816 5.481122970581055
Loss :  1.8741132020950317 4.080617427825928 5.95473051071167
Loss :  1.838173508644104 3.9149978160858154 5.753171443939209
Loss :  1.8949739933013916 4.207311153411865 6.102285385131836
Loss :  1.873393177986145 3.6667990684509277 5.540192127227783
Loss :  1.9012696743011475 3.955073356628418 5.8563432693481445
Loss :  1.8704243898391724 3.987180471420288 5.85760498046875
  batch 60 loss: 1.8704243898391724, 3.987180471420288, 5.85760498046875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8973830938339233 3.7222824096679688 5.619665622711182
Loss :  1.850872278213501 4.039742946624756 5.890615463256836
Loss :  1.886396884918213 4.115464210510254 6.001861095428467
Loss :  1.8672112226486206 3.912666082382202 5.779877185821533
Loss :  1.8872265815734863 3.3866026401519775 5.273829460144043
Loss :  1.86888587474823 4.397218704223633 6.266104698181152
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.831480860710144 4.387176513671875 6.218657493591309
Loss :  1.8622148036956787 4.213253498077393 6.075468063354492
Loss :  1.893440842628479 4.085832595825195 5.979273319244385
Total LOSS train 5.869033586061918 valid 6.1348758935928345
CE LOSS train 1.8823354464310866 valid 0.47336021065711975
Contrastive LOSS train 3.9866981396308314 valid 1.0214581489562988
EPOCH 8:
Loss :  1.8787167072296143 4.168652534484863 6.047369003295898
Loss :  1.8747860193252563 4.357041835784912 6.231827735900879
Loss :  1.8657219409942627 3.966301918029785 5.832023620605469
Loss :  1.8466342687606812 4.146806240081787 5.993440628051758
Loss :  1.8993088006973267 4.293104648590088 6.192413330078125
Loss :  1.8391220569610596 3.914290428161621 5.753412246704102
Loss :  1.9025299549102783 3.7711257934570312 5.6736555099487305
Loss :  1.8803166151046753 4.09731388092041 5.977630615234375
Loss :  1.8737941980361938 3.99227237701416 5.8660664558410645
Loss :  1.8598262071609497 3.926443576812744 5.786269664764404
Loss :  1.8579200506210327 3.9213738441467285 5.779294013977051
Loss :  1.8890202045440674 3.7671635150909424 5.65618371963501
Loss :  1.8646557331085205 4.067541599273682 5.932197570800781
Loss :  1.849038004875183 4.209510326385498 6.058548450469971
Loss :  1.8636232614517212 4.107624053955078 5.97124719619751
Loss :  1.8883280754089355 4.005013465881348 5.893341541290283
Loss :  1.8765345811843872 4.032697677612305 5.909232139587402
Loss :  1.889504075050354 3.7043073177337646 5.593811511993408
Loss :  1.822488784790039 3.89350962638855 5.715998649597168
Loss :  1.8371820449829102 3.9568564891815186 5.794038772583008
  batch 20 loss: 1.8371820449829102, 3.9568564891815186, 5.794038772583008
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8558094501495361 4.064828872680664 5.920638084411621
Loss :  1.8377177715301514 4.1821370124816895 6.019854545593262
Loss :  1.852278232574463 3.9508519172668457 5.803130149841309
Loss :  1.833923578262329 3.941462516784668 5.775385856628418
Loss :  1.866042971611023 3.9736428260803223 5.839685916900635
Loss :  1.8487244844436646 3.9336249828338623 5.782349586486816
Loss :  1.8758978843688965 4.165475845336914 6.0413737297058105
Loss :  1.8422349691390991 3.910888195037842 5.7531232833862305
Loss :  1.8970680236816406 3.6831865310668945 5.580254554748535
Loss :  1.846567153930664 3.784165143966675 5.630732536315918
Loss :  1.8828670978546143 3.8555328845977783 5.738399982452393
Loss :  1.8341572284698486 3.7483816146850586 5.582538604736328
Loss :  1.835179090499878 3.882456064224243 5.717635154724121
Loss :  1.8612849712371826 3.6812744140625 5.542559623718262
Loss :  1.8658552169799805 3.834223508834839 5.700078964233398
Loss :  1.913506031036377 3.6566810607910156 5.570187091827393
Loss :  1.84541916847229 3.580339193344116 5.425758361816406
Loss :  1.8435348272323608 3.615741014480591 5.459275722503662
Loss :  1.8673464059829712 3.75527286529541 5.622619152069092
Loss :  1.8717175722122192 3.707918643951416 5.579636096954346
  batch 40 loss: 1.8717175722122192, 3.707918643951416, 5.579636096954346
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.868192434310913 3.8048453330993652 5.673037528991699
Loss :  1.865632176399231 3.5503129959106445 5.415945053100586
Loss :  1.874374508857727 3.6891674995422363 5.563541889190674
Loss :  1.8743987083435059 4.085186004638672 5.959584712982178
Loss :  1.8571897745132446 3.7270472049713135 5.584237098693848
Loss :  1.86527419090271 3.676704168319702 5.541978359222412
Loss :  1.868706226348877 3.649308919906616 5.518014907836914
Loss :  1.857770562171936 4.035476207733154 5.893246650695801
Loss :  1.8622239828109741 3.7613375186920166 5.623561382293701
Loss :  1.8772456645965576 3.7729835510253906 5.650229454040527
Loss :  1.8372513055801392 3.79198956489563 5.629240989685059
Loss :  1.8645597696304321 3.8143739700317383 5.678933620452881
Loss :  1.8725591897964478 3.877788782119751 5.750348091125488
Loss :  1.8644702434539795 3.9088008403778076 5.773271083831787
Loss :  1.8799858093261719 4.027612686157227 5.907598495483398
Loss :  1.83401620388031 3.9457812309265137 5.779797554016113
Loss :  1.8735408782958984 4.006153583526611 5.87969446182251
Loss :  1.8669971227645874 3.938084363937378 5.805081367492676
Loss :  1.8711687326431274 3.9580225944519043 5.829191207885742
Loss :  1.8213458061218262 3.783118963241577 5.604464530944824
  batch 60 loss: 1.8213458061218262, 3.783118963241577, 5.604464530944824
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8734512329101562 3.928992509841919 5.802443504333496
Loss :  1.8494919538497925 3.707529306411743 5.557021141052246
Loss :  1.840467095375061 3.6220765113830566 5.462543487548828
Loss :  1.8649965524673462 3.8774590492248535 5.74245548248291
Loss :  1.8833343982696533 3.343980312347412 5.2273149490356445
Loss :  1.8364250659942627 4.390921592712402 6.227346420288086
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8404557704925537 4.271847724914551 6.112303733825684
Loss :  1.8532698154449463 4.188350200653076 6.041620254516602
Loss :  1.9015346765518188 4.107367515563965 6.008902072906494
Total LOSS train 5.747630713536189 valid 6.097543120384216
CE LOSS train 1.8631512036690345 valid 0.4753836691379547
Contrastive LOSS train 3.8844795373769907 valid 1.0268418788909912
EPOCH 9:
Loss :  1.86500883102417 3.9427664279937744 5.807775497436523
Loss :  1.8941959142684937 4.035028457641602 5.929224491119385
Loss :  1.8470027446746826 3.9049465656280518 5.751949310302734
Loss :  1.840998888015747 4.143808364868164 5.984807014465332
Loss :  1.901545763015747 4.0914435386657715 5.992989540100098
Loss :  1.8447606563568115 3.833059787750244 5.677820205688477
Loss :  1.8363161087036133 3.7214102745056152 5.5577263832092285
Loss :  1.847727656364441 3.6823790073394775 5.530106544494629
Loss :  1.8444664478302002 4.030038833618164 5.874505043029785
Loss :  1.8270796537399292 3.898348093032837 5.725427627563477
Loss :  1.854246973991394 4.043342113494873 5.897589206695557
Loss :  1.8695857524871826 3.8416030406951904 5.711188793182373
Loss :  1.8762263059616089 3.8771705627441406 5.753396987915039
Loss :  1.85153067111969 4.003110408782959 5.854640960693359
Loss :  1.8488723039627075 3.901442766189575 5.750315189361572
Loss :  1.8649786710739136 3.713860273361206 5.57883882522583
Loss :  1.886367917060852 3.9279024600982666 5.814270496368408
Loss :  1.8686575889587402 3.8074772357940674 5.676135063171387
Loss :  1.8288636207580566 3.8611207008361816 5.689984321594238
Loss :  1.8360564708709717 3.9545836448669434 5.790639877319336
  batch 20 loss: 1.8360564708709717, 3.9545836448669434, 5.790639877319336
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8504005670547485 4.041392803192139 5.891793251037598
Loss :  1.8556259870529175 3.6243176460266113 5.479943752288818
Loss :  1.8424004316329956 3.823882579803467 5.666283130645752
Loss :  1.8405286073684692 3.9166808128356934 5.757209300994873
Loss :  1.879470705986023 4.101321220397949 5.980792045593262
Loss :  1.8671345710754395 3.914623975753784 5.7817583084106445
Loss :  1.8593297004699707 4.056005001068115 5.915334701538086
Loss :  1.8425123691558838 4.150543212890625 5.99305534362793
Loss :  1.88727867603302 4.099245548248291 5.9865241050720215
Loss :  1.8614482879638672 3.761080503463745 5.622529029846191
Loss :  1.8900840282440186 3.8280704021453857 5.718154430389404
Loss :  1.8518046140670776 4.138888359069824 5.990693092346191
Loss :  1.8533375263214111 3.740203857421875 5.593541145324707
Loss :  1.8772375583648682 3.7962138652801514 5.6734514236450195
Loss :  1.8557872772216797 3.82007098197937 5.675858497619629
Loss :  1.9050594568252563 3.7803406715393066 5.685400009155273
Loss :  1.8522982597351074 4.344366073608398 6.196664333343506
Loss :  1.8503131866455078 3.816631317138672 5.66694450378418
Loss :  1.8509927988052368 4.055511474609375 5.906504154205322
Loss :  1.8669840097427368 3.9296672344207764 5.796651363372803
  batch 40 loss: 1.8669840097427368, 3.9296672344207764, 5.796651363372803
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8586980104446411 4.089862823486328 5.94856071472168
Loss :  1.8485628366470337 3.7437241077423096 5.592287063598633
Loss :  1.8802675008773804 3.5146329402923584 5.394900321960449
Loss :  1.8731961250305176 4.002675533294678 5.875871658325195
Loss :  1.880784273147583 3.9194300174713135 5.8002142906188965
Loss :  1.8765184879302979 3.7235336303710938 5.6000518798828125
Loss :  1.8676378726959229 4.090980052947998 5.9586181640625
Loss :  1.8739432096481323 3.958390712738037 5.832334041595459
Loss :  1.8663488626480103 3.7248337268829346 5.591182708740234
Loss :  1.8654377460479736 3.816378116607666 5.681816101074219
Loss :  1.8350766897201538 4.022669315338135 5.857746124267578
Loss :  1.8465436697006226 3.8839991092681885 5.7305426597595215
Loss :  1.850624918937683 4.113148212432861 5.963773250579834
Loss :  1.8373253345489502 3.7995340824127197 5.63685941696167
Loss :  1.8457480669021606 4.079751014709473 5.925498962402344
Loss :  1.816678524017334 3.755255937576294 5.571934700012207
Loss :  1.885025978088379 4.019840240478516 5.9048662185668945
Loss :  1.84422767162323 3.694042921066284 5.538270473480225
Loss :  1.8653674125671387 4.072869300842285 5.938236713409424
Loss :  1.839054822921753 3.886247396469116 5.725302219390869
  batch 60 loss: 1.839054822921753, 3.886247396469116, 5.725302219390869
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8429057598114014 3.7731070518493652 5.6160125732421875
Loss :  1.835129737854004 3.7983171939849854 5.63344669342041
Loss :  1.8489665985107422 3.714299440383911 5.563265800476074
Loss :  1.8546146154403687 3.720801830291748 5.575416564941406
Loss :  1.8552234172821045 3.3351783752441406 5.190402030944824
Loss :  1.837423324584961 4.377762794494629 6.21518611907959
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.830088496208191 4.419323921203613 6.249412536621094
Loss :  1.8336986303329468 4.201942443847656 6.035641193389893
Loss :  1.8846931457519531 4.111581802368164 5.996274948120117
Total LOSS train 5.7534742868863615 valid 6.124128699302673
CE LOSS train 1.8579757800469032 valid 0.4711732864379883
Contrastive LOSS train 3.895498510507437 valid 1.027895450592041
EPOCH 10:
Loss :  1.844704031944275 3.7087883949279785 5.553492546081543
Loss :  1.8800894021987915 3.606306314468384 5.486395835876465
Loss :  1.8435760736465454 4.143568992614746 5.987144947052002
Loss :  1.8227565288543701 3.668179512023926 5.490936279296875
Loss :  1.8665717840194702 3.9971096515655518 5.863681316375732
Loss :  1.819372296333313 3.887021541595459 5.706393718719482
Loss :  1.8718535900115967 3.6951141357421875 5.566967964172363
Loss :  1.836456298828125 3.714261770248413 5.550718307495117
Loss :  1.8341726064682007 3.821819543838501 5.655992031097412
Loss :  1.8196569681167603 4.239424705505371 6.059081554412842
Loss :  1.8418322801589966 4.1278276443481445 5.969659805297852
Loss :  1.8747013807296753 3.948989152908325 5.823690414428711
Loss :  1.8535873889923096 3.9851884841918945 5.838775634765625
Loss :  1.8314071893692017 3.9694108963012695 5.800817966461182
Loss :  1.8534451723098755 4.129484176635742 5.982929229736328
Loss :  1.867908000946045 3.891364574432373 5.759272575378418
Loss :  1.8633549213409424 3.9977052211761475 5.86106014251709
Loss :  1.8556182384490967 3.7150442600250244 5.570662498474121
Loss :  1.8276962041854858 3.7203500270843506 5.548046112060547
Loss :  1.838819146156311 3.8867640495300293 5.725583076477051
  batch 20 loss: 1.838819146156311, 3.8867640495300293, 5.725583076477051
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.840480923652649 4.032433032989502 5.872913837432861
Loss :  1.8422385454177856 4.181006908416748 6.023245334625244
Loss :  1.8340240716934204 4.029544830322266 5.8635687828063965
Loss :  1.8535375595092773 4.131213188171387 5.984750747680664
Loss :  1.8544869422912598 4.410700798034668 6.265187740325928
Loss :  1.8419275283813477 3.987989902496338 5.8299174308776855
Loss :  1.874003291130066 4.339421272277832 6.2134246826171875
Loss :  1.8360882997512817 4.261916160583496 6.098004341125488
Loss :  1.9012720584869385 4.328430652618408 6.229702949523926
Loss :  1.8323999643325806 4.023481369018555 5.855881214141846
Loss :  1.8797731399536133 4.0133748054504395 5.893147945404053
Loss :  1.8665883541107178 3.9579226970672607 5.8245110511779785
Loss :  1.8666212558746338 3.704552173614502 5.571173667907715
Loss :  1.8940752744674683 4.138409614562988 6.032485008239746
Loss :  1.8661421537399292 4.139542579650879 6.005684852600098
Loss :  1.906691551208496 3.9367308616638184 5.8434224128723145
Loss :  1.868309497833252 3.9817490577697754 5.850058555603027
Loss :  1.869066834449768 4.116745471954346 5.985812187194824
Loss :  1.8644495010375977 3.9827616214752197 5.847210884094238
Loss :  1.8665693998336792 3.6483283042907715 5.51489782333374
  batch 40 loss: 1.8665693998336792, 3.6483283042907715, 5.51489782333374
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8551496267318726 4.127249717712402 5.9823994636535645
Loss :  1.879153847694397 4.1873087882995605 6.066462516784668
Loss :  1.8782997131347656 4.195491790771484 6.07379150390625
Loss :  1.875685214996338 4.258601665496826 6.134286880493164
Loss :  1.874344825744629 3.8211188316345215 5.69546365737915
Loss :  1.9052553176879883 4.276216506958008 6.181471824645996
Loss :  1.8793363571166992 4.115026473999023 5.994362831115723
Loss :  1.8451489210128784 4.02149772644043 5.866646766662598
Loss :  1.875673532485962 4.154650688171387 6.0303239822387695
Loss :  1.8819273710250854 4.262454986572266 6.144382476806641
Loss :  1.8394508361816406 3.959195852279663 5.798646926879883
Loss :  1.8411154747009277 3.7594380378723145 5.600553512573242
Loss :  1.8386249542236328 3.9557418823242188 5.794366836547852
Loss :  1.8573334217071533 4.011585712432861 5.868919372558594
Loss :  1.8648066520690918 3.7975451946258545 5.662351608276367
Loss :  1.8386458158493042 4.0514116287231445 5.890057563781738
Loss :  1.8683098554611206 4.178061485290527 6.0463714599609375
Loss :  1.8644222021102905 3.9496161937713623 5.814038276672363
Loss :  1.857727289199829 4.0732526779174805 5.9309797286987305
Loss :  1.836324691772461 3.917639970779419 5.753964424133301
  batch 60 loss: 1.836324691772461, 3.917639970779419, 5.753964424133301
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8508867025375366 4.027268886566162 5.878155708312988
Loss :  1.8341349363327026 4.2385687828063965 6.072703838348389
Loss :  1.8545444011688232 3.6696908473968506 5.524235248565674
Loss :  1.844042181968689 3.6587789058685303 5.50282096862793
Loss :  1.866830587387085 3.285264015197754 5.152094841003418
Loss :  1.8448082208633423 4.407607078552246 6.252415180206299
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.841665506362915 4.386044979095459 6.227710723876953
Loss :  1.8470544815063477 4.26352596282959 6.1105804443359375
Loss :  1.834007740020752 4.260491847991943 6.094499588012695
Total LOSS train 5.844094701913687 valid 6.171301484107971
CE LOSS train 1.8571307750848622 valid 0.458501935005188
Contrastive LOSS train 3.986963932330792 valid 1.0651229619979858
EPOCH 11:
Loss :  1.8285822868347168 3.5292418003082275 5.357824325561523
Loss :  1.8712183237075806 4.012310981750488 5.883529186248779
Loss :  1.857653260231018 3.9394166469573975 5.797070026397705
Loss :  1.8349275588989258 4.114484786987305 5.9494123458862305
Loss :  1.8656558990478516 4.014920711517334 5.8805766105651855
Loss :  1.8367198705673218 3.7599146366119385 5.596634387969971
Loss :  1.8816710710525513 3.6927974224090576 5.574468612670898
Loss :  1.8463202714920044 3.739006996154785 5.5853271484375
Loss :  1.8566550016403198 4.190195083618164 6.046850204467773
Loss :  1.8366763591766357 3.9015426635742188 5.738219261169434
Loss :  1.8627099990844727 4.044351577758789 5.907061576843262
Loss :  1.8976776599884033 4.140069961547852 6.037747383117676
Loss :  1.8686959743499756 3.98980712890625 5.858503341674805
Loss :  1.8200355768203735 3.777928113937378 5.597963809967041
Loss :  1.804404854774475 3.7954177856445312 5.599822521209717
Loss :  1.8264213800430298 3.9924256801605225 5.818847179412842
Loss :  1.8265724182128906 3.927745819091797 5.7543182373046875
Loss :  1.8376766443252563 3.7057275772094727 5.5434041023254395
Loss :  1.8401050567626953 4.251780986785889 6.091886043548584
Loss :  1.8339154720306396 4.042923450469971 5.876838684082031
  batch 20 loss: 1.8339154720306396, 4.042923450469971, 5.876838684082031
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8558580875396729 3.981191873550415 5.837049961090088
Loss :  1.8473432064056396 3.7989513874053955 5.646294593811035
Loss :  1.825679898262024 3.6776041984558105 5.503283977508545
Loss :  1.8549816608428955 3.755385398864746 5.6103668212890625
Loss :  1.847172737121582 4.015606880187988 5.86277961730957
Loss :  1.8355070352554321 3.937427043914795 5.7729339599609375
Loss :  1.8645848035812378 4.352602005004883 6.21718692779541
Loss :  1.8401243686676025 3.9381721019744873 5.77829647064209
Loss :  1.8707925081253052 4.226648807525635 6.09744119644165
Loss :  1.823056936264038 4.407412052154541 6.23046875
Loss :  1.866516351699829 4.059573173522949 5.926089286804199
Loss :  1.8326189517974854 4.098750591278076 5.931369781494141
Loss :  1.8207123279571533 3.779578924179077 5.6002912521362305
Loss :  1.8573237657546997 3.8670806884765625 5.724404335021973
Loss :  1.8517855405807495 4.035538673400879 5.887324333190918
Loss :  1.8720734119415283 4.273887634277344 6.145960807800293
Loss :  1.8337340354919434 4.1138081550598145 5.947542190551758
Loss :  1.8105844259262085 4.061787128448486 5.872371673583984
Loss :  1.8458445072174072 4.135488510131836 5.981332778930664
Loss :  1.8333451747894287 4.022700786590576 5.856045722961426
  batch 40 loss: 1.8333451747894287, 4.022700786590576, 5.856045722961426
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8224173784255981 4.140386581420898 5.962803840637207
Loss :  1.8359911441802979 3.7744803428649902 5.610471725463867
Loss :  1.8483433723449707 4.13428258895874 5.982625961303711
Loss :  1.8430203199386597 3.960284948348999 5.803305149078369
Loss :  1.8361704349517822 3.7936532497406006 5.629823684692383
Loss :  1.8453762531280518 3.7148492336273193 5.560225486755371
Loss :  1.8389370441436768 3.6280224323272705 5.466959476470947
Loss :  1.8292689323425293 3.7546486854553223 5.583917617797852
Loss :  1.8386436700820923 3.8913521766662598 5.7299957275390625
Loss :  1.8660166263580322 4.046082973480225 5.912099838256836
Loss :  1.8196783065795898 3.9710471630096436 5.7907257080078125
Loss :  1.8346668481826782 3.9480650424957275 5.782732009887695
Loss :  1.832144856452942 3.6123900413513184 5.444534778594971
Loss :  1.8376084566116333 3.6264870166778564 5.464095592498779
Loss :  1.8329570293426514 3.7224552631378174 5.555412292480469
Loss :  1.7779371738433838 3.829646348953247 5.607583522796631
Loss :  1.8365694284439087 3.9885473251342773 5.8251166343688965
Loss :  1.823742151260376 3.706028938293457 5.529770851135254
Loss :  1.8456283807754517 3.9183192253112793 5.763947486877441
Loss :  1.80020272731781 3.626725196838379 5.4269280433654785
  batch 60 loss: 1.80020272731781, 3.626725196838379, 5.4269280433654785
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8156920671463013 3.635035991668701 5.450727939605713
Loss :  1.8035099506378174 3.8405163288116455 5.644026279449463
Loss :  1.8024790287017822 3.6041784286499023 5.4066572189331055
Loss :  1.818527102470398 3.4300129413604736 5.248539924621582
Loss :  1.8183711767196655 3.0715315341949463 4.889902591705322
Loss :  1.8518134355545044 4.1949968338012695 6.046810150146484
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8623812198638916 4.278448104858398 6.140829086303711
Loss :  1.878782033920288 3.8157765865325928 5.694558620452881
Loss :  1.8851349353790283 3.80440092086792 5.689536094665527
Total LOSS train 5.738431827838604 valid 5.892933487892151
CE LOSS train 1.8388897620714628 valid 0.4712837338447571
Contrastive LOSS train 3.8995420896089996 valid 0.95110023021698
Saved best model. Old loss 6.044376730918884 and new best loss 5.892933487892151
EPOCH 12:
Loss :  1.791339635848999 3.386777639389038 5.178117275238037
Loss :  1.811461329460144 3.716968297958374 5.5284295082092285
Loss :  1.8108165264129639 3.541712522506714 5.352529048919678
Loss :  1.7934954166412354 3.301589250564575 5.0950846672058105
Loss :  1.8367869853973389 3.675135612487793 5.511922836303711
Loss :  1.7897655963897705 3.4814000129699707 5.27116584777832
Loss :  1.832014560699463 3.579413890838623 5.411428451538086
Loss :  1.805105447769165 3.326586961746216 5.131692409515381
Loss :  1.810534119606018 3.3288941383361816 5.13942813873291
Loss :  1.7917139530181885 3.50746750831604 5.2991814613342285
Loss :  1.8126863241195679 3.4692044258117676 5.281890869140625
Loss :  1.857427954673767 3.76503849029541 5.622466564178467
Loss :  1.8212532997131348 3.774000883102417 5.595253944396973
Loss :  1.8083665370941162 3.406644582748413 5.215011119842529
Loss :  1.801400065422058 3.4293158054351807 5.230715751647949
Loss :  1.7990882396697998 3.6124513149261475 5.411539554595947
Loss :  1.811813473701477 3.3980636596679688 5.209877014160156
Loss :  1.800927758216858 3.5632851123809814 5.364212989807129
Loss :  1.8079414367675781 3.2775156497955322 5.085456848144531
Loss :  1.7899583578109741 3.5210862159729004 5.311044692993164
  batch 20 loss: 1.7899583578109741, 3.5210862159729004, 5.311044692993164
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8267899751663208 3.262423515319824 5.0892133712768555
Loss :  1.812441349029541 3.4871137142181396 5.299554824829102
Loss :  1.8135699033737183 3.3742575645446777 5.1878275871276855
Loss :  1.8159760236740112 3.3659956455230713 5.181971549987793
Loss :  1.823484182357788 3.645169496536255 5.468653678894043
Loss :  1.8107882738113403 3.507497549057007 5.318285942077637
Loss :  1.8213779926300049 3.606903076171875 5.428280830383301
Loss :  1.7945512533187866 3.2540197372436523 5.0485711097717285
Loss :  1.837033748626709 3.1644816398620605 5.0015153884887695
Loss :  1.7924127578735352 3.3329460620880127 5.125358581542969
Loss :  1.8462047576904297 3.6522793769836426 5.498484134674072
Loss :  1.8111746311187744 3.5906031131744385 5.401777744293213
Loss :  1.799568772315979 3.325317859649658 5.124886512756348
Loss :  1.8114593029022217 3.543428897857666 5.354887962341309
Loss :  1.8184764385223389 3.7353949546813965 5.553871154785156
Loss :  1.8407920598983765 3.2995080947875977 5.140300273895264
Loss :  1.817928671836853 3.309319257736206 5.1272478103637695
Loss :  1.8000354766845703 3.25616717338562 5.0562028884887695
Loss :  1.8189347982406616 3.3815178871154785 5.20045280456543
Loss :  1.8035898208618164 3.2792129516601562 5.082802772521973
  batch 40 loss: 1.8035898208618164, 3.2792129516601562, 5.082802772521973
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.820217490196228 3.309666156768799 5.129883766174316
Loss :  1.8249444961547852 3.161726951599121 4.986671447753906
Loss :  1.8265753984451294 3.4818124771118164 5.308387756347656
Loss :  1.822343111038208 3.2600247859954834 5.082367897033691
Loss :  1.8297200202941895 3.173491954803467 5.003211975097656
Loss :  1.8246406316757202 3.4270079135894775 5.251648426055908
Loss :  1.8122445344924927 3.187102794647217 4.99934720993042
Loss :  1.8192298412322998 3.1559932231903076 4.975223064422607
Loss :  1.803072452545166 3.3134262561798096 5.116498947143555
Loss :  1.8325616121292114 3.538768768310547 5.371330261230469
Loss :  1.7841037511825562 3.4743220806121826 5.258425712585449
Loss :  1.799754023551941 3.2463486194610596 5.046102523803711
Loss :  1.8137786388397217 3.191049814224243 5.004828453063965
Loss :  1.8065804243087769 3.157059669494629 4.963640213012695
Loss :  1.8184633255004883 3.2385900020599365 5.057053565979004
Loss :  1.787520408630371 3.115513324737549 4.90303373336792
Loss :  1.8217189311981201 3.435734510421753 5.257453441619873
Loss :  1.8359153270721436 3.3764383792877197 5.212353706359863
Loss :  1.831713318824768 3.7279696464538574 5.559682846069336
Loss :  1.808285117149353 3.7210617065429688 5.529346942901611
  batch 60 loss: 1.808285117149353, 3.7210617065429688, 5.529346942901611
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8131392002105713 3.6748530864715576 5.487992286682129
Loss :  1.8134123086929321 3.616457462310791 5.429869651794434
Loss :  1.8227368593215942 3.6463325023651123 5.469069480895996
Loss :  1.8216348886489868 3.2477214336395264 5.069356441497803
Loss :  1.8231607675552368 2.5960617065429688 4.419222354888916
Loss :  1.7943644523620605 4.34677267074585 6.14113712310791
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7817679643630981 4.434431076049805 6.216198921203613
Loss :  1.770186424255371 4.360108375549316 6.1302947998046875
Loss :  1.7919635772705078 4.138038158416748 5.930001735687256
Total LOSS train 5.22813227726863 valid 6.104408144950867
CE LOSS train 1.8141223705731906 valid 0.44799089431762695
Contrastive LOSS train 3.414009919533363 valid 1.034509539604187
EPOCH 13:
Loss :  1.816584587097168 2.891329765319824 4.707914352416992
Loss :  1.8138768672943115 3.4354746341705322 5.249351501464844
Loss :  1.8151791095733643 3.108196258544922 4.923375129699707
Loss :  1.8140895366668701 3.135101556777954 4.949191093444824
Loss :  1.8430200815200806 3.4938313961029053 5.336851596832275
Loss :  1.7986968755722046 3.540706157684326 5.33940315246582
Loss :  1.815016746520996 3.2601683139801025 5.0751848220825195
Loss :  1.8059524297714233 3.3257343769073486 5.131686687469482
Loss :  1.8251413106918335 3.466977596282959 5.292119026184082
Loss :  1.788967490196228 3.2955706119537354 5.084537982940674
Loss :  1.816711664199829 3.549976110458374 5.366687774658203
Loss :  1.8500137329101562 3.530221462249756 5.380235195159912
Loss :  1.8242557048797607 3.446185350418091 5.270441055297852
Loss :  1.8122344017028809 3.2748427391052246 5.0870771408081055
Loss :  1.8232427835464478 3.37310791015625 5.196350574493408
Loss :  1.8196680545806885 3.2686402797698975 5.088308334350586
Loss :  1.825069785118103 3.3171257972717285 5.142195701599121
Loss :  1.822277545928955 3.3800997734069824 5.2023773193359375
Loss :  1.820178747177124 3.096095085144043 4.916274070739746
Loss :  1.8070347309112549 3.103132486343384 4.910167217254639
  batch 20 loss: 1.8070347309112549, 3.103132486343384, 4.910167217254639
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8286700248718262 3.4963290691375732 5.32499885559082
Loss :  1.818636417388916 3.4391567707061768 5.257793426513672
Loss :  1.8158584833145142 3.5431251525878906 5.358983516693115
Loss :  1.8350690603256226 3.6598544120788574 5.4949235916137695
Loss :  1.8350231647491455 3.666156768798828 5.5011796951293945
Loss :  1.8150407075881958 3.7078967094421387 5.522937297821045
Loss :  1.8342463970184326 3.635820150375366 5.470066547393799
Loss :  1.8040401935577393 3.792048215866089 5.596088409423828
Loss :  1.838796854019165 3.5428149700164795 5.3816118240356445
Loss :  1.8034322261810303 3.815150737762451 5.618582725524902
Loss :  1.8507061004638672 3.682976245880127 5.533682346343994
Loss :  1.829629898071289 3.709362268447876 5.538991928100586
Loss :  1.8203312158584595 3.855342149734497 5.675673484802246
Loss :  1.8233227729797363 3.429034948348999 5.252357482910156
Loss :  1.8249295949935913 3.5134503841400146 5.338379859924316
Loss :  1.8476582765579224 3.5605733394622803 5.408231735229492
Loss :  1.8350803852081299 3.7917838096618652 5.626864433288574
Loss :  1.804511547088623 3.000312328338623 4.804823875427246
Loss :  1.8134393692016602 3.5794081687927246 5.392847537994385
Loss :  1.8011471033096313 3.018984317779541 4.820131301879883
  batch 40 loss: 1.8011471033096313, 3.018984317779541, 4.820131301879883
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8106865882873535 3.3826913833618164 5.19337797164917
Loss :  1.8239680528640747 3.018979787826538 4.842947959899902
Loss :  1.8235795497894287 3.193040370941162 5.016619682312012
Loss :  1.819813847541809 2.928194284439087 4.7480082511901855
Loss :  1.8350732326507568 2.8439431190490723 4.67901611328125
Loss :  1.8258934020996094 3.624825954437256 5.450719356536865
Loss :  1.8071364164352417 3.1762914657592773 4.983428001403809
Loss :  1.8226640224456787 3.1401305198669434 4.962794303894043
Loss :  1.8102757930755615 2.9869468212127686 4.79722261428833
Loss :  1.836555004119873 3.360153913497925 5.196708679199219
Loss :  1.791382074356079 3.6769566535949707 5.468338966369629
Loss :  1.8030450344085693 3.7076985836029053 5.510743618011475
Loss :  1.8109389543533325 3.710085391998291 5.521024227142334
Loss :  1.8248306512832642 3.5359604358673096 5.360791206359863
Loss :  1.818399429321289 3.5240731239318848 5.342472553253174
Loss :  1.7974272966384888 2.973775625228882 4.77120304107666
Loss :  1.8349931240081787 3.4539191722869873 5.288912296295166
Loss :  1.8387792110443115 3.090203285217285 4.928982734680176
Loss :  1.8383480310440063 3.4216694831848145 5.260017395019531
Loss :  1.8221039772033691 2.883995294570923 4.706099510192871
  batch 60 loss: 1.8221039772033691, 2.883995294570923, 4.706099510192871
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8229154348373413 3.552403450012207 5.375319004058838
Loss :  1.82466721534729 3.244037389755249 5.068704605102539
Loss :  1.826524257659912 3.346935510635376 5.173460006713867
Loss :  1.8247414827346802 3.142918825149536 4.967660427093506
Loss :  1.8268052339553833 2.917306900024414 4.744112014770508
Loss :  1.862562656402588 4.159788131713867 6.022350788116455
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8439311981201172 4.178724765777588 6.022655963897705
Loss :  1.8485584259033203 4.018795967102051 5.867354393005371
Loss :  1.878872275352478 3.962442636489868 5.841314792633057
Total LOSS train 5.198885602217454 valid 5.938418984413147
CE LOSS train 1.8208973737863394 valid 0.4697180688381195
Contrastive LOSS train 3.377988235767071 valid 0.990610659122467
EPOCH 14:
Loss :  1.803381085395813 3.1269774436950684 4.930358409881592
Loss :  1.80876886844635 3.2793779373168945 5.088146686553955
Loss :  1.8134441375732422 3.1847808361053467 4.998225212097168
Loss :  1.808754324913025 3.1455421447753906 4.954296588897705
Loss :  1.8439525365829468 3.2821385860443115 5.126091003417969
Loss :  1.8032686710357666 3.3311216831207275 5.134390354156494
Loss :  1.8110179901123047 3.425265312194824 5.236283302307129
Loss :  1.803666114807129 3.0746042728424072 4.878270149230957
Loss :  1.8286447525024414 3.183014392852783 5.011659145355225
Loss :  1.7931653261184692 2.7572224140167236 4.550387859344482
Loss :  1.8264366388320923 2.955054759979248 4.781491279602051
Loss :  1.85848867893219 3.2542989253997803 5.11278772354126
Loss :  1.8325138092041016 3.1551578044891357 4.987671852111816
Loss :  1.8198602199554443 3.0563347339630127 4.876194953918457
Loss :  1.8190096616744995 3.0819458961486816 4.900955677032471
Loss :  1.8200756311416626 3.451659679412842 5.271735191345215
Loss :  1.8281835317611694 3.654517412185669 5.482700824737549
Loss :  1.8190199136734009 3.2232513427734375 5.042271137237549
Loss :  1.825181245803833 3.543578863143921 5.368760108947754
Loss :  1.7988646030426025 3.672062873840332 5.4709272384643555
  batch 20 loss: 1.7988646030426025, 3.672062873840332, 5.4709272384643555
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.830806016921997 3.6264638900756836 5.457269668579102
Loss :  1.8267228603363037 3.3622031211853027 5.188925743103027
Loss :  1.8224622011184692 3.272113084793091 5.09457540512085
Loss :  1.8328973054885864 3.5375568866729736 5.37045431137085
Loss :  1.8411270380020142 3.55784010887146 5.398967266082764
Loss :  1.8276057243347168 2.990478992462158 4.818084716796875
Loss :  1.8411887884140015 3.4129154682159424 5.254104137420654
Loss :  1.807375431060791 2.7396581172943115 4.547033309936523
Loss :  1.8526880741119385 2.7872092723846436 4.639897346496582
Loss :  1.804104208946228 3.1426901817321777 4.946794509887695
Loss :  1.8551452159881592 3.189034938812256 5.044179916381836
Loss :  1.8270739316940308 3.0980427265167236 4.925116539001465
Loss :  1.8226419687271118 3.0687623023986816 4.891404151916504
Loss :  1.8238118886947632 2.9821789264678955 4.805990695953369
Loss :  1.826245665550232 3.2340128421783447 5.060258388519287
Loss :  1.8498495817184448 3.3073372840881348 5.157186985015869
Loss :  1.8304680585861206 3.2285096645355225 5.0589776039123535
Loss :  1.8019771575927734 3.087662696838379 4.889639854431152
Loss :  1.818904161453247 3.306891441345215 5.125795364379883
Loss :  1.8111560344696045 3.169508695602417 4.9806647300720215
  batch 40 loss: 1.8111560344696045, 3.169508695602417, 4.9806647300720215
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.816379189491272 2.8896100521087646 4.705989360809326
Loss :  1.8228397369384766 2.7056543827056885 4.528493881225586
Loss :  1.8280340433120728 2.812868356704712 4.640902519226074
Loss :  1.830087423324585 3.0324294567108154 4.8625168800354
Loss :  1.8370307683944702 2.9164328575134277 4.7534637451171875
Loss :  1.8302702903747559 3.2140297889709473 5.044300079345703
Loss :  1.8136965036392212 3.0026254653930664 4.816321849822998
Loss :  1.8183717727661133 3.102938175201416 4.921309947967529
Loss :  1.8174095153808594 3.150773286819458 4.968182563781738
Loss :  1.83962881565094 3.0705559253692627 4.910184860229492
Loss :  1.808186411857605 2.8638219833374023 4.672008514404297
Loss :  1.8127548694610596 2.9617960453033447 4.774550914764404
Loss :  1.8246400356292725 3.0257375240325928 4.850377559661865
Loss :  1.8303219079971313 2.885838747024536 4.716160774230957
Loss :  1.827099323272705 2.9998364448547363 4.826935768127441
Loss :  1.8014105558395386 2.9386744499206543 4.740085124969482
Loss :  1.844160556793213 3.0164575576782227 4.8606181144714355
Loss :  1.8325543403625488 2.937525510787964 4.770079612731934
Loss :  1.8378169536590576 3.1863443851470947 5.024161338806152
Loss :  1.8261504173278809 3.1418890953063965 4.968039512634277
  batch 60 loss: 1.8261504173278809, 3.1418890953063965, 4.968039512634277
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8319594860076904 3.1213326454162598 4.953291893005371
Loss :  1.8467408418655396 3.8815691471099854 5.7283101081848145
Loss :  1.8409597873687744 3.5903472900390625 5.431306838989258
Loss :  1.8206180334091187 3.5130774974823 5.333695411682129
Loss :  1.821486234664917 3.48458194732666 5.306068420410156
Loss :  1.7833642959594727 4.032543659210205 5.815907955169678
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.766870141029358 4.1178717613220215 5.88474178314209
Loss :  1.7485190629959106 3.975994825363159 5.724514007568359
Loss :  1.8009597063064575 3.7048728466033936 5.505832672119141
Total LOSS train 4.999481245187613 valid 5.732749104499817
CE LOSS train 1.8243162595308744 valid 0.4502399265766144
Contrastive LOSS train 3.1751650150005633 valid 0.9262182116508484
Saved best model. Old loss 5.892933487892151 and new best loss 5.732749104499817
EPOCH 15:
Loss :  1.8030328750610352 4.171027660369873 5.974060535430908
Loss :  1.8341041803359985 4.078046798706055 5.912150859832764
Loss :  1.8247077465057373 3.9900941848754883 5.814802169799805
Loss :  1.8178876638412476 3.851006031036377 5.668893814086914
Loss :  1.8459923267364502 3.6613147258758545 5.507307052612305
Loss :  1.8151638507843018 3.8845250606536865 5.699688911437988
Loss :  1.8344359397888184 3.60532546043396 5.439761161804199
Loss :  1.8222349882125854 3.4874343872070312 5.309669494628906
Loss :  1.8299283981323242 3.31826114654541 5.148189544677734
Loss :  1.8008569478988647 3.534048557281494 5.334905624389648
Loss :  1.8336665630340576 3.6327884197235107 5.466454982757568
Loss :  1.8616567850112915 3.452486753463745 5.314143657684326
Loss :  1.8338230848312378 3.022137403488159 4.855960369110107
Loss :  1.8205476999282837 2.901047945022583 4.721595764160156
Loss :  1.8171741962432861 2.8864028453826904 4.703577041625977
Loss :  1.8126009702682495 2.9971401691436768 4.809741020202637
Loss :  1.8261808156967163 3.647557497024536 5.473738193511963
Loss :  1.8107534646987915 3.888855218887329 5.69960880279541
Loss :  1.8125978708267212 3.3361570835113525 5.148755073547363
Loss :  1.788711667060852 3.5325937271118164 5.321305274963379
  batch 20 loss: 1.788711667060852, 3.5325937271118164, 5.321305274963379
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8055503368377686 3.4861950874328613 5.291745185852051
Loss :  1.8159972429275513 3.456144094467163 5.272141456604004
Loss :  1.8106893301010132 3.198180675506592 5.0088701248168945
Loss :  1.830029845237732 3.0572099685668945 4.887239933013916
Loss :  1.8296524286270142 3.251267194747925 5.0809197425842285
Loss :  1.807548999786377 3.098860263824463 4.90640926361084
Loss :  1.8297008275985718 3.5215024948120117 5.351203441619873
Loss :  1.808268666267395 3.068932056427002 4.877200603485107
Loss :  1.8398927450180054 2.713932991027832 4.553825855255127
Loss :  1.7947680950164795 2.717273235321045 4.512041091918945
Loss :  1.8431531190872192 2.920821189880371 4.763974189758301
Loss :  1.8095694780349731 2.790623188018799 4.600192546844482
Loss :  1.808992624282837 2.6180245876312256 4.4270172119140625
Loss :  1.8039628267288208 2.741783857345581 4.545746803283691
Loss :  1.813218355178833 3.0593652725219727 4.872583389282227
Loss :  1.833575963973999 3.0383408069610596 4.871916770935059
Loss :  1.8143765926361084 3.035606622695923 4.849983215332031
Loss :  1.7879489660263062 2.7467355728149414 4.534684658050537
Loss :  1.799487829208374 2.9350168704986572 4.734504699707031
Loss :  1.8026165962219238 2.90018630027771 4.702802658081055
  batch 40 loss: 1.8026165962219238, 2.90018630027771, 4.702802658081055
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8050286769866943 3.202740430831909 5.0077691078186035
Loss :  1.8020532131195068 3.117208242416382 4.919261455535889
Loss :  1.8057072162628174 3.3191981315612793 5.124905586242676
Loss :  1.8171414136886597 3.1869513988494873 5.004092693328857
Loss :  1.816253900527954 3.036740303039551 4.852993965148926
Loss :  1.8125252723693848 3.0769712924957275 4.889496803283691
Loss :  1.8009393215179443 3.04461669921875 4.845556259155273
Loss :  1.7930349111557007 3.117016553878784 4.910051345825195
Loss :  1.7968993186950684 2.9345009326934814 4.731400489807129
Loss :  1.8167356252670288 3.3340389728546143 5.1507744789123535
Loss :  1.7786314487457275 3.517636299133301 5.296267509460449
Loss :  1.781027913093567 3.4026219844818115 5.183650016784668
Loss :  1.8106915950775146 3.3436503410339355 5.154341697692871
Loss :  1.8072410821914673 2.9595935344696045 4.766834735870361
Loss :  1.7965291738510132 3.4246206283569336 5.221149921417236
Loss :  1.7776224613189697 3.2071642875671387 4.9847869873046875
Loss :  1.8172041177749634 3.067159414291382 4.884363651275635
Loss :  1.8126364946365356 3.1648173332214355 4.977453708648682
Loss :  1.8160741329193115 3.361621856689453 5.177696228027344
Loss :  1.7935980558395386 3.2554867267608643 5.049084663391113
  batch 60 loss: 1.7935980558395386, 3.2554867267608643, 5.049084663391113
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8131687641143799 3.3348097801208496 5.147978782653809
Loss :  1.8097890615463257 3.5044822692871094 5.314271450042725
Loss :  1.817030906677246 3.2690188884735107 5.086050033569336
Loss :  1.8072458505630493 3.135719060897827 4.942965030670166
Loss :  1.8121693134307861 2.603184461593628 4.415353775024414
Loss :  1.5898128747940063 3.974379301071167 5.564192295074463
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6322587728500366 4.081300735473633 5.713559627532959
Loss :  1.6665127277374268 3.943861484527588 5.610374450683594
Loss :  1.6180883646011353 3.647834539413452 5.265923023223877
Total LOSS train 5.061659424121563 valid 5.538512349128723
CE LOSS train 1.813108279154851 valid 0.4045220911502838
Contrastive LOSS train 3.248551126626822 valid 0.911958634853363
Saved best model. Old loss 5.732749104499817 and new best loss 5.538512349128723
EPOCH 16:
Loss :  1.7859842777252197 3.081345319747925 4.8673295974731445
Loss :  1.808941125869751 3.2088623046875 5.017803192138672
Loss :  1.7996240854263306 3.1804020404815674 4.9800262451171875
Loss :  1.7924799919128418 2.876063346862793 4.668543338775635
Loss :  1.8403582572937012 3.414759635925293 5.255117893218994
Loss :  1.788266897201538 3.7273671627044678 5.515634059906006
Loss :  1.8073396682739258 3.328092336654663 5.135432243347168
Loss :  1.7806117534637451 3.54598069190979 5.326592445373535
Loss :  1.8086599111557007 3.356894016265869 5.165554046630859
Loss :  1.7631691694259644 3.23917293548584 5.002342224121094
Loss :  1.8017489910125732 3.306112766265869 5.107861518859863
Loss :  1.8335272073745728 3.381565809249878 5.21509313583374
Loss :  1.8031872510910034 3.3662338256835938 5.169421195983887
Loss :  1.800075888633728 3.5743658542633057 5.374441623687744
Loss :  1.8009659051895142 3.5352706909179688 5.336236476898193
Loss :  1.789325475692749 3.3107635974884033 5.100089073181152
Loss :  1.8122366666793823 3.117363691329956 4.929600238800049
Loss :  1.7949742078781128 3.1019561290740967 4.89693021774292
Loss :  1.8047006130218506 2.763935089111328 4.568635940551758
Loss :  1.7885600328445435 2.8221375942230225 4.6106977462768555
  batch 20 loss: 1.7885600328445435, 2.8221375942230225, 4.6106977462768555
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8011701107025146 2.8817567825317383 4.682927131652832
Loss :  1.805975079536438 3.19242262840271 4.9983978271484375
Loss :  1.80501127243042 3.065540075302124 4.870551109313965
Loss :  1.808470368385315 3.0859344005584717 4.894404888153076
Loss :  1.8273147344589233 3.4662115573883057 5.2935261726379395
Loss :  1.7977122068405151 3.37359881401062 5.171310901641846
Loss :  1.8092352151870728 3.6598901748657227 5.469125270843506
Loss :  1.7861993312835693 3.177741527557373 4.963940620422363
Loss :  1.8248927593231201 2.956141710281372 4.781034469604492
Loss :  1.7976136207580566 2.9745688438415527 4.772182464599609
Loss :  1.843151569366455 3.167861223220825 5.011013031005859
Loss :  1.8040440082550049 3.2258780002593994 5.029922008514404
Loss :  1.8014962673187256 2.7772886753082275 4.578784942626953
Loss :  1.8011852502822876 2.747480630874634 4.548666000366211
Loss :  1.8160051107406616 2.9078283309936523 4.7238335609436035
Loss :  1.827677607536316 2.7370197772979736 4.564697265625
Loss :  1.816896915435791 2.7610204219818115 4.577917098999023
Loss :  1.7770198583602905 2.9708991050720215 4.747919082641602
Loss :  1.8018755912780762 3.1975691318511963 4.999444961547852
Loss :  1.7957345247268677 3.5251846313476562 5.320919036865234
  batch 40 loss: 1.7957345247268677, 3.5251846313476562, 5.320919036865234
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7913620471954346 3.286463737487793 5.077825546264648
Loss :  1.7889468669891357 3.255580425262451 5.044527053833008
Loss :  1.8078314065933228 3.092290163040161 4.900121688842773
Loss :  1.7936774492263794 2.9016897678375244 4.695367336273193
Loss :  1.8030505180358887 3.181485652923584 4.984536170959473
Loss :  1.7978184223175049 2.8167004585266113 4.614519119262695
Loss :  1.785356879234314 2.8710005283355713 4.656357288360596
Loss :  1.7933589220046997 3.084759473800659 4.878118515014648
Loss :  1.7841248512268066 3.0428197383880615 4.826944351196289
Loss :  1.8153469562530518 3.304527997970581 5.119874954223633
Loss :  1.7877734899520874 3.724287748336792 5.51206111907959
Loss :  1.7852317094802856 3.2827417850494385 5.067973613739014
Loss :  1.805216908454895 3.3413896560668945 5.1466064453125
Loss :  1.8156028985977173 3.2099716663360596 5.025574684143066
Loss :  1.8087213039398193 3.180469512939453 4.989191055297852
Loss :  1.7824184894561768 3.5971405506134033 5.37955904006958
Loss :  1.8288339376449585 3.2396576404571533 5.068491458892822
Loss :  1.8134764432907104 3.538759469985962 5.352235794067383
Loss :  1.8219799995422363 3.4453468322753906 5.267326831817627
Loss :  1.7992390394210815 3.257328748703003 5.056567668914795
  batch 60 loss: 1.7992390394210815, 3.257328748703003, 5.056567668914795
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8075283765792847 3.2210309505462646 5.02855920791626
Loss :  1.807816743850708 3.311051368713379 5.118867874145508
Loss :  1.8093385696411133 2.7973179817199707 4.606656551361084
Loss :  1.8055592775344849 2.8993985652923584 4.704957962036133
Loss :  1.809234380722046 2.3880372047424316 4.197271347045898
Loss :  1.5033310651779175 3.849703550338745 5.353034496307373
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.559817910194397 4.013108730316162 5.5729265213012695
Loss :  1.5875900983810425 3.935512065887451 5.523102283477783
Loss :  1.5797268152236938 3.6078500747680664 5.187576770782471
Total LOSS train 4.977907584263728 valid 5.409160017967224
CE LOSS train 1.8031117641008818 valid 0.39493170380592346
Contrastive LOSS train 3.174795829332792 valid 0.9019625186920166
Saved best model. Old loss 5.538512349128723 and new best loss 5.409160017967224
EPOCH 17:
Loss :  1.7895317077636719 2.679478406906128 4.469010353088379
Loss :  1.8023202419281006 3.48922061920166 5.29154109954834
Loss :  1.7940584421157837 3.2020630836486816 4.996121406555176
Loss :  1.7917543649673462 3.1675126552581787 4.9592671394348145
Loss :  1.831218957901001 3.1539053916931152 4.985124588012695
Loss :  1.7920299768447876 3.411621570587158 5.203651428222656
Loss :  1.7929878234863281 3.239198923110962 5.032186508178711
Loss :  1.776540994644165 3.0149600505828857 4.791501045227051
Loss :  1.808953881263733 2.9595446586608887 4.768498420715332
Loss :  1.7675118446350098 2.7575337886810303 4.525045394897461
Loss :  1.8070528507232666 3.2200217247009277 5.027074813842773
Loss :  1.8413317203521729 2.929785966873169 4.771117687225342
Loss :  1.8066246509552002 2.668748617172241 4.475373268127441
Loss :  1.8100825548171997 2.636378765106201 4.446461200714111
Loss :  1.8252308368682861 2.632227897644043 4.45745849609375
Loss :  1.7964061498641968 3.143266201019287 4.939672470092773
Loss :  1.8216408491134644 2.99367094039917 4.815311908721924
Loss :  1.8008493185043335 3.1976470947265625 4.9984965324401855
Loss :  1.806628704071045 2.754767417907715 4.56139612197876
Loss :  1.7892570495605469 2.9883625507354736 4.777619361877441
  batch 20 loss: 1.7892570495605469, 2.9883625507354736, 4.777619361877441
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.801580786705017 2.788466215133667 4.5900468826293945
Loss :  1.8054076433181763 2.981976270675659 4.787384033203125
Loss :  1.7998358011245728 2.549149751663208 4.34898567199707
Loss :  1.7988864183425903 3.180389404296875 4.979275703430176
Loss :  1.827103614807129 3.52972674369812 5.356830596923828
Loss :  1.8000844717025757 3.2907824516296387 5.090867042541504
Loss :  1.808321475982666 3.5956802368164062 5.404001712799072
Loss :  1.777136206626892 3.1346726417541504 4.911808967590332
Loss :  1.8339601755142212 3.1930460929870605 5.027006149291992
Loss :  1.7976391315460205 3.485658884048462 5.283298015594482
Loss :  1.8473433256149292 3.377948045730591 5.2252912521362305
Loss :  1.7961755990982056 3.396210193634033 5.192385673522949
Loss :  1.7936774492263794 3.2823736667633057 5.076051235198975
Loss :  1.7956072092056274 3.802839517593384 5.598446846008301
Loss :  1.8081809282302856 3.476445436477661 5.284626483917236
Loss :  1.8208503723144531 3.3102331161499023 5.1310834884643555
Loss :  1.8030577898025513 3.604138135910034 5.407196044921875
Loss :  1.760082483291626 3.1949610710144043 4.955043792724609
Loss :  1.7900851964950562 3.0793793201446533 4.86946439743042
Loss :  1.794950246810913 3.2115578651428223 5.006507873535156
  batch 40 loss: 1.794950246810913, 3.2115578651428223, 5.006507873535156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7907973527908325 3.510645627975464 5.301443099975586
Loss :  1.7870430946350098 3.0695700645446777 4.8566131591796875
Loss :  1.80281400680542 3.1395089626312256 4.942322731018066
Loss :  1.8022348880767822 2.936447858810425 4.738682746887207
Loss :  1.8047993183135986 2.8419387340545654 4.646738052368164
Loss :  1.803363561630249 2.835902690887451 4.639266014099121
Loss :  1.7975245714187622 2.862098217010498 4.659622669219971
Loss :  1.7897398471832275 2.786820411682129 4.576560020446777
Loss :  1.7927932739257812 2.9558181762695312 4.7486114501953125
Loss :  1.809987187385559 3.6040115356445312 5.413998603820801
Loss :  1.7830619812011719 3.42364239692688 5.206704139709473
Loss :  1.7852115631103516 3.1569714546203613 4.942183017730713
Loss :  1.809697151184082 3.0169732570648193 4.8266706466674805
Loss :  1.8082399368286133 2.9998579025268555 4.808097839355469
Loss :  1.799685001373291 3.2310616970062256 5.0307464599609375
Loss :  1.7774507999420166 3.1757612228393555 4.953211784362793
Loss :  1.8141305446624756 3.2501626014709473 5.064292907714844
Loss :  1.8013219833374023 3.6171584129333496 5.418480396270752
Loss :  1.8092389106750488 3.36580753326416 5.175046443939209
Loss :  1.7971278429031372 3.5572686195373535 5.354396343231201
  batch 60 loss: 1.7971278429031372, 3.5572686195373535, 5.354396343231201
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.804661750793457 3.162391185760498 4.967052936553955
Loss :  1.805069088935852 3.397350311279297 5.202419281005859
Loss :  1.8116450309753418 2.827913761138916 4.639558792114258
Loss :  1.796147346496582 3.306492328643799 5.102639675140381
Loss :  1.7857335805892944 2.6397018432617188 4.425435543060303
Loss :  1.438012719154358 4.197609901428223 5.635622501373291
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.5239557027816772 4.070992469787598 5.5949482917785645
Loss :  1.5561070442199707 3.968308925628662 5.524415969848633
Loss :  1.5800665616989136 3.586383581161499 5.166450023651123
Total LOSS train 4.945512705582839 valid 5.480359196662903
CE LOSS train 1.8012538286355826 valid 0.3950166404247284
Contrastive LOSS train 3.144258895287147 valid 0.8965958952903748
EPOCH 18:
Loss :  1.7766152620315552 3.051334857940674 4.8279500007629395
Loss :  1.7905889749526978 3.2016587257385254 4.992247581481934
Loss :  1.7782264947891235 2.958130121231079 4.736356735229492
Loss :  1.7860760688781738 2.7138147354125977 4.4998908042907715
Loss :  1.8218975067138672 2.924799919128418 4.746697425842285
Loss :  1.7890218496322632 3.081911325454712 4.8709330558776855
Loss :  1.788801670074463 3.2910029888153076 5.079804420471191
Loss :  1.7669672966003418 3.717054843902588 5.48402214050293
Loss :  1.7988940477371216 3.332958698272705 5.131852626800537
Loss :  1.7509498596191406 3.0772995948791504 4.828249454498291
Loss :  1.792097806930542 3.4075586795806885 5.1996564865112305
Loss :  1.8298622369766235 3.150730609893799 4.980592727661133
Loss :  1.7901561260223389 3.1332316398620605 4.92338752746582
Loss :  1.7906099557876587 3.214237689971924 5.004847526550293
Loss :  1.7929168939590454 2.9083456993103027 4.701262474060059
Loss :  1.7759575843811035 2.823479413986206 4.5994367599487305
Loss :  1.7996461391448975 2.824651002883911 4.624297142028809
Loss :  1.784993052482605 2.777461290359497 4.5624542236328125
Loss :  1.7944353818893433 2.9339616298675537 4.728396892547607
Loss :  1.7824196815490723 2.826493978500366 4.608913421630859
  batch 20 loss: 1.7824196815490723, 2.826493978500366, 4.608913421630859
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7857316732406616 3.0617527961730957 4.847484588623047
Loss :  1.7880558967590332 2.961855411529541 4.749911308288574
Loss :  1.7932268381118774 3.4697444438934326 5.2629714012146
Loss :  1.7989935874938965 3.4966421127319336 5.29563570022583
Loss :  1.8154264688491821 3.527759313583374 5.343185901641846
Loss :  1.7810527086257935 3.483576774597168 5.264629364013672
Loss :  1.7977395057678223 3.345831871032715 5.143571376800537
Loss :  1.775712251663208 3.197722911834717 4.973435401916504
Loss :  1.81643545627594 3.0513572692871094 4.86779260635376
Loss :  1.8024903535842896 3.1184160709381104 4.9209065437316895
Loss :  1.8427047729492188 2.9049148559570312 4.74761962890625
Loss :  1.7953236103057861 3.1548845767974854 4.9502081871032715
Loss :  1.7889474630355835 3.176569700241089 4.965517044067383
Loss :  1.7933658361434937 3.1588785648345947 4.952244281768799
Loss :  1.8049919605255127 3.0366151332855225 4.841607093811035
Loss :  1.8145735263824463 2.915283203125 4.729856491088867
Loss :  1.802661418914795 2.8719472885131836 4.6746087074279785
Loss :  1.7563453912734985 2.7562968730926514 4.5126423835754395
Loss :  1.7800971269607544 2.8264646530151367 4.606561660766602
Loss :  1.7816226482391357 2.9115469455718994 4.693169593811035
  batch 40 loss: 1.7816226482391357, 2.9115469455718994, 4.693169593811035
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7739053964614868 3.061650037765503 4.835555553436279
Loss :  1.7677205801010132 2.875105619430542 4.642826080322266
Loss :  1.7888898849487305 2.9928786754608154 4.781768798828125
Loss :  1.7816450595855713 3.5108156204223633 5.2924604415893555
Loss :  1.791664719581604 2.967109203338623 4.7587738037109375
Loss :  1.7839699983596802 3.1488304138183594 4.93280029296875
Loss :  1.7784615755081177 2.9981207847595215 4.77658224105835
Loss :  1.7770922183990479 2.5409770011901855 4.3180694580078125
Loss :  1.7814781665802002 2.6540727615356445 4.435550689697266
Loss :  1.8000847101211548 2.9944727420806885 4.794557571411133
Loss :  1.7763397693634033 2.896721363067627 4.673061370849609
Loss :  1.7763117551803589 3.330518960952759 5.106830596923828
Loss :  1.7920584678649902 2.9535744190216064 4.745633125305176
Loss :  1.798116683959961 2.783048391342163 4.581165313720703
Loss :  1.80336332321167 3.494520664215088 5.297883987426758
Loss :  1.7677844762802124 3.0283029079437256 4.796087265014648
Loss :  1.8160934448242188 3.147473096847534 4.963566780090332
Loss :  1.796429991722107 3.287471055984497 5.0839009284973145
Loss :  1.8083544969558716 3.5534989833831787 5.36185359954834
Loss :  1.7949717044830322 3.037666082382202 4.832637786865234
  batch 60 loss: 1.7949717044830322, 3.037666082382202, 4.832637786865234
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8026844263076782 3.380622386932373 5.183306694030762
Loss :  1.8038679361343384 3.6279327869415283 5.431800842285156
Loss :  1.8079289197921753 3.1648173332214355 4.9727463722229
Loss :  1.7917602062225342 3.1846001148223877 4.976360321044922
Loss :  1.781640887260437 2.9828128814697266 4.764453887939453
Loss :  1.3873217105865479 4.098564147949219 5.4858856201171875
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.500193476676941 4.14496374130249 5.645157337188721
Loss :  1.4957374334335327 4.021717071533203 5.517454624176025
Loss :  1.5543839931488037 3.828866720199585 5.383250713348389
Total LOSS train 4.889462192241962 valid 5.507937073707581
CE LOSS train 1.7913730951455924 valid 0.3885959982872009
Contrastive LOSS train 3.0980891154362604 valid 0.9572166800498962
EPOCH 19:
Loss :  1.7761588096618652 3.162489652633667 4.938648223876953
Loss :  1.7885419130325317 3.3846077919006348 5.173149585723877
Loss :  1.7737382650375366 3.572359800338745 5.346097946166992
Loss :  1.7763450145721436 2.9915659427642822 4.767910957336426
Loss :  1.8187369108200073 3.175107717514038 4.993844509124756
Loss :  1.788488745689392 3.3250246047973633 5.113513469696045
Loss :  1.7874523401260376 3.0534069538116455 4.840859413146973
Loss :  1.7657699584960938 3.2093429565429688 4.9751129150390625
Loss :  1.7946432828903198 2.9856913089752197 4.78033447265625
Loss :  1.7471320629119873 2.6854143142700195 4.432546615600586
Loss :  1.785942554473877 2.9182116985321045 4.704154014587402
Loss :  1.8292075395584106 3.2259140014648438 5.055121421813965
Loss :  1.7851083278656006 3.3899447917938232 5.175053119659424
Loss :  1.788561463356018 3.247058153152466 5.035619735717773
Loss :  1.7884103059768677 3.0341238975524902 4.822534084320068
Loss :  1.7756452560424805 2.7405283451080322 4.516173362731934
Loss :  1.7992697954177856 2.672776460647583 4.472046375274658
Loss :  1.7819710969924927 2.5540568828582764 4.336028099060059
Loss :  1.7832554578781128 2.9334585666656494 4.716713905334473
Loss :  1.7720119953155518 2.923218250274658 4.695230484008789
  batch 20 loss: 1.7720119953155518, 2.923218250274658, 4.695230484008789
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7774639129638672 2.705329179763794 4.482792854309082
Loss :  1.7830605506896973 2.9160361289978027 4.6990966796875
Loss :  1.781001091003418 2.863680601119995 4.644681930541992
Loss :  1.7850391864776611 2.8079609870910645 4.593000411987305
Loss :  1.808766484260559 3.2459828853607178 5.054749488830566
Loss :  1.7831733226776123 3.2243075370788574 5.007480621337891
Loss :  1.7945023775100708 3.234785795211792 5.029288291931152
Loss :  1.7608133554458618 3.1445655822753906 4.905378818511963
Loss :  1.8120895624160767 3.443437337875366 5.255527019500732
Loss :  1.7853680849075317 3.1951794624328613 4.9805474281311035
Loss :  1.8331151008605957 3.3627066612243652 5.195821762084961
Loss :  1.7810783386230469 3.474792003631592 5.255870342254639
Loss :  1.7756940126419067 3.5108320713043213 5.286526203155518
Loss :  1.7798194885253906 3.1650192737579346 4.944838523864746
Loss :  1.7964128255844116 3.3325083255767822 5.128921031951904
Loss :  1.8078173398971558 3.4290685653686523 5.236886024475098
Loss :  1.7951774597167969 2.9916374683380127 4.7868146896362305
Loss :  1.7449203729629517 2.8309850692749023 4.5759053230285645
Loss :  1.7771118879318237 3.0792059898376465 4.85631799697876
Loss :  1.7838995456695557 3.3824496269226074 5.166349411010742
  batch 40 loss: 1.7838995456695557, 3.3824496269226074, 5.166349411010742
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7736492156982422 3.016636371612549 4.790285587310791
Loss :  1.7656192779541016 3.0967907905578613 4.862410068511963
Loss :  1.788015365600586 3.125643253326416 4.913658618927002
Loss :  1.7846403121948242 3.1164004802703857 4.901041030883789
Loss :  1.7865360975265503 2.971611976623535 4.758148193359375
Loss :  1.7750256061553955 3.065443515777588 4.8404693603515625
Loss :  1.768017292022705 3.0884902477264404 4.856507301330566
Loss :  1.7694292068481445 3.0609397888183594 4.830368995666504
Loss :  1.7464709281921387 3.1969053745269775 4.943376541137695
Loss :  1.7899185419082642 3.3985722064971924 5.188490867614746
Loss :  1.7583664655685425 3.1791365146636963 4.937502861022949
Loss :  1.755557656288147 3.2267050743103027 4.98226261138916
Loss :  1.7799618244171143 3.0452165603637695 4.825178146362305
Loss :  1.7710754871368408 3.127821207046509 4.89889669418335
Loss :  1.7947847843170166 3.4172329902648926 5.212018013000488
Loss :  1.7480089664459229 3.3081085681915283 5.056117534637451
Loss :  1.7970688343048096 3.446105480194092 5.2431745529174805
Loss :  1.7848047018051147 3.45646595954895 5.241270542144775
Loss :  1.7931692600250244 3.44728684425354 5.2404561042785645
Loss :  1.7740553617477417 2.968888282775879 4.74294376373291
  batch 60 loss: 1.7740553617477417, 2.968888282775879, 4.74294376373291
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7785208225250244 2.844223976135254 4.622744560241699
Loss :  1.790261149406433 2.8457188606262207 4.635980129241943
Loss :  1.7845096588134766 2.6271631717681885 4.411672592163086
Loss :  1.7779241800308228 2.8712832927703857 4.649207592010498
Loss :  1.7717959880828857 2.6770951747894287 4.4488911628723145
Loss :  1.3500053882598877 4.1972784996032715 5.547284126281738
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.4719867706298828 4.2678751945495605 5.739861965179443
Loss :  1.4715436697006226 4.250697135925293 5.722240924835205
Loss :  1.5116857290267944 4.098629474639893 5.610315322875977
Total LOSS train 4.892470169067383 valid 5.654925584793091
CE LOSS train 1.7824600366445689 valid 0.3779214322566986
Contrastive LOSS train 3.110010132422814 valid 1.0246573686599731
EPOCH 20:
Loss :  1.7484184503555298 3.288071870803833 5.036490440368652
Loss :  1.7600550651550293 3.4014127254486084 5.161467552185059
Loss :  1.7425090074539185 3.278820037841797 5.021328926086426
Loss :  1.754388689994812 3.446922540664673 5.201311111450195
Loss :  1.7969156503677368 3.4143033027648926 5.21121883392334
Loss :  1.7560285329818726 3.416306257247925 5.172334671020508
Loss :  1.7631449699401855 3.4512147903442383 5.214359760284424
Loss :  1.7455024719238281 3.5228848457336426 5.268387317657471
Loss :  1.7832282781600952 3.6916821002960205 5.474910259246826
Loss :  1.729838490486145 2.9186830520629883 4.648521423339844
Loss :  1.7735505104064941 2.984473943710327 4.758024215698242
Loss :  1.8192286491394043 3.078627347946167 4.897855758666992
Loss :  1.775234341621399 3.0808892250061035 4.856123447418213
Loss :  1.7785584926605225 2.7359108924865723 4.514469146728516
Loss :  1.7799639701843262 2.9516611099243164 4.731625080108643
Loss :  1.7620856761932373 2.8119773864746094 4.574063301086426
Loss :  1.7902902364730835 2.7682220935821533 4.558512210845947
Loss :  1.7666003704071045 2.907155752182007 4.673756122589111
Loss :  1.7754688262939453 2.520280122756958 4.295748710632324
Loss :  1.7594780921936035 2.8603804111480713 4.619858741760254
  batch 20 loss: 1.7594780921936035, 2.8603804111480713, 4.619858741760254
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7675790786743164 2.9787468910217285 4.746325969696045
Loss :  1.7796157598495483 3.2856481075286865 5.065263748168945
Loss :  1.7717989683151245 3.444063901901245 5.21586275100708
Loss :  1.780155897140503 3.193220376968384 4.973376274108887
Loss :  1.803679347038269 3.28820538520813 5.091884613037109
Loss :  1.7736972570419312 3.2976253032684326 5.071322441101074
Loss :  1.789994239807129 3.1638307571411133 4.953824996948242
Loss :  1.7601560354232788 3.09802508354187 4.858180999755859
Loss :  1.7993109226226807 2.8720877170562744 4.671398639678955
Loss :  1.7737152576446533 3.227257251739502 5.000972747802734
Loss :  1.8241652250289917 3.3533976078033447 5.177562713623047
Loss :  1.7747572660446167 3.1409528255462646 4.915709972381592
Loss :  1.771112084388733 2.662712812423706 4.4338250160217285
Loss :  1.7738715410232544 2.841048240661621 4.614919662475586
Loss :  1.7928028106689453 2.968590021133423 4.761392593383789
Loss :  1.8062984943389893 2.902676820755005 4.708975315093994
Loss :  1.7956793308258057 2.953108072280884 4.7487874031066895
Loss :  1.7574244737625122 2.911045789718628 4.66847038269043
Loss :  1.79185950756073 3.0778236389160156 4.869683265686035
Loss :  1.7875016927719116 3.1537864208221436 4.941287994384766
  batch 40 loss: 1.7875016927719116, 3.1537864208221436, 4.941287994384766
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7801148891448975 3.5906548500061035 5.370769500732422
Loss :  1.770997166633606 3.3918914794921875 5.162888526916504
Loss :  1.7830780744552612 3.5411698818206787 5.32424783706665
Loss :  1.7790935039520264 3.2764790058135986 5.055572509765625
Loss :  1.7833828926086426 2.826289176940918 4.6096720695495605
Loss :  1.7763792276382446 2.8470826148986816 4.623461723327637
Loss :  1.7716388702392578 2.95857572555542 4.730214595794678
Loss :  1.7660754919052124 3.057582378387451 4.823657989501953
Loss :  1.760652780532837 3.252957344055176 5.013609886169434
Loss :  1.78651762008667 3.2607672214508057 5.047285079956055
Loss :  1.766203761100769 3.0811586380004883 4.847362518310547
Loss :  1.7642816305160522 3.0117576122283936 4.776039123535156
Loss :  1.7853186130523682 3.233686685562134 5.019005298614502
Loss :  1.7856462001800537 2.86982798576355 4.6554741859436035
Loss :  1.7863126993179321 3.208651304244995 4.994964122772217
Loss :  1.7521405220031738 3.042067766189575 4.794208526611328
Loss :  1.8020014762878418 3.0609753131866455 4.862977027893066
Loss :  1.7829618453979492 2.9147493839263916 4.697710990905762
Loss :  1.7971203327178955 3.2532591819763184 5.050379753112793
Loss :  1.7777624130249023 2.9234800338745117 4.701242446899414
  batch 60 loss: 1.7777624130249023, 2.9234800338745117, 4.701242446899414
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7828218936920166 2.757603168487549 4.5404253005981445
Loss :  1.7791588306427002 2.9370789527893066 4.716238021850586
Loss :  1.7846370935440063 3.2768514156341553 5.061488628387451
Loss :  1.7706378698349 3.3497281074523926 5.120366096496582
Loss :  1.7635340690612793 2.633537769317627 4.397071838378906
Loss :  1.5603258609771729 4.1089982986450195 5.669323921203613
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.606856107711792 4.185210704803467 5.79206657409668
Loss :  1.655099868774414 3.9797186851501465 5.6348185539245605
Loss :  1.6512107849121094 3.7954328060150146 5.446643829345703
Total LOSS train 4.8827035096975475 valid 5.635713219642639
CE LOSS train 1.7765251343066877 valid 0.41280269622802734
Contrastive LOSS train 3.1061783973987285 valid 0.9488582015037537
EPOCH 21:
Loss :  1.7513347864151 3.4875903129577637 5.238924980163574
Loss :  1.7692484855651855 3.363985776901245 5.133234024047852
Loss :  1.7454522848129272 3.4575083255767822 5.20296049118042
Loss :  1.747969150543213 3.1395559310913086 4.8875250816345215
Loss :  1.798459529876709 2.96189546585083 4.760354995727539
Loss :  1.7609440088272095 3.2356460094451904 4.9965901374816895
Loss :  1.7733891010284424 3.372136116027832 5.145524978637695
Loss :  1.746983528137207 3.416964054107666 5.163947582244873
Loss :  1.785400390625 2.9516208171844482 4.737021446228027
Loss :  1.7364482879638672 3.1542766094207764 4.890725135803223
Loss :  1.7743802070617676 3.0209715366363525 4.795351982116699
Loss :  1.8099229335784912 3.1370396614074707 4.946962356567383
Loss :  1.7745411396026611 3.0811591148376465 4.855700492858887
Loss :  1.7770578861236572 2.7095823287963867 4.486639976501465
Loss :  1.7771711349487305 3.2292563915252686 5.006427764892578
Loss :  1.7545599937438965 3.0888094902038574 4.843369483947754
Loss :  1.782164454460144 2.811324119567871 4.593488693237305
Loss :  1.763366937637329 3.088355302810669 4.851722240447998
Loss :  1.770577073097229 2.6196560859680176 4.390233039855957
Loss :  1.7643637657165527 3.0137393474578857 4.778102874755859
  batch 20 loss: 1.7643637657165527, 3.0137393474578857, 4.778102874755859
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7728127241134644 2.8724756240844727 4.645288467407227
Loss :  1.7787261009216309 3.6746745109558105 5.453400611877441
Loss :  1.7812966108322144 3.392082452774048 5.173378944396973
Loss :  1.7846009731292725 3.2961037158966064 5.080704689025879
Loss :  1.8073086738586426 3.55953049659729 5.366839408874512
Loss :  1.7799102067947388 3.328892230987549 5.108802318572998
Loss :  1.7880191802978516 3.108320474624634 4.896339416503906
Loss :  1.7646721601486206 3.3882851600646973 5.152957439422607
Loss :  1.7983884811401367 3.152148485183716 4.950536727905273
Loss :  1.7817678451538086 3.532374382019043 5.314142227172852
Loss :  1.8234875202178955 3.5853257179260254 5.4088134765625
Loss :  1.7851061820983887 3.270294427871704 5.055400848388672
Loss :  1.7729753255844116 3.481529712677002 5.254505157470703
Loss :  1.7733533382415771 3.249936103820801 5.023289680480957
Loss :  1.793008804321289 3.173875093460083 4.966883659362793
Loss :  1.799504041671753 3.3586676120758057 5.158171653747559
Loss :  1.7900562286376953 3.0821280479431152 4.8721842765808105
Loss :  1.7563800811767578 3.055762529373169 4.812142372131348
Loss :  1.7876228094100952 2.9116883277893066 4.699311256408691
Loss :  1.7858766317367554 3.086045026779175 4.871921539306641
  batch 40 loss: 1.7858766317367554, 3.086045026779175, 4.871921539306641
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7805274724960327 3.349271297454834 5.129798889160156
Loss :  1.7679357528686523 2.9512383937835693 4.719174385070801
Loss :  1.7815701961517334 3.185091257095337 4.96666145324707
Loss :  1.7830382585525513 3.583739995956421 5.366778373718262
Loss :  1.7837616205215454 2.8580069541931152 4.641768455505371
Loss :  1.7745970487594604 3.413465738296509 5.18806266784668
Loss :  1.773518681526184 3.066730260848999 4.840249061584473
Loss :  1.7681580781936646 2.9366252422332764 4.7047834396362305
Loss :  1.7591298818588257 3.2122840881347656 4.971414089202881
Loss :  1.7875086069107056 3.2311174869537354 5.0186262130737305
Loss :  1.7614730596542358 2.9096367359161377 4.671109676361084
Loss :  1.7615702152252197 2.8938803672790527 4.655450820922852
Loss :  1.7827680110931396 2.9870080947875977 4.769776344299316
Loss :  1.7828869819641113 2.6915478706359863 4.474434852600098
Loss :  1.7795103788375854 3.065335988998413 4.844846248626709
Loss :  1.7463338375091553 2.818798542022705 4.565132141113281
Loss :  1.7874342203140259 3.0252373218536377 4.812671661376953
Loss :  1.773034691810608 2.9329326152801514 4.705967426300049
Loss :  1.7847521305084229 3.2599005699157715 5.044652938842773
Loss :  1.766268253326416 2.6592891216278076 4.4255571365356445
  batch 60 loss: 1.766268253326416, 2.6592891216278076, 4.4255571365356445
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7752362489700317 2.976191759109497 4.751428127288818
Loss :  1.7750672101974487 2.835686683654785 4.610754013061523
Loss :  1.78138267993927 2.5380022525787354 4.319385051727295
Loss :  1.7642654180526733 3.148515224456787 4.91278076171875
Loss :  1.7560018301010132 2.9952173233032227 4.751219272613525
Loss :  1.3630684614181519 3.9762065410614014 5.339274883270264
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.4681391716003418 4.076383590698242 5.544522762298584
Loss :  1.4948152303695679 3.8370394706726074 5.331854820251465
Loss :  1.5300527811050415 3.6811254024505615 5.211178302764893
Total LOSS train 4.905112391251784 valid 5.356707692146301
CE LOSS train 1.775482150224539 valid 0.3825131952762604
Contrastive LOSS train 3.129630217185387 valid 0.9202813506126404
Saved best model. Old loss 5.409160017967224 and new best loss 5.356707692146301
EPOCH 22:
Loss :  1.7448210716247559 2.827755928039551 4.572576999664307
Loss :  1.764966607093811 2.9846441745758057 4.749610900878906
Loss :  1.7458182573318481 2.9821245670318604 4.727942943572998
Loss :  1.7528833150863647 2.7097396850585938 4.462623119354248
Loss :  1.7947442531585693 2.9000473022460938 4.694791793823242
Loss :  1.7633088827133179 3.0457892417907715 4.809098243713379
Loss :  1.7676643133163452 3.1305887699127197 4.898252964019775
Loss :  1.7440379858016968 3.22285532951355 4.966893196105957
Loss :  1.7785664796829224 3.1782453060150146 4.956811904907227
Loss :  1.7240817546844482 3.1122539043426514 4.8363356590271
Loss :  1.7667100429534912 3.1663172245025635 4.933027267456055
Loss :  1.8089135885238647 3.2332329750061035 5.042146682739258
Loss :  1.7715946435928345 3.3527421951293945 5.1243367195129395
Loss :  1.7785577774047852 3.1530869007110596 4.931644439697266
Loss :  1.7735990285873413 3.2475850582122803 5.021183967590332
Loss :  1.7551119327545166 3.090304136276245 4.845416069030762
Loss :  1.7791812419891357 2.960066795349121 4.739248275756836
Loss :  1.7599090337753296 3.1877832412719727 4.947692394256592
Loss :  1.7716445922851562 2.7657155990600586 4.537360191345215
Loss :  1.7680970430374146 3.2058756351470947 4.973972797393799
  batch 20 loss: 1.7680970430374146, 3.2058756351470947, 4.973972797393799
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7767959833145142 2.6120541095733643 4.388850212097168
Loss :  1.7852556705474854 2.8598809242248535 4.645136833190918
Loss :  1.7895770072937012 2.7242963314056396 4.513873100280762
Loss :  1.7920812368392944 2.857980251312256 4.65006160736084
Loss :  1.8085755109786987 3.183910846710205 4.992486476898193
Loss :  1.7820301055908203 3.1956913471221924 4.977721214294434
Loss :  1.7906566858291626 3.2488298416137695 5.039486408233643
Loss :  1.7633317708969116 3.038456678390503 4.801788330078125
Loss :  1.795864462852478 3.3275694847106934 5.123434066772461
Loss :  1.77178955078125 3.4346258640289307 5.206415176391602
Loss :  1.8192825317382812 3.3925578594207764 5.211840629577637
Loss :  1.7799351215362549 3.305156946182251 5.085092067718506
Loss :  1.7700977325439453 2.8717777729034424 4.641875267028809
Loss :  1.7673959732055664 2.996335506439209 4.763731479644775
Loss :  1.791624665260315 2.969625234603882 4.761250019073486
Loss :  1.8005541563034058 3.037506341934204 4.83806037902832
Loss :  1.789297103881836 2.9452598094940186 4.734557151794434
Loss :  1.7627488374710083 2.7217206954956055 4.484469413757324
Loss :  1.7841641902923584 3.164341688156128 4.948505878448486
Loss :  1.7800116539001465 3.501995801925659 5.282007217407227
  batch 40 loss: 1.7800116539001465, 3.501995801925659, 5.282007217407227
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7737390995025635 3.6921143531799316 5.465853691101074
Loss :  1.7611805200576782 3.5859479904174805 5.347128391265869
Loss :  1.776678204536438 3.391658306121826 5.168336391448975
Loss :  1.7733780145645142 3.268862247467041 5.042240142822266
Loss :  1.7779748439788818 3.2383944988250732 5.016369342803955
Loss :  1.7697362899780273 3.3091413974761963 5.0788774490356445
Loss :  1.771407127380371 3.500377893447876 5.271784782409668
Loss :  1.7652194499969482 2.932265043258667 4.697484493255615
Loss :  1.7652862071990967 2.863408088684082 4.628694534301758
Loss :  1.7884211540222168 2.684540033340454 4.47296142578125
Loss :  1.766319751739502 2.6835579872131348 4.449877738952637
Loss :  1.7655552625656128 2.7347617149353027 4.500317096710205
Loss :  1.7860041856765747 2.8779938220977783 4.663998126983643
Loss :  1.7848854064941406 2.627666711807251 4.4125518798828125
Loss :  1.7874236106872559 2.832113027572632 4.619536399841309
Loss :  1.7608261108398438 2.488999605178833 4.249825477600098
Loss :  1.8024054765701294 2.7220680713653564 4.524473667144775
Loss :  1.777477741241455 2.7324881553649902 4.509965896606445
Loss :  1.792007327079773 3.21921706199646 5.011224269866943
Loss :  1.7771008014678955 2.721445322036743 4.498546123504639
  batch 60 loss: 1.7771008014678955, 2.721445322036743, 4.498546123504639
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.780056118965149 2.8714051246643066 4.651461124420166
Loss :  1.7748289108276367 3.153088331222534 4.92791748046875
Loss :  1.7859833240509033 3.158068895339966 4.944052219390869
Loss :  1.767201542854309 3.1096739768981934 4.876875400543213
Loss :  1.7618165016174316 2.5697364807128906 4.331552982330322
Loss :  1.563819408416748 4.158848285675049 5.722667694091797
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.598048210144043 4.146800518035889 5.744848728179932
Loss :  1.6527410745620728 4.082982063293457 5.73572301864624
Loss :  1.6623263359069824 3.67494535446167 5.337271690368652
Total LOSS train 4.818823322883019 valid 5.635127782821655
CE LOSS train 1.7755106889284573 valid 0.4155815839767456
Contrastive LOSS train 3.04331263762254 valid 0.9187363386154175
EPOCH 23:
Loss :  1.7522239685058594 2.629960536956787 4.3821845054626465
Loss :  1.772297739982605 3.1415305137634277 4.913828372955322
Loss :  1.7488607168197632 2.87684965133667 4.625710487365723
Loss :  1.748824954032898 2.9694643020629883 4.718289375305176
Loss :  1.7912211418151855 3.2858927249908447 5.077114105224609
Loss :  1.7591265439987183 3.3020215034484863 5.061148166656494
Loss :  1.7648276090621948 3.1692111492156982 4.9340386390686035
Loss :  1.7418346405029297 3.327022075653076 5.068856716156006
Loss :  1.7803213596343994 2.8319103717803955 4.612231731414795
Loss :  1.7259597778320312 2.8489532470703125 4.574913024902344
Loss :  1.7673712968826294 3.1744682788848877 4.941839694976807
Loss :  1.806397795677185 3.101562738418579 4.907960414886475
Loss :  1.7693616151809692 3.4591751098632812 5.228536605834961
Loss :  1.7701008319854736 3.3657186031341553 5.135819435119629
Loss :  1.769778847694397 2.9063339233398438 4.676112651824951
Loss :  1.7478669881820679 3.0209460258483887 4.768813133239746
Loss :  1.7764205932617188 2.904597759246826 4.681018352508545
Loss :  1.757413625717163 3.302128553390503 5.059542179107666
Loss :  1.7687712907791138 2.4814133644104004 4.250184535980225
Loss :  1.7585986852645874 2.941753387451172 4.700352191925049
  batch 20 loss: 1.7585986852645874, 2.941753387451172, 4.700352191925049
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.770382285118103 2.995070695877075 4.765452861785889
Loss :  1.772255539894104 3.0069568157196045 4.779212474822998
Loss :  1.7759710550308228 2.9727118015289307 4.748682975769043
Loss :  1.7835471630096436 2.9652321338653564 4.748779296875
Loss :  1.8045454025268555 3.185161590576172 4.989706993103027
Loss :  1.7758123874664307 3.1663246154785156 4.942136764526367
Loss :  1.785840392112732 3.176483631134033 4.962324142456055
Loss :  1.7619651556015015 3.0417609214782715 4.8037261962890625
Loss :  1.8002805709838867 3.078005790710449 4.878286361694336
Loss :  1.7798491716384888 3.0298280715942383 4.8096771240234375
Loss :  1.823726773262024 3.108219861984253 4.931946754455566
Loss :  1.7833772897720337 3.243117094039917 5.02649450302124
Loss :  1.7697809934616089 3.1294877529144287 4.899268627166748
Loss :  1.7704813480377197 3.07717227935791 4.847653388977051
Loss :  1.7862104177474976 3.1707842350006104 4.956994533538818
Loss :  1.7927147150039673 3.205321788787842 4.9980363845825195
Loss :  1.7818477153778076 3.120249032974243 4.902096748352051
Loss :  1.753869891166687 2.895970582962036 4.649840354919434
Loss :  1.778507947921753 3.2582876682281494 5.036795616149902
Loss :  1.7824409008026123 2.8901214599609375 4.672562599182129
  batch 40 loss: 1.7824409008026123, 2.8901214599609375, 4.672562599182129
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.776383876800537 3.3435912132263184 5.1199750900268555
Loss :  1.7600116729736328 3.007094621658325 4.767106056213379
Loss :  1.7753239870071411 3.0500009059906006 4.825325012207031
Loss :  1.7732452154159546 3.067617893218994 4.840863227844238
Loss :  1.7815256118774414 2.877260446548462 4.658785820007324
Loss :  1.7776893377304077 3.103762626647949 4.8814520835876465
Loss :  1.7813892364501953 2.758763551712036 4.540152549743652
Loss :  1.7710068225860596 3.000640392303467 4.7716474533081055
Loss :  1.7727028131484985 2.8104541301727295 4.583157062530518
Loss :  1.7844754457473755 3.0338785648345947 4.81835412979126
Loss :  1.7634048461914062 2.8743655681610107 4.637770652770996
Loss :  1.7639517784118652 2.845472812652588 4.609424591064453
Loss :  1.7812414169311523 2.646817684173584 4.428059101104736
Loss :  1.7739588022232056 2.8336968421936035 4.6076555252075195
Loss :  1.7854360342025757 2.7508585453033447 4.536294460296631
Loss :  1.7601240873336792 2.837690830230713 4.597815036773682
Loss :  1.7939221858978271 2.9420270919799805 4.735949516296387
Loss :  1.7758285999298096 3.0584144592285156 4.834242820739746
Loss :  1.7869549989700317 2.9903616905212402 4.777316570281982
Loss :  1.7793197631835938 2.7104029655456543 4.489722728729248
  batch 60 loss: 1.7793197631835938, 2.7104029655456543, 4.489722728729248
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7739914655685425 2.8073620796203613 4.581353664398193
Loss :  1.7792853116989136 3.15053391456604 4.929819107055664
Loss :  1.7827801704406738 3.0115270614624023 4.794307231903076
Loss :  1.764223575592041 2.8182382583618164 4.582461833953857
Loss :  1.7554553747177124 2.6912105083465576 4.4466657638549805
Loss :  1.4881393909454346 4.265806674957275 5.753946304321289
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.5399701595306396 4.1854963302612305 5.725466728210449
Loss :  1.5782195329666138 4.033993721008301 5.612213134765625
Loss :  1.5963916778564453 3.9526281356811523 5.549019813537598
Total LOSS train 4.785597632481502 valid 5.66016149520874
CE LOSS train 1.7736095318427452 valid 0.39909791946411133
Contrastive LOSS train 3.011988096970778 valid 0.9881570339202881
EPOCH 24:
Loss :  1.7511780261993408 2.9282948970794678 4.679472923278809
Loss :  1.7649768590927124 3.4827003479003906 5.247677326202393
Loss :  1.7467825412750244 3.2092599868774414 4.956042289733887
Loss :  1.7510788440704346 3.1361570358276367 4.887235641479492
Loss :  1.7947889566421509 3.01173996925354 4.8065290451049805
Loss :  1.7646721601486206 3.1327836513519287 4.89745569229126
Loss :  1.765535593032837 3.3605122566223145 5.1260480880737305
Loss :  1.7445437908172607 3.357142210006714 5.101686000823975
Loss :  1.7785252332687378 3.153721809387207 4.932247161865234
Loss :  1.722372055053711 3.197404623031616 4.919776916503906
Loss :  1.7657939195632935 3.46638560295105 5.232179641723633
Loss :  1.8073499202728271 3.386780261993408 5.194129943847656
Loss :  1.767391562461853 3.3505783081054688 5.117969989776611
Loss :  1.7679831981658936 3.350872278213501 5.1188554763793945
Loss :  1.7623474597930908 3.236433982849121 4.998781204223633
Loss :  1.7429862022399902 3.102729320526123 4.845715522766113
Loss :  1.7709554433822632 3.2135658264160156 4.984521389007568
Loss :  1.7521055936813354 2.875340461730957 4.627446174621582
Loss :  1.754506230354309 2.4964230060577393 4.250929355621338
Loss :  1.747766375541687 3.0525832176208496 4.800349712371826
  batch 20 loss: 1.747766375541687, 3.0525832176208496, 4.800349712371826
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.755725383758545 2.806593179702759 4.562318801879883
Loss :  1.7612974643707275 3.102125406265259 4.863422870635986
Loss :  1.764980673789978 2.912505865097046 4.677486419677734
Loss :  1.771074891090393 3.1626811027526855 4.933755874633789
Loss :  1.7909764051437378 3.0910181999206543 4.881994724273682
Loss :  1.7548346519470215 3.037982940673828 4.79281759262085
Loss :  1.7711665630340576 3.4865355491638184 5.257701873779297
Loss :  1.7380914688110352 3.275237560272217 5.013329029083252
Loss :  1.7778030633926392 3.4524428844451904 5.230246067047119
Loss :  1.7489678859710693 3.371687412261963 5.120655059814453
Loss :  1.8146588802337646 3.5397307872772217 5.354389667510986
Loss :  1.7569657564163208 3.6592869758605957 5.416252613067627
Loss :  1.74517023563385 3.198054075241089 4.9432244300842285
Loss :  1.752961277961731 3.3188908100128174 5.071852207183838
Loss :  1.774749994277954 2.8305513858795166 4.605301380157471
Loss :  1.7826156616210938 3.1177515983581543 4.900367259979248
Loss :  1.7726821899414062 2.6612746715545654 4.433957099914551
Loss :  1.7291396856307983 2.582937479019165 4.312077045440674
Loss :  1.7603816986083984 2.66096830368042 4.421350002288818
Loss :  1.7691961526870728 2.5647873878479004 4.333983421325684
  batch 40 loss: 1.7691961526870728, 2.5647873878479004, 4.333983421325684
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7571905851364136 2.65169620513916 4.408886909484863
Loss :  1.7452229261398315 2.622542381286621 4.367765426635742
Loss :  1.7699635028839111 2.783190965652466 4.553154468536377
Loss :  1.7629456520080566 2.6545653343200684 4.417510986328125
Loss :  1.7696961164474487 2.548301935195923 4.317997932434082
Loss :  1.7592979669570923 3.351271867752075 5.110569953918457
Loss :  1.7556254863739014 3.1099255084991455 4.865550994873047
Loss :  1.749548316001892 3.211127519607544 4.9606757164001465
Loss :  1.746620535850525 3.506965398788452 5.2535858154296875
Loss :  1.7667746543884277 3.3690056800842285 5.135780334472656
Loss :  1.7459129095077515 3.4433321952819824 5.189245223999023
Loss :  1.7507203817367554 3.339176893234253 5.089897155761719
Loss :  1.7762517929077148 2.980588674545288 4.756840705871582
Loss :  1.7685898542404175 3.053588628768921 4.822178363800049
Loss :  1.7710813283920288 3.130260467529297 4.901341915130615
Loss :  1.7512328624725342 2.926173448562622 4.677406311035156
Loss :  1.7916326522827148 3.357799530029297 5.149432182312012
Loss :  1.7695392370224 3.1307449340820312 4.900284290313721
Loss :  1.7871553897857666 3.359354257583618 5.146509647369385
Loss :  1.772973895072937 2.7737090587615967 4.546682834625244
  batch 60 loss: 1.772973895072937, 2.7737090587615967, 4.546682834625244
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.77947998046875 3.058985948562622 4.838465690612793
Loss :  1.7785333395004272 2.948035717010498 4.726569175720215
Loss :  1.7860615253448486 3.036475419998169 4.822536945343018
Loss :  1.767154335975647 3.111814260482788 4.878968715667725
Loss :  1.7546286582946777 2.483901023864746 4.238529682159424
Loss :  1.7024775743484497 3.6415247917175293 5.3440022468566895
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7075947523117065 3.6892573833465576 5.396852016448975
Loss :  1.7324918508529663 3.4601995944976807 5.192691326141357
Loss :  1.744510531425476 3.3951187133789062 5.139629364013672
Total LOSS train 4.8604292356050935 valid 5.268293738365173
CE LOSS train 1.7638602128395668 valid 0.436127632856369
Contrastive LOSS train 3.0965690135955812 valid 0.8487796783447266
Saved best model. Old loss 5.356707692146301 and new best loss 5.268293738365173
EPOCH 25:
Loss :  1.7557216882705688 3.226952314376831 4.9826741218566895
Loss :  1.7757790088653564 3.153625726699829 4.9294047355651855
Loss :  1.749337077140808 3.3441686630249023 5.093505859375
Loss :  1.753056287765503 3.3496503829956055 5.1027069091796875
Loss :  1.7955819368362427 3.474842071533203 5.270423889160156
Loss :  1.7764981985092163 3.2987518310546875 5.075250148773193
Loss :  1.7657079696655273 3.0556020736694336 4.821310043334961
Loss :  1.7533645629882812 3.521381139755249 5.274745941162109
Loss :  1.7864201068878174 3.479517936706543 5.265937805175781
Loss :  1.7337628602981567 3.0548057556152344 4.788568496704102
Loss :  1.7747142314910889 3.387505531311035 5.162220001220703
Loss :  1.8173577785491943 3.585583209991455 5.40294075012207
Loss :  1.7689779996871948 3.0311570167541504 4.800135135650635
Loss :  1.7741775512695312 2.8049378395080566 4.579115390777588
Loss :  1.7723543643951416 2.844940423965454 4.617294788360596
Loss :  1.7471295595169067 3.2318758964538574 4.979005336761475
Loss :  1.7732971906661987 2.8191304206848145 4.592427730560303
Loss :  1.7563608884811401 2.8168983459472656 4.573259353637695
Loss :  1.7607883214950562 2.6214890480041504 4.382277488708496
Loss :  1.7524663209915161 2.815725803375244 4.568192005157471
  batch 20 loss: 1.7524663209915161, 2.815725803375244, 4.568192005157471
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.749763011932373 3.0074050426483154 4.757167816162109
Loss :  1.7639355659484863 2.943950653076172 4.707886219024658
Loss :  1.7605538368225098 3.5165445804595947 5.277098655700684
Loss :  1.7721445560455322 3.3039934635162354 5.076138019561768
Loss :  1.7893749475479126 3.2369658946990967 5.026340961456299
Loss :  1.755925178527832 3.4847283363342285 5.2406535148620605
Loss :  1.7782505750656128 3.704439401626587 5.48268985748291
Loss :  1.749610424041748 3.374124765396118 5.123735427856445
Loss :  1.7844396829605103 3.2015419006347656 4.985981464385986
Loss :  1.7564562559127808 3.1344590187072754 4.890915393829346
Loss :  1.8121310472488403 3.073322296142578 4.885453224182129
Loss :  1.7641510963439941 3.020840644836426 4.78499174118042
Loss :  1.7493505477905273 3.0868475437164307 4.836197853088379
Loss :  1.754769206047058 2.81229305267334 4.5670623779296875
Loss :  1.7733941078186035 2.971111297607422 4.744505405426025
Loss :  1.7864261865615845 3.0774195194244385 4.8638458251953125
Loss :  1.7719677686691284 3.0518455505371094 4.823813438415527
Loss :  1.7445948123931885 2.9317336082458496 4.676328659057617
Loss :  1.7609918117523193 3.1833837032318115 4.944375514984131
Loss :  1.7712947130203247 3.0131678581237793 4.7844624519348145
  batch 40 loss: 1.7712947130203247, 3.0131678581237793, 4.7844624519348145
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7598155736923218 2.7573323249816895 4.517148017883301
Loss :  1.7498468160629272 3.086083173751831 4.835929870605469
Loss :  1.760858178138733 2.9922919273376465 4.75314998626709
Loss :  1.7750006914138794 2.697873115539551 4.472873687744141
Loss :  1.766084909439087 2.490529775619507 4.256614685058594
Loss :  1.7646452188491821 2.9766931533813477 4.74133825302124
Loss :  1.7651959657669067 2.5514724254608154 4.316668510437012
Loss :  1.7512363195419312 2.960817575454712 4.7120537757873535
Loss :  1.753281593322754 2.988535165786743 4.741816520690918
Loss :  1.7694791555404663 3.1190245151519775 4.888503551483154
Loss :  1.7445595264434814 3.5170392990112305 5.261598587036133
Loss :  1.7470036745071411 2.944992780685425 4.6919965744018555
Loss :  1.7704225778579712 3.0385324954986572 4.808955192565918
Loss :  1.7655742168426514 3.316279649734497 5.081853866577148
Loss :  1.7738515138626099 2.935805320739746 4.709656715393066
Loss :  1.7418363094329834 3.092139720916748 4.833975791931152
Loss :  1.7858059406280518 3.2211415767669678 5.0069475173950195
Loss :  1.7646147012710571 2.728717088699341 4.4933319091796875
Loss :  1.775925636291504 3.345813035964966 5.121738433837891
Loss :  1.7608942985534668 3.1275508403778076 4.888444900512695
  batch 60 loss: 1.7608942985534668, 3.1275508403778076, 4.888444900512695
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7683593034744263 3.159499168395996 4.927858352661133
Loss :  1.7672251462936401 3.47827410697937 5.245499134063721
Loss :  1.7757965326309204 3.1397664546966553 4.915563106536865
Loss :  1.7565186023712158 3.067294120788574 4.823812484741211
Loss :  1.7481006383895874 2.587043285369873 4.33514404296875
Loss :  1.6315895318984985 4.207910060882568 5.839499473571777
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6433345079421997 4.2211713790893555 5.864505767822266
Loss :  1.7142146825790405 4.069990634918213 5.784205436706543
Loss :  1.7098205089569092 3.971787929534912 5.681608200073242
Total LOSS train 4.863438664949857 valid 5.792454719543457
CE LOSS train 1.765450958105234 valid 0.4274551272392273
Contrastive LOSS train 3.0979877178485578 valid 0.992946982383728
EPOCH 26:
Loss :  1.7434161901474 3.3092293739318848 5.052645683288574
Loss :  1.7701209783554077 3.349074125289917 5.119194984436035
Loss :  1.7465202808380127 2.7178688049316406 4.464388847351074
Loss :  1.7469539642333984 3.189049005508423 4.936002731323242
Loss :  1.7896250486373901 2.7580645084381104 4.547689437866211
Loss :  1.771540641784668 2.825099468231201 4.596640110015869
Loss :  1.768052339553833 2.7563674449920654 4.524419784545898
Loss :  1.7537466287612915 2.875054121017456 4.628800868988037
Loss :  1.7765874862670898 2.8596291542053223 4.636216640472412
Loss :  1.728960394859314 2.7074809074401855 4.436441421508789
Loss :  1.7706657648086548 3.1231281757354736 4.893794059753418
Loss :  1.8099414110183716 2.8904900550842285 4.7004313468933105
Loss :  1.7657887935638428 3.124183416366577 4.88997220993042
Loss :  1.7696572542190552 3.0032310485839844 4.77288818359375
Loss :  1.7681907415390015 2.8204526901245117 4.588643550872803
Loss :  1.7512015104293823 2.766786813735962 4.517988204956055
Loss :  1.7676125764846802 2.9525091648101807 4.72012186050415
Loss :  1.7550078630447388 3.0440664291381836 4.799074172973633
Loss :  1.7604167461395264 3.323108196258545 5.083524703979492
Loss :  1.745073914527893 3.3570570945739746 5.102130889892578
  batch 20 loss: 1.745073914527893, 3.3570570945739746, 5.102130889892578
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7483739852905273 3.3009824752807617 5.049356460571289
Loss :  1.759153962135315 2.737623453140259 4.496777534484863
Loss :  1.756207823753357 2.8406691551208496 4.596877098083496
Loss :  1.7693971395492554 2.9811713695526123 4.750568389892578
Loss :  1.788158655166626 2.931765079498291 4.719923973083496
Loss :  1.7545973062515259 3.0930635929107666 4.847661018371582
Loss :  1.7715108394622803 2.9213385581970215 4.692849159240723
Loss :  1.739085078239441 2.7357187271118164 4.474803924560547
Loss :  1.77584969997406 2.9189045429229736 4.694754123687744
Loss :  1.759524941444397 2.9087815284729004 4.668306350708008
Loss :  1.8094085454940796 3.139786958694458 4.949195384979248
Loss :  1.760684609413147 2.69341778755188 4.454102516174316
Loss :  1.7498162984848022 3.011354684829712 4.761170864105225
Loss :  1.7536064386367798 2.989039421081543 4.742645740509033
Loss :  1.7732750177383423 3.159975290298462 4.933250427246094
Loss :  1.7804622650146484 3.2363436222076416 5.016805648803711
Loss :  1.7706047296524048 3.2170937061309814 4.987698554992676
Loss :  1.7339696884155273 3.301013708114624 5.0349836349487305
Loss :  1.7615216970443726 3.265014886856079 5.026536464691162
Loss :  1.7726263999938965 3.2409067153930664 5.013533115386963
  batch 40 loss: 1.7726263999938965, 3.2409067153930664, 5.013533115386963
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.761555790901184 3.583632469177246 5.345188140869141
Loss :  1.7489911317825317 3.060045003890991 4.8090362548828125
Loss :  1.7729049921035767 3.1789276599884033 4.9518327713012695
Loss :  1.7676090002059937 3.3300881385803223 5.0976972579956055
Loss :  1.7700031995773315 2.8857548236846924 4.655757904052734
Loss :  1.7633496522903442 3.1964709758758545 4.959820747375488
Loss :  1.7680377960205078 3.0273404121398926 4.7953782081604
Loss :  1.7545589208602905 2.9808332920074463 4.735392093658447
Loss :  1.7540886402130127 2.929569959640503 4.683658599853516
Loss :  1.7752777338027954 3.059842586517334 4.83512020111084
Loss :  1.7485356330871582 3.049881935119629 4.798417568206787
Loss :  1.7537591457366943 3.1644978523254395 4.918256759643555
Loss :  1.7724612951278687 2.944965362548828 4.717426776885986
Loss :  1.763846755027771 2.868788957595825 4.632635593414307
Loss :  1.770168662071228 3.3055951595306396 5.075763702392578
Loss :  1.7413256168365479 3.192009449005127 4.933335304260254
Loss :  1.782690167427063 3.300246000289917 5.0829362869262695
Loss :  1.7632189989089966 3.4037604331970215 5.1669793128967285
Loss :  1.77748703956604 3.3417553901672363 5.1192426681518555
Loss :  1.7644895315170288 2.6291186809539795 4.393608093261719
  batch 60 loss: 1.7644895315170288, 2.6291186809539795, 4.393608093261719
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.765203833580017 2.4748375415802 4.240041255950928
Loss :  1.7728228569030762 2.334592342376709 4.107415199279785
Loss :  1.7734758853912354 2.5257046222686768 4.299180507659912
Loss :  1.7571978569030762 2.5713014602661133 4.3284993171691895
Loss :  1.7486090660095215 2.0107758045196533 3.759384870529175
Loss :  1.625653862953186 4.182882785797119 5.808536529541016
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.644798755645752 4.163637161254883 5.808435916900635
Loss :  1.7141417264938354 4.161799907684326 5.875941753387451
Loss :  1.7052123546600342 3.953551769256592 5.658763885498047
Total LOSS train 4.759458699593178 valid 5.787919521331787
CE LOSS train 1.763670536187979 valid 0.42630308866500854
Contrastive LOSS train 2.995788178077111 valid 0.988387942314148
EPOCH 27:
Loss :  1.7422784566879272 2.452920436859131 4.195199012756348
Loss :  1.7583199739456177 2.9729111194610596 4.731231212615967
Loss :  1.738099455833435 3.1374499797821045 4.87554931640625
Loss :  1.746050238609314 2.790078639984131 4.536128997802734
Loss :  1.783272624015808 3.205822706222534 4.989095211029053
Loss :  1.7589036226272583 2.9443750381469727 4.703278541564941
Loss :  1.7648417949676514 3.2753560543060303 5.040197849273682
Loss :  1.7497190237045288 3.4298148155212402 5.179533958435059
Loss :  1.7763608694076538 3.5655786991119385 5.341939449310303
Loss :  1.7230510711669922 3.2517409324645996 4.974792003631592
Loss :  1.7716896533966064 3.357372999191284 5.129062652587891
Loss :  1.8094149827957153 3.05430006980896 4.863715171813965
Loss :  1.769740104675293 3.334523916244507 5.104264259338379
Loss :  1.767185926437378 3.333479404449463 5.100665092468262
Loss :  1.7627301216125488 3.3855020999908447 5.148232460021973
Loss :  1.7418442964553833 3.241666555404663 4.983510971069336
Loss :  1.771755576133728 3.0192337036132812 4.790989398956299
Loss :  1.7546159029006958 3.2711260318756104 5.025742053985596
Loss :  1.755267858505249 2.9823484420776367 4.737616539001465
Loss :  1.7543658018112183 3.0332086086273193 4.787574291229248
  batch 20 loss: 1.7543658018112183, 3.0332086086273193, 4.787574291229248
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7541675567626953 3.0474562644958496 4.801623821258545
Loss :  1.7635829448699951 3.5208194255828857 5.284402370452881
Loss :  1.759305715560913 3.2683920860290527 5.027697563171387
Loss :  1.7688493728637695 3.356947422027588 5.125796794891357
Loss :  1.784393072128296 3.266601800918579 5.050994873046875
Loss :  1.7558504343032837 3.1078004837036133 4.863650798797607
Loss :  1.7728567123413086 3.004229784011841 4.77708625793457
Loss :  1.7395118474960327 3.1792643070220947 4.918776035308838
Loss :  1.7765506505966187 2.6945929527282715 4.47114372253418
Loss :  1.7534626722335815 2.76206111907959 4.515523910522461
Loss :  1.8091914653778076 2.6703686714172363 4.479559898376465
Loss :  1.7564033269882202 2.7277703285217285 4.484173774719238
Loss :  1.7452117204666138 2.5826828479766846 4.327894687652588
Loss :  1.7503238916397095 2.8784329891204834 4.628756999969482
Loss :  1.7660775184631348 2.874130964279175 4.6402082443237305
Loss :  1.7766544818878174 3.0495591163635254 4.826213836669922
Loss :  1.7665587663650513 2.931363105773926 4.6979217529296875
Loss :  1.7257007360458374 2.6537392139434814 4.379439830780029
Loss :  1.7558001279830933 3.311215877532959 5.067016124725342
Loss :  1.7643017768859863 3.402575969696045 5.166877746582031
  batch 40 loss: 1.7643017768859863, 3.402575969696045, 5.166877746582031
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7505006790161133 3.136991262435913 4.8874921798706055
Loss :  1.7406522035598755 3.2664027214050293 5.007054805755615
Loss :  1.7557202577590942 3.1064648628234863 4.862185001373291
Loss :  1.7624421119689941 3.2682013511657715 5.030643463134766
Loss :  1.7593756914138794 3.289644956588745 5.049020767211914
Loss :  1.7530192136764526 3.3478150367736816 5.100834369659424
Loss :  1.7512197494506836 3.4251725673675537 5.176392555236816
Loss :  1.7451062202453613 3.3141939640045166 5.059300422668457
Loss :  1.7440497875213623 3.6442861557006836 5.388336181640625
Loss :  1.7709451913833618 3.2163262367248535 4.987271308898926
Loss :  1.7444220781326294 3.0286600589752197 4.773082256317139
Loss :  1.7481250762939453 3.263413667678833 5.011538505554199
Loss :  1.7770036458969116 2.666827440261841 4.443830966949463
Loss :  1.773721694946289 2.8136634826660156 4.587385177612305
Loss :  1.766070008277893 2.703866720199585 4.469936847686768
Loss :  1.7458699941635132 2.624605655670166 4.370475769042969
Loss :  1.783321499824524 2.8917229175567627 4.675044536590576
Loss :  1.7631797790527344 3.0172152519226074 4.780395030975342
Loss :  1.7752467393875122 3.158940076828003 4.934186935424805
Loss :  1.7655407190322876 3.131667137145996 4.897207736968994
  batch 60 loss: 1.7655407190322876, 3.131667137145996, 4.897207736968994
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7684364318847656 3.169762372970581 4.938199043273926
Loss :  1.7665178775787354 2.9501731395721436 4.716691017150879
Loss :  1.778132438659668 3.079608678817749 4.857741355895996
Loss :  1.7526317834854126 2.9134418964385986 4.666073799133301
Loss :  1.7449424266815186 2.6778054237365723 4.422747611999512
Loss :  1.5766533613204956 4.211236000061035 5.78788948059082
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.5957858562469482 4.270840167999268 5.866625785827637
Loss :  1.6779485940933228 4.117812156677246 5.795760631561279
Loss :  1.7021175622940063 4.059635639190674 5.761753082275391
Total LOSS train 4.844094540522649 valid 5.803007245063782
CE LOSS train 1.7604685453268198 valid 0.4255293905735016
Contrastive LOSS train 3.083625969519982 valid 1.0149089097976685
EPOCH 28:
Loss :  1.7408825159072876 3.0476925373077393 4.788575172424316
Loss :  1.7639609575271606 2.8080341815948486 4.571995258331299
Loss :  1.7396868467330933 2.8643486499786377 4.604035377502441
Loss :  1.743449091911316 3.4671876430511475 5.210636615753174
Loss :  1.78145170211792 3.2823379039764404 5.063789367675781
Loss :  1.760291576385498 3.598968982696533 5.359260559082031
Loss :  1.7605922222137451 3.2490487098693848 5.009640693664551
Loss :  1.7426279783248901 3.2572762966156006 4.999904155731201
Loss :  1.7728034257888794 3.4894824028015137 5.2622857093811035
Loss :  1.718559741973877 3.263777732849121 4.982337474822998
Loss :  1.7649544477462769 3.3205337524414062 5.085488319396973
Loss :  1.8026255369186401 3.1491658687591553 4.951791286468506
Loss :  1.7619260549545288 3.3970236778259277 5.158949851989746
Loss :  1.7604345083236694 3.0571084022521973 4.817543029785156
Loss :  1.7554963827133179 2.71307373046875 4.468570232391357
Loss :  1.7391211986541748 2.7359397411346436 4.475060939788818
Loss :  1.7633754014968872 3.139845609664917 4.903221130371094
Loss :  1.7512010335922241 2.9722747802734375 4.723475933074951
Loss :  1.752411961555481 2.675865411758423 4.428277492523193
Loss :  1.7465252876281738 2.3813555240631104 4.127881050109863
  batch 20 loss: 1.7465252876281738, 2.3813555240631104, 4.127881050109863
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7486672401428223 2.3434231281280518 4.092090606689453
Loss :  1.760784387588501 2.4169652462005615 4.1777496337890625
Loss :  1.762455701828003 2.474120855331421 4.236576557159424
Loss :  1.7716518640518188 2.570634365081787 4.342286109924316
Loss :  1.7874995470046997 2.6711370944976807 4.45863676071167
Loss :  1.7610489130020142 2.491450309753418 4.252499103546143
Loss :  1.7784262895584106 2.9098317623138428 4.688258171081543
Loss :  1.7459368705749512 2.7604408264160156 4.506377696990967
Loss :  1.7813743352890015 2.79138445854187 4.572758674621582
Loss :  1.7536637783050537 3.1890134811401367 4.9426774978637695
Loss :  1.8099786043167114 3.3406643867492676 5.1506428718566895
Loss :  1.7645190954208374 3.2425482273101807 5.0070672035217285
Loss :  1.7494152784347534 3.1798694133758545 4.929284572601318
Loss :  1.748229742050171 2.7989423274993896 4.5471720695495605
Loss :  1.771032452583313 2.997431993484497 4.7684645652771
Loss :  1.7766844034194946 3.09171462059021 4.868399143218994
Loss :  1.7696295976638794 3.0443825721740723 4.814012050628662
Loss :  1.7344063520431519 2.764585256576538 4.4989914894104
Loss :  1.7561590671539307 3.342910051345825 5.099069118499756
Loss :  1.7676410675048828 3.0983974933624268 4.8660383224487305
  batch 40 loss: 1.7676410675048828, 3.0983974933624268, 4.8660383224487305
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7552480697631836 3.2632088661193848 5.018456935882568
Loss :  1.7416490316390991 3.3290927410125732 5.070741653442383
Loss :  1.7626440525054932 3.124140977859497 4.88678503036499
Loss :  1.7642784118652344 3.002052068710327 4.766330718994141
Loss :  1.7638914585113525 2.7419092655181885 4.505800724029541
Loss :  1.7600663900375366 2.8490207195281982 4.609086990356445
Loss :  1.7568506002426147 2.8138203620910645 4.570671081542969
Loss :  1.75041663646698 3.125586748123169 4.876003265380859
Loss :  1.7518240213394165 2.971065044403076 4.722888946533203
Loss :  1.7701610326766968 2.909514904022217 4.679676055908203
Loss :  1.7496923208236694 2.9933412075042725 4.743033409118652
Loss :  1.7534174919128418 2.6866488456726074 4.440066337585449
Loss :  1.7770699262619019 2.670542001724243 4.4476118087768555
Loss :  1.7700599431991577 2.8980517387390137 4.668111801147461
Loss :  1.764238953590393 2.9394609928131104 4.703700065612793
Loss :  1.7567421197891235 2.6550660133361816 4.411808013916016
Loss :  1.7885406017303467 2.9365029335021973 4.725043296813965
Loss :  1.767669439315796 2.988717794418335 4.756387233734131
Loss :  1.7812954187393188 3.3979361057281494 5.179231643676758
Loss :  1.773158311843872 3.409404754638672 5.182562828063965
  batch 60 loss: 1.773158311843872, 3.409404754638672, 5.182562828063965
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7772308588027954 2.976346015930176 4.753576755523682
Loss :  1.7770768404006958 2.872663736343384 4.649740695953369
Loss :  1.7891017198562622 3.166595935821533 4.955697536468506
Loss :  1.7606793642044067 3.187160015106201 4.947839260101318
Loss :  1.7516403198242188 2.3348605632781982 4.086501121520996
Loss :  1.5977661609649658 4.168795108795166 5.766561508178711
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.611549735069275 4.142099857330322 5.753649711608887
Loss :  1.7002513408660889 4.07750129699707 5.777752876281738
Loss :  1.7499654293060303 3.737786054611206 5.487751483917236
Total LOSS train 4.741063462770902 valid 5.696428894996643
CE LOSS train 1.7620957814730132 valid 0.43749135732650757
Contrastive LOSS train 2.9789676886338454 valid 0.9344465136528015
EPOCH 29:
Loss :  1.755075216293335 3.1952192783355713 4.950294494628906
Loss :  1.7678947448730469 3.4176130294799805 5.185507774353027
Loss :  1.7496908903121948 3.3645806312561035 5.114271640777588
Loss :  1.7540332078933716 3.2065892219543457 4.960622310638428
Loss :  1.7912380695343018 3.260315179824829 5.051553249359131
Loss :  1.7704752683639526 3.041015148162842 4.811490535736084
Loss :  1.7671329975128174 3.323157548904419 5.090290546417236
Loss :  1.75094473361969 2.862032651901245 4.612977504730225
Loss :  1.7788888216018677 3.174527406692505 4.953416347503662
Loss :  1.7238739728927612 2.9662816524505615 4.690155506134033
Loss :  1.7658119201660156 3.0055181980133057 4.771329879760742
Loss :  1.8070141077041626 2.984609842300415 4.791624069213867
Loss :  1.7597202062606812 2.9178426265716553 4.677562713623047
Loss :  1.762253999710083 2.8510043621063232 4.613258361816406
Loss :  1.7645149230957031 2.6125075817108154 4.377022743225098
Loss :  1.7413073778152466 2.5429437160491943 4.2842512130737305
Loss :  1.7658932209014893 3.0245118141174316 4.7904052734375
Loss :  1.7479130029678345 2.868006467819214 4.615919589996338
Loss :  1.754408359527588 3.065337896347046 4.819746017456055
Loss :  1.743861436843872 3.219276189804077 4.963137626647949
  batch 20 loss: 1.743861436843872, 3.219276189804077, 4.963137626647949
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7496424913406372 3.214130163192749 4.963772773742676
Loss :  1.7601152658462524 3.1245319843292236 4.884647369384766
Loss :  1.758506417274475 3.0994153022766113 4.857921600341797
Loss :  1.771297812461853 3.1113226413726807 4.882620334625244
Loss :  1.7896775007247925 3.264857292175293 5.054534912109375
Loss :  1.755863070487976 3.2992920875549316 5.055155277252197
Loss :  1.7771921157836914 3.512697458267212 5.289889335632324
Loss :  1.746658205986023 3.3618950843811035 5.108553409576416
Loss :  1.779081106185913 2.874706745147705 4.653787612915039
Loss :  1.7518221139907837 3.070618152618408 4.822440147399902
Loss :  1.8088891506195068 3.1740853786468506 4.982974529266357
Loss :  1.7665417194366455 2.9861180782318115 4.752659797668457
Loss :  1.7440390586853027 2.7048628330230713 4.448902130126953
Loss :  1.748974084854126 2.6083953380584717 4.357369422912598
Loss :  1.7681329250335693 3.192345142364502 4.960477828979492
Loss :  1.7750176191329956 2.950016498565674 4.725034236907959
Loss :  1.7683179378509521 3.012390375137329 4.780708312988281
Loss :  1.729388952255249 2.7080790996551514 4.4374680519104
Loss :  1.7519408464431763 3.141594409942627 4.893535137176514
Loss :  1.7676935195922852 3.0384907722473145 4.8061842918396
  batch 40 loss: 1.7676935195922852, 3.0384907722473145, 4.8061842918396
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7549004554748535 3.4365410804748535 5.191441535949707
Loss :  1.7376224994659424 3.3211281299591064 5.058750629425049
Loss :  1.762721300125122 2.9496524333953857 4.712373733520508
Loss :  1.755843997001648 2.898800849914551 4.654644966125488
Loss :  1.7660573720932007 2.7266509532928467 4.492708206176758
Loss :  1.7584236860275269 2.862436056137085 4.620859622955322
Loss :  1.7542556524276733 2.616671085357666 4.370926856994629
Loss :  1.7523314952850342 2.725534200668335 4.477865695953369
Loss :  1.7467153072357178 3.3284497261047363 5.075164794921875
Loss :  1.769112229347229 3.3157193660736084 5.084831714630127
Loss :  1.7514808177947998 3.44519305229187 5.19667387008667
Loss :  1.7568237781524658 3.308675527572632 5.065499305725098
Loss :  1.773889422416687 3.33675479888916 5.110644340515137
Loss :  1.764639139175415 3.225539207458496 4.990178108215332
Loss :  1.7712072134017944 3.1116678714752197 4.882874965667725
Loss :  1.7637889385223389 2.8424603939056396 4.6062493324279785
Loss :  1.7933635711669922 2.792008876800537 4.585372447967529
Loss :  1.7709327936172485 2.626132011413574 4.397064685821533
Loss :  1.7834532260894775 2.9175801277160645 4.701033592224121
Loss :  1.7829663753509521 2.637079954147339 4.420046329498291
  batch 60 loss: 1.7829663753509521, 2.637079954147339, 4.420046329498291
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7790004014968872 2.8178274631500244 4.596827983856201
Loss :  1.7784744501113892 2.5937154293060303 4.372189998626709
Loss :  1.7906919717788696 3.0352284908294678 4.825920581817627
Loss :  1.7631990909576416 3.0446479320526123 4.807847023010254
Loss :  1.7542791366577148 2.8720405101776123 4.626319885253906
Loss :  1.6327351331710815 4.23282527923584 5.865560531616211
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6406176090240479 4.1566948890686035 5.7973127365112305
Loss :  1.7354555130004883 3.99526309967041 5.730718612670898
Loss :  1.763717770576477 3.8115601539611816 5.575277805328369
Total LOSS train 4.796427447979267 valid 5.742217421531677
CE LOSS train 1.7634909648161667 valid 0.44092944264411926
Contrastive LOSS train 3.032936473993155 valid 0.9528900384902954
EPOCH 30:
Loss :  1.7569477558135986 2.9671523571014404 4.724100112915039
Loss :  1.7722532749176025 2.8574981689453125 4.629751205444336
Loss :  1.7499696016311646 2.9165189266204834 4.6664886474609375
Loss :  1.7558355331420898 3.395536184310913 5.151371955871582
Loss :  1.7929409742355347 2.9469692707061768 4.739910125732422
Loss :  1.770455002784729 3.2628934383392334 5.033348560333252
Loss :  1.7737959623336792 3.126093626022339 4.8998894691467285
Loss :  1.7569998502731323 3.0126593112945557 4.769659042358398
Loss :  1.7818686962127686 3.083244800567627 4.865113258361816
Loss :  1.7293373346328735 3.178431987762451 4.907769203186035
Loss :  1.769128441810608 3.483293294906616 5.252421855926514
Loss :  1.8019074201583862 3.291107654571533 5.093015193939209
Loss :  1.7635999917984009 3.480114459991455 5.243714332580566
Loss :  1.7622616291046143 3.3277504444122314 5.090012073516846
Loss :  1.7571771144866943 3.4295239448547363 5.186700820922852
Loss :  1.7421904802322388 3.4074411392211914 5.149631500244141
Loss :  1.7622625827789307 2.994600772857666 4.756863594055176
Loss :  1.7517569065093994 3.561328887939453 5.313085556030273
Loss :  1.7528913021087646 3.792842388153076 5.545733451843262
Loss :  1.750244140625 3.581371307373047 5.331615447998047
  batch 20 loss: 1.750244140625, 3.581371307373047, 5.331615447998047
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7545994520187378 3.845503091812134 5.600102424621582
Loss :  1.7650823593139648 3.382030487060547 5.147112846374512
Loss :  1.7685118913650513 4.1324334144592285 5.90094518661499
Loss :  1.7812055349349976 3.4195399284362793 5.200745582580566
Loss :  1.7884666919708252 3.462803602218628 5.251270294189453
Loss :  1.7617424726486206 3.7126829624176025 5.474425315856934
Loss :  1.7871794700622559 3.4485044479370117 5.235683917999268
Loss :  1.763655185699463 3.3587987422943115 5.122453689575195
Loss :  1.785662293434143 3.260040521621704 5.045702934265137
Loss :  1.7530739307403564 3.672551393508911 5.425625324249268
Loss :  1.8093563318252563 3.6374621391296387 5.4468183517456055
Loss :  1.7676811218261719 3.7145373821258545 5.4822187423706055
Loss :  1.7458022832870483 3.482816219329834 5.228618621826172
Loss :  1.7514519691467285 3.550039529800415 5.301491737365723
Loss :  1.7701112031936646 3.2758467197418213 5.045958042144775
Loss :  1.7790769338607788 3.6733016967773438 5.452378749847412
Loss :  1.769633173942566 3.2649900913238525 5.034623146057129
Loss :  1.7396596670150757 2.576538562774658 4.316198348999023
Loss :  1.7506011724472046 2.8294575214385986 4.580058574676514
Loss :  1.7718087434768677 3.176581382751465 4.948390007019043
  batch 40 loss: 1.7718087434768677, 3.176581382751465, 4.948390007019043
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.758942723274231 3.5838351249694824 5.342777729034424
Loss :  1.7455390691757202 2.824953556060791 4.570492744445801
Loss :  1.7575104236602783 3.1613364219665527 4.91884708404541
Loss :  1.7718980312347412 2.9105608463287354 4.682458877563477
Loss :  1.7625776529312134 2.9089102745056152 4.671487808227539
Loss :  1.7605693340301514 2.522763729095459 4.283332824707031
Loss :  1.7598986625671387 2.569192409515381 4.3290910720825195
Loss :  1.7475389242172241 2.5485944747924805 4.296133518218994
Loss :  1.7555595636367798 2.9159867763519287 4.671546459197998
Loss :  1.766398549079895 2.738380193710327 4.504778861999512
Loss :  1.7452607154846191 3.050248622894287 4.795509338378906
Loss :  1.7493857145309448 3.0507705211639404 4.800156116485596
Loss :  1.778111457824707 3.352186441421509 5.130297660827637
Loss :  1.7720471620559692 3.1665971279144287 4.9386444091796875
Loss :  1.761169195175171 3.076542854309082 4.837712287902832
Loss :  1.7599765062332153 2.960676908493042 4.720653533935547
Loss :  1.7833855152130127 2.9004547595977783 4.683840274810791
Loss :  1.7672569751739502 3.3027055263519287 5.069962501525879
Loss :  1.7746968269348145 3.1906983852386475 4.965394973754883
Loss :  1.7694860696792603 3.5644023418426514 5.333888530731201
  batch 60 loss: 1.7694860696792603, 3.5644023418426514, 5.333888530731201
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7735848426818848 3.8715391159057617 5.6451239585876465
Loss :  1.7731293439865112 3.6161277294158936 5.389256954193115
Loss :  1.7878223657608032 3.886019468307495 5.673841953277588
Loss :  1.7596780061721802 3.8830947875976562 5.642772674560547
Loss :  1.7495875358581543 3.3030266761779785 5.052614212036133
Loss :  1.7024593353271484 3.950316905975342 5.65277624130249
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7081873416900635 4.061887264251709 5.770074844360352
Loss :  1.7571945190429688 3.7958929538726807 5.55308723449707
Loss :  1.7917635440826416 3.7084145545959473 5.500178337097168
Total LOSS train 5.039102055476262 valid 5.61902916431427
CE LOSS train 1.7647568776057316 valid 0.4479408860206604
Contrastive LOSS train 3.274345188874465 valid 0.9271036386489868
EPOCH 31:
Loss :  1.7489866018295288 3.5098776817321777 5.258864402770996
Loss :  1.7704479694366455 3.50386381149292 5.2743120193481445
Loss :  1.7471290826797485 3.2494747638702393 4.996603965759277
Loss :  1.747864007949829 3.3701281547546387 5.117992401123047
Loss :  1.7948163747787476 4.201745510101318 5.9965620040893555
Loss :  1.7736707925796509 3.5095818042755127 5.283252716064453
Loss :  1.7776631116867065 3.7238781452178955 5.5015411376953125
Loss :  1.7611454725265503 2.869521379470825 4.630666732788086
Loss :  1.7780166864395142 2.8409204483032227 4.618937015533447
Loss :  1.7341254949569702 2.4590673446655273 4.193192958831787
Loss :  1.771578073501587 2.8607213497161865 4.632299423217773
Loss :  1.8028866052627563 2.9445104598999023 4.747396945953369
Loss :  1.7644364833831787 3.309410572052002 5.073846817016602
Loss :  1.7623103857040405 3.4209773540496826 5.183287620544434
Loss :  1.7594374418258667 2.782147169113159 4.541584491729736
Loss :  1.7433122396469116 3.0208356380462646 4.764147758483887
Loss :  1.7664986848831177 2.8299777507781982 4.5964765548706055
Loss :  1.7493237257003784 3.5281665325164795 5.277490139007568
Loss :  1.754555106163025 3.245114803314209 4.999670028686523
Loss :  1.7503173351287842 2.8340036869049072 4.584321022033691
  batch 20 loss: 1.7503173351287842, 2.8340036869049072, 4.584321022033691
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7498090267181396 3.3083913326263428 5.058200359344482
Loss :  1.758812427520752 2.89858078956604 4.657393455505371
Loss :  1.7537763118743896 3.1756551265716553 4.929431438446045
Loss :  1.77122962474823 3.7257304191589355 5.496960163116455
Loss :  1.7839634418487549 3.9536354541778564 5.737598896026611
Loss :  1.7551534175872803 3.7132914066314697 5.46844482421875
Loss :  1.7802543640136719 3.876047372817993 5.656301498413086
Loss :  1.7465980052947998 3.5692222118377686 5.315820217132568
Loss :  1.7812399864196777 3.4271323680877686 5.208372116088867
Loss :  1.7470887899398804 3.4904353618621826 5.237524032592773
Loss :  1.8034286499023438 3.70756196975708 5.510990619659424
Loss :  1.7647570371627808 3.6330206394195557 5.397777557373047
Loss :  1.7466660737991333 3.4485249519348145 5.195190906524658
Loss :  1.7488033771514893 3.4986495971679688 5.247452735900879
Loss :  1.7647355794906616 3.6160075664520264 5.380743026733398
Loss :  1.7764074802398682 3.5317156314849854 5.3081231117248535
Loss :  1.7659645080566406 2.939598798751831 4.705563545227051
Loss :  1.7402853965759277 2.4992716312408447 4.239557266235352
Loss :  1.7519477605819702 2.76259708404541 4.51454496383667
Loss :  1.773486852645874 2.6563994884490967 4.429886341094971
  batch 40 loss: 1.773486852645874, 2.6563994884490967, 4.429886341094971
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7590382099151611 3.5650246143341064 5.324062824249268
Loss :  1.7430152893066406 3.7181448936462402 5.461160182952881
Loss :  1.7589248418807983 3.06266450881958 4.821589469909668
Loss :  1.7649554014205933 3.3448357582092285 5.109791278839111
Loss :  1.7625476121902466 3.3711273670196533 5.1336750984191895
Loss :  1.766899585723877 2.9115850925445557 4.678484916687012
Loss :  1.759770393371582 3.0006067752838135 4.760376930236816
Loss :  1.7476836442947388 3.44264554977417 5.190329074859619
Loss :  1.7593334913253784 3.048783540725708 4.808116912841797
Loss :  1.769210934638977 2.6699092388153076 4.439120292663574
Loss :  1.7465542554855347 2.7182352542877197 4.464789390563965
Loss :  1.7468421459197998 2.8110008239746094 4.557843208312988
Loss :  1.7723705768585205 2.926145076751709 4.698515892028809
Loss :  1.7708820104599 2.7231547832489014 4.494036674499512
Loss :  1.7606199979782104 2.6452245712280273 4.405844688415527
Loss :  1.7517894506454468 2.3998196125030518 4.151608943939209
Loss :  1.780454158782959 2.772355318069458 4.552809715270996
Loss :  1.7691024541854858 3.0675947666168213 4.836697101593018
Loss :  1.776334285736084 3.3300747871398926 5.106409072875977
Loss :  1.7739801406860352 3.0704615116119385 4.8444414138793945
  batch 60 loss: 1.7739801406860352, 3.0704615116119385, 4.8444414138793945
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.777423620223999 2.9119112491607666 4.689334869384766
Loss :  1.777000904083252 2.7546324729919434 4.531633377075195
Loss :  1.790297031402588 2.9891557693481445 4.779452800750732
Loss :  1.756888508796692 3.159332752227783 4.9162211418151855
Loss :  1.752539038658142 2.7674355506896973 4.519974708557129
Loss :  1.8076276779174805 4.168086528778076 5.975714206695557
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7950762510299683 4.262106418609619 6.057182788848877
Loss :  1.8017911911010742 4.164797782897949 5.966588973999023
Loss :  1.8181227445602417 3.9637868404388428 5.781909465789795
Total LOSS train 4.942225280174842 valid 5.945348858833313
CE LOSS train 1.7633444272554837 valid 0.4545306861400604
Contrastive LOSS train 3.1788808492513803 valid 0.9909467101097107
EPOCH 32:
Loss :  1.7503081560134888 3.1589765548706055 4.909284591674805
Loss :  1.762369155883789 3.409916639328003 5.172286033630371
Loss :  1.7441452741622925 2.7422056198120117 4.486351013183594
Loss :  1.7486801147460938 2.896771192550659 4.645451545715332
Loss :  1.7880467176437378 3.058159589767456 4.846206188201904
Loss :  1.7611565589904785 3.047981023788452 4.809137344360352
Loss :  1.768122673034668 3.1310811042785645 4.899203777313232
Loss :  1.7494233846664429 3.366535186767578 5.1159586906433105
Loss :  1.7747585773468018 3.224261999130249 4.999020576477051
Loss :  1.7316858768463135 2.5936713218688965 4.325357437133789
Loss :  1.7664384841918945 2.6358067989349365 4.40224552154541
Loss :  1.797627568244934 3.4484550952911377 5.246082782745361
Loss :  1.7667373418807983 3.2353336811065674 5.002070903778076
Loss :  1.7653777599334717 3.4473001956939697 5.212677955627441
Loss :  1.7620670795440674 3.357030153274536 5.1190972328186035
Loss :  1.745904564857483 3.1055901050567627 4.851494789123535
Loss :  1.7678636312484741 3.5347719192504883 5.302635669708252
Loss :  1.7612724304199219 3.2914483547210693 5.05272102355957
Loss :  1.7556309700012207 2.924994468688965 4.6806254386901855
Loss :  1.7480456829071045 3.1055755615234375 4.853621482849121
  batch 20 loss: 1.7480456829071045, 3.1055755615234375, 4.853621482849121
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7516274452209473 3.172260046005249 4.923887252807617
Loss :  1.7561124563217163 2.8842389583587646 4.640351295471191
Loss :  1.7544060945510864 3.2230470180511475 4.977453231811523
Loss :  1.7664134502410889 3.542879343032837 5.309292793273926
Loss :  1.7783139944076538 3.73866868019104 5.516982555389404
Loss :  1.7472023963928223 3.6284613609313965 5.375663757324219
Loss :  1.777116298675537 4.113654136657715 5.890770435333252
Loss :  1.7480210065841675 3.3354222774505615 5.0834431648254395
Loss :  1.7778176069259644 3.6010892391204834 5.378906726837158
Loss :  1.7428280115127563 3.7728469371795654 5.515675067901611
Loss :  1.7997366189956665 3.7672173976898193 5.566954135894775
Loss :  1.7666820287704468 3.4040324687957764 5.170714378356934
Loss :  1.7423843145370483 3.396926164627075 5.139310359954834
Loss :  1.7476236820220947 3.4375808238983154 5.18520450592041
Loss :  1.7668001651763916 3.3448750972747803 5.111675262451172
Loss :  1.7745826244354248 3.0375454425811768 4.812128067016602
Loss :  1.7626746892929077 2.783026933670044 4.545701503753662
Loss :  1.7370309829711914 2.940096378326416 4.677127361297607
Loss :  1.7468806505203247 2.8798532485961914 4.626733779907227
Loss :  1.7649747133255005 2.7805747985839844 4.545549392700195
  batch 40 loss: 1.7649747133255005, 2.7805747985839844, 4.545549392700195
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.752150535583496 3.5528838634490967 5.305034637451172
Loss :  1.7391067743301392 3.285912275314331 5.02501916885376
Loss :  1.7555636167526245 3.3922295570373535 5.147793292999268
Loss :  1.7595068216323853 3.713672637939453 5.473179340362549
Loss :  1.756772756576538 3.281911611557007 5.038684368133545
Loss :  1.757698655128479 3.4042928218841553 5.161991596221924
Loss :  1.757489800453186 3.040813684463501 4.798303604125977
Loss :  1.7459200620651245 2.7671446800231934 4.513064861297607
Loss :  1.7528873682022095 3.2313168048858643 4.984204292297363
Loss :  1.7693381309509277 3.2744486331939697 5.043787002563477
Loss :  1.7413166761398315 3.364234447479248 5.105551242828369
Loss :  1.7444696426391602 3.493746757507324 5.238216400146484
Loss :  1.7661689519882202 3.55019211769104 5.316360950469971
Loss :  1.767905831336975 3.110149383544922 4.878055095672607
Loss :  1.7578045129776 3.015361785888672 4.773166179656982
Loss :  1.73953378200531 2.7309396266937256 4.470473289489746
Loss :  1.770072102546692 2.721397876739502 4.491469860076904
Loss :  1.7610511779785156 3.2173497676849365 4.978401184082031
Loss :  1.7680617570877075 3.3402464389801025 5.1083083152771
Loss :  1.759150743484497 2.8776016235351562 4.636752128601074
  batch 60 loss: 1.759150743484497, 2.8776016235351562, 4.636752128601074
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7644636631011963 3.2636895179748535 5.028153419494629
Loss :  1.7657002210617065 2.9394211769104004 4.7051215171813965
Loss :  1.7751582860946655 2.9790709018707275 4.7542290687561035
Loss :  1.7508233785629272 2.9039642810821533 4.654787540435791
Loss :  1.7468076944351196 2.859447479248047 4.606255054473877
Loss :  1.7469402551651 3.8543310165405273 5.601271152496338
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7406418323516846 4.029988765716553 5.770630836486816
Loss :  1.7715123891830444 4.007712364196777 5.779224872589111
Loss :  1.7939828634262085 3.501086473464966 5.295069217681885
Total LOSS train 4.9717141298147345 valid 5.611549019813538
CE LOSS train 1.7592279104086068 valid 0.4484957158565521
Contrastive LOSS train 3.212486201066237 valid 0.8752716183662415
EPOCH 33:
Loss :  1.740575909614563 3.50783371925354 5.248409748077393
Loss :  1.7522556781768799 3.8448867797851562 5.597142219543457
Loss :  1.736732006072998 2.8516886234283447 4.588420867919922
Loss :  1.7423202991485596 3.2276358604431152 4.969956398010254
Loss :  1.7813419103622437 3.762371063232422 5.543713092803955
Loss :  1.7550885677337646 3.072252035140991 4.827340602874756
Loss :  1.7589778900146484 3.8591108322143555 5.618088722229004
Loss :  1.7379326820373535 3.771617889404297 5.50955057144165
Loss :  1.7651383876800537 3.150846004486084 4.915984153747559
Loss :  1.7195931673049927 3.589163064956665 5.308756351470947
Loss :  1.759313702583313 3.6284353733062744 5.387749195098877
Loss :  1.7938029766082764 3.768887996673584 5.562690734863281
Loss :  1.7592966556549072 3.5323421955108643 5.2916388511657715
Loss :  1.7580417394638062 3.215116262435913 4.97315788269043
Loss :  1.7481985092163086 3.503706216812134 5.251904487609863
Loss :  1.7382173538208008 3.2401297092437744 4.978346824645996
Loss :  1.7672839164733887 3.4020838737487793 5.169367790222168
Loss :  1.756716012954712 3.0130460262298584 4.76976203918457
Loss :  1.7535253763198853 3.272080183029175 5.02560567855835
Loss :  1.7458513975143433 3.109217405319214 4.855068683624268
  batch 20 loss: 1.7458513975143433, 3.109217405319214, 4.855068683624268
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.742634654045105 3.034924030303955 4.77755880355835
Loss :  1.757817029953003 3.054189443588257 4.81200647354126
Loss :  1.74856698513031 2.7311758995056152 4.479743003845215
Loss :  1.7662602663040161 2.824986219406128 4.591246604919434
Loss :  1.7747167348861694 2.7935147285461426 4.568231582641602
Loss :  1.7478727102279663 2.6822638511657715 4.430136680603027
Loss :  1.7741100788116455 2.7799227237701416 4.554032802581787
Loss :  1.7448877096176147 2.8695123195648193 4.6143999099731445
Loss :  1.777921438217163 2.647688150405884 4.425609588623047
Loss :  1.7503169775009155 3.2055466175079346 4.9558634757995605
Loss :  1.796177864074707 3.133568525314331 4.929746627807617
Loss :  1.771353840827942 3.4289298057556152 5.200283527374268
Loss :  1.7495545148849487 3.1911773681640625 4.940732002258301
Loss :  1.7494646310806274 3.4413557052612305 5.190820217132568
Loss :  1.7720671892166138 3.3444607257843018 5.116528034210205
Loss :  1.7768305540084839 3.0641489028930664 4.84097957611084
Loss :  1.7689533233642578 3.2816877365112305 5.050641059875488
Loss :  1.7498788833618164 2.8880937099456787 4.637972831726074
Loss :  1.754209041595459 3.4532511234283447 5.207460403442383
Loss :  1.7698662281036377 3.6235063076019287 5.393372535705566
  batch 40 loss: 1.7698662281036377, 3.6235063076019287, 5.393372535705566
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7605706453323364 3.5616953372955322 5.322266101837158
Loss :  1.74871027469635 3.6393771171569824 5.388087272644043
Loss :  1.7650638818740845 3.7964892387390137 5.561553001403809
Loss :  1.765283465385437 3.3937158584594727 5.158999443054199
Loss :  1.7650365829467773 3.919372320175171 5.684409141540527
Loss :  1.7602906227111816 3.851390838623047 5.6116814613342285
Loss :  1.7554340362548828 3.283231019973755 5.038664817810059
Loss :  1.7488820552825928 3.021221876144409 4.770103931427002
Loss :  1.7486903667449951 3.734363317489624 5.483053684234619
Loss :  1.7711318731307983 3.6454224586486816 5.4165544509887695
Loss :  1.7410897016525269 3.678133726119995 5.419223308563232
Loss :  1.7410246133804321 3.297912836074829 5.038937568664551
Loss :  1.7618619203567505 3.135077714920044 4.896939754486084
Loss :  1.7591204643249512 3.263996124267578 5.023116588592529
Loss :  1.7530380487442017 3.1995840072631836 4.952621936798096
Loss :  1.7277463674545288 2.953918218612671 4.68166446685791
Loss :  1.7674461603164673 3.0548737049102783 4.822319984436035
Loss :  1.7634516954421997 3.4305648803710938 5.194016456604004
Loss :  1.7714595794677734 3.713330030441284 5.484789848327637
Loss :  1.7463343143463135 2.9372141361236572 4.683548450469971
  batch 60 loss: 1.7463343143463135, 2.9372141361236572, 4.683548450469971
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7664097547531128 2.8987932205200195 4.665203094482422
Loss :  1.760868787765503 2.9548563957214355 4.715724945068359
Loss :  1.7744247913360596 2.888988494873047 4.663413047790527
Loss :  1.7484796047210693 3.0022051334381104 4.75068473815918
Loss :  1.7433180809020996 2.502121686935425 4.245439529418945
Loss :  1.7921812534332275 4.0466485023498535 5.83882999420166
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7799029350280762 4.018339157104492 5.798242092132568
Loss :  1.7951080799102783 3.8749947547912598 5.670103073120117
Loss :  1.8059242963790894 3.5545289516448975 5.360453128814697
Total LOSS train 5.02743134865394 valid 5.666907072067261
CE LOSS train 1.757366653589102 valid 0.45148107409477234
Contrastive LOSS train 3.270064687728882 valid 0.8886322379112244
EPOCH 34:
Loss :  1.7373311519622803 3.0941660404205322 4.8314971923828125
Loss :  1.754305362701416 3.481786012649536 5.236091613769531
Loss :  1.741120457649231 3.50689697265625 5.248017311096191
Loss :  1.744027853012085 2.825054407119751 4.569082260131836
Loss :  1.7816861867904663 3.670482635498047 5.452168941497803
Loss :  1.7603198289871216 3.1050071716308594 4.865326881408691
Loss :  1.7621114253997803 3.4849660396575928 5.247077465057373
Loss :  1.7459077835083008 2.841984272003174 4.587892055511475
Loss :  1.7667574882507324 2.872396230697632 4.639153480529785
Loss :  1.7271827459335327 2.595048666000366 4.322231292724609
Loss :  1.7673512697219849 2.972839117050171 4.740190505981445
Loss :  1.792617917060852 2.918149471282959 4.7107672691345215
Loss :  1.761716365814209 3.0468363761901855 4.8085527420043945
Loss :  1.7597121000289917 2.9853036403656006 4.745015621185303
Loss :  1.747624158859253 3.036738634109497 4.78436279296875
Loss :  1.7403610944747925 2.7243661880493164 4.464727401733398
Loss :  1.7641373872756958 2.947714328765869 4.711851596832275
Loss :  1.7531893253326416 2.846802234649658 4.599991798400879
Loss :  1.7538589239120483 2.3945720195770264 4.148430824279785
Loss :  1.7435249090194702 3.5512800216674805 5.29480504989624
  batch 20 loss: 1.7435249090194702, 3.5512800216674805, 5.29480504989624
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7415751218795776 2.93430495262146 4.675879955291748
Loss :  1.7573728561401367 3.5634312629699707 5.320804119110107
Loss :  1.7502721548080444 3.41819429397583 5.168466567993164
Loss :  1.7616077661514282 3.342641592025757 5.104249477386475
Loss :  1.773492693901062 2.981083631515503 4.754576206207275
Loss :  1.7532368898391724 2.84153413772583 4.594770908355713
Loss :  1.7712688446044922 2.9508063793182373 4.722075462341309
Loss :  1.7378801107406616 3.738894462585449 5.4767746925354
Loss :  1.7826288938522339 2.8968353271484375 4.679464340209961
Loss :  1.7557042837142944 3.032508373260498 4.788212776184082
Loss :  1.8005647659301758 2.988490104675293 4.789054870605469
Loss :  1.765511393547058 3.13415265083313 4.899663925170898
Loss :  1.7538964748382568 3.3221468925476074 5.076043128967285
Loss :  1.7586029767990112 2.953213930130005 4.711816787719727
Loss :  1.7704795598983765 3.071477174758911 4.841956615447998
Loss :  1.7768118381500244 2.7072250843048096 4.484036922454834
Loss :  1.7797311544418335 2.5709168910980225 4.350647926330566
Loss :  1.75076425075531 2.6525769233703613 4.403341293334961
Loss :  1.7646974325180054 2.5162193775177 4.280916690826416
Loss :  1.7682255506515503 2.8594250679016113 4.627650737762451
  batch 40 loss: 1.7682255506515503, 2.8594250679016113, 4.627650737762451
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7593151330947876 3.1737611293792725 4.93307638168335
Loss :  1.7468490600585938 3.04919695854187 4.796046257019043
Loss :  1.7570178508758545 3.0692291259765625 4.826247215270996
Loss :  1.7605888843536377 3.606776475906372 5.36736536026001
Loss :  1.7619463205337524 3.984545946121216 5.746492385864258
Loss :  1.766938328742981 3.8702805042266846 5.637218952178955
Loss :  1.7626895904541016 3.798414707183838 5.5611042976379395
Loss :  1.7547394037246704 4.112365245819092 5.867104530334473
Loss :  1.7470022439956665 3.355448007583618 5.102450370788574
Loss :  1.763797640800476 3.5581579208374023 5.321955680847168
Loss :  1.7382266521453857 3.9216020107269287 5.6598286628723145
Loss :  1.7389495372772217 3.6322851181030273 5.371234893798828
Loss :  1.7637144327163696 3.8381028175354004 5.6018171310424805
Loss :  1.762874960899353 3.729300022125244 5.492175102233887
Loss :  1.7535367012023926 3.6004130840301514 5.353949546813965
Loss :  1.7451215982437134 3.6787545680999756 5.4238762855529785
Loss :  1.7760528326034546 3.532897472381592 5.308950424194336
Loss :  1.7584837675094604 3.0195183753967285 4.7780022621154785
Loss :  1.7656409740447998 3.3946402072906494 5.160281181335449
Loss :  1.765783429145813 2.8965213298797607 4.662304878234863
  batch 60 loss: 1.765783429145813, 2.8965213298797607, 4.662304878234863
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7653725147247314 3.110656261444092 4.876029014587402
Loss :  1.7665332555770874 2.4957377910614014 4.262270927429199
Loss :  1.7781130075454712 2.701857805252075 4.479970932006836
Loss :  1.7505505084991455 2.4926533699035645 4.243204116821289
Loss :  1.7468314170837402 2.128767490386963 3.875598907470703
Loss :  1.724636435508728 3.6463277339935303 5.370964050292969
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7318493127822876 3.982238531112671 5.714087963104248
Loss :  1.746251106262207 3.4246087074279785 5.1708598136901855
Loss :  1.7541908025741577 3.5875909328460693 5.3417816162109375
Total LOSS train 4.9148645107562725 valid 5.399423360824585
CE LOSS train 1.759012904533973 valid 0.43854770064353943
Contrastive LOSS train 3.1558515805464524 valid 0.8968977332115173
EPOCH 35:
Loss :  1.7390556335449219 2.4029951095581055 4.142050743103027
Loss :  1.7582571506500244 2.7163896560668945 4.47464656829834
Loss :  1.7432295083999634 2.3760385513305664 4.11926794052124
Loss :  1.7490121126174927 2.2817447185516357 4.030756950378418
Loss :  1.7783464193344116 3.278048276901245 5.056394577026367
Loss :  1.7528140544891357 2.8306000232696533 4.583414077758789
Loss :  1.7577811479568481 2.8235864639282227 4.581367492675781
Loss :  1.742342472076416 2.861145257949829 4.603487968444824
Loss :  1.7609679698944092 2.917872905731201 4.678840637207031
Loss :  1.7200125455856323 3.178173542022705 4.898186206817627
Loss :  1.7597529888153076 3.5757217407226562 5.335474967956543
Loss :  1.7867194414138794 3.382434844970703 5.169154167175293
Loss :  1.7609773874282837 3.2434802055358887 5.004457473754883
Loss :  1.7569774389266968 3.2037270069122314 4.960704326629639
Loss :  1.7486786842346191 3.0444321632385254 4.7931108474731445
Loss :  1.7362418174743652 3.3175621032714844 5.05380392074585
Loss :  1.7619061470031738 2.7979185581207275 4.5598249435424805
Loss :  1.7476624250411987 2.8412327766418457 4.588895320892334
Loss :  1.7500766515731812 2.6268932819366455 4.376969814300537
Loss :  1.7369186878204346 3.0863630771636963 4.823281764984131
  batch 20 loss: 1.7369186878204346, 3.0863630771636963, 4.823281764984131
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7417656183242798 2.992863893508911 4.7346296310424805
Loss :  1.7496765851974487 3.1449873447418213 4.8946638107299805
Loss :  1.7446836233139038 2.9456589221954346 4.690342426300049
Loss :  1.7559493780136108 3.18847918510437 4.944428443908691
Loss :  1.7697205543518066 3.279811143875122 5.049531936645508
Loss :  1.7405965328216553 3.049715995788574 4.790312767028809
Loss :  1.7611181735992432 3.0795507431030273 4.840668678283691
Loss :  1.7258888483047485 2.877636432647705 4.603525161743164
Loss :  1.766553521156311 2.6731650829315186 4.439718723297119
Loss :  1.7389451265335083 2.700615882873535 4.439560890197754
Loss :  1.7910748720169067 2.811737537384033 4.60281229019165
Loss :  1.757435917854309 2.723952293395996 4.481388092041016
Loss :  1.739741325378418 2.5620827674865723 4.30182409286499
Loss :  1.7438944578170776 2.554790735244751 4.298685073852539
Loss :  1.7602980136871338 2.7721288204193115 4.532426834106445
Loss :  1.7685974836349487 2.9925026893615723 4.7611002922058105
Loss :  1.7647557258605957 2.9911422729492188 4.7558979988098145
Loss :  1.7324409484863281 2.3931610584259033 4.125601768493652
Loss :  1.7574677467346191 2.627662181854248 4.385129928588867
Loss :  1.7561901807785034 2.438767910003662 4.194958209991455
  batch 40 loss: 1.7561901807785034, 2.438767910003662, 4.194958209991455
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7504299879074097 2.389000415802002 4.139430522918701
Loss :  1.736481785774231 2.4201295375823975 4.156611442565918
Loss :  1.7512391805648804 2.710484027862549 4.461723327636719
Loss :  1.7493654489517212 2.9149863719940186 4.664351940155029
Loss :  1.7519396543502808 3.1738693714141846 4.925808906555176
Loss :  1.7510038614273071 2.8816990852355957 4.632702827453613
Loss :  1.7452863454818726 3.3937666416168213 5.139052867889404
Loss :  1.7446794509887695 3.0089385509490967 4.753618240356445
Loss :  1.7346550226211548 3.3466758728027344 5.0813307762146
Loss :  1.7562470436096191 3.377472400665283 5.133719444274902
Loss :  1.736940860748291 3.3198516368865967 5.056792259216309
Loss :  1.7358956336975098 3.3744401931762695 5.110335826873779
Loss :  1.756937026977539 3.5749547481536865 5.331892013549805
Loss :  1.7563254833221436 3.4520039558410645 5.208329200744629
Loss :  1.7519139051437378 3.4932923316955566 5.245206356048584
Loss :  1.7358002662658691 2.5598978996276855 4.295698165893555
Loss :  1.7695493698120117 3.031829357147217 4.8013787269592285
Loss :  1.755543828010559 2.7153055667877197 4.470849514007568
Loss :  1.7620139122009277 2.770615816116333 4.53262996673584
Loss :  1.7552094459533691 2.667973279953003 4.423182487487793
  batch 60 loss: 1.7552094459533691, 2.667973279953003, 4.423182487487793
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7545188665390015 2.8703596591949463 4.624878406524658
Loss :  1.7583874464035034 3.2341842651367188 4.992571830749512
Loss :  1.7631663084030151 3.1164848804473877 4.879651069641113
Loss :  1.7443434000015259 3.3119688034057617 5.056312084197998
Loss :  1.7415004968643188 2.319288730621338 4.060789108276367
Loss :  1.7683887481689453 3.9461042881011963 5.7144927978515625
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7725149393081665 4.11557149887085 5.888086318969727
Loss :  1.7829498052597046 3.9113049507141113 5.6942548751831055
Loss :  1.7720165252685547 3.736945390701294 5.5089616775512695
Total LOSS train 4.690463770352877 valid 5.701448917388916
CE LOSS train 1.7517522976948665 valid 0.44300413131713867
Contrastive LOSS train 2.938711485495934 valid 0.9342363476753235
EPOCH 36:
Loss :  1.734257459640503 2.919438362121582 4.653696060180664
Loss :  1.7481911182403564 3.400921583175659 5.149112701416016
Loss :  1.7350317239761353 2.9348645210266113 4.669896125793457
Loss :  1.7433489561080933 2.692687511444092 4.436036586761475
Loss :  1.7738755941390991 2.4749066829681396 4.248782157897949
Loss :  1.7512258291244507 2.7979328632354736 4.549158573150635
Loss :  1.7551450729370117 2.5186874866485596 4.273832321166992
Loss :  1.7409498691558838 2.389460325241089 4.130410194396973
Loss :  1.7603552341461182 2.61396861076355 4.374323844909668
Loss :  1.7180908918380737 2.6567983627319336 4.374889373779297
Loss :  1.7578346729278564 2.798562526702881 4.556397438049316
Loss :  1.786486029624939 3.0506463050842285 4.837132453918457
Loss :  1.753866195678711 2.9025156497955322 4.656381607055664
Loss :  1.7534741163253784 2.8553967475891113 4.608870983123779
Loss :  1.7447484731674194 3.02789568901062 4.77264404296875
Loss :  1.739198923110962 3.1643564701080322 4.903555393218994
Loss :  1.7547727823257446 3.464562177658081 5.219335079193115
Loss :  1.7478851079940796 3.2927980422973633 5.040683269500732
Loss :  1.7484334707260132 3.2085564136505127 4.956989765167236
Loss :  1.7334786653518677 3.1799609661102295 4.913439750671387
  batch 20 loss: 1.7334786653518677, 3.1799609661102295, 4.913439750671387
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.73982834815979 3.5051281452178955 5.2449564933776855
Loss :  1.7490421533584595 3.646087408065796 5.395129680633545
Loss :  1.7426015138626099 3.341628074645996 5.084229469299316
Loss :  1.7582316398620605 2.941798448562622 4.700030326843262
Loss :  1.7741330862045288 3.0656440258026123 4.839776992797852
Loss :  1.7441229820251465 2.85947322845459 4.603596210479736
Loss :  1.764561653137207 3.055129051208496 4.819690704345703
Loss :  1.7279168367385864 3.2585389614105225 4.986455917358398
Loss :  1.7641159296035767 3.064666986465454 4.82878303527832
Loss :  1.743975043296814 3.5789310932159424 5.322906017303467
Loss :  1.7941347360610962 3.4615020751953125 5.255636692047119
Loss :  1.7523421049118042 3.0808138847351074 4.833156108856201
Loss :  1.7361762523651123 3.2717373371124268 5.007913589477539
Loss :  1.7417913675308228 3.1996073722839355 4.941398620605469
Loss :  1.756590485572815 3.0360093116760254 4.792599678039551
Loss :  1.7685432434082031 2.6934149265289307 4.461957931518555
Loss :  1.7563598155975342 2.7519633769989014 4.5083231925964355
Loss :  1.7215436697006226 2.963412046432495 4.684955596923828
Loss :  1.7466424703598022 3.0338127613067627 4.780455112457275
Loss :  1.7543716430664062 2.838482141494751 4.592853546142578
  batch 40 loss: 1.7543716430664062, 2.838482141494751, 4.592853546142578
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.74349844455719 2.9518585205078125 4.695356845855713
Loss :  1.7342778444290161 2.761042356491089 4.4953203201293945
Loss :  1.7487763166427612 2.7966010570526123 4.545377254486084
Loss :  1.7505203485488892 3.0064001083374023 4.756920337677002
Loss :  1.749247431755066 3.238642692565918 4.987890243530273
Loss :  1.7481857538223267 3.299913167953491 5.048099040985107
Loss :  1.7407441139221191 3.823580741882324 5.564324855804443
Loss :  1.7384347915649414 3.2679483890533447 5.006382942199707
Loss :  1.7360912561416626 3.5015552043914795 5.237646579742432
Loss :  1.7574584484100342 3.558872938156128 5.316331386566162
Loss :  1.733798861503601 3.3055145740509033 5.039313316345215
Loss :  1.7324074506759644 3.2847838401794434 5.017191410064697
Loss :  1.7532620429992676 3.0824334621429443 4.835695266723633
Loss :  1.7567391395568848 3.261375904083252 5.018115043640137
Loss :  1.749513030052185 3.7370598316192627 5.486572742462158
Loss :  1.7282681465148926 3.2361040115356445 4.964372158050537
Loss :  1.7624073028564453 3.935098171234131 5.697505474090576
Loss :  1.7535948753356934 3.6399595737457275 5.3935546875
Loss :  1.7614820003509521 3.6826870441436768 5.444169044494629
Loss :  1.7558751106262207 3.728759527206421 5.4846343994140625
  batch 60 loss: 1.7558751106262207, 3.728759527206421, 5.4846343994140625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7579113245010376 3.61722731590271 5.375138759613037
Loss :  1.762768030166626 3.232862949371338 4.995631217956543
Loss :  1.7696928977966309 3.423201084136963 5.192893981933594
Loss :  1.7502360343933105 3.710249662399292 5.460485458374023
Loss :  1.7441867589950562 2.9208872318267822 4.665073871612549
Loss :  1.8514060974121094 4.132997512817383 5.984403610229492
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8431349992752075 4.146212100982666 5.989346981048584
Loss :  1.8376649618148804 4.053905010223389 5.891570091247559
Loss :  1.8379276990890503 3.9700231552124023 5.807950973510742
Total LOSS train 4.903605681199294 valid 5.918317914009094
CE LOSS train 1.749800814115084 valid 0.4594819247722626
Contrastive LOSS train 3.1538048817561224 valid 0.9925057888031006
EPOCH 37:
Loss :  1.7372649908065796 3.9070425033569336 5.644307613372803
Loss :  1.7458059787750244 3.44889235496521 5.194698333740234
Loss :  1.741528868675232 2.736345052719116 4.477873802185059
Loss :  1.7527778148651123 2.844184160232544 4.596961975097656
Loss :  1.7817572355270386 2.9799180030822754 4.7616753578186035
Loss :  1.755871295928955 3.0583159923553467 4.814187049865723
Loss :  1.7655199766159058 2.866201162338257 4.631721019744873
Loss :  1.7493672370910645 2.916389226913452 4.6657562255859375
Loss :  1.766636610031128 2.586045742034912 4.352682113647461
Loss :  1.7226053476333618 2.9044559001922607 4.627061367034912
Loss :  1.7605396509170532 3.814897298812866 5.575437068939209
Loss :  1.7909750938415527 3.7578413486480713 5.548816680908203
Loss :  1.755492925643921 3.4108381271362305 5.1663312911987305
Loss :  1.7561300992965698 3.4100725650787354 5.166202545166016
Loss :  1.7467049360275269 3.740474224090576 5.487179279327393
Loss :  1.7358211278915405 3.3644189834594727 5.100240230560303
Loss :  1.7587618827819824 3.3044445514678955 5.063206672668457
Loss :  1.7534408569335938 3.3583526611328125 5.111793518066406
Loss :  1.7498809099197388 3.4310848712921143 5.180965900421143
Loss :  1.740276575088501 3.566103458404541 5.306380271911621
  batch 20 loss: 1.740276575088501, 3.566103458404541, 5.306380271911621
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.744831919670105 3.721389055252075 5.466220855712891
Loss :  1.7504771947860718 3.728649377822876 5.479126453399658
Loss :  1.7456244230270386 3.603682518005371 5.349307060241699
Loss :  1.765573501586914 3.5042128562927246 5.269786357879639
Loss :  1.7742090225219727 2.7897419929504395 4.563951015472412
Loss :  1.7456929683685303 3.155945301055908 4.901638031005859
Loss :  1.774519443511963 2.870851755142212 4.645371437072754
Loss :  1.7465635538101196 3.0389790534973145 4.7855424880981445
Loss :  1.7763265371322632 3.2568612098693848 5.0331878662109375
Loss :  1.7459455728530884 2.994460105895996 4.740405559539795
Loss :  1.7944227457046509 3.1557939052581787 4.950216770172119
Loss :  1.764074444770813 2.9563448429107666 4.720419406890869
Loss :  1.7448185682296753 2.9526078701019287 4.6974263191223145
Loss :  1.7416884899139404 2.816349983215332 4.558038711547852
Loss :  1.7606017589569092 3.027940034866333 4.788541793823242
Loss :  1.7699223756790161 2.6863529682159424 4.456275463104248
Loss :  1.7600924968719482 3.020634889602661 4.780727386474609
Loss :  1.7373780012130737 3.3866875171661377 5.124065399169922
Loss :  1.7484722137451172 3.066694736480713 4.81516695022583
Loss :  1.7607277631759644 2.8921751976013184 4.652903079986572
  batch 40 loss: 1.7607277631759644, 2.8921751976013184, 4.652903079986572
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.750517725944519 3.0052616596221924 4.755779266357422
Loss :  1.736561894416809 3.1815829277038574 4.918144702911377
Loss :  1.751134991645813 2.7577860355377197 4.508921146392822
Loss :  1.7552311420440674 2.8734169006347656 4.628647804260254
Loss :  1.7521836757659912 2.7197163105010986 4.47189998626709
Loss :  1.7511050701141357 2.9588687419891357 4.7099738121032715
Loss :  1.7460222244262695 3.146609306335449 4.892631530761719
Loss :  1.7434778213500977 3.2866828441619873 5.030160903930664
Loss :  1.7375352382659912 3.2468109130859375 4.984346389770508
Loss :  1.7593445777893066 3.102957010269165 4.862301826477051
Loss :  1.7378982305526733 3.431736469268799 5.169634819030762
Loss :  1.735206127166748 3.298628807067871 5.033834934234619
Loss :  1.7593443393707275 3.0870440006256104 4.846388339996338
Loss :  1.7605533599853516 3.0678484439849854 4.828401565551758
Loss :  1.7561343908309937 3.3223698139190674 5.0785040855407715
Loss :  1.7401379346847534 2.9112753868103027 4.651413440704346
Loss :  1.7700642347335815 3.1291539669036865 4.8992180824279785
Loss :  1.7593947649002075 3.4317352771759033 5.1911301612854
Loss :  1.7656071186065674 3.751801013946533 5.51740837097168
Loss :  1.7581430673599243 3.450141429901123 5.208284378051758
  batch 60 loss: 1.7581430673599243, 3.450141429901123, 5.208284378051758
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7578985691070557 3.3770244121551514 5.134922981262207
Loss :  1.757843255996704 3.348400831222534 5.106244087219238
Loss :  1.7720632553100586 3.288792848587036 5.060855865478516
Loss :  1.7438499927520752 2.9509196281433105 4.694769859313965
Loss :  1.7371667623519897 2.7599356174468994 4.4971022605896
Loss :  1.8312344551086426 4.014223098754883 5.845457553863525
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8222713470458984 4.118643760681152 5.940915107727051
Loss :  1.823535442352295 4.00268030166626 5.826215744018555
Loss :  1.8352171182632446 3.77007794380188 5.605295181274414
Total LOSS train 4.937426420358511 valid 5.804470896720886
CE LOSS train 1.7540544950045072 valid 0.45880427956581116
Contrastive LOSS train 3.1833719070141133 valid 0.94251948595047
EPOCH 38:
Loss :  1.7354713678359985 2.9825243949890137 4.717995643615723
Loss :  1.7494198083877563 3.3048741817474365 5.054294109344482
Loss :  1.731209397315979 3.223468780517578 4.954678058624268
Loss :  1.7368696928024292 3.0770184993743896 4.813888072967529
Loss :  1.7748526334762573 2.8122215270996094 4.587074279785156
Loss :  1.7503235340118408 2.9743311405181885 4.724654674530029
Loss :  1.7605594396591187 3.116529941558838 4.877089500427246
Loss :  1.7429579496383667 2.962430477142334 4.70538854598999
Loss :  1.759985327720642 3.260510206222534 5.020495414733887
Loss :  1.720637321472168 3.345672607421875 5.066309928894043
Loss :  1.7621703147888184 3.6873748302459717 5.449544906616211
Loss :  1.7865886688232422 3.4152309894561768 5.20181941986084
Loss :  1.7599766254425049 3.3173367977142334 5.077313423156738
Loss :  1.756273865699768 3.5524656772613525 5.30873966217041
Loss :  1.7463356256484985 3.5883750915527344 5.334710597991943
Loss :  1.731791615486145 3.4615559577941895 5.193347454071045
Loss :  1.757271647453308 4.0216169357299805 5.778888702392578
Loss :  1.752575159072876 3.991180419921875 5.743755340576172
Loss :  1.742594599723816 3.717212200164795 5.4598069190979
Loss :  1.7356982231140137 3.516564130783081 5.252262115478516
  batch 20 loss: 1.7356982231140137, 3.516564130783081, 5.252262115478516
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7375949621200562 3.609919309616089 5.3475141525268555
Loss :  1.7460857629776 3.4720406532287598 5.21812629699707
Loss :  1.737101435661316 3.250067949295044 4.98716926574707
Loss :  1.7542749643325806 3.559448003768921 5.313723087310791
Loss :  1.7620712518692017 3.4527909755706787 5.21486234664917
Loss :  1.7351481914520264 2.9399590492248535 4.675107002258301
Loss :  1.7631832361221313 3.347330331802368 5.110513687133789
Loss :  1.7309399843215942 2.900912046432495 4.631852149963379
Loss :  1.7652109861373901 2.8510639667510986 4.616274833679199
Loss :  1.7334587574005127 3.222628355026245 4.956087112426758
Loss :  1.7882161140441895 2.858349084854126 4.6465654373168945
Loss :  1.758352279663086 3.098389148712158 4.856741428375244
Loss :  1.7351049184799194 2.9338440895080566 4.668949127197266
Loss :  1.7362720966339111 2.900770425796509 4.63704252243042
Loss :  1.758126139640808 2.8931376934051514 4.65126371383667
Loss :  1.763582706451416 3.160613536834717 4.924196243286133
Loss :  1.755150318145752 2.7816505432128906 4.536800861358643
Loss :  1.7281123399734497 2.748520851135254 4.476633071899414
Loss :  1.7390459775924683 3.0697414875030518 4.8087873458862305
Loss :  1.75002121925354 2.449387788772583 4.199409008026123
  batch 40 loss: 1.75002121925354, 2.449387788772583, 4.199409008026123
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7453868389129639 2.8256821632385254 4.57106876373291
Loss :  1.732584834098816 2.3661298751831055 4.098714828491211
Loss :  1.750040888786316 2.6502761840820312 4.400317192077637
Loss :  1.7513759136199951 2.9041950702667236 4.655570983886719
Loss :  1.7537822723388672 2.6251633167266846 4.378945350646973
Loss :  1.7526090145111084 2.508403778076172 4.261013031005859
Loss :  1.7472598552703857 2.46966290473938 4.216922760009766
Loss :  1.7476247549057007 2.350992441177368 4.098617076873779
Loss :  1.742143154144287 2.8647284507751465 4.606871604919434
Loss :  1.7580831050872803 3.0689687728881836 4.827052116394043
Loss :  1.7389224767684937 2.866645336151123 4.605567932128906
Loss :  1.7372676134109497 2.9044642448425293 4.6417317390441895
Loss :  1.7582554817199707 3.3427374362945557 5.1009931564331055
Loss :  1.7580199241638184 3.2612764835357666 5.019296646118164
Loss :  1.7553887367248535 3.1660430431365967 4.921431541442871
Loss :  1.7403584718704224 2.9642527103424072 4.704611301422119
Loss :  1.7679567337036133 2.993881940841675 4.761838912963867
Loss :  1.755499005317688 2.828962564468384 4.584461688995361
Loss :  1.7627439498901367 3.116131067276001 4.878874778747559
Loss :  1.7578119039535522 2.6238787174224854 4.381690502166748
  batch 60 loss: 1.7578119039535522, 2.6238787174224854, 4.381690502166748
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7609145641326904 2.712672233581543 4.4735870361328125
Loss :  1.7634000778198242 2.813961982727051 4.577362060546875
Loss :  1.7743330001831055 2.5225157737731934 4.296848773956299
Loss :  1.7481634616851807 2.9644405841827393 4.71260404586792
Loss :  1.7480230331420898 2.576228141784668 4.324251174926758
Loss :  1.8298958539962769 4.302109241485596 6.132005214691162
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8225371837615967 4.323354721069336 6.145892143249512
Loss :  1.8166429996490479 4.291037559509277 6.107680320739746
Loss :  1.8084787130355835 4.217865943908691 6.0263447761535645
Total LOSS train 4.813844930208647 valid 6.102980613708496
CE LOSS train 1.7504394696309016 valid 0.4521196782588959
Contrastive LOSS train 3.063405466079712 valid 1.0544664859771729
EPOCH 39:
Loss :  1.7450776100158691 2.754927635192871 4.50000524520874
Loss :  1.7563146352767944 3.1609442234039307 4.9172587394714355
Loss :  1.7402503490447998 2.690589189529419 4.430839538574219
Loss :  1.7455110549926758 2.96354079246521 4.709052085876465
Loss :  1.7727482318878174 2.949307680130005 4.722055912017822
Loss :  1.7523088455200195 3.1199734210968018 4.872282028198242
Loss :  1.7532553672790527 3.40293288230896 5.156188011169434
Loss :  1.7428838014602661 3.357395648956299 5.100279331207275
Loss :  1.760677456855774 3.237114667892456 4.9977922439575195
Loss :  1.7193968296051025 3.543921709060669 5.2633185386657715
Loss :  1.7594255208969116 3.4603688716888428 5.219794273376465
Loss :  1.7818833589553833 3.233433723449707 5.015316963195801
Loss :  1.7577561140060425 2.8438973426818848 4.601653575897217
Loss :  1.7578414678573608 2.995586395263672 4.753427982330322
Loss :  1.7500662803649902 3.7236721515655518 5.473738670349121
Loss :  1.7366867065429688 3.990835666656494 5.727522373199463
Loss :  1.761234998703003 3.4536657333374023 5.214900970458984
Loss :  1.7517385482788086 2.992149591445923 4.743887901306152
Loss :  1.7491719722747803 2.265272855758667 4.014444828033447
Loss :  1.7377005815505981 2.4906229972839355 4.228323459625244
  batch 20 loss: 1.7377005815505981, 2.4906229972839355, 4.228323459625244
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.747450590133667 2.6604645252227783 4.407915115356445
Loss :  1.7554740905761719 3.1747562885284424 4.930230140686035
Loss :  1.7461962699890137 3.0191454887390137 4.765341758728027
Loss :  1.7622498273849487 2.8123738765716553 4.5746235847473145
Loss :  1.7647658586502075 3.098353624343872 4.863119602203369
Loss :  1.7480254173278809 2.7873682975769043 4.535393714904785
Loss :  1.7656821012496948 2.9257655143737793 4.691447734832764
Loss :  1.734405517578125 2.931307554244995 4.665713310241699
Loss :  1.7704355716705322 3.196800470352173 4.967236042022705
Loss :  1.7407934665679932 3.1290781497955322 4.869871616363525
Loss :  1.784551978111267 3.3655717372894287 5.150123596191406
Loss :  1.7602601051330566 3.494614601135254 5.2548747062683105
Loss :  1.7423661947250366 3.0084598064422607 4.750825881958008
Loss :  1.7401387691497803 2.9571523666381836 4.697291374206543
Loss :  1.7566362619400024 3.2051687240600586 4.9618048667907715
Loss :  1.7641081809997559 3.088473320007324 4.85258150100708
Loss :  1.7573679685592651 3.2426064014434814 4.999974250793457
Loss :  1.7298213243484497 3.5983357429504395 5.3281569480896
Loss :  1.750482439994812 3.84863018989563 5.599112510681152
Loss :  1.7523890733718872 3.1556625366210938 4.908051490783691
  batch 40 loss: 1.7523890733718872, 3.1556625366210938, 4.908051490783691
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7451359033584595 3.8577613830566406 5.6028971672058105
Loss :  1.7306815385818481 3.005805253982544 4.736486911773682
Loss :  1.7473101615905762 3.428931951522827 5.176241874694824
Loss :  1.7441600561141968 3.718287229537964 5.462447166442871
Loss :  1.7454203367233276 2.6523396968841553 4.397759914398193
Loss :  1.744231104850769 2.8157830238342285 4.560014247894287
Loss :  1.736130952835083 3.023158073425293 4.759288787841797
Loss :  1.741194486618042 2.976146697998047 4.717341423034668
Loss :  1.7254817485809326 2.976761817932129 4.702243804931641
Loss :  1.7510738372802734 2.67398738861084 4.425061225891113
Loss :  1.7340749502182007 2.7619223594665527 4.495997428894043
Loss :  1.7320220470428467 2.7817537784576416 4.513775825500488
Loss :  1.748335361480713 2.468266248703003 4.216601371765137
Loss :  1.7497907876968384 2.4764087200164795 4.226199626922607
Loss :  1.750841498374939 2.741173267364502 4.4920148849487305
Loss :  1.72454035282135 2.72204327583313 4.4465837478637695
Loss :  1.758800983428955 2.5453314781188965 4.304132461547852
Loss :  1.7510960102081299 3.0604872703552246 4.811583518981934
Loss :  1.7582263946533203 3.476679801940918 5.234906196594238
Loss :  1.7509211301803589 3.215468645095825 4.9663896560668945
  batch 60 loss: 1.7509211301803589, 3.215468645095825, 4.9663896560668945
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7534836530685425 2.460151195526123 4.213634967803955
Loss :  1.7582169771194458 2.549281597137451 4.307498455047607
Loss :  1.7630527019500732 2.4074063301086426 4.170458793640137
Loss :  1.745233178138733 2.5432541370391846 4.288487434387207
Loss :  1.7414519786834717 2.2603681087493896 4.001820087432861
Loss :  1.8385334014892578 4.230208396911621 6.068741798400879
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8299548625946045 4.255460262298584 6.085414886474609
Loss :  1.8255648612976074 4.081115245819092 5.906680107116699
Loss :  1.824841022491455 3.7931618690490723 5.618002891540527
Total LOSS train 4.779471375392034 valid 5.919709920883179
CE LOSS train 1.7497913672373846 valid 0.45621025562286377
Contrastive LOSS train 3.029680017324594 valid 0.9482904672622681
EPOCH 40:
Loss :  1.742539644241333 2.5302586555480957 4.272798538208008
Loss :  1.7502834796905518 2.94399094581604 4.694274425506592
Loss :  1.7412644624710083 2.7194786071777344 4.460742950439453
Loss :  1.7496955394744873 3.109496593475342 4.85919189453125
Loss :  1.7754141092300415 2.761821985244751 4.537236213684082
Loss :  1.7564834356307983 3.418051242828369 5.174534797668457
Loss :  1.7582707405090332 3.2233026027679443 4.981573104858398
Loss :  1.743709921836853 3.345822811126709 5.089532852172852
Loss :  1.7573202848434448 3.6827659606933594 5.440086364746094
Loss :  1.720522165298462 3.6967294216156006 5.4172515869140625
Loss :  1.751896858215332 4.018310070037842 5.770206928253174
Loss :  1.7879793643951416 3.6422224044799805 5.430201530456543
Loss :  1.750718116760254 3.3206567764282227 5.071374893188477
Loss :  1.7525545358657837 3.612429141998291 5.364983558654785
Loss :  1.7385802268981934 3.5778324604034424 5.316412925720215
Loss :  1.7299574613571167 3.385532855987549 5.115490436553955
Loss :  1.7430243492126465 3.740665912628174 5.48369026184082
Loss :  1.737653136253357 3.6972248554229736 5.434877872467041
Loss :  1.7388908863067627 3.8732383251190186 5.612129211425781
Loss :  1.7229660749435425 3.7079319953918457 5.430898189544678
  batch 20 loss: 1.7229660749435425, 3.7079319953918457, 5.430898189544678
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7352629899978638 4.070152759552002 5.805415630340576
Loss :  1.7393810749053955 3.621493339538574 5.360874176025391
Loss :  1.7394306659698486 3.936687469482422 5.676117897033691
Loss :  1.7529933452606201 3.7932803630828857 5.546273708343506
Loss :  1.764467477798462 3.72710919380188 5.491576671600342
Loss :  1.7360376119613647 3.6543030738830566 5.390340805053711
Loss :  1.7573407888412476 3.7818603515625 5.539201259613037
Loss :  1.7264424562454224 3.672166109085083 5.398608684539795
Loss :  1.7595734596252441 3.0470385551452637 4.806612014770508
Loss :  1.737607479095459 3.2561659812927246 4.993773460388184
Loss :  1.7838197946548462 3.053758144378662 4.837577819824219
Loss :  1.7491505146026611 2.94545316696167 4.69460391998291
Loss :  1.7286969423294067 2.9304654598236084 4.659162521362305
Loss :  1.7367370128631592 2.629629611968994 4.366366386413574
Loss :  1.7497979402542114 2.803105115890503 4.552903175354004
Loss :  1.7574597597122192 2.7092206478118896 4.466680526733398
Loss :  1.7462997436523438 2.442678689956665 4.18897819519043
Loss :  1.7203491926193237 2.410482168197632 4.130831241607666
Loss :  1.7351018190383911 2.601823568344116 4.336925506591797
Loss :  1.7465524673461914 2.4249634742736816 4.171515941619873
  batch 40 loss: 1.7465524673461914, 2.4249634742736816, 4.171515941619873
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.737757682800293 2.6841330528259277 4.421890735626221
Loss :  1.7304463386535645 3.048825979232788 4.779272079467773
Loss :  1.7456072568893433 2.897036075592041 4.642643451690674
Loss :  1.748239278793335 2.7489688396453857 4.497208118438721
Loss :  1.7459973096847534 2.864292621612549 4.610290050506592
Loss :  1.741949439048767 2.764488458633423 4.5064377784729
Loss :  1.7368574142456055 2.8009655475616455 4.537822723388672
Loss :  1.7383852005004883 2.7174181938171387 4.455803394317627
Loss :  1.7266217470169067 2.970162868499756 4.696784496307373
Loss :  1.7530620098114014 2.867600440979004 4.620662689208984
Loss :  1.7301722764968872 3.044949769973755 4.775122165679932
Loss :  1.72892427444458 2.9098501205444336 4.638774394989014
Loss :  1.7452640533447266 3.1298818588256836 4.87514591217041
Loss :  1.749942421913147 2.73054838180542 4.480490684509277
Loss :  1.7448805570602417 2.764252185821533 4.5091328620910645
Loss :  1.7173283100128174 2.7916207313537598 4.508949279785156
Loss :  1.752063512802124 3.0687954425811768 4.820858955383301
Loss :  1.7488189935684204 2.782062530517578 4.530881404876709
Loss :  1.7533468008041382 2.984830617904663 4.738177299499512
Loss :  1.7441458702087402 3.211932420730591 4.95607852935791
  batch 60 loss: 1.7441458702087402, 3.211932420730591, 4.95607852935791
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7490428686141968 3.2485125064849854 4.997555255889893
Loss :  1.7527168989181519 2.4481899738311768 4.200906753540039
Loss :  1.7583979368209839 2.6965839862823486 4.454981803894043
Loss :  1.7394747734069824 2.659064531326294 4.3985395431518555
Loss :  1.7348812818527222 2.1906445026397705 3.925525665283203
Loss :  1.7656689882278442 4.005526065826416 5.771194934844971
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.762838363647461 4.067080974578857 5.829919338226318
Loss :  1.7687346935272217 3.877777576446533 5.646512031555176
Loss :  1.763767957687378 3.774073600769043 5.537841796875
Total LOSS train 4.860796370873085 valid 5.696367025375366
CE LOSS train 1.7447161821218637 valid 0.4409419894218445
Contrastive LOSS train 3.116080192419199 valid 0.9435184001922607
EPOCH 41:
Loss :  1.7329639196395874 3.1625258922576904 4.895489692687988
Loss :  1.7468619346618652 3.458540201187134 5.205402374267578
Loss :  1.7335723638534546 2.675945281982422 4.409517765045166
Loss :  1.7436692714691162 2.7912583351135254 4.5349273681640625
Loss :  1.7658882141113281 2.7124714851379395 4.478359699249268
Loss :  1.7470954656600952 2.7467219829559326 4.493817329406738
Loss :  1.7535816431045532 2.874216318130493 4.627798080444336
Loss :  1.7383029460906982 3.181354284286499 4.919657230377197
Loss :  1.7521380186080933 2.887040615081787 4.63917875289917
Loss :  1.7135794162750244 2.9336061477661133 4.647185325622559
Loss :  1.752174973487854 2.907303810119629 4.659478664398193
Loss :  1.7796605825424194 2.653114080429077 4.432774543762207
Loss :  1.7491093873977661 2.799600124359131 4.548709392547607
Loss :  1.748855710029602 3.484649658203125 5.2335052490234375
Loss :  1.7426457405090332 3.145209789276123 4.887855529785156
Loss :  1.7311142683029175 3.4338722229003906 5.164986610412598
Loss :  1.7505942583084106 3.3868424892425537 5.137436866760254
Loss :  1.7421447038650513 3.553804874420166 5.295949459075928
Loss :  1.7398730516433716 3.275425672531128 5.015298843383789
Loss :  1.737166166305542 3.1693317890167236 4.906497955322266
  batch 20 loss: 1.737166166305542, 3.1693317890167236, 4.906497955322266
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7421551942825317 3.1221272945404053 4.864282608032227
Loss :  1.7492222785949707 3.442805767059326 5.192028045654297
Loss :  1.7429397106170654 3.057914972305298 4.800854682922363
Loss :  1.7619035243988037 2.733520269393921 4.495423793792725
Loss :  1.7588355541229248 2.7906992435455322 4.549534797668457
Loss :  1.738677978515625 2.5937743186950684 4.332452297210693
Loss :  1.7618999481201172 3.5314879417419434 5.2933878898620605
Loss :  1.730381727218628 2.547653913497925 4.278035640716553
Loss :  1.7594228982925415 2.353184461593628 4.112607479095459
Loss :  1.7329652309417725 3.3848648071289062 5.117830276489258
Loss :  1.7821484804153442 3.328568935394287 5.110717296600342
Loss :  1.7500666379928589 2.9316630363464355 4.681729793548584
Loss :  1.7316393852233887 3.1382861137390137 4.869925498962402
Loss :  1.733399510383606 2.6100239753723145 4.343423366546631
Loss :  1.7500284910202026 3.207132577896118 4.957160949707031
Loss :  1.7597072124481201 3.1396384239196777 4.899345397949219
Loss :  1.7502034902572632 2.7841758728027344 4.534379482269287
Loss :  1.7270119190216064 3.382338285446167 5.109350204467773
Loss :  1.7425850629806519 3.4792888164520264 5.221873760223389
Loss :  1.7508220672607422 3.182054281234741 4.9328765869140625
  batch 40 loss: 1.7508220672607422, 3.182054281234741, 4.9328765869140625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7433608770370483 3.3894689083099365 5.132829666137695
Loss :  1.7302038669586182 3.3231747150421143 5.053378582000732
Loss :  1.7445805072784424 3.2530226707458496 4.997603416442871
Loss :  1.741743803024292 3.246333360671997 4.988077163696289
Loss :  1.7490185499191284 3.1606366634368896 4.9096550941467285
Loss :  1.7448148727416992 3.6942050457000732 5.439020156860352
Loss :  1.7368439435958862 3.4822962284088135 5.21914005279541
Loss :  1.7362191677093506 3.4140565395355225 5.150275707244873
Loss :  1.7294113636016846 3.5226986408233643 5.252110004425049
Loss :  1.7477757930755615 3.4810869693756104 5.228862762451172
Loss :  1.730692982673645 3.320449113845825 5.05114221572876
Loss :  1.7281689643859863 3.138070821762085 4.866239547729492
Loss :  1.7461822032928467 3.042762517929077 4.788944721221924
Loss :  1.7533235549926758 3.2436940670013428 4.997017860412598
Loss :  1.744301438331604 3.058469295501709 4.802770614624023
Loss :  1.720048427581787 2.6315486431121826 4.351596832275391
Loss :  1.7550334930419922 2.998812198638916 4.753845691680908
Loss :  1.7501975297927856 2.913684606552124 4.663882255554199
Loss :  1.7574814558029175 3.222109317779541 4.979590892791748
Loss :  1.7451391220092773 2.639781951904297 4.384921073913574
  batch 60 loss: 1.7451391220092773, 2.639781951904297, 4.384921073913574
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7481645345687866 3.1879892349243164 4.936153888702393
Loss :  1.7541688680648804 3.1893503665924072 4.943519115447998
Loss :  1.7553960084915161 3.418487548828125 5.173883438110352
Loss :  1.739862322807312 3.3243327140808105 5.064195156097412
Loss :  1.7368062734603882 2.808367967605591 4.5451741218566895
Loss :  1.7457342147827148 4.215570449829102 5.961304664611816
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.74222731590271 4.306998252868652 6.049225807189941
Loss :  1.7545117139816284 3.958482265472412 5.71299409866333
Loss :  1.7495683431625366 3.938683032989502 5.688251495361328
Total LOSS train 4.85392117867103 valid 5.852944016456604
CE LOSS train 1.7450145886494564 valid 0.43739208579063416
Contrastive LOSS train 3.1089065918555625 valid 0.9846707582473755
EPOCH 42:
Loss :  1.7310833930969238 3.542588949203491 5.273672103881836
Loss :  1.7395575046539307 3.536165475845337 5.275722980499268
Loss :  1.7286913394927979 3.2383856773376465 4.967077255249023
Loss :  1.7360434532165527 3.756209135055542 5.492252349853516
Loss :  1.762967586517334 2.811920404434204 4.574888229370117
Loss :  1.7414873838424683 3.516540288925171 5.25802755355835
Loss :  1.750605821609497 3.1554768085479736 4.906082630157471
Loss :  1.7263273000717163 3.64993953704834 5.376266956329346
Loss :  1.7425651550292969 3.8269760608673096 5.569540977478027
Loss :  1.7062220573425293 3.075833320617676 4.782055377960205
Loss :  1.7392961978912354 3.3062334060668945 5.045529365539551
Loss :  1.7706018686294556 3.4700262546539307 5.240628242492676
Loss :  1.741331934928894 3.7063803672790527 5.447712421417236
Loss :  1.74007248878479 3.3343937397003174 5.074466228485107
Loss :  1.728414535522461 3.1115527153015137 4.839967250823975
Loss :  1.726427674293518 3.391244649887085 5.117672443389893
Loss :  1.7380095720291138 3.7211754322052 5.4591851234436035
Loss :  1.732466459274292 3.032066583633423 4.764533042907715
Loss :  1.7342166900634766 2.759448528289795 4.4936652183532715
Loss :  1.718478798866272 2.4799563884735107 4.198435306549072
  batch 20 loss: 1.718478798866272, 2.4799563884735107, 4.198435306549072
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7265163660049438 2.516958713531494 4.243474960327148
Loss :  1.7346912622451782 2.749319314956665 4.484010696411133
Loss :  1.7339816093444824 2.5846409797668457 4.318622589111328
Loss :  1.7486311197280884 2.7537851333618164 4.502416133880615
Loss :  1.7608438730239868 3.02946400642395 4.790307998657227
Loss :  1.737755298614502 2.7854764461517334 4.523231506347656
Loss :  1.758467674255371 2.5939018726348877 4.35236930847168
Loss :  1.7272982597351074 2.722013235092163 4.449311256408691
Loss :  1.760296106338501 2.793771982192993 4.554068088531494
Loss :  1.736356496810913 3.276275157928467 5.012631416320801
Loss :  1.7804142236709595 3.240278482437134 5.020692825317383
Loss :  1.752856969833374 3.2354393005371094 4.9882965087890625
Loss :  1.7341296672821045 2.8680176734924316 4.602147102355957
Loss :  1.7362544536590576 2.615199327468872 4.35145378112793
Loss :  1.75210440158844 2.567769765853882 4.319874286651611
Loss :  1.7593858242034912 2.856013774871826 4.615399360656738
Loss :  1.7530566453933716 3.029554843902588 4.78261137008667
Loss :  1.7305599451065063 2.803293466567993 4.533853530883789
Loss :  1.7460403442382812 3.1037495136260986 4.849789619445801
Loss :  1.7511178255081177 3.1374049186706543 4.888522624969482
  batch 40 loss: 1.7511178255081177, 3.1374049186706543, 4.888522624969482
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7459291219711304 3.6427695751190186 5.388698577880859
Loss :  1.733668565750122 3.1870882511138916 4.920756816864014
Loss :  1.7449899911880493 3.0731236934661865 4.818113803863525
Loss :  1.749118685722351 3.4601309299468994 5.209249496459961
Loss :  1.7480279207229614 3.2698426246643066 5.0178704261779785
Loss :  1.7461787462234497 3.8372159004211426 5.583394527435303
Loss :  1.7431761026382446 3.337416172027588 5.080592155456543
Loss :  1.7360259294509888 3.29221773147583 5.028243541717529
Loss :  1.7354736328125 3.5724611282348633 5.307934761047363
Loss :  1.749839425086975 3.8214404582977295 5.571280002593994
Loss :  1.7264856100082397 3.698413610458374 5.424899101257324
Loss :  1.7251673936843872 3.2068960666656494 4.932063579559326
Loss :  1.7469594478607178 3.2681801319122314 5.015139579772949
Loss :  1.746311068534851 3.2491023540496826 4.995413303375244
Loss :  1.7375999689102173 3.343020439147949 5.080620288848877
Loss :  1.7180614471435547 3.06133770942688 4.7793989181518555
Loss :  1.7515164613723755 3.23453426361084 4.986050605773926
Loss :  1.7440491914749146 3.0852274894714355 4.8292765617370605
Loss :  1.7479422092437744 3.118821859359741 4.866764068603516
Loss :  1.7411695718765259 2.8955793380737305 4.636748790740967
  batch 60 loss: 1.7411695718765259, 2.8955793380737305, 4.636748790740967
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7448517084121704 2.5785484313964844 4.323400020599365
Loss :  1.7480603456497192 2.978614568710327 4.726675033569336
Loss :  1.7538777589797974 2.645197868347168 4.399075508117676
Loss :  1.7338347434997559 3.4231772422790527 5.157011985778809
Loss :  1.72786545753479 1.9729065895080566 3.7007720470428467
Loss :  1.8070223331451416 4.348222732543945 6.155244827270508
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.801969289779663 4.319189071655273 6.121158599853516
Loss :  1.7972197532653809 4.315739154815674 6.112958908081055
Loss :  1.7799047231674194 4.165913105010986 5.945817947387695
Total LOSS train 4.878767838844886 valid 6.083795070648193
CE LOSS train 1.7412585551922137 valid 0.44497618079185486
Contrastive LOSS train 3.137509324000432 valid 1.0414782762527466
EPOCH 43:
Loss :  1.7296879291534424 2.551494836807251 4.281182765960693
Loss :  1.7394938468933105 2.836564540863037 4.576058387756348
Loss :  1.7257483005523682 2.9446845054626465 4.670433044433594
Loss :  1.7315819263458252 3.1773478984832764 4.908929824829102
Loss :  1.762394905090332 2.9527862071990967 4.715181350708008
Loss :  1.7457858324050903 2.9816536903381348 4.7274394035339355
Loss :  1.7454712390899658 3.1713814735412598 4.916852951049805
Loss :  1.7335631847381592 2.7772183418273926 4.510781288146973
Loss :  1.7512967586517334 2.8239171504974365 4.57521390914917
Loss :  1.713980793952942 2.3935232162475586 4.107503890991211
Loss :  1.7519786357879639 2.6610846519470215 4.413063049316406
Loss :  1.779776930809021 2.664238929748535 4.444015979766846
Loss :  1.7486507892608643 2.4993176460266113 4.247968673706055
Loss :  1.7477240562438965 2.727452039718628 4.475175857543945
Loss :  1.7410809993743896 3.185778856277466 4.9268598556518555
Loss :  1.7312146425247192 2.666980028152466 4.398194789886475
Loss :  1.7504088878631592 3.664989948272705 5.415398597717285
Loss :  1.7414745092391968 3.8235747814178467 5.565049171447754
Loss :  1.740590214729309 2.9553568363189697 4.695947170257568
Loss :  1.7260242700576782 3.4287781715393066 5.154802322387695
  batch 20 loss: 1.7260242700576782, 3.4287781715393066, 5.154802322387695
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7329989671707153 2.940892219543457 4.673891067504883
Loss :  1.7392898797988892 3.2637391090393066 5.003028869628906
Loss :  1.7354320287704468 3.179141044616699 4.9145731925964355
Loss :  1.7531259059906006 3.8077876567840576 5.560913562774658
Loss :  1.7573336362838745 3.499197483062744 5.256531238555908
Loss :  1.738072156906128 3.380711793899536 5.118783950805664
Loss :  1.7607754468917847 3.240051746368408 5.000827312469482
Loss :  1.728018879890442 3.142582893371582 4.870601654052734
Loss :  1.7635964155197144 3.3970627784729004 5.160659313201904
Loss :  1.7363337278366089 3.2943103313446045 5.030643939971924
Loss :  1.7814513444900513 2.857490062713623 4.638941287994385
Loss :  1.7512246370315552 2.4666433334350586 4.217867851257324
Loss :  1.7357372045516968 2.363431215286255 4.099168300628662
Loss :  1.7370260953903198 2.494513988494873 4.231540203094482
Loss :  1.75052809715271 2.6071279048919678 4.357656002044678
Loss :  1.760265588760376 3.2403151988983154 5.000580787658691
Loss :  1.7502293586730957 2.9097683429718018 4.659997940063477
Loss :  1.7272588014602661 2.9594085216522217 4.686667442321777
Loss :  1.7399438619613647 3.2087464332580566 4.948690414428711
Loss :  1.747815728187561 2.535707950592041 4.2835235595703125
  batch 40 loss: 1.747815728187561, 2.535707950592041, 4.2835235595703125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7393791675567627 2.919651985168457 4.659030914306641
Loss :  1.7281718254089355 2.9432461261749268 4.671418190002441
Loss :  1.7442553043365479 3.2410457134246826 4.9853010177612305
Loss :  1.7448928356170654 3.3855741024017334 5.130466938018799
Loss :  1.7455646991729736 2.967381238937378 4.712945938110352
Loss :  1.7422702312469482 2.7286503314971924 4.470920562744141
Loss :  1.7334973812103271 2.68230938911438 4.415806770324707
Loss :  1.7375543117523193 2.9073920249938965 4.644946098327637
Loss :  1.7266902923583984 3.0627450942993164 4.789435386657715
Loss :  1.7484606504440308 2.7419049739837646 4.490365505218506
Loss :  1.7295341491699219 2.8677847385406494 4.597318649291992
Loss :  1.726684808731079 2.8323473930358887 4.559032440185547
Loss :  1.7442877292633057 2.706834316253662 4.451122283935547
Loss :  1.7465957403182983 2.853938102722168 4.600533962249756
Loss :  1.7434237003326416 2.562389373779297 4.305812835693359
Loss :  1.725813627243042 2.540156364440918 4.265970230102539
Loss :  1.7554103136062622 3.176517963409424 4.9319281578063965
Loss :  1.7478628158569336 3.452129364013672 5.1999921798706055
Loss :  1.7533011436462402 3.1975810527801514 4.9508819580078125
Loss :  1.7502764463424683 2.8887906074523926 4.63906717300415
  batch 60 loss: 1.7502764463424683, 2.8887906074523926, 4.63906717300415
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7492787837982178 3.036670446395874 4.785949230194092
Loss :  1.7546865940093994 2.8310139179229736 4.585700511932373
Loss :  1.757893681526184 3.3179891109466553 5.075882911682129
Loss :  1.7383668422698975 3.299426794052124 5.0377936363220215
Loss :  1.7383944988250732 3.3308377265930176 5.069231986999512
Loss :  1.7983862161636353 4.280722141265869 6.079108238220215
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7910606861114502 4.310549736022949 6.10161018371582
Loss :  1.7836825847625732 4.229290008544922 6.012972831726074
Loss :  1.7754099369049072 4.122892379760742 5.89830207824707
Total LOSS train 4.730276856055626 valid 6.022998332977295
CE LOSS train 1.7433374459926898 valid 0.4438524842262268
Contrastive LOSS train 2.986939415564904 valid 1.0307230949401855
EPOCH 44:
Loss :  1.7315701246261597 3.6733059883117676 5.404876232147217
Loss :  1.7399235963821411 3.664088726043701 5.404012203216553
Loss :  1.7304505109786987 3.1064040660858154 4.836854457855225
Loss :  1.7410246133804321 3.372358798980713 5.1133832931518555
Loss :  1.7605637311935425 3.463390827178955 5.223954677581787
Loss :  1.7389605045318604 3.136857748031616 4.875818252563477
Loss :  1.7470159530639648 3.2033395767211914 4.950355529785156
Loss :  1.7316113710403442 3.9870924949645996 5.718703746795654
Loss :  1.7441692352294922 3.51335072517395 5.257519721984863
Loss :  1.705532431602478 3.5122878551483154 5.217820167541504
Loss :  1.7450871467590332 3.909585475921631 5.654672622680664
Loss :  1.7678110599517822 3.7121503353118896 5.479961395263672
Loss :  1.7450743913650513 3.3850817680358887 5.13015604019165
Loss :  1.7418487071990967 3.3096048831939697 5.051453590393066
Loss :  1.7318655252456665 3.144183397293091 4.876049041748047
Loss :  1.7249950170516968 2.9902517795562744 4.715246677398682
Loss :  1.7411918640136719 2.9141669273376465 4.655358791351318
Loss :  1.7413502931594849 3.5927774906158447 5.334127902984619
Loss :  1.7326843738555908 2.612212657928467 4.344897270202637
Loss :  1.7228891849517822 2.4202044010162354 4.143093585968018
  batch 20 loss: 1.7228891849517822, 2.4202044010162354, 4.143093585968018
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7308554649353027 2.640434503555298 4.37129020690918
Loss :  1.7372424602508545 2.680344820022583 4.4175872802734375
Loss :  1.7356549501419067 2.775892972946167 4.511548042297363
Loss :  1.7498021125793457 3.0431735515594482 4.792975425720215
Loss :  1.7578396797180176 2.656647205352783 4.414486885070801
Loss :  1.7347556352615356 2.6701011657714844 4.4048566818237305
Loss :  1.75628662109375 2.8028347492218018 4.559121131896973
Loss :  1.7253623008728027 2.4886889457702637 4.214051246643066
Loss :  1.7526637315750122 2.89992618560791 4.652589797973633
Loss :  1.729729175567627 3.270547389984131 5.000276565551758
Loss :  1.778607964515686 3.307443857192993 5.086051940917969
Loss :  1.7474417686462402 3.585557699203491 5.332999229431152
Loss :  1.7269976139068604 3.2069664001464844 4.933963775634766
Loss :  1.725556492805481 3.3325395584106445 5.058095932006836
Loss :  1.747057318687439 3.32305908203125 5.0701165199279785
Loss :  1.7542084455490112 3.894658327102661 5.648866653442383
Loss :  1.7433449029922485 3.3776211738586426 5.120965957641602
Loss :  1.7166283130645752 2.790044069290161 4.506672382354736
Loss :  1.736478328704834 3.320706367492676 5.05718469619751
Loss :  1.7423783540725708 2.6833138465881348 4.425692081451416
  batch 40 loss: 1.7423783540725708, 2.6833138465881348, 4.425692081451416
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7367675304412842 2.9395976066589355 4.676364898681641
Loss :  1.7304468154907227 2.6773953437805176 4.40784215927124
Loss :  1.7441800832748413 2.5621707439422607 4.3063507080078125
Loss :  1.745051383972168 2.7365689277648926 4.4816203117370605
Loss :  1.7461540699005127 2.357276439666748 4.10343074798584
Loss :  1.7447450160980225 2.884406328201294 4.629151344299316
Loss :  1.7367528676986694 2.6262876987457275 4.363040447235107
Loss :  1.7408617734909058 2.7409427165985107 4.481804370880127
Loss :  1.732234001159668 2.5410380363464355 4.2732720375061035
Loss :  1.7480000257492065 2.9568026065826416 4.704802513122559
Loss :  1.730936050415039 3.588512659072876 5.319448471069336
Loss :  1.7281461954116821 3.6754150390625 5.403561115264893
Loss :  1.7470849752426147 3.2960314750671387 5.043116569519043
Loss :  1.7472124099731445 2.8510067462921143 4.59821891784668
Loss :  1.7468537092208862 3.1382980346679688 4.8851518630981445
Loss :  1.7275816202163696 3.1017425060272217 4.829324245452881
Loss :  1.7533446550369263 3.8908638954162598 5.6442084312438965
Loss :  1.749363660812378 3.4216508865356445 5.171014785766602
Loss :  1.751395344734192 3.73988938331604 5.4912848472595215
Loss :  1.748832106590271 3.672254800796509 5.42108678817749
  batch 60 loss: 1.748832106590271, 3.672254800796509, 5.42108678817749
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.750126838684082 3.529555082321167 5.279682159423828
Loss :  1.7517951726913452 3.5084335803985596 5.260228633880615
Loss :  1.7602232694625854 3.370436429977417 5.130659580230713
Loss :  1.737466812133789 3.0819571018218994 4.819423675537109
Loss :  1.7318907976150513 3.0924127101898193 4.82430362701416
Loss :  1.7925492525100708 4.184452533721924 5.977001667022705
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.787526249885559 4.188401222229004 5.975927352905273
Loss :  1.783898949623108 3.946340799331665 5.7302398681640625
Loss :  1.7822011709213257 4.022514820098877 5.804716110229492
Total LOSS train 4.900247705899752 valid 5.871971249580383
CE LOSS train 1.7409532070159912 valid 0.4455502927303314
Contrastive LOSS train 3.159294531895564 valid 1.0056287050247192
EPOCH 45:
Loss :  1.7334792613983154 2.8062896728515625 4.539769172668457
Loss :  1.7436420917510986 2.7292702198028564 4.472912311553955
Loss :  1.729110836982727 2.5270636081695557 4.256174564361572
Loss :  1.7374320030212402 2.67287015914917 4.41030216217041
Loss :  1.7618892192840576 2.732327461242676 4.4942169189453125
Loss :  1.7460684776306152 2.7741668224334717 4.520235061645508
Loss :  1.744826316833496 2.447951555252075 4.192777633666992
Loss :  1.7344439029693604 2.661670446395874 4.396114349365234
Loss :  1.7502950429916382 2.6785385608673096 4.428833484649658
Loss :  1.7112853527069092 2.6823525428771973 4.393637657165527
Loss :  1.7475589513778687 2.8441247940063477 4.591683864593506
Loss :  1.775785207748413 3.101891040802002 4.877676010131836
Loss :  1.7463899850845337 2.8937771320343018 4.640167236328125
Loss :  1.7436107397079468 3.256028175354004 4.99963903427124
Loss :  1.732869029045105 3.044114112854004 4.776983261108398
Loss :  1.7282073497772217 2.8257529735565186 4.55396032333374
Loss :  1.7456203699111938 2.9807281494140625 4.726348400115967
Loss :  1.7367405891418457 2.8781096935272217 4.614850044250488
Loss :  1.7407039403915405 2.458890438079834 4.199594497680664
Loss :  1.7250831127166748 2.5555412769317627 4.2806243896484375
  batch 20 loss: 1.7250831127166748, 2.5555412769317627, 4.2806243896484375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7316726446151733 2.7542922496795654 4.485964775085449
Loss :  1.7399933338165283 2.8721072673797607 4.612100601196289
Loss :  1.7321950197219849 2.5083017349243164 4.240496635437012
Loss :  1.7490659952163696 3.0569889545440674 4.806055068969727
Loss :  1.7563201189041138 3.538696765899658 5.295016765594482
Loss :  1.7355531454086304 3.1545016765594482 4.890054702758789
Loss :  1.7554343938827515 3.414280891418457 5.169715404510498
Loss :  1.721293330192566 3.33168625831604 5.052979469299316
Loss :  1.7547496557235718 3.392533779144287 5.147283554077148
Loss :  1.7312393188476562 3.0517544746398926 4.782993793487549
Loss :  1.7765636444091797 2.8237404823303223 4.600304126739502
Loss :  1.7433890104293823 3.152400493621826 4.895789623260498
Loss :  1.722749948501587 2.9516477584838867 4.6743974685668945
Loss :  1.7280750274658203 2.9788708686828613 4.706945896148682
Loss :  1.7431268692016602 2.6182143688201904 4.36134147644043
Loss :  1.7524147033691406 2.7627437114715576 4.515158653259277
Loss :  1.7420283555984497 2.6391193866729736 4.381147861480713
Loss :  1.7153024673461914 3.2278175354003906 4.943120002746582
Loss :  1.7280449867248535 3.0993828773498535 4.827427864074707
Loss :  1.7397539615631104 2.941077947616577 4.6808319091796875
  batch 40 loss: 1.7397539615631104, 2.941077947616577, 4.6808319091796875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.73170804977417 3.3197076320648193 5.05141544342041
Loss :  1.7238810062408447 3.1719751358032227 4.895855903625488
Loss :  1.7384612560272217 3.561915874481201 5.300376892089844
Loss :  1.7396951913833618 3.3249247074127197 5.064620018005371
Loss :  1.7419824600219727 3.5343027114868164 5.276285171508789
Loss :  1.7370136976242065 2.8785581588745117 4.615571975708008
Loss :  1.7265918254852295 3.0364644527435303 4.76305627822876
Loss :  1.7282483577728271 2.9729397296905518 4.701188087463379
Loss :  1.7229915857315063 3.180184841156006 4.903176307678223
Loss :  1.742983102798462 3.0610759258270264 4.804059028625488
Loss :  1.7198290824890137 3.4051613807678223 5.124990463256836
Loss :  1.7179511785507202 3.171166181564331 4.889117240905762
Loss :  1.7396763563156128 3.047640323638916 4.787316799163818
Loss :  1.7445377111434937 3.4035706520080566 5.14810848236084
Loss :  1.7337435483932495 3.706611394882202 5.440354824066162
Loss :  1.717519760131836 3.1919045448303223 4.909424304962158
Loss :  1.7486423254013062 3.5944085121154785 5.343050956726074
Loss :  1.7378196716308594 3.1113502979278564 4.849169731140137
Loss :  1.7448277473449707 3.7404205799102783 5.485248565673828
Loss :  1.7423224449157715 2.6973471641540527 4.439669609069824
  batch 60 loss: 1.7423224449157715, 2.6973471641540527, 4.439669609069824
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7431964874267578 2.8030953407287598 4.546291828155518
Loss :  1.7451287508010864 2.976363182067871 4.721491813659668
Loss :  1.748963713645935 2.8239619731903076 4.572925567626953
Loss :  1.7287970781326294 2.6651947498321533 4.393991947174072
Loss :  1.7213490009307861 3.259915590286255 4.981264591217041
Loss :  1.7807719707489014 4.295032024383545 6.075803756713867
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.775558590888977 4.284394264221191 6.059952735900879
Loss :  1.767256498336792 4.217772960662842 5.985029220581055
Loss :  1.7629810571670532 4.16560173034668 5.928582668304443
Total LOSS train 4.745286890176627 valid 6.012342095375061
CE LOSS train 1.738182601561913 valid 0.4407452642917633
Contrastive LOSS train 3.0071042977846587 valid 1.04140043258667
EPOCH 46:
Loss :  1.7201032638549805 3.170146942138672 4.890250205993652
Loss :  1.7282824516296387 3.238835573196411 4.967118263244629
Loss :  1.7181190252304077 2.5052459239959717 4.22336483001709
Loss :  1.7313214540481567 2.401926279067993 4.1332478523254395
Loss :  1.7527695894241333 2.6248931884765625 4.377662658691406
Loss :  1.7354942560195923 2.756613254547119 4.492107391357422
Loss :  1.7402204275131226 2.9697470664978027 4.709967613220215
Loss :  1.7254629135131836 2.944546699523926 4.670009613037109
Loss :  1.7425423860549927 3.4054415225982666 5.147984027862549
Loss :  1.7030858993530273 3.1007604598999023 4.80384635925293
Loss :  1.7406673431396484 3.385493755340576 5.126161098480225
Loss :  1.765956163406372 3.7216033935546875 5.4875593185424805
Loss :  1.739498257637024 3.1370606422424316 4.876558780670166
Loss :  1.7381863594055176 3.1913492679595947 4.929535865783691
Loss :  1.726309895515442 3.0655229091644287 4.79183292388916
Loss :  1.7187343835830688 3.317054271697998 5.035788536071777
Loss :  1.74301278591156 2.8031651973724365 4.546177864074707
Loss :  1.733777403831482 2.7603132724761963 4.494090557098389
Loss :  1.73345148563385 2.6586146354675293 4.39206600189209
Loss :  1.7198342084884644 3.330671787261963 5.050506114959717
  batch 20 loss: 1.7198342084884644, 3.330671787261963, 5.050506114959717
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7291492223739624 2.365959882736206 4.095108985900879
Loss :  1.7390027046203613 2.923050880432129 4.66205358505249
Loss :  1.7327953577041626 3.033886194229126 4.766681671142578
Loss :  1.7449556589126587 2.8362724781036377 4.581228256225586
Loss :  1.7555675506591797 3.2498793601989746 5.005446910858154
Loss :  1.7360461950302124 3.015934944152832 4.751981258392334
Loss :  1.7558650970458984 2.9372401237487793 4.693105220794678
Loss :  1.7211250066757202 2.7442612648010254 4.465386390686035
Loss :  1.7568196058273315 2.9202799797058105 4.677099704742432
Loss :  1.7323405742645264 2.871591091156006 4.603931427001953
Loss :  1.775988221168518 3.1039586067199707 4.879946708679199
Loss :  1.75009024143219 3.35491681098938 5.105007171630859
Loss :  1.7322314977645874 3.4753148555755615 5.207546234130859
Loss :  1.7314146757125854 3.2370734214782715 4.9684882164001465
Loss :  1.7471803426742554 3.540886640548706 5.288066864013672
Loss :  1.754099726676941 3.1762282848358154 4.930327892303467
Loss :  1.7503948211669922 2.9713902473449707 4.721785068511963
Loss :  1.723371148109436 3.492558240890503 5.2159295082092285
Loss :  1.7425707578659058 3.4043166637420654 5.146887302398682
Loss :  1.744066596031189 3.820342779159546 5.564409255981445
  batch 40 loss: 1.744066596031189, 3.820342779159546, 5.564409255981445
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7385579347610474 3.308410406112671 5.046968460083008
Loss :  1.7239586114883423 2.748650074005127 4.47260856628418
Loss :  1.7376065254211426 2.4452080726623535 4.182814598083496
Loss :  1.734113097190857 2.951850652694702 4.6859636306762695
Loss :  1.7397555112838745 2.6175129413604736 4.357268333435059
Loss :  1.7366336584091187 2.707566261291504 4.444200038909912
Loss :  1.731365442276001 2.4273924827575684 4.158758163452148
Loss :  1.730893611907959 2.2718241214752197 4.002717971801758
Loss :  1.7287136316299438 2.3350272178649902 4.0637407302856445
Loss :  1.7422791719436646 2.3968818187713623 4.139161109924316
Loss :  1.726131796836853 2.5006396770477295 4.226771354675293
Loss :  1.727683424949646 2.5208888053894043 4.24857234954834
Loss :  1.7435728311538696 2.177518367767334 3.921091079711914
Loss :  1.7431896924972534 2.3100390434265137 4.053228855133057
Loss :  1.7393518686294556 2.546949625015259 4.286301612854004
Loss :  1.7221730947494507 2.2353739738464355 3.957547187805176
Loss :  1.749894618988037 2.583932638168335 4.333827018737793
Loss :  1.7432684898376465 2.536043405532837 4.2793121337890625
Loss :  1.7480336427688599 3.0063529014587402 4.7543864250183105
Loss :  1.7479476928710938 2.6046366691589355 4.352584362030029
  batch 60 loss: 1.7479476928710938, 2.6046366691589355, 4.352584362030029
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7423235177993774 3.074742317199707 4.817065715789795
Loss :  1.7498780488967896 3.386996030807495 5.136874198913574
Loss :  1.7498910427093506 3.694263458251953 5.444154739379883
Loss :  1.7352449893951416 4.124362945556641 5.859607696533203
Loss :  1.7271074056625366 3.5900349617004395 5.317142486572266
Loss :  1.7586512565612793 4.12769889831543 5.886350154876709
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7511588335037231 4.1570000648498535 5.908158779144287
Loss :  1.752069115638733 3.9727652072906494 5.724834442138672
Loss :  1.7242404222488403 3.923649311065674 5.647889614105225
Total LOSS train 4.692598805060753 valid 5.791808247566223
CE LOSS train 1.7377149893687323 valid 0.4310601055622101
Contrastive LOSS train 2.9548838101900543 valid 0.9809123277664185
EPOCH 47:
Loss :  1.7253961563110352 4.052171230316162 5.777567386627197
Loss :  1.727379322052002 3.53147554397583 5.258854866027832
Loss :  1.7181931734085083 3.359471082687378 5.077664375305176
Loss :  1.729650855064392 3.40122652053833 5.130877494812012
Loss :  1.7510040998458862 3.5967695713043213 5.347773551940918
Loss :  1.7320454120635986 3.1789422035217285 4.910987854003906
Loss :  1.7348613739013672 3.494631052017212 5.2294921875
Loss :  1.7177677154541016 3.7340712547302246 5.451838970184326
Loss :  1.7364166975021362 3.6994426250457764 5.435859203338623
Loss :  1.6981431245803833 3.5898377895355225 5.287981033325195
Loss :  1.7320948839187622 3.9665677547454834 5.698662757873535
Loss :  1.7628629207611084 4.075608253479004 5.838471412658691
Loss :  1.732650876045227 3.351752758026123 5.0844035148620605
Loss :  1.7298295497894287 3.396235466003418 5.126065254211426
Loss :  1.7234020233154297 2.7998337745666504 4.52323579788208
Loss :  1.716475486755371 2.7467663288116455 4.4632415771484375
Loss :  1.7328399419784546 2.6829864978790283 4.415826320648193
Loss :  1.7273308038711548 2.4086289405822754 4.135959625244141
Loss :  1.7235252857208252 2.434235095977783 4.1577606201171875
Loss :  1.7162116765975952 2.6500372886657715 4.366249084472656
  batch 20 loss: 1.7162116765975952, 2.6500372886657715, 4.366249084472656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7222881317138672 2.465214729309082 4.187502861022949
Loss :  1.727090835571289 2.4582359790802 4.18532657623291
Loss :  1.7266136407852173 2.0993924140930176 3.8260059356689453
Loss :  1.741955041885376 2.3287644386291504 4.0707197189331055
Loss :  1.7532641887664795 2.7829794883728027 4.536243438720703
Loss :  1.7276462316513062 2.45369815826416 4.181344509124756
Loss :  1.7491120100021362 2.805579662322998 4.554691791534424
Loss :  1.7189606428146362 2.7296197414398193 4.448580265045166
Loss :  1.750309944152832 2.905235767364502 4.655545711517334
Loss :  1.7284438610076904 3.1363327503204346 4.864776611328125
Loss :  1.7740731239318848 2.9725260734558105 4.746599197387695
Loss :  1.7454607486724854 2.590738534927368 4.3361992835998535
Loss :  1.7298245429992676 2.8398027420043945 4.569627285003662
Loss :  1.7300752401351929 3.0330960750579834 4.763171195983887
Loss :  1.7459222078323364 2.862347364425659 4.608269691467285
Loss :  1.75454580783844 3.2327113151550293 4.98725700378418
Loss :  1.748991847038269 2.4620919227600098 4.211083889007568
Loss :  1.7252873182296753 2.6013801097869873 4.326667308807373
Loss :  1.7448618412017822 3.2862088680267334 5.031070709228516
Loss :  1.7426822185516357 3.332759380340576 5.075441360473633
  batch 40 loss: 1.7426822185516357, 3.332759380340576, 5.075441360473633
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7374448776245117 3.7202723026275635 5.457716941833496
Loss :  1.7269047498703003 3.8265645503997803 5.553469181060791
Loss :  1.739070177078247 2.860603094100952 4.599673271179199
Loss :  1.7333348989486694 2.853336811065674 4.586671829223633
Loss :  1.7409374713897705 2.8785159587860107 4.619453430175781
Loss :  1.7376209497451782 2.6955647468566895 4.433185577392578
Loss :  1.733647346496582 2.651143789291382 4.384791374206543
Loss :  1.7378660440444946 2.783958911895752 4.521824836730957
Loss :  1.7305108308792114 2.9518213272094727 4.6823320388793945
Loss :  1.7436283826828003 2.709510087966919 4.45313835144043
Loss :  1.7299296855926514 2.751584768295288 4.4815144538879395
Loss :  1.7335045337677002 2.478905439376831 4.212409973144531
Loss :  1.743665099143982 2.756751298904419 4.500416278839111
Loss :  1.7394944429397583 2.796520948410034 4.536015510559082
Loss :  1.747436285018921 2.9034664630889893 4.65090274810791
Loss :  1.726407527923584 3.2965102195739746 5.022917747497559
Loss :  1.7552874088287354 3.5858218669891357 5.341109275817871
Loss :  1.7479240894317627 3.2809646129608154 5.028888702392578
Loss :  1.7515530586242676 3.0545167922973633 4.806069850921631
Loss :  1.7474896907806396 2.8583881855010986 4.605877876281738
  batch 60 loss: 1.7474896907806396, 2.8583881855010986, 4.605877876281738
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7416595220565796 2.9819161891937256 4.723575592041016
Loss :  1.750848650932312 2.6260557174682617 4.376904487609863
Loss :  1.7493175268173218 3.0444138050079346 4.793731212615967
Loss :  1.7347667217254639 2.6581614017486572 4.392928123474121
Loss :  1.7347644567489624 2.5517423152923584 4.286506652832031
Loss :  1.7692601680755615 4.32120418548584 6.0904645919799805
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.771398901939392 4.327163219451904 6.098562240600586
Loss :  1.7659924030303955 4.2760820388793945 6.042074203491211
Loss :  1.748323678970337 4.158034324645996 5.906357765197754
Total LOSS train 4.737491116156945 valid 6.034364700317383
CE LOSS train 1.7361616189663227 valid 0.43708091974258423
Contrastive LOSS train 3.0013295100285458 valid 1.039508581161499
EPOCH 48:
Loss :  1.736021637916565 2.3496968746185303 4.085718631744385
Loss :  1.7340810298919678 2.607795476913452 4.34187650680542
Loss :  1.7236289978027344 2.425562858581543 4.149191856384277
Loss :  1.7302861213684082 3.0257744789123535 4.756060600280762
Loss :  1.7582777738571167 2.642993688583374 4.401271343231201
Loss :  1.7360312938690186 2.7308459281921387 4.466876983642578
Loss :  1.739821195602417 2.549834966659546 4.289656162261963
Loss :  1.7243380546569824 2.3082990646362305 4.032637119293213
Loss :  1.7415350675582886 2.3417816162109375 4.083316802978516
Loss :  1.7059587240219116 2.4907777309417725 4.1967363357543945
Loss :  1.7347575426101685 2.673405170440674 4.408162593841553
Loss :  1.765081763267517 2.62239146232605 4.387473106384277
Loss :  1.7332690954208374 3.391132116317749 5.124401092529297
Loss :  1.7348098754882812 2.8110814094543457 4.545891284942627
Loss :  1.728297233581543 2.7747788429260254 4.503076076507568
Loss :  1.7234313488006592 2.7911691665649414 4.51460075378418
Loss :  1.7341254949569702 2.7131154537200928 4.447240829467773
Loss :  1.7264673709869385 2.7165307998657227 4.442997932434082
Loss :  1.7322098016738892 2.791358470916748 4.523568153381348
Loss :  1.7132596969604492 2.9492578506469727 4.662517547607422
  batch 20 loss: 1.7132596969604492, 2.9492578506469727, 4.662517547607422
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.726450800895691 3.407503366470337 5.133954048156738
Loss :  1.7322452068328857 3.391782283782959 5.124027252197266
Loss :  1.7247377634048462 3.6709792613983154 5.395717144012451
Loss :  1.7349681854248047 3.3880631923675537 5.1230316162109375
Loss :  1.7515581846237183 3.3723013401031494 5.123859405517578
Loss :  1.7308512926101685 3.209439516067505 4.940290927886963
Loss :  1.7451763153076172 3.331028461456299 5.076204776763916
Loss :  1.7107853889465332 2.7802255153656006 4.491010665893555
Loss :  1.7474931478500366 2.4691274166107178 4.216620445251465
Loss :  1.727951169013977 2.859874725341797 4.587825775146484
Loss :  1.774930477142334 3.2251462936401367 5.000076770782471
Loss :  1.7382384538650513 3.321852922439575 5.060091495513916
Loss :  1.7225821018218994 3.340839385986328 5.063421249389648
Loss :  1.7264481782913208 3.66756534576416 5.394013404846191
Loss :  1.7420498132705688 3.4712331295013428 5.213283061981201
Loss :  1.750705599784851 2.935295343399048 4.686000823974609
Loss :  1.7373206615447998 3.0998501777648926 4.837170600891113
Loss :  1.7063095569610596 3.0244572162628174 4.730766773223877
Loss :  1.7321686744689941 3.140418529510498 4.872587203979492
Loss :  1.7357033491134644 2.5417027473449707 4.277406215667725
  batch 40 loss: 1.7357033491134644, 2.5417027473449707, 4.277406215667725
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.729435682296753 3.1766536235809326 4.9060893058776855
Loss :  1.7236912250518799 3.276496410369873 5.000187873840332
Loss :  1.740000605583191 3.3626091480255127 5.102609634399414
Loss :  1.7329225540161133 3.73457932472229 5.467501640319824
Loss :  1.739033579826355 3.8126754760742188 5.551709175109863
Loss :  1.736328363418579 2.839398145675659 4.575726509094238
Loss :  1.7246835231781006 3.116096258163452 4.840779781341553
Loss :  1.7329047918319702 3.553292751312256 5.286197662353516
Loss :  1.721938967704773 3.4372072219848633 5.159146308898926
Loss :  1.7422853708267212 3.240229368209839 4.98251485824585
Loss :  1.7275724411010742 3.0109047889709473 4.7384772300720215
Loss :  1.7271133661270142 3.057840585708618 4.784954071044922
Loss :  1.7384499311447144 3.4876561164855957 5.2261061668396
Loss :  1.7429455518722534 3.000523328781128 4.743468761444092
Loss :  1.739673137664795 3.009821891784668 4.749495029449463
Loss :  1.7107125520706177 3.3021535873413086 5.012866020202637
Loss :  1.7471064329147339 2.8206403255462646 4.567746639251709
Loss :  1.7405744791030884 2.495197057723999 4.235771656036377
Loss :  1.745855450630188 2.877697467803955 4.6235527992248535
Loss :  1.7371197938919067 2.54361891746521 4.280738830566406
  batch 60 loss: 1.7371197938919067, 2.54361891746521, 4.280738830566406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7365187406539917 2.622100353240967 4.358619213104248
Loss :  1.7426174879074097 2.513458490371704 4.256075859069824
Loss :  1.742491364479065 2.1379640102386475 3.880455493927002
Loss :  1.7295838594436646 2.6299033164978027 4.359487056732178
Loss :  1.7255070209503174 2.0319149494171143 3.7574219703674316
Loss :  1.7489675283432007 4.193460464477539 5.942428112030029
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7486618757247925 4.197731971740723 5.946393966674805
Loss :  1.746170163154602 3.964099884033203 5.710269927978516
Loss :  1.7197141647338867 4.066732883453369 5.786447048187256
Total LOSS train 4.694743552574745 valid 5.846384763717651
CE LOSS train 1.7340219956177931 valid 0.4299285411834717
Contrastive LOSS train 2.960721577130831 valid 1.0166832208633423
EPOCH 49:
Loss :  1.725918173789978 3.312208414077759 5.038126468658447
Loss :  1.7269209623336792 3.0085878372192383 4.735508918762207
Loss :  1.7188047170639038 2.811701536178589 4.530506134033203
Loss :  1.7309173345565796 2.852642059326172 4.583559513092041
Loss :  1.753371238708496 3.0500290393829346 4.803400039672852
Loss :  1.7347605228424072 2.9082586765289307 4.643019199371338
Loss :  1.734728217124939 2.91507887840271 4.649806976318359
Loss :  1.723365306854248 2.5470786094665527 4.270443916320801
Loss :  1.740572214126587 2.6966395378112793 4.437211990356445
Loss :  1.7025173902511597 2.6367123126983643 4.339229583740234
Loss :  1.7350130081176758 3.2044050693511963 4.939417839050293
Loss :  1.7644248008728027 3.127800464630127 4.89222526550293
Loss :  1.7343813180923462 3.4478490352630615 5.182230472564697
Loss :  1.7362169027328491 3.4563207626342773 5.192537784576416
Loss :  1.728844165802002 3.2302393913269043 4.959083557128906
Loss :  1.720484733581543 3.540602922439575 5.261087417602539
Loss :  1.7336474657058716 3.479769706726074 5.213417053222656
Loss :  1.7319709062576294 3.107764720916748 4.839735507965088
Loss :  1.7259128093719482 3.1764745712280273 4.902387619018555
Loss :  1.7149357795715332 3.582275390625 5.297211170196533
  batch 20 loss: 1.7149357795715332, 3.582275390625, 5.297211170196533
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.726636528968811 2.9221556186676025 4.648792266845703
Loss :  1.7276281118392944 3.478809356689453 5.206437587738037
Loss :  1.7286453247070312 2.8339908123016357 4.562636375427246
Loss :  1.7399526834487915 2.560511350631714 4.300464153289795
Loss :  1.7466763257980347 2.927644968032837 4.674321174621582
Loss :  1.728375792503357 3.191070556640625 4.9194464683532715
Loss :  1.748405933380127 3.688467502593994 5.436873435974121
Loss :  1.715601921081543 2.768601894378662 4.484203815460205
Loss :  1.7489290237426758 2.29256010055542 4.041489124298096
Loss :  1.7207714319229126 3.0696678161621094 4.790439128875732
Loss :  1.7700979709625244 2.773303985595703 4.543401718139648
Loss :  1.737588882446289 2.6044065952301025 4.3419952392578125
Loss :  1.7190966606140137 3.3750674724578857 5.09416389465332
Loss :  1.7243154048919678 3.470604658126831 5.194920063018799
Loss :  1.7371981143951416 3.2139556407928467 4.951153755187988
Loss :  1.7479857206344604 3.0527713298797607 4.800756931304932
Loss :  1.7391877174377441 3.562732219696045 5.301919937133789
Loss :  1.7143876552581787 3.5821168422698975 5.296504497528076
Loss :  1.7313611507415771 3.6543474197387695 5.385708808898926
Loss :  1.7370392084121704 2.9213881492614746 4.6584272384643555
  batch 40 loss: 1.7370392084121704, 2.9213881492614746, 4.6584272384643555
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7276149988174438 3.403787851333618 5.131402969360352
Loss :  1.71278977394104 2.620945453643799 4.333735466003418
Loss :  1.7284259796142578 2.8380472660064697 4.566473007202148
Loss :  1.7284971475601196 2.842343330383301 4.570840358734131
Loss :  1.7339822053909302 2.921858310699463 4.6558403968811035
Loss :  1.732454776763916 2.6158039569854736 4.348258972167969
Loss :  1.7258989810943604 2.5611348152160645 4.287034034729004
Loss :  1.7229421138763428 2.571938991546631 4.2948808670043945
Loss :  1.7242847681045532 2.825152635574341 4.549437522888184
Loss :  1.7362453937530518 2.544475793838501 4.280721187591553
Loss :  1.7175116539001465 2.5159096717834473 4.233421325683594
Loss :  1.7184431552886963 2.554070234298706 4.272513389587402
Loss :  1.7371324300765991 2.377540111541748 4.114672660827637
Loss :  1.736481785774231 2.4512126445770264 4.187694549560547
Loss :  1.732012391090393 2.5482544898986816 4.280266761779785
Loss :  1.7121057510375977 2.394695281982422 4.1068010330200195
Loss :  1.7435280084609985 3.0847079753875732 4.828236103057861
Loss :  1.736356496810913 2.7398903369903564 4.4762468338012695
Loss :  1.741905689239502 2.9565930366516113 4.698498725891113
Loss :  1.737613558769226 2.493635416030884 4.23124885559082
  batch 60 loss: 1.737613558769226, 2.493635416030884, 4.23124885559082
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7347934246063232 2.7717177867889404 4.506511211395264
Loss :  1.7407124042510986 2.841661214828491 4.58237361907959
Loss :  1.7431411743164062 2.783112049102783 4.5262532234191895
Loss :  1.7264530658721924 3.027813196182251 4.754266262054443
Loss :  1.7200589179992676 2.3522355556488037 4.072294235229492
Loss :  1.7634958028793335 4.318922519683838 6.082418441772461
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7579610347747803 4.246510028839111 6.0044708251953125
Loss :  1.7613180875778198 4.200466156005859 5.961784362792969
Loss :  1.7288264036178589 4.103204727172852 5.83203125
Total LOSS train 4.680525009448711 valid 5.9701762199401855
CE LOSS train 1.7316765473439144 valid 0.4322066009044647
Contrastive LOSS train 2.9488484712747427 valid 1.025801181793213
EPOCH 50:
Loss :  1.7220419645309448 2.8381011486053467 4.560142993927002
Loss :  1.7251245975494385 2.8839404582977295 4.609065055847168
Loss :  1.715208888053894 2.8886797428131104 4.603888511657715
Loss :  1.7259125709533691 2.504141092300415 4.230053901672363
Loss :  1.7505347728729248 2.7671940326690674 4.517728805541992
Loss :  1.731656789779663 3.3590400218963623 5.090696811676025
Loss :  1.7365254163742065 3.0290987491607666 4.765624046325684
Loss :  1.7213104963302612 3.1761727333068848 4.8974833488464355
Loss :  1.7374085187911987 3.3732759952545166 5.110684394836426
Loss :  1.7003673315048218 3.2877814769744873 4.9881486892700195
Loss :  1.7337379455566406 3.396416664123535 5.130154609680176
Loss :  1.7605011463165283 3.3010988235473633 5.0615997314453125
Loss :  1.7339414358139038 2.6678812503814697 4.401822566986084
Loss :  1.7315653562545776 2.646186113357544 4.377751350402832
Loss :  1.722293734550476 2.7145960330963135 4.4368896484375
Loss :  1.7175827026367188 2.4701642990112305 4.187747001647949
Loss :  1.7351584434509277 2.508280038833618 4.243438720703125
Loss :  1.7310346364974976 2.451277256011963 4.18231201171875
Loss :  1.7259986400604248 2.2221341133117676 3.9481327533721924
Loss :  1.7171133756637573 2.306090831756592 4.023204326629639
  batch 20 loss: 1.7171133756637573, 2.306090831756592, 4.023204326629639
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7252833843231201 2.4222772121429443 4.1475605964660645
Loss :  1.729936957359314 2.659471035003662 4.389408111572266
Loss :  1.7257277965545654 2.52274227142334 4.248470306396484
Loss :  1.7420804500579834 2.5111851692199707 4.253265380859375
Loss :  1.749241828918457 3.05979061126709 4.809032440185547
Loss :  1.7314226627349854 2.4696006774902344 4.201023101806641
Loss :  1.7503381967544556 3.089677572250366 4.840015888214111
Loss :  1.7211774587631226 2.5439341068267822 4.265111446380615
Loss :  1.7528716325759888 3.337449550628662 5.090321063995361
Loss :  1.7348333597183228 2.987229824066162 4.722063064575195
Loss :  1.7728853225708008 2.9628546237945557 4.735739707946777
Loss :  1.7531908750534058 3.521822929382324 5.2750139236450195
Loss :  1.736555814743042 3.00032639503479 4.736882209777832
Loss :  1.7332568168640137 2.83555006980896 4.5688066482543945
Loss :  1.7473087310791016 2.9606919288635254 4.708000659942627
Loss :  1.7529258728027344 2.7119193077087402 4.464845180511475
Loss :  1.7426960468292236 2.7950732707977295 4.537769317626953
Loss :  1.718584418296814 2.417449474334717 4.13603401184082
Loss :  1.7355644702911377 2.9123189449310303 4.647883415222168
Loss :  1.7368382215499878 2.8954720497131348 4.632310390472412
  batch 40 loss: 1.7368382215499878, 2.8954720497131348, 4.632310390472412
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7302356958389282 3.026935577392578 4.757171154022217
Loss :  1.7176671028137207 2.855532646179199 4.57319974899292
Loss :  1.7320865392684937 3.260775327682495 4.992861747741699
Loss :  1.7242097854614258 3.306436777114868 5.030646324157715
Loss :  1.7343436479568481 2.733394145965576 4.467737674713135
Loss :  1.7332810163497925 3.046217918395996 4.779499053955078
Loss :  1.7231966257095337 2.462153434753418 4.185349941253662
Loss :  1.7277226448059082 2.557497978210449 4.285220623016357
Loss :  1.7272132635116577 3.378910541534424 5.106123924255371
Loss :  1.7380493879318237 3.2882583141326904 5.026307582855225
Loss :  1.7222849130630493 3.0162901878356934 4.738574981689453
Loss :  1.7267611026763916 3.0763750076293945 4.803135871887207
Loss :  1.737658143043518 2.565333843231201 4.30299186706543
Loss :  1.7398368120193481 2.6961617469787598 4.435998439788818
Loss :  1.740228295326233 2.840233564376831 4.5804619789123535
Loss :  1.7129595279693604 2.7687361240386963 4.481695652008057
Loss :  1.745821237564087 2.5933873653411865 4.339208602905273
Loss :  1.7379854917526245 2.537287712097168 4.275273323059082
Loss :  1.7444196939468384 3.0713534355163574 4.815773010253906
Loss :  1.734541893005371 2.9946813583374023 4.729223251342773
  batch 60 loss: 1.734541893005371, 2.9946813583374023, 4.729223251342773
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7275806665420532 2.9150185585021973 4.642599105834961
Loss :  1.7392070293426514 3.4187893867492676 5.15799617767334
Loss :  1.732714056968689 3.3485493659973145 5.081263542175293
Loss :  1.7252565622329712 3.496864080429077 5.222120761871338
Loss :  1.7193318605422974 2.7557947635650635 4.47512674331665
Loss :  1.7007216215133667 3.8886101245880127 5.58933162689209
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7118629217147827 3.9273080825805664 5.639171123504639
Loss :  1.6988335847854614 3.664959669113159 5.36379337310791
Loss :  1.6798409223556519 3.698073387145996 5.3779144287109375
Total LOSS train 4.616333649708674 valid 5.492552638053894
CE LOSS train 1.7329589550311748 valid 0.41996023058891296
Contrastive LOSS train 2.8833747240213246 valid 0.924518346786499
EPOCH 51:
Loss :  1.7215873003005981 2.412829637527466 4.1344170570373535
Loss :  1.7229080200195312 3.061450481414795 4.784358501434326
Loss :  1.7087870836257935 3.3693487644195557 5.078135967254639
Loss :  1.7180598974227905 3.5421454906463623 5.260205268859863
Loss :  1.7464561462402344 3.716958522796631 5.463414669036865
Loss :  1.7275707721710205 3.202604055404663 4.930174827575684
Loss :  1.734851360321045 2.7795369625091553 4.514388084411621
Loss :  1.7149498462677002 3.4667305946350098 5.181680679321289
Loss :  1.7326844930648804 3.1472065448760986 4.8798909187316895
Loss :  1.6977709531784058 2.8164446353912354 4.514215469360352
Loss :  1.7281672954559326 3.5734448432922363 5.30161190032959
Loss :  1.7555689811706543 3.4106028079986572 5.166172027587891
Loss :  1.728795051574707 3.5025289058685303 5.231324195861816
Loss :  1.7302130460739136 3.7356958389282227 5.465909004211426
Loss :  1.7254799604415894 3.824214220046997 5.549694061279297
Loss :  1.7215217351913452 3.824467658996582 5.545989513397217
Loss :  1.7276825904846191 3.3761556148529053 5.103837966918945
Loss :  1.726863980293274 3.337167501449585 5.064031600952148
Loss :  1.7228882312774658 3.241692304611206 4.964580535888672
Loss :  1.7124812602996826 2.6747546195983887 4.387235641479492
  batch 20 loss: 1.7124812602996826, 2.6747546195983887, 4.387235641479492
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7259966135025024 2.5127012729644775 4.2386980056762695
Loss :  1.7274473905563354 2.8176887035369873 4.545135974884033
Loss :  1.7260099649429321 2.7582390308380127 4.484249114990234
Loss :  1.735289216041565 2.65057373046875 4.385862827301025
Loss :  1.7476837635040283 2.7753989696502686 4.523082733154297
Loss :  1.7301585674285889 2.566143751144409 4.296302318572998
Loss :  1.7429922819137573 2.6259145736694336 4.3689069747924805
Loss :  1.712325096130371 2.5988998413085938 4.311224937438965
Loss :  1.7442692518234253 2.3929810523986816 4.1372504234313965
Loss :  1.7242860794067383 2.470367431640625 4.194653511047363
Loss :  1.76833176612854 2.7591545581817627 4.527486324310303
Loss :  1.738018274307251 2.804224967956543 4.542243003845215
Loss :  1.7221251726150513 2.992751121520996 4.714876174926758
Loss :  1.7245938777923584 2.728167772293091 4.452761650085449
Loss :  1.7433334589004517 2.7540717124938965 4.497405052185059
Loss :  1.7489690780639648 2.8178467750549316 4.5668158531188965
Loss :  1.741697907447815 2.9922707080841064 4.733968734741211
Loss :  1.716629981994629 2.803825855255127 4.520455837249756
Loss :  1.7362425327301025 3.1804466247558594 4.916688919067383
Loss :  1.7368680238723755 2.4216127395629883 4.158480644226074
  batch 40 loss: 1.7368680238723755, 2.4216127395629883, 4.158480644226074
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7337788343429565 2.5703887939453125 4.304167747497559
Loss :  1.7264617681503296 2.4787180423736572 4.205179691314697
Loss :  1.7451609373092651 3.1112401485443115 4.856400966644287
Loss :  1.7328393459320068 2.7265548706054688 4.459394454956055
Loss :  1.7444064617156982 2.7959187030792236 4.540325164794922
Loss :  1.733262062072754 3.165386438369751 4.898648262023926
Loss :  1.7203996181488037 3.100891590118408 4.821290969848633
Loss :  1.7348265647888184 2.968803644180298 4.703630447387695
Loss :  1.7168290615081787 2.5285303592681885 4.245359420776367
Loss :  1.7411186695098877 2.7252469062805176 4.466365814208984
Loss :  1.7226670980453491 2.46593976020813 4.1886067390441895
Loss :  1.7246860265731812 2.4118564128875732 4.136542320251465
Loss :  1.7298778295516968 2.422930955886841 4.152808666229248
Loss :  1.7326600551605225 2.3518283367156982 4.084488391876221
Loss :  1.7345036268234253 2.8102643489837646 4.5447678565979
Loss :  1.7032496929168701 2.2737553119659424 3.9770050048828125
Loss :  1.741646409034729 2.818373441696167 4.5600199699401855
Loss :  1.735764741897583 2.708918333053589 4.444683074951172
Loss :  1.7424437999725342 2.950515031814575 4.692958831787109
Loss :  1.733357548713684 2.4629476070404053 4.196305274963379
  batch 60 loss: 1.733357548713684, 2.4629476070404053, 4.196305274963379
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7278492450714111 2.9329779148101807 4.660827159881592
Loss :  1.740417718887329 2.7369117736816406 4.477329254150391
Loss :  1.7342664003372192 2.834230661392212 4.568497180938721
Loss :  1.7277607917785645 2.8928136825561523 4.620574474334717
Loss :  1.7282252311706543 2.58319354057312 4.311418533325195
Loss :  1.6302132606506348 4.197477340698242 5.827690601348877
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6559944152832031 4.181557655334473 5.837552070617676
Loss :  1.6309150457382202 4.000739574432373 5.631654739379883
Loss :  1.6140393018722534 3.944765329360962 5.558804512023926
Total LOSS train 4.627006347362812 valid 5.71392548084259
CE LOSS train 1.7305694745137141 valid 0.40350982546806335
Contrastive LOSS train 2.8964368893549994 valid 0.9861913323402405
EPOCH 52:
Loss :  1.7297345399856567 2.603740692138672 4.333475112915039
Loss :  1.726373314857483 2.76432466506958 4.490697860717773
Loss :  1.7216582298278809 2.269993782043457 3.991652011871338
Loss :  1.7352149486541748 2.324310541152954 4.059525489807129
Loss :  1.7534867525100708 2.433905839920044 4.187392711639404
Loss :  1.7328534126281738 2.598719596862793 4.331573009490967
Loss :  1.7379733324050903 2.7702348232269287 4.508208274841309
Loss :  1.7219336032867432 2.653618812561035 4.375552177429199
Loss :  1.738157868385315 2.6808526515960693 4.419010639190674
Loss :  1.7051043510437012 2.6477837562561035 4.352888107299805
Loss :  1.7317891120910645 2.899961233139038 4.631750106811523
Loss :  1.7597765922546387 2.911895751953125 4.671672344207764
Loss :  1.7324196100234985 2.686115026473999 4.418534755706787
Loss :  1.732935905456543 3.1079018115997314 4.840837478637695
Loss :  1.7268321514129639 2.8847312927246094 4.611563682556152
Loss :  1.7251067161560059 3.1076817512512207 4.832788467407227
Loss :  1.7310419082641602 2.64142107963562 4.372463226318359
Loss :  1.7312859296798706 2.601102352142334 4.332388401031494
Loss :  1.7292402982711792 2.5742228031158447 4.303462982177734
Loss :  1.7137151956558228 3.7679269313812256 5.481642246246338
  batch 20 loss: 1.7137151956558228, 3.7679269313812256, 5.481642246246338
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.728024959564209 3.4726061820983887 5.200631141662598
Loss :  1.7279971837997437 3.277092456817627 5.00508975982666
Loss :  1.7279845476150513 2.5389115810394287 4.2668962478637695
Loss :  1.738554835319519 2.685774803161621 4.42432975769043
Loss :  1.7482755184173584 3.3979132175445557 5.146188735961914
Loss :  1.73037850856781 2.8199994564056396 4.55037784576416
Loss :  1.7465096712112427 3.1661341190338135 4.912643909454346
Loss :  1.7153021097183228 2.7665886878967285 4.481890678405762
Loss :  1.743546962738037 2.2549874782562256 3.9985344409942627
Loss :  1.721043586730957 2.5556704998016357 4.276714324951172
Loss :  1.768676519393921 2.6503231525421143 4.418999671936035
Loss :  1.7336082458496094 2.5960593223571777 4.329667568206787
Loss :  1.7165684700012207 2.58138370513916 4.297952175140381
Loss :  1.7237927913665771 2.4135847091674805 4.137377738952637
Loss :  1.7379957437515259 3.2160747051239014 4.954070568084717
Loss :  1.7468339204788208 2.8489394187927246 4.595773220062256
Loss :  1.7335602045059204 2.846846103668213 4.580406188964844
Loss :  1.70522940158844 2.79439377784729 4.4996232986450195
Loss :  1.7255443334579468 2.7947354316711426 4.520279884338379
Loss :  1.7263189554214478 2.5668952465057373 4.293214321136475
  batch 40 loss: 1.7263189554214478, 2.5668952465057373, 4.293214321136475
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7241759300231934 3.2521777153015137 4.976353645324707
Loss :  1.722176432609558 2.0472159385681152 3.769392490386963
Loss :  1.7402173280715942 2.3988406658172607 4.1390581130981445
Loss :  1.729268193244934 2.468778371810913 4.198046684265137
Loss :  1.7405188083648682 3.324974298477173 5.065493106842041
Loss :  1.732210397720337 3.4972317218780518 5.229442119598389
Loss :  1.7232954502105713 3.5881459712982178 5.311441421508789
Loss :  1.7370017766952515 4.116822719573975 5.853824615478516
Loss :  1.7194737195968628 4.231556415557861 5.951030254364014
Loss :  1.7409472465515137 3.6833739280700684 5.424321174621582
Loss :  1.7287871837615967 3.260453462600708 4.989240646362305
Loss :  1.729304313659668 3.3900773525238037 5.119381904602051
Loss :  1.7360025644302368 3.133082866668701 4.869085311889648
Loss :  1.7381120920181274 3.0888149738311768 4.826927185058594
Loss :  1.738100528717041 2.908869504928589 4.646969795227051
Loss :  1.7098420858383179 2.70670223236084 4.416544437408447
Loss :  1.741930365562439 2.757733106613159 4.499663352966309
Loss :  1.7364073991775513 3.5975027084350586 5.33390998840332
Loss :  1.7449589967727661 3.247788429260254 4.9927473068237305
Loss :  1.7389826774597168 3.115027666091919 4.854010581970215
  batch 60 loss: 1.7389826774597168, 3.115027666091919, 4.854010581970215
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.729707956314087 2.9911370277404785 4.7208452224731445
Loss :  1.7407563924789429 2.9826581478118896 4.723414421081543
Loss :  1.7342246770858765 2.2695083618164062 4.003733158111572
Loss :  1.7276711463928223 2.436743974685669 4.16441535949707
Loss :  1.7227139472961426 2.1025643348693848 3.8252782821655273
Loss :  1.6778926849365234 3.9465856552124023 5.624478340148926
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6869949102401733 4.023406505584717 5.71040153503418
Loss :  1.676025152206421 3.9171128273010254 5.593137741088867
Loss :  1.6560479402542114 3.8810057640075684 5.53705358505249
Total LOSS train 4.620650940675002 valid 5.616267800331116
CE LOSS train 1.7318333515754112 valid 0.41401198506355286
Contrastive LOSS train 2.8888175560877873 valid 0.9702514410018921
EPOCH 53:
Loss :  1.7242549657821655 2.6549465656280518 4.379201412200928
Loss :  1.7213386297225952 3.4034368991851807 5.124775409698486
Loss :  1.714274287223816 2.8282711505889893 4.542545318603516
Loss :  1.7250720262527466 3.374863862991333 5.099936008453369
Loss :  1.748320460319519 3.001760244369507 4.750080585479736
Loss :  1.7265527248382568 3.0743868350982666 4.800939559936523
Loss :  1.732113003730774 3.681849241256714 5.413962364196777
Loss :  1.7181035280227661 3.5945067405700684 5.312610149383545
Loss :  1.7380563020706177 4.080377101898193 5.8184332847595215
Loss :  1.700050950050354 3.662442445755005 5.362493515014648
Loss :  1.7371270656585693 4.0987420082092285 5.835868835449219
Loss :  1.7676522731781006 3.8825697898864746 5.650221824645996
Loss :  1.7363020181655884 4.031017303466797 5.767319202423096
Loss :  1.7313863039016724 3.5810699462890625 5.312456130981445
Loss :  1.7289643287658691 3.347813606262207 5.076777935028076
Loss :  1.7245124578475952 3.3750624656677246 5.099575042724609
Loss :  1.7287530899047852 3.307101249694824 5.035854339599609
Loss :  1.7256510257720947 3.7125773429870605 5.438228607177734
Loss :  1.7261767387390137 2.997429132461548 4.723606109619141
Loss :  1.7135627269744873 3.311619520187378 5.025182247161865
  batch 20 loss: 1.7135627269744873, 3.311619520187378, 5.025182247161865
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7276095151901245 3.454319953918457 5.181929588317871
Loss :  1.7282829284667969 3.3850128650665283 5.113295555114746
Loss :  1.7276753187179565 2.8958547115325928 4.62352991104126
Loss :  1.7353222370147705 2.7446508407592773 4.479972839355469
Loss :  1.7457575798034668 3.362541675567627 5.108299255371094
Loss :  1.724532961845398 3.1615781784057617 4.886111259460449
Loss :  1.7394288778305054 3.1525003910064697 4.8919291496276855
Loss :  1.7077751159667969 2.522631883621216 4.230406761169434
Loss :  1.7383687496185303 2.6425795555114746 4.380948066711426
Loss :  1.7199229001998901 2.57352614402771 4.2934489250183105
Loss :  1.7694447040557861 2.7597038745880127 4.529148578643799
Loss :  1.7314624786376953 2.7209296226501465 4.452392101287842
Loss :  1.7136845588684082 2.7088401317596436 4.422524452209473
Loss :  1.7221894264221191 2.62231707572937 4.34450626373291
Loss :  1.7345693111419678 2.962552070617676 4.697121620178223
Loss :  1.7468469142913818 2.7884089946746826 4.5352559089660645
Loss :  1.7327451705932617 2.814692258834839 4.54743766784668
Loss :  1.7046936750411987 3.2085824012756348 4.913276195526123
Loss :  1.7237759828567505 2.741633892059326 4.465409755706787
Loss :  1.72748601436615 2.7555296421051025 4.483015537261963
  batch 40 loss: 1.72748601436615, 2.7555296421051025, 4.483015537261963
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7192935943603516 2.9825563430786133 4.701849937438965
Loss :  1.710189938545227 2.7625021934509277 4.472692012786865
Loss :  1.7268376350402832 2.7337021827697754 4.460539817810059
Loss :  1.716872215270996 2.457653522491455 4.174525737762451
Loss :  1.7264478206634521 2.849374294281006 4.575821876525879
Loss :  1.722333550453186 2.684337615966797 4.406671047210693
Loss :  1.7081362009048462 2.3780431747436523 4.086179256439209
Loss :  1.7214525938034058 2.692432403564453 4.413885116577148
Loss :  1.7073135375976562 2.6437957286834717 4.351109504699707
Loss :  1.7349791526794434 2.739096164703369 4.4740753173828125
Loss :  1.7199400663375854 3.1634421348571777 4.883382320404053
Loss :  1.7233822345733643 2.9741666316986084 4.697548866271973
Loss :  1.7316815853118896 3.194981813430786 4.926663398742676
Loss :  1.7386577129364014 3.4917237758636475 5.230381488800049
Loss :  1.7405303716659546 3.3610997200012207 5.101630210876465
Loss :  1.7137224674224854 3.0653305053710938 4.779052734375
Loss :  1.7463005781173706 3.223677635192871 4.969978332519531
Loss :  1.7377654314041138 2.827115058898926 4.56488037109375
Loss :  1.745874047279358 3.07212233543396 4.817996501922607
Loss :  1.7366516590118408 2.982393741607666 4.719045639038086
  batch 60 loss: 1.7366516590118408, 2.982393741607666, 4.719045639038086
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7285363674163818 3.1696579456329346 4.898194313049316
Loss :  1.7422406673431396 2.9628541469573975 4.705094814300537
Loss :  1.7366344928741455 2.6248092651367188 4.361443519592285
Loss :  1.729480266571045 2.7279374599456787 4.4574174880981445
Loss :  1.7279990911483765 2.288280725479126 4.016279697418213
Loss :  1.6266059875488281 3.967318058013916 5.593924045562744
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.652322769165039 4.1414971351623535 5.793819904327393
Loss :  1.631136417388916 4.011067867279053 5.642204284667969
Loss :  1.607627272605896 3.8497939109802246 5.45742130279541
Total LOSS train 4.790713332249568 valid 5.621842384338379
CE LOSS train 1.7286623477935792 valid 0.401906818151474
Contrastive LOSS train 3.0620510174677924 valid 0.9624484777450562
EPOCH 54:
Loss :  1.7286957502365112 2.9645578861236572 4.693253517150879
Loss :  1.7272177934646606 3.4870097637176514 5.214227676391602
Loss :  1.7211755514144897 2.465608835220337 4.186784267425537
Loss :  1.732927918434143 2.529215097427368 4.262143135070801
Loss :  1.7513163089752197 2.666109323501587 4.417425632476807
Loss :  1.730442762374878 2.8048582077026367 4.535301208496094
Loss :  1.732313871383667 2.7706139087677 4.502927780151367
Loss :  1.7207276821136475 2.6004638671875 4.321191787719727
Loss :  1.732028603553772 2.9536664485931396 4.685695171356201
Loss :  1.6967344284057617 2.30627703666687 4.003011703491211
Loss :  1.7309781312942505 2.469193458557129 4.20017147064209
Loss :  1.7561557292938232 2.4626986980438232 4.2188544273376465
Loss :  1.7294373512268066 2.4301793575286865 4.159616470336914
Loss :  1.7285635471343994 2.3574106693267822 4.085974216461182
Loss :  1.7228550910949707 2.4460623264312744 4.168917655944824
Loss :  1.7193242311477661 2.7194178104400635 4.438742160797119
Loss :  1.725876808166504 2.618765115737915 4.34464168548584
Loss :  1.7250056266784668 2.6353142261505127 4.360320091247559
Loss :  1.72713041305542 2.329474687576294 4.056605339050293
Loss :  1.706924557685852 2.4665746688842773 4.17349910736084
  batch 20 loss: 1.706924557685852, 2.4665746688842773, 4.17349910736084
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7291765213012695 2.2373270988464355 3.966503620147705
Loss :  1.728704810142517 2.4786734580993652 4.207378387451172
Loss :  1.7253479957580566 2.5759003162384033 4.301248550415039
Loss :  1.739559292793274 2.4046823978424072 4.144241809844971
Loss :  1.7490493059158325 2.709744930267334 4.458794116973877
Loss :  1.7301708459854126 2.8665502071380615 4.596721172332764
Loss :  1.7434531450271606 2.715162992477417 4.458616256713867
Loss :  1.7110490798950195 2.9820938110351562 4.693142890930176
Loss :  1.7429993152618408 2.466567277908325 4.209566593170166
Loss :  1.7228397130966187 2.435783863067627 4.158623695373535
Loss :  1.7673194408416748 2.7845888137817383 4.551908493041992
Loss :  1.7340974807739258 2.692840576171875 4.426938056945801
Loss :  1.718584656715393 2.934270143508911 4.652854919433594
Loss :  1.721082091331482 2.5252349376678467 4.246316909790039
Loss :  1.7385722398757935 2.6870689392089844 4.425641059875488
Loss :  1.7451679706573486 2.5988974571228027 4.3440656661987305
Loss :  1.734021782875061 2.703885793685913 4.437907695770264
Loss :  1.7044556140899658 2.577272891998291 4.281728744506836
Loss :  1.7314642667770386 2.9123756885528564 4.6438398361206055
Loss :  1.7294070720672607 2.9869813919067383 4.716388702392578
  batch 40 loss: 1.7294070720672607, 2.9869813919067383, 4.716388702392578
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7280874252319336 2.4106106758117676 4.138698101043701
Loss :  1.7208532094955444 2.2294819355010986 3.9503350257873535
Loss :  1.7367743253707886 2.418379068374634 4.155153274536133
Loss :  1.7272121906280518 2.3105785846710205 4.037790775299072
Loss :  1.7381962537765503 2.1358864307403564 3.874082565307617
Loss :  1.728856086730957 2.756023406982422 4.484879493713379
Loss :  1.7188371419906616 2.779946804046631 4.498784065246582
Loss :  1.7315541505813599 2.6875810623168945 4.419135093688965
Loss :  1.7173535823822021 2.380319118499756 4.097672462463379
Loss :  1.7370895147323608 2.3029425144195557 4.040031909942627
Loss :  1.7235583066940308 2.3522772789001465 4.075835704803467
Loss :  1.7268884181976318 2.3441083431243896 4.0709967613220215
Loss :  1.7371925115585327 2.2584593296051025 3.9956517219543457
Loss :  1.7332737445831299 2.4173901081085205 4.15066385269165
Loss :  1.7403641939163208 2.9876174926757812 4.7279815673828125
Loss :  1.7196704149246216 2.702874183654785 4.422544479370117
Loss :  1.7469054460525513 2.7812154293060303 4.528120994567871
Loss :  1.7391743659973145 2.783338785171509 4.522513389587402
Loss :  1.7439757585525513 3.0763800144195557 4.8203558921813965
Loss :  1.7392748594284058 2.350844621658325 4.090119361877441
  batch 60 loss: 1.7392748594284058, 2.350844621658325, 4.090119361877441
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7310768365859985 2.6457953453063965 4.3768720626831055
Loss :  1.7429569959640503 2.560002088546753 4.302958965301514
Loss :  1.736313819885254 2.3730835914611816 4.1093974113464355
Loss :  1.7285162210464478 2.7536587715148926 4.482174873352051
Loss :  1.7268362045288086 2.220182418823242 3.947018623352051
Loss :  1.6001571416854858 4.111219882965088 5.711377143859863
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6405789852142334 4.240980625152588 5.881559371948242
Loss :  1.605078101158142 4.022663593292236 5.627741813659668
Loss :  1.6058720350265503 3.822514295578003 5.428386211395264
Total LOSS train 4.327253847855788 valid 5.662266135215759
CE LOSS train 1.7306637965715848 valid 0.4014680087566376
Contrastive LOSS train 2.5965900274423452 valid 0.9556285738945007
EPOCH 55:
Loss :  1.7296921014785767 2.607220411300659 4.336912631988525
Loss :  1.7232990264892578 2.580019474029541 4.303318500518799
Loss :  1.713546872138977 2.332390785217285 4.045937538146973
Loss :  1.7228728532791138 2.2843410968780518 4.007214069366455
Loss :  1.7467169761657715 2.634140968322754 4.380857944488525
Loss :  1.7267963886260986 2.7821593284606934 4.508955955505371
Loss :  1.7292832136154175 2.7632367610931396 4.492519855499268
Loss :  1.718667984008789 2.7889626026153564 4.507630348205566
Loss :  1.7325139045715332 2.4027187824249268 4.135232925415039
Loss :  1.6986547708511353 3.1800501346588135 4.878705024719238
Loss :  1.7325254678726196 3.343317747116089 5.075843334197998
Loss :  1.7596123218536377 2.8767178058624268 4.6363301277160645
Loss :  1.7293386459350586 3.0794248580932617 4.80876350402832
Loss :  1.7322711944580078 2.485507011413574 4.217778205871582
Loss :  1.7278003692626953 3.6081573963165283 5.3359575271606445
Loss :  1.722108006477356 3.371891498565674 5.09399938583374
Loss :  1.7297030687332153 2.5767500400543213 4.306453227996826
Loss :  1.7283579111099243 2.3460311889648438 4.0743889808654785
Loss :  1.7300183773040771 2.1040546894073486 3.834073066711426
Loss :  1.7094054222106934 2.827017307281494 4.5364227294921875
  batch 20 loss: 1.7094054222106934, 2.827017307281494, 4.5364227294921875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7298814058303833 3.265475273132324 4.995356559753418
Loss :  1.7271558046340942 3.1660208702087402 4.893176555633545
Loss :  1.7289814949035645 2.3120622634887695 4.041043758392334
Loss :  1.7399404048919678 3.2605295181274414 5.000470161437988
Loss :  1.792102336883545 4.5394697189331055 6.33157205581665
Loss :  1.8839744329452515 4.513869285583496 6.397843837738037
Loss :  1.7902379035949707 3.5681021213531494 5.358340263366699
Loss :  1.775768518447876 2.970574378967285 4.746342658996582
Loss :  1.7821977138519287 3.225698709487915 5.007896423339844
Loss :  1.755545735359192 3.041569471359253 4.797115325927734
Loss :  1.7687711715698242 3.011486053466797 4.780257225036621
Loss :  1.7723370790481567 3.012934446334839 4.785271644592285
Loss :  1.7529120445251465 3.055408000946045 4.808320045471191
Loss :  1.7411748170852661 2.961052894592285 4.702227592468262
Loss :  1.7597178220748901 3.0531537532806396 4.81287145614624
Loss :  1.7623744010925293 2.752927303314209 4.515301704406738
Loss :  1.7609671354293823 2.767296075820923 4.528263092041016
Loss :  1.7722212076187134 2.520286798477173 4.292508125305176
Loss :  1.7591413259506226 3.0784459114074707 4.837587356567383
Loss :  1.7633520364761353 3.1216578483581543 4.885009765625
  batch 40 loss: 1.7633520364761353, 3.1216578483581543, 4.885009765625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7678906917572021 3.1456565856933594 4.913547515869141
Loss :  1.7538989782333374 2.996821165084839 4.750720024108887
Loss :  1.7531074285507202 3.2930891513824463 5.046196460723877
Loss :  1.7720226049423218 2.956305503845215 4.728328227996826
Loss :  1.7629625797271729 2.9266507625579834 4.689613342285156
Loss :  1.7558128833770752 2.951307535171509 4.707120418548584
Loss :  1.7739337682724 3.352954387664795 5.126888275146484
Loss :  1.7615445852279663 3.0462052822113037 4.8077497482299805
Loss :  1.782775640487671 3.205098867416382 4.987874507904053
Loss :  1.7836813926696777 3.152878999710083 4.93656063079834
Loss :  1.7529596090316772 3.5375285148620605 5.290488243103027
Loss :  1.7544052600860596 3.376986265182495 5.131391525268555
Loss :  1.7725781202316284 3.5695807933807373 5.342158794403076
Loss :  1.7702224254608154 3.309513568878174 5.07973575592041
Loss :  1.7679733037948608 3.532546281814575 5.3005194664001465
Loss :  1.7597837448120117 3.6482346057891846 5.408018112182617
Loss :  1.76338529586792 3.493565320968628 5.256950378417969
Loss :  1.7706485986709595 3.4220666885375977 5.192715167999268
Loss :  1.7576934099197388 3.938527822494507 5.696221351623535
Loss :  1.7706409692764282 3.4285781383514404 5.199219226837158
  batch 60 loss: 1.7706409692764282, 3.4285781383514404, 5.199219226837158
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7687617540359497 3.5977489948272705 5.36651086807251
Loss :  1.762932300567627 3.575019121170044 5.33795166015625
Loss :  1.7853206396102905 3.5922727584838867 5.377593517303467
Loss :  1.7421586513519287 3.792532205581665 5.534690856933594
Loss :  1.7422207593917847 3.6666476726531982 5.408868312835693
Loss :  1.8588955402374268 3.9792819023132324 5.838177680969238
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8490755558013916 3.9857065677642822 5.834782123565674
Loss :  1.8400706052780151 3.8835883140563965 5.723659038543701
Loss :  1.8730148077011108 3.7159135341644287 5.58892822265625
Total LOSS train 4.871564659705529 valid 5.746386766433716
CE LOSS train 1.7534039240617019 valid 0.4682537019252777
Contrastive LOSS train 3.118160731975849 valid 0.9289783835411072
EPOCH 56:
Loss :  1.7576175928115845 4.155877590179443 5.913495063781738
Loss :  1.7755844593048096 3.710061550140381 5.4856462478637695
Loss :  1.7602678537368774 3.3862292766571045 5.1464972496032715
Loss :  1.7669594287872314 3.6534807682037354 5.420440196990967
Loss :  1.774466872215271 3.736825704574585 5.511292457580566
Loss :  1.7720993757247925 3.73903489112854 5.511134147644043
Loss :  1.7660317420959473 3.4272868633270264 5.1933183670043945
Loss :  1.769820213317871 3.4581634998321533 5.227983474731445
Loss :  1.7642171382904053 2.978646993637085 4.74286413192749
Loss :  1.743560552597046 3.130350351333618 4.873910903930664
Loss :  1.7739828824996948 3.306983709335327 5.080966472625732
Loss :  1.7866427898406982 3.3401594161987305 5.126802444458008
Loss :  1.7718878984451294 3.3136394023895264 5.085527420043945
Loss :  1.7674286365509033 2.9742867946624756 4.741715431213379
Loss :  1.7642525434494019 3.0486831665039062 4.812935829162598
Loss :  1.7525275945663452 3.066789388656616 4.819316864013672
Loss :  1.7767115831375122 3.0325095653533936 4.809221267700195
Loss :  1.760634422302246 2.9016687870025635 4.6623029708862305
Loss :  1.7776399850845337 2.8691694736480713 4.6468095779418945
Loss :  1.7499068975448608 2.8097941875457764 4.559700965881348
  batch 20 loss: 1.7499068975448608, 2.8097941875457764, 4.559700965881348
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7628341913223267 2.605076313018799 4.367910385131836
Loss :  1.7712715864181519 3.0554845333099365 4.826756000518799
Loss :  1.7521041631698608 3.0223867893218994 4.774490833282471
Loss :  1.7704521417617798 2.886361598968506 4.656813621520996
Loss :  1.766074776649475 3.593327522277832 5.359402179718018
Loss :  1.7379266023635864 3.818434476852417 5.556361198425293
Loss :  1.7597041130065918 3.8686184883117676 5.628322601318359
Loss :  1.7450275421142578 4.1327996253967285 5.877827167510986
Loss :  1.7647655010223389 4.265227317810059 6.029993057250977
Loss :  1.741834044456482 4.188354015350342 5.930188179016113
Loss :  1.7725543975830078 3.6230854988098145 5.395639896392822
Loss :  1.7636421918869019 3.906419277191162 5.6700615882873535
Loss :  1.7512211799621582 3.8787930011749268 5.630014419555664
Loss :  1.7290096282958984 3.8639535903930664 5.592963218688965
Loss :  1.7527153491973877 3.1664206981658936 4.919136047363281
Loss :  1.7501164674758911 3.191511631011963 4.9416279792785645
Loss :  1.7497698068618774 3.472054958343506 5.221824645996094
Loss :  1.7570174932479858 3.0395007133483887 4.796518325805664
Loss :  1.7462738752365112 3.2205111980438232 4.966784954071045
Loss :  1.7503002882003784 3.2841925621032715 5.0344929695129395
  batch 40 loss: 1.7503002882003784, 3.2841925621032715, 5.0344929695129395
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7627803087234497 3.231733560562134 4.994513988494873
Loss :  1.7590842247009277 3.116581439971924 4.875665664672852
Loss :  1.7487590312957764 3.474278211593628 5.223037242889404
Loss :  1.7677192687988281 3.200759172439575 4.968478202819824
Loss :  1.7559492588043213 3.3868930339813232 5.1428422927856445
Loss :  1.7459514141082764 3.724212408065796 5.470163822174072
Loss :  1.7723875045776367 3.768224000930786 5.540611267089844
Loss :  1.7461580038070679 3.2541098594665527 5.00026798248291
Loss :  1.75210440158844 3.4378278255462646 5.189932346343994
Loss :  1.7499866485595703 3.251128911972046 5.001115798950195
Loss :  1.7255702018737793 3.300337791442871 5.02590799331665
Loss :  1.7410928010940552 3.367556095123291 5.108648777008057
Loss :  1.7630276679992676 3.116787910461426 4.879815578460693
Loss :  1.7478001117706299 3.3651957511901855 5.1129961013793945
Loss :  1.7352845668792725 3.367144823074341 5.102429389953613
Loss :  1.7531334161758423 3.5125224590301514 5.265655994415283
Loss :  1.7502098083496094 3.404120922088623 5.154330730438232
Loss :  1.7461622953414917 3.6027207374572754 5.348883152008057
Loss :  1.7478663921356201 3.5587875843048096 5.30665397644043
Loss :  1.7464786767959595 2.9074769020080566 4.653955459594727
  batch 60 loss: 1.7464786767959595, 2.9074769020080566, 4.653955459594727
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7628229856491089 3.0459063053131104 4.80872917175293
Loss :  1.7508678436279297 3.14805006980896 4.898918151855469
Loss :  1.7766916751861572 3.3310275077819824 5.107719421386719
Loss :  1.7432386875152588 3.731132984161377 5.474371910095215
Loss :  1.732663869857788 2.8602585792541504 4.592922210693359
Loss :  1.9105850458145142 3.9983160495758057 5.908901214599609
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8815888166427612 4.064596652984619 5.94618558883667
Loss :  1.889615774154663 3.8003461360931396 5.689961910247803
Loss :  1.9420769214630127 3.82430362701416 5.766380310058594
Total LOSS train 5.135347344325139 valid 5.827857255935669
CE LOSS train 1.7570868748884934 valid 0.4855192303657532
Contrastive LOSS train 3.3782604621006893 valid 0.95607590675354
EPOCH 57:
Loss :  1.7459992170333862 3.2537293434143066 4.999728679656982
Loss :  1.7680387496948242 3.165764093399048 4.933802604675293
Loss :  1.7494479417800903 3.2662625312805176 5.015710353851318
Loss :  1.754808783531189 3.396646022796631 5.151454925537109
Loss :  1.7662519216537476 3.111589193344116 4.877840995788574
Loss :  1.7665880918502808 3.03928542137146 4.805873394012451
Loss :  1.7620588541030884 2.9481050968170166 4.7101640701293945
Loss :  1.7595124244689941 3.250967264175415 5.010479927062988
Loss :  1.7551982402801514 3.2778537273406982 5.03305196762085
Loss :  1.725764274597168 3.1578726768493652 4.883636951446533
Loss :  1.7627170085906982 3.1137728691101074 4.876489639282227
Loss :  1.7790712118148804 3.3531129360198975 5.132184028625488
Loss :  1.7575700283050537 3.379565715789795 5.1371355056762695
Loss :  1.7574454545974731 3.9986870288848877 5.75613260269165
Loss :  1.752891182899475 3.866619348526001 5.619510650634766
Loss :  1.7337762117385864 3.705352544784546 5.439128875732422
Loss :  1.7659013271331787 3.3885128498077393 5.154414176940918
Loss :  1.7532531023025513 3.806823253631592 5.5600762367248535
Loss :  1.7505390644073486 2.991544485092163 4.742083549499512
Loss :  1.7484683990478516 3.6844139099121094 5.432882308959961
  batch 20 loss: 1.7484683990478516, 3.6844139099121094, 5.432882308959961
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7485231161117554 3.343766689300537 5.092289924621582
Loss :  1.7592302560806274 3.3137893676757812 5.073019504547119
Loss :  1.74159574508667 2.9371516704559326 4.678747177124023
Loss :  1.7678124904632568 3.0237905979156494 4.791603088378906
Loss :  1.7504984140396118 3.1279966831207275 4.878495216369629
Loss :  1.7393078804016113 2.982909679412842 4.722217559814453
Loss :  1.7661947011947632 2.835543394088745 4.601737976074219
Loss :  1.7560956478118896 2.9588029384613037 4.714898586273193
Loss :  1.7698317766189575 3.0214731693267822 4.791305065155029
Loss :  1.7429203987121582 3.45302677154541 5.195947170257568
Loss :  1.7601467370986938 3.468778371810913 5.2289252281188965
Loss :  1.7599369287490845 3.320482015609741 5.080419063568115
Loss :  1.747460961341858 3.5654232501983643 5.312884330749512
Loss :  1.7298214435577393 3.077068328857422 4.806889533996582
Loss :  1.7498843669891357 3.576425075531006 5.3263092041015625
Loss :  1.7545050382614136 3.739161252975464 5.493666172027588
Loss :  1.750659704208374 3.855113983154297 5.60577392578125
Loss :  1.7538856267929077 3.915017604827881 5.668903350830078
Loss :  1.7497514486312866 4.00997257232666 5.759724140167236
Loss :  1.74667227268219 3.875572681427002 5.622244834899902
  batch 40 loss: 1.74667227268219, 3.875572681427002, 5.622244834899902
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7500739097595215 3.897008180618286 5.647082328796387
Loss :  1.743276596069336 3.8131167888641357 5.556393623352051
Loss :  1.7386800050735474 3.415907859802246 5.154587745666504
Loss :  1.7601038217544556 3.7029550075531006 5.463058948516846
Loss :  1.750999927520752 3.398033618927002 5.149033546447754
Loss :  1.7435078620910645 3.350519895553589 5.094027519226074
Loss :  1.7645418643951416 3.0124638080596924 4.777005672454834
Loss :  1.7408361434936523 3.456855058670044 5.197690963745117
Loss :  1.753670334815979 3.853461980819702 5.607132434844971
Loss :  1.7417123317718506 3.4778854846954346 5.219597816467285
Loss :  1.7254610061645508 3.6900787353515625 5.415539741516113
Loss :  1.7348625659942627 3.356431245803833 5.091293811798096
Loss :  1.7582460641860962 3.1553168296813965 4.913562774658203
Loss :  1.7436264753341675 3.215172290802002 4.958798885345459
Loss :  1.7326456308364868 3.264474630355835 4.997120380401611
Loss :  1.7517058849334717 3.1452577114105225 4.896963596343994
Loss :  1.7438364028930664 3.3361129760742188 5.079949378967285
Loss :  1.7410296201705933 3.3862061500549316 5.1272358894348145
Loss :  1.7435557842254639 3.2045323848724365 4.9480881690979
Loss :  1.747009515762329 3.289673089981079 5.036682605743408
  batch 60 loss: 1.747009515762329, 3.289673089981079, 5.036682605743408
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7575818300247192 3.2893788814544678 5.046960830688477
Loss :  1.7475370168685913 2.9698941707611084 4.71743106842041
Loss :  1.7721573114395142 3.2139174938201904 4.986074924468994
Loss :  1.732983946800232 3.429090738296509 5.162074565887451
Loss :  1.7249925136566162 3.252533197402954 4.97752571105957
Loss :  1.753021001815796 4.017857074737549 5.770877838134766
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7613697052001953 4.181975841522217 5.943345546722412
Loss :  1.7459989786148071 3.806622266769409 5.552621364593506
Loss :  1.8110859394073486 4.026632785797119 5.837718963623047
Total LOSS train 5.122133775857779 valid 5.776140928268433
CE LOSS train 1.7508410893953763 valid 0.45277148485183716
Contrastive LOSS train 3.3712926864624024 valid 1.0066581964492798
EPOCH 58:
Loss :  1.7397348880767822 3.348147392272949 5.087882041931152
Loss :  1.7592791318893433 3.313323974609375 5.072603225708008
Loss :  1.7435230016708374 3.2011466026306152 4.944669723510742
Loss :  1.748966932296753 2.9597795009613037 4.708746433258057
Loss :  1.7580972909927368 3.155999183654785 4.914096355438232
Loss :  1.7614706754684448 3.252589702606201 5.0140604972839355
Loss :  1.7515538930892944 3.2612383365631104 5.012792110443115
Loss :  1.7516385316848755 3.2863950729370117 5.038033485412598
Loss :  1.7466351985931396 3.450777292251587 5.197412490844727
Loss :  1.7188701629638672 3.506171941757202 5.225042343139648
Loss :  1.7544887065887451 3.2933082580566406 5.047797203063965
Loss :  1.776381492614746 3.325282573699951 5.101664066314697
Loss :  1.750125765800476 3.5354812145233154 5.285606861114502
Loss :  1.7461613416671753 2.864422559738159 4.610583782196045
Loss :  1.7414546012878418 3.0702452659606934 4.811699867248535
Loss :  1.7230212688446045 3.4687693119049072 5.191790580749512
Loss :  1.7559901475906372 3.109668254852295 4.865658283233643
Loss :  1.7416971921920776 3.132009744644165 4.873706817626953
Loss :  1.743965744972229 2.9335036277770996 4.677469253540039
Loss :  1.7384092807769775 3.350914716720581 5.089323997497559
  batch 20 loss: 1.7384092807769775, 3.350914716720581, 5.089323997497559
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7390674352645874 3.32746958732605 5.066536903381348
Loss :  1.7491719722747803 3.4579527378082275 5.207124710083008
Loss :  1.7388083934783936 3.9318323135375977 5.67064094543457
Loss :  1.7661747932434082 3.1505544185638428 4.916728973388672
Loss :  1.74380624294281 3.352539539337158 5.096345901489258
Loss :  1.736242651939392 3.112074375152588 4.8483171463012695
Loss :  1.7636138200759888 3.076056480407715 4.839670181274414
Loss :  1.7507330179214478 3.227660655975342 4.9783935546875
Loss :  1.7652002573013306 3.085456609725952 4.850656986236572
Loss :  1.737337589263916 3.220546245574951 4.957883834838867
Loss :  1.7598462104797363 2.9451591968536377 4.705005645751953
Loss :  1.7564375400543213 3.2471916675567627 5.003629207611084
Loss :  1.7431743144989014 2.860464572906494 4.603638648986816
Loss :  1.7262787818908691 2.6919076442718506 4.418186187744141
Loss :  1.7471802234649658 3.0226101875305176 4.7697906494140625
Loss :  1.7515954971313477 3.0135209560394287 4.7651166915893555
Loss :  1.7462701797485352 2.797438144683838 4.543708324432373
Loss :  1.7502487897872925 3.349609851837158 5.09985876083374
Loss :  1.7432907819747925 3.1966543197631836 4.939945220947266
Loss :  1.7395957708358765 2.712099313735962 4.451694965362549
  batch 40 loss: 1.7395957708358765, 2.712099313735962, 4.451694965362549
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.747286081314087 3.1766977310180664 4.923983573913574
Loss :  1.7402269840240479 3.019235849380493 4.759462833404541
Loss :  1.7336148023605347 3.4929816722869873 5.226596355438232
Loss :  1.75528085231781 3.307610273361206 5.062891006469727
Loss :  1.747124195098877 2.661144256591797 4.408268451690674
Loss :  1.7379289865493774 3.0393404960632324 4.77726936340332
Loss :  1.7557952404022217 2.918774366378784 4.674569606781006
Loss :  1.7389297485351562 3.316171169281006 5.055100917816162
Loss :  1.74762761592865 3.4184014797210693 5.16602897644043
Loss :  1.740533709526062 3.6413865089416504 5.381920337677002
Loss :  1.7237025499343872 3.3577487468719482 5.081451416015625
Loss :  1.736240267753601 3.453183650970459 5.18942403793335
Loss :  1.7561829090118408 3.2538552284240723 5.010038375854492
Loss :  1.7432554960250854 3.695995330810547 5.439250946044922
Loss :  1.7322856187820435 3.332165002822876 5.064450740814209
Loss :  1.7475301027297974 2.390533924102783 4.138063907623291
Loss :  1.7446988821029663 3.3644845485687256 5.109183311462402
Loss :  1.740893006324768 3.067131757736206 4.808024883270264
Loss :  1.7480864524841309 2.873020887374878 4.62110710144043
Loss :  1.7508749961853027 2.8289761543273926 4.579851150512695
  batch 60 loss: 1.7508749961853027, 2.8289761543273926, 4.579851150512695
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7518856525421143 3.015714168548584 4.767600059509277
Loss :  1.7477202415466309 3.4357872009277344 5.183507442474365
Loss :  1.7668370008468628 3.5326645374298096 5.299501419067383
Loss :  1.733163833618164 3.558781623840332 5.291945457458496
Loss :  1.723577618598938 2.9891366958618164 4.712714195251465
Loss :  1.8648250102996826 4.050389766693115 5.915214538574219
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8477215766906738 3.82468843460083 5.672410011291504
Loss :  1.836373209953308 3.7601332664489746 5.596506595611572
Loss :  1.904617190361023 3.7537596225738525 5.658376693725586
Total LOSS train 4.942087980417105 valid 5.71062695980072
CE LOSS train 1.746104959341196 valid 0.47615429759025574
Contrastive LOSS train 3.195983024743887 valid 0.9384399056434631
EPOCH 59:
Loss :  1.73859703540802 3.32680606842041 5.065402984619141
Loss :  1.7529540061950684 3.4320218563079834 5.184975624084473
Loss :  1.7386101484298706 3.201770305633545 4.940380573272705
Loss :  1.7478097677230835 3.035985231399536 4.78379487991333
Loss :  1.7540606260299683 2.8984358310699463 4.652496337890625
Loss :  1.7543691396713257 3.4360687732696533 5.1904377937316895
Loss :  1.7459794282913208 3.156489610671997 4.902469158172607
Loss :  1.7468920946121216 2.902906894683838 4.64979887008667
Loss :  1.743888258934021 3.096360683441162 4.840249061584473
Loss :  1.7131450176239014 2.831040620803833 4.544185638427734
Loss :  1.749152660369873 2.934542655944824 4.683695316314697
Loss :  1.7735390663146973 3.1750056743621826 4.948544502258301
Loss :  1.7436413764953613 3.0806164741516113 4.824257850646973
Loss :  1.7407772541046143 2.7681643962860107 4.508941650390625
Loss :  1.739795446395874 3.373894214630127 5.113689422607422
Loss :  1.722814917564392 3.6020569801330566 5.324872016906738
Loss :  1.7480628490447998 3.0699269771575928 4.817989826202393
Loss :  1.7372080087661743 3.232840061187744 4.970047950744629
Loss :  1.7373944520950317 2.822930097579956 4.560324668884277
Loss :  1.728716492652893 2.7577061653137207 4.486422538757324
  batch 20 loss: 1.728716492652893, 2.7577061653137207, 4.486422538757324
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7367192506790161 2.776564598083496 4.513283729553223
Loss :  1.7445094585418701 3.4657163619995117 5.210226058959961
Loss :  1.730852484703064 3.2056193351745605 4.936471939086914
Loss :  1.7528643608093262 3.4144070148468018 5.167271614074707
Loss :  1.740142822265625 3.2813260555267334 5.0214691162109375
Loss :  1.7320700883865356 3.494936943054199 5.227006912231445
Loss :  1.7518991231918335 3.236198663711548 4.988097667694092
Loss :  1.7351206541061401 3.0383641719818115 4.773484706878662
Loss :  1.7534055709838867 2.8402364253997803 4.593642234802246
Loss :  1.727818250656128 3.427143096923828 5.154961585998535
Loss :  1.7602019309997559 3.198789358139038 4.958991050720215
Loss :  1.7425684928894043 2.969064950942993 4.711633682250977
Loss :  1.7319976091384888 2.772894859313965 4.504892349243164
Loss :  1.7209926843643188 3.2093734741210938 4.930366039276123
Loss :  1.742903232574463 2.903747081756592 4.646650314331055
Loss :  1.7468047142028809 3.0532336235046387 4.8000383377075195
Loss :  1.7415623664855957 2.642334222793579 4.383896827697754
Loss :  1.7369693517684937 2.7581143379211426 4.495083808898926
Loss :  1.7387336492538452 3.1434829235076904 4.882216453552246
Loss :  1.7343159914016724 2.8529250621795654 4.587241172790527
  batch 40 loss: 1.7343159914016724, 2.8529250621795654, 4.587241172790527
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7403193712234497 3.1612093448638916 4.901528835296631
Loss :  1.7340831756591797 3.5176737308502197 5.25175666809082
Loss :  1.733128547668457 2.8460464477539062 4.579174995422363
Loss :  1.745527744293213 3.5164055824279785 5.261933326721191
Loss :  1.7430399656295776 3.485699415206909 5.228739261627197
Loss :  1.733949899673462 3.418412446975708 5.15236234664917
Loss :  1.7401072978973389 3.701951742172241 5.44205904006958
Loss :  1.7296531200408936 3.1052627563476562 4.834916114807129
Loss :  1.7336931228637695 3.182314157485962 4.916007041931152
Loss :  1.7395051717758179 3.8138513565063477 5.553356647491455
Loss :  1.7180734872817993 3.1005465984344482 4.818620204925537
Loss :  1.7270151376724243 3.1341445446014404 4.861159801483154
Loss :  1.7448512315750122 3.2237801551818848 4.968631267547607
Loss :  1.737424373626709 3.281358003616333 5.018782615661621
Loss :  1.732017993927002 3.770432233810425 5.502449989318848
Loss :  1.7367836236953735 3.8380539417266846 5.574837684631348
Loss :  1.7423163652420044 3.2300186157226562 4.972334861755371
Loss :  1.7363282442092896 3.4079434871673584 5.1442718505859375
Loss :  1.7418806552886963 3.5304408073425293 5.272321701049805
Loss :  1.7385677099227905 3.189885139465332 4.928452968597412
  batch 60 loss: 1.7385677099227905, 3.189885139465332, 4.928452968597412
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7489968538284302 3.2620086669921875 5.011005401611328
Loss :  1.7429378032684326 3.0082616806030273 4.751199722290039
Loss :  1.7610198259353638 3.4359261989593506 5.196946144104004
Loss :  1.7305891513824463 2.908384084701538 4.638973236083984
Loss :  1.722192645072937 2.4627201557159424 4.18491268157959
Loss :  1.9120409488677979 3.9758803844451904 5.887921333312988
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.888126015663147 3.7965810298919678 5.684707164764404
Loss :  1.889599084854126 3.8796989917755127 5.769298076629639
Loss :  1.9521558284759521 3.6771364212036133 5.6292924880981445
Total LOSS train 4.914563641181359 valid 5.742804765701294
CE LOSS train 1.7399055792735174 valid 0.48803895711898804
Contrastive LOSS train 3.1746580527378963 valid 0.9192841053009033
EPOCH 60:
Loss :  1.736535906791687 3.1202802658081055 4.856816291809082
Loss :  1.7494676113128662 3.2434353828430176 4.992902755737305
Loss :  1.7299714088439941 3.173682451248169 4.903654098510742
Loss :  1.7370773553848267 3.102205991744995 4.839283466339111
Loss :  1.7495367527008057 3.332871675491333 5.082408428192139
Loss :  1.7488715648651123 3.050774335861206 4.799645900726318
Loss :  1.7417229413986206 3.0625710487365723 4.804294109344482
Loss :  1.742270827293396 3.2095768451690674 4.951847553253174
Loss :  1.740670084953308 3.116042137145996 4.856712341308594
Loss :  1.7057665586471558 3.2316927909851074 4.937459468841553
Loss :  1.743457555770874 3.2099993228912354 4.953456878662109
Loss :  1.7686235904693604 2.9839694499969482 4.752593040466309
Loss :  1.7396844625473022 2.89253306388855 4.6322174072265625
Loss :  1.7363773584365845 3.0757429599761963 4.81212043762207
Loss :  1.7364249229431152 3.806041717529297 5.542466640472412
Loss :  1.718045711517334 3.6625967025756836 5.380642414093018
Loss :  1.746518611907959 2.9563605785369873 4.702878952026367
Loss :  1.7339787483215332 3.1331253051757812 4.8671040534973145
Loss :  1.7334814071655273 2.5093178749084473 4.242799282073975
Loss :  1.7289586067199707 2.973706007003784 4.702664375305176
  batch 20 loss: 1.7289586067199707, 2.973706007003784, 4.702664375305176
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7363550662994385 3.1928954124450684 4.929250717163086
Loss :  1.7419981956481934 3.0075020790100098 4.749500274658203
Loss :  1.7353014945983887 2.359710454940796 4.0950117111206055
Loss :  1.7534617185592651 3.0403823852539062 4.793844223022461
Loss :  1.7387717962265015 3.635737180709839 5.374508857727051
Loss :  1.732987403869629 2.9027864933013916 4.635773658752441
Loss :  1.7540010213851929 3.4605095386505127 5.214510440826416
Loss :  1.7395923137664795 3.1588549613952637 4.898447036743164
Loss :  1.755236268043518 3.4812045097351074 5.236440658569336
Loss :  1.7277854681015015 3.6164097785949707 5.344195365905762
Loss :  1.7596185207366943 3.3657917976379395 5.125410079956055
Loss :  1.74138343334198 3.290860414505005 5.032243728637695
Loss :  1.7309285402297974 3.2271342277526855 4.958062648773193
Loss :  1.7218425273895264 2.9862773418426514 4.708119869232178
Loss :  1.7401763200759888 2.8289196491241455 4.569096088409424
Loss :  1.7455037832260132 3.148658514022827 4.894162178039551
Loss :  1.7398327589035034 2.8254785537719727 4.565311431884766
Loss :  1.7397100925445557 3.0049901008605957 4.7447004318237305
Loss :  1.7370624542236328 2.738140344619751 4.475202560424805
Loss :  1.7349185943603516 2.510930299758911 4.245848655700684
  batch 40 loss: 1.7349185943603516, 2.510930299758911, 4.245848655700684
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.73992919921875 2.704514503479004 4.444443702697754
Loss :  1.7314053773880005 2.3852717876434326 4.116677284240723
Loss :  1.7286686897277832 2.452606439590454 4.181275367736816
Loss :  1.7454406023025513 3.240199327468872 4.985640048980713
Loss :  1.7419058084487915 3.0827956199645996 4.824701309204102
Loss :  1.7313932180404663 2.6205389499664307 4.351932048797607
Loss :  1.7439494132995605 2.564244031906128 4.308193206787109
Loss :  1.7300790548324585 2.8679895401000977 4.598068714141846
Loss :  1.7376683950424194 3.097490072250366 4.835158348083496
Loss :  1.733474612236023 3.471102476119995 5.2045769691467285
Loss :  1.7106701135635376 2.8579421043395996 4.568612098693848
Loss :  1.7222787141799927 2.8251118659973145 4.547390460968018
Loss :  1.7443331480026245 2.6698880195617676 4.414221286773682
Loss :  1.733573079109192 2.941023826599121 4.674596786499023
Loss :  1.727818250656128 2.7489068508148193 4.476725101470947
Loss :  1.7351917028427124 2.7099504470825195 4.4451422691345215
Loss :  1.7421356439590454 2.6728546619415283 4.414990425109863
Loss :  1.7369811534881592 2.830522298812866 4.567503452301025
Loss :  1.7383339405059814 3.7229654788970947 5.461299419403076
Loss :  1.7405956983566284 4.173906326293945 5.914502143859863
  batch 60 loss: 1.7405956983566284, 4.173906326293945, 5.914502143859863
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7432489395141602 3.555874824523926 5.299123764038086
Loss :  1.7421600818634033 3.8813955783843994 5.623555660247803
Loss :  1.7560276985168457 3.441270589828491 5.197298049926758
Loss :  1.7267200946807861 3.9117674827575684 5.638487815856934
Loss :  1.7200391292572021 3.3115603923797607 5.031599521636963
Loss :  1.9751430749893188 3.9940574169158936 5.969200611114502
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9307814836502075 4.0134053230285645 5.944186687469482
Loss :  1.9433057308197021 3.846585750579834 5.789891242980957
Loss :  2.0153205394744873 3.982891082763672 5.998211860656738
Total LOSS train 4.835835750286396 valid 5.92537260055542
CE LOSS train 1.7378143310546874 valid 0.5038301348686218
Contrastive LOSS train 3.098021437571599 valid 0.995722770690918
EPOCH 61:
Loss :  1.7316060066223145 3.5623178482055664 5.293923854827881
Loss :  1.7446986436843872 3.2757534980773926 5.02045202255249
Loss :  1.7251484394073486 3.0911238193511963 4.816272258758545
Loss :  1.734060287475586 2.8586809635162354 4.592741012573242
Loss :  1.7444878816604614 2.7956206798553467 4.540108680725098
Loss :  1.7438944578170776 3.9140217304229736 5.657916069030762
Loss :  1.7380852699279785 2.803194284439087 4.5412797927856445
Loss :  1.7376275062561035 2.8836824893951416 4.621310234069824
Loss :  1.736384391784668 3.142144203186035 4.878528594970703
Loss :  1.7023042440414429 2.7542402744293213 4.456544399261475
Loss :  1.738327980041504 2.778736114501953 4.517064094543457
Loss :  1.766144871711731 2.7984986305236816 4.564643383026123
Loss :  1.7343112230300903 2.6148393154144287 4.349150657653809
Loss :  1.729800820350647 2.61195969581604 4.341760635375977
Loss :  1.7369905710220337 2.9192240238189697 4.656214714050293
Loss :  1.7154088020324707 2.4936554431915283 4.209064483642578
Loss :  1.73923659324646 3.128243923187256 4.867480278015137
Loss :  1.7275595664978027 3.339249610900879 5.066809177398682
Loss :  1.7306917905807495 2.766443967819214 4.497135639190674
Loss :  1.7224279642105103 2.993732213973999 4.716160297393799
  batch 20 loss: 1.7224279642105103, 2.993732213973999, 4.716160297393799
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7306503057479858 3.0206844806671143 4.7513346672058105
Loss :  1.734850287437439 3.0738515853881836 4.808701992034912
Loss :  1.7289605140686035 2.7863552570343018 4.515316009521484
Loss :  1.7427107095718384 3.014599561691284 4.757310390472412
Loss :  1.7352476119995117 3.2660436630249023 5.001291275024414
Loss :  1.7275689840316772 3.2491776943206787 4.976746559143066
Loss :  1.743301510810852 3.153017282485962 4.8963189125061035
Loss :  1.7233086824417114 3.444399118423462 5.167707920074463
Loss :  1.7452194690704346 3.3565025329589844 5.10172176361084
Loss :  1.7233294248580933 3.6661534309387207 5.3894829750061035
Loss :  1.7614192962646484 3.927123546600342 5.68854284286499
Loss :  1.7309179306030273 4.089876174926758 5.820794105529785
Loss :  1.7235361337661743 3.9008336067199707 5.6243696212768555
Loss :  1.7146830558776855 3.4154679775238037 5.13015079498291
Loss :  1.7358603477478027 3.497429132461548 5.23328971862793
Loss :  1.7408491373062134 3.2995877265930176 5.040436744689941
Loss :  1.7340017557144165 2.9434149265289307 4.677416801452637
Loss :  1.7189626693725586 2.727292776107788 4.446255683898926
Loss :  1.7303935289382935 2.524359703063965 4.254753112792969
Loss :  1.7286639213562012 2.666308879852295 4.394972801208496
  batch 40 loss: 1.7286639213562012, 2.666308879852295, 4.394972801208496
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.731844186782837 2.5889880657196045 4.320832252502441
Loss :  1.7278438806533813 2.4541404247283936 4.1819844245910645
Loss :  1.7270104885101318 2.5767393112182617 4.303750038146973
Loss :  1.741148591041565 2.5038702487945557 4.24501895904541
Loss :  1.7363510131835938 2.266326904296875 4.002677917480469
Loss :  1.728891134262085 3.376399517059326 5.105290412902832
Loss :  1.73684823513031 3.091125965118408 4.827974319458008
Loss :  1.7282947301864624 3.339465379714966 5.067759990692139
Loss :  1.73223078250885 3.0391554832458496 4.77138614654541
Loss :  1.7328459024429321 3.486621856689453 5.219467639923096
Loss :  1.711274266242981 3.0512495040893555 4.762523651123047
Loss :  1.718379259109497 2.7903940677642822 4.508773326873779
Loss :  1.743135690689087 2.9261057376861572 4.669241428375244
Loss :  1.734816312789917 3.145481824874878 4.880298137664795
Loss :  1.7276883125305176 3.1443426609039307 4.872031211853027
Loss :  1.7274576425552368 2.7021679878234863 4.429625511169434
Loss :  1.7393863201141357 3.110161781311035 4.84954833984375
Loss :  1.733547329902649 3.474308967590332 5.207856178283691
Loss :  1.736142635345459 3.6198034286499023 5.355946063995361
Loss :  1.7364224195480347 4.0493950843811035 5.785817623138428
  batch 60 loss: 1.7364224195480347, 4.0493950843811035, 5.785817623138428
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7418711185455322 3.440115451812744 5.1819868087768555
Loss :  1.740605115890503 3.1432321071624756 4.8838372230529785
Loss :  1.7518455982208252 3.8160006999969482 5.567846298217773
Loss :  1.7240705490112305 3.5888781547546387 5.312948703765869
Loss :  1.7169897556304932 2.941464424133301 4.658453941345215
Loss :  1.99081552028656 3.947136878967285 5.937952518463135
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9503282308578491 4.063815116882324 6.014143466949463
Loss :  1.9623076915740967 3.7380027770996094 5.700310707092285
Loss :  2.036621570587158 3.8430817127227783 5.879703521728516
Total LOSS train 4.843913100315974 valid 5.88302755355835
CE LOSS train 1.7329319055263812 valid 0.5091553926467896
Contrastive LOSS train 3.1109811819516695 valid 0.9607704281806946
EPOCH 62:
Loss :  1.7237690687179565 3.286160469055176 5.009929656982422
Loss :  1.740276575088501 3.579815149307251 5.320091724395752
Loss :  1.7201124429702759 2.9096343517303467 4.629746913909912
Loss :  1.7302892208099365 2.8870937824249268 4.617383003234863
Loss :  1.741014003753662 3.0319972038269043 4.773011207580566
Loss :  1.7407644987106323 2.721909761428833 4.462674140930176
Loss :  1.7389427423477173 2.7702298164367676 4.509172439575195
Loss :  1.7376132011413574 2.6424009799957275 4.380014419555664
Loss :  1.7349376678466797 2.5689167976379395 4.303854465484619
Loss :  1.701257348060608 2.297649383544922 3.9989066123962402
Loss :  1.7380309104919434 3.022188425064087 4.760219573974609
Loss :  1.7644187211990356 2.9880857467651367 4.752504348754883
Loss :  1.732987403869629 2.810502052307129 4.543489456176758
Loss :  1.7309659719467163 2.7103474140167236 4.44131326675415
Loss :  1.7321311235427856 2.91750168800354 4.649632930755615
Loss :  1.714705467224121 2.6432526111602783 4.35795783996582
Loss :  1.7409027814865112 2.5027880668640137 4.2436909675598145
Loss :  1.7285698652267456 2.6157586574554443 4.3443284034729
Loss :  1.7283658981323242 2.5921900272369385 4.320555686950684
Loss :  1.7233210802078247 2.854414701461792 4.577735900878906
  batch 20 loss: 1.7233210802078247, 2.854414701461792, 4.577735900878906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7296912670135498 2.829087495803833 4.558778762817383
Loss :  1.735913634300232 3.107306718826294 4.843220233917236
Loss :  1.7290116548538208 2.6074094772338867 4.336421012878418
Loss :  1.7439777851104736 2.890549659729004 4.634527206420898
Loss :  1.733384370803833 3.362189769744873 5.095574378967285
Loss :  1.7266559600830078 3.1663548946380615 4.893011093139648
Loss :  1.7462446689605713 3.128730535507202 4.874975204467773
Loss :  1.7307465076446533 3.18190860748291 4.912654876708984
Loss :  1.7483086585998535 3.3876500129699707 5.135958671569824
Loss :  1.7238678932189941 3.601661205291748 5.325529098510742
Loss :  1.757800817489624 2.9325296878814697 4.690330505371094
Loss :  1.7359768152236938 3.286850690841675 5.022827625274658
Loss :  1.725512146949768 2.805716037750244 4.531228065490723
Loss :  1.716109037399292 3.3604235649108887 5.076532363891602
Loss :  1.7379231452941895 3.4635119438171387 5.201435089111328
Loss :  1.7410629987716675 3.813066005706787 5.554129123687744
Loss :  1.73790442943573 3.020580291748047 4.758484840393066
Loss :  1.728294849395752 3.338488817214966 5.066783905029297
Loss :  1.7305457592010498 2.9909074306488037 4.7214531898498535
Loss :  1.7296282052993774 2.6557157039642334 4.3853440284729
  batch 40 loss: 1.7296282052993774, 2.6557157039642334, 4.3853440284729
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7317276000976562 2.954124927520752 4.685852527618408
Loss :  1.7268931865692139 2.770301580429077 4.497194766998291
Loss :  1.7248814105987549 3.2684226036071777 4.993304252624512
Loss :  1.740999698638916 3.694164514541626 5.435164451599121
Loss :  1.7365880012512207 3.2522170543670654 4.988804817199707
Loss :  1.7270861864089966 3.4083454608917236 5.13543176651001
Loss :  1.7360296249389648 3.7478420734405518 5.4838714599609375
Loss :  1.7250430583953857 3.209529399871826 4.934572219848633
Loss :  1.7312471866607666 3.7191081047058105 5.450355529785156
Loss :  1.729932427406311 3.0117318630218506 4.741664409637451
Loss :  1.709222435951233 3.189913511276245 4.899136066436768
Loss :  1.7165709733963013 3.088747978210449 4.805318832397461
Loss :  1.7394222021102905 2.679635763168335 4.419057846069336
Loss :  1.7349731922149658 3.334669351577759 5.069642543792725
Loss :  1.727054238319397 2.9096672534942627 4.636721611022949
Loss :  1.725562572479248 3.204801082611084 4.930363655090332
Loss :  1.7377030849456787 3.158186674118042 4.895889759063721
Loss :  1.7344133853912354 2.9668264389038086 4.701239585876465
Loss :  1.7359685897827148 3.19223952293396 4.928208351135254
Loss :  1.7366902828216553 3.2981984615325928 5.034888744354248
  batch 60 loss: 1.7366902828216553, 3.2981984615325928, 5.034888744354248
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7419744729995728 3.1804916858673096 4.922466278076172
Loss :  1.7401703596115112 2.681258201599121 4.421428680419922
Loss :  1.7549508810043335 3.11620831489563 4.871159076690674
Loss :  1.7221084833145142 3.4875874519348145 5.209695816040039
Loss :  1.7183489799499512 2.7155823707580566 4.433931350708008
Loss :  1.9707971811294556 4.094553470611572 6.065350532531738
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9396562576293945 4.041955471038818 5.981611728668213
Loss :  1.9479269981384277 4.005756855010986 5.953683853149414
Loss :  2.011110305786133 3.877009391784668 5.888119697570801
Total LOSS train 4.786781178987943 valid 5.9721914529800415
CE LOSS train 1.7325769094320445 valid 0.5027775764465332
Contrastive LOSS train 3.0542042658879205 valid 0.969252347946167
EPOCH 63:
Loss :  1.7265686988830566 3.120004892349243 4.846573829650879
Loss :  1.7409271001815796 3.0888102054595947 4.829737186431885
Loss :  1.7221750020980835 2.6613857746124268 4.383560657501221
Loss :  1.7344927787780762 2.6792616844177246 4.413754463195801
Loss :  1.7433944940567017 2.612562894821167 4.355957508087158
Loss :  1.7424037456512451 2.6228206157684326 4.365224361419678
Loss :  1.7372335195541382 3.186593532562256 4.923827171325684
Loss :  1.7368824481964111 3.093425989151001 4.830308437347412
Loss :  1.737573504447937 3.110156536102295 4.8477301597595215
Loss :  1.6992167234420776 3.6222386360168457 5.321455478668213
Loss :  1.7401940822601318 3.104581594467163 4.844775676727295
Loss :  1.7630255222320557 2.7051291465759277 4.4681549072265625
Loss :  1.7349555492401123 3.273998498916626 5.008954048156738
Loss :  1.727736473083496 2.827864170074463 4.555600643157959
Loss :  1.7317144870758057 2.9014577865600586 4.633172035217285
Loss :  1.7116749286651611 2.860995292663574 4.572669982910156
Loss :  1.7407863140106201 2.74456524848938 4.4853515625
Loss :  1.7295881509780884 3.2532410621643066 4.9828290939331055
Loss :  1.7298229932785034 3.133376121520996 4.863199234008789
Loss :  1.723100185394287 3.3970468044281006 5.120146751403809
  batch 20 loss: 1.723100185394287, 3.3970468044281006, 5.120146751403809
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.726484775543213 3.005469799041748 4.731954574584961
Loss :  1.733756422996521 2.938035249710083 4.6717915534973145
Loss :  1.7277525663375854 2.6197001934051514 4.347452640533447
Loss :  1.7420933246612549 2.9790236949920654 4.72111701965332
Loss :  1.733465313911438 3.3831465244293213 5.116611957550049
Loss :  1.7245436906814575 2.898736000061035 4.623279571533203
Loss :  1.7433819770812988 2.573716402053833 4.317098617553711
Loss :  1.7266420125961304 3.047842264175415 4.774484157562256
Loss :  1.7448257207870483 2.697160005569458 4.441985607147217
Loss :  1.7230899333953857 2.8089444637298584 4.532034397125244
Loss :  1.758134365081787 3.6407320499420166 5.398866653442383
Loss :  1.7325526475906372 3.852114200592041 5.584666728973389
Loss :  1.7215546369552612 3.282383680343628 5.0039381980896
Loss :  1.7150063514709473 3.353557825088501 5.068564414978027
Loss :  1.73594331741333 3.208451271057129 4.944394588470459
Loss :  1.7397797107696533 3.1435158252716064 4.88329553604126
Loss :  1.7334800958633423 2.77885103225708 4.512331008911133
Loss :  1.7223148345947266 2.322741985321045 4.0450568199157715
Loss :  1.729053258895874 2.5797345638275146 4.308787822723389
Loss :  1.7274857759475708 2.762789726257324 4.4902753829956055
  batch 40 loss: 1.7274857759475708, 2.762789726257324, 4.4902753829956055
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7294845581054688 3.090099573135376 4.819583892822266
Loss :  1.7273019552230835 2.726735830307007 4.454037666320801
Loss :  1.7253620624542236 2.5928900241851807 4.318252086639404
Loss :  1.7405818700790405 2.716461658477783 4.457043647766113
Loss :  1.7367885112762451 2.744825601577759 4.481614112854004
Loss :  1.727502465248108 2.418574571609497 4.1460771560668945
Loss :  1.7362117767333984 2.41300892829895 4.1492204666137695
Loss :  1.726112723350525 2.1462714672088623 3.8723840713500977
Loss :  1.7337011098861694 2.1957905292510986 3.9294915199279785
Loss :  1.73190176486969 2.2005503177642822 3.9324522018432617
Loss :  1.7104133367538452 2.841884136199951 4.552297592163086
Loss :  1.7207270860671997 2.6390459537506104 4.3597731590271
Loss :  1.7410516738891602 2.7460408210754395 4.4870924949646
Loss :  1.7363661527633667 3.4118993282318115 5.148265361785889
Loss :  1.7310553789138794 3.437131643295288 5.168187141418457
Loss :  1.728047490119934 2.7614057064056396 4.489453315734863
Loss :  1.7369003295898438 3.224604368209839 4.961504936218262
Loss :  1.7350960969924927 2.862013339996338 4.597109317779541
Loss :  1.7364330291748047 3.2551677227020264 4.99160099029541
Loss :  1.7372218370437622 2.8386330604553223 4.575854778289795
  batch 60 loss: 1.7372218370437622, 2.8386330604553223, 4.575854778289795
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7387384176254272 2.8476407527923584 4.586379051208496
Loss :  1.740788221359253 2.9720044136047363 4.71279239654541
Loss :  1.7498668432235718 2.9559481143951416 4.705814838409424
Loss :  1.7228680849075317 3.128368854522705 4.851236820220947
Loss :  1.7210055589675903 3.378411054611206 5.099416732788086
Loss :  1.9695799350738525 4.096986770629883 6.066566467285156
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.939578890800476 4.096699237823486 6.036278247833252
Loss :  1.9474165439605713 3.871209144592285 5.818625450134277
Loss :  2.0056450366973877 3.9241526126861572 5.929797649383545
Total LOSS train 4.66181394136869 valid 5.962816953659058
CE LOSS train 1.7322513195184561 valid 0.5014112591743469
Contrastive LOSS train 2.929562631020179 valid 0.9810381531715393
EPOCH 64:
Loss :  1.7275854349136353 2.887211561203003 4.614797115325928
Loss :  1.739158272743225 3.4092488288879395 5.148406982421875
Loss :  1.7229118347167969 3.8933613300323486 5.616272926330566
Loss :  1.7374500036239624 3.3850440979003906 5.122494220733643
Loss :  1.7412972450256348 3.1863110065460205 4.927608489990234
Loss :  1.7400758266448975 3.1799826622009277 4.920058250427246
Loss :  1.7341785430908203 3.359325647354126 5.093503952026367
Loss :  1.735162615776062 3.58158540725708 5.316748142242432
Loss :  1.7350523471832275 3.3826003074645996 5.117652893066406
Loss :  1.6980817317962646 3.7521884441375732 5.450270175933838
Loss :  1.738607406616211 3.123246669769287 4.861854076385498
Loss :  1.7608939409255981 3.8426668643951416 5.603560924530029
Loss :  1.7316755056381226 3.216487407684326 4.948163032531738
Loss :  1.7271716594696045 2.4969801902770996 4.224151611328125
Loss :  1.7303211688995361 2.4652795791625977 4.195600509643555
Loss :  1.7111713886260986 2.409224510192871 4.120395660400391
Loss :  1.7389198541641235 2.5928092002868652 4.331728935241699
Loss :  1.7270333766937256 2.204040765762329 3.9310741424560547
Loss :  1.729564905166626 2.0910494327545166 3.8206143379211426
Loss :  1.7194342613220215 2.6177632808685303 4.337197303771973
  batch 20 loss: 1.7194342613220215, 2.6177632808685303, 4.337197303771973
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.725725769996643 2.75648832321167 4.482213973999023
Loss :  1.733076810836792 2.8721096515655518 4.605186462402344
Loss :  1.726973056793213 2.949977159500122 4.676950454711914
Loss :  1.739577293395996 3.056401252746582 4.795978546142578
Loss :  1.7326542139053345 3.7036402225494385 5.4362945556640625
Loss :  1.7235151529312134 2.7017853260040283 4.425300598144531
Loss :  1.7428590059280396 3.2029025554656982 4.945761680603027
Loss :  1.7248892784118652 3.204702138900757 4.929591178894043
Loss :  1.744569182395935 2.8964641094207764 4.641033172607422
Loss :  1.7209793329238892 3.36372709274292 5.0847063064575195
Loss :  1.7573119401931763 3.022554636001587 4.779866695404053
Loss :  1.7323131561279297 2.9040815830230713 4.636394500732422
Loss :  1.7209290266036987 3.145782232284546 4.866711139678955
Loss :  1.714158296585083 2.496490716934204 4.210649013519287
Loss :  1.7365161180496216 2.5825300216674805 4.3190460205078125
Loss :  1.7391207218170166 2.3573503494262695 4.096470832824707
Loss :  1.7340787649154663 2.3734850883483887 4.1075639724731445
Loss :  1.7227916717529297 2.327255964279175 4.050047874450684
Loss :  1.7286152839660645 3.5963454246520996 5.324960708618164
Loss :  1.7283915281295776 3.3621697425842285 5.090561389923096
  batch 40 loss: 1.7283915281295776, 3.3621697425842285, 5.090561389923096
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7322028875350952 3.3834283351898193 5.115631103515625
Loss :  1.7278820276260376 3.5264668464660645 5.2543487548828125
Loss :  1.7254276275634766 2.9757943153381348 4.701221942901611
Loss :  1.7424834966659546 3.432248592376709 5.174732208251953
Loss :  1.7359503507614136 3.055663585662842 4.791614055633545
Loss :  1.7267488241195679 3.572512149810791 5.299261093139648
Loss :  1.7355003356933594 3.7806084156036377 5.516108512878418
Loss :  1.7245287895202637 3.706815481185913 5.431344032287598
Loss :  1.7253774404525757 3.47292160987854 5.198298931121826
Loss :  1.7342419624328613 3.5652825832366943 5.299524307250977
Loss :  1.7103179693222046 3.897122383117676 5.60744047164917
Loss :  1.7146474123001099 3.446460485458374 5.161108016967773
Loss :  1.7340238094329834 3.2454302310943604 4.979454040527344
Loss :  1.7425479888916016 3.3060784339904785 5.04862642288208
Loss :  1.7341463565826416 3.449176073074341 5.183322429656982
Loss :  1.7175250053405762 3.405074119567871 5.122599124908447
Loss :  1.7358405590057373 3.656501293182373 5.392341613769531
Loss :  1.7335410118103027 3.3003766536712646 5.033917427062988
Loss :  1.7335408926010132 3.3867814540863037 5.120322227478027
Loss :  1.7291505336761475 2.876288652420044 4.605439186096191
  batch 60 loss: 1.7291505336761475, 2.876288652420044, 4.605439186096191
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7339391708374023 2.424602746963501 4.158541679382324
Loss :  1.7364280223846436 2.3925704956054688 4.128998756408691
Loss :  1.7445385456085205 2.18996524810791 3.9345037937164307
Loss :  1.7203896045684814 2.4573867321014404 4.177776336669922
Loss :  1.717492938041687 2.063610553741455 3.7811036109924316
Loss :  1.8237019777297974 4.1316752433776855 5.955377101898193
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8119829893112183 4.096269607543945 5.908252716064453
Loss :  1.8217531442642212 3.732865333557129 5.5546183586120605
Loss :  1.8609076738357544 3.8424222469329834 5.703330039978027
Total LOSS train 4.806538812930768 valid 5.780394554138184
CE LOSS train 1.730818561407236 valid 0.4652269184589386
Contrastive LOSS train 3.0757202808673565 valid 0.9606055617332458
EPOCH 65:
Loss :  1.7231591939926147 2.3105690479278564 4.033728122711182
Loss :  1.7332541942596436 2.607114315032959 4.340368270874023
Loss :  1.7206330299377441 2.8866000175476074 4.607233047485352
Loss :  1.7375309467315674 2.831310987472534 4.568841934204102
Loss :  1.738077998161316 2.8565261363983154 4.594604015350342
Loss :  1.7397562265396118 3.043766498565674 4.783522605895996
Loss :  1.7337285280227661 3.118093252182007 4.8518218994140625
Loss :  1.7316232919692993 3.576324939727783 5.307948112487793
Loss :  1.7370857000350952 2.6456727981567383 4.382758617401123
Loss :  1.6958575248718262 2.6057541370391846 4.30161190032959
Loss :  1.7371537685394287 3.045732021331787 4.782885551452637
Loss :  1.7613176107406616 2.9169278144836426 4.678245544433594
Loss :  1.7322819232940674 2.677121639251709 4.4094038009643555
Loss :  1.7274878025054932 3.0882458686828613 4.815733909606934
Loss :  1.7296605110168457 2.4541537761688232 4.18381404876709
Loss :  1.7095038890838623 2.4009323120117188 4.11043643951416
Loss :  1.7397745847702026 2.990950345993042 4.730724811553955
Loss :  1.7264835834503174 3.3646512031555176 5.091135025024414
Loss :  1.7296154499053955 3.2513551712036133 4.98097038269043
Loss :  1.7213677167892456 3.1411774158477783 4.862545013427734
  batch 20 loss: 1.7213677167892456, 3.1411774158477783, 4.862545013427734
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7242050170898438 3.1093380451202393 4.833542823791504
Loss :  1.7321044206619263 3.058361768722534 4.79046630859375
Loss :  1.7248730659484863 2.712615489959717 4.437488555908203
Loss :  1.7390315532684326 3.5401670932769775 5.27919864654541
Loss :  1.734293818473816 3.0648982524871826 4.799191951751709
Loss :  1.7224928140640259 2.5385029315948486 4.260995864868164
Loss :  1.740118384361267 2.8544905185699463 4.594608783721924
Loss :  1.7198799848556519 2.639914035797119 4.3597941398620605
Loss :  1.7401492595672607 2.9478964805603027 4.688045501708984
Loss :  1.7232309579849243 3.4673445224761963 5.19057559967041
Loss :  1.7585232257843018 3.0669007301330566 4.8254241943359375
Loss :  1.730420470237732 3.172278881072998 4.9026994705200195
Loss :  1.7190190553665161 3.7190330028533936 5.438052177429199
Loss :  1.7114403247833252 3.187211513519287 4.898652076721191
Loss :  1.738147497177124 2.6636576652526855 4.4018049240112305
Loss :  1.7366855144500732 2.468426465988159 4.205111980438232
Loss :  1.732852816581726 2.389910936355591 4.122763633728027
Loss :  1.7175148725509644 2.456244468688965 4.173759460449219
Loss :  1.7258658409118652 2.3918967247009277 4.117762565612793
Loss :  1.7234362363815308 2.282008409500122 4.005444526672363
  batch 40 loss: 1.7234362363815308, 2.282008409500122, 4.005444526672363
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7258152961730957 2.8152740001678467 4.541089057922363
Loss :  1.727006435394287 2.2441136837005615 3.9711201190948486
Loss :  1.7274495363235474 2.2844042778015137 4.0118536949157715
Loss :  1.7403593063354492 2.1623122692108154 3.9026715755462646
Loss :  1.7365732192993164 2.5043282508850098 4.240901470184326
Loss :  1.7252955436706543 2.786658525466919 4.511954307556152
Loss :  1.7297228574752808 2.7635245323181152 4.4932475090026855
Loss :  1.7260597944259644 3.3572847843170166 5.083344459533691
Loss :  1.7246274948120117 3.666562795639038 5.391190528869629
Loss :  1.7334312200546265 3.568697452545166 5.302128791809082
Loss :  1.709407925605774 3.1568238735198975 4.866231918334961
Loss :  1.7159080505371094 3.1260874271392822 4.8419952392578125
Loss :  1.733921766281128 3.3030874729156494 5.037009239196777
Loss :  1.7390415668487549 3.1392552852630615 4.878296852111816
Loss :  1.734568476676941 3.0288314819335938 4.763400077819824
Loss :  1.7160344123840332 3.1199772357940674 4.83601188659668
Loss :  1.7325910329818726 3.5352940559387207 5.267885208129883
Loss :  1.7350399494171143 3.244077205657959 4.979117393493652
Loss :  1.7345216274261475 3.7512099742889404 5.485731601715088
Loss :  1.7260054349899292 3.5464470386505127 5.272452354431152
  batch 60 loss: 1.7260054349899292, 3.5464470386505127, 5.272452354431152
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.728882074356079 3.9261879920959473 5.6550703048706055
Loss :  1.7349380254745483 3.3703513145446777 5.105289459228516
Loss :  1.737804889678955 3.0833475589752197 4.821152687072754
Loss :  1.7164723873138428 3.3154635429382324 5.031935691833496
Loss :  1.7145744562149048 3.075784921646118 4.7903594970703125
Loss :  1.9241259098052979 4.0471367835998535 5.9712629318237305
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9021166563034058 3.9964094161987305 5.898526191711426
Loss :  1.9138940572738647 3.898850202560425 5.812744140625
Loss :  1.9595611095428467 3.7787024974823 5.7382636070251465
Total LOSS train 4.70500241793119 valid 5.855199217796326
CE LOSS train 1.729318329004141 valid 0.48989027738571167
Contrastive LOSS train 2.975684070587158 valid 0.944675624370575
EPOCH 66:
Loss :  1.7194604873657227 3.0861074924468994 4.805567741394043
Loss :  1.7250090837478638 3.1402463912963867 4.865255355834961
Loss :  1.7157562971115112 2.889080286026001 4.604836463928223
Loss :  1.735769510269165 2.9373087882995605 4.673078536987305
Loss :  1.732890248298645 3.0384209156036377 4.771311283111572
Loss :  1.7353774309158325 3.003063440322876 4.738440990447998
Loss :  1.7273163795471191 2.7555899620056152 4.482906341552734
Loss :  1.7251259088516235 3.1094284057617188 4.834554195404053
Loss :  1.7320244312286377 2.819082736968994 4.551107406616211
Loss :  1.6942832469940186 2.6516430377960205 4.345926284790039
Loss :  1.7312504053115845 2.925830125808716 4.65708065032959
Loss :  1.7601529359817505 2.70536732673645 4.46552038192749
Loss :  1.7264549732208252 2.547687292098999 4.274142265319824
Loss :  1.724595308303833 2.3811211585998535 4.105716705322266
Loss :  1.7286263704299927 2.6627280712127686 4.391354560852051
Loss :  1.708351492881775 2.3862874507904053 4.094638824462891
Loss :  1.7356849908828735 2.3797144889831543 4.115399360656738
Loss :  1.7230775356292725 2.5149827003479004 4.238059997558594
Loss :  1.7282830476760864 2.8696186542510986 4.597901821136475
Loss :  1.7181949615478516 2.870300054550171 4.588495254516602
  batch 20 loss: 1.7181949615478516, 2.870300054550171, 4.588495254516602
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.723692774772644 2.666621446609497 4.390314102172852
Loss :  1.7316709756851196 2.7257256507873535 4.457396507263184
Loss :  1.7240456342697144 3.6481525897979736 5.372198104858398
Loss :  1.7393159866333008 3.2577104568481445 4.997026443481445
Loss :  1.7327252626419067 3.16762375831604 4.900349140167236
Loss :  1.7227598428726196 3.237464427947998 4.960224151611328
Loss :  1.7388139963150024 3.5467026233673096 5.285516738891602
Loss :  1.7195912599563599 2.6193947792053223 4.338985919952393
Loss :  1.740670084953308 2.433029890060425 4.173699855804443
Loss :  1.7208911180496216 3.5293943881988525 5.250285625457764
Loss :  1.7550626993179321 3.6542253494262695 5.409287929534912
Loss :  1.7296080589294434 3.4567744731903076 5.186382293701172
Loss :  1.7189228534698486 2.6528899669647217 4.37181282043457
Loss :  1.7114118337631226 2.774186611175537 4.485598564147949
Loss :  1.7361819744110107 3.0791685581207275 4.815350532531738
Loss :  1.7339438199996948 3.1046478748321533 4.838591575622559
Loss :  1.7332978248596191 2.818284273147583 4.551582336425781
Loss :  1.7156939506530762 2.705699920654297 4.421393871307373
Loss :  1.7227874994277954 2.910611867904663 4.633399486541748
Loss :  1.7222627401351929 3.3680710792541504 5.090333938598633
  batch 40 loss: 1.7222627401351929, 3.3680710792541504, 5.090333938598633
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7243398427963257 3.032994270324707 4.757334232330322
Loss :  1.725143551826477 2.9066579341888428 4.631801605224609
Loss :  1.7243692874908447 2.8409106731414795 4.565279960632324
Loss :  1.7384001016616821 3.031020402908325 4.769420623779297
Loss :  1.7345374822616577 3.376722812652588 5.111260414123535
Loss :  1.7243107557296753 2.6379218101501465 4.362232685089111
Loss :  1.72785484790802 2.163893222808838 3.8917479515075684
Loss :  1.7237486839294434 2.1369993686676025 3.860748052597046
Loss :  1.7242170572280884 2.205448865890503 3.929666042327881
Loss :  1.7307212352752686 2.7270190715789795 4.457740306854248
Loss :  1.7077691555023193 3.0026206970214844 4.710390090942383
Loss :  1.7128772735595703 2.9074151515960693 4.620292663574219
Loss :  1.7349419593811035 3.3002443313598633 5.035186290740967
Loss :  1.733871340751648 2.4793269634246826 4.213198184967041
Loss :  1.731783151626587 2.642491102218628 4.374274253845215
Loss :  1.7180445194244385 2.768454074859619 4.486498832702637
Loss :  1.7327237129211426 2.724487781524658 4.457211494445801
Loss :  1.7328850030899048 3.3192734718322754 5.052158355712891
Loss :  1.7326719760894775 2.7222065925598145 4.454878807067871
Loss :  1.7303706407546997 2.517592430114746 4.247962951660156
  batch 60 loss: 1.7303706407546997, 2.517592430114746, 4.247962951660156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7341715097427368 2.3601648807525635 4.09433650970459
Loss :  1.7356003522872925 2.473609447479248 4.20920991897583
Loss :  1.744590401649475 2.7472236156463623 4.491814136505127
Loss :  1.7186155319213867 3.453641176223755 5.1722564697265625
Loss :  1.7179934978485107 2.0973594188690186 3.8153529167175293
Loss :  1.8708701133728027 3.885929822921753 5.756799697875977
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8549072742462158 4.041045188903809 5.895952224731445
Loss :  1.8663874864578247 3.836864471435547 5.703251838684082
Loss :  1.8986883163452148 3.7208824157714844 5.619570732116699
Total LOSS train 4.583127355575561 valid 5.743893623352051
CE LOSS train 1.7276552016918476 valid 0.4746720790863037
Contrastive LOSS train 2.8554721282078668 valid 0.9302206039428711
EPOCH 67:
Loss :  1.723095417022705 2.467233419418335 4.190328598022461
Loss :  1.7312078475952148 2.4735195636749268 4.2047271728515625
Loss :  1.719427466392517 2.267864942550659 3.9872922897338867
Loss :  1.7354127168655396 2.1929054260253906 3.9283180236816406
Loss :  1.7360572814941406 2.244788646697998 3.9808459281921387
Loss :  1.735131859779358 2.552912950515747 4.2880449295043945
Loss :  1.7282408475875854 2.689800262451172 4.418041229248047
Loss :  1.7273048162460327 2.18778133392334 3.915086269378662
Loss :  1.7311367988586426 1.9918484687805176 3.72298526763916
Loss :  1.6932884454727173 1.8924921751022339 3.585780620574951
Loss :  1.733159065246582 2.444869041442871 4.178028106689453
Loss :  1.7585999965667725 2.8291215896606445 4.587721824645996
Loss :  1.7289701700210571 2.8734257221221924 4.602396011352539
Loss :  1.7242227792739868 2.7766764163970947 4.500899314880371
Loss :  1.7275424003601074 2.632997751235962 4.360540390014648
Loss :  1.709610939025879 2.487243175506592 4.196854114532471
Loss :  1.735647201538086 2.7522194385528564 4.487866401672363
Loss :  1.721859335899353 3.0779993534088135 4.799858570098877
Loss :  1.730293869972229 2.900350332260132 4.63064432144165
Loss :  1.7160457372665405 3.4319403171539307 5.147985935211182
  batch 20 loss: 1.7160457372665405, 3.4319403171539307, 5.147985935211182
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7225488424301147 2.323838233947754 4.046387195587158
Loss :  1.73046875 2.723149299621582 4.453618049621582
Loss :  1.7225322723388672 2.4149768352508545 4.137509346008301
Loss :  1.7346383333206177 3.0169615745544434 4.7515997886657715
Loss :  1.7350937128067017 3.2915124893188477 5.02660608291626
Loss :  1.7200075387954712 3.5598158836364746 5.279823303222656
Loss :  1.7351868152618408 2.7055647373199463 4.440751552581787
Loss :  1.713557481765747 2.7699365615844727 4.483493804931641
Loss :  1.7354174852371216 2.6306376457214355 4.366055011749268
Loss :  1.7206695079803467 2.9789011478424072 4.699570655822754
Loss :  1.756325364112854 3.0194318294525146 4.775757312774658
Loss :  1.7268346548080444 2.669140100479126 4.395974636077881
Loss :  1.716711401939392 2.6230170726776123 4.339728355407715
Loss :  1.7139620780944824 2.556962013244629 4.270924091339111
Loss :  1.7366293668746948 3.751328945159912 5.4879584312438965
Loss :  1.734866738319397 3.068342924118042 4.8032097816467285
Loss :  1.7293131351470947 2.548978328704834 4.278291702270508
Loss :  1.7092264890670776 2.3638863563537598 4.073112964630127
Loss :  1.7225147485733032 2.879965305328369 4.602479934692383
Loss :  1.7188602685928345 2.6302454471588135 4.3491058349609375
  batch 40 loss: 1.7188602685928345, 2.6302454471588135, 4.3491058349609375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7214118242263794 3.107887029647827 4.829298973083496
Loss :  1.7215403318405151 3.2785327434539795 5.000072956085205
Loss :  1.7246692180633545 3.1261606216430664 4.850830078125
Loss :  1.7353408336639404 2.941819190979004 4.677160263061523
Loss :  1.732763409614563 2.1953999996185303 3.928163528442383
Loss :  1.7205556631088257 3.1161530017852783 4.8367085456848145
Loss :  1.7245397567749023 3.1248791217803955 4.849418640136719
Loss :  1.7236372232437134 2.593486785888672 4.317123889923096
Loss :  1.7213126420974731 2.8046934604644775 4.52600622177124
Loss :  1.7299336194992065 3.338862895965576 5.068796634674072
Loss :  1.705997109413147 3.078463077545166 4.784460067749023
Loss :  1.7140495777130127 3.1109459400177 4.824995517730713
Loss :  1.732351541519165 3.0342907905578613 4.7666425704956055
Loss :  1.732358455657959 2.7331669330596924 4.4655256271362305
Loss :  1.7281256914138794 2.709170341491699 4.437295913696289
Loss :  1.7151089906692505 2.557445764541626 4.272554874420166
Loss :  1.7298835515975952 3.177790880203247 4.907674312591553
Loss :  1.7298318147659302 2.6337594985961914 4.363591194152832
Loss :  1.7305686473846436 3.47066330909729 5.201231956481934
Loss :  1.726999044418335 3.0327188968658447 4.75971794128418
  batch 60 loss: 1.726999044418335, 3.0327188968658447, 4.75971794128418
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7291836738586426 3.230281352996826 4.959465026855469
Loss :  1.7319287061691284 3.599787712097168 5.331716537475586
Loss :  1.7389777898788452 3.688491106033325 5.427468776702881
Loss :  1.7140159606933594 2.5290515422821045 4.243067741394043
Loss :  1.714361310005188 1.926674485206604 3.641035795211792
Loss :  1.9053930044174194 4.17573356628418 6.081126689910889
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8877841234207153 4.229791164398193 6.117575168609619
Loss :  1.8939400911331177 3.960756778717041 5.854696750640869
Loss :  1.937567114830017 4.091073513031006 6.0286407470703125
Total LOSS train 4.5237881036905145 valid 6.020509839057922
CE LOSS train 1.7263240979268 valid 0.4843917787075043
Contrastive LOSS train 2.7974639929257905 valid 1.0227683782577515
EPOCH 68:
Loss :  1.7177149057388306 2.0393733978271484 3.7570881843566895
Loss :  1.7256910800933838 2.440528154373169 4.166219234466553
Loss :  1.7165330648422241 2.162642240524292 3.8791751861572266
Loss :  1.7354458570480347 2.3599352836608887 4.095381259918213
Loss :  1.7326306104660034 2.5343337059020996 4.266964435577393
Loss :  1.7340288162231445 2.8597629070281982 4.593791961669922
Loss :  1.7272124290466309 2.9274494647979736 4.654662132263184
Loss :  1.7295520305633545 3.9002485275268555 5.629800796508789
Loss :  1.7313694953918457 3.634434938430786 5.365804672241211
Loss :  1.692558765411377 3.6189820766448975 5.311540603637695
Loss :  1.7349317073822021 3.64510178565979 5.380033493041992
Loss :  1.756188988685608 3.325125217437744 5.0813140869140625
Loss :  1.728106141090393 3.1547036170959473 4.882809638977051
Loss :  1.7254130840301514 3.0329487323760986 4.75836181640625
Loss :  1.7282435894012451 3.009915351867676 4.7381591796875
Loss :  1.7076630592346191 3.4023001194000244 5.109963417053223
Loss :  1.7349051237106323 3.2884480953216553 5.023353099822998
Loss :  1.7203658819198608 2.903850793838501 4.624216556549072
Loss :  1.7259501218795776 2.3179240226745605 4.043874263763428
Loss :  1.7150905132293701 2.6305181980133057 4.345608711242676
  batch 20 loss: 1.7150905132293701, 2.6305181980133057, 4.345608711242676
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7244073152542114 2.2428033351898193 3.9672107696533203
Loss :  1.7304712533950806 2.505722999572754 4.236194133758545
Loss :  1.72389817237854 2.695021629333496 4.418919563293457
Loss :  1.7363349199295044 2.980380058288574 4.716714859008789
Loss :  1.730269193649292 3.0040762424468994 4.734345436096191
Loss :  1.7218762636184692 3.2676784992218018 4.9895548820495605
Loss :  1.7355403900146484 3.5791471004486084 5.314687728881836
Loss :  1.7183810472488403 3.1845498085021973 4.902930736541748
Loss :  1.739725947380066 3.1371161937713623 4.876842021942139
Loss :  1.7202539443969727 2.6704933643341064 4.3907470703125
Loss :  1.7538752555847168 2.54211688041687 4.295991897583008
Loss :  1.7268074750900269 2.293210506439209 4.020018100738525
Loss :  1.7178921699523926 2.3732879161834717 4.091179847717285
Loss :  1.7136127948760986 2.340501070022583 4.054113864898682
Loss :  1.7342884540557861 2.7670743465423584 4.5013628005981445
Loss :  1.7353209257125854 2.9274585247039795 4.662779331207275
Loss :  1.7302252054214478 2.6466851234436035 4.376910209655762
Loss :  1.7103112936019897 2.561753749847412 4.272065162658691
Loss :  1.7256654500961304 2.5352702140808105 4.2609357833862305
Loss :  1.7190104722976685 2.3943495750427246 4.1133599281311035
  batch 40 loss: 1.7190104722976685, 2.3943495750427246, 4.1133599281311035
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.721148133277893 2.4489927291870117 4.170140743255615
Loss :  1.723049283027649 3.0117878913879395 4.734837055206299
Loss :  1.725458025932312 2.77911639213562 4.504574298858643
Loss :  1.7339510917663574 2.9988491535186768 4.732800483703613
Loss :  1.7326085567474365 3.0588815212249756 4.791490077972412
Loss :  1.7189176082611084 3.5168488025665283 5.235766410827637
Loss :  1.7213927507400513 2.589812755584717 4.3112053871154785
Loss :  1.726331114768982 2.821063280105591 4.547394275665283
Loss :  1.7175374031066895 2.626300811767578 4.343838214874268
Loss :  1.730015516281128 2.7158966064453125 4.4459123611450195
Loss :  1.7111724615097046 3.040008068084717 4.751180648803711
Loss :  1.7157039642333984 3.3068556785583496 5.022559642791748
Loss :  1.734099268913269 2.660712957382202 4.394812107086182
Loss :  1.7311787605285645 2.882288932800293 4.613467693328857
Loss :  1.7342082262039185 3.4002413749694824 5.134449481964111
Loss :  1.7199499607086182 3.081237554550171 4.801187515258789
Loss :  1.7332549095153809 2.8495335578918457 4.582788467407227
Loss :  1.7326253652572632 2.704914093017578 4.437539577484131
Loss :  1.736114740371704 3.137448310852051 4.873562812805176
Loss :  1.732592225074768 3.257530450820923 4.9901227951049805
  batch 60 loss: 1.732592225074768, 3.257530450820923, 4.9901227951049805
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7292454242706299 2.7253782749176025 4.454623699188232
Loss :  1.736661672592163 2.6354196071624756 4.372081279754639
Loss :  1.735845923423767 3.1282665729522705 4.864112377166748
Loss :  1.721919059753418 3.0921432971954346 4.814062118530273
Loss :  1.723372220993042 3.2753849029541016 4.998757362365723
Loss :  1.6815211772918701 4.293280124664307 5.974801063537598
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6899724006652832 4.302673816680908 5.992646217346191
Loss :  1.6927450895309448 4.120869159698486 5.813614368438721
Loss :  1.6858655214309692 4.0612053871154785 5.747070789337158
Total LOSS train 4.61274236532358 valid 5.882033109664917
CE LOSS train 1.7268633365631103 valid 0.4214663803577423
Contrastive LOSS train 2.8858790360964264 valid 1.0153013467788696
EPOCH 69:
Loss :  1.728336215019226 4.093424320220947 5.821760654449463
Loss :  1.7236835956573486 3.663442850112915 5.387126445770264
Loss :  1.7198606729507446 3.0052454471588135 4.725106239318848
Loss :  1.7343649864196777 3.9604649543762207 5.694829940795898
Loss :  1.7358448505401611 3.3264479637145996 5.06229305267334
Loss :  1.7339859008789062 3.3805923461914062 5.1145782470703125
Loss :  1.7253496646881104 2.554692268371582 4.280041694641113
Loss :  1.7251887321472168 3.0625009536743164 4.787689685821533
Loss :  1.7293373346328735 2.601384162902832 4.330721378326416
Loss :  1.6945291757583618 1.9619367122650146 3.656466007232666
Loss :  1.7278255224227905 2.2042510509490967 3.9320764541625977
Loss :  1.7606972455978394 2.5562283992767334 4.316925525665283
Loss :  1.724902868270874 2.4292662143707275 4.154169082641602
Loss :  1.7239223718643188 2.1913504600524902 3.9152727127075195
Loss :  1.7292791604995728 2.80271053314209 4.531989574432373
Loss :  1.712410569190979 2.8349335193634033 4.547344207763672
Loss :  1.7323018312454224 3.1723859310150146 4.904687881469727
Loss :  1.718366265296936 3.302677869796753 5.0210442543029785
Loss :  1.730696439743042 2.8524272441864014 4.583123683929443
Loss :  1.7144609689712524 3.498077154159546 5.212538242340088
  batch 20 loss: 1.7144609689712524, 3.498077154159546, 5.212538242340088
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7256693840026855 4.215986251831055 5.94165563583374
Loss :  1.7299537658691406 3.1987082958221436 4.928662300109863
Loss :  1.7245291471481323 3.055760145187378 4.780289173126221
Loss :  1.7379509210586548 3.132153272628784 4.8701043128967285
Loss :  1.7347439527511597 2.773747205734253 4.508491039276123
Loss :  1.7219960689544678 2.5977680683135986 4.319764137268066
Loss :  1.7332006692886353 2.5766890048980713 4.309889793395996
Loss :  1.7124117612838745 2.410998821258545 4.123410701751709
Loss :  1.733566164970398 2.2951393127441406 4.028705596923828
Loss :  1.7236512899398804 2.733776330947876 4.457427501678467
Loss :  1.7574409246444702 2.6214022636413574 4.378843307495117
Loss :  1.7263518571853638 2.9427976608276367 4.669149398803711
Loss :  1.7183454036712646 3.5484519004821777 5.266797065734863
Loss :  1.7166348695755005 2.7228522300720215 4.439486980438232
Loss :  1.73562490940094 2.882359266281128 4.617984294891357
Loss :  1.7351958751678467 2.707310914993286 4.442506790161133
Loss :  1.7318377494812012 3.0459654331207275 4.777803421020508
Loss :  1.709276795387268 2.780156135559082 4.4894328117370605
Loss :  1.7225193977355957 3.4370484352111816 5.159567832946777
Loss :  1.719430685043335 3.47865891456604 5.198089599609375
  batch 40 loss: 1.719430685043335, 3.47865891456604, 5.198089599609375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7211263179779053 3.355785608291626 5.076911926269531
Loss :  1.7227309942245483 3.1924545764923096 4.915185451507568
Loss :  1.7248523235321045 2.924713134765625 4.649565696716309
Loss :  1.7342783212661743 3.391130208969116 5.12540864944458
Loss :  1.7326823472976685 2.229551076889038 3.962233543395996
Loss :  1.7202770709991455 3.259260416030884 4.979537487030029
Loss :  1.722765326499939 2.9485671520233154 4.671332359313965
Loss :  1.7222537994384766 2.878075122833252 4.6003289222717285
Loss :  1.7205674648284912 2.935275077819824 4.6558427810668945
Loss :  1.7278125286102295 3.080742835998535 4.808555603027344
Loss :  1.7053651809692383 3.2966063022613525 5.001971244812012
Loss :  1.7114241123199463 3.0518786907196045 4.763302803039551
Loss :  1.7317371368408203 3.0245590209960938 4.756296157836914
Loss :  1.7275053262710571 2.8304357528686523 4.55794095993042
Loss :  1.728454351425171 2.9728333950042725 4.701287746429443
Loss :  1.7130125761032104 3.1523630619049072 4.865375518798828
Loss :  1.7293217182159424 2.3685142993927 4.097836017608643
Loss :  1.7277156114578247 2.335425853729248 4.063141345977783
Loss :  1.7285081148147583 2.610508441925049 4.339016437530518
Loss :  1.7257810831069946 2.2626359462738037 3.988417148590088
  batch 60 loss: 1.7257810831069946, 2.2626359462738037, 3.988417148590088
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7286616563796997 2.1995770931243896 3.928238868713379
Loss :  1.732616662979126 2.130530595779419 3.863147258758545
Loss :  1.7386544942855835 2.196016788482666 3.934671401977539
Loss :  1.7183858156204224 2.3048436641693115 4.023229598999023
Loss :  1.7160065174102783 1.8787264823913574 3.5947329998016357
Loss :  1.7375227212905884 4.180967807769775 5.918490409851074
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7379578351974487 4.244708061218262 5.982666015625
Loss :  1.7414090633392334 4.1031107902526855 5.84451961517334
Loss :  1.7685543298721313 3.969508647918701 5.738062858581543
Total LOSS train 4.609405455222497 valid 5.870934724807739
CE LOSS train 1.7259718894958496 valid 0.44213858246803284
Contrastive LOSS train 2.8834335510547344 valid 0.9923771619796753
EPOCH 70:
Loss :  1.720889687538147 2.1629631519317627 3.883852958679199
Loss :  1.7248597145080566 2.850886106491089 4.575745582580566
Loss :  1.715437889099121 2.60252046585083 4.317958354949951
Loss :  1.7293481826782227 2.398008108139038 4.12735652923584
Loss :  1.732338547706604 2.3797483444213867 4.112086772918701
Loss :  1.7312438488006592 2.554731607437134 4.285975456237793
Loss :  1.7238436937332153 2.5625650882720947 4.2864089012146
Loss :  1.7227271795272827 2.276538610458374 3.999265670776367
Loss :  1.7276636362075806 2.3754804134368896 4.10314416885376
Loss :  1.6934607028961182 2.190194845199585 3.883655548095703
Loss :  1.7280322313308716 2.3779053688049316 4.105937480926514
Loss :  1.7545125484466553 2.5956618785858154 4.350174427032471
Loss :  1.725812315940857 2.539672374725342 4.265484809875488
Loss :  1.723130702972412 2.279832363128662 4.002963066101074
Loss :  1.7273951768875122 2.3715145587921143 4.098909854888916
Loss :  1.7079664468765259 2.5798075199127197 4.287774085998535
Loss :  1.7316339015960693 2.64699649810791 4.378630638122559
Loss :  1.7190091609954834 2.4385528564453125 4.157562255859375
Loss :  1.7280352115631104 2.3646178245544434 4.092653274536133
Loss :  1.7121602296829224 2.496661424636841 4.208821773529053
  batch 20 loss: 1.7121602296829224, 2.496661424636841, 4.208821773529053
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.72471284866333 2.3558056354522705 4.08051872253418
Loss :  1.728790521621704 2.893637180328369 4.622427940368652
Loss :  1.7235947847366333 2.8036038875579834 4.527198791503906
Loss :  1.7390611171722412 2.7691311836242676 4.50819206237793
Loss :  1.7297202348709106 2.556950569152832 4.286670684814453
Loss :  1.7208755016326904 2.8528594970703125 4.573735237121582
Loss :  1.736514925956726 2.7835779190063477 4.520092964172363
Loss :  1.7163687944412231 2.6014411449432373 4.31781005859375
Loss :  1.737216830253601 2.640169620513916 4.377386569976807
Loss :  1.7180474996566772 2.6017253398895264 4.319772720336914
Loss :  1.7528941631317139 2.5687661170959473 4.321660041809082
Loss :  1.7288901805877686 2.4603171348571777 4.189207077026367
Loss :  1.7194563150405884 2.46878981590271 4.188246250152588
Loss :  1.714516520500183 2.6080894470214844 4.322606086730957
Loss :  1.7367037534713745 3.1840147972106934 4.920718669891357
Loss :  1.7352266311645508 2.5874862670898438 4.3227128982543945
Loss :  1.733575701713562 2.6723780632019043 4.405953884124756
Loss :  1.7124309539794922 2.558561086654663 4.270992279052734
Loss :  1.724753975868225 2.5600931644439697 4.284847259521484
Loss :  1.721056342124939 3.1700103282928467 4.891066551208496
  batch 40 loss: 1.721056342124939, 3.1700103282928467, 4.891066551208496
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.721972107887268 2.6141197681427 4.336091995239258
Loss :  1.726318120956421 2.2488043308258057 3.9751224517822266
Loss :  1.729190468788147 2.392130136489868 4.121320724487305
Loss :  1.7358168363571167 2.420616626739502 4.156433582305908
Loss :  1.735266089439392 2.394481658935547 4.1297478675842285
Loss :  1.7229053974151611 2.5510408878326416 4.273946285247803
Loss :  1.7207465171813965 2.602569341659546 4.323315620422363
Loss :  1.7277140617370605 2.3508217334747314 4.078536033630371
Loss :  1.7182657718658447 2.4720022678375244 4.190268039703369
Loss :  1.7342761754989624 2.7637879848480225 4.498064041137695
Loss :  1.7122606039047241 2.710801124572754 4.423061847686768
Loss :  1.7182137966156006 2.4082248210906982 4.126438617706299
Loss :  1.7314506769180298 2.301640033721924 4.033090591430664
Loss :  1.7338110208511353 2.285637855529785 4.019448757171631
Loss :  1.7325434684753418 2.44158935546875 4.174132823944092
Loss :  1.7165194749832153 2.0581471920013428 3.7746667861938477
Loss :  1.7317285537719727 2.5825934410095215 4.314321994781494
Loss :  1.7312384843826294 2.9192957878112793 4.650534152984619
Loss :  1.7334105968475342 2.4843287467956543 4.217739105224609
Loss :  1.7304946184158325 3.0895392894744873 4.820034027099609
  batch 60 loss: 1.7304946184158325, 3.0895392894744873, 4.820034027099609
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.728276252746582 2.45754075050354 4.185816764831543
Loss :  1.733655571937561 2.549640417098999 4.28329610824585
Loss :  1.737771987915039 2.4217405319213867 4.159512519836426
Loss :  1.717382788658142 2.6174886226654053 4.334871292114258
Loss :  1.7179644107818604 2.297114849090576 4.015079498291016
Loss :  1.826921820640564 3.81503963470459 5.641961574554443
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8224021196365356 3.9777779579162598 5.800179958343506
Loss :  1.8306405544281006 3.7263739109039307 5.557014465332031
Loss :  1.8380731344223022 3.806062936782837 5.64413595199585
Total LOSS train 4.267554921370286 valid 5.6608229875564575
CE LOSS train 1.7263554224601159 valid 0.45951828360557556
Contrastive LOSS train 2.541199464064378 valid 0.9515157341957092
EPOCH 71:
Loss :  1.7231099605560303 2.5192739963531494 4.24238395690918
Loss :  1.7269959449768066 2.6221892833709717 4.349184989929199
Loss :  1.7176415920257568 2.5241191387176514 4.241760730743408
Loss :  1.7328180074691772 2.4466235637664795 4.179441452026367
Loss :  1.734527587890625 2.6265830993652344 4.361110687255859
Loss :  1.732498288154602 2.732799768447876 4.465298175811768
Loss :  1.7245551347732544 2.6315619945526123 4.356117248535156
Loss :  1.7268874645233154 2.514378309249878 4.241265773773193
Loss :  1.7290050983428955 2.373929023742676 4.102933883666992
Loss :  1.6955019235610962 2.1691184043884277 3.8646202087402344
Loss :  1.730347990989685 2.1440465450286865 3.874394416809082
Loss :  1.7541475296020508 2.154169797897339 3.9083173274993896
Loss :  1.7257951498031616 2.1784791946411133 3.9042744636535645
Loss :  1.7246729135513306 2.418224811553955 4.142897605895996
Loss :  1.7282754182815552 3.216840982437134 4.9451165199279785
Loss :  1.711222529411316 2.210350513458252 3.9215731620788574
Loss :  1.7308944463729858 2.1627614498138428 3.893655776977539
Loss :  1.720007300376892 2.1330645084381104 3.853071689605713
Loss :  1.7276169061660767 2.018017053604126 3.745634078979492
Loss :  1.7115432024002075 2.1430280208587646 3.8545713424682617
  batch 20 loss: 1.7115432024002075, 2.1430280208587646, 3.8545713424682617
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7265616655349731 2.805384397506714 4.531946182250977
Loss :  1.7276360988616943 3.3846981525421143 5.112334251403809
Loss :  1.7239117622375488 2.360323667526245 4.084235191345215
Loss :  1.7360347509384155 2.385673999786377 4.121708869934082
Loss :  1.7363760471343994 2.9506804943084717 4.687056541442871
Loss :  1.7249857187271118 3.777179718017578 5.5021653175354
Loss :  1.735512375831604 3.4357995986938477 5.171311855316162
Loss :  1.7157928943634033 3.4581520557403564 5.17394495010376
Loss :  1.7357370853424072 2.496061086654663 4.23179817199707
Loss :  1.7234063148498535 2.7298660278320312 4.453272342681885
Loss :  1.7571415901184082 3.487269401550293 5.244410991668701
Loss :  1.7268216609954834 3.7599785327911377 5.486800193786621
Loss :  1.7206354141235352 3.7756285667419434 5.4962639808654785
Loss :  1.7188799381256104 3.488265037536621 5.207144737243652
Loss :  1.736473560333252 3.6506433486938477 5.3871169090271
Loss :  1.7376340627670288 3.1637725830078125 4.901406764984131
Loss :  1.7338038682937622 3.223269462585449 4.957073211669922
Loss :  1.707785725593567 3.228257179260254 4.936042785644531
Loss :  1.7257513999938965 2.7336232662200928 4.45937442779541
Loss :  1.720799446105957 2.4805593490600586 4.201358795166016
  batch 40 loss: 1.720799446105957, 2.4805593490600586, 4.201358795166016
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7213513851165771 2.787299394607544 4.508650779724121
Loss :  1.7287662029266357 2.2904269695281982 4.019193172454834
Loss :  1.7301863431930542 2.913593053817749 4.643779277801514
Loss :  1.737273931503296 2.297719717025757 4.034993648529053
Loss :  1.7329323291778564 2.1349546909332275 3.867887020111084
Loss :  1.722562313079834 2.2302377223968506 3.9528000354766846
Loss :  1.721956729888916 2.2991445064544678 4.021100997924805
Loss :  1.7250888347625732 2.327529191970825 4.052618026733398
Loss :  1.7228610515594482 2.3200602531433105 4.04292106628418
Loss :  1.7347142696380615 2.39096999168396 4.1256842613220215
Loss :  1.7126307487487793 2.47585129737854 4.188482284545898
Loss :  1.7210156917572021 2.4529902935028076 4.17400598526001
Loss :  1.7347153425216675 2.2386221885681152 3.9733376502990723
Loss :  1.7370797395706177 2.9236886501312256 4.660768508911133
Loss :  1.7299548387527466 2.7736997604370117 4.503654479980469
Loss :  1.718283772468567 2.5883405208587646 4.306624412536621
Loss :  1.734904408454895 2.300877332687378 4.0357818603515625
Loss :  1.7318732738494873 2.2297823429107666 3.961655616760254
Loss :  1.7380605936050415 2.734846353530884 4.472907066345215
Loss :  1.7304877042770386 2.108210563659668 3.838698387145996
  batch 60 loss: 1.7304877042770386, 2.108210563659668, 3.838698387145996
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7295587062835693 2.5952303409576416 4.324789047241211
Loss :  1.73483407497406 2.326981544494629 4.0618157386779785
Loss :  1.7367905378341675 2.6237757205963135 4.360566139221191
Loss :  1.7200909852981567 2.6922249794006348 4.412315845489502
Loss :  1.7184499502182007 2.150845527648926 3.869295597076416
Loss :  1.7007697820663452 4.315136432647705 6.01590633392334
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6905053853988647 4.247338771820068 5.937844276428223
Loss :  1.7011289596557617 4.073350429534912 5.774479389190674
Loss :  1.7205595970153809 3.8556814193725586 5.5762410163879395
Total LOSS train 4.372441797990065 valid 5.826117753982544
CE LOSS train 1.7274795312147875 valid 0.4301398992538452
Contrastive LOSS train 2.6449622814471905 valid 0.9639203548431396
EPOCH 72:
Loss :  1.7257705926895142 2.4569666385650635 4.182737350463867
Loss :  1.7256170511245728 2.6997146606445312 4.4253315925598145
Loss :  1.7184045314788818 2.9762589931488037 4.6946635246276855
Loss :  1.7326161861419678 2.6025543212890625 4.335170745849609
Loss :  1.7377878427505493 2.323272228240967 4.061059951782227
Loss :  1.735864520072937 3.2252414226531982 4.961105823516846
Loss :  1.727787733078003 2.994837522506714 4.722625255584717
Loss :  1.7277976274490356 2.511359453201294 4.239157199859619
Loss :  1.7320860624313354 2.652726888656616 4.384812831878662
Loss :  1.7012972831726074 2.331698179244995 4.032995223999023
Loss :  1.7311955690383911 3.041106939315796 4.772302627563477
Loss :  1.7578773498535156 2.5406978130340576 4.298575401306152
Loss :  1.728479027748108 2.802548408508301 4.531027317047119
Loss :  1.7259407043457031 2.208591938018799 3.934532642364502
Loss :  1.7306114435195923 2.5427963733673096 4.273407936096191
Loss :  1.7157293558120728 2.5367531776428223 4.2524824142456055
Loss :  1.7335892915725708 2.523876667022705 4.257465839385986
Loss :  1.7242918014526367 3.2252843379974365 4.949576377868652
Loss :  1.729650616645813 2.7248611450195312 4.454511642456055
Loss :  1.7140761613845825 2.489616632461548 4.20369291305542
  batch 20 loss: 1.7140761613845825, 2.489616632461548, 4.20369291305542
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7264256477355957 3.067169427871704 4.793595314025879
Loss :  1.7293261289596558 2.209411382675171 3.938737392425537
Loss :  1.7274304628372192 2.222719430923462 3.9501500129699707
Loss :  1.73955237865448 2.628462791442871 4.368015289306641
Loss :  1.7347338199615479 2.4732420444488525 4.2079758644104
Loss :  1.7256397008895874 2.7657229900360107 4.491362571716309
Loss :  1.7372957468032837 2.317786693572998 4.055082321166992
Loss :  1.718093991279602 2.752807855606079 4.470901966094971
Loss :  1.7376025915145874 2.326521873474121 4.064124584197998
Loss :  1.7218486070632935 2.7447664737701416 4.466615200042725
Loss :  1.7533520460128784 2.5383241176605225 4.291676044464111
Loss :  1.730974555015564 2.3219969272613525 4.052971363067627
Loss :  1.7208541631698608 3.202843427658081 4.923697471618652
Loss :  1.7193244695663452 3.5132157802581787 5.232540130615234
Loss :  1.7346293926239014 3.1793935298919678 4.914022922515869
Loss :  1.736499547958374 2.8973114490509033 4.633810997009277
Loss :  1.733049750328064 3.095552921295166 4.8286027908325195
Loss :  1.7167022228240967 2.564988851547241 4.281691074371338
Loss :  1.7266687154769897 2.795761823654175 4.522430419921875
Loss :  1.7221754789352417 2.497572660446167 4.219748020172119
  batch 40 loss: 1.7221754789352417, 2.497572660446167, 4.219748020172119
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7221715450286865 2.562603235244751 4.2847747802734375
Loss :  1.7236897945404053 2.8512847423553467 4.574974536895752
Loss :  1.7250205278396606 2.6589128971099854 4.3839335441589355
Loss :  1.7321075201034546 2.7676517963409424 4.499759197235107
Loss :  1.7324144840240479 2.778714418411255 4.511128902435303
Loss :  1.722314715385437 2.0918056964874268 3.814120292663574
Loss :  1.7222977876663208 2.037731409072876 3.7600293159484863
Loss :  1.7254925966262817 2.0261781215667725 3.7516708374023438
Loss :  1.7248457670211792 2.0979502201080322 3.822795867919922
Loss :  1.7289642095565796 2.504666805267334 4.233631134033203
Loss :  1.7107131481170654 2.4616761207580566 4.172389030456543
Loss :  1.7207279205322266 2.9312353134155273 4.651963233947754
Loss :  1.733760118484497 2.8429510593414307 4.576711177825928
Loss :  1.7283055782318115 3.389420986175537 5.1177263259887695
Loss :  1.7309623956680298 3.008150339126587 4.739112854003906
Loss :  1.7186535596847534 2.6349525451660156 4.353606224060059
Loss :  1.732601284980774 3.7118074893951416 5.444408893585205
Loss :  1.7325303554534912 3.9640910625457764 5.696621417999268
Loss :  1.7383508682250977 3.9453012943267822 5.683651924133301
Loss :  1.7352123260498047 3.1829679012298584 4.918180465698242
  batch 60 loss: 1.7352123260498047, 3.1829679012298584, 4.918180465698242
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7293092012405396 3.637037754058838 5.366346836090088
Loss :  1.736877202987671 3.436189889907837 5.173067092895508
Loss :  1.73537015914917 4.2992167472839355 6.0345869064331055
Loss :  1.7185842990875244 3.6972923278808594 5.415876388549805
Loss :  1.720540165901184 3.5036163330078125 5.224156379699707
Loss :  1.8574203252792358 3.7463173866271973 5.603737831115723
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8543936014175415 3.9638566970825195 5.8182501792907715
Loss :  1.8542958498001099 3.703549385070801 5.557845115661621
Loss :  1.86785089969635 3.8066108226776123 5.674461841583252
Total LOSS train 4.536618614196778 valid 5.663573741912842
CE LOSS train 1.7281610415532038 valid 0.4669627249240875
Contrastive LOSS train 2.80845757997953 valid 0.9516527056694031
EPOCH 73:
Loss :  1.7240923643112183 2.922321319580078 4.646413803100586
Loss :  1.7199301719665527 3.1705377101898193 4.890467643737793
Loss :  1.716883659362793 2.8752927780151367 4.59217643737793
Loss :  1.7382386922836304 2.5840604305267334 4.322299003601074
Loss :  1.733284592628479 2.7660574913024902 4.49934196472168
Loss :  1.7307016849517822 2.3640544414520264 4.094756126403809
Loss :  1.7234458923339844 2.921874761581421 4.645320892333984
Loss :  1.7224184274673462 2.2241320610046387 3.9465503692626953
Loss :  1.7278753519058228 2.7860677242279053 4.513943195343018
Loss :  1.694033145904541 2.708637237548828 4.402670383453369
Loss :  1.7259552478790283 2.8820536136627197 4.608008861541748
Loss :  1.7551355361938477 2.721273899078369 4.476409435272217
Loss :  1.7255995273590088 3.096219062805176 4.8218183517456055
Loss :  1.7244752645492554 3.4066619873046875 5.131137371063232
Loss :  1.73175847530365 2.731424570083618 4.4631829261779785
Loss :  1.7108354568481445 2.98833966255188 4.699174880981445
Loss :  1.7315599918365479 3.074648380279541 4.806208610534668
Loss :  1.7221741676330566 3.079298734664917 4.8014726638793945
Loss :  1.7280384302139282 2.6681761741638184 4.396214485168457
Loss :  1.713257908821106 3.0303187370300293 4.743576526641846
  batch 20 loss: 1.713257908821106, 3.0303187370300293, 4.743576526641846
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7299960851669312 2.7234549522399902 4.453451156616211
Loss :  1.7319015264511108 2.798069715499878 4.529971122741699
Loss :  1.7277803421020508 2.6376795768737793 4.36545991897583
Loss :  1.7390656471252441 3.1913671493530273 4.9304327964782715
Loss :  1.729362964630127 3.791795492172241 5.521158218383789
Loss :  1.7252559661865234 3.4292337894439697 5.154489517211914
Loss :  1.7364094257354736 3.0517029762268066 4.788112640380859
Loss :  1.7176339626312256 2.6076135635375977 4.325247764587402
Loss :  1.7368522882461548 2.909299373626709 4.646151542663574
Loss :  1.7204502820968628 2.5704617500305176 4.29091215133667
Loss :  1.752685546875 2.5139851570129395 4.2666707038879395
Loss :  1.7281570434570312 2.4120800495147705 4.140236854553223
Loss :  1.72212553024292 2.1585922241210938 3.8807177543640137
Loss :  1.7216508388519287 2.1325559616088867 3.8542068004608154
Loss :  1.7383017539978027 2.397040843963623 4.135342597961426
Loss :  1.7385821342468262 2.3228325843811035 4.06141471862793
Loss :  1.7356667518615723 2.252239942550659 3.9879066944122314
Loss :  1.707995891571045 2.0490236282348633 3.757019519805908
Loss :  1.7298312187194824 2.1706440448760986 3.900475263595581
Loss :  1.7226512432098389 2.0595204830169678 3.7821717262268066
  batch 40 loss: 1.7226512432098389, 2.0595204830169678, 3.7821717262268066
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.722852349281311 2.222303628921509 3.9451560974121094
Loss :  1.7306361198425293 2.047792673110962 3.778428792953491
Loss :  1.7371355295181274 2.250610828399658 3.987746238708496
Loss :  1.7352659702301025 2.134781837463379 3.8700478076934814
Loss :  1.7373616695404053 1.9077696800231934 3.6451313495635986
Loss :  1.7222545146942139 2.451535940170288 4.173790454864502
Loss :  1.7194161415100098 3.021404504776001 4.74082088470459
Loss :  1.7324310541152954 2.5717594623565674 4.304190635681152
Loss :  1.7162715196609497 2.5756113529205322 4.2918829917907715
Loss :  1.7356504201889038 2.7036550045013428 4.439305305480957
Loss :  1.720231533050537 2.6788532733917236 4.39908504486084
Loss :  1.72136652469635 2.6064209938049316 4.327787399291992
Loss :  1.7291760444641113 2.4827723503112793 4.211948394775391
Loss :  1.7364158630371094 2.4968349933624268 4.233250617980957
Loss :  1.7423354387283325 2.58150053024292 4.323835849761963
Loss :  1.715335726737976 2.75663685798645 4.471972465515137
Loss :  1.7325087785720825 2.664235830307007 4.396744728088379
Loss :  1.7365199327468872 2.5888283252716064 4.325348377227783
Loss :  1.7399364709854126 2.900186061859131 4.640122413635254
Loss :  1.7308324575424194 2.6949169635772705 4.4257493019104
  batch 60 loss: 1.7308324575424194, 2.6949169635772705, 4.4257493019104
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7257333993911743 2.574331521987915 4.300065040588379
Loss :  1.7379800081253052 2.565821647644043 4.303801536560059
Loss :  1.7323912382125854 2.3753273487091064 4.107718467712402
Loss :  1.7260098457336426 2.5092084407806396 4.235218048095703
Loss :  1.7278172969818115 2.3525989055633545 4.080416202545166
Loss :  1.847123146057129 3.5619425773620605 5.4090657234191895
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8442940711975098 3.748293161392212 5.592587471008301
Loss :  1.8460726737976074 3.536262273788452 5.3823347091674805
Loss :  1.8437081575393677 3.6774237155914307 5.521131992340088
Total LOSS train 4.372803966815655 valid 5.476279973983765
CE LOSS train 1.728244865857638 valid 0.4609270393848419
Contrastive LOSS train 2.644559122965886 valid 0.9193559288978577
EPOCH 74:
Loss :  1.7269465923309326 3.0553791522979736 4.782325744628906
Loss :  1.7228353023529053 3.087782621383667 4.810617923736572
Loss :  1.7223862409591675 2.2168397903442383 3.9392261505126953
Loss :  1.7395635843276978 3.163029193878174 4.902592658996582
Loss :  1.734567403793335 2.319439172744751 4.054006576538086
Loss :  1.7344753742218018 2.7876241207122803 4.522099494934082
Loss :  1.7307020425796509 3.090939521789551 4.821641445159912
Loss :  1.7283121347427368 2.6212375164031982 4.349549770355225
Loss :  1.7331633567810059 3.2389798164367676 4.972143173217773
Loss :  1.7047626972198486 2.8487510681152344 4.553513526916504
Loss :  1.730391502380371 3.047750949859619 4.77814245223999
Loss :  1.7600293159484863 2.888735294342041 4.648764610290527
Loss :  1.7291994094848633 3.1091482639312744 4.838347434997559
Loss :  1.725799798965454 3.137615919113159 4.863415718078613
Loss :  1.7333407402038574 3.362032651901245 5.095373153686523
Loss :  1.7165279388427734 3.814894676208496 5.5314226150512695
Loss :  1.730514645576477 3.585444450378418 5.3159589767456055
Loss :  1.7203967571258545 3.3852932453155518 5.105690002441406
Loss :  1.7302215099334717 3.195197820663452 4.925419330596924
Loss :  1.7144441604614258 2.5203750133514404 4.234819412231445
  batch 20 loss: 1.7144441604614258, 2.5203750133514404, 4.234819412231445
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7288063764572144 2.4182913303375244 4.147097587585449
Loss :  1.7305647134780884 2.1753146648406982 3.905879497528076
Loss :  1.7287286520004272 2.192983388900757 3.9217119216918945
Loss :  1.7360615730285645 2.866610527038574 4.602672100067139
Loss :  1.7381986379623413 2.60331392288208 4.341512680053711
Loss :  1.7256966829299927 2.879213809967041 4.604910373687744
Loss :  1.7323646545410156 3.2955918312072754 5.027956485748291
Loss :  1.7132147550582886 3.0921103954315186 4.805325031280518
Loss :  1.7330986261367798 3.5384016036987305 5.271500110626221
Loss :  1.7252013683319092 2.878113031387329 4.603314399719238
Loss :  1.7553043365478516 3.519266366958618 5.274570465087891
Loss :  1.72713303565979 2.9315061569213867 4.658638954162598
Loss :  1.7223732471466064 2.5892956256866455 4.311668872833252
Loss :  1.7201591730117798 2.42798113822937 4.1481404304504395
Loss :  1.738116979598999 2.5527567863464355 4.2908735275268555
Loss :  1.7370984554290771 2.387310266494751 4.124408721923828
Loss :  1.7316945791244507 3.025186061859131 4.756880760192871
Loss :  1.71006178855896 2.624664306640625 4.334726333618164
Loss :  1.7258169651031494 2.926771402359009 4.652588367462158
Loss :  1.7208157777786255 3.2223520278930664 4.943167686462402
  batch 40 loss: 1.7208157777786255, 3.2223520278930664, 4.943167686462402
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7207504510879517 3.4747586250305176 5.19550895690918
Loss :  1.7223174571990967 2.9350507259368896 4.657368183135986
Loss :  1.7266274690628052 2.9873573780059814 4.713984966278076
Loss :  1.7295198440551758 2.940831422805786 4.670351028442383
Loss :  1.7329986095428467 2.9860219955444336 4.719020843505859
Loss :  1.719644546508789 3.7443480491638184 5.463992595672607
Loss :  1.71832275390625 3.610788583755493 5.329111099243164
Loss :  1.7239391803741455 3.4340240955352783 5.157963275909424
Loss :  1.7194652557373047 3.1717655658721924 4.891230583190918
Loss :  1.7280491590499878 3.215135097503662 4.9431843757629395
Loss :  1.709938645362854 2.9116744995117188 4.621613025665283
Loss :  1.718288779258728 2.613117218017578 4.331406116485596
Loss :  1.7305748462677002 2.742417812347412 4.472992897033691
Loss :  1.7287824153900146 2.732856273651123 4.461638450622559
Loss :  1.7335282564163208 2.5530903339385986 4.286618709564209
Loss :  1.7167332172393799 2.3716342449188232 4.088367462158203
Loss :  1.7323919534683228 2.4716084003448486 4.204000473022461
Loss :  1.7301937341690063 2.3900868892669678 4.120280742645264
Loss :  1.7354315519332886 2.707195520401001 4.442626953125
Loss :  1.7304589748382568 2.193896770477295 3.9243557453155518
  batch 60 loss: 1.7304589748382568, 2.193896770477295, 3.9243557453155518
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7253347635269165 2.3673171997070312 4.092651844024658
Loss :  1.7347487211227417 2.2911202907562256 4.025868892669678
Loss :  1.7316040992736816 1.8922340869903564 3.623838186264038
Loss :  1.7219822406768799 2.169856309890747 3.891838550567627
Loss :  1.723164677619934 1.9267858266830444 3.6499505043029785
Loss :  1.8352864980697632 4.169069766998291 6.004356384277344
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.829250693321228 4.234257698059082 6.0635085105896
Loss :  1.8338149785995483 3.953810214996338 5.787625312805176
Loss :  1.8415913581848145 3.8355863094329834 5.677177429199219
Total LOSS train 4.5808058298551115 valid 5.8831669092178345
CE LOSS train 1.7275981921416061 valid 0.4603978395462036
Contrastive LOSS train 2.8532076633893526 valid 0.9588965773582458
EPOCH 75:
Loss :  1.7270615100860596 2.129154920578003 3.8562164306640625
Loss :  1.7231545448303223 2.240755796432495 3.9639103412628174
Loss :  1.719484567642212 2.225651502609253 3.945136070251465
Loss :  1.7362022399902344 2.176751136779785 3.9129533767700195
Loss :  1.734068512916565 2.557805061340332 4.291873455047607
Loss :  1.7316784858703613 2.599663257598877 4.331341743469238
Loss :  1.7254871129989624 2.5938191413879395 4.319306373596191
Loss :  1.7222514152526855 2.2807726860046387 4.003024101257324
Loss :  1.7272478342056274 2.21280837059021 3.940056324005127
Loss :  1.6975845098495483 2.091792106628418 3.789376735687256
Loss :  1.724810242652893 2.3576056957244873 4.08241605758667
Loss :  1.7538901567459106 2.5952913761138916 4.349181652069092
Loss :  1.7248867750167847 2.8009326457977295 4.525819301605225
Loss :  1.7235385179519653 2.6858441829681396 4.4093828201293945
Loss :  1.730772852897644 2.833101511001587 4.563874244689941
Loss :  1.7177069187164307 2.9896562099456787 4.707363128662109
Loss :  1.7312198877334595 2.9035563468933105 4.6347761154174805
Loss :  1.7229857444763184 3.811832904815674 5.534818649291992
Loss :  1.729870319366455 2.612285852432251 4.342156410217285
Loss :  1.7139657735824585 2.7074785232543945 4.421444416046143
  batch 20 loss: 1.7139657735824585, 2.7074785232543945, 4.421444416046143
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7273746728897095 2.5445358753204346 4.271910667419434
Loss :  1.7300986051559448 2.53774094581604 4.267839431762695
Loss :  1.7266181707382202 3.1333670616149902 4.8599853515625
Loss :  1.7371537685394287 3.000214099884033 4.737367630004883
Loss :  1.7319443225860596 3.0013413429260254 4.733285903930664
Loss :  1.7257169485092163 2.5088045597076416 4.234521389007568
Loss :  1.7359790802001953 2.972097158432007 4.708076477050781
Loss :  1.7170034646987915 3.073256015777588 4.79025936126709
Loss :  1.7376940250396729 2.896244525909424 4.633938789367676
Loss :  1.7200284004211426 3.5540292263031006 5.274057388305664
Loss :  1.7512129545211792 2.893334150314331 4.644546985626221
Loss :  1.7279752492904663 3.1312379837036133 4.859213352203369
Loss :  1.7204838991165161 2.797150135040283 4.51763391494751
Loss :  1.717407464981079 2.8311102390289307 4.54851770401001
Loss :  1.7330256700515747 2.5768556594848633 4.309881210327148
Loss :  1.7357573509216309 2.471193552017212 4.206951141357422
Loss :  1.7312343120574951 2.621398687362671 4.352632999420166
Loss :  1.7176322937011719 2.715275287628174 4.432907581329346
Loss :  1.7267979383468628 2.558990478515625 4.285788536071777
Loss :  1.7225816249847412 2.8647892475128174 4.587370872497559
  batch 40 loss: 1.7225816249847412, 2.8647892475128174, 4.587370872497559
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.721680998802185 2.4744982719421387 4.196179389953613
Loss :  1.7224071025848389 2.549813747406006 4.272220611572266
Loss :  1.724314570426941 2.5646438598632812 4.288958549499512
Loss :  1.7289202213287354 2.829814910888672 4.558734893798828
Loss :  1.7327865362167358 2.474432945251465 4.20721960067749
Loss :  1.7235283851623535 3.494793653488159 5.218321800231934
Loss :  1.7211030721664429 3.6922030448913574 5.41330623626709
Loss :  1.7245557308197021 3.338351249694824 5.0629072189331055
Loss :  1.7234126329421997 2.532870292663574 4.256282806396484
Loss :  1.726422905921936 2.629615068435669 4.3560380935668945
Loss :  1.7105621099472046 3.5240588188171387 5.234621047973633
Loss :  1.7210438251495361 2.5643417835235596 4.285385608673096
Loss :  1.7319159507751465 2.3724114894866943 4.104327201843262
Loss :  1.729863166809082 2.6598968505859375 4.3897600173950195
Loss :  1.7308350801467896 3.43125581741333 5.16209077835083
Loss :  1.7208912372589111 3.2204806804656982 4.941371917724609
Loss :  1.735061526298523 2.435299873352051 4.170361518859863
Loss :  1.7336434125900269 2.1342310905456543 3.8678746223449707
Loss :  1.739021897315979 2.559556007385254 4.298577785491943
Loss :  1.7319966554641724 2.1089696884155273 3.84096622467041
  batch 60 loss: 1.7319966554641724, 2.1089696884155273, 3.84096622467041
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7298022508621216 2.1654579639434814 3.8952603340148926
Loss :  1.7360116243362427 2.0675017833709717 3.803513526916504
Loss :  1.7346307039260864 2.123715400695801 3.8583459854125977
Loss :  1.7253735065460205 2.383816719055176 4.109189987182617
Loss :  1.7250100374221802 2.4075098037719727 4.132519721984863
Loss :  1.7852327823638916 4.296953201293945 6.082185745239258
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7823176383972168 4.2368669509887695 6.019184589385986
Loss :  1.7854944467544556 4.1589508056640625 5.9444451332092285
Loss :  1.7915762662887573 4.154803276062012 5.946379661560059
Total LOSS train 4.417006921768189 valid 5.998048782348633
CE LOSS train 1.727328973550063 valid 0.44789406657218933
Contrastive LOSS train 2.6896779427161586 valid 1.038700819015503
EPOCH 76:
Loss :  1.7295632362365723 2.6952719688415527 4.424835205078125
Loss :  1.7256324291229248 2.7412402629852295 4.466872692108154
Loss :  1.720594882965088 2.7633121013641357 4.4839067459106445
Loss :  1.7334504127502441 3.0230071544647217 4.756457328796387
Loss :  1.7352406978607178 3.471034049987793 5.20627498626709
Loss :  1.7312067747116089 3.4886605739593506 5.21986722946167
Loss :  1.7253762483596802 3.4104056358337402 5.135781764984131
Loss :  1.7228752374649048 3.653470516204834 5.376345634460449
Loss :  1.7227909564971924 3.463904619216919 5.186695575714111
Loss :  1.6956487894058228 3.3840272426605225 5.079676151275635
Loss :  1.7293540239334106 3.428013324737549 5.15736722946167
Loss :  1.751452922821045 3.0544166564941406 4.8058695793151855
Loss :  1.7276349067687988 3.059528112411499 4.787162780761719
Loss :  1.7240993976593018 3.6496365070343018 5.3737359046936035
Loss :  1.7287275791168213 3.2403218746185303 4.969049453735352
Loss :  1.71448814868927 3.0291860103607178 4.743674278259277
Loss :  1.7298506498336792 2.6387054920196533 4.368556022644043
Loss :  1.7233490943908691 2.817577362060547 4.540926456451416
Loss :  1.72734797000885 2.517305612564087 4.244653701782227
Loss :  1.7124603986740112 2.791806221008301 4.504266738891602
  batch 20 loss: 1.7124603986740112, 2.791806221008301, 4.504266738891602
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.725829839706421 2.436701536178589 4.16253137588501
Loss :  1.72799813747406 3.0789780616760254 4.806976318359375
Loss :  1.7269989252090454 3.0731921195983887 4.8001909255981445
Loss :  1.738344669342041 2.858166217803955 4.596510887145996
Loss :  1.7261725664138794 2.815964698791504 4.542137145996094
Loss :  1.7242772579193115 2.640584707260132 4.364861965179443
Loss :  1.7380197048187256 2.899914026260376 4.637933731079102
Loss :  1.7209141254425049 2.6936941146850586 4.414608001708984
Loss :  1.7396619319915771 2.659949779510498 4.399611473083496
Loss :  1.7170860767364502 2.905686616897583 4.622772693634033
Loss :  1.7477699518203735 2.8962762355804443 4.644046306610107
Loss :  1.731329083442688 2.595954179763794 4.3272833824157715
Loss :  1.7210558652877808 2.570476531982422 4.291532516479492
Loss :  1.7169287204742432 2.7503788471221924 4.4673075675964355
Loss :  1.7316052913665771 2.623000383377075 4.354605674743652
Loss :  1.7329926490783691 3.0696017742156982 4.802594184875488
Loss :  1.730704426765442 2.84641170501709 4.577116012573242
Loss :  1.721827745437622 3.809377908706665 5.531205654144287
Loss :  1.7247475385665894 3.1309001445770264 4.855647563934326
Loss :  1.7234904766082764 3.1734745502471924 4.896965026855469
  batch 40 loss: 1.7234904766082764, 3.1734745502471924, 4.896965026855469
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7219033241271973 2.7787563800811768 4.500659942626953
Loss :  1.7211707830429077 2.706667423248291 4.427838325500488
Loss :  1.719838261604309 2.631608247756958 4.351446628570557
Loss :  1.7286871671676636 2.6501705646514893 4.378857612609863
Loss :  1.7300219535827637 2.6227028369903564 4.352725028991699
Loss :  1.7249397039413452 2.645036220550537 4.369976043701172
Loss :  1.7260595560073853 3.312335729598999 5.038395404815674
Loss :  1.7219761610031128 2.527655601501465 4.249631881713867
Loss :  1.7319378852844238 2.443779706954956 4.175717353820801
Loss :  1.7222890853881836 2.6510391235351562 4.37332820892334
Loss :  1.7061409950256348 3.352093458175659 5.058234214782715
Loss :  1.7194147109985352 2.826746940612793 4.546161651611328
Loss :  1.7353920936584473 3.000957727432251 4.736350059509277
Loss :  1.7232567071914673 3.4651384353637695 5.188395023345947
Loss :  1.720322608947754 3.8350930213928223 5.555415630340576
Loss :  1.7205177545547485 4.096784591674805 5.817302227020264
Loss :  1.7315351963043213 3.4376795291900635 5.169214725494385
Loss :  1.726875901222229 2.989894390106201 4.716770172119141
Loss :  1.7298429012298584 3.188265085220337 4.918107986450195
Loss :  1.730467438697815 3.3587911128997803 5.089258670806885
  batch 60 loss: 1.730467438697815, 3.3587911128997803, 5.089258670806885
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7299983501434326 3.2037088871002197 4.933707237243652
Loss :  1.7291396856307983 3.185899019241333 4.915038585662842
Loss :  1.7387233972549438 2.4159934520721436 4.154716968536377
Loss :  1.7171740531921387 2.8212671279907227 4.538441181182861
Loss :  1.7155838012695312 2.3351171016693115 4.050701141357422
Loss :  1.7961775064468384 3.9740538597106934 5.770231246948242
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.793594241142273 4.0656208992004395 5.859215259552002
Loss :  1.793453335762024 3.8841147422790527 5.677567958831787
Loss :  1.8118994235992432 3.930879831314087 5.74277925491333
Total LOSS train 4.715920088841365 valid 5.76244843006134
CE LOSS train 1.7261862956560574 valid 0.4529748558998108
Contrastive LOSS train 2.9897338023552527 valid 0.9827199578285217
EPOCH 77:
Loss :  1.721983790397644 2.553678274154663 4.275661945343018
Loss :  1.7278728485107422 2.920549154281616 4.6484222412109375
Loss :  1.716972827911377 2.427424669265747 4.144397735595703
Loss :  1.7249090671539307 2.5579659938812256 4.282875061035156
Loss :  1.7347337007522583 2.4721460342407227 4.206879615783691
Loss :  1.7324316501617432 2.2080137729644775 3.9404454231262207
Loss :  1.7290773391723633 2.251049280166626 3.9801266193389893
Loss :  1.7303913831710815 1.9364532232284546 3.666844606399536
Loss :  1.7256262302398682 2.3220205307006836 4.047646522521973
Loss :  1.7009522914886475 2.139511823654175 3.8404641151428223
Loss :  1.7320294380187988 2.396353006362915 4.128382682800293
Loss :  1.749911904335022 2.5508389472961426 4.300750732421875
Loss :  1.7283399105072021 2.650564432144165 4.378904342651367
Loss :  1.7246577739715576 2.7780916690826416 4.502749443054199
Loss :  1.7284973859786987 3.146850824356079 4.875348091125488
Loss :  1.7146241664886475 2.9803972244262695 4.695021629333496
Loss :  1.7311253547668457 3.5602400302886963 5.291365623474121
Loss :  1.7269346714019775 2.5599024295806885 4.286837100982666
Loss :  1.724054217338562 3.240699052810669 4.964753150939941
Loss :  1.7133859395980835 2.548255205154419 4.261641025543213
  batch 20 loss: 1.7133859395980835, 2.548255205154419, 4.261641025543213
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7246862649917603 2.3911097049713135 4.115796089172363
Loss :  1.727785348892212 2.6100034713745117 4.3377885818481445
Loss :  1.7280676364898682 2.442972421646118 4.171040058135986
Loss :  1.7385187149047852 2.699420213699341 4.437938690185547
Loss :  1.7223567962646484 2.629998207092285 4.352355003356934
Loss :  1.7224513292312622 2.824488401412964 4.546939849853516
Loss :  1.737339973449707 2.5264251232147217 4.263765335083008
Loss :  1.7210125923156738 2.2357332706451416 3.9567458629608154
Loss :  1.7384248971939087 2.1331355571746826 3.871560573577881
Loss :  1.714564561843872 2.92964506149292 4.644209861755371
Loss :  1.7453428506851196 3.0250954627990723 4.770438194274902
Loss :  1.729897379875183 2.3859074115753174 4.115804672241211
Loss :  1.7191355228424072 2.357166290283203 4.076301574707031
Loss :  1.7148545980453491 2.9710192680358887 4.685873985290527
Loss :  1.7309478521347046 2.682216167449951 4.413164138793945
Loss :  1.7315294742584229 2.6129090785980225 4.344438552856445
Loss :  1.7293223142623901 2.855539083480835 4.5848612785339355
Loss :  1.7204296588897705 2.234921455383301 3.9553511142730713
Loss :  1.7224524021148682 2.91717529296875 4.639627456665039
Loss :  1.7225980758666992 2.6481025218963623 4.370700836181641
  batch 40 loss: 1.7225980758666992, 2.6481025218963623, 4.370700836181641
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7212988138198853 2.9693117141723633 4.690610408782959
Loss :  1.7197656631469727 3.3977973461151123 5.117563247680664
Loss :  1.7205983400344849 2.775186061859131 4.495784282684326
Loss :  1.7276053428649902 2.335686445236206 4.063291549682617
Loss :  1.7292848825454712 2.1057932376861572 3.835078239440918
Loss :  1.722691535949707 2.3334364891052246 4.056128025054932
Loss :  1.7240396738052368 2.2266767024993896 3.950716495513916
Loss :  1.7212564945220947 2.2673237323760986 3.9885802268981934
Loss :  1.7306557893753052 2.324497938156128 4.055153846740723
Loss :  1.7242958545684814 2.536191701889038 4.2604875564575195
Loss :  1.7095179557800293 3.052696704864502 4.762214660644531
Loss :  1.721299171447754 3.2902374267578125 5.011536598205566
Loss :  1.7342157363891602 3.4188334941864014 5.153049468994141
Loss :  1.725488305091858 3.374677896499634 5.100166320800781
Loss :  1.7208747863769531 3.1265289783477783 4.847403526306152
Loss :  1.7199457883834839 2.42330265045166 4.143248558044434
Loss :  1.7308927774429321 2.4007201194763184 4.131612777709961
Loss :  1.7294477224349976 2.418671131134033 4.14811897277832
Loss :  1.7318716049194336 2.5715537071228027 4.303425312042236
Loss :  1.729081153869629 3.5182716846466064 5.247352600097656
  batch 60 loss: 1.729081153869629, 3.5182716846466064, 5.247352600097656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7288063764572144 2.948065757751465 4.676872253417969
Loss :  1.7308636903762817 3.0287363529205322 4.7596001625061035
Loss :  1.7379555702209473 4.264251708984375 6.002207279205322
Loss :  1.7155038118362427 3.8847856521606445 5.600289344787598
Loss :  1.717219591140747 3.4488179683685303 5.166037559509277
Loss :  1.8024535179138184 4.334530830383301 6.136984348297119
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8101235628128052 4.241921424865723 6.052044868469238
Loss :  1.8044309616088867 4.095466136932373 5.89989709854126
Loss :  1.8220032453536987 3.9484589099884033 5.7704620361328125
Total LOSS train 4.4452422875624436 valid 5.964847087860107
CE LOSS train 1.7259185625956608 valid 0.4555008113384247
Contrastive LOSS train 2.7193237176308265 valid 0.9871147274971008
EPOCH 78:
Loss :  1.7211707830429077 3.677300453186035 5.398471355438232
Loss :  1.7545090913772583 4.3561692237854 6.110678195953369
Loss :  1.734238624572754 4.140170097351074 5.874408721923828
Loss :  1.7208162546157837 4.516962051391602 6.237778186798096
Loss :  1.7577685117721558 4.1283345222473145 5.88610315322876
Loss :  1.7536144256591797 4.167438983917236 5.921053409576416
Loss :  1.7310919761657715 3.921302080154419 5.6523942947387695
Loss :  1.7242788076400757 3.889028787612915 5.613307476043701
Loss :  1.7401498556137085 3.470953941345215 5.211103916168213
Loss :  1.7217248678207397 3.229898691177368 4.951623439788818
Loss :  1.724636197090149 3.5151541233062744 5.239790439605713
Loss :  1.75684654712677 3.72763729095459 5.48448371887207
Loss :  1.71242094039917 3.653323173522949 5.365744113922119
Loss :  1.741128921508789 3.7048943042755127 5.446022987365723
Loss :  1.7577141523361206 3.574298858642578 5.332013130187988
Loss :  1.7470042705535889 3.520665168762207 5.267669677734375
Loss :  1.739100694656372 3.439434051513672 5.178534507751465
Loss :  1.7312067747116089 3.4238030910491943 5.155009746551514
Loss :  1.7222963571548462 3.2657878398895264 4.988084316253662
Loss :  1.7240194082260132 4.0385026931762695 5.762522220611572
  batch 20 loss: 1.7240194082260132, 4.0385026931762695, 5.762522220611572
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7668309211730957 3.4614500999450684 5.228281021118164
Loss :  1.7575063705444336 4.216413497924805 5.973919868469238
Loss :  1.7364872694015503 4.244554042816162 5.981041431427002
Loss :  1.7652926445007324 3.9219648838043213 5.687257766723633
Loss :  1.7477301359176636 3.6045877933502197 5.352317810058594
Loss :  1.7278612852096558 3.5664217472076416 5.294282913208008
Loss :  1.7342568635940552 3.571497678756714 5.305754661560059
Loss :  1.7115412950515747 3.1303532123565674 4.841894626617432
Loss :  1.7301322221755981 3.2573814392089844 4.987513542175293
Loss :  1.740801453590393 3.364219903945923 5.1050214767456055
Loss :  1.7585198879241943 3.2965497970581055 5.055069923400879
Loss :  1.7303955554962158 3.489980936050415 5.220376491546631
Loss :  1.722639560699463 3.3995769023895264 5.12221622467041
Loss :  1.7258529663085938 3.2962467670440674 5.022099494934082
Loss :  1.730516791343689 3.2846310138702393 5.015147686004639
Loss :  1.7279143333435059 3.2651658058166504 4.993080139160156
Loss :  1.7259539365768433 3.3309335708618164 5.056887626647949
Loss :  1.6841906309127808 3.0575544834136963 4.7417449951171875
Loss :  1.7173289060592651 3.1096715927124023 4.827000617980957
Loss :  1.7159326076507568 3.176673173904419 4.892605781555176
  batch 40 loss: 1.7159326076507568, 3.176673173904419, 4.892605781555176
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7127658128738403 3.3582963943481445 5.071062088012695
Loss :  1.7144519090652466 2.711824893951416 4.426276683807373
Loss :  1.728254795074463 3.3685660362243652 5.096820831298828
Loss :  1.7328150272369385 3.3205838203430176 5.053399085998535
Loss :  1.716450572013855 3.112668752670288 4.8291192054748535
Loss :  1.707819938659668 3.221804618835449 4.929624557495117
Loss :  1.7000411748886108 3.174858331680298 4.874899387359619
Loss :  1.7080332040786743 3.3462064266204834 5.054239749908447
Loss :  1.6896357536315918 3.3048698902130127 4.994505882263184
Loss :  1.7356027364730835 3.4665491580963135 5.202151775360107
Loss :  1.7111438512802124 3.530548572540283 5.241692543029785
Loss :  1.6970782279968262 3.825734853744507 5.522812843322754
Loss :  1.7149157524108887 3.6149957180023193 5.329911231994629
Loss :  1.7274963855743408 3.799060106277466 5.526556491851807
Loss :  1.7366756200790405 3.368468761444092 5.105144500732422
Loss :  1.705817699432373 3.596950054168701 5.302767753601074
Loss :  1.7266976833343506 3.772366523742676 5.4990644454956055
Loss :  1.7278127670288086 3.3365538120269775 5.064366340637207
Loss :  1.7287585735321045 3.554569959640503 5.283328533172607
Loss :  1.7162259817123413 3.1158676147460938 4.832093715667725
  batch 60 loss: 1.7162259817123413, 3.1158676147460938, 4.832093715667725
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7128961086273193 3.1185848712921143 4.831480979919434
Loss :  1.7169710397720337 3.426133394241333 5.143104553222656
Loss :  1.7110902070999146 3.250096559524536 4.96118688583374
Loss :  1.7105622291564941 3.5914294719696045 5.3019914627075195
Loss :  1.7067776918411255 3.2063353061676025 4.913113117218018
Loss :  1.805845022201538 3.4448721408843994 5.2507171630859375
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.817368507385254 3.3997652530670166 5.217133522033691
Loss :  1.8023494482040405 3.2312698364257812 5.033619403839111
Loss :  1.8206712007522583 3.2588553428649902 5.079526424407959
Total LOSS train 5.248785011584943 valid 5.145249128341675
CE LOSS train 1.7272340591137225 valid 0.4551678001880646
Contrastive LOSS train 3.5215509488032413 valid 0.8147138357162476
Saved best model. Old loss 5.268293738365173 and new best loss 5.145249128341675
EPOCH 79:
Loss :  1.7100921869277954 3.367478847503662 5.077570915222168
Loss :  1.707881212234497 3.4266371726989746 5.134518623352051
Loss :  1.7095763683319092 3.219464063644409 4.929040431976318
Loss :  1.7177420854568481 3.3865888118743896 5.104331016540527
Loss :  1.7247543334960938 4.038299083709717 5.7630534172058105
Loss :  1.7102651596069336 3.4500579833984375 5.160323143005371
Loss :  1.7246577739715576 3.2240188121795654 4.948676586151123
Loss :  1.7112678289413452 3.431751251220703 5.143019199371338
Loss :  1.7227215766906738 3.67136287689209 5.394084453582764
Loss :  1.7002571821212769 3.577176332473755 5.277433395385742
Loss :  1.7198960781097412 3.418475389480591 5.138371467590332
Loss :  1.7501169443130493 4.084750175476074 5.834867000579834
Loss :  1.7145203351974487 3.7715342044830322 5.486054420471191
Loss :  1.7156673669815063 3.3363373279571533 5.052004814147949
Loss :  1.7257518768310547 3.440535306930542 5.166287422180176
Loss :  1.712909460067749 3.203876256942749 4.916785717010498
Loss :  1.719320297241211 3.2966458797454834 5.015966415405273
Loss :  1.7166224718093872 3.437640905380249 5.154263496398926
Loss :  1.7218934297561646 3.12555193901062 4.847445487976074
Loss :  1.7179327011108398 2.9365360736846924 4.654468536376953
  batch 20 loss: 1.7179327011108398, 2.9365360736846924, 4.654468536376953
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7211121320724487 2.9833602905273438 4.704472541809082
Loss :  1.7257109880447388 3.1861650943756104 4.911876201629639
Loss :  1.7253063917160034 3.8935582637786865 5.6188645362854
Loss :  1.7336485385894775 3.2399370670318604 4.973585605621338
Loss :  1.7340316772460938 3.3142435550689697 5.048274993896484
Loss :  1.7236396074295044 3.822225332260132 5.545865058898926
Loss :  1.7391070127487183 3.8671555519104004 5.606262683868408
Loss :  1.7061412334442139 3.1163227558135986 4.8224639892578125
Loss :  1.7287715673446655 3.3728370666503906 5.101608753204346
Loss :  1.7125356197357178 3.1755995750427246 4.888134956359863
Loss :  1.7471818923950195 3.5396435260772705 5.286825180053711
Loss :  1.7311131954193115 3.4204297065734863 5.151542663574219
Loss :  1.7176733016967773 3.660931348800659 5.378604888916016
Loss :  1.7220150232315063 3.4924850463867188 5.2144999504089355
Loss :  1.730000376701355 3.536860227584839 5.266860485076904
Loss :  1.733886480331421 3.3921940326690674 5.126080513000488
Loss :  1.727636456489563 3.2436203956604004 4.971256732940674
Loss :  1.7087364196777344 3.7528235912323 5.461560249328613
Loss :  1.7262721061706543 3.4279239177703857 5.154195785522461
Loss :  1.7239956855773926 3.3494057655334473 5.07340145111084
  batch 40 loss: 1.7239956855773926, 3.3494057655334473, 5.07340145111084
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7165789604187012 3.553379535675049 5.26995849609375
Loss :  1.713619589805603 3.49839448928833 5.212014198303223
Loss :  1.7276872396469116 3.8167433738708496 5.544430732727051
Loss :  1.717189908027649 4.06772518157959 5.784914970397949
Loss :  1.7229260206222534 3.2724225521087646 4.9953484535217285
Loss :  1.717061996459961 3.8985753059387207 5.615637302398682
Loss :  1.711997628211975 3.299929618835449 5.011927127838135
Loss :  1.7184869050979614 3.5838522911071777 5.30233907699585
Loss :  1.7159677743911743 3.4248366355895996 5.140804290771484
Loss :  1.7225422859191895 3.08693528175354 4.809477806091309
Loss :  1.7171252965927124 3.255667209625244 4.972792625427246
Loss :  1.7223501205444336 3.3992626667022705 5.121612548828125
Loss :  1.7371859550476074 3.26800537109375 5.005191326141357
Loss :  1.7293967008590698 3.122799873352051 4.85219669342041
Loss :  1.7249267101287842 3.3668181896209717 5.091744899749756
Loss :  1.7128043174743652 3.4705379009246826 5.183341979980469
Loss :  1.732416033744812 3.84391188621521 5.576327800750732
Loss :  1.7280796766281128 3.360069990158081 5.088149547576904
Loss :  1.7354165315628052 3.8352246284484863 5.570641040802002
Loss :  1.7250730991363525 3.7371320724487305 5.462204933166504
  batch 60 loss: 1.7250730991363525, 3.7371320724487305, 5.462204933166504
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7211759090423584 3.618885040283203 5.340061187744141
Loss :  1.7268989086151123 3.6885197162628174 5.41541862487793
Loss :  1.7183209657669067 3.6134533882141113 5.3317742347717285
Loss :  1.7185554504394531 3.3243370056152344 5.0428924560546875
Loss :  1.7164908647537231 3.1561472415924072 4.87263822555542
Loss :  1.7807985544204712 3.421513319015503 5.202311992645264
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.783096194267273 3.356205463409424 5.139301776885986
Loss :  1.789440393447876 3.1244168281555176 4.913857460021973
Loss :  1.806447148323059 3.1585919857025146 4.965039253234863
Total LOSS train 5.186440673241249 valid 5.0551276206970215
CE LOSS train 1.7218867265261137 valid 0.45161178708076477
Contrastive LOSS train 3.4645539577190694 valid 0.7896479964256287
Saved best model. Old loss 5.145249128341675 and new best loss 5.0551276206970215
EPOCH 80:
Loss :  1.7113291025161743 3.4964609146118164 5.207789897918701
Loss :  1.709488868713379 3.69685697555542 5.406345844268799
Loss :  1.7110439538955688 3.4671695232391357 5.178213596343994
Loss :  1.7131091356277466 3.134906768798828 4.848015785217285
Loss :  1.7276633977890015 3.5754013061523438 5.303064823150635
Loss :  1.7167253494262695 3.9314053058624268 5.648130416870117
Loss :  1.7141185998916626 3.900550603866577 5.614669322967529
Loss :  1.7085635662078857 3.533250570297241 5.241814136505127
Loss :  1.7180991172790527 3.2172842025756836 4.935383319854736
Loss :  1.69521963596344 3.3064308166503906 5.001650333404541
Loss :  1.7191084623336792 3.1841533184051514 4.903261661529541
Loss :  1.7449166774749756 3.584087610244751 5.329004287719727
Loss :  1.7185145616531372 3.332749366760254 5.051263809204102
Loss :  1.7186493873596191 3.1921541690826416 4.91080379486084
Loss :  1.7191535234451294 3.457054853439331 5.17620849609375
Loss :  1.7155479192733765 3.475055694580078 5.190603733062744
Loss :  1.7236086130142212 3.8299522399902344 5.553560733795166
Loss :  1.719781517982483 3.608193874359131 5.327975273132324
Loss :  1.7132216691970825 3.3599493503570557 5.073171138763428
Loss :  1.7094779014587402 2.8786303997039795 4.588108062744141
  batch 20 loss: 1.7094779014587402, 2.8786303997039795, 4.588108062744141
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.721657395362854 3.0905983448028564 4.812255859375
Loss :  1.7208582162857056 3.2959108352661133 5.016768932342529
Loss :  1.7228609323501587 3.545306444168091 5.268167495727539
Loss :  1.7258658409118652 3.173017740249634 4.898883819580078
Loss :  1.7245055437088013 3.487818956375122 5.212324619293213
Loss :  1.7189785242080688 3.4120607376098633 5.131039142608643
Loss :  1.7334190607070923 3.346604824066162 5.080023765563965
Loss :  1.7134500741958618 3.640444278717041 5.353894233703613
Loss :  1.7341052293777466 3.86125111579895 5.595356464385986
Loss :  1.709023118019104 3.544546365737915 5.253569602966309
Loss :  1.7467598915100098 3.562887668609619 5.309647560119629
Loss :  1.7282620668411255 3.311530113220215 5.039792060852051
Loss :  1.7149676084518433 3.0212316513061523 4.736199378967285
Loss :  1.71844482421875 2.9997317790985107 4.71817684173584
Loss :  1.7299256324768066 3.188737392425537 4.918663024902344
Loss :  1.7306759357452393 3.4139339923858643 5.1446099281311035
Loss :  1.7248057126998901 3.0223214626312256 4.747127056121826
Loss :  1.7030723094940186 3.457602024078369 5.160674095153809
Loss :  1.7180790901184082 3.291125774383545 5.009204864501953
Loss :  1.7199889421463013 3.257918357849121 4.977907180786133
  batch 40 loss: 1.7199889421463013, 3.257918357849121, 4.977907180786133
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7164571285247803 2.9770023822784424 4.693459510803223
Loss :  1.7051771879196167 3.047765016555786 4.752942085266113
Loss :  1.7206146717071533 3.3579323291778564 5.07854700088501
Loss :  1.7070488929748535 3.180553674697876 4.887602806091309
Loss :  1.7185285091400146 2.8692502975463867 4.5877790451049805
Loss :  1.7250301837921143 3.0344362258911133 4.759466171264648
Loss :  1.715713381767273 3.0549538135528564 4.77066707611084
Loss :  1.7136998176574707 3.381488800048828 5.095188617706299
Loss :  1.7222956418991089 3.3775994777679443 5.099895000457764
Loss :  1.716988444328308 3.232938051223755 4.949926376342773
Loss :  1.7118252515792847 3.194023609161377 4.905848979949951
Loss :  1.7173938751220703 2.7716827392578125 4.489076614379883
Loss :  1.7354885339736938 2.744717836380005 4.480206489562988
Loss :  1.7135004997253418 2.5278878211975098 4.241388320922852
Loss :  1.7201881408691406 2.551100969314575 4.271288871765137
Loss :  1.7152799367904663 2.400707960128784 4.115987777709961
Loss :  1.7298599481582642 2.700794219970703 4.430654048919678
Loss :  1.725379467010498 3.1247291564941406 4.850108623504639
Loss :  1.7358531951904297 3.1687169075012207 4.90457010269165
Loss :  1.7292066812515259 2.747309923171997 4.4765167236328125
  batch 60 loss: 1.7292066812515259, 2.747309923171997, 4.4765167236328125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7271349430084229 3.0384647846221924 4.765599727630615
Loss :  1.723464846611023 2.8362815380096436 4.559746265411377
Loss :  1.7287465333938599 2.9322290420532227 4.660975456237793
Loss :  1.7205629348754883 3.0994906425476074 4.820053577423096
Loss :  1.7163341045379639 2.595330238342285 4.311664581298828
Loss :  1.7320433855056763 3.5915350914001465 5.323578357696533
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7282602787017822 3.5727157592773438 5.300975799560547
Loss :  1.748884677886963 3.4507603645324707 5.199645042419434
Loss :  1.7522660493850708 3.4673361778259277 5.219602108001709
Total LOSS train 4.951268988389235 valid 5.260950326919556
CE LOSS train 1.7199818409406222 valid 0.4380665123462677
Contrastive LOSS train 3.2312871566185586 valid 0.8668340444564819
EPOCH 81:
Loss :  1.7167367935180664 3.3221521377563477 5.038888931274414
Loss :  1.7204357385635376 3.1136882305145264 4.8341240882873535
Loss :  1.7152997255325317 3.248246431350708 4.963546276092529
Loss :  1.7150009870529175 3.8821942806243896 5.597195148468018
Loss :  1.7332698106765747 3.6817402839660645 5.41500997543335
Loss :  1.7172584533691406 3.4158873558044434 5.133145809173584
Loss :  1.7252320051193237 2.9893293380737305 4.714561462402344
Loss :  1.7166112661361694 3.3102328777313232 5.026844024658203
Loss :  1.7208292484283447 3.399033308029175 5.1198625564575195
Loss :  1.6956613063812256 3.0887527465820312 4.784414291381836
Loss :  1.719026803970337 3.4274697303771973 5.146496772766113
Loss :  1.7327989339828491 3.0740773677825928 4.806876182556152
Loss :  1.7205970287322998 3.4099948406219482 5.130591869354248
Loss :  1.7154685258865356 2.9389963150024414 4.6544647216796875
Loss :  1.717950701713562 3.126082420349121 4.844033241271973
Loss :  1.7087278366088867 3.1968390941619873 4.905567169189453
Loss :  1.7181906700134277 3.170574426651001 4.888765335083008
Loss :  1.7149348258972168 3.600104808807373 5.31503963470459
Loss :  1.7038031816482544 4.040934085845947 5.744737148284912
Loss :  1.706160068511963 2.9561033248901367 4.6622633934021
  batch 20 loss: 1.706160068511963, 2.9561033248901367, 4.6622633934021
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7188411951065063 3.0316035747528076 4.7504448890686035
Loss :  1.71257483959198 3.0512986183166504 4.76387357711792
Loss :  1.719539999961853 2.59316349029541 4.312703609466553
Loss :  1.7239614725112915 3.080137014389038 4.804098606109619
Loss :  1.717098593711853 3.120417594909668 4.8375163078308105
Loss :  1.7170367240905762 2.964315176010132 4.681351661682129
Loss :  1.728793978691101 3.199068307876587 4.927862167358398
Loss :  1.7070783376693726 3.0485992431640625 4.755677700042725
Loss :  1.7294530868530273 3.1675479412078857 4.897001266479492
Loss :  1.7033579349517822 3.490877628326416 5.194235801696777
Loss :  1.73807954788208 3.3694257736206055 5.1075053215026855
Loss :  1.720260739326477 3.2371761798858643 4.957437038421631
Loss :  1.7056270837783813 3.2456881999969482 4.951315402984619
Loss :  1.7078511714935303 3.064180612564087 4.772031784057617
Loss :  1.7192096710205078 2.9122726917266846 4.631482124328613
Loss :  1.7241705656051636 2.984682559967041 4.708853244781494
Loss :  1.7186256647109985 2.6346917152404785 4.3533172607421875
Loss :  1.6964657306671143 2.754103899002075 4.4505696296691895
Loss :  1.7117919921875 2.5979859828948975 4.309778213500977
Loss :  1.711826205253601 2.587412118911743 4.299238204956055
  batch 40 loss: 1.711826205253601, 2.587412118911743, 4.299238204956055
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.710761547088623 2.788987398147583 4.499749183654785
Loss :  1.701025128364563 2.472238063812256 4.173263072967529
Loss :  1.71074378490448 2.844609260559082 4.555353164672852
Loss :  1.705532193183899 2.69892954826355 4.404461860656738
Loss :  1.712575912475586 2.595717191696167 4.308293342590332
Loss :  1.7159911394119263 2.964891195297241 4.680882453918457
Loss :  1.7108886241912842 3.2574682235717773 4.968357086181641
Loss :  1.7053446769714355 2.7239127159118652 4.429257392883301
Loss :  1.7182292938232422 2.901188373565674 4.619417667388916
Loss :  1.7120546102523804 3.2281858921051025 4.940240383148193
Loss :  1.7042864561080933 3.18182110786438 4.886107444763184
Loss :  1.7150509357452393 3.001835584640503 4.716886520385742
Loss :  1.7256282567977905 2.9771950244903564 4.702823162078857
Loss :  1.7143232822418213 2.9651684761047363 4.679491996765137
Loss :  1.715061068534851 2.5512874126434326 4.266348361968994
Loss :  1.7099195718765259 2.573089122772217 4.283008575439453
Loss :  1.728867769241333 3.283207654953003 5.012075424194336
Loss :  1.7217077016830444 3.0016896724700928 4.723397254943848
Loss :  1.7296769618988037 3.136413812637329 4.866090774536133
Loss :  1.7266865968704224 2.8386685848236084 4.56535530090332
  batch 60 loss: 1.7266865968704224, 2.8386685848236084, 4.56535530090332
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.719109058380127 3.0758676528930664 4.794976711273193
Loss :  1.7199878692626953 3.167180061340332 4.887167930603027
Loss :  1.7244529724121094 2.9428980350494385 4.667350769042969
Loss :  1.7161810398101807 3.0912671089172363 4.807448387145996
Loss :  1.7095011472702026 2.6565940380096436 4.366095066070557
Loss :  1.7133485078811646 3.4738190174102783 5.187167644500732
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7178257703781128 3.414865255355835 5.132690906524658
Loss :  1.739763617515564 3.214083671569824 4.953847408294678
Loss :  1.7445037364959717 3.349820137023926 5.094324111938477
Total LOSS train 4.7845634020291845 valid 5.092007517814636
CE LOSS train 1.716141939163208 valid 0.4361259341239929
Contrastive LOSS train 3.068421429854173 valid 0.8374550342559814
EPOCH 82:
Loss :  1.7096281051635742 3.152111530303955 4.861739635467529
Loss :  1.7121434211730957 3.1490821838378906 4.861225605010986
Loss :  1.7064400911331177 3.058216094970703 4.764656066894531
Loss :  1.7073826789855957 2.670557737350464 4.3779401779174805
Loss :  1.7248371839523315 3.003882884979248 4.728720188140869
Loss :  1.707108736038208 3.248469829559326 4.955578804016113
Loss :  1.7179226875305176 3.2605044841766357 4.978426933288574
Loss :  1.706402063369751 3.1175942420959473 4.823996543884277
Loss :  1.7135310173034668 3.1980843544006348 4.911615371704102
Loss :  1.6868425607681274 2.8680636882781982 4.554906368255615
Loss :  1.7102717161178589 2.6753499507904053 4.385621547698975
Loss :  1.734074592590332 3.142303943634033 4.876378536224365
Loss :  1.7150136232376099 2.710068702697754 4.425082206726074
Loss :  1.7100456953048706 2.756256103515625 4.466301918029785
Loss :  1.7151834964752197 2.839247703552246 4.554430961608887
Loss :  1.7051002979278564 2.6541359424591064 4.359236240386963
Loss :  1.7149907350540161 2.438448429107666 4.153439044952393
Loss :  1.7120544910430908 2.7786245346069336 4.490678787231445
Loss :  1.7055764198303223 2.340000867843628 4.045577049255371
Loss :  1.707838773727417 3.1340034008026123 4.841842174530029
  batch 20 loss: 1.707838773727417, 3.1340034008026123, 4.841842174530029
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.719845175743103 3.118147611618042 4.8379926681518555
Loss :  1.7145779132843018 2.491877317428589 4.206455230712891
Loss :  1.7218713760375977 2.4921975135803223 4.21406888961792
Loss :  1.7242480516433716 2.311155080795288 4.035403251647949
Loss :  1.7182080745697021 2.659644365310669 4.377852439880371
Loss :  1.7180922031402588 2.5494470596313477 4.267539024353027
Loss :  1.729171872138977 2.596484661102295 4.325656414031982
Loss :  1.7094351053237915 2.7232372760772705 4.432672500610352
Loss :  1.7310842275619507 3.148683786392212 4.879767894744873
Loss :  1.705411434173584 3.366367816925049 5.071779251098633
Loss :  1.7400132417678833 3.826399803161621 5.566412925720215
Loss :  1.7235369682312012 3.498230218887329 5.221767425537109
Loss :  1.7087146043777466 3.454390287399292 5.163105010986328
Loss :  1.7099599838256836 3.0181968212127686 4.728157043457031
Loss :  1.721145510673523 3.2242555618286133 4.945401191711426
Loss :  1.728108286857605 3.2719953060150146 5.00010347366333
Loss :  1.7196568250656128 2.7190420627593994 4.438698768615723
Loss :  1.7034556865692139 2.765874147415161 4.469329833984375
Loss :  1.7148752212524414 3.2503409385681152 4.965216159820557
Loss :  1.7161636352539062 3.356387138366699 5.0725507736206055
  batch 40 loss: 1.7161636352539062, 3.356387138366699, 5.0725507736206055
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7145439386367798 2.78646183013916 4.50100564956665
Loss :  1.704054594039917 2.0939888954162598 3.7980434894561768
Loss :  1.7111166715621948 2.5867960453033447 4.29791259765625
Loss :  1.7085908651351929 2.840327024459839 4.548917770385742
Loss :  1.7122201919555664 2.3988683223724365 4.111088752746582
Loss :  1.715610146522522 2.8779144287109375 4.59352445602417
Loss :  1.7114773988723755 2.905935049057007 4.617412567138672
Loss :  1.7032471895217896 2.814744234085083 4.517991542816162
Loss :  1.7175384759902954 3.0305604934692383 4.748098850250244
Loss :  1.7110211849212646 3.111949920654297 4.822971343994141
Loss :  1.7047514915466309 2.915654420852661 4.620406150817871
Loss :  1.7189099788665771 2.613417863845825 4.332327842712402
Loss :  1.7283821105957031 2.6518683433532715 4.380250453948975
Loss :  1.7168618440628052 3.3717477321624756 5.08860969543457
Loss :  1.714739203453064 2.986447334289551 4.701186656951904
Loss :  1.7067071199417114 3.3216969966888428 5.028404235839844
Loss :  1.730024814605713 3.017777919769287 4.747802734375
Loss :  1.719389796257019 3.0842225551605225 4.803612232208252
Loss :  1.7292760610580444 3.1727473735809326 4.9020233154296875
Loss :  1.722880244255066 2.633676767349243 4.3565568923950195
  batch 60 loss: 1.722880244255066, 2.633676767349243, 4.3565568923950195
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7178481817245483 3.2429733276367188 4.960821628570557
Loss :  1.7170093059539795 3.2940456867218018 5.011054992675781
Loss :  1.724454402923584 2.7921946048736572 4.51664924621582
Loss :  1.718674898147583 2.6725754737854004 4.3912506103515625
Loss :  1.714105248451233 2.5651886463165283 4.279294013977051
Loss :  1.691219687461853 3.937227964401245 5.628447532653809
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7229900360107422 4.067991733551025 5.790981769561768
Loss :  1.7236171960830688 3.794257164001465 5.517874240875244
Loss :  1.7322221994400024 3.918529748916626 5.650752067565918
Total LOSS train 4.635608339309693 valid 5.647013902664185
CE LOSS train 1.7152830637418306 valid 0.4330555498600006
Contrastive LOSS train 2.9203252718998836 valid 0.9796324372291565
EPOCH 83:
Loss :  1.7116838693618774 2.3197121620178223 4.03139591217041
Loss :  1.7134705781936646 2.6900112628936768 4.403481960296631
Loss :  1.7102540731430054 2.928701877593994 4.638956069946289
Loss :  1.7127467393875122 2.7168681621551514 4.429615020751953
Loss :  1.729837417602539 2.2119815349578857 3.941818952560425
Loss :  1.7131670713424683 2.907383441925049 4.620550632476807
Loss :  1.7211956977844238 3.798431158065796 5.519626617431641
Loss :  1.7099896669387817 3.7094545364379883 5.4194440841674805
Loss :  1.7186373472213745 2.675020456314087 4.393657684326172
Loss :  1.692367434501648 2.5177955627441406 4.210163116455078
Loss :  1.7166659832000732 2.508234977722168 4.22490119934082
Loss :  1.737966537475586 2.982020139694214 4.719986915588379
Loss :  1.7175034284591675 3.1719777584075928 4.889481067657471
Loss :  1.713294506072998 2.9749581813812256 4.6882524490356445
Loss :  1.7177566289901733 2.6770529747009277 4.394809722900391
Loss :  1.7083154916763306 2.544332265853882 4.252647876739502
Loss :  1.7157779932022095 2.5920825004577637 4.307860374450684
Loss :  1.71212899684906 2.4808506965637207 4.19297981262207
Loss :  1.710218906402588 2.3945062160491943 4.104724884033203
Loss :  1.706063151359558 2.798784017562866 4.504847049713135
  batch 20 loss: 1.706063151359558, 2.798784017562866, 4.504847049713135
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.719274878501892 3.121202230453491 4.840476989746094
Loss :  1.7135306596755981 3.07711124420166 4.790641784667969
Loss :  1.7227340936660767 2.4573183059692383 4.180052280426025
Loss :  1.725784420967102 3.1730690002441406 4.898853302001953
Loss :  1.7223289012908936 3.0143511295318604 4.736680030822754
Loss :  1.7179992198944092 2.8004066944122314 4.518405914306641
Loss :  1.728662371635437 2.752760171890259 4.481422424316406
Loss :  1.7057887315750122 2.465060234069824 4.170848846435547
Loss :  1.7293546199798584 3.030813694000244 4.760168075561523
Loss :  1.70511794090271 3.2193493843078613 4.924467086791992
Loss :  1.74134361743927 3.015038013458252 4.756381511688232
Loss :  1.7237017154693604 3.06138277053833 4.7850847244262695
Loss :  1.7090954780578613 2.4750452041625977 4.184140682220459
Loss :  1.7133123874664307 3.147019147872925 4.8603315353393555
Loss :  1.7232669591903687 3.2452855110168457 4.968552589416504
Loss :  1.7302181720733643 3.11875581741333 4.848974227905273
Loss :  1.722883701324463 4.301146507263184 6.0240302085876465
Loss :  1.7205642461776733 4.011373043060303 5.731937408447266
Loss :  1.71663498878479 3.7844090461730957 5.501044273376465
Loss :  1.7104002237319946 3.3751895427703857 5.08558988571167
  batch 40 loss: 1.7104002237319946, 3.3751895427703857, 5.08558988571167
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7310209274291992 3.727973461151123 5.458994388580322
Loss :  1.736312985420227 3.1139602661132812 4.850273132324219
Loss :  1.730610966682434 3.092771291732788 4.823382377624512
Loss :  1.7455289363861084 3.3077986240386963 5.053327560424805
Loss :  1.731950044631958 3.0354387760162354 4.767388820648193
Loss :  1.719873309135437 2.7631309032440186 4.483004093170166
Loss :  1.7253378629684448 2.6422410011291504 4.367578983306885
Loss :  1.7243616580963135 2.608860492706299 4.333222389221191
Loss :  1.7204737663269043 2.7062580585479736 4.426732063293457
Loss :  1.7259308099746704 3.0119900703430176 4.737920761108398
Loss :  1.7089827060699463 3.1973111629486084 4.906293869018555
Loss :  1.7220019102096558 3.219623565673828 4.941625595092773
Loss :  1.7347965240478516 3.148207426071167 4.883004188537598
Loss :  1.725494384765625 3.1710729598999023 4.896567344665527
Loss :  1.7252010107040405 3.633700370788574 5.358901500701904
Loss :  1.7224379777908325 3.7881548404693604 5.510592937469482
Loss :  1.727299451828003 3.2143187522888184 4.941617965698242
Loss :  1.7295840978622437 3.4576947689056396 5.187278747558594
Loss :  1.7320239543914795 3.6832032203674316 5.415226936340332
Loss :  1.7296578884124756 3.4598770141601562 5.189535140991211
  batch 60 loss: 1.7296578884124756, 3.4598770141601562, 5.189535140991211
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.728468894958496 3.0459401607513428 4.774409294128418
Loss :  1.7294092178344727 3.0430870056152344 4.772496223449707
Loss :  1.7340127229690552 3.5271658897399902 5.261178493499756
Loss :  1.719170093536377 3.537292718887329 5.256463050842285
Loss :  1.7138200998306274 3.307615041732788 5.021435260772705
Loss :  1.8526800870895386 4.045691013336182 5.89837121963501
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8550846576690674 4.171973705291748 6.0270586013793945
Loss :  1.8490689992904663 3.958491802215576 5.807560920715332
Loss :  1.840847134590149 3.799429416656494 5.6402764320373535
Total LOSS train 4.777780558512761 valid 5.8433167934417725
CE LOSS train 1.7209969392189612 valid 0.46021178364753723
Contrastive LOSS train 3.0567836064558764 valid 0.9498573541641235
EPOCH 84:
Loss :  1.725043773651123 3.382596731185913 5.107640266418457
Loss :  1.7229530811309814 3.2230372428894043 4.945990562438965
Loss :  1.7151914834976196 3.140795946121216 4.855987548828125
Loss :  1.7280375957489014 3.15850830078125 4.8865461349487305
Loss :  1.7269953489303589 3.3985695838928223 5.125565052032471
Loss :  1.7290796041488647 3.5219566822052 5.251036167144775
Loss :  1.719168782234192 3.482510566711426 5.201679229736328
Loss :  1.722399353981018 3.1990575790405273 4.921456813812256
Loss :  1.7226932048797607 3.390828847885132 5.113522052764893
Loss :  1.695375919342041 3.051602363586426 4.746978282928467
Loss :  1.72190260887146 3.539675235748291 5.261577606201172
Loss :  1.7477210760116577 3.1504416465759277 4.898162841796875
Loss :  1.7198785543441772 2.7809317111968994 4.500810146331787
Loss :  1.7196621894836426 2.378272294998169 4.097934722900391
Loss :  1.7299166917800903 2.994013786315918 4.723930358886719
Loss :  1.7101854085922241 2.7238879203796387 4.434073448181152
Loss :  1.7261371612548828 2.7121493816375732 4.438286781311035
Loss :  1.713283896446228 2.6155037879943848 4.328787803649902
Loss :  1.7207306623458862 2.79992938041687 4.520659923553467
Loss :  1.7123724222183228 3.180246591567993 4.8926191329956055
  batch 20 loss: 1.7123724222183228, 3.180246591567993, 4.8926191329956055
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7224054336547852 2.4951364994049072 4.217541694641113
Loss :  1.727152705192566 3.5182788372039795 5.245431423187256
Loss :  1.721985101699829 3.4126267433166504 5.134612083435059
Loss :  1.7346055507659912 3.2902462482452393 5.0248517990112305
Loss :  1.721662998199463 3.1402769088745117 4.861939907073975
Loss :  1.7191500663757324 3.414522886276245 5.133672714233398
Loss :  1.73012113571167 3.764908790588379 5.495029926300049
Loss :  1.7161099910736084 3.459134578704834 5.175244331359863
Loss :  1.7330045700073242 2.885922908782959 4.618927478790283
Loss :  1.7158586978912354 2.69058895111084 4.406447410583496
Loss :  1.7404581308364868 2.6704821586608887 4.410940170288086
Loss :  1.7221399545669556 2.5435450077056885 4.265685081481934
Loss :  1.7152551412582397 2.8103787899017334 4.525633811950684
Loss :  1.710833191871643 2.904601573944092 4.615434646606445
Loss :  1.7259281873703003 2.770474672317505 4.496402740478516
Loss :  1.727949857711792 2.910677194595337 4.638627052307129
Loss :  1.7253444194793701 3.2801153659820557 5.005459785461426
Loss :  1.7148548364639282 2.720757007598877 4.435611724853516
Loss :  1.7237788438796997 3.04195237159729 4.765731334686279
Loss :  1.719282627105713 2.9657375812530518 4.685020446777344
  batch 40 loss: 1.719282627105713, 2.9657375812530518, 4.685020446777344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7210543155670166 2.886115074157715 4.607169151306152
Loss :  1.7208534479141235 2.7812273502349854 4.502080917358398
Loss :  1.7190638780593872 2.6890006065368652 4.408064365386963
Loss :  1.7257345914840698 2.5402982234954834 4.266032695770264
Loss :  1.72648024559021 2.1515910625457764 3.8780713081359863
Loss :  1.7184062004089355 2.374814748764038 4.0932207107543945
Loss :  1.7227182388305664 2.5075807571411133 4.23029899597168
Loss :  1.7186107635498047 2.513840675354004 4.232451438903809
Loss :  1.7219350337982178 2.5366034507751465 4.258538246154785
Loss :  1.718337893486023 2.6719858646392822 4.390323638916016
Loss :  1.7071038484573364 2.7912936210632324 4.498397350311279
Loss :  1.7197589874267578 2.9627773761749268 4.6825361251831055
Loss :  1.7302130460739136 2.875232458114624 4.605445384979248
Loss :  1.7186100482940674 2.800520181655884 4.519130229949951
Loss :  1.7227569818496704 2.495213031768799 4.21796989440918
Loss :  1.718761682510376 2.486560821533203 4.205322265625
Loss :  1.7246407270431519 3.0509238243103027 4.775564670562744
Loss :  1.723278522491455 3.3167967796325684 5.040075302124023
Loss :  1.7272393703460693 2.8533506393432617 4.58059024810791
Loss :  1.7273364067077637 2.7671988010406494 4.494535446166992
  batch 60 loss: 1.7273364067077637, 2.7671988010406494, 4.494535446166992
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7209137678146362 2.6697123050689697 4.390625953674316
Loss :  1.7249361276626587 2.576251745223999 4.301187992095947
Loss :  1.7278236150741577 2.721693515777588 4.449517250061035
Loss :  1.7124292850494385 2.876450300216675 4.588879585266113
Loss :  1.7102599143981934 1.9550726413726807 3.665332555770874
Loss :  1.900427222251892 4.192117691040039 6.092545032501221
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8943148851394653 4.218137264251709 6.112452030181885
Loss :  1.893017053604126 4.088727951049805 5.981744766235352
Loss :  1.8851163387298584 3.9908640384674072 5.875980377197266
Total LOSS train 4.635274648666382 valid 6.015680551528931
CE LOSS train 1.7219364184599657 valid 0.4712790846824646
Contrastive LOSS train 2.913338254048274 valid 0.9977160096168518
EPOCH 85:
Loss :  1.7218408584594727 2.647657632827759 4.369498252868652
Loss :  1.721185326576233 2.752990484237671 4.474175930023193
Loss :  1.7122381925582886 2.7417356967926025 4.453973770141602
Loss :  1.7245094776153564 2.6271157264709473 4.351625442504883
Loss :  1.7262523174285889 2.6859660148620605 4.41221809387207
Loss :  1.7270333766937256 3.0951268672943115 4.822160243988037
Loss :  1.7202881574630737 2.9672508239746094 4.687539100646973
Loss :  1.7205448150634766 3.0743703842163086 4.794915199279785
Loss :  1.7200102806091309 2.5958340167999268 4.315844535827637
Loss :  1.6916061639785767 2.869379758834839 4.560986042022705
Loss :  1.7205183506011963 3.11812162399292 4.838640213012695
Loss :  1.7445299625396729 2.9370107650756836 4.681540489196777
Loss :  1.7175101041793823 2.7548987865448 4.472408771514893
Loss :  1.7163523435592651 2.8348124027252197 4.551164627075195
Loss :  1.728183627128601 2.409817934036255 4.138001441955566
Loss :  1.7058275938034058 2.546701192855835 4.252528667449951
Loss :  1.722118854522705 3.0912013053894043 4.813320159912109
Loss :  1.7130439281463623 3.0765910148620605 4.789634704589844
Loss :  1.7182084321975708 2.425664186477661 4.1438727378845215
Loss :  1.7094014883041382 2.972472906112671 4.6818742752075195
  batch 20 loss: 1.7094014883041382, 2.972472906112671, 4.6818742752075195
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7212457656860352 3.3079380989074707 5.029183864593506
Loss :  1.7234001159667969 3.429623603820801 5.153023719787598
Loss :  1.7206995487213135 3.1794395446777344 4.900138854980469
Loss :  1.731552004814148 3.311101198196411 5.0426530838012695
Loss :  1.7206957340240479 3.7168593406677246 5.437555313110352
Loss :  1.7169897556304932 3.33944034576416 5.056429862976074
Loss :  1.728471279144287 3.3503286838531494 5.078800201416016
Loss :  1.7124640941619873 3.417330741882324 5.129795074462891
Loss :  1.727336049079895 3.381768226623535 5.109104156494141
Loss :  1.7130409479141235 3.475055456161499 5.188096523284912
Loss :  1.7390093803405762 3.748389959335327 5.487399101257324
Loss :  1.716673493385315 3.2047274112701416 4.921401023864746
Loss :  1.7126390933990479 2.7555155754089355 4.4681549072265625
Loss :  1.7076740264892578 3.603902578353882 5.311576843261719
Loss :  1.721155047416687 2.655379056930542 4.3765339851379395
Loss :  1.7248107194900513 2.3445680141448975 4.069378852844238
Loss :  1.7205220460891724 2.335038423538208 4.05556058883667
Loss :  1.7089287042617798 2.2889509201049805 3.9978795051574707
Loss :  1.718723177909851 2.359988212585449 4.07871150970459
Loss :  1.7158845663070679 2.8911173343658447 4.607001781463623
  batch 40 loss: 1.7158845663070679, 2.8911173343658447, 4.607001781463623
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.717326045036316 3.190690755844116 4.908016681671143
Loss :  1.7169551849365234 2.8717141151428223 4.588669300079346
Loss :  1.7148056030273438 2.830972909927368 4.545778274536133
Loss :  1.7230076789855957 2.831953287124634 4.554961204528809
Loss :  1.723358392715454 2.708263397216797 4.431621551513672
Loss :  1.7198538780212402 2.4771475791931152 4.1970014572143555
Loss :  1.7238973379135132 2.3612301349639893 4.085127353668213
Loss :  1.7165743112564087 3.082768440246582 4.799342632293701
Loss :  1.7264211177825928 2.8692712783813477 4.5956926345825195
Loss :  1.7158310413360596 3.814570188522339 5.530401229858398
Loss :  1.7061063051223755 3.024561643600464 4.730668067932129
Loss :  1.7196284532546997 2.513956308364868 4.233584880828857
Loss :  1.73284113407135 2.734041929244995 4.466883182525635
Loss :  1.7152632474899292 2.343424081802368 4.058687210083008
Loss :  1.7172551155090332 2.885632038116455 4.602887153625488
Loss :  1.7219337224960327 3.2876205444335938 5.009554386138916
Loss :  1.7261408567428589 2.761796474456787 4.4879374504089355
Loss :  1.7219032049179077 3.1248419284820557 4.846745014190674
Loss :  1.7262682914733887 3.257514715194702 4.983782768249512
Loss :  1.7293034791946411 3.0817387104034424 4.811042308807373
  batch 60 loss: 1.7293034791946411, 3.0817387104034424, 4.811042308807373
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7242047786712646 3.117356061935425 4.8415608406066895
Loss :  1.724684238433838 3.165653705596924 4.890337944030762
Loss :  1.7305521965026855 3.4538800716400146 5.184432029724121
Loss :  1.712006688117981 3.3135221004486084 5.025528907775879
Loss :  1.7082023620605469 3.924265146255493 5.632467269897461
Loss :  1.8876631259918213 3.7566733360290527 5.644336700439453
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8834089040756226 3.7325940132141113 5.616003036499023
Loss :  1.8807929754257202 3.66070556640625 5.54149866104126
Loss :  1.8745265007019043 3.4119136333465576 5.286439895629883
Total LOSS train 4.694569433652438 valid 5.522069573402405
CE LOSS train 1.7199606748727652 valid 0.4686316251754761
Contrastive LOSS train 2.9746087661156286 valid 0.8529784083366394
EPOCH 86:
Loss :  1.717829704284668 2.6018316745758057 4.3196611404418945
Loss :  1.7206701040267944 2.5848593711853027 4.305529594421387
Loss :  1.709879755973816 2.993661642074585 4.703541278839111
Loss :  1.7185113430023193 3.2161307334899902 4.9346418380737305
Loss :  1.726524829864502 2.2749149799346924 4.001440048217773
Loss :  1.7257548570632935 2.9528720378875732 4.678627014160156
Loss :  1.7221336364746094 3.0851521492004395 4.807285785675049
Loss :  1.721701741218567 2.529923677444458 4.2516255378723145
Loss :  1.7186282873153687 2.4570136070251465 4.175642013549805
Loss :  1.691210389137268 2.2932255268096924 3.98443603515625
Loss :  1.72036612033844 2.8824620246887207 4.602828025817871
Loss :  1.7428069114685059 3.357304573059082 5.100111484527588
Loss :  1.7178733348846436 2.7819924354553223 4.499865531921387
Loss :  1.71686589717865 2.8054652214050293 4.522331237792969
Loss :  1.7223984003067017 2.54714035987854 4.269538879394531
Loss :  1.702544093132019 3.03109073638916 4.733634948730469
Loss :  1.7216438055038452 2.7993876934051514 4.521031379699707
Loss :  1.7116847038269043 2.121598243713379 3.833282947540283
Loss :  1.714487910270691 1.9025540351867676 3.617042064666748
Loss :  1.710706353187561 2.6936888694763184 4.40439510345459
  batch 20 loss: 1.710706353187561, 2.6936888694763184, 4.40439510345459
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7199139595031738 2.5796470642089844 4.299561023712158
Loss :  1.7235231399536133 2.616816282272339 4.340339660644531
Loss :  1.7222639322280884 2.6814088821411133 4.403672695159912
Loss :  1.7313334941864014 2.85653018951416 4.587863922119141
Loss :  1.7212072610855103 2.829298496246338 4.550505638122559
Loss :  1.7203316688537598 2.426828622817993 4.147160530090332
Loss :  1.7325124740600586 2.5889437198638916 4.321455955505371
Loss :  1.7176496982574463 2.4048585891723633 4.1225080490112305
Loss :  1.7323302030563354 3.0334010124206543 4.765731334686279
Loss :  1.7136093378067017 2.955416679382324 4.669025897979736
Loss :  1.7404768466949463 3.4902853965759277 5.230762481689453
Loss :  1.7222157716751099 3.3927865028381348 5.115002155303955
Loss :  1.7131234407424927 2.7193121910095215 4.432435512542725
Loss :  1.7092580795288086 3.403736114501953 5.112994194030762
Loss :  1.7237892150878906 3.1608941555023193 4.884683609008789
Loss :  1.7246224880218506 2.502469062805176 4.2270917892456055
Loss :  1.7247045040130615 2.6749701499938965 4.399674415588379
Loss :  1.71284818649292 2.5181074142456055 4.230955600738525
Loss :  1.7185922861099243 3.049532890319824 4.768125057220459
Loss :  1.7164924144744873 2.8241586685180664 4.540651321411133
  batch 40 loss: 1.7164924144744873, 2.8241586685180664, 4.540651321411133
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.718375563621521 2.6046054363250732 4.322980880737305
Loss :  1.713029146194458 2.7769775390625 4.490006446838379
Loss :  1.7139312028884888 2.858558416366577 4.5724897384643555
Loss :  1.7200260162353516 2.8982458114624023 4.618271827697754
Loss :  1.721521258354187 2.6066126823425293 4.328134059906006
Loss :  1.7171210050582886 3.301172971725464 5.018293857574463
Loss :  1.7200223207473755 3.283764123916626 5.003786563873291
Loss :  1.7128870487213135 4.256882667541504 5.969769477844238
Loss :  1.7233142852783203 3.9467554092407227 5.670069694519043
Loss :  1.7124565839767456 3.7859606742858887 5.498417377471924
Loss :  1.70395028591156 3.12495756149292 4.8289079666137695
Loss :  1.7144838571548462 2.534367084503174 4.2488508224487305
Loss :  1.7284870147705078 2.8969085216522217 4.625395774841309
Loss :  1.715011715888977 3.418792724609375 5.1338043212890625
Loss :  1.7167812585830688 3.140840768814087 4.857622146606445
Loss :  1.717903733253479 2.163095235824585 3.8809990882873535
Loss :  1.7240235805511475 3.634838581085205 5.358861923217773
Loss :  1.7206063270568848 3.184100389480591 4.904706954956055
Loss :  1.724299669265747 3.4610748291015625 5.1853742599487305
Loss :  1.72695791721344 2.940159320831299 4.667117118835449
  batch 60 loss: 1.72695791721344, 2.940159320831299, 4.667117118835449
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7239707708358765 3.2077207565307617 4.931691646575928
Loss :  1.7232189178466797 2.937891960144043 4.661110877990723
Loss :  1.7312861680984497 2.7298595905303955 4.461145877838135
Loss :  1.7144365310668945 3.147569417953491 4.862006187438965
Loss :  1.7098631858825684 3.111219882965088 4.821083068847656
Loss :  1.871518850326538 4.258041858673096 6.129560470581055
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8703221082687378 4.252969264984131 6.123291492462158
Loss :  1.8650990724563599 4.133755683898926 5.998854637145996
Loss :  1.8642441034317017 4.071971416473389 5.936215400695801
Total LOSS train 4.620578318375808 valid 6.046980500221252
CE LOSS train 1.719430552996122 valid 0.4660610258579254
Contrastive LOSS train 2.901147754375751 valid 1.0179928541183472
EPOCH 87:
Loss :  1.7159456014633179 2.8159031867980957 4.531848907470703
Loss :  1.7226923704147339 2.8857581615448 4.608450412750244
Loss :  1.710268259048462 2.593789577484131 4.304058074951172
Loss :  1.7170161008834839 2.7521824836730957 4.469198703765869
Loss :  1.7242838144302368 2.9001500606536865 4.624433994293213
Loss :  1.723166584968567 2.830872058868408 4.5540385246276855
Loss :  1.7196470499038696 3.006279945373535 4.725926876068115
Loss :  1.7190557718276978 2.4851033687591553 4.204159259796143
Loss :  1.717589020729065 2.367794990539551 4.085383892059326
Loss :  1.690603256225586 2.1049294471740723 3.795532703399658
Loss :  1.719226360321045 2.6817245483398438 4.400950908660889
Loss :  1.7407751083374023 3.273698329925537 5.0144734382629395
Loss :  1.7173889875411987 2.985504150390625 4.702893257141113
Loss :  1.716318964958191 2.989380121231079 4.7056989669799805
Loss :  1.7215995788574219 2.7280702590942383 4.44966983795166
Loss :  1.7004514932632446 2.7610955238342285 4.461546897888184
Loss :  1.7194997072219849 2.779205322265625 4.49870491027832
Loss :  1.7110213041305542 3.412118434906006 5.12313985824585
Loss :  1.7116831541061401 2.9795453548431396 4.69122838973999
Loss :  1.705506682395935 3.8436813354492188 5.549188137054443
  batch 20 loss: 1.705506682395935, 3.8436813354492188, 5.549188137054443
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7178235054016113 2.9679408073425293 4.685764312744141
Loss :  1.720446228981018 3.100231409072876 4.820677757263184
Loss :  1.7185614109039307 2.9547743797302246 4.673336029052734
Loss :  1.7273653745651245 2.74306583404541 4.470431327819824
Loss :  1.717310905456543 2.902158260345459 4.619469165802002
Loss :  1.7139208316802979 2.356484889984131 4.070405960083008
Loss :  1.725582242012024 2.6555821895599365 4.38116455078125
Loss :  1.710841178894043 2.3455464839935303 4.056387901306152
Loss :  1.7260929346084595 2.1254794597625732 3.8515725135803223
Loss :  1.710580825805664 2.3583056926727295 4.068886756896973
Loss :  1.7385966777801514 2.697021961212158 4.4356184005737305
Loss :  1.7193876504898071 2.422266721725464 4.1416544914245605
Loss :  1.7121673822402954 2.3614237308502197 4.073591232299805
Loss :  1.7108765840530396 2.6183416843414307 4.32921838760376
Loss :  1.7251992225646973 2.8338088989257812 4.5590081214904785
Loss :  1.7273104190826416 2.566998243331909 4.294308662414551
Loss :  1.7220957279205322 2.6889424324035645 4.411038398742676
Loss :  1.7110620737075806 2.6316957473754883 4.342757701873779
Loss :  1.717355728149414 2.9323713779449463 4.649726867675781
Loss :  1.7157062292099 2.1957011222839355 3.911407470703125
  batch 40 loss: 1.7157062292099, 2.1957011222839355, 3.911407470703125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.716736078262329 2.7710962295532227 4.487832069396973
Loss :  1.7140939235687256 2.98185396194458 4.695947647094727
Loss :  1.7160594463348389 3.3130383491516113 5.029097557067871
Loss :  1.7198854684829712 4.1667890548706055 5.886674404144287
Loss :  1.7208247184753418 3.172304630279541 4.893129348754883
Loss :  1.7178095579147339 3.547178030014038 5.264987468719482
Loss :  1.717982292175293 3.0544488430023193 4.772431373596191
Loss :  1.7148367166519165 3.23030161857605 4.945138454437256
Loss :  1.7227610349655151 3.1208994388580322 4.843660354614258
Loss :  1.7140498161315918 3.4232239723205566 5.137273788452148
Loss :  1.7032629251480103 3.2530741691589355 4.956336975097656
Loss :  1.7150291204452515 2.9416158199310303 4.656644821166992
Loss :  1.7309919595718384 3.0467584133148193 4.777750492095947
Loss :  1.7123775482177734 3.275986909866333 4.988364219665527
Loss :  1.714543342590332 2.7130963802337646 4.427639961242676
Loss :  1.7169042825698853 2.6724984645843506 4.389402866363525
Loss :  1.7238507270812988 3.2957983016967773 5.019649028778076
Loss :  1.7202891111373901 2.594601631164551 4.3148908615112305
Loss :  1.7257014513015747 2.7148373126983643 4.4405388832092285
Loss :  1.7239097356796265 2.396613836288452 4.120523452758789
  batch 60 loss: 1.7239097356796265, 2.396613836288452, 4.120523452758789
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7229118347167969 2.2761950492858887 3.9991068840026855
Loss :  1.7234303951263428 2.826958417892456 4.550388813018799
Loss :  1.729157567024231 2.328282356262207 4.057439804077148
Loss :  1.7156269550323486 2.342778444290161 4.05840539932251
Loss :  1.711342692375183 1.7568130493164062 3.468155860900879
Loss :  1.858763575553894 4.098567008972168 5.957330703735352
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8598014116287231 4.027512073516846 5.887313365936279
Loss :  1.8519508838653564 3.856766939163208 5.7087178230285645
Loss :  1.8540899753570557 3.785234212875366 5.639324188232422
Total LOSS train 4.531205580784724 valid 5.798171520233154
CE LOSS train 1.7180983231617855 valid 0.4635224938392639
Contrastive LOSS train 2.8131072411170375 valid 0.9463085532188416
EPOCH 88:
Loss :  1.7181899547576904 2.719343900680542 4.437533855438232
Loss :  1.7227305173873901 2.482907772064209 4.205638408660889
Loss :  1.714998722076416 3.1109063625335693 4.825904846191406
Loss :  1.718427062034607 2.372795343399048 4.091222286224365
Loss :  1.725569486618042 2.595280170440674 4.320849418640137
Loss :  1.7243375778198242 2.838871479034424 4.563209056854248
Loss :  1.7197952270507812 3.3451929092407227 5.064988136291504
Loss :  1.7192095518112183 2.735698938369751 4.45490837097168
Loss :  1.719117522239685 2.4603352546691895 4.179452896118164
Loss :  1.6929385662078857 2.4742891788482666 4.167227745056152
Loss :  1.7216084003448486 2.61923885345459 4.340847015380859
Loss :  1.741910457611084 2.522461414337158 4.264371871948242
Loss :  1.719138741493225 2.5579190254211426 4.277057647705078
Loss :  1.7179620265960693 2.9205756187438965 4.638537406921387
Loss :  1.7218637466430664 2.8316495418548584 4.553513526916504
Loss :  1.7047494649887085 3.0051543712615967 4.709903717041016
Loss :  1.7227787971496582 2.981156587600708 4.703935623168945
Loss :  1.714573860168457 2.8425419330596924 4.55711555480957
Loss :  1.7153923511505127 2.709036111831665 4.424428462982178
Loss :  1.709749698638916 2.3694028854370117 4.079152584075928
  batch 20 loss: 1.709749698638916, 2.3694028854370117, 4.079152584075928
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7176920175552368 2.3896055221557617 4.107297420501709
Loss :  1.7221213579177856 2.1190953254699707 3.841216564178467
Loss :  1.7194181680679321 1.8036131858825684 3.523031234741211
Loss :  1.7283495664596558 2.0231637954711914 3.7515134811401367
Loss :  1.7178895473480225 3.1739306449890137 4.891819953918457
Loss :  1.7156352996826172 2.624885320663452 4.340520858764648
Loss :  1.7287406921386719 3.238065242767334 4.966805934906006
Loss :  1.7142013311386108 2.8495371341705322 4.5637383460998535
Loss :  1.7292996644973755 2.5568974018096924 4.286197185516357
Loss :  1.7091560363769531 2.956609010696411 4.665764808654785
Loss :  1.7367268800735474 2.3925554752349854 4.129282474517822
Loss :  1.719962477684021 2.6824073791503906 4.402369976043701
Loss :  1.7102673053741455 2.781667947769165 4.4919352531433105
Loss :  1.7071813344955444 2.557659149169922 4.264840602874756
Loss :  1.7219209671020508 3.3240725994110107 5.045993804931641
Loss :  1.7241023778915405 2.6586687564849854 4.382771015167236
Loss :  1.7193468809127808 2.5500173568725586 4.269364356994629
Loss :  1.7091519832611084 3.017695426940918 4.7268476486206055
Loss :  1.7154349088668823 3.3947455883026123 5.110180377960205
Loss :  1.7145949602127075 2.8109824657440186 4.525577545166016
  batch 40 loss: 1.7145949602127075, 2.8109824657440186, 4.525577545166016
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7137411832809448 3.052893877029419 4.766634941101074
Loss :  1.7088590860366821 3.3516805171966553 5.060539722442627
Loss :  1.7113178968429565 2.6815121173858643 4.392829895019531
Loss :  1.7142269611358643 2.4265635013580322 4.1407904624938965
Loss :  1.7161929607391357 2.163362979888916 3.8795559406280518
Loss :  1.7142627239227295 2.285649538040161 3.9999122619628906
Loss :  1.7156850099563599 2.5902140140533447 4.305899143218994
Loss :  1.711809515953064 2.323909044265747 4.0357184410095215
Loss :  1.7205756902694702 2.6772494316101074 4.397825241088867
Loss :  1.7103497982025146 2.246864080429077 3.957213878631592
Loss :  1.701619029045105 3.1865930557250977 4.888212203979492
Loss :  1.7124927043914795 2.5679967403411865 4.280489444732666
Loss :  1.7285335063934326 2.8596999645233154 4.588233470916748
Loss :  1.710452914237976 2.753079414367676 4.463532447814941
Loss :  1.7141417264938354 2.4921650886535645 4.2063069343566895
Loss :  1.7147270441055298 2.831561803817749 4.546288967132568
Loss :  1.7236992120742798 2.958954334259033 4.682653427124023
Loss :  1.720393180847168 2.7885429859161377 4.508935928344727
Loss :  1.7255934476852417 2.821444511413574 4.5470380783081055
Loss :  1.7232258319854736 2.499267339706421 4.2224931716918945
  batch 60 loss: 1.7232258319854736, 2.499267339706421, 4.2224931716918945
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7211848497390747 2.562546491622925 4.283731460571289
Loss :  1.7222509384155273 2.356307029724121 4.078557968139648
Loss :  1.7271232604980469 2.1655449867248535 3.8926682472229004
Loss :  1.7141952514648438 3.28446626663208 4.998661518096924
Loss :  1.709833025932312 1.91895592212677 3.628788948059082
Loss :  1.880183458328247 3.9861416816711426 5.866325378417969
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8753656148910522 3.993190050125122 5.868555545806885
Loss :  1.874330997467041 3.8900904655456543 5.764421463012695
Loss :  1.871488332748413 3.8619842529296875 5.73347282409668
Total LOSS train 4.398467375681951 valid 5.808193802833557
CE LOSS train 1.7177341882999126 valid 0.46787208318710327
Contrastive LOSS train 2.680733191050016 valid 0.9654960632324219
EPOCH 89:
Loss :  1.7138800621032715 3.210705518722534 4.924585342407227
Loss :  1.7188783884048462 3.395977735519409 5.114856243133545
Loss :  1.7088360786437988 2.763638734817505 4.472475051879883
Loss :  1.7135761976242065 3.131525993347168 4.845102310180664
Loss :  1.7236316204071045 3.2444279193878174 4.968059539794922
Loss :  1.7201642990112305 3.001356363296509 4.72152042388916
Loss :  1.7175567150115967 3.060776710510254 4.77833366394043
Loss :  1.7153915166854858 2.274082660675049 3.989474296569824
Loss :  1.7161533832550049 2.256403684616089 3.9725570678710938
Loss :  1.6909301280975342 2.7462573051452637 4.437187194824219
Loss :  1.7194191217422485 2.6052582263946533 4.324677467346191
Loss :  1.7404885292053223 2.5846917629241943 4.3251800537109375
Loss :  1.7179921865463257 2.7373578548431396 4.455349922180176
Loss :  1.7150344848632812 2.943640947341919 4.658675193786621
Loss :  1.7202012538909912 2.090942144393921 3.811143398284912
Loss :  1.7049747705459595 2.775540351867676 4.480515003204346
Loss :  1.7190028429031372 2.5529587268829346 4.271961688995361
Loss :  1.712316632270813 2.5360424518585205 4.248359203338623
Loss :  1.7132914066314697 2.632540702819824 4.345831871032715
Loss :  1.7047889232635498 2.62105131149292 4.325839996337891
  batch 20 loss: 1.7047889232635498, 2.62105131149292, 4.325839996337891
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.715575098991394 2.971775531768799 4.687350749969482
Loss :  1.7192614078521729 3.5794548988342285 5.2987165451049805
Loss :  1.717301845550537 2.628067970275879 4.345369815826416
Loss :  1.7259690761566162 2.2483983039855957 3.974367380142212
Loss :  1.7172365188598633 2.407209634780884 4.124445915222168
Loss :  1.715248703956604 3.1404919624328613 4.855740547180176
Loss :  1.7287356853485107 3.4019882678985596 5.13072395324707
Loss :  1.7135223150253296 3.28564715385437 4.99916934967041
Loss :  1.7269632816314697 3.270775318145752 4.997738838195801
Loss :  1.7082834243774414 2.8926382064819336 4.600921630859375
Loss :  1.7376505136489868 2.535330295562744 4.272980690002441
Loss :  1.7196468114852905 2.108621120452881 3.828268051147461
Loss :  1.7095016241073608 1.9097905158996582 3.6192922592163086
Loss :  1.7081140279769897 2.142584800720215 3.850698947906494
Loss :  1.7215049266815186 2.581501007080078 4.303006172180176
Loss :  1.7241017818450928 2.5176260471343994 4.241727828979492
Loss :  1.7191269397735596 2.493950843811035 4.213077545166016
Loss :  1.7086782455444336 2.9104597568511963 4.619137763977051
Loss :  1.715922236442566 3.214620351791382 4.930542469024658
Loss :  1.7152950763702393 2.802582025527954 4.517877101898193
  batch 40 loss: 1.7152950763702393, 2.802582025527954, 4.517877101898193
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.714497447013855 3.004002809524536 4.718500137329102
Loss :  1.7118866443634033 3.851405620574951 5.563292503356934
Loss :  1.7134790420532227 3.0653371810913086 4.778816223144531
Loss :  1.7163522243499756 2.6891653537750244 4.405517578125
Loss :  1.7179362773895264 2.0143189430236816 3.732255220413208
Loss :  1.7151139974594116 1.9785759449005127 3.6936898231506348
Loss :  1.7158461809158325 3.415956974029541 5.131803035736084
Loss :  1.7113529443740845 3.4538185596466064 5.1651716232299805
Loss :  1.7210785150527954 2.9453864097595215 4.666464805603027
Loss :  1.7098946571350098 3.693141460418701 5.403036117553711
Loss :  1.6988157033920288 3.938443183898926 5.637259006500244
Loss :  1.714391827583313 4.068456649780273 5.782848358154297
Loss :  1.7296606302261353 3.4379773139953613 5.167637825012207
Loss :  1.7097392082214355 3.817944049835205 5.527683258056641
Loss :  1.7115195989608765 2.3760159015655518 4.087535381317139
Loss :  1.7159394025802612 2.5100321769714355 4.225971698760986
Loss :  1.7223987579345703 2.2744550704956055 3.996853828430176
Loss :  1.7171906232833862 2.05902099609375 3.776211738586426
Loss :  1.7237015962600708 2.549433469772339 4.273135185241699
Loss :  1.723454475402832 2.24897837638855 3.972432851791382
  batch 60 loss: 1.723454475402832, 2.24897837638855, 3.972432851791382
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7210807800292969 2.3798208236694336 4.1009016036987305
Loss :  1.7218867540359497 2.4820873737335205 4.20397424697876
Loss :  1.728149652481079 2.2542474269866943 3.9823970794677734
Loss :  1.7124122381210327 2.263437509536743 3.9758496284484863
Loss :  1.707905650138855 1.568495512008667 3.2764010429382324
Loss :  1.8864567279815674 4.1670637130737305 6.053520202636719
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8824234008789062 4.241530895233154 6.1239542961120605
Loss :  1.8800705671310425 4.09452486038208 5.974595546722412
Loss :  1.8752084970474243 4.096917629241943 5.972126007080078
Total LOSS train 4.494284281363854 valid 6.031049013137817
CE LOSS train 1.7166128140229446 valid 0.4688021242618561
Contrastive LOSS train 2.777671480178833 valid 1.0242294073104858
EPOCH 90:
Loss :  1.7142434120178223 2.286512851715088 4.00075626373291
Loss :  1.7203673124313354 2.3882763385772705 4.108643531799316
Loss :  1.7092229127883911 2.510399580001831 4.219622611999512
Loss :  1.7149531841278076 2.6886403560638428 4.40359354019165
Loss :  1.725235104560852 3.101938486099243 4.827173709869385
Loss :  1.721199870109558 2.3628246784210205 4.084024429321289
Loss :  1.7190572023391724 3.148505449295044 4.867562770843506
Loss :  1.7170734405517578 2.932520627975464 4.649594306945801
Loss :  1.7162476778030396 2.5455803871154785 4.2618279457092285
Loss :  1.68986177444458 2.610445261001587 4.300307273864746
Loss :  1.7197537422180176 3.0459322929382324 4.76568603515625
Loss :  1.7404756546020508 3.3181886672973633 5.058664321899414
Loss :  1.7167295217514038 2.701355218887329 4.418084621429443
Loss :  1.7165262699127197 3.2190778255462646 4.935604095458984
Loss :  1.7210613489151 3.356569766998291 5.077630996704102
Loss :  1.7033228874206543 2.6027896404266357 4.306112289428711
Loss :  1.7198271751403809 2.8206472396850586 4.5404744148254395
Loss :  1.7122550010681152 2.5788373947143555 4.291092395782471
Loss :  1.713185429573059 3.373911142349243 5.087096691131592
Loss :  1.704063057899475 2.569545269012451 4.273608207702637
  batch 20 loss: 1.704063057899475, 2.569545269012451, 4.273608207702637
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7149053812026978 2.3784775733947754 4.093382835388184
Loss :  1.7192919254302979 2.709357261657715 4.428648948669434
Loss :  1.716264247894287 2.182307004928589 3.898571252822876
Loss :  1.7267378568649292 2.4078402519226074 4.134578227996826
Loss :  1.715429663658142 2.2957448959350586 4.01117467880249
Loss :  1.7117687463760376 2.1874401569366455 3.8992090225219727
Loss :  1.726340651512146 2.342893123626709 4.0692338943481445
Loss :  1.711885690689087 2.1695926189422607 3.8814783096313477
Loss :  1.7277889251708984 2.101689100265503 3.8294780254364014
Loss :  1.7071471214294434 2.465744733810425 4.172891616821289
Loss :  1.7355375289916992 2.7256040573120117 4.461141586303711
Loss :  1.7196407318115234 2.7324471473693848 4.452087879180908
Loss :  1.7108770608901978 2.6940178871154785 4.404894828796387
Loss :  1.707143783569336 3.5375454425811768 5.244688987731934
Loss :  1.7224136590957642 2.7205898761749268 4.4430036544799805
Loss :  1.7240777015686035 2.861562967300415 4.585640907287598
Loss :  1.7194510698318481 2.885608196258545 4.6050591468811035
Loss :  1.7090072631835938 2.5772252082824707 4.2862324714660645
Loss :  1.7163442373275757 2.849605083465576 4.565949440002441
Loss :  1.7143453359603882 3.7417898178100586 5.456135272979736
  batch 40 loss: 1.7143453359603882, 3.7417898178100586, 5.456135272979736
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.714147925376892 3.7894463539123535 5.503594398498535
Loss :  1.7125879526138306 3.167180061340332 4.879767894744873
Loss :  1.7153629064559937 3.684030055999756 5.399393081665039
Loss :  1.7167540788650513 2.1908485889434814 3.9076027870178223
Loss :  1.7192652225494385 2.22653865814209 3.9458038806915283
Loss :  1.7145347595214844 2.5055084228515625 4.220043182373047
Loss :  1.7134485244750977 3.0908305644989014 4.804279327392578
Loss :  1.713724970817566 2.8058226108551025 4.519547462463379
Loss :  1.7155972719192505 3.1522483825683594 4.86784553527832
Loss :  1.7132103443145752 3.0284197330474854 4.7416300773620605
Loss :  1.7019174098968506 3.1095008850097656 4.811418533325195
Loss :  1.7118313312530518 3.359225034713745 5.071056365966797
Loss :  1.7259804010391235 3.9102835655212402 5.636263847351074
Loss :  1.7123844623565674 2.3675954341888428 4.07997989654541
Loss :  1.7146704196929932 2.287344217300415 4.002014636993408
Loss :  1.7118382453918457 2.6893608570098877 4.4011993408203125
Loss :  1.7213255167007446 2.536107063293457 4.257432460784912
Loss :  1.718170404434204 2.5735418796539307 4.291712284088135
Loss :  1.723242163658142 2.8199682235717773 4.543210506439209
Loss :  1.720840334892273 2.727489948272705 4.448330402374268
  batch 60 loss: 1.720840334892273, 2.727489948272705, 4.448330402374268
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7181802988052368 2.638947010040283 4.3571271896362305
Loss :  1.720278024673462 2.6942803859710693 4.414558410644531
Loss :  1.7250458002090454 2.424985647201538 4.150031566619873
Loss :  1.7109452486038208 2.3634567260742188 4.07440185546875
Loss :  1.707747459411621 1.9640977382659912 3.6718451976776123
Loss :  1.8840274810791016 4.151882648468018 6.035910129547119
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8801223039627075 4.011354923248291 5.891477108001709
Loss :  1.8771616220474243 3.961528778076172 5.838690280914307
Loss :  1.8772833347320557 3.7505412101745605 5.627824783325195
Total LOSS train 4.467703562516433 valid 5.8484755754470825
CE LOSS train 1.7163706467701838 valid 0.4693208336830139
Contrastive LOSS train 2.7513329065763035 valid 0.9376353025436401
EPOCH 91:
Loss :  1.714206337928772 2.9837286472320557 4.697935104370117
Loss :  1.7183469533920288 3.95686674118042 5.675213813781738
Loss :  1.7087715864181519 4.126485347747803 5.835257053375244
Loss :  1.71403169631958 3.2629661560058594 4.9769978523254395
Loss :  1.723778247833252 3.2780685424804688 5.001846790313721
Loss :  1.720459222793579 4.2483086585998535 5.968768119812012
Loss :  1.716650366783142 3.690493106842041 5.407143592834473
Loss :  1.7145048379898071 3.308002233505249 5.022507190704346
Loss :  1.7152315378189087 3.0226686000823975 4.737900257110596
Loss :  1.688511610031128 2.747584581375122 4.43609619140625
Loss :  1.7167565822601318 3.6787424087524414 5.395499229431152
Loss :  1.7403582334518433 2.842506170272827 4.582864284515381
Loss :  1.7157330513000488 2.3927114009857178 4.1084442138671875
Loss :  1.7133996486663818 2.320988655090332 4.034388542175293
Loss :  1.718930959701538 2.385683536529541 4.1046142578125
Loss :  1.7035514116287231 2.305431842803955 4.008983135223389
Loss :  1.718152642250061 2.4850029945373535 4.203155517578125
Loss :  1.712254285812378 2.2491559982299805 3.9614102840423584
Loss :  1.7127500772476196 2.5313687324523926 4.244118690490723
Loss :  1.7040598392486572 2.5413408279418945 4.245400428771973
  batch 20 loss: 1.7040598392486572, 2.5413408279418945, 4.245400428771973
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7152239084243774 2.311825752258301 4.027049541473389
Loss :  1.718462347984314 2.8675334453582764 4.585995674133301
Loss :  1.7169650793075562 3.3363707065582275 5.053335666656494
Loss :  1.7252601385116577 2.743525266647339 4.468785285949707
Loss :  1.7137491703033447 3.176851749420166 4.89060115814209
Loss :  1.713057279586792 3.0547499656677246 4.7678070068359375
Loss :  1.7275718450546265 2.757131338119507 4.484703063964844
Loss :  1.712833046913147 2.88165020942688 4.594483375549316
Loss :  1.7283382415771484 3.3713796138763428 5.09971809387207
Loss :  1.7039332389831543 2.6605641841888428 4.364497184753418
Loss :  1.734223484992981 3.04742169380188 4.78164529800415
Loss :  1.7196357250213623 2.551966905593872 4.271602630615234
Loss :  1.709367036819458 2.1692211627960205 3.8785881996154785
Loss :  1.7056493759155273 2.1360650062561035 3.841714382171631
Loss :  1.7202227115631104 2.6450753211975098 4.365298271179199
Loss :  1.7231709957122803 2.5456783771514893 4.2688493728637695
Loss :  1.718551516532898 2.5127451419830322 4.231296539306641
Loss :  1.712699055671692 2.759568452835083 4.4722676277160645
Loss :  1.7152694463729858 2.6100223064422607 4.325291633605957
Loss :  1.718058705329895 2.915187120437622 4.633245944976807
  batch 40 loss: 1.718058705329895, 2.915187120437622, 4.633245944976807
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7155715227127075 2.3710126876831055 4.086584091186523
Loss :  1.7088176012039185 2.422245740890503 4.131063461303711
Loss :  1.7116916179656982 2.3454136848449707 4.05710506439209
Loss :  1.7155669927597046 2.0168943405151367 3.732461452484131
Loss :  1.7176334857940674 2.1426854133605957 3.860318899154663
Loss :  1.7149378061294556 2.1075847148895264 3.8225226402282715
Loss :  1.715731143951416 2.044173240661621 3.759904384613037
Loss :  1.7115142345428467 2.8230106830596924 4.534524917602539
Loss :  1.720234751701355 2.496964454650879 4.217199325561523
Loss :  1.7111246585845947 2.8597893714904785 4.570914268493652
Loss :  1.7006620168685913 4.047097682952881 5.747759819030762
Loss :  1.7135313749313354 3.462728977203369 5.176260471343994
Loss :  1.7280375957489014 3.321308135986328 5.049345970153809
Loss :  1.711635708808899 2.7234315872192383 4.435067176818848
Loss :  1.7113131284713745 3.415062427520752 5.126375675201416
Loss :  1.7144227027893066 3.6137287616729736 5.328151702880859
Loss :  1.7215583324432373 3.7315707206726074 5.453128814697266
Loss :  1.716059923171997 3.128574848175049 4.844635009765625
Loss :  1.7224724292755127 3.452910900115967 5.175383567810059
Loss :  1.7214053869247437 2.7120842933654785 4.433489799499512
  batch 60 loss: 1.7214053869247437, 2.7120842933654785, 4.433489799499512
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7187371253967285 2.235125780105591 3.9538629055023193
Loss :  1.7190029621124268 2.0766656398773193 3.795668601989746
Loss :  1.7266687154769897 2.64225435256958 4.368923187255859
Loss :  1.7105530500411987 3.3858182430267334 5.096371173858643
Loss :  1.704878807067871 3.170477867126465 4.875356674194336
Loss :  1.8815823793411255 3.836256742477417 5.717839241027832
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8775179386138916 3.909327268600464 5.7868452072143555
Loss :  1.8744522333145142 3.7860958576202393 5.660548210144043
Loss :  1.873896598815918 3.7121636867523193 5.586060523986816
Total LOSS train 4.579810762405396 valid 5.687823295593262
CE LOSS train 1.7157914546819835 valid 0.4684741497039795
Contrastive LOSS train 2.864019283881554 valid 0.9280409216880798
EPOCH 92:
Loss :  1.7120980024337769 2.644714593887329 4.356812477111816
Loss :  1.7179968357086182 2.2269928455352783 3.9449896812438965
Loss :  1.7049827575683594 1.8445589542388916 3.549541711807251
Loss :  1.7098360061645508 2.238882541656494 3.948718547821045
Loss :  1.721605658531189 2.656982898712158 4.378588676452637
Loss :  1.7181763648986816 2.6112658977508545 4.329442024230957
Loss :  1.716225028038025 2.172734498977661 3.8889594078063965
Loss :  1.7145183086395264 2.630028247833252 4.344546318054199
Loss :  1.7136926651000977 2.5970957279205322 4.310788154602051
Loss :  1.688611626625061 2.830976724624634 4.519588470458984
Loss :  1.7183609008789062 2.8899788856506348 4.608339786529541
Loss :  1.7380207777023315 2.7303810119628906 4.468401908874512
Loss :  1.7162866592407227 2.33754825592041 4.053834915161133
Loss :  1.714068055152893 2.2328104972839355 3.946878433227539
Loss :  1.7179986238479614 2.2118070125579834 3.9298057556152344
Loss :  1.7012959718704224 3.3784689903259277 5.0797648429870605
Loss :  1.7187570333480835 2.9674363136291504 4.686193466186523
Loss :  1.713731288909912 3.0794408321380615 4.7931718826293945
Loss :  1.7098134756088257 2.9149856567382812 4.6247992515563965
Loss :  1.7057856321334839 2.9623501300811768 4.668135643005371
  batch 20 loss: 1.7057856321334839, 2.9623501300811768, 4.668135643005371
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7161201238632202 2.7010838985443115 4.417203903198242
Loss :  1.7205207347869873 2.540656566619873 4.261177062988281
Loss :  1.7193846702575684 3.0447652339935303 4.7641496658325195
Loss :  1.7285370826721191 2.661586046218872 4.39012336730957
Loss :  1.7129520177841187 2.4597601890563965 4.172712326049805
Loss :  1.71368408203125 2.1811301708221436 3.8948142528533936
Loss :  1.7295243740081787 2.479142189025879 4.208666801452637
Loss :  1.7158610820770264 2.2402775287628174 3.9561386108398438
Loss :  1.7299507856369019 2.2434866428375244 3.9734373092651367
Loss :  1.7047547101974487 2.5960638523101807 4.30081844329834
Loss :  1.7332143783569336 2.698477268218994 4.431691646575928
Loss :  1.7211312055587769 2.736621379852295 4.457752704620361
Loss :  1.7110745906829834 2.906080484390259 4.617155075073242
Loss :  1.7063860893249512 2.656205654144287 4.362591743469238
Loss :  1.720491886138916 3.3687024116516113 5.089194297790527
Loss :  1.7231370210647583 3.1756889820098877 4.8988261222839355
Loss :  1.718371868133545 2.7010419368743896 4.4194135665893555
Loss :  1.714098572731018 2.5057647228240967 4.219863414764404
Loss :  1.7154507637023926 2.2548513412475586 3.970302104949951
Loss :  1.7168437242507935 1.8418320417404175 3.558675765991211
  batch 40 loss: 1.7168437242507935, 1.8418320417404175, 3.558675765991211
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.715319275856018 2.013759136199951 3.7290782928466797
Loss :  1.707007884979248 1.9710781574249268 3.678086042404175
Loss :  1.7100040912628174 2.5863497257232666 4.296353816986084
Loss :  1.7147959470748901 2.471339464187622 4.186135292053223
Loss :  1.7164192199707031 2.342106580734253 4.058526039123535
Loss :  1.7151010036468506 2.583750009536743 4.298851013183594
Loss :  1.7179332971572876 2.294834613800049 4.012767791748047
Loss :  1.7094123363494873 3.3996658325195312 5.109078407287598
Loss :  1.7222199440002441 2.912447690963745 4.63466739654541
Loss :  1.7073496580123901 2.229489803314209 3.9368395805358887
Loss :  1.6985669136047363 2.442898988723755 4.14146614074707
Loss :  1.7106819152832031 2.796373128890991 4.507055282592773
Loss :  1.72689950466156 2.6463100910186768 4.373209476470947
Loss :  1.7059582471847534 2.4239957332611084 4.129953861236572
Loss :  1.7074248790740967 2.9104740619659424 4.617898941040039
Loss :  1.7140601873397827 3.5927937030792236 5.306853771209717
Loss :  1.72031569480896 3.3369364738464355 5.057251930236816
Loss :  1.7146002054214478 2.581780195236206 4.296380519866943
Loss :  1.7203563451766968 2.834927558898926 4.555284023284912
Loss :  1.7211798429489136 2.6814541816711426 4.402634143829346
  batch 60 loss: 1.7211798429489136, 2.6814541816711426, 4.402634143829346
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7192014455795288 3.0434763431549072 4.7626776695251465
Loss :  1.7180284261703491 2.7854082584381104 4.50343656539917
Loss :  1.7279539108276367 3.2782480716705322 5.00620174407959
Loss :  1.7083660364151 2.8284730911254883 4.536839008331299
Loss :  1.7029536962509155 1.872019648551941 3.5749733448028564
Loss :  1.890916347503662 3.9687657356262207 5.859682083129883
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8854326009750366 3.896343469619751 5.781775951385498
Loss :  1.8834142684936523 3.895338296890259 5.778752326965332
Loss :  1.8818426132202148 3.668170213699341 5.550012588500977
Total LOSS train 4.34628476362962 valid 5.742555737495422
CE LOSS train 1.7153147899187529 valid 0.4704606533050537
Contrastive LOSS train 2.630969993884747 valid 0.9170425534248352
EPOCH 93:
Loss :  1.7111196517944336 2.223456621170044 3.9345762729644775
Loss :  1.7192838191986084 2.735384941101074 4.454668998718262
Loss :  1.7054603099822998 2.8668317794799805 4.572292327880859
Loss :  1.709529995918274 2.789185047149658 4.498714923858643
Loss :  1.7218022346496582 2.89746356010437 4.619265556335449
Loss :  1.7189244031906128 2.958409309387207 4.677333831787109
Loss :  1.7167420387268066 2.712644338607788 4.429386138916016
Loss :  1.714890718460083 2.731942653656006 4.446833610534668
Loss :  1.71340012550354 2.4423797130584717 4.155779838562012
Loss :  1.6876744031906128 2.3609983921051025 4.048672676086426
Loss :  1.7177069187164307 3.200435161590576 4.918142318725586
Loss :  1.7374597787857056 3.1632511615753174 4.9007110595703125
Loss :  1.7155929803848267 4.111998081207275 5.8275909423828125
Loss :  1.713120460510254 2.5001227855682373 4.21324348449707
Loss :  1.7173993587493896 2.1224915981292725 3.839890956878662
Loss :  1.699324369430542 1.920272707939148 3.6195969581604004
Loss :  1.7163370847702026 2.315289258956909 4.031626224517822
Loss :  1.710963249206543 2.4466397762298584 4.1576032638549805
Loss :  1.7099324464797974 2.688988208770752 4.39892053604126
Loss :  1.7014988660812378 3.116365909576416 4.817864894866943
  batch 20 loss: 1.7014988660812378, 3.116365909576416, 4.817864894866943
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.71176278591156 3.160748243331909 4.87251091003418
Loss :  1.716477394104004 2.351991891860962 4.068469047546387
Loss :  1.715062141418457 2.584455728530884 4.299517631530762
Loss :  1.7247588634490967 3.114197015762329 4.838955879211426
Loss :  1.714091420173645 3.083725929260254 4.797817230224609
Loss :  1.7098933458328247 2.7925198078155518 4.502413272857666
Loss :  1.7247453927993774 3.2499067783355713 4.974652290344238
Loss :  1.7091196775436401 2.7350528240203857 4.444172382354736
Loss :  1.7239044904708862 2.398364782333374 4.122269153594971
Loss :  1.7026994228363037 3.2143547534942627 4.917054176330566
Loss :  1.7323555946350098 2.887951612472534 4.620306968688965
Loss :  1.71680748462677 2.771522045135498 4.4883294105529785
Loss :  1.7063040733337402 2.763580799102783 4.469884872436523
Loss :  1.7037899494171143 3.6483359336853027 5.352126121520996
Loss :  1.718043565750122 3.1735923290252686 4.891635894775391
Loss :  1.7199501991271973 3.4104726314544678 5.130422592163086
Loss :  1.7143815755844116 3.135345697402954 4.849727153778076
Loss :  1.707148790359497 1.9961029291152954 3.703251838684082
Loss :  1.7108503580093384 2.5687081813812256 4.2795586585998535
Loss :  1.7132123708724976 3.0553934574127197 4.768605709075928
  batch 40 loss: 1.7132123708724976, 3.0553934574127197, 4.768605709075928
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7118163108825684 3.2302565574645996 4.942072868347168
Loss :  1.7067277431488037 2.8755197525024414 4.582247734069824
Loss :  1.7105700969696045 3.5692121982574463 5.279782295227051
Loss :  1.7150001525878906 2.848888874053955 4.563889026641846
Loss :  1.7165226936340332 2.1614811420440674 3.8780038356781006
Loss :  1.7156168222427368 1.9491816759109497 3.6647984981536865
Loss :  1.7189770936965942 1.9474492073059082 3.666426181793213
Loss :  1.7093391418457031 2.8206167221069336 4.529955863952637
Loss :  1.7258923053741455 2.5449204444885254 4.27081298828125
Loss :  1.7084283828735352 2.83864688873291 4.547075271606445
Loss :  1.6995316743850708 2.4025418758392334 4.102073669433594
Loss :  1.712030291557312 2.011350393295288 3.7233805656433105
Loss :  1.7298556566238403 2.2166926860809326 3.9465484619140625
Loss :  1.7056553363800049 2.16611647605896 3.871771812438965
Loss :  1.7080130577087402 2.1078953742980957 3.815908432006836
Loss :  1.7183529138565063 2.0433058738708496 3.7616586685180664
Loss :  1.7235386371612549 2.216641426086426 3.9401800632476807
Loss :  1.7197623252868652 2.3745920658111572 4.094354629516602
Loss :  1.723287582397461 2.9431750774383545 4.6664628982543945
Loss :  1.7263680696487427 2.523165464401245 4.249533653259277
  batch 60 loss: 1.7263680696487427, 2.523165464401245, 4.249533653259277
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.724501371383667 2.9303781986236572 4.654879570007324
Loss :  1.7225112915039062 3.2434589862823486 4.965970039367676
Loss :  1.7334954738616943 2.662139892578125 4.395635604858398
Loss :  1.7115380764007568 3.200900077819824 4.91243839263916
Loss :  1.7075791358947754 2.919581413269043 4.627160549163818
Loss :  1.8878625631332397 4.059211254119873 5.947073936462402
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.882509469985962 4.010207653045654 5.892717361450195
Loss :  1.880375623703003 3.792555093765259 5.672930717468262
Loss :  1.8791676759719849 3.8723220825195312 5.751489639282227
Total LOSS train 4.440114116668701 valid 5.8160529136657715
CE LOSS train 1.7147451345737164 valid 0.4697919189929962
Contrastive LOSS train 2.7253689710910503 valid 0.9680805206298828
EPOCH 94:
Loss :  1.7172293663024902 2.513150930404663 4.230380058288574
Loss :  1.7253004312515259 2.7987618446350098 4.524062156677246
Loss :  1.7114708423614502 2.509316921234131 4.22078800201416
Loss :  1.7136956453323364 3.3238637447357178 5.037559509277344
Loss :  1.7282384634017944 3.317551374435425 5.04578971862793
Loss :  1.7250906229019165 3.194753408432007 4.919844150543213
Loss :  1.724345088005066 2.0103626251220703 3.734707832336426
Loss :  1.7240557670593262 2.2303171157836914 3.9543728828430176
Loss :  1.7185039520263672 2.811091899871826 4.529595851898193
Loss :  1.6948548555374146 1.9857999086380005 3.680654764175415
Loss :  1.723392128944397 2.3500752449035645 4.073467254638672
Loss :  1.737019419670105 2.6684019565582275 4.405421257019043
Loss :  1.7193082571029663 2.954226493835449 4.673534870147705
Loss :  1.715299367904663 3.2672553062438965 4.9825544357299805
Loss :  1.718881607055664 3.0896410942077637 4.808522701263428
Loss :  1.7018193006515503 2.507675886154175 4.2094950675964355
Loss :  1.7188098430633545 2.413512945175171 4.132322788238525
Loss :  1.7145895957946777 2.789583683013916 4.504173278808594
Loss :  1.7103095054626465 2.505608081817627 4.215917587280273
Loss :  1.7055293321609497 2.7967538833618164 4.502283096313477
  batch 20 loss: 1.7055293321609497, 2.7967538833618164, 4.502283096313477
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7166180610656738 2.1509296894073486 3.8675477504730225
Loss :  1.7201120853424072 2.2064497470855713 3.9265618324279785
Loss :  1.7174440622329712 1.9936310052871704 3.7110750675201416
Loss :  1.728778600692749 2.3569982051849365 4.0857768058776855
Loss :  1.7119311094284058 2.298264980316162 4.010196208953857
Loss :  1.7118879556655884 2.2875444889068604 3.9994325637817383
Loss :  1.728173851966858 2.27699875831604 4.0051727294921875
Loss :  1.7144315242767334 2.0576300621032715 3.772061586380005
Loss :  1.727469563484192 2.0084550380706787 3.73592472076416
Loss :  1.7031612396240234 2.474520683288574 4.177681922912598
Loss :  1.7317858934402466 2.424133777618408 4.155919551849365
Loss :  1.7193963527679443 2.4231529235839844 4.142549514770508
Loss :  1.708385944366455 2.4922235012054443 4.20060920715332
Loss :  1.7031296491622925 2.286858081817627 3.989987850189209
Loss :  1.7180883884429932 2.58487606048584 4.302964210510254
Loss :  1.7202173471450806 2.2824103832244873 4.002627849578857
Loss :  1.7166752815246582 2.641244649887085 4.357919692993164
Loss :  1.7122119665145874 2.337735176086426 4.049947261810303
Loss :  1.7135311365127563 2.3542609214782715 4.067791938781738
Loss :  1.7143151760101318 2.284696340560913 3.999011516571045
  batch 40 loss: 1.7143151760101318, 2.284696340560913, 3.999011516571045
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7131701707839966 2.6161091327667236 4.32927942276001
Loss :  1.7046149969100952 2.5443241596221924 4.248939037322998
Loss :  1.707451343536377 2.4687681198120117 4.176219463348389
Loss :  1.7141178846359253 2.6823251247406006 4.396442890167236
Loss :  1.7153135538101196 2.657520055770874 4.372833728790283
Loss :  1.7116634845733643 2.3618967533111572 4.0735602378845215
Loss :  1.715703010559082 2.0843260288238525 3.8000290393829346
Loss :  1.7092013359069824 2.6878738403320312 4.397075176239014
Loss :  1.7216898202896118 2.1847012042999268 3.906391143798828
Loss :  1.7088143825531006 1.7895393371582031 3.4983537197113037
Loss :  1.6994531154632568 2.5135884284973145 4.213041305541992
Loss :  1.7112504243850708 1.9140722751617432 3.6253228187561035
Loss :  1.7272452116012573 2.504718065261841 4.231963157653809
Loss :  1.7074143886566162 2.9601783752441406 4.667593002319336
Loss :  1.707384705543518 3.1685574054718018 4.875942230224609
Loss :  1.7150156497955322 3.29976749420166 5.014782905578613
Loss :  1.7215259075164795 3.744110345840454 5.465636253356934
Loss :  1.7162714004516602 2.7250325679779053 4.4413042068481445
Loss :  1.7212672233581543 3.066641330718994 4.787908554077148
Loss :  1.7238426208496094 3.4270589351654053 5.150901794433594
  batch 60 loss: 1.7238426208496094, 3.4270589351654053, 5.150901794433594
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7206586599349976 3.30256724357605 5.023225784301758
Loss :  1.7200676202774048 3.3081469535827637 5.028214454650879
Loss :  1.7278865575790405 2.4372315406799316 4.165118217468262
Loss :  1.7081646919250488 2.774250030517578 4.482414722442627
Loss :  1.7042068243026733 2.444491147994995 4.148697853088379
Loss :  1.8762718439102173 3.6657004356384277 5.5419721603393555
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8720273971557617 3.6077511310577393 5.479778289794922
Loss :  1.8685146570205688 3.467003107070923 5.335517883300781
Loss :  1.8712596893310547 3.3417162895202637 5.212975978851318
Total LOSS train 4.299498433333177 valid 5.392561078071594
CE LOSS train 1.7159828241054829 valid 0.46781492233276367
Contrastive LOSS train 2.5835156110616833 valid 0.8354290723800659
EPOCH 95:
Loss :  1.7143871784210205 2.2505598068237305 3.964946985244751
Loss :  1.7192308902740479 2.2823901176452637 4.001621246337891
Loss :  1.7062182426452637 1.8547011613845825 3.5609192848205566
Loss :  1.710775375366211 2.431466817855835 4.142242431640625
Loss :  1.7229644060134888 2.4667742252349854 4.189738750457764
Loss :  1.7212852239608765 2.4831931591033936 4.2044782638549805
Loss :  1.7176406383514404 3.3418312072753906 5.05947208404541
Loss :  1.718087911605835 2.217379093170166 3.935467004776001
Loss :  1.715710163116455 2.8697898387908936 4.5854997634887695
Loss :  1.6926243305206299 3.4032845497131348 5.095909118652344
Loss :  1.720874309539795 2.989659070968628 4.710533142089844
Loss :  1.736781120300293 3.4409139156341553 5.177695274353027
Loss :  1.7188140153884888 2.9574265480041504 4.67624044418335
Loss :  1.7151325941085815 2.85690975189209 4.572042465209961
Loss :  1.7214701175689697 2.3435628414154053 4.065032958984375
Loss :  1.7030082941055298 3.0545835494995117 4.757591724395752
Loss :  1.7194650173187256 2.682605504989624 4.40207052230835
Loss :  1.7137311697006226 3.178494930267334 4.892226219177246
Loss :  1.7113243341445923 3.250840663909912 4.962164878845215
Loss :  1.7047487497329712 3.5928356647491455 5.297584533691406
  batch 20 loss: 1.7047487497329712, 3.5928356647491455, 5.297584533691406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7167487144470215 2.780413866043091 4.497162818908691
Loss :  1.719869613647461 2.2595152854919434 3.9793848991394043
Loss :  1.7166075706481934 2.3346362113952637 4.051243782043457
Loss :  1.7277586460113525 2.645956516265869 4.373715400695801
Loss :  1.7120180130004883 2.629244804382324 4.3412628173828125
Loss :  1.7119110822677612 3.518836498260498 5.230747699737549
Loss :  1.7273260354995728 2.787879467010498 4.515205383300781
Loss :  1.7137151956558228 2.5407872200012207 4.254502296447754
Loss :  1.72714102268219 2.3089723587036133 4.036113262176514
Loss :  1.7035009860992432 2.5031423568725586 4.206643104553223
Loss :  1.7312785387039185 2.723625421524048 4.454904079437256
Loss :  1.7191680669784546 2.4435462951660156 4.16271448135376
Loss :  1.7087961435317993 2.7191994190216064 4.427995681762695
Loss :  1.7028939723968506 2.947227954864502 4.650121688842773
Loss :  1.7175213098526 2.5523366928100586 4.269857883453369
Loss :  1.7199726104736328 2.669948101043701 4.389920711517334
Loss :  1.716277837753296 2.301372766494751 4.017650604248047
Loss :  1.712432861328125 2.143378496170044 3.855811357498169
Loss :  1.712487816810608 2.0631520748138428 3.7756400108337402
Loss :  1.71426522731781 1.9738835096359253 3.6881487369537354
  batch 40 loss: 1.71426522731781, 1.9738835096359253, 3.6881487369537354
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.713257074356079 2.6526076793670654 4.3658647537231445
Loss :  1.7051321268081665 2.7494914531707764 4.454623699188232
Loss :  1.707501769065857 2.8499269485473633 4.55742883682251
Loss :  1.7134541273117065 2.7477340698242188 4.461188316345215
Loss :  1.7153599262237549 3.504620313644409 5.219980239868164
Loss :  1.7115535736083984 2.727987766265869 4.439541339874268
Loss :  1.714221715927124 2.829045295715332 4.543267250061035
Loss :  1.7085293531417847 2.381540536880493 4.090069770812988
Loss :  1.7178527116775513 2.3789494037628174 4.096802234649658
Loss :  1.7067642211914062 2.2644829750061035 3.9712471961975098
Loss :  1.6972640752792358 2.4684836864471436 4.16574764251709
Loss :  1.7093459367752075 2.4169974327087402 4.126343250274658
Loss :  1.724521517753601 3.1007347106933594 4.82525634765625
Loss :  1.7069790363311768 2.2282092571258545 3.9351882934570312
Loss :  1.70748770236969 2.651996612548828 4.3594841957092285
Loss :  1.7127907276153564 2.6801185607910156 4.392909049987793
Loss :  1.7193326950073242 2.368807554244995 4.088140487670898
Loss :  1.7162095308303833 2.1371028423309326 3.8533124923706055
Loss :  1.72061026096344 2.5189239978790283 4.239534378051758
Loss :  1.7210190296173096 1.8323981761932373 3.553417205810547
  batch 60 loss: 1.7210190296173096, 1.8323981761932373, 3.553417205810547
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7182331085205078 1.950953722000122 3.66918683052063
Loss :  1.718204140663147 1.8639435768127441 3.5821475982666016
Loss :  1.7272000312805176 2.831967830657959 4.559167861938477
Loss :  1.7080116271972656 2.5427587032318115 4.250770568847656
Loss :  1.7047126293182373 1.9554227590560913 3.660135269165039
Loss :  1.8929510116577148 3.741569995880127 5.634521007537842
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.886873722076416 3.6999361515045166 5.586810111999512
Loss :  1.8850078582763672 3.5686216354370117 5.453629493713379
Loss :  1.8853689432144165 3.7449934482574463 5.630362510681152
Total LOSS train 4.321430444717407 valid 5.576330780982971
CE LOSS train 1.7147925230172965 valid 0.4713422358036041
Contrastive LOSS train 2.606637901526231 valid 0.9362483620643616
EPOCH 96:
Loss :  1.713947057723999 1.9124219417572021 3.626368999481201
Loss :  1.7195372581481934 2.212064266204834 3.9316015243530273
Loss :  1.7072980403900146 1.9773260354995728 3.684624195098877
Loss :  1.7117058038711548 1.8773083686828613 3.5890140533447266
Loss :  1.7231330871582031 2.572662353515625 4.295795440673828
Loss :  1.7215547561645508 2.087059736251831 3.808614492416382
Loss :  1.7186253070831299 2.372663974761963 4.091289520263672
Loss :  1.7171425819396973 1.8248056173324585 3.5419483184814453
Loss :  1.7158013582229614 3.0686795711517334 4.784481048583984
Loss :  1.6924092769622803 2.1667091846466064 3.8591184616088867
Loss :  1.7219130992889404 2.422132968902588 4.144045829772949
Loss :  1.7373499870300293 2.483273506164551 4.22062349319458
Loss :  1.7187836170196533 2.2721965312957764 3.9909801483154297
Loss :  1.7144229412078857 2.3787240982055664 4.093147277832031
Loss :  1.7177022695541382 2.892932176589966 4.6106343269348145
Loss :  1.7033560276031494 2.418924570083618 4.122280597686768
Loss :  1.7189373970031738 2.2238283157348633 3.942765712738037
Loss :  1.715239405632019 2.563756227493286 4.278995513916016
Loss :  1.712058186531067 1.8733047246932983 3.5853629112243652
Loss :  1.7046229839324951 2.0275354385375977 3.7321584224700928
  batch 20 loss: 1.7046229839324951, 2.0275354385375977, 3.7321584224700928
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7155344486236572 2.1003670692443848 3.815901517868042
Loss :  1.7199602127075195 2.840125560760498 4.560085773468018
Loss :  1.7169702053070068 2.339205265045166 4.056175231933594
Loss :  1.7287927865982056 2.984079599380493 4.712872505187988
Loss :  1.7134264707565308 3.0644419193267822 4.777868270874023
Loss :  1.711903691291809 3.2569243907928467 4.968828201293945
Loss :  1.7275382280349731 3.2331435680389404 4.960681915283203
Loss :  1.7125815153121948 3.4355502128601074 5.148131847381592
Loss :  1.72518789768219 3.4615817070007324 5.186769485473633
Loss :  1.7048178911209106 3.47880220413208 5.183619976043701
Loss :  1.7323154211044312 2.6661202907562256 4.398435592651367
Loss :  1.7197896242141724 3.0483553409576416 4.7681450843811035
Loss :  1.7080161571502686 2.861445426940918 4.569461822509766
Loss :  1.7035014629364014 2.3819453716278076 4.085446834564209
Loss :  1.7185462713241577 2.849100351333618 4.567646503448486
Loss :  1.7195641994476318 2.125279426574707 3.844843626022339
Loss :  1.7149567604064941 2.051607370376587 3.766564130783081
Loss :  1.7106605768203735 1.7914223670959473 3.5020828247070312
Loss :  1.7126283645629883 2.1811957359313965 3.8938241004943848
Loss :  1.7141592502593994 2.1542625427246094 3.868421792984009
  batch 40 loss: 1.7141592502593994, 2.1542625427246094, 3.868421792984009
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7137119770050049 2.862107038497925 4.57581901550293
Loss :  1.7075289487838745 2.7909927368164062 4.49852180480957
Loss :  1.7111459970474243 2.7708709239959717 4.4820170402526855
Loss :  1.7153823375701904 2.8883423805236816 4.603724479675293
Loss :  1.71822190284729 2.0131301879882812 3.7313520908355713
Loss :  1.713136911392212 2.043842077255249 3.756978988647461
Loss :  1.7151347398757935 3.325958013534546 5.041092872619629
Loss :  1.71133291721344 3.636802911758423 5.348135948181152
Loss :  1.719929575920105 3.204550266265869 4.924479961395264
Loss :  1.7084046602249146 3.129826545715332 4.838231086730957
Loss :  1.698950171470642 3.1513497829437256 4.850299835205078
Loss :  1.7120603322982788 3.02632999420166 4.7383904457092285
Loss :  1.7255195379257202 2.2836034297943115 4.009122848510742
Loss :  1.7046241760253906 2.362406015396118 4.06702995300293
Loss :  1.7080683708190918 2.707592487335205 4.415660858154297
Loss :  1.7139873504638672 2.798088550567627 4.512075901031494
Loss :  1.7180440425872803 3.7082884311676025 5.426332473754883
Loss :  1.715829849243164 3.8142354488372803 5.530065536499023
Loss :  1.718458890914917 3.197664737701416 4.916123390197754
Loss :  1.7196942567825317 2.7620186805725098 4.481712818145752
  batch 60 loss: 1.7196942567825317, 2.7620186805725098, 4.481712818145752
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7174330949783325 2.9184470176696777 4.635879993438721
Loss :  1.7176991701126099 2.4809508323669434 4.198649883270264
Loss :  1.725988745689392 2.9893884658813477 4.715377330780029
Loss :  1.7065709829330444 3.2116265296936035 4.9181976318359375
Loss :  1.7032427787780762 3.0523502826690674 4.755593299865723
Loss :  1.7842153310775757 3.72908353805542 5.513298988342285
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7837697267532349 3.660195827484131 5.443965435028076
Loss :  1.7710386514663696 3.444450855255127 5.215489387512207
Loss :  1.8000346422195435 3.225672960281372 5.025707721710205
Total LOSS train 4.377546504827646 valid 5.299615383148193
CE LOSS train 1.715022978415856 valid 0.45000866055488586
Contrastive LOSS train 2.662523524577801 valid 0.806418240070343
EPOCH 97:
Loss :  1.7125941514968872 2.6663928031921387 4.378986835479736
Loss :  1.7183830738067627 2.7179503440856934 4.436333656311035
Loss :  1.7061123847961426 2.208481788635254 3.9145941734313965
Loss :  1.7105737924575806 2.181673765182495 3.8922476768493652
Loss :  1.7215049266815186 2.2429494857788086 3.964454412460327
Loss :  1.7189644575119019 2.3585798740386963 4.077544212341309
Loss :  1.7173030376434326 2.566629648208618 4.283932685852051
Loss :  1.716064214706421 2.9094462394714355 4.625510215759277
Loss :  1.7130976915359497 2.715104818344116 4.4282026290893555
Loss :  1.68841552734375 3.6001172065734863 5.288532733917236
Loss :  1.7177060842514038 3.261160373687744 4.9788665771484375
Loss :  1.7345364093780518 3.13714861869812 4.871685028076172
Loss :  1.7153987884521484 2.5687241554260254 4.284122943878174
Loss :  1.7114293575286865 2.258723258972168 3.9701526165008545
Loss :  1.7183481454849243 2.1203534603118896 3.8387017250061035
Loss :  1.6990951299667358 2.1242058277130127 3.823300838470459
Loss :  1.7144252061843872 2.470761299133301 4.185186386108398
Loss :  1.7095454931259155 2.2213351726531982 3.930880546569824
Loss :  1.710628867149353 1.910555362701416 3.6211843490600586
Loss :  1.6999009847640991 2.558840751647949 4.258741855621338
  batch 20 loss: 1.6999009847640991, 2.558840751647949, 4.258741855621338
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7119014263153076 2.5421056747436523 4.254007339477539
Loss :  1.7162375450134277 2.6313939094543457 4.347631454467773
Loss :  1.712414264678955 2.963707447052002 4.676121711730957
Loss :  1.7239562273025513 3.0032739639282227 4.727230072021484
Loss :  1.7141867876052856 2.849278211593628 4.563465118408203
Loss :  1.709359884262085 3.7626118659973145 5.47197151184082
Loss :  1.7233725786209106 3.0538156032562256 4.777188301086426
Loss :  1.706987977027893 2.8527886867523193 4.559776782989502
Loss :  1.7223162651062012 2.519862651824951 4.242178916931152
Loss :  1.7040811777114868 2.8214032649993896 4.525484561920166
Loss :  1.7311383485794067 2.663822889328003 4.394961357116699
Loss :  1.7167410850524902 2.521911382675171 4.238652229309082
Loss :  1.7065472602844238 2.9275503158569336 4.634097576141357
Loss :  1.7023977041244507 2.9895684719085693 4.6919660568237305
Loss :  1.7182241678237915 2.8901352882385254 4.608359336853027
Loss :  1.7191768884658813 3.3125038146972656 5.031680583953857
Loss :  1.714444637298584 2.874642848968506 4.58908748626709
Loss :  1.7063939571380615 2.455914258956909 4.162308216094971
Loss :  1.710398554801941 2.73275089263916 4.443149566650391
Loss :  1.708916187286377 2.8155622482299805 4.524478435516357
  batch 40 loss: 1.708916187286377, 2.8155622482299805, 4.524478435516357
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7077986001968384 2.7183425426483154 4.426141262054443
Loss :  1.7035815715789795 2.3853771686553955 4.088958740234375
Loss :  1.7079460620880127 1.9943593740463257 3.702305316925049
Loss :  1.711075782775879 2.004695177078247 3.715770959854126
Loss :  1.714248538017273 1.952136754989624 3.6663851737976074
Loss :  1.709077000617981 2.481398820877075 4.190475940704346
Loss :  1.7117712497711182 3.3532555103302 5.065026760101318
Loss :  1.7097028493881226 2.7778544425964355 4.487557411193848
Loss :  1.7159682512283325 3.370227813720703 5.086195945739746
Loss :  1.708720088005066 2.9768903255462646 4.685610294342041
Loss :  1.700015902519226 3.2216508388519287 4.921666622161865
Loss :  1.7105008363723755 3.1442112922668457 4.854712009429932
Loss :  1.7221622467041016 2.4220190048217773 4.144181251525879
Loss :  1.7075687646865845 2.5174946784973145 4.225063323974609
Loss :  1.7086600065231323 2.7513506412506104 4.460010528564453
Loss :  1.710555911064148 2.311954975128174 4.022511005401611
Loss :  1.7180187702178955 2.5542290210723877 4.272247791290283
Loss :  1.7155029773712158 3.2921206951141357 5.007623672485352
Loss :  1.7205688953399658 3.9861037731170654 5.706672668457031
Loss :  1.7193028926849365 3.252760887145996 4.972064018249512
  batch 60 loss: 1.7193028926849365, 3.252760887145996, 4.972064018249512
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7168691158294678 3.068995714187622 4.78586483001709
Loss :  1.718954086303711 2.888049840927124 4.607004165649414
Loss :  1.7244389057159424 3.603555202484131 5.327994346618652
Loss :  1.7086142301559448 3.292097568511963 5.000711917877197
Loss :  1.7055344581604004 2.6952335834503174 4.400768280029297
Loss :  1.878866195678711 4.169138431549072 6.048004627227783
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.877467155456543 4.0219244956970215 5.8993916511535645
Loss :  1.8718992471694946 4.063361167907715 5.93526029586792
Loss :  1.8740489482879639 3.9104576110839844 5.784506797790527
Total LOSS train 4.46677666077247 valid 5.916790843009949
CE LOSS train 1.7129289021858802 valid 0.46851223707199097
Contrastive LOSS train 2.7538477475826557 valid 0.9776144027709961
EPOCH 98:
Loss :  1.7138915061950684 3.094770908355713 4.808662414550781
Loss :  1.715288758277893 3.5129330158233643 5.228221893310547
Loss :  1.705471158027649 3.36065936088562 5.066130638122559
Loss :  1.712096095085144 2.4688405990600586 4.180936813354492
Loss :  1.7201135158538818 2.4503543376922607 4.170467853546143
Loss :  1.718423843383789 2.1331658363342285 3.8515896797180176
Loss :  1.7147949934005737 2.509658098220825 4.224452972412109
Loss :  1.714685082435608 1.9206690788269043 3.6353540420532227
Loss :  1.7130271196365356 2.2049143314361572 3.9179415702819824
Loss :  1.6892460584640503 2.169349431991577 3.858595371246338
Loss :  1.7171494960784912 2.2579078674316406 3.975057363510132
Loss :  1.736378788948059 2.8406853675842285 4.577064037322998
Loss :  1.7164907455444336 2.6593563556671143 4.375846862792969
Loss :  1.7142221927642822 3.2503552436828613 4.964577674865723
Loss :  1.720500111579895 2.7837460041046143 4.504246234893799
Loss :  1.703560709953308 3.088524580001831 4.79208517074585
Loss :  1.718342661857605 2.448911428451538 4.1672539710998535
Loss :  1.7121312618255615 2.4766926765441895 4.188823699951172
Loss :  1.71211838722229 2.591144323348999 4.303262710571289
Loss :  1.7020983695983887 3.067180633544922 4.7692790031433105
  batch 20 loss: 1.7020983695983887, 3.067180633544922, 4.7692790031433105
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7132834196090698 2.895076036453247 4.608359336853027
Loss :  1.7181446552276611 3.4694371223449707 5.187582015991211
Loss :  1.7148542404174805 3.376347303390503 5.0912017822265625
Loss :  1.7266255617141724 2.6296916007995605 4.356317043304443
Loss :  1.7145013809204102 2.593611001968384 4.308112144470215
Loss :  1.7113244533538818 2.6428205966949463 4.354145050048828
Loss :  1.7253574132919312 3.0163075923919678 4.741664886474609
Loss :  1.7103068828582764 2.432109832763672 4.142416954040527
Loss :  1.7243324518203735 2.125516653060913 3.849849224090576
Loss :  1.7048838138580322 2.751460313796997 4.456344127655029
Loss :  1.7321277856826782 2.6225831508636475 4.354711055755615
Loss :  1.7174458503723145 2.7752890586853027 4.492734909057617
Loss :  1.7069664001464844 2.5378482341766357 4.244814872741699
Loss :  1.7029287815093994 2.4433095455169678 4.146238327026367
Loss :  1.718267560005188 2.695918321609497 4.414186000823975
Loss :  1.720112681388855 3.0245273113250732 4.744639873504639
Loss :  1.7155948877334595 2.5476784706115723 4.263273239135742
Loss :  1.7086846828460693 2.1767776012420654 3.8854622840881348
Loss :  1.7129260301589966 2.469109535217285 4.182035446166992
Loss :  1.7125585079193115 2.3826112747192383 4.095170021057129
  batch 40 loss: 1.7125585079193115, 2.3826112747192383, 4.095170021057129
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7115404605865479 2.5357296466827393 4.247270107269287
Loss :  1.7071112394332886 2.25596022605896 3.963071346282959
Loss :  1.7094876766204834 2.6257870197296143 4.335274696350098
Loss :  1.7138900756835938 2.40883469581604 4.122724533081055
Loss :  1.7158701419830322 2.8573522567749023 4.5732221603393555
Loss :  1.7096613645553589 2.8012564182281494 4.510917663574219
Loss :  1.7127318382263184 2.9650917053222656 4.677823543548584
Loss :  1.7087730169296265 2.3175148963928223 4.026288032531738
Loss :  1.7153456211090088 2.4409797191619873 4.156325340270996
Loss :  1.7078347206115723 2.5298287868499756 4.237663269042969
Loss :  1.699349045753479 2.5322585105895996 4.231607437133789
Loss :  1.7112399339675903 2.593379020690918 4.304618835449219
Loss :  1.7247709035873413 2.1426713466644287 3.8674421310424805
Loss :  1.7089985609054565 2.5407748222351074 4.2497735023498535
Loss :  1.708798885345459 2.535888195037842 4.244687080383301
Loss :  1.714192509651184 3.02557635307312 4.739768981933594
Loss :  1.7198463678359985 2.568932056427002 4.288778305053711
Loss :  1.716072916984558 2.4558117389678955 4.171884536743164
Loss :  1.7214224338531494 3.17153000831604 4.8929524421691895
Loss :  1.7225043773651123 2.744812488555908 4.467316627502441
  batch 60 loss: 1.7225043773651123, 2.744812488555908, 4.467316627502441
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7196615934371948 3.9798038005828857 5.699465274810791
Loss :  1.7207082509994507 3.3633105754852295 5.084018707275391
Loss :  1.7280572652816772 3.0300424098968506 4.758099555969238
Loss :  1.7105238437652588 2.933820962905884 4.644344806671143
Loss :  1.7075920104980469 2.174420118331909 3.882012128829956
Loss :  1.9071438312530518 3.8551859855651855 5.762330055236816
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.90117609500885 3.759202241897583 5.660378456115723
Loss :  1.8993570804595947 3.644207715988159 5.543564796447754
Loss :  1.895813226699829 3.6005587577819824 5.496372222900391
Total LOSS train 4.397791686424842 valid 5.615661382675171
CE LOSS train 1.7142652823374822 valid 0.4739533066749573
Contrastive LOSS train 2.683526427929218 valid 0.9001396894454956
EPOCH 99:
Loss :  1.716741681098938 3.3261468410491943 5.042888641357422
Loss :  1.7203112840652466 3.0255966186523438 4.745907783508301
Loss :  1.7080236673355103 3.039207696914673 4.747231483459473
Loss :  1.7128841876983643 3.1202263832092285 4.833110809326172
Loss :  1.724349021911621 2.7402405738830566 4.464589595794678
Loss :  1.7227822542190552 3.3704490661621094 5.093231201171875
Loss :  1.7198339700698853 3.163555145263672 4.883388996124268
Loss :  1.7204198837280273 2.718524217605591 4.438943862915039
Loss :  1.7167024612426758 3.391406297683716 5.1081085205078125
Loss :  1.6940593719482422 3.284337043762207 4.978396415710449
Loss :  1.7220356464385986 3.156409978866577 4.878445625305176
Loss :  1.7373542785644531 2.3073534965515137 4.044707775115967
Loss :  1.7200132608413696 2.2384159564971924 3.9584293365478516
Loss :  1.7180163860321045 2.4173784255981445 4.135395050048828
Loss :  1.7214405536651611 2.6005187034606934 4.321959495544434
Loss :  1.7046364545822144 3.0177981853485107 4.7224345207214355
Loss :  1.7213349342346191 2.6803154945373535 4.401650428771973
Loss :  1.715394139289856 2.884157419204712 4.599551677703857
Loss :  1.7124892473220825 3.1849329471588135 4.8974223136901855
Loss :  1.7059990167617798 3.343919038772583 5.049918174743652
  batch 20 loss: 1.7059990167617798, 3.343919038772583, 5.049918174743652
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7176990509033203 2.7840778827667236 4.501776695251465
Loss :  1.7207694053649902 2.633357524871826 4.354126930236816
Loss :  1.718306064605713 2.2708089351654053 3.989114999771118
Loss :  1.7285370826721191 2.703874349594116 4.432411193847656
Loss :  1.712969422340393 2.5886521339416504 4.301621437072754
Loss :  1.7135310173034668 2.57541561126709 4.288946628570557
Loss :  1.7287098169326782 2.2503509521484375 3.979060649871826
Loss :  1.7153716087341309 2.1819727420806885 3.8973443508148193
Loss :  1.7289042472839355 2.4753706455230713 4.204275131225586
Loss :  1.7045502662658691 2.6734161376953125 4.377966403961182
Loss :  1.7318246364593506 2.63875412940979 4.370578765869141
Loss :  1.7197037935256958 2.2826087474823 4.002312660217285
Loss :  1.709432601928711 2.5720503330230713 4.281482696533203
Loss :  1.7036867141723633 3.2692599296569824 4.972946643829346
Loss :  1.7182849645614624 3.0817480087280273 4.800033092498779
Loss :  1.7208302021026611 2.5896294116973877 4.310459613800049
Loss :  1.7161428928375244 2.803802013397217 4.51994514465332
Loss :  1.7119556665420532 2.386692762374878 4.098648548126221
Loss :  1.7143194675445557 2.744382381439209 4.458702087402344
Loss :  1.7152215242385864 1.9196604490280151 3.6348819732666016
  batch 40 loss: 1.7152215242385864, 1.9196604490280151, 3.6348819732666016
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7137477397918701 2.5044569969177246 4.218204498291016
Loss :  1.7077363729476929 1.971031665802002 3.6787681579589844
Loss :  1.7092636823654175 1.9817472696304321 3.6910109519958496
Loss :  1.7146005630493164 1.87083899974823 3.585439682006836
Loss :  1.716288685798645 1.9442079067230225 3.660496711730957
Loss :  1.7137030363082886 2.2690553665161133 3.9827585220336914
Loss :  1.7169090509414673 2.8247387409210205 4.541647911071777
Loss :  1.7100006341934204 2.167586326599121 3.877586841583252
Loss :  1.7215579748153687 2.122143030166626 3.843700885772705
Loss :  1.7092801332473755 2.5410852432250977 4.250365257263184
Loss :  1.7017666101455688 1.9500656127929688 3.651832103729248
Loss :  1.7134864330291748 2.1152827739715576 3.8287692070007324
Loss :  1.726381540298462 3.025038719177246 4.751420021057129
Loss :  1.7088035345077515 3.2198781967163086 4.92868185043335
Loss :  1.7093462944030762 2.6629412174224854 4.372287750244141
Loss :  1.7145103216171265 2.2412147521972656 3.9557251930236816
Loss :  1.7201334238052368 2.398505210876465 4.118638515472412
Loss :  1.716450572013855 3.1702802181243896 4.886730670928955
Loss :  1.7209876775741577 2.553406000137329 4.274393558502197
Loss :  1.7223570346832275 1.9731059074401855 3.695462942123413
  batch 60 loss: 1.7223570346832275, 1.9731059074401855, 3.695462942123413
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7190130949020386 2.322124481201172 4.0411376953125
Loss :  1.7196577787399292 2.265821933746338 3.9854798316955566
Loss :  1.726219654083252 2.2977912425994873 4.02401065826416
Loss :  1.7088326215744019 3.145244598388672 4.854077339172363
Loss :  1.7061316967010498 3.5615594387054443 5.267691135406494
Loss :  1.8665952682495117 4.254744052886963 6.121339321136475
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8628990650177002 4.15304708480835 6.015946388244629
Loss :  1.8587511777877808 4.018224239349365 5.8769755363464355
Loss :  1.860440731048584 4.222527027130127 6.082967758178711
Total LOSS train 4.355210234568669 valid 6.0243072509765625
CE LOSS train 1.7161959739831778 valid 0.465110182762146
Contrastive LOSS train 2.639014253249535 valid 1.0556317567825317
EPOCH 100:
Loss :  1.716958999633789 3.440251350402832 5.157210350036621
Loss :  1.718641757965088 3.2498507499694824 4.96849250793457
Loss :  1.7076117992401123 2.2754249572753906 3.983036756515503
Loss :  1.7134016752243042 1.9695507287979126 3.682952404022217
Loss :  1.7242043018341064 2.1387410163879395 3.862945318222046
Loss :  1.7221105098724365 2.016474723815918 3.7385852336883545
Loss :  1.7185884714126587 3.446056604385376 5.164645195007324
Loss :  1.712862253189087 3.670832872390747 5.383695125579834
Loss :  1.7161544561386108 2.3670995235443115 4.083253860473633
Loss :  1.693892478942871 2.1178817749023438 3.811774253845215
Loss :  1.720963954925537 3.1652252674102783 4.8861894607543945
Loss :  1.7371647357940674 3.8899426460266113 5.627107620239258
Loss :  1.707008957862854 2.7401745319366455 4.447183609008789
Loss :  1.7146726846694946 2.680347442626953 4.395020008087158
Loss :  1.7171345949172974 2.5816798210144043 4.298814296722412
Loss :  1.7036727666854858 2.754688024520874 4.45836067199707
Loss :  1.7182810306549072 2.8247745037078857 4.543055534362793
Loss :  1.70647394657135 2.817551374435425 4.5240254402160645
Loss :  1.7160333395004272 2.8671646118164062 4.583198070526123
Loss :  1.7047210931777954 2.857841730117798 4.562562942504883
  batch 20 loss: 1.7047210931777954, 2.857841730117798, 4.562562942504883
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.714518666267395 2.9303691387176514 4.644887924194336
Loss :  1.7271658182144165 2.7766151428222656 4.503780841827393
Loss :  1.7113929986953735 2.4434456825256348 4.154838562011719
Loss :  1.726300835609436 2.5120954513549805 4.238396167755127
Loss :  1.7102876901626587 2.851980447769165 4.562268257141113
Loss :  1.7058500051498413 2.383387565612793 4.089237689971924
Loss :  1.7279151678085327 2.7617852687835693 4.4897003173828125
Loss :  1.71869695186615 2.7018120288848877 4.420508861541748
Loss :  1.729367971420288 2.5805318355560303 4.309899806976318
Loss :  1.7112573385238647 2.748974561691284 4.460231781005859
Loss :  1.7323901653289795 3.0676844120025635 4.800074577331543
Loss :  1.7219064235687256 3.3174140453338623 5.039320468902588
Loss :  1.715014934539795 3.0261001586914062 4.741115093231201
Loss :  1.7057900428771973 2.70579195022583 4.411581993103027
Loss :  1.722969651222229 3.1527349948883057 4.875704765319824
Loss :  1.7234658002853394 3.248562812805176 4.972028732299805
Loss :  1.721892237663269 3.188112497329712 4.910004615783691
Loss :  1.7135154008865356 4.275049686431885 5.988564968109131
Loss :  1.7229610681533813 3.0549604892730713 4.777921676635742
Loss :  1.7168349027633667 3.157087564468384 4.873922348022461
  batch 40 loss: 1.7168349027633667, 3.157087564468384, 4.873922348022461
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7218354940414429 3.4771578311920166 5.19899320602417
Loss :  1.710123896598816 3.36215877532959 5.072282791137695
Loss :  1.7166376113891602 2.9251558780670166 4.641793251037598
Loss :  1.7197784185409546 2.9449687004089355 4.66474723815918
Loss :  1.7215616703033447 3.134850025177002 4.856411933898926
Loss :  1.7162530422210693 2.7891345024108887 4.505387306213379
Loss :  1.7253297567367554 2.7805025577545166 4.505832195281982
Loss :  1.7186222076416016 3.1993377208709717 4.917960166931152
Loss :  1.7219226360321045 3.2503175735473633 4.972240447998047
Loss :  1.713303804397583 3.0303232669830322 4.743627071380615
Loss :  1.7119184732437134 2.900918483734131 4.612836837768555
Loss :  1.720317006111145 2.7399911880493164 4.460308074951172
Loss :  1.740270733833313 2.563023805618286 4.303294658660889
Loss :  1.7108913660049438 3.0595076084136963 4.77039909362793
Loss :  1.7186477184295654 3.39624285697937 5.1148905754089355
Loss :  1.7391273975372314 3.1534626483917236 4.892590045928955
Loss :  1.7312554121017456 3.227745771408081 4.959001064300537
Loss :  1.7249441146850586 3.495089054107666 5.220033168792725
Loss :  1.7288767099380493 3.543395519256592 5.272272109985352
Loss :  1.7353817224502563 2.9495062828063965 4.684887886047363
  batch 60 loss: 1.7353817224502563, 2.9495062828063965, 4.684887886047363
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7337818145751953 3.2436599731445312 4.977441787719727
Loss :  1.7333377599716187 2.300506591796875 4.033844470977783
Loss :  1.7426217794418335 2.459294557571411 4.201916217803955
Loss :  1.7237095832824707 3.3144686222076416 5.038178443908691
Loss :  1.7204643487930298 2.250610113143921 3.9710745811462402
Loss :  1.73314368724823 3.6526620388031006 5.385805606842041
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7249443531036377 3.644442319869995 5.369386672973633
Loss :  1.7464282512664795 3.5118489265441895 5.25827693939209
Loss :  1.7620829343795776 3.5311453342437744 5.2932281494140625
Total LOSS train 4.6464360420520485 valid 5.3266743421554565
CE LOSS train 1.7195532670387854 valid 0.4405207335948944
Contrastive LOSS train 2.926882767677307 valid 0.8827863335609436
EPOCH 101:
Loss :  1.7262407541275024 3.688626527786255 5.414867401123047
Loss :  1.7376049757003784 3.6086111068725586 5.346216201782227
Loss :  1.7257636785507202 3.455075740814209 5.180839538574219
Loss :  1.725251317024231 3.3931071758270264 5.118358612060547
Loss :  1.7346186637878418 2.8805644512176514 4.615182876586914
Loss :  1.7349523305892944 3.3828134536743164 5.1177659034729
Loss :  1.7298402786254883 3.2074973583221436 4.937337875366211
Loss :  1.732736587524414 3.000964879989624 4.733701705932617
Loss :  1.7306007146835327 3.3149573802948 5.045557975769043
Loss :  1.7091877460479736 3.1301701068878174 4.839357852935791
Loss :  1.7335748672485352 3.317521572113037 5.051096439361572
Loss :  1.7489584684371948 3.561127185821533 5.310085773468018
Loss :  1.7325416803359985 3.1948883533477783 4.927430152893066
Loss :  1.73081374168396 3.0951051712036133 4.825919151306152
Loss :  1.7321043014526367 2.665207862854004 4.397312164306641
Loss :  1.7189663648605347 2.2001426219940186 3.9191088676452637
Loss :  1.734979510307312 1.9952950477600098 3.7302746772766113
Loss :  1.7241567373275757 2.267487049102783 3.9916439056396484
Loss :  1.724743127822876 2.6142799854278564 4.339023113250732
Loss :  1.7216936349868774 2.5722906589508057 4.293984413146973
  batch 20 loss: 1.7216936349868774, 2.5722906589508057, 4.293984413146973
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7258044481277466 2.3339521884918213 4.059756755828857
Loss :  1.7334014177322388 2.6719141006469727 4.405315399169922
Loss :  1.7237478494644165 2.877547264099121 4.601294994354248
Loss :  1.7380709648132324 2.832040548324585 4.570111274719238
Loss :  1.7208251953125 3.240612030029297 4.961437225341797
Loss :  1.7231481075286865 3.582707166671753 5.3058552742004395
Loss :  1.7362390756607056 2.5140817165374756 4.250320911407471
Loss :  1.7237286567687988 2.4105539321899414 4.13428258895874
Loss :  1.733750581741333 2.670815944671631 4.404566764831543
Loss :  1.7216017246246338 2.9924509525299072 4.714052677154541
Loss :  1.7386438846588135 2.834207057952881 4.572851181030273
Loss :  1.7305808067321777 3.0417745113372803 4.772355079650879
Loss :  1.7224657535552979 2.887726306915283 4.61019229888916
Loss :  1.716219186782837 2.8461358547210693 4.562355041503906
Loss :  1.7259361743927002 2.642763376235962 4.368699550628662
Loss :  1.7305430173873901 2.443903684616089 4.1744465827941895
Loss :  1.7254741191864014 2.568495035171509 4.29396915435791
Loss :  1.7214816808700562 2.450701951980591 4.172183513641357
Loss :  1.7260863780975342 2.4692347049713135 4.195321083068848
Loss :  1.7233328819274902 3.020453453063965 4.743786334991455
  batch 40 loss: 1.7233328819274902, 3.020453453063965, 4.743786334991455
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.72617506980896 2.6104795932769775 4.3366546630859375
Loss :  1.7192378044128418 2.361003875732422 4.080241680145264
Loss :  1.7155084609985352 2.456247568130493 4.171755790710449
Loss :  1.726577877998352 3.4913759231567383 5.217953681945801
Loss :  1.725080132484436 2.862114906311035 4.587194919586182
Loss :  1.7210928201675415 2.9903883934020996 4.711481094360352
Loss :  1.72796630859375 3.1021878719329834 4.8301544189453125
Loss :  1.7171869277954102 2.950326442718506 4.667513370513916
Loss :  1.7250999212265015 3.899528741836548 5.62462854385376
Loss :  1.7155241966247559 3.1250624656677246 4.8405866622924805
Loss :  1.7084791660308838 3.1562278270721436 4.864706993103027
Loss :  1.7197810411453247 3.3469769954681396 5.066758155822754
Loss :  1.734528660774231 3.5443174839019775 5.278846263885498
Loss :  1.716239094734192 3.188413143157959 4.904652118682861
Loss :  1.7175966501235962 2.379519462585449 4.097115993499756
Loss :  1.7282851934432983 2.578435182571411 4.30672025680542
Loss :  1.7278364896774292 2.854015588760376 4.581851959228516
Loss :  1.7240432500839233 2.690385103225708 4.414428234100342
Loss :  1.7274483442306519 3.036851167678833 4.764299392700195
Loss :  1.7327337265014648 2.6942479610443115 4.4269819259643555
  batch 60 loss: 1.7327337265014648, 2.6942479610443115, 4.4269819259643555
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7285377979278564 2.715909004211426 4.444446563720703
Loss :  1.7289185523986816 2.905545473098755 4.634464263916016
Loss :  1.7331799268722534 2.5855789184570312 4.318758964538574
Loss :  1.7171459197998047 3.1464850902557373 4.863631248474121
Loss :  1.713212013244629 2.442880392074585 4.156092643737793
Loss :  1.5378329753875732 3.7052159309387207 5.243048667907715
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.55866539478302 3.691643476486206 5.250308990478516
Loss :  1.5681328773498535 3.5485377311706543 5.116670608520508
Loss :  1.6159859895706177 3.54550838470459 5.161494255065918
Total LOSS train 4.633848278339093 valid 5.192880630493164
CE LOSS train 1.7262742574398333 valid 0.4039964973926544
Contrastive LOSS train 2.907574000725379 valid 0.8863770961761475
EPOCH 102:
Loss :  1.7212481498718262 2.9534051418304443 4.674653053283691
Loss :  1.7283117771148682 2.9207475185394287 4.649059295654297
Loss :  1.7185524702072144 3.3512375354766846 5.069789886474609
Loss :  1.7215689420700073 2.881380796432495 4.602949619293213
Loss :  1.7284541130065918 2.6582694053649902 4.386723518371582
Loss :  1.7312202453613281 2.780367374420166 4.511587619781494
Loss :  1.726710319519043 3.3764355182647705 5.103145599365234
Loss :  1.7262630462646484 3.992509365081787 5.7187724113464355
Loss :  1.7250757217407227 2.920081853866577 4.645157814025879
Loss :  1.7020152807235718 3.141684055328369 4.8436994552612305
Loss :  1.7270278930664062 3.178514242172241 4.905542373657227
Loss :  1.7457035779953003 2.8683559894561768 4.6140594482421875
Loss :  1.7266349792480469 3.255255937576294 4.981890678405762
Loss :  1.722535490989685 3.121936798095703 4.844472408294678
Loss :  1.7258875370025635 2.9675180912017822 4.693405628204346
Loss :  1.7123956680297852 2.9112534523010254 4.6236491203308105
Loss :  1.7302824258804321 2.915966033935547 4.6462483406066895
Loss :  1.7192250490188599 2.6961655616760254 4.415390491485596
Loss :  1.7194832563400269 2.7840821743011475 4.503565311431885
Loss :  1.7198179960250854 2.4808573722839355 4.2006754875183105
  batch 20 loss: 1.7198179960250854, 2.4808573722839355, 4.2006754875183105
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.722725510597229 2.7321536540985107 4.454879283905029
Loss :  1.7278558015823364 3.1069869995117188 4.834842681884766
Loss :  1.7248716354370117 2.986192226409912 4.711063861846924
Loss :  1.734420895576477 2.8701908588409424 4.604611873626709
Loss :  1.7169229984283447 3.2099125385284424 4.926835536956787
Loss :  1.7211707830429077 3.3741462230682373 5.0953168869018555
Loss :  1.7361254692077637 3.777871608734131 5.5139970779418945
Loss :  1.7210338115692139 3.198225498199463 4.919259071350098
Loss :  1.732998251914978 3.3903353214263916 5.12333345413208
Loss :  1.7169983386993408 2.9547035694122314 4.671701908111572
Loss :  1.736769199371338 2.5465776920318604 4.283347129821777
Loss :  1.7260217666625977 2.369600534439087 4.0956220626831055
Loss :  1.717594861984253 2.859905242919922 4.577500343322754
Loss :  1.7140204906463623 2.2148895263671875 3.92891001701355
Loss :  1.7220340967178345 2.786499500274658 4.508533477783203
Loss :  1.7276616096496582 3.245788097381592 4.97344970703125
Loss :  1.7242151498794556 2.352924346923828 4.077139377593994
Loss :  1.717930555343628 2.129870653152466 3.8478012084960938
Loss :  1.720924735069275 2.9470136165618896 4.667938232421875
Loss :  1.721405029296875 2.0994656085968018 3.8208706378936768
  batch 40 loss: 1.721405029296875, 2.0994656085968018, 3.8208706378936768
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7224335670471191 2.4650754928588867 4.187509059906006
Loss :  1.7142525911331177 2.274763345718384 3.989016056060791
Loss :  1.7124308347702026 2.6783065795898438 4.390737533569336
Loss :  1.7200993299484253 2.034980058670044 3.7550792694091797
Loss :  1.7204413414001465 2.1092100143432617 3.829651355743408
Loss :  1.7183347940444946 2.677882194519043 4.396216869354248
Loss :  1.7266597747802734 2.4094157218933105 4.136075496673584
Loss :  1.7145938873291016 2.6322665214538574 4.346860408782959
Loss :  1.7279412746429443 2.636845111846924 4.364786148071289
Loss :  1.7115329504013062 3.202012300491333 4.91354513168335
Loss :  1.7080870866775513 3.32108211517334 5.029169082641602
Loss :  1.7165647745132446 2.7235913276672363 4.440155982971191
Loss :  1.7356666326522827 2.6149251461029053 4.350591659545898
Loss :  1.712231993675232 2.561886787414551 4.274118900299072
Loss :  1.7157597541809082 3.722031593322754 5.437791347503662
Loss :  1.7261899709701538 2.555166006088257 4.281355857849121
Loss :  1.727591633796692 2.893242359161377 4.620833873748779
Loss :  1.7222399711608887 2.8124799728393555 4.534719944000244
Loss :  1.724611759185791 3.0468316078186035 4.7714433670043945
Loss :  1.7288086414337158 2.754573106765747 4.483381748199463
  batch 60 loss: 1.7288086414337158, 2.754573106765747, 4.483381748199463
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7268733978271484 2.5118212699890137 4.238694667816162
Loss :  1.7254304885864258 2.8079564571380615 4.533387184143066
Loss :  1.7285699844360352 2.7440457344055176 4.472615718841553
Loss :  1.714164137840271 3.13218092918396 4.846344947814941
Loss :  1.7093788385391235 2.785433530807495 4.494812488555908
Loss :  1.5435783863067627 3.535738468170166 5.079317092895508
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.5441375970840454 3.5323359966278076 5.076473712921143
Loss :  1.5794705152511597 3.287996768951416 4.867467403411865
Loss :  1.6389760971069336 3.406665563583374 5.045641899108887
Total LOSS train 4.575235161414513 valid 5.017225027084351
CE LOSS train 1.7226616052480845 valid 0.4097440242767334
Contrastive LOSS train 2.8525735818422757 valid 0.8516663908958435
Saved best model. Old loss 5.0551276206970215 and new best loss 5.017225027084351
EPOCH 103:
Loss :  1.7168846130371094 2.644070625305176 4.360955238342285
Loss :  1.7249178886413574 3.6850197315216064 5.409937858581543
Loss :  1.7135432958602905 3.1324360370635986 4.8459792137146
Loss :  1.7154159545898438 3.0505056381225586 4.765921592712402
Loss :  1.722988486289978 2.1997170448303223 3.92270565032959
Loss :  1.7249728441238403 2.775660514831543 4.500633239746094
Loss :  1.7221283912658691 2.7290945053100586 4.451222896575928
Loss :  1.7223138809204102 2.574812650680542 4.297126770019531
Loss :  1.7205616235733032 2.3572115898132324 4.077773094177246
Loss :  1.6971505880355835 2.224675178527832 3.921825885772705
Loss :  1.722817301750183 3.4407336711883545 5.163550853729248
Loss :  1.7405657768249512 2.745128870010376 4.485694885253906
Loss :  1.7216136455535889 2.784108877182007 4.505722522735596
Loss :  1.7202703952789307 2.3342134952545166 4.054483890533447
Loss :  1.726746916770935 2.623192548751831 4.349939346313477
Loss :  1.7067314386367798 2.4410672187805176 4.147798538208008
Loss :  1.7246237993240356 1.997070550918579 3.7216944694519043
Loss :  1.7141051292419434 2.403719902038574 4.117825031280518
Loss :  1.7144107818603516 2.349827527999878 4.064238548278809
Loss :  1.7167736291885376 2.7152349948883057 4.432008743286133
  batch 20 loss: 1.7167736291885376, 2.7152349948883057, 4.432008743286133
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7179919481277466 2.3011999130249023 4.019191741943359
Loss :  1.7265150547027588 2.2618520259857178 3.9883670806884766
Loss :  1.7191954851150513 2.6130244731903076 4.332220077514648
Loss :  1.7295417785644531 2.967938184738159 4.697480201721191
Loss :  1.7155958414077759 4.091032981872559 5.806628704071045
Loss :  1.7163147926330566 2.7507052421569824 4.467020034790039
Loss :  1.7299392223358154 2.7464559078216553 4.476395130157471
Loss :  1.7177255153656006 2.4099056720733643 4.127631187438965
Loss :  1.7282323837280273 2.634674310684204 4.362906455993652
Loss :  1.7141838073730469 2.880807638168335 4.594991683959961
Loss :  1.7344390153884888 2.888324022293091 4.622763156890869
Loss :  1.7205396890640259 2.4944067001342773 4.214946269989014
Loss :  1.7135473489761353 2.160835027694702 3.874382495880127
Loss :  1.7115923166275024 2.385105609893799 4.096697807312012
Loss :  1.7204774618148804 2.683119535446167 4.403596878051758
Loss :  1.7244014739990234 2.7980051040649414 4.522406578063965
Loss :  1.720727562904358 2.841993570327759 4.562721252441406
Loss :  1.7120420932769775 2.7075181007385254 4.419560432434082
Loss :  1.7205331325531006 2.4639151096343994 4.1844482421875
Loss :  1.7209786176681519 2.6771562099456787 4.398134708404541
  batch 40 loss: 1.7209786176681519, 2.6771562099456787, 4.398134708404541
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.721451759338379 2.477142095565796 4.198594093322754
Loss :  1.7115846872329712 2.503344774246216 4.214929580688477
Loss :  1.7116014957427979 2.1665430068969727 3.8781445026397705
Loss :  1.717578411102295 2.0808379650115967 3.7984163761138916
Loss :  1.7177293300628662 1.9423335790634155 3.660062789916992
Loss :  1.7171411514282227 2.270890474319458 3.9880316257476807
Loss :  1.724002480506897 2.421783447265625 4.145785808563232
Loss :  1.7119075059890747 2.647681713104248 4.359589099884033
Loss :  1.725690484046936 3.494753122329712 5.2204437255859375
Loss :  1.7101428508758545 3.3209376335144043 5.03108024597168
Loss :  1.7068442106246948 3.4749348163604736 5.181778907775879
Loss :  1.7153503894805908 2.9818358421325684 4.697186470031738
Loss :  1.7344324588775635 2.5026369094848633 4.237069129943848
Loss :  1.7116363048553467 2.686694860458374 4.398331165313721
Loss :  1.7148231267929077 2.7746617794036865 4.489484786987305
Loss :  1.7259864807128906 2.5576488971710205 4.283635139465332
Loss :  1.7270587682724 2.365853786468506 4.092912673950195
Loss :  1.720861792564392 2.7712113857269287 4.492073059082031
Loss :  1.72438645362854 3.3029520511627197 5.02733850479126
Loss :  1.7283254861831665 2.6919097900390625 4.4202351570129395
  batch 60 loss: 1.7283254861831665, 2.6919097900390625, 4.4202351570129395
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.726595163345337 2.8288776874542236 4.5554728507995605
Loss :  1.7267591953277588 2.965257167816162 4.6920166015625
Loss :  1.730459213256836 2.195674419403076 3.926133632659912
Loss :  1.716131567955017 2.117961883544922 3.8340935707092285
Loss :  1.7112348079681396 1.5903438329696655 3.3015785217285156
Loss :  1.5099644660949707 3.6917293071746826 5.201693534851074
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.532318353652954 3.634599447250366 5.16691780090332
Loss :  1.563151240348816 3.4487757682800293 5.011927127838135
Loss :  1.6153054237365723 3.5336339473724365 5.14893913269043
Total LOSS train 4.3675991755265455 valid 5.13236939907074
CE LOSS train 1.7199040999779334 valid 0.40382635593414307
Contrastive LOSS train 2.6476950682126557 valid 0.8834084868431091
EPOCH 104:
Loss :  1.718083381652832 2.6851203441619873 4.403203964233398
Loss :  1.725712776184082 2.4375572204589844 4.163269996643066
Loss :  1.7146611213684082 1.9567290544509888 3.6713900566101074
Loss :  1.717053771018982 2.251014471054077 3.9680681228637695
Loss :  1.724332571029663 2.239567518234253 3.963900089263916
Loss :  1.726156234741211 2.2675251960754395 3.9936814308166504
Loss :  1.7216087579727173 2.130232572555542 3.851841449737549
Loss :  1.7219756841659546 1.9949710369110107 3.716946601867676
Loss :  1.721976637840271 2.5765786170959473 4.298555374145508
Loss :  1.698547124862671 2.6321842670440674 4.330731391906738
Loss :  1.7232177257537842 2.982116460800171 4.705334186553955
Loss :  1.7410807609558105 2.612506151199341 4.3535871505737305
Loss :  1.721065640449524 2.557680606842041 4.278746128082275
Loss :  1.720755696296692 3.3237009048461914 5.044456481933594
Loss :  1.7248717546463013 3.2543184757232666 4.979190349578857
Loss :  1.710270881652832 3.589003324508667 5.299274444580078
Loss :  1.7264118194580078 2.7752878665924072 4.501699447631836
Loss :  1.715747594833374 3.1015865802764893 4.817334175109863
Loss :  1.7150909900665283 2.787872076034546 4.502963066101074
Loss :  1.7188234329223633 3.4589805603027344 5.177803993225098
  batch 20 loss: 1.7188234329223633, 3.4589805603027344, 5.177803993225098
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7208456993103027 2.105328321456909 3.826174020767212
Loss :  1.7262358665466309 2.6390881538391113 4.365324020385742
Loss :  1.7223197221755981 2.462080955505371 4.18440055847168
Loss :  1.729442834854126 3.2727410793304443 5.00218391418457
Loss :  1.7168306112289429 3.036487579345703 4.7533183097839355
Loss :  1.7177417278289795 2.5412304401397705 4.25897216796875
Loss :  1.7307723760604858 2.6738288402557373 4.404601097106934
Loss :  1.7193591594696045 2.456219434738159 4.175578594207764
Loss :  1.730043888092041 2.575786828994751 4.305830955505371
Loss :  1.7125954627990723 2.6795663833618164 4.392161846160889
Loss :  1.7340151071548462 2.5870726108551025 4.321087837219238
Loss :  1.7209043502807617 2.2730116844177246 3.9939160346984863
Loss :  1.7134519815444946 2.9471921920776367 4.660644054412842
Loss :  1.7115377187728882 2.70707368850708 4.418611526489258
Loss :  1.7203547954559326 2.4358036518096924 4.156158447265625
Loss :  1.724644660949707 2.6706740856170654 4.395318984985352
Loss :  1.7216286659240723 2.424755811691284 4.146384239196777
Loss :  1.7138476371765137 2.5658223628997803 4.279669761657715
Loss :  1.7203444242477417 2.6745054721832275 4.39484977722168
Loss :  1.7218871116638184 2.8374807834625244 4.559368133544922
  batch 40 loss: 1.7218871116638184, 2.8374807834625244, 4.559368133544922
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7217026948928833 3.140507936477661 4.862210750579834
Loss :  1.7109661102294922 3.7788474559783936 5.489813804626465
Loss :  1.7112797498703003 2.5437471866607666 4.255026817321777
Loss :  1.717315912246704 2.836057424545288 4.553373336791992
Loss :  1.7181990146636963 2.56891131401062 4.287110328674316
Loss :  1.717090129852295 2.2129855155944824 3.9300756454467773
Loss :  1.7240378856658936 2.853410005569458 4.577447891235352
Loss :  1.711767554283142 2.954035520553589 4.665802955627441
Loss :  1.7247503995895386 3.1093077659606934 4.8340582847595215
Loss :  1.7089180946350098 3.336798667907715 5.045716762542725
Loss :  1.7056219577789307 2.7148633003234863 4.420485496520996
Loss :  1.7136023044586182 2.8277251720428467 4.541327476501465
Loss :  1.7318917512893677 3.8374807834625244 5.569372653961182
Loss :  1.7097638845443726 2.744323492050171 4.454087257385254
Loss :  1.7126067876815796 3.472815990447998 5.185422897338867
Loss :  1.7223020792007446 3.006516933441162 4.728818893432617
Loss :  1.725115418434143 3.8054728507995605 5.530588150024414
Loss :  1.719172477722168 3.0760388374328613 4.795211315155029
Loss :  1.722319483757019 3.0543479919433594 4.776667594909668
Loss :  1.725622534751892 2.3203506469726562 4.045973300933838
  batch 60 loss: 1.725622534751892, 2.3203506469726562, 4.045973300933838
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.724012851715088 2.8548226356506348 4.578835487365723
Loss :  1.7237448692321777 2.5846364498138428 4.308381080627441
Loss :  1.727320909500122 2.360443592071533 4.087764739990234
Loss :  1.7137008905410767 2.3903443813323975 4.104045391082764
Loss :  1.7093440294265747 1.879953145980835 3.589297294616699
Loss :  1.5526562929153442 3.838102102279663 5.390758514404297
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.5667396783828735 3.794487237930298 5.361227035522461
Loss :  1.6006397008895874 3.6106011867523193 5.211240768432617
Loss :  1.6463433504104614 3.5432589054107666 5.189602375030518
Total LOSS train 4.4651299660022445 valid 5.288207173347473
CE LOSS train 1.7198218143903292 valid 0.41158583760261536
Contrastive LOSS train 2.745308133272024 valid 0.8858147263526917
EPOCH 105:
Loss :  1.716898798942566 2.3550381660461426 4.071937084197998
Loss :  1.7237935066223145 2.420807361602783 4.144600868225098
Loss :  1.7143478393554688 2.565438747406006 4.279786586761475
Loss :  1.716370701789856 3.353316307067871 5.0696868896484375
Loss :  1.7249984741210938 3.126575469970703 4.851573944091797
Loss :  1.7260392904281616 2.670844554901123 4.396883964538574
Loss :  1.7233927249908447 2.656933546066284 4.380326271057129
Loss :  1.723605990409851 2.816077709197998 4.539683818817139
Loss :  1.722299575805664 2.974971294403076 4.69727087020874
Loss :  1.6998196840286255 2.6695666313171387 4.369386196136475
Loss :  1.7233953475952148 2.6047427654266357 4.32813835144043
Loss :  1.7409625053405762 2.7397921085357666 4.480754852294922
Loss :  1.7211685180664062 3.3295512199401855 5.050719738006592
Loss :  1.7211790084838867 2.8974978923797607 4.618677139282227
Loss :  1.7236571311950684 2.9321446418762207 4.655801773071289
Loss :  1.7087421417236328 2.458740711212158 4.167482852935791
Loss :  1.7252012491226196 2.324096441268921 4.04929780960083
Loss :  1.7156755924224854 2.697944402694702 4.4136199951171875
Loss :  1.7146053314208984 2.0276296138763428 3.742234945297241
Loss :  1.7157989740371704 2.1159508228302 3.83174991607666
  batch 20 loss: 1.7157989740371704, 2.1159508228302, 3.83174991607666
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7197654247283936 2.2151150703430176 3.934880495071411
Loss :  1.726060390472412 2.9182679653167725 4.6443281173706055
Loss :  1.7225346565246582 2.5816636085510254 4.304198265075684
Loss :  1.7314114570617676 3.1050338745117188 4.836445331573486
Loss :  1.7169429063796997 2.8565750122070312 4.573517799377441
Loss :  1.7191965579986572 2.988913059234619 4.7081098556518555
Loss :  1.7317817211151123 3.444669485092163 5.176451206207275
Loss :  1.7199956178665161 2.6604788303375244 4.38047456741333
Loss :  1.731321096420288 2.715092897415161 4.446413993835449
Loss :  1.7131837606430054 2.764054775238037 4.477238655090332
Loss :  1.735974907875061 2.567568302154541 4.3035430908203125
Loss :  1.723313570022583 2.581773281097412 4.305087089538574
Loss :  1.71511709690094 3.139275550842285 4.8543925285339355
Loss :  1.7128808498382568 3.3956868648529053 5.108567714691162
Loss :  1.7214268445968628 3.5996792316436768 5.32110595703125
Loss :  1.7260416746139526 3.298821449279785 5.024863243103027
Loss :  1.72267746925354 2.675536632537842 4.398214340209961
Loss :  1.7149401903152466 2.0129194259643555 3.7278594970703125
Loss :  1.721449375152588 2.853724956512451 4.575174331665039
Loss :  1.722275733947754 2.6772241592407227 4.399499893188477
  batch 40 loss: 1.722275733947754, 2.6772241592407227, 4.399499893188477
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7216624021530151 2.5904641151428223 4.312126636505127
Loss :  1.7111555337905884 1.8602455854415894 3.5714011192321777
Loss :  1.7109614610671997 2.6860153675079346 4.396976947784424
Loss :  1.7165801525115967 2.773206949234009 4.4897871017456055
Loss :  1.7171999216079712 1.8574316501617432 3.574631690979004
Loss :  1.7165578603744507 2.2994275093078613 4.015985488891602
Loss :  1.723266363143921 3.073500156402588 4.79676628112793
Loss :  1.7111924886703491 3.2921533584594727 5.003345966339111
Loss :  1.727056860923767 3.348569393157959 5.075626373291016
Loss :  1.7082090377807617 3.325176954269409 5.03338623046875
Loss :  1.7059016227722168 3.5830976963043213 5.288999557495117
Loss :  1.7152984142303467 2.806262254714966 4.5215606689453125
Loss :  1.733831524848938 3.2214114665985107 4.955243110656738
Loss :  1.7104089260101318 3.0204250812530518 4.730834007263184
Loss :  1.7133599519729614 2.9068562984466553 4.620216369628906
Loss :  1.7238680124282837 2.586961269378662 4.310829162597656
Loss :  1.7273755073547363 3.3666088581085205 5.093984603881836
Loss :  1.7212183475494385 2.7725942134857178 4.493812561035156
Loss :  1.725059986114502 2.8403892517089844 4.565449237823486
Loss :  1.7294896841049194 1.941574215888977 3.6710638999938965
  batch 60 loss: 1.7294896841049194, 1.941574215888977, 3.6710638999938965
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.726138710975647 2.7084314823150635 4.4345703125
Loss :  1.7251964807510376 2.3281846046447754 4.053380966186523
Loss :  1.7299669981002808 2.8836324214935303 4.6135993003845215
Loss :  1.7158366441726685 2.5536012649536133 4.269437789916992
Loss :  1.7110471725463867 1.7265101671218872 3.4375572204589844
Loss :  1.544711947441101 3.752887725830078 5.297599792480469
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.5607842206954956 3.731516122817993 5.292300224304199
Loss :  1.5883898735046387 3.5388128757476807 5.127202987670898
Loss :  1.6171468496322632 3.6372900009155273 5.25443696975708
Total LOSS train 4.476470037607046 valid 5.242884993553162
CE LOSS train 1.720432057747474 valid 0.4042867124080658
Contrastive LOSS train 2.7560379450137797 valid 0.9093225002288818
EPOCH 106:
Loss :  1.71851646900177 2.45177960395813 4.1702961921691895
Loss :  1.72446870803833 2.344137668609619 4.068606376647949
Loss :  1.7147873640060425 2.394625425338745 4.109412670135498
Loss :  1.7166837453842163 2.3776400089263916 4.094323635101318
Loss :  1.7257885932922363 2.3527793884277344 4.078567981719971
Loss :  1.7259509563446045 2.539496898651123 4.265447616577148
Loss :  1.7230416536331177 2.458874225616455 4.181915760040283
Loss :  1.7227723598480225 2.739776134490967 4.46254825592041
Loss :  1.7222379446029663 2.769709587097168 4.491947650909424
Loss :  1.6997548341751099 3.131206750869751 4.83096170425415
Loss :  1.722651481628418 3.418931007385254 5.141582489013672
Loss :  1.7414419651031494 3.5152885913848877 5.256730556488037
Loss :  1.72139573097229 3.49554181098938 5.21693754196167
Loss :  1.7212932109832764 3.1363489627838135 4.85764217376709
Loss :  1.7247505187988281 2.2385261058807373 3.9632766246795654
Loss :  1.7093987464904785 2.582221508026123 4.291620254516602
Loss :  1.726041316986084 2.9470794200897217 4.673120498657227
Loss :  1.7160898447036743 3.563875436782837 5.279965400695801
Loss :  1.7147823572158813 2.7767131328582764 4.491495609283447
Loss :  1.7156568765640259 2.7835500240325928 4.499207019805908
  batch 20 loss: 1.7156568765640259, 2.7835500240325928, 4.499207019805908
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7192586660385132 2.726071834564209 4.445330619812012
Loss :  1.7255009412765503 2.7661287784576416 4.491629600524902
Loss :  1.7221037149429321 2.7423949241638184 4.464498519897461
Loss :  1.73209547996521 2.7554125785827637 4.4875078201293945
Loss :  1.7157593965530396 2.737725257873535 4.453484535217285
Loss :  1.7179100513458252 3.4580230712890625 5.175932884216309
Loss :  1.7321581840515137 2.6007730960845947 4.3329315185546875
Loss :  1.7202352285385132 3.214803457260132 4.9350385665893555
Loss :  1.731040596961975 2.414062261581421 4.1451029777526855
Loss :  1.7117564678192139 2.537309169769287 4.249065399169922
Loss :  1.7344348430633545 2.4485924243927 4.183027267456055
Loss :  1.722504734992981 2.7505133152008057 4.473018169403076
Loss :  1.7132333517074585 2.34926176071167 4.062495231628418
Loss :  1.7115309238433838 2.7676503658294678 4.479181289672852
Loss :  1.7203567028045654 2.852301597595215 4.572658538818359
Loss :  1.7250832319259644 3.082854747772217 4.807938098907471
Loss :  1.722395658493042 2.623814582824707 4.346210479736328
Loss :  1.7154691219329834 2.5483593940734863 4.263828277587891
Loss :  1.721331000328064 3.0556728839874268 4.777003765106201
Loss :  1.7224043607711792 3.192408800125122 4.914813041687012
  batch 40 loss: 1.7224043607711792, 3.192408800125122, 4.914813041687012
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7224962711334229 3.355689764022827 5.07818603515625
Loss :  1.7114207744598389 2.4736263751983643 4.185047149658203
Loss :  1.7112807035446167 2.1405341625213623 3.8518147468566895
Loss :  1.717671275138855 2.1893391609191895 3.907010555267334
Loss :  1.7182475328445435 2.044074058532715 3.7623214721679688
Loss :  1.7179837226867676 2.5535924434661865 4.271575927734375
Loss :  1.7250012159347534 2.0484821796417236 3.7734832763671875
Loss :  1.712807536125183 2.1472373008728027 3.8600449562072754
Loss :  1.7280129194259644 3.0814168453216553 4.80942964553833
Loss :  1.7096059322357178 3.571937084197998 5.281542778015137
Loss :  1.706199049949646 3.3222577571868896 5.028456687927246
Loss :  1.7149710655212402 2.91204833984375 4.62701940536499
Loss :  1.7339255809783936 2.7805356979370117 4.514461517333984
Loss :  1.7120212316513062 2.4263789653778076 4.138400077819824
Loss :  1.7132568359375 2.234349489212036 3.947606325149536
Loss :  1.7240935564041138 2.6236109733581543 4.3477044105529785
Loss :  1.7255264520645142 2.6161844730377197 4.341711044311523
Loss :  1.7188374996185303 2.434389352798462 4.153226852416992
Loss :  1.723890781402588 3.6228179931640625 5.34670877456665
Loss :  1.728502631187439 3.499608278274536 5.2281107902526855
  batch 60 loss: 1.728502631187439, 3.499608278274536, 5.2281107902526855
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7249811887741089 3.624800205230713 5.349781513214111
Loss :  1.7250374555587769 3.5563015937805176 5.281339168548584
Loss :  1.7296632528305054 3.576798915863037 5.306462287902832
Loss :  1.7145811319351196 3.443146228790283 5.157727241516113
Loss :  1.7098230123519897 3.2303435802459717 4.940166473388672
Loss :  1.4854352474212646 3.882309913635254 5.367745399475098
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.4981589317321777 3.8320343494415283 5.330193519592285
Loss :  1.5468263626098633 3.617696523666382 5.164523124694824
Loss :  1.5439317226409912 3.658313512802124 5.202245235443115
Total LOSS train 4.538086795806885 valid 5.266176819801331
CE LOSS train 1.7203677837665265 valid 0.3859829306602478
Contrastive LOSS train 2.817719034048227 valid 0.914578378200531
EPOCH 107:
Loss :  1.7178577184677124 1.9865736961364746 3.7044315338134766
Loss :  1.7253175973892212 2.4009459018707275 4.126263618469238
Loss :  1.7147334814071655 1.956229567527771 3.6709630489349365
Loss :  1.7161027193069458 1.9468414783477783 3.6629443168640137
Loss :  1.7265284061431885 2.7133944034576416 4.43992280960083
Loss :  1.726871132850647 2.3824963569641113 4.109367370605469
Loss :  1.7241756916046143 3.102905035018921 4.827080726623535
Loss :  1.7239487171173096 3.369321584701538 5.093270301818848
Loss :  1.7218270301818848 2.614116907119751 4.335944175720215
Loss :  1.6995584964752197 2.609703540802002 4.309262275695801
Loss :  1.7234357595443726 3.388392448425293 5.111828327178955
Loss :  1.7412418127059937 3.3714709281921387 5.112712860107422
Loss :  1.7217062711715698 2.591484546661377 4.313190937042236
Loss :  1.720927119255066 3.460179328918457 5.1811065673828125
Loss :  1.7223644256591797 3.234096050262451 4.956460475921631
Loss :  1.7090356349945068 2.6592187881469727 4.368254661560059
Loss :  1.7241147756576538 2.179126024246216 3.90324068069458
Loss :  1.7155946493148804 3.0274670124053955 4.743061542510986
Loss :  1.7137296199798584 1.85627019405365 3.5699996948242188
Loss :  1.7136482000350952 1.952684998512268 3.6663331985473633
  batch 20 loss: 1.7136482000350952, 1.952684998512268, 3.6663331985473633
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.719136118888855 2.1697661876678467 3.888902187347412
Loss :  1.724733829498291 2.032128095626831 3.756861925125122
Loss :  1.7218208312988281 1.8703175783157349 3.5921382904052734
Loss :  1.7327995300292969 2.28973650932312 4.022536277770996
Loss :  1.7155377864837646 2.8388679027557373 4.554405689239502
Loss :  1.7176811695098877 2.702834129333496 4.420515060424805
Loss :  1.7323635816574097 3.2175190448760986 4.949882507324219
Loss :  1.7214571237564087 2.407646417617798 4.129103660583496
Loss :  1.7320411205291748 2.5635948181152344 4.295636177062988
Loss :  1.7131115198135376 2.7204928398132324 4.4336042404174805
Loss :  1.7341915369033813 2.4481332302093506 4.1823248863220215
Loss :  1.7256091833114624 3.407649517059326 5.133258819580078
Loss :  1.7164102792739868 2.857802629470825 4.574213027954102
Loss :  1.7125918865203857 2.3460843563079834 4.058676242828369
Loss :  1.7218830585479736 2.7546770572662354 4.476560115814209
Loss :  1.7262128591537476 2.795048236846924 4.521261215209961
Loss :  1.723218560218811 2.665911912918091 4.389130592346191
Loss :  1.7188432216644287 2.5862345695495605 4.30507755279541
Loss :  1.722032904624939 2.726543426513672 4.4485764503479
Loss :  1.723941445350647 3.102004051208496 4.8259453773498535
  batch 40 loss: 1.723941445350647, 3.102004051208496, 4.8259453773498535
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7236566543579102 2.770648241043091 4.494304656982422
Loss :  1.7119704484939575 2.641629695892334 4.353600025177002
Loss :  1.7119834423065186 2.954939603805542 4.6669230461120605
Loss :  1.7182729244232178 3.0429563522338867 4.761229515075684
Loss :  1.7193771600723267 2.8671090602874756 4.586486339569092
Loss :  1.7192609310150146 3.4578003883361816 5.177061080932617
Loss :  1.7261180877685547 2.3706400394439697 4.096757888793945
Loss :  1.712735891342163 2.3931078910827637 4.105843544006348
Loss :  1.728585958480835 2.575688123703003 4.304274082183838
Loss :  1.7087031602859497 1.9411085844039917 3.6498117446899414
Loss :  1.7057483196258545 2.184361696243286 3.8901100158691406
Loss :  1.7159010171890259 2.7819292545318604 4.497830390930176
Loss :  1.7343697547912598 2.2437903881073 3.9781601428985596
Loss :  1.7088279724121094 2.6329548358917236 4.341782569885254
Loss :  1.7118991613388062 2.513108253479004 4.2250075340271
Loss :  1.725321888923645 2.1194605827331543 3.8447823524475098
Loss :  1.7266772985458374 2.68428111076355 4.410958290100098
Loss :  1.7193453311920166 3.0012292861938477 4.720574378967285
Loss :  1.7246041297912598 3.4555516242980957 5.1801557540893555
Loss :  1.7291171550750732 2.793637752532959 4.522754669189453
  batch 60 loss: 1.7291171550750732, 2.793637752532959, 4.522754669189453
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7271095514297485 2.8012425899505615 4.5283522605896
Loss :  1.7264872789382935 3.3436944484710693 5.070181846618652
Loss :  1.732591152191162 2.768280267715454 4.500871658325195
Loss :  1.7168554067611694 3.089352607727051 4.80620813369751
Loss :  1.7120355367660522 1.8775838613510132 3.5896193981170654
Loss :  1.621902585029602 3.620999574661255 5.2429022789001465
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6129186153411865 3.4213993549346924 5.034317970275879
Loss :  1.6582083702087402 3.318638801574707 4.976847171783447
Loss :  1.6881153583526611 3.209577798843384 4.897693157196045
Total LOSS train 4.376429088299091 valid 5.037940144538879
CE LOSS train 1.7208598833817703 valid 0.4220288395881653
Contrastive LOSS train 2.6555691975813644 valid 0.802394449710846
EPOCH 108:
Loss :  1.720762848854065 2.2637453079223633 3.9845080375671387
Loss :  1.7288297414779663 2.8937323093414307 4.622561931610107
Loss :  1.7166590690612793 2.4859824180603027 4.202641487121582
Loss :  1.7167003154754639 2.7705228328704834 4.487223148345947
Loss :  1.728851556777954 3.854451894760132 5.583303451538086
Loss :  1.7278107404708862 3.0570497512817383 4.784860610961914
Loss :  1.727220892906189 3.1673150062561035 4.894536018371582
Loss :  1.7256574630737305 2.5740914344787598 4.29974889755249
Loss :  1.7231332063674927 2.047841787338257 3.770975112915039
Loss :  1.7031675577163696 2.115915298461914 3.819082736968994
Loss :  1.7240769863128662 2.0570266246795654 3.7811036109924316
Loss :  1.740971565246582 2.3139047622680664 4.054876327514648
Loss :  1.7222315073013306 2.2033016681671143 3.9255332946777344
Loss :  1.7230956554412842 2.9088315963745117 4.631927490234375
Loss :  1.7251560688018799 2.8233375549316406 4.548493385314941
Loss :  1.7103445529937744 2.443223714828491 4.153568267822266
Loss :  1.7256860733032227 2.260922908782959 3.9866089820861816
Loss :  1.7168327569961548 2.652275800704956 4.3691086769104
Loss :  1.7140036821365356 2.298130989074707 4.012134552001953
Loss :  1.7159919738769531 2.748828649520874 4.464820861816406
  batch 20 loss: 1.7159919738769531, 2.748828649520874, 4.464820861816406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7211172580718994 2.5213825702667236 4.242499828338623
Loss :  1.7266271114349365 3.281046152114868 5.007673263549805
Loss :  1.7236086130142212 2.0721395015716553 3.795748233795166
Loss :  1.7336252927780151 2.344266653060913 4.077891826629639
Loss :  1.7168561220169067 2.9478840827941895 4.664740085601807
Loss :  1.7196507453918457 2.613957405090332 4.333608150482178
Loss :  1.7328696250915527 2.300557851791382 4.0334272384643555
Loss :  1.723207950592041 2.913924217224121 4.637132167816162
Loss :  1.7337822914123535 2.8582239151000977 4.592006206512451
Loss :  1.7137036323547363 2.662816047668457 4.376519680023193
Loss :  1.7351492643356323 2.6411471366882324 4.376296520233154
Loss :  1.7260960340499878 2.741521120071411 4.467617034912109
Loss :  1.7171096801757812 2.911008358001709 4.62811803817749
Loss :  1.7135711908340454 2.7480216026306152 4.461592674255371
Loss :  1.7228703498840332 2.943922281265259 4.666792869567871
Loss :  1.7276179790496826 3.5826470851898193 5.310265064239502
Loss :  1.7248071432113647 3.2402076721191406 4.965014934539795
Loss :  1.719050645828247 2.284825325012207 4.003875732421875
Loss :  1.7227987051010132 2.088575601577759 3.8113741874694824
Loss :  1.7244493961334229 2.6458237171173096 4.370273113250732
  batch 40 loss: 1.7244493961334229, 2.6458237171173096, 4.370273113250732
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7235240936279297 2.5179715156555176 4.241495609283447
Loss :  1.7122019529342651 2.2567708492279053 3.968972682952881
Loss :  1.713191032409668 2.866939067840576 4.580130100250244
Loss :  1.7178641557693481 3.6541688442230225 5.37203311920166
Loss :  1.7203762531280518 3.4276134967803955 5.147989749908447
Loss :  1.7187719345092773 2.3606982231140137 4.079470157623291
Loss :  1.7245389223098755 3.1358439922332764 4.860383033752441
Loss :  1.7127264738082886 2.6413323879241943 4.354058742523193
Loss :  1.7262537479400635 2.8745579719543457 4.600811958312988
Loss :  1.7093777656555176 2.672701835632324 4.382079601287842
Loss :  1.7068307399749756 2.249056339263916 3.9558870792388916
Loss :  1.714896321296692 2.50115704536438 4.216053485870361
Loss :  1.7328808307647705 2.897444725036621 4.6303253173828125
Loss :  1.7106654644012451 2.4040443897247314 4.114709854125977
Loss :  1.7139860391616821 2.437459707260132 4.1514458656311035
Loss :  1.7224094867706299 2.8100461959838867 4.5324554443359375
Loss :  1.7267537117004395 2.9441187381744385 4.670872688293457
Loss :  1.7204859256744385 1.9895961284637451 3.7100820541381836
Loss :  1.7240737676620483 2.965848445892334 4.689922332763672
Loss :  1.7274059057235718 3.0215229988098145 4.748929023742676
  batch 60 loss: 1.7274059057235718, 3.0215229988098145, 4.748929023742676
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7251577377319336 2.7069809436798096 4.432138442993164
Loss :  1.724782943725586 3.5499699115753174 5.274752616882324
Loss :  1.7288395166397095 3.645960807800293 5.374800205230713
Loss :  1.7162137031555176 3.5299999713897705 5.246213912963867
Loss :  1.7118183374404907 2.7718505859375 4.483668804168701
Loss :  1.4952586889266968 3.361525535583496 4.856784343719482
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.5033525228500366 3.347856044769287 4.851208686828613
Loss :  1.5420172214508057 3.2365317344665527 4.7785491943359375
Loss :  1.5625141859054565 3.17038893699646 4.732903003692627
Total LOSS train 4.446457932545589 valid 4.804861307144165
CE LOSS train 1.7215658462964571 valid 0.39062854647636414
Contrastive LOSS train 2.7248920880831204 valid 0.792597234249115
Saved best model. Old loss 5.017225027084351 and new best loss 4.804861307144165
EPOCH 109:
Loss :  1.716875672340393 3.8072633743286133 5.524138927459717
Loss :  1.7235805988311768 3.670637369155884 5.3942179679870605
Loss :  1.7154572010040283 2.7593648433685303 4.474822044372559
Loss :  1.7164340019226074 2.1798832416534424 3.89631724357605
Loss :  1.7260652780532837 2.313594341278076 4.03965950012207
Loss :  1.725122094154358 2.1942434310913086 3.919365406036377
Loss :  1.7229238748550415 2.6201930046081543 4.343116760253906
Loss :  1.7223395109176636 2.0126912593841553 3.7350306510925293
Loss :  1.7219886779785156 2.7193105220794678 4.4412994384765625
Loss :  1.700130820274353 2.7264060974121094 4.426537036895752
Loss :  1.7218053340911865 2.6986911296844482 4.420496463775635
Loss :  1.7407861948013306 2.806309938430786 4.547096252441406
Loss :  1.7202765941619873 2.8844797611236572 4.6047563552856445
Loss :  1.7210206985473633 2.863539457321167 4.584560394287109
Loss :  1.7233080863952637 3.2340087890625 4.957316875457764
Loss :  1.709397315979004 3.584852695465088 5.294250011444092
Loss :  1.7244501113891602 3.196323871612549 4.920773983001709
Loss :  1.715804934501648 3.3226821422576904 5.038486957550049
Loss :  1.7149746417999268 2.5172717571258545 4.232246398925781
Loss :  1.7143008708953857 2.3280065059661865 4.042307376861572
  batch 20 loss: 1.7143008708953857, 2.3280065059661865, 4.042307376861572
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7191627025604248 2.311732769012451 4.030895233154297
Loss :  1.7256730794906616 2.836249828338623 4.561923027038574
Loss :  1.7211658954620361 3.7521185874938965 5.473284721374512
Loss :  1.732328176498413 2.8420300483703613 4.574357986450195
Loss :  1.7167338132858276 2.276637554168701 3.9933714866638184
Loss :  1.7184754610061646 2.9079244136810303 4.626399993896484
Loss :  1.731244444847107 3.197465181350708 4.928709506988525
Loss :  1.7191436290740967 2.98343825340271 4.702581882476807
Loss :  1.7303928136825562 3.0289690494537354 4.759361743927002
Loss :  1.7132318019866943 3.4972243309020996 5.210455894470215
Loss :  1.7343919277191162 3.4138762950897217 5.148268222808838
Loss :  1.722978115081787 2.7173547744750977 4.440332889556885
Loss :  1.7141872644424438 2.9221222400665283 4.636309623718262
Loss :  1.7116386890411377 2.6483047008514404 4.359943389892578
Loss :  1.7204320430755615 2.782789707183838 4.50322151184082
Loss :  1.725233554840088 2.4242923259735107 4.1495256423950195
Loss :  1.7220784425735474 2.5677216053009033 4.28980016708374
Loss :  1.7165099382400513 2.8084280490875244 4.524938106536865
Loss :  1.7206404209136963 3.0232160091400146 4.743856430053711
Loss :  1.7234385013580322 3.5490596294403076 5.27249813079834
  batch 40 loss: 1.7234385013580322, 3.5490596294403076, 5.27249813079834
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7230002880096436 3.455425500869751 5.1784257888793945
Loss :  1.7122743129730225 2.938293933868408 4.650568008422852
Loss :  1.7127989530563354 3.0686492919921875 4.7814483642578125
Loss :  1.7174361944198608 3.6620707511901855 5.379507064819336
Loss :  1.719413161277771 2.845567464828491 4.564980506896973
Loss :  1.7190440893173218 2.597378969192505 4.316422939300537
Loss :  1.7260013818740845 2.3877885341644287 4.113790035247803
Loss :  1.7126203775405884 2.106581449508667 3.819201946258545
Loss :  1.7294312715530396 2.3512513637542725 4.080682754516602
Loss :  1.7092792987823486 3.0252907276153564 4.734570026397705
Loss :  1.7065021991729736 3.3391828536987305 5.045684814453125
Loss :  1.7164205312728882 2.7351667881011963 4.451587200164795
Loss :  1.73433256149292 3.4465525150299072 5.180885314941406
Loss :  1.710404634475708 3.182029962539673 4.892434597015381
Loss :  1.7125028371810913 2.7099838256835938 4.422486782073975
Loss :  1.7244024276733398 2.7054667472839355 4.429869174957275
Loss :  1.7264468669891357 2.8569962978363037 4.5834431648254395
Loss :  1.7201898097991943 2.735089063644409 4.4552788734436035
Loss :  1.7240073680877686 2.6222100257873535 4.346217155456543
Loss :  1.7276091575622559 2.5569143295288086 4.2845234870910645
  batch 60 loss: 1.7276091575622559, 2.5569143295288086, 4.2845234870910645
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7259495258331299 2.3457181453704834 4.071667671203613
Loss :  1.7242484092712402 2.0824127197265625 3.8066611289978027
Loss :  1.729891300201416 2.296090841293335 4.025981903076172
Loss :  1.7149182558059692 2.1846160888671875 3.899534225463867
Loss :  1.7102199792861938 1.730066180229187 3.440286159515381
Loss :  1.5002142190933228 3.8268940448760986 5.327108383178711
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.5151236057281494 3.7874062061309814 5.302529811859131
Loss :  1.5588297843933105 3.5643818378448486 5.123211860656738
Loss :  1.5462822914123535 3.7146081924438477 5.260890483856201
Total LOSS train 4.534199549601628 valid 5.253435134887695
CE LOSS train 1.720391821861267 valid 0.3865705728530884
Contrastive LOSS train 2.8138077424122736 valid 0.9286520481109619
EPOCH 110:
Loss :  1.7169207334518433 2.819399118423462 4.536319732666016
Loss :  1.7243754863739014 2.5780117511749268 4.302387237548828
Loss :  1.7139918804168701 1.9022823572158813 3.616274356842041
Loss :  1.7150307893753052 2.241701602935791 3.9567322731018066
Loss :  1.7258471250534058 1.9890731573104858 3.7149202823638916
Loss :  1.7254371643066406 2.384249210357666 4.109686374664307
Loss :  1.7229958772659302 2.815002679824829 4.537998676300049
Loss :  1.7222790718078613 2.6162593364715576 4.33853816986084
Loss :  1.7213133573532104 2.7222607135772705 4.443573951721191
Loss :  1.7007153034210205 2.0881507396698 3.7888660430908203
Loss :  1.7229244709014893 3.006915807723999 4.729840278625488
Loss :  1.7402026653289795 3.8744468688964844 5.614649772644043
Loss :  1.7216676473617554 2.6613502502441406 4.3830180168151855
Loss :  1.7218540906906128 2.7204809188842773 4.44233512878418
Loss :  1.723799467086792 2.5102343559265137 4.234033584594727
Loss :  1.7107921838760376 2.5973024368286133 4.308094501495361
Loss :  1.7254183292388916 2.421294927597046 4.1467132568359375
Loss :  1.7182058095932007 2.5241687297821045 4.242374420166016
Loss :  1.7158828973770142 2.571082592010498 4.286965370178223
Loss :  1.7148760557174683 2.4404900074005127 4.155365943908691
  batch 20 loss: 1.7148760557174683, 2.4404900074005127, 4.155365943908691
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.720170259475708 2.1307380199432373 3.8509082794189453
Loss :  1.7263497114181519 2.7902591228485107 4.516608715057373
Loss :  1.7231380939483643 2.2995848655700684 4.022723197937012
Loss :  1.7325797080993652 2.49910569190979 4.231685638427734
Loss :  1.7182176113128662 2.8240559101104736 4.54227352142334
Loss :  1.7202844619750977 2.516146659851074 4.236431121826172
Loss :  1.7335319519042969 2.5226011276245117 4.256133079528809
Loss :  1.7212791442871094 2.4669735431671143 4.1882524490356445
Loss :  1.7320412397384644 1.957770586013794 3.6898117065429688
Loss :  1.7142431735992432 2.4322052001953125 4.146448135375977
Loss :  1.7363873720169067 2.4103286266326904 4.146716117858887
Loss :  1.7255173921585083 2.8832991123199463 4.608816623687744
Loss :  1.7163221836090088 2.875255823135376 4.591578006744385
Loss :  1.7141586542129517 2.7882235050201416 4.502382278442383
Loss :  1.7224916219711304 3.2501211166381836 4.9726128578186035
Loss :  1.7267093658447266 3.522444248199463 5.2491536140441895
Loss :  1.722812533378601 3.1127142906188965 4.835526943206787
Loss :  1.7163184881210327 3.1416003704071045 4.857918739318848
Loss :  1.7211155891418457 2.7075908184051514 4.428706169128418
Loss :  1.7215393781661987 2.5204355716705322 4.241974830627441
  batch 40 loss: 1.7215393781661987, 2.5204355716705322, 4.241974830627441
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7212812900543213 2.2769343852996826 3.998215675354004
Loss :  1.711094856262207 2.214439868927002 3.925534725189209
Loss :  1.7119810581207275 2.209974527359009 3.9219555854797363
Loss :  1.7169809341430664 1.9712529182434082 3.6882338523864746
Loss :  1.7183643579483032 2.1946334838867188 3.9129977226257324
Loss :  1.717893362045288 2.0847575664520264 3.8026509284973145
Loss :  1.7230758666992188 1.8851979970932007 3.608273983001709
Loss :  1.7137407064437866 2.352715492248535 4.066456317901611
Loss :  1.7278544902801514 2.469264507293701 4.197118759155273
Loss :  1.711706280708313 3.6987013816833496 5.410407543182373
Loss :  1.7081952095031738 3.037109375 4.745304584503174
Loss :  1.717758297920227 2.738755226135254 4.456513404846191
Loss :  1.735194206237793 2.6190359592437744 4.354229927062988
Loss :  1.71234929561615 2.9314310550689697 4.64378023147583
Loss :  1.7150614261627197 2.404750108718872 4.119811534881592
Loss :  1.7246347665786743 2.4753644466400146 4.1999993324279785
Loss :  1.7290284633636475 2.4025301933288574 4.131558418273926
Loss :  1.7227489948272705 2.6843621730804443 4.407111167907715
Loss :  1.7271003723144531 3.390387535095215 5.117487907409668
Loss :  1.7292468547821045 1.9884624481201172 3.7177093029022217
  batch 60 loss: 1.7292468547821045, 1.9884624481201172, 3.7177093029022217
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7276415824890137 2.711831569671631 4.4394731521606445
Loss :  1.7261360883712769 3.555467128753662 5.2816033363342285
Loss :  1.7310619354248047 4.266073226928711 5.997135162353516
Loss :  1.7171252965927124 3.4775917530059814 5.194716930389404
Loss :  1.7124558687210083 3.166832685470581 4.879288673400879
Loss :  1.5539512634277344 3.6324045658111572 5.1863555908203125
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.5627028942108154 3.567941904067993 5.130644798278809
Loss :  1.5956478118896484 3.3678507804870605 4.963498592376709
Loss :  1.6310863494873047 3.42700457572937 5.058091163635254
Total LOSS train 4.372660270104041 valid 5.084647536277771
CE LOSS train 1.7212223107998188 valid 0.40777158737182617
Contrastive LOSS train 2.6514379813120916 valid 0.8567511439323425
EPOCH 111:
Loss :  1.7180255651474 2.5233778953552246 4.241403579711914
Loss :  1.7243798971176147 2.4749293327331543 4.199309349060059
Loss :  1.7145086526870728 2.4422495365142822 4.1567583084106445
Loss :  1.7145382165908813 2.097355842590332 3.811893939971924
Loss :  1.7260098457336426 2.0391385555267334 3.765148401260376
Loss :  1.7252193689346313 2.006732940673828 3.73195219039917
Loss :  1.7234063148498535 2.136462450027466 3.8598687648773193
Loss :  1.7230675220489502 1.9097663164138794 3.632833957672119
Loss :  1.7226152420043945 2.38006591796875 4.1026811599731445
Loss :  1.7022653818130493 2.8676939010620117 4.5699591636657715
Loss :  1.723967432975769 2.6773414611816406 4.401309013366699
Loss :  1.7408075332641602 2.802442789077759 4.54325008392334
Loss :  1.7231364250183105 3.088451385498047 4.811587810516357
Loss :  1.7227505445480347 2.7448017597198486 4.467552185058594
Loss :  1.7244815826416016 3.177370309829712 4.901851654052734
Loss :  1.7110991477966309 3.0774223804473877 4.788521766662598
Loss :  1.7259478569030762 3.2070224285125732 4.93297004699707
Loss :  1.7192078828811646 3.4145164489746094 5.133724212646484
Loss :  1.715989589691162 3.690537452697754 5.406527042388916
Loss :  1.7162022590637207 3.235459089279175 4.951661109924316
  batch 20 loss: 1.7162022590637207, 3.235459089279175, 4.951661109924316
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7215200662612915 3.0492074489593506 4.770727634429932
Loss :  1.7273945808410645 2.820741891860962 4.5481367111206055
Loss :  1.7246661186218262 2.3202707767486572 4.0449371337890625
Loss :  1.7338581085205078 2.2721452713012695 4.006003379821777
Loss :  1.7176977396011353 2.878312349319458 4.596010208129883
Loss :  1.7202370166778564 2.7773027420043945 4.497539520263672
Loss :  1.7339938879013062 2.3909049034118652 4.124898910522461
Loss :  1.7233569622039795 2.263399600982666 3.9867565631866455
Loss :  1.733230471611023 2.5454397201538086 4.278670310974121
Loss :  1.713017225265503 3.1012678146362305 4.8142852783203125
Loss :  1.7339177131652832 3.214672088623047 4.94858980178833
Loss :  1.7256255149841309 3.3663582801818848 5.091983795166016
Loss :  1.7153172492980957 3.0176637172698975 4.732980728149414
Loss :  1.7120144367218018 2.792872905731201 4.504887580871582
Loss :  1.720778226852417 3.3059098720550537 5.026688098907471
Loss :  1.7251344919204712 2.874037504196167 4.599172115325928
Loss :  1.7218860387802124 2.499361991882324 4.221248149871826
Loss :  1.7171008586883545 2.1955041885375977 3.912605047225952
Loss :  1.721889853477478 2.087769031524658 3.809659004211426
Loss :  1.7232743501663208 2.3375086784362793 4.0607829093933105
  batch 40 loss: 1.7232743501663208, 2.3375086784362793, 4.0607829093933105
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7223533391952515 2.516993761062622 4.239346981048584
Loss :  1.7117935419082642 2.3104312419891357 4.0222249031066895
Loss :  1.7129958868026733 2.835601329803467 4.54859733581543
Loss :  1.7187633514404297 3.5776095390319824 5.296372890472412
Loss :  1.7199300527572632 2.6962454319000244 4.416175365447998
Loss :  1.7201507091522217 2.577831983566284 4.297982692718506
Loss :  1.7250555753707886 2.6951284408569336 4.420184135437012
Loss :  1.714583396911621 3.254532814025879 4.9691162109375
Loss :  1.7289389371871948 3.0158727169036865 4.744811534881592
Loss :  1.7125811576843262 2.9494786262512207 4.662059783935547
Loss :  1.708824634552002 2.711911678314209 4.420736312866211
Loss :  1.7176204919815063 2.7713770866394043 4.488997459411621
Loss :  1.7352144718170166 2.8656935691833496 4.600908279418945
Loss :  1.7134490013122559 2.7399818897247314 4.453431129455566
Loss :  1.7141331434249878 2.4578614234924316 4.171994686126709
Loss :  1.7251936197280884 2.539207935333252 4.264401435852051
Loss :  1.728350043296814 2.65287184715271 4.381221771240234
Loss :  1.7215758562088013 2.1942086219787598 3.9157843589782715
Loss :  1.7255518436431885 3.202000379562378 4.927552223205566
Loss :  1.7290763854980469 2.835914134979248 4.564990520477295
  batch 60 loss: 1.7290763854980469, 2.835914134979248, 4.564990520477295
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7270619869232178 2.44966459274292 4.176726341247559
Loss :  1.7259474992752075 3.211229085922241 4.937176704406738
Loss :  1.7318276166915894 2.233797311782837 3.9656248092651367
Loss :  1.7166085243225098 2.50276780128479 4.219376564025879
Loss :  1.712378978729248 2.6897854804992676 4.402164459228516
Loss :  1.5347768068313599 3.77812123298645 5.3128981590271
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.5380969047546387 3.751861810684204 5.289958953857422
Loss :  1.5808366537094116 3.6047170162200928 5.185553550720215
Loss :  1.5885816812515259 3.588538885116577 5.177120685577393
Total LOSS train 4.438389007861797 valid 5.241382837295532
CE LOSS train 1.7215922649090107 valid 0.39714542031288147
Contrastive LOSS train 2.7167967337828416 valid 0.8971347212791443
EPOCH 112:
Loss :  1.7197637557983398 2.9499671459198 4.669731140136719
Loss :  1.727465271949768 2.893718957901001 4.621184349060059
Loss :  1.7167699337005615 2.200132131576538 3.9169020652770996
Loss :  1.7162729501724243 2.7802674770355225 4.496540546417236
Loss :  1.7284183502197266 2.04795241355896 3.7763707637786865
Loss :  1.7275854349136353 3.212277412414551 4.9398627281188965
Loss :  1.7256102561950684 2.8304457664489746 4.556056022644043
Loss :  1.7251447439193726 2.263443946838379 3.988588809967041
Loss :  1.7226805686950684 2.5070037841796875 4.229684352874756
Loss :  1.7016969919204712 2.3499388694763184 4.0516357421875
Loss :  1.7242636680603027 2.7179946899414062 4.442258358001709
Loss :  1.7404422760009766 3.328395128250122 5.0688371658325195
Loss :  1.722965955734253 2.603346824645996 4.326313018798828
Loss :  1.7231802940368652 2.6409478187561035 4.364128112792969
Loss :  1.724222183227539 2.3922152519226074 4.1164374351501465
Loss :  1.7105954885482788 2.186983346939087 3.897578716278076
Loss :  1.7260361909866333 3.1336066722869873 4.85964298248291
Loss :  1.7183784246444702 3.1372296810150146 4.855607986450195
Loss :  1.7156883478164673 2.5327348709106445 4.248423099517822
Loss :  1.714519739151001 3.000425100326538 4.714944839477539
  batch 20 loss: 1.714519739151001, 3.000425100326538, 4.714944839477539
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7196283340454102 2.643887519836426 4.363515853881836
Loss :  1.7262508869171143 2.6908071041107178 4.417057991027832
Loss :  1.7213523387908936 2.4048120975494385 4.126164436340332
Loss :  1.7320359945297241 2.4496636390686035 4.181699752807617
Loss :  1.7164217233657837 2.951683282852173 4.668105125427246
Loss :  1.7168885469436646 2.8887393474578857 4.60562801361084
Loss :  1.731573462486267 3.673412799835205 5.404986381530762
Loss :  1.7189114093780518 2.9410176277160645 4.659929275512695
Loss :  1.729601263999939 2.5068347454071045 4.236435890197754
Loss :  1.711344599723816 2.888780355453491 4.600124835968018
Loss :  1.7343326807022095 2.8079617023468018 4.542294502258301
Loss :  1.72416090965271 3.60087251663208 5.325033187866211
Loss :  1.714469075202942 3.215819835662842 4.930288791656494
Loss :  1.7125582695007324 2.8706037998199463 4.583162307739258
Loss :  1.72104811668396 2.787672758102417 4.508720874786377
Loss :  1.7252341508865356 2.4615964889526367 4.186830520629883
Loss :  1.7216618061065674 3.2349841594696045 4.956645965576172
Loss :  1.7154269218444824 2.2614340782165527 3.976861000061035
Loss :  1.720650315284729 2.8050618171691895 4.525712013244629
Loss :  1.7211980819702148 1.9743274450302124 3.695525646209717
  batch 40 loss: 1.7211980819702148, 1.9743274450302124, 3.695525646209717
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.720707654953003 2.1732091903686523 3.8939168453216553
Loss :  1.7110837697982788 1.9696824550628662 3.6807661056518555
Loss :  1.7127577066421509 2.3871753215789795 4.09993314743042
Loss :  1.7166725397109985 2.396498441696167 4.113171100616455
Loss :  1.7185168266296387 2.4224252700805664 4.140942096710205
Loss :  1.717280626296997 2.6893327236175537 4.406613349914551
Loss :  1.7215765714645386 2.8617095947265625 4.583286285400391
Loss :  1.7134233713150024 2.7283685207366943 4.441792011260986
Loss :  1.7234350442886353 3.077244997024536 4.800680160522461
Loss :  1.7106497287750244 3.228170156478882 4.938819885253906
Loss :  1.7075589895248413 3.208181619644165 4.915740489959717
Loss :  1.715865135192871 2.6892940998077393 4.405158996582031
Loss :  1.7330983877182007 2.810105085372925 4.543203353881836
Loss :  1.7117750644683838 2.3604562282562256 4.072231292724609
Loss :  1.715268611907959 3.1953001022338867 4.910568714141846
Loss :  1.722981333732605 2.5589096546173096 4.281890869140625
Loss :  1.7270673513412476 2.776646852493286 4.503714084625244
Loss :  1.7208114862442017 2.645293951034546 4.366105556488037
Loss :  1.7247174978256226 2.96054744720459 4.685265064239502
Loss :  1.7266191244125366 1.843906283378601 3.5705254077911377
  batch 60 loss: 1.7266191244125366, 1.843906283378601, 3.5705254077911377
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7250008583068848 2.7119736671447754 4.43697452545166
Loss :  1.725056767463684 2.340304374694824 4.065361022949219
Loss :  1.7286056280136108 2.1080095767974854 3.8366150856018066
Loss :  1.716269850730896 2.3842132091522217 4.100482940673828
Loss :  1.7131423950195312 2.3402700424194336 4.053412437438965
Loss :  1.5231112241744995 3.621964693069458 5.145075798034668
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.525227427482605 3.629263162612915 5.1544904708862305
Loss :  1.5600800514221191 3.553713321685791 5.11379337310791
Loss :  1.5640604496002197 3.5599448680877686 5.124005317687988
Total LOSS train 4.407424945097703 valid 5.134341239929199
CE LOSS train 1.7207137236228356 valid 0.39101511240005493
Contrastive LOSS train 2.6867112196408787 valid 0.8899862170219421
EPOCH 113:
Loss :  1.7184672355651855 3.0321877002716064 4.750655174255371
Loss :  1.7233785390853882 3.274672746658325 4.998051166534424
Loss :  1.7157880067825317 3.7167470455169678 5.432535171508789
Loss :  1.7171391248703003 2.5874056816101074 4.304544925689697
Loss :  1.7262303829193115 2.176433801651001 3.9026641845703125
Loss :  1.724988579750061 1.9754447937011719 3.7004332542419434
Loss :  1.7224212884902954 2.861600160598755 4.58402156829834
Loss :  1.7216163873672485 2.7915432453155518 4.51315975189209
Loss :  1.7219101190567017 2.3497414588928223 4.071651458740234
Loss :  1.7011159658432007 3.2582292556762695 4.95934534072876
Loss :  1.7222843170166016 3.6499481201171875 5.372232437133789
Loss :  1.740368366241455 3.4554052352905273 5.195773601531982
Loss :  1.7219159603118896 2.8267159461975098 4.54863166809082
Loss :  1.7222309112548828 3.1107819080352783 4.833012580871582
Loss :  1.7238703966140747 3.0786309242248535 4.802501201629639
Loss :  1.7108005285263062 2.4875729084014893 4.198373317718506
Loss :  1.724727749824524 2.8150415420532227 4.539769172668457
Loss :  1.716706395149231 3.0224030017852783 4.739109516143799
Loss :  1.715906023979187 2.556307554244995 4.272213459014893
Loss :  1.7126878499984741 2.3388826847076416 4.051570415496826
  batch 20 loss: 1.7126878499984741, 2.3388826847076416, 4.051570415496826
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.719106912612915 2.7797493934631348 4.498856544494629
Loss :  1.7245748043060303 2.4603381156921387 4.18491268157959
Loss :  1.7215349674224854 2.7961490154266357 4.517683982849121
Loss :  1.731616735458374 3.025869846343994 4.757486343383789
Loss :  1.7173521518707275 2.360664129257202 4.07801628112793
Loss :  1.7175339460372925 2.0863969326019287 3.8039307594299316
Loss :  1.7305614948272705 2.955244541168213 4.6858062744140625
Loss :  1.7177761793136597 3.4338717460632324 5.151648044586182
Loss :  1.7300580739974976 3.3574094772338867 5.087467670440674
Loss :  1.7124578952789307 3.262223482131958 4.974681377410889
Loss :  1.734928011894226 2.8208465576171875 4.555774688720703
Loss :  1.7243629693984985 2.904608964920044 4.628972053527832
Loss :  1.7150439023971558 2.3909034729003906 4.105947494506836
Loss :  1.7128609418869019 2.559669017791748 4.2725300788879395
Loss :  1.7218942642211914 2.7711923122406006 4.493086814880371
Loss :  1.7261828184127808 2.5892271995544434 4.315410137176514
Loss :  1.722199559211731 2.9275763034820557 4.649775981903076
Loss :  1.7156494855880737 2.317532539367676 4.033182144165039
Loss :  1.720931887626648 2.3432655334472656 4.064197540283203
Loss :  1.7214837074279785 2.4094014167785645 4.130885124206543
  batch 40 loss: 1.7214837074279785, 2.4094014167785645, 4.130885124206543
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7207248210906982 3.1032094955444336 4.823934555053711
Loss :  1.7119760513305664 1.9473576545715332 3.6593337059020996
Loss :  1.7137600183486938 3.3021016120910645 5.015861511230469
Loss :  1.716832160949707 3.1154661178588867 4.832298278808594
Loss :  1.7192188501358032 2.9462597370147705 4.665478706359863
Loss :  1.717329978942871 3.471038579940796 5.188368797302246
Loss :  1.7209419012069702 3.5541863441467285 5.275128364562988
Loss :  1.7129952907562256 3.010035276412964 4.7230305671691895
Loss :  1.7226208448410034 3.1256802082061768 4.848300933837891
Loss :  1.7101409435272217 3.041531562805176 4.751672744750977
Loss :  1.7066453695297241 3.600733757019043 5.307379245758057
Loss :  1.7140073776245117 3.7061679363250732 5.420175552368164
Loss :  1.730546474456787 2.6809778213500977 4.411524295806885
Loss :  1.710391879081726 2.4382436275482178 4.148635387420654
Loss :  1.7133779525756836 2.6696341037750244 4.383011817932129
Loss :  1.7216113805770874 2.4790258407592773 4.200637340545654
Loss :  1.726393222808838 2.2201642990112305 3.9465575218200684
Loss :  1.7202452421188354 2.13272762298584 3.852972984313965
Loss :  1.7256014347076416 3.071884870529175 4.797486305236816
Loss :  1.7279784679412842 2.424203395843506 4.152181625366211
  batch 60 loss: 1.7279784679412842, 2.424203395843506, 4.152181625366211
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7264045476913452 2.878304958343506 4.604709625244141
Loss :  1.7254080772399902 2.8840079307556152 4.6094160079956055
Loss :  1.7301939725875854 2.584819793701172 4.315013885498047
Loss :  1.7173576354980469 3.301762819290161 5.019120216369629
Loss :  1.7138193845748901 2.3375160694122314 4.051335334777832
Loss :  1.6495325565338135 3.795046329498291 5.444579124450684
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6356141567230225 3.7381787300109863 5.37379264831543
Loss :  1.6690083742141724 3.6236610412597656 5.292669296264648
Loss :  1.6619000434875488 3.6590051651000977 5.3209052085876465
Total LOSS train 4.550216410710261 valid 5.357986569404602
CE LOSS train 1.7202952018150917 valid 0.4154750108718872
Contrastive LOSS train 2.829921186887301 valid 0.9147512912750244
EPOCH 114:
Loss :  1.7191755771636963 2.6670010089874268 4.386176586151123
Loss :  1.7240252494812012 2.732806921005249 4.456831932067871
Loss :  1.7151719331741333 2.6256449222564697 4.340816974639893
Loss :  1.7155452966690063 2.676020383834839 4.391565799713135
Loss :  1.7270056009292603 2.601912021636963 4.328917503356934
Loss :  1.725601315498352 2.8480513095855713 4.573652744293213
Loss :  1.7235695123672485 2.5678138732910156 4.291383266448975
Loss :  1.7222541570663452 3.3577191829681396 5.079973220825195
Loss :  1.721510648727417 2.4049298763275146 4.126440525054932
Loss :  1.7016640901565552 3.0121073722839355 4.713771343231201
Loss :  1.7220797538757324 3.0470571517944336 4.769136905670166
Loss :  1.73955500125885 2.8108227252960205 4.55037784576416
Loss :  1.7223650217056274 3.4002952575683594 5.122660160064697
Loss :  1.7224886417388916 2.9340600967407227 4.656548500061035
Loss :  1.7229136228561401 3.2356135845184326 4.958527088165283
Loss :  1.7105367183685303 3.183642625808716 4.894179344177246
Loss :  1.7240939140319824 2.828590154647827 4.5526838302612305
Loss :  1.7174016237258911 2.7323050498962402 4.449706554412842
Loss :  1.7162163257598877 2.2746453285217285 3.990861654281616
Loss :  1.7135553359985352 2.439483404159546 4.15303897857666
  batch 20 loss: 1.7135553359985352, 2.439483404159546, 4.15303897857666
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.719528317451477 2.234023332595825 3.953551769256592
Loss :  1.7258780002593994 2.6572253704071045 4.383103370666504
Loss :  1.7222068309783936 3.2538986206054688 4.976105690002441
Loss :  1.7323912382125854 3.078331708908081 4.810722827911377
Loss :  1.7188631296157837 3.055389642715454 4.774252891540527
Loss :  1.7176874876022339 2.6720941066741943 4.389781475067139
Loss :  1.731080412864685 2.6079723834991455 4.339052677154541
Loss :  1.7185312509536743 2.1584794521331787 3.8770108222961426
Loss :  1.7299928665161133 2.6628167629241943 4.392809867858887
Loss :  1.7117819786071777 3.066312551498413 4.778094291687012
Loss :  1.7345703840255737 2.7393434047698975 4.473913669586182
Loss :  1.7247902154922485 3.060614585876465 4.785404682159424
Loss :  1.7148749828338623 2.838221549987793 4.553096771240234
Loss :  1.7124965190887451 3.0628583431243896 4.775354862213135
Loss :  1.7216485738754272 3.8776488304138184 5.599297523498535
Loss :  1.7253813743591309 3.510664463043213 5.236045837402344
Loss :  1.720539927482605 2.346892833709717 4.067432880401611
Loss :  1.7139471769332886 2.0882740020751953 3.8022212982177734
Loss :  1.719667911529541 2.0030999183654785 3.7227678298950195
Loss :  1.720213770866394 2.2335622310638428 3.9537758827209473
  batch 40 loss: 1.720213770866394, 2.2335622310638428, 3.9537758827209473
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7201414108276367 3.6777825355529785 5.397923946380615
Loss :  1.7112102508544922 2.7385427951812744 4.4497528076171875
Loss :  1.7135863304138184 2.137059211730957 3.8506455421447754
Loss :  1.7175706624984741 2.0619008541107178 3.7794713973999023
Loss :  1.7197688817977905 2.1479685306549072 3.867737293243408
Loss :  1.7185192108154297 2.571854591369629 4.290373802185059
Loss :  1.723005771636963 1.895490050315857 3.6184959411621094
Loss :  1.7153397798538208 2.2439067363739014 3.9592466354370117
Loss :  1.7255749702453613 2.7979700565338135 4.523545265197754
Loss :  1.7126976251602173 2.265416383743286 3.978114128112793
Loss :  1.7091000080108643 2.5643129348754883 4.273412704467773
Loss :  1.7166236639022827 2.729001522064209 4.445625305175781
Loss :  1.7326469421386719 2.7893218994140625 4.521968841552734
Loss :  1.7123595476150513 2.374032735824585 4.086392402648926
Loss :  1.7147152423858643 2.3752260208129883 4.089941024780273
Loss :  1.722585678100586 2.440955400466919 4.163540840148926
Loss :  1.7265828847885132 2.364920139312744 4.091503143310547
Loss :  1.7204315662384033 2.7223668098449707 4.442798614501953
Loss :  1.7254945039749146 3.590894937515259 5.316389560699463
Loss :  1.728039264678955 3.5185177326202393 5.246557235717773
  batch 60 loss: 1.728039264678955, 3.5185177326202393, 5.246557235717773
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7244502305984497 3.1390035152435303 4.8634538650512695
Loss :  1.7254923582077026 3.359154462814331 5.084646701812744
Loss :  1.73311448097229 4.046849727630615 5.779964447021484
Loss :  1.715408205986023 3.542649269104004 5.258057594299316
Loss :  1.7120633125305176 3.4459221363067627 5.157985687255859
Loss :  1.6678887605667114 3.6921730041503906 5.3600616455078125
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.65078604221344 3.7327771186828613 5.383563041687012
Loss :  1.6866382360458374 3.447493553161621 5.134131908416748
Loss :  1.695846676826477 3.3653228282928467 5.061169624328613
Total LOSS train 4.5072091139279875 valid 5.234731554985046
CE LOSS train 1.7206049919128419 valid 0.42396166920661926
Contrastive LOSS train 2.786604112845201 valid 0.8413307070732117
EPOCH 115:
Loss :  1.7189433574676514 1.9435539245605469 3.6624972820281982
Loss :  1.7238831520080566 2.295050859451294 4.01893424987793
Loss :  1.714985728263855 1.849575161933899 3.564560890197754
Loss :  1.7162752151489258 2.3914806842803955 4.107755661010742
Loss :  1.7270534038543701 2.146442413330078 3.8734958171844482
Loss :  1.7253493070602417 1.9201768636703491 3.645526170730591
Loss :  1.7223401069641113 2.1386945247650146 3.861034631729126
Loss :  1.72154700756073 2.208050489425659 3.9295973777770996
Loss :  1.7210972309112549 2.7603447437286377 4.481441974639893
Loss :  1.7015639543533325 3.100327491760254 4.801891326904297
Loss :  1.7235242128372192 3.7367594242095947 5.4602837562561035
Loss :  1.7405061721801758 3.154902935028076 4.895409107208252
Loss :  1.7236006259918213 3.126720428466797 4.850320816040039
Loss :  1.7230406999588013 2.5745785236358643 4.297619342803955
Loss :  1.7235631942749023 2.9320576190948486 4.655620574951172
Loss :  1.7120436429977417 2.3346545696258545 4.046698093414307
Loss :  1.725217342376709 2.733954906463623 4.459172248840332
Loss :  1.7191894054412842 3.2714428901672363 4.990632057189941
Loss :  1.7166775465011597 2.5357511043548584 4.2524285316467285
Loss :  1.7140003442764282 2.336859941482544 4.050860404968262
  batch 20 loss: 1.7140003442764282, 2.336859941482544, 4.050860404968262
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7205345630645752 2.797809362411499 4.518343925476074
Loss :  1.7263396978378296 3.133384943008423 4.859724521636963
Loss :  1.7233000993728638 2.1651487350463867 3.888448715209961
Loss :  1.7336117029190063 2.6984238624572754 4.432035446166992
Loss :  1.717584252357483 2.994978666305542 4.7125630378723145
Loss :  1.7189522981643677 2.7496185302734375 4.468570709228516
Loss :  1.733012318611145 3.563185214996338 5.296197414398193
Loss :  1.7213523387908936 3.08097243309021 4.8023247718811035
Loss :  1.7322633266448975 2.7353243827819824 4.467587471008301
Loss :  1.7124223709106445 2.6607306003570557 4.373152732849121
Loss :  1.7337573766708374 2.5550625324249268 4.288819789886475
Loss :  1.7267674207687378 2.6960439682006836 4.422811508178711
Loss :  1.7163175344467163 2.8781254291534424 4.594442844390869
Loss :  1.7121028900146484 2.295586585998535 4.007689476013184
Loss :  1.7226369380950928 2.6266942024230957 4.349330902099609
Loss :  1.7264236211776733 2.858914852142334 4.585338592529297
Loss :  1.7222822904586792 2.3128018379211426 4.035084247589111
Loss :  1.71791672706604 2.440241575241089 4.158158302307129
Loss :  1.7203797101974487 2.934669017791748 4.655048847198486
Loss :  1.721835970878601 2.8212692737579346 4.543105125427246
  batch 40 loss: 1.721835970878601, 2.8212692737579346, 4.543105125427246
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7219359874725342 2.967158794403076 4.689094543457031
Loss :  1.7122132778167725 2.4717581272125244 4.183971405029297
Loss :  1.7135969400405884 2.833970546722412 4.547567367553711
Loss :  1.7185643911361694 2.2264115810394287 3.9449758529663086
Loss :  1.7198625802993774 2.15673565864563 3.876598358154297
Loss :  1.718671202659607 2.1411354541778564 3.859806537628174
Loss :  1.723190426826477 2.8901569843292236 4.61334753036499
Loss :  1.7145806550979614 2.2450664043426514 3.9596471786499023
Loss :  1.725772738456726 2.719590663909912 4.445363521575928
Loss :  1.7129874229431152 3.0056564807891846 4.718644142150879
Loss :  1.709092378616333 3.020264148712158 4.72935676574707
Loss :  1.7178261280059814 3.257690191268921 4.975516319274902
Loss :  1.732726812362671 2.774029493331909 4.50675630569458
Loss :  1.7132625579833984 2.6420276165008545 4.355290412902832
Loss :  1.7139780521392822 3.0543324947357178 4.768310546875
Loss :  1.7238973379135132 2.4837915897369385 4.207688808441162
Loss :  1.7275224924087524 1.980887532234192 3.7084100246429443
Loss :  1.7215609550476074 2.8301913738250732 4.551752090454102
Loss :  1.7255518436431885 2.337721347808838 4.0632734298706055
Loss :  1.727769136428833 2.513727903366089 4.241497039794922
  batch 60 loss: 1.727769136428833, 2.513727903366089, 4.241497039794922
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7262598276138306 3.1607275009155273 4.886987209320068
Loss :  1.7247451543807983 2.3789660930633545 4.103711128234863
Loss :  1.7306281328201294 2.236010789871216 3.9666390419006348
Loss :  1.716903567314148 2.7365174293518066 4.453421115875244
Loss :  1.7130910158157349 2.9582536220550537 4.671344757080078
Loss :  1.5826756954193115 3.723524570465088 5.30620002746582
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.5642706155776978 3.7642247676849365 5.328495502471924
Loss :  1.610406517982483 3.634032726287842 5.244439125061035
Loss :  1.6204490661621094 3.3961286544799805 5.01657772064209
Total LOSS train 4.3752850789290205 valid 5.223928093910217
CE LOSS train 1.7212367094480074 valid 0.40511226654052734
Contrastive LOSS train 2.654048389654893 valid 0.8490321636199951
EPOCH 116:
Loss :  1.7199985980987549 2.8413593769073486 4.5613579750061035
Loss :  1.726515769958496 3.057687282562256 4.784203052520752
Loss :  1.7166571617126465 2.311978578567505 4.0286359786987305
Loss :  1.7168018817901611 2.3806498050689697 4.097451686859131
Loss :  1.728185772895813 3.007006883621216 4.735192775726318
Loss :  1.727664589881897 2.252164363861084 3.9798288345336914
Loss :  1.7260156869888306 2.759171724319458 4.485187530517578
Loss :  1.7243739366531372 2.3203206062316895 4.044694423675537
Loss :  1.7234041690826416 2.4085850715637207 4.131989479064941
Loss :  1.7038596868515015 2.403588056564331 4.107447624206543
Loss :  1.7251063585281372 2.4189794063568115 4.144085884094238
Loss :  1.7396330833435059 3.3726437091827393 5.112277030944824
Loss :  1.723501443862915 2.975847005844116 4.699348449707031
Loss :  1.7233394384384155 2.9339492321014404 4.657288551330566
Loss :  1.724489688873291 2.216216802597046 3.940706491470337
Loss :  1.710676670074463 2.7064411640167236 4.417118072509766
Loss :  1.7244257926940918 2.5201058387756348 4.244531631469727
Loss :  1.7177388668060303 2.689004421234131 4.406743049621582
Loss :  1.7146542072296143 2.3746180534362793 4.089272499084473
Loss :  1.714342713356018 2.7857491970062256 4.500092029571533
  batch 20 loss: 1.714342713356018, 2.7857491970062256, 4.500092029571533
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.719009280204773 2.7042062282562256 4.423215389251709
Loss :  1.725246548652649 3.095487356185913 4.820734024047852
Loss :  1.7227846384048462 2.1332294940948486 3.8560142517089844
Loss :  1.732904076576233 2.4440488815307617 4.176952838897705
Loss :  1.7181100845336914 2.589261531829834 4.307371616363525
Loss :  1.719300389289856 2.7203736305236816 4.439673900604248
Loss :  1.7329394817352295 2.8026819229125977 4.535621643066406
Loss :  1.7225621938705444 2.996537208557129 4.719099521636963
Loss :  1.7334657907485962 2.76544189453125 4.498907566070557
Loss :  1.7129619121551514 3.1143126487731934 4.827274322509766
Loss :  1.7346839904785156 3.0657958984375 4.800479888916016
Loss :  1.7274715900421143 2.5382843017578125 4.265755653381348
Loss :  1.7171785831451416 3.0740292072296143 4.791207790374756
Loss :  1.7135450839996338 2.795769691467285 4.50931453704834
Loss :  1.722830057144165 2.763186454772949 4.486016273498535
Loss :  1.7270082235336304 2.7654049396514893 4.49241304397583
Loss :  1.7236872911453247 2.7899866104125977 4.513673782348633
Loss :  1.719398856163025 2.634401798248291 4.3538007736206055
Loss :  1.7215708494186401 2.885392189025879 4.606963157653809
Loss :  1.7240840196609497 2.8563358783721924 4.580420017242432
  batch 40 loss: 1.7240840196609497, 2.8563358783721924, 4.580420017242432
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7224442958831787 3.667994260787964 5.390438556671143
Loss :  1.711838722229004 2.4963812828063965 4.2082200050354
Loss :  1.713667631149292 2.517188310623169 4.230855941772461
Loss :  1.717392921447754 2.3476290702819824 4.065021991729736
Loss :  1.7204729318618774 2.684840202331543 4.405313014984131
Loss :  1.718421459197998 2.6068613529205322 4.325283050537109
Loss :  1.7229876518249512 2.6896519660949707 4.412639617919922
Loss :  1.7138296365737915 2.655707836151123 4.369537353515625
Loss :  1.7268959283828735 2.746206045150757 4.47310209274292
Loss :  1.7111623287200928 3.0241923332214355 4.735354423522949
Loss :  1.7061861753463745 3.1213691234588623 4.827555179595947
Loss :  1.7154247760772705 2.687818765640259 4.403243541717529
Loss :  1.7310683727264404 2.879988193511963 4.611056327819824
Loss :  1.7109273672103882 2.517982244491577 4.228909492492676
Loss :  1.7130622863769531 2.461042881011963 4.174105167388916
Loss :  1.7208532094955444 2.0109846591949463 3.731837749481201
Loss :  1.7252748012542725 2.047560691833496 3.7728354930877686
Loss :  1.7206897735595703 1.9019925594329834 3.6226823329925537
Loss :  1.7243051528930664 2.4287874698638916 4.153092384338379
Loss :  1.7271100282669067 2.0249462127685547 3.752056121826172
  batch 60 loss: 1.7271100282669067, 2.0249462127685547, 3.752056121826172
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7248772382736206 3.093724012374878 4.818601131439209
Loss :  1.7236310243606567 3.062415361404419 4.786046504974365
Loss :  1.7289849519729614 2.770092010498047 4.499076843261719
Loss :  1.7161470651626587 2.292300224304199 4.008447170257568
Loss :  1.7135045528411865 1.6753395795822144 3.3888440132141113
Loss :  1.5822293758392334 3.4495842456817627 5.031813621520996
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5711840391159058 3.6109118461608887 5.182096004486084
Loss :  1.6131130456924438 3.3310954570770264 4.94420862197876
Loss :  1.6086084842681885 3.331958055496216 4.940566539764404
Total LOSS train 4.377946439156165 valid 5.024671196937561
CE LOSS train 1.7212813652478731 valid 0.4021521210670471
Contrastive LOSS train 2.656665092248183 valid 0.832989513874054
EPOCH 117:
Loss :  1.7193721532821655 3.519476890563965 5.23884916305542
Loss :  1.723038673400879 2.5659868717193604 4.28902530670166
Loss :  1.715520977973938 1.922356367111206 3.6378774642944336
Loss :  1.7173610925674438 1.9617912769317627 3.679152488708496
Loss :  1.7264145612716675 2.7221856117248535 4.4486002922058105
Loss :  1.7256710529327393 2.129120349884033 3.8547914028167725
Loss :  1.7236766815185547 2.28179669380188 4.0054731369018555
Loss :  1.7216979265213013 2.21924090385437 3.940938949584961
Loss :  1.7219078540802002 2.0712368488311768 3.793144702911377
Loss :  1.7006665468215942 2.508171319961548 4.208837985992432
Loss :  1.7220243215560913 2.2849929332733154 4.007017135620117
Loss :  1.739007830619812 2.093073844909668 3.8320817947387695
Loss :  1.7213714122772217 2.2387914657592773 3.960162878036499
Loss :  1.721317172050476 3.141888380050659 4.863205432891846
Loss :  1.7229470014572144 3.339850425720215 5.062797546386719
Loss :  1.7110702991485596 2.185309886932373 3.8963801860809326
Loss :  1.7242798805236816 3.072091579437256 4.7963714599609375
Loss :  1.7174067497253418 4.0968427658081055 5.814249515533447
Loss :  1.716552734375 2.540207862854004 4.256760597229004
Loss :  1.7135885953903198 2.836029529571533 4.549618244171143
  batch 20 loss: 1.7135885953903198, 2.836029529571533, 4.549618244171143
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7193365097045898 3.4176619052886963 5.136998176574707
Loss :  1.7247048616409302 3.2688076496124268 4.9935126304626465
Loss :  1.7233866453170776 3.096618175506592 4.820004940032959
Loss :  1.7309677600860596 2.7073166370391846 4.438284397125244
Loss :  1.717899203300476 2.860765218734741 4.578664302825928
Loss :  1.718180537223816 2.606720209121704 4.3249006271362305
Loss :  1.7305498123168945 2.480006217956543 4.2105560302734375
Loss :  1.7178610563278198 2.3229262828826904 4.040787220001221
Loss :  1.7289599180221558 2.3895041942596436 4.11846399307251
Loss :  1.7126448154449463 2.0124893188476562 3.7251341342926025
Loss :  1.7348151206970215 1.980048418045044 3.7148635387420654
Loss :  1.724081039428711 2.524552822113037 4.248633861541748
Loss :  1.7143516540527344 2.2500526905059814 3.964404344558716
Loss :  1.7121574878692627 2.8313589096069336 4.543516159057617
Loss :  1.7222175598144531 2.5005061626434326 4.222723960876465
Loss :  1.7255749702453613 2.359311580657959 4.08488655090332
Loss :  1.7217750549316406 2.8119356632232666 4.533710479736328
Loss :  1.7149845361709595 3.568847417831421 5.28383207321167
Loss :  1.719420075416565 3.1792991161346436 4.898719310760498
Loss :  1.7192920446395874 2.9284098148345947 4.647701740264893
  batch 40 loss: 1.7192920446395874, 2.9284098148345947, 4.647701740264893
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.719400405883789 2.679185152053833 4.398585319519043
Loss :  1.7123860120773315 2.5881831645965576 4.3005690574646
Loss :  1.714328646659851 2.8618667125701904 4.576195240020752
Loss :  1.71791672706604 2.666630744934082 4.384547233581543
Loss :  1.7198172807693481 2.406017541885376 4.125834941864014
Loss :  1.7168910503387451 2.845311403274536 4.562202453613281
Loss :  1.72083580493927 3.342623710632324 5.063459396362305
Loss :  1.7145110368728638 2.807823419570923 4.522334575653076
Loss :  1.724971055984497 2.754041910171509 4.479012966156006
Loss :  1.7126743793487549 2.318620204925537 4.031294822692871
Loss :  1.7091056108474731 2.3526723384857178 4.0617780685424805
Loss :  1.7179205417633057 2.184522867202759 3.9024434089660645
Loss :  1.7324579954147339 2.116271734237671 3.8487296104431152
Loss :  1.7151312828063965 2.654771089553833 4.369902610778809
Loss :  1.7160848379135132 2.5410115718841553 4.257096290588379
Loss :  1.7214977741241455 2.8243472576141357 4.545845031738281
Loss :  1.7271531820297241 2.9165639877319336 4.643717288970947
Loss :  1.7229477167129517 2.6918063163757324 4.4147539138793945
Loss :  1.7262192964553833 2.9854376316070557 4.7116570472717285
Loss :  1.7271702289581299 2.707214832305908 4.434385299682617
  batch 60 loss: 1.7271702289581299, 2.707214832305908, 4.434385299682617
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7254128456115723 2.8209829330444336 4.546395778656006
Loss :  1.7243071794509888 2.44950270652771 4.173810005187988
Loss :  1.7285492420196533 2.742706298828125 4.471255302429199
Loss :  1.7170586585998535 2.455498218536377 4.1725568771362305
Loss :  1.7132538557052612 2.257612943649292 3.9708666801452637
Loss :  1.569556713104248 3.290205478668213 4.859762191772461
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.557546854019165 3.4514667987823486 5.009013652801514
Loss :  1.6027201414108276 3.1759464740753174 4.7786664962768555
Loss :  1.6043015718460083 3.036726474761963 4.641027927398682
Total LOSS train 4.363613282717191 valid 4.822117567062378
CE LOSS train 1.7205547204384437 valid 0.4010753929615021
Contrastive LOSS train 2.643058567780715 valid 0.7591816186904907
EPOCH 118:
Loss :  1.7195730209350586 2.3575658798217773 4.077138900756836
Loss :  1.7228916883468628 2.0938467979431152 3.8167386054992676
Loss :  1.7138978242874146 2.447155237197876 4.16105318069458
Loss :  1.7158253192901611 3.3358278274536133 5.051652908325195
Loss :  1.7254472970962524 2.501323699951172 4.226770877838135
Loss :  1.7239935398101807 2.443037986755371 4.167031288146973
Loss :  1.7224198579788208 2.954113245010376 4.676533222198486
Loss :  1.721244215965271 2.337310791015625 4.0585551261901855
Loss :  1.7209138870239258 2.3813815116882324 4.102295398712158
Loss :  1.7011851072311401 3.4234116077423096 5.12459659576416
Loss :  1.722801685333252 2.486407518386841 4.209209442138672
Loss :  1.7395219802856445 2.8168210983276367 4.556343078613281
Loss :  1.7234899997711182 2.6913905143737793 4.414880752563477
Loss :  1.7223626375198364 2.7889530658721924 4.511315822601318
Loss :  1.7238588333129883 2.529240846633911 4.25309944152832
Loss :  1.7110401391983032 2.5527031421661377 4.2637434005737305
Loss :  1.7249095439910889 3.2170917987823486 4.9420013427734375
Loss :  1.718854308128357 3.143968105316162 4.862822532653809
Loss :  1.7164278030395508 2.081092357635498 3.797520160675049
Loss :  1.7133578062057495 3.028752565383911 4.742110252380371
  batch 20 loss: 1.7133578062057495, 3.028752565383911, 4.742110252380371
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7207307815551758 2.1331777572631836 3.8539085388183594
Loss :  1.725240707397461 2.563678503036499 4.288919448852539
Loss :  1.7225297689437866 2.019850969314575 3.7423806190490723
Loss :  1.7318826913833618 2.409367799758911 4.1412506103515625
Loss :  1.7182098627090454 3.03781795501709 4.756027698516846
Loss :  1.7188584804534912 2.556116819381714 4.274975299835205
Loss :  1.7312288284301758 3.0051071643829346 4.736335754394531
Loss :  1.7187108993530273 2.1442103385925293 3.8629212379455566
Loss :  1.7300771474838257 2.965334892272949 4.6954121589660645
Loss :  1.7115505933761597 3.277480125427246 4.989030838012695
Loss :  1.7342880964279175 3.1126186847686768 4.846906661987305
Loss :  1.72450852394104 2.906039237976074 4.630547523498535
Loss :  1.7149375677108765 2.1150500774383545 3.8299875259399414
Loss :  1.7118176221847534 2.2163751125335693 3.928192615509033
Loss :  1.7225650548934937 2.493250846862793 4.215816020965576
Loss :  1.725294589996338 2.199688673019409 3.924983263015747
Loss :  1.7213022708892822 2.342350959777832 4.063652992248535
Loss :  1.7149901390075684 2.1108641624450684 3.8258543014526367
Loss :  1.7186672687530518 2.556849956512451 4.275517463684082
Loss :  1.7189652919769287 2.362452745437622 4.081418037414551
  batch 40 loss: 1.7189652919769287, 2.362452745437622, 4.081418037414551
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7193660736083984 2.1259636878967285 3.845329761505127
Loss :  1.7117193937301636 1.997427225112915 3.709146499633789
Loss :  1.7144830226898193 1.803159236907959 3.5176422595977783
Loss :  1.7181661128997803 2.0772242546081543 3.7953903675079346
Loss :  1.720218300819397 2.2813963890075684 4.001614570617676
Loss :  1.7169904708862305 2.3562722206115723 4.073262691497803
Loss :  1.719758152961731 2.62947154045105 4.34922981262207
Loss :  1.7152246236801147 2.0901901721954346 3.8054146766662598
Loss :  1.722387433052063 1.8559271097183228 3.5783145427703857
Loss :  1.71435546875 2.062842607498169 3.777198076248169
Loss :  1.7096240520477295 2.1329522132873535 3.842576265335083
Loss :  1.7173408269882202 2.908658266067505 4.6259989738464355
Loss :  1.7310978174209595 2.8229074478149414 4.554005146026611
Loss :  1.7166279554367065 3.4131102561950684 5.1297383308410645
Loss :  1.717647910118103 3.7928054332733154 5.510453224182129
Loss :  1.7218679189682007 2.4119958877563477 4.133863925933838
Loss :  1.7265136241912842 2.934058427810669 4.660572052001953
Loss :  1.7220078706741333 3.8486034870147705 5.570611476898193
Loss :  1.7263767719268799 3.662522792816162 5.388899803161621
Loss :  1.726043701171875 2.165313720703125 3.891357421875
  batch 60 loss: 1.726043701171875, 2.165313720703125, 3.891357421875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7231805324554443 2.701237201690674 4.424417495727539
Loss :  1.7245532274246216 2.429964780807495 4.154518127441406
Loss :  1.7254494428634644 2.1369516849517822 3.862401008605957
Loss :  1.7147544622421265 2.25724196434021 3.971996307373047
Loss :  1.7119098901748657 1.6509966850280762 3.3629064559936523
Loss :  1.56882905960083 3.8505845069885254 5.4194135665893555
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.5718982219696045 3.8956682682037354 5.46756649017334
Loss :  1.6027634143829346 3.796691417694092 5.3994550704956055
Loss :  1.6000617742538452 3.706599473953247 5.306661128997803
Total LOSS train 4.284866310999944 valid 5.398274064064026
CE LOSS train 1.720431349827693 valid 0.4000154435634613
Contrastive LOSS train 2.564434970342196 valid 0.9266498684883118
EPOCH 119:
Loss :  1.719022512435913 1.962301254272461 3.681323766708374
Loss :  1.7206661701202393 2.1340556144714355 3.854721784591675
Loss :  1.7134510278701782 2.1553025245666504 3.868753433227539
Loss :  1.7167432308197021 2.949615955352783 4.666358947753906
Loss :  1.7255799770355225 2.1483700275421143 3.8739500045776367
Loss :  1.7247920036315918 2.174992799758911 3.899784803390503
Loss :  1.7227041721343994 2.299417018890381 4.022121429443359
Loss :  1.7213722467422485 3.3714230060577393 5.092795372009277
Loss :  1.7219562530517578 2.8481526374816895 4.570108890533447
Loss :  1.7033976316452026 3.004645586013794 4.708043098449707
Loss :  1.7235243320465088 3.2482895851135254 4.971814155578613
Loss :  1.7407011985778809 3.293698787689209 5.03439998626709
Loss :  1.7235058546066284 2.760014772415161 4.4835205078125
Loss :  1.7225496768951416 2.9625091552734375 4.68505859375
Loss :  1.7246333360671997 2.526660919189453 4.251294136047363
Loss :  1.7126768827438354 2.18145489692688 3.894131660461426
Loss :  1.7243441343307495 2.4861364364624023 4.210480690002441
Loss :  1.7181373834609985 2.5510668754577637 4.269204139709473
Loss :  1.7175008058547974 2.0744214057922363 3.791922092437744
Loss :  1.7123167514801025 3.0254604816436768 4.737777233123779
  batch 20 loss: 1.7123167514801025, 3.0254604816436768, 4.737777233123779
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7182250022888184 2.4404447078704834 4.158669471740723
Loss :  1.724548101425171 2.4444515705108643 4.168999671936035
Loss :  1.7211610078811646 2.200221538543701 3.921382427215576
Loss :  1.7324026823043823 2.770413398742676 4.502816200256348
Loss :  1.7207379341125488 2.6343119144439697 4.355050086975098
Loss :  1.7181552648544312 2.530146837234497 4.248301982879639
Loss :  1.7312077283859253 2.52018404006958 4.251391887664795
Loss :  1.7177497148513794 2.7121472358703613 4.429896831512451
Loss :  1.7294962406158447 2.21474027633667 3.9442365169525146
Loss :  1.7131537199020386 2.464858293533325 4.178011894226074
Loss :  1.7354716062545776 3.102987766265869 4.838459491729736
Loss :  1.7246414422988892 2.5478997230529785 4.272541046142578
Loss :  1.7154663801193237 2.3653366565704346 4.080802917480469
Loss :  1.7128313779830933 2.562126874923706 4.27495813369751
Loss :  1.7229361534118652 2.4914515018463135 4.214387893676758
Loss :  1.7257001399993896 2.2944085597991943 4.020108699798584
Loss :  1.7209548950195312 3.005363702774048 4.726318359375
Loss :  1.7146637439727783 2.884161949157715 4.598825454711914
Loss :  1.7196886539459229 3.488647699356079 5.208336353302002
Loss :  1.7194600105285645 3.278329372406006 4.99778938293457
  batch 40 loss: 1.7194600105285645, 3.278329372406006, 4.99778938293457
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.720585823059082 3.1339423656463623 4.854528427124023
Loss :  1.7108794450759888 2.584254503250122 4.2951340675354
Loss :  1.7125543355941772 2.7853176593780518 4.4978718757629395
Loss :  1.7174845933914185 3.1160287857055664 4.833513259887695
Loss :  1.7196074724197388 3.1008613109588623 4.820468902587891
Loss :  1.718266487121582 2.6476714611053467 4.365938186645508
Loss :  1.7235835790634155 2.018789529800415 3.742372989654541
Loss :  1.7139428853988647 2.0216009616851807 3.735543727874756
Loss :  1.728883147239685 2.7455642223358154 4.474447250366211
Loss :  1.71364164352417 2.8965728282928467 4.6102142333984375
Loss :  1.709933876991272 2.6769425868988037 4.386876583099365
Loss :  1.7200253009796143 2.409437894821167 4.129463195800781
Loss :  1.7348169088363647 2.4382405281066895 4.173057556152344
Loss :  1.7139688730239868 2.816002607345581 4.529971599578857
Loss :  1.7152063846588135 2.355670213699341 4.070876598358154
Loss :  1.7243863344192505 2.5105254650115967 4.234911918640137
Loss :  1.728452444076538 2.4571597576141357 4.185612201690674
Loss :  1.7231841087341309 2.616940498352051 4.340124607086182
Loss :  1.7273273468017578 2.704784631729126 4.432111740112305
Loss :  1.7296578884124756 2.4918692111968994 4.221527099609375
  batch 60 loss: 1.7296578884124756, 2.4918692111968994, 4.221527099609375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.727195382118225 2.4026596546173096 4.129855155944824
Loss :  1.725339412689209 2.817211151123047 4.542550563812256
Loss :  1.731038212776184 2.23825740814209 3.9692955017089844
Loss :  1.7173758745193481 1.9927898645401 3.7101657390594482
Loss :  1.713455319404602 1.6221815347671509 3.335636854171753
Loss :  1.557897925376892 3.558525562286377 5.116423606872559
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.553487777709961 3.483375310897827 5.036863327026367
Loss :  1.5975769758224487 3.3536031246185303 4.9511799812316895
Loss :  1.614161729812622 3.185851812362671 4.800013542175293
Total LOSS train 4.31662952716534 valid 4.976120114326477
CE LOSS train 1.7210618532620943 valid 0.4035404324531555
Contrastive LOSS train 2.595567692243136 valid 0.7964629530906677
EPOCH 120:
Loss :  1.72025465965271 1.8617584705352783 3.5820131301879883
Loss :  1.7250101566314697 2.096392869949341 3.8214030265808105
Loss :  1.7148269414901733 1.751949429512024 3.4667763710021973
Loss :  1.7157032489776611 2.0188212394714355 3.7345244884490967
Loss :  1.7263312339782715 1.8082940578460693 3.534625291824341
Loss :  1.7252466678619385 1.9020884037017822 3.6273350715637207
Loss :  1.7230651378631592 2.182565212249756 3.905630350112915
Loss :  1.7223976850509644 2.4065425395965576 4.128940105438232
Loss :  1.7214750051498413 2.3747944831848145 4.096269607543945
Loss :  1.7012382745742798 2.6307084560394287 4.331946849822998
Loss :  1.7232171297073364 3.0596072673797607 4.782824516296387
Loss :  1.7404149770736694 2.591376304626465 4.331791400909424
Loss :  1.7228174209594727 2.437274694442749 4.160092353820801
Loss :  1.722220778465271 2.588045597076416 4.310266494750977
Loss :  1.7249517440795898 2.826118230819702 4.551070213317871
Loss :  1.7121878862380981 2.583134651184082 4.295322418212891
Loss :  1.7255250215530396 2.6328301429748535 4.3583550453186035
Loss :  1.719773769378662 2.149785041809082 3.869558811187744
Loss :  1.7170984745025635 1.848760724067688 3.565859317779541
Loss :  1.7150746583938599 2.495051622390747 4.2101263999938965
  batch 20 loss: 1.7150746583938599, 2.495051622390747, 4.2101263999938965
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7217317819595337 2.254969596862793 3.976701259613037
Loss :  1.7262378931045532 2.616764545440674 4.3430023193359375
Loss :  1.725414514541626 2.3917551040649414 4.117169380187988
Loss :  1.7348957061767578 2.5446243286132812 4.279520034790039
Loss :  1.7200464010238647 2.6064348220825195 4.326481342315674
Loss :  1.7199543714523315 2.136066198348999 3.856020450592041
Loss :  1.73275625705719 2.370102643966675 4.102859020233154
Loss :  1.7214821577072144 3.176180601119995 4.89766263961792
Loss :  1.7320365905761719 2.323761463165283 4.055798053741455
Loss :  1.7134469747543335 2.938518762588501 4.651965618133545
Loss :  1.7350150346755981 2.694514751434326 4.429529666900635
Loss :  1.726270079612732 2.532219171524048 4.25848913192749
Loss :  1.7164055109024048 3.1087405681610107 4.825146198272705
Loss :  1.7132772207260132 2.8997111320495605 4.612988471984863
Loss :  1.7230396270751953 2.9835023880004883 4.706542015075684
Loss :  1.7268277406692505 3.3473870754241943 5.074214935302734
Loss :  1.7233667373657227 2.6076598167419434 4.331026554107666
Loss :  1.7193177938461304 2.4407455921173096 4.16006326675415
Loss :  1.7222414016723633 2.6933343410491943 4.415575981140137
Loss :  1.7246472835540771 2.1066110134124756 3.8312582969665527
  batch 40 loss: 1.7246472835540771, 2.1066110134124756, 3.8312582969665527
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.723078727722168 2.6104423999786377 4.333520889282227
Loss :  1.714293360710144 3.393048048019409 5.107341289520264
Loss :  1.7156933546066284 2.878373861312866 4.594067096710205
Loss :  1.7194064855575562 3.052915096282959 4.772321701049805
Loss :  1.7219533920288086 3.810206890106201 5.53216028213501
Loss :  1.7214852571487427 2.74414324760437 4.465628623962402
Loss :  1.724708914756775 3.1281371116638184 4.852846145629883
Loss :  1.7159150838851929 2.755122661590576 4.471037864685059
Loss :  1.728010654449463 2.8136544227600098 4.541665077209473
Loss :  1.7136815786361694 3.3082799911499023 5.021961688995361
Loss :  1.709995150566101 3.499988555908203 5.209983825683594
Loss :  1.718941569328308 2.6968865394592285 4.415828227996826
Loss :  1.7333173751831055 3.2582192420959473 4.991536617279053
Loss :  1.7139242887496948 2.187910556793213 3.9018349647521973
Loss :  1.7150835990905762 2.1334588527679443 3.8485424518585205
Loss :  1.7227776050567627 2.4823880195617676 4.205165863037109
Loss :  1.7273417711257935 3.249526023864746 4.97686767578125
Loss :  1.721817970275879 2.183082342147827 3.904900312423706
Loss :  1.7251811027526855 2.6565184593200684 4.381699562072754
Loss :  1.7274116277694702 2.3871266841888428 4.114538192749023
  batch 60 loss: 1.7274116277694702, 2.3871266841888428, 4.114538192749023
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7255455255508423 2.822373867034912 4.547919273376465
Loss :  1.724310040473938 3.26896333694458 4.9932732582092285
Loss :  1.7301874160766602 2.6601643562316895 4.39035177230835
Loss :  1.7168684005737305 3.016317844390869 4.7331862449646
Loss :  1.7138493061065674 1.9720289707183838 3.685878276824951
Loss :  1.563787817955017 3.7194433212280273 5.283231258392334
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.5613409280776978 3.6773078441619873 5.238648891448975
Loss :  1.6070787906646729 3.459374189376831 5.066452980041504
Loss :  1.6095139980316162 3.418034791946411 5.027548789978027
Total LOSS train 4.337089278147771 valid 5.15397047996521
CE LOSS train 1.721877253972567 valid 0.40237849950790405
Contrastive LOSS train 2.61521201133728 valid 0.8545086979866028
EPOCH 121:
Loss :  1.7212839126586914 2.291809558868408 4.0130934715271
Loss :  1.7268610000610352 2.40748929977417 4.134350299835205
Loss :  1.7173333168029785 3.0549376010894775 4.772271156311035
Loss :  1.7176568508148193 2.852553129196167 4.570209980010986
Loss :  1.7295454740524292 3.0222079753875732 4.751753330230713
Loss :  1.7274150848388672 2.7445669174194336 4.471982002258301
Loss :  1.7280129194259644 2.3632726669311523 4.091285705566406
Loss :  1.725236415863037 2.730031728744507 4.455267906188965
Loss :  1.7229841947555542 2.5694642066955566 4.2924485206604
Loss :  1.7051678895950317 2.334047794342041 4.039215564727783
Loss :  1.7249178886413574 2.495227575302124 4.220145225524902
Loss :  1.73794686794281 2.6298861503601074 4.367833137512207
Loss :  1.7238905429840088 2.2726733684539795 3.9965639114379883
Loss :  1.7223786115646362 2.246614933013916 3.968993663787842
Loss :  1.7251594066619873 2.8232905864715576 4.548449993133545
Loss :  1.71160089969635 3.643890142440796 5.3554911613464355
Loss :  1.724982738494873 3.450695753097534 5.175678253173828
Loss :  1.7181129455566406 3.097553253173828 4.815666198730469
Loss :  1.715722680091858 2.255162477493286 3.9708852767944336
Loss :  1.7158340215682983 2.678929567337036 4.394763469696045
  batch 20 loss: 1.7158340215682983, 2.678929567337036, 4.394763469696045
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7215224504470825 1.9655251502990723 3.6870474815368652
Loss :  1.7261790037155151 2.5810773372650146 4.30725622177124
Loss :  1.7246167659759521 2.5474047660827637 4.272021293640137
Loss :  1.733744502067566 2.2632365226745605 3.996981143951416
Loss :  1.7181463241577148 2.760528087615967 4.478674411773682
Loss :  1.7192671298980713 2.316413164138794 4.035680294036865
Loss :  1.7319978475570679 2.1025898456573486 3.834587574005127
Loss :  1.7218431234359741 2.6235873699188232 4.345430374145508
Loss :  1.731735110282898 3.4378983974456787 5.169633388519287
Loss :  1.7125722169876099 3.0933187007904053 4.805891036987305
Loss :  1.7337837219238281 3.1215102672576904 4.855294227600098
Loss :  1.7258349657058716 3.0223569869995117 4.748191833496094
Loss :  1.7166248559951782 2.5884902477264404 4.305115222930908
Loss :  1.7126275300979614 2.464048147201538 4.176675796508789
Loss :  1.722978949546814 2.8616583347320557 4.58463716506958
Loss :  1.725627064704895 2.6714026927948 4.397029876708984
Loss :  1.7221671342849731 2.4571990966796875 4.179366111755371
Loss :  1.7173908948898315 2.6873533725738525 4.4047441482543945
Loss :  1.7214233875274658 2.6121878623962402 4.333611488342285
Loss :  1.7210637331008911 2.741884708404541 4.462948322296143
  batch 40 loss: 1.7210637331008911, 2.741884708404541, 4.462948322296143
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.721322774887085 3.0253584384918213 4.746681213378906
Loss :  1.7123373746871948 2.9730491638183594 4.685386657714844
Loss :  1.7138861417770386 2.7676079273223877 4.481493949890137
Loss :  1.718018651008606 3.89357852935791 5.611597061157227
Loss :  1.7198721170425415 3.15584135055542 4.875713348388672
Loss :  1.7166293859481812 3.113974094390869 4.83060359954834
Loss :  1.7199286222457886 2.3262088298797607 4.04613733291626
Loss :  1.714051365852356 2.409945249557495 4.123996734619141
Loss :  1.723132848739624 2.651883125305176 4.375016212463379
Loss :  1.7135170698165894 2.506504535675049 4.220021724700928
Loss :  1.7086620330810547 2.7456042766571045 4.454266548156738
Loss :  1.717360019683838 2.8949432373046875 4.612303256988525
Loss :  1.7321301698684692 3.145625114440918 4.877755165100098
Loss :  1.7156875133514404 3.4817848205566406 5.19747257232666
Loss :  1.715973138809204 2.6853418350219727 4.401314735412598
Loss :  1.7226427793502808 2.298074245452881 4.020717144012451
Loss :  1.7275464534759521 2.2223997116088867 3.949946165084839
Loss :  1.7214280366897583 2.07513165473938 3.7965598106384277
Loss :  1.7256211042404175 2.2191667556762695 3.9447879791259766
Loss :  1.7271268367767334 2.0348665714263916 3.761993408203125
  batch 60 loss: 1.7271268367767334, 2.0348665714263916, 3.761993408203125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.723759412765503 2.262701988220215 3.9864614009857178
Loss :  1.7246441841125488 2.0095455646514893 3.734189748764038
Loss :  1.7291053533554077 2.7354400157928467 4.464545249938965
Loss :  1.716488003730774 2.8147239685058594 4.531211853027344
Loss :  1.7131787538528442 1.5072674751281738 3.2204461097717285
Loss :  1.5880203247070312 3.8298773765563965 5.417897701263428
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.580162525177002 3.852626085281372 5.432788848876953
Loss :  1.6238782405853271 3.615647792816162 5.23952579498291
Loss :  1.6167125701904297 3.5757670402526855 5.192479610443115
Total LOSS train 4.380581301909227 valid 5.320672988891602
CE LOSS train 1.721496007992671 valid 0.4041781425476074
Contrastive LOSS train 2.6590852957505446 valid 0.8939417600631714
EPOCH 122:
Loss :  1.7208819389343262 1.955079436302185 3.675961494445801
Loss :  1.7248709201812744 2.2868247032165527 4.011695861816406
Loss :  1.7160640954971313 2.1798391342163086 3.8959031105041504
Loss :  1.7173950672149658 2.4762613773345947 4.1936564445495605
Loss :  1.7276359796524048 3.169281482696533 4.896917343139648
Loss :  1.7267177104949951 3.1839845180511475 4.910702228546143
Loss :  1.7250208854675293 2.826910972595215 4.551931858062744
Loss :  1.7234405279159546 3.0231122970581055 4.74655294418335
Loss :  1.7223886251449585 2.7157347202301025 4.4381232261657715
Loss :  1.7035956382751465 2.54449200630188 4.2480878829956055
Loss :  1.7239997386932373 2.6399428844451904 4.363942623138428
Loss :  1.7391332387924194 2.467989444732666 4.207122802734375
Loss :  1.7247205972671509 2.97809100151062 4.7028117179870605
Loss :  1.7229745388031006 2.566193103790283 4.289167404174805
Loss :  1.7252594232559204 2.4547150135040283 4.179974555969238
Loss :  1.712316632270813 2.8072052001953125 4.519521713256836
Loss :  1.7250940799713135 2.47255802154541 4.1976518630981445
Loss :  1.7195411920547485 2.875417470932007 4.594958782196045
Loss :  1.7161835432052612 2.8281893730163574 4.544373035430908
Loss :  1.7139002084732056 2.4451093673706055 4.1590094566345215
  batch 20 loss: 1.7139002084732056, 2.4451093673706055, 4.1590094566345215
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7199888229370117 2.4183876514434814 4.138376235961914
Loss :  1.7245597839355469 2.264295816421509 3.9888556003570557
Loss :  1.7231460809707642 2.395245313644409 4.118391513824463
Loss :  1.732925534248352 3.1221468448638916 4.855072498321533
Loss :  1.717593789100647 2.3786818981170654 4.096275806427002
Loss :  1.717721939086914 1.9458919763565063 3.663613796234131
Loss :  1.73078191280365 2.365110158920288 4.095891952514648
Loss :  1.719675898551941 3.0226101875305176 4.742286205291748
Loss :  1.7300175428390503 3.334944725036621 5.064962387084961
Loss :  1.7122008800506592 3.0826451778411865 4.794846057891846
Loss :  1.733891248703003 3.3537704944610596 5.0876617431640625
Loss :  1.725016474723816 2.3062081336975098 4.031224727630615
Loss :  1.716052770614624 2.571922540664673 4.287975311279297
Loss :  1.7135001420974731 2.717710256576538 4.431210517883301
Loss :  1.7229927778244019 3.573866367340088 5.296859264373779
Loss :  1.725905179977417 2.682192087173462 4.408097267150879
Loss :  1.7228032350540161 2.6559102535247803 4.378713607788086
Loss :  1.7161319255828857 2.191004753112793 3.9071366786956787
Loss :  1.720609426498413 2.6743040084838867 4.394913673400879
Loss :  1.7214298248291016 2.1275076866149902 3.848937511444092
  batch 40 loss: 1.7214298248291016, 2.1275076866149902, 3.848937511444092
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7205915451049805 2.307476043701172 4.028067588806152
Loss :  1.7121448516845703 2.214815855026245 3.9269607067108154
Loss :  1.7137342691421509 2.4641025066375732 4.177836894989014
Loss :  1.7175461053848267 1.932608962059021 3.6501550674438477
Loss :  1.7194926738739014 2.725275754928589 4.44476842880249
Loss :  1.7178577184677124 2.8198800086975098 4.537737846374512
Loss :  1.7203842401504517 3.005256175994873 4.725640296936035
Loss :  1.7139289379119873 3.03495454788208 4.748883247375488
Loss :  1.723045825958252 2.8576748371124268 4.580720901489258
Loss :  1.7119240760803223 2.031510591506958 3.7434346675872803
Loss :  1.7084709405899048 2.2012689113616943 3.9097399711608887
Loss :  1.7165628671646118 2.416295289993286 4.1328582763671875
Loss :  1.7314462661743164 2.319018840789795 4.050465106964111
Loss :  1.7133325338363647 2.196932315826416 3.9102649688720703
Loss :  1.7159485816955566 3.045491933822632 4.761440277099609
Loss :  1.7232608795166016 2.70615816116333 4.429419040679932
Loss :  1.7272061109542847 2.63299298286438 4.360198974609375
Loss :  1.722875714302063 3.7967660427093506 5.519641876220703
Loss :  1.728843092918396 3.9554905891418457 5.684333801269531
Loss :  1.7289899587631226 3.424421787261963 5.153411865234375
  batch 60 loss: 1.7289899587631226, 3.424421787261963, 5.153411865234375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7273598909378052 4.175199031829834 5.90255880355835
Loss :  1.7336663007736206 4.013787746429443 5.7474541664123535
Loss :  1.7312438488006592 3.554478406906128 5.285722255706787
Loss :  1.7178467512130737 3.0407063961029053 4.7585530281066895
Loss :  1.7145973443984985 2.9202797412872314 4.6348772048950195
Loss :  1.664519190788269 3.8230650424957275 5.487584114074707
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6508234739303589 3.7199079990386963 5.370731353759766
Loss :  1.6837003231048584 3.534430742263794 5.218131065368652
Loss :  1.6950818300247192 3.4614572525024414 5.156538963317871
Total LOSS train 4.45791559952956 valid 5.308246374130249
CE LOSS train 1.7213904784275935 valid 0.4237704575061798
Contrastive LOSS train 2.7365250972601083 valid 0.8653643131256104
EPOCH 123:
Loss :  1.7217313051223755 2.8574001789093018 4.579131603240967
Loss :  1.7289501428604126 2.081127643585205 3.810077667236328
Loss :  1.7178609371185303 1.967871904373169 3.685732841491699
Loss :  1.7178844213485718 2.17002010345459 3.887904644012451
Loss :  1.7289760112762451 2.3871567249298096 4.116132736206055
Loss :  1.7280442714691162 2.3100361824035645 4.038080215454102
Loss :  1.7267266511917114 2.346550941467285 4.073277473449707
Loss :  1.724867820739746 2.19051456451416 3.9153823852539062
Loss :  1.7222344875335693 1.957303762435913 3.6795382499694824
Loss :  1.703708529472351 1.9566606283187866 3.6603691577911377
Loss :  1.7245583534240723 2.1569180488586426 3.881476402282715
Loss :  1.7384942770004272 2.5302703380584717 4.268764495849609
Loss :  1.7243436574935913 3.7378880977630615 5.462231636047363
Loss :  1.7221139669418335 3.210646152496338 4.932760238647461
Loss :  1.724573016166687 2.773369789123535 4.497942924499512
Loss :  1.7119648456573486 2.374579906463623 4.086544990539551
Loss :  1.724584937095642 2.9547173976898193 4.679302215576172
Loss :  1.7191053628921509 3.114811658859253 4.833917140960693
Loss :  1.7168110609054565 2.8882627487182617 4.605073928833008
Loss :  1.7130868434906006 3.3377318382263184 5.05081844329834
  batch 20 loss: 1.7130868434906006, 3.3377318382263184, 5.05081844329834
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.718237280845642 2.7886359691619873 4.50687313079834
Loss :  1.7237937450408936 2.822875738143921 4.5466694831848145
Loss :  1.7212508916854858 2.84297513961792 4.564226150512695
Loss :  1.7310762405395508 2.3857505321502686 4.116827011108398
Loss :  1.7190645933151245 2.765371561050415 4.48443603515625
Loss :  1.71749746799469 3.5051751136779785 5.222672462463379
Loss :  1.7306861877441406 3.6421446800231934 5.372830867767334
Loss :  1.7184333801269531 3.3194282054901123 5.0378618240356445
Loss :  1.7289047241210938 3.012934684753418 4.741839408874512
Loss :  1.7102711200714111 2.5209057331085205 4.231176853179932
Loss :  1.7332897186279297 2.605909824371338 4.339199542999268
Loss :  1.7241830825805664 2.6067559719085693 4.330939292907715
Loss :  1.713948130607605 1.9013400077819824 3.615288257598877
Loss :  1.7105755805969238 2.231384515762329 3.941960096359253
Loss :  1.722013235092163 2.6710429191589355 4.3930559158325195
Loss :  1.724579930305481 2.5870048999786377 4.311584949493408
Loss :  1.721508502960205 2.6162562370300293 4.337764739990234
Loss :  1.7181951999664307 2.1907958984375 3.9089910984039307
Loss :  1.720499873161316 2.3831610679626465 4.103661060333252
Loss :  1.7225912809371948 2.3117403984069824 4.034331798553467
  batch 40 loss: 1.7225912809371948, 2.3117403984069824, 4.034331798553467
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.722072720527649 3.1341612339019775 4.856234073638916
Loss :  1.7120413780212402 2.836306571960449 4.5483479499816895
Loss :  1.71450936794281 2.84013032913208 4.55463981628418
Loss :  1.7178254127502441 3.3839802742004395 5.101805686950684
Loss :  1.7200478315353394 2.101123571395874 3.821171283721924
Loss :  1.719364047050476 2.768439769744873 4.487803936004639
Loss :  1.7234617471694946 2.363095998764038 4.086557865142822
Loss :  1.7143045663833618 2.82015323638916 4.534457683563232
Loss :  1.726340413093567 2.5239510536193848 4.250291347503662
Loss :  1.712541937828064 2.402902364730835 4.115444183349609
Loss :  1.7089457511901855 2.8389177322387695 4.547863483428955
Loss :  1.7187139987945557 3.170027017593384 4.8887410163879395
Loss :  1.732635736465454 3.257699489593506 4.990335464477539
Loss :  1.7127383947372437 2.6834988594055176 4.396237373352051
Loss :  1.713850736618042 2.9414260387420654 4.655276775360107
Loss :  1.723986268043518 2.3621363639831543 4.086122512817383
Loss :  1.7264573574066162 2.085021734237671 3.811479091644287
Loss :  1.7206488847732544 2.070007801055908 3.790656566619873
Loss :  1.7245365381240845 2.3038346767425537 4.028371334075928
Loss :  1.7268239259719849 1.7647725343704224 3.4915964603424072
  batch 60 loss: 1.7268239259719849, 1.7647725343704224, 3.4915964603424072
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7236968278884888 1.997533917427063 3.7212307453155518
Loss :  1.7232894897460938 1.8557411432266235 3.5790305137634277
Loss :  1.728991985321045 1.8531101942062378 3.5821022987365723
Loss :  1.7155064344406128 2.558725595474243 4.274231910705566
Loss :  1.7123888731002808 2.6042158603668213 4.3166046142578125
Loss :  1.5781641006469727 3.674722194671631 5.2528862953186035
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.5737531185150146 3.6957664489746094 5.269519805908203
Loss :  1.6123878955841064 3.3948135375976562 5.007201194763184
Loss :  1.6199642419815063 3.4483532905578613 5.068317413330078
Total LOSS train 4.313896667040312 valid 5.149481177330017
CE LOSS train 1.7210298721606916 valid 0.4049910604953766
Contrastive LOSS train 2.5928667857096745 valid 0.8620883226394653
EPOCH 124:
Loss :  1.7191041707992554 3.5107510089874268 5.229855060577393
Loss :  1.7248268127441406 3.1812455654144287 4.906072616577148
Loss :  1.715458631515503 3.397364377975464 5.112823009490967
Loss :  1.7164616584777832 2.778136968612671 4.494598388671875
Loss :  1.7270286083221436 2.21480655670166 3.9418351650238037
Loss :  1.726204514503479 2.9023163318634033 4.628520965576172
Loss :  1.7252882719039917 3.0400471687316895 4.765335559844971
Loss :  1.7229747772216797 3.44588565826416 5.16886043548584
Loss :  1.7218165397644043 3.3207201957702637 5.042536735534668
Loss :  1.7034178972244263 2.4248416423797607 4.128259658813477
Loss :  1.7234750986099243 2.842559814453125 4.56603479385376
Loss :  1.7380563020706177 2.6328470706939697 4.370903491973877
Loss :  1.7235429286956787 2.9692609310150146 4.692803859710693
Loss :  1.7209725379943848 3.0627729892730713 4.783745765686035
Loss :  1.7220807075500488 3.0312180519104004 4.753298759460449
Loss :  1.7091846466064453 2.253312826156616 3.9624974727630615
Loss :  1.722733497619629 2.0576062202453613 3.7803397178649902
Loss :  1.7177122831344604 2.031134605407715 3.748847007751465
Loss :  1.715768575668335 1.8151482343673706 3.530916690826416
Loss :  1.712798833847046 2.3132107257843018 4.026009559631348
  batch 20 loss: 1.712798833847046, 2.3132107257843018, 4.026009559631348
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7198293209075928 2.7669451236724854 4.486774444580078
Loss :  1.7250094413757324 2.4424216747283936 4.167430877685547
Loss :  1.723292589187622 2.5688207149505615 4.292113304138184
Loss :  1.732479214668274 2.3220419883728027 4.054521083831787
Loss :  1.7188611030578613 2.6544511318206787 4.373311996459961
Loss :  1.7189536094665527 2.5798919200897217 4.298845291137695
Loss :  1.7319109439849854 2.2360076904296875 3.967918634414673
Loss :  1.7202616930007935 2.3324315547943115 4.0526933670043945
Loss :  1.730210304260254 3.4513907432556152 5.181601047515869
Loss :  1.7135896682739258 2.94657564163208 4.660165309906006
Loss :  1.734607219696045 2.6161515712738037 4.3507585525512695
Loss :  1.725966215133667 2.143768787384033 3.8697350025177
Loss :  1.7160335779190063 2.5370519161224365 4.253085613250732
Loss :  1.7131826877593994 2.1388845443725586 3.852067232131958
Loss :  1.723157525062561 3.114901542663574 4.838058948516846
Loss :  1.7254819869995117 2.374171257019043 4.099653244018555
Loss :  1.7213037014007568 2.1409831047058105 3.8622868061065674
Loss :  1.71523916721344 2.33962345123291 4.0548624992370605
Loss :  1.719516396522522 2.654879093170166 4.374395370483398
Loss :  1.7197470664978027 2.5040364265441895 4.223783493041992
  batch 40 loss: 1.7197470664978027, 2.5040364265441895, 4.223783493041992
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7198429107666016 2.8619658946990967 4.581809043884277
Loss :  1.712081789970398 2.635136842727661 4.3472185134887695
Loss :  1.7145296335220337 2.8277642726898193 4.542294025421143
Loss :  1.717765212059021 2.644766330718994 4.362531661987305
Loss :  1.7198724746704102 2.227989435195923 3.947861909866333
Loss :  1.717941403388977 2.3691461086273193 4.087087631225586
Loss :  1.7210780382156372 2.4040398597717285 4.125117778778076
Loss :  1.7151414155960083 2.4536328315734863 4.168774127960205
Loss :  1.722909688949585 2.5855188369750977 4.308428764343262
Loss :  1.7133736610412598 2.741398334503174 4.454771995544434
Loss :  1.709022045135498 3.401625871658325 5.110648155212402
Loss :  1.717556118965149 2.4982595443725586 4.215815544128418
Loss :  1.7314153909683228 2.5782177448272705 4.309633255004883
Loss :  1.7142928838729858 2.022717237472534 3.7370100021362305
Loss :  1.7163463830947876 2.2176551818847656 3.9340014457702637
Loss :  1.7222263813018799 1.8841395378112793 3.606365919113159
Loss :  1.727097988128662 3.195615291595459 4.922713279724121
Loss :  1.721921443939209 2.395707845687866 4.117629051208496
Loss :  1.726119041442871 3.0426113605499268 4.768730163574219
Loss :  1.7301671504974365 3.9030988216400146 5.633265972137451
  batch 60 loss: 1.7301671504974365, 3.9030988216400146, 5.633265972137451
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7252951860427856 2.7769017219543457 4.502196788787842
Loss :  1.731013298034668 4.187008380889893 5.9180216789245605
Loss :  1.7149094343185425 3.836536169052124 5.551445484161377
Loss :  1.7172287702560425 4.303833484649658 6.02106237411499
Loss :  1.719323992729187 4.027976036071777 5.747300148010254
Loss :  1.7089061737060547 4.283265113830566 5.992171287536621
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6988505125045776 4.089019775390625 5.787870407104492
Loss :  1.7180135250091553 3.8281843662261963 5.546197891235352
Loss :  1.6974128484725952 3.451700448989868 5.149113178253174
Total LOSS train 4.461075254587027 valid 5.61883819103241
CE LOSS train 1.7208617302087637 valid 0.4243532121181488
Contrastive LOSS train 2.7402135353821975 valid 0.862925112247467
EPOCH 125:
Loss :  1.7153760194778442 4.286146640777588 6.001522541046143
Loss :  1.7257413864135742 3.492844820022583 5.218585968017578
Loss :  1.7152750492095947 3.141234874725342 4.856510162353516
Loss :  1.715965747833252 2.925367832183838 4.64133358001709
Loss :  1.7247438430786133 2.7276697158813477 4.452413558959961
Loss :  1.7243117094039917 2.188546895980835 3.912858486175537
Loss :  1.7217413187026978 2.3682312965393066 4.089972496032715
Loss :  1.7210869789123535 2.603668212890625 4.3247551918029785
Loss :  1.7193398475646973 2.3299827575683594 4.049322605133057
Loss :  1.699967861175537 1.8843300342559814 3.5842978954315186
Loss :  1.7210314273834229 2.1728172302246094 3.8938486576080322
Loss :  1.7387017011642456 2.466489553451538 4.205191135406494
Loss :  1.7214982509613037 2.1894705295562744 3.910968780517578
Loss :  1.720506191253662 2.361774206161499 4.082280158996582
Loss :  1.724057912826538 2.458120584487915 4.182178497314453
Loss :  1.711628794670105 2.2866594791412354 3.998288154602051
Loss :  1.7234407663345337 2.6604723930358887 4.383913040161133
Loss :  1.7179540395736694 2.495924711227417 4.213878631591797
Loss :  1.7167266607284546 2.989863634109497 4.706590175628662
Loss :  1.7121167182922363 2.537762403488159 4.249878883361816
  batch 20 loss: 1.7121167182922363, 2.537762403488159, 4.249878883361816
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7189171314239502 2.667332172393799 4.386249542236328
Loss :  1.7238458395004272 2.3541295528411865 4.077975273132324
Loss :  1.7214908599853516 2.057446002960205 3.7789368629455566
Loss :  1.7315070629119873 2.558150291442871 4.2896575927734375
Loss :  1.7200746536254883 2.955127716064453 4.675202369689941
Loss :  1.7183030843734741 2.5554380416870117 4.273741245269775
Loss :  1.7304418087005615 2.472836971282959 4.203278541564941
Loss :  1.7180432081222534 2.9083924293518066 4.62643575668335
Loss :  1.729036569595337 2.550706386566162 4.279743194580078
Loss :  1.7132703065872192 2.8299779891967773 4.543248176574707
Loss :  1.734712839126587 2.543079137802124 4.277791976928711
Loss :  1.723834753036499 2.265888214111328 3.989722967147827
Loss :  1.7149325609207153 2.239945650100708 3.954878330230713
Loss :  1.7121350765228271 2.450348138809204 4.162483215332031
Loss :  1.7232708930969238 2.7114861011505127 4.434757232666016
Loss :  1.7248252630233765 2.626222848892212 4.351047992706299
Loss :  1.7210793495178223 2.6190733909606934 4.340152740478516
Loss :  1.714408040046692 2.4788386821746826 4.193246841430664
Loss :  1.718901515007019 2.3701272010803223 4.089028835296631
Loss :  1.7197470664978027 1.917270541191101 3.6370177268981934
  batch 40 loss: 1.7197470664978027, 1.917270541191101, 3.6370177268981934
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7203166484832764 2.2772762775421143 3.9975929260253906
Loss :  1.7137373685836792 2.4159178733825684 4.129655361175537
Loss :  1.71478271484375 2.2763049602508545 3.9910876750946045
Loss :  1.7199701070785522 2.5182762145996094 4.238246440887451
Loss :  1.7201467752456665 1.909380555152893 3.6295273303985596
Loss :  1.7174785137176514 2.2236692905426025 3.941147804260254
Loss :  1.721228003501892 2.8405046463012695 4.561732769012451
Loss :  1.7140593528747559 2.411095142364502 4.125154495239258
Loss :  1.7228041887283325 2.49480938911438 4.217613697052002
Loss :  1.7129372358322144 2.380730390548706 4.093667507171631
Loss :  1.7080496549606323 2.7008583545684814 4.408907890319824
Loss :  1.7169604301452637 2.3624186515808105 4.079379081726074
Loss :  1.7305865287780762 2.3269553184509277 4.057541847229004
Loss :  1.7134543657302856 2.5461981296539307 4.259652614593506
Loss :  1.7158888578414917 2.3738009929656982 4.0896897315979
Loss :  1.7226773500442505 2.59666109085083 4.319338321685791
Loss :  1.72611665725708 3.0081989765167236 4.734315872192383
Loss :  1.7215591669082642 2.7789306640625 4.500489711761475
Loss :  1.7258650064468384 2.2837209701538086 4.009585857391357
Loss :  1.7274866104125977 1.7666592597961426 3.4941458702087402
  batch 60 loss: 1.7274866104125977, 1.7666592597961426, 3.4941458702087402
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7247885465621948 1.8997889757156372 3.624577522277832
Loss :  1.725141167640686 2.042912244796753 3.7680535316467285
Loss :  1.7296444177627563 1.95464026927948 3.6842846870422363
Loss :  1.7173992395401 2.214588165283203 3.9319872856140137
Loss :  1.714707374572754 1.9200539588928223 3.634761333465576
Loss :  1.5630518198013306 3.995166540145874 5.558218479156494
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.5652114152908325 3.940964937210083 5.506176471710205
Loss :  1.596814513206482 3.855994462966919 5.452808856964111
Loss :  1.6134483814239502 3.811537265777588 5.424985885620117
Total LOSS train 4.2006972643045275 valid 5.485547423362732
CE LOSS train 1.7203350213857798 valid 0.40336209535598755
Contrastive LOSS train 2.480362246586726 valid 0.952884316444397
EPOCH 126:
Loss :  1.721358299255371 2.8367760181427 4.558134078979492
Loss :  1.7260503768920898 2.5337347984313965 4.259785175323486
Loss :  1.71744704246521 2.1793479919433594 3.8967950344085693
Loss :  1.7186989784240723 2.1133487224578857 3.832047700881958
Loss :  1.7282993793487549 2.16414475440979 3.892444133758545
Loss :  1.727062702178955 2.684896230697632 4.411958694458008
Loss :  1.7262202501296997 2.343505382537842 4.069725513458252
Loss :  1.7240641117095947 2.4156267642974854 4.13969087600708
Loss :  1.7233288288116455 2.150942802429199 3.8742716312408447
Loss :  1.7053154706954956 1.8913543224334717 3.5966696739196777
Loss :  1.7243125438690186 2.242156744003296 3.9664692878723145
Loss :  1.7400262355804443 2.4904067516326904 4.230432987213135
Loss :  1.7243002653121948 2.473994255065918 4.198294639587402
Loss :  1.723116397857666 2.4755823612213135 4.198698997497559
Loss :  1.725382685661316 2.161186695098877 3.8865694999694824
Loss :  1.7132246494293213 2.186898946762085 3.9001235961914062
Loss :  1.725002646446228 2.22287654876709 3.9478793144226074
Loss :  1.719823956489563 2.1931676864624023 3.912991523742676
Loss :  1.7178922891616821 1.9286459684371948 3.646538257598877
Loss :  1.7144207954406738 2.7548348903656006 4.469255447387695
  batch 20 loss: 1.7144207954406738, 2.7548348903656006, 4.469255447387695
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7208151817321777 1.982873797416687 3.7036890983581543
Loss :  1.725003957748413 2.2061190605163574 3.9311230182647705
Loss :  1.7239419221878052 1.6766817569732666 3.4006237983703613
Loss :  1.7328118085861206 2.0172274112701416 3.7500391006469727
Loss :  1.719450831413269 2.1533241271972656 3.872775077819824
Loss :  1.7181020975112915 2.13974928855896 3.857851505279541
Loss :  1.7308299541473389 2.6066155433654785 4.337445259094238
Loss :  1.7187843322753906 1.765882134437561 3.484666347503662
Loss :  1.7286772727966309 1.8881419897079468 3.616819381713867
Loss :  1.7132089138031006 2.221177816390991 3.934386730194092
Loss :  1.7343734502792358 2.5878679752349854 4.322241306304932
Loss :  1.7248278856277466 2.3288986682891846 4.053726673126221
Loss :  1.7156097888946533 2.4942054748535156 4.20981502532959
Loss :  1.7134202718734741 1.9464384317398071 3.6598587036132812
Loss :  1.7237944602966309 2.116297721862793 3.840092182159424
Loss :  1.726560354232788 2.171621084213257 3.898181438446045
Loss :  1.7232038974761963 2.4720377922058105 4.195241928100586
Loss :  1.7175688743591309 2.4766414165496826 4.194210052490234
Loss :  1.7222342491149902 2.077620029449463 3.799854278564453
Loss :  1.7233866453170776 2.211930751800537 3.9353175163269043
  batch 40 loss: 1.7233866453170776, 2.211930751800537, 3.9353175163269043
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7226040363311768 2.7554728984832764 4.478076934814453
Loss :  1.7144310474395752 2.2425665855407715 3.9569976329803467
Loss :  1.7163337469100952 2.2318050861358643 3.94813871383667
Loss :  1.7194420099258423 2.091590404510498 3.811032295227051
Loss :  1.7215873003005981 1.8123897314071655 3.5339770317077637
Loss :  1.7202647924423218 1.9036247730255127 3.623889446258545
Loss :  1.7229292392730713 1.902184009552002 3.6251132488250732
Loss :  1.7159899473190308 1.7519469261169434 3.4679369926452637
Loss :  1.7251838445663452 2.9370834827423096 4.662267208099365
Loss :  1.7140436172485352 2.306499481201172 4.020543098449707
Loss :  1.710068702697754 2.2044410705566406 3.9145097732543945
Loss :  1.718384027481079 3.1015889644622803 4.819972991943359
Loss :  1.7320128679275513 2.1874306201934814 3.9194436073303223
Loss :  1.7146213054656982 2.8620266914367676 4.576647758483887
Loss :  1.7170809507369995 2.731520652770996 4.448601722717285
Loss :  1.7220990657806396 2.2172791957855225 3.939378261566162
Loss :  1.7268304824829102 2.2450859546661377 3.971916437149048
Loss :  1.7225289344787598 1.9365729093551636 3.659101963043213
Loss :  1.7264212369918823 2.419631004333496 4.146052360534668
Loss :  1.7277040481567383 2.4771108627319336 4.204814910888672
  batch 60 loss: 1.7277040481567383, 2.4771108627319336, 4.204814910888672
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.725223422050476 2.117300271987915 3.8425235748291016
Loss :  1.725921630859375 1.9956436157226562 3.7215652465820312
Loss :  1.7296708822250366 1.742544412612915 3.472215175628662
Loss :  1.7179944515228271 2.1089746952056885 3.8269691467285156
Loss :  1.7150559425354004 1.3962594270706177 3.1113152503967285
Loss :  1.5639996528625488 4.003458023071289 5.567457675933838
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.567031979560852 4.0931782722473145 5.660210132598877
Loss :  1.5961169004440308 3.9783968925476074 5.574513912200928
Loss :  1.6065205335617065 3.9753024578094482 5.581822872161865
Total LOSS train 3.9629190041468694 valid 5.596001148223877
CE LOSS train 1.7219443321228027 valid 0.40163013339042664
Contrastive LOSS train 2.240974686695979 valid 0.9938256144523621
EPOCH 127:
Loss :  1.7216241359710693 2.4564666748046875 4.178091049194336
Loss :  1.7235946655273438 2.482651472091675 4.206246376037598
Loss :  1.7165300846099854 2.1561806201934814 3.872710704803467
Loss :  1.7190005779266357 2.3068838119506836 4.025884628295898
Loss :  1.7271816730499268 2.277451753616333 4.00463342666626
Loss :  1.7261440753936768 2.36979341506958 4.095937728881836
Loss :  1.7246040105819702 2.2525112628936768 3.9771151542663574
Loss :  1.7227368354797363 2.156092643737793 3.8788294792175293
Loss :  1.7229154109954834 2.660081386566162 4.382996559143066
Loss :  1.705684781074524 2.8861656188964844 4.591850280761719
Loss :  1.7242356538772583 2.6475861072540283 4.371821880340576
Loss :  1.7398566007614136 2.4461607933044434 4.1860175132751465
Loss :  1.724393367767334 3.0282092094421387 4.752602577209473
Loss :  1.7225054502487183 2.587167501449585 4.309672832489014
Loss :  1.7254984378814697 2.6688666343688965 4.394365310668945
Loss :  1.713736891746521 2.3612289428710938 4.074965953826904
Loss :  1.7247233390808105 2.2830140590667725 4.007737159729004
Loss :  1.7200709581375122 2.2353479862213135 3.9554190635681152
Loss :  1.7176237106323242 1.9114009141921997 3.6290245056152344
Loss :  1.7144149541854858 1.8795585632324219 3.5939736366271973
  batch 20 loss: 1.7144149541854858, 1.8795585632324219, 3.5939736366271973
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7215873003005981 1.9268063306808472 3.6483936309814453
Loss :  1.725406527519226 2.0656278133392334 3.79103422164917
Loss :  1.7240101099014282 2.338473320007324 4.062483310699463
Loss :  1.7331883907318115 2.6993348598480225 4.432523250579834
Loss :  1.7185025215148926 3.181952476501465 4.900454998016357
Loss :  1.7178525924682617 2.6175248622894287 4.3353776931762695
Loss :  1.7309331893920898 2.7066216468811035 4.437554836273193
Loss :  1.7199397087097168 2.2063450813293457 3.9262847900390625
Loss :  1.730063796043396 2.156113386154175 3.8861770629882812
Loss :  1.712425708770752 3.4670798778533936 5.179505348205566
Loss :  1.7326083183288574 3.3335201740264893 5.066128730773926
Loss :  1.7248741388320923 2.360633134841919 4.085507392883301
Loss :  1.7167476415634155 2.358931541442871 4.075679302215576
Loss :  1.7142413854599 2.5429606437683105 4.2572021484375
Loss :  1.723038673400879 2.709411859512329 4.432450294494629
Loss :  1.7261602878570557 2.3386733531951904 4.064833641052246
Loss :  1.7236167192459106 2.729703664779663 4.453320503234863
Loss :  1.7188653945922852 2.6878550052642822 4.406720161437988
Loss :  1.7221734523773193 2.64062237739563 4.362795829772949
Loss :  1.7234349250793457 2.3071436882019043 4.03057861328125
  batch 40 loss: 1.7234349250793457, 2.3071436882019043, 4.03057861328125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7225558757781982 3.0096638202667236 4.732219696044922
Loss :  1.714872121810913 2.338797092437744 4.053668975830078
Loss :  1.7168012857437134 2.395430564880371 4.112231731414795
Loss :  1.7199338674545288 2.244168758392334 3.9641027450561523
Loss :  1.7224153280258179 1.6145360469818115 3.33695125579834
Loss :  1.719704031944275 1.6909974813461304 3.4107015132904053
Loss :  1.7217856645584106 2.5113823413848877 4.233168125152588
Loss :  1.7177523374557495 2.642051935195923 4.359804153442383
Loss :  1.7238072156906128 2.779778003692627 4.503585338592529
Loss :  1.7165377140045166 2.9373419284820557 4.653879642486572
Loss :  1.7117011547088623 2.363677501678467 4.07537841796875
Loss :  1.7191215753555298 2.3461110591888428 4.065232753753662
Loss :  1.7309201955795288 2.6522653102874756 4.383185386657715
Loss :  1.7175028324127197 2.728339910507202 4.445842742919922
Loss :  1.7186115980148315 3.0544657707214355 4.773077487945557
Loss :  1.7215282917022705 3.2236387729644775 4.945167064666748
Loss :  1.7267831563949585 2.9003303050994873 4.627113342285156
Loss :  1.723760962486267 2.73016357421875 4.453924655914307
Loss :  1.7274246215820312 3.2976009845733643 5.025025367736816
Loss :  1.7271897792816162 2.275703191757202 4.002892971038818
  batch 60 loss: 1.7271897792816162, 2.275703191757202, 4.002892971038818
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.724230408668518 2.4886579513549805 4.212888240814209
Loss :  1.7248777151107788 2.405489683151245 4.130367279052734
Loss :  1.7276365756988525 2.6395046710968018 4.367141246795654
Loss :  1.7178581953048706 3.7889907360076904 5.5068488121032715
Loss :  1.715072751045227 1.6281828880310059 3.3432555198669434
Loss :  1.603248119354248 3.7230799198150635 5.326328277587891
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6007130146026611 3.8258864879608154 5.426599502563477
Loss :  1.6308776140213013 3.573289394378662 5.204166889190674
Loss :  1.626822829246521 3.71738600730896 5.344208717346191
Total LOSS train 4.246808554575994 valid 5.325325846672058
CE LOSS train 1.7219558715820313 valid 0.40670570731163025
Contrastive LOSS train 2.5248526884959293 valid 0.92934650182724
EPOCH 128:
Loss :  1.720538854598999 1.980053186416626 3.700592041015625
Loss :  1.7213213443756104 2.0802645683288574 3.8015859127044678
Loss :  1.716349482536316 1.7816439867019653 3.4979934692382812
Loss :  1.720398187637329 2.706963300704956 4.427361488342285
Loss :  1.7272217273712158 1.8698556423187256 3.5970773696899414
Loss :  1.7263963222503662 2.669890880584717 4.396286964416504
Loss :  1.725256085395813 2.365964412689209 4.091220378875732
Loss :  1.7226125001907349 2.843520164489746 4.566132545471191
Loss :  1.7233256101608276 3.5214107036590576 5.244736194610596
Loss :  1.7063031196594238 2.5148777961730957 4.2211809158325195
Loss :  1.7245146036148071 2.9938929080963135 4.71840763092041
Loss :  1.740617275238037 4.055166244506836 5.795783519744873
Loss :  1.7262042760849 2.686065912246704 4.4122700691223145
Loss :  1.7233127355575562 2.873849630355835 4.597162246704102
Loss :  1.7271058559417725 3.3111214637756348 5.038227081298828
Loss :  1.7140402793884277 3.073270082473755 4.787310600280762
Loss :  1.7257503271102905 2.54791522026062 4.273665428161621
Loss :  1.7202503681182861 3.1117751598358154 4.832025527954102
Loss :  1.7193772792816162 2.7210283279418945 4.44040584564209
Loss :  1.71511709690094 3.1502230167388916 4.865340232849121
  batch 20 loss: 1.71511709690094, 3.1502230167388916, 4.865340232849121
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.722412347793579 3.055433750152588 4.777846336364746
Loss :  1.7254092693328857 2.5607666969299316 4.286175727844238
Loss :  1.7253093719482422 1.8385180234909058 3.5638275146484375
Loss :  1.733378529548645 1.970832109451294 3.7042107582092285
Loss :  1.7201601266860962 2.0378947257995605 3.758054733276367
Loss :  1.7199591398239136 2.173222780227661 3.893181800842285
Loss :  1.7312579154968262 2.3976967334747314 4.128954887390137
Loss :  1.719604253768921 2.44128680229187 4.160891056060791
Loss :  1.7298673391342163 2.263383150100708 3.9932503700256348
Loss :  1.7143744230270386 2.4028103351593018 4.117184638977051
Loss :  1.7348363399505615 2.368661403656006 4.103497505187988
Loss :  1.7248564958572388 2.037841558456421 3.762698173522949
Loss :  1.7162737846374512 3.208033800125122 4.924307823181152
Loss :  1.7146295309066772 2.3323452472686768 4.0469746589660645
Loss :  1.7247004508972168 2.8293750286102295 4.554075241088867
Loss :  1.7257333993911743 3.023390531539917 4.749124050140381
Loss :  1.7227706909179688 2.8638916015625 4.586662292480469
Loss :  1.7139605283737183 2.4063780307769775 4.120338439941406
Loss :  1.7208819389343262 3.6820433139801025 5.402925491333008
Loss :  1.7205052375793457 2.13870906829834 3.8592143058776855
  batch 40 loss: 1.7205052375793457, 2.13870906829834, 3.8592143058776855
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.721247673034668 3.0292069911956787 4.750454902648926
Loss :  1.7153244018554688 3.3679587841033936 5.083283424377441
Loss :  1.7166815996170044 2.9333574771881104 4.650039196014404
Loss :  1.7201733589172363 3.607910394668579 5.3280839920043945
Loss :  1.7211967706680298 2.6583235263824463 4.379520416259766
Loss :  1.719582200050354 2.2464816570281982 3.966063976287842
Loss :  1.7221709489822388 3.1069095134735107 4.829080581665039
Loss :  1.7161959409713745 2.780191659927368 4.496387481689453
Loss :  1.7246742248535156 2.3838531970977783 4.108527183532715
Loss :  1.7142196893692017 1.896275281906128 3.610495090484619
Loss :  1.7112045288085938 2.1937644481658936 3.9049689769744873
Loss :  1.7187495231628418 2.531001329421997 4.249751091003418
Loss :  1.732519268989563 2.5568416118621826 4.289361000061035
Loss :  1.7152434587478638 2.252680778503418 3.967924118041992
Loss :  1.717968225479126 2.6832971572875977 4.4012651443481445
Loss :  1.7233600616455078 3.0296518802642822 4.753011703491211
Loss :  1.727580189704895 2.568377733230591 4.295958042144775
Loss :  1.723068118095398 2.322566032409668 4.0456342697143555
Loss :  1.7271140813827515 2.4566450119018555 4.1837592124938965
Loss :  1.7278344631195068 2.6938111782073975 4.421645641326904
  batch 60 loss: 1.7278344631195068, 2.6938111782073975, 4.421645641326904
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7243437767028809 2.368068218231201 4.092411994934082
Loss :  1.7244281768798828 2.4790704250335693 4.203498840332031
Loss :  1.728922963142395 2.767943859100342 4.496866703033447
Loss :  1.7175216674804688 2.599714756011963 4.317236423492432
Loss :  1.71477472782135 2.8476414680480957 4.562416076660156
Loss :  1.6118966341018677 3.8443682193756104 5.456264972686768
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6076593399047852 3.89094614982605 5.498605728149414
Loss :  1.635407567024231 3.7338080406188965 5.369215488433838
Loss :  1.6534532308578491 3.6641058921813965 5.317559242248535
Total LOSS train 4.356704719250018 valid 5.410411357879639
CE LOSS train 1.7220460689984836 valid 0.4133633077144623
Contrastive LOSS train 2.63465864108159 valid 0.9160264730453491
EPOCH 129:
Loss :  1.7209346294403076 3.420039176940918 5.140974044799805
Loss :  1.7238106727600098 2.9844329357147217 4.708243370056152
Loss :  1.716151475906372 2.1559934616088867 3.872144937515259
Loss :  1.7180944681167603 2.536468982696533 4.254563331604004
Loss :  1.726548194885254 1.9786269664764404 3.7051751613616943
Loss :  1.7251821756362915 2.5957627296447754 4.320944786071777
Loss :  1.724243402481079 2.6522371768951416 4.376480579376221
Loss :  1.7213432788848877 3.324157238006592 5.045500755310059
Loss :  1.7211947441101074 2.127241611480713 3.8484363555908203
Loss :  1.7029556035995483 2.067688226699829 3.770643711090088
Loss :  1.7217769622802734 2.3898730278015137 4.111649990081787
Loss :  1.7380001544952393 2.0603296756744385 3.7983298301696777
Loss :  1.7227116823196411 2.0840530395507812 3.806764602661133
Loss :  1.7204827070236206 2.1882786750793457 3.908761501312256
Loss :  1.723144769668579 2.666271924972534 4.389416694641113
Loss :  1.7111879587173462 2.5009713172912598 4.212159156799316
Loss :  1.7229422330856323 2.864039659500122 4.586981773376465
Loss :  1.7174044847488403 2.4032399654388428 4.120644569396973
Loss :  1.7173001766204834 2.668893575668335 4.386193752288818
Loss :  1.7124723196029663 1.8104602098464966 3.522932529449463
  batch 20 loss: 1.7124723196029663, 1.8104602098464966, 3.522932529449463
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7198984622955322 2.0632548332214355 3.7831532955169678
Loss :  1.7236382961273193 1.9814049005508423 3.705043315887451
Loss :  1.7224047183990479 2.6917598247528076 4.4141645431518555
Loss :  1.7325090169906616 2.587909460067749 4.320418357849121
Loss :  1.7194958925247192 2.6295197010040283 4.349015712738037
Loss :  1.7179592847824097 2.395921468734741 4.113880634307861
Loss :  1.7302818298339844 2.535027265548706 4.2653093338012695
Loss :  1.7180954217910767 2.781501531600952 4.499597072601318
Loss :  1.728502869606018 3.073943853378296 4.8024468421936035
Loss :  1.713697075843811 2.8127200603485107 4.526417255401611
Loss :  1.7349364757537842 2.450303792953491 4.185240268707275
Loss :  1.7241593599319458 2.26497220993042 3.989131450653076
Loss :  1.7155730724334717 1.9157822132110596 3.6313552856445312
Loss :  1.7135252952575684 2.747967481613159 4.461492538452148
Loss :  1.723750114440918 3.0701985359191895 4.793948650360107
Loss :  1.725387692451477 2.8121719360351562 4.537559509277344
Loss :  1.72163724899292 2.1789841651916504 3.9006214141845703
Loss :  1.713724136352539 1.9561326503753662 3.6698567867279053
Loss :  1.7190771102905273 2.430375337600708 4.149452209472656
Loss :  1.7185171842575073 2.0065817832946777 3.7250990867614746
  batch 40 loss: 1.7185171842575073, 2.0065817832946777, 3.7250990867614746
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.719196081161499 2.2520594596862793 3.9712555408477783
Loss :  1.7134790420532227 1.9159437417984009 3.629422664642334
Loss :  1.7159786224365234 2.6892309188842773 4.405209541320801
Loss :  1.7199078798294067 2.684192180633545 4.404099941253662
Loss :  1.7206498384475708 2.405121326446533 4.1257710456848145
Loss :  1.716928482055664 2.1178574562072754 3.8347859382629395
Loss :  1.7195780277252197 1.922926664352417 3.6425046920776367
Loss :  1.7162644863128662 2.067331314086914 3.7835958003997803
Loss :  1.7213189601898193 2.618161678314209 4.339480400085449
Loss :  1.715848445892334 4.141998291015625 5.857846736907959
Loss :  1.7119697332382202 2.926608085632324 4.638577938079834
Loss :  1.71882164478302 3.254427194595337 4.9732489585876465
Loss :  1.7326420545578003 2.6717782020568848 4.404420375823975
Loss :  1.716274380683899 3.297455310821533 5.013729572296143
Loss :  1.7204688787460327 4.818458080291748 6.53892707824707
Loss :  1.7230380773544312 3.3132245540618896 5.036262512207031
Loss :  1.7292381525039673 2.842189311981201 4.571427345275879
Loss :  1.7251161336898804 2.9075303077697754 4.632646560668945
Loss :  1.7287541627883911 3.6575632095336914 5.386317253112793
Loss :  1.7295125722885132 2.9452965259552 4.674808979034424
  batch 60 loss: 1.7295125722885132, 2.9452965259552, 4.674808979034424
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7276878356933594 3.539257287979126 5.266944885253906
Loss :  1.7274839878082275 4.0625104904174805 5.789994239807129
Loss :  1.7300060987472534 3.530703067779541 5.260709285736084
Loss :  1.7205477952957153 2.77134370803833 4.491891384124756
Loss :  1.717301845550537 2.465136766433716 4.182438850402832
Loss :  1.5775607824325562 4.090457439422607 5.668018341064453
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.5764912366867065 3.829393148422241 5.405884265899658
Loss :  1.600974678993225 3.827915668487549 5.428890228271484
Loss :  1.6182329654693604 3.7618918418884277 5.380125045776367
Total LOSS train 4.377945577181302 valid 5.470729470252991
CE LOSS train 1.7212717826549824 valid 0.4045582413673401
Contrastive LOSS train 2.656673811032222 valid 0.9404729604721069
EPOCH 130:
Loss :  1.721309781074524 3.051859140396118 4.773169040679932
Loss :  1.7246423959732056 2.535229444503784 4.259871959686279
Loss :  1.7171962261199951 1.8095561265945435 3.526752471923828
Loss :  1.7191946506500244 1.9786741733551025 3.697868824005127
Loss :  1.7270091772079468 2.062478542327881 3.789487838745117
Loss :  1.7263555526733398 2.0088729858398438 3.7352285385131836
Loss :  1.7244778871536255 2.313964605331421 4.038442611694336
Loss :  1.721984624862671 2.3255209922790527 4.0475053787231445
Loss :  1.7222671508789062 2.122108221054077 3.8443753719329834
Loss :  1.7041665315628052 2.516256093978882 4.220422744750977
Loss :  1.7218066453933716 2.6261937618255615 4.348000526428223
Loss :  1.7409543991088867 2.541465997695923 4.2824201583862305
Loss :  1.7233396768569946 2.248760938644409 3.9721007347106934
Loss :  1.7217000722885132 1.9027982950210571 3.6244983673095703
Loss :  1.7251298427581787 1.9819427728652954 3.7070727348327637
Loss :  1.714699625968933 2.283848762512207 3.9985485076904297
Loss :  1.7248951196670532 2.138843297958374 3.863738536834717
Loss :  1.719844102859497 2.7574262619018555 4.477270126342773
Loss :  1.7210584878921509 1.9987200498580933 3.719778537750244
Loss :  1.7151519060134888 2.375152349472046 4.090304374694824
  batch 20 loss: 1.7151519060134888, 2.375152349472046, 4.090304374694824
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.722257375717163 2.7915124893188477 4.51377010345459
Loss :  1.7259163856506348 2.363434076309204 4.089350700378418
Loss :  1.7257435321807861 2.3171918392181396 4.042935371398926
Loss :  1.7347970008850098 2.3416049480438232 4.076401710510254
Loss :  1.723409652709961 2.506394624710083 4.229804039001465
Loss :  1.7210408449172974 2.271026849746704 3.992067813873291
Loss :  1.73166823387146 2.6050875186920166 4.336755752563477
Loss :  1.7187139987945557 2.252490758895874 3.9712047576904297
Loss :  1.7290760278701782 2.072636365890503 3.8017125129699707
Loss :  1.7147291898727417 2.6784658432006836 4.393195152282715
Loss :  1.7360444068908691 2.639657735824585 4.375701904296875
Loss :  1.7238341569900513 2.430325746536255 4.154160022735596
Loss :  1.7153048515319824 2.3488619327545166 4.064167022705078
Loss :  1.7140246629714966 2.3266897201538086 4.040714263916016
Loss :  1.7246932983398438 2.4536750316619873 4.17836856842041
Loss :  1.725841760635376 2.2956738471984863 4.021515846252441
Loss :  1.7219278812408447 1.9377098083496094 3.659637689590454
Loss :  1.7142045497894287 2.2708044052124023 3.985008955001831
Loss :  1.7210521697998047 2.2649953365325928 3.9860475063323975
Loss :  1.720969557762146 1.8569661378860474 3.5779356956481934
  batch 40 loss: 1.720969557762146, 1.8569661378860474, 3.5779356956481934
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.720435619354248 2.015075206756592 3.73551082611084
Loss :  1.7158279418945312 2.2953803539276123 4.011208534240723
Loss :  1.7176259756088257 3.0703744888305664 4.788000583648682
Loss :  1.7200422286987305 3.2426862716674805 4.962728500366211
Loss :  1.7217124700546265 3.93205189704895 5.653764247894287
Loss :  1.719832420349121 3.834101438522339 5.553934097290039
Loss :  1.7237416505813599 3.2831990718841553 5.006940841674805
Loss :  1.717492699623108 2.9199774265289307 4.637470245361328
Loss :  1.7252042293548584 4.010547637939453 5.735752105712891
Loss :  1.7166626453399658 2.8149328231811523 4.531595230102539
Loss :  1.7131403684616089 2.71687388420105 4.430014133453369
Loss :  1.719757318496704 2.734328508377075 4.454085826873779
Loss :  1.7329343557357788 2.2528884410858154 3.9858226776123047
Loss :  1.7171226739883423 2.212235450744629 3.9293580055236816
Loss :  1.719238042831421 2.2283542156219482 3.947592258453369
Loss :  1.7236196994781494 2.4034531116485596 4.127072811126709
Loss :  1.7278871536254883 2.4119503498077393 4.139837265014648
Loss :  1.7235920429229736 1.9714778661727905 3.6950697898864746
Loss :  1.7271093130111694 2.4214210510253906 4.14853048324585
Loss :  1.7281306982040405 2.077756404876709 3.805887222290039
  batch 60 loss: 1.7281306982040405, 2.077756404876709, 3.805887222290039
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7251790761947632 2.111189842224121 3.836369037628174
Loss :  1.725825548171997 2.1623682975769043 3.8881938457489014
Loss :  1.7292850017547607 2.621011257171631 4.3502960205078125
Loss :  1.7177425622940063 2.875403881072998 4.593146324157715
Loss :  1.7152425050735474 2.219087600708008 3.9343299865722656
Loss :  1.6407893896102905 3.7233283519744873 5.364117622375488
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6271640062332153 3.431598424911499 5.058762550354004
Loss :  1.6611223220825195 3.3816170692443848 5.042739391326904
Loss :  1.673272728919983 3.475456714630127 5.14872932434082
Total LOSS train 4.175228056540856 valid 5.153587222099304
CE LOSS train 1.7222587328690748 valid 0.4183181822299957
Contrastive LOSS train 2.4529693016639125 valid 0.8688641786575317
EPOCH 131:
Loss :  1.720831274986267 2.405761241912842 4.126592636108398
Loss :  1.7246201038360596 2.895601749420166 4.620222091674805
Loss :  1.7170672416687012 2.5718014240264893 4.2888689041137695
Loss :  1.7194147109985352 2.0450563430786133 3.7644710540771484
Loss :  1.727092981338501 2.5254671573638916 4.252560138702393
Loss :  1.726517915725708 3.4664323329925537 5.192950248718262
Loss :  1.72416353225708 3.0167253017425537 4.740888595581055
Loss :  1.7223873138427734 2.2048075199127197 3.927194833755493
Loss :  1.7225496768951416 2.292320489883423 4.0148701667785645
Loss :  1.7036759853363037 3.1453354358673096 4.849011421203613
Loss :  1.7311004400253296 4.10584831237793 5.836948871612549
Loss :  1.7395520210266113 2.541365385055542 4.280917167663574
Loss :  1.7193169593811035 3.5089094638824463 5.228226661682129
Loss :  1.7215306758880615 2.471647024154663 4.193177700042725
Loss :  1.726055383682251 2.396730899810791 4.122786521911621
Loss :  1.712178349494934 3.280174493789673 4.9923529624938965
Loss :  1.7245819568634033 2.6973378658294678 4.421919822692871
Loss :  1.7191162109375 2.994741678237915 4.713857650756836
Loss :  1.7182866334915161 2.5995490550994873 4.317835807800293
Loss :  1.715841293334961 2.220736503601074 3.936577796936035
  batch 20 loss: 1.715841293334961, 2.220736503601074, 3.936577796936035
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.722489595413208 2.5371134281158447 4.259603023529053
Loss :  1.726168155670166 3.1545586585998535 4.8807268142700195
Loss :  1.7248584032058716 3.1053271293640137 4.830185413360596
Loss :  1.7338014841079712 2.0200586318969727 3.7538599967956543
Loss :  1.719947338104248 3.058217763900757 4.778164863586426
Loss :  1.719721794128418 2.6919164657592773 4.411638259887695
Loss :  1.731043815612793 2.5030317306518555 4.234075546264648
Loss :  1.719584584236145 2.1970362663269043 3.9166207313537598
Loss :  1.729435920715332 2.196732521057129 3.926168441772461
Loss :  1.714383840560913 2.5164480209350586 4.230832099914551
Loss :  1.7351456880569458 2.346714496612549 4.081860065460205
Loss :  1.7249234914779663 2.8825316429138184 4.607455253601074
Loss :  1.7164456844329834 2.3798913955688477 4.09633731842041
Loss :  1.7142938375473022 1.9430853128433228 3.657379150390625
Loss :  1.7255746126174927 2.4580790996551514 4.183653831481934
Loss :  1.72636878490448 2.272418260574341 3.9987869262695312
Loss :  1.7233916521072388 2.271867275238037 3.9952588081359863
Loss :  1.7173185348510742 2.0736265182495117 3.790945053100586
Loss :  1.7221295833587646 2.4118525981903076 4.133982181549072
Loss :  1.7218573093414307 1.9228395223617554 3.6446967124938965
  batch 40 loss: 1.7218573093414307, 1.9228395223617554, 3.6446967124938965
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.722659707069397 2.2082459926605225 3.930905818939209
Loss :  1.7162121534347534 2.639063835144043 4.355276107788086
Loss :  1.7181451320648193 3.365159273147583 5.083304405212402
Loss :  1.721168041229248 3.336214303970337 5.057382583618164
Loss :  1.723400592803955 2.1176669597625732 3.8410675525665283
Loss :  1.721823811531067 2.4013187885284424 4.123142719268799
Loss :  1.7247735261917114 2.29488468170166 4.019658088684082
Loss :  1.718735933303833 2.1872305870056152 3.9059665203094482
Loss :  1.7286475896835327 2.568087100982666 4.296734809875488
Loss :  1.71687650680542 2.5855369567871094 4.302413463592529
Loss :  1.7138093709945679 2.506730079650879 4.220539569854736
Loss :  1.7210776805877686 2.5221683979034424 4.243246078491211
Loss :  1.7341561317443848 2.3802003860473633 4.114356517791748
Loss :  1.71743905544281 2.3661584854125977 4.083597660064697
Loss :  1.719555139541626 2.3538622856140137 4.073417663574219
Loss :  1.7246155738830566 2.1395277976989746 3.8641433715820312
Loss :  1.728844165802002 2.0885112285614014 3.8173553943634033
Loss :  1.724026083946228 2.8433473110198975 4.567373275756836
Loss :  1.728037714958191 2.5283284187316895 4.25636625289917
Loss :  1.7289931774139404 2.533076763153076 4.2620697021484375
  batch 60 loss: 1.7289931774139404, 2.533076763153076, 4.2620697021484375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.725646734237671 1.9226540327072144 3.6483006477355957
Loss :  1.7261292934417725 1.8637311458587646 3.589860439300537
Loss :  1.7293070554733276 2.068977117538452 3.7982840538024902
Loss :  1.717532753944397 2.1968495845794678 3.9143824577331543
Loss :  1.7148698568344116 1.6346726417541504 3.3495426177978516
Loss :  1.589340090751648 4.1597113609313965 5.749051570892334
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.5881215333938599 4.069413185119629 5.657534599304199
Loss :  1.6140599250793457 3.8477020263671875 5.461761951446533
Loss :  1.6230179071426392 3.864655017852783 5.487672805786133
Total LOSS train 4.245433066441462 valid 5.5890052318573
CE LOSS train 1.7226345777511596 valid 0.4057544767856598
Contrastive LOSS train 2.5227984703504123 valid 0.9661637544631958
EPOCH 132:
Loss :  1.7214839458465576 1.7365714311599731 3.4580554962158203
Loss :  1.7244569063186646 3.2277677059173584 4.9522247314453125
Loss :  1.7171413898468018 2.2464845180511475 3.963625907897949
Loss :  1.7207775115966797 2.821575164794922 4.542352676391602
Loss :  1.7283093929290771 2.135288953781128 3.863598346710205
Loss :  1.7283967733383179 2.3027446269989014 4.03114128112793
Loss :  1.7260159254074097 3.7323429584503174 5.4583587646484375
Loss :  1.724350929260254 1.879149317741394 3.6035003662109375
Loss :  1.7238688468933105 1.8272125720977783 3.551081418991089
Loss :  1.7062855958938599 1.7865715026855469 3.492856979370117
Loss :  1.7242283821105957 2.1256120204925537 3.8498404026031494
Loss :  1.7402349710464478 2.013371467590332 3.7536063194274902
Loss :  1.7234407663345337 2.1836822032928467 3.90712308883667
Loss :  1.722183346748352 1.981493353843689 3.703676700592041
Loss :  1.725190281867981 2.1953341960906982 3.9205245971679688
Loss :  1.713554859161377 2.3115999698638916 4.025155067443848
Loss :  1.725480079650879 2.2484121322631836 3.9738922119140625
Loss :  1.719810962677002 1.8448182344436646 3.564629077911377
Loss :  1.7204220294952393 1.7617775201797485 3.4821996688842773
Loss :  1.7170417308807373 1.7684550285339355 3.485496759414673
  batch 20 loss: 1.7170417308807373, 1.7684550285339355, 3.485496759414673
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7236722707748413 2.120293140411377 3.843965530395508
Loss :  1.7274333238601685 2.3612184524536133 4.088651657104492
Loss :  1.7277507781982422 2.295362949371338 4.02311372756958
Loss :  1.7359453439712524 2.0741019248962402 3.810047149658203
Loss :  1.7241106033325195 2.335205554962158 4.059316158294678
Loss :  1.7243547439575195 2.2273223400115967 3.951677083969116
Loss :  1.734794020652771 2.4302656650543213 4.165059566497803
Loss :  1.723317265510559 2.5712170600891113 4.294534206390381
Loss :  1.732088565826416 2.1483051776885986 3.8803937435150146
Loss :  1.7174291610717773 2.453829288482666 4.171258449554443
Loss :  1.7372056245803833 2.463616371154785 4.200821876525879
Loss :  1.726733922958374 2.2834370136260986 4.010170936584473
Loss :  1.7182830572128296 2.218355655670166 3.936638832092285
Loss :  1.716873049736023 2.166614055633545 3.8834872245788574
Loss :  1.7256935834884644 2.5038979053497314 4.229591369628906
Loss :  1.7285311222076416 2.300374746322632 4.028905868530273
Loss :  1.7256072759628296 2.390345335006714 4.115952491760254
Loss :  1.7199584245681763 1.9721219539642334 3.692080497741699
Loss :  1.7236709594726562 1.9020441770553589 3.6257152557373047
Loss :  1.7251111268997192 1.8519119024276733 3.5770230293273926
  batch 40 loss: 1.7251111268997192, 1.8519119024276733, 3.5770230293273926
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.723549723625183 2.413619041442871 4.137168884277344
Loss :  1.7168896198272705 1.690767765045166 3.4076573848724365
Loss :  1.7178618907928467 2.2432451248168945 3.961107015609741
Loss :  1.7211085557937622 2.13352632522583 3.8546347618103027
Loss :  1.722814679145813 2.0681183338165283 3.790933132171631
Loss :  1.720439076423645 2.1922919750213623 3.912731170654297
Loss :  1.722794771194458 2.11480450630188 3.837599277496338
Loss :  1.7179973125457764 2.487218141555786 4.2052154541015625
Loss :  1.7246919870376587 2.2069878578186035 3.9316797256469727
Loss :  1.7164465188980103 2.458674907684326 4.175121307373047
Loss :  1.7120709419250488 2.3544704914093018 4.06654167175293
Loss :  1.719834327697754 2.265847682952881 3.9856820106506348
Loss :  1.733098030090332 1.9438735246658325 3.676971435546875
Loss :  1.7165673971176147 2.137812376022339 3.854379653930664
Loss :  1.7207163572311401 2.089308977127075 3.810025215148926
Loss :  1.7244036197662354 2.242137908935547 3.9665415287017822
Loss :  1.728903889656067 2.2539446353912354 3.982848644256592
Loss :  1.7246356010437012 2.277425527572632 4.002060890197754
Loss :  1.7290807962417603 2.621121644973755 4.350202560424805
Loss :  1.7294479608535767 2.1846706867218018 3.914118766784668
  batch 60 loss: 1.7294479608535767, 2.1846706867218018, 3.914118766784668
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7262418270111084 2.0405075550079346 3.766749382019043
Loss :  1.727779746055603 2.6042866706848145 4.332066535949707
Loss :  1.7294821739196777 1.9648947715759277 3.6943769454956055
Loss :  1.7196044921875 2.5988805294036865 4.318485260009766
Loss :  1.717137336730957 1.9202439785003662 3.6373813152313232
Loss :  1.608139991760254 3.963148832321167 5.5712890625
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6036577224731445 3.967459201812744 5.571116924285889
Loss :  1.6245988607406616 3.8787269592285156 5.503325939178467
Loss :  1.6420704126358032 3.8152225017547607 5.4572930335998535
Total LOSS train 3.9499019145965577 valid 5.525756239891052
CE LOSS train 1.7236128843747653 valid 0.4105176031589508
Contrastive LOSS train 2.2262890228858363 valid 0.9538056254386902
EPOCH 133:
Loss :  1.7219101190567017 2.5340986251831055 4.256008625030518
Loss :  1.7242763042449951 2.3427462577819824 4.067022323608398
Loss :  1.718347430229187 2.2460196018218994 3.964366912841797
Loss :  1.7207835912704468 2.298031806945801 4.018815517425537
Loss :  1.728592872619629 1.7539702653884888 3.482563018798828
Loss :  1.7275439500808716 2.045722723007202 3.7732667922973633
Loss :  1.726954460144043 2.211873769760132 3.938828229904175
Loss :  1.7247403860092163 2.3273532390594482 4.052093505859375
Loss :  1.7256629467010498 1.8553180694580078 3.5809810161590576
Loss :  1.7088000774383545 2.3804359436035156 4.089236259460449
Loss :  1.7263481616973877 2.4332761764526367 4.159624099731445
Loss :  1.740816354751587 2.353767156600952 4.094583511352539
Loss :  1.7271842956542969 2.2566752433776855 3.9838595390319824
Loss :  1.725385308265686 2.183502674102783 3.9088878631591797
Loss :  1.7286239862442017 2.225419521331787 3.954043388366699
Loss :  1.7182154655456543 2.6452829837799072 4.363498687744141
Loss :  1.7290881872177124 2.5821237564086914 4.311212062835693
Loss :  1.7238819599151611 2.0337636470794678 3.757645606994629
Loss :  1.7231717109680176 3.1419339179992676 4.865105628967285
Loss :  1.7193220853805542 3.5492982864379883 5.268620491027832
  batch 20 loss: 1.7193220853805542, 3.5492982864379883, 5.268620491027832
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.725070595741272 3.1080009937286377 4.833071708679199
Loss :  1.7277065515518188 2.7524819374084473 4.480188369750977
Loss :  1.7282010316848755 2.2710378170013428 3.999238967895508
Loss :  1.734176516532898 2.635277509689331 4.3694539070129395
Loss :  1.7237993478775024 2.5574073791503906 4.2812066078186035
Loss :  1.7220598459243774 1.9837040901184082 3.705763816833496
Loss :  1.731811285018921 2.645655393600464 4.377466678619385
Loss :  1.7195526361465454 3.4498097896575928 5.169362545013428
Loss :  1.7292641401290894 2.4851889610290527 4.214453220367432
Loss :  1.7151025533676147 2.406528949737549 4.121631622314453
Loss :  1.7364416122436523 2.4805588722229004 4.217000484466553
Loss :  1.7252875566482544 1.9268275499343872 3.6521151065826416
Loss :  1.7165515422821045 2.186445951461792 3.9029974937438965
Loss :  1.7159010171890259 2.4715921878814697 4.187493324279785
Loss :  1.726270318031311 2.9955084323883057 4.721778869628906
Loss :  1.727733850479126 2.7671406269073486 4.494874477386475
Loss :  1.7241920232772827 2.4092369079589844 4.133429050445557
Loss :  1.7175747156143188 2.1628642082214355 3.880438804626465
Loss :  1.7224946022033691 2.4788849353790283 4.201379776000977
Loss :  1.7232377529144287 3.817897319793701 5.541134834289551
  batch 40 loss: 1.7232377529144287, 3.817897319793701, 5.541134834289551
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7233222723007202 3.4775896072387695 5.200911998748779
Loss :  1.7175110578536987 3.0077626705169678 4.725273609161377
Loss :  1.7195912599563599 3.095290184020996 4.814881324768066
Loss :  1.7221747636795044 3.054016351699829 4.776191234588623
Loss :  1.7247737646102905 2.8275301456451416 4.552303791046143
Loss :  1.7219916582107544 3.2310121059417725 4.953003883361816
Loss :  1.724650502204895 3.4047694206237793 5.129419803619385
Loss :  1.7198717594146729 2.864039897918701 4.583911895751953
Loss :  1.7286758422851562 2.5557894706726074 4.284465312957764
Loss :  1.7181540727615356 2.482527494430542 4.200681686401367
Loss :  1.712934970855713 2.308925151824951 4.021860122680664
Loss :  1.7214325666427612 2.633784532546997 4.355216979980469
Loss :  1.7326985597610474 1.784568190574646 3.5172667503356934
Loss :  1.7167198657989502 1.8805625438690186 3.5972824096679688
Loss :  1.7184478044509888 2.024559259414673 3.743007183074951
Loss :  1.7238661050796509 2.347283124923706 4.0711493492126465
Loss :  1.728176474571228 2.442418336868286 4.170594692230225
Loss :  1.7250910997390747 2.7397828102111816 4.464873790740967
Loss :  1.7289105653762817 3.006058692932129 4.734969139099121
Loss :  1.7305880784988403 2.4843504428863525 4.214938640594482
  batch 60 loss: 1.7305880784988403, 2.4843504428863525, 4.214938640594482
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7280808687210083 3.0887367725372314 4.816817760467529
Loss :  1.7277601957321167 2.8752553462982178 4.603015422821045
Loss :  1.733542799949646 3.636246681213379 5.3697896003723145
Loss :  1.7206037044525146 3.4074082374572754 5.128011703491211
Loss :  1.7180454730987549 3.103630781173706 4.821676254272461
Loss :  1.5954259634017944 3.769453763961792 5.364879608154297
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.5906670093536377 3.5500662326812744 5.140733242034912
Loss :  1.6186691522598267 3.5105350017547607 5.129204273223877
Loss :  1.6452648639678955 3.5850605964660645 5.230325698852539
Total LOSS train 4.326557801320003 valid 5.216285705566406
CE LOSS train 1.7241491574507493 valid 0.4113162159919739
Contrastive LOSS train 2.602408642035264 valid 0.8962651491165161
EPOCH 134:
Loss :  1.7238883972167969 2.624361991882324 4.348250389099121
Loss :  1.7287875413894653 2.8305699825286865 4.559357643127441
Loss :  1.7193603515625 2.565962553024292 4.285323143005371
Loss :  1.7202115058898926 3.104858636856079 4.825070381164551
Loss :  1.7303178310394287 2.803914785385132 4.5342326164245605
Loss :  1.7288691997528076 2.02915358543396 3.7580227851867676
Loss :  1.7295633554458618 2.0986831188201904 3.828246593475342
Loss :  1.727034568786621 1.6369590759277344 3.3639936447143555
Loss :  1.7243599891662598 1.7630600929260254 3.487420082092285
Loss :  1.7089550495147705 1.6614538431167603 3.3704090118408203
Loss :  1.7279638051986694 2.1730668544769287 3.9010305404663086
Loss :  1.7371426820755005 2.158188581466675 3.895331382751465
Loss :  1.7277590036392212 2.044224262237549 3.7719831466674805
Loss :  1.7252695560455322 2.8763322830200195 4.601601600646973
Loss :  1.7301770448684692 2.5347344875335693 4.264911651611328
Loss :  1.7157514095306396 2.5028340816497803 4.21858549118042
Loss :  1.7284483909606934 2.806166410446167 4.534614562988281
Loss :  1.7241547107696533 1.8827475309371948 3.6069021224975586
Loss :  1.7198216915130615 1.840433120727539 3.5602548122406006
Loss :  1.719979166984558 2.573683261871338 4.2936625480651855
  batch 20 loss: 1.719979166984558, 2.573683261871338, 4.2936625480651855
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7251731157302856 2.176335096359253 3.901508331298828
Loss :  1.7283579111099243 2.271411657333374 3.999769687652588
Loss :  1.72798752784729 3.7979350090026855 5.525922775268555
Loss :  1.7354986667633057 2.2864158153533936 4.021914482116699
Loss :  1.721347689628601 2.2278246879577637 3.9491724967956543
Loss :  1.7216726541519165 2.643009901046753 4.364682674407959
Loss :  1.7335000038146973 2.24434232711792 3.977842330932617
Loss :  1.7237446308135986 2.743316650390625 4.4670610427856445
Loss :  1.7324466705322266 2.5490739345550537 4.281520843505859
Loss :  1.714290738105774 3.3179447650909424 5.032235622406006
Loss :  1.734876036643982 2.7229440212249756 4.457819938659668
Loss :  1.7276419401168823 2.6830179691314697 4.4106597900390625
Loss :  1.7190831899642944 2.3881821632385254 4.107265472412109
Loss :  1.7155917882919312 2.4828476905822754 4.198439598083496
Loss :  1.7261784076690674 2.5347726345062256 4.260951042175293
Loss :  1.7275879383087158 2.4061591625213623 4.133747100830078
Loss :  1.725016713142395 2.618360996246338 4.343377590179443
Loss :  1.7205809354782104 2.317983865737915 4.038564682006836
Loss :  1.7220991849899292 3.488004446029663 5.210103511810303
Loss :  1.7228699922561646 2.50411057472229 4.226980686187744
  batch 40 loss: 1.7228699922561646, 2.50411057472229, 4.226980686187744
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.722334623336792 2.684142827987671 4.406477451324463
Loss :  1.7142246961593628 2.740831136703491 4.4550557136535645
Loss :  1.7170778512954712 1.80879807472229 3.525876045227051
Loss :  1.7203818559646606 1.6952773332595825 3.415659189224243
Loss :  1.7230379581451416 1.5382248163223267 3.261262893676758
Loss :  1.7213364839553833 1.7807401418685913 3.5020766258239746
Loss :  1.7230666875839233 2.3097872734069824 4.032854080200195
Loss :  1.7197624444961548 2.426757335662842 4.146519660949707
Loss :  1.725691318511963 2.2738072872161865 3.9994986057281494
Loss :  1.7191376686096191 2.1337714195251465 3.8529090881347656
Loss :  1.7146084308624268 2.3977060317993164 4.112314224243164
Loss :  1.721832036972046 2.624129056930542 4.345961093902588
Loss :  1.7324714660644531 2.950446844100952 4.682918548583984
Loss :  1.7200977802276611 1.9956824779510498 3.715780258178711
Loss :  1.7235180139541626 2.0746607780456543 3.7981786727905273
Loss :  1.7239774465560913 3.151460886001587 4.875438213348389
Loss :  1.7303955554962158 3.3473973274230957 5.077793121337891
Loss :  1.7271209955215454 3.7565929889678955 5.4837141036987305
Loss :  1.7305915355682373 3.781034231185913 5.51162576675415
Loss :  1.7300134897232056 3.486086845397949 5.216100215911865
  batch 60 loss: 1.7300134897232056, 3.486086845397949, 5.216100215911865
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7269697189331055 3.476203203201294 5.20317268371582
Loss :  1.7281628847122192 3.3930823802948 5.121245384216309
Loss :  1.729305624961853 3.013315439224243 4.742620944976807
Loss :  1.7201775312423706 3.1449644565582275 4.865141868591309
Loss :  1.717807650566101 2.0880556106567383 3.805863380432129
Loss :  1.6767230033874512 3.768446683883667 5.445169448852539
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6659386157989502 3.621858835220337 5.287797451019287
Loss :  1.6850091218948364 3.651519775390625 5.336528778076172
Loss :  1.6852220296859741 3.4179153442382812 5.103137493133545
Total LOSS train 4.262689256668091 valid 5.293158292770386
CE LOSS train 1.7244071190173809 valid 0.42130550742149353
Contrastive LOSS train 2.5382821248127865 valid 0.8544788360595703
EPOCH 135:
Loss :  1.722391128540039 2.750153064727783 4.472544193267822
Loss :  1.7236697673797607 2.1178455352783203 3.841515302658081
Loss :  1.7174221277236938 2.0861892700195312 3.8036112785339355
Loss :  1.7202460765838623 1.992280125617981 3.712526321411133
Loss :  1.7276628017425537 1.7548028230667114 3.4824657440185547
Loss :  1.7254631519317627 1.717647671699524 3.443110942840576
Loss :  1.7248694896697998 1.9666749238967896 3.691544532775879
Loss :  1.722245693206787 1.7968285083770752 3.5190742015838623
Loss :  1.7238240242004395 1.9158613681793213 3.6396853923797607
Loss :  1.7067900896072388 1.9307094812393188 3.6374995708465576
Loss :  1.7246509790420532 2.2467527389526367 3.9714035987854004
Loss :  1.7396775484085083 2.4896225929260254 4.229300022125244
Loss :  1.7255560159683228 3.2206344604492188 4.946190357208252
Loss :  1.7236629724502563 2.886899709701538 4.610562801361084
Loss :  1.727938175201416 3.6514980792999268 5.379436492919922
Loss :  1.716178297996521 1.9301151037216187 3.6462934017181396
Loss :  1.7266972064971924 2.2409470081329346 3.967644214630127
Loss :  1.72196626663208 2.3912100791931152 4.113176345825195
Loss :  1.7208997011184692 2.7747654914855957 4.495665073394775
Loss :  1.7179123163223267 3.5901756286621094 5.3080878257751465
  batch 20 loss: 1.7179123163223267, 3.5901756286621094, 5.3080878257751465
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7244477272033691 3.5404562950134277 5.264904022216797
Loss :  1.7273404598236084 3.6305508613586426 5.357891082763672
Loss :  1.7272642850875854 3.6206393241882324 5.347903728485107
Loss :  1.7349233627319336 2.8838276863098145 4.618751049041748
Loss :  1.7230257987976074 2.768671751022339 4.491697311401367
Loss :  1.7216947078704834 2.3946962356567383 4.116391181945801
Loss :  1.7329922914505005 3.1633312702178955 4.8963236808776855
Loss :  1.7214586734771729 2.6790616512298584 4.400520324707031
Loss :  1.7308608293533325 2.273881196975708 4.00474214553833
Loss :  1.7147693634033203 2.257230043411255 3.971999406814575
Loss :  1.7351408004760742 2.2200520038604736 3.955192804336548
Loss :  1.726120114326477 2.0256295204162598 3.7517495155334473
Loss :  1.7175424098968506 2.7094130516052246 4.426955223083496
Loss :  1.7150375843048096 2.1998467445373535 3.914884328842163
Loss :  1.7256914377212524 2.5317819118499756 4.257473468780518
Loss :  1.726765513420105 2.5350821018218994 4.261847496032715
Loss :  1.72418212890625 2.3255646228790283 4.049746513366699
Loss :  1.7195619344711304 1.9623143672943115 3.6818761825561523
Loss :  1.7231063842773438 2.116626262664795 3.8397326469421387
Loss :  1.724547266960144 2.772594928741455 4.497142314910889
  batch 40 loss: 1.724547266960144, 2.772594928741455, 4.497142314910889
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.724133014678955 2.5483906269073486 4.272523880004883
Loss :  1.7174628973007202 2.3964383602142334 4.113901138305664
Loss :  1.719735860824585 2.9694976806640625 4.689233779907227
Loss :  1.7220005989074707 2.335679769515991 4.057680130004883
Loss :  1.724653720855713 2.9361374378204346 4.660791397094727
Loss :  1.7224843502044678 2.917983055114746 4.640467643737793
Loss :  1.7246098518371582 2.305011034011841 4.029621124267578
Loss :  1.7205923795700073 2.426457643508911 4.147049903869629
Loss :  1.7279728651046753 2.282172203063965 4.01014518737793
Loss :  1.7196227312088013 2.062734365463257 3.7823572158813477
Loss :  1.715919017791748 2.803215980529785 4.519134998321533
Loss :  1.7230067253112793 2.0942888259887695 3.817295551300049
Loss :  1.7344428300857544 1.9119757413864136 3.646418571472168
Loss :  1.720497488975525 2.0814573764801025 3.801954746246338
Loss :  1.7223906517028809 2.2863056659698486 4.008696556091309
Loss :  1.7256959676742554 2.74357271194458 4.469268798828125
Loss :  1.730404257774353 3.151634931564331 4.8820390701293945
Loss :  1.7270123958587646 2.846935987472534 4.573948383331299
Loss :  1.7301685810089111 2.652543783187866 4.382712364196777
Loss :  1.7296266555786133 1.9817085266113281 3.7113351821899414
  batch 60 loss: 1.7296266555786133, 1.9817085266113281, 3.7113351821899414
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.727115273475647 1.8987241983413696 3.6258394718170166
Loss :  1.7282178401947021 1.9798306226730347 3.7080483436584473
Loss :  1.7299671173095703 1.9455974102020264 3.6755645275115967
Loss :  1.7208603620529175 2.630643367767334 4.351503849029541
Loss :  1.7186018228530884 1.6836079359054565 3.402209758758545
Loss :  1.5665525197982788 4.250350475311279 5.816903114318848
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.566260814666748 4.310439109802246 5.876699924468994
Loss :  1.5869332551956177 4.155893325805664 5.742826461791992
Loss :  1.5899319648742676 4.308290481567383 5.89822244644165
Total LOSS train 4.184596978701078 valid 5.833662986755371
CE LOSS train 1.7240521871126615 valid 0.3974829912185669
Contrastive LOSS train 2.460544780584482 valid 1.0770726203918457
EPOCH 136:
Loss :  1.7230647802352905 2.0690207481384277 3.792085647583008
Loss :  1.7248388528823853 2.369473457336426 4.0943121910095215
Loss :  1.7194079160690308 2.2070212364196777 3.926429271697998
Loss :  1.7215601205825806 2.9754889011383057 4.697049140930176
Loss :  1.7291244268417358 2.2411820888519287 3.970306396484375
Loss :  1.7273203134536743 2.2508604526519775 3.9781808853149414
Loss :  1.727677345275879 2.8929786682128906 4.6206560134887695
Loss :  1.7246631383895874 2.404383897781372 4.12904691696167
Loss :  1.7251524925231934 2.8494067192077637 4.574559211730957
Loss :  1.7087544202804565 2.316810131072998 4.025564670562744
Loss :  1.7259221076965332 2.4512414932250977 4.177163600921631
Loss :  1.739689588546753 2.68009090423584 4.419780731201172
Loss :  1.726639986038208 3.1903598308563232 4.916999816894531
Loss :  1.7246580123901367 2.616297483444214 4.34095573425293
Loss :  1.7275335788726807 2.3115715980529785 4.039105415344238
Loss :  1.7165340185165405 1.8355090618133545 3.5520429611206055
Loss :  1.7269136905670166 1.9834057092666626 3.7103195190429688
Loss :  1.7228050231933594 2.471097230911255 4.193902015686035
Loss :  1.7214622497558594 1.96779203414917 3.6892542839050293
Loss :  1.7181956768035889 2.6620988845825195 4.3802947998046875
  batch 20 loss: 1.7181956768035889, 2.6620988845825195, 4.3802947998046875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7244644165039062 1.971629023551941 3.6960935592651367
Loss :  1.7276992797851562 2.4147822856903076 4.142481803894043
Loss :  1.7278211116790771 1.944449782371521 3.6722707748413086
Loss :  1.7346603870391846 3.044818639755249 4.779479026794434
Loss :  1.723783016204834 2.3751137256622314 4.0988969802856445
Loss :  1.7232979536056519 3.3886029720306396 5.111900806427002
Loss :  1.7336235046386719 3.062016487121582 4.795639991760254
Loss :  1.7233346700668335 3.5115835666656494 5.234918117523193
Loss :  1.7322840690612793 3.5462937355041504 5.27857780456543
Loss :  1.7168818712234497 2.753103494644165 4.469985485076904
Loss :  1.7362713813781738 3.0876946449279785 4.823966026306152
Loss :  1.7274110317230225 2.4024593830108643 4.129870414733887
Loss :  1.7187005281448364 2.3059306144714355 4.024631023406982
Loss :  1.716976284980774 3.069918394088745 4.786894798278809
Loss :  1.7260221242904663 2.6156415939331055 4.341663837432861
Loss :  1.7279523611068726 2.3417742252349854 4.069726467132568
Loss :  1.7260596752166748 2.7233316898345947 4.4493913650512695
Loss :  1.7196186780929565 2.395688056945801 4.115306854248047
Loss :  1.7245501279830933 2.396397590637207 4.12094783782959
Loss :  1.725890874862671 2.6564218997955322 4.382312774658203
  batch 40 loss: 1.725890874862671, 2.6564218997955322, 4.382312774658203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7246358394622803 2.725212574005127 4.449848175048828
Loss :  1.7188454866409302 2.8506064414978027 4.569451808929443
Loss :  1.7215008735656738 3.3489983081817627 5.070499420166016
Loss :  1.7234981060028076 2.304506778717041 4.0280046463012695
Loss :  1.7256938219070435 2.6385412216186523 4.364234924316406
Loss :  1.7242259979248047 3.25429105758667 4.978517055511475
Loss :  1.7265253067016602 2.9215760231018066 4.648101329803467
Loss :  1.7214868068695068 3.0910239219665527 4.8125104904174805
Loss :  1.7298681735992432 2.6161255836486816 4.345993995666504
Loss :  1.7206488847732544 3.124809741973877 4.845458507537842
Loss :  1.7160624265670776 3.226870536804199 4.942933082580566
Loss :  1.7233541011810303 2.953115224838257 4.676469326019287
Loss :  1.7342915534973145 2.9793057441711426 4.713597297668457
Loss :  1.719345211982727 2.15451717376709 3.8738622665405273
Loss :  1.7202061414718628 2.38120174407959 4.101408004760742
Loss :  1.7233946323394775 2.287310838699341 4.010705471038818
Loss :  1.7276322841644287 2.183685779571533 3.911318063735962
Loss :  1.7242076396942139 1.798987627029419 3.523195266723633
Loss :  1.7278110980987549 2.4032304286956787 4.131041526794434
Loss :  1.7289600372314453 2.465517520904541 4.194477558135986
  batch 60 loss: 1.7289600372314453, 2.465517520904541, 4.194477558135986
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7264182567596436 3.197283983230591 4.923702239990234
Loss :  1.7274945974349976 2.4568543434143066 4.184349060058594
Loss :  1.7324274778366089 2.9619719982147217 4.694399356842041
Loss :  1.721426010131836 3.331974744796753 5.053400993347168
Loss :  1.719198226928711 3.2629902362823486 4.9821882247924805
Loss :  1.5904743671417236 3.9164724349975586 5.506946563720703
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.5836764574050903 3.7134134769439697 5.29709005355835
Loss :  1.6104427576065063 3.8304245471954346 5.4408674240112305
Loss :  1.6217517852783203 3.75622820854187 5.3779802322387695
Total LOSS train 4.365886662556575 valid 5.405721068382263
CE LOSS train 1.724744308911837 valid 0.4054379463195801
Contrastive LOSS train 2.6411423371388363 valid 0.9390570521354675
EPOCH 137:
Loss :  1.7256180047988892 2.492124319076538 4.217742443084717
Loss :  1.7285685539245605 2.8896074295043945 4.618175983428955
Loss :  1.7215487957000732 2.8811540603637695 4.602703094482422
Loss :  1.7228524684906006 3.339871883392334 5.0627241134643555
Loss :  1.7315406799316406 3.473813533782959 5.2053542137146
Loss :  1.72934889793396 2.8396008014678955 4.5689496994018555
Loss :  1.7308883666992188 2.336120843887329 4.067008972167969
Loss :  1.7277765274047852 2.5562620162963867 4.284038543701172
Loss :  1.7274764776229858 2.6530263423919678 4.380502700805664
Loss :  1.7126142978668213 2.3661866188049316 4.078801155090332
Loss :  1.7292293310165405 2.1426784992218018 3.8719077110290527
Loss :  1.739915370941162 2.038142204284668 3.77805757522583
Loss :  1.7294784784317017 1.987396240234375 3.716874599456787
Loss :  1.7274539470672607 2.4090256690979004 4.136479377746582
Loss :  1.7309199571609497 2.808704376220703 4.539624214172363
Loss :  1.7202893495559692 2.023547410964966 3.7438368797302246
Loss :  1.729972004890442 2.008415937423706 3.7383880615234375
Loss :  1.7260774374008179 1.9811402559280396 3.7072176933288574
Loss :  1.7233331203460693 1.894162893295288 3.6174960136413574
Loss :  1.7210224866867065 2.623671054840088 4.344693660736084
  batch 20 loss: 1.7210224866867065, 2.623671054840088, 4.344693660736084
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7265031337738037 3.4637181758880615 5.190221309661865
Loss :  1.7294248342514038 3.6864395141601562 5.41586446762085
Loss :  1.7292654514312744 4.011368274688721 5.740633964538574
Loss :  1.7362419366836548 4.47847843170166 6.214720249176025
Loss :  1.7215771675109863 3.7891077995300293 5.510684967041016
Loss :  1.7255334854125977 3.4703898429870605 5.195923328399658
Loss :  1.7369413375854492 3.4221882820129395 5.159129619598389
Loss :  1.7291029691696167 2.855421543121338 4.584524631500244
Loss :  1.734235167503357 2.8768391609191895 4.611074447631836
Loss :  1.7266845703125 2.9462218284606934 4.672906398773193
Loss :  1.7357120513916016 2.906449317932129 4.6421613693237305
Loss :  1.7280604839324951 2.5332717895507812 4.2613325119018555
Loss :  1.7211205959320068 2.2568907737731934 3.9780113697052
Loss :  1.717230200767517 2.3634631633758545 4.080693244934082
Loss :  1.7272676229476929 2.4892337322235107 4.216501235961914
Loss :  1.7297283411026 2.142982006072998 3.8727102279663086
Loss :  1.727230429649353 2.102755546569824 3.829986095428467
Loss :  1.7251886129379272 2.383058786392212 4.10824728012085
Loss :  1.7288552522659302 3.0671675205230713 4.796022891998291
Loss :  1.726989507675171 2.389946222305298 4.116935729980469
  batch 40 loss: 1.726989507675171, 2.389946222305298, 4.116935729980469
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7281159162521362 3.361433982849121 5.089550018310547
Loss :  1.725042462348938 3.039846658706665 4.764889240264893
Loss :  1.7232458591461182 3.3048713207244873 5.0281171798706055
Loss :  1.7289668321609497 3.1341216564178467 4.863088607788086
Loss :  1.728528618812561 3.136120319366455 4.864648818969727
Loss :  1.7237370014190674 3.0832884311676025 4.80702543258667
Loss :  1.729191541671753 3.7639963626861572 5.49318790435791
Loss :  1.7243696451187134 2.721207857131958 4.445577621459961
Loss :  1.7303128242492676 3.2031774520874023 4.93349027633667
Loss :  1.720689296722412 2.7600629329681396 4.480751991271973
Loss :  1.7159123420715332 2.892936944961548 4.60884952545166
Loss :  1.7217934131622314 3.5236101150512695 5.245403289794922
Loss :  1.7352057695388794 3.087820529937744 4.823026180267334
Loss :  1.7200334072113037 2.4562761783599854 4.176309585571289
Loss :  1.7225887775421143 2.3543496131896973 4.076938629150391
Loss :  1.7289122343063354 2.16516375541687 3.894075870513916
Loss :  1.7298275232315063 2.6418282985687256 4.3716559410095215
Loss :  1.7277600765228271 2.173011302947998 3.900771379470825
Loss :  1.7304099798202515 2.657181739807129 4.38759183883667
Loss :  1.7317155599594116 2.1352696418762207 3.866985321044922
  batch 60 loss: 1.7317155599594116, 2.1352696418762207, 3.866985321044922
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.729727029800415 3.4820845127105713 5.211811542510986
Loss :  1.7302062511444092 3.1381380558013916 4.868344306945801
Loss :  1.734336495399475 3.2280430793762207 4.962379455566406
Loss :  1.7209383249282837 2.8772473335266113 4.5981855392456055
Loss :  1.717853307723999 1.7772274017333984 3.4950807094573975
Loss :  1.8717267513275146 4.36535120010376 6.237077713012695
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.866398811340332 4.346171855926514 6.212570667266846
Loss :  1.8674557209014893 4.187163352966309 6.054618835449219
Loss :  1.8530195951461792 4.128065586090088 5.981085300445557
Total LOSS train 4.519024588511541 valid 6.121338129043579
CE LOSS train 1.7270498184057383 valid 0.4632548987865448
Contrastive LOSS train 2.7919747627698457 valid 1.032016396522522
EPOCH 138:
Loss :  1.7237834930419922 2.2697412967681885 3.9935247898101807
Loss :  1.7277594804763794 3.494025230407715 5.221784591674805
Loss :  1.7199413776397705 2.6609506607055664 4.380891799926758
Loss :  1.7231101989746094 2.405592679977417 4.1287031173706055
Loss :  1.7287765741348267 2.3170788288116455 4.045855522155762
Loss :  1.7283843755722046 2.2782702445983887 4.006654739379883
Loss :  1.7256360054016113 3.0503499507904053 4.7759857177734375
Loss :  1.7253785133361816 2.478935718536377 4.204314231872559
Loss :  1.7235554456710815 1.8382741212844849 3.5618295669555664
Loss :  1.7053507566452026 1.771471619606018 3.4768223762512207
Loss :  1.7244075536727905 2.6020705699920654 4.326478004455566
Loss :  1.7407712936401367 2.5276787281036377 4.268449783325195
Loss :  1.7238497734069824 2.6968114376068115 4.420660972595215
Loss :  1.7239230871200562 2.5870296955108643 4.310952663421631
Loss :  1.729854702949524 3.4011194705963135 5.130974292755127
Loss :  1.7154760360717773 2.1229348182678223 3.8384108543395996
Loss :  1.726701021194458 2.8800699710845947 4.606770992279053
Loss :  1.719976544380188 3.219994306564331 4.939970970153809
Loss :  1.721699595451355 3.2990987300872803 5.020798206329346
Loss :  1.7180488109588623 3.348998785018921 5.067047595977783
  batch 20 loss: 1.7180488109588623, 3.348998785018921, 5.067047595977783
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.726030945777893 2.278254985809326 4.00428581237793
Loss :  1.7289183139801025 2.2927587032318115 4.021677017211914
Loss :  1.7283916473388672 2.4623970985412598 4.190788745880127
Loss :  1.7368013858795166 2.751326322555542 4.488127708435059
Loss :  1.7240476608276367 2.5230588912963867 4.247106552124023
Loss :  1.7236666679382324 2.8155221939086914 4.539188861846924
Loss :  1.7332724332809448 2.4327874183654785 4.166059970855713
Loss :  1.721463680267334 3.7159154415130615 5.437378883361816
Loss :  1.730881929397583 2.7620697021484375 4.492951393127441
Loss :  1.7202504873275757 2.8852317333221436 4.60548210144043
Loss :  1.7375147342681885 2.756253242492676 4.493767738342285
Loss :  1.7257137298583984 3.127553701400757 4.853267669677734
Loss :  1.7193111181259155 2.363189697265625 4.08250093460083
Loss :  1.7168856859207153 2.7236366271972656 4.440522193908691
Loss :  1.7259411811828613 2.7702314853668213 4.496172904968262
Loss :  1.7275974750518799 2.7506139278411865 4.478211402893066
Loss :  1.7256263494491577 3.0152902603149414 4.740916728973389
Loss :  1.7200307846069336 2.4835751056671143 4.203605651855469
Loss :  1.7255057096481323 2.2933833599090576 4.0188889503479
Loss :  1.724574089050293 2.065455198287964 3.790029287338257
  batch 40 loss: 1.724574089050293, 2.065455198287964, 3.790029287338257
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.724283218383789 2.713721513748169 4.438004493713379
Loss :  1.7205439805984497 2.0662569999694824 3.7868008613586426
Loss :  1.7210007905960083 2.4907824993133545 4.211783409118652
Loss :  1.7246698141098022 2.8036000728607178 4.5282697677612305
Loss :  1.7271275520324707 2.2329962253570557 3.9601237773895264
Loss :  1.724182367324829 2.5537428855895996 4.277925491333008
Loss :  1.7279655933380127 2.4656729698181152 4.193638801574707
Loss :  1.72248375415802 2.84098482131958 4.5634684562683105
Loss :  1.7313625812530518 3.602431297302246 5.333793640136719
Loss :  1.7186115980148315 2.6876275539398193 4.406239032745361
Loss :  1.7166825532913208 2.7971549034118652 4.5138373374938965
Loss :  1.7224605083465576 2.5984294414520264 4.320889949798584
Loss :  1.7343811988830566 3.3766791820526123 5.11106014251709
Loss :  1.718653917312622 2.904144763946533 4.622798919677734
Loss :  1.7227680683135986 3.153419017791748 4.876187324523926
Loss :  1.7268438339233398 2.8434112071990967 4.570255279541016
Loss :  1.7295440435409546 3.3189847469329834 5.048528671264648
Loss :  1.7261476516723633 3.137661933898926 4.863809585571289
Loss :  1.7294889688491821 3.3362913131713867 5.065780162811279
Loss :  1.7304726839065552 2.8939967155456543 4.62446928024292
  batch 60 loss: 1.7304726839065552, 2.8939967155456543, 4.62446928024292
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7272064685821533 2.5704727172851562 4.2976789474487305
Loss :  1.728283405303955 3.7445929050445557 5.47287654876709
Loss :  1.731024146080017 2.075784921646118 3.8068089485168457
Loss :  1.7200133800506592 2.4094672203063965 4.129480361938477
Loss :  1.7172130346298218 1.6080113649368286 3.3252243995666504
Loss :  1.8926713466644287 4.343303203582764 6.235974311828613
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8814291954040527 4.270627975463867 6.15205717086792
Loss :  1.8867366313934326 4.279633522033691 6.166370391845703
Loss :  1.8762409687042236 4.21401309967041 6.090253829956055
Total LOSS train 4.428731459837693 valid 6.161163926124573
CE LOSS train 1.7249573194063628 valid 0.4690602421760559
Contrastive LOSS train 2.703774171609145 valid 1.0535032749176025
EPOCH 139:
Loss :  1.7248812913894653 2.270314931869507 3.9951963424682617
Loss :  1.7261910438537598 2.053042411804199 3.779233455657959
Loss :  1.7186222076416016 1.8141247034072876 3.5327467918395996
Loss :  1.7233082056045532 1.948893427848816 3.672201633453369
Loss :  1.7291605472564697 3.36299204826355 5.0921525955200195
Loss :  1.7287184000015259 2.2910335063934326 4.019752025604248
Loss :  1.7277675867080688 2.6368589401245117 4.364626407623291
Loss :  1.7271333932876587 3.0139360427856445 4.741069316864014
Loss :  1.7257795333862305 2.658747434616089 4.384527206420898
Loss :  1.708146333694458 2.8608055114746094 4.568951606750488
Loss :  1.7266029119491577 3.0168256759643555 4.743428707122803
Loss :  1.7405049800872803 3.2254018783569336 4.965907096862793
Loss :  1.7265013456344604 2.428467035293579 4.15496826171875
Loss :  1.7247637510299683 2.6121137142181396 4.336877346038818
Loss :  1.7305879592895508 2.8060302734375 4.536618232727051
Loss :  1.7173243761062622 2.291386604309082 4.008710861206055
Loss :  1.7275712490081787 2.69712233543396 4.424693584442139
Loss :  1.7212437391281128 3.2279906272888184 4.949234485626221
Loss :  1.7206109762191772 2.907898187637329 4.628509044647217
Loss :  1.7174739837646484 2.9648287296295166 4.682302474975586
  batch 20 loss: 1.7174739837646484, 2.9648287296295166, 4.682302474975586
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7241075038909912 2.5526299476623535 4.276737213134766
Loss :  1.726501703262329 2.461979866027832 4.188481330871582
Loss :  1.726205825805664 3.606686592102051 5.332892417907715
Loss :  1.7315890789031982 3.4296319484710693 5.161221027374268
Loss :  1.7216496467590332 2.935133218765259 4.656783103942871
Loss :  1.722888708114624 3.4946999549865723 5.217588424682617
Loss :  1.7322081327438354 3.7897098064422607 5.521917819976807
Loss :  1.7212880849838257 3.3858752250671387 5.107163429260254
Loss :  1.7321363687515259 2.900240421295166 4.632376670837402
Loss :  1.716548204421997 2.6423110961914062 4.358859062194824
Loss :  1.7353848218917847 3.2846996784210205 5.020084381103516
Loss :  1.7250664234161377 3.022390842437744 4.747457504272461
Loss :  1.718693494796753 2.8377838134765625 4.5564775466918945
Loss :  1.71450936794281 2.9223132133483887 4.636822700500488
Loss :  1.7247815132141113 3.2536003589630127 4.978382110595703
Loss :  1.7267454862594604 3.154371738433838 4.881117343902588
Loss :  1.7242454290390015 2.8913207054138184 4.615566253662109
Loss :  1.71962571144104 2.122807741165161 3.842433452606201
Loss :  1.7238916158676147 2.322031259536743 4.045922756195068
Loss :  1.7243990898132324 2.5487916469573975 4.273190498352051
  batch 40 loss: 1.7243990898132324, 2.5487916469573975, 4.273190498352051
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7227116823196411 2.306898593902588 4.0296101570129395
Loss :  1.720209002494812 2.3870317935943604 4.107240676879883
Loss :  1.7205946445465088 2.3591277599334717 4.0797224044799805
Loss :  1.725399374961853 2.660759210586548 4.386158466339111
Loss :  1.7273619174957275 3.089183807373047 4.816545486450195
Loss :  1.7242263555526733 2.255723714828491 3.979949951171875
Loss :  1.7268803119659424 2.063462257385254 3.7903425693511963
Loss :  1.7228597402572632 2.175428628921509 3.8982882499694824
Loss :  1.7307385206222534 3.209052801132202 4.939791202545166
Loss :  1.7210501432418823 2.5680809020996094 4.289131164550781
Loss :  1.715988278388977 2.8470845222473145 4.563072681427002
Loss :  1.7225860357284546 2.9000868797302246 4.622673034667969
Loss :  1.7335973978042603 2.65785551071167 4.391452789306641
Loss :  1.7189829349517822 2.448112726211548 4.16709566116333
Loss :  1.7219146490097046 2.2800328731536865 4.001947402954102
Loss :  1.7247730493545532 2.56508731842041 4.289860248565674
Loss :  1.7275744676589966 2.8819777965545654 4.609552383422852
Loss :  1.7251040935516357 2.9842872619628906 4.7093915939331055
Loss :  1.7289085388183594 2.745105504989624 4.4740142822265625
Loss :  1.7299864292144775 2.00758957862854 3.7375760078430176
  batch 60 loss: 1.7299864292144775, 2.00758957862854, 3.7375760078430176
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7264190912246704 2.17805552482605 3.9044747352600098
Loss :  1.7279176712036133 2.4246017932891846 4.152519226074219
Loss :  1.7313904762268066 2.0189976692199707 3.7503881454467773
Loss :  1.7192150354385376 3.1436123847961426 4.862827301025391
Loss :  1.7162069082260132 3.2874226570129395 5.003629684448242
Loss :  1.863281488418579 4.185788154602051 6.049069404602051
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8502823114395142 4.213829040527344 6.064111232757568
Loss :  1.8547115325927734 4.082733154296875 5.937444686889648
Loss :  1.8541042804718018 4.173737049102783 6.027841567993164
Total LOSS train 4.449083662033081 valid 6.019616723060608
CE LOSS train 1.7245839504095224 valid 0.46352607011795044
Contrastive LOSS train 2.7244997317974384 valid 1.0434342622756958
EPOCH 140:
Loss :  1.724566102027893 3.385847330093384 5.110413551330566
Loss :  1.7260208129882812 3.3746469020843506 5.100667953491211
Loss :  1.717764139175415 3.210958957672119 4.928723335266113
Loss :  1.7221704721450806 2.108673095703125 3.830843448638916
Loss :  1.7291122674942017 2.0914175510406494 3.8205299377441406
Loss :  1.7283740043640137 3.148940324783325 4.877314567565918
Loss :  1.727077603340149 3.379241943359375 5.106319427490234
Loss :  1.726102352142334 3.074594020843506 4.80069637298584
Loss :  1.7237533330917358 2.3200247287750244 4.043777942657471
Loss :  1.7065132856369019 2.3198771476745605 4.026390552520752
Loss :  1.72614586353302 2.9264578819274902 4.652603626251221
Loss :  1.7382097244262695 2.7739858627319336 4.512195587158203
Loss :  1.7261534929275513 2.3840415477752686 4.110195159912109
Loss :  1.7240839004516602 2.015709638595581 3.739793539047241
Loss :  1.7285422086715698 2.2000982761383057 3.928640365600586
Loss :  1.7161439657211304 2.1258838176727295 3.8420276641845703
Loss :  1.7271673679351807 1.9559084177017212 3.6830759048461914
Loss :  1.7224959135055542 3.0243775844573975 4.746873378753662
Loss :  1.7212008237838745 2.086308240890503 3.807508945465088
Loss :  1.7188060283660889 2.586085319519043 4.304891586303711
  batch 20 loss: 1.7188060283660889, 2.586085319519043, 4.304891586303711
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7271738052368164 3.0131428241729736 4.740316390991211
Loss :  1.7280924320220947 2.090022325515747 3.818114757537842
Loss :  1.728485107421875 2.342921018600464 4.071406364440918
Loss :  1.7345556020736694 2.542135715484619 4.276691436767578
Loss :  1.7226663827896118 2.4973411560058594 4.220007419586182
Loss :  1.724176049232483 2.5653023719787598 4.289478302001953
Loss :  1.7343353033065796 3.4411542415618896 5.17548942565918
Loss :  1.7237287759780884 2.4956724643707275 4.2194013595581055
Loss :  1.7326412200927734 2.3832650184631348 4.115906238555908
Loss :  1.7185648679733276 2.6898422241210938 4.408407211303711
Loss :  1.7369163036346436 2.623906135559082 4.360822677612305
Loss :  1.7278120517730713 2.8831610679626465 4.610973358154297
Loss :  1.719972848892212 3.084162473678589 4.804135322570801
Loss :  1.7169201374053955 2.6159560680389404 4.332876205444336
Loss :  1.7261359691619873 3.0218288898468018 4.747964859008789
Loss :  1.7277131080627441 2.8223414421081543 4.550054550170898
Loss :  1.725630760192871 2.1623103618621826 3.8879411220550537
Loss :  1.7204209566116333 2.0357449054718018 3.7561659812927246
Loss :  1.7245408296585083 1.9452884197235107 3.6698293685913086
Loss :  1.7247852087020874 2.501086711883545 4.225872039794922
  batch 40 loss: 1.7247852087020874, 2.501086711883545, 4.225872039794922
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.723070740699768 2.8688924312591553 4.591963291168213
Loss :  1.7187833786010742 2.7582030296325684 4.476986408233643
Loss :  1.72014319896698 2.4971940517425537 4.217337131500244
Loss :  1.7231557369232178 2.0304059982299805 3.7535617351531982
Loss :  1.726099967956543 1.686882495880127 3.41298246383667
Loss :  1.7223209142684937 1.9261797666549683 3.648500680923462
Loss :  1.7251044511795044 2.271704912185669 3.996809482574463
Loss :  1.7222130298614502 2.3440184593200684 4.066231727600098
Loss :  1.7289971113204956 2.482996702194214 4.21199369430542
Loss :  1.7190920114517212 2.120051383972168 3.8391432762145996
Loss :  1.7159844636917114 3.133547782897949 4.849532127380371
Loss :  1.7224514484405518 2.9430503845214844 4.665501594543457
Loss :  1.733076572418213 1.9481168985366821 3.6811933517456055
Loss :  1.7181018590927124 2.605992317199707 4.324094295501709
Loss :  1.722678303718567 2.9269018173217773 4.649580001831055
Loss :  1.7250434160232544 2.163099527359009 3.8881430625915527
Loss :  1.728914499282837 2.6714162826538086 4.400330543518066
Loss :  1.7280246019363403 2.3050475120544434 4.033071994781494
Loss :  1.7300524711608887 2.8486781120300293 4.578730583190918
Loss :  1.7305129766464233 2.7864990234375 4.517012119293213
  batch 60 loss: 1.7305129766464233, 2.7864990234375, 4.517012119293213
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7273480892181396 2.306131601333618 4.033479690551758
Loss :  1.7280656099319458 1.9253079891204834 3.6533737182617188
Loss :  1.732547640800476 2.2749547958374023 4.007502555847168
Loss :  1.7211922407150269 3.105128288269043 4.826320648193359
Loss :  1.7189286947250366 2.5030789375305176 4.222007751464844
Loss :  1.8642328977584839 4.356543064117432 6.220776081085205
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8610880374908447 4.397527694702148 6.258615493774414
Loss :  1.8602573871612549 4.287527561187744 6.147785186767578
Loss :  1.8590847253799438 4.244594573974609 6.103679180145264
Total LOSS train 4.273857248746432 valid 6.182713985443115
CE LOSS train 1.724885766322796 valid 0.46477118134498596
Contrastive LOSS train 2.5489714604157667 valid 1.0611486434936523
EPOCH 141:
Loss :  1.7248727083206177 2.5873351097106934 4.3122076988220215
Loss :  1.7260630130767822 2.746110677719116 4.472173690795898
Loss :  1.718529224395752 2.7160000801086426 4.4345293045043945
Loss :  1.722989797592163 2.7180254459381104 4.441015243530273
Loss :  1.7294329404830933 2.378192186355591 4.1076250076293945
Loss :  1.7281371355056763 2.5524840354919434 4.28062105178833
Loss :  1.7279531955718994 2.2070775032043457 3.935030698776245
Loss :  1.7259548902511597 1.7766995429992676 3.502654552459717
Loss :  1.7249099016189575 2.4227993488311768 4.147709369659424
Loss :  1.7084176540374756 2.2740044593811035 3.982422113418579
Loss :  1.7269450426101685 2.7203257083892822 4.44727087020874
Loss :  1.7404364347457886 2.6256966590881348 4.366133213043213
Loss :  1.726313591003418 2.497326612472534 4.223640441894531
Loss :  1.724291205406189 2.3588662147521973 4.083157539367676
Loss :  1.7293154001235962 2.1615827083587646 3.8908982276916504
Loss :  1.7161495685577393 2.886357069015503 4.602506637573242
Loss :  1.7262589931488037 2.305155038833618 4.031414031982422
Loss :  1.7213060855865479 2.25536847114563 3.9766745567321777
Loss :  1.7220232486724854 2.2628586292266846 3.98488187789917
Loss :  1.7168689966201782 2.106295585632324 3.823164463043213
  batch 20 loss: 1.7168689966201782, 2.106295585632324, 3.823164463043213
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7261713743209839 1.8998020887374878 3.6259734630584717
Loss :  1.727746844291687 2.1102020740509033 3.837948799133301
Loss :  1.7286485433578491 2.256911039352417 3.9855594635009766
Loss :  1.7354332208633423 2.4132962226867676 4.14872932434082
Loss :  1.724940299987793 2.379619598388672 4.104559898376465
Loss :  1.7242157459259033 2.946786642074585 4.671002388000488
Loss :  1.7338635921478271 2.4821951389312744 4.216058731079102
Loss :  1.7229177951812744 2.52524733543396 4.248165130615234
Loss :  1.7323061227798462 3.7700271606445312 5.502333164215088
Loss :  1.7184969186782837 2.4657461643218994 4.184243202209473
Loss :  1.7365306615829468 3.405075788497925 5.141606330871582
Loss :  1.7277007102966309 2.9011337757110596 4.6288347244262695
Loss :  1.7195020914077759 2.176121473312378 3.8956236839294434
Loss :  1.7167346477508545 2.1045145988464355 3.82124924659729
Loss :  1.7267606258392334 2.69754958152771 4.424310207366943
Loss :  1.7276784181594849 2.7250561714172363 4.452734470367432
Loss :  1.725483775138855 2.5570852756500244 4.28256893157959
Loss :  1.7211694717407227 1.9613388776779175 3.6825084686279297
Loss :  1.7250829935073853 2.0520944595336914 3.777177333831787
Loss :  1.7249699831008911 2.473568916320801 4.198538780212402
  batch 40 loss: 1.7249699831008911, 2.473568916320801, 4.198538780212402
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7262426614761353 3.9441962242126465 5.670438766479492
Loss :  1.7186331748962402 3.223060369491577 4.941693305969238
Loss :  1.7204413414001465 3.475022315979004 5.19546365737915
Loss :  1.7230056524276733 2.807999610900879 4.531005382537842
Loss :  1.7257459163665771 3.4966914653778076 5.222437381744385
Loss :  1.7262288331985474 3.9695799350738525 5.6958088874816895
Loss :  1.7259547710418701 2.5010874271392822 4.227042198181152
Loss :  1.721186876296997 2.3778913021087646 4.099078178405762
Loss :  1.7299468517303467 2.0751662254333496 3.8051130771636963
Loss :  1.719645619392395 1.9197138547897339 3.639359474182129
Loss :  1.7156139612197876 3.122605323791504 4.838219165802002
Loss :  1.7229020595550537 2.6529479026794434 4.375849723815918
Loss :  1.7333450317382812 2.612145185470581 4.345490455627441
Loss :  1.718827247619629 1.9697240591049194 3.688551425933838
Loss :  1.7210322618484497 2.5706748962402344 4.2917070388793945
Loss :  1.7269350290298462 2.9544224739074707 4.681357383728027
Loss :  1.7286797761917114 2.4645400047302246 4.1932196617126465
Loss :  1.72674560546875 2.539790630340576 4.266536235809326
Loss :  1.7300711870193481 3.2003724575042725 4.93044376373291
Loss :  1.7310603857040405 3.1845638751983643 4.915624141693115
  batch 60 loss: 1.7310603857040405, 3.1845638751983643, 4.915624141693115
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7281142473220825 2.9717564582824707 4.699870586395264
Loss :  1.7275395393371582 2.8010776042938232 4.528616905212402
Loss :  1.732718586921692 3.2117888927459717 4.944507598876953
Loss :  1.7208446264266968 2.9573779106140137 4.67822265625
Loss :  1.717092514038086 2.5739073753356934 4.290999889373779
Loss :  1.8545830249786377 4.314176082611084 6.168759346008301
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8497287034988403 4.352442264556885 6.2021708488464355
Loss :  1.8496813774108887 4.271771430969238 6.121452808380127
Loss :  1.8500665426254272 4.222940921783447 6.073007583618164
Total LOSS train 4.347263281161968 valid 6.141347646713257
CE LOSS train 1.725108871093163 valid 0.4625166356563568
Contrastive LOSS train 2.622154419238751 valid 1.0557352304458618
EPOCH 142:
Loss :  1.7227507829666138 3.0165112018585205 4.739262104034424
Loss :  1.726594090461731 3.2755253314971924 5.002119541168213
Loss :  1.7179549932479858 3.2411015033721924 4.959056377410889
Loss :  1.7209193706512451 2.633145570755005 4.35406494140625
Loss :  1.7300740480422974 2.080958604812622 3.811032772064209
Loss :  1.7287726402282715 1.866024136543274 3.594796657562256
Loss :  1.7295929193496704 2.0019590854644775 3.7315521240234375
Loss :  1.7277889251708984 2.385392904281616 4.113182067871094
Loss :  1.7244622707366943 1.7169426679611206 3.4414048194885254
Loss :  1.7091964483261108 1.7765496969223022 3.485746145248413
Loss :  1.7259387969970703 1.986303448677063 3.7122421264648438
Loss :  1.7377077341079712 2.0011794567108154 3.738887310028076
Loss :  1.7249810695648193 1.8817726373672485 3.6067538261413574
Loss :  1.723825454711914 2.087001323699951 3.8108267784118652
Loss :  1.7286237478256226 2.2602076530456543 3.9888315200805664
Loss :  1.7152999639511108 2.4435815811157227 4.158881664276123
Loss :  1.7257027626037598 2.3679261207580566 4.093628883361816
Loss :  1.7215205430984497 2.1856532096862793 3.9071736335754395
Loss :  1.720871090888977 2.021792411804199 3.7426633834838867
Loss :  1.717820405960083 2.1522021293640137 3.8700225353240967
  batch 20 loss: 1.717820405960083, 2.1522021293640137, 3.8700225353240967
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.726475477218628 2.1593551635742188 3.8858306407928467
Loss :  1.7281566858291626 2.4860470294952393 4.214203834533691
Loss :  1.7287479639053345 2.379033327102661 4.107781410217285
Loss :  1.7358168363571167 2.400637626647949 4.1364545822143555
Loss :  1.7243880033493042 2.65061092376709 4.374999046325684
Loss :  1.7240345478057861 2.1512489318847656 3.8752834796905518
Loss :  1.733939290046692 2.420114040374756 4.154053211212158
Loss :  1.723892092704773 2.31803560256958 4.041927814483643
Loss :  1.7331507205963135 2.879878520965576 4.613029479980469
Loss :  1.7185397148132324 3.0214929580688477 4.74003267288208
Loss :  1.7372705936431885 2.7651166915893555 4.502387046813965
Loss :  1.728756070137024 2.357752799987793 4.086508750915527
Loss :  1.7210147380828857 2.0058844089508057 3.7268991470336914
Loss :  1.7186923027038574 2.226775646209717 3.945467948913574
Loss :  1.7282707691192627 2.604497194290161 4.332767963409424
Loss :  1.7296643257141113 2.6351053714752197 4.36476993560791
Loss :  1.7269470691680908 2.671769142150879 4.398715972900391
Loss :  1.7231673002243042 2.447861433029175 4.1710286140441895
Loss :  1.7257614135742188 2.0034067630767822 3.729168176651001
Loss :  1.7261788845062256 1.946775197982788 3.6729540824890137
  batch 40 loss: 1.7261788845062256, 1.946775197982788, 3.6729540824890137
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7245110273361206 2.1715283393859863 3.8960394859313965
Loss :  1.7216562032699585 1.7872503995895386 3.508906602859497
Loss :  1.722938060760498 2.6346054077148438 4.357543468475342
Loss :  1.7251085042953491 2.4385221004486084 4.163630485534668
Loss :  1.7279373407363892 2.278132915496826 4.006070137023926
Loss :  1.7248167991638184 2.3313052654266357 4.056121826171875
Loss :  1.7272194623947144 2.189640760421753 3.9168601036071777
Loss :  1.7234785556793213 3.8539183139801025 5.577396869659424
Loss :  1.7318968772888184 3.3787851333618164 5.110682010650635
Loss :  1.720819115638733 3.320305347442627 5.04112434387207
Loss :  1.7175010442733765 2.9277217388153076 4.6452226638793945
Loss :  1.7250584363937378 2.933471918106079 4.658530235290527
Loss :  1.7343976497650146 2.26558518409729 3.9999828338623047
Loss :  1.7192819118499756 2.6355507373809814 4.354832649230957
Loss :  1.7220323085784912 2.7666091918945312 4.488641738891602
Loss :  1.7268109321594238 3.361680746078491 5.088491439819336
Loss :  1.7298892736434937 3.3841681480407715 5.114057540893555
Loss :  1.7271554470062256 2.230377435684204 3.9575328826904297
Loss :  1.730505347251892 2.4090609550476074 4.139566421508789
Loss :  1.7310535907745361 1.860491394996643 3.5915451049804688
  batch 60 loss: 1.7310535907745361, 1.860491394996643, 3.5915451049804688
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7280563116073608 2.061922550201416 3.7899789810180664
Loss :  1.7284600734710693 2.2236740589141846 3.952134132385254
Loss :  1.7328041791915894 1.6917227506637573 3.4245269298553467
Loss :  1.7218211889266968 2.1273648738861084 3.8491859436035156
Loss :  1.7191789150238037 2.27943754196167 3.9986164569854736
Loss :  1.874870777130127 4.337409019470215 6.212279796600342
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8681304454803467 4.405588626861572 6.27371883392334
Loss :  1.8701506853103638 4.295629024505615 6.1657795906066895
Loss :  1.870808482170105 4.320413589477539 6.191222190856934
Total LOSS train 4.148055604787973 valid 6.210750102996826
CE LOSS train 1.7256561756134032 valid 0.46770212054252625
Contrastive LOSS train 2.4223994255065917 valid 1.0801033973693848
EPOCH 143:
Loss :  1.7266488075256348 2.002939224243164 3.729588031768799
Loss :  1.7274409532546997 2.91044545173645 4.6378865242004395
Loss :  1.7199318408966064 2.0777809619903564 3.797712802886963
Loss :  1.72463059425354 2.6690585613250732 4.393689155578613
Loss :  1.7311385869979858 2.2918860912323 4.023024559020996
Loss :  1.729488492012024 3.166248083114624 4.8957366943359375
Loss :  1.7288535833358765 2.975299119949341 4.704152584075928
Loss :  1.7269529104232788 2.735456943511963 4.462409973144531
Loss :  1.7247227430343628 3.00126576423645 4.725988388061523
Loss :  1.7094216346740723 2.5877904891967773 4.29721212387085
Loss :  1.7261015176773071 2.49104380607605 4.2171454429626465
Loss :  1.7382988929748535 3.2250092029571533 4.963308334350586
Loss :  1.72677481174469 3.0477912425994873 4.774566173553467
Loss :  1.7244294881820679 3.23097562789917 4.955405235290527
Loss :  1.72860848903656 2.344743251800537 4.073351860046387
Loss :  1.7164220809936523 2.5006537437438965 4.217075824737549
Loss :  1.726625680923462 3.339585542678833 5.066211223602295
Loss :  1.7268078327178955 4.665734767913818 6.392542839050293
Loss :  1.722396969795227 3.3549680709838867 5.077364921569824
Loss :  1.717891812324524 2.956240653991699 4.674132347106934
  batch 20 loss: 1.717891812324524, 2.956240653991699, 4.674132347106934
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.727454662322998 3.8085014820098877 5.535956382751465
Loss :  1.7276782989501953 2.1167519092559814 3.8444302082061768
Loss :  1.7275460958480835 2.189448833465576 3.916995048522949
Loss :  1.7333424091339111 2.210759162902832 3.944101572036743
Loss :  1.7225072383880615 2.36014461517334 4.0826520919799805
Loss :  1.723244547843933 2.649228811264038 4.372473239898682
Loss :  1.7334556579589844 2.514477491378784 4.247933387756348
Loss :  1.723999261856079 2.589388370513916 4.313387870788574
Loss :  1.7325854301452637 2.4488141536712646 4.181399345397949
Loss :  1.7186641693115234 2.4713194370269775 4.189983367919922
Loss :  1.735798954963684 2.328141927719116 4.06394100189209
Loss :  1.7283204793930054 2.0734610557556152 3.80178165435791
Loss :  1.7206401824951172 1.956704020500183 3.67734432220459
Loss :  1.716884732246399 2.3614630699157715 4.078347682952881
Loss :  1.727475881576538 3.424194574356079 5.151670455932617
Loss :  1.7284126281738281 3.0842928886413574 4.8127055168151855
Loss :  1.7262694835662842 2.969071626663208 4.695341110229492
Loss :  1.7237069606781006 2.265476703643799 3.9891836643218994
Loss :  1.7258095741271973 2.444194793701172 4.170004367828369
Loss :  1.7259269952774048 2.660869598388672 4.386796474456787
  batch 40 loss: 1.7259269952774048, 2.660869598388672, 4.386796474456787
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7244808673858643 2.461686611175537 4.1861677169799805
Loss :  1.7203001976013184 2.7111384868621826 4.431438446044922
Loss :  1.7212666273117065 3.0211870670318604 4.742453575134277
Loss :  1.7240068912506104 2.393949270248413 4.117956161499023
Loss :  1.7262771129608154 2.3596880435943604 4.085965156555176
Loss :  1.7225764989852905 3.339158535003662 5.061735153198242
Loss :  1.725914478302002 2.2824130058288574 4.008327484130859
Loss :  1.7209712266921997 2.556680917739868 4.277652263641357
Loss :  1.7284389734268188 2.101029872894287 3.8294687271118164
Loss :  1.7176640033721924 1.8012241125106812 3.518887996673584
Loss :  1.7145507335662842 1.8387846946716309 3.553335428237915
Loss :  1.72279953956604 1.9871160984039307 3.7099156379699707
Loss :  1.732301115989685 1.7981762886047363 3.530477523803711
Loss :  1.71669602394104 2.031352996826172 3.748049020767212
Loss :  1.720579981803894 1.8596491813659668 3.5802292823791504
Loss :  1.7268420457839966 2.3295600414276123 4.056402206420898
Loss :  1.7285212278366089 2.4718399047851562 4.200361251831055
Loss :  1.726973056793213 2.1099014282226562 3.836874485015869
Loss :  1.7298177480697632 2.170062303543091 3.8998799324035645
Loss :  1.7310075759887695 1.6704967021942139 3.4015042781829834
  batch 60 loss: 1.7310075759887695, 1.6704967021942139, 3.4015042781829834
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7287660837173462 1.8854202032089233 3.6141862869262695
Loss :  1.7288907766342163 2.7242624759674072 4.453153133392334
Loss :  1.7344120740890503 2.02364182472229 3.758053779602051
Loss :  1.7222708463668823 2.1150434017181396 3.8373141288757324
Loss :  1.7201775312423706 1.79843008518219 3.5186076164245605
Loss :  1.8757795095443726 4.409065246582031 6.284844875335693
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8695554733276367 4.421164512634277 6.290719985961914
Loss :  1.8715510368347168 4.198563575744629 6.070114612579346
Loss :  1.869225025177002 4.212378978729248 6.08160400390625
Total LOSS train 4.253743545825665 valid 6.181820869445801
CE LOSS train 1.7253817631648136 valid 0.4673062562942505
Contrastive LOSS train 2.5283617643209606 valid 1.053094744682312
EPOCH 144:
Loss :  1.7262301445007324 2.220949411392212 3.9471795558929443
Loss :  1.7280081510543823 2.4712159633636475 4.19922399520874
Loss :  1.7209163904190063 1.943857192993164 3.664773464202881
Loss :  1.724460482597351 2.2169952392578125 3.941455841064453
Loss :  1.7311288118362427 2.2104403972625732 3.9415693283081055
Loss :  1.7300711870193481 1.8512165546417236 3.5812878608703613
Loss :  1.7297797203063965 2.3739395141601562 4.103719234466553
Loss :  1.7283865213394165 1.961378812789917 3.689765453338623
Loss :  1.7265138626098633 2.0502891540527344 3.7768030166625977
Loss :  1.710873007774353 1.9149445295333862 3.6258175373077393
Loss :  1.7282246351242065 2.6951935291290283 4.423418045043945
Loss :  1.739251971244812 2.6997275352478027 4.438979625701904
Loss :  1.7276785373687744 2.136794090270996 3.8644726276397705
Loss :  1.7257319688796997 2.330925703048706 4.056657791137695
Loss :  1.7302460670471191 2.296168327331543 4.026414394378662
Loss :  1.7180702686309814 2.020726203918457 3.7387964725494385
Loss :  1.7276782989501953 2.383864164352417 4.111542701721191
Loss :  1.7238305807113647 2.414161443710327 4.137991905212402
Loss :  1.7235242128372192 1.6892880201339722 3.4128122329711914
Loss :  1.7185845375061035 2.383000135421753 4.101584434509277
  batch 20 loss: 1.7185845375061035, 2.383000135421753, 4.101584434509277
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.726895809173584 2.0862834453582764 3.8131792545318604
Loss :  1.7282381057739258 2.221604585647583 3.949842691421509
Loss :  1.728682279586792 2.02700138092041 3.755683660507202
Loss :  1.7341173887252808 2.099210262298584 3.8333277702331543
Loss :  1.7249373197555542 2.3929226398468018 4.117859840393066
Loss :  1.7234156131744385 2.0186281204223633 3.7420437335968018
Loss :  1.7331303358078003 2.208428382873535 3.941558837890625
Loss :  1.7222501039505005 2.0454156398773193 3.7676658630371094
Loss :  1.73175048828125 2.032442569732666 3.764193058013916
Loss :  1.7196789979934692 2.3991124629974365 4.118791580200195
Loss :  1.7380671501159668 2.6461899280548096 4.3842573165893555
Loss :  1.7287664413452148 2.568354845046997 4.297121047973633
Loss :  1.7212069034576416 2.5101866722106934 4.231393814086914
Loss :  1.7191659212112427 2.514287233352661 4.233453273773193
Loss :  1.728879690170288 2.474273920059204 4.203153610229492
Loss :  1.7298146486282349 1.9464575052261353 3.67627215385437
Loss :  1.726641058921814 2.2691102027893066 3.99575138092041
Loss :  1.7215244770050049 2.146498918533325 3.86802339553833
Loss :  1.7255403995513916 2.2264010906219482 3.95194149017334
Loss :  1.7251126766204834 1.7137378454208374 3.4388504028320312
  batch 40 loss: 1.7251126766204834, 1.7137378454208374, 3.4388504028320312
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.723804235458374 2.2770111560821533 4.000815391540527
Loss :  1.7223172187805176 2.2314844131469727 3.9538016319274902
Loss :  1.7232862710952759 2.1712591648101807 3.894545555114746
Loss :  1.725579857826233 2.2639033794403076 3.98948335647583
Loss :  1.7282601594924927 2.0930237770080566 3.8212838172912598
Loss :  1.7235944271087646 2.1436028480529785 3.867197275161743
Loss :  1.7249178886413574 2.3458445072174072 4.070762634277344
Loss :  1.7233775854110718 2.6577842235565186 4.381161689758301
Loss :  1.7285709381103516 1.93539559841156 3.663966655731201
Loss :  1.7216402292251587 2.0368714332580566 3.758511543273926
Loss :  1.7180191278457642 2.6710362434387207 4.389055252075195
Loss :  1.7249119281768799 2.705893039703369 4.430805206298828
Loss :  1.732604742050171 2.9671003818511963 4.699705123901367
Loss :  1.7216527462005615 3.2864596843719482 5.00811243057251
Loss :  1.7244490385055542 2.4161322116851807 4.140581130981445
Loss :  1.7251157760620117 2.9619061946868896 4.6870222091674805
Loss :  1.7292792797088623 2.1090993881225586 3.838378667831421
Loss :  1.7280774116516113 2.456799030303955 4.184876441955566
Loss :  1.7309939861297607 2.386845827102661 4.117839813232422
Loss :  1.7308186292648315 2.313462257385254 4.044281005859375
  batch 60 loss: 1.7308186292648315, 2.313462257385254, 4.044281005859375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7273337841033936 2.597384214401245 4.324717998504639
Loss :  1.7291721105575562 2.4197754859924316 4.148947715759277
Loss :  1.7314631938934326 2.2547733783721924 3.986236572265625
Loss :  1.7226773500442505 2.4858744144439697 4.20855188369751
Loss :  1.7207179069519043 1.6358219385147095 3.356539726257324
Loss :  1.883844256401062 4.4459547996521 6.329799175262451
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8767930269241333 4.363479137420654 6.240272045135498
Loss :  1.879411220550537 4.222514629364014 6.101925849914551
Loss :  1.8753334283828735 4.237339019775391 6.112672328948975
Total LOSS train 4.012858621890729 valid 6.196167349815369
CE LOSS train 1.7261483229123629 valid 0.4688333570957184
Contrastive LOSS train 2.286710273302518 valid 1.0593347549438477
EPOCH 145:
Loss :  1.7258176803588867 2.50781512260437 4.233633041381836
Loss :  1.7247287034988403 2.584035634994507 4.308764457702637
Loss :  1.7192391157150269 2.5063512325286865 4.225590229034424
Loss :  1.7241930961608887 2.5437490940093994 4.267942428588867
Loss :  1.7296640872955322 3.0437076091766357 4.773371696472168
Loss :  1.7278327941894531 2.7797539234161377 4.507586479187012
Loss :  1.7275478839874268 2.4224960803985596 4.150043964385986
Loss :  1.7254087924957275 2.924133062362671 4.649541854858398
Loss :  1.7249970436096191 3.6647379398345947 5.389735221862793
Loss :  1.7091959714889526 2.011129379272461 3.720325469970703
Loss :  1.7252780199050903 2.8140852451324463 4.539363384246826
Loss :  1.7402701377868652 3.767503261566162 5.507773399353027
Loss :  1.7256767749786377 3.4182586669921875 5.143935203552246
Loss :  1.724540114402771 3.6531312465667725 5.377671241760254
Loss :  1.731779932975769 4.325930118560791 6.05771017074585
Loss :  1.7198083400726318 3.5290756225585938 5.248884201049805
Loss :  1.7253779172897339 2.6622314453125 4.387609481811523
Loss :  1.7217135429382324 2.526853322982788 4.248566627502441
Loss :  1.7225134372711182 1.8084536790847778 3.5309672355651855
Loss :  1.7169673442840576 1.9368563890457153 3.6538238525390625
  batch 20 loss: 1.7169673442840576, 1.9368563890457153, 3.6538238525390625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7259761095046997 2.221179485321045 3.947155475616455
Loss :  1.7275406122207642 1.895491361618042 3.6230320930480957
Loss :  1.7280863523483276 3.3246092796325684 5.0526957511901855
Loss :  1.7348253726959229 2.469756603240967 4.204582214355469
Loss :  1.7266688346862793 2.6081480979919434 4.334816932678223
Loss :  1.724368929862976 3.192657709121704 4.917026519775391
Loss :  1.73326575756073 2.4574339389801025 4.190699577331543
Loss :  1.722692608833313 2.165846586227417 3.8885393142700195
Loss :  1.7324587106704712 2.249199390411377 3.9816579818725586
Loss :  1.7187556028366089 2.2473092079162598 3.966064929962158
Loss :  1.738123893737793 3.177476644515991 4.915600776672363
Loss :  1.7288790941238403 2.660127878189087 4.389007091522217
Loss :  1.7210153341293335 3.030463933944702 4.751479148864746
Loss :  1.7201862335205078 3.271139621734619 4.991325855255127
Loss :  1.7294092178344727 3.30365252494812 5.033061981201172
Loss :  1.7312711477279663 2.8285701274871826 4.559841156005859
Loss :  1.7279393672943115 2.7737374305725098 4.501676559448242
Loss :  1.7235076427459717 2.576253652572632 4.2997612953186035
Loss :  1.7262766361236572 2.2234303951263428 3.94970703125
Loss :  1.7269055843353271 2.9573047161102295 4.684210300445557
  batch 40 loss: 1.7269055843353271, 2.9573047161102295, 4.684210300445557
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7243069410324097 3.2659871578216553 4.990293979644775
Loss :  1.7213261127471924 2.566335439682007 4.287661552429199
Loss :  1.7233463525772095 3.0132675170898438 4.736613750457764
Loss :  1.7221201658248901 2.694032907485962 4.4161529541015625
Loss :  1.7261898517608643 2.583258867263794 4.309448719024658
Loss :  1.7239471673965454 2.7957842350006104 4.519731521606445
Loss :  1.7253031730651855 2.305166721343994 4.03046989440918
Loss :  1.7214678525924683 1.7616087198257446 3.483076572418213
Loss :  1.7308099269866943 2.6366002559661865 4.367410182952881
Loss :  1.719506859779358 2.9059653282165527 4.625472068786621
Loss :  1.716661810874939 3.591665744781494 5.308327674865723
Loss :  1.7248836755752563 3.74635648727417 5.471240043640137
Loss :  1.7329387664794922 3.2332961559295654 4.966235160827637
Loss :  1.719304084777832 2.760680913925171 4.479985237121582
Loss :  1.7217826843261719 2.5401437282562256 4.261926651000977
Loss :  1.7257812023162842 2.1889233589172363 3.9147045612335205
Loss :  1.7294708490371704 2.127525806427002 3.856996536254883
Loss :  1.7269302606582642 2.7615952491760254 4.488525390625
Loss :  1.7306177616119385 2.6961545944213867 4.426772117614746
Loss :  1.7317306995391846 1.9249674081802368 3.656698226928711
  batch 60 loss: 1.7317306995391846, 1.9249674081802368, 3.656698226928711
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7279717922210693 2.392047643661499 4.120019435882568
Loss :  1.728690266609192 2.3560802936553955 4.084770679473877
Loss :  1.7332661151885986 2.041534662246704 3.7748007774353027
Loss :  1.7231050729751587 2.2379140853881836 3.9610190391540527
Loss :  1.7208564281463623 2.644777774810791 4.365633964538574
Loss :  1.8843145370483398 4.41756010055542 6.30187463760376
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8753595352172852 4.40562105178833 6.280980587005615
Loss :  1.8798280954360962 4.2917094230651855 6.171537399291992
Loss :  1.8746618032455444 4.111483573913574 5.986145496368408
Total LOSS train 4.446288743385902 valid 6.185134530067444
CE LOSS train 1.725738763809204 valid 0.4686654508113861
Contrastive LOSS train 2.720549964904785 valid 1.0278708934783936
EPOCH 146:
Loss :  1.726915717124939 3.253798007965088 4.980713844299316
Loss :  1.7281855344772339 3.1248772144317627 4.853062629699707
Loss :  1.7211918830871582 3.2994892597198486 5.020681381225586
Loss :  1.7246015071868896 2.8698744773864746 4.594475746154785
Loss :  1.7334967851638794 3.287928819656372 5.021425724029541
Loss :  1.7307008504867554 3.330353260040283 5.061054229736328
Loss :  1.7329109907150269 2.1460390090942383 3.8789501190185547
Loss :  1.7302050590515137 2.3955001831054688 4.125705242156982
Loss :  1.727919101715088 2.1261518001556396 3.8540709018707275
Loss :  1.7146297693252563 2.2708654403686523 3.985495090484619
Loss :  1.730244755744934 2.238381862640381 3.9686264991760254
Loss :  1.7378121614456177 2.5915281772613525 4.32934045791626
Loss :  1.7302982807159424 2.965897560119629 4.696195602416992
Loss :  1.7260345220565796 2.428830146789551 4.15486478805542
Loss :  1.732248306274414 2.228031873703003 3.960280179977417
Loss :  1.718695044517517 2.629197835922241 4.347892761230469
Loss :  1.7285356521606445 2.7638401985168457 4.49237585067749
Loss :  1.726335048675537 2.590144157409668 4.316479206085205
Loss :  1.7221816778182983 2.3990049362182617 4.12118673324585
Loss :  1.7214553356170654 2.8807246685028076 4.602180004119873
  batch 20 loss: 1.7214553356170654, 2.8807246685028076, 4.602180004119873
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7286691665649414 2.295391798019409 4.02406120300293
Loss :  1.7288093566894531 2.349600076675415 4.078409194946289
Loss :  1.731452465057373 2.3597631454467773 4.09121561050415
Loss :  1.7355222702026367 1.974155068397522 3.709677219390869
Loss :  1.7244019508361816 2.1901087760925293 3.914510726928711
Loss :  1.7247346639633179 2.352884531021118 4.0776190757751465
Loss :  1.734220266342163 2.50187611579895 4.236096382141113
Loss :  1.7244952917099 2.2396934032440186 3.964188575744629
Loss :  1.7328687906265259 3.0448544025421143 4.77772331237793
Loss :  1.7180877923965454 3.63088321685791 5.348970890045166
Loss :  1.7362825870513916 3.277631998062134 5.013914585113525
Loss :  1.7291369438171387 3.124021053314209 4.853157997131348
Loss :  1.7211508750915527 2.0065484046936035 3.7276992797851562
Loss :  1.719319462776184 2.563694953918457 4.283014297485352
Loss :  1.7277687788009644 2.767777919769287 4.495546817779541
Loss :  1.7305102348327637 2.0491151809692383 3.779625415802002
Loss :  1.7288626432418823 2.1235592365264893 3.852421760559082
Loss :  1.72561514377594 1.9482659101486206 3.6738810539245605
Loss :  1.7281867265701294 2.25247859954834 3.9806652069091797
Loss :  1.7294790744781494 2.2805116176605225 4.009990692138672
  batch 40 loss: 1.7294790744781494, 2.2805116176605225, 4.009990692138672
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7262822389602661 3.3355183601379395 5.061800479888916
Loss :  1.7219526767730713 3.0628933906555176 4.784846305847168
Loss :  1.7231472730636597 2.2204909324645996 3.943638324737549
Loss :  1.7242237329483032 2.339420795440674 4.0636444091796875
Loss :  1.7275993824005127 2.2872800827026367 4.01487922668457
Loss :  1.7259465456008911 2.774667978286743 4.500614643096924
Loss :  1.7278306484222412 2.3219361305236816 4.049766540527344
Loss :  1.723248839378357 2.695213794708252 4.418462753295898
Loss :  1.7321412563323975 2.406458854675293 4.1386003494262695
Loss :  1.7209771871566772 2.340130090713501 4.061107158660889
Loss :  1.7180871963500977 2.7092697620391846 4.427356719970703
Loss :  1.7247593402862549 3.075181245803833 4.799940586090088
Loss :  1.7340080738067627 2.562711477279663 4.296719551086426
Loss :  1.7189985513687134 2.3743176460266113 4.093316078186035
Loss :  1.7229458093643188 2.0687713623046875 3.791717052459717
Loss :  1.7251324653625488 1.9616692066192627 3.6868016719818115
Loss :  1.7288097143173218 2.1774353981018066 3.906245231628418
Loss :  1.7275844812393188 2.065091371536255 3.7926759719848633
Loss :  1.7301489114761353 2.5189199447631836 4.249068737030029
Loss :  1.7307549715042114 2.168332099914551 3.8990869522094727
  batch 60 loss: 1.7307549715042114, 2.168332099914551, 3.8990869522094727
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7277801036834717 2.1857035160064697 3.9134836196899414
Loss :  1.7290542125701904 2.7051820755004883 4.434236526489258
Loss :  1.7324036359786987 2.7436296939849854 4.4760332107543945
Loss :  1.7226618528366089 2.5127389430999756 4.235400676727295
Loss :  1.7204047441482544 1.8512738943099976 3.571678638458252
Loss :  1.8817247152328491 4.312211990356445 6.193936824798584
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8739701509475708 4.455437660217285 6.329407691955566
Loss :  1.8773853778839111 4.353419780731201 6.230805397033691
Loss :  1.8729939460754395 4.422080993652344 6.295074939727783
Total LOSS train 4.259516426233145 valid 6.262306213378906
CE LOSS train 1.7269393279002263 valid 0.46824848651885986
Contrastive LOSS train 2.532577113004831 valid 1.105520248413086
EPOCH 147:
Loss :  1.7261208295822144 2.471362590789795 4.197483539581299
Loss :  1.7267346382141113 2.5911667346954346 4.317901611328125
Loss :  1.7201966047286987 2.1689200401306152 3.8891167640686035
Loss :  1.723689079284668 2.1837146282196045 3.9074037075042725
Loss :  1.7301512956619263 3.138292074203491 4.868443489074707
Loss :  1.7280945777893066 1.8282324075698853 3.5563268661499023
Loss :  1.7289828062057495 2.3686752319335938 4.097658157348633
Loss :  1.7271018028259277 2.1855010986328125 3.9126029014587402
Loss :  1.7258251905441284 2.6407601833343506 4.3665852546691895
Loss :  1.7106915712356567 1.9730013608932495 3.6836929321289062
Loss :  1.7273916006088257 2.782154083251953 4.509545803070068
Loss :  1.7392537593841553 2.719336986541748 4.458590507507324
Loss :  1.7272098064422607 3.288775682449341 5.015985488891602
Loss :  1.7258154153823853 3.938070774078369 5.663886070251465
Loss :  1.7307026386260986 3.379514455795288 5.110217094421387
Loss :  1.7184114456176758 2.8131186962127686 4.531530380249023
Loss :  1.727779746055603 3.247270107269287 4.97504997253418
Loss :  1.7230989933013916 2.672743320465088 4.395842552185059
Loss :  1.7226492166519165 2.0306286811828613 3.7532777786254883
Loss :  1.7201273441314697 2.4471471309661865 4.167274475097656
  batch 20 loss: 1.7201273441314697, 2.4471471309661865, 4.167274475097656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.726973533630371 2.6528964042663574 4.3798699378967285
Loss :  1.7287644147872925 1.7734006643295288 3.5021650791168213
Loss :  1.7294460535049438 2.3718652725219727 4.101311206817627
Loss :  1.734600305557251 2.8775200843811035 4.612120628356934
Loss :  1.724722981452942 2.8597145080566406 4.584437370300293
Loss :  1.7246311902999878 2.3298280239105225 4.054459095001221
Loss :  1.7337223291397095 1.9059576988220215 3.6396799087524414
Loss :  1.723465919494629 2.04065203666687 3.764117956161499
Loss :  1.732558012008667 1.8381816148757935 3.57073974609375
Loss :  1.7195786237716675 2.3002331256866455 4.019811630249023
Loss :  1.7372479438781738 2.292785406112671 4.030033111572266
Loss :  1.7287908792495728 2.106689691543579 3.8354806900024414
Loss :  1.7214497327804565 2.0660808086395264 3.7875304222106934
Loss :  1.7190169095993042 2.4560656547546387 4.175082683563232
Loss :  1.7287929058074951 2.463714361190796 4.192507266998291
Loss :  1.730359435081482 2.5976102352142334 4.327969551086426
Loss :  1.727904200553894 2.3651585578918457 4.093062877655029
Loss :  1.7245049476623535 2.473501682281494 4.198006629943848
Loss :  1.7273943424224854 2.6520321369171143 4.3794264793396
Loss :  1.7271442413330078 3.1609175205230713 4.8880615234375
  batch 40 loss: 1.7271442413330078, 3.1609175205230713, 4.8880615234375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.725500464439392 3.3511345386505127 5.076634883880615
Loss :  1.72163724899292 2.4507036209106445 4.1723408699035645
Loss :  1.7228705883026123 2.5249884128570557 4.247859001159668
Loss :  1.725468397140503 2.6581368446350098 4.383605003356934
Loss :  1.7281033992767334 2.6503491401672363 4.378452301025391
Loss :  1.726040244102478 2.904217004776001 4.6302571296691895
Loss :  1.7280460596084595 2.3318850994110107 4.05993127822876
Loss :  1.7230935096740723 2.8846421241760254 4.607735633850098
Loss :  1.732527494430542 2.844194173812866 4.576721668243408
Loss :  1.7201669216156006 1.906391978263855 3.626558780670166
Loss :  1.71812903881073 2.0862224102020264 3.804351329803467
Loss :  1.72573983669281 2.1353986263275146 3.861138343811035
Loss :  1.734394907951355 2.538198947906494 4.272593975067139
Loss :  1.7184785604476929 2.4257473945617676 4.14422607421875
Loss :  1.723224401473999 3.117687702178955 4.840911865234375
Loss :  1.7267813682556152 2.863551616668701 4.590332984924316
Loss :  1.7301826477050781 2.560959815979004 4.291142463684082
Loss :  1.7281745672225952 2.0372872352600098 3.7654619216918945
Loss :  1.73153555393219 2.210094690322876 3.9416303634643555
Loss :  1.7329460382461548 1.7878594398498535 3.5208053588867188
  batch 60 loss: 1.7329460382461548, 1.7878594398498535, 3.5208053588867188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.729158878326416 1.9375938177108765 3.666752815246582
Loss :  1.729404330253601 2.064394950866699 3.79379940032959
Loss :  1.7339411973953247 1.6528927087783813 3.386833906173706
Loss :  1.723445177078247 2.4563825130462646 4.179827690124512
Loss :  1.7206975221633911 1.6619493961334229 3.3826470375061035
Loss :  1.8793282508850098 4.466223239898682 6.345551490783691
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8722769021987915 4.390956401824951 6.263233184814453
Loss :  1.8758445978164673 4.350924015045166 6.226768493652344
Loss :  1.8690745830535889 4.328474521636963 6.197548866271973
Total LOSS train 4.195643711090088 valid 6.258275508880615
CE LOSS train 1.7264736248896673 valid 0.4672686457633972
Contrastive LOSS train 2.469170091702388 valid 1.0821186304092407
EPOCH 148:
Loss :  1.7264829874038696 3.535656690597534 5.262139797210693
Loss :  1.7282849550247192 3.581704616546631 5.3099894523620605
Loss :  1.7203590869903564 3.047929048538208 4.7682881355285645
Loss :  1.7241153717041016 2.3042984008789062 4.028413772583008
Loss :  1.7313395738601685 2.339108467102051 4.07044792175293
Loss :  1.729017972946167 1.8138529062271118 3.5428709983825684
Loss :  1.730168104171753 2.898533582687378 4.628701686859131
Loss :  1.7277201414108276 2.6240265369415283 4.351746559143066
Loss :  1.7257589101791382 2.941385507583618 4.667144298553467
Loss :  1.7102611064910889 3.4798154830932617 5.19007682800293
Loss :  1.7269315719604492 3.6077091693878174 5.3346405029296875
Loss :  1.7380669116973877 3.435717821121216 5.1737847328186035
Loss :  1.727461576461792 2.971187114715576 4.698648452758789
Loss :  1.7250735759735107 2.2848849296569824 4.009958267211914
Loss :  1.729838252067566 3.3365566730499268 5.066394805908203
Loss :  1.7168850898742676 2.6377623081207275 4.354647636413574
Loss :  1.7261985540390015 2.2596383094787598 3.985836982727051
Loss :  1.722415566444397 2.3981082439422607 4.120523929595947
Loss :  1.7218269109725952 2.551630973815918 4.273458003997803
Loss :  1.719064474105835 2.579439640045166 4.298503875732422
  batch 20 loss: 1.719064474105835, 2.579439640045166, 4.298503875732422
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7274359464645386 1.9456642866134644 3.673100233078003
Loss :  1.7275867462158203 2.8691070079803467 4.596693992614746
Loss :  1.7291635274887085 2.28120756149292 4.010371208190918
Loss :  1.733292579650879 2.0843799114227295 3.8176724910736084
Loss :  1.7248681783676147 2.1542108058929443 3.8790788650512695
Loss :  1.7243279218673706 2.613556146621704 4.337883949279785
Loss :  1.7328847646713257 3.3991494178771973 5.1320343017578125
Loss :  1.722908616065979 3.0175020694732666 4.740410804748535
Loss :  1.7324541807174683 2.7687764167785645 4.501230716705322
Loss :  1.7190853357315063 2.699415445327759 4.418500900268555
Loss :  1.7379990816116333 2.582965612411499 4.320964813232422
Loss :  1.7281004190444946 2.6603739261627197 4.388474464416504
Loss :  1.7212384939193726 2.823009490966797 4.544248104095459
Loss :  1.7203071117401123 2.6120283603668213 4.332335472106934
Loss :  1.7293246984481812 3.237152099609375 4.966476917266846
Loss :  1.7316627502441406 2.4700498580932617 4.201712608337402
Loss :  1.7284444570541382 2.4889636039733887 4.217408180236816
Loss :  1.7232967615127563 2.5313973426818848 4.254693984985352
Loss :  1.7271324396133423 2.0330891609191895 3.760221481323242
Loss :  1.7267063856124878 2.5877175331115723 4.31442403793335
  batch 40 loss: 1.7267063856124878, 2.5877175331115723, 4.31442403793335
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7243404388427734 3.1787750720977783 4.903115272521973
Loss :  1.7213099002838135 2.1384971141815186 3.859807014465332
Loss :  1.7223907709121704 2.228008270263672 3.9503989219665527
Loss :  1.72355055809021 2.3105459213256836 4.034096717834473
Loss :  1.7265598773956299 1.882700800895691 3.6092605590820312
Loss :  1.7237805128097534 1.90172278881073 3.6255033016204834
Loss :  1.7247756719589233 1.8038758039474487 3.528651475906372
Loss :  1.7223256826400757 1.8054882287979126 3.5278139114379883
Loss :  1.729641318321228 1.9453201293945312 3.674961566925049
Loss :  1.720190167427063 1.5976805686950684 3.317870616912842
Loss :  1.718784213066101 1.9715771675109863 3.690361499786377
Loss :  1.7257235050201416 2.1461336612701416 3.871857166290283
Loss :  1.734542727470398 2.974515676498413 4.7090582847595215
Loss :  1.7206259965896606 3.546708106994629 5.267333984375
Loss :  1.7250440120697021 2.6671831607818604 4.3922271728515625
Loss :  1.726904273033142 2.044804811477661 3.7717089653015137
Loss :  1.7314919233322144 2.229884386062622 3.961376190185547
Loss :  1.7287830114364624 2.197510242462158 3.92629337310791
Loss :  1.7325806617736816 2.872161626815796 4.604742050170898
Loss :  1.7324186563491821 3.4427061080932617 5.175124645233154
  batch 60 loss: 1.7324186563491821, 3.4427061080932617, 5.175124645233154
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7285562753677368 2.990549325942993 4.7191057205200195
Loss :  1.7295843362808228 2.610191583633423 4.339776039123535
Loss :  1.732791543006897 2.137925863265991 3.8707175254821777
Loss :  1.7239389419555664 3.081653356552124 4.8055925369262695
Loss :  1.7214151620864868 2.273984670639038 3.9953999519348145
Loss :  1.8782364130020142 4.439858913421631 6.3180952072143555
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.871333122253418 4.45917272567749 6.330505847930908
Loss :  1.8745951652526855 4.318807125091553 6.193402290344238
Loss :  1.868826150894165 4.454662799835205 6.323489189147949
Total LOSS train 4.318097055875338 valid 6.291373133659363
CE LOSS train 1.7262698650360107 valid 0.46720653772354126
Contrastive LOSS train 2.591827183503371 valid 1.1136656999588013
EPOCH 149:
Loss :  1.726432204246521 2.4374709129333496 4.16390323638916
Loss :  1.7271883487701416 2.328455686569214 4.0556440353393555
Loss :  1.721044659614563 2.7925500869750977 4.513594627380371
Loss :  1.7241460084915161 2.1114792823791504 3.835625171661377
Loss :  1.7311217784881592 1.859453797340393 3.590575695037842
Loss :  1.728963851928711 2.385693311691284 4.114657402038574
Loss :  1.7297277450561523 2.3195724487304688 4.049300193786621
Loss :  1.7273632287979126 2.874229907989502 4.601593017578125
Loss :  1.7261375188827515 2.2338714599609375 3.9600090980529785
Loss :  1.7108570337295532 1.770770788192749 3.481627941131592
Loss :  1.7271512746810913 2.287320375442505 4.014471530914307
Loss :  1.7394359111785889 2.302219867706299 4.041655540466309
Loss :  1.7264989614486694 2.4057364463806152 4.132235527038574
Loss :  1.7261383533477783 2.9436795711517334 4.669817924499512
Loss :  1.7316317558288574 2.344882011413574 4.076513767242432
Loss :  1.7188708782196045 3.086609363555908 4.805480003356934
Loss :  1.7277929782867432 2.3330159187316895 4.060809135437012
Loss :  1.7233972549438477 2.6672523021698 4.390649795532227
Loss :  1.7235305309295654 2.392320156097412 4.115850448608398
Loss :  1.7209548950195312 2.654294490814209 4.37524938583374
  batch 20 loss: 1.7209548950195312, 2.654294490814209, 4.37524938583374
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7284660339355469 3.363922595977783 5.09238862991333
Loss :  1.7292784452438354 2.8014767169952393 4.530755043029785
Loss :  1.7315300703048706 2.324068307876587 4.055598258972168
Loss :  1.7355022430419922 2.165374279022217 3.900876522064209
Loss :  1.7256842851638794 2.4157650470733643 4.141449451446533
Loss :  1.726090431213379 2.8405370712280273 4.566627502441406
Loss :  1.7352126836776733 3.017622709274292 4.752835273742676
Loss :  1.7254979610443115 3.085442543029785 4.810940742492676
Loss :  1.7349084615707397 2.861147880554199 4.5960564613342285
Loss :  1.7214741706848145 3.5458364486694336 5.267310619354248
Loss :  1.7386596202850342 2.7575440406799316 4.496203422546387
Loss :  1.729426622390747 3.1214704513549805 4.850896835327148
Loss :  1.7219927310943604 2.3652844429016113 4.087277412414551
Loss :  1.7200863361358643 2.246856927871704 3.9669432640075684
Loss :  1.7283461093902588 2.4522712230682373 4.180617332458496
Loss :  1.7303807735443115 2.0796115398406982 3.8099923133850098
Loss :  1.7279272079467773 2.3673558235168457 4.095283031463623
Loss :  1.7245720624923706 2.1932971477508545 3.9178690910339355
Loss :  1.7267524003982544 3.057884693145752 4.784636974334717
Loss :  1.7275587320327759 2.6555755138397217 4.383134365081787
  batch 40 loss: 1.7275587320327759, 2.6555755138397217, 4.383134365081787
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7257832288742065 2.706936836242676 4.432720184326172
Loss :  1.720504641532898 1.68428635597229 3.4047908782958984
Loss :  1.7223364114761353 1.8857364654541016 3.6080727577209473
Loss :  1.7237635850906372 1.8495854139328003 3.5733489990234375
Loss :  1.727164626121521 2.0526018142700195 3.77976655960083
Loss :  1.7267436981201172 2.085556983947754 3.812300682067871
Loss :  1.7295140027999878 2.6480679512023926 4.37758207321167
Loss :  1.7230942249298096 2.5317256450653076 4.254819869995117
Loss :  1.7352463006973267 2.846118688583374 4.58136510848999
Loss :  1.7205673456192017 2.24243426322937 3.9630017280578613
Loss :  1.7188022136688232 2.778268575668335 4.497070789337158
Loss :  1.7269359827041626 2.7166640758514404 4.443600177764893
Loss :  1.736258864402771 2.0841455459594727 3.820404529571533
Loss :  1.719003438949585 3.0244829654693604 4.743486404418945
Loss :  1.7228977680206299 2.3460514545440674 4.068949222564697
Loss :  1.7284166812896729 2.3023645877838135 4.030781269073486
Loss :  1.731014609336853 2.8674051761627197 4.598419666290283
Loss :  1.7290596961975098 2.522191286087036 4.251251220703125
Loss :  1.7305351495742798 2.4979989528656006 4.22853422164917
Loss :  1.7324415445327759 1.96220064163208 3.6946420669555664
  batch 60 loss: 1.7324415445327759, 1.96220064163208, 3.6946420669555664
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.729459524154663 2.0657742023468018 3.795233726501465
Loss :  1.7284356355667114 2.1437222957611084 3.8721580505371094
Loss :  1.7347108125686646 2.0625808238983154 3.7972917556762695
Loss :  1.7224154472351074 2.2614424228668213 3.9838578701019287
Loss :  1.7195367813110352 2.440842866897583 4.160379409790039
Loss :  1.8804343938827515 4.337403774261475 6.217838287353516
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8729650974273682 4.355905055999756 6.228870391845703
Loss :  1.8761907815933228 4.298601150512695 6.1747918128967285
Loss :  1.872784972190857 4.188681602478027 6.061466693878174
Total LOSS train 4.20170438839839 valid 6.17074179649353
CE LOSS train 1.7269596118193407 valid 0.46819624304771423
Contrastive LOSS train 2.4747447674091045 valid 1.0471704006195068
EPOCH 150:
Loss :  1.7254869937896729 1.8749206066131592 3.600407600402832
Loss :  1.7285407781600952 1.972180962562561 3.7007217407226562
Loss :  1.7212817668914795 2.0370118618011475 3.758293628692627
Loss :  1.723966121673584 2.0584282875061035 3.7823944091796875
Loss :  1.732447862625122 2.0738062858581543 3.8062541484832764
Loss :  1.7297630310058594 2.2107114791870117 3.940474510192871
Loss :  1.7314444780349731 2.2880592346191406 4.019503593444824
Loss :  1.7290881872177124 1.922790288925171 3.6518783569335938
Loss :  1.7264200448989868 2.3121039867401123 4.038524150848389
Loss :  1.7109671831130981 2.3876941204071045 4.098661422729492
Loss :  1.7277940511703491 2.4108026027679443 4.138596534729004
Loss :  1.7380423545837402 2.1568570137023926 3.894899368286133
Loss :  1.7273681163787842 2.390758752822876 4.11812686920166
Loss :  1.7260414361953735 2.239434242248535 3.965475559234619
Loss :  1.7305065393447876 2.163473606109619 3.893980026245117
Loss :  1.7181044816970825 2.1880438327789307 3.9061484336853027
Loss :  1.7280781269073486 1.9345275163650513 3.6626057624816895
Loss :  1.7242224216461182 2.135855197906494 3.8600776195526123
Loss :  1.7230762243270874 2.165971040725708 3.889047145843506
Loss :  1.7198917865753174 2.158372163772583 3.8782639503479004
  batch 20 loss: 1.7198917865753174, 2.158372163772583, 3.8782639503479004
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.727720022201538 2.579986095428467 4.307705879211426
Loss :  1.728545069694519 2.435680627822876 4.1642255783081055
Loss :  1.729592204093933 1.8548437356948853 3.5844359397888184
Loss :  1.735180377960205 1.889738917350769 3.6249194145202637
Loss :  1.7249259948730469 2.7489333152770996 4.4738593101501465
Loss :  1.7244694232940674 2.0578882694244385 3.782357692718506
Loss :  1.7339661121368408 2.138643741607666 3.872609853744507
Loss :  1.7241158485412598 2.3723907470703125 4.096506595611572
Loss :  1.732966661453247 1.9646133184432983 3.697579860687256
Loss :  1.7202692031860352 2.373046398162842 4.093315601348877
Loss :  1.738235354423523 2.7330474853515625 4.471282958984375
Loss :  1.7293051481246948 2.386951208114624 4.116256237030029
Loss :  1.7219018936157227 2.13136887550354 3.8532707691192627
Loss :  1.7197489738464355 2.670433282852173 4.3901824951171875
Loss :  1.7289024591445923 2.6774353981018066 4.406337738037109
Loss :  1.7304335832595825 2.8960158824920654 4.6264495849609375
Loss :  1.727500319480896 2.569953680038452 4.297453880310059
Loss :  1.7216529846191406 2.1823809146881104 3.904033899307251
Loss :  1.7268660068511963 2.5180675983428955 4.244933605194092
Loss :  1.726359248161316 2.5083909034729004 4.234750270843506
  batch 40 loss: 1.726359248161316, 2.5083909034729004, 4.234750270843506
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.724674105644226 2.1672415733337402 3.891915798187256
Loss :  1.7231223583221436 1.4719889163970947 3.1951112747192383
Loss :  1.7238858938217163 2.554361581802368 4.278247356414795
Loss :  1.725168228149414 2.79194712638855 4.517115592956543
Loss :  1.7278263568878174 2.268411159515381 3.9962375164031982
Loss :  1.7235989570617676 2.4291656017303467 4.152764320373535
Loss :  1.7246160507202148 2.1103827953338623 3.834998846054077
Loss :  1.723002552986145 2.1361806392669678 3.8591833114624023
Loss :  1.727459192276001 2.5497796535491943 4.277238845825195
Loss :  1.7204856872558594 2.696516275405884 4.417001724243164
Loss :  1.7179771661758423 2.6151928901672363 4.333169937133789
Loss :  1.724173903465271 2.065854787826538 3.7900285720825195
Loss :  1.733260154724121 1.9486641883850098 3.681924343109131
Loss :  1.7203291654586792 2.3425331115722656 4.062862396240234
Loss :  1.725019097328186 2.6650912761688232 4.390110492706299
Loss :  1.7254687547683716 3.0711557865142822 4.796624660491943
Loss :  1.7303545475006104 2.545740842819214 4.276095390319824
Loss :  1.7284092903137207 3.059030294418335 4.787439346313477
Loss :  1.731519103050232 3.0929365158081055 4.824455738067627
Loss :  1.7304935455322266 3.4389407634735107 5.169434547424316
  batch 60 loss: 1.7304935455322266, 3.4389407634735107, 5.169434547424316
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7274078130722046 2.7084155082702637 4.435823440551758
Loss :  1.7287341356277466 2.913670301437378 4.642404556274414
Loss :  1.7308374643325806 1.9093996286392212 3.6402370929718018
Loss :  1.7220393419265747 2.543363094329834 4.265402317047119
Loss :  1.7193900346755981 2.168771743774414 3.8881616592407227
Loss :  1.86906099319458 4.383297443389893 6.252358436584473
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.863342523574829 4.329963684082031 6.193305969238281
Loss :  1.8652899265289307 4.294563293457031 6.159852981567383
Loss :  1.861896276473999 4.127537727355957 5.989434242248535
Total LOSS train 4.080781401120699 valid 6.148737907409668
CE LOSS train 1.7263760273273174 valid 0.46547406911849976
Contrastive LOSS train 2.3544053774613602 valid 1.0318844318389893
EPOCH 151:
Loss :  1.725061058998108 2.62760329246521 4.352664470672607
Loss :  1.7247488498687744 1.9650013446807861 3.6897501945495605
Loss :  1.7191649675369263 1.5510274171829224 3.2701923847198486
Loss :  1.7247878313064575 1.8527098894119263 3.577497720718384
Loss :  1.7294450998306274 2.437469005584717 4.166913986206055
Loss :  1.7283885478973389 2.925567388534546 4.653955936431885
Loss :  1.7275967597961426 3.041231393814087 4.768828392028809
Loss :  1.7254091501235962 3.1883552074432373 4.913764476776123
Loss :  1.7259095907211304 4.314427375793457 6.040337085723877
Loss :  1.709345817565918 3.124469041824341 4.83381462097168
Loss :  1.726261854171753 2.9502763748168945 4.676538467407227
Loss :  1.7397282123565674 2.3359315395355225 4.07565975189209
Loss :  1.7263768911361694 2.262500524520874 3.988877296447754
Loss :  1.72539484500885 2.355503797531128 4.080898761749268
Loss :  1.7306511402130127 1.9286521673202515 3.6593031883239746
Loss :  1.7189884185791016 2.5470449924468994 4.266033172607422
Loss :  1.7271002531051636 2.4128763675689697 4.139976501464844
Loss :  1.7220573425292969 1.8502559661865234 3.5723133087158203
Loss :  1.723514199256897 1.7546037435531616 3.4781179428100586
Loss :  1.7199684381484985 2.3743882179260254 4.094356536865234
  batch 20 loss: 1.7199684381484985, 2.3743882179260254, 4.094356536865234
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.727075219154358 2.018583059310913 3.7456583976745605
Loss :  1.7284770011901855 2.1911981105804443 3.91967511177063
Loss :  1.7302719354629517 2.1253459453582764 3.8556180000305176
Loss :  1.7352663278579712 3.529473066329956 5.264739513397217
Loss :  1.7259035110473633 2.44292950630188 4.168832778930664
Loss :  1.7254414558410645 2.378054141998291 4.1034955978393555
Loss :  1.7341370582580566 2.28920316696167 4.023340225219727
Loss :  1.7241333723068237 2.1128880977630615 3.8370213508605957
Loss :  1.733678936958313 3.3882343769073486 5.121913433074951
Loss :  1.722395658493042 3.0525476932525635 4.7749433517456055
Loss :  1.7391409873962402 3.344666004180908 5.083806991577148
Loss :  1.7294700145721436 3.4275457859039307 5.157015800476074
Loss :  1.7227473258972168 2.8352203369140625 4.557967662811279
Loss :  1.7198965549468994 2.4151089191436768 4.135005474090576
Loss :  1.729469895362854 2.911353349685669 4.6408233642578125
Loss :  1.7302483320236206 2.692216396331787 4.422464847564697
Loss :  1.7270087003707886 2.5052945613861084 4.232303142547607
Loss :  1.7227401733398438 1.7391951084136963 3.46193528175354
Loss :  1.7265478372573853 2.2102365493774414 3.936784267425537
Loss :  1.726765751838684 2.1040990352630615 3.830864906311035
  batch 40 loss: 1.726765751838684, 2.1040990352630615, 3.830864906311035
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7249066829681396 2.515746831893921 4.2406535148620605
Loss :  1.7223809957504272 2.254464626312256 3.9768457412719727
Loss :  1.7237106561660767 2.2618894577026367 3.985599994659424
Loss :  1.7242305278778076 2.21284818649292 3.9370787143707275
Loss :  1.7276544570922852 2.188314199447632 3.915968656539917
Loss :  1.7251158952713013 2.5123255252838135 4.237441539764404
Loss :  1.7269017696380615 1.9687474966049194 3.6956491470336914
Loss :  1.723486065864563 2.7345194816589355 4.458005428314209
Loss :  1.7314012050628662 2.004955768585205 3.7363569736480713
Loss :  1.721773386001587 2.1464874744415283 3.8682608604431152
Loss :  1.7199662923812866 2.5982866287231445 4.318253040313721
Loss :  1.7272119522094727 2.3780148029327393 4.105226516723633
Loss :  1.7344143390655518 2.1829278469085693 3.917342185974121
Loss :  1.7213491201400757 2.7072806358337402 4.4286298751831055
Loss :  1.7245866060256958 2.6984903812408447 4.42307710647583
Loss :  1.7275049686431885 3.072127103805542 4.7996320724487305
Loss :  1.7310608625411987 2.0886614322662354 3.8197221755981445
Loss :  1.7282205820083618 2.174074649810791 3.9022951126098633
Loss :  1.7321991920471191 2.139068126678467 3.871267318725586
Loss :  1.7321916818618774 1.593420386314392 3.3256120681762695
  batch 60 loss: 1.7321916818618774, 1.593420386314392, 3.3256120681762695
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7289113998413086 1.9366627931594849 3.665574073791504
Loss :  1.7295788526535034 1.9372880458831787 3.6668667793273926
Loss :  1.7325654029846191 2.0642261505126953 3.7967915534973145
Loss :  1.7240744829177856 1.8598865270614624 3.583961009979248
Loss :  1.7210882902145386 2.397709369659424 4.118797779083252
Loss :  1.8842190504074097 4.4424333572387695 6.326652526855469
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8764649629592896 4.381103992462158 6.257568836212158
Loss :  1.8798922300338745 4.35692834854126 6.236820697784424
Loss :  1.876883625984192 4.317458629608154 6.194342136383057
Total LOSS train 4.159521737465492 valid 6.253846049308777
CE LOSS train 1.726572322845459 valid 0.469220906496048
Contrastive LOSS train 2.4329494182880107 valid 1.0793646574020386
EPOCH 152:
Loss :  1.727279782295227 3.2205545902252197 4.947834491729736
Loss :  1.7275596857070923 2.559626579284668 4.287186145782471
Loss :  1.721217393875122 2.250256061553955 3.971473455429077
Loss :  1.724981427192688 2.6435117721557617 4.36849308013916
Loss :  1.7324119806289673 3.11745285987854 4.849864959716797
Loss :  1.7285007238388062 4.513872146606445 6.242372989654541
Loss :  1.7301993370056152 3.4758460521698 5.206045150756836
Loss :  1.732450246810913 3.550913095474243 5.283363342285156
Loss :  1.7307686805725098 3.605534791946411 5.3363037109375
Loss :  1.7161937952041626 2.690936803817749 4.407130718231201
Loss :  1.7269808053970337 2.6207706928253174 4.347751617431641
Loss :  1.7421215772628784 2.5550856590270996 4.297207355499268
Loss :  1.7262119054794312 2.4668238162994385 4.19303560256958
Loss :  1.7285133600234985 2.753218650817871 4.48173189163208
Loss :  1.7352724075317383 2.250044345855713 3.985316753387451
Loss :  1.720317006111145 2.3319292068481445 4.05224609375
Loss :  1.7303221225738525 2.2722561359405518 4.002578258514404
Loss :  1.7236194610595703 2.764481544494629 4.488101005554199
Loss :  1.7254823446273804 2.2414486408233643 3.966930866241455
Loss :  1.7272024154663086 2.230736017227173 3.9579384326934814
  batch 20 loss: 1.7272024154663086, 2.230736017227173, 3.9579384326934814
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7314389944076538 2.4462318420410156 4.177670955657959
Loss :  1.733276128768921 2.596820831298828 4.330097198486328
Loss :  1.732882022857666 2.1781537532806396 3.9110357761383057
Loss :  1.7401022911071777 2.7233707904815674 4.463473320007324
Loss :  1.7245022058486938 2.3125662803649902 4.0370683670043945
Loss :  1.7274730205535889 1.991167664527893 3.7186408042907715
Loss :  1.736006498336792 2.5532066822052 4.289213180541992
Loss :  1.7289037704467773 3.3567404747009277 5.085644245147705
Loss :  1.735251545906067 4.129500389099121 5.864751815795898
Loss :  1.7230912446975708 4.000179767608643 5.723270893096924
Loss :  1.7367496490478516 4.347541809082031 6.084291458129883
Loss :  1.7296799421310425 3.1090967655181885 4.838776588439941
Loss :  1.7238233089447021 3.5303444862365723 5.254167556762695
Loss :  1.7179816961288452 3.744319200515747 5.462300777435303
Loss :  1.7278193235397339 3.45580792427063 5.183627128601074
Loss :  1.7289206981658936 3.8467581272125244 5.575678825378418
Loss :  1.728108525276184 3.7858057022094727 5.513914108276367
Loss :  1.7224549055099487 3.0574305057525635 4.779885292053223
Loss :  1.7293239831924438 3.3788230419158936 5.108147144317627
Loss :  1.7268787622451782 3.242358922958374 4.969237804412842
  batch 40 loss: 1.7268787622451782, 3.242358922958374, 4.969237804412842
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7266058921813965 3.419764995574951 5.146370887756348
Loss :  1.7244646549224854 2.6814451217651367 4.405909538269043
Loss :  1.7247791290283203 2.9718520641326904 4.69663143157959
Loss :  1.7279613018035889 2.7680630683898926 4.496024131774902
Loss :  1.730312705039978 2.9903724193573 4.720685005187988
Loss :  1.7244725227355957 2.548525333404541 4.272997856140137
Loss :  1.731183648109436 2.1721184253692627 3.9033021926879883
Loss :  1.7255010604858398 3.1955440044403076 4.921045303344727
Loss :  1.7339632511138916 3.3903019428253174 5.124265193939209
Loss :  1.7225157022476196 3.0900368690490723 4.812552452087402
Loss :  1.7194640636444092 2.743060827255249 4.462524890899658
Loss :  1.7269188165664673 2.246281862258911 3.973200798034668
Loss :  1.7364450693130493 2.203941583633423 3.9403867721557617
Loss :  1.7214010953903198 2.8067314624786377 4.528132438659668
Loss :  1.7268483638763428 2.484584093093872 4.211432456970215
Loss :  1.7313244342803955 2.636650323867798 4.367974758148193
Loss :  1.7312783002853394 2.4332895278930664 4.164567947387695
Loss :  1.730521321296692 2.2087526321411133 3.9392738342285156
Loss :  1.7331386804580688 2.6355161666870117 4.368654727935791
Loss :  1.734748363494873 2.963090181350708 4.69783878326416
  batch 60 loss: 1.734748363494873, 2.963090181350708, 4.69783878326416
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7301645278930664 3.0688576698303223 4.799022197723389
Loss :  1.7323715686798096 2.663949728012085 4.3963212966918945
Loss :  1.7333253622055054 2.264078378677368 3.997403621673584
Loss :  1.7243387699127197 2.522294759750366 4.246633529663086
Loss :  1.7210748195648193 1.6955130100250244 3.4165878295898438
Loss :  1.6415427923202515 3.779667377471924 5.421210289001465
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6380271911621094 3.7552871704101562 5.393314361572266
Loss :  1.659014344215393 3.5280134677886963 5.187027931213379
Loss :  1.6992946863174438 3.5733489990234375 5.272643566131592
Total LOSS train 4.600823677503145 valid 5.318549036979675
CE LOSS train 1.7284215138508723 valid 0.42482367157936096
Contrastive LOSS train 2.8724021673202516 valid 0.8933372497558594
EPOCH 153:
Loss :  1.728963017463684 2.2949328422546387 4.023895740509033
Loss :  1.7295655012130737 2.7804830074310303 4.5100483894348145
Loss :  1.72212553024292 3.119745969772339 4.84187126159668
Loss :  1.7284154891967773 2.788139820098877 4.516555309295654
Loss :  1.7301089763641357 2.929558277130127 4.659667015075684
Loss :  1.7324655055999756 3.5427942276000977 5.275259971618652
Loss :  1.7296180725097656 3.0265095233917236 4.75612735748291
Loss :  1.7302427291870117 2.3334603309631348 4.0637030601501465
Loss :  1.7274730205535889 2.158785104751587 3.886258125305176
Loss :  1.7117223739624023 2.6991183757781982 4.41084098815918
Loss :  1.7294774055480957 3.226731538772583 4.956209182739258
Loss :  1.7421014308929443 2.9188520908355713 4.660953521728516
Loss :  1.728588581085205 3.1808559894561768 4.909444808959961
Loss :  1.7284029722213745 3.372584342956543 5.100987434387207
Loss :  1.7343827486038208 2.522385835647583 4.256768703460693
Loss :  1.7206000089645386 2.0986270904541016 3.8192272186279297
Loss :  1.7312759160995483 2.276793956756592 4.00806999206543
Loss :  1.723085880279541 1.9725010395050049 3.695586919784546
Loss :  1.7251046895980835 1.5085716247558594 3.2336764335632324
Loss :  1.7239001989364624 2.0239546298980713 3.747854709625244
  batch 20 loss: 1.7239001989364624, 2.0239546298980713, 3.747854709625244
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7285226583480835 2.519721746444702 4.248244285583496
Loss :  1.7306725978851318 3.2811434268951416 5.011816024780273
Loss :  1.7294100522994995 3.1688597202301025 4.8982696533203125
Loss :  1.7372219562530518 2.9934678077697754 4.730690002441406
Loss :  1.7254835367202759 3.248502254486084 4.97398567199707
Loss :  1.725966453552246 3.8091957569122314 5.535161972045898
Loss :  1.7331560850143433 2.4692790508270264 4.20243501663208
Loss :  1.7235524654388428 2.665478229522705 4.389030456542969
Loss :  1.7307076454162598 2.8562309741973877 4.586938858032227
Loss :  1.7231659889221191 3.227142572402954 4.950308799743652
Loss :  1.7376912832260132 3.8913638591766357 5.629055023193359
Loss :  1.7274314165115356 3.311509132385254 5.0389404296875
Loss :  1.7233437299728394 2.5856094360351562 4.308953285217285
Loss :  1.7205010652542114 2.5435373783111572 4.264038562774658
Loss :  1.729227900505066 3.4967703819274902 5.225998401641846
Loss :  1.7298716306686401 3.045274496078491 4.775146007537842
Loss :  1.7290749549865723 2.5062663555145264 4.2353410720825195
Loss :  1.7227100133895874 2.1387851238250732 3.861495018005371
Loss :  1.7301100492477417 2.7653956413269043 4.4955058097839355
Loss :  1.7287302017211914 2.1222825050354004 3.851012706756592
  batch 40 loss: 1.7287302017211914, 2.1222825050354004, 3.851012706756592
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.726989507675171 3.235100269317627 4.962089538574219
Loss :  1.725975513458252 2.5579264163970947 4.283902168273926
Loss :  1.7248073816299438 2.318646192550659 4.043453693389893
Loss :  1.7266759872436523 2.8205313682556152 4.547207355499268
Loss :  1.72907292842865 1.8169585466384888 3.5460314750671387
Loss :  1.7247790098190308 2.3871090412139893 4.1118879318237305
Loss :  1.7290380001068115 2.146249294281006 3.8752872943878174
Loss :  1.7242711782455444 2.144963502883911 3.869234561920166
Loss :  1.7325475215911865 3.17022705078125 4.902774810791016
Loss :  1.7229502201080322 3.190089702606201 4.9130401611328125
Loss :  1.7210091352462769 2.9403185844421387 4.661327838897705
Loss :  1.7282317876815796 2.9423317909240723 4.670563697814941
Loss :  1.736783742904663 2.5134119987487793 4.250195503234863
Loss :  1.7228628396987915 2.9012598991394043 4.624122619628906
Loss :  1.7267652750015259 2.1390488147735596 3.865814208984375
Loss :  1.7305631637573242 2.4090025424957275 4.139565467834473
Loss :  1.7313750982284546 2.169299840927124 3.900674819946289
Loss :  1.7294286489486694 2.3023643493652344 4.031793117523193
Loss :  1.7333012819290161 2.792522430419922 4.525823593139648
Loss :  1.7346253395080566 2.8553576469421387 4.589982986450195
  batch 60 loss: 1.7346253395080566, 2.8553576469421387, 4.589982986450195
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7309890985488892 2.41446852684021 4.145457744598389
Loss :  1.7327898740768433 2.79843807220459 4.531228065490723
Loss :  1.7338039875030518 3.3474044799804688 5.081208229064941
Loss :  1.7253473997116089 3.826988935470581 5.5523362159729
Loss :  1.7214462757110596 3.3624372482299805 5.083883285522461
Loss :  1.6460739374160767 3.933392286300659 5.579466342926025
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6371195316314697 3.614764451980591 5.2518839836120605
Loss :  1.6688470840454102 3.6234700679779053 5.2923173904418945
Loss :  1.7075080871582031 3.3240976333618164 5.0316057205200195
Total LOSS train 4.480834762866681 valid 5.288818359375
CE LOSS train 1.7281631066248968 valid 0.4268770217895508
Contrastive LOSS train 2.7526716617437508 valid 0.8310244083404541
EPOCH 154:
Loss :  1.728299617767334 2.286426067352295 4.014725685119629
Loss :  1.730180025100708 3.3841307163238525 5.1143107414245605
Loss :  1.7229571342468262 2.9901998043060303 4.713156700134277
Loss :  1.7269675731658936 3.3667616844177246 5.093729019165039
Loss :  1.731562852859497 2.758660078048706 4.490222930908203
Loss :  1.7314355373382568 2.0940067768096924 3.825442314147949
Loss :  1.7307249307632446 3.09228777885437 4.823012828826904
Loss :  1.729435682296753 2.3260293006896973 4.055464744567871
Loss :  1.7270153760910034 2.277290105819702 4.004305362701416
Loss :  1.712848424911499 2.527395725250244 4.240243911743164
Loss :  1.7278085947036743 2.8962135314941406 4.624022006988525
Loss :  1.739566683769226 2.846510648727417 4.5860772132873535
Loss :  1.7271947860717773 3.4425246715545654 5.169719696044922
Loss :  1.7272379398345947 2.442885160446167 4.170123100280762
Loss :  1.7351137399673462 2.467073678970337 4.202187538146973
Loss :  1.7202198505401611 2.773331880569458 4.493551731109619
Loss :  1.7298942804336548 3.1164791584014893 4.846373558044434
Loss :  1.723934531211853 3.106574773788452 4.830509185791016
Loss :  1.7231793403625488 4.415341854095459 6.138521194458008
Loss :  1.7242441177368164 3.992208957672119 5.7164530754089355
  batch 20 loss: 1.7242441177368164, 3.992208957672119, 5.7164530754089355
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7298953533172607 2.2137043476104736 3.9435997009277344
Loss :  1.7298083305358887 2.311166286468506 4.0409746170043945
Loss :  1.7328675985336304 2.0032804012298584 3.736147880554199
Loss :  1.7382471561431885 2.9580018520355225 4.696249008178711
Loss :  1.7267929315567017 2.2690703868865967 3.995863437652588
Loss :  1.7274025678634644 2.242013692855835 3.9694161415100098
Loss :  1.734579086303711 2.3937759399414062 4.128355026245117
Loss :  1.7255781888961792 2.5906262397766113 4.31620454788208
Loss :  1.7326833009719849 1.9064831733703613 3.6391663551330566
Loss :  1.7226076126098633 2.4020447731018066 4.12465238571167
Loss :  1.736158847808838 2.5378496646881104 4.274008750915527
Loss :  1.7274757623672485 2.398130178451538 4.125606060028076
Loss :  1.721131682395935 2.293564796447754 4.0146965980529785
Loss :  1.7184697389602661 3.133180856704712 4.851650714874268
Loss :  1.7271591424942017 2.3622753620147705 4.089434623718262
Loss :  1.7288990020751953 2.0842278003692627 3.813126802444458
Loss :  1.7278646230697632 2.093296766281128 3.8211612701416016
Loss :  1.7238812446594238 2.2575669288635254 3.981448173522949
Loss :  1.7277857065200806 2.6835858821868896 4.41137170791626
Loss :  1.7279256582260132 3.052159547805786 4.78008508682251
  batch 40 loss: 1.7279256582260132, 3.052159547805786, 4.78008508682251
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7265392541885376 3.390629768371582 5.11716890335083
Loss :  1.7225819826126099 3.623871088027954 5.3464531898498535
Loss :  1.7236700057983398 2.3503689765930176 4.074038982391357
Loss :  1.7264925241470337 2.823687791824341 4.550180435180664
Loss :  1.728063702583313 2.7846832275390625 4.512746810913086
Loss :  1.7261040210723877 3.0720107555389404 4.798114776611328
Loss :  1.728145956993103 2.678396701812744 4.406542778015137
Loss :  1.7232868671417236 2.7709083557128906 4.494194984436035
Loss :  1.7312058210372925 2.439105749130249 4.170311450958252
Loss :  1.7210835218429565 2.1153905391693115 3.8364739418029785
Loss :  1.718226432800293 2.152824640274048 3.871051073074341
Loss :  1.724614143371582 1.9988411664962769 3.7234554290771484
Loss :  1.7335186004638672 1.9874107837677002 3.7209293842315674
Loss :  1.7179139852523804 2.2273061275482178 3.9452199935913086
Loss :  1.72407865524292 2.223452091217041 3.947530746459961
Loss :  1.7259591817855835 2.3899290561676025 4.1158881187438965
Loss :  1.7288538217544556 2.9800503253936768 4.708904266357422
Loss :  1.7269458770751953 2.502332925796509 4.229278564453125
Loss :  1.7303179502487183 2.757512092590332 4.48783016204834
Loss :  1.7324330806732178 2.097106456756592 3.8295395374298096
  batch 60 loss: 1.7324330806732178, 2.097106456756592, 3.8295395374298096
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7280091047286987 2.6647729873657227 4.392782211303711
Loss :  1.7291715145111084 2.9484968185424805 4.677668571472168
Loss :  1.732228398323059 1.8594129085540771 3.591641426086426
Loss :  1.7231377363204956 3.239140510559082 4.962278366088867
Loss :  1.7203093767166138 2.15018367767334 3.870492935180664
Loss :  1.6683788299560547 3.8582346439361572 5.526613235473633
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6586674451828003 3.79925274848938 5.457920074462891
Loss :  1.6872931718826294 3.725111484527588 5.412404537200928
Loss :  1.7188615798950195 3.471231698989868 5.190093040466309
Total LOSS train 4.358247514871451 valid 5.39675772190094
CE LOSS train 1.7272296318641076 valid 0.4297153949737549
Contrastive LOSS train 2.63101788850931 valid 0.867807924747467
EPOCH 155:
Loss :  1.7274173498153687 2.937492847442627 4.664910316467285
Loss :  1.7286673784255981 3.002977132797241 4.731644630432129
Loss :  1.7217210531234741 2.8935282230377197 4.615249156951904
Loss :  1.7256360054016113 2.867213249206543 4.592849254608154
Loss :  1.730141043663025 2.028437376022339 3.758578300476074
Loss :  1.7301757335662842 2.092789649963379 3.822965383529663
Loss :  1.7291290760040283 2.24416184425354 3.9732909202575684
Loss :  1.7282779216766357 1.8400132656097412 3.568291187286377
Loss :  1.726582407951355 3.0299360752105713 4.756518363952637
Loss :  1.7112269401550293 2.0571887493133545 3.768415689468384
Loss :  1.7274792194366455 2.4716684818267822 4.199147701263428
Loss :  1.7405282258987427 2.3180978298187256 4.058626174926758
Loss :  1.7265986204147339 2.361220121383667 4.087818622589111
Loss :  1.7273448705673218 2.6149520874023438 4.342297077178955
Loss :  1.7320204973220825 1.815300464630127 3.54732084274292
Loss :  1.7195080518722534 2.5681252479553223 4.287633419036865
Loss :  1.7290472984313965 2.3383491039276123 4.06739616394043
Loss :  1.7220460176467896 2.4194138050079346 4.141459941864014
Loss :  1.72555673122406 2.4395413398742676 4.165098190307617
Loss :  1.7224843502044678 2.7717056274414062 4.494190216064453
  batch 20 loss: 1.7224843502044678, 2.7717056274414062, 4.494190216064453
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7288711071014404 2.4918107986450195 4.220682144165039
Loss :  1.7306312322616577 2.4940311908721924 4.2246623039245605
Loss :  1.7307265996932983 2.635826587677002 4.36655330657959
Loss :  1.7360761165618896 2.202631950378418 3.9387080669403076
Loss :  1.7284730672836304 2.595191240310669 4.32366418838501
Loss :  1.7259016036987305 2.358820915222168 4.084722518920898
Loss :  1.7325780391693115 2.4259676933288574 4.15854549407959
Loss :  1.7230734825134277 2.2473642826080322 3.97043776512146
Loss :  1.7324450016021729 2.4382059574127197 4.170650959014893
Loss :  1.7243794202804565 2.623380422592163 4.34775972366333
Loss :  1.7398184537887573 2.722025156021118 4.461843490600586
Loss :  1.728484869003296 2.86234188079834 4.590826988220215
Loss :  1.7235974073410034 2.382840156555176 4.106437683105469
Loss :  1.7226532697677612 2.844012975692749 4.566666126251221
Loss :  1.7308454513549805 2.9250893592834473 4.655934810638428
Loss :  1.7322626113891602 3.136317729949951 4.868580341339111
Loss :  1.7283179759979248 2.6288259029388428 4.357143878936768
Loss :  1.7213047742843628 2.4583308696746826 4.179635524749756
Loss :  1.727799892425537 2.195139169692993 3.9229390621185303
Loss :  1.7264037132263184 2.7728707790374756 4.499274253845215
  batch 40 loss: 1.7264037132263184, 2.7728707790374756, 4.499274253845215
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7254844903945923 2.605820417404175 4.331305027008057
Loss :  1.725097417831421 2.3236820697784424 4.048779487609863
Loss :  1.7260456085205078 2.72062349319458 4.446669101715088
Loss :  1.7263085842132568 2.3828186988830566 4.109127044677734
Loss :  1.7290871143341064 2.0460245609283447 3.775111675262451
Loss :  1.7240756750106812 2.447011709213257 4.171087265014648
Loss :  1.7254586219787598 2.303072214126587 4.028531074523926
Loss :  1.724900245666504 2.624859571456909 4.349760055541992
Loss :  1.7272731065750122 2.376429796218872 4.103703022003174
Loss :  1.7227784395217896 3.1547293663024902 4.87750768661499
Loss :  1.7209463119506836 3.0636746883392334 4.784621238708496
Loss :  1.7267613410949707 3.076036214828491 4.802797317504883
Loss :  1.734139084815979 3.353539228439331 5.0876784324646
Loss :  1.7238467931747437 3.4273293018341064 5.1511759757995605
Loss :  1.7279075384140015 2.9680674076080322 4.695974826812744
Loss :  1.7278159856796265 2.944797992706299 4.672614097595215
Loss :  1.7295634746551514 3.0950660705566406 4.824629783630371
Loss :  1.7295901775360107 2.673434257507324 4.403024673461914
Loss :  1.7328758239746094 3.1042816638946533 4.837157249450684
Loss :  1.732118010520935 2.3312861919403076 4.063404083251953
  batch 60 loss: 1.732118010520935, 2.3312861919403076, 4.063404083251953
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7277162075042725 2.363412380218506 4.091128349304199
Loss :  1.730827808380127 2.1550402641296387 3.8858680725097656
Loss :  1.7307734489440918 1.9835716485977173 3.7143449783325195
Loss :  1.7243565320968628 2.4904494285583496 4.214806079864502
Loss :  1.7219058275222778 2.3973000049591064 4.119205951690674
Loss :  1.665497064590454 3.649096727371216 5.31459379196167
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6598623991012573 3.4884541034698486 5.148316383361816
Loss :  1.6832932233810425 3.3608906269073486 5.044183731079102
Loss :  1.7146873474121094 3.314887762069702 5.029575347900391
Total LOSS train 4.296144349758442 valid 5.134167313575745
CE LOSS train 1.7274444084901077 valid 0.42867183685302734
Contrastive LOSS train 2.568699941268334 valid 0.8287219405174255
EPOCH 156:
Loss :  1.7276670932769775 2.6420953273773193 4.369762420654297
Loss :  1.7263818979263306 2.8878824710845947 4.614264488220215
Loss :  1.7217607498168945 1.8919812440872192 3.613741874694824
Loss :  1.7270457744598389 3.0034291744232178 4.730474948883057
Loss :  1.7304104566574097 2.7715609073638916 4.501971244812012
Loss :  1.730111837387085 3.0190811157226562 4.74919319152832
Loss :  1.7295315265655518 2.867717742919922 4.5972490310668945
Loss :  1.7277827262878418 2.580603837966919 4.30838680267334
Loss :  1.7277752161026 2.765007257461548 4.4927825927734375
Loss :  1.7133288383483887 2.583393096923828 4.296721935272217
Loss :  1.7275115251541138 3.150312662124634 4.877824306488037
Loss :  1.7410842180252075 2.347423553466797 4.088507652282715
Loss :  1.7281755208969116 2.7755329608917236 4.503708362579346
Loss :  1.7273619174957275 3.20827054977417 4.935632705688477
Loss :  1.7328548431396484 2.3445844650268555 4.077439308166504
Loss :  1.7197909355163574 2.6352407932281494 4.355031967163086
Loss :  1.7289891242980957 3.0512521266937256 4.780241012573242
Loss :  1.7242151498794556 2.6422383785247803 4.366453647613525
Loss :  1.7254005670547485 1.9317729473114014 3.6571736335754395
Loss :  1.721937894821167 2.784217357635498 4.506155014038086
  batch 20 loss: 1.721937894821167, 2.784217357635498, 4.506155014038086
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7285338640213013 2.226306200027466 3.9548401832580566
Loss :  1.729202389717102 3.0584707260131836 4.787672996520996
Loss :  1.730494737625122 2.4078991413116455 4.138393878936768
Loss :  1.7349785566329956 2.04020094871521 3.775179386138916
Loss :  1.726090908050537 2.096299648284912 3.822390556335449
Loss :  1.7248905897140503 2.383000612258911 4.107891082763672
Loss :  1.732544183731079 2.5677618980407715 4.30030632019043
Loss :  1.7228164672851562 2.4914958477020264 4.214312553405762
Loss :  1.7320725917816162 2.382622480392456 4.114695072174072
Loss :  1.7218698263168335 2.728478193283081 4.450347900390625
Loss :  1.7378069162368774 2.74104905128479 4.478856086730957
Loss :  1.7292792797088623 3.558480978012085 5.287760257720947
Loss :  1.7225712537765503 2.509251117706299 4.231822490692139
Loss :  1.7206648588180542 2.5862202644348145 4.306885242462158
Loss :  1.730136752128601 2.9874656200408936 4.717602252960205
Loss :  1.7311251163482666 3.0426976680755615 4.773822784423828
Loss :  1.7273186445236206 2.522700071334839 4.25001859664917
Loss :  1.7221275568008423 2.3291208744049072 4.051248550415039
Loss :  1.726439356803894 2.139340400695801 3.8657798767089844
Loss :  1.7256470918655396 2.173391819000244 3.899038791656494
  batch 40 loss: 1.7256470918655396, 2.173391819000244, 3.899038791656494
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7243152856826782 2.041578769683838 3.7658939361572266
Loss :  1.7232449054718018 1.6943467855453491 3.4175915718078613
Loss :  1.724776268005371 2.132443904876709 3.85722017288208
Loss :  1.7240219116210938 2.434706926345825 4.15872859954834
Loss :  1.7274415493011475 2.5883755683898926 4.315816879272461
Loss :  1.723055362701416 3.026848316192627 4.749903678894043
Loss :  1.7244365215301514 2.1871941089630127 3.911630630493164
Loss :  1.724314570426941 2.3834633827209473 4.107778072357178
Loss :  1.7265254259109497 2.8702845573425293 4.5968098640441895
Loss :  1.7225143909454346 2.9993653297424316 4.721879959106445
Loss :  1.72003972530365 2.8691864013671875 4.589226245880127
Loss :  1.7258679866790771 2.8683555126190186 4.594223499298096
Loss :  1.7323862314224243 2.715123176574707 4.447509288787842
Loss :  1.7214598655700684 3.192366361618042 4.913825988769531
Loss :  1.7267725467681885 3.2784764766693115 5.0052490234375
Loss :  1.7262049913406372 3.5033295154571533 5.22953462600708
Loss :  1.729723334312439 3.0340592861175537 4.763782501220703
Loss :  1.7291980981826782 3.349938154220581 5.079136371612549
Loss :  1.7323466539382935 3.593320369720459 5.325666904449463
Loss :  1.7313263416290283 2.302506685256958 4.033833026885986
  batch 60 loss: 1.7313263416290283, 2.302506685256958, 4.033833026885986
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7270777225494385 2.4929416179656982 4.220019340515137
Loss :  1.729939579963684 2.313403844833374 4.043343544006348
Loss :  1.7305010557174683 2.1132652759552 3.843766212463379
Loss :  1.7235525846481323 3.0329437255859375 4.756496429443359
Loss :  1.7215849161148071 1.3695957660675049 3.0911808013916016
Loss :  1.6513856649398804 3.6906447410583496 5.3420305252075195
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.645830750465393 3.691678285598755 5.3375091552734375
Loss :  1.6691946983337402 3.602830648422241 5.272025108337402
Loss :  1.6975401639938354 3.4726455211639404 5.170185565948486
Total LOSS train 4.361409664154053 valid 5.280437588691711
CE LOSS train 1.7268977935497578 valid 0.42438504099845886
Contrastive LOSS train 2.634511866936317 valid 0.8681613802909851
EPOCH 157:
Loss :  1.727795124053955 1.7162219285964966 3.444016933441162
Loss :  1.72721529006958 2.2527403831481934 3.9799556732177734
Loss :  1.7231316566467285 1.8455332517623901 3.568665027618408
Loss :  1.7288483381271362 2.3965139389038086 4.125362396240234
Loss :  1.731370210647583 3.017470598220825 4.748840808868408
Loss :  1.7304463386535645 2.620457172393799 4.350903511047363
Loss :  1.7311851978302002 3.1141083240509033 4.8452935218811035
Loss :  1.7283817529678345 2.120763063430786 3.84914493560791
Loss :  1.7281863689422607 2.5233564376831055 4.251543045043945
Loss :  1.714916706085205 2.4543237686157227 4.169240474700928
Loss :  1.7291616201400757 2.678008794784546 4.407170295715332
Loss :  1.741174340248108 2.9426627159118652 4.683836936950684
Loss :  1.7303112745285034 2.8848366737365723 4.615148067474365
Loss :  1.7306649684906006 3.650304079055786 5.380969047546387
Loss :  1.7355810403823853 3.747920036315918 5.483500957489014
Loss :  1.7232952117919922 3.598175525665283 5.321470737457275
Loss :  1.7306783199310303 3.385876178741455 5.116554260253906
Loss :  1.725273609161377 3.018483877182007 4.743757247924805
Loss :  1.7264447212219238 2.381524085998535 4.107968807220459
Loss :  1.7239550352096558 2.300598382949829 4.024553298950195
  batch 20 loss: 1.7239550352096558, 2.300598382949829, 4.024553298950195
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7284016609191895 2.699763059616089 4.428164482116699
Loss :  1.7302285432815552 2.8187713623046875 4.548999786376953
Loss :  1.7297163009643555 3.6228203773498535 5.352536678314209
Loss :  1.734785556793213 2.663203239440918 4.397988796234131
Loss :  1.7273982763290405 2.7305550575256348 4.457953453063965
Loss :  1.7254152297973633 2.3633687496185303 4.088784217834473
Loss :  1.733049988746643 2.4182775020599365 4.151327610015869
Loss :  1.7238831520080566 2.5969367027282715 4.320819854736328
Loss :  1.7322289943695068 2.471825361251831 4.204054355621338
Loss :  1.7241801023483276 2.631826639175415 4.356006622314453
Loss :  1.7392029762268066 2.9801015853881836 4.71930456161499
Loss :  1.729222059249878 3.3419291973114014 5.071151256561279
Loss :  1.7240262031555176 2.6815531253814697 4.405579566955566
Loss :  1.7220386266708374 2.2543985843658447 3.9764370918273926
Loss :  1.731579065322876 2.586826801300049 4.318406105041504
Loss :  1.7326328754425049 2.503512382507324 4.23614501953125
Loss :  1.7285964488983154 2.409186840057373 4.137783050537109
Loss :  1.7228463888168335 3.062556743621826 4.785403251647949
Loss :  1.7284135818481445 2.7593748569488525 4.487788200378418
Loss :  1.7266061305999756 2.633805513381958 4.360411643981934
  batch 40 loss: 1.7266061305999756, 2.633805513381958, 4.360411643981934
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7263762950897217 3.1913673877716064 4.917743682861328
Loss :  1.7244153022766113 3.041321039199829 4.7657365798950195
Loss :  1.7254825830459595 2.5926101207733154 4.3180928230285645
Loss :  1.725536584854126 2.1410419940948486 3.8665785789489746
Loss :  1.7284433841705322 2.2561984062194824 3.9846417903900146
Loss :  1.722977638244629 2.3409321308135986 4.063909530639648
Loss :  1.7255194187164307 2.006873607635498 3.7323930263519287
Loss :  1.7253950834274292 2.026007890701294 3.7514028549194336
Loss :  1.7284128665924072 2.2391936779022217 3.967606544494629
Loss :  1.7240365743637085 2.298210620880127 4.022247314453125
Loss :  1.7211629152297974 3.025103807449341 4.746266841888428
Loss :  1.727211594581604 2.3296384811401367 4.056849956512451
Loss :  1.7344306707382202 2.734870433807373 4.469301223754883
Loss :  1.7231560945510864 2.6163833141326904 4.339539527893066
Loss :  1.727274775505066 2.6718251705169678 4.399099826812744
Loss :  1.727570652961731 2.790721893310547 4.518292427062988
Loss :  1.7313756942749023 2.473498821258545 4.204874515533447
Loss :  1.7318265438079834 2.1424524784088135 3.874279022216797
Loss :  1.7337548732757568 2.704357862472534 4.438112735748291
Loss :  1.7320935726165771 2.2653186321258545 3.9974122047424316
  batch 60 loss: 1.7320935726165771, 2.2653186321258545, 3.9974122047424316
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7298305034637451 3.2362825870513916 4.966113090515137
Loss :  1.7316571474075317 2.816065788269043 4.547722816467285
Loss :  1.7325527667999268 1.9707322120666504 3.703284978866577
Loss :  1.7255942821502686 2.467902183532715 4.1934967041015625
Loss :  1.7232917547225952 2.5862505435943604 4.309542179107666
Loss :  1.7076280117034912 3.815218687057495 5.522846698760986
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7049187421798706 3.9720072746276855 5.676926136016846
Loss :  1.7126935720443726 3.758255958557129 5.470949649810791
Loss :  1.7211066484451294 3.7017312049865723 5.422837734222412
Total LOSS train 4.371961267177875 valid 5.523390054702759
CE LOSS train 1.728182290150569 valid 0.43027666211128235
Contrastive LOSS train 2.6437789843632626 valid 0.9254328012466431
EPOCH 158:
Loss :  1.7284984588623047 3.994725227355957 5.723223686218262
Loss :  1.7268881797790527 2.075474500656128 3.8023626804351807
Loss :  1.7226338386535645 1.649177074432373 3.3718109130859375
Loss :  1.7275819778442383 1.8906584978103638 3.6182403564453125
Loss :  1.7308441400527954 1.8748564720153809 3.6057004928588867
Loss :  1.7295410633087158 2.202270269393921 3.9318113327026367
Loss :  1.7288005352020264 2.1843981742858887 3.913198709487915
Loss :  1.7270033359527588 1.6847279071807861 3.411731243133545
Loss :  1.7272189855575562 2.641974687576294 4.3691935539245605
Loss :  1.7135741710662842 2.9132683277130127 4.626842498779297
Loss :  1.7276324033737183 3.2345433235168457 4.9621758460998535
Loss :  1.7396669387817383 2.4417333602905273 4.181400299072266
Loss :  1.7292377948760986 2.3321683406829834 4.061406135559082
Loss :  1.7296279668807983 2.026571035385132 3.7561988830566406
Loss :  1.7330656051635742 3.2888569831848145 5.021922588348389
Loss :  1.7233799695968628 2.3315927982330322 4.0549726486206055
Loss :  1.7309894561767578 2.274702548980713 4.005692005157471
Loss :  1.7261028289794922 2.281276226043701 4.007379055023193
Loss :  1.7285901308059692 2.3811497688293457 4.109739780426025
Loss :  1.7245666980743408 2.2409632205963135 3.9655299186706543
  batch 20 loss: 1.7245666980743408, 2.2409632205963135, 3.9655299186706543
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7309160232543945 1.8655935525894165 3.5965094566345215
Loss :  1.7311174869537354 2.0312142372131348 3.76233172416687
Loss :  1.7316738367080688 2.3757753372192383 4.107449054718018
Loss :  1.7364282608032227 1.6431900262832642 3.3796181678771973
Loss :  1.7294272184371948 1.7902512550354004 3.5196785926818848
Loss :  1.7281392812728882 1.6434506177902222 3.3715898990631104
Loss :  1.735473394393921 2.1581718921661377 3.8936452865600586
Loss :  1.7270981073379517 3.3354289531707764 5.062527179718018
Loss :  1.7357375621795654 2.348142385482788 4.0838799476623535
Loss :  1.7280359268188477 3.1261160373687744 4.854151725769043
Loss :  1.7411972284317017 2.7791926860809326 4.520390033721924
Loss :  1.7333382368087769 4.521946430206299 6.255284786224365
Loss :  1.7269368171691895 3.095630168914795 4.822566986083984
Loss :  1.7249213457107544 3.7334046363830566 5.4583258628845215
Loss :  1.734011173248291 3.2200682163238525 4.954079627990723
Loss :  1.735007643699646 2.4781315326690674 4.213139057159424
Loss :  1.7307264804840088 2.552006721496582 4.282732963562012
Loss :  1.7255252599716187 1.882093071937561 3.6076183319091797
Loss :  1.7299717664718628 2.2192118167877197 3.949183464050293
Loss :  1.7288280725479126 2.560565233230591 4.289393424987793
  batch 40 loss: 1.7288280725479126, 2.560565233230591, 4.289393424987793
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7280505895614624 3.036857843399048 4.764908313751221
Loss :  1.7289031744003296 1.793125867843628 3.522028923034668
Loss :  1.7288562059402466 2.0218915939331055 3.7507476806640625
Loss :  1.7289423942565918 2.45837664604187 4.187318801879883
Loss :  1.7315925359725952 2.3504605293273926 4.082053184509277
Loss :  1.727652668952942 1.9270751476287842 3.6547279357910156
Loss :  1.7286064624786377 1.7614415884017944 3.4900479316711426
Loss :  1.7283320426940918 2.0018303394317627 3.7301623821258545
Loss :  1.7313482761383057 2.4448788166046143 4.17622709274292
Loss :  1.7265204191207886 2.9547579288482666 4.681278228759766
Loss :  1.7245227098464966 2.364853620529175 4.089376449584961
Loss :  1.729960560798645 2.189354181289673 3.9193148612976074
Loss :  1.735876202583313 2.118265390396118 3.8541417121887207
Loss :  1.7267729043960571 3.2864038944244385 5.013176918029785
Loss :  1.730073094367981 2.055799722671509 3.7858729362487793
Loss :  1.7311605215072632 1.9699290990829468 3.70108962059021
Loss :  1.733412742614746 2.2730071544647217 4.006420135498047
Loss :  1.7334723472595215 1.9765430688858032 3.710015296936035
Loss :  1.736676812171936 2.351303815841675 4.0879807472229
Loss :  1.734797477722168 1.99083411693573 3.7256317138671875
  batch 60 loss: 1.734797477722168, 1.99083411693573, 3.7256317138671875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7311943769454956 2.3918538093566895 4.123048305511475
Loss :  1.7335714101791382 2.8389534950256348 4.5725250244140625
Loss :  1.7332093715667725 1.5949981212615967 3.328207492828369
Loss :  1.727434515953064 1.9187062978744507 3.6461408138275146
Loss :  1.7247174978256226 1.6992592811584473 3.4239768981933594
Loss :  1.6737661361694336 3.68076753616333 5.354533672332764
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6755070686340332 3.7065701484680176 5.382077217102051
Loss :  1.6810848712921143 3.5601706504821777 5.241255760192871
Loss :  1.6893471479415894 3.433403968811035 5.122751235961914
Total LOSS train 4.115554578487689 valid 5.2751544713974
CE LOSS train 1.729778660260714 valid 0.42233678698539734
Contrastive LOSS train 2.385775921894954 valid 0.8583509922027588
EPOCH 159:
Loss :  1.7312091588974 3.333522081375122 5.064731121063232
Loss :  1.7296504974365234 3.923935890197754 5.653586387634277
Loss :  1.7245246171951294 3.014017343521118 4.738542079925537
Loss :  1.729469895362854 3.646214723587036 5.37568473815918
Loss :  1.7328686714172363 3.2328946590423584 4.965763092041016
Loss :  1.73183012008667 3.1804614067077637 4.912291526794434
Loss :  1.7307357788085938 3.258172035217285 4.988907814025879
Loss :  1.7288116216659546 3.178091526031494 4.906903266906738
Loss :  1.7285326719284058 3.0630528926849365 4.791585445404053
Loss :  1.7153042554855347 2.9100732803344727 4.625377655029297
Loss :  1.728143334388733 2.727834939956665 4.4559783935546875
Loss :  1.7411730289459229 2.445544719696045 4.186717987060547
Loss :  1.7287321090698242 2.7727410793304443 4.501473426818848
Loss :  1.730267882347107 2.327620267868042 4.057888031005859
Loss :  1.737618327140808 2.1459035873413086 3.8835220336914062
Loss :  1.7247085571289062 2.288382053375244 4.01309061050415
Loss :  1.7307214736938477 2.4905340671539307 4.221255302429199
Loss :  1.7258970737457275 2.649890661239624 4.375787734985352
Loss :  1.728676676750183 3.164543390274048 4.893219947814941
Loss :  1.7258182764053345 2.8377249240875244 4.563543319702148
  batch 20 loss: 1.7258182764053345, 2.8377249240875244, 4.563543319702148
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7326024770736694 3.038036346435547 4.770638942718506
Loss :  1.7321653366088867 3.123427629470825 4.855592727661133
Loss :  1.7340922355651855 3.8191633224487305 5.553255558013916
Loss :  1.73811936378479 3.8057920932769775 5.543911457061768
Loss :  1.7308868169784546 2.7623589038848877 4.493245601654053
Loss :  1.7292976379394531 2.2482473850250244 3.9775450229644775
Loss :  1.7353923320770264 2.5420427322387695 4.277435302734375
Loss :  1.7274370193481445 2.4180989265441895 4.145535945892334
Loss :  1.7361418008804321 3.631997585296631 5.368139266967773
Loss :  1.726702332496643 2.6646642684936523 4.391366481781006
Loss :  1.7407996654510498 2.479513645172119 4.22031307220459
Loss :  1.7321676015853882 2.0306074619293213 3.76277494430542
Loss :  1.726563811302185 2.3714725971221924 4.098036289215088
Loss :  1.7251245975494385 2.3222782611846924 4.047402858734131
Loss :  1.7327971458435059 3.815681219100952 5.548478126525879
Loss :  1.7341712713241577 3.7319836616516113 5.466155052185059
Loss :  1.730391263961792 2.8904733657836914 4.6208648681640625
Loss :  1.7248828411102295 3.1843137741088867 4.909196853637695
Loss :  1.7302495241165161 3.8645942211151123 5.594843864440918
Loss :  1.728663444519043 3.97622013092041 5.704883575439453
  batch 40 loss: 1.728663444519043, 3.97622013092041, 5.704883575439453
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7276853322982788 3.5675289630889893 5.2952141761779785
Loss :  1.7250908613204956 3.460637331008911 5.185728073120117
Loss :  1.7269262075424194 3.2854740619659424 5.012400150299072
Loss :  1.7265560626983643 3.157548189163208 4.884104251861572
Loss :  1.729783535003662 2.433001756668091 4.162785530090332
Loss :  1.724469780921936 2.277022361755371 4.001492023468018
Loss :  1.7265771627426147 2.634268283843994 4.360845565795898
Loss :  1.727543830871582 2.4632065296173096 4.1907501220703125
Loss :  1.7284016609191895 2.550511360168457 4.2789130210876465
Loss :  1.7256672382354736 2.078930139541626 3.8045973777770996
Loss :  1.723281741142273 2.078516960144043 3.8017988204956055
Loss :  1.729390025138855 1.917650580406189 3.647040605545044
Loss :  1.7349109649658203 1.9474223852157593 3.682333469390869
Loss :  1.7259279489517212 2.1440682411193848 3.8699960708618164
Loss :  1.72983717918396 1.934637188911438 3.6644744873046875
Loss :  1.7308980226516724 2.8283767700195312 4.559274673461914
Loss :  1.7328382730484009 2.923656463623047 4.656494617462158
Loss :  1.7336084842681885 2.2894551753997803 4.023063659667969
Loss :  1.7364230155944824 3.0609004497528076 4.797323226928711
Loss :  1.7355765104293823 3.5954694747924805 5.331046104431152
  batch 60 loss: 1.7355765104293823, 3.5954694747924805, 5.331046104431152
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7342435121536255 3.4205689430236816 5.154812335968018
Loss :  1.7358962297439575 3.5793919563293457 5.315288066864014
Loss :  1.737754225730896 3.2208454608917236 4.95859956741333
Loss :  1.7325085401535034 3.2101049423217773 4.94261360168457
Loss :  1.7299846410751343 2.0734777450561523 3.803462505340576
Loss :  1.69657564163208 3.471156597137451 5.167732238769531
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6911183595657349 3.3995604515075684 5.090678691864014
Loss :  1.7050758600234985 3.258277177810669 4.963353157043457
Loss :  1.714780569076538 3.270019769668579 4.984800338745117
Total LOSS train 4.613998735868014 valid 5.05164110660553
CE LOSS train 1.7302326697569628 valid 0.4286951422691345
Contrastive LOSS train 2.883766073447007 valid 0.8175049424171448
EPOCH 160:
Loss :  1.732771635055542 2.467046022415161 4.199817657470703
Loss :  1.7324632406234741 3.032588005065918 4.765051364898682
Loss :  1.7276647090911865 2.6389036178588867 4.366568565368652
Loss :  1.7306121587753296 2.8512494564056396 4.58186149597168
Loss :  1.7354737520217896 2.508087396621704 4.243561267852783
Loss :  1.7337843179702759 2.1023762226104736 3.836160659790039
Loss :  1.7361706495285034 2.325855016708374 4.062025547027588
Loss :  1.7327289581298828 1.8359930515289307 3.5687220096588135
Loss :  1.7305917739868164 1.9606744050979614 3.6912660598754883
Loss :  1.7205004692077637 1.7440365552902222 3.4645371437072754
Loss :  1.7324050664901733 2.1821444034576416 3.9145493507385254
Loss :  1.7392024993896484 1.9873162508010864 3.7265186309814453
Loss :  1.7324292659759521 2.2482588291168213 3.9806880950927734
Loss :  1.7323533296585083 2.0635461807250977 3.7958993911743164
Loss :  1.73689866065979 1.8090527057647705 3.5459513664245605
Loss :  1.7245166301727295 2.203047513961792 3.9275641441345215
Loss :  1.733268141746521 2.3297829627990723 4.063051223754883
Loss :  1.7281116247177124 2.685624361038208 4.413735866546631
Loss :  1.7280930280685425 2.0835654735565186 3.8116583824157715
Loss :  1.72747802734375 2.3628077507019043 4.090285778045654
  batch 20 loss: 1.72747802734375, 2.3628077507019043, 4.090285778045654
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7335004806518555 2.4857938289642334 4.219294548034668
Loss :  1.7335751056671143 2.917280435562134 4.650855541229248
Loss :  1.7342826128005981 2.898860216140747 4.633142948150635
Loss :  1.740648865699768 3.344069480895996 5.084718227386475
Loss :  1.7298436164855957 3.877868890762329 5.607712745666504
Loss :  1.728240728378296 3.072113513946533 4.80035400390625
Loss :  1.7358297109603882 2.7908742427825928 4.526703834533691
Loss :  1.7273181676864624 3.025693416595459 4.753011703491211
Loss :  1.7351999282836914 2.4218499660491943 4.157050132751465
Loss :  1.7256942987442017 3.572596549987793 5.298290729522705
Loss :  1.7389558553695679 3.328251600265503 5.067207336425781
Loss :  1.7320835590362549 2.7689566612243652 4.501040458679199
Loss :  1.726158618927002 2.2682673931121826 3.9944260120391846
Loss :  1.7224574089050293 2.38213849067688 4.104596138000488
Loss :  1.7320839166641235 2.381906747817993 4.113990783691406
Loss :  1.7327855825424194 2.501648187637329 4.234433650970459
Loss :  1.73061203956604 2.196803331375122 3.927415370941162
Loss :  1.7291462421417236 2.05245304107666 3.781599283218384
Loss :  1.730993628501892 1.9869602918624878 3.71795392036438
Loss :  1.7307955026626587 2.283156394958496 4.013951778411865
  batch 40 loss: 1.7307955026626587, 2.283156394958496, 4.013951778411865
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7301045656204224 2.6093671321868896 4.339471817016602
Loss :  1.726735234260559 2.262976884841919 3.9897122383117676
Loss :  1.7277438640594482 2.002617597579956 3.7303614616394043
Loss :  1.727491855621338 1.9763271808624268 3.7038190364837646
Loss :  1.7314528226852417 1.9884681701660156 3.719921112060547
Loss :  1.7295844554901123 2.2970237731933594 4.026608467102051
Loss :  1.7315832376480103 2.0690319538116455 3.8006153106689453
Loss :  1.7282856702804565 2.2106196880340576 3.9389052391052246
Loss :  1.7348241806030273 2.9512476921081543 4.686071872711182
Loss :  1.7261478900909424 2.666846752166748 4.3929948806762695
Loss :  1.7248672246932983 3.943624973297119 5.668492317199707
Loss :  1.7299000024795532 4.422406196594238 6.152306079864502
Loss :  1.7387571334838867 4.02579927444458 5.764556407928467
Loss :  1.7254998683929443 4.250227451324463 5.975727081298828
Loss :  1.7303624153137207 4.110654830932617 5.841017246246338
Loss :  1.733451008796692 3.829676389694214 5.563127517700195
Loss :  1.7334119081497192 3.870861053466797 5.604272842407227
Loss :  1.7320111989974976 3.4134347438812256 5.145445823669434
Loss :  1.7344861030578613 3.003682851791382 4.738168716430664
Loss :  1.733489990234375 2.2072882652282715 3.9407782554626465
  batch 60 loss: 1.733489990234375, 2.2072882652282715, 3.9407782554626465
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7310667037963867 2.325906276702881 4.056972980499268
Loss :  1.7327815294265747 2.9540977478027344 4.6868791580200195
Loss :  1.7348867654800415 2.0298819541931152 3.764768600463867
Loss :  1.7270922660827637 2.2960450649261475 4.023137092590332
Loss :  1.7249934673309326 1.8842588663101196 3.609252452850342
Loss :  1.636042594909668 4.187260150909424 5.823302745819092
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.640760064125061 4.339024066925049 5.97978401184082
Loss :  1.640460729598999 4.249751567840576 5.890212059020996
Loss :  1.6391955614089966 4.123416900634766 5.762612342834473
Total LOSS train 4.370778571642362 valid 5.863977789878845
CE LOSS train 1.731057464159452 valid 0.40979889035224915
Contrastive LOSS train 2.639721101980943 valid 1.0308542251586914
EPOCH 161:
Loss :  1.732133388519287 2.3830037117004395 4.115137100219727
Loss :  1.7309378385543823 2.693378210067749 4.424315929412842
Loss :  1.7247849702835083 2.5560622215270996 4.280847072601318
Loss :  1.7295706272125244 2.7376527786254883 4.467223167419434
Loss :  1.733168125152588 2.7539613246917725 4.487129211425781
Loss :  1.7321592569351196 2.251525640487671 3.98368501663208
Loss :  1.7314658164978027 3.3112454414367676 5.04271125793457
Loss :  1.729783296585083 2.871474504470825 4.601257801055908
Loss :  1.7284257411956787 2.5221333503723145 4.250558853149414
Loss :  1.7162411212921143 2.7790751457214355 4.495316505432129
Loss :  1.730592966079712 2.827176332473755 4.557769298553467
Loss :  1.7419987916946411 2.3185667991638184 4.06056547164917
Loss :  1.7311385869979858 2.439135789871216 4.170274257659912
Loss :  1.7310473918914795 3.5807626247406006 5.31181001663208
Loss :  1.7352131605148315 2.3991518020629883 4.134365081787109
Loss :  1.724258542060852 2.616429328918457 4.3406877517700195
Loss :  1.7315400838851929 2.5422375202178955 4.273777484893799
Loss :  1.7277699708938599 1.9641644954681396 3.691934585571289
Loss :  1.7295984029769897 1.6799981594085693 3.4095964431762695
Loss :  1.7255377769470215 1.8918137550354004 3.617351531982422
  batch 20 loss: 1.7255377769470215, 1.8918137550354004, 3.617351531982422
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.733141541481018 2.2012040615081787 3.9343457221984863
Loss :  1.7337315082550049 1.8890923261642456 3.622823715209961
Loss :  1.7341482639312744 2.2469184398651123 3.9810667037963867
Loss :  1.7413851022720337 2.7068302631378174 4.448215484619141
Loss :  1.731148362159729 2.3503265380859375 4.081474781036377
Loss :  1.729655146598816 2.1942710876464844 3.92392635345459
Loss :  1.737413763999939 2.336578130722046 4.073991775512695
Loss :  1.729633092880249 2.089372158050537 3.819005250930786
Loss :  1.737431287765503 2.2005510330200195 3.9379823207855225
Loss :  1.7270746231079102 2.299919366836548 4.026993751525879
Loss :  1.7401320934295654 2.755204916000366 4.495337009429932
Loss :  1.7352720499038696 2.9107344150543213 4.6460065841674805
Loss :  1.7290791273117065 2.7468743324279785 4.475953578948975
Loss :  1.7254172563552856 2.5549840927124023 4.280401229858398
Loss :  1.7347146272659302 2.8089492321014404 4.54366397857666
Loss :  1.7357580661773682 3.4851014614105225 5.220859527587891
Loss :  1.7319459915161133 2.524651527404785 4.256597518920898
Loss :  1.73026442527771 2.4433753490448 4.17363977432251
Loss :  1.7314167022705078 1.6760008335113525 3.4074175357818604
Loss :  1.7305853366851807 1.8639845848083496 3.5945699214935303
  batch 40 loss: 1.7305853366851807, 1.8639845848083496, 3.5945699214935303
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7301255464553833 2.8123927116394043 4.542518138885498
Loss :  1.7298927307128906 2.584094762802124 4.313987731933594
Loss :  1.731120228767395 2.953803539276123 4.6849236488342285
Loss :  1.7308892011642456 3.014145851135254 4.745035171508789
Loss :  1.734134316444397 3.589705228805542 5.3238396644592285
Loss :  1.7296135425567627 2.388200044631958 4.117813587188721
Loss :  1.730367660522461 2.7576966285705566 4.488064289093018
Loss :  1.7307382822036743 2.7599775791168213 4.490715980529785
Loss :  1.733561396598816 2.4165523052215576 4.150113582611084
Loss :  1.728041172027588 2.29508900642395 4.023130416870117
Loss :  1.725225567817688 2.6712656021118164 4.396491050720215
Loss :  1.731371521949768 1.9786014556884766 3.709972858428955
Loss :  1.736564040184021 3.0211281776428223 4.757692337036133
Loss :  1.7272920608520508 2.32953143119812 4.05682373046875
Loss :  1.73014235496521 2.256040334701538 3.986182689666748
Loss :  1.7312417030334473 3.1615142822265625 4.89275598526001
Loss :  1.7328686714172363 1.941657304763794 3.6745259761810303
Loss :  1.7338793277740479 2.1037709712982178 3.8376502990722656
Loss :  1.7360156774520874 2.696591377258301 4.432607173919678
Loss :  1.7338532209396362 1.96263587474823 3.696489095687866
  batch 60 loss: 1.7338532209396362, 1.96263587474823, 3.696489095687866
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7322123050689697 2.345721960067749 4.077934265136719
Loss :  1.7339624166488647 2.2603867053985596 3.9943490028381348
Loss :  1.7355788946151733 2.135741710662842 3.8713207244873047
Loss :  1.7294211387634277 2.751713514328003 4.481134414672852
Loss :  1.7274712324142456 3.5135672092437744 5.2410383224487305
Loss :  1.6609092950820923 3.9968478679656982 5.65775728225708
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.665894865989685 4.080458641052246 5.746353626251221
Loss :  1.6653144359588623 3.9415624141693115 5.606876850128174
Loss :  1.6687783002853394 3.8195230960845947 5.4883012771606445
Total LOSS train 4.256118407616248 valid 5.62482225894928
CE LOSS train 1.7314199759409978 valid 0.41719457507133484
Contrastive LOSS train 2.5246984408451962 valid 0.9548807740211487
EPOCH 162:
Loss :  1.7327773571014404 2.674640417098999 4.4074177742004395
Loss :  1.7310712337493896 2.3921847343444824 4.123255729675293
Loss :  1.7263771295547485 2.189946174621582 3.916323184967041
Loss :  1.7306097745895386 2.1857872009277344 3.9163970947265625
Loss :  1.734287142753601 1.6203235387802124 3.3546106815338135
Loss :  1.7331658601760864 2.5275156497955322 4.260681629180908
Loss :  1.732704520225525 3.3523175716400146 5.08502197265625
Loss :  1.7302886247634888 2.2808127403259277 4.011101245880127
Loss :  1.7299842834472656 2.4456839561462402 4.175668239593506
Loss :  1.7180145978927612 2.0007593631744385 3.71877384185791
Loss :  1.7306420803070068 2.453134059906006 4.183775901794434
Loss :  1.7419112920761108 2.188011646270752 3.9299230575561523
Loss :  1.731454849243164 1.9657764434814453 3.6972312927246094
Loss :  1.7314589023590088 2.1527905464172363 3.884249448776245
Loss :  1.7352125644683838 1.8510891199111938 3.586301803588867
Loss :  1.7260364294052124 2.503842353820801 4.229878902435303
Loss :  1.7322251796722412 2.481604814529419 4.21382999420166
Loss :  1.7283008098602295 2.4344727993011475 4.162773609161377
Loss :  1.7305198907852173 2.7404978275299072 4.471017837524414
Loss :  1.7256947755813599 2.7863729000091553 4.512067794799805
  batch 20 loss: 1.7256947755813599, 2.7863729000091553, 4.512067794799805
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7323859930038452 2.215715169906616 3.948101043701172
Loss :  1.7332377433776855 2.4937238693237305 4.226961612701416
Loss :  1.7339625358581543 2.9154207706451416 4.649383544921875
Loss :  1.7393226623535156 2.7547781467437744 4.494100570678711
Loss :  1.7318059206008911 2.905885934829712 4.637691974639893
Loss :  1.729675054550171 3.237577199935913 4.967252254486084
Loss :  1.7362116575241089 2.88724684715271 4.623458385467529
Loss :  1.7278205156326294 1.962138056755066 3.6899585723876953
Loss :  1.7353647947311401 2.712489604949951 4.447854518890381
Loss :  1.727444052696228 2.6514439582824707 4.378888130187988
Loss :  1.7408915758132935 2.6703691482543945 4.411260604858398
Loss :  1.7335638999938965 2.1385281085968018 3.8720920085906982
Loss :  1.727662205696106 2.62009596824646 4.3477582931518555
Loss :  1.7256828546524048 2.4483964443206787 4.174079418182373
Loss :  1.7346822023391724 2.7011361122131348 4.435818195343018
Loss :  1.7355263233184814 2.298332691192627 4.0338592529296875
Loss :  1.7317581176757812 2.10307240486145 3.8348305225372314
Loss :  1.728278636932373 1.906379222869873 3.634657859802246
Loss :  1.7315729856491089 2.887429714202881 4.619002819061279
Loss :  1.7304496765136719 2.9102869033813477 4.6407365798950195
  batch 40 loss: 1.7304496765136719, 2.9102869033813477, 4.6407365798950195
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7295676469802856 2.268638849258423 3.998206615447998
Loss :  1.7299704551696777 2.015456438064575 3.745426893234253
Loss :  1.730460524559021 2.5453789234161377 4.275839328765869
Loss :  1.7306071519851685 2.522732973098755 4.253340244293213
Loss :  1.7335232496261597 2.1435706615448 3.87709379196167
Loss :  1.7282662391662598 2.471015214920044 4.199281692504883
Loss :  1.7290902137756348 2.3330438137054443 4.0621337890625
Loss :  1.7297638654708862 2.0193240642547607 3.7490878105163574
Loss :  1.731946349143982 2.1370203495025635 3.868966579437256
Loss :  1.72706937789917 2.290879249572754 4.017948627471924
Loss :  1.7248667478561401 2.23215389251709 3.9570207595825195
Loss :  1.7312989234924316 3.0059285163879395 4.737227439880371
Loss :  1.7362264394760132 2.6894125938415527 4.4256391525268555
Loss :  1.72677743434906 2.018641471862793 3.7454190254211426
Loss :  1.7298249006271362 2.260867118835449 3.990692138671875
Loss :  1.7323147058486938 2.093372344970703 3.8256869316101074
Loss :  1.7340312004089355 2.2676823139190674 4.001713752746582
Loss :  1.734190821647644 2.2160258293151855 3.950216770172119
Loss :  1.7368242740631104 2.4475045204162598 4.184329032897949
Loss :  1.7353137731552124 2.2259504795074463 3.961264133453369
  batch 60 loss: 1.7353137731552124, 2.2259504795074463, 3.961264133453369
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7333253622055054 2.5827488899230957 4.316074371337891
Loss :  1.7343145608901978 2.628558874130249 4.362873554229736
Loss :  1.7367444038391113 2.248882532119751 3.9856269359588623
Loss :  1.72990882396698 2.71663761138916 4.44654655456543
Loss :  1.7278896570205688 1.9922370910644531 3.7201266288757324
Loss :  1.6391804218292236 4.073498725891113 5.712678909301758
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6456440687179565 4.001887321472168 5.647531509399414
Loss :  1.646429181098938 4.067555904388428 5.713984966278076
Loss :  1.6493303775787354 3.708360433578491 5.357690811157227
Total LOSS train 4.147166611598088 valid 5.607971549034119
CE LOSS train 1.7314485201468834 valid 0.41233259439468384
Contrastive LOSS train 2.415718073111314 valid 0.9270901083946228
EPOCH 163:
Loss :  1.733719825744629 3.028242349624634 4.761961936950684
Loss :  1.731763243675232 3.6144769191741943 5.346240043640137
Loss :  1.726881504058838 2.358660936355591 4.085542678833008
Loss :  1.7308388948440552 2.7159411907196045 4.446780204772949
Loss :  1.734870433807373 2.199566125869751 3.934436559677124
Loss :  1.7327767610549927 1.818934679031372 3.5517115592956543
Loss :  1.7334126234054565 2.1273038387298584 3.8607163429260254
Loss :  1.730409860610962 2.4861767292022705 4.216586589813232
Loss :  1.7301075458526611 1.8949886560440063 3.625096321105957
Loss :  1.7185957431793213 2.3189685344696045 4.037564277648926
Loss :  1.7313004732131958 2.7601377964019775 4.491438388824463
Loss :  1.7416834831237793 2.7110466957092285 4.452730178833008
Loss :  1.7323744297027588 2.5531668663024902 4.285541534423828
Loss :  1.731603741645813 2.718506097793579 4.450109958648682
Loss :  1.7358629703521729 3.3588132858276367 5.0946760177612305
Loss :  1.7257719039916992 2.3947606086730957 4.120532512664795
Loss :  1.7329285144805908 3.240222692489624 4.973151206970215
Loss :  1.7293379306793213 3.1748645305633545 4.904202461242676
Loss :  1.7305797338485718 2.086139440536499 3.8167190551757812
Loss :  1.7267448902130127 1.9893755912780762 3.716120481491089
  batch 20 loss: 1.7267448902130127, 1.9893755912780762, 3.716120481491089
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7328885793685913 2.1852948665618896 3.9181833267211914
Loss :  1.7332819700241089 2.2902188301086426 4.023500919342041
Loss :  1.7341448068618774 3.252882719039917 4.987027645111084
Loss :  1.7398605346679688 3.1733200550079346 4.913180351257324
Loss :  1.7311688661575317 2.646709680557251 4.377878665924072
Loss :  1.729038119316101 2.233759880065918 3.9627981185913086
Loss :  1.7366260290145874 2.0319089889526367 3.7685351371765137
Loss :  1.7288134098052979 2.6749155521392822 4.40372896194458
Loss :  1.7365671396255493 2.9690558910369873 4.705623149871826
Loss :  1.728041172027588 3.230149984359741 4.95819091796875
Loss :  1.7406854629516602 3.1950347423553467 4.935720443725586
Loss :  1.734447717666626 3.6768765449523926 5.411324501037598
Loss :  1.7285164594650269 3.28525710105896 5.013773441314697
Loss :  1.7259398698806763 3.49078631401062 5.216726303100586
Loss :  1.7355632781982422 3.876293182373047 5.611856460571289
Loss :  1.7356771230697632 3.6731369495391846 5.408813953399658
Loss :  1.7315285205841064 2.784855604171753 4.516384124755859
Loss :  1.7280336618423462 1.9292340278625488 3.6572675704956055
Loss :  1.7325479984283447 3.000973701477051 4.733521461486816
Loss :  1.7308541536331177 3.043445587158203 4.774299621582031
  batch 40 loss: 1.7308541536331177, 3.043445587158203, 4.774299621582031
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7295881509780884 2.2875616550445557 4.017149925231934
Loss :  1.727429986000061 2.5680770874023438 4.295506954193115
Loss :  1.729513168334961 2.1360244750976562 3.865537643432617
Loss :  1.7282185554504395 1.9660701751708984 3.694288730621338
Loss :  1.7327643632888794 1.8727946281433105 3.6055588722229004
Loss :  1.7308424711227417 2.689466953277588 4.420309543609619
Loss :  1.7316992282867432 2.464087724685669 4.195786952972412
Loss :  1.732077717781067 2.4202497005462646 4.152327537536621
Loss :  1.7364404201507568 2.4405603408813477 4.177000999450684
Loss :  1.7299187183380127 2.522212028503418 4.252130508422852
Loss :  1.7286665439605713 2.7934014797210693 4.522068023681641
Loss :  1.7331764698028564 2.4987905025482178 4.231966972351074
Loss :  1.7385525703430176 3.0385966300964355 4.777149200439453
Loss :  1.729425311088562 2.8092641830444336 4.538689613342285
Loss :  1.7313674688339233 2.3342156410217285 4.065583229064941
Loss :  1.732968807220459 2.3743045330047607 4.107273101806641
Loss :  1.7354562282562256 2.074815511703491 3.810271739959717
Loss :  1.7357401847839355 1.83762526512146 3.5733654499053955
Loss :  1.7376998662948608 2.4962844848632812 4.233984470367432
Loss :  1.7363393306732178 1.890107274055481 3.6264467239379883
  batch 60 loss: 1.7363393306732178, 1.890107274055481, 3.6264467239379883
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.733637809753418 2.1978378295898438 3.9314756393432617
Loss :  1.7344775199890137 2.9361793994903564 4.670657157897949
Loss :  1.7353618144989014 3.7097833156585693 5.445145130157471
Loss :  1.7297651767730713 2.3644869327545166 4.094252109527588
Loss :  1.7268589735031128 2.965094566345215 4.691953659057617
Loss :  1.6008094549179077 3.9838385581970215 5.584648132324219
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6062183380126953 3.956580877304077 5.562799453735352
Loss :  1.6114298105239868 3.880765438079834 5.492195129394531
Loss :  1.630110740661621 4.002094268798828 5.632205009460449
Total LOSS train 4.37640112730173 valid 5.567961931228638
CE LOSS train 1.7320580959320069 valid 0.4075276851654053
Contrastive LOSS train 2.6443430166978104 valid 1.000523567199707
EPOCH 164:
Loss :  1.7324014902114868 1.7174681425094604 3.4498696327209473
Loss :  1.7311331033706665 2.315617799758911 4.046751022338867
Loss :  1.7263665199279785 1.916688323020935 3.643054962158203
Loss :  1.7304102182388306 2.064225673675537 3.794635772705078
Loss :  1.734757900238037 1.8799840211868286 3.614741802215576
Loss :  1.7329816818237305 1.9257839918136597 3.6587657928466797
Loss :  1.7339400053024292 2.068589210510254 3.8025293350219727
Loss :  1.731075406074524 2.5795936584472656 4.3106689453125
Loss :  1.7308595180511475 1.847108006477356 3.577967643737793
Loss :  1.719435214996338 2.2694664001464844 3.9889016151428223
Loss :  1.731860637664795 3.0600497722625732 4.791910171508789
Loss :  1.7413102388381958 2.4792115688323975 4.220521926879883
Loss :  1.7332186698913574 2.104644298553467 3.837862968444824
Loss :  1.7326310873031616 2.9930832386016846 4.725714206695557
Loss :  1.7372647523880005 2.4141769409179688 4.15144157409668
Loss :  1.726690411567688 2.0765974521636963 3.803287982940674
Loss :  1.73395574092865 2.5936014652252197 4.32755708694458
Loss :  1.7306081056594849 1.7816054821014404 3.512213706970215
Loss :  1.7312458753585815 1.5943677425384521 3.325613498687744
Loss :  1.7275726795196533 2.4482460021972656 4.17581844329834
  batch 20 loss: 1.7275726795196533, 2.4482460021972656, 4.17581844329834
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7343547344207764 2.7817060947418213 4.516060829162598
Loss :  1.734200358390808 2.113288640975952 3.8474888801574707
Loss :  1.735347867012024 2.166632890701294 3.9019808769226074
Loss :  1.7403770685195923 2.495051860809326 4.235428810119629
Loss :  1.7317767143249512 2.2586863040924072 3.9904630184173584
Loss :  1.7301924228668213 2.0729377269744873 3.8031301498413086
Loss :  1.7375636100769043 2.54556941986084 4.283133029937744
Loss :  1.729553461074829 2.443370819091797 4.172924041748047
Loss :  1.7371941804885864 2.34768009185791 4.084874153137207
Loss :  1.727207899093628 2.4393961429595947 4.166604042053223
Loss :  1.740549921989441 2.0442075729370117 3.784757614135742
Loss :  1.7349523305892944 1.864814281463623 3.599766731262207
Loss :  1.72916579246521 1.9874786138534546 3.716644287109375
Loss :  1.725764274597168 2.758190393447876 4.483954429626465
Loss :  1.7353099584579468 2.589482545852661 4.324792385101318
Loss :  1.7358201742172241 2.8017172813415527 4.537537574768066
Loss :  1.732622742652893 2.117959499359131 3.8505821228027344
Loss :  1.731249451637268 1.7883275747299194 3.5195770263671875
Loss :  1.7322925329208374 1.7445024251937866 3.476794958114624
Loss :  1.731779932975769 2.062119722366333 3.7938995361328125
  batch 40 loss: 1.731779932975769, 2.062119722366333, 3.7938995361328125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7312132120132446 2.1606454849243164 3.8918585777282715
Loss :  1.729391098022461 1.9391688108444214 3.668560028076172
Loss :  1.7300255298614502 2.2070629596710205 3.9370884895324707
Loss :  1.7296650409698486 2.1563191413879395 3.885984182357788
Loss :  1.7327443361282349 1.7985363006591797 3.531280517578125
Loss :  1.7299777269363403 2.006237268447876 3.736215114593506
Loss :  1.7324389219284058 1.7357685565948486 3.468207359313965
Loss :  1.728835105895996 2.077517032623291 3.806352138519287
Loss :  1.7357605695724487 2.6397335529327393 4.375494003295898
Loss :  1.726334810256958 1.873945951461792 3.60028076171875
Loss :  1.7254396677017212 2.3644022941589355 4.089841842651367
Loss :  1.7316312789916992 1.8009036779403687 3.5325350761413574
Loss :  1.7381333112716675 1.8498244285583496 3.5879578590393066
Loss :  1.7266061305999756 1.9572296142578125 3.683835744857788
Loss :  1.7310616970062256 1.8976632356643677 3.628725051879883
Loss :  1.7336965799331665 2.23651123046875 3.970207691192627
Loss :  1.7361465692520142 3.3226773738861084 5.058824062347412
Loss :  1.7346502542495728 2.4786996841430664 4.21334981918335
Loss :  1.738170862197876 2.737170696258545 4.475341796875
Loss :  1.7378876209259033 2.8532822132110596 4.591169834136963
  batch 60 loss: 1.7378876209259033, 2.8532822132110596, 4.591169834136963
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7357418537139893 2.462653636932373 4.198395729064941
Loss :  1.7372632026672363 1.9703388214111328 3.707602024078369
Loss :  1.7376532554626465 1.855427622795105 3.593080997467041
Loss :  1.7315102815628052 2.482257604598999 4.213768005371094
Loss :  1.7291570901870728 1.8840287923812866 3.6131858825683594
Loss :  1.6135228872299194 4.140397071838379 5.753920078277588
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.6231759786605835 4.276637554168701 5.899813652038574
Loss :  1.6233689785003662 4.098033428192139 5.721402168273926
Loss :  1.6339499950408936 3.960927724838257 5.59487771987915
Total LOSS train 3.9524517719562238 valid 5.74250340461731
CE LOSS train 1.7324327798990102 valid 0.4084874987602234
Contrastive LOSS train 2.2200190012271586 valid 0.9902319312095642
EPOCH 165:
Loss :  1.7343246936798096 2.327488660812378 4.0618133544921875
Loss :  1.7328431606292725 2.4644391536712646 4.197282314300537
Loss :  1.728742003440857 1.901554822921753 3.6302967071533203
Loss :  1.7335020303726196 2.673729419708252 4.407231330871582
Loss :  1.7363896369934082 2.89802622795105 4.634415626525879
Loss :  1.7358920574188232 2.384946584701538 4.120838642120361
Loss :  1.7364537715911865 2.0669455528259277 3.8033993244171143
Loss :  1.733682632446289 1.8782247304916382 3.611907482147217
Loss :  1.7331264019012451 1.9985718727111816 3.7316982746124268
Loss :  1.7213817834854126 2.0860888957977295 3.8074707984924316
Loss :  1.7327252626419067 3.1777474880218506 4.910472869873047
Loss :  1.742484450340271 2.7640955448150635 4.506579875946045
Loss :  1.7322694063186646 3.7214725017547607 5.453742027282715
Loss :  1.7310549020767212 3.949404716491699 5.680459499359131
Loss :  1.7363817691802979 3.231715679168701 4.968097686767578
Loss :  1.7243329286575317 3.38551926612854 5.109852313995361
Loss :  1.7310863733291626 3.8837807178497314 5.614867210388184
Loss :  1.7267626523971558 3.6895811557769775 5.416343688964844
Loss :  1.7354962825775146 4.357169151306152 6.092665672302246
Loss :  1.7275649309158325 3.53100323677063 5.258568286895752
  batch 20 loss: 1.7275649309158325, 3.53100323677063, 5.258568286895752
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7293014526367188 4.171295642852783 5.900597095489502
Loss :  1.7335282564163208 4.423113822937012 6.156641960144043
Loss :  1.7401610612869263 4.294655799865723 6.034816741943359
Loss :  1.747794508934021 4.069096565246582 5.816891193389893
Loss :  1.7368619441986084 4.01754903793335 5.754410743713379
Loss :  1.729982852935791 3.8464157581329346 5.576398849487305
Loss :  1.7330800294876099 3.655857801437378 5.388937950134277
Loss :  1.7270275354385376 3.2916886806488037 5.018716335296631
Loss :  1.7350974082946777 2.772622585296631 4.507719993591309
Loss :  1.732846975326538 2.988060474395752 4.720907211303711
Loss :  1.7464544773101807 2.9799351692199707 4.7263898849487305
Loss :  1.7335638999938965 2.9991276264190674 4.732691764831543
Loss :  1.7276235818862915 2.557673692703247 4.285297393798828
Loss :  1.7311291694641113 2.612072229385376 4.343201637268066
Loss :  1.7374658584594727 2.8501057624816895 4.587571620941162
Loss :  1.738761067390442 2.9610610008239746 4.699821949005127
Loss :  1.7333884239196777 2.808217763900757 4.5416059494018555
Loss :  1.7253689765930176 2.6722781658172607 4.397646903991699
Loss :  1.7327913045883179 2.710520029067993 4.4433112144470215
Loss :  1.7339104413986206 2.5168979167938232 4.250808238983154
  batch 40 loss: 1.7339104413986206, 2.5168979167938232, 4.250808238983154
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7319992780685425 2.7370810508728027 4.469080448150635
Loss :  1.7374284267425537 2.2414441108703613 3.978872537612915
Loss :  1.7385538816452026 2.371129274368286 4.109683036804199
Loss :  1.7386023998260498 2.827369451522827 4.565971851348877
Loss :  1.7399637699127197 2.523277521133423 4.263241291046143
Loss :  1.736506462097168 3.1541521549224854 4.890658378601074
Loss :  1.735859990119934 3.938446283340454 5.674306392669678
Loss :  1.7396128177642822 3.2314746379852295 4.971087455749512
Loss :  1.7412793636322021 3.4810736179351807 5.222352981567383
Loss :  1.7408623695373535 2.93397855758667 4.674840927124023
Loss :  1.7358121871948242 3.3175439834594727 5.053356170654297
Loss :  1.739530086517334 2.9232242107391357 4.662754058837891
Loss :  1.743485927581787 3.0040667057037354 4.747552871704102
Loss :  1.7398484945297241 2.443026542663574 4.182875156402588
Loss :  1.73955237865448 2.824516773223877 4.5640692710876465
Loss :  1.7376457452774048 2.7731428146362305 4.510788440704346
Loss :  1.7404987812042236 2.9905924797058105 4.731091499328613
Loss :  1.7426137924194336 2.612409830093384 4.355023384094238
Loss :  1.7432126998901367 2.423556327819824 4.166769027709961
Loss :  1.7379918098449707 2.2330758571624756 3.9710676670074463
  batch 60 loss: 1.7379918098449707, 2.2330758571624756, 3.9710676670074463
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7382794618606567 2.13678240776062 3.8750619888305664
Loss :  1.7392171621322632 2.393914222717285 4.133131504058838
Loss :  1.7367680072784424 2.010432720184326 3.7472007274627686
Loss :  1.735245943069458 2.1049563884735107 3.8402023315429688
Loss :  1.7338745594024658 1.834959626197815 3.5688343048095703
Loss :  1.699321985244751 3.9274866580963135 5.6268086433410645
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7107255458831787 4.096712589263916 5.807437896728516
Loss :  1.702038288116455 3.812370777130127 5.514409065246582
Loss :  1.717516541481018 3.7746400833129883 5.492156505584717
Total LOSS train 4.6743424819065975 valid 5.61020302772522
CE LOSS train 1.7353366485008828 valid 0.4293791353702545
Contrastive LOSS train 2.9390058224017803 valid 0.9436600208282471
EPOCH 166:
Loss :  1.7369179725646973 2.2989354133605957 4.035853385925293
Loss :  1.7333590984344482 2.357051372528076 4.090410232543945
Loss :  1.7317298650741577 2.181694746017456 3.913424491882324
Loss :  1.7360061407089233 1.8667086362838745 3.602714776992798
Loss :  1.7368844747543335 2.363713264465332 4.100597858428955
Loss :  1.7349869012832642 2.2699689865112305 4.004955768585205
Loss :  1.734291911125183 2.780877113342285 4.515169143676758
Loss :  1.7315943241119385 2.492278814315796 4.223873138427734
Loss :  1.7334847450256348 2.3504297733306885 4.083914756774902
Loss :  1.7225556373596191 2.1557774543762207 3.87833309173584
Loss :  1.7344821691513062 2.670384168624878 4.4048662185668945
Loss :  1.7442009449005127 2.2816872596740723 4.025888442993164
Loss :  1.7364404201507568 2.14225697517395 3.878697395324707
Loss :  1.7333147525787354 2.499884843826294 4.233199596405029
Loss :  1.737377405166626 2.886523723602295 4.6239013671875
Loss :  1.73263418674469 2.263645887374878 3.9962801933288574
Loss :  1.7372881174087524 2.2409448623657227 3.9782328605651855
Loss :  1.7355841398239136 1.9660871028900146 3.7016711235046387
Loss :  1.737884283065796 1.9987396001815796 3.736623764038086
Loss :  1.729059100151062 2.6875202655792236 4.416579246520996
  batch 20 loss: 1.729059100151062, 2.6875202655792236, 4.416579246520996
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7354352474212646 2.286682605743408 4.022117614746094
Loss :  1.736894130706787 2.22220778465271 3.959101915359497
Loss :  1.736908197402954 2.3534317016601562 4.090339660644531
Loss :  1.742515206336975 2.2974205017089844 4.03993558883667
Loss :  1.7366160154342651 2.7307181358337402 4.467334270477295
Loss :  1.731856346130371 2.304809808731079 4.036665916442871
Loss :  1.739809274673462 2.434809923171997 4.174619197845459
Loss :  1.7301892042160034 1.8872257471084595 3.617414951324463
Loss :  1.7374722957611084 2.4614789485931396 4.198951244354248
Loss :  1.7311242818832397 2.5411975383758545 4.272321701049805
Loss :  1.7443106174468994 2.4284932613372803 4.17280387878418
Loss :  1.738190770149231 2.5968048572540283 4.334995746612549
Loss :  1.7320479154586792 2.3593661785125732 4.091413974761963
Loss :  1.7321441173553467 2.163611888885498 3.8957560062408447
Loss :  1.739312767982483 2.711049795150757 4.450362682342529
Loss :  1.739785075187683 2.33982253074646 4.0796074867248535
Loss :  1.735132098197937 2.510450839996338 4.2455830574035645
Loss :  1.7317533493041992 2.33347225189209 4.065225601196289
Loss :  1.7342926263809204 2.8158674240112305 4.550159931182861
Loss :  1.7351090908050537 2.701702356338501 4.436811447143555
  batch 40 loss: 1.7351090908050537, 2.701702356338501, 4.436811447143555
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7328208684921265 2.8755273818969727 4.608348369598389
Loss :  1.7359204292297363 2.2930819988250732 4.0290021896362305
Loss :  1.7382148504257202 2.206550121307373 3.944765090942383
Loss :  1.738274097442627 2.458221673965454 4.19649600982666
Loss :  1.739696979522705 2.15979266166687 3.899489641189575
Loss :  1.7342913150787354 2.2570338249206543 3.9913251399993896
Loss :  1.733223795890808 2.2042949199676514 3.93751859664917
Loss :  1.7382004261016846 1.8498634099960327 3.5880637168884277
Loss :  1.735926866531372 2.356220245361328 4.092146873474121
Loss :  1.7374683618545532 2.0739800930023193 3.811448574066162
Loss :  1.7322075366973877 2.740781307220459 4.472989082336426
Loss :  1.7353006601333618 2.0128445625305176 3.74814510345459
Loss :  1.7392865419387817 2.5463345050811768 4.285621166229248
Loss :  1.734765887260437 2.471087694168091 4.205853462219238
Loss :  1.7360928058624268 2.331331491470337 4.067424297332764
Loss :  1.7324750423431396 2.054436445236206 3.7869114875793457
Loss :  1.7357516288757324 2.45296311378479 4.188714981079102
Loss :  1.7392789125442505 2.719444990158081 4.458724021911621
Loss :  1.7401785850524902 2.5628397464752197 4.303018569946289
Loss :  1.735697627067566 2.47200608253479 4.207703590393066
  batch 60 loss: 1.735697627067566, 2.47200608253479, 4.207703590393066
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.735951542854309 2.01910662651062 3.7550582885742188
Loss :  1.738215684890747 2.059067964553833 3.79728364944458
Loss :  1.7367143630981445 2.5874714851379395 4.324185848236084
Loss :  1.7343859672546387 2.6169943809509277 4.351380348205566
Loss :  1.7339979410171509 1.661703109741211 3.3957009315490723
Loss :  1.7134045362472534 3.8467023372650146 5.5601067543029785
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.7220085859298706 3.9669694900512695 5.68897819519043
Loss :  1.7146028280258179 3.7458302974700928 5.460433006286621
Loss :  1.7337143421173096 3.7510600090026855 5.484774589538574
Total LOSS train 4.093754273194533 valid 5.548573136329651
CE LOSS train 1.7356202143889208 valid 0.4334285855293274
Contrastive LOSS train 2.3581340643075794 valid 0.9377650022506714
EPOCH 167:
Loss :  1.73835027217865 2.0494587421417236 3.787808895111084
Loss :  1.7351142168045044 2.3240811824798584 4.059195518493652
Loss :  1.7323992252349854 2.5060338973999023 4.238432884216309
Loss :  1.7361226081848145 2.141195058822632 3.8773176670074463
Loss :  1.7377537488937378 2.028231143951416 3.7659850120544434
Loss :  1.7365655899047852 1.9162060022354126 3.652771472930908
Loss :  1.73600172996521 2.243079900741577 3.979081630706787
Loss :  1.7335212230682373 1.8591123819351196 3.5926337242126465
Loss :  1.7353618144989014 2.422316074371338 4.15767765045166
Loss :  1.7255288362503052 2.3721418380737305 4.097670555114746
Loss :  1.7369141578674316 2.597254514694214 4.334168434143066
Loss :  1.7445433139801025 2.342564105987549 4.0871076583862305
Loss :  1.737532615661621 2.685408592224121 4.422941207885742
Loss :  1.7339783906936646 2.3394522666931152 4.07343053817749
Loss :  1.7375348806381226 2.0207717418670654 3.7583065032958984
Loss :  1.7323439121246338 1.9750442504882812 3.707388162612915
Loss :  1.7361207008361816 2.013099193572998 3.7492198944091797
Loss :  1.733487844467163 1.909692645072937 3.6431803703308105
Loss :  1.7362905740737915 1.6541696786880493 3.390460252761841
Loss :  1.7279971837997437 1.8973219394683838 3.625319004058838
  batch 20 loss: 1.7279971837997437, 1.8973219394683838, 3.625319004058838
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.734505295753479 1.7807611227035522 3.5152664184570312
Loss :  1.7355568408966064 2.276475429534912 4.012032508850098
Loss :  1.7352418899536133 1.992452621459961 3.727694511413574
Loss :  1.7402093410491943 2.1754720211029053 3.9156813621520996
Loss :  1.735624074935913 2.553054094314575 4.288678169250488
Loss :  1.7308447360992432 2.352893590927124 4.083738327026367
Loss :  1.7387235164642334 2.5682051181793213 4.306928634643555
Loss :  1.729089379310608 1.929512619972229 3.658601999282837
Loss :  1.7372517585754395 2.361635446548462 4.0988874435424805
Loss :  1.7304935455322266 2.278674840927124 4.00916862487793
Loss :  1.7455668449401855 2.6575937271118164 4.403160572052002
Loss :  1.7373714447021484 2.503505229949951 4.2408766746521
Loss :  1.7314496040344238 2.27658748626709 4.008037090301514
Loss :  1.731062889099121 2.76041579246521 4.49147891998291
Loss :  1.7398141622543335 3.0085346698760986 4.748348712921143
Loss :  1.7399042844772339 2.746396064758301 4.486300468444824
Loss :  1.7350168228149414 2.7449867725372314 4.480003356933594
Loss :  1.731123685836792 1.7562472820281982 3.4873709678649902
Loss :  1.734075665473938 2.4385271072387695 4.172602653503418
Loss :  1.734345555305481 2.583984136581421 4.318329811096191
  batch 40 loss: 1.734345555305481, 2.583984136581421, 4.318329811096191
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7321529388427734 2.866281032562256 4.598433971405029
Loss :  1.734533429145813 1.7135149240493774 3.4480483531951904
Loss :  1.7371933460235596 1.8621424436569214 3.5993356704711914
Loss :  1.7357282638549805 2.634131669998169 4.36985969543457
Loss :  1.7376915216445923 2.23726487159729 3.974956512451172
Loss :  1.7323360443115234 2.253484010696411 3.9858200550079346
Loss :  1.7303836345672607 2.4652159214019775 4.195599555969238
Loss :  1.7362083196640015 2.343447685241699 4.07965612411499
Loss :  1.7332029342651367 2.8615293502807617 4.594732284545898
Loss :  1.7347180843353271 2.762877941131592 4.49759578704834
Loss :  1.7307859659194946 3.0512850284576416 4.782071113586426
Loss :  1.7328675985336304 2.3850724697113037 4.1179399490356445
Loss :  1.7374968528747559 2.4892287254333496 4.2267255783081055
Loss :  1.7334821224212646 2.3220772743225098 4.055559158325195
Loss :  1.736191987991333 2.075424909591675 3.811616897583008
Loss :  1.7307652235031128 2.5753681659698486 4.306133270263672
Loss :  1.7353813648223877 2.764859676361084 4.500241279602051
Loss :  1.7396175861358643 2.7743613719940186 4.513978958129883
Loss :  1.7400988340377808 2.6507885456085205 4.390887260437012
Loss :  1.7343130111694336 2.2709767818450928 4.0052900314331055
  batch 60 loss: 1.7343130111694336, 2.2709767818450928, 4.0052900314331055
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.735397219657898 3.7818405628204346 5.517237663269043
Loss :  1.7381653785705566 3.529796600341797 5.2679619789123535
Loss :  1.7358391284942627 4.065781593322754 5.8016204833984375
Loss :  1.7340532541275024 2.9349961280822754 4.669049263000488
Loss :  1.7342188358306885 2.098724842071533 3.8329436779022217
Loss :  1.7215970754623413 3.907743215560913 5.629340171813965
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
Loss :  1.7310107946395874 4.100363254547119 5.831374168395996
Loss :  1.7217775583267212 3.8536603450775146 5.575438022613525
Loss :  1.7363190650939941 3.9797277450561523 5.7160468101501465
Total LOSS train 4.147639674406785 valid 5.688049793243408
CE LOSS train 1.7351316470366258 valid 0.43407976627349854
Contrastive LOSS train 2.412508043876061 valid 0.9949319362640381
EPOCH 168:
Loss :  1.7379523515701294 2.6070077419281006 4.3449602127075195
Loss :  1.7350168228149414 3.557295322418213 5.292312145233154
Loss :  1.7328742742538452 2.2268331050872803 3.959707260131836
Loss :  1.7374026775360107 2.451184034347534 4.188586711883545
Loss :  1.7385748624801636 2.396183490753174 4.134758472442627
Loss :  1.7380255460739136 2.8531556129455566 4.59118127822876
Loss :  1.7368464469909668 2.7124412059783936 4.449287414550781
Loss :  1.7343894243240356 2.305462121963501 4.039851665496826
Loss :  1.7355870008468628 2.712207078933716 4.447793960571289
Loss :  1.7252672910690308 2.5927605628967285 4.318027973175049
Loss :  1.7371656894683838 2.8989291191101074 4.63609504699707
Loss :  1.7456332445144653 2.909193992614746 4.654827117919922
Loss :  1.7383302450180054 2.803452968597412 4.541783332824707
Loss :  1.733870506286621 2.7388641834259033 4.472734451293945
Loss :  1.737101435661316 2.524458885192871 4.261560440063477
Loss :  1.7325317859649658 2.269831895828247 4.002363681793213
Loss :  1.7368580102920532 3.3355915546417236 5.072449684143066
Loss :  1.7350163459777832 2.8333451747894287 4.568361282348633
Loss :  1.7369376420974731 2.4870147705078125 4.223952293395996
Loss :  1.7289347648620605 2.6979265213012695 4.42686128616333
  batch 20 loss: 1.7289347648620605, 2.6979265213012695, 4.42686128616333
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.733947992324829 2.412457227706909 4.146405220031738
Loss :  1.734893798828125 2.774766445159912 4.509660243988037
Loss :  1.7355287075042725 3.060528516769409 4.796057224273682
Loss :  1.7392678260803223 2.685023546218872 4.424291610717773
Loss :  1.7347581386566162 2.690768003463745 4.425526142120361
Loss :  1.7303946018218994 2.0594642162323 3.789858818054199
Loss :  1.737768530845642 2.1761927604675293 3.913961410522461
Loss :  1.7286620140075684 2.147397518157959 3.8760595321655273
Loss :  1.7366760969161987 2.5297882556915283 4.2664642333984375
Loss :  1.7316553592681885 2.515329599380493 4.246984958648682
Loss :  1.7454153299331665 2.82173752784729 4.567152976989746
Loss :  1.7393471002578735 2.403291940689087 4.14263916015625
Loss :  1.7344417572021484 2.351001501083374 4.085443496704102
Loss :  1.7337408065795898 2.399500846862793 4.133241653442383
Loss :  1.7416008710861206 2.681258201599121 4.422859191894531
Loss :  1.7412109375 2.3960978984832764 4.1373090744018555
Loss :  1.7365683317184448 2.2106757164001465 3.947244167327881
Loss :  1.732951045036316 2.17576003074646 3.9087109565734863
Loss :  1.7355177402496338 2.529879331588745 4.265397071838379
Loss :  1.7359594106674194 3.21962571144104 4.95558500289917
  batch 40 loss: 1.7359594106674194, 3.21962571144104, 4.95558500289917
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.734068512916565 2.432722806930542 4.1667914390563965
Loss :  1.7370765209197998 2.0966172218322754 3.833693742752075
Loss :  1.7382830381393433 2.9132580757141113 4.651541233062744
Loss :  1.7375766038894653 2.4936747550964355 4.231251239776611
Loss :  1.739148497581482 2.3232977390289307 4.062446117401123
Loss :  1.734476923942566 2.568202495574951 4.302679538726807
Loss :  1.7329678535461426 2.4748246669769287 4.207792282104492
Loss :  1.7377012968063354 2.429490566253662 4.167191982269287
Loss :  1.737000584602356 2.4933717250823975 4.230372428894043
Loss :  1.73759126663208 2.294074058532715 4.031665325164795
Loss :  1.7328307628631592 2.673802137374878 4.406632900238037
Loss :  1.735619068145752 2.449676513671875 4.185295581817627
Loss :  1.7394077777862549 2.3911397457122803 4.130547523498535
Loss :  1.7353565692901611 2.0694072246551514 3.8047637939453125
Loss :  1.7378250360488892 2.1625237464904785 3.900348663330078
Loss :  1.7327553033828735 2.284911632537842 4.017666816711426
Loss :  1.7364509105682373 3.4233527183532715 5.15980339050293
Loss :  1.7405275106430054 3.63810133934021 5.378628730773926
Loss :  1.7408093214035034 3.6559653282165527 5.396774768829346
Loss :  1.7367502450942993 2.782343864440918 4.519093990325928
  batch 60 loss: 1.7367502450942993, 2.782343864440918, 4.519093990325928
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7375599145889282 2.732347249984741 4.469907283782959
Loss :  1.7398988008499146 2.44096302986145 4.180861949920654
Loss :  1.738741397857666 2.4998559951782227 4.238597393035889
Loss :  1.7357943058013916 2.516667366027832 4.2524614334106445
Loss :  1.7356098890304565 2.0792434215545654 3.8148531913757324
Loss :  1.7178168296813965 3.8483242988586426 5.566141128540039
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
Loss :  1.725124478340149 3.9850218296051025 5.710146427154541
Loss :  1.717779278755188 3.7558043003082275 5.473583698272705
Loss :  1.7289564609527588 3.8925249576568604 5.621481418609619
Total LOSS train 4.3281533938187815 valid 5.592838168144226
CE LOSS train 1.7362530873372004 valid 0.4322391152381897
Contrastive LOSS train 2.5919003009796144 valid 0.9731312394142151
EPOCH 169:
Loss :  1.739368200302124 2.7046446800231934 4.444012641906738
Loss :  1.7360894680023193 2.7225394248962402 4.4586286544799805
Loss :  1.7323542833328247 2.652578115463257 4.384932518005371
Loss :  1.7363132238388062 2.156994581222534 3.893307685852051
Loss :  1.7366870641708374 2.0328450202941895 3.7695322036743164
Loss :  1.7363089323043823 1.91853666305542 3.654845714569092
Loss :  1.7348045110702515 2.064028739929199 3.7988333702087402
Loss :  1.7324824333190918 1.8116384744644165 3.5441207885742188
Loss :  1.7341341972351074 1.911426305770874 3.6455605030059814
Loss :  1.7228354215621948 1.7480028867721558 3.4708383083343506
Loss :  1.7350308895111084 1.9301928281784058 3.6652235984802246
Loss :  1.74460768699646 2.1938812732696533 3.9384889602661133
Loss :  1.735797643661499 2.0831100940704346 3.8189077377319336
Loss :  1.7334245443344116 2.00868558883667 3.742110252380371
Loss :  1.7399122714996338 2.0300886631011963 3.77000093460083
Loss :  1.7320107221603394 2.4234349727630615 4.155445575714111
Loss :  1.7379511594772339 3.0088517665863037 4.746802806854248
Loss :  1.734797716140747 2.627091646194458 4.361889362335205
Loss :  1.7378110885620117 2.6262106895446777 4.3640217781066895
Loss :  1.7324987649917603 2.4935595989227295 4.226058483123779
  batch 20 loss: 1.7324987649917603, 2.4935595989227295, 4.226058483123779
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.737351655960083 2.6249613761901855 4.362313270568848
Loss :  1.738111972808838 2.4788448810577393 4.216957092285156
Loss :  1.7391175031661987 2.3717000484466553 4.1108174324035645
Loss :  1.742700219154358 2.4977545738220215 4.24045467376709
Loss :  1.7380002737045288 2.372802495956421 4.11080265045166
Loss :  1.7333546876907349 2.634899854660034 4.368254661560059
Loss :  1.7397860288619995 2.336674451828003 4.076460361480713
Loss :  1.7299132347106934 1.9983354806900024 3.7282485961914062
Loss :  1.7379165887832642 1.961427092552185 3.699343681335449
Loss :  1.7319797277450562 2.7478597164154053 4.479839324951172
Loss :  1.7446602582931519 3.463716745376587 5.208376884460449
Loss :  1.7374926805496216 2.723231077194214 4.460723876953125
Loss :  1.7315586805343628 2.8054590225219727 4.537017822265625
Loss :  1.731143832206726 2.2929940223693848 4.0241379737854
Loss :  1.7391053438186646 2.5847880840301514 4.3238935470581055
Loss :  1.7387510538101196 2.9324662685394287 4.671217441558838
Loss :  1.7342915534973145 2.571418523788452 4.3057098388671875
Loss :  1.7294726371765137 2.6204452514648438 4.349917888641357
Loss :  1.733429193496704 2.374195098876953 4.107624053955078
Loss :  1.7325559854507446 2.595374584197998 4.327930450439453
  batch 40 loss: 1.7325559854507446, 2.595374584197998, 4.327930450439453
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7305638790130615 2.2025563716888428 3.9331202507019043
Loss :  1.7334507703781128 1.5900981426239014 3.3235487937927246
Loss :  1.7350279092788696 1.8154804706573486 3.550508499145508
Loss :  1.7341036796569824 1.6982120275497437 3.4323158264160156
Loss :  1.7364158630371094 1.5815011262893677 3.3179168701171875
Loss :  1.729781150817871 1.9535045623779297 3.683285713195801
Loss :  1.7288461923599243 2.0673627853393555 3.7962088584899902
Loss :  1.7345397472381592 2.8499808311462402 4.58452033996582
Loss :  1.7313780784606934 2.0876874923706055 3.819065570831299
Loss :  1.7338688373565674 2.226081371307373 3.9599502086639404
Loss :  1.7287790775299072 2.942133903503418 4.670912742614746
Loss :  1.7315399646759033 2.40256667137146 4.134106636047363
Loss :  1.7364013195037842 1.7156486511230469 3.452049970626831
Loss :  1.7318899631500244 2.1570587158203125 3.888948678970337
Loss :  1.7348051071166992 2.2763125896453857 4.011117935180664
Loss :  1.7287721633911133 1.97687828540802 3.7056503295898438
Loss :  1.7341804504394531 2.399400472640991 4.133581161499023
Loss :  1.7371524572372437 2.2923619747161865 4.029514312744141
Loss :  1.7383825778961182 3.1020655632019043 4.840448379516602
Loss :  1.7346702814102173 1.9798033237457275 3.7144737243652344
  batch 60 loss: 1.7346702814102173, 1.9798033237457275, 3.7144737243652344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7349134683609009 4.637781620025635 6.372694969177246
Loss :  1.7362679243087769 3.580533742904663 5.31680154800415
Loss :  1.7353957891464233 3.193448543548584 4.928844451904297
Loss :  1.7320255041122437 4.5692033767700195 6.301229000091553
Loss :  1.7304508686065674 3.7995920181274414 5.53004264831543
Loss :  1.7659562826156616 3.9562580585479736 5.722214221954346
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.768875241279602 4.086581230163574 5.855456352233887
Loss :  1.7639355659484863 3.8290109634399414 5.592946529388428
Loss :  1.7741363048553467 3.7625374794006348 5.536673545837402
Total LOSS train 4.184530166479258 valid 5.676822662353516
CE LOSS train 1.7347617901288546 valid 0.44353407621383667
Contrastive LOSS train 2.449768389188326 valid 0.9406343698501587
EPOCH 170:
Loss :  1.7327039241790771 3.6233885288238525 5.35609245300293
Loss :  1.7322458028793335 4.585129737854004 6.317375659942627
Loss :  1.7301028966903687 3.8096015453338623 5.539704322814941
Loss :  1.729996681213379 4.133872032165527 5.863868713378906
Loss :  1.7363789081573486 3.5873594284057617 5.323738098144531
Loss :  1.734466791152954 4.186232566833496 5.920699119567871
Loss :  1.7329415082931519 4.264760971069336 5.997702598571777
Loss :  1.7321290969848633 3.114833116531372 4.846961975097656
Loss :  1.7331559658050537 3.9279327392578125 5.661088943481445
Loss :  1.7207363843917847 3.924135446548462 5.644871711730957
Loss :  1.7334288358688354 3.7647602558135986 5.4981889724731445
Loss :  1.744537115097046 3.9969825744628906 5.741519927978516
Loss :  1.7366570234298706 3.9862112998962402 5.7228684425354
Loss :  1.7345693111419678 3.9261903762817383 5.660759925842285
Loss :  1.742008090019226 3.3389477729797363 5.080955982208252
Loss :  1.7326581478118896 3.1094753742218018 4.842133522033691
Loss :  1.7370623350143433 3.6444852352142334 5.381547451019287
Loss :  1.7347118854522705 3.669114351272583 5.4038262367248535
Loss :  1.738281488418579 3.9484076499938965 5.686689376831055
Loss :  1.7300972938537598 3.5148444175720215 5.244941711425781
  batch 20 loss: 1.7300972938537598, 3.5148444175720215, 5.244941711425781
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.73885178565979 3.0742337703704834 4.813085556030273
Loss :  1.7390682697296143 3.1147587299346924 4.853826999664307
Loss :  1.736868977546692 2.931116819381714 4.667985916137695
Loss :  1.7432290315628052 2.563384771347046 4.306613922119141
Loss :  1.7354495525360107 2.630812406539917 4.366261959075928
Loss :  1.7304943799972534 2.4705724716186523 4.201066970825195
Loss :  1.7376734018325806 2.7328219413757324 4.470495223999023
Loss :  1.7277597188949585 2.5307343006134033 4.258493900299072
Loss :  1.7360777854919434 2.886296033859253 4.622373580932617
Loss :  1.731843113899231 3.8133018016815186 5.545145034790039
Loss :  1.7448172569274902 3.2472453117370605 4.992062568664551
Loss :  1.7366546392440796 2.7055933475494385 4.4422478675842285
Loss :  1.732254147529602 2.759068727493286 4.491322994232178
Loss :  1.730964183807373 2.491722822189331 4.222686767578125
Loss :  1.739288568496704 2.5790858268737793 4.3183746337890625
Loss :  1.7389695644378662 2.7793924808502197 4.518362045288086
Loss :  1.7353622913360596 2.5727498531341553 4.308112144470215
Loss :  1.7323825359344482 2.6412065029144287 4.373589038848877
Loss :  1.7358089685440063 3.2408382892608643 4.97664737701416
Loss :  1.7342889308929443 3.272475004196167 5.006763935089111
  batch 40 loss: 1.7342889308929443, 3.272475004196167, 5.006763935089111
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7332489490509033 2.4086363315582275 4.141885280609131
Loss :  1.7360912561416626 1.8705971240997314 3.6066884994506836
Loss :  1.7360600233078003 2.197683811187744 3.933743953704834
Loss :  1.7374165058135986 3.3929731845855713 5.13038969039917
Loss :  1.7382017374038696 2.958604574203491 4.69680643081665
Loss :  1.7313820123672485 2.6634089946746826 4.394791126251221
Loss :  1.73099946975708 2.9367778301239014 4.667777061462402
Loss :  1.7346701622009277 2.511774778366089 4.2464447021484375
Loss :  1.732532262802124 2.266045093536377 3.998577356338501
Loss :  1.7328184843063354 2.300929307937622 4.033747673034668
Loss :  1.7264997959136963 2.6094071865081787 4.335906982421875
Loss :  1.7304857969284058 2.5185399055480957 4.249025821685791
Loss :  1.736167073249817 2.2065846920013428 3.942751884460449
Loss :  1.7293202877044678 2.917496681213379 4.646817207336426
Loss :  1.7317982912063599 2.481311082839966 4.213109493255615
Loss :  1.729349970817566 2.353468179702759 4.082818031311035
Loss :  1.7324719429016113 3.106398582458496 4.838870525360107
Loss :  1.7360833883285522 3.1348297595977783 4.870913028717041
Loss :  1.7362099885940552 3.0036354064941406 4.739845275878906
Loss :  1.7338281869888306 2.4983651638031006 4.232193470001221
  batch 60 loss: 1.7338281869888306, 2.4983651638031006, 4.232193470001221
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7337894439697266 2.3884618282318115 4.122251510620117
Loss :  1.7351487874984741 2.511890172958374 4.247038841247559
Loss :  1.735954761505127 1.9465590715408325 3.68251371383667
Loss :  1.7272850275039673 2.6264734268188477 4.353758335113525
Loss :  1.7269214391708374 2.4515380859375 4.178459644317627
Loss :  1.769468069076538 4.088346481323242 5.857814788818359
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.7751896381378174 4.069393157958984 5.844582557678223
Loss :  1.770011305809021 3.9130475521087646 5.683058738708496
Loss :  1.7856980562210083 4.059270858764648 5.844968795776367
Total LOSS train 4.77043352493873 valid 5.807606220245361
CE LOSS train 1.7341801790090707 valid 0.4464245140552521
Contrastive LOSS train 3.0362533367597138 valid 1.014817714691162
EPOCH 171:
Loss :  1.7329572439193726 2.5634727478027344 4.2964301109313965
Loss :  1.7319796085357666 2.3933701515197754 4.125349998474121
Loss :  1.7273069620132446 2.3228025436401367 4.050109386444092
Loss :  1.731580138206482 2.7574303150177 4.489010334014893
Loss :  1.733036756515503 2.7053678035736084 4.438404560089111
Loss :  1.7328767776489258 2.2329561710357666 3.9658329486846924
Loss :  1.729927659034729 1.9571588039398193 3.687086582183838
Loss :  1.7292509078979492 1.5650296211242676 3.294280529022217
Loss :  1.7297770977020264 1.7032225131988525 3.432999610900879
Loss :  1.7157646417617798 1.5879762172698975 3.303740978240967
Loss :  1.7312949895858765 1.862707257270813 3.5940022468566895
Loss :  1.7415435314178467 1.8544094562530518 3.5959529876708984
Loss :  1.7321996688842773 2.047163724899292 3.7793633937835693
Loss :  1.7294394969940186 2.1916003227233887 3.9210398197174072
Loss :  1.7349995374679565 2.326275110244751 4.061274528503418
Loss :  1.7261384725570679 1.9578795433044434 3.684018135070801
Loss :  1.7335872650146484 1.721862554550171 3.4554498195648193
Loss :  1.7294471263885498 2.0582964420318604 3.78774356842041
Loss :  1.731555461883545 2.2093935012817383 3.940948963165283
Loss :  1.7263184785842896 1.9940985441207886 3.720417022705078
  batch 20 loss: 1.7263184785842896, 1.9940985441207886, 3.720417022705078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.731510043144226 2.0349979400634766 3.766508102416992
Loss :  1.7335041761398315 2.144930839538574 3.8784351348876953
Loss :  1.7334426641464233 2.005378246307373 3.738821029663086
Loss :  1.7399463653564453 2.0907514095306396 3.830697774887085
Loss :  1.7311644554138184 2.3386049270629883 4.069769382476807
Loss :  1.7279319763183594 2.2785685062408447 4.006500244140625
Loss :  1.7370589971542358 2.3761210441589355 4.113180160522461
Loss :  1.7287229299545288 2.325732469558716 4.054455280303955
Loss :  1.735930323600769 2.1889002323150635 3.924830436706543
Loss :  1.7268708944320679 2.554600715637207 4.2814717292785645
Loss :  1.7412481307983398 2.4294955730438232 4.170743942260742
Loss :  1.7349945306777954 2.34671950340271 4.081714153289795
Loss :  1.728878378868103 1.7404236793518066 3.469302177429199
Loss :  1.7272522449493408 2.242291212081909 3.96954345703125
Loss :  1.735560655593872 2.741290807723999 4.476851463317871
Loss :  1.7359603643417358 2.41031813621521 4.146278381347656
Loss :  1.733707070350647 3.0246779918670654 4.758385181427002
Loss :  1.7306617498397827 1.8190585374832153 3.549720287322998
Loss :  1.7327905893325806 2.61124849319458 4.344038963317871
Loss :  1.733781337738037 2.6439895629882812 4.377770900726318
  batch 40 loss: 1.733781337738037, 2.6439895629882812, 4.377770900726318
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7313276529312134 2.865506649017334 4.596834182739258
Loss :  1.731031060218811 2.1298816204071045 3.860912799835205
Loss :  1.7327510118484497 1.943393349647522 3.6761443614959717
Loss :  1.7328232526779175 2.167898654937744 3.900722026824951
Loss :  1.7355821132659912 2.088372230529785 3.8239543437957764
Loss :  1.7308369874954224 2.480621576309204 4.211458683013916
Loss :  1.7311972379684448 2.175335168838501 3.9065322875976562
Loss :  1.732780933380127 2.3554728031158447 4.088253974914551
Loss :  1.7352327108383179 2.5920181274414062 4.327250957489014
Loss :  1.7314555644989014 2.6042916774749756 4.335747241973877
Loss :  1.7270610332489014 2.282628059387207 4.0096893310546875
Loss :  1.7318296432495117 2.8930599689483643 4.624889373779297
Loss :  1.7375730276107788 2.2904775142669678 4.028050422668457
Loss :  1.7298833131790161 2.1718780994415283 3.901761531829834
Loss :  1.7328110933303833 2.270658016204834 4.003468990325928
Loss :  1.7304481267929077 2.2007412910461426 3.93118953704834
Loss :  1.7341578006744385 2.5526299476623535 4.286787986755371
Loss :  1.7367885112762451 1.709601879119873 3.446390390396118
Loss :  1.736754298210144 2.410400152206421 4.147154331207275
Loss :  1.7342684268951416 1.920931100845337 3.6551995277404785
  batch 60 loss: 1.7342684268951416, 1.920931100845337, 3.6551995277404785
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7338755130767822 1.9972941875457764 3.7311697006225586
Loss :  1.7353969812393188 1.9526808261871338 3.688077926635742
Loss :  1.7359850406646729 1.7719764709472656 3.5079615116119385
Loss :  1.7299102544784546 2.127713441848755 3.85762357711792
Loss :  1.7291945219039917 1.783753514289856 3.5129480361938477
Loss :  1.7680211067199707 3.98146653175354 5.74948787689209
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.7735074758529663 4.145181179046631 5.918688774108887
Loss :  1.766849398612976 3.9205551147460938 5.687404632568359
Loss :  1.7819240093231201 3.9943113327026367 5.776235580444336
Total LOSS train 3.9491176421825704 valid 5.782954216003418
CE LOSS train 1.7321977817095242 valid 0.44548100233078003
Contrastive LOSS train 2.216919838465177 valid 0.9985778331756592
EPOCH 172:
Loss :  1.7342873811721802 1.892797589302063 3.627084970474243
Loss :  1.731514811515808 2.3574681282043457 4.088983058929443
Loss :  1.7274538278579712 1.9487407207489014 3.676194667816162
Loss :  1.731151819229126 2.1411924362182617 3.8723442554473877
Loss :  1.7339986562728882 1.9299547672271729 3.6639533042907715
Loss :  1.7327805757522583 2.0642104148864746 3.7969908714294434
Loss :  1.7320040464401245 2.0686216354370117 3.800625801086426
Loss :  1.7305898666381836 1.5623223781585693 3.292912244796753
Loss :  1.731663465499878 1.939825177192688 3.6714887619018555
Loss :  1.7193189859390259 1.9295203685760498 3.6488394737243652
Loss :  1.7332878112792969 2.279595375061035 4.012883186340332
Loss :  1.7418290376663208 1.8976192474365234 3.6394481658935547
Loss :  1.7343169450759888 2.088862657546997 3.8231797218322754
Loss :  1.7308382987976074 1.875777006149292 3.6066153049468994
Loss :  1.735970377922058 1.7994177341461182 3.5353879928588867
Loss :  1.7276906967163086 1.9617207050323486 3.6894114017486572
Loss :  1.734084129333496 1.8917256593704224 3.625809669494629
Loss :  1.731184959411621 1.9191340208053589 3.6503190994262695
Loss :  1.7329809665679932 1.762833833694458 3.495814800262451
Loss :  1.7265316247940063 1.8538845777511597 3.580416202545166
  batch 20 loss: 1.7265316247940063, 1.8538845777511597, 3.580416202545166
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.731462001800537 1.834227442741394 3.5656895637512207
Loss :  1.7335938215255737 2.0116169452667236 3.745210647583008
Loss :  1.7335538864135742 2.447915554046631 4.181469440460205
Loss :  1.739900827407837 1.6083379983901978 3.348238945007324
Loss :  1.7327947616577148 1.7313215732574463 3.464116334915161
Loss :  1.7279771566390991 1.553269624710083 3.2812466621398926
Loss :  1.7365198135375977 1.935031533241272 3.67155122756958
Loss :  1.7263450622558594 1.7984291315078735 3.5247740745544434
Loss :  1.7344071865081787 2.0216121673583984 3.756019353866577
Loss :  1.7262552976608276 2.3022053241729736 4.028460502624512
Loss :  1.7419304847717285 2.406076192855835 4.148006439208984
Loss :  1.7343865633010864 1.7822400331497192 3.5166265964508057
Loss :  1.7285871505737305 2.121523857116699 3.8501110076904297
Loss :  1.7276654243469238 2.2190768718719482 3.946742296218872
Loss :  1.7364921569824219 2.6786623001098633 4.415154457092285
Loss :  1.7366688251495361 2.2834322452545166 4.020101070404053
Loss :  1.733109474182129 1.9111498594284058 3.644259452819824
Loss :  1.7291178703308105 1.8407961130142212 3.569913864135742
Loss :  1.7319843769073486 2.2124547958374023 3.944439172744751
Loss :  1.7317779064178467 2.057631731033325 3.789409637451172
  batch 40 loss: 1.7317779064178467, 2.057631731033325, 3.789409637451172
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7301312685012817 2.2223141193389893 3.9524455070495605
Loss :  1.7313755750656128 1.699158787727356 3.4305343627929688
Loss :  1.7331607341766357 2.2395739555358887 3.9727346897125244
Loss :  1.7335823774337769 1.9866772890090942 3.720259666442871
Loss :  1.7358169555664062 2.02361798286438 3.759434938430786
Loss :  1.7300856113433838 2.1084587574005127 3.8385443687438965
Loss :  1.7299286127090454 2.001932382583618 3.731861114501953
Loss :  1.7323758602142334 2.1300876140594482 3.8624634742736816
Loss :  1.7321799993515015 1.8055546283721924 3.5377345085144043
Loss :  1.7303327322006226 1.69273042678833 3.423063278198242
Loss :  1.724988341331482 1.952919602394104 3.677907943725586
Loss :  1.729546070098877 1.8962358236312866 3.625782012939453
Loss :  1.735897183418274 1.895107388496399 3.631004571914673
Loss :  1.7281795740127563 2.0754916667938232 3.803671360015869
Loss :  1.7309679985046387 2.1060938835144043 3.837061882019043
Loss :  1.7276923656463623 2.2147905826568604 3.9424829483032227
Loss :  1.7326040267944336 2.108438730239868 3.8410427570343018
Loss :  1.7349095344543457 1.732703447341919 3.4676129817962646
Loss :  1.7355751991271973 2.548180103302002 4.283755302429199
Loss :  1.732945442199707 1.9249688386917114 3.657914161682129
  batch 60 loss: 1.732945442199707, 1.9249688386917114, 3.657914161682129
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7329630851745605 2.229135274887085 3.9620983600616455
Loss :  1.7342957258224487 2.148916006088257 3.883211612701416
Loss :  1.7352139949798584 1.9272009134292603 3.662415027618408
Loss :  1.729167103767395 2.0526740550994873 3.781841278076172
Loss :  1.728278398513794 1.8857781887054443 3.6140565872192383
Loss :  1.7837762832641602 3.9812703132629395 5.7650465965271
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.7869952917099 4.165802478790283 5.952797889709473
Loss :  1.7829219102859497 3.851146697998047 5.634068489074707
Loss :  1.797411322593689 3.932020425796509 5.729431629180908
Total LOSS train 3.740171975355882 valid 5.770336151123047
CE LOSS train 1.7320031092717096 valid 0.44935283064842224
Contrastive LOSS train 2.008168864250183 valid 0.9830051064491272
EPOCH 173:
Loss :  1.7326304912567139 2.233527421951294 3.966157913208008
Loss :  1.7310795783996582 2.5109670162200928 4.242046356201172
Loss :  1.7277112007141113 2.9174487590789795 4.645159721374512
Loss :  1.7305785417556763 2.3606483936309814 4.091227054595947
Loss :  1.7342183589935303 1.6219604015350342 3.3561787605285645
Loss :  1.7324206829071045 1.4432663917541504 3.175687074661255
Loss :  1.7323718070983887 2.554121494293213 4.286493301391602
Loss :  1.7308571338653564 2.5480964183807373 4.278953552246094
Loss :  1.731472373008728 2.115492582321167 3.8469648361206055
Loss :  1.7190474271774292 2.029136896133423 3.7481842041015625
Loss :  1.7330321073532104 2.6511998176574707 4.384232044219971
Loss :  1.7413661479949951 3.1834065914154053 4.9247727394104
Loss :  1.733612298965454 1.9633668661117554 3.69697904586792
Loss :  1.731078863143921 1.9820796251296997 3.71315860748291
Loss :  1.735673189163208 2.1709606647491455 3.9066338539123535
Loss :  1.727616786956787 2.206298828125 3.933915615081787
Loss :  1.7343426942825317 2.2423207759857178 3.976663589477539
Loss :  1.7307757139205933 2.1567790508270264 3.88755464553833
Loss :  1.7318893671035767 2.0023460388183594 3.7342352867126465
Loss :  1.7264443635940552 1.9479504823684692 3.6743948459625244
  batch 20 loss: 1.7264443635940552, 1.9479504823684692, 3.6743948459625244
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7314006090164185 2.1370747089385986 3.8684754371643066
Loss :  1.7332830429077148 2.2286429405212402 3.961925983428955
Loss :  1.7333040237426758 1.7871588468551636 3.520462989807129
Loss :  1.739542841911316 1.8919767141342163 3.6315195560455322
Loss :  1.7325032949447632 2.1080033779144287 3.8405065536499023
Loss :  1.7283631563186646 2.0573384761810303 3.7857017517089844
Loss :  1.7371387481689453 1.8663735389709473 3.6035122871398926
Loss :  1.7275198698043823 1.9061729907989502 3.633692741394043
Loss :  1.7356128692626953 2.140103816986084 3.8757166862487793
Loss :  1.7268296480178833 2.6625471115112305 4.389376640319824
Loss :  1.7422257661819458 2.28996205329895 4.0321879386901855
Loss :  1.734365701675415 1.8460922241210938 3.580457925796509
Loss :  1.7284528017044067 1.6888381242752075 3.4172909259796143
Loss :  1.7274481058120728 2.2423386573791504 3.9697866439819336
Loss :  1.7362322807312012 2.325866460800171 4.062098503112793
Loss :  1.7364431619644165 1.7952800989151 3.5317232608795166
Loss :  1.732751488685608 1.8653218746185303 3.5980734825134277
Loss :  1.7286895513534546 2.030484437942505 3.75917387008667
Loss :  1.7319109439849854 1.8486467599868774 3.5805578231811523
Loss :  1.731810212135315 1.793662428855896 3.525472640991211
  batch 40 loss: 1.731810212135315, 1.793662428855896, 3.525472640991211
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.730018138885498 2.4746851921081543 4.204703330993652
Loss :  1.7308586835861206 1.8806118965148926 3.6114706993103027
Loss :  1.732286810874939 2.058070182800293 3.7903571128845215
Loss :  1.7324254512786865 2.139286756515503 3.8717122077941895
Loss :  1.7349138259887695 2.082975387573242 3.8178892135620117
Loss :  1.7299880981445312 2.03025221824646 3.760240316390991
Loss :  1.7297462224960327 1.8802504539489746 3.609996795654297
Loss :  1.7322605848312378 2.658802032470703 4.3910627365112305
Loss :  1.7333678007125854 2.0621986389160156 3.7955665588378906
Loss :  1.7304799556732178 1.8831759691238403 3.6136560440063477
Loss :  1.7261959314346313 2.974947690963745 4.701143741607666
Loss :  1.7305407524108887 2.0174384117126465 3.747979164123535
Loss :  1.736930012702942 1.9376411437988281 3.6745710372924805
Loss :  1.7283960580825806 2.061387777328491 3.7897839546203613
Loss :  1.731120228767395 2.1232805252075195 3.854400634765625
Loss :  1.729049563407898 1.986682653427124 3.7157320976257324
Loss :  1.7339471578598022 2.4763834476470947 4.210330486297607
Loss :  1.736063003540039 2.8556320667266846 4.5916948318481445
Loss :  1.736569881439209 2.6729843616485596 4.409554481506348
Loss :  1.7339195013046265 1.8333746194839478 3.567294120788574
  batch 60 loss: 1.7339195013046265, 1.8333746194839478, 3.567294120788574
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7338500022888184 1.9259669780731201 3.6598169803619385
Loss :  1.7349821329116821 2.2223806381225586 3.957362651824951
Loss :  1.7363027334213257 2.1236696243286133 3.8599724769592285
Loss :  1.7294023036956787 2.4141347408294678 4.1435370445251465
Loss :  1.7289819717407227 1.5565581321716309 3.2855401039123535
Loss :  1.7699899673461914 4.1581807136535645 5.928170680999756
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.7733008861541748 4.237631797790527 6.010932922363281
Loss :  1.7692060470581055 3.9287257194519043 5.69793176651001
Loss :  1.7824106216430664 3.870598793029785 5.653009414672852
Total LOSS train 3.881118084834172 valid 5.822511196136475
CE LOSS train 1.732102216207064 valid 0.4456026554107666
Contrastive LOSS train 2.1490158722950863 valid 0.9676496982574463
EPOCH 174:
Loss :  1.7340880632400513 2.384132146835327 4.118220329284668
Loss :  1.7329435348510742 2.3197388648986816 4.052682399749756
Loss :  1.7290295362472534 1.885578989982605 3.6146085262298584
Loss :  1.7327215671539307 2.5251080989837646 4.257829666137695
Loss :  1.7357292175292969 1.8660240173339844 3.6017532348632812
Loss :  1.7342180013656616 2.2229766845703125 3.9571948051452637
Loss :  1.7333019971847534 3.669478416442871 5.402780532836914
Loss :  1.73166823387146 2.4150280952453613 4.146696090698242
Loss :  1.7320040464401245 2.397974967956543 4.129979133605957
Loss :  1.719380497932434 1.9684545993804932 3.687835216522217
Loss :  1.732938528060913 2.6048319339752197 4.337770462036133
Loss :  1.7415732145309448 2.259899616241455 4.0014729499816895
Loss :  1.7340428829193115 2.06821346282959 3.8022563457489014
Loss :  1.7319501638412476 2.277198076248169 4.009148120880127
Loss :  1.7371207475662231 2.1511504650115967 3.8882713317871094
Loss :  1.7291377782821655 2.4765279293060303 4.205665588378906
Loss :  1.7352566719055176 2.2148072719573975 3.950063943862915
Loss :  1.7319706678390503 2.0032339096069336 3.7352046966552734
Loss :  1.7325741052627563 1.8472539186477661 3.5798280239105225
Loss :  1.728253722190857 1.622256875038147 3.350510597229004
  batch 20 loss: 1.728253722190857, 1.622256875038147, 3.350510597229004
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7328521013259888 1.8099267482757568 3.542778968811035
Loss :  1.7342350482940674 1.980968713760376 3.7152037620544434
Loss :  1.7349501848220825 2.305222511291504 4.040172576904297
Loss :  1.7402958869934082 2.6585540771484375 4.398849964141846
Loss :  1.7329375743865967 2.1745431423187256 3.9074807167053223
Loss :  1.729597806930542 2.3230786323547363 4.052676200866699
Loss :  1.7377220392227173 2.459451913833618 4.197174072265625
Loss :  1.7282613515853882 2.141993284225464 3.8702545166015625
Loss :  1.7360121011734009 2.2973744869232178 4.033386707305908
Loss :  1.727091670036316 2.6719744205474854 4.399065971374512
Loss :  1.742435336112976 2.551520824432373 4.293956279754639
Loss :  1.7344608306884766 2.726207733154297 4.460668563842773
Loss :  1.7282378673553467 2.816894054412842 4.545131683349609
Loss :  1.7274225950241089 2.8920822143554688 4.619504928588867
Loss :  1.7350242137908936 2.6422855854034424 4.377309799194336
Loss :  1.7361072301864624 1.774497389793396 3.5106046199798584
Loss :  1.7326782941818237 1.9351427555084229 3.667820930480957
Loss :  1.7285085916519165 1.897475004196167 3.625983715057373
Loss :  1.7321468591690063 2.4603872299194336 4.19253396987915
Loss :  1.7327394485473633 2.016815662384033 3.7495551109313965
  batch 40 loss: 1.7327394485473633, 2.016815662384033, 3.7495551109313965
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7304619550704956 3.297618865966797 5.028080940246582
Loss :  1.7299293279647827 2.8005897998809814 4.530519008636475
Loss :  1.7313284873962402 2.1389782428741455 3.8703067302703857
Loss :  1.7308095693588257 2.61312198638916 4.343931674957275
Loss :  1.7337496280670166 2.3120646476745605 4.045814514160156
Loss :  1.7303571701049805 2.584084987640381 4.314442157745361
Loss :  1.7304515838623047 2.349679708480835 4.080131530761719
Loss :  1.7303820848464966 2.635688543319702 4.366070747375488
Loss :  1.7344071865081787 1.8063725233078003 3.5407795906066895
Loss :  1.7287343740463257 2.430483102798462 4.159217357635498
Loss :  1.7260347604751587 2.6501314640045166 4.376166343688965
Loss :  1.7302024364471436 2.856325626373291 4.5865278244018555
Loss :  1.7379944324493408 2.089784622192383 3.8277790546417236
Loss :  1.7274075746536255 1.7956527471542358 3.5230603218078613
Loss :  1.7307668924331665 3.294849157333374 5.02561616897583
Loss :  1.7288771867752075 1.8819448947906494 3.6108222007751465
Loss :  1.734696388244629 2.6270124912261963 4.361708641052246
Loss :  1.7350538969039917 3.112048625946045 4.847102642059326
Loss :  1.736228346824646 2.9192910194396973 4.655519485473633
Loss :  1.7340974807739258 3.2537131309509277 4.9878106117248535
  batch 60 loss: 1.7340974807739258, 3.2537131309509277, 4.9878106117248535
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7348783016204834 3.2283525466918945 4.963231086730957
Loss :  1.735343098640442 3.2672622203826904 5.002605438232422
Loss :  1.7372411489486694 2.728627920150757 4.465868949890137
Loss :  1.7300957441329956 3.086719274520874 4.81681489944458
Loss :  1.7289351224899292 2.9875504970550537 4.716485500335693
Loss :  1.7084640264511108 4.057481288909912 5.7659454345703125
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4], device='cuda:0')
Loss :  1.714834213256836 4.165879726409912 5.880713939666748
Loss :  1.7056021690368652 3.961627960205078 5.667230129241943
Loss :  1.7208130359649658 3.9896962642669678 5.710509300231934
Total LOSS train 4.170404591927162 valid 5.756099700927734
CE LOSS train 1.7324936059805063 valid 0.43020325899124146
Contrastive LOSS train 2.437910974942721 valid 0.9974240660667419
EPOCH 175:
Loss :  1.733148217201233 2.6008496284484863 4.33399772644043
Loss :  1.7335902452468872 3.0712008476257324 4.80479097366333
Loss :  1.7288414239883423 2.298375368118286 4.027216911315918
Loss :  1.7309452295303345 1.9612600803375244 3.6922054290771484
Loss :  1.7349743843078613 1.8602334260940552 3.595207691192627
Loss :  1.7331479787826538 1.684765100479126 3.4179129600524902
Loss :  1.732647180557251 2.557049512863159 4.28969669342041
Loss :  1.730908989906311 1.9829761981964111 3.7138853073120117
Loss :  1.7313588857650757 1.8557450771331787 3.587103843688965
Loss :  1.7183713912963867 2.0308048725128174 3.749176263809204
Loss :  1.731593370437622 2.6659698486328125 4.3975629806518555
Loss :  1.7418699264526367 2.4166417121887207 4.158511638641357
Loss :  1.7323318719863892 2.975886583328247 4.708218574523926
Loss :  1.7305399179458618 2.4208199977874756 4.151360034942627
Loss :  1.7349931001663208 2.2853875160217285 4.02038049697876
Loss :  1.7268270254135132 2.2913429737091064 4.01816987991333
Loss :  1.7325658798217773 1.9928467273712158 3.725412607192993
Loss :  1.7281681299209595 2.0504236221313477 3.7785916328430176
Loss :  1.7319128513336182 1.8618645668029785 3.5937774181365967
Loss :  1.724265456199646 2.234919309616089 3.9591846466064453
  batch 20 loss: 1.724265456199646, 2.234919309616089, 3.9591846466064453
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.729929804801941 2.147926092147827 3.8778557777404785
Loss :  1.7327359914779663 2.153035879135132 3.8857717514038086
Loss :  1.7325166463851929 2.28995680809021 4.022473335266113
Loss :  1.7390880584716797 2.6374611854553223 4.376549243927002
Loss :  1.7330158948898315 2.3522610664367676 4.085277080535889
Loss :  1.7282142639160156 2.113619804382324 3.84183406829834
Loss :  1.736632227897644 2.201934814453125 3.9385671615600586
Loss :  1.72622811794281 1.6965469121932983 3.4227750301361084
Loss :  1.73509681224823 1.6663405895233154 3.401437282562256
Loss :  1.7271934747695923 2.2691547870635986 3.9963483810424805
Loss :  1.7427290678024292 2.7561073303222656 4.498836517333984
Loss :  1.7344567775726318 2.3303046226501465 4.064761161804199
Loss :  1.7286827564239502 2.028261423110962 3.756944179534912
Loss :  1.7280317544937134 2.2498180866241455 3.9778499603271484
Loss :  1.7364314794540405 2.2512030601501465 3.9876346588134766
Loss :  1.736907958984375 2.5702009201049805 4.3071088790893555
Loss :  1.7329927682876587 2.216975212097168 3.949967861175537
Loss :  1.7289923429489136 2.106546640396118 3.835538864135742
Loss :  1.731805682182312 2.820160388946533 4.551966190338135
Loss :  1.7324217557907104 1.7463757991790771 3.478797435760498
  batch 40 loss: 1.7324217557907104, 1.7463757991790771, 3.478797435760498
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7302707433700562 2.4994490146636963 4.229719638824463
Loss :  1.730411171913147 2.1451809406280518 3.8755922317504883
Loss :  1.7322221994400024 1.9919360876083374 3.72415828704834
Loss :  1.7316105365753174 2.7768607139587402 4.508471488952637
Loss :  1.7345858812332153 3.082411289215088 4.816997051239014
Loss :  1.7304238080978394 2.772529363632202 4.502953052520752
Loss :  1.7296764850616455 2.60780668258667 4.3374834060668945
Loss :  1.7316796779632568 2.683938503265381 4.415617942810059
Loss :  1.7338666915893555 3.2450273036956787 4.978894233703613
Loss :  1.7305102348327637 2.259749412536621 3.9902596473693848
Loss :  1.7265607118606567 2.1995859146118164 3.9261465072631836
Loss :  1.730928897857666 2.539891004562378 4.270819664001465
Loss :  1.7372664213180542 2.80182147026062 4.539087772369385
Loss :  1.729287028312683 2.4127182960510254 4.142005443572998
Loss :  1.7318294048309326 2.055440664291382 3.7872700691223145
Loss :  1.7297390699386597 2.5846831798553467 4.314422130584717
Loss :  1.7327966690063477 3.813079357147217 5.5458760261535645
Loss :  1.7368323802947998 3.7637710571289062 5.500603675842285
Loss :  1.7365694046020508 2.9517099857330322 4.688279151916504
Loss :  1.7340092658996582 1.5529747009277344 3.2869839668273926
  batch 60 loss: 1.7340092658996582, 1.5529747009277344, 3.2869839668273926
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.733817219734192 2.0161216259002686 3.74993896484375
Loss :  1.7351690530776978 1.6051443815231323 3.34031343460083
Loss :  1.7349419593811035 1.4353233575820923 3.1702651977539062
Loss :  1.729177474975586 1.8357516527175903 3.5649290084838867
Loss :  1.72770094871521 1.6774235963821411 3.4051246643066406
Loss :  1.7069677114486694 3.6357274055480957 5.342695236206055
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
Loss :  1.713112711906433 3.8458547592163086 5.558967590332031
Loss :  1.705681324005127 3.55098032951355 5.256661415100098
Loss :  1.724549412727356 3.5196330547332764 5.244182586669922
Total LOSS train 4.055274941371038 valid 5.350626707077026
CE LOSS train 1.7319844374289879 valid 0.431137353181839
Contrastive LOSS train 2.32329052228194 valid 0.8799082636833191
EPOCH 176:
Loss :  1.7318235635757446 2.066051721572876 3.79787540435791
Loss :  1.7302284240722656 2.191110372543335 3.9213387966156006
Loss :  1.7266477346420288 2.5559592247009277 4.282607078552246
Loss :  1.7303000688552856 3.1826252937316895 4.9129252433776855
Loss :  1.73332941532135 2.1004724502563477 3.833801746368408
Loss :  1.7323685884475708 2.942589044570923 4.674957752227783
Loss :  1.7308987379074097 2.9720053672790527 4.702904224395752
Loss :  1.72958242893219 2.2944416999816895 4.02402400970459
Loss :  1.7305196523666382 2.1229372024536133 3.853456974029541
Loss :  1.7166309356689453 2.6943886280059814 4.411019325256348
Loss :  1.7321096658706665 3.21043062210083 4.942540168762207
Loss :  1.742403507232666 3.299910545349121 5.042314052581787
Loss :  1.7322231531143188 2.40038800239563 4.132611274719238
Loss :  1.7307096719741821 2.5362095832824707 4.266919136047363
Loss :  1.7350058555603027 2.9142611026763916 4.649267196655273
Loss :  1.72659432888031 2.636277675628662 4.362872123718262
Loss :  1.733859896659851 2.8791608810424805 4.613020896911621
Loss :  1.7301169633865356 3.0461959838867188 4.776312828063965
Loss :  1.7310062646865845 2.3462588787078857 4.07726526260376
Loss :  1.7261228561401367 1.8297919034957886 3.555914878845215
  batch 20 loss: 1.7261228561401367, 1.8297919034957886, 3.555914878845215
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.730608344078064 2.299689292907715 4.030297756195068
Loss :  1.7329778671264648 2.097041606903076 3.830019474029541
Loss :  1.732058048248291 1.863019347190857 3.5950775146484375
Loss :  1.7391092777252197 2.5533323287963867 4.292441368103027
Loss :  1.7313252687454224 2.2272789478302 3.958604335784912
Loss :  1.7273796796798706 1.9593334197998047 3.686713218688965
Loss :  1.7372910976409912 1.7955747842788696 3.5328660011291504
Loss :  1.7277946472167969 2.129106044769287 3.856900691986084
Loss :  1.7355968952178955 2.378399133682251 4.1139960289001465
Loss :  1.7262219190597534 2.5579469203948975 4.284168720245361
Loss :  1.7413049936294556 2.6462862491607666 4.387591361999512
Loss :  1.7344549894332886 1.9645150899887085 3.698970079421997
Loss :  1.7276618480682373 2.139291286468506 3.866953134536743
Loss :  1.7257462739944458 1.9795736074447632 3.705319881439209
Loss :  1.7347582578659058 2.6440837383270264 4.378841876983643
Loss :  1.734557867050171 2.2813880443573 4.015945911407471
Loss :  1.7311086654663086 2.1698544025421143 3.900963068008423
Loss :  1.7276986837387085 3.17637300491333 4.904071807861328
Loss :  1.7302991151809692 1.6920077800750732 3.422307014465332
Loss :  1.7309759855270386 1.527353048324585 3.258328914642334
  batch 40 loss: 1.7309759855270386, 1.527353048324585, 3.258328914642334
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7294617891311646 2.335334300994873 4.064795970916748
Loss :  1.7283248901367188 1.7639033794403076 3.4922282695770264
Loss :  1.7309597730636597 1.8745568990707397 3.6055166721343994
Loss :  1.7315208911895752 3.7454192638397217 5.476940155029297
Loss :  1.7333801984786987 3.4802539348602295 5.213634014129639
Loss :  1.7311391830444336 3.2158966064453125 4.947035789489746
Loss :  1.7380256652832031 3.4914095401763916 5.229434967041016
Loss :  1.7318848371505737 3.417290210723877 5.14917516708374
Loss :  1.7403804063796997 2.9806067943573 4.720987319946289
Loss :  1.7322570085525513 2.746809959411621 4.479066848754883
Loss :  1.7337093353271484 2.7167956829071045 4.450505256652832
Loss :  1.7382148504257202 2.85262393951416 4.59083890914917
Loss :  1.74538254737854 2.569610834121704 4.314993381500244
Loss :  1.7361081838607788 2.5465779304504395 4.282686233520508
Loss :  1.737552523612976 2.5864744186401367 4.324027061462402
Loss :  1.7399533987045288 2.456547737121582 4.1965012550354
Loss :  1.740146517753601 2.471616268157959 4.21176290512085
Loss :  1.7391397953033447 3.0390000343322754 4.778140068054199
Loss :  1.7395997047424316 3.2098755836486816 4.949475288391113
Loss :  1.7408140897750854 2.913360118865967 4.654174327850342
  batch 60 loss: 1.7408140897750854, 2.913360118865967, 4.654174327850342
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7465994358062744 2.915382146835327 4.661981582641602
Loss :  1.740339994430542 3.5333850383758545 5.2737250328063965
Loss :  1.7519844770431519 3.021609306335449 4.773593902587891
Loss :  1.7394607067108154 2.978668689727783 4.7181291580200195
Loss :  1.7340117692947388 2.5168955326080322 4.2509074211120605
Loss :  1.758827567100525 3.9764297008514404 5.735257148742676
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7531025409698486 4.013415336608887 5.766517639160156
Loss :  1.7566287517547607 3.8641233444213867 5.620752334594727
Loss :  1.7681583166122437 3.8761801719665527 5.644338607788086
Total LOSS train 4.3133012844966006 valid 5.691716432571411
CE LOSS train 1.733565590931819 valid 0.4420395791530609
Contrastive LOSS train 2.5797356678889347 valid 0.9690450429916382
EPOCH 177:
Loss :  1.7382934093475342 2.7433836460113525 4.481677055358887
Loss :  1.73752760887146 3.134554147720337 4.872081756591797
Loss :  1.7332559823989868 3.247819662094116 4.981075763702393
Loss :  1.7394301891326904 3.5703818798065186 5.309812068939209
Loss :  1.7381558418273926 3.7016665935516357 5.439822196960449
Loss :  1.7400215864181519 3.3134994506835938 5.053521156311035
Loss :  1.7370134592056274 3.3020033836364746 5.0390167236328125
Loss :  1.7389781475067139 3.253864049911499 4.992842197418213
Loss :  1.7376459836959839 2.664621591567993 4.4022674560546875
Loss :  1.725302815437317 2.4295225143432617 4.154825210571289
Loss :  1.739575982093811 2.3292737007141113 4.068849563598633
Loss :  1.7466078996658325 2.378844738006592 4.125452518463135
Loss :  1.7385624647140503 2.496772289276123 4.235334873199463
Loss :  1.7378000020980835 2.316044569015503 4.053844451904297
Loss :  1.743843913078308 3.07889461517334 4.8227386474609375
Loss :  1.7345060110092163 2.546126365661621 4.280632495880127
Loss :  1.7422313690185547 2.5659890174865723 4.308220386505127
Loss :  1.7363941669464111 2.4715118408203125 4.2079057693481445
Loss :  1.7372357845306396 3.315525770187378 5.052761554718018
Loss :  1.7355767488479614 3.435692071914673 5.171268939971924
  batch 20 loss: 1.7355767488479614, 3.435692071914673, 5.171268939971924
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7375158071517944 2.5463156700134277 4.283831596374512
Loss :  1.739714503288269 3.4679994583129883 5.207714080810547
Loss :  1.7374277114868164 4.005655288696289 5.7430830001831055
Loss :  1.742321252822876 3.3411967754364014 5.083518028259277
Loss :  1.7347525358200073 3.4852538108825684 5.220006465911865
Loss :  1.7324936389923096 3.369612693786621 5.102106094360352
Loss :  1.7395933866500854 2.409263849258423 4.148857116699219
Loss :  1.7305712699890137 2.103924036026001 3.8344953060150146
Loss :  1.7372492551803589 1.895220160484314 3.632469415664673
Loss :  1.72975754737854 2.3700649738311768 4.099822521209717
Loss :  1.743139386177063 2.2975924015045166 4.040731906890869
Loss :  1.7349748611450195 2.3196492195129395 4.054624080657959
Loss :  1.7289679050445557 2.37831711769104 4.107285022735596
Loss :  1.7287108898162842 2.6804087162017822 4.409119606018066
Loss :  1.7363287210464478 2.983004570007324 4.719333171844482
Loss :  1.7367225885391235 2.7797389030456543 4.516461372375488
Loss :  1.7337087392807007 3.2182838916778564 4.951992511749268
Loss :  1.7285743951797485 2.5789718627929688 4.307546138763428
Loss :  1.7324438095092773 2.552049160003662 4.2844929695129395
Loss :  1.7327497005462646 2.4590628147125244 4.191812515258789
  batch 40 loss: 1.7327497005462646, 2.4590628147125244, 4.191812515258789
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.730912685394287 2.7210590839385986 4.451972007751465
Loss :  1.7307145595550537 2.92903995513916 4.659754753112793
Loss :  1.7316070795059204 2.3260550498962402 4.057662010192871
Loss :  1.7314497232437134 2.4378232955932617 4.1692728996276855
Loss :  1.7338393926620483 2.6175906658172607 4.3514299392700195
Loss :  1.7296817302703857 2.4309346675872803 4.160616397857666
Loss :  1.7294883728027344 2.682769775390625 4.412258148193359
Loss :  1.7301462888717651 1.9355521202087402 3.665698528289795
Loss :  1.7323325872421265 2.2263667583465576 3.9586992263793945
Loss :  1.7286521196365356 2.0583338737487793 3.7869858741760254
Loss :  1.725496768951416 2.0038862228393555 3.7293829917907715
Loss :  1.729681134223938 2.3098556995391846 4.039536952972412
Loss :  1.736944556236267 1.7703673839569092 3.5073118209838867
Loss :  1.7281465530395508 2.7017509937286377 4.429897308349609
Loss :  1.7317005395889282 2.1678285598754883 3.899528980255127
Loss :  1.7285501956939697 2.0559022426605225 3.784452438354492
Loss :  1.733462929725647 2.2209670543670654 3.954430103302002
Loss :  1.7353312969207764 2.3394935131073 4.074824810028076
Loss :  1.7357629537582397 2.42985200881958 4.165615081787109
Loss :  1.7330927848815918 1.8766388893127441 3.609731674194336
  batch 60 loss: 1.7330927848815918, 1.8766388893127441, 3.609731674194336
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7326463460922241 2.2533457279205322 3.985991954803467
Loss :  1.7345093488693237 2.7854063510894775 4.519915580749512
Loss :  1.734645962715149 2.646218776702881 4.38086462020874
Loss :  1.7283607721328735 2.9193532466888428 4.647714138031006
Loss :  1.7274143695831299 1.9096474647521973 3.637061834335327
Loss :  1.7034449577331543 4.273529052734375 5.976974010467529
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.7113949060440063 4.2489728927612305 5.960367679595947
Loss :  1.707181453704834 4.095751762390137 5.802933216094971
Loss :  1.7174503803253174 4.066450119018555 5.783900260925293
Total LOSS train 4.385136365890503 valid 5.881043791770935
CE LOSS train 1.7344653588074905 valid 0.42936259508132935
Contrastive LOSS train 2.6506710254229033 valid 1.0166125297546387
EPOCH 178:
Loss :  1.7324987649917603 1.623680591583252 3.3561792373657227
Loss :  1.7310229539871216 1.8835914134979248 3.614614486694336
Loss :  1.7273963689804077 1.509809970855713 3.23720645904541
Loss :  1.7309035062789917 1.6819764375686646 3.4128799438476562
Loss :  1.7337126731872559 1.8470381498336792 3.5807509422302246
Loss :  1.732853651046753 1.9905587434768677 3.72341251373291
Loss :  1.7319822311401367 2.5436928272247314 4.275674819946289
Loss :  1.7304434776306152 2.0312626361846924 3.7617061138153076
Loss :  1.7314504384994507 1.9838632345199585 3.715313673019409
Loss :  1.718431830406189 2.0050721168518066 3.723504066467285
Loss :  1.7323192358016968 2.465928554534912 4.198247909545898
Loss :  1.7417190074920654 2.107630968093872 3.8493499755859375
Loss :  1.7327440977096558 1.6416547298431396 3.374398708343506
Loss :  1.7307389974594116 1.9308514595031738 3.661590576171875
Loss :  1.7349733114242554 1.8109990358352661 3.5459723472595215
Loss :  1.7272467613220215 2.0950489044189453 3.822295665740967
Loss :  1.733177661895752 2.1649131774902344 3.8980908393859863
Loss :  1.7296756505966187 2.288421630859375 4.018097400665283
Loss :  1.7309060096740723 1.6162253618240356 3.3471312522888184
Loss :  1.7262446880340576 1.769878625869751 3.4961233139038086
  batch 20 loss: 1.7262446880340576, 1.769878625869751, 3.4961233139038086
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.730529546737671 2.031963586807251 3.762493133544922
Loss :  1.7325987815856934 2.2470133304595947 3.979612112045288
Loss :  1.7330832481384277 2.1956374645233154 3.928720712661743
Loss :  1.739128589630127 2.1586859226226807 3.8978145122528076
Loss :  1.7327862977981567 2.0510761737823486 3.783862590789795
Loss :  1.728277564048767 2.103837251663208 3.8321146965026855
Loss :  1.7369126081466675 2.169365406036377 3.906278133392334
Loss :  1.727080225944519 1.8719220161437988 3.5990023612976074
Loss :  1.7348926067352295 1.650464415550232 3.385356903076172
Loss :  1.7264245748519897 2.5426557064056396 4.26908016204834
Loss :  1.7418361902236938 2.617316961288452 4.3591532707214355
Loss :  1.734292984008789 2.6947991847991943 4.4290924072265625
Loss :  1.7277743816375732 1.9949110746383667 3.7226853370666504
Loss :  1.7273468971252441 2.1897737979888916 3.9171206951141357
Loss :  1.7354494333267212 2.3020434379577637 4.037492752075195
Loss :  1.7358723878860474 2.3606934547424316 4.0965657234191895
Loss :  1.7323354482650757 2.0644826889038086 3.796818256378174
Loss :  1.7283262014389038 1.9614776372909546 3.6898038387298584
Loss :  1.7317752838134766 2.3895058631896973 4.121281147003174
Loss :  1.7324215173721313 1.5489451885223389 3.2813668251037598
  batch 40 loss: 1.7324215173721313, 1.5489451885223389, 3.2813668251037598
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7303580045700073 1.7720668315887451 3.502424716949463
Loss :  1.7300214767456055 2.1742162704467773 3.904237747192383
Loss :  1.732030987739563 1.9720724821090698 3.704103469848633
Loss :  1.731522560119629 1.913185477256775 3.6447081565856934
Loss :  1.7344835996627808 1.7112778425216675 3.4457614421844482
Loss :  1.7306548357009888 1.8861116170883179 3.6167664527893066
Loss :  1.7303334474563599 1.8539332151412964 3.5842666625976562
Loss :  1.7319657802581787 1.8649711608886719 3.5969369411468506
Loss :  1.7349114418029785 2.118277072906494 3.8531885147094727
Loss :  1.7312511205673218 2.046754837036133 3.778006076812744
Loss :  1.727446436882019 2.4786274433135986 4.206073760986328
Loss :  1.7318964004516602 2.0490264892578125 3.7809228897094727
Loss :  1.7376209497451782 1.8432124853134155 3.5808334350585938
Loss :  1.729413628578186 2.0274174213409424 3.756831169128418
Loss :  1.7311680316925049 2.435645818710327 4.166813850402832
Loss :  1.7284125089645386 1.7387444972991943 3.4671568870544434
Loss :  1.733528971672058 1.917112946510315 3.650641918182373
Loss :  1.735810399055481 1.7504312992095947 3.4862418174743652
Loss :  1.7358988523483276 2.3500895500183105 4.085988521575928
Loss :  1.732479453086853 1.9898618459701538 3.722341299057007
  batch 60 loss: 1.732479453086853, 1.9898618459701538, 3.722341299057007
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.732437252998352 2.021138906478882 3.7535762786865234
Loss :  1.7340070009231567 2.1499850749969482 3.8839921951293945
Loss :  1.7349413633346558 2.4124207496643066 4.147362232208252
Loss :  1.7281901836395264 2.117924928665161 3.8461151123046875
Loss :  1.7274938821792603 2.4164772033691406 4.143970966339111
Loss :  1.7109657526016235 4.304296016693115 6.015261650085449
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.7165892124176025 4.297930717468262 6.014519691467285
Loss :  1.713767647743225 4.1511712074279785 5.864938735961914
Loss :  1.7263208627700806 4.087011814117432 5.813332557678223
Total LOSS train 3.78026951276339 valid 5.927013158798218
CE LOSS train 1.731813302406898 valid 0.43158021569252014
Contrastive LOSS train 2.0484561938505905 valid 1.021752953529358
EPOCH 179:
Loss :  1.7325280904769897 2.6982297897338867 4.430757999420166
Loss :  1.7306216955184937 3.0615479946136475 4.792169570922852
Loss :  1.7270718812942505 2.58612322807312 4.31319522857666
Loss :  1.7303204536437988 1.7479195594787598 3.4782400131225586
Loss :  1.7341774702072144 2.4402856826782227 4.174463272094727
Loss :  1.7322744131088257 3.286595344543457 5.018869876861572
Loss :  1.7316997051239014 2.6560046672821045 4.387704372406006
Loss :  1.7291617393493652 1.6994634866714478 3.4286251068115234
Loss :  1.7305594682693481 1.4817992448806763 3.2123587131500244
Loss :  1.717405915260315 2.0932955741882324 3.810701370239258
Loss :  1.7313612699508667 2.2767813205718994 4.008142471313477
Loss :  1.7405086755752563 1.9929269552230835 3.73343563079834
Loss :  1.7325173616409302 2.0126123428344727 3.7451295852661133
Loss :  1.7300362586975098 1.9227429628372192 3.6527791023254395
Loss :  1.734440565109253 2.930152416229248 4.664592742919922
Loss :  1.7275044918060303 2.983869791030884 4.711374282836914
Loss :  1.7335206270217896 3.7640719413757324 5.497592449188232
Loss :  1.7366102933883667 3.9034197330474854 5.6400299072265625
Loss :  1.7336130142211914 2.1454787254333496 3.879091739654541
Loss :  1.7351300716400146 2.588625192642212 4.323755264282227
  batch 20 loss: 1.7351300716400146, 2.588625192642212, 4.323755264282227
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7357827425003052 2.5723631381988525 4.308145999908447
Loss :  1.7376115322113037 2.5243942737579346 4.262005805969238
Loss :  1.738318920135498 2.4595706462860107 4.19788932800293
Loss :  1.741924524307251 2.358283281326294 4.100207805633545
Loss :  1.7342519760131836 2.400707483291626 4.1349592208862305
Loss :  1.7333502769470215 2.2386796474456787 3.9720299243927
Loss :  1.739759922027588 2.200340509414673 3.9401004314422607
Loss :  1.733027696609497 2.3488259315490723 4.081853866577148
Loss :  1.738481879234314 2.1974427700042725 3.935924530029297
Loss :  1.7329638004302979 2.577221393585205 4.310185432434082
Loss :  1.7443996667861938 2.6365549564361572 4.380954742431641
Loss :  1.7370214462280273 2.3015565872192383 4.038578033447266
Loss :  1.7327255010604858 2.025308847427368 3.7580342292785645
Loss :  1.7327361106872559 2.315662145614624 4.048398017883301
Loss :  1.7391763925552368 3.0881118774414062 4.8272881507873535
Loss :  1.7392210960388184 2.895266056060791 4.634487152099609
Loss :  1.7382622957229614 3.047553062438965 4.785815238952637
Loss :  1.7329756021499634 2.687546730041504 4.420522212982178
Loss :  1.736788272857666 2.810272455215454 4.547060966491699
Loss :  1.7373204231262207 3.199692487716675 4.937012672424316
  batch 40 loss: 1.7373204231262207, 3.199692487716675, 4.937012672424316
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7364134788513184 3.2630083560943604 4.999422073364258
Loss :  1.735500454902649 3.470489263534546 5.205989837646484
Loss :  1.7357585430145264 2.5797929763793945 4.3155517578125
Loss :  1.7360166311264038 3.3237080574035645 5.059724807739258
Loss :  1.737910270690918 2.5428683757781982 4.280778884887695
Loss :  1.7387536764144897 3.371701955795288 5.110455513000488
Loss :  1.7343389987945557 3.6891930103302 5.423532009124756
Loss :  1.734214186668396 3.5964393615722656 5.330653667449951
Loss :  1.7368605136871338 3.552353620529175 5.289214134216309
Loss :  1.73227059841156 3.1295692920684814 4.861839771270752
Loss :  1.729160189628601 3.6060359477996826 5.335196018218994
Loss :  1.7344657182693481 3.969141721725464 5.703607559204102
Loss :  1.743119239807129 3.4383723735809326 5.181491851806641
Loss :  1.7328583002090454 3.0619637966156006 4.7948222160339355
Loss :  1.7368184328079224 2.673473834991455 4.410292148590088
Loss :  1.734830617904663 3.1337954998016357 4.868626117706299
Loss :  1.7374686002731323 3.2228598594665527 4.960328578948975
Loss :  1.740968108177185 3.0790345668792725 4.820002555847168
Loss :  1.7399797439575195 3.261767625808716 5.001747131347656
Loss :  1.737392544746399 3.2368247509002686 4.974217414855957
  batch 60 loss: 1.737392544746399, 3.2368247509002686, 4.974217414855957
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7390902042388916 2.9644453525543213 4.703535556793213
Loss :  1.7388036251068115 2.380284309387207 4.119088172912598
Loss :  1.7408788204193115 3.187288522720337 4.928167343139648
Loss :  1.7334944009780884 3.0833241939544678 4.816818714141846
Loss :  1.7346705198287964 2.967228889465332 4.701899528503418
Loss :  1.8136664628982544 4.32228422164917 6.135950565338135
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8088794946670532 4.271601676940918 6.080481052398682
Loss :  1.8102456331253052 4.249056816101074 6.05930233001709
Loss :  1.8123947381973267 4.0428242683410645 5.855218887329102
Total LOSS train 4.518791781938993 valid 6.032738208770752
CE LOSS train 1.7350646147361168 valid 0.45309868454933167
Contrastive LOSS train 2.7837271653688873 valid 1.0107060670852661
EPOCH 180:
Loss :  1.7398059368133545 3.1448581218719482 4.884664058685303
Loss :  1.737015724182129 3.256395101547241 4.993411064147949
Loss :  1.7357522249221802 3.3021860122680664 5.037938117980957
Loss :  1.7401193380355835 2.856424570083618 4.596543788909912
Loss :  1.7416962385177612 3.0274996757507324 4.769196033477783
Loss :  1.7402602434158325 2.4628050327301025 4.203065395355225
Loss :  1.7395390272140503 3.0899252891540527 4.829464435577393
Loss :  1.7377293109893799 3.658217191696167 5.395946502685547
Loss :  1.739237666130066 2.843355417251587 4.582592964172363
Loss :  1.7285146713256836 2.736778974533081 4.465293884277344
Loss :  1.738773226737976 3.2860488891601562 5.024822235107422
Loss :  1.7430912256240845 3.0662801265716553 4.809371471405029
Loss :  1.7389116287231445 2.9523003101348877 4.691211700439453
Loss :  1.7349213361740112 2.542128801345825 4.277050018310547
Loss :  1.7402368783950806 2.649744987487793 4.389981746673584
Loss :  1.7317125797271729 2.221635341644287 3.95334792137146
Loss :  1.7371095418930054 2.409165143966675 4.146274566650391
Loss :  1.7347742319107056 1.826494574546814 3.5612688064575195
Loss :  1.7325114011764526 1.600521206855774 3.3330326080322266
Loss :  1.7304141521453857 2.247863292694092 3.9782774448394775
  batch 20 loss: 1.7304141521453857, 2.247863292694092, 3.9782774448394775
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7326358556747437 2.3104586601257324 4.043094635009766
Loss :  1.7346768379211426 2.8478283882141113 4.582505226135254
Loss :  1.7362455129623413 2.536456346511841 4.272701740264893
Loss :  1.7398185729980469 2.3203213214874268 4.0601396560668945
Loss :  1.7342206239700317 2.333423376083374 4.067644119262695
Loss :  1.731203556060791 2.5376102924346924 4.2688140869140625
Loss :  1.7389888763427734 2.2820520401000977 4.021040916442871
Loss :  1.7309684753417969 2.634146213531494 4.365114688873291
Loss :  1.737455129623413 2.0550832748413086 3.7925384044647217
Loss :  1.7306811809539795 2.711402416229248 4.442083358764648
Loss :  1.7430795431137085 2.3664727210998535 4.109552383422852
Loss :  1.7389037609100342 2.7493155002593994 4.488219261169434
Loss :  1.733797311782837 3.048252582550049 4.782050132751465
Loss :  1.7331018447875977 2.5823490619659424 4.315450668334961
Loss :  1.7402735948562622 2.6714391708374023 4.411712646484375
Loss :  1.7404444217681885 2.6993534564971924 4.439797878265381
Loss :  1.7365081310272217 3.1816000938415527 4.918107986450195
Loss :  1.7346818447113037 3.343907594680786 5.07858943939209
Loss :  1.7352371215820312 2.7475600242614746 4.482797145843506
Loss :  1.7373275756835938 2.8974502086639404 4.634778022766113
  batch 40 loss: 1.7373275756835938, 2.8974502086639404, 4.634778022766113
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.735516905784607 2.974529266357422 4.710046291351318
Loss :  1.7371165752410889 2.590906858444214 4.328023433685303
Loss :  1.7393234968185425 2.406740427017212 4.146063804626465
Loss :  1.73823881149292 2.418914318084717 4.157153129577637
Loss :  1.7402887344360352 2.5844998359680176 4.324788570404053
Loss :  1.7372212409973145 2.509157419204712 4.2463788986206055
Loss :  1.7372790575027466 2.1304986476898193 3.8677778244018555
Loss :  1.7382384538650513 2.355340003967285 4.093578338623047
Loss :  1.7405472993850708 3.206608533859253 4.947155952453613
Loss :  1.737609624862671 2.073371648788452 3.810981273651123
Loss :  1.734670639038086 2.9855051040649414 4.720175743103027
Loss :  1.7374982833862305 2.0454177856445312 3.7829160690307617
Loss :  1.7423322200775146 2.7050323486328125 4.447364807128906
Loss :  1.736581802368164 2.3202574253082275 4.0568389892578125
Loss :  1.737545371055603 2.2699127197265625 4.007458209991455
Loss :  1.7352544069290161 2.523690938949585 4.258945465087891
Loss :  1.7380136251449585 3.222653865814209 4.960667610168457
Loss :  1.7417898178100586 2.2150065898895264 3.956796407699585
Loss :  1.739837884902954 3.3790833950042725 5.118921279907227
Loss :  1.7362618446350098 3.1786067485809326 4.914868354797363
  batch 60 loss: 1.7362618446350098, 3.1786067485809326, 4.914868354797363
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7372395992279053 2.03263258934021 3.7698721885681152
Loss :  1.7370048761367798 2.383040428161621 4.120045185089111
Loss :  1.7384477853775024 2.57621169090271 4.314659595489502
Loss :  1.7323510646820068 2.6084489822387695 4.3408002853393555
Loss :  1.7316412925720215 1.9619431495666504 3.693584442138672
Loss :  1.8327546119689941 4.419034481048584 6.251789093017578
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 5], device='cuda:0')
Loss :  1.82448148727417 4.407465934753418 6.231947422027588
Loss :  1.8291124105453491 4.269444942474365 6.098557472229004
Loss :  1.8260395526885986 4.242849349975586 6.0688886642456055
Total LOSS train 4.378389989412748 valid 6.162795662879944
CE LOSS train 1.7369265703054575 valid 0.45650988817214966
Contrastive LOSS train 2.641463408103356 valid 1.0607123374938965
EPOCH 181:
Loss :  1.7360097169876099 2.2686705589294434 4.004680156707764
Loss :  1.7352217435836792 3.1298794746398926 4.865101337432861
Loss :  1.7313581705093384 2.720770835876465 4.452128887176514
Loss :  1.7331715822219849 2.815849542617798 4.549021244049072
Loss :  1.7371643781661987 2.256531238555908 3.9936957359313965
Loss :  1.733964443206787 2.134669542312622 3.868633985519409
Loss :  1.7356948852539062 1.9869197607040405 3.7226147651672363
Loss :  1.7319331169128418 1.7007097005844116 3.432642936706543
Loss :  1.7313213348388672 2.4394171237945557 4.170738220214844
Loss :  1.7203145027160645 1.9406428337097168 3.6609573364257812
Loss :  1.7326194047927856 2.107722759246826 3.8403420448303223
Loss :  1.740112066268921 2.3852086067199707 4.1253204345703125
Loss :  1.7338956594467163 1.8392664194107056 3.573162078857422
Loss :  1.731396198272705 2.337963819503784 4.06935977935791
Loss :  1.7367364168167114 2.4230103492736816 4.1597466468811035
Loss :  1.729028344154358 2.405790090560913 4.1348185539245605
Loss :  1.7359625101089478 2.6481735706329346 4.384136199951172
Loss :  1.7333579063415527 3.262974977493286 4.996333122253418
Loss :  1.7334266901016235 2.455207109451294 4.188633918762207
Loss :  1.7320095300674438 2.724102020263672 4.456111431121826
  batch 20 loss: 1.7320095300674438, 2.724102020263672, 4.456111431121826
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7340302467346191 1.9721356630325317 3.7061657905578613
Loss :  1.7363646030426025 2.6839630603790283 4.420327663421631
Loss :  1.7370578050613403 3.134056329727173 4.871114253997803
Loss :  1.7410067319869995 3.1803934574127197 4.92140007019043
Loss :  1.7347248792648315 2.5141286849975586 4.24885368347168
Loss :  1.730813980102539 2.224116325378418 3.954930305480957
Loss :  1.738930106163025 2.3925931453704834 4.131523132324219
Loss :  1.7309517860412598 2.7758877277374268 4.506839752197266
Loss :  1.7362914085388184 3.6812849044799805 5.417576313018799
Loss :  1.7289345264434814 4.0720062255859375 5.80094051361084
Loss :  1.7413842678070068 3.8057851791381836 5.5471696853637695
Loss :  1.7361725568771362 2.1613383293151855 3.8975110054016113
Loss :  1.7288436889648438 2.1139440536499023 3.842787742614746
Loss :  1.727461576461792 2.1835110187530518 3.9109725952148438
Loss :  1.735602855682373 2.7506654262542725 4.486268043518066
Loss :  1.7350802421569824 2.4706008434295654 4.205680847167969
Loss :  1.7323710918426514 2.542447328567505 4.274818420410156
Loss :  1.7299126386642456 2.9581189155578613 4.6880316734313965
Loss :  1.730379343032837 2.883039712905884 4.613419055938721
Loss :  1.7320902347564697 2.843425750732422 4.5755157470703125
  batch 40 loss: 1.7320902347564697, 2.843425750732422, 4.5755157470703125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.731290340423584 2.6544063091278076 4.3856964111328125
Loss :  1.7283744812011719 1.643156886100769 3.3715314865112305
Loss :  1.7309736013412476 2.1138031482696533 3.8447766304016113
Loss :  1.731011986732483 2.1504619121551514 3.881474018096924
Loss :  1.7341090440750122 2.010836362838745 3.744945526123047
Loss :  1.7302502393722534 1.9894282817840576 3.7196784019470215
Loss :  1.7305976152420044 1.882279872894287 3.612877368927002
Loss :  1.7306350469589233 1.722274899482727 3.4529099464416504
Loss :  1.7323564291000366 1.867578387260437 3.5999348163604736
Loss :  1.728969931602478 1.9367715120315552 3.665741443634033
Loss :  1.724995493888855 2.125189781188965 3.8501853942871094
Loss :  1.730150818824768 2.191281318664551 3.9214320182800293
Loss :  1.7363364696502686 2.690122365951538 4.426458835601807
Loss :  1.7282578945159912 2.516979217529297 4.245237350463867
Loss :  1.7301080226898193 2.3112058639526367 4.041314125061035
Loss :  1.7271803617477417 2.1782455444335938 3.905426025390625
Loss :  1.7313287258148193 2.279561758041382 4.010890483856201
Loss :  1.7354638576507568 1.9066561460494995 3.642119884490967
Loss :  1.7345479726791382 2.661320686340332 4.39586877822876
Loss :  1.7300935983657837 2.6928398609161377 4.422933578491211
  batch 60 loss: 1.7300935983657837, 2.6928398609161377, 4.422933578491211
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7314207553863525 2.617366313934326 4.348787307739258
Loss :  1.7335450649261475 2.5517518520355225 4.28529691696167
Loss :  1.7335479259490967 2.4718668460845947 4.205414772033691
Loss :  1.7270441055297852 2.950488328933716 4.677532196044922
Loss :  1.7268036603927612 2.29776930809021 4.024572849273682
Loss :  1.8176956176757812 4.393906116485596 6.211601734161377
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8120390176773071 4.40047550201416 6.212514400482178
Loss :  1.81460702419281 4.269120693206787 6.083727836608887
Loss :  1.8126752376556396 4.10856294631958 5.921237945556641
Total LOSS train 4.190047102708083 valid 6.1072704792022705
CE LOSS train 1.7324999479147105 valid 0.4531688094139099
Contrastive LOSS train 2.4575471566273617 valid 1.027140736579895
EPOCH 182:
Loss :  1.7319406270980835 2.7007713317871094 4.432712078094482
Loss :  1.7300677299499512 3.2106339931488037 4.940701484680176
Loss :  1.7279534339904785 2.850095272064209 4.5780487060546875
Loss :  1.7317370176315308 2.1285340785980225 3.8602709770202637
Loss :  1.7331771850585938 2.001654863357544 3.7348320484161377
Loss :  1.731722354888916 1.8985835313796997 3.630305767059326
Loss :  1.7313575744628906 2.352583169937134 4.083940505981445
Loss :  1.7278474569320679 3.2791011333465576 5.006948471069336
Loss :  1.7303053140640259 2.135329008102417 3.8656344413757324
Loss :  1.7174098491668701 2.012990951538086 3.730400800704956
Loss :  1.7298743724822998 2.4323525428771973 4.162226676940918
Loss :  1.7402963638305664 2.8999831676483154 4.640279769897461
Loss :  1.7314749956130981 2.3216309547424316 4.05310583114624
Loss :  1.7286772727966309 2.521972894668579 4.250650405883789
Loss :  1.733325481414795 2.7503855228424072 4.483711242675781
Loss :  1.7257037162780762 2.4100091457366943 4.135712623596191
Loss :  1.7315558195114136 1.8118270635604858 3.5433828830718994
Loss :  1.7283285856246948 2.1022844314575195 3.830613136291504
Loss :  1.7309101819992065 2.164429187774658 3.8953394889831543
Loss :  1.7253836393356323 2.637681007385254 4.363064765930176
  batch 20 loss: 1.7253836393356323, 2.637681007385254, 4.363064765930176
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7285677194595337 2.687596559524536 4.416164398193359
Loss :  1.7316871881484985 3.0969197750091553 4.828607082366943
Loss :  1.7306114435195923 2.6977226734161377 4.4283342361450195
Loss :  1.7380567789077759 2.5386769771575928 4.276733875274658
Loss :  1.7329117059707642 3.2755212783813477 5.008432865142822
Loss :  1.7256784439086914 3.162848949432373 4.8885273933410645
Loss :  1.7346397638320923 2.2066123485565186 3.9412522315979004
Loss :  1.725164771080017 2.1062819957733154 3.831446647644043
Loss :  1.732290267944336 3.1409196853637695 4.8732099533081055
Loss :  1.7255862951278687 2.7947170734405518 4.520303249359131
Loss :  1.7397794723510742 2.87247633934021 4.612256050109863
Loss :  1.7340131998062134 2.8640477657318115 4.5980610847473145
Loss :  1.7262446880340576 3.0071983337402344 4.733443260192871
Loss :  1.7257006168365479 3.1512389183044434 4.87693977355957
Loss :  1.7338311672210693 3.549264430999756 5.283095359802246
Loss :  1.7333530187606812 3.2354214191436768 4.968774318695068
Loss :  1.7309921979904175 2.286827564239502 4.017819881439209
Loss :  1.7261030673980713 2.487549304962158 4.213652610778809
Loss :  1.728892207145691 2.3241584300994873 4.053050518035889
Loss :  1.7308142185211182 2.5193679332733154 4.250182151794434
  batch 40 loss: 1.7308142185211182, 2.5193679332733154, 4.250182151794434
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.72873055934906 2.93325138092041 4.66198205947876
Loss :  1.7261890172958374 2.1241843700408936 3.8503732681274414
Loss :  1.7296167612075806 2.364980936050415 4.094597816467285
Loss :  1.7281084060668945 2.4167134761810303 4.144822120666504
Loss :  1.7322592735290527 2.1944711208343506 3.9267303943634033
Loss :  1.7293879985809326 3.064330816268921 4.7937188148498535
Loss :  1.7280601263046265 2.6382765769958496 4.366336822509766
Loss :  1.7296206951141357 2.834655284881592 4.564275741577148
Loss :  1.7310974597930908 2.922203302383423 4.653300762176514
Loss :  1.727696180343628 3.2955729961395264 5.023269176483154
Loss :  1.7241379022598267 2.6616461277008057 4.385784149169922
Loss :  1.7284619808197021 2.067981004714966 3.796442985534668
Loss :  1.733595848083496 1.5341380834579468 3.2677340507507324
Loss :  1.7265175580978394 1.7457325458526611 3.472249984741211
Loss :  1.7287514209747314 3.039700984954834 4.7684526443481445
Loss :  1.723915934562683 2.943565607070923 4.667481422424316
Loss :  1.7300190925598145 3.1109845638275146 4.84100341796875
Loss :  1.7337275743484497 2.8146162033081055 4.548343658447266
Loss :  1.7335104942321777 3.1479554176330566 4.881465911865234
Loss :  1.7303093671798706 2.9577972888946533 4.688106536865234
  batch 60 loss: 1.7303093671798706, 2.9577972888946533, 4.688106536865234
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7294132709503174 3.229793071746826 4.959206581115723
Loss :  1.7320319414138794 2.8048465251922607 4.53687858581543
Loss :  1.732193946838379 2.441211462020874 4.173405647277832
Loss :  1.7262897491455078 2.683472156524658 4.409761905670166
Loss :  1.7263729572296143 1.6364202499389648 3.362793207168579
Loss :  1.804937720298767 4.3113274574279785 6.116265296936035
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.799822211265564 4.434615612030029 6.234437942504883
Loss :  1.8026819229125977 4.156181812286377 5.958863735198975
Loss :  1.8004915714263916 4.332010269165039 6.132501602172852
Total LOSS train 4.348933641727154 valid 6.110517144203186
CE LOSS train 1.7299074264673087 valid 0.4501228928565979
Contrastive LOSS train 2.6190261932519765 valid 1.0830025672912598
EPOCH 183:
Loss :  1.7306396961212158 2.868880271911621 4.599519729614258
Loss :  1.7288447618484497 2.9852616786956787 4.714106559753418
Loss :  1.7264097929000854 2.528825521469116 4.255235195159912
Loss :  1.729906678199768 2.394030809402466 4.123937606811523
Loss :  1.7325485944747925 1.7245631217956543 3.4571118354797363
Loss :  1.73069429397583 2.567255735397339 4.29794979095459
Loss :  1.7308599948883057 2.8889083862304688 4.619768142700195
Loss :  1.7283436059951782 2.4761388301849365 4.204482555389404
Loss :  1.7302500009536743 2.2666218280792236 3.9968719482421875
Loss :  1.7169159650802612 2.165147542953491 3.882063388824463
Loss :  1.7300560474395752 2.1300268173217773 3.8600828647613525
Loss :  1.7404900789260864 1.8449244499206543 3.585414409637451
Loss :  1.7312809228897095 2.134699821472168 3.865980625152588
Loss :  1.7292330265045166 2.5821964740753174 4.311429500579834
Loss :  1.7337089776992798 2.8332159519195557 4.566925048828125
Loss :  1.7267837524414062 3.164259672164917 4.891043663024902
Loss :  1.7326221466064453 2.5213310718536377 4.253952980041504
Loss :  1.728187918663025 2.5480246543884277 4.276212692260742
Loss :  1.7299220561981201 1.9859983921051025 3.7159204483032227
Loss :  1.7260059118270874 1.7176581621170044 3.443664073944092
  batch 20 loss: 1.7260059118270874, 1.7176581621170044, 3.443664073944092
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7277456521987915 2.162818193435669 3.89056396484375
Loss :  1.7308282852172852 1.8213016986846924 3.5521299839019775
Loss :  1.7297191619873047 2.0972585678100586 3.8269777297973633
Loss :  1.7371149063110352 2.404869318008423 4.141983985900879
Loss :  1.7309370040893555 2.035081148147583 3.7660181522369385
Loss :  1.7249205112457275 2.5589568614959717 4.283877372741699
Loss :  1.7346687316894531 2.5896072387695312 4.324275970458984
Loss :  1.7246932983398438 2.184260845184326 3.90895414352417
Loss :  1.7321408987045288 2.5337300300598145 4.265871047973633
Loss :  1.7236459255218506 2.838500499725342 4.562146186828613
Loss :  1.7397598028182983 2.322666883468628 4.062426567077637
Loss :  1.7318005561828613 2.2297093868255615 3.961509943008423
Loss :  1.7249693870544434 2.1815731525421143 3.9065425395965576
Loss :  1.7249232530593872 2.6115028858184814 4.336426258087158
Loss :  1.733721137046814 2.6967642307281494 4.430485248565674
Loss :  1.733688473701477 2.1114370822906494 3.845125675201416
Loss :  1.7306019067764282 2.4446001052856445 4.175201892852783
Loss :  1.7257477045059204 2.2133123874664307 3.9390602111816406
Loss :  1.7282564640045166 2.596662759780884 4.3249192237854
Loss :  1.7306402921676636 2.429539918899536 4.16018009185791
  batch 40 loss: 1.7306402921676636, 2.429539918899536, 4.16018009185791
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.728017807006836 2.736365795135498 4.464383602142334
Loss :  1.7262126207351685 2.140652656555176 3.8668651580810547
Loss :  1.7297381162643433 1.7911477088928223 3.520885944366455
Loss :  1.7274194955825806 2.3071281909942627 4.034547805786133
Loss :  1.7314064502716064 2.1538045406341553 3.8852109909057617
Loss :  1.7277038097381592 2.409919023513794 4.137622833251953
Loss :  1.7269072532653809 2.5566565990448 4.283563613891602
Loss :  1.7289327383041382 2.458014965057373 4.186947822570801
Loss :  1.7301735877990723 2.70098614692688 4.431159973144531
Loss :  1.7276891469955444 2.552448034286499 4.280137062072754
Loss :  1.7246334552764893 2.5141191482543945 4.238752365112305
Loss :  1.7290551662445068 2.341571092605591 4.070626258850098
Loss :  1.7347208261489868 2.17606782913208 3.9107885360717773
Loss :  1.7275619506835938 2.6531577110290527 4.3807196617126465
Loss :  1.7298895120620728 2.3974106311798096 4.127300262451172
Loss :  1.7257089614868164 2.95735239982605 4.683061599731445
Loss :  1.7324769496917725 2.126117467880249 3.8585944175720215
Loss :  1.7350049018859863 2.1248672008514404 3.8598721027374268
Loss :  1.735718011856079 2.852241039276123 4.587959289550781
Loss :  1.7325338125228882 2.5753495693206787 4.307883262634277
  batch 60 loss: 1.7325338125228882, 2.5753495693206787, 4.307883262634277
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7313522100448608 2.4428515434265137 4.174203872680664
Loss :  1.7333871126174927 2.037728786468506 3.771115779876709
Loss :  1.7328609228134155 1.803062915802002 3.535923957824707
Loss :  1.727407455444336 2.5065126419067383 4.233920097351074
Loss :  1.7263257503509521 1.4584393501281738 3.184765100479126
Loss :  1.8106404542922974 4.3346052169799805 6.145245552062988
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.806782841682434 4.390771389007568 6.197554111480713
Loss :  1.8083906173706055 4.265161037445068 6.073551654815674
Loss :  1.8061045408248901 4.199517726898193 6.005622386932373
Total LOSS train 4.101587086457473 valid 6.105493426322937
CE LOSS train 1.7298010092515212 valid 0.45152613520622253
Contrastive LOSS train 2.3717860827079185 valid 1.0498794317245483
EPOCH 184:
Loss :  1.7312382459640503 1.8046740293502808 3.535912275314331
Loss :  1.7288203239440918 2.4710745811462402 4.199894905090332
Loss :  1.7261898517608643 1.7926925420761108 3.5188822746276855
Loss :  1.7301585674285889 2.2785375118255615 4.00869607925415
Loss :  1.7321317195892334 2.860142707824707 4.5922746658325195
Loss :  1.7299383878707886 2.1857693195343018 3.915707588195801
Loss :  1.7308765649795532 2.4427077770233154 4.173584461212158
Loss :  1.7268617153167725 2.3436391353607178 4.07050085067749
Loss :  1.7296401262283325 2.4176862239837646 4.147326469421387
Loss :  1.7172538042068481 2.7442851066589355 4.461538791656494
Loss :  1.7283896207809448 1.9040272235870361 3.6324167251586914
Loss :  1.7386271953582764 2.256831169128418 3.9954583644866943
Loss :  1.73043692111969 2.1442136764526367 3.874650478363037
Loss :  1.728956937789917 2.0354526042938232 3.7644095420837402
Loss :  1.7335574626922607 2.6155714988708496 4.349128723144531
Loss :  1.7273298501968384 3.0048184394836426 4.732148170471191
Loss :  1.7320891618728638 2.7285537719726562 4.4606428146362305
Loss :  1.7291675806045532 2.4513885974884033 4.180556297302246
Loss :  1.7307181358337402 2.1040420532226562 3.8347601890563965
Loss :  1.7269747257232666 2.325536012649536 4.052510738372803
  batch 20 loss: 1.7269747257232666, 2.325536012649536, 4.052510738372803
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7296655178070068 1.7132107019424438 3.4428763389587402
Loss :  1.7321913242340088 2.196589469909668 3.9287807941436768
Loss :  1.7326545715332031 3.0808255672454834 4.813480377197266
Loss :  1.7381247282028198 2.51177978515625 4.249904632568359
Loss :  1.733017086982727 2.3523662090301514 4.085383415222168
Loss :  1.7275028228759766 2.6157379150390625 4.343240737915039
Loss :  1.7356096506118774 2.4423980712890625 4.17800760269165
Loss :  1.7262670993804932 1.6627482175827026 3.3890151977539062
Loss :  1.7335516214370728 1.6155076026916504 3.3490591049194336
Loss :  1.7270835638046265 2.7800681591033936 4.5071516036987305
Loss :  1.7415635585784912 2.7137327194213867 4.455296516418457
Loss :  1.7332556247711182 2.172149896621704 3.9054055213928223
Loss :  1.7269631624221802 2.517716884613037 4.244679927825928
Loss :  1.7270485162734985 1.9172394275665283 3.6442880630493164
Loss :  1.7348308563232422 2.3007700443267822 4.035600662231445
Loss :  1.7350436449050903 2.2178685665130615 3.9529123306274414
Loss :  1.7317005395889282 3.2979331016540527 5.029633522033691
Loss :  1.726830005645752 2.115262508392334 3.842092514038086
Loss :  1.7301934957504272 1.6745145320892334 3.404707908630371
Loss :  1.7314268350601196 1.8460562229156494 3.5774831771850586
  batch 40 loss: 1.7314268350601196, 1.8460562229156494, 3.5774831771850586
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7292282581329346 2.443058490753174 4.1722869873046875
Loss :  1.7281055450439453 1.9174401760101318 3.645545721054077
Loss :  1.730289101600647 2.162217140197754 3.8925061225891113
Loss :  1.7292234897613525 3.366344451904297 5.09556770324707
Loss :  1.732418179512024 3.2131383419036865 4.945556640625
Loss :  1.72915780544281 3.176126718521118 4.905284404754639
Loss :  1.727611780166626 2.1914072036743164 3.9190189838409424
Loss :  1.7287431955337524 1.8242253065109253 3.5529685020446777
Loss :  1.7306780815124512 1.8519505262374878 3.5826287269592285
Loss :  1.72718346118927 1.7596580982208252 3.4868416786193848
Loss :  1.7248512506484985 1.5638400316238403 3.288691282272339
Loss :  1.72940993309021 1.8034961223602295 3.5329060554504395
Loss :  1.7349722385406494 1.6442482471466064 3.379220485687256
Loss :  1.7271780967712402 2.1054277420043945 3.8326058387756348
Loss :  1.729828953742981 3.1952030658721924 4.925032138824463
Loss :  1.7259935140609741 2.245333433151245 3.9713268280029297
Loss :  1.7327592372894287 1.7965489625930786 3.529308319091797
Loss :  1.735184669494629 1.590962290763855 3.3261470794677734
Loss :  1.7351794242858887 2.5287530422210693 4.263932228088379
Loss :  1.7316429615020752 2.0734755992889404 3.8051185607910156
  batch 60 loss: 1.7316429615020752, 2.0734755992889404, 3.8051185607910156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7315609455108643 2.3570618629455566 4.088623046875
Loss :  1.7325701713562012 2.182544231414795 3.915114402770996
Loss :  1.7338260412216187 1.8395358324050903 3.573361873626709
Loss :  1.727062463760376 2.6621694564819336 4.3892316818237305
Loss :  1.7261906862258911 1.6979955434799194 3.4241862297058105
Loss :  1.8105791807174683 4.365169048309326 6.175748348236084
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8058578968048096 4.356379508972168 6.162237167358398
Loss :  1.8084357976913452 4.236283302307129 6.044719219207764
Loss :  1.8056771755218506 4.240031719207764 6.045708656311035
Total LOSS train 4.004969413463886 valid 6.10710334777832
CE LOSS train 1.730380470936115 valid 0.45141929388046265
Contrastive LOSS train 2.274588946195749 valid 1.060007929801941
EPOCH 185:
Loss :  1.7307318449020386 2.633387565612793 4.364119529724121
Loss :  1.7290195226669312 2.4108474254608154 4.139866828918457
Loss :  1.7262195348739624 2.0228185653686523 3.7490382194519043
Loss :  1.729349136352539 2.230215311050415 3.959564447402954
Loss :  1.733376383781433 2.2202980518341064 3.95367431640625
Loss :  1.7303212881088257 1.8148664236068726 3.5451877117156982
Loss :  1.7315016984939575 1.9106605052947998 3.642162322998047
Loss :  1.7280982732772827 2.109204053878784 3.8373022079467773
Loss :  1.7294343709945679 2.292941093444824 4.022375583648682
Loss :  1.7172248363494873 2.4456660747528076 4.162890911102295
Loss :  1.729376196861267 2.3387560844421387 4.068132400512695
Loss :  1.738347053527832 2.319488286972046 4.057835578918457
Loss :  1.730900764465332 2.835463285446167 4.566364288330078
Loss :  1.7282456159591675 2.4191665649414062 4.147412300109863
Loss :  1.7332357168197632 2.2084028720855713 3.941638469696045
Loss :  1.7264894247055054 2.0273897647857666 3.7538790702819824
Loss :  1.7313382625579834 2.263648271560669 3.9949865341186523
Loss :  1.7288237810134888 2.138096809387207 3.8669204711914062
Loss :  1.7288291454315186 1.7409530878067017 3.4697823524475098
Loss :  1.725785732269287 1.9240813255310059 3.649867057800293
  batch 20 loss: 1.725785732269287, 1.9240813255310059, 3.649867057800293
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.728608250617981 1.824388027191162 3.5529961585998535
Loss :  1.7303787469863892 2.207689046859741 3.93806791305542
Loss :  1.7317694425582886 2.483931064605713 4.215700626373291
Loss :  1.7373355627059937 2.1201088428497314 3.8574442863464355
Loss :  1.7322750091552734 2.0741095542907715 3.806384563446045
Loss :  1.726351022720337 1.7690844535827637 3.4954354763031006
Loss :  1.735115647315979 1.7656222581863403 3.5007379055023193
Loss :  1.7254475355148315 2.0123226642608643 3.7377700805664062
Loss :  1.7327626943588257 1.9613040685653687 3.6940667629241943
Loss :  1.7237869501113892 2.186994791030884 3.9107818603515625
Loss :  1.7408713102340698 2.8046364784240723 4.545507907867432
Loss :  1.732793927192688 3.2064125537872314 4.939206600189209
Loss :  1.7260065078735352 2.6433005332946777 4.369307041168213
Loss :  1.7261813879013062 2.8306052684783936 4.55678653717041
Loss :  1.7348604202270508 3.6855900287628174 5.420450210571289
Loss :  1.734384536743164 2.509657621383667 4.24404239654541
Loss :  1.7311922311782837 3.0829432010650635 4.814135551452637
Loss :  1.7260968685150146 2.7851295471191406 4.511226654052734
Loss :  1.7296254634857178 2.1138410568237305 3.8434665203094482
Loss :  1.7307045459747314 2.5545835494995117 4.285287857055664
  batch 40 loss: 1.7307045459747314, 2.5545835494995117, 4.285287857055664
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7289694547653198 2.874591112136841 4.603560447692871
Loss :  1.7278051376342773 1.5239920616149902 3.2517971992492676
Loss :  1.729690670967102 1.550499439239502 3.2801899909973145
Loss :  1.7292838096618652 2.0290350914001465 3.7583189010620117
Loss :  1.7322684526443481 2.0262434482574463 3.758512020111084
Loss :  1.7279096841812134 2.346998453140259 4.074908256530762
Loss :  1.7278825044631958 2.124027729034424 3.85191011428833
Loss :  1.7300264835357666 1.957452416419983 3.687479019165039
Loss :  1.7304071187973022 2.305108070373535 4.035515308380127
Loss :  1.7286964654922485 1.788323998451233 3.5170204639434814
Loss :  1.7248870134353638 2.7422902584075928 4.467177391052246
Loss :  1.7293446063995361 2.8228790760040283 4.5522236824035645
Loss :  1.7351583242416382 3.641453266143799 5.376611709594727
Loss :  1.7271426916122437 2.7121269702911377 4.439269542694092
Loss :  1.7293365001678467 2.543553590774536 4.272890090942383
Loss :  1.7252968549728394 3.9804189205169678 5.705715656280518
Loss :  1.7314201593399048 3.196105718612671 4.927525997161865
Loss :  1.7345147132873535 3.457913637161255 5.1924285888671875
Loss :  1.733988642692566 3.2816314697265625 5.015620231628418
Loss :  1.7312089204788208 2.271312713623047 4.002521514892578
  batch 60 loss: 1.7312089204788208, 2.271312713623047, 4.002521514892578
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7301167249679565 2.2242588996887207 3.954375743865967
Loss :  1.7322396039962769 1.8741275072097778 3.6063671112060547
Loss :  1.7326689958572388 2.6770474910736084 4.409716606140137
Loss :  1.726058006286621 2.6533262729644775 4.3793840408325195
Loss :  1.725472331047058 1.8698054552078247 3.595277786254883
Loss :  1.8086355924606323 4.377048015594482 6.185683727264404
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8036600351333618 4.370384216308594 6.174044132232666
Loss :  1.8064203262329102 4.258300304412842 6.064720630645752
Loss :  1.8041727542877197 4.259144306182861 6.06331729888916
Total LOSS train 4.120740321966318 valid 6.121941447257996
CE LOSS train 1.7299229310109066 valid 0.45104318857192993
Contrastive LOSS train 2.390817370781532 valid 1.0647860765457153
EPOCH 186:
Loss :  1.7307013273239136 1.6575061082839966 3.38820743560791
Loss :  1.7299453020095825 2.1999852657318115 3.9299306869506836
Loss :  1.7268067598342896 1.5899579524993896 3.3167648315429688
Loss :  1.72996985912323 1.9281092882156372 3.658079147338867
Loss :  1.7336173057556152 1.7510302066802979 3.484647512435913
Loss :  1.7309883832931519 1.377066969871521 3.108055353164673
Loss :  1.7324366569519043 2.291538953781128 4.023975372314453
Loss :  1.729402780532837 2.589616537094116 4.319019317626953
Loss :  1.7310125827789307 2.1334986686706543 3.864511251449585
Loss :  1.7191122770309448 2.027235746383667 3.7463479042053223
Loss :  1.7317771911621094 2.912020444869995 4.643797874450684
Loss :  1.739420771598816 2.902604103088379 4.642024993896484
Loss :  1.7327772378921509 1.966382384300232 3.699159622192383
Loss :  1.7305694818496704 2.403770685195923 4.134340286254883
Loss :  1.735548734664917 2.240485191345215 3.976033926010132
Loss :  1.727688193321228 2.6352009773254395 4.362889289855957
Loss :  1.7328882217407227 2.797290563583374 4.530179023742676
Loss :  1.7296804189682007 2.6097822189331055 4.339462757110596
Loss :  1.7293791770935059 2.080329418182373 3.809708595275879
Loss :  1.7270349264144897 1.676159381866455 3.4031944274902344
  batch 20 loss: 1.7270349264144897, 1.676159381866455, 3.4031944274902344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7293707132339478 1.7246671915054321 3.45403790473938
Loss :  1.7305693626403809 1.848164677619934 3.5787339210510254
Loss :  1.7316197156906128 2.632420539855957 4.364040374755859
Loss :  1.7367624044418335 2.440697431564331 4.177459716796875
Loss :  1.7317277193069458 2.7297921180725098 4.461519718170166
Loss :  1.7263284921646118 1.799553394317627 3.525881767272949
Loss :  1.7352020740509033 2.433090925216675 4.168292999267578
Loss :  1.725666880607605 3.520289182662964 5.245955944061279
Loss :  1.732764720916748 1.9637809991836548 3.6965456008911133
Loss :  1.724086880683899 2.466726064682007 4.190813064575195
Loss :  1.7399197816848755 2.6066415309906006 4.346561431884766
Loss :  1.7329260110855103 1.9852638244628906 3.7181897163391113
Loss :  1.725881576538086 2.3935699462890625 4.119451522827148
Loss :  1.7259738445281982 3.0279524326324463 4.7539262771606445
Loss :  1.7343882322311401 3.0358550548553467 4.770243167877197
Loss :  1.7341455221176147 2.902400493621826 4.6365461349487305
Loss :  1.7312153577804565 3.053156614303589 4.784371852874756
Loss :  1.7270935773849487 2.2865257263183594 4.013619422912598
Loss :  1.7290009260177612 1.7444241046905518 3.4734249114990234
Loss :  1.7315196990966797 1.6480991840362549 3.3796188831329346
  batch 40 loss: 1.7315196990966797, 1.6480991840362549, 3.3796188831329346
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7291427850723267 2.6945531368255615 4.423696041107178
Loss :  1.727783203125 2.292860746383667 4.020644187927246
Loss :  1.7310023307800293 2.79677152633667 4.527773857116699
Loss :  1.7291508913040161 2.10672926902771 3.8358802795410156
Loss :  1.7324570417404175 2.422607183456421 4.155064105987549
Loss :  1.7293884754180908 3.2096548080444336 4.939043045043945
Loss :  1.72822105884552 2.3532323837280273 4.081453323364258
Loss :  1.7294480800628662 2.2409257888793945 3.9703738689422607
Loss :  1.731858730316162 2.50130295753479 4.233161926269531
Loss :  1.727597951889038 2.778467893600464 4.506065845489502
Loss :  1.7252222299575806 2.8325533866882324 4.557775497436523
Loss :  1.7300316095352173 2.147451639175415 3.877483367919922
Loss :  1.7356257438659668 3.117068290710449 4.852694034576416
Loss :  1.7274901866912842 2.5288920402526855 4.256381988525391
Loss :  1.729743480682373 2.733388662338257 4.463131904602051
Loss :  1.7265617847442627 2.4003031253814697 4.126864910125732
Loss :  1.7321583032608032 1.8901056051254272 3.6222639083862305
Loss :  1.734431266784668 1.7829391956329346 3.5173704624176025
Loss :  1.734907627105713 2.443145275115967 4.17805290222168
Loss :  1.7315372228622437 3.158109188079834 4.889646530151367
  batch 60 loss: 1.7315372228622437, 3.158109188079834, 4.889646530151367
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7312933206558228 2.584336996078491 4.3156304359436035
Loss :  1.733304738998413 2.468855857849121 4.202160835266113
Loss :  1.7338311672210693 2.6766164302825928 4.410447597503662
Loss :  1.7283833026885986 2.5264370441436768 4.254820346832275
Loss :  1.7276785373687744 1.6672440767288208 3.3949227333068848
Loss :  1.8239424228668213 4.428350448608398 6.252292633056641
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 5], device='cuda:0')
Loss :  1.8167622089385986 4.401933193206787 6.218695640563965
Loss :  1.8209013938903809 4.308845043182373 6.129746437072754
Loss :  1.8154653310775757 4.2795538902282715 6.095019340515137
Total LOSS train 4.105421044276311 valid 6.173938512802124
CE LOSS train 1.7305411100387573 valid 0.4538663327693939
Contrastive LOSS train 2.3748799232336193 valid 1.0698884725570679
EPOCH 187:
Loss :  1.7319837808609009 2.4315431118011475 4.163527011871338
Loss :  1.7300833463668823 3.513951539993286 5.244034767150879
Loss :  1.7273340225219727 2.3351528644561768 4.06248664855957
Loss :  1.7301652431488037 2.0162429809570312 3.746408224105835
Loss :  1.7343789339065552 2.909370183944702 4.643749237060547
Loss :  1.7313711643218994 3.2164905071258545 4.947861671447754
Loss :  1.732419729232788 2.2411885261535645 3.9736082553863525
Loss :  1.7288241386413574 1.9423171281814575 3.6711411476135254
Loss :  1.730588674545288 2.2406904697418213 3.9712791442871094
Loss :  1.7197624444961548 3.5125062465667725 5.232268810272217
Loss :  1.7310409545898438 2.699568271636963 4.430609226226807
Loss :  1.7383952140808105 2.162562370300293 3.9009575843811035
Loss :  1.73190176486969 1.9065289497375488 3.638430595397949
Loss :  1.7290304899215698 2.430446147918701 4.1594767570495605
Loss :  1.7335435152053833 2.7166054248809814 4.450149059295654
Loss :  1.7274178266525269 2.0766351222991943 3.8040528297424316
Loss :  1.731343388557434 2.2059264183044434 3.937269687652588
Loss :  1.7295358180999756 2.1548783779144287 3.8844141960144043
Loss :  1.7283167839050293 2.220820665359497 3.9491374492645264
Loss :  1.726138710975647 2.410921573638916 4.137060165405273
  batch 20 loss: 1.726138710975647, 2.410921573638916, 4.137060165405273
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7286453247070312 1.7526224851608276 3.4812679290771484
Loss :  1.7299749851226807 2.501655101776123 4.231630325317383
Loss :  1.7318603992462158 1.9426493644714355 3.6745097637176514
Loss :  1.7373508214950562 2.1541457176208496 3.8914966583251953
Loss :  1.7319138050079346 2.539503574371338 4.271417617797852
Loss :  1.726768136024475 2.3515772819519043 4.07834529876709
Loss :  1.736148715019226 2.2649381160736084 4.001086711883545
Loss :  1.7278008460998535 1.9845271110534668 3.7123279571533203
Loss :  1.7341777086257935 2.6509525775909424 4.385130405426025
Loss :  1.7252038717269897 2.673790693283081 4.398994445800781
Loss :  1.7407668828964233 2.8828227519989014 4.623589515686035
Loss :  1.735018253326416 2.619999408721924 4.35501766204834
Loss :  1.7278228998184204 2.9116384983062744 4.639461517333984
Loss :  1.727439284324646 2.190012216567993 3.9174513816833496
Loss :  1.7361100912094116 2.4391863346099854 4.175296306610107
Loss :  1.736055850982666 3.0931484699249268 4.829204559326172
Loss :  1.7334133386611938 2.8254647254943848 4.558877944946289
Loss :  1.7308036088943481 2.89437198638916 4.625175476074219
Loss :  1.7318036556243896 3.2414262294769287 4.973229885101318
Loss :  1.7343600988388062 2.480902910232544 4.2152628898620605
  batch 40 loss: 1.7343600988388062, 2.480902910232544, 4.2152628898620605
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7325633764266968 2.78686785697937 4.519431114196777
Loss :  1.7314363718032837 2.2276620864868164 3.9590983390808105
Loss :  1.7328797578811646 2.505922555923462 4.238802433013916
Loss :  1.7322628498077393 3.237276315689087 4.969539165496826
Loss :  1.735122561454773 2.9285078048706055 4.663630485534668
Loss :  1.7321839332580566 1.9998418092727661 3.732025623321533
Loss :  1.7331589460372925 1.9533658027648926 3.6865248680114746
Loss :  1.73235285282135 2.7245912551879883 4.456943988800049
Loss :  1.737047791481018 2.977759838104248 4.714807510375977
Loss :  1.7300130128860474 3.3170435428619385 5.047056674957275
Loss :  1.7269251346588135 3.605902671813965 5.332827568054199
Loss :  1.7318710088729858 2.9687659740448 4.700636863708496
Loss :  1.738042950630188 3.1497199535369873 4.887763023376465
Loss :  1.7284706830978394 3.0826194286346436 4.811089992523193
Loss :  1.7294946908950806 2.3941686153411865 4.123663425445557
Loss :  1.7294992208480835 2.5283408164978027 4.257840156555176
Loss :  1.7326363325119019 2.235034942626953 3.9676713943481445
Loss :  1.73540198802948 1.9605515003204346 3.695953369140625
Loss :  1.7348511219024658 2.465782403945923 4.200633525848389
Loss :  1.7324719429016113 1.6057257652282715 3.338197708129883
  batch 60 loss: 1.7324719429016113, 1.6057257652282715, 3.338197708129883
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7320533990859985 1.9965674877166748 3.728621006011963
Loss :  1.732659935951233 1.8010855913162231 3.533745527267456
Loss :  1.734116792678833 2.0302231311798096 3.7643399238586426
Loss :  1.7276394367218018 2.2806882858276367 4.008327484130859
Loss :  1.7267568111419678 2.6858136653900146 4.412570476531982
Loss :  1.8084630966186523 4.3219523429870605 6.130415439605713
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8028247356414795 4.268873691558838 6.071698188781738
Loss :  1.8059173822402954 4.278681755065918 6.084599018096924
Loss :  1.8026772737503052 4.324857234954834 6.12753438949585
Total LOSS train 4.242129821043748 valid 6.103561758995056
CE LOSS train 1.731521960405203 valid 0.4506693184375763
Contrastive LOSS train 2.5106078698084904 valid 1.0812143087387085
EPOCH 188:
Loss :  1.7310620546340942 2.736149311065674 4.4672112464904785
Loss :  1.7300512790679932 2.443220376968384 4.173271656036377
Loss :  1.726394534111023 1.5606844425201416 3.287078857421875
Loss :  1.7287012338638306 1.475708246231079 3.204409599304199
Loss :  1.734252691268921 2.1499862670898438 3.8842389583587646
Loss :  1.729659080505371 2.095266580581665 3.824925661087036
Loss :  1.732226848602295 2.5335090160369873 4.265735626220703
Loss :  1.7281131744384766 2.2942821979522705 4.022395133972168
Loss :  1.7297264337539673 1.7678372859954834 3.4975638389587402
Loss :  1.7184622287750244 2.9147121906280518 4.633174419403076
Loss :  1.7304052114486694 3.094264507293701 4.82466983795166
Loss :  1.738051414489746 2.5767805576324463 4.314831733703613
Loss :  1.7321244478225708 2.4442946910858154 4.176419258117676
Loss :  1.7294141054153442 2.6904327869415283 4.419847011566162
Loss :  1.7332913875579834 2.2858505249023438 4.019142150878906
Loss :  1.7273554801940918 2.2441117763519287 3.9714672565460205
Loss :  1.7322165966033936 2.2063026428222656 3.938519239425659
Loss :  1.7299307584762573 2.3062140941619873 4.036144733428955
Loss :  1.7298284769058228 2.355217933654785 4.085046291351318
Loss :  1.7271957397460938 2.509035587310791 4.236231327056885
  batch 20 loss: 1.7271957397460938, 2.509035587310791, 4.236231327056885
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.729739785194397 2.252690553665161 3.9824304580688477
Loss :  1.7319231033325195 2.334404706954956 4.066328048706055
Loss :  1.7322618961334229 2.2286224365234375 3.9608843326568604
Loss :  1.738250970840454 2.2524142265319824 3.9906651973724365
Loss :  1.731954574584961 2.0518274307250977 3.7837820053100586
Loss :  1.7267986536026 1.736956238746643 3.463754892349243
Loss :  1.735885739326477 1.8752843141555786 3.6111700534820557
Loss :  1.7271116971969604 2.4851138591766357 4.212225437164307
Loss :  1.7335702180862427 2.5719525814056396 4.305522918701172
Loss :  1.7272367477416992 2.9677376747131348 4.694974422454834
Loss :  1.7408760786056519 2.843708038330078 4.5845842361450195
Loss :  1.7332996129989624 2.7256600856781006 4.458959579467773
Loss :  1.7276588678359985 3.103874921798706 4.831533908843994
Loss :  1.7267426252365112 2.7453887462615967 4.472131252288818
Loss :  1.7347989082336426 2.255790948867798 3.9905898571014404
Loss :  1.7347902059555054 2.451993465423584 4.186783790588379
Loss :  1.7319509983062744 3.0152125358581543 4.747163772583008
Loss :  1.7274473905563354 3.0865657329559326 4.8140130043029785
Loss :  1.7311426401138306 3.909189462661743 5.640332221984863
Loss :  1.731790542602539 2.544314384460449 4.276104927062988
  batch 40 loss: 1.731790542602539, 2.544314384460449, 4.276104927062988
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.730263352394104 2.7087910175323486 4.439054489135742
Loss :  1.7290738821029663 2.808600425720215 4.537674427032471
Loss :  1.7297981977462769 2.8225317001342773 4.552330017089844
Loss :  1.730180263519287 2.3363301753997803 4.066510200500488
Loss :  1.7327678203582764 2.645214080810547 4.377982139587402
Loss :  1.7287710905075073 1.9127174615859985 3.641488552093506
Loss :  1.7291749715805054 2.195082187652588 3.924257278442383
Loss :  1.7285417318344116 1.8235758543014526 3.5521175861358643
Loss :  1.7327616214752197 1.9433343410491943 3.676095962524414
Loss :  1.7269941568374634 2.4128623008728027 4.139856338500977
Loss :  1.7249712944030762 2.0075132846832275 3.7324845790863037
Loss :  1.7294808626174927 2.0713202953338623 3.8008012771606445
Loss :  1.735627293586731 2.509267568588257 4.244894981384277
Loss :  1.72698974609375 2.044650077819824 3.771639823913574
Loss :  1.7302937507629395 2.3802616596221924 4.110555648803711
Loss :  1.7266569137573242 2.4589860439300537 4.185643196105957
Loss :  1.7334882020950317 1.8836647272109985 3.6171529293060303
Loss :  1.7358187437057495 1.63239324092865 3.3682119846343994
Loss :  1.7358673810958862 2.3990490436553955 4.134916305541992
Loss :  1.7327262163162231 2.7214255332946777 4.454151630401611
  batch 60 loss: 1.7327262163162231, 2.7214255332946777, 4.454151630401611
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7325928211212158 2.1410796642303467 3.8736724853515625
Loss :  1.7336848974227905 2.0520706176757812 3.7857556343078613
Loss :  1.734683871269226 2.0700230598449707 3.8047070503234863
Loss :  1.728872537612915 2.5234172344207764 4.252289772033691
Loss :  1.7277148962020874 1.6030564308166504 3.3307714462280273
Loss :  1.8246948719024658 4.407637596130371 6.232332229614258
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.818163514137268 4.434891223907471 6.253054618835449
Loss :  1.821975827217102 4.345705509185791 6.1676812171936035
Loss :  1.8167659044265747 4.307854175567627 6.124619960784912
Total LOSS train 4.103558059839102 valid 6.194422006607056
CE LOSS train 1.7307921684705294 valid 0.4541914761066437
Contrastive LOSS train 2.3727658675267147 valid 1.0769635438919067
EPOCH 189:
Loss :  1.7315092086791992 2.3400213718414307 4.071530342102051
Loss :  1.7301141023635864 2.8457882404327393 4.575902462005615
Loss :  1.726913571357727 2.612454414367676 4.339367866516113
Loss :  1.729196548461914 2.2530341148376465 3.9822306632995605
Loss :  1.7342056035995483 1.7260706424713135 3.4602761268615723
Loss :  1.730011224746704 2.389291524887085 4.119302749633789
Loss :  1.7323628664016724 2.4796032905578613 4.211966037750244
Loss :  1.7282124757766724 2.002753257751465 3.7309656143188477
Loss :  1.7301461696624756 1.8269498348236084 3.557096004486084
Loss :  1.7182143926620483 1.5603755712509155 3.278589963912964
Loss :  1.7302101850509644 2.0917062759399414 3.8219165802001953
Loss :  1.7381988763809204 2.797321319580078 4.535520076751709
Loss :  1.7313637733459473 2.4413657188415527 4.1727294921875
Loss :  1.7290853261947632 2.6347198486328125 4.363805294036865
Loss :  1.7336244583129883 2.304806709289551 4.038431167602539
Loss :  1.727375864982605 1.807428002357483 3.534803867340088
Loss :  1.7320998907089233 1.9039454460144043 3.636045455932617
Loss :  1.7295770645141602 2.279120922088623 4.008697986602783
Loss :  1.7297179698944092 1.5558034181594849 3.2855215072631836
Loss :  1.7267204523086548 1.6303225755691528 3.3570430278778076
  batch 20 loss: 1.7267204523086548, 1.6303225755691528, 3.3570430278778076
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7297394275665283 2.2981467247009277 4.027886390686035
Loss :  1.7315161228179932 2.0165863037109375 3.7481024265289307
Loss :  1.733072280883789 1.9617754220962524 3.694847583770752
Loss :  1.7379279136657715 2.133146047592163 3.8710739612579346
Loss :  1.7328717708587646 2.4493043422698975 4.182176113128662
Loss :  1.7272881269454956 2.6668741703033447 4.394162178039551
Loss :  1.735681414604187 2.2609755992889404 3.996656894683838
Loss :  1.725912094116211 2.7551677227020264 4.481080055236816
Loss :  1.732987403869629 2.7537882328033447 4.4867753982543945
Loss :  1.724259853363037 3.8691134452819824 5.5933732986450195
Loss :  1.7405725717544556 3.3845205307006836 5.12509298324585
Loss :  1.73322331905365 3.2280523777008057 4.961275577545166
Loss :  1.7260017395019531 2.703472852706909 4.429474830627441
Loss :  1.7262552976608276 2.1691534519195557 3.8954086303710938
Loss :  1.7346930503845215 3.474412441253662 5.209105491638184
Loss :  1.7350937128067017 3.4586145877838135 5.193708419799805
Loss :  1.731270670890808 2.53613018989563 4.267400741577148
Loss :  1.7261269092559814 1.791221261024475 3.517348289489746
Loss :  1.7295516729354858 1.9180470705032349 3.6475987434387207
Loss :  1.7315096855163574 2.139442205429077 3.8709518909454346
  batch 40 loss: 1.7315096855163574, 2.139442205429077, 3.8709518909454346
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.728520393371582 2.143467426300049 3.871987819671631
Loss :  1.7279706001281738 2.495612621307373 4.223583221435547
Loss :  1.7309125661849976 1.9399205446243286 3.670833110809326
Loss :  1.7288439273834229 1.7830009460449219 3.5118448734283447
Loss :  1.732460379600525 1.9056382179260254 3.63809871673584
Loss :  1.729509949684143 2.0853352546691895 3.814845085144043
Loss :  1.7283965349197388 2.4106557369232178 4.139052391052246
Loss :  1.7305651903152466 1.958855390548706 3.689420700073242
Loss :  1.7322503328323364 2.439196825027466 4.171447277069092
Loss :  1.7292755842208862 2.0729472637176514 3.802222728729248
Loss :  1.7263472080230713 2.4810118675231934 4.207359313964844
Loss :  1.7305476665496826 2.3115453720092773 4.042093276977539
Loss :  1.736204743385315 2.378338575363159 4.114543437957764
Loss :  1.7284879684448242 2.6025230884552 4.331010818481445
Loss :  1.7309975624084473 2.0820915699005127 3.81308913230896
Loss :  1.7275909185409546 1.765590786933899 3.4931817054748535
Loss :  1.7328517436981201 1.93067467212677 3.6635265350341797
Loss :  1.734850287437439 1.517785906791687 3.252636194229126
Loss :  1.7350071668624878 1.8857237100601196 3.6207308769226074
Loss :  1.731385588645935 1.2616934776306152 2.99307918548584
  batch 60 loss: 1.731385588645935, 1.2616934776306152, 2.99307918548584
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7307888269424438 1.4589650630950928 3.189754009246826
Loss :  1.7325849533081055 1.8716950416564941 3.6042799949645996
Loss :  1.7321970462799072 2.2626237869262695 3.9948208332061768
Loss :  1.7266157865524292 2.251488208770752 3.9781041145324707
Loss :  1.7256979942321777 1.4402999877929688 3.1659979820251465
Loss :  1.820028305053711 4.406831741333008 6.226860046386719
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8139137029647827 4.4587812423706055 6.272695064544678
Loss :  1.8174484968185425 4.311030387878418 6.12847900390625
Loss :  1.8130143880844116 4.266838073730469 6.07985258102417
Total LOSS train 3.9795813157008246 valid 6.176971673965454
CE LOSS train 1.7305427074432373 valid 0.4532535970211029
Contrastive LOSS train 2.249038597253653 valid 1.0667095184326172
EPOCH 190:
Loss :  1.7305926084518433 2.1030519008636475 3.833644390106201
Loss :  1.7283440828323364 2.1491873264312744 3.8775315284729004
Loss :  1.7256897687911987 2.1849429607391357 3.910632610321045
Loss :  1.7287729978561401 2.239323854446411 3.9680967330932617
Loss :  1.7329884767532349 1.968190312385559 3.701178789138794
Loss :  1.7291375398635864 2.3668479919433594 4.095985412597656
Loss :  1.7302708625793457 2.4929380416870117 4.223208904266357
Loss :  1.726212978363037 2.3542163372039795 4.0804290771484375
Loss :  1.7293298244476318 2.4213900566101074 4.15071964263916
Loss :  1.7164939641952515 2.334703207015991 4.051197052001953
Loss :  1.7286477088928223 2.7145681381225586 4.443215847015381
Loss :  1.738360047340393 2.504253625869751 4.242613792419434
Loss :  1.730258822441101 2.4987170696258545 4.228975772857666
Loss :  1.7284599542617798 2.567836046218872 4.296296119689941
Loss :  1.7325955629348755 3.2918713092803955 5.0244669914245605
Loss :  1.726836085319519 2.0128018856048584 3.739637851715088
Loss :  1.7314679622650146 2.651320457458496 4.38278865814209
Loss :  1.7287237644195557 2.5197863578796387 4.248510360717773
Loss :  1.7295098304748535 1.7337539196014404 3.463263750076294
Loss :  1.726394534111023 2.5087711811065674 4.235165596008301
  batch 20 loss: 1.726394534111023, 2.5087711811065674, 4.235165596008301
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7295256853103638 2.4969322681427 4.2264580726623535
Loss :  1.7314245700836182 2.8504295349121094 4.581853866577148
Loss :  1.732784628868103 2.5337412357330322 4.266525745391846
Loss :  1.737310528755188 2.7343976497650146 4.471708297729492
Loss :  1.7326315641403198 2.301619291305542 4.034250736236572
Loss :  1.7275755405426025 2.8300440311431885 4.557619571685791
Loss :  1.7356204986572266 2.2777650356292725 4.013385772705078
Loss :  1.7265013456344604 1.9509327411651611 3.677433967590332
Loss :  1.732863426208496 1.9827179908752441 3.7155814170837402
Loss :  1.7257400751113892 2.727780818939209 4.453520774841309
Loss :  1.7407950162887573 2.0726850032806396 3.8134799003601074
Loss :  1.7330552339553833 2.2166969776153564 3.9497523307800293
Loss :  1.7273039817810059 2.3475406169891357 4.0748443603515625
Loss :  1.7269587516784668 2.0605826377868652 3.787541389465332
Loss :  1.735426664352417 2.2875938415527344 4.0230207443237305
Loss :  1.7351270914077759 2.5934722423553467 4.328599452972412
Loss :  1.7318834066390991 3.9291162490844727 5.660999774932861
Loss :  1.7272844314575195 2.4400975704193115 4.16738224029541
Loss :  1.7301441431045532 1.9255657196044922 3.655709743499756
Loss :  1.7311753034591675 1.8375149965286255 3.568690299987793
  batch 40 loss: 1.7311753034591675, 1.8375149965286255, 3.568690299987793
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7297194004058838 2.109823703765869 3.839543104171753
Loss :  1.7286230325698853 1.6141103506088257 3.342733383178711
Loss :  1.7301108837127686 2.4516079425811768 4.181718826293945
Loss :  1.7291104793548584 1.7787744998931885 3.507884979248047
Loss :  1.732384204864502 1.7730393409729004 3.5054235458374023
Loss :  1.729427456855774 2.0818309783935547 3.811258316040039
Loss :  1.7283744812011719 2.885023593902588 4.61339807510376
Loss :  1.7289117574691772 1.7069363594055176 3.4358482360839844
Loss :  1.7315205335617065 1.7746987342834473 3.5062193870544434
Loss :  1.7282264232635498 1.6410809755325317 3.369307518005371
Loss :  1.725880742073059 1.9336601495742798 3.659540891647339
Loss :  1.7300875186920166 3.029937267303467 4.7600250244140625
Loss :  1.7344980239868164 2.3192639350891113 4.053761959075928
Loss :  1.7281525135040283 2.66853928565979 4.396691799163818
Loss :  1.7312873601913452 2.956082582473755 4.6873698234558105
Loss :  1.7244874238967896 3.018357753753662 4.742845058441162
Loss :  1.733033299446106 3.0327486991882324 4.765781879425049
Loss :  1.7354658842086792 2.453834295272827 4.189300060272217
Loss :  1.735883116722107 2.5639290809631348 4.299812316894531
Loss :  1.7311100959777832 2.348146915435791 4.079257011413574
  batch 60 loss: 1.7311100959777832, 2.348146915435791, 4.079257011413574
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7306028604507446 2.5954408645629883 4.326043605804443
Loss :  1.7328104972839355 2.7819290161132812 4.514739513397217
Loss :  1.7321261167526245 3.3738763332366943 5.106002330780029
Loss :  1.728959321975708 2.7643840312957764 4.493343353271484
Loss :  1.7279740571975708 3.0725035667419434 4.800477504730225
Loss :  1.8161964416503906 4.424340724945068 6.240537166595459
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8107850551605225 4.466601848602295 6.277386665344238
Loss :  1.8132898807525635 4.233883857727051 6.047173500061035
Loss :  1.8105379343032837 4.372735023498535 6.183272838592529
Total LOSS train 4.141757612961989 valid 6.187092542648315
CE LOSS train 1.7302921038407546 valid 0.4526344835758209
Contrastive LOSS train 2.41146551829118 valid 1.0931837558746338
EPOCH 191:
Loss :  1.7316157817840576 2.1854844093322754 3.917100191116333
Loss :  1.728590488433838 2.745203733444214 4.473793983459473
Loss :  1.7268590927124023 1.6451363563537598 3.371995449066162
Loss :  1.7301266193389893 1.920099139213562 3.6502256393432617
Loss :  1.7346434593200684 2.5059139728546143 4.240557670593262
Loss :  1.7299585342407227 2.7320046424865723 4.461963176727295
Loss :  1.7327282428741455 2.1961050033569336 3.928833246231079
Loss :  1.7277686595916748 2.2057881355285645 3.9335567951202393
Loss :  1.7305516004562378 2.1694297790527344 3.8999814987182617
Loss :  1.720266342163086 2.855180501937866 4.575447082519531
Loss :  1.730102300643921 2.170241594314575 3.900343894958496
Loss :  1.7392067909240723 2.6365644931793213 4.375771522521973
Loss :  1.7314832210540771 2.0408473014831543 3.7723305225372314
Loss :  1.7290867567062378 2.230234384536743 3.9593210220336914
Loss :  1.734073519706726 2.4870426654815674 4.221116065979004
Loss :  1.728337049484253 2.7717299461364746 4.500066757202148
Loss :  1.732052206993103 2.9278907775878906 4.659943103790283
Loss :  1.7291851043701172 2.6450612545013428 4.374246597290039
Loss :  1.729514718055725 1.7825024127960205 3.512017250061035
Loss :  1.7269192934036255 1.9068876504898071 3.6338069438934326
  batch 20 loss: 1.7269192934036255, 1.9068876504898071, 3.6338069438934326
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7296618223190308 1.7647168636322021 3.4943785667419434
Loss :  1.7306017875671387 2.172189950942993 3.902791738510132
Loss :  1.7327277660369873 1.6697622537612915 3.4024901390075684
Loss :  1.736613154411316 1.787684679031372 3.5242977142333984
Loss :  1.7326797246932983 1.8091413974761963 3.541821002960205
Loss :  1.7280465364456177 2.5193324089050293 4.247378826141357
Loss :  1.7359793186187744 2.417823553085327 4.153802871704102
Loss :  1.7273975610733032 2.4946210384368896 4.222018718719482
Loss :  1.7340108156204224 2.3520987033843994 4.086109638214111
Loss :  1.7266310453414917 1.874944806098938 3.6015758514404297
Loss :  1.7413963079452515 2.187025785446167 3.928421974182129
Loss :  1.7334752082824707 3.366222620010376 5.099698066711426
Loss :  1.727134108543396 3.6106338500976562 5.337768077850342
Loss :  1.7272762060165405 2.996245861053467 4.723522186279297
Loss :  1.734394907951355 2.9695515632629395 4.703946590423584
Loss :  1.7342860698699951 1.8001011610031128 3.5343871116638184
Loss :  1.7308763265609741 1.7241246700286865 3.455000877380371
Loss :  1.7259385585784912 1.8582149744033813 3.584153652191162
Loss :  1.729573369026184 1.8268717527389526 3.5564451217651367
Loss :  1.7313514947891235 1.8641513586044312 3.5955028533935547
  batch 40 loss: 1.7313514947891235, 1.8641513586044312, 3.5955028533935547
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.729356050491333 2.5594875812530518 4.288843631744385
Loss :  1.7286792993545532 2.204326868057251 3.9330062866210938
Loss :  1.7314341068267822 2.5878748893737793 4.319309234619141
Loss :  1.7295588254928589 3.0347585678100586 4.764317512512207
Loss :  1.733389973640442 2.9959728717803955 4.729362964630127
Loss :  1.7294126749038696 4.081412315368652 5.810824871063232
Loss :  1.7286689281463623 2.2619564533233643 3.9906253814697266
Loss :  1.7294586896896362 2.4403557777404785 4.169814586639404
Loss :  1.730565071105957 2.3018276691436768 4.032392501831055
Loss :  1.72738516330719 1.928456425666809 3.655841588973999
Loss :  1.7239818572998047 2.195488929748535 3.91947078704834
Loss :  1.7288079261779785 2.000735282897949 3.7295432090759277
Loss :  1.7351524829864502 2.3900413513183594 4.1251935958862305
Loss :  1.7263420820236206 1.910208821296692 3.6365509033203125
Loss :  1.729919195175171 2.5664098262786865 4.296329021453857
Loss :  1.7256708145141602 2.4192113876342773 4.1448822021484375
Loss :  1.7322052717208862 2.653611183166504 4.38581657409668
Loss :  1.7346056699752808 2.1087636947631836 3.843369483947754
Loss :  1.7349157333374023 2.69169020652771 4.426606178283691
Loss :  1.731361746788025 2.7773375511169434 4.508699417114258
  batch 60 loss: 1.731361746788025, 2.7773375511169434, 4.508699417114258
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7311210632324219 2.3847854137420654 4.115906715393066
Loss :  1.732880711555481 2.7318480014801025 4.464728832244873
Loss :  1.7333989143371582 2.377079486846924 4.110478401184082
Loss :  1.7274967432022095 2.505798578262329 4.233295440673828
Loss :  1.7262979745864868 1.8729188442230225 3.599216938018799
Loss :  1.8291023969650269 4.396719455718994 6.2258219718933105
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8222551345825195 4.381718158721924 6.203973293304443
Loss :  1.8261219263076782 4.299010753631592 6.1251325607299805
Loss :  1.8201202154159546 4.239295482635498 6.059415817260742
Total LOSS train 4.096805480810312 valid 6.153585910797119
CE LOSS train 1.7305413667972271 valid 0.45503005385398865
Contrastive LOSS train 2.366264082835271 valid 1.0598238706588745
EPOCH 192:
Loss :  1.729723334312439 1.9210830926895142 3.650806427001953
Loss :  1.7285056114196777 3.4469313621520996 5.175436973571777
Loss :  1.725650429725647 3.2010605335235596 4.926711082458496
Loss :  1.7280151844024658 3.1764750480651855 4.9044904708862305
Loss :  1.7330361604690552 2.701889753341675 4.4349260330200195
Loss :  1.7290613651275635 2.4524991512298584 4.181560516357422
Loss :  1.7316921949386597 1.9034645557403564 3.6351566314697266
Loss :  1.727656602859497 1.8546220064163208 3.5822787284851074
Loss :  1.7298916578292847 2.7477872371673584 4.4776787757873535
Loss :  1.7172051668167114 2.545397996902466 4.262603282928467
Loss :  1.7302367687225342 2.7724547386169434 4.502691268920898
Loss :  1.737762689590454 2.7307989597320557 4.46856164932251
Loss :  1.731319546699524 2.536867380142212 4.268187046051025
Loss :  1.7289597988128662 2.250657558441162 3.9796173572540283
Loss :  1.7332879304885864 2.317396402359009 4.050684452056885
Loss :  1.7271840572357178 2.2297511100769043 3.956935167312622
Loss :  1.7309362888336182 2.3322513103485107 4.063187599182129
Loss :  1.7284988164901733 3.358755588531494 5.087254524230957
Loss :  1.7284877300262451 2.9002060890197754 4.628693580627441
Loss :  1.725146770477295 1.9754866361618042 3.7006335258483887
  batch 20 loss: 1.725146770477295, 1.9754866361618042, 3.7006335258483887
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7288841009140015 1.8034336566925049 3.532317638397217
Loss :  1.729397177696228 1.994088888168335 3.7234859466552734
Loss :  1.7313145399093628 2.0868723392486572 3.8181867599487305
Loss :  1.735550880432129 2.0762341022491455 3.8117849826812744
Loss :  1.7320247888565063 1.9617087841033936 3.6937336921691895
Loss :  1.7262746095657349 2.8006703853607178 4.526945114135742
Loss :  1.734161138534546 2.65006160736084 4.384222984313965
Loss :  1.7237790822982788 2.411529064178467 4.135308265686035
Loss :  1.7323358058929443 2.1127665042877197 3.845102310180664
Loss :  1.7241997718811035 2.4456374645233154 4.16983699798584
Loss :  1.74093496799469 3.0662896633148193 4.807224750518799
Loss :  1.7315298318862915 2.1743314266204834 3.9058613777160645
Loss :  1.725205421447754 2.1022369861602783 3.8274424076080322
Loss :  1.7254711389541626 2.8308868408203125 4.5563578605651855
Loss :  1.7338097095489502 2.830317258834839 4.564126968383789
Loss :  1.7341423034667969 2.7621684074401855 4.496310710906982
Loss :  1.729878306388855 2.4060652256011963 4.135943412780762
Loss :  1.7247143983840942 1.8836098909378052 3.6083242893218994
Loss :  1.7284164428710938 2.160616636276245 3.889033079147339
Loss :  1.7296574115753174 2.0089950561523438 3.738652467727661
  batch 40 loss: 1.7296574115753174, 2.0089950561523438, 3.738652467727661
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7273503541946411 2.5702006816864014 4.297551155090332
Loss :  1.7276333570480347 2.6091578006744385 4.336791038513184
Loss :  1.7301876544952393 2.7235567569732666 4.453744411468506
Loss :  1.7293078899383545 2.6812379360198975 4.410545825958252
Loss :  1.7323004007339478 2.1644256114959717 3.896726131439209
Loss :  1.7275532484054565 1.9045878648757935 3.63214111328125
Loss :  1.72720205783844 1.9837040901184082 3.7109060287475586
Loss :  1.72946298122406 2.532459259033203 4.261922359466553
Loss :  1.7300337553024292 1.9811227321624756 3.7111563682556152
Loss :  1.7279505729675293 2.4753293991088867 4.203279972076416
Loss :  1.7241421937942505 1.8777737617492676 3.6019158363342285
Loss :  1.7291545867919922 1.8595484495162964 3.588703155517578
Loss :  1.73533034324646 1.7128379344940186 3.4481682777404785
Loss :  1.728233814239502 2.923157215118408 4.65139102935791
Loss :  1.7304168939590454 2.887251853942871 4.617668628692627
Loss :  1.7272955179214478 2.33976411819458 4.067059516906738
Loss :  1.7329846620559692 2.53503155708313 4.268016338348389
Loss :  1.7356092929840088 2.4337003231048584 4.169309616088867
Loss :  1.7358044385910034 2.717611789703369 4.453416347503662
Loss :  1.732597827911377 2.186387538909912 3.918985366821289
  batch 60 loss: 1.732597827911377, 2.186387538909912, 3.918985366821289
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7320369482040405 2.209808111190796 3.941844940185547
Loss :  1.7332146167755127 2.6014750003814697 4.334689617156982
Loss :  1.734013319015503 2.1499710083007812 3.883984327316284
Loss :  1.7281672954559326 2.249397039413452 3.9775643348693848
Loss :  1.7272963523864746 2.0192184448242188 3.7465147972106934
Loss :  1.815983533859253 4.417587757110596 6.2335710525512695
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.8110333681106567 4.389509201049805 6.200542449951172
Loss :  1.813694953918457 4.378232955932617 6.191927909851074
Loss :  1.8098411560058594 4.174278736114502 5.984119892120361
Total LOSS train 4.1337275945223295 valid 6.152540326118469
CE LOSS train 1.7298341586039616 valid 0.45246028900146484
Contrastive LOSS train 2.403893430416401 valid 1.0435696840286255
EPOCH 193:
Loss :  1.7315795421600342 2.7431979179382324 4.4747772216796875
Loss :  1.7299271821975708 2.836801290512085 4.566728591918945
Loss :  1.7269976139068604 2.595109462738037 4.322107315063477
Loss :  1.729835033416748 2.3949389457702637 4.124773979187012
Loss :  1.7342126369476318 2.004815101623535 3.739027738571167
Loss :  1.730499505996704 1.6375805139541626 3.3680801391601562
Loss :  1.7332731485366821 1.8041032552719116 3.5373764038085938
Loss :  1.7291053533554077 2.692516803741455 4.421622276306152
Loss :  1.7311805486679077 2.2079851627349854 3.9391655921936035
Loss :  1.7190083265304565 2.158992052078247 3.878000259399414
Loss :  1.731236219406128 2.4088022708892822 4.14003849029541
Loss :  1.739713191986084 2.6688523292541504 4.408565521240234
Loss :  1.7319538593292236 3.1784415245056152 4.910395622253418
Loss :  1.7295023202896118 2.186880350112915 3.9163827896118164
Loss :  1.7344796657562256 2.996452808380127 4.730932235717773
Loss :  1.72763991355896 1.950247883796692 3.6778879165649414
Loss :  1.7334545850753784 2.003591775894165 3.737046241760254
Loss :  1.7293732166290283 2.052595615386963 3.781968832015991
Loss :  1.7302336692810059 1.6574256420135498 3.3876593112945557
Loss :  1.7279767990112305 1.8101255893707275 3.538102388381958
  batch 20 loss: 1.7279767990112305, 1.8101255893707275, 3.538102388381958
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7296963930130005 2.225924491882324 3.955620765686035
Loss :  1.7317228317260742 2.7614738941192627 4.493196487426758
Loss :  1.7327967882156372 2.44195818901062 4.174755096435547
Loss :  1.7372305393218994 2.0153868198394775 3.752617359161377
Loss :  1.732691764831543 2.2703754901885986 4.0030670166015625
Loss :  1.7271636724472046 1.9040589332580566 3.631222724914551
Loss :  1.7357295751571655 2.379072666168213 4.114802360534668
Loss :  1.726333498954773 2.3888394832611084 4.115172863006592
Loss :  1.7331019639968872 2.947770357131958 4.680872440338135
Loss :  1.7241135835647583 2.3914167881011963 4.115530490875244
Loss :  1.7400513887405396 2.128540515899658 3.868591785430908
Loss :  1.7329647541046143 2.2398595809936523 3.9728243350982666
Loss :  1.7254822254180908 3.2356560230255127 4.9611382484436035
Loss :  1.725964069366455 2.453406572341919 4.179370880126953
Loss :  1.7343363761901855 2.762333393096924 4.496669769287109
Loss :  1.733950138092041 2.5966506004333496 4.330600738525391
Loss :  1.7309436798095703 2.792449951171875 4.523393630981445
Loss :  1.7263624668121338 2.6228413581848145 4.349204063415527
Loss :  1.7292957305908203 1.944387435913086 3.6736831665039062
Loss :  1.731389045715332 2.1704416275024414 3.9018306732177734
  batch 40 loss: 1.731389045715332, 2.1704416275024414, 3.9018306732177734
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7296916246414185 2.7636759281158447 4.493367671966553
Loss :  1.7281506061553955 1.8308236598968506 3.558974266052246
Loss :  1.731278419494629 2.426070213317871 4.1573486328125
Loss :  1.730136513710022 2.3733983039855957 4.103534698486328
Loss :  1.7336249351501465 2.058851957321167 3.7924768924713135
Loss :  1.730272889137268 2.139840602874756 3.8701133728027344
Loss :  1.7298583984375 2.2335946559906006 3.9634530544281006
Loss :  1.7313991785049438 2.4737131595611572 4.205112457275391
Loss :  1.733154535293579 2.7594311237335205 4.4925856590271
Loss :  1.7305108308792114 2.199489116668701 3.929999828338623
Loss :  1.7272188663482666 2.265791177749634 3.9930100440979004
Loss :  1.731036901473999 2.2016091346740723 3.9326460361480713
Loss :  1.736449122428894 2.5526459217071533 4.289094924926758
Loss :  1.7291821241378784 2.161306142807007 3.8904881477355957
Loss :  1.731503963470459 2.4780845642089844 4.209588527679443
Loss :  1.7272624969482422 2.396700143814087 4.12396240234375
Loss :  1.7329853773117065 3.641932487487793 5.374917984008789
Loss :  1.7354756593704224 2.294074535369873 4.029550075531006
Loss :  1.7358882427215576 3.3791098594665527 5.114997863769531
Loss :  1.7320486307144165 2.5475666522979736 4.27961540222168
  batch 60 loss: 1.7320486307144165, 2.5475666522979736, 4.27961540222168
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7318464517593384 2.6506989002227783 4.382545471191406
Loss :  1.733347773551941 3.0946316719055176 4.827979564666748
Loss :  1.7331753969192505 1.718450903892517 3.4516263008117676
Loss :  1.7284679412841797 2.2096428871154785 3.938110828399658
Loss :  1.7272462844848633 1.4511172771453857 3.178363561630249
Loss :  1.822313904762268 4.497616767883301 6.319930553436279
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8159936666488647 4.480440616607666 6.29643440246582
Loss :  1.8193870782852173 4.357106685638428 6.1764936447143555
Loss :  1.8153129816055298 4.135666847229004 5.950979709625244
Total LOSS train 4.115050268173218 valid 6.185959577560425
CE LOSS train 1.7309802458836483 valid 0.45382824540138245
Contrastive LOSS train 2.3840700222895697 valid 1.033916711807251
EPOCH 194:
Loss :  1.7318967580795288 2.4139974117279053 4.1458940505981445
Loss :  1.7307237386703491 3.2502942085266113 4.98101806640625
Loss :  1.7272686958312988 1.802134394645691 3.5294032096862793
Loss :  1.7300949096679688 2.730433225631714 4.460528373718262
Loss :  1.7340503931045532 2.108208656311035 3.842258930206299
Loss :  1.7316209077835083 2.4037668704986572 4.135387897491455
Loss :  1.732208251953125 2.8242902755737305 4.5564985275268555
Loss :  1.7293059825897217 3.260986566543579 4.990292549133301
Loss :  1.730668544769287 1.7578414678573608 3.4885101318359375
Loss :  1.718248963356018 2.1555707454681396 3.8738198280334473
Loss :  1.731061339378357 2.6760454177856445 4.407106876373291
Loss :  1.7392045259475708 1.8832472562789917 3.6224517822265625
Loss :  1.7316553592681885 2.3051443099975586 4.036799430847168
Loss :  1.7291641235351562 2.0656521320343018 3.794816255569458
Loss :  1.7331840991973877 2.657454490661621 4.39063835144043
Loss :  1.7269923686981201 1.9444628953933716 3.6714553833007812
Loss :  1.7314939498901367 2.213029146194458 3.9445230960845947
Loss :  1.7290147542953491 3.9719247817993164 5.700939655303955
Loss :  1.7291184663772583 3.6064422130584717 5.3355607986450195
Loss :  1.726159691810608 2.2715020179748535 3.997661590576172
  batch 20 loss: 1.726159691810608, 2.2715020179748535, 3.997661590576172
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7294507026672363 2.385256767272949 4.1147074699401855
Loss :  1.7307039499282837 1.890546441078186 3.6212503910064697
Loss :  1.7320410013198853 2.63748836517334 4.3695292472839355
Loss :  1.736754298210144 2.4376251697540283 4.174379348754883
Loss :  1.732718825340271 3.389030933380127 5.1217498779296875
Loss :  1.7260996103286743 1.6893185377120972 3.4154181480407715
Loss :  1.7344835996627808 1.5514919757843018 3.285975456237793
Loss :  1.7249592542648315 1.5193259716033936 3.2442851066589355
Loss :  1.7321677207946777 2.2278788089752197 3.9600465297698975
Loss :  1.7247843742370605 2.5045974254608154 4.229381561279297
Loss :  1.7399256229400635 1.9567915201187134 3.6967172622680664
Loss :  1.7320446968078613 1.9811644554138184 3.7132091522216797
Loss :  1.7257912158966064 1.6685234308242798 3.394314765930176
Loss :  1.725221037864685 1.7939200401306152 3.51914119720459
Loss :  1.7344028949737549 2.2251124382019043 3.959515333175659
Loss :  1.734214186668396 1.8446261882781982 3.5788402557373047
Loss :  1.7309914827346802 2.106182336807251 3.8371739387512207
Loss :  1.727212905883789 1.9838745594024658 3.711087465286255
Loss :  1.7292879819869995 2.070737838745117 3.8000259399414062
Loss :  1.7308975458145142 2.415534257888794 4.146431922912598
  batch 40 loss: 1.7308975458145142, 2.415534257888794, 4.146431922912598
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.729526400566101 2.776559352874756 4.5060858726501465
Loss :  1.727726697921753 2.0129282474517822 3.740654945373535
Loss :  1.7303766012191772 2.401444435119629 4.131821155548096
Loss :  1.7292919158935547 2.3831398487091064 4.112431526184082
Loss :  1.73258376121521 2.2018778324127197 3.9344615936279297
Loss :  1.7290624380111694 2.0635194778442383 3.7925820350646973
Loss :  1.72843337059021 2.3258216381073 4.05425500869751
Loss :  1.729091763496399 1.8101388216018677 3.5392305850982666
Loss :  1.7310420274734497 2.45928955078125 4.19033145904541
Loss :  1.7278121709823608 1.919289231300354 3.647101402282715
Loss :  1.723923921585083 2.3653159141540527 4.089240074157715
Loss :  1.7292789220809937 1.500852108001709 3.230131149291992
Loss :  1.734273910522461 1.832806944847107 3.5670809745788574
Loss :  1.7270071506500244 1.7638249397277832 3.4908320903778076
Loss :  1.729332447052002 1.7191731929779053 3.4485056400299072
Loss :  1.7246692180633545 2.6121230125427246 4.3367919921875
Loss :  1.7310547828674316 2.8260395526885986 4.557094573974609
Loss :  1.7344499826431274 2.582761287689209 4.317211151123047
Loss :  1.7350000143051147 3.3714394569396973 5.106439590454102
Loss :  1.7306562662124634 2.218879222869873 3.949535369873047
  batch 60 loss: 1.7306562662124634, 2.218879222869873, 3.949535369873047
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7303707599639893 1.6838620901107788 3.4142327308654785
Loss :  1.7324985265731812 1.68734610080719 3.419844627380371
Loss :  1.7326269149780273 1.5289732217788696 3.2616000175476074
Loss :  1.727346658706665 2.421433448791504 4.14877986907959
Loss :  1.7264175415039062 2.3511428833007812 4.0775604248046875
Loss :  1.809493899345398 4.39589262008667 6.205386638641357
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.804173231124878 4.405412197113037 6.209585189819336
Loss :  1.8068560361862183 4.306850910186768 6.113707065582275
Loss :  1.804121494293213 4.192977428436279 5.997098922729492
Total LOSS train 3.9978858617635873 valid 6.131444454193115
CE LOSS train 1.730171429193937 valid 0.4510303735733032
Contrastive LOSS train 2.2677144270676832 valid 1.0482443571090698
EPOCH 195:
Loss :  1.7309973239898682 2.305338144302368 4.036335468292236
Loss :  1.7291730642318726 2.618018865585327 4.34719181060791
Loss :  1.7261908054351807 2.7941575050354004 4.52034854888916
Loss :  1.7294527292251587 2.1595265865325928 3.888979434967041
Loss :  1.733258843421936 1.699249505996704 3.4325084686279297
Loss :  1.7297139167785645 1.6856658458709717 3.415379762649536
Loss :  1.7316781282424927 2.6651017665863037 4.396780014038086
Loss :  1.7273763418197632 2.2059125900268555 3.933289051055908
Loss :  1.7302626371383667 2.2460789680480957 3.976341724395752
Loss :  1.7180464267730713 1.8331884145736694 3.551234722137451
Loss :  1.7301026582717896 2.269279956817627 3.999382495880127
Loss :  1.7390321493148804 2.241729736328125 3.980762004852295
Loss :  1.7313719987869263 1.9382405281066895 3.669612407684326
Loss :  1.7291349172592163 2.9982948303222656 4.7274298667907715
Loss :  1.7329256534576416 2.280895948410034 4.013821601867676
Loss :  1.7274253368377686 1.6949433088302612 3.4223685264587402
Loss :  1.7310948371887207 1.8543740510940552 3.5854687690734863
Loss :  1.7290961742401123 2.8500144481658936 4.579110622406006
Loss :  1.7295162677764893 2.7357490062713623 4.465265274047852
Loss :  1.7256946563720703 2.9314308166503906 4.657125473022461
  batch 20 loss: 1.7256946563720703, 2.9314308166503906, 4.657125473022461
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7292122840881348 1.9741873741149902 3.703399658203125
Loss :  1.7305372953414917 3.1537389755249023 4.884276390075684
Loss :  1.7314269542694092 2.9898202419281006 4.72124719619751
Loss :  1.7364845275878906 2.2594659328460693 3.99595046043396
Loss :  1.7321209907531738 2.2314820289611816 3.9636030197143555
Loss :  1.7263500690460205 1.9444878101348877 3.670837879180908
Loss :  1.7347527742385864 1.8740508556365967 3.6088037490844727
Loss :  1.7250940799713135 2.1432814598083496 3.868375539779663
Loss :  1.7324717044830322 1.8255778551101685 3.5580496788024902
Loss :  1.7239296436309814 2.796841859817505 4.520771503448486
Loss :  1.7403192520141602 2.2213377952575684 3.9616570472717285
Loss :  1.7330923080444336 2.5635058879852295 4.296598434448242
Loss :  1.7265247106552124 2.9288599491119385 4.655384540557861
Loss :  1.7264750003814697 2.701331615447998 4.427806854248047
Loss :  1.7349778413772583 3.1313796043395996 4.866357326507568
Loss :  1.7351710796356201 3.7901461124420166 5.525317192077637
Loss :  1.7318427562713623 2.3728818893432617 4.104724884033203
Loss :  1.7279547452926636 1.9281727075576782 3.656127452850342
Loss :  1.7298787832260132 2.4737370014190674 4.203615665435791
Loss :  1.7321255207061768 2.350559949874878 4.082685470581055
  batch 40 loss: 1.7321255207061768, 2.350559949874878, 4.082685470581055
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.729547142982483 3.3069024085998535 5.036449432373047
Loss :  1.728262186050415 3.0587856769561768 4.787047863006592
Loss :  1.730514645576477 2.0908961296081543 3.821410655975342
Loss :  1.7295159101486206 2.4775965213775635 4.2071123123168945
Loss :  1.7324975728988647 2.5642693042755127 4.296766757965088
Loss :  1.7287184000015259 2.6459672451019287 4.374685764312744
Loss :  1.7279287576675415 2.0061559677124023 3.7340846061706543
Loss :  1.7289454936981201 1.9530117511749268 3.681957244873047
Loss :  1.7315791845321655 1.84454345703125 3.576122760772705
Loss :  1.7276949882507324 1.6718887090682983 3.3995838165283203
Loss :  1.7250897884368896 2.0485310554504395 3.773620843887329
Loss :  1.73023521900177 3.3075594902038574 5.037794589996338
Loss :  1.736190915107727 2.457254648208618 4.193445682525635
Loss :  1.7276829481124878 2.303952217102051 4.031635284423828
Loss :  1.7305241823196411 2.515786647796631 4.246310710906982
Loss :  1.7273930311203003 2.931558132171631 4.658951282501221
Loss :  1.7337883710861206 2.837735652923584 4.571524143218994
Loss :  1.736063003540039 2.0450870990753174 3.7811501026153564
Loss :  1.7361527681350708 3.2657253742218018 5.001878261566162
Loss :  1.7328625917434692 1.6620928049087524 3.3949553966522217
  batch 60 loss: 1.7328625917434692, 1.6620928049087524, 3.3949553966522217
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7327970266342163 2.925083637237549 4.657880783081055
Loss :  1.73381507396698 2.0513696670532227 3.785184860229492
Loss :  1.7349153757095337 1.706735372543335 3.441650867462158
Loss :  1.728407859802246 2.4850447177886963 4.213452339172363
Loss :  1.7272120714187622 1.2352421283721924 2.962454319000244
Loss :  1.823380470275879 4.41832971572876 6.241710186004639
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8165313005447388 4.401118278503418 6.217649459838867
Loss :  1.820469856262207 4.329010009765625 6.149479866027832
Loss :  1.8160127401351929 4.216856479644775 6.032869338989258
Total LOSS train 4.116022102649396 valid 6.160427212715149
CE LOSS train 1.730471103007977 valid 0.4540031850337982
Contrastive LOSS train 2.3855509776335495 valid 1.0542141199111938
EPOCH 196:
Loss :  1.7313313484191895 2.3977012634277344 4.129032611846924
Loss :  1.7305710315704346 2.0150418281555176 3.745612859725952
Loss :  1.7269731760025024 1.934393286705017 3.6613664627075195
Loss :  1.7298705577850342 2.839182138442993 4.569052696228027
Loss :  1.733609914779663 2.228930950164795 3.962540864944458
Loss :  1.7307593822479248 3.684004783630371 5.414764404296875
Loss :  1.7308722734451294 3.759995460510254 5.490867614746094
Loss :  1.7281466722488403 2.9910330772399902 4.719179630279541
Loss :  1.7298905849456787 1.9847114086151123 3.714601993560791
Loss :  1.717210292816162 2.6141819953918457 4.331392288208008
Loss :  1.7302064895629883 3.143059492111206 4.873266220092773
Loss :  1.7394353151321411 2.6617510318756104 4.401186466217041
Loss :  1.7318452596664429 3.2675671577453613 4.999412536621094
Loss :  1.7288875579833984 2.0981831550598145 3.827070713043213
Loss :  1.733246088027954 2.3828582763671875 4.1161041259765625
Loss :  1.7269893884658813 2.5867345333099365 4.313724040985107
Loss :  1.7316393852233887 2.2509493827819824 3.982588768005371
Loss :  1.7285478115081787 1.9012463092803955 3.629794120788574
Loss :  1.7300978899002075 2.139021635055542 3.869119644165039
Loss :  1.725426197052002 3.070279359817505 4.795705795288086
  batch 20 loss: 1.725426197052002, 3.070279359817505, 4.795705795288086
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7287862300872803 2.916923761367798 4.645709991455078
Loss :  1.7311795949935913 2.5379722118377686 4.26915168762207
Loss :  1.7312880754470825 2.1294684410095215 3.8607563972473145
Loss :  1.7375394105911255 2.3726422786712646 4.11018180847168
Loss :  1.7326661348342896 2.99470591545105 4.727372169494629
Loss :  1.7271817922592163 2.5319087505340576 4.259090423583984
Loss :  1.7364147901535034 2.5648488998413086 4.301263809204102
Loss :  1.727582573890686 2.2529494762420654 3.980532169342041
Loss :  1.7347180843353271 3.115980625152588 4.850698471069336
Loss :  1.7263203859329224 3.686697483062744 5.413017749786377
Loss :  1.741646409034729 2.1304852962493896 3.872131824493408
Loss :  1.7348954677581787 2.0346696376800537 3.7695651054382324
Loss :  1.728549838066101 2.024362802505493 3.7529125213623047
Loss :  1.7280617952346802 2.871211528778076 4.599273204803467
Loss :  1.7367480993270874 2.6762402057647705 4.412988185882568
Loss :  1.736441969871521 2.9411392211914062 4.677581310272217
Loss :  1.7332967519760132 3.0255637168884277 4.7588605880737305
Loss :  1.7294628620147705 2.948237657546997 4.677700519561768
Loss :  1.731508731842041 2.2593321800231934 3.9908409118652344
Loss :  1.7331680059432983 2.62296986579895 4.356137752532959
  batch 40 loss: 1.7331680059432983, 2.62296986579895, 4.356137752532959
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.730932354927063 2.10813045501709 3.8390626907348633
Loss :  1.730289101600647 1.6598584651947021 3.3901476860046387
Loss :  1.732090950012207 1.8891509771347046 3.621242046356201
Loss :  1.7315120697021484 2.19254732131958 3.9240593910217285
Loss :  1.7338718175888062 2.459061861038208 4.192933559417725
Loss :  1.7302974462509155 1.693529725074768 3.4238271713256836
Loss :  1.7301267385482788 1.4181418418884277 3.148268699645996
Loss :  1.7308564186096191 1.441292405128479 3.1721487045288086
Loss :  1.7324148416519165 1.7002959251403809 3.432710647583008
Loss :  1.7297977209091187 2.1226422786712646 3.8524398803710938
Loss :  1.7264013290405273 1.9027513265609741 3.629152774810791
Loss :  1.7306712865829468 1.745155930519104 3.475827217102051
Loss :  1.7363237142562866 2.2739665508270264 4.010290145874023
Loss :  1.7286499738693237 2.476569890975952 4.205219745635986
Loss :  1.7309354543685913 2.206388235092163 3.937323570251465
Loss :  1.7271685600280762 2.424724817276001 4.151893615722656
Loss :  1.7329641580581665 2.7437446117401123 4.476708889007568
Loss :  1.7352858781814575 2.0596983432769775 3.7949843406677246
Loss :  1.7354457378387451 2.8068745136260986 4.542320251464844
Loss :  1.731453776359558 3.3924572467803955 5.123910903930664
  batch 60 loss: 1.731453776359558, 3.3924572467803955, 5.123910903930664
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.731419563293457 2.14096999168396 3.872389554977417
Loss :  1.7329128980636597 2.09316086769104 3.82607364654541
Loss :  1.7333704233169556 1.8155099153518677 3.5488803386688232
Loss :  1.7280305624008179 2.801640272140503 4.529670715332031
Loss :  1.7270487546920776 2.365988254547119 4.093037128448486
Loss :  1.8158499002456665 4.3933587074279785 6.2092084884643555
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8098320960998535 4.43170166015625 6.2415337562561035
Loss :  1.8129962682724 4.376189708709717 6.189186096191406
Loss :  1.8104099035263062 4.16246223449707 5.972872257232666
Total LOSS train 4.169948827303373 valid 6.153200149536133
CE LOSS train 1.7311274638542762 valid 0.45260247588157654
Contrastive LOSS train 2.4388213616151075 valid 1.0406155586242676
EPOCH 197:
Loss :  1.7313560247421265 4.176886081695557 5.908242225646973
Loss :  1.7292611598968506 3.023984432220459 4.7532453536987305
Loss :  1.7265523672103882 1.7892175912857056 3.5157699584960938
Loss :  1.7294503450393677 2.9607126712799072 4.6901631355285645
Loss :  1.7340575456619263 2.3310306072235107 4.065088272094727
Loss :  1.7300701141357422 2.4030556678771973 4.1331257820129395
Loss :  1.7324304580688477 2.299952983856201 4.032383441925049
Loss :  1.7277452945709229 2.717634916305542 4.445380210876465
Loss :  1.7303680181503296 3.3134312629699707 5.04379940032959
Loss :  1.7184454202651978 2.1715540885925293 3.8899993896484375
Loss :  1.7307263612747192 2.9416019916534424 4.672328472137451
Loss :  1.7400044202804565 2.790703535079956 4.530707836151123
Loss :  1.7322447299957275 2.0486786365509033 3.780923366546631
Loss :  1.7292678356170654 1.9976528882980347 3.7269206047058105
Loss :  1.733129858970642 3.0239319801330566 4.757061958312988
Loss :  1.7278469800949097 2.1248939037323 3.85274076461792
Loss :  1.7321453094482422 2.6905481815338135 4.422693252563477
Loss :  1.7300440073013306 2.53242826461792 4.262472152709961
Loss :  1.7305474281311035 2.1276543140411377 3.858201742172241
Loss :  1.7261627912521362 2.7634692192077637 4.4896321296691895
  batch 20 loss: 1.7261627912521362, 2.7634692192077637, 4.4896321296691895
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7292131185531616 2.0206964015960693 3.7499094009399414
Loss :  1.7309290170669556 1.9981924295425415 3.729121446609497
Loss :  1.7312625646591187 1.7582638263702393 3.4895262718200684
Loss :  1.7366923093795776 1.869378685951233 3.6060709953308105
Loss :  1.7326245307922363 2.72967529296875 4.462299823760986
Loss :  1.7269397974014282 2.0317556858062744 3.758695602416992
Loss :  1.7356228828430176 2.2559401988983154 3.991563081741333
Loss :  1.7257304191589355 2.9390618801116943 4.664792060852051
Loss :  1.7332640886306763 2.1940412521362305 3.927305221557617
Loss :  1.725327730178833 2.3146400451660156 4.0399675369262695
Loss :  1.741241455078125 2.2129061222076416 3.9541475772857666
Loss :  1.7337502241134644 2.076162815093994 3.809913158416748
Loss :  1.7270772457122803 2.70538067817688 4.43245792388916
Loss :  1.727247953414917 3.211005449295044 4.938253402709961
Loss :  1.735958456993103 3.1899378299713135 4.925896167755127
Loss :  1.7354958057403564 2.4958083629608154 4.231304168701172
Loss :  1.7318631410598755 3.358614683151245 5.09047794342041
Loss :  1.727621078491211 2.495366334915161 4.222987174987793
Loss :  1.7298486232757568 2.811964273452759 4.541812896728516
Loss :  1.731651782989502 2.2538633346557617 3.9855151176452637
  batch 40 loss: 1.731651782989502, 2.2538633346557617, 3.9855151176452637
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7292941808700562 2.554835557937622 4.284129619598389
Loss :  1.7288877964019775 2.6886160373687744 4.417503833770752
Loss :  1.7315322160720825 2.5707297325134277 4.302261829376221
Loss :  1.7294259071350098 2.773919105529785 4.503345012664795
Loss :  1.7328367233276367 1.5653938055038452 3.2982306480407715
Loss :  1.7300840616226196 1.795888066291809 3.5259721279144287
Loss :  1.7284526824951172 1.6397324800491333 3.368185043334961
Loss :  1.7307868003845215 1.828052043914795 3.5588388442993164
Loss :  1.7330389022827148 1.8979076147079468 3.630946636199951
Loss :  1.7302082777023315 2.116406202316284 3.846614360809326
Loss :  1.7273130416870117 2.4661805629730225 4.193493843078613
Loss :  1.7313381433486938 2.3890020847320557 4.120340347290039
Loss :  1.7360838651657104 2.5902018547058105 4.3262858390808105
Loss :  1.7290056943893433 1.724521517753601 3.4535272121429443
Loss :  1.7313474416732788 1.6307523250579834 3.3620996475219727
Loss :  1.7266961336135864 2.2393975257873535 3.9660935401916504
Loss :  1.7334219217300415 2.45902943611145 4.192451477050781
Loss :  1.736058235168457 1.9258744716644287 3.6619327068328857
Loss :  1.736240267753601 2.3899996280670166 4.126239776611328
Loss :  1.7320921421051025 2.3141930103302 4.046285152435303
  batch 60 loss: 1.7320921421051025, 2.3141930103302, 4.046285152435303
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7319419384002686 1.7714167833328247 3.503358840942383
Loss :  1.733720302581787 1.7271592617034912 3.4608795642852783
Loss :  1.7333799600601196 1.5578736066818237 3.2912535667419434
Loss :  1.7279998064041138 2.0846807956695557 3.812680721282959
Loss :  1.727101445198059 1.4813816547393799 3.2084832191467285
Loss :  1.817808747291565 4.433987617492676 6.251796245574951
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8114898204803467 4.400475025177002 6.2119646072387695
Loss :  1.8149406909942627 4.2747368812561035 6.089677810668945
Loss :  1.8108152151107788 4.295778274536133 6.106593608856201
Total LOSS train 4.089912766676683 valid 6.165008068084717
CE LOSS train 1.730915485895597 valid 0.4527038037776947
Contrastive LOSS train 2.35899729178502 valid 1.0739445686340332
EPOCH 198:
Loss :  1.7315406799316406 1.7047183513641357 3.4362590312957764
Loss :  1.7297123670578003 2.04010272026062 3.769814968109131
Loss :  1.7267792224884033 1.907875895500183 3.634654998779297
Loss :  1.7302988767623901 2.255497455596924 3.9857964515686035
Loss :  1.73369300365448 2.014198064804077 3.7478909492492676
Loss :  1.7305532693862915 1.979108452796936 3.7096617221832275
Loss :  1.7319527864456177 2.221466541290283 3.9534192085266113
Loss :  1.7277339696884155 2.247115135192871 3.974849224090576
Loss :  1.7304184436798096 2.1057629585266113 3.836181402206421
Loss :  1.7179993391036987 2.2636940479278564 3.9816932678222656
Loss :  1.7302430868148804 3.251401424407959 4.981644630432129
Loss :  1.7391960620880127 2.103093385696411 3.842289447784424
Loss :  1.731858253479004 2.352739095687866 4.084597587585449
Loss :  1.7292640209197998 3.6247148513793945 5.353979110717773
Loss :  1.732590913772583 2.8583054542541504 4.5908966064453125
Loss :  1.7273824214935303 1.8584901094436646 3.5858726501464844
Loss :  1.7314674854278564 2.2605412006378174 3.992008686065674
Loss :  1.7293434143066406 2.533001661300659 4.262345314025879
Loss :  1.7306501865386963 2.206296443939209 3.9369466304779053
Loss :  1.7260804176330566 2.216392755508423 3.9424731731414795
  batch 20 loss: 1.7260804176330566, 2.216392755508423, 3.9424731731414795
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7299546003341675 1.956371784210205 3.686326503753662
Loss :  1.7313228845596313 2.0810844898223877 3.8124074935913086
Loss :  1.7326244115829468 2.3203623294830322 4.0529866218566895
Loss :  1.7372053861618042 1.8748337030410767 3.612039089202881
Loss :  1.7334356307983398 1.8485180139541626 3.581953525543213
Loss :  1.7277045249938965 2.183223247528076 3.9109277725219727
Loss :  1.7359591722488403 2.237705945968628 3.973665237426758
Loss :  1.7262791395187378 2.1181323528289795 3.8444113731384277
Loss :  1.7336682081222534 2.7081096172332764 4.44177770614624
Loss :  1.7247587442398071 2.200446844100952 3.925205707550049
Loss :  1.7412124872207642 2.7622129917144775 4.503425598144531
Loss :  1.7351796627044678 3.4285900592803955 5.163769721984863
Loss :  1.726866602897644 2.7662880420684814 4.493154525756836
Loss :  1.7289656400680542 3.933727502822876 5.662693023681641
Loss :  1.7407547235488892 2.969848155975342 4.710602760314941
Loss :  1.7415019273757935 3.833757162094116 5.575259208679199
Loss :  1.731533408164978 1.9884989261627197 3.720032215118408
Loss :  1.7264543771743774 2.0361275672912598 3.7625818252563477
Loss :  1.7292526960372925 1.7668856382369995 3.496138334274292
Loss :  1.731750726699829 1.67768394947052 3.4094347953796387
  batch 40 loss: 1.731750726699829, 1.67768394947052, 3.4094347953796387
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7289294004440308 2.1638331413269043 3.8927626609802246
Loss :  1.7289025783538818 1.9188588857650757 3.647761344909668
Loss :  1.7320129871368408 1.972980260848999 3.70499324798584
Loss :  1.7310023307800293 1.6261792182922363 3.3571815490722656
Loss :  1.7341527938842773 2.3379065990448 4.072059631347656
Loss :  1.7304857969284058 2.1255300045013428 3.856015682220459
Loss :  1.7301875352859497 2.160564661026001 3.8907523155212402
Loss :  1.7319296598434448 2.259608507156372 3.9915380477905273
Loss :  1.7333329916000366 3.255291700363159 4.988624572753906
Loss :  1.7310981750488281 3.725466251373291 5.456564426422119
Loss :  1.7283669710159302 3.8322627544403076 5.560629844665527
Loss :  1.7297642230987549 4.404486179351807 6.134250640869141
Loss :  1.7369922399520874 4.339007377624512 6.075999736785889
Loss :  1.7300547361373901 2.9624505043029785 4.692505359649658
Loss :  1.7313990592956543 2.4503090381622314 4.181708335876465
Loss :  1.728204369544983 2.784773111343384 4.512977600097656
Loss :  1.7331792116165161 2.257765293121338 3.9909443855285645
Loss :  1.7360941171646118 2.391902208328247 4.127996444702148
Loss :  1.735562801361084 2.5579965114593506 4.2935590744018555
Loss :  1.7325477600097656 2.506167411804199 4.238715171813965
  batch 60 loss: 1.7325477600097656, 2.506167411804199, 4.238715171813965
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7318761348724365 2.5316548347473145 4.263530731201172
Loss :  1.733201026916504 2.824658155441284 4.557859420776367
Loss :  1.7331525087356567 2.4194304943084717 4.152583122253418
Loss :  1.7269607782363892 2.9498274326324463 4.676788330078125
Loss :  1.726017951965332 1.8582994937896729 3.584317445755005
Loss :  1.7927221059799194 4.319978713989258 6.112700939178467
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7889808416366577 4.32848596572876 6.117466926574707
Loss :  1.7899564504623413 4.232664108276367 6.022620677947998
Loss :  1.7928731441497803 4.314737319946289 6.107610702514648
Total LOSS train 4.212995188052838 valid 6.090099811553955
CE LOSS train 1.7312392509900607 valid 0.44821828603744507
Contrastive LOSS train 2.481755913220919 valid 1.0786843299865723
EPOCH 199:
Loss :  1.7303513288497925 1.7673417329788208 3.4976930618286133
Loss :  1.7289904356002808 1.8675096035003662 3.5964999198913574
Loss :  1.7258269786834717 1.6625218391418457 3.3883488178253174
Loss :  1.7295291423797607 1.7799535989761353 3.5094828605651855
Loss :  1.7330557107925415 1.6850394010543823 3.418095111846924
Loss :  1.7308239936828613 2.7213971614837646 4.452220916748047
Loss :  1.7315257787704468 2.459113121032715 4.190639019012451
Loss :  1.7290126085281372 2.728795289993286 4.457808017730713
Loss :  1.7313096523284912 2.152108907699585 3.883418560028076
Loss :  1.719751238822937 2.9571783542633057 4.676929473876953
Loss :  1.7328249216079712 2.9596760272979736 4.692501068115234
Loss :  1.739551305770874 3.4502367973327637 5.189787864685059
Loss :  1.7342921495437622 2.469949960708618 4.20424222946167
Loss :  1.7317497730255127 2.415045976638794 4.146795749664307
Loss :  1.7351101636886597 2.3279831409454346 4.063093185424805
Loss :  1.7300070524215698 2.0731289386749268 3.803135871887207
Loss :  1.7347115278244019 2.7025747299194336 4.437286376953125
Loss :  1.7328709363937378 2.0361194610595703 3.7689905166625977
Loss :  1.73214590549469 1.9378072023391724 3.6699531078338623
Loss :  1.7293083667755127 2.118121862411499 3.8474302291870117
  batch 20 loss: 1.7293083667755127, 2.118121862411499, 3.8474302291870117
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7328234910964966 2.0411159992218018 3.773939609527588
Loss :  1.7338051795959473 2.2334887981414795 3.9672939777374268
Loss :  1.7348684072494507 2.1355316638946533 3.8703999519348145
Loss :  1.7389135360717773 1.9896830320358276 3.7285966873168945
Loss :  1.7337440252304077 1.9656966924667358 3.6994407176971436
Loss :  1.729697585105896 2.171910285949707 3.9016079902648926
Loss :  1.737816333770752 2.058429479598999 3.796245813369751
Loss :  1.7291181087493896 1.854012131690979 3.583130359649658
Loss :  1.7359209060668945 2.3950355052948 4.130956649780273
Loss :  1.7272223234176636 2.5851733684539795 4.3123955726623535
Loss :  1.7416636943817139 2.0325534343719482 3.774217128753662
Loss :  1.7353811264038086 1.82157564163208 3.5569567680358887
Loss :  1.7291090488433838 2.0835046768188477 3.8126137256622314
Loss :  1.728537678718567 2.740830421447754 4.469367980957031
Loss :  1.736256718635559 2.3457250595092773 4.081981658935547
Loss :  1.7362779378890991 2.17091965675354 3.9071974754333496
Loss :  1.7328115701675415 1.8698978424072266 3.6027092933654785
Loss :  1.729287028312683 2.421950578689575 4.151237487792969
Loss :  1.7316408157348633 2.3287103176116943 4.060351371765137
Loss :  1.7327752113342285 1.5104396343231201 3.2432148456573486
  batch 40 loss: 1.7327752113342285, 1.5104396343231201, 3.2432148456573486
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7310904264450073 2.1794345378875732 3.910524845123291
Loss :  1.7307907342910767 2.936340570449829 4.667131423950195
Loss :  1.7329075336456299 1.9775965213775635 3.7105040550231934
Loss :  1.7318912744522095 2.195167303085327 3.927058696746826
Loss :  1.734522819519043 1.4362319707870483 3.170754909515381
Loss :  1.7308270931243896 2.454639434814453 4.185466766357422
Loss :  1.729994297027588 2.206963539123535 3.936957836151123
Loss :  1.731896996498108 2.1652040481567383 3.8971009254455566
Loss :  1.7334234714508057 1.6320298910140991 3.3654532432556152
Loss :  1.7309918403625488 1.7645797729492188 3.4955716133117676
Loss :  1.7265344858169556 2.370598793029785 4.097133159637451
Loss :  1.7311363220214844 1.8191536664962769 3.550290107727051
Loss :  1.7364568710327148 1.5596065521240234 3.2960634231567383
Loss :  1.7288542985916138 1.8366715908050537 3.565526008605957
Loss :  1.7314069271087646 2.31673526763916 4.048142433166504
Loss :  1.7285332679748535 3.03210186958313 4.7606353759765625
Loss :  1.734107494354248 2.7914483547210693 4.525555610656738
Loss :  1.7373160123825073 3.006941556930542 4.74425745010376
Loss :  1.7370561361312866 2.6740081310272217 4.411064147949219
Loss :  1.7331721782684326 2.292405128479004 4.025577545166016
  batch 60 loss: 1.7331721782684326, 2.292405128479004, 4.025577545166016
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7337048053741455 1.9076703786849976 3.6413750648498535
Loss :  1.7351768016815186 1.6647123098373413 3.3998889923095703
Loss :  1.7354695796966553 2.7118771076202393 4.4473466873168945
Loss :  1.7301117181777954 2.663104295730591 4.393216133117676
Loss :  1.7290549278259277 1.460652470588684 3.1897072792053223
Loss :  1.8244121074676514 4.391580581665039 6.2159929275512695
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8177560567855835 4.372278690338135 6.190034866333008
Loss :  1.8218151330947876 4.315213203430176 6.137028217315674
Loss :  1.8168727159500122 4.266509532928467 6.0833821296691895
Total LOSS train 3.9489309347592867 valid 6.156609535217285
CE LOSS train 1.7322592001694899 valid 0.45421817898750305
Contrastive LOSS train 2.2166717290878295 valid 1.0666273832321167
EPOCH 200:
Loss :  1.73300302028656 1.788438081741333 3.5214409828186035
Loss :  1.7323404550552368 2.255479574203491 3.9878201484680176
Loss :  1.7286858558654785 2.2045352458953857 3.9332211017608643
Loss :  1.7308179140090942 3.2202255725860596 4.951043605804443
Loss :  1.7353241443634033 3.6200039386749268 5.35532808303833
Loss :  1.7315559387207031 2.8884365558624268 4.619992256164551
Loss :  1.733749270439148 2.366921901702881 4.100671291351318
Loss :  1.729166865348816 2.3697688579559326 4.098935604095459
Loss :  1.7310923337936401 2.3816497325897217 4.112741947174072
Loss :  1.719497799873352 2.6932945251464844 4.412792205810547
Loss :  1.731478214263916 2.8110153675079346 4.54249382019043
Loss :  1.7396295070648193 3.4764950275421143 5.216124534606934
Loss :  1.7326525449752808 2.473924398422241 4.206576824188232
Loss :  1.7294479608535767 3.252955198287964 4.98240327835083
Loss :  1.7321369647979736 2.2982029914855957 4.030340194702148
Loss :  1.727858543395996 2.216156005859375 3.944014549255371
Loss :  1.7321912050247192 1.5880392789840698 3.320230484008789
Loss :  1.7305245399475098 1.7375977039337158 3.4681222438812256
Loss :  1.7303056716918945 1.7000178098678589 3.430323600769043
Loss :  1.7266404628753662 2.013082504272461 3.739722967147827
  batch 20 loss: 1.7266404628753662, 2.013082504272461, 3.739722967147827
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.730115294456482 2.0726330280303955 3.802748203277588
Loss :  1.7324795722961426 2.7780301570892334 4.510509490966797
Loss :  1.732783555984497 2.401901960372925 4.134685516357422
Loss :  1.7388306856155396 2.3363306522369385 4.075161457061768
Loss :  1.7326877117156982 2.4095065593719482 4.1421942710876465
Loss :  1.7282147407531738 2.224595308303833 3.952810049057007
Loss :  1.737046718597412 2.296074151992798 4.033121109008789
Loss :  1.7283037900924683 2.3279812335968018 4.0562849044799805
Loss :  1.7345690727233887 2.0528340339660645 3.787403106689453
Loss :  1.7259809970855713 2.9392952919006348 4.665276527404785
Loss :  1.740920901298523 2.315769672393799 4.056690692901611
Loss :  1.734574794769287 2.913398027420044 4.64797306060791
Loss :  1.7281615734100342 3.769432544708252 5.497593879699707
Loss :  1.7269304990768433 2.3099756240844727 4.0369062423706055
Loss :  1.73538076877594 2.307996988296509 4.043377876281738
Loss :  1.7349014282226562 2.110337734222412 3.8452391624450684
Loss :  1.73171067237854 2.479947328567505 4.211658000946045
Loss :  1.7284053564071655 2.738250970840454 4.46665620803833
Loss :  1.7309975624084473 3.4453864097595215 5.176383972167969
Loss :  1.730985164642334 4.106560707092285 5.837545871734619
  batch 40 loss: 1.730985164642334, 4.106560707092285, 5.837545871734619
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7302501201629639 2.519465208053589 4.249715328216553
Loss :  1.7301416397094727 2.2783706188201904 4.008512496948242
Loss :  1.7306057214736938 2.5509872436523438 4.281592845916748
Loss :  1.7319138050079346 1.6084630489349365 3.340376853942871
Loss :  1.7332814931869507 1.3965473175048828 3.129828929901123
Loss :  1.7287960052490234 1.8446581363677979 3.5734541416168213
Loss :  1.7289016246795654 1.707696557044983 3.436598300933838
Loss :  1.7303372621536255 1.750951886177063 3.4812891483306885
Loss :  1.7320144176483154 2.14292573928833 3.8749401569366455
Loss :  1.7298678159713745 2.2045724391937256 3.9344401359558105
Loss :  1.725515604019165 2.3441386222839355 4.06965446472168
Loss :  1.730860710144043 2.284125804901123 4.014986515045166
Loss :  1.7370829582214355 1.9234377145767212 3.660520553588867
Loss :  1.7285329103469849 2.246473550796509 3.975006580352783
Loss :  1.7315744161605835 2.0004522800445557 3.7320265769958496
Loss :  1.7279192209243774 2.500335216522217 4.228254318237305
Loss :  1.7345486879348755 2.31847882270813 4.053027629852295
Loss :  1.7376103401184082 1.9634522199630737 3.7010626792907715
Loss :  1.7373623847961426 2.483705997467041 4.221068382263184
Loss :  1.7335015535354614 2.4268412590026855 4.160342693328857
  batch 60 loss: 1.7335015535354614, 2.4268412590026855, 4.160342693328857
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7343449592590332 2.2127654552459717 3.947110414505005
Loss :  1.735427737236023 1.9937533140182495 3.7291810512542725
Loss :  1.7366962432861328 1.776633620262146 3.5133299827575684
Loss :  1.7311501502990723 2.166818141937256 3.897968292236328
Loss :  1.7303894758224487 2.3626549243927 4.093044281005859
Loss :  1.82517671585083 4.458211421966553 6.283388137817383
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8187354803085327 4.4616217613220215 6.280357360839844
Loss :  1.8224560022354126 4.301361560821533 6.123817443847656
Loss :  1.8184130191802979 4.256124496459961 6.07453727722168
Total LOSS train 4.1117214166201075 valid 6.190525054931641
CE LOSS train 1.7317031897031343 valid 0.45460325479507446
Contrastive LOSS train 2.380018212245061 valid 1.0640311241149902
EPOCH 201:
Loss :  1.7332677841186523 3.104074716567993 4.837342262268066
Loss :  1.7325258255004883 3.202103853225708 4.934629440307617
Loss :  1.7294059991836548 2.9846651554107666 4.714071273803711
Loss :  1.7318588495254517 2.51340913772583 4.245267868041992
Loss :  1.735795021057129 3.319591522216797 5.055386543273926
Loss :  1.732837200164795 3.4705536365509033 5.203391075134277
Loss :  1.7333984375 2.1463310718536377 3.8797295093536377
Loss :  1.7305388450622559 2.780238389968872 4.510777473449707
Loss :  1.7316820621490479 2.7077367305755615 4.439418792724609
Loss :  1.7193325757980347 1.8113977909088135 3.5307302474975586
Loss :  1.731698751449585 2.428830623626709 4.160529136657715
Loss :  1.739719033241272 3.7901666164398193 5.529885768890381
Loss :  1.732393503189087 3.1132619380950928 4.84565544128418
Loss :  1.7298357486724854 2.1029322147369385 3.832767963409424
Loss :  1.7334296703338623 2.3633475303649902 4.096776962280273
Loss :  1.7279832363128662 2.203343629837036 3.9313268661499023
Loss :  1.732322335243225 2.1273555755615234 3.859677791595459
Loss :  1.7348816394805908 3.988359212875366 5.723240852355957
Loss :  1.7307345867156982 2.6932971477508545 4.424031734466553
Loss :  1.7267240285873413 2.4181387424468994 4.144862651824951
  batch 20 loss: 1.7267240285873413, 2.4181387424468994, 4.144862651824951
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.729776382446289 1.5848524570465088 3.314628839492798
Loss :  1.7323492765426636 2.239105224609375 3.971454620361328
Loss :  1.7336925268173218 2.0419723987579346 3.775664806365967
Loss :  1.7386668920516968 2.0322983264923096 3.770965099334717
Loss :  1.7342413663864136 2.549302101135254 4.283543586730957
Loss :  1.729893684387207 2.567566156387329 4.297459602355957
Loss :  1.738092303276062 2.3324058055877686 4.070497989654541
Loss :  1.7295056581497192 2.3354110717773438 4.064916610717773
Loss :  1.7366304397583008 3.4111289978027344 5.147759437561035
Loss :  1.7340575456619263 3.625136613845825 5.359194278717041
Loss :  1.744799256324768 3.096097230911255 4.8408966064453125
Loss :  1.7392889261245728 2.4862184524536133 4.2255072593688965
Loss :  1.7353821992874146 2.686248540878296 4.421630859375
Loss :  1.732883095741272 2.335019111633301 4.067902088165283
Loss :  1.7421263456344604 2.4160075187683105 4.1581339836120605
Loss :  1.7406659126281738 2.241575002670288 3.982240915298462
Loss :  1.737446665763855 2.1680123805999756 3.905458927154541
Loss :  1.7354001998901367 2.223055601119995 3.958455801010132
Loss :  1.737454891204834 2.928558349609375 4.666013240814209
Loss :  1.7359648942947388 2.073300361633301 3.80926513671875
  batch 40 loss: 1.7359648942947388, 2.073300361633301, 3.80926513671875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7369052171707153 1.8840609788894653 3.6209661960601807
Loss :  1.7396141290664673 3.1727147102355957 4.912328720092773
Loss :  1.7381823062896729 3.2428717613220215 4.981054306030273
Loss :  1.737955093383789 3.728635311126709 5.466590404510498
Loss :  1.737227439880371 2.8586013317108154 4.595829010009766
Loss :  1.7323137521743774 2.6779866218566895 4.410300254821777
Loss :  1.7328472137451172 2.2378289699554443 3.9706761837005615
Loss :  1.7340795993804932 3.4090030193328857 5.143082618713379
Loss :  1.7330347299575806 3.3983352184295654 5.1313700675964355
Loss :  1.7328225374221802 4.097817420959473 5.830639839172363
Loss :  1.7274686098098755 2.7298922538757324 4.457360744476318
Loss :  1.7309749126434326 2.3775601387023926 4.108534812927246
Loss :  1.7366719245910645 2.175980806350708 3.9126527309417725
Loss :  1.730521321296692 2.5210201740264893 4.251541614532471
Loss :  1.7321192026138306 2.1522438526153564 3.8843631744384766
Loss :  1.7283036708831787 2.201509952545166 3.9298136234283447
Loss :  1.733301043510437 2.5417354106903076 4.275036334991455
Loss :  1.7360625267028809 2.2355620861053467 3.9716246128082275
Loss :  1.7359247207641602 2.6098692417144775 4.345793724060059
Loss :  1.732833743095398 2.4082512855529785 4.141085147857666
  batch 60 loss: 1.732833743095398, 2.4082512855529785, 4.141085147857666
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7322052717208862 3.002976417541504 4.73518180847168
Loss :  1.7348253726959229 2.3847105503082275 4.11953592300415
Loss :  1.7337701320648193 3.0190775394439697 4.752847671508789
Loss :  1.7288349866867065 2.079547643661499 3.808382511138916
Loss :  1.7277755737304688 1.4354945421218872 3.1632699966430664
Loss :  1.826069951057434 4.391034126281738 6.217103958129883
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8194468021392822 4.365756034851074 6.185202598571777
Loss :  1.8219141960144043 4.304287433624268 6.126201629638672
Loss :  1.8204628229141235 4.203321933746338 6.023784637451172
Total LOSS train 4.367860790399405 valid 6.138073205947876
CE LOSS train 1.7336193634913517 valid 0.4551157057285309
Contrastive LOSS train 2.6342414489159216 valid 1.0508304834365845
EPOCH 202:
Loss :  1.7318596839904785 1.9548593759536743 3.6867189407348633
Loss :  1.7301299571990967 2.9130003452301025 4.643130302429199
Loss :  1.7274903059005737 3.0884029865264893 4.815893173217773
Loss :  1.730655550956726 2.996159791946411 4.726815223693848
Loss :  1.7356818914413452 3.390571355819702 5.126253128051758
Loss :  1.7317649126052856 2.968698740005493 4.700463771820068
Loss :  1.733736515045166 2.7781717777252197 4.511908531188965
Loss :  1.7300771474838257 3.0275797843933105 4.757657051086426
Loss :  1.7323414087295532 3.3830041885375977 5.115345478057861
Loss :  1.7209079265594482 3.136342763900757 4.857250690460205
Loss :  1.7333331108093262 3.8159379959106445 5.549271106719971
Loss :  1.740315556526184 2.473351240158081 4.213666915893555
Loss :  1.733903169631958 2.507960557937622 4.24186372756958
Loss :  1.7317739725112915 2.681532621383667 4.413306713104248
Loss :  1.7364394664764404 2.174734592437744 3.9111740589141846
Loss :  1.7303307056427002 2.2631161212921143 3.9934468269348145
Loss :  1.7340946197509766 2.5277676582336426 4.261862277984619
Loss :  1.7316551208496094 2.3020052909851074 4.033660411834717
Loss :  1.7315691709518433 2.7679800987243652 4.499549388885498
Loss :  1.7299754619598389 2.318302631378174 4.048277854919434
  batch 20 loss: 1.7299754619598389, 2.318302631378174, 4.048277854919434
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.732546091079712 1.9353898763656616 3.667935848236084
Loss :  1.7337740659713745 2.1180622577667236 3.8518362045288086
Loss :  1.7348037958145142 2.110107898712158 3.844911575317383
Loss :  1.7387349605560303 2.485687255859375 4.224422454833984
Loss :  1.7346513271331787 2.651103973388672 4.38575553894043
Loss :  1.7306281328201294 3.2094192504882812 4.940047264099121
Loss :  1.7374720573425293 2.8850526809692383 4.622524738311768
Loss :  1.7289024591445923 3.0520882606506348 4.7809906005859375
Loss :  1.7358019351959229 2.690843105316162 4.426645278930664
Loss :  1.7287358045578003 2.768655300140381 4.497391223907471
Loss :  1.7426408529281616 2.177119731903076 3.9197607040405273
Loss :  1.7344284057617188 2.1487810611724854 3.883209466934204
Loss :  1.7286555767059326 1.6235935688018799 3.3522491455078125
Loss :  1.7292674779891968 2.3701422214508057 4.099409580230713
Loss :  1.7364449501037598 2.4352118968963623 4.171656608581543
Loss :  1.736795425415039 3.1172304153442383 4.854025840759277
Loss :  1.7330851554870605 2.7242329120635986 4.457318305969238
Loss :  1.7287522554397583 3.141573190689087 4.870325565338135
Loss :  1.7314989566802979 2.64045786857605 4.371956825256348
Loss :  1.7333877086639404 2.471527099609375 4.2049150466918945
  batch 40 loss: 1.7333877086639404, 2.471527099609375, 4.2049150466918945
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7309502363204956 2.4853591918945312 4.216309547424316
Loss :  1.7302534580230713 2.1793577671051025 3.909611225128174
Loss :  1.7328625917434692 2.375767469406128 4.108630180358887
Loss :  1.7309859991073608 2.242398500442505 3.973384380340576
Loss :  1.7341217994689941 2.1786553859710693 3.9127771854400635
Loss :  1.730800747871399 2.2103781700134277 3.941178798675537
Loss :  1.730019211769104 1.8105710744857788 3.540590286254883
Loss :  1.7313731908798218 1.7765545845031738 3.507927894592285
Loss :  1.7329156398773193 2.4552488327026367 4.188164710998535
Loss :  1.7302621603012085 2.289484977722168 4.019747257232666
Loss :  1.726511836051941 2.3079230785369873 4.034434795379639
Loss :  1.7311612367630005 2.1590821743011475 3.8902435302734375
Loss :  1.7361477613449097 1.7973535060882568 3.533501148223877
Loss :  1.729391098022461 2.0843875408172607 3.8137786388397217
Loss :  1.7318341732025146 2.0180916786193848 3.7499258518218994
Loss :  1.7273635864257812 2.9962122440338135 4.723575592041016
Loss :  1.733621597290039 1.8684040307998657 3.6020255088806152
Loss :  1.7364181280136108 2.0555784702301025 3.791996479034424
Loss :  1.7364933490753174 2.392922878265381 4.129416465759277
Loss :  1.7327353954315186 1.6026222705841064 3.335357666015625
  batch 60 loss: 1.7327353954315186, 1.6026222705841064, 3.335357666015625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7322838306427002 2.5123071670532227 4.244590759277344
Loss :  1.7343183755874634 2.017059803009033 3.751378059387207
Loss :  1.7342079877853394 1.6798768043518066 3.4140849113464355
Loss :  1.7291852235794067 2.0344021320343018 3.763587474822998
Loss :  1.7279788255691528 1.3173097372055054 3.045288562774658
Loss :  1.82490074634552 4.4312543869018555 6.256155014038086
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8186959028244019 4.467066287994385 6.285762310028076
Loss :  1.821620225906372 4.288021564483643 6.109642028808594
Loss :  1.8184500932693481 4.211849212646484 6.030299186706543
Total LOSS train 4.179635543089646 valid 6.170464634895325
CE LOSS train 1.7323575459993803 valid 0.45461252331733704
Contrastive LOSS train 2.447277987920321 valid 1.052962303161621
EPOCH 203:
Loss :  1.7325029373168945 1.8472397327423096 3.579742670059204
Loss :  1.7313340902328491 2.631899833679199 4.363234043121338
Loss :  1.7277488708496094 2.280226707458496 4.0079755783081055
Loss :  1.731235384941101 2.535764217376709 4.2669997215271
Loss :  1.7340693473815918 2.1197357177734375 3.8538050651550293
Loss :  1.7320834398269653 2.022066831588745 3.754150390625
Loss :  1.732351303100586 2.752176523208618 4.484527587890625
Loss :  1.7291287183761597 1.9647057056427002 3.6938343048095703
Loss :  1.7317429780960083 1.6997498273849487 3.431492805480957
Loss :  1.7195414304733276 1.529394268989563 3.2489356994628906
Loss :  1.7317662239074707 2.19396710395813 3.9257333278656006
Loss :  1.7402538061141968 2.1909568309783936 3.931210517883301
Loss :  1.7332148551940918 2.176586151123047 3.9098010063171387
Loss :  1.7303506135940552 2.5084190368652344 4.23876953125
Loss :  1.7353631258010864 2.332275390625 4.067638397216797
Loss :  1.7289518117904663 2.309826374053955 4.038778305053711
Loss :  1.7334760427474976 2.549572467803955 4.283048629760742
Loss :  1.7309978008270264 2.5300920009613037 4.26108980178833
Loss :  1.731502652168274 2.0085461139678955 3.740048885345459
Loss :  1.728623390197754 2.118189811706543 3.846813201904297
  batch 20 loss: 1.728623390197754, 2.118189811706543, 3.846813201904297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7315363883972168 1.8897875547409058 3.621324062347412
Loss :  1.7330844402313232 1.8495066165924072 3.5825910568237305
Loss :  1.7338919639587402 1.7597073316574097 3.4935994148254395
Loss :  1.7387073040008545 3.07578182220459 4.814489364624023
Loss :  1.7345346212387085 2.219072103500366 3.953606605529785
Loss :  1.7294824123382568 1.9632779359817505 3.692760467529297
Loss :  1.7370984554290771 2.138942241668701 3.8760406970977783
Loss :  1.728129506111145 2.4501254558563232 4.178255081176758
Loss :  1.7347491979599 2.3598382472991943 4.094587326049805
Loss :  1.7268798351287842 2.8078453540802 4.534725189208984
Loss :  1.741882562637329 2.2141518592834473 3.9560344219207764
Loss :  1.7347962856292725 2.403455972671509 4.138252258300781
Loss :  1.727894902229309 2.0645430088043213 3.79243803024292
Loss :  1.7284111976623535 1.9495123624801636 3.6779236793518066
Loss :  1.7359824180603027 2.226722002029419 3.9627044200897217
Loss :  1.7361176013946533 1.8826630115509033 3.6187806129455566
Loss :  1.7322912216186523 2.2156624794006348 3.947953701019287
Loss :  1.7280417680740356 2.628666639328003 4.356708526611328
Loss :  1.7306959629058838 3.466522216796875 5.19721794128418
Loss :  1.7323158979415894 3.065032482147217 4.797348499298096
  batch 40 loss: 1.7323158979415894, 3.065032482147217, 4.797348499298096
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7299586534500122 2.0178041458129883 3.747762680053711
Loss :  1.7293606996536255 1.8827918767929077 3.612152576446533
Loss :  1.7321076393127441 2.3102617263793945 4.042369365692139
Loss :  1.7304911613464355 2.5610415935516357 4.291532516479492
Loss :  1.7333563566207886 2.894415855407715 4.627772331237793
Loss :  1.7300320863723755 2.5342161655426025 4.264248371124268
Loss :  1.7289413213729858 1.800830364227295 3.5297718048095703
Loss :  1.7313499450683594 1.9421570301055908 3.67350697517395
Loss :  1.7322949171066284 1.892110824584961 3.624405860900879
Loss :  1.7306804656982422 2.4582836627960205 4.188963890075684
Loss :  1.7269175052642822 1.9657301902770996 3.692647695541382
Loss :  1.7312581539154053 1.8910048007965088 3.622262954711914
Loss :  1.7359905242919922 2.1406753063201904 3.8766658306121826
Loss :  1.7292943000793457 2.051231861114502 3.7805261611938477
Loss :  1.7315646409988403 2.9173481464385986 4.6489129066467285
Loss :  1.7278878688812256 2.926791191101074 4.654679298400879
Loss :  1.7333933115005493 2.6020548343658447 4.335448265075684
Loss :  1.7357462644577026 2.502868890762329 4.238615036010742
Loss :  1.7360246181488037 3.1015784740448 4.8376030921936035
Loss :  1.732404351234436 2.7547755241394043 4.487179756164551
  batch 60 loss: 1.732404351234436, 2.7547755241394043, 4.487179756164551
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.732054591178894 2.5343422889709473 4.266396999359131
Loss :  1.7342548370361328 2.3958983421325684 4.130153179168701
Loss :  1.7340075969696045 2.148806571960449 3.8828141689300537
Loss :  1.7287087440490723 2.3445663452148438 4.073275089263916
Loss :  1.727670431137085 1.1478917598724365 2.8755621910095215
Loss :  1.8214293718338013 4.377937316894531 6.199366569519043
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8149020671844482 4.4468607902526855 6.261762619018555
Loss :  1.8183883428573608 4.294604301452637 6.112992763519287
Loss :  1.814741611480713 4.251410007476807 6.0661516189575195
Total LOSS train 4.018772304975069 valid 6.160068392753601
CE LOSS train 1.731823288477384 valid 0.4536854028701782
Contrastive LOSS train 2.2869490018257728 valid 1.0628525018692017
EPOCH 204:
Loss :  1.732372522354126 2.0825905799865723 3.8149631023406982
Loss :  1.7313216924667358 2.4079718589782715 4.139293670654297
Loss :  1.727868914604187 2.68628191947937 4.414150714874268
Loss :  1.7311345338821411 2.7246205806732178 4.455755233764648
Loss :  1.7349953651428223 2.928821325302124 4.663816452026367
Loss :  1.732576608657837 2.1948587894439697 3.9274353981018066
Loss :  1.7339890003204346 2.543398141860962 4.2773871421813965
Loss :  1.7307184934616089 2.4452998638153076 4.176018238067627
Loss :  1.73249351978302 2.2196807861328125 3.952174186706543
Loss :  1.7210050821304321 2.1830801963806152 3.904085159301758
Loss :  1.733488917350769 2.5475547313690186 4.281043529510498
Loss :  1.7409076690673828 2.7208051681518555 4.461712837219238
Loss :  1.7347825765609741 2.798434257507324 4.533216953277588
Loss :  1.7316302061080933 3.1037280559539795 4.835358142852783
Loss :  1.7364397048950195 2.66312313079834 4.399562835693359
Loss :  1.7304036617279053 2.7729411125183105 4.503344535827637
Loss :  1.7350499629974365 2.7400588989257812 4.475109100341797
Loss :  1.7327345609664917 1.9105719327926636 3.6433064937591553
Loss :  1.7316919565200806 2.0727789402008057 3.804471015930176
Loss :  1.7293026447296143 2.6944360733032227 4.423738479614258
  batch 20 loss: 1.7293026447296143, 2.6944360733032227, 4.423738479614258
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.731571912765503 2.0953171253204346 3.8268890380859375
Loss :  1.732572078704834 2.889124631881714 4.621696472167969
Loss :  1.7342323064804077 2.552062749862671 4.286294937133789
Loss :  1.738193154335022 1.4913023710250854 3.2294955253601074
Loss :  1.7343647480010986 2.3123538494110107 4.046718597412109
Loss :  1.729899287223816 2.5325820446014404 4.262481212615967
Loss :  1.7376677989959717 2.7805609703063965 4.518228530883789
Loss :  1.7284361124038696 2.7163984775543213 4.4448347091674805
Loss :  1.7356561422348022 2.6304304599761963 4.366086483001709
Loss :  1.7280727624893188 2.623037099838257 4.351109981536865
Loss :  1.7425618171691895 2.5195906162261963 4.262152671813965
Loss :  1.735796332359314 2.4845094680786133 4.220305919647217
Loss :  1.7296035289764404 2.13511323928833 3.8647167682647705
Loss :  1.7298229932785034 2.449371576309204 4.179194450378418
Loss :  1.7374188899993896 2.7461471557617188 4.4835662841796875
Loss :  1.7371442317962646 3.3727152347564697 5.109859466552734
Loss :  1.7335638999938965 2.7055504322052 4.439114570617676
Loss :  1.7296066284179688 2.118903875350952 3.848510503768921
Loss :  1.7324340343475342 2.4018564224243164 4.13429069519043
Loss :  1.7335737943649292 1.9800065755844116 3.713580369949341
  batch 40 loss: 1.7335737943649292, 1.9800065755844116, 3.713580369949341
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.731079339981079 2.9899895191192627 4.721068859100342
Loss :  1.7307908535003662 2.1671698093414307 3.897960662841797
Loss :  1.7333451509475708 2.1167104244232178 3.850055694580078
Loss :  1.7309353351593018 2.6480143070220947 4.3789496421813965
Loss :  1.7341066598892212 1.4987719058990479 3.2328786849975586
Loss :  1.7314281463623047 2.2469818592071533 3.978410005569458
Loss :  1.7300047874450684 2.5793354511260986 4.309340476989746
Loss :  1.7313507795333862 2.6815779209136963 4.412928581237793
Loss :  1.7329692840576172 1.7660845518112183 3.499053955078125
Loss :  1.7299619913101196 1.8724550008773804 3.6024169921875
Loss :  1.7264494895935059 2.461169958114624 4.187619209289551
Loss :  1.730918526649475 2.557196855545044 4.288115501403809
Loss :  1.7359380722045898 1.805503010749817 3.541440963745117
Loss :  1.7292046546936035 2.092376470565796 3.8215811252593994
Loss :  1.7318655252456665 1.7650073766708374 3.496872901916504
Loss :  1.7272311449050903 2.0742642879486084 3.8014955520629883
Loss :  1.7334259748458862 2.296990394592285 4.030416488647461
Loss :  1.7363401651382446 2.2825331687927246 4.01887321472168
Loss :  1.7367089986801147 2.5456583499908447 4.28236722946167
Loss :  1.7330284118652344 3.015556573867798 4.748584747314453
  batch 60 loss: 1.7330284118652344, 3.015556573867798, 4.748584747314453
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7324318885803223 2.833366632461548 4.565798759460449
Loss :  1.7343928813934326 2.0951671600341797 3.8295600414276123
Loss :  1.734100103378296 2.2341649532318115 3.9682650566101074
Loss :  1.7295914888381958 3.0854833126068115 4.815074920654297
Loss :  1.7285658121109009 1.673097014427185 3.401662826538086
Loss :  1.8289711475372314 4.45205545425415 6.281026840209961
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.822505235671997 4.400991916656494 6.22349739074707
Loss :  1.825697898864746 4.244167327880859 6.0698652267456055
Loss :  1.8214247226715088 4.1616387367248535 5.983063697814941
Total LOSS train 4.153474807739258 valid 6.1393632888793945
CE LOSS train 1.732511777144212 valid 0.4553561806678772
Contrastive LOSS train 2.420963030595046 valid 1.0404096841812134
EPOCH 205:
Loss :  1.733053207397461 1.9410611391067505 3.674114227294922
Loss :  1.7306386232376099 1.744503140449524 3.475141763687134
Loss :  1.727617621421814 1.2344056367874146 2.9620232582092285
Loss :  1.7311484813690186 1.6476473808288574 3.378795862197876
Loss :  1.7342448234558105 1.938268780708313 3.672513484954834
Loss :  1.7316864728927612 1.7145296335220337 3.446216106414795
Loss :  1.7322670221328735 2.8139660358428955 4.546233177185059
Loss :  1.729634404182434 2.394157886505127 4.1237921714782715
Loss :  1.7317826747894287 3.0360567569732666 4.767839431762695
Loss :  1.7199214696884155 2.994025707244873 4.713947296142578
Loss :  1.732202410697937 2.8559982776641846 4.588200569152832
Loss :  1.741057276725769 2.9939026832580566 4.734960079193115
Loss :  1.7333379983901978 3.2311935424804688 4.964531421661377
Loss :  1.7311162948608398 2.7586374282836914 4.489753723144531
Loss :  1.734609842300415 3.4666786193847656 5.201288223266602
Loss :  1.732040524482727 3.4474940299987793 5.179534435272217
Loss :  1.741952657699585 3.2557356357574463 4.997688293457031
Loss :  1.7369053363800049 2.649287223815918 4.386192321777344
Loss :  1.7395670413970947 2.3782010078430176 4.117768287658691
Loss :  1.7365442514419556 2.2895400524139404 4.0260844230651855
  batch 20 loss: 1.7365442514419556, 2.2895400524139404, 4.0260844230651855
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7369123697280884 2.4604392051696777 4.197351455688477
Loss :  1.741577386856079 2.5065677165985107 4.24814510345459
Loss :  1.7373489141464233 2.6098525524139404 4.347201347351074
Loss :  1.7463643550872803 2.618762254714966 4.365126609802246
Loss :  1.7386531829833984 2.79148530960083 4.5301384925842285
Loss :  1.7355246543884277 3.1115500926971436 4.847074508666992
Loss :  1.7419370412826538 2.6038105487823486 4.345747470855713
Loss :  1.7342429161071777 2.771075963973999 4.505318641662598
Loss :  1.7390635013580322 1.8367865085601807 3.575850009918213
Loss :  1.7338751554489136 3.2588067054748535 4.992681980133057
Loss :  1.7427729368209839 2.69736647605896 4.440139293670654
Loss :  1.7364983558654785 2.3308451175689697 4.067343711853027
Loss :  1.7315155267715454 1.9056873321533203 3.637202739715576
Loss :  1.729516625404358 1.9457621574401855 3.675278663635254
Loss :  1.7380906343460083 2.425642251968384 4.163733005523682
Loss :  1.7380651235580444 2.1700689792633057 3.9081339836120605
Loss :  1.7351112365722656 2.0626919269561768 3.7978031635284424
Loss :  1.7319238185882568 1.7297728061676025 3.4616966247558594
Loss :  1.7336912155151367 2.1384427547454834 3.87213397026062
Loss :  1.735727310180664 2.2510807514190674 3.9868080615997314
  batch 40 loss: 1.735727310180664, 2.2510807514190674, 3.9868080615997314
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7327862977981567 2.4492790699005127 4.182065486907959
Loss :  1.7325332164764404 2.9495463371276855 4.682079315185547
Loss :  1.7359275817871094 2.573306083679199 4.309233665466309
Loss :  1.7333247661590576 3.107736349105835 4.841061115264893
Loss :  1.7361115217208862 2.430799722671509 4.1669111251831055
Loss :  1.733307957649231 2.10780930519104 3.8411173820495605
Loss :  1.7321653366088867 2.1077511310577393 3.839916467666626
Loss :  1.7336386442184448 2.0331931114196777 3.766831874847412
Loss :  1.7359554767608643 2.47078275680542 4.206738471984863
Loss :  1.7330242395401 2.438441276550293 4.1714653968811035
Loss :  1.7296503782272339 1.9268724918365479 3.656522750854492
Loss :  1.7335436344146729 2.8703527450561523 4.603896141052246
Loss :  1.7377468347549438 2.0277512073516846 3.765498161315918
Loss :  1.7308772802352905 2.8555026054382324 4.5863800048828125
Loss :  1.732244849205017 2.1588757038116455 3.891120433807373
Loss :  1.7287514209747314 1.7614028453826904 3.490154266357422
Loss :  1.7349636554718018 1.7154563665390015 3.4504199028015137
Loss :  1.7374088764190674 2.0002317428588867 3.737640619277954
Loss :  1.7368650436401367 2.9927737712860107 4.729639053344727
Loss :  1.732803225517273 2.0997610092163086 3.832564353942871
  batch 60 loss: 1.732803225517273, 2.0997610092163086, 3.832564353942871
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7325083017349243 1.9503761529922485 3.682884454727173
Loss :  1.7336878776550293 2.5499513149261475 4.283638954162598
Loss :  1.7342653274536133 2.143326759338379 3.877592086791992
Loss :  1.7289308309555054 1.9533864259719849 3.6823172569274902
Loss :  1.727757453918457 1.2114951610565186 2.9392526149749756
Loss :  1.838517427444458 4.4209418296813965 6.259459495544434
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8320066928863525 4.362452983856201 6.194459915161133
Loss :  1.8352975845336914 4.300838947296143 6.136136531829834
Loss :  1.8314695358276367 4.263859748840332 6.095329284667969
Total LOSS train 4.132745673106267 valid 6.171346306800842
CE LOSS train 1.7343157034653884 valid 0.4578673839569092
Contrastive LOSS train 2.3984299916487473 valid 1.065964937210083
EPOCH 206:
Loss :  1.7326208353042603 2.6920418739318848 4.4246625900268555
Loss :  1.7315266132354736 2.061718702316284 3.793245315551758
Loss :  1.727583646774292 2.5351245403289795 4.2627081871032715
Loss :  1.7308074235916138 2.104719400405884 3.835526943206787
Loss :  1.7335145473480225 2.5404257774353027 4.273940086364746
Loss :  1.7323113679885864 2.479053020477295 4.211364269256592
Loss :  1.730967402458191 3.0129828453063965 4.743950366973877
Loss :  1.7293339967727661 2.5113096237182617 4.240643501281738
Loss :  1.7311891317367554 2.4733827114105225 4.204571723937988
Loss :  1.7182374000549316 2.908132791519165 4.626370429992676
Loss :  1.7320117950439453 2.664914131164551 4.396925926208496
Loss :  1.741377592086792 2.7182605266571045 4.4596381187438965
Loss :  1.7329682111740112 2.24873948097229 3.9817075729370117
Loss :  1.730143666267395 2.1364951133728027 3.866638660430908
Loss :  1.7334082126617432 2.3731892108917236 4.106597423553467
Loss :  1.7282943725585938 2.6113345623016357 4.339629173278809
Loss :  1.7331684827804565 2.145660400390625 3.878829002380371
Loss :  1.730373501777649 1.7669929265975952 3.497366428375244
Loss :  1.7317689657211304 2.4894192218780518 4.221188068389893
Loss :  1.7278614044189453 2.2030036449432373 3.9308650493621826
  batch 20 loss: 1.7278614044189453, 2.2030036449432373, 3.9308650493621826
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7310571670532227 3.1313843727111816 4.862441539764404
Loss :  1.7328028678894043 2.45286226272583 4.185665130615234
Loss :  1.7331620454788208 2.522447347640991 4.255609512329102
Loss :  1.7390128374099731 2.61247181892395 4.351484775543213
Loss :  1.7343822717666626 1.8266388177871704 3.561021089553833
Loss :  1.7294306755065918 2.273688554763794 4.003119468688965
Loss :  1.7377058267593384 2.4610836505889893 4.198789596557617
Loss :  1.7294191122055054 2.693131923675537 4.422551155090332
Loss :  1.7363793849945068 2.864837408065796 4.601216793060303
Loss :  1.7286237478256226 3.432018280029297 5.160642147064209
Loss :  1.7424615621566772 2.995645046234131 4.738106727600098
Loss :  1.7358179092407227 2.9098424911499023 4.645660400390625
Loss :  1.7304319143295288 2.2057266235351562 3.9361586570739746
Loss :  1.7292165756225586 2.1739792823791504 3.903195858001709
Loss :  1.7373789548873901 3.3588006496429443 5.096179485321045
Loss :  1.7374187707901 3.1607279777526855 4.898146629333496
Loss :  1.7344558238983154 2.237992763519287 3.9724485874176025
Loss :  1.7304643392562866 2.113213539123535 3.8436779975891113
Loss :  1.7337300777435303 1.78669273853302 3.52042293548584
Loss :  1.7340152263641357 1.7409119606018066 3.4749271869659424
  batch 40 loss: 1.7340152263641357, 1.7409119606018066, 3.4749271869659424
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7322078943252563 2.0294878482818604 3.7616958618164062
Loss :  1.7318482398986816 1.5109773874282837 3.242825508117676
Loss :  1.7336426973342896 1.7320395708084106 3.4656822681427
Loss :  1.7317590713500977 2.117697238922119 3.849456310272217
Loss :  1.7344478368759155 2.692213535308838 4.426661491394043
Loss :  1.731627345085144 2.924621105194092 4.656248569488525
Loss :  1.731094241142273 2.4784021377563477 4.20949649810791
Loss :  1.7318229675292969 3.507591724395752 5.239414691925049
Loss :  1.7341365814208984 2.2121896743774414 3.94632625579834
Loss :  1.7305641174316406 1.8488332033157349 3.579397201538086
Loss :  1.7275577783584595 3.2616398334503174 4.989197731018066
Loss :  1.7318559885025024 2.676208972930908 4.408064842224121
Loss :  1.7370768785476685 2.974581003189087 4.711658000946045
Loss :  1.7287805080413818 3.298804759979248 5.027585029602051
Loss :  1.7310870885849 3.9948792457580566 5.725966453552246
Loss :  1.7275395393371582 2.4833102226257324 4.210849761962891
Loss :  1.7347469329833984 2.1751110553741455 3.909857988357544
Loss :  1.7359888553619385 1.9755980968475342 3.7115869522094727
Loss :  1.736128330230713 2.263017177581787 3.9991455078125
Loss :  1.7327853441238403 2.0293707847595215 3.7621560096740723
  batch 60 loss: 1.7327853441238403, 2.0293707847595215, 3.7621560096740723
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.732243299484253 2.6271698474884033 4.359413146972656
Loss :  1.7337923049926758 2.7010409832000732 4.434833526611328
Loss :  1.733849048614502 2.4947900772094727 4.228639125823975
Loss :  1.729727864265442 2.475900411605835 4.205628395080566
Loss :  1.728326439857483 1.8298691511154175 3.5581955909729004
Loss :  1.809715747833252 4.3847551345825195 6.1944708824157715
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8059146404266357 4.4594035148620605 6.265317916870117
Loss :  1.8072409629821777 4.310651779174805 6.117892742156982
Loss :  1.8065581321716309 4.231115341186523 6.037673473358154
Total LOSS train 4.223812572772687 valid 6.153838753700256
CE LOSS train 1.7323303516094501 valid 0.4516395330429077
Contrastive LOSS train 2.4914822009893562 valid 1.0577788352966309
EPOCH 207:
Loss :  1.732033371925354 1.7612147331237793 3.4932479858398438
Loss :  1.730123519897461 2.1362268924713135 3.8663504123687744
Loss :  1.7275320291519165 2.4777865409851074 4.205318450927734
Loss :  1.7302321195602417 2.4431233406066895 4.173355579376221
Loss :  1.7352092266082764 2.3943560123443604 4.129565238952637
Loss :  1.7305796146392822 1.7249715328216553 3.4555511474609375
Loss :  1.7339210510253906 2.4465763568878174 4.180497169494629
Loss :  1.729500651359558 2.096470594406128 3.8259711265563965
Loss :  1.7321921586990356 2.3334126472473145 4.0656046867370605
Loss :  1.7210502624511719 2.9102096557617188 4.631259918212891
Loss :  1.7325870990753174 2.887275457382202 4.6198625564575195
Loss :  1.7404537200927734 2.84443736076355 4.584891319274902
Loss :  1.733920693397522 2.232994794845581 3.9669156074523926
Loss :  1.7315324544906616 2.470801830291748 4.202334403991699
Loss :  1.7359586954116821 1.6400883197784424 3.376047134399414
Loss :  1.7299261093139648 1.6971460580825806 3.427072048187256
Loss :  1.7339671850204468 1.8024832010269165 3.5364503860473633
Loss :  1.7313628196716309 1.9040961265563965 3.6354589462280273
Loss :  1.7313772439956665 1.7226930856704712 3.4540703296661377
Loss :  1.7285492420196533 2.0340840816497803 3.7626333236694336
  batch 20 loss: 1.7285492420196533, 2.0340840816497803, 3.7626333236694336
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.731095552444458 2.092977523803711 3.824073076248169
Loss :  1.7323025465011597 1.638329267501831 3.370631694793701
Loss :  1.7327959537506104 1.5625510215759277 3.295346975326538
Loss :  1.7381807565689087 1.547195315361023 3.2853760719299316
Loss :  1.734200119972229 1.9411962032318115 3.67539644241333
Loss :  1.7283636331558228 1.827826738357544 3.5561904907226562
Loss :  1.7363362312316895 1.7916380167007446 3.5279741287231445
Loss :  1.7272920608520508 2.038691520690918 3.7659835815429688
Loss :  1.7346047163009644 2.2333688735961914 3.9679737091064453
Loss :  1.726839542388916 2.6033897399902344 4.33022928237915
Loss :  1.7425422668457031 1.8872878551483154 3.6298301219940186
Loss :  1.7355278730392456 1.745152235031128 3.480679988861084
Loss :  1.7287392616271973 1.6100366115570068 3.338775873184204
Loss :  1.729445457458496 1.8964492082595825 3.625894546508789
Loss :  1.7374770641326904 2.211409091949463 3.9488861560821533
Loss :  1.737263798713684 2.1235172748565674 3.860781192779541
Loss :  1.7336366176605225 1.7459750175476074 3.47961163520813
Loss :  1.7289541959762573 1.9122411012649536 3.641195297241211
Loss :  1.731372356414795 1.9349629878997803 3.666335344314575
Loss :  1.7334917783737183 1.875076413154602 3.6085681915283203
  batch 40 loss: 1.7334917783737183, 1.875076413154602, 3.6085681915283203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7312828302383423 3.5525543689727783 5.28383731842041
Loss :  1.730116605758667 3.041565179824829 4.771681785583496
Loss :  1.7342530488967896 2.5855660438537598 4.31981897354126
Loss :  1.7309327125549316 2.3042173385620117 4.035150051116943
Loss :  1.7350114583969116 1.9411903619766235 3.676201820373535
Loss :  1.7314187288284302 2.096524715423584 3.8279433250427246
Loss :  1.7298383712768555 2.124394655227661 3.8542330265045166
Loss :  1.7326104640960693 2.939929723739624 4.672540187835693
Loss :  1.7337461709976196 2.8491711616516113 4.582917213439941
Loss :  1.7316515445709229 2.698077440261841 4.429728984832764
Loss :  1.7280166149139404 2.494306802749634 4.222323417663574
Loss :  1.7320905923843384 2.702422857284546 4.434513568878174
Loss :  1.736735224723816 2.3325042724609375 4.069239616394043
Loss :  1.729749321937561 2.9211618900299072 4.650911331176758
Loss :  1.731631875038147 1.9589840173721313 3.6906158924102783
Loss :  1.7276322841644287 2.803300619125366 4.530932903289795
Loss :  1.73411226272583 2.8577210903167725 4.591833114624023
Loss :  1.7359832525253296 1.9863555431365967 3.7223386764526367
Loss :  1.7362252473831177 2.1157782077789307 3.852003574371338
Loss :  1.7326087951660156 1.4908981323242188 3.2235069274902344
  batch 60 loss: 1.7326087951660156, 1.4908981323242188, 3.2235069274902344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7323201894760132 1.718437910079956 3.4507579803466797
Loss :  1.7341996431350708 2.912142276763916 4.646341800689697
Loss :  1.734772801399231 3.0933499336242676 4.828122615814209
Loss :  1.729170560836792 2.4039292335510254 4.133099555969238
Loss :  1.7283239364624023 1.9206311702728271 3.6489551067352295
Loss :  1.825853705406189 4.483243942260742 6.309097766876221
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8200074434280396 4.395947456359863 6.215954780578613
Loss :  1.822678804397583 4.348174571990967 6.170853614807129
Loss :  1.8202180862426758 4.294267654418945 6.114485740661621
Total LOSS train 3.9480267744797928 valid 6.202597975730896
CE LOSS train 1.7322600859862107 valid 0.45505452156066895
Contrastive LOSS train 2.215766701331505 valid 1.0735669136047363
EPOCH 208:
Loss :  1.732779622077942 2.3488049507141113 4.081584453582764
Loss :  1.7319806814193726 2.611525297164917 4.343505859375
Loss :  1.7279738187789917 2.282485008239746 4.010458946228027
Loss :  1.730992078781128 2.451209306716919 4.182201385498047
Loss :  1.7342023849487305 2.3700690269470215 4.104271411895752
Loss :  1.7317192554473877 2.2961459159851074 4.027865409851074
Loss :  1.7320970296859741 1.9419687986373901 3.6740658283233643
Loss :  1.7295281887054443 1.963402509689331 3.6929306983947754
Loss :  1.7310481071472168 1.9734162092208862 3.7044644355773926
Loss :  1.7189912796020508 2.876681327819824 4.595672607421875
Loss :  1.7320985794067383 3.282315731048584 5.014414310455322
Loss :  1.7399072647094727 2.343106508255005 4.083013534545898
Loss :  1.7337803840637207 2.0775954723358154 3.811375856399536
Loss :  1.730778694152832 2.4026620388031006 4.133440971374512
Loss :  1.7344383001327515 2.3554646968841553 4.089902877807617
Loss :  1.728771686553955 2.4261748790740967 4.154946327209473
Loss :  1.734529733657837 2.8369593620300293 4.571489334106445
Loss :  1.7321956157684326 2.8577799797058105 4.589975357055664
Loss :  1.7300961017608643 1.585720419883728 3.3158164024353027
Loss :  1.7288521528244019 2.2057621479034424 3.9346141815185547
  batch 20 loss: 1.7288521528244019, 2.2057621479034424, 3.9346141815185547
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.731674313545227 1.8499966859817505 3.5816709995269775
Loss :  1.7320361137390137 2.022280216217041 3.7543163299560547
Loss :  1.733641505241394 2.622692823410034 4.356334209442139
Loss :  1.7383935451507568 1.8804787397384644 3.6188721656799316
Loss :  1.733194351196289 2.3030166625976562 4.036211013793945
Loss :  1.7282600402832031 1.9226861000061035 3.6509461402893066
Loss :  1.737180471420288 1.8488423824310303 3.5860228538513184
Loss :  1.7290377616882324 1.7744725942611694 3.5035104751586914
Loss :  1.7350752353668213 1.9361493587493896 3.671224594116211
Loss :  1.7263377904891968 2.3974273204803467 4.123764991760254
Loss :  1.7409323453903198 2.7675654888153076 4.508497714996338
Loss :  1.735460638999939 2.0949089527130127 3.830369472503662
Loss :  1.7293806076049805 2.5747275352478027 4.304108142852783
Loss :  1.7283586263656616 2.0640435218811035 3.7924022674560547
Loss :  1.7366224527359009 2.6164703369140625 4.353092670440674
Loss :  1.7362592220306396 2.460291624069214 4.1965508460998535
Loss :  1.7334985733032227 2.5178542137145996 4.251352787017822
Loss :  1.7294620275497437 2.2870306968688965 4.01649284362793
Loss :  1.732273817062378 2.113891124725342 3.8461649417877197
Loss :  1.7328224182128906 1.9315564632415771 3.6643788814544678
  batch 40 loss: 1.7328224182128906, 1.9315564632415771, 3.6643788814544678
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7314265966415405 2.180316209793091 3.911742687225342
Loss :  1.730595588684082 1.4543536901474 3.1849493980407715
Loss :  1.731940746307373 1.859878659248352 3.5918192863464355
Loss :  1.7325280904769897 1.6378628015518188 3.3703908920288086
Loss :  1.7345471382141113 1.9975253343582153 3.732072353363037
Loss :  1.730374813079834 3.0544581413269043 4.784832954406738
Loss :  1.731826663017273 2.36017107963562 4.0919976234436035
Loss :  1.732453465461731 2.2621891498565674 3.994642734527588
Loss :  1.7341700792312622 2.9973554611206055 4.731525421142578
Loss :  1.7314397096633911 2.7308156490325928 4.462255477905273
Loss :  1.7281019687652588 2.684727668762207 4.412829399108887
Loss :  1.7326754331588745 2.7038071155548096 4.4364824295043945
Loss :  1.738444209098816 3.117550849914551 4.855995178222656
Loss :  1.730596899986267 2.2057149410247803 3.936311721801758
Loss :  1.7329986095428467 2.2582309246063232 3.99122953414917
Loss :  1.730435848236084 2.459411859512329 4.189847946166992
Loss :  1.7359015941619873 2.1910529136657715 3.926954507827759
Loss :  1.7375357151031494 2.1395938396453857 3.877129554748535
Loss :  1.7381700277328491 2.3384792804718018 4.076649188995361
Loss :  1.7348079681396484 3.7948076725006104 5.52961540222168
  batch 60 loss: 1.7348079681396484, 3.7948076725006104, 5.52961540222168
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.734458565711975 2.7946462631225586 4.529104709625244
Loss :  1.7358949184417725 2.9947023391723633 4.730597496032715
Loss :  1.7388306856155396 3.972231388092041 5.711061954498291
Loss :  1.731685757637024 2.366499900817871 4.0981855392456055
Loss :  1.7304506301879883 1.3051944971084595 3.035645008087158
Loss :  1.8072806596755981 4.389503002166748 6.196783542633057
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8035681247711182 4.342747688293457 6.146315574645996
Loss :  1.8046836853027344 4.295069217681885 6.099752902984619
Loss :  1.804975152015686 4.208398342132568 6.013373374938965
Total LOSS train 4.091633275838999 valid 6.114056348800659
CE LOSS train 1.7325993006046 valid 0.4512437880039215
Contrastive LOSS train 2.3590340009102455 valid 1.052099585533142
EPOCH 209:
Loss :  1.7345988750457764 1.4984222650527954 3.2330212593078613
Loss :  1.732116937637329 1.9946377277374268 3.726754665374756
Loss :  1.7295477390289307 1.68215012550354 3.4116978645324707
Loss :  1.7325721979141235 2.1630539894104004 3.8956260681152344
Loss :  1.7367339134216309 2.318969249725342 4.055703163146973
Loss :  1.7328110933303833 1.7075518369674683 3.4403629302978516
Loss :  1.734322428703308 2.5098650455474854 4.244187355041504
Loss :  1.7303828001022339 3.095877170562744 4.826260089874268
Loss :  1.7323479652404785 2.805788278579712 4.5381364822387695
Loss :  1.7206087112426758 2.8224058151245117 4.5430145263671875
Loss :  1.7320456504821777 2.6588404178619385 4.390886306762695
Loss :  1.741162657737732 2.4365248680114746 4.177687644958496
Loss :  1.7336697578430176 2.7333836555480957 4.467053413391113
Loss :  1.7317123413085938 2.3763201236724854 4.1080322265625
Loss :  1.7354298830032349 2.5447752475738525 4.280205249786377
Loss :  1.7300667762756348 2.116880178451538 3.846946954727173
Loss :  1.7334846258163452 2.7416751384735107 4.475159645080566
Loss :  1.7314924001693726 2.399571418762207 4.131063938140869
Loss :  1.7316017150878906 2.6569910049438477 4.388592720031738
Loss :  1.7282921075820923 3.7447755336761475 5.473067760467529
  batch 20 loss: 1.7282921075820923, 3.7447755336761475, 5.473067760467529
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.731495976448059 3.1844828128814697 4.915978908538818
Loss :  1.7326877117156982 2.6045784950256348 4.337265968322754
Loss :  1.7335469722747803 1.9832255840301514 3.7167725563049316
Loss :  1.738668441772461 2.0983760356903076 3.8370444774627686
Loss :  1.7338840961456299 2.5340704917907715 4.2679548263549805
Loss :  1.7291109561920166 2.591717004776001 4.320827960968018
Loss :  1.737109661102295 3.0029332637786865 4.740042686462402
Loss :  1.7284884452819824 3.268038034439087 4.996526718139648
Loss :  1.735049843788147 2.689772367477417 4.4248223304748535
Loss :  1.7264922857284546 3.209027051925659 4.935519218444824
Loss :  1.7411389350891113 2.7088277339935303 4.4499664306640625
Loss :  1.7351795434951782 1.9018371105194092 3.637016773223877
Loss :  1.7285144329071045 1.6886680126190186 3.417182445526123
Loss :  1.7278923988342285 1.8258782625198364 3.5537705421447754
Loss :  1.735742449760437 2.459319829940796 4.195062160491943
Loss :  1.7357840538024902 2.1737072467803955 3.9094913005828857
Loss :  1.732883334159851 2.24107027053833 3.9739537239074707
Loss :  1.7295838594436646 1.812227725982666 3.541811466217041
Loss :  1.732157588005066 2.7133545875549316 4.445512294769287
Loss :  1.733312726020813 1.6769232749938965 3.41023588180542
  batch 40 loss: 1.733312726020813, 1.6769232749938965, 3.41023588180542
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7317367792129517 1.8739588260650635 3.6056957244873047
Loss :  1.7302618026733398 1.5789839029312134 3.3092455863952637
Loss :  1.733270525932312 2.5418312549591064 4.275101661682129
Loss :  1.7321256399154663 1.9740389585494995 3.706164598464966
Loss :  1.735681176185608 2.6528806686401367 4.388561725616455
Loss :  1.732983946800232 2.1591451168060303 3.8921289443969727
Loss :  1.7332427501678467 2.8199517726898193 4.553194522857666
Loss :  1.733124852180481 1.9820665121078491 3.71519136428833
Loss :  1.7372334003448486 1.9665706157684326 3.7038040161132812
Loss :  1.732314944267273 1.822363257408142 3.554678201675415
Loss :  1.7305235862731934 2.3377747535705566 4.06829833984375
Loss :  1.7341537475585938 2.229020357131958 3.9631741046905518
Loss :  1.7396001815795898 2.091599464416504 3.8311996459960938
Loss :  1.7309738397598267 2.6351704597473145 4.366144180297852
Loss :  1.7329323291778564 2.14652943611145 3.8794617652893066
Loss :  1.7302197217941284 2.367724895477295 4.097944736480713
Loss :  1.7360693216323853 1.7816203832626343 3.5176897048950195
Loss :  1.737817645072937 1.6085293292999268 3.346346855163574
Loss :  1.7377612590789795 2.227199077606201 3.9649603366851807
Loss :  1.7350994348526 1.7667077779769897 3.50180721282959
  batch 60 loss: 1.7350994348526, 1.7667077779769897, 3.50180721282959
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7345949411392212 2.137521982192993 3.872117042541504
Loss :  1.7361196279525757 2.1958587169647217 3.931978225708008
Loss :  1.7360804080963135 1.9835467338562012 3.7196271419525146
Loss :  1.7309017181396484 3.4308693408966064 5.161770820617676
Loss :  1.7297931909561157 2.3556325435638428 4.085425853729248
Loss :  1.8294310569763184 4.392247200012207 6.221678256988525
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8240694999694824 4.454615592956543 6.278685092926025
Loss :  1.8260221481323242 4.340819835662842 6.166841983795166
Loss :  1.8255771398544312 4.308917045593262 6.134494304656982
Total LOSS train 4.0721835576570955 valid 6.200424909591675
CE LOSS train 1.733051400918227 valid 0.4563942849636078
Contrastive LOSS train 2.3391321604068462 valid 1.0772292613983154
EPOCH 210:
Loss :  1.7347943782806396 1.7247480154037476 3.4595422744750977
Loss :  1.7330427169799805 1.965826153755188 3.698868751525879
Loss :  1.7296218872070312 2.012331247329712 3.741953134536743
Loss :  1.7327594757080078 1.935730218887329 3.668489694595337
Loss :  1.7353276014328003 2.5842878818511963 4.319615364074707
Loss :  1.7334017753601074 3.330829381942749 5.064230918884277
Loss :  1.7332274913787842 2.156325340270996 3.8895528316497803
Loss :  1.730443000793457 2.515120029449463 4.24556303024292
Loss :  1.7320650815963745 1.8863980770111084 3.6184630393981934
Loss :  1.71980881690979 2.0697219371795654 3.7895307540893555
Loss :  1.7319791316986084 2.3241539001464844 4.056133270263672
Loss :  1.740600347518921 2.1476073265075684 3.8882076740264893
Loss :  1.7332215309143066 2.449941396713257 4.183162689208984
Loss :  1.731202244758606 2.4196765422821045 4.15087890625
Loss :  1.7347184419631958 2.8068206310272217 4.541539192199707
Loss :  1.7292944192886353 2.490051031112671 4.219345569610596
Loss :  1.733741283416748 2.528567314147949 4.262308597564697
Loss :  1.7309174537658691 2.62753963470459 4.358457088470459
Loss :  1.7308547496795654 2.050909996032715 3.7817647457122803
Loss :  1.729189157485962 2.0219974517822266 3.7511866092681885
  batch 20 loss: 1.729189157485962, 2.0219974517822266, 3.7511866092681885
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.731839895248413 1.864650845527649 3.5964908599853516
Loss :  1.732567310333252 3.1339547634124756 4.866521835327148
Loss :  1.7341408729553223 1.921694040298462 3.655834913253784
Loss :  1.7382800579071045 2.554922342300415 4.2932024002075195
Loss :  1.7340160608291626 2.808251142501831 4.542267322540283
Loss :  1.729740858078003 2.6096980571746826 4.3394389152526855
Loss :  1.737226128578186 2.8163692951202393 4.553595542907715
Loss :  1.7275735139846802 2.5354502201080322 4.263023853302002
Loss :  1.735290765762329 2.137094736099243 3.8723855018615723
Loss :  1.7274844646453857 3.5012857913970947 5.2287702560424805
Loss :  1.7424108982086182 2.788743734359741 4.531154632568359
Loss :  1.7367254495620728 2.2337121963500977 3.970437526702881
Loss :  1.7293436527252197 1.8215547800064087 3.550898551940918
Loss :  1.7302166223526 2.2287180423736572 3.958934783935547
Loss :  1.7358956336975098 2.7665553092956543 4.502450942993164
Loss :  1.7368004322052002 2.0052707195281982 3.7420711517333984
Loss :  1.7330325841903687 2.1786794662475586 3.911712169647217
Loss :  1.7286230325698853 2.371933937072754 4.10055685043335
Loss :  1.7317289113998413 2.769334554672241 4.501063346862793
Loss :  1.7335010766983032 1.7017886638641357 3.4352898597717285
  batch 40 loss: 1.7335010766983032, 1.7017886638641357, 3.4352898597717285
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.73050057888031 1.8871805667877197 3.6176810264587402
Loss :  1.7297921180725098 1.711026906967163 3.440819025039673
Loss :  1.7338972091674805 2.1826584339141846 3.916555643081665
Loss :  1.7305189371109009 2.3929858207702637 4.123504638671875
Loss :  1.7345380783081055 2.3586935997009277 4.093231678009033
Loss :  1.7321523427963257 1.7506673336029053 3.4828195571899414
Loss :  1.730596899986267 1.7310869693756104 3.461683750152588
Loss :  1.7327098846435547 1.7260500192642212 3.4587597846984863
Loss :  1.7338663339614868 3.5380544662475586 5.271920680999756
Loss :  1.7316125631332397 2.3986332416534424 4.130245685577393
Loss :  1.7279977798461914 2.2386114597320557 3.966609239578247
Loss :  1.7319605350494385 1.9910210371017456 3.7229814529418945
Loss :  1.7371190786361694 2.9905076026916504 4.727626800537109
Loss :  1.7296444177627563 2.779151678085327 4.508796215057373
Loss :  1.7322726249694824 2.718461036682129 4.450733661651611
Loss :  1.7288230657577515 2.345729112625122 4.074552059173584
Loss :  1.7353627681732178 2.5723114013671875 4.307674407958984
Loss :  1.7380542755126953 2.7720119953155518 4.510066032409668
Loss :  1.738013744354248 2.49247407913208 4.230487823486328
Loss :  1.7341958284378052 2.5778756141662598 4.312071323394775
  batch 60 loss: 1.7341958284378052, 2.5778756141662598, 4.312071323394775
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7343461513519287 2.4729833602905273 4.207329750061035
Loss :  1.7355386018753052 3.111345052719116 4.846883773803711
Loss :  1.736334204673767 2.0578255653381348 3.7941598892211914
Loss :  1.7315720319747925 2.6530673503875732 4.384639263153076
Loss :  1.7308008670806885 1.5370495319366455 3.267850399017334
Loss :  1.812425971031189 4.398133277893066 6.210559368133545
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8075958490371704 4.357295036315918 6.164890766143799
Loss :  1.8096619844436646 4.311445713043213 6.121107578277588
Loss :  1.8089853525161743 4.243385314941406 6.052370548248291
Total LOSS train 4.098685829456036 valid 6.137232065200806
CE LOSS train 1.7327518481474655 valid 0.4522463381290436
Contrastive LOSS train 2.3659339904785157 valid 1.0608463287353516
EPOCH 211:
Loss :  1.7340655326843262 2.25247859954834 3.986544132232666
Loss :  1.7324515581130981 2.4323108196258545 4.164762496948242
Loss :  1.7294994592666626 2.397002696990967 4.12650203704834
Loss :  1.731856107711792 2.1008505821228027 3.8327066898345947
Loss :  1.7367438077926636 1.9023783206939697 3.6391220092773438
Loss :  1.7326805591583252 1.6357680559158325 3.3684487342834473
Loss :  1.7347134351730347 1.7691516876220703 3.5038652420043945
Loss :  1.7311675548553467 1.6022391319274902 3.333406686782837
Loss :  1.7325518131256104 1.7755495309829712 3.508101463317871
Loss :  1.721963882446289 1.9907649755477905 3.712728977203369
Loss :  1.733341932296753 2.753034830093384 4.486376762390137
Loss :  1.7401477098464966 2.5120882987976074 4.2522358894348145
Loss :  1.7344015836715698 1.8972324132919312 3.631633996963501
Loss :  1.7315335273742676 1.9700406789779663 3.7015743255615234
Loss :  1.735195279121399 2.018240451812744 3.7534356117248535
Loss :  1.7301636934280396 2.2529022693634033 3.9830660820007324
Loss :  1.733846664428711 2.0789601802825928 3.8128068447113037
Loss :  1.7323532104492188 2.1486666202545166 3.8810198307037354
Loss :  1.731982946395874 1.8319027423858643 3.5638856887817383
Loss :  1.7285635471343994 2.2869338989257812 4.015497207641602
  batch 20 loss: 1.7285635471343994, 2.2869338989257812, 4.015497207641602
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.73173987865448 1.9034935235977173 3.6352334022521973
Loss :  1.7332179546356201 2.170933246612549 3.904151201248169
Loss :  1.7336456775665283 1.8816691637039185 3.6153149604797363
Loss :  1.7395457029342651 2.6107842922210693 4.350329875946045
Loss :  1.7342352867126465 1.7904975414276123 3.524732828140259
Loss :  1.728723168373108 1.6301745176315308 3.3588976860046387
Loss :  1.7372045516967773 2.0332632064819336 3.770467758178711
Loss :  1.7281043529510498 2.4960732460021973 4.224177360534668
Loss :  1.7349854707717896 2.567964792251587 4.302950382232666
Loss :  1.7264031171798706 3.1475136280059814 4.8739166259765625
Loss :  1.7418217658996582 2.7125394344329834 4.4543609619140625
Loss :  1.7368285655975342 3.898348569869995 5.635177135467529
Loss :  1.7321921586990356 4.505959510803223 6.238151550292969
Loss :  1.7292547225952148 4.242921829223633 5.972176551818848
Loss :  1.7368113994598389 4.072243690490723 5.809055328369141
Loss :  1.73524010181427 3.63043475151062 5.36567497253418
Loss :  1.7321475744247437 3.0095207691192627 4.741668224334717
Loss :  1.7279618978500366 2.263261079788208 3.991222858428955
Loss :  1.730910301208496 2.956328868865967 4.687239170074463
Loss :  1.7331154346466064 3.1478323936462402 4.880948066711426
  batch 40 loss: 1.7331154346466064, 3.1478323936462402, 4.880948066711426
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7309293746948242 2.884931802749634 4.615860939025879
Loss :  1.7292143106460571 2.2663774490356445 3.995591640472412
Loss :  1.7326306104660034 2.467634916305542 4.200265407562256
Loss :  1.7300426959991455 2.583130359649658 4.313173294067383
Loss :  1.7335764169692993 1.854843258857727 3.5884196758270264
Loss :  1.7313045263290405 2.1112546920776367 3.842559337615967
Loss :  1.7309772968292236 1.876733422279358 3.607710838317871
Loss :  1.731493353843689 1.5718327760696411 3.30332612991333
Loss :  1.735329031944275 1.847125768661499 3.5824546813964844
Loss :  1.7303690910339355 2.089029550552368 3.8193986415863037
Loss :  1.7285138368606567 2.116880178451538 3.8453941345214844
Loss :  1.7328722476959229 2.359293222427368 4.092165470123291
Loss :  1.7385016679763794 2.4003520011901855 4.138853549957275
Loss :  1.7301459312438965 3.617854356765747 5.348000526428223
Loss :  1.7326399087905884 2.557424306869507 4.290064334869385
Loss :  1.7309993505477905 2.2885549068450928 4.019554138183594
Loss :  1.7363206148147583 2.2881252765655518 4.0244460105896
Loss :  1.7383127212524414 2.2640554904937744 4.002367973327637
Loss :  1.738296389579773 2.737983226776123 4.4762797355651855
Loss :  1.735504150390625 2.5000805854797363 4.235584735870361
  batch 60 loss: 1.735504150390625, 2.5000805854797363, 4.235584735870361
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7359169721603394 2.8400847911834717 4.5760016441345215
Loss :  1.7369318008422852 2.9264895915985107 4.663421630859375
Loss :  1.7377562522888184 2.3210301399230957 4.058786392211914
Loss :  1.7323496341705322 2.4934260845184326 4.225775718688965
Loss :  1.730981469154358 1.444924235343933 3.175905704498291
Loss :  1.8175877332687378 4.333169937133789 6.150757789611816
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8119175434112549 4.432626247406006 6.24454402923584
Loss :  1.8150972127914429 4.35295295715332 6.168050289154053
Loss :  1.8124489784240723 4.274991035461426 6.087440013885498
Total LOSS train 4.148229690698477 valid 6.162698030471802
CE LOSS train 1.7330034237641554 valid 0.45311224460601807
Contrastive LOSS train 2.4152262651003324 valid 1.0687477588653564
EPOCH 212:
Loss :  1.7348657846450806 2.631119966506958 4.365985870361328
Loss :  1.735065221786499 2.4776294231414795 4.2126946449279785
Loss :  1.7305397987365723 2.293912172317505 4.024452209472656
Loss :  1.7327665090560913 1.8894333839416504 3.6222000122070312
Loss :  1.7367395162582397 2.0419812202453613 3.7787208557128906
Loss :  1.7334376573562622 1.8191171884536743 3.5525548458099365
Loss :  1.7354915142059326 2.623156785964966 4.358648300170898
Loss :  1.7317299842834473 2.899294376373291 4.631024360656738
Loss :  1.7330318689346313 1.8106924295425415 3.543724298477173
Loss :  1.722192645072937 1.6420481204986572 3.3642406463623047
Loss :  1.733883023262024 2.18845534324646 3.9223384857177734
Loss :  1.740842342376709 2.3864645957946777 4.127306938171387
Loss :  1.7357549667358398 2.2305262088775635 3.9662811756134033
Loss :  1.7327439785003662 2.298203945159912 4.030947685241699
Loss :  1.736660122871399 2.7953598499298096 4.532020092010498
Loss :  1.7310779094696045 1.6552814245224 3.386359214782715
Loss :  1.7360625267028809 2.246290922164917 3.982353448867798
Loss :  1.7340420484542847 2.0425686836242676 3.776610851287842
Loss :  1.733262538909912 1.879773497581482 3.6130361557006836
Loss :  1.7311851978302002 2.024770736694336 3.755955934524536
  batch 20 loss: 1.7311851978302002, 2.024770736694336, 3.755955934524536
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7335489988327026 1.8978466987609863 3.6313958168029785
Loss :  1.7353358268737793 2.115067958831787 3.8504037857055664
Loss :  1.7358298301696777 2.1445395946502686 3.8803694248199463
Loss :  1.740605354309082 2.1128880977630615 3.8534934520721436
Loss :  1.7344411611557007 2.5613818168640137 4.295823097229004
Loss :  1.729950189590454 1.92996084690094 3.6599111557006836
Loss :  1.7384850978851318 2.098565101623535 3.837050199508667
Loss :  1.73003351688385 2.079458236694336 3.8094916343688965
Loss :  1.736223816871643 2.004115343093872 3.7403392791748047
Loss :  1.7279609441757202 3.042860269546509 4.7708210945129395
Loss :  1.742058277130127 1.979175329208374 3.721233606338501
Loss :  1.7357654571533203 2.056962490081787 3.7927279472351074
Loss :  1.7292829751968384 1.823943018913269 3.5532259941101074
Loss :  1.7283281087875366 2.157402753829956 3.885730743408203
Loss :  1.7363851070404053 2.5647380352020264 4.301123142242432
Loss :  1.7360328435897827 2.4734127521514893 4.209445476531982
Loss :  1.7330671548843384 1.8609507083892822 3.59401798248291
Loss :  1.7299500703811646 1.6681619882583618 3.3981120586395264
Loss :  1.7323564291000366 1.5568121671676636 3.2891685962677
Loss :  1.733986735343933 2.744370460510254 4.478357315063477
  batch 40 loss: 1.733986735343933, 2.744370460510254, 4.478357315063477
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7328038215637207 2.9503629207611084 4.68316650390625
Loss :  1.7321133613586426 2.1896426677703857 3.9217560291290283
Loss :  1.7341817617416382 1.6615256071090698 3.395707368850708
Loss :  1.734161615371704 2.6359775066375732 4.370139122009277
Loss :  1.7364287376403809 2.5747883319854736 4.311217308044434
Loss :  1.7328822612762451 3.133173942565918 4.866056442260742
Loss :  1.7330124378204346 2.764218807220459 4.497231483459473
Loss :  1.7336194515228271 3.393456220626831 5.127075672149658
Loss :  1.735311508178711 3.2193074226379395 4.95461893081665
Loss :  1.7323460578918457 2.704058885574341 4.436405181884766
Loss :  1.7285758256912231 1.8865406513214111 3.615116596221924
Loss :  1.7327799797058105 1.9109317064285278 3.643711566925049
Loss :  1.7385226488113403 2.0364248752593994 3.7749476432800293
Loss :  1.7303189039230347 2.1860876083374023 3.9164066314697266
Loss :  1.7321832180023193 2.2179207801818848 3.950103998184204
Loss :  1.7300238609313965 2.232182741165161 3.9622066020965576
Loss :  1.7349196672439575 2.350170373916626 4.085090160369873
Loss :  1.7364845275878906 1.7248142957687378 3.461298942565918
Loss :  1.7366323471069336 2.660132884979248 4.396765232086182
Loss :  1.7337836027145386 1.9075833559036255 3.641366958618164
  batch 60 loss: 1.7337836027145386, 1.9075833559036255, 3.641366958618164
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7331740856170654 2.156040668487549 3.8892147541046143
Loss :  1.734942078590393 2.4219861030578613 4.156928062438965
Loss :  1.7350356578826904 1.5043981075286865 3.239433765411377
Loss :  1.7301056385040283 1.9713889360427856 3.7014946937561035
Loss :  1.7295321226119995 1.5419681072235107 3.2715001106262207
Loss :  1.8101431131362915 4.37752628326416 6.187669277191162
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8049101829528809 4.4275007247924805 6.232410907745361
Loss :  1.8073499202728271 4.427395343780518 6.234745025634766
Loss :  1.806355595588684 4.092596054077148 5.898951530456543
Total LOSS train 3.9595178090609036 valid 6.138444185256958
CE LOSS train 1.7336135112322293 valid 0.451588898897171
Contrastive LOSS train 2.2259042684848493 valid 1.023149013519287
EPOCH 213:
Loss :  1.7338225841522217 2.3814659118652344 4.115288734436035
Loss :  1.7323379516601562 2.3587400913238525 4.09107780456543
Loss :  1.7294716835021973 1.6099395751953125 3.3394112586975098
Loss :  1.7321988344192505 3.018955707550049 4.75115442276001
Loss :  1.737291932106018 1.8058925867080688 3.543184518814087
Loss :  1.7333199977874756 1.7800428867340088 3.5133628845214844
Loss :  1.7359598875045776 1.9827739000320435 3.718733787536621
Loss :  1.7319608926773071 1.4722778797149658 3.2042388916015625
Loss :  1.7337558269500732 2.730414390563965 4.464170455932617
Loss :  1.723714828491211 1.9904276132583618 3.714142322540283
Loss :  1.7344173192977905 2.3539021015167236 4.088319301605225
Loss :  1.740659236907959 1.8410388231277466 3.581697940826416
Loss :  1.7357065677642822 2.2269318103790283 3.9626383781433105
Loss :  1.7329308986663818 2.0176453590393066 3.7505762577056885
Loss :  1.736883521080017 1.7940315008163452 3.5309150218963623
Loss :  1.7314774990081787 2.717069149017334 4.448546409606934
Loss :  1.7351220846176147 2.025252342224121 3.7603745460510254
Loss :  1.7331551313400269 1.9657295942306519 3.6988847255706787
Loss :  1.7325538396835327 1.7434262037277222 3.475980043411255
Loss :  1.730452299118042 1.812999963760376 3.543452262878418
  batch 20 loss: 1.730452299118042, 1.812999963760376, 3.543452262878418
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7326847314834595 1.9276881217956543 3.660372734069824
Loss :  1.7344403266906738 2.729809522628784 4.464249610900879
Loss :  1.7351789474487305 2.2613284587860107 3.996507406234741
Loss :  1.7399919033050537 2.1498289108276367 3.8898208141326904
Loss :  1.7347149848937988 2.2991321086883545 4.033846855163574
Loss :  1.7303270101547241 2.012556791305542 3.7428836822509766
Loss :  1.7384536266326904 2.875234365463257 4.613687992095947
Loss :  1.7300467491149902 2.509169816970825 4.2392168045043945
Loss :  1.7364602088928223 2.2464828491210938 3.982943058013916
Loss :  1.7284777164459229 2.3733925819396973 4.101870536804199
Loss :  1.7426879405975342 2.3509488105773926 4.093636512756348
Loss :  1.735522985458374 1.894126296043396 3.6296491622924805
Loss :  1.729467749595642 1.9767786264419556 3.7062463760375977
Loss :  1.7293585538864136 2.1640279293060303 3.8933863639831543
Loss :  1.7367299795150757 3.276031732559204 5.01276159286499
Loss :  1.7369412183761597 2.8872458934783936 4.624186992645264
Loss :  1.7337061166763306 2.7604589462280273 4.494164943695068
Loss :  1.7298719882965088 2.0133583545684814 3.7432303428649902
Loss :  1.7333204746246338 1.8867805004119873 3.620100975036621
Loss :  1.7342113256454468 2.631312370300293 4.365523815155029
  batch 40 loss: 1.7342113256454468, 2.631312370300293, 4.365523815155029
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.732867956161499 3.5122931003570557 5.245161056518555
Loss :  1.7316827774047852 2.482912540435791 4.214595317840576
Loss :  1.7341299057006836 2.6234099864959717 4.357540130615234
Loss :  1.732638955116272 2.197230100631714 3.9298691749572754
Loss :  1.735880732536316 2.495323657989502 4.231204509735107
Loss :  1.7324275970458984 2.2159125804901123 3.9483401775360107
Loss :  1.7324106693267822 2.3070499897003174 4.0394606590271
Loss :  1.7325528860092163 2.7281715869903564 4.460724353790283
Loss :  1.7349231243133545 2.4245665073394775 4.159489631652832
Loss :  1.7315161228179932 2.747725009918213 4.479241371154785
Loss :  1.7276437282562256 3.197370767593384 4.925014495849609
Loss :  1.7325490713119507 1.808719277381897 3.5412683486938477
Loss :  1.7374247312545776 2.2242212295532227 3.96164608001709
Loss :  1.7300951480865479 2.2955758571624756 4.025671005249023
Loss :  1.731736183166504 3.2842841148376465 5.01602029800415
Loss :  1.7287906408309937 2.8727331161499023 4.6015238761901855
Loss :  1.7346762418746948 3.8397583961486816 5.574434757232666
Loss :  1.736917495727539 4.31029748916626 6.047214984893799
Loss :  1.737013816833496 3.578356981277466 5.315370559692383
Loss :  1.7333850860595703 2.4680027961730957 4.201387882232666
  batch 60 loss: 1.7333850860595703, 2.4680027961730957, 4.201387882232666
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7330995798110962 2.5379793643951416 4.271079063415527
Loss :  1.7346211671829224 2.2615861892700195 3.9962072372436523
Loss :  1.7349913120269775 2.4855432510375977 4.220534324645996
Loss :  1.7284409999847412 3.4742796421051025 5.202720642089844
Loss :  1.728151798248291 1.546752691268921 3.274904489517212
Loss :  1.8103148937225342 4.472550392150879 6.282865524291992
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8055782318115234 4.443643093109131 6.249221324920654
Loss :  1.8074969053268433 4.396780014038086 6.204277038574219
Loss :  1.8073575496673584 4.23007869720459 6.037436485290527
Total LOSS train 4.145308630283062 valid 6.193450093269348
CE LOSS train 1.7333900781778189 valid 0.4518393874168396
Contrastive LOSS train 2.411918563109178 valid 1.0575196743011475
EPOCH 214:
Loss :  1.7329974174499512 2.0696394443511963 3.8026368618011475
Loss :  1.7319746017456055 2.338799476623535 4.070774078369141
Loss :  1.7284599542617798 1.396809458732605 3.1252694129943848
Loss :  1.7316972017288208 1.7563092708587646 3.488006591796875
Loss :  1.7360278367996216 2.7091751098632812 4.445202827453613
Loss :  1.7327519655227661 2.431910276412964 4.1646623611450195
Loss :  1.7348750829696655 2.2627005577087402 3.9975757598876953
Loss :  1.7312127351760864 1.8611502647399902 3.592362880706787
Loss :  1.732931137084961 2.0529260635375977 3.7858572006225586
Loss :  1.7218295335769653 2.606350898742676 4.328180313110352
Loss :  1.7331438064575195 3.325789451599121 5.058933258056641
Loss :  1.7406319379806519 2.8266255855560303 4.567257404327393
Loss :  1.7343817949295044 1.998080849647522 3.7324626445770264
Loss :  1.7320747375488281 1.918413519859314 3.6504883766174316
Loss :  1.7365951538085938 2.313223361968994 4.049818515777588
Loss :  1.7305288314819336 2.911235809326172 4.6417646408081055
Loss :  1.7350859642028809 2.8398683071136475 4.574954032897949
Loss :  1.7335580587387085 2.3372902870178223 4.07084846496582
Loss :  1.7321940660476685 2.262526035308838 3.994719982147217
Loss :  1.7301307916641235 2.0759408473968506 3.8060717582702637
  batch 20 loss: 1.7301307916641235, 2.0759408473968506, 3.8060717582702637
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.732838749885559 2.0998270511627197 3.8326659202575684
Loss :  1.7336944341659546 2.2590959072113037 3.9927902221679688
Loss :  1.735566258430481 2.015082597732544 3.7506489753723145
Loss :  1.7394063472747803 2.073373556137085 3.8127799034118652
Loss :  1.7350893020629883 2.2357747554779053 3.9708640575408936
Loss :  1.7307007312774658 2.2875890731811523 4.018289566040039
Loss :  1.73826003074646 2.1161134243011475 3.8543734550476074
Loss :  1.7294127941131592 2.5390920639038086 4.268505096435547
Loss :  1.7357159852981567 2.29974365234375 4.035459518432617
Loss :  1.7274755239486694 2.769437313079834 4.496912956237793
Loss :  1.7417515516281128 2.741847515106201 4.4835991859436035
Loss :  1.7355589866638184 2.574357748031616 4.3099164962768555
Loss :  1.7290514707565308 2.4448068141937256 4.173858165740967
Loss :  1.7287465333938599 2.7953834533691406 4.524129867553711
Loss :  1.736768364906311 2.487563371658325 4.224331855773926
Loss :  1.7363380193710327 3.181342601776123 4.917680740356445
Loss :  1.733068823814392 2.895028591156006 4.6280975341796875
Loss :  1.729390263557434 2.3682286739349365 4.09761905670166
Loss :  1.732096791267395 1.8264251947402954 3.5585219860076904
Loss :  1.732911467552185 1.9701755046844482 3.7030868530273438
  batch 40 loss: 1.732911467552185, 1.9701755046844482, 3.7030868530273438
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7316453456878662 2.0013628005981445 3.7330081462860107
Loss :  1.7314308881759644 1.9845942258834839 3.7160251140594482
Loss :  1.7328755855560303 2.186079263687134 3.918954849243164
Loss :  1.7334272861480713 2.2446508407592773 3.9780781269073486
Loss :  1.7352917194366455 1.9387149810791016 3.674006700515747
Loss :  1.7309309244155884 2.176945686340332 3.907876491546631
Loss :  1.732218861579895 2.092238664627075 3.8244576454162598
Loss :  1.7327907085418701 1.8949623107910156 3.6277530193328857
Loss :  1.7335946559906006 2.623584032058716 4.357178688049316
Loss :  1.7315609455108643 2.054539442062378 3.786100387573242
Loss :  1.7278474569320679 1.5414738655090332 3.2693214416503906
Loss :  1.7325682640075684 1.795263648033142 3.52783203125
Loss :  1.7387752532958984 2.020465135574341 3.7592403888702393
Loss :  1.7305775880813599 2.996922492980957 4.727499961853027
Loss :  1.7327264547348022 2.7154793739318848 4.448205947875977
Loss :  1.7310428619384766 2.391509532928467 4.122552394866943
Loss :  1.7355892658233643 2.5118284225463867 4.247417449951172
Loss :  1.7372658252716064 2.011420488357544 3.7486863136291504
Loss :  1.737549066543579 2.418570041656494 4.156119346618652
Loss :  1.7353754043579102 2.4574170112609863 4.1927924156188965
  batch 60 loss: 1.7353754043579102, 2.4574170112609863, 4.1927924156188965
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7349557876586914 2.244528293609619 3.9794840812683105
Loss :  1.7361949682235718 2.282766103744507 4.018960952758789
Loss :  1.7369135618209839 1.6503793001174927 3.3872928619384766
Loss :  1.7303895950317383 2.1747002601623535 3.905089855194092
Loss :  1.7292883396148682 1.472603678703308 3.2018918991088867
Loss :  1.802919864654541 4.395894527435303 6.198814392089844
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.798756718635559 4.342549800872803 6.141306400299072
Loss :  1.8005404472351074 4.311850547790527 6.112390995025635
Loss :  1.801490306854248 4.158479690551758 5.959969997406006
Total LOSS train 4.012581619849572 valid 6.103120446205139
CE LOSS train 1.7332269485180194 valid 0.450372576713562
Contrastive LOSS train 2.2793546713315522 valid 1.0396199226379395
EPOCH 215:
Loss :  1.7343263626098633 2.716104745864868 4.450430870056152
Loss :  1.7330833673477173 3.179001808166504 4.912085056304932
Loss :  1.7293981313705444 2.5698578357696533 4.299255847930908
Loss :  1.7325142621994019 2.905451774597168 4.637966156005859
Loss :  1.7359617948532104 1.843109369277954 3.579071044921875
Loss :  1.7330995798110962 1.895013451576233 3.628113031387329
Loss :  1.7346572875976562 2.3284151554107666 4.063072204589844
Loss :  1.7311134338378906 3.2206852436065674 4.951798439025879
Loss :  1.7329214811325073 2.61372447013855 4.346645832061768
Loss :  1.7220656871795654 1.7379564046859741 3.46002197265625
Loss :  1.7329105138778687 2.0700676441192627 3.802978038787842
Loss :  1.7416235208511353 2.7967522144317627 4.5383758544921875
Loss :  1.7342219352722168 2.358563184738159 4.092784881591797
Loss :  1.732603907585144 2.1510651111602783 3.883668899536133
Loss :  1.736857533454895 2.331317663192749 4.068175315856934
Loss :  1.731727123260498 2.835476875305176 4.567203998565674
Loss :  1.735298752784729 2.597094774246216 4.332393646240234
Loss :  1.7334816455841064 2.419469118118286 4.152950763702393
Loss :  1.733605980873108 1.9221502542495728 3.6557562351226807
Loss :  1.730836033821106 1.9001917839050293 3.6310276985168457
  batch 20 loss: 1.730836033821106, 1.9001917839050293, 3.6310276985168457
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.732500672340393 2.595038890838623 4.327539443969727
Loss :  1.7347314357757568 2.083104372024536 3.817835807800293
Loss :  1.7357523441314697 2.4279708862304688 4.163722991943359
Loss :  1.7400985956192017 2.44865345954895 4.188752174377441
Loss :  1.7356048822402954 3.0047717094421387 4.7403764724731445
Loss :  1.7309197187423706 2.764669179916382 4.495588779449463
Loss :  1.7380317449569702 1.7100708484649658 3.4481024742126465
Loss :  1.7288033962249756 1.5115190744400024 3.2403225898742676
Loss :  1.735013723373413 2.425475835800171 4.160489559173584
Loss :  1.7279224395751953 2.332005739212036 4.059927940368652
Loss :  1.7416330575942993 2.815650701522827 4.557283878326416
Loss :  1.7345043420791626 2.4078726768493652 4.142376899719238
Loss :  1.7288404703140259 2.732133388519287 4.460973739624023
Loss :  1.72897207736969 1.8572415113449097 3.5862135887145996
Loss :  1.7372864484786987 2.4467060565948486 4.183992385864258
Loss :  1.7372338771820068 2.0346357822418213 3.771869659423828
Loss :  1.734772801399231 2.690002918243408 4.42477560043335
Loss :  1.7317498922348022 2.591373920440674 4.323123931884766
Loss :  1.7327442169189453 3.031785249710083 4.764529228210449
Loss :  1.7357451915740967 2.987473487854004 4.72321891784668
  batch 40 loss: 1.7357451915740967, 2.987473487854004, 4.72321891784668
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7334827184677124 3.002812147140503 4.736294746398926
Loss :  1.7312672138214111 1.6278722286224365 3.3591394424438477
Loss :  1.7358320951461792 2.201664447784424 3.9374966621398926
Loss :  1.7324185371398926 2.1587629318237305 3.891181468963623
Loss :  1.7368457317352295 1.7352806329727173 3.4721264839172363
Loss :  1.7352651357650757 1.9791512489318848 3.71441650390625
Loss :  1.7335501909255981 3.5821878910064697 5.315738201141357
Loss :  1.7342967987060547 2.126561164855957 3.8608579635620117
Loss :  1.7378349304199219 2.8021726608276367 4.540007591247559
Loss :  1.7326959371566772 2.5772035121917725 4.30989933013916
Loss :  1.7286276817321777 3.572338104248047 5.300965785980225
Loss :  1.7336997985839844 2.6109166145324707 4.344616413116455
Loss :  1.7380917072296143 3.4009313583374023 5.1390228271484375
Loss :  1.7305389642715454 2.893493890762329 4.624032974243164
Loss :  1.7322014570236206 1.9962596893310547 3.728461265563965
Loss :  1.7289663553237915 1.6252458095550537 3.3542122840881348
Loss :  1.7346261739730835 2.454327344894409 4.188953399658203
Loss :  1.7380306720733643 2.1384522914886475 3.8764829635620117
Loss :  1.7375742197036743 2.6724307537078857 4.41000509262085
Loss :  1.7340008020401 2.3980653285980225 4.132066249847412
  batch 60 loss: 1.7340008020401, 2.3980653285980225, 4.132066249847412
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.734257698059082 2.7523815631866455 4.486639022827148
Loss :  1.7354116439819336 2.853355646133423 4.588767051696777
Loss :  1.7366037368774414 2.295759439468384 4.032362937927246
Loss :  1.7303282022476196 2.4552481174468994 4.185576438903809
Loss :  1.7296218872070312 2.6658196449279785 4.39544153213501
Loss :  1.819250464439392 4.3719482421875 6.191198825836182
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8141204118728638 4.435719013214111 6.2498393058776855
Loss :  1.8156228065490723 4.2815470695495605 6.097169876098633
Loss :  1.81781005859375 4.1268486976623535 5.9446587562561035
Total LOSS train 4.1932239459111145 valid 6.120716691017151
CE LOSS train 1.7337113838929397 valid 0.4544525146484375
Contrastive LOSS train 2.4595126005319448 valid 1.0317121744155884
EPOCH 216:
Loss :  1.7340688705444336 2.482063055038452 4.216132164001465
Loss :  1.7334213256835938 2.257920742034912 3.991342067718506
Loss :  1.7292675971984863 1.5376051664352417 3.2668728828430176
Loss :  1.732071876525879 2.0743496417999268 3.8064215183258057
Loss :  1.7354729175567627 2.544982671737671 4.280455589294434
Loss :  1.733036994934082 2.3257763385772705 4.058813095092773
Loss :  1.7338508367538452 2.8758575916290283 4.609708309173584
Loss :  1.7310459613800049 2.0402824878692627 3.7713284492492676
Loss :  1.733003854751587 2.3403525352478027 4.073356628417969
Loss :  1.7210712432861328 2.431637763977051 4.152709007263184
Loss :  1.7334188222885132 2.219590187072754 3.9530091285705566
Loss :  1.7411103248596191 1.8775742053985596 3.6186845302581787
Loss :  1.734858751296997 1.8546236753463745 3.589482307434082
Loss :  1.7324379682540894 2.123122215270996 3.855560302734375
Loss :  1.7369743585586548 2.7607569694519043 4.4977312088012695
Loss :  1.7315518856048584 2.371854305267334 4.103405952453613
Loss :  1.7354686260223389 2.2513132095336914 3.9867818355560303
Loss :  1.7341300249099731 2.0372631549835205 3.771393299102783
Loss :  1.732630729675293 1.873802900314331 3.606433629989624
Loss :  1.7313932180404663 2.1803832054138184 3.911776542663574
  batch 20 loss: 1.7313932180404663, 2.1803832054138184, 3.911776542663574
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.73379647731781 2.2497172355651855 3.983513832092285
Loss :  1.7348051071166992 2.657829523086548 4.392634391784668
Loss :  1.736382007598877 2.1903076171875 3.926689624786377
Loss :  1.740268588066101 1.939204454421997 3.6794729232788086
Loss :  1.7350856065750122 2.5656979084014893 4.300783634185791
Loss :  1.7309015989303589 2.6840426921844482 4.414944171905518
Loss :  1.7388521432876587 2.3553881645202637 4.094240188598633
Loss :  1.7304465770721436 2.6005964279174805 4.331043243408203
Loss :  1.736669898033142 1.9806818962097168 3.7173519134521484
Loss :  1.727737307548523 1.9057745933532715 3.633512020111084
Loss :  1.7420378923416138 2.4009809494018555 4.14301872253418
Loss :  1.735859990119934 2.151536703109741 3.887396812438965
Loss :  1.7288740873336792 1.9557572603225708 3.68463134765625
Loss :  1.7290000915527344 1.7637025117874146 3.4927024841308594
Loss :  1.736473560333252 2.2683937549591064 4.0048675537109375
Loss :  1.7361868619918823 1.8291776180267334 3.565364360809326
Loss :  1.7332303524017334 2.293642997741699 4.026873588562012
Loss :  1.729082465171814 2.146806001663208 3.8758883476257324
Loss :  1.7320088148117065 2.4376540184020996 4.169662952423096
Loss :  1.7340195178985596 1.800919771194458 3.5349392890930176
  batch 40 loss: 1.7340195178985596, 1.800919771194458, 3.5349392890930176
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7315582036972046 2.5522570610046387 4.283815383911133
Loss :  1.7299896478652954 1.786293864250183 3.5162835121154785
Loss :  1.7338942289352417 2.1390318870544434 3.8729262351989746
Loss :  1.7312126159667969 2.1894142627716064 3.9206268787384033
Loss :  1.7349096536636353 1.9080142974853516 3.6429238319396973
Loss :  1.732521653175354 1.995728611946106 3.72825026512146
Loss :  1.7314509153366089 2.1097216606140137 3.841172695159912
Loss :  1.7336232662200928 1.8245409727096558 3.558164119720459
Loss :  1.7355990409851074 1.9754003286361694 3.7109994888305664
Loss :  1.7327219247817993 1.8017699718475342 3.534492015838623
Loss :  1.7292886972427368 1.8818389177322388 3.6111276149749756
Loss :  1.7330970764160156 1.8654828071594238 3.5985798835754395
Loss :  1.7376925945281982 1.8713659048080444 3.609058380126953
Loss :  1.7309073209762573 2.0172665119171143 3.748173713684082
Loss :  1.732365369796753 2.069856882095337 3.80222225189209
Loss :  1.7291446924209595 2.109452247619629 3.838596820831299
Loss :  1.7354800701141357 1.9314364194869995 3.6669163703918457
Loss :  1.7376190423965454 1.8491984605789185 3.586817502975464
Loss :  1.7380633354187012 2.276684284210205 4.014747619628906
Loss :  1.7343425750732422 1.8290858268737793 3.5634284019470215
  batch 60 loss: 1.7343425750732422, 1.8290858268737793, 3.5634284019470215
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7344962358474731 1.7970728874206543 3.531569004058838
Loss :  1.7358591556549072 2.060914993286133 3.79677414894104
Loss :  1.7362719774246216 1.8294323682785034 3.565704345703125
Loss :  1.7316290140151978 2.236304998397827 3.9679341316223145
Loss :  1.7307415008544922 1.3408476114273071 3.0715889930725098
Loss :  1.8148788213729858 4.420278549194336 6.235157489776611
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.809801697731018 4.405348300933838 6.215149879455566
Loss :  1.8117576837539673 4.415126323699951 6.226883888244629
Loss :  1.8123619556427002 4.3791890144348145 6.191551208496094
Total LOSS train 3.8548280532543475 valid 6.217185616493225
CE LOSS train 1.733484383729788 valid 0.45309048891067505
Contrastive LOSS train 2.1213436640225924 valid 1.0947972536087036
EPOCH 217:
Loss :  1.734342336654663 1.629084825515747 3.36342716217041
Loss :  1.7328343391418457 1.9107284545898438 3.6435627937316895
Loss :  1.7301220893859863 1.6845463514328003 3.414668560028076
Loss :  1.7324411869049072 1.4388427734375 3.1712839603424072
Loss :  1.7374546527862549 1.3444194793701172 3.081874132156372
Loss :  1.7331814765930176 1.9400333166122437 3.673214912414551
Loss :  1.7357783317565918 2.0986404418945312 3.834418773651123
Loss :  1.7312456369400024 1.934704065322876 3.665949821472168
Loss :  1.7332086563110352 1.8313231468200684 3.5645318031311035
Loss :  1.7227379083633423 2.089991569519043 3.8127293586730957
Loss :  1.7333189249038696 2.109482526779175 3.842801570892334
Loss :  1.7413736581802368 1.796055555343628 3.5374293327331543
Loss :  1.735150933265686 1.5720354318618774 3.3071863651275635
Loss :  1.7327046394348145 2.007375478744507 3.7400801181793213
Loss :  1.736401915550232 2.0763368606567383 3.8127388954162598
Loss :  1.7315930128097534 2.447856903076172 4.179450035095215
Loss :  1.735224962234497 1.6867339611053467 3.4219589233398438
Loss :  1.733258605003357 1.7997862100601196 3.5330448150634766
Loss :  1.733639121055603 1.7565867900848389 3.4902257919311523
Loss :  1.7301112413406372 1.8314329385757446 3.561544179916382
  batch 20 loss: 1.7301112413406372, 1.8314329385757446, 3.561544179916382
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7330689430236816 1.7383328676223755 3.4714016914367676
Loss :  1.7344266176223755 2.4111077785491943 4.145534515380859
Loss :  1.7343612909317017 1.7323437929153442 3.466705083847046
Loss :  1.7399342060089111 1.808058261871338 3.547992467880249
Loss :  1.7348700761795044 1.8939335346221924 3.6288037300109863
Loss :  1.729777455329895 1.7731581926345825 3.5029356479644775
Loss :  1.7380019426345825 1.9300715923309326 3.6680736541748047
Loss :  1.7293692827224731 2.045560121536255 3.7749295234680176
Loss :  1.7365312576293945 1.944953203201294 3.6814844608306885
Loss :  1.728666067123413 2.2934911251068115 4.022157192230225
Loss :  1.7434513568878174 2.3509397506713867 4.094390869140625
Loss :  1.7371387481689453 2.0002570152282715 3.737395763397217
Loss :  1.7306329011917114 1.972320556640625 3.702953338623047
Loss :  1.7311362028121948 2.0417604446411133 3.7728967666625977
Loss :  1.7384456396102905 2.2309534549713135 3.9693989753723145
Loss :  1.738364577293396 2.075451374053955 3.8138160705566406
Loss :  1.7355073690414429 2.264622688293457 4.0001301765441895
Loss :  1.7313064336776733 1.9954609870910645 3.7267675399780273
Loss :  1.7334121465682983 1.9961180686950684 3.7295303344726562
Loss :  1.7353416681289673 1.7877358198165894 3.5230774879455566
  batch 40 loss: 1.7353416681289673, 1.7877358198165894, 3.5230774879455566
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7332227230072021 2.1412277221679688 3.874450445175171
Loss :  1.7313612699508667 2.0779714584350586 3.809332847595215
Loss :  1.735064148902893 3.1843008995056152 4.919364929199219
Loss :  1.732454776763916 2.876014232635498 4.608469009399414
Loss :  1.735749363899231 3.5915873050689697 5.32733678817749
Loss :  1.7329121828079224 2.67334246635437 4.406254768371582
Loss :  1.731994867324829 1.9850585460662842 3.7170534133911133
Loss :  1.7332146167755127 2.264207124710083 3.9974217414855957
Loss :  1.7342894077301025 2.006107807159424 3.7403972148895264
Loss :  1.7316944599151611 1.586240291595459 3.31793475151062
Loss :  1.7284290790557861 1.8423653841018677 3.5707945823669434
Loss :  1.7324401140213013 1.760246992111206 3.492687225341797
Loss :  1.7373180389404297 2.401028633117676 4.1383466720581055
Loss :  1.730844259262085 2.0057475566864014 3.7365918159484863
Loss :  1.7326200008392334 1.9637188911437988 3.6963388919830322
Loss :  1.7295688390731812 2.2797648906707764 4.009333610534668
Loss :  1.735262393951416 2.103642225265503 3.838904619216919
Loss :  1.7371551990509033 2.237034797668457 3.9741899967193604
Loss :  1.7374151945114136 3.571530818939209 5.308946132659912
Loss :  1.7343555688858032 1.4684566259384155 3.2028121948242188
  batch 60 loss: 1.7343555688858032, 1.4684566259384155, 3.2028121948242188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7344021797180176 1.903159260749817 3.637561321258545
Loss :  1.7361761331558228 1.7628637552261353 3.499039888381958
Loss :  1.7370460033416748 2.802680015563965 4.539726257324219
Loss :  1.7315003871917725 2.6356751918792725 4.367175579071045
Loss :  1.7303558588027954 1.82377290725708 3.554128646850586
Loss :  1.8132381439208984 4.373509883880615 6.186748027801514
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8078025579452515 4.3924970626831055 6.2002997398376465
Loss :  1.810199499130249 4.3034820556640625 6.113681793212891
Loss :  1.8115944862365723 4.345312595367432 6.156907081604004
Total LOSS train 3.7987244606018065 valid 6.164409160614014
CE LOSS train 1.733795613508958 valid 0.45289862155914307
Contrastive LOSS train 2.0649288232509906 valid 1.086328148841858
EPOCH 218:
Loss :  1.7348443269729614 2.832500696182251 4.567345142364502
Loss :  1.7338030338287354 2.126905679702759 3.860708713531494
Loss :  1.7301405668258667 1.6680024862289429 3.3981430530548096
Loss :  1.7334465980529785 1.7887061834335327 3.522152900695801
Loss :  1.7365614175796509 1.802953839302063 3.539515256881714
Loss :  1.7339637279510498 1.788397192955017 3.5223608016967773
Loss :  1.735323190689087 1.9408234357833862 3.6761465072631836
Loss :  1.7320234775543213 1.749137043952942 3.4811606407165527
Loss :  1.7340766191482544 3.019517421722412 4.753593921661377
Loss :  1.723084568977356 2.0420053005218506 3.765089988708496
Loss :  1.7333241701126099 2.4219295978546143 4.155253887176514
Loss :  1.7428942918777466 1.9705508947372437 3.7134451866149902
Loss :  1.735138177871704 1.7172091007232666 3.4523472785949707
Loss :  1.7332018613815308 1.8069764375686646 3.5401782989501953
Loss :  1.737461805343628 1.9726386070251465 3.7101004123687744
Loss :  1.731788992881775 2.05055832862854 3.7823472023010254
Loss :  1.7356020212173462 1.9039034843444824 3.639505386352539
Loss :  1.7331539392471313 2.250119924545288 3.983273983001709
Loss :  1.7342064380645752 3.3322441577911377 5.066450595855713
Loss :  1.7305623292922974 1.7759004831314087 3.506462812423706
  batch 20 loss: 1.7305623292922974, 1.7759004831314087, 3.506462812423706
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7336814403533936 2.3051857948303223 4.038866996765137
Loss :  1.7350579500198364 2.003755569458008 3.7388134002685547
Loss :  1.7360540628433228 2.1698434352874756 3.905897617340088
Loss :  1.740581750869751 2.214522123336792 3.955103874206543
Loss :  1.7359856367111206 2.1331090927124023 3.8690948486328125
Loss :  1.7318980693817139 2.122958183288574 3.854856252670288
Loss :  1.7397297620773315 2.0040037631988525 3.7437334060668945
Loss :  1.7312977313995361 2.166372537612915 3.897670269012451
Loss :  1.7379347085952759 2.3287734985351562 4.066708087921143
Loss :  1.7290778160095215 2.3822124004364014 4.111289978027344
Loss :  1.7438154220581055 2.254335880279541 3.9981513023376465
Loss :  1.73713219165802 2.068190336227417 3.8053226470947266
Loss :  1.730532169342041 2.2661569118499756 3.9966890811920166
Loss :  1.730666160583496 2.2275476455688477 3.9582138061523438
Loss :  1.737992525100708 2.431997537612915 4.169990062713623
Loss :  1.7377854585647583 2.4151573181152344 4.152942657470703
Loss :  1.7345978021621704 2.228541135787964 3.963139057159424
Loss :  1.7301359176635742 2.1949474811553955 3.9250833988189697
Loss :  1.733336329460144 2.34260630607605 4.075942516326904
Loss :  1.7343826293945312 2.21777081489563 3.952153444290161
  batch 40 loss: 1.7343826293945312, 2.21777081489563, 3.952153444290161
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.732167363166809 2.8492681980133057 4.581435680389404
Loss :  1.731756567955017 2.0791826248168945 3.810939311981201
Loss :  1.7339388132095337 2.130474805831909 3.8644137382507324
Loss :  1.7333077192306519 1.92080557346344 3.654113292694092
Loss :  1.735604166984558 1.723912000656128 3.4595160484313965
Loss :  1.7320547103881836 1.657339096069336 3.3893938064575195
Loss :  1.7319834232330322 2.3029415607452393 4.0349249839782715
Loss :  1.7339460849761963 2.564053535461426 4.297999382019043
Loss :  1.7347513437271118 2.227248191833496 3.9619994163513184
Loss :  1.733325481414795 2.193321466445923 3.9266469478607178
Loss :  1.7296857833862305 2.5657567977905273 4.295442581176758
Loss :  1.7335976362228394 3.1719868183135986 4.905584335327148
Loss :  1.738546371459961 1.9007176160812378 3.6392641067504883
Loss :  1.7315744161605835 2.0480453968048096 3.7796196937561035
Loss :  1.733272910118103 2.0797507762908936 3.813023567199707
Loss :  1.7298848628997803 2.072157621383667 3.8020424842834473
Loss :  1.7359071969985962 2.3748090267181396 4.110716342926025
Loss :  1.7377313375473022 2.6455817222595215 4.383313179016113
Loss :  1.7384827136993408 3.596008539199829 5.33449125289917
Loss :  1.73516845703125 2.205902576446533 3.941071033477783
  batch 60 loss: 1.73516845703125, 2.205902576446533, 3.941071033477783
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7351425886154175 2.6650402545928955 4.400182723999023
Loss :  1.73652982711792 2.7777087688446045 4.514238357543945
Loss :  1.7369545698165894 2.0094728469848633 3.746427536010742
Loss :  1.7325078248977661 2.1920294761657715 3.924537181854248
Loss :  1.731489896774292 1.8353862762451172 3.566876173019409
Loss :  1.8083457946777344 4.3457560539245605 6.154101848602295
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.803995966911316 4.384219646453857 6.188215732574463
Loss :  1.8053748607635498 4.283812522888184 6.0891876220703125
Loss :  1.808195948600769 4.168123722076416 5.976319789886475
Total LOSS train 3.9531301204974834 valid 6.101956248283386
CE LOSS train 1.734239833171551 valid 0.45204898715019226
Contrastive LOSS train 2.218890301997845 valid 1.042030930519104
EPOCH 219:
Loss :  1.73489248752594 2.8088762760162354 4.543768882751465
Loss :  1.733190655708313 3.1113007068634033 4.844491481781006
Loss :  1.7305405139923096 3.404839038848877 5.135379791259766
Loss :  1.732519507408142 1.934072732925415 3.6665921211242676
Loss :  1.7379287481307983 2.283850908279419 4.021779537200928
Loss :  1.733848214149475 2.3195502758026123 4.053398609161377
Loss :  1.7364988327026367 2.139854907989502 3.8763537406921387
Loss :  1.7326529026031494 2.562175750732422 4.294828414916992
Loss :  1.7342183589935303 2.036299705505371 3.7705180644989014
Loss :  1.7240042686462402 1.9323965311050415 3.656400680541992
Loss :  1.7351242303848267 2.7121009826660156 4.447225093841553
Loss :  1.7425377368927002 2.6966700553894043 4.439208030700684
Loss :  1.7366191148757935 2.6454811096191406 4.3821001052856445
Loss :  1.733446478843689 2.59162974357605 4.325076103210449
Loss :  1.738013744354248 1.8636512756347656 3.6016650199890137
Loss :  1.7322471141815186 1.9121333360671997 3.644380569458008
Loss :  1.7363654375076294 1.729181170463562 3.4655466079711914
Loss :  1.7348829507827759 1.6505818367004395 3.385464668273926
Loss :  1.7336523532867432 1.6730669736862183 3.406719207763672
Loss :  1.7315034866333008 1.8989278078079224 3.6304311752319336
  batch 20 loss: 1.7315034866333008, 1.8989278078079224, 3.6304311752319336
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7339290380477905 1.9010910987854004 3.6350202560424805
Loss :  1.7349426746368408 1.9436211585998535 3.6785638332366943
Loss :  1.736151099205017 1.718651294708252 3.4548025131225586
Loss :  1.7410225868225098 1.8088475465774536 3.549870014190674
Loss :  1.7354499101638794 1.990389347076416 3.725839138031006
Loss :  1.731319546699524 2.1218228340148926 3.853142261505127
Loss :  1.7397316694259644 2.5527915954589844 4.292523384094238
Loss :  1.7316055297851562 2.223858118057251 3.9554636478424072
Loss :  1.737457275390625 2.257296562194824 3.994753837585449
Loss :  1.7296903133392334 2.093630075454712 3.8233203887939453
Loss :  1.7438905239105225 2.6680781841278076 4.41196870803833
Loss :  1.7372105121612549 3.511212110519409 5.248422622680664
Loss :  1.7347381114959717 3.810321807861328 5.545060157775879
Loss :  1.732298493385315 2.2319753170013428 3.9642739295959473
Loss :  1.7410684823989868 2.5414395332336426 4.28250789642334
Loss :  1.739424467086792 2.2526447772979736 3.9920692443847656
Loss :  1.737654209136963 2.5584139823913574 4.29606819152832
Loss :  1.7351278066635132 2.2442736625671387 3.9794015884399414
Loss :  1.7380379438400269 2.7034101486206055 4.441448211669922
Loss :  1.7362298965454102 1.9178661108016968 3.6540961265563965
  batch 40 loss: 1.7362298965454102, 1.9178661108016968, 3.6540961265563965
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7373712062835693 2.1947426795959473 3.9321138858795166
Loss :  1.737337350845337 1.7191240787506104 3.4564614295959473
Loss :  1.7360732555389404 1.9489063024520874 3.6849794387817383
Loss :  1.7394556999206543 2.2924723625183105 4.031928062438965
Loss :  1.7392525672912598 2.3577935695648193 4.0970458984375
Loss :  1.7341654300689697 2.106679677963257 3.8408451080322266
Loss :  1.7382428646087646 3.311596632003784 5.049839496612549
Loss :  1.735230565071106 3.3273696899414062 5.062600135803223
Loss :  1.7383707761764526 4.0788960456848145 5.817266941070557
Loss :  1.735348105430603 3.3511881828308105 5.086536407470703
Loss :  1.7309339046478271 3.1577348709106445 4.888669013977051
Loss :  1.7343757152557373 2.2731614112854004 4.007536888122559
Loss :  1.7408190965652466 1.73258376121521 3.473402976989746
Loss :  1.731656789779663 1.9013779163360596 3.6330347061157227
Loss :  1.733842134475708 2.3007419109344482 4.034584045410156
Loss :  1.7328567504882812 2.229245185852051 3.962101936340332
Loss :  1.7375767230987549 2.620793581008911 4.358370304107666
Loss :  1.7384682893753052 1.8073357343673706 3.545804023742676
Loss :  1.7388113737106323 2.736131429672241 4.474942684173584
Loss :  1.7362926006317139 2.7879858016967773 4.52427864074707
  batch 60 loss: 1.7362926006317139, 2.7879858016967773, 4.52427864074707
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7354196310043335 2.4987146854400635 4.234134197235107
Loss :  1.7370543479919434 2.2757856845855713 4.012840270996094
Loss :  1.736871361732483 2.548908233642578 4.2857794761657715
Loss :  1.7314453125 2.7103779315948486 4.4418230056762695
Loss :  1.7303975820541382 2.922058582305908 4.652456283569336
Loss :  1.779975414276123 4.228672027587891 6.008647441864014
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7793115377426147 4.2536725997924805 6.032984256744385
Loss :  1.777744174003601 4.103670597076416 5.881414890289307
Loss :  1.7830840349197388 3.8425192832946777 5.625603199005127
Total LOSS train 4.137835678687463 valid 5.887162446975708
CE LOSS train 1.7355282563429613 valid 0.4457710087299347
Contrastive LOSS train 2.402307420510512 valid 0.9606298208236694
EPOCH 220:
Loss :  1.7347997426986694 2.8187949657440186 4.553594589233398
Loss :  1.732898235321045 2.117751121520996 3.850649356842041
Loss :  1.7296991348266602 1.5949625968933105 3.3246617317199707
Loss :  1.7328832149505615 2.407784938812256 4.140667915344238
Loss :  1.7368844747543335 2.163710832595825 3.900595188140869
Loss :  1.7336729764938354 2.63232421875 4.365997314453125
Loss :  1.7361845970153809 2.5001070499420166 4.236291885375977
Loss :  1.732612133026123 3.08394455909729 4.816556930541992
Loss :  1.7347110509872437 2.199589729309082 3.9343008995056152
Loss :  1.7237540483474731 1.857666254043579 3.581420421600342
Loss :  1.735464096069336 2.251295328140259 3.9867594242095947
Loss :  1.743540644645691 2.7376654148101807 4.481205940246582
Loss :  1.7370901107788086 2.372230291366577 4.109320640563965
Loss :  1.7342766523361206 2.242633819580078 3.9769105911254883
Loss :  1.7388924360275269 2.427170753479004 4.16606330871582
Loss :  1.7325493097305298 2.283745765686035 4.016294956207275
Loss :  1.7382433414459229 1.9499155282974243 3.6881589889526367
Loss :  1.735169768333435 2.224395275115967 3.9595651626586914
Loss :  1.7355278730392456 1.9649803638458252 3.7005081176757812
Loss :  1.7325279712677002 1.8944746255874634 3.627002716064453
  batch 20 loss: 1.7325279712677002, 1.8944746255874634, 3.627002716064453
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7346588373184204 1.8593136072158813 3.5939724445343018
Loss :  1.736844539642334 1.7828670740127563 3.519711494445801
Loss :  1.7362781763076782 2.2854323387145996 4.021710395812988
Loss :  1.7412993907928467 2.337524890899658 4.078824043273926
Loss :  1.7353911399841309 1.7333714962005615 3.4687626361846924
Loss :  1.731080174446106 2.394056558609009 4.125136852264404
Loss :  1.739114761352539 2.5798261165618896 4.318941116333008
Loss :  1.7309136390686035 1.783349633216858 3.514263153076172
Loss :  1.7369188070297241 2.3391222953796387 4.076041221618652
Loss :  1.7294732332229614 2.0687108039855957 3.7981839179992676
Loss :  1.7430499792099 2.487943172454834 4.230993270874023
Loss :  1.737158179283142 2.82131290435791 4.558471202850342
Loss :  1.7310807704925537 2.285059928894043 4.016140937805176
Loss :  1.730554461479187 2.4489657878875732 4.179520130157471
Loss :  1.7382136583328247 2.7157251834869385 4.453938961029053
Loss :  1.738235592842102 2.614483594894409 4.352719306945801
Loss :  1.73574960231781 2.0381081104278564 3.773857593536377
Loss :  1.7326408624649048 2.263599395751953 3.9962401390075684
Loss :  1.7345279455184937 1.8939595222473145 3.6284875869750977
Loss :  1.7364000082015991 1.7437801361083984 3.480180263519287
  batch 40 loss: 1.7364000082015991, 1.7437801361083984, 3.480180263519287
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7339869737625122 2.0356011390686035 3.769587993621826
Loss :  1.7328652143478394 1.6135210990905762 3.346386432647705
Loss :  1.735222339630127 2.0394279956817627 3.7746503353118896
Loss :  1.7342371940612793 2.3716046810150146 4.105841636657715
Loss :  1.7369158267974854 2.033267021179199 3.7701828479766846
Loss :  1.7332016229629517 1.847550868988037 3.580752372741699
Loss :  1.7330210208892822 1.8869287967681885 3.6199498176574707
Loss :  1.7348440885543823 1.8971314430236816 3.6319756507873535
Loss :  1.7346464395523071 1.837717890739441 3.572364330291748
Loss :  1.733995795249939 1.650698184967041 3.3846940994262695
Loss :  1.730363368988037 1.8622019290924072 3.5925652980804443
Loss :  1.7340799570083618 1.8709155321121216 3.6049954891204834
Loss :  1.7384082078933716 1.6667619943618774 3.405170202255249
Loss :  1.7333002090454102 1.8699090480804443 3.6032092571258545
Loss :  1.7350209951400757 1.7820740938186646 3.5170950889587402
Loss :  1.7297950983047485 1.7012778520584106 3.431072950363159
Loss :  1.7360762357711792 1.969454050064087 3.7055301666259766
Loss :  1.7387162446975708 1.4728338718414307 3.211550235748291
Loss :  1.739212989807129 2.1017825603485107 3.8409955501556396
Loss :  1.7346549034118652 1.7335412502288818 3.468196153640747
  batch 60 loss: 1.7346549034118652, 1.7335412502288818, 3.468196153640747
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7341972589492798 1.7947003841400146 3.528897762298584
Loss :  1.7364540100097656 1.6596218347549438 3.39607572555542
Loss :  1.7357127666473389 1.6086044311523438 3.3443171977996826
Loss :  1.7327907085418701 2.2106516361236572 3.9434423446655273
Loss :  1.7322373390197754 1.3726632595062256 3.104900598526001
Loss :  1.7923173904418945 4.323307991027832 6.115625381469727
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.7894474267959595 4.305429935455322 6.094877243041992
Loss :  1.7906007766723633 4.2547688484191895 6.045369625091553
Loss :  1.7926331758499146 4.094542980194092 5.887176036834717
Total LOSS train 3.8285695736224836 valid 6.035762071609497
CE LOSS train 1.7348449596991906 valid 0.44815829396247864
Contrastive LOSS train 2.0937245974173915 valid 1.023635745048523
EPOCH 221:
Loss :  1.735665202140808 1.8376144170761108 3.573279619216919
Loss :  1.7328907251358032 1.8708871603012085 3.6037778854370117
Loss :  1.7309460639953613 1.8307640552520752 3.5617101192474365
Loss :  1.7335985898971558 1.6464556455612183 3.380054235458374
Loss :  1.7375919818878174 1.3016692399978638 3.0392613410949707
Loss :  1.7332748174667358 1.3451939821243286 3.0784687995910645
Loss :  1.7353639602661133 1.8661094903945923 3.601473331451416
Loss :  1.7308744192123413 1.4758068323135376 3.206681251525879
Loss :  1.7334709167480469 1.8230079412460327 3.556478977203369
Loss :  1.7231776714324951 1.8902745246887207 3.613452196121216
Loss :  1.7338804006576538 2.150038480758667 3.8839187622070312
Loss :  1.7425423860549927 1.721774935722351 3.4643173217773438
Loss :  1.7357946634292603 1.5849930047988892 3.3207876682281494
Loss :  1.7333847284317017 2.072211503982544 3.805596351623535
Loss :  1.7373167276382446 2.01275372505188 3.750070571899414
Loss :  1.7319624423980713 2.1442787647247314 3.8762412071228027
Loss :  1.7359060049057007 2.1719090938568115 3.9078149795532227
Loss :  1.733530044555664 1.517323613166809 3.2508535385131836
Loss :  1.7345081567764282 1.3879657983779907 3.122473955154419
Loss :  1.7301822900772095 1.719185471534729 3.4493677616119385
  batch 20 loss: 1.7301822900772095, 1.719185471534729, 3.4493677616119385
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7333407402038574 1.9567712545394897 3.6901121139526367
Loss :  1.734696865081787 2.0857150554656982 3.8204119205474854
Loss :  1.7354819774627686 1.9697279930114746 3.705209970474243
Loss :  1.7403690814971924 2.123370409011841 3.863739490509033
Loss :  1.7362264394760132 2.279512643814087 4.0157389640808105
Loss :  1.731483817100525 2.3763792514801025 4.107862949371338
Loss :  1.7395355701446533 2.23335862159729 3.9728941917419434
Loss :  1.7306253910064697 2.530911922454834 4.261537551879883
Loss :  1.7375645637512207 2.2003042697906494 3.93786883354187
Loss :  1.7286746501922607 2.3225913047790527 4.051265716552734
Loss :  1.7433364391326904 1.913732647895813 3.657069206237793
Loss :  1.7377513647079468 1.6536983251571655 3.3914496898651123
Loss :  1.7304331064224243 1.670277714729309 3.4007108211517334
Loss :  1.7308048009872437 1.8171285390853882 3.547933340072632
Loss :  1.7379003763198853 2.219777822494507 3.9576783180236816
Loss :  1.7379616498947144 2.48513126373291 4.223093032836914
Loss :  1.7348846197128296 2.5265696048736572 4.261454105377197
Loss :  1.7308533191680908 1.7105478048324585 3.4414010047912598
Loss :  1.7340247631072998 2.6042320728302 4.3382568359375
Loss :  1.7350623607635498 2.518907308578491 4.253969669342041
  batch 40 loss: 1.7350623607635498, 2.518907308578491, 4.253969669342041
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7331832647323608 2.3216140270233154 4.054797172546387
Loss :  1.7330065965652466 2.3652002811431885 4.098206996917725
Loss :  1.7353068590164185 2.0940370559692383 3.829343795776367
Loss :  1.734775424003601 1.9599355459213257 3.6947109699249268
Loss :  1.7371485233306885 1.6308571100234985 3.3680057525634766
Loss :  1.7326551675796509 1.4913636445999146 3.2240188121795654
Loss :  1.7329849004745483 1.418012261390686 3.1509971618652344
Loss :  1.7348086833953857 1.752896785736084 3.4877054691314697
Loss :  1.7346431016921997 1.8384718894958496 3.5731148719787598
Loss :  1.733781337738037 1.7259715795516968 3.4597530364990234
Loss :  1.7294481992721558 1.9989997148513794 3.728447914123535
Loss :  1.7336221933364868 2.265249013900757 3.998871326446533
Loss :  1.7388566732406616 1.9178216457366943 3.6566781997680664
Loss :  1.7319759130477905 2.0915281772613525 3.8235039710998535
Loss :  1.7336324453353882 2.1025679111480713 3.83620023727417
Loss :  1.7310320138931274 2.3819470405578613 4.112978935241699
Loss :  1.735846996307373 2.155714273452759 3.891561269760132
Loss :  1.7384248971939087 2.0791754722595215 3.8176002502441406
Loss :  1.7383556365966797 2.5239243507385254 4.262279987335205
Loss :  1.7351163625717163 1.7926496267318726 3.527765989303589
  batch 60 loss: 1.7351163625717163, 1.7926496267318726, 3.527765989303589
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7355314493179321 1.8658677339553833 3.6013991832733154
Loss :  1.7365069389343262 1.6875635385513306 3.424070358276367
Loss :  1.737623691558838 1.460357427597046 3.197981119155884
Loss :  1.7322280406951904 2.151951551437378 3.8841795921325684
Loss :  1.7317451238632202 1.4698905944824219 3.2016358375549316
Loss :  1.799759030342102 4.416764259338379 6.216523170471191
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.795860767364502 4.345763683319092 6.141624450683594
Loss :  1.7974684238433838 4.240084171295166 6.037552833557129
Loss :  1.7993206977844238 4.297834873199463 6.097155570983887
Total LOSS train 3.6900237817030686 valid 6.12321400642395
CE LOSS train 1.7344479157374455 valid 0.44983017444610596
Contrastive LOSS train 1.9555758733015793 valid 1.0744587182998657
EPOCH 222:
Loss :  1.7358793020248413 1.9156930446624756 3.6515722274780273
Loss :  1.735244631767273 2.6722617149353027 4.407506465911865
Loss :  1.7321144342422485 1.8825714588165283 3.6146860122680664
Loss :  1.7349449396133423 1.8930577039718628 3.628002643585205
Loss :  1.7385269403457642 1.8235121965408325 3.5620391368865967
Loss :  1.7356693744659424 1.8342887163162231 3.569958209991455
Loss :  1.7367842197418213 1.823593020439148 3.5603771209716797
Loss :  1.7335623502731323 1.6462881565093994 3.379850387573242
Loss :  1.7346910238265991 1.7134528160095215 3.44814395904541
Loss :  1.7243059873580933 1.582578420639038 3.306884288787842
Loss :  1.735006332397461 1.8441426753997803 3.579149007797241
Loss :  1.7422659397125244 2.1992714405059814 3.941537380218506
Loss :  1.7363605499267578 2.110633611679077 3.846994161605835
Loss :  1.7337654829025269 2.0722157955169678 3.805981159210205
Loss :  1.7376642227172852 2.2900023460388184 4.0276665687561035
Loss :  1.7323803901672363 2.2083818912506104 3.9407622814178467
Loss :  1.736220359802246 2.2422430515289307 3.9784634113311768
Loss :  1.73439359664917 2.2962658405303955 4.0306596755981445
Loss :  1.7343356609344482 1.918312907218933 3.652648448944092
Loss :  1.7309564352035522 3.2419593334198 4.9729156494140625
  batch 20 loss: 1.7309564352035522, 3.2419593334198, 4.9729156494140625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.733725666999817 1.8002839088439941 3.5340094566345215
Loss :  1.734959602355957 2.0295631885528564 3.7645227909088135
Loss :  1.7356051206588745 1.5420035123825073 3.277608633041382
Loss :  1.7407910823822021 1.973019003868103 3.7138099670410156
Loss :  1.736525297164917 1.957517385482788 3.694042682647705
Loss :  1.7310752868652344 1.8038274049758911 3.534902572631836
Loss :  1.7390674352645874 2.635464906692505 4.374532222747803
Loss :  1.7304662466049194 2.6845600605010986 4.4150261878967285
Loss :  1.7372297048568726 2.751455545425415 4.488685131072998
Loss :  1.7303228378295898 2.310777187347412 4.041100025177002
Loss :  1.7438544034957886 2.2056784629821777 3.949532985687256
Loss :  1.7372691631317139 1.7857533693313599 3.5230226516723633
Loss :  1.7318559885025024 1.893628716468811 3.6254847049713135
Loss :  1.7308621406555176 1.8150759935379028 3.545938014984131
Loss :  1.7391642332077026 2.4567501544952393 4.195914268493652
Loss :  1.7385873794555664 2.667987585067749 4.4065752029418945
Loss :  1.7356905937194824 3.8651235103607178 5.600813865661621
Loss :  1.7320972681045532 2.8962481021881104 4.628345489501953
Loss :  1.7345558404922485 2.5980989933013916 4.33265495300293
Loss :  1.735225796699524 2.163918972015381 3.8991446495056152
  batch 40 loss: 1.735225796699524, 2.163918972015381, 3.8991446495056152
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7337757349014282 3.1826696395874023 4.916445255279541
Loss :  1.73257577419281 2.609417676925659 4.34199333190918
Loss :  1.7348265647888184 2.7975308895111084 4.532357215881348
Loss :  1.7336065769195557 2.6149115562438965 4.348518371582031
Loss :  1.7360349893569946 1.7781389951705933 3.514173984527588
Loss :  1.7326371669769287 3.11205792427063 4.844695091247559
Loss :  1.7320224046707153 2.828906774520874 4.560929298400879
Loss :  1.7335205078125 1.9069432020187378 3.6404638290405273
Loss :  1.7353547811508179 2.564012289047241 4.2993669509887695
Loss :  1.732871651649475 2.084334135055542 3.8172059059143066
Loss :  1.7296254634857178 2.2274301052093506 3.9570555686950684
Loss :  1.733190655708313 2.781832218170166 4.5150227546691895
Loss :  1.7384581565856934 1.4771877527236938 3.2156457901000977
Loss :  1.731561541557312 1.9430034160614014 3.674564838409424
Loss :  1.734066367149353 2.0731120109558105 3.807178497314453
Loss :  1.7304346561431885 1.9893109798431396 3.719745635986328
Loss :  1.7364695072174072 1.8999884128570557 3.636457920074463
Loss :  1.7393770217895508 1.9492005109786987 3.688577651977539
Loss :  1.7395576238632202 3.0150392055511475 4.754596710205078
Loss :  1.735879898071289 2.2481701374053955 3.9840500354766846
  batch 60 loss: 1.735879898071289, 2.2481701374053955, 3.9840500354766846
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7361871004104614 1.5957785844802856 3.331965684890747
Loss :  1.7374716997146606 2.222709894180298 3.960181713104248
Loss :  1.7383671998977661 2.387958526611328 4.126325607299805
Loss :  1.7332022190093994 2.0463104248046875 3.779512643814087
Loss :  1.7326326370239258 1.5183900594711304 3.2510228157043457
Loss :  1.8107725381851196 4.374887466430664 6.185659885406494
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8065147399902344 4.472710132598877 6.279224872589111
Loss :  1.8079413175582886 4.1474175453186035 5.955358982086182
Loss :  1.8099337816238403 4.14713716506958 5.957070827484131
Total LOSS train 3.9487618116232066 valid 6.0943286418914795
CE LOSS train 1.7348878640394945 valid 0.4524834454059601
Contrastive LOSS train 2.2138739604216355 valid 1.036784291267395
EPOCH 223:
Loss :  1.7361456155776978 2.0014376640319824 3.7375831604003906
Loss :  1.73507821559906 1.807832956314087 3.5429110527038574
Loss :  1.7319577932357788 1.8064305782318115 3.538388252258301
Loss :  1.7346364259719849 2.0463144779205322 3.7809510231018066
Loss :  1.737937092781067 1.7059307098388672 3.4438676834106445
Loss :  1.7348109483718872 1.9611239433288574 3.695934772491455
Loss :  1.7355291843414307 2.002335548400879 3.7378647327423096
Loss :  1.732596755027771 2.3970510959625244 4.129647731781006
Loss :  1.7340682744979858 2.450369119644165 4.184437274932861
Loss :  1.7228444814682007 2.155961513519287 3.8788061141967773
Loss :  1.73387610912323 2.499825954437256 4.233702182769775
Loss :  1.7430285215377808 3.4938268661499023 5.236855506896973
Loss :  1.7349915504455566 2.5972700119018555 4.332261562347412
Loss :  1.732348084449768 2.581606149673462 4.3139543533325195
Loss :  1.735485553741455 3.310624122619629 5.046109676361084
Loss :  1.731376051902771 3.0474424362182617 4.778818607330322
Loss :  1.7344653606414795 2.69842529296875 4.432890892028809
Loss :  1.7336504459381104 1.9634153842926025 3.697065830230713
Loss :  1.7335762977600098 2.360485076904297 4.094061374664307
Loss :  1.7301944494247437 2.3136610984802246 4.043855667114258
  batch 20 loss: 1.7301944494247437, 2.3136610984802246, 4.043855667114258
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7333914041519165 3.0234711170196533 4.756862640380859
Loss :  1.7352056503295898 2.4483132362365723 4.183518886566162
Loss :  1.735593557357788 2.2432470321655273 3.9788405895233154
Loss :  1.7412770986557007 2.6107096672058105 4.351986885070801
Loss :  1.7367613315582275 2.467709541320801 4.204470634460449
Loss :  1.7312949895858765 1.7691620588302612 3.5004570484161377
Loss :  1.739356517791748 1.688348412513733 3.4277048110961914
Loss :  1.7303723096847534 1.8070534467697144 3.5374257564544678
Loss :  1.7376877069473267 1.9832327365875244 3.7209205627441406
Loss :  1.7299466133117676 2.1226115226745605 3.852558135986328
Loss :  1.744508981704712 2.3902814388275146 4.134790420532227
Loss :  1.7378878593444824 2.363431453704834 4.101319313049316
Loss :  1.7315459251403809 2.976393222808838 4.707939147949219
Loss :  1.7308375835418701 1.983716607093811 3.7145543098449707
Loss :  1.7389633655548096 2.587611436843872 4.326574802398682
Loss :  1.7382149696350098 2.6725680828094482 4.410782814025879
Loss :  1.7351833581924438 1.827393651008606 3.56257700920105
Loss :  1.7316477298736572 2.120178699493408 3.8518264293670654
Loss :  1.733896255493164 2.1336331367492676 3.8675293922424316
Loss :  1.735368013381958 1.655352234840393 3.3907203674316406
  batch 40 loss: 1.735368013381958, 1.655352234840393, 3.3907203674316406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7331488132476807 2.4265029430389404 4.159651756286621
Loss :  1.7324693202972412 1.6143667697906494 3.3468360900878906
Loss :  1.7354152202606201 2.1677069664001465 3.9031221866607666
Loss :  1.7332578897476196 1.8061549663543701 3.5394129753112793
Loss :  1.7366529703140259 1.7960221767425537 3.532675266265869
Loss :  1.732948899269104 1.35828697681427 3.091235876083374
Loss :  1.732187271118164 1.3011536598205566 3.0333409309387207
Loss :  1.7346267700195312 1.6591013669967651 3.393728256225586
Loss :  1.7346152067184448 1.8267053365707397 3.5613205432891846
Loss :  1.7340220212936401 2.0647497177124023 3.798771858215332
Loss :  1.7306283712387085 2.200723171234131 3.931351661682129
Loss :  1.7344640493392944 2.4232828617095947 4.1577467918396
Loss :  1.7387030124664307 2.1969563961029053 3.935659408569336
Loss :  1.7324963808059692 2.908987522125244 4.641483783721924
Loss :  1.7343530654907227 2.2870266437530518 4.021379470825195
Loss :  1.7294889688491821 2.4901134967803955 4.219602584838867
Loss :  1.735653042793274 1.4819914102554321 3.217644453048706
Loss :  1.7382527589797974 1.395803689956665 3.134056568145752
Loss :  1.7381877899169922 2.8247311115264893 4.562918663024902
Loss :  1.7342950105667114 2.1302640438079834 3.8645591735839844
  batch 60 loss: 1.7342950105667114, 2.1302640438079834, 3.8645591735839844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7337599992752075 1.7901266813278198 3.5238866806030273
Loss :  1.7357043027877808 1.9583494663238525 3.6940536499023438
Loss :  1.736031174659729 1.6782927513122559 3.4143238067626953
Loss :  1.7316460609436035 2.2064623832702637 3.938108444213867
Loss :  1.7309446334838867 1.3724415302276611 3.103386163711548
Loss :  1.7960783243179321 4.36392879486084 6.160006999969482
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.792887806892395 4.376277446746826 6.169165134429932
Loss :  1.7941197156906128 4.265544414520264 6.059664249420166
Loss :  1.7968400716781616 4.323671340942383 6.120511531829834
Total LOSS train 3.9105166838719296 valid 6.1273369789123535
CE LOSS train 1.7344844836455124 valid 0.4492100179195404
Contrastive LOSS train 2.1760321965584386 valid 1.0809178352355957
EPOCH 224:
Loss :  1.7354484796524048 1.6508718729019165 3.3863203525543213
Loss :  1.734062671661377 2.163564443588257 3.897627115249634
Loss :  1.7315564155578613 1.8361371755599976 3.5676937103271484
Loss :  1.734741449356079 2.0791335105895996 3.8138749599456787
Loss :  1.7379710674285889 2.003310441970825 3.741281509399414
Loss :  1.7360471487045288 1.6497946977615356 3.3858418464660645
Loss :  1.7369588613510132 2.6565403938293457 4.393499374389648
Loss :  1.7349096536636353 2.294309139251709 4.029218673706055
Loss :  1.736592411994934 2.306887626647949 4.043479919433594
Loss :  1.7249728441238403 2.3595352172851562 4.084507942199707
Loss :  1.7366113662719727 2.3118133544921875 4.04842472076416
Loss :  1.7452234029769897 2.037426710128784 3.7826499938964844
Loss :  1.7372286319732666 2.5263569355010986 4.263585567474365
Loss :  1.7355093955993652 2.193307399749756 3.928816795349121
Loss :  1.7405568361282349 1.864463448524475 3.60502028465271
Loss :  1.7333828210830688 1.8036407232284546 3.5370235443115234
Loss :  1.738948941230774 2.5070443153381348 4.245993137359619
Loss :  1.7343249320983887 1.9349777698516846 3.6693027019500732
Loss :  1.735480785369873 1.8180954456329346 3.5535762310028076
Loss :  1.7341262102127075 2.1848742961883545 3.9190006256103516
  batch 20 loss: 1.7341262102127075, 2.1848742961883545, 3.9190006256103516
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.734871506690979 2.0594606399536133 3.7943320274353027
Loss :  1.7371089458465576 2.103238582611084 3.8403475284576416
Loss :  1.7373250722885132 2.0456790924072266 3.7830042839050293
Loss :  1.7418937683105469 2.1470742225646973 3.888967990875244
Loss :  1.736742377281189 2.025547742843628 3.7622900009155273
Loss :  1.7326316833496094 2.171581268310547 3.9042129516601562
Loss :  1.739782691001892 2.971724510192871 4.711507320404053
Loss :  1.7310967445373535 2.637676477432251 4.368773460388184
Loss :  1.7373535633087158 1.8802623748779297 3.6176159381866455
Loss :  1.7303555011749268 1.9599168300628662 3.690272331237793
Loss :  1.7442213296890259 2.127810001373291 3.8720312118530273
Loss :  1.7378015518188477 2.0250139236450195 3.762815475463867
Loss :  1.7315022945404053 1.8627991676330566 3.594301462173462
Loss :  1.7318428754806519 1.8416049480438232 3.5734477043151855
Loss :  1.738968849182129 2.356205701828003 4.095174789428711
Loss :  1.7391988039016724 2.085925579071045 3.8251242637634277
Loss :  1.7364658117294312 2.4755423069000244 4.212007999420166
Loss :  1.7325891256332397 1.989596962928772 3.7221860885620117
Loss :  1.7351247072219849 1.719374179840088 3.454498767852783
Loss :  1.7374622821807861 1.5826643705368042 3.320126533508301
  batch 40 loss: 1.7374622821807861, 1.5826643705368042, 3.320126533508301
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7344303131103516 2.057171106338501 3.7916014194488525
Loss :  1.733378291130066 1.8497841358184814 3.583162307739258
Loss :  1.7367632389068604 2.694366693496704 4.4311299324035645
Loss :  1.734122395515442 1.6263492107391357 3.360471725463867
Loss :  1.7376004457473755 1.3725849390029907 3.110185384750366
Loss :  1.7347921133041382 1.940793752670288 3.6755857467651367
Loss :  1.7335957288742065 1.9278583526611328 3.661454200744629
Loss :  1.7351524829864502 1.6245888471603394 3.3597412109375
Loss :  1.736796259880066 1.755333423614502 3.4921298027038574
Loss :  1.733971118927002 1.7386037111282349 3.4725747108459473
Loss :  1.7306989431381226 1.9567457437515259 3.6874446868896484
Loss :  1.7348690032958984 1.8982104063034058 3.6330795288085938
Loss :  1.739405632019043 1.6787728071212769 3.4181785583496094
Loss :  1.7325544357299805 2.093247175216675 3.8258016109466553
Loss :  1.7341316938400269 1.8948646783828735 3.6289963722229004
Loss :  1.7315868139266968 1.9578437805175781 3.6894307136535645
Loss :  1.7370997667312622 1.603790521621704 3.340890407562256
Loss :  1.7391706705093384 1.9419406652450562 3.6811113357543945
Loss :  1.7393649816513062 2.124124526977539 3.8634896278381348
Loss :  1.7364484071731567 2.110386371612549 3.846834659576416
  batch 60 loss: 1.7364484071731567, 2.110386371612549, 3.846834659576416
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7362569570541382 1.9511210918426514 3.6873779296875
Loss :  1.7376635074615479 1.9979748725891113 3.735638380050659
Loss :  1.7379320859909058 1.1436172723770142 2.88154935836792
Loss :  1.73268461227417 1.879495620727539 3.612180233001709
Loss :  1.7316168546676636 1.4258332252502441 3.1574501991271973
Loss :  1.7936031818389893 4.351505756378174 6.145109176635742
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.7908802032470703 4.447547912597656 6.238428115844727
Loss :  1.7915534973144531 4.232513904571533 6.024067401885986
Loss :  1.7950392961502075 4.271303653717041 6.066342830657959
Total LOSS train 3.7433425719921405 valid 6.1184868812561035
CE LOSS train 1.7357089317761936 valid 0.4487598240375519
Contrastive LOSS train 2.007633642049936 valid 1.0678259134292603
EPOCH 225:
Loss :  1.736029028892517 1.7515455484390259 3.487574577331543
Loss :  1.7346773147583008 2.108569622039795 3.8432469367980957
Loss :  1.7315298318862915 1.581895351409912 3.313425064086914
Loss :  1.734426736831665 1.7151734828948975 3.4496002197265625
Loss :  1.7382158041000366 1.7645443677902222 3.502760171890259
Loss :  1.7354575395584106 1.3405461311340332 3.0760035514831543
Loss :  1.7365425825119019 1.721092939376831 3.4576354026794434
Loss :  1.73350191116333 2.0085721015930176 3.7420740127563477
Loss :  1.7352256774902344 1.9220441579818726 3.6572699546813965
Loss :  1.7242976427078247 1.5853530168533325 3.3096506595611572
Loss :  1.7353010177612305 2.3716115951538086 4.106912612915039
Loss :  1.7433546781539917 1.860447645187378 3.60380220413208
Loss :  1.7365219593048096 1.9804617166519165 3.7169837951660156
Loss :  1.7340357303619385 2.157374620437622 3.8914103507995605
Loss :  1.738181471824646 1.8869072198867798 3.625088691711426
Loss :  1.7324907779693604 2.1073312759399414 3.8398220539093018
Loss :  1.73646080493927 1.9148534536361694 3.6513142585754395
Loss :  1.7345741987228394 1.7173329591751099 3.451907157897949
Loss :  1.7344709634780884 1.6367557048797607 3.3712267875671387
Loss :  1.7315666675567627 1.7970417737960815 3.5286083221435547
  batch 20 loss: 1.7315666675567627, 1.7970417737960815, 3.5286083221435547
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7342562675476074 1.4998772144317627 3.23413348197937
Loss :  1.7357568740844727 1.777182936668396 3.512939929962158
Loss :  1.7365580797195435 1.9361436367034912 3.672701835632324
Loss :  1.7417166233062744 2.0203003883361816 3.762017011642456
Loss :  1.7367695569992065 2.0582427978515625 3.7950124740600586
Loss :  1.732164978981018 2.167569398880005 3.8997344970703125
Loss :  1.7398172616958618 1.9495785236358643 3.6893959045410156
Loss :  1.7313036918640137 2.051273822784424 3.7825775146484375
Loss :  1.7378568649291992 1.9913262128829956 3.7291831970214844
Loss :  1.730715036392212 2.7013704776763916 4.4320855140686035
Loss :  1.7441438436508179 2.406445264816284 4.1505889892578125
Loss :  1.7377220392227173 1.9220852851867676 3.6598072052001953
Loss :  1.7318167686462402 1.8142305612564087 3.5460472106933594
Loss :  1.7314658164978027 1.9195058345794678 3.6509716510772705
Loss :  1.739105224609375 2.482862949371338 4.221968173980713
Loss :  1.7389543056488037 3.1003479957580566 4.839302062988281
Loss :  1.735514521598816 2.6662755012512207 4.401790142059326
Loss :  1.731502890586853 2.4763565063476562 4.207859516143799
Loss :  1.7347688674926758 1.7492269277572632 3.4839959144592285
Loss :  1.73552405834198 1.9660491943359375 3.701573371887207
  batch 40 loss: 1.73552405834198, 1.9660491943359375, 3.701573371887207
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7338817119598389 2.5914955139160156 4.325377464294434
Loss :  1.7335726022720337 2.207334280014038 3.9409070014953613
Loss :  1.7361055612564087 2.0027565956115723 3.7388620376586914
Loss :  1.7342520952224731 2.1596193313598633 3.893871307373047
Loss :  1.7371975183486938 1.9552890062332153 3.692486524581909
Loss :  1.73383367061615 1.8114943504333496 3.545328140258789
Loss :  1.7329853773117065 1.7130814790725708 3.4460668563842773
Loss :  1.735244631767273 1.8763564825057983 3.6116011142730713
Loss :  1.7356791496276855 1.8577780723571777 3.5934572219848633
Loss :  1.734310269355774 1.8675960302352905 3.6019062995910645
Loss :  1.7310060262680054 2.1583917140960693 3.889397621154785
Loss :  1.7351924180984497 1.951551079750061 3.6867434978485107
Loss :  1.7395602464675903 1.7294366359710693 3.468997001647949
Loss :  1.733618140220642 2.560490131378174 4.2941083908081055
Loss :  1.7357770204544067 2.0326552391052246 3.768432140350342
Loss :  1.731890082359314 2.453219413757324 4.185109615325928
Loss :  1.7379058599472046 2.5947484970092773 4.3326544761657715
Loss :  1.7403080463409424 1.919882893562317 3.660191059112549
Loss :  1.7407058477401733 2.4141781330108643 4.154883861541748
Loss :  1.7371476888656616 2.3740460872650146 4.111193656921387
  batch 60 loss: 1.7371476888656616, 2.3740460872650146, 4.111193656921387
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7372947931289673 1.8356791734695435 3.5729739665985107
Loss :  1.7386595010757446 1.6080691814422607 3.346728801727295
Loss :  1.7391310930252075 2.3611435890197754 4.100274562835693
Loss :  1.7342687845230103 1.803533673286438 3.5378024578094482
Loss :  1.7334381341934204 1.0572590827941895 2.7906970977783203
Loss :  1.8150129318237305 4.3358259201049805 6.150838851928711
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.8102152347564697 4.337460517883301 6.147675514221191
Loss :  1.8119006156921387 4.352475643157959 6.164376258850098
Loss :  1.813860535621643 4.230082988739014 6.043943405151367
Total LOSS train 3.7429239163031944 valid 6.126708507537842
CE LOSS train 1.735496341265165 valid 0.45346513390541077
Contrastive LOSS train 2.0074275658680842 valid 1.0575207471847534
EPOCH 226:
Loss :  1.7372101545333862 1.8878597021102905 3.6250698566436768
Loss :  1.7358413934707642 2.218358039855957 3.9541993141174316
Loss :  1.732755184173584 2.210843563079834 3.943598747253418
Loss :  1.7354663610458374 1.6717396974563599 3.4072060585021973
Loss :  1.7386196851730347 1.9999011754989624 3.738520860671997
Loss :  1.7363516092300415 2.566962957382202 4.303314685821533
Loss :  1.736201286315918 3.355541706085205 5.091742992401123
Loss :  1.7340004444122314 1.67782723903656 3.411827564239502
Loss :  1.736647129058838 3.5042314529418945 5.240878582000732
Loss :  1.7253345251083374 1.7300150394439697 3.4553494453430176
Loss :  1.7355972528457642 2.036980628967285 3.7725777626037598
Loss :  1.7432607412338257 2.5455939769744873 4.288854598999023
Loss :  1.7368931770324707 1.7350963354110718 3.471989631652832
Loss :  1.7347862720489502 1.7766461372375488 3.511432409286499
Loss :  1.7395731210708618 2.1243510246276855 3.863924026489258
Loss :  1.734341025352478 2.520683765411377 4.2550249099731445
Loss :  1.7389256954193115 2.328345775604248 4.0672712326049805
Loss :  1.737684726715088 1.8852835893630981 3.6229681968688965
Loss :  1.7366834878921509 2.363536834716797 4.100220203399658
Loss :  1.7348049879074097 1.9815436601638794 3.716348648071289
  batch 20 loss: 1.7348049879074097, 1.9815436601638794, 3.716348648071289
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7374088764190674 2.0509421825408936 3.788351058959961
Loss :  1.7386846542358398 2.102586269378662 3.841270923614502
Loss :  1.740033745765686 1.804443597793579 3.5444774627685547
Loss :  1.7440979480743408 2.058999538421631 3.8030974864959717
Loss :  1.7384684085845947 2.428436756134033 4.166905403137207
Loss :  1.7350980043411255 2.565652370452881 4.300750255584717
Loss :  1.7428115606307983 1.945077657699585 3.6878890991210938
Loss :  1.734942078590393 2.1328017711639404 3.867743968963623
Loss :  1.7407796382904053 2.6250574588775635 4.365837097167969
Loss :  1.731994867324829 2.665945291519165 4.397940158843994
Loss :  1.7452641725540161 2.3819453716278076 4.127209663391113
Loss :  1.739865779876709 2.898223638534546 4.638089179992676
Loss :  1.7334926128387451 1.928945779800415 3.66243839263916
Loss :  1.733007550239563 2.315636396408081 4.048644065856934
Loss :  1.740323543548584 2.547856330871582 4.288179874420166
Loss :  1.7401684522628784 3.219268798828125 4.959437370300293
Loss :  1.737404227256775 2.2241432666778564 3.961547374725342
Loss :  1.7344024181365967 2.199824094772339 3.9342265129089355
Loss :  1.736445426940918 1.626088261604309 3.3625335693359375
Loss :  1.737890601158142 1.612533688545227 3.350424289703369
  batch 40 loss: 1.737890601158142, 1.612533688545227, 3.350424289703369
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7355940341949463 1.8698146343231201 3.6054086685180664
Loss :  1.735145092010498 1.4640564918518066 3.1992015838623047
Loss :  1.737623691558838 1.5554567575454712 3.2930803298950195
Loss :  1.7356597185134888 1.3267302513122559 3.062389850616455
Loss :  1.7385810613632202 1.2362937927246094 2.974874973297119
Loss :  1.7360821962356567 3.208064317703247 4.944146633148193
Loss :  1.735530138015747 2.070943593978882 3.806473731994629
Loss :  1.7367441654205322 1.8310456275939941 3.5677897930145264
Loss :  1.7393248081207275 1.5243560075759888 3.263680934906006
Loss :  1.7358617782592773 1.4917831420898438 3.227644920349121
Loss :  1.7326256036758423 1.4398847818374634 3.1725103855133057
Loss :  1.7366724014282227 2.337775468826294 4.0744476318359375
Loss :  1.7415549755096436 2.104076623916626 3.8456315994262695
Loss :  1.7347241640090942 1.9987775087356567 3.733501672744751
Loss :  1.7367002964019775 1.854431390762329 3.5911316871643066
Loss :  1.7346062660217285 1.8959789276123047 3.630585193634033
Loss :  1.7391952276229858 2.1200761795043945 3.85927152633667
Loss :  1.741764783859253 1.8512314558029175 3.592996120452881
Loss :  1.7416319847106934 2.468956470489502 4.210588455200195
Loss :  1.7387489080429077 1.7181042432785034 3.456853151321411
  batch 60 loss: 1.7387489080429077, 1.7181042432785034, 3.456853151321411
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7387722730636597 1.6683669090270996 3.407139301300049
Loss :  1.7400002479553223 1.8370293378829956 3.5770297050476074
Loss :  1.7401753664016724 2.348114252090454 4.088289737701416
Loss :  1.7353171110153198 2.3600172996520996 4.095334529876709
Loss :  1.7346243858337402 1.2182397842407227 2.952864170074463
Loss :  1.8050920963287354 4.319779396057129 6.124871253967285
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.8009675741195679 4.36616849899292 6.167136192321777
Loss :  1.8027429580688477 4.2816948890686035 6.084437847137451
Loss :  1.8054451942443848 4.353336811065674 6.158782005310059
Total LOSS train 3.833418141878568 valid 6.133806824684143
CE LOSS train 1.7371819000977737 valid 0.4513612985610962
Contrastive LOSS train 2.0962362472827616 valid 1.0883342027664185
EPOCH 227:
Loss :  1.7384917736053467 2.4701087474823 4.2086005210876465
Loss :  1.7367689609527588 1.9199711084365845 3.656740188598633
Loss :  1.7342537641525269 1.6868547201156616 3.4211084842681885
Loss :  1.7371832132339478 1.729614019393921 3.466797351837158
Loss :  1.7398006916046143 1.4877370595932007 3.2275376319885254
Loss :  1.7376220226287842 1.9896748065948486 3.727296829223633
Loss :  1.7384700775146484 2.743999719619751 4.48246955871582
Loss :  1.736268401145935 2.299910306930542 4.0361785888671875
Loss :  1.7375293970108032 1.9903985261917114 3.7279279232025146
Loss :  1.726542353630066 2.5203912258148193 4.246933460235596
Loss :  1.7375810146331787 3.363868474960327 5.101449489593506
Loss :  1.7450459003448486 3.0577199459075928 4.802765846252441
Loss :  1.7385281324386597 2.6009445190429688 4.339472770690918
Loss :  1.7355738878250122 2.2746031284332275 4.010177135467529
Loss :  1.7406851053237915 2.595648765563965 4.336333751678467
Loss :  1.7333590984344482 2.3822405338287354 4.115599632263184
Loss :  1.7383288145065308 2.639636754989624 4.377965450286865
Loss :  1.734974980354309 1.8419502973556519 3.576925277709961
Loss :  1.7356641292572021 2.3007612228393555 4.036425590515137
Loss :  1.7336692810058594 2.4736266136169434 4.207295894622803
  batch 20 loss: 1.7336692810058594, 2.4736266136169434, 4.207295894622803
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.735785961151123 2.339299440383911 4.075085639953613
Loss :  1.7407200336456299 2.6246085166931152 4.365328788757324
Loss :  1.7402184009552002 2.718280792236328 4.458498954772949
Loss :  1.7432955503463745 2.1156065464019775 3.8589019775390625
Loss :  1.7387285232543945 2.648333787918091 4.387062072753906
Loss :  1.7348099946975708 2.715794086456299 4.45060396194458
Loss :  1.741585612297058 2.106868028640747 3.8484535217285156
Loss :  1.7336214780807495 2.7922892570495605 4.5259108543396
Loss :  1.7394154071807861 2.3218061923980713 4.061221599578857
Loss :  1.7325172424316406 2.6820759773254395 4.41459321975708
Loss :  1.7455406188964844 3.2886903285980225 5.034231185913086
Loss :  1.7396042346954346 2.6669485569000244 4.406552791595459
Loss :  1.7334141731262207 2.3887689113616943 4.122182846069336
Loss :  1.7337452173233032 1.8647098541259766 3.5984549522399902
Loss :  1.740123987197876 2.2480926513671875 3.9882166385650635
Loss :  1.740283489227295 1.8443492650985718 3.5846328735351562
Loss :  1.737484335899353 2.266496181488037 4.00398063659668
Loss :  1.7339651584625244 2.0018985271453857 3.73586368560791
Loss :  1.7361356019973755 1.7948929071426392 3.5310285091400146
Loss :  1.7381865978240967 1.6331398487091064 3.371326446533203
  batch 40 loss: 1.7381865978240967, 1.6331398487091064, 3.371326446533203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.735630989074707 2.375605344772339 4.111236572265625
Loss :  1.7344918251037598 2.637418508529663 4.371910095214844
Loss :  1.7367130517959595 2.7447519302368164 4.481464862823486
Loss :  1.7360966205596924 2.9096570014953613 4.645753860473633
Loss :  1.7387229204177856 3.0895509719848633 4.828273773193359
Loss :  1.735207200050354 3.791677713394165 5.526885032653809
Loss :  1.735258936882019 3.5343716144561768 5.269630432128906
Loss :  1.735823154449463 2.980170965194702 4.715993881225586
Loss :  1.7374491691589355 1.5552804470062256 3.292729616165161
Loss :  1.7349072694778442 1.4130491018295288 3.147956371307373
Loss :  1.7308205366134644 2.390172243118286 4.120992660522461
Loss :  1.7359917163848877 2.177647352218628 3.9136390686035156
Loss :  1.741381049156189 2.137058734893799 3.8784399032592773
Loss :  1.734180212020874 2.225719928741455 3.959900140762329
Loss :  1.7361582517623901 2.0977864265441895 3.833944797515869
Loss :  1.7338541746139526 2.1804463863372803 3.9143004417419434
Loss :  1.737780213356018 3.2889132499694824 5.026693344116211
Loss :  1.739901065826416 1.8506087064743042 3.5905098915100098
Loss :  1.7406717538833618 2.388883352279663 4.1295552253723145
Loss :  1.7384357452392578 2.611008644104004 4.349444389343262
  batch 60 loss: 1.7384357452392578, 2.611008644104004, 4.349444389343262
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7384825944900513 2.3308820724487305 4.069364547729492
Loss :  1.7399640083312988 2.757887125015259 4.497851371765137
Loss :  1.740277647972107 3.9169840812683105 5.657261848449707
Loss :  1.7359710931777954 3.4836249351501465 5.219595909118652
Loss :  1.73459792137146 2.5136349201202393 4.248232841491699
Loss :  1.785780429840088 4.287579536437988 6.073359966278076
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.784073829650879 4.268495559692383 6.052569389343262
Loss :  1.7843815088272095 4.131550312042236 5.915931701660156
Loss :  1.7874127626419067 4.15363883972168 5.941051483154297
Total LOSS train 4.180456821735088 valid 5.995728135108948
CE LOSS train 1.7371429498379047 valid 0.4468531906604767
Contrastive LOSS train 2.443313875565162 valid 1.03840970993042
EPOCH 228:
Loss :  1.7391881942749023 2.4856743812561035 4.224862575531006
Loss :  1.7378919124603271 2.831047296524048 4.568939208984375
Loss :  1.735013723373413 2.670154571533203 4.405168533325195
Loss :  1.7387290000915527 2.871142625808716 4.609871864318848
Loss :  1.7405911684036255 2.2598445415496826 4.000435829162598
Loss :  1.7373965978622437 1.7459574937820435 3.483354091644287
Loss :  1.7397077083587646 2.0692057609558105 3.808913469314575
Loss :  1.7363992929458618 1.6501480340957642 3.386547327041626
Loss :  1.7372628450393677 2.071669816970825 3.8089327812194824
Loss :  1.7271970510482788 1.779666781425476 3.506863832473755
Loss :  1.7374483346939087 2.0963406562805176 3.8337888717651367
Loss :  1.7447837591171265 2.0342602729797363 3.7790441513061523
Loss :  1.7389072179794312 1.6423735618591309 3.3812808990478516
Loss :  1.7365360260009766 2.3810009956359863 4.117537021636963
Loss :  1.7415744066238403 2.4987270832061768 4.240301609039307
Loss :  1.7354934215545654 2.5762906074523926 4.311783790588379
Loss :  1.7400528192520142 2.1383204460144043 3.878373146057129
Loss :  1.7369991540908813 2.1703479290008545 3.9073472023010254
Loss :  1.7381556034088135 2.581326961517334 4.319482803344727
Loss :  1.7351430654525757 2.4843332767486572 4.219476222991943
  batch 20 loss: 1.7351430654525757, 2.4843332767486572, 4.219476222991943
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7373147010803223 1.9644426107406616 3.7017574310302734
Loss :  1.7390291690826416 2.258669853210449 3.997699022293091
Loss :  1.739797592163086 2.405503749847412 4.145301342010498
Loss :  1.7447717189788818 2.95649790763855 4.701269626617432
Loss :  1.7394570112228394 3.3010170459747314 5.040473937988281
Loss :  1.7347080707550049 1.8268824815750122 3.5615906715393066
Loss :  1.741756558418274 2.3955323696136475 4.137289047241211
Loss :  1.7342323064804077 1.8409903049468994 3.5752224922180176
Loss :  1.7401058673858643 2.1064159870147705 3.8465218544006348
Loss :  1.732926368713379 2.3668017387390137 4.099728107452393
Loss :  1.7453160285949707 2.3586809635162354 4.103997230529785
Loss :  1.7395110130310059 1.851462483406067 3.590973377227783
Loss :  1.7330751419067383 1.8287748098373413 3.561850070953369
Loss :  1.7331310510635376 2.6517844200134277 4.384915351867676
Loss :  1.7403634786605835 2.41983962059021 4.160202980041504
Loss :  1.7399156093597412 1.7832143306732178 3.523129940032959
Loss :  1.7371453046798706 2.440751791000366 4.177896976470947
Loss :  1.7336410284042358 2.2794435024261475 4.013084411621094
Loss :  1.7359609603881836 2.1126773357391357 3.8486382961273193
Loss :  1.7373626232147217 2.019946813583374 3.7573094367980957
  batch 40 loss: 1.7373626232147217, 2.019946813583374, 3.7573094367980957
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7355214357376099 2.344273328781128 4.079794883728027
Loss :  1.7356257438659668 1.8869203329086304 3.6225461959838867
Loss :  1.7385112047195435 2.130385398864746 3.868896484375
Loss :  1.7367701530456543 2.05643630027771 3.7932064533233643
Loss :  1.739582896232605 2.1322383880615234 3.871821403503418
Loss :  1.7362760305404663 2.060577869415283 3.796854019165039
Loss :  1.7354708909988403 1.7848894596099854 3.5203604698181152
Loss :  1.737538456916809 2.1041460037231445 3.841684341430664
Loss :  1.737635612487793 2.4793646335601807 4.2170000076293945
Loss :  1.736588716506958 2.03316068649292 3.769749402999878
Loss :  1.732503890991211 2.2770397663116455 4.009543418884277
Loss :  1.7361502647399902 2.2947306632995605 4.030880928039551
Loss :  1.7402269840240479 2.054140567779541 3.794367551803589
Loss :  1.7348122596740723 2.228259325027466 3.963071584701538
Loss :  1.7361345291137695 2.3897809982299805 4.12591552734375
Loss :  1.7321484088897705 1.8367425203323364 3.5688910484313965
Loss :  1.737870454788208 2.0304253101348877 3.7682957649230957
Loss :  1.7403923273086548 1.9072240591049194 3.647616386413574
Loss :  1.74076509475708 2.2355523109436035 3.9763174057006836
Loss :  1.736862063407898 1.911485195159912 3.6483473777770996
  batch 60 loss: 1.736862063407898, 1.911485195159912, 3.6483473777770996
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7365995645523071 2.4526419639587402 4.189241409301758
Loss :  1.737980842590332 1.6688276529312134 3.406808376312256
Loss :  1.7380722761154175 1.853829026222229 3.5919013023376465
Loss :  1.733992099761963 2.3766634464263916 4.110655784606934
Loss :  1.7335081100463867 1.4159369468688965 3.149445056915283
Loss :  1.7917580604553223 3.871124744415283 5.6628828048706055
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.789717435836792 3.9306530952453613 5.720370292663574
Loss :  1.7900185585021973 3.9012861251831055 5.691304683685303
Loss :  1.7935928106307983 3.8203859329223633 5.613978862762451
Total LOSS train 3.9243749141693116 valid 5.672134160995483
CE LOSS train 1.7374082033450786 valid 0.4483982026576996
Contrastive LOSS train 2.186966697986309 valid 0.9550964832305908
EPOCH 229:
Loss :  1.7369146347045898 1.5748153924942017 3.311729907989502
Loss :  1.7349355220794678 2.200148344039917 3.9350838661193848
Loss :  1.7330601215362549 1.804983139038086 3.538043260574341
Loss :  1.73587167263031 2.6394567489624023 4.375328540802002
Loss :  1.7399790287017822 1.4570378065109253 3.197016716003418
Loss :  1.7367231845855713 1.4199793338775635 3.1567025184631348
Loss :  1.73849618434906 2.040764808654785 3.7792611122131348
Loss :  1.7351007461547852 2.1799776554107666 3.9150784015655518
Loss :  1.7364531755447388 1.5691152811050415 3.3055684566497803
Loss :  1.7263574600219727 1.60537850856781 3.3317360877990723
Loss :  1.7372548580169678 2.3823976516723633 4.11965274810791
Loss :  1.744182825088501 1.9323577880859375 3.6765406131744385
Loss :  1.7390552759170532 1.8951880931854248 3.6342434883117676
Loss :  1.735998272895813 1.6476460695266724 3.3836443424224854
Loss :  1.7401095628738403 2.0652215480804443 3.805331230163574
Loss :  1.7343592643737793 2.2877542972564697 4.022113800048828
Loss :  1.7383629083633423 2.0588552951812744 3.7972183227539062
Loss :  1.7367727756500244 2.2209625244140625 3.957735300064087
Loss :  1.7364622354507446 2.459825277328491 4.196287631988525
Loss :  1.7333368062973022 1.9034327268600464 3.6367695331573486
  batch 20 loss: 1.7333368062973022, 1.9034327268600464, 3.6367695331573486
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.735817313194275 1.3468888998031616 3.0827062129974365
Loss :  1.7367281913757324 1.4873521327972412 3.2240803241729736
Loss :  1.737754464149475 1.650240421295166 3.3879947662353516
Loss :  1.7421520948410034 1.6315670013427734 3.3737192153930664
Loss :  1.7377761602401733 1.8472118377685547 3.5849881172180176
Loss :  1.7329449653625488 2.317096471786499 4.050041198730469
Loss :  1.7409112453460693 1.8820204734802246 3.622931718826294
Loss :  1.732810378074646 2.8334920406341553 4.566302299499512
Loss :  1.7391663789749146 2.2714476585388184 4.010613918304443
Loss :  1.7314635515213013 2.4109628200531006 4.142426490783691
Loss :  1.7452952861785889 2.3380815982818604 4.083376884460449
Loss :  1.7396399974822998 2.6047675609588623 4.344407558441162
Loss :  1.733634114265442 1.9204288721084595 3.6540629863739014
Loss :  1.73348069190979 2.8744914531707764 4.607972145080566
Loss :  1.741409420967102 2.8750829696655273 4.61649227142334
Loss :  1.7411062717437744 3.1760177612304688 4.917123794555664
Loss :  1.7384577989578247 2.967275857925415 4.705733776092529
Loss :  1.734963297843933 2.4391889572143555 4.174152374267578
Loss :  1.737001657485962 3.0871307849884033 4.824132442474365
Loss :  1.7379523515701294 2.58105731010437 4.319009780883789
  batch 40 loss: 1.7379523515701294, 2.58105731010437, 4.319009780883789
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7366496324539185 3.4568209648132324 5.193470478057861
Loss :  1.7370092868804932 2.71366286277771 4.450672149658203
Loss :  1.7388068437576294 2.8909921646118164 4.629798889160156
Loss :  1.7380647659301758 1.9247380495071411 3.6628026962280273
Loss :  1.740402102470398 2.1876909732818604 3.9280929565429688
Loss :  1.7369130849838257 2.7471184730529785 4.484031677246094
Loss :  1.736664056777954 2.149017333984375 3.885681390762329
Loss :  1.7379742860794067 1.6745051145553589 3.4124794006347656
Loss :  1.7393144369125366 2.788963556289673 4.52827787399292
Loss :  1.7367486953735352 2.4310355186462402 4.167784214019775
Loss :  1.7328228950500488 1.9855786561965942 3.7184014320373535
Loss :  1.7366799116134644 1.821761965751648 3.5584418773651123
Loss :  1.7411483526229858 2.187777280807495 3.9289255142211914
Loss :  1.734614610671997 2.1517138481140137 3.8863284587860107
Loss :  1.7360832691192627 2.048419952392578 3.784503221511841
Loss :  1.7333937883377075 2.130053758621216 3.863447666168213
Loss :  1.7386620044708252 2.101555109024048 3.840217113494873
Loss :  1.741015076637268 1.8643932342529297 3.605408191680908
Loss :  1.7405784130096436 2.6035170555114746 4.344095230102539
Loss :  1.7373336553573608 2.721745014190674 4.459078788757324
  batch 60 loss: 1.7373336553573608, 2.721745014190674, 4.459078788757324
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.737113356590271 2.740184783935547 4.477298259735107
Loss :  1.7382452487945557 1.8479390144348145 3.58618426322937
Loss :  1.7393240928649902 3.197505474090576 4.936829566955566
Loss :  1.7335405349731445 2.4029541015625 4.1364946365356445
Loss :  1.7330085039138794 1.6359119415283203 3.36892032623291
Loss :  1.8090537786483765 4.053393363952637 5.862447261810303
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8057607412338257 3.911149024963379 5.716909885406494
Loss :  1.8064424991607666 3.7663424015045166 5.572784900665283
Loss :  1.809916377067566 3.722245693206787 5.532162189483643
Total LOSS train 3.957000314272367 valid 5.671076059341431
CE LOSS train 1.737113277728741 valid 0.4524790942668915
Contrastive LOSS train 2.219887036543626 valid 0.9305614233016968
EPOCH 230:
Loss :  1.7373965978622437 2.034714698791504 3.772111415863037
Loss :  1.7361477613449097 2.8392081260681152 4.5753560066223145
Loss :  1.7334007024765015 2.289494037628174 4.022894859313965
Loss :  1.7361736297607422 2.2331442832946777 3.96931791305542
Loss :  1.739623785018921 2.491964101791382 4.231587886810303
Loss :  1.7370572090148926 1.7139226198196411 3.450979709625244
Loss :  1.7376878261566162 2.0011675357818604 3.7388553619384766
Loss :  1.7347939014434814 2.0937788486480713 3.8285727500915527
Loss :  1.736162543296814 1.6584330797195435 3.3945956230163574
Loss :  1.7254328727722168 1.6301369667053223 3.355569839477539
Loss :  1.7365981340408325 2.6325628757476807 4.369161128997803
Loss :  1.7437400817871094 3.006080389022827 4.749820709228516
Loss :  1.738265037536621 1.7842055559158325 3.522470474243164
Loss :  1.7354854345321655 2.9960625171661377 4.731547832489014
Loss :  1.7396903038024902 2.8149566650390625 4.554646968841553
Loss :  1.7341253757476807 3.043567657470703 4.777692794799805
Loss :  1.7386162281036377 3.159600019454956 4.898216247558594
Loss :  1.7372221946716309 3.7066571712493896 5.443879127502441
Loss :  1.73616361618042 3.012122392654419 4.748286247253418
Loss :  1.7341845035552979 3.727757692337036 5.461942195892334
  batch 20 loss: 1.7341845035552979, 3.727757692337036, 5.461942195892334
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7364647388458252 3.32962703704834 5.066091537475586
Loss :  1.7374775409698486 2.404005289077759 4.141482830047607
Loss :  1.7388079166412354 2.4804623126983643 4.2192702293396
Loss :  1.743100881576538 2.828730821609497 4.571831703186035
Loss :  1.7375712394714355 2.077709913253784 3.8152811527252197
Loss :  1.7337790727615356 2.400023937225342 4.133802890777588
Loss :  1.7413781881332397 1.7036501169204712 3.445028305053711
Loss :  1.733604073524475 3.4567978382110596 5.190402030944824
Loss :  1.7397639751434326 2.2795257568359375 4.019289970397949
Loss :  1.7325785160064697 3.1198465824127197 4.8524250984191895
Loss :  1.7455086708068848 2.5502610206604004 4.295769691467285
Loss :  1.7398080825805664 2.3824429512023926 4.122251033782959
Loss :  1.7341550588607788 1.988155484199524 3.7223105430603027
Loss :  1.7333585023880005 1.8268077373504639 3.560166358947754
Loss :  1.740583896636963 2.3045709133148193 4.045154571533203
Loss :  1.7400550842285156 2.3109445571899414 4.050999641418457
Loss :  1.7370221614837646 2.546315908432007 4.2833380699157715
Loss :  1.7333167791366577 1.7703860998153687 3.5037028789520264
Loss :  1.7364012002944946 1.5940768718719482 3.3304781913757324
Loss :  1.736829161643982 1.7466294765472412 3.4834585189819336
  batch 40 loss: 1.736829161643982, 1.7466294765472412, 3.4834585189819336
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.735676646232605 2.1957650184631348 3.9314417839050293
Loss :  1.7359917163848877 1.7438863515853882 3.4798779487609863
Loss :  1.7382780313491821 1.9452680349349976 3.6835460662841797
Loss :  1.7368137836456299 2.295088052749634 4.031901836395264
Loss :  1.7393195629119873 1.7193266153335571 3.458646297454834
Loss :  1.7356972694396973 2.3834221363067627 4.119119644165039
Loss :  1.7346360683441162 2.3765084743499756 4.111144542694092
Loss :  1.7371255159378052 2.2062089443206787 3.9433345794677734
Loss :  1.7369431257247925 3.3326265811920166 5.0695695877075195
Loss :  1.736454963684082 2.181894540786743 3.918349504470825
Loss :  1.732549786567688 1.6997184753417969 3.4322681427001953
Loss :  1.7360917329788208 2.745426654815674 4.481518268585205
Loss :  1.7403647899627686 2.345478057861328 4.085843086242676
Loss :  1.734995722770691 2.379314422607422 4.114310264587402
Loss :  1.7366358041763306 1.9667582511901855 3.7033939361572266
Loss :  1.7325153350830078 1.5359481573104858 3.268463611602783
Loss :  1.7378761768341064 2.6434996128082275 4.381375789642334
Loss :  1.7403624057769775 2.4735360145568848 4.213898658752441
Loss :  1.7409169673919678 1.8771716356277466 3.618088722229004
Loss :  1.7375673055648804 1.8552099466323853 3.5927772521972656
  batch 60 loss: 1.7375673055648804, 1.8552099466323853, 3.5927772521972656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7370102405548096 1.8738807439804077 3.6108908653259277
Loss :  1.7387146949768066 1.7658212184906006 3.5045359134674072
Loss :  1.7389169931411743 1.9888910055160522 3.7278079986572266
Loss :  1.7343356609344482 2.406532049179077 4.140867710113525
Loss :  1.7338066101074219 1.5116673707962036 3.245473861694336
Loss :  1.805414080619812 4.1562819480896 5.961696147918701
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.8017362356185913 4.170536518096924 5.972272872924805
Loss :  1.8029696941375732 4.145318508148193 5.9482879638671875
Loss :  1.8062838315963745 3.9339661598205566 5.740250110626221
Total LOSS train 4.066438249441293 valid 5.9056267738342285
CE LOSS train 1.7369096829341009 valid 0.45157095789909363
Contrastive LOSS train 2.3295285573372473 valid 0.9834915399551392
EPOCH 231:
Loss :  1.7379601001739502 2.3528032302856445 4.090763092041016
Loss :  1.7366187572479248 1.961574673652649 3.6981935501098633
Loss :  1.73377525806427 1.8731499910354614 3.6069252490997314
Loss :  1.7367115020751953 1.949798345565796 3.686509847640991
Loss :  1.7399511337280273 2.308210849761963 4.04816198348999
Loss :  1.7377243041992188 1.4731485843658447 3.2108728885650635
Loss :  1.7386062145233154 2.3500514030456543 4.088657379150391
Loss :  1.7359628677368164 2.3304715156555176 4.066434383392334
Loss :  1.737135410308838 1.99045991897583 3.727595329284668
Loss :  1.726831078529358 2.4977223873138428 4.22455358505249
Loss :  1.7374908924102783 1.9578986167907715 3.69538950920105
Loss :  1.7441332340240479 2.4474539756774902 4.191587448120117
Loss :  1.7389702796936035 2.328941822052002 4.0679121017456055
Loss :  1.736055850982666 2.4785847663879395 4.2146406173706055
Loss :  1.7403395175933838 2.302738666534424 4.043078422546387
Loss :  1.7346928119659424 1.6830028295516968 3.4176955223083496
Loss :  1.7387017011642456 1.9432244300842285 3.6819262504577637
Loss :  1.7370256185531616 1.7636595964431763 3.500685214996338
Loss :  1.736639142036438 2.0393266677856445 3.775965690612793
Loss :  1.734046459197998 2.8653995990753174 4.5994462966918945
  batch 20 loss: 1.734046459197998, 2.8653995990753174, 4.5994462966918945
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7365620136260986 1.7236368656158447 3.4601988792419434
Loss :  1.737635612487793 2.031003475189209 3.768639087677002
Loss :  1.7389793395996094 2.329123020172119 4.0681023597717285
Loss :  1.7430963516235352 3.538281202316284 5.281377792358398
Loss :  1.7382644414901733 2.8498947620391846 4.588159084320068
Loss :  1.7340391874313354 1.735539197921753 3.469578266143799
Loss :  1.7414109706878662 1.7331981658935547 3.474609136581421
Loss :  1.7334938049316406 2.1347219944000244 3.868215799331665
Loss :  1.7398186922073364 2.1252145767211914 3.8650331497192383
Loss :  1.7326208353042603 3.495511770248413 5.228132724761963
Loss :  1.7458305358886719 2.8947415351867676 4.6405720710754395
Loss :  1.7403451204299927 2.4643049240112305 4.204649925231934
Loss :  1.7349802255630493 1.8987950086593628 3.633775234222412
Loss :  1.734729290008545 2.091409206390381 3.826138496398926
Loss :  1.7414381504058838 2.537276268005371 4.278714179992676
Loss :  1.7416777610778809 2.193122625350952 3.934800386428833
Loss :  1.7392199039459229 2.384993076324463 4.124213218688965
Loss :  1.7360206842422485 1.7781842947006226 3.514204978942871
Loss :  1.7377116680145264 2.6932780742645264 4.430989742279053
Loss :  1.7390769720077515 2.975193977355957 4.714271068572998
  batch 40 loss: 1.7390769720077515, 2.975193977355957, 4.714271068572998
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7365416288375854 2.906257390975952 4.642798900604248
Loss :  1.7357845306396484 1.4247825145721436 3.160567045211792
Loss :  1.737733006477356 1.900277853012085 3.6380109786987305
Loss :  1.7365350723266602 1.404876947402954 3.1414120197296143
Loss :  1.7395695447921753 1.7076889276504517 3.447258472442627
Loss :  1.7373034954071045 2.872981309890747 4.610284805297852
Loss :  1.7374845743179321 3.420584201812744 5.158068656921387
Loss :  1.7382277250289917 3.4088804721832275 5.14710807800293
Loss :  1.7409226894378662 3.820918083190918 5.561841011047363
Loss :  1.7371604442596436 3.29531192779541 5.032472610473633
Loss :  1.7344238758087158 3.2191059589385986 4.9535298347473145
Loss :  1.7383620738983154 3.3338160514831543 5.072177886962891
Loss :  1.7439805269241333 2.4066293239593506 4.150609970092773
Loss :  1.736086130142212 2.9326045513153076 4.6686906814575195
Loss :  1.7378044128417969 2.8699913024902344 4.607795715332031
Loss :  1.736659288406372 2.833728551864624 4.570387840270996
Loss :  1.74065363407135 2.8889238834381104 4.62957763671875
Loss :  1.743302345275879 2.7374536991119385 4.480755805969238
Loss :  1.7427955865859985 2.4604861736297607 4.203281879425049
Loss :  1.7400658130645752 2.589087963104248 4.329154014587402
  batch 60 loss: 1.7400658130645752, 2.589087963104248, 4.329154014587402
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7401041984558105 2.4243085384368896 4.164412498474121
Loss :  1.7413345575332642 2.55922269821167 4.3005571365356445
Loss :  1.7417840957641602 1.8812830448150635 3.6230671405792236
Loss :  1.737307071685791 2.052788496017456 3.790095567703247
Loss :  1.736411213874817 1.9894235134124756 3.725834846496582
Loss :  1.8066470623016357 4.309889316558838 6.1165361404418945
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.802399754524231 4.3689494132995605 6.171349048614502
Loss :  1.8040851354599 4.162475109100342 5.966560363769531
Loss :  1.8069963455200195 4.281449794769287 6.088446140289307
Total LOSS train 4.135709553498488 valid 6.085722923278809
CE LOSS train 1.7380717112467838 valid 0.4517490863800049
Contrastive LOSS train 2.397637834915748 valid 1.0703624486923218
EPOCH 232:
Loss :  1.7402390241622925 2.5292768478393555 4.2695159912109375
Loss :  1.7387582063674927 2.233203649520874 3.9719619750976562
Loss :  1.7355701923370361 1.661001443862915 3.396571636199951
Loss :  1.7386997938156128 1.8233451843261719 3.562045097351074
Loss :  1.7407050132751465 2.6891326904296875 4.429837703704834
Loss :  1.739149570465088 2.5997314453125 4.338881015777588
Loss :  1.738566279411316 2.3284494876861572 4.067015647888184
Loss :  1.7363908290863037 2.9251389503479004 4.661529541015625
Loss :  1.7374038696289062 3.2325491905212402 4.9699530601501465
Loss :  1.727336049079895 1.6557172536849976 3.3830533027648926
Loss :  1.7382407188415527 2.1735029220581055 3.911743640899658
Loss :  1.7456185817718506 2.7115797996520996 4.457198143005371
Loss :  1.7396260499954224 2.800017833709717 4.53964376449585
Loss :  1.7368674278259277 2.3635377883911133 4.100405216217041
Loss :  1.7412749528884888 2.3485732078552246 4.089848041534424
Loss :  1.735751986503601 2.659554958343506 4.3953070640563965
Loss :  1.7398875951766968 3.2799925804138184 5.019880294799805
Loss :  1.7385869026184082 3.206683397293091 4.945270538330078
Loss :  1.7381242513656616 2.810187339782715 4.548311710357666
Loss :  1.7355945110321045 2.8807339668273926 4.616328239440918
  batch 20 loss: 1.7355945110321045, 2.8807339668273926, 4.616328239440918
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7381691932678223 2.7846336364746094 4.522802829742432
Loss :  1.7393615245819092 2.263441801071167 4.002803325653076
Loss :  1.7410351037979126 2.4411826133728027 4.182217597961426
Loss :  1.7447589635849 2.998366594314575 4.7431254386901855
Loss :  1.7397863864898682 2.11771559715271 3.857501983642578
Loss :  1.7367192506790161 2.5814340114593506 4.318153381347656
Loss :  1.743796467781067 3.0676729679107666 4.811469554901123
Loss :  1.7365696430206299 2.3937056064605713 4.130275249481201
Loss :  1.7420309782028198 2.1758270263671875 3.917858123779297
Loss :  1.735115885734558 2.7796361446380615 4.51475191116333
Loss :  1.7467775344848633 2.2770256996154785 4.023803234100342
Loss :  1.7419254779815674 2.1981403827667236 3.940065860748291
Loss :  1.7360303401947021 2.053453207015991 3.7894835472106934
Loss :  1.7350322008132935 1.9824951887130737 3.717527389526367
Loss :  1.7418795824050903 2.189570426940918 3.9314498901367188
Loss :  1.7413647174835205 2.608834981918335 4.3501996994018555
Loss :  1.7387179136276245 2.6177895069122314 4.356507301330566
Loss :  1.7361910343170166 2.226900577545166 3.9630916118621826
Loss :  1.7376413345336914 2.158768892288208 3.8964102268218994
Loss :  1.7381951808929443 2.2397215366363525 3.977916717529297
  batch 40 loss: 1.7381951808929443, 2.2397215366363525, 3.977916717529297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7365798950195312 2.5116288661956787 4.248208999633789
Loss :  1.7357902526855469 2.577216625213623 4.31300687789917
Loss :  1.7379904985427856 2.765249729156494 4.50324010848999
Loss :  1.7372723817825317 2.0261776447296143 3.7634501457214355
Loss :  1.7397396564483643 1.8847535848617554 3.62449312210083
Loss :  1.7362301349639893 1.9017589092254639 3.637989044189453
Loss :  1.7363725900650024 1.862235426902771 3.5986080169677734
Loss :  1.7381062507629395 2.3478190898895264 4.085925102233887
Loss :  1.7393931150436401 2.4271762371063232 4.166569232940674
Loss :  1.7378789186477661 1.9484920501708984 3.686370849609375
Loss :  1.7338112592697144 2.3710010051727295 4.104812145233154
Loss :  1.7379409074783325 2.092982530593872 3.830923557281494
Loss :  1.7434054613113403 2.6415913105010986 4.3849968910217285
Loss :  1.735970139503479 2.295105218887329 4.031075477600098
Loss :  1.737812876701355 2.699211359024048 4.437024116516113
Loss :  1.7350990772247314 2.088740110397339 3.8238391876220703
Loss :  1.7404240369796753 2.3582241535186768 4.0986480712890625
Loss :  1.7423840761184692 2.2328598499298096 3.9752440452575684
Loss :  1.7424139976501465 2.5656824111938477 4.308096408843994
Loss :  1.7398419380187988 2.2125563621520996 3.9523983001708984
  batch 60 loss: 1.7398419380187988, 2.2125563621520996, 3.9523983001708984
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7400676012039185 2.2222540378570557 3.9623217582702637
Loss :  1.7408826351165771 2.326209783554077 4.067092418670654
Loss :  1.7415639162063599 1.9424471855163574 3.6840109825134277
Loss :  1.7361645698547363 2.2260804176330566 3.962244987487793
Loss :  1.735113501548767 1.4594874382019043 3.194601058959961
Loss :  1.8037418127059937 4.315112590789795 6.118854522705078
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8002983331680298 4.399072170257568 6.199370384216309
Loss :  1.8014415502548218 4.183427333831787 5.984869003295898
Loss :  1.8050601482391357 4.1697211265563965 5.974781036376953
Total LOSS train 4.124106267782358 valid 6.06946873664856
CE LOSS train 1.7386421570411095 valid 0.45126503705978394
Contrastive LOSS train 2.3854641180772047 valid 1.0424302816390991
EPOCH 233:
Loss :  1.7390844821929932 2.6048059463500977 4.343890190124512
Loss :  1.7378531694412231 2.9211888313293457 4.659041881561279
Loss :  1.7347220182418823 1.4652522802352905 3.199974298477173
Loss :  1.737186074256897 1.6399239301681519 3.377110004425049
Loss :  1.740556240081787 1.5402823686599731 3.2808384895324707
Loss :  1.7381469011306763 1.9542893171310425 3.6924362182617188
Loss :  1.7401789426803589 2.0459182262420654 3.7860970497131348
Loss :  1.7372468709945679 2.5339176654815674 4.271164417266846
Loss :  1.7384437322616577 2.004326105117798 3.742769718170166
Loss :  1.7295221090316772 1.8319286108016968 3.561450719833374
Loss :  1.7395761013031006 2.194204807281494 3.9337809085845947
Loss :  1.745462417602539 2.149059772491455 3.894522190093994
Loss :  1.7411519289016724 2.057551860809326 3.798703670501709
Loss :  1.7382844686508179 1.7194138765335083 3.457698345184326
Loss :  1.7423065900802612 1.7262015342712402 3.468508243560791
Loss :  1.7371422052383423 2.183159351348877 3.9203014373779297
Loss :  1.7413108348846436 2.237165927886963 3.9784767627716064
Loss :  1.7394427061080933 2.082798480987549 3.8222413063049316
Loss :  1.7389448881149292 1.878556728363037 3.617501735687256
Loss :  1.7369903326034546 2.090419054031372 3.827409267425537
  batch 20 loss: 1.7369903326034546, 2.090419054031372, 3.827409267425537
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7389116287231445 1.8697000741958618 3.608611583709717
Loss :  1.7400647401809692 2.1268529891967773 3.866917610168457
Loss :  1.7404859066009521 2.065279960632324 3.8057658672332764
Loss :  1.7449883222579956 2.440377950668335 4.185366153717041
Loss :  1.7397164106369019 2.43068790435791 4.170404434204102
Loss :  1.7356057167053223 3.018263578414917 4.75386905670166
Loss :  1.7428333759307861 2.4589853286743164 4.201818466186523
Loss :  1.7350878715515137 1.9598422050476074 3.694930076599121
Loss :  1.7406415939331055 2.4863531589508057 4.226994514465332
Loss :  1.7336063385009766 2.5378222465515137 4.27142858505249
Loss :  1.7462049722671509 1.6847131252288818 3.4309182167053223
Loss :  1.7409393787384033 1.797629714012146 3.5385689735412598
Loss :  1.7350544929504395 2.1206488609313965 3.855703353881836
Loss :  1.734799861907959 2.0130414962768555 3.7478413581848145
Loss :  1.741745114326477 2.040839910507202 3.7825851440429688
Loss :  1.7415845394134521 1.665310025215149 3.4068946838378906
Loss :  1.739416480064392 1.5934085845947266 3.332825183868408
Loss :  1.7367092370986938 2.2941057682037354 4.030815124511719
Loss :  1.738596796989441 2.0926878452301025 3.831284523010254
Loss :  1.7397255897521973 1.8788849115371704 3.618610382080078
  batch 40 loss: 1.7397255897521973, 1.8788849115371704, 3.618610382080078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7385061979293823 2.282208204269409 4.020714282989502
Loss :  1.738688588142395 1.8128713369369507 3.5515599250793457
Loss :  1.7397913932800293 2.0631918907165527 3.802983283996582
Loss :  1.7402812242507935 2.0002455711364746 3.7405266761779785
Loss :  1.7418218851089478 1.7497378587722778 3.4915597438812256
Loss :  1.738125205039978 2.8827719688415527 4.62089729309082
Loss :  1.739143967628479 1.7646312713623047 3.503775119781494
Loss :  1.7396008968353271 1.7927417755126953 3.5323426723480225
Loss :  1.740435004234314 2.209352731704712 3.9497876167297363
Loss :  1.7385720014572144 2.2411489486694336 3.9797210693359375
Loss :  1.734536051750183 2.190323829650879 3.9248600006103516
Loss :  1.7383008003234863 2.1829211711883545 3.921221971511841
Loss :  1.7434937953948975 2.0386292934417725 3.78212308883667
Loss :  1.7370076179504395 2.0396106243133545 3.776618242263794
Loss :  1.7388584613800049 1.616849422454834 3.355707883834839
Loss :  1.7368203401565552 1.378103494644165 3.1149239540100098
Loss :  1.7397104501724243 2.007805347442627 3.7475156784057617
Loss :  1.7422504425048828 2.296142816543579 4.038393020629883
Loss :  1.7421512603759766 2.5554254055023193 4.297576904296875
Loss :  1.739312767982483 2.000903367996216 3.7402162551879883
  batch 60 loss: 1.739312767982483, 2.000903367996216, 3.7402162551879883
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.739122986793518 1.8338550329208374 3.5729780197143555
Loss :  1.740989327430725 1.7581766843795776 3.4991660118103027
Loss :  1.740779161453247 1.5619271993637085 3.302706241607666
Loss :  1.7357914447784424 2.374173164367676 4.109964370727539
Loss :  1.7349926233291626 2.3473494052886963 4.082342147827148
Loss :  1.783626675605774 4.057441234588623 5.841067790985107
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7816075086593628 4.125396251678467 5.907003879547119
Loss :  1.7821277379989624 3.995894432067871 5.778022289276123
Loss :  1.7863904237747192 4.148688316345215 5.9350786209106445
Total LOSS train 3.807019255711482 valid 5.8652931451797485
CE LOSS train 1.7390670042771559 valid 0.4465976059436798
Contrastive LOSS train 2.067952278944162 valid 1.0371720790863037
EPOCH 234:
Loss :  1.7401671409606934 3.314396619796753 5.054563522338867
Loss :  1.7380183935165405 2.3939156532287598 4.13193416595459
Loss :  1.7352937459945679 2.722740650177002 4.458034515380859
Loss :  1.7378910779953003 1.9797120094299316 3.7176032066345215
Loss :  1.740144968032837 2.0005929470062256 3.7407379150390625
Loss :  1.7401809692382812 2.1496360301971436 3.889816999435425
Loss :  1.7391396760940552 3.0008862018585205 4.740025997161865
Loss :  1.7371327877044678 2.404357433319092 4.1414899826049805
Loss :  1.7383509874343872 2.478281259536743 4.21663236618042
Loss :  1.7276525497436523 2.334026575088501 4.061678886413574
Loss :  1.7381750345230103 1.7461826801300049 3.4843578338623047
Loss :  1.7461204528808594 2.027123212814331 3.7732436656951904
Loss :  1.7390902042388916 2.027330160140991 3.766420364379883
Loss :  1.736836314201355 2.1870884895324707 3.9239249229431152
Loss :  1.7415882349014282 2.5836760997772217 4.3252644538879395
Loss :  1.7355821132659912 2.593327045440674 4.328908920288086
Loss :  1.739621639251709 2.4232404232025146 4.1628618240356445
Loss :  1.7369633913040161 2.0601911544799805 3.797154426574707
Loss :  1.736789584159851 2.102750539779663 3.8395400047302246
Loss :  1.7350547313690186 2.2107462882995605 3.945801019668579
  batch 20 loss: 1.7350547313690186, 2.2107462882995605, 3.945801019668579
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7370481491088867 2.844486951828003 4.581535339355469
Loss :  1.7381402254104614 2.201641321182251 3.939781665802002
Loss :  1.7393665313720703 2.6239633560180664 4.363329887390137
Loss :  1.743711233139038 2.609238624572754 4.352950096130371
Loss :  1.7387182712554932 2.6360106468200684 4.374729156494141
Loss :  1.735016942024231 2.8499755859375 4.584992408752441
Loss :  1.742543339729309 1.8641005754470825 3.6066439151763916
Loss :  1.7349764108657837 2.504300832748413 4.239277362823486
Loss :  1.7404742240905762 2.537226676940918 4.277700901031494
Loss :  1.7333528995513916 1.9329575300216675 3.6663103103637695
Loss :  1.746011734008789 1.9747627973556519 3.7207746505737305
Loss :  1.7404875755310059 2.0167415142059326 3.7572290897369385
Loss :  1.7350066900253296 2.2297630310058594 3.9647698402404785
Loss :  1.7345322370529175 2.3016018867492676 4.036134243011475
Loss :  1.741195559501648 2.7671914100646973 4.508387088775635
Loss :  1.7411060333251953 2.9892847537994385 4.730390548706055
Loss :  1.738651990890503 2.2559568881988525 3.9946088790893555
Loss :  1.7351020574569702 1.578818917274475 3.3139209747314453
Loss :  1.737675428390503 2.8081676959991455 4.545843124389648
Loss :  1.7384474277496338 2.016052007675171 3.7544994354248047
  batch 40 loss: 1.7384474277496338, 2.016052007675171, 3.7544994354248047
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7367291450500488 2.543142795562744 4.279871940612793
Loss :  1.7357927560806274 2.3648250102996826 4.1006178855896
Loss :  1.7369884252548218 2.263979911804199 4.0009684562683105
Loss :  1.736575722694397 2.4588842391967773 4.195459842681885
Loss :  1.7388595342636108 2.4368410110473633 4.175700664520264
Loss :  1.7351535558700562 1.730427622795105 3.465581178665161
Loss :  1.7363804578781128 3.2152023315429688 4.951582908630371
Loss :  1.7365835905075073 2.1567442417144775 3.8933277130126953
Loss :  1.7383466958999634 3.1408398151397705 4.879186630249023
Loss :  1.7353371381759644 1.9871479272842407 3.722485065460205
Loss :  1.7324990034103394 2.3838436603546143 4.116342544555664
Loss :  1.737187147140503 2.6446518898010254 4.381838798522949
Loss :  1.743082046508789 2.1956000328063965 3.9386820793151855
Loss :  1.7349282503128052 2.269620656967163 4.004549026489258
Loss :  1.7369736433029175 3.1809890270233154 4.917962551116943
Loss :  1.7371848821640015 2.6743721961975098 4.411557197570801
Loss :  1.740751028060913 2.6751620769500732 4.415913105010986
Loss :  1.7424559593200684 2.5765979290008545 4.319053649902344
Loss :  1.7425609827041626 2.3593478202819824 4.1019086837768555
Loss :  1.7411396503448486 2.6480154991149902 4.389155387878418
  batch 60 loss: 1.7411396503448486, 2.6480154991149902, 4.389155387878418
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7410688400268555 2.4937870502471924 4.234855651855469
Loss :  1.7414568662643433 1.9539873600006104 3.695444107055664
Loss :  1.7431185245513916 2.1890275478363037 3.9321460723876953
Loss :  1.7367980480194092 2.5540390014648438 4.290837287902832
Loss :  1.735561490058899 1.6580291986465454 3.3935906887054443
Loss :  1.8106580972671509 4.332142353057861 6.142800331115723
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8065205812454224 4.25498628616333 6.061506748199463
Loss :  1.807843565940857 4.141660690307617 5.949504375457764
Loss :  1.8110085725784302 4.0894951820373535 5.900503635406494
Total LOSS train 4.123421892753014 valid 6.013578772544861
CE LOSS train 1.738228805248554 valid 0.45275214314460754
Contrastive LOSS train 2.385193082002493 valid 1.0223737955093384
EPOCH 235:
Loss :  1.7394717931747437 2.1614720821380615 3.9009437561035156
Loss :  1.7398546934127808 2.596984624862671 4.336839199066162
Loss :  1.7354176044464111 2.1422548294067383 3.8776724338531494
Loss :  1.7370504140853882 2.474822521209717 4.2118730545043945
Loss :  1.740861177444458 1.8650439977645874 3.605905055999756
Loss :  1.7381759881973267 2.815274953842163 4.553451061248779
Loss :  1.7398430109024048 2.240347385406494 3.9801902770996094
Loss :  1.736485481262207 2.1777822971343994 3.9142677783966064
Loss :  1.7369476556777954 2.6155662536621094 4.352513790130615
Loss :  1.726595163345337 2.589134693145752 4.315730094909668
Loss :  1.737136721611023 2.2527546882629395 3.989891529083252
Loss :  1.7439513206481934 2.096107244491577 3.8400585651397705
Loss :  1.7383679151535034 2.319617748260498 4.057985782623291
Loss :  1.736289620399475 1.9318821430206299 3.6681718826293945
Loss :  1.7419025897979736 1.7305986881256104 3.472501277923584
Loss :  1.7363592386245728 2.3573427200317383 4.0937018394470215
Loss :  1.74104642868042 2.032641887664795 3.773688316345215
Loss :  1.7407110929489136 1.9768929481506348 3.717604160308838
Loss :  1.7383482456207275 1.741723895072937 3.480072021484375
Loss :  1.7395390272140503 1.8957645893096924 3.635303497314453
  batch 20 loss: 1.7395390272140503, 1.8957645893096924, 3.635303497314453
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.741655707359314 1.7765920162200928 3.518247604370117
Loss :  1.7416489124298096 2.554513692855835 4.2961626052856445
Loss :  1.7452332973480225 2.2053275108337402 3.9505608081817627
Loss :  1.7466117143630981 2.825430154800415 4.572041988372803
Loss :  1.7412090301513672 2.2406907081604004 3.9818997383117676
Loss :  1.73944091796875 2.2163031101226807 3.9557440280914307
Loss :  1.745849847793579 2.388533115386963 4.134383201599121
Loss :  1.739705204963684 2.469085216522217 4.208790302276611
Loss :  1.7442034482955933 2.06113600730896 3.8053393363952637
Loss :  1.7352343797683716 2.0771031379699707 3.8123373985290527
Loss :  1.746890664100647 2.3086259365081787 4.055516719818115
Loss :  1.7430559396743774 2.152808666229248 3.895864486694336
Loss :  1.736900806427002 2.0021493434906006 3.7390501499176025
Loss :  1.7363238334655762 1.929880976676941 3.6662049293518066
Loss :  1.742611289024353 2.611384868621826 4.353996276855469
Loss :  1.7425355911254883 2.1464247703552246 3.888960361480713
Loss :  1.7412598133087158 2.058270215988159 3.799530029296875
Loss :  1.7412309646606445 2.4305317401885986 4.171762466430664
Loss :  1.7431620359420776 2.063758134841919 3.806920051574707
Loss :  1.7421220541000366 1.9105478525161743 3.652669906616211
  batch 40 loss: 1.7421220541000366, 1.9105478525161743, 3.652669906616211
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7427432537078857 2.127509832382202 3.870253086090088
Loss :  1.7431646585464478 1.78337562084198 3.5265402793884277
Loss :  1.7412675619125366 2.3019723892211914 4.043240070343018
Loss :  1.7441425323486328 2.335796356201172 4.079938888549805
Loss :  1.7440073490142822 2.0878522396087646 3.831859588623047
Loss :  1.739198923110962 2.342541217803955 4.081740379333496
Loss :  1.7425466775894165 2.1892735958099365 3.9318203926086426
Loss :  1.7415472269058228 2.0620217323303223 3.8035688400268555
Loss :  1.7419378757476807 2.3559792041778564 4.097917079925537
Loss :  1.7399381399154663 2.5320370197296143 4.271975040435791
Loss :  1.7356913089752197 3.495070219039917 5.230761528015137
Loss :  1.7392603158950806 4.173983097076416 5.913243293762207
Loss :  1.7447664737701416 3.1203536987304688 4.865119934082031
Loss :  1.7365511655807495 3.2998008728027344 5.036352157592773
Loss :  1.7376158237457275 3.1218345165252686 4.859450340270996
Loss :  1.7372139692306519 2.639308214187622 4.376522064208984
Loss :  1.7396804094314575 2.381042003631592 4.12072229385376
Loss :  1.741707444190979 2.1795220375061035 3.921229362487793
Loss :  1.741621732711792 2.444714069366455 4.186335563659668
Loss :  1.7395826578140259 2.2999253273010254 4.039507865905762
  batch 60 loss: 1.7395826578140259, 2.2999253273010254, 4.039507865905762
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7396140098571777 2.9852967262268066 4.724910736083984
Loss :  1.7404797077178955 2.8089113235473633 4.54939079284668
Loss :  1.7411431074142456 2.3851752281188965 4.126318454742432
Loss :  1.7357977628707886 2.6806161403656006 4.4164137840271
Loss :  1.7347568273544312 1.9027220010757446 3.637478828430176
Loss :  1.79446542263031 3.9708731174468994 5.76533842086792
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7920271158218384 3.983457088470459 5.775484085083008
Loss :  1.7920717000961304 3.913086414337158 5.705158233642578
Loss :  1.798712134361267 3.799736499786377 5.598448753356934
Total LOSS train 4.0859532062823956 valid 5.71110737323761
CE LOSS train 1.7401110392350416 valid 0.4496780335903168
Contrastive LOSS train 2.3458421853872444 valid 0.9499341249465942
EPOCH 236:
Loss :  1.7384358644485474 2.116992235183716 3.8554282188415527
Loss :  1.7374478578567505 2.347198486328125 4.084646224975586
Loss :  1.7346227169036865 2.0396006107330322 3.7742233276367188
Loss :  1.7367957830429077 2.235063076019287 3.9718589782714844
Loss :  1.7404677867889404 2.4970037937164307 4.237471580505371
Loss :  1.738182544708252 2.44819712638855 4.186379432678223
Loss :  1.7396107912063599 3.5440101623535156 5.283620834350586
Loss :  1.7371070384979248 2.759399175643921 4.496506214141846
Loss :  1.7383396625518799 2.0792953968048096 3.8176350593566895
Loss :  1.728337049484253 2.348477363586426 4.076814651489258
Loss :  1.7387809753417969 2.1809568405151367 3.9197378158569336
Loss :  1.7451958656311035 2.4403276443481445 4.185523509979248
Loss :  1.7393736839294434 2.2904281616210938 4.029801845550537
Loss :  1.7369043827056885 2.086728096008301 3.8236324787139893
Loss :  1.7412970066070557 2.12455153465271 3.8658485412597656
Loss :  1.735197901725769 2.578549385070801 4.313747406005859
Loss :  1.7394517660140991 2.271763563156128 4.0112152099609375
Loss :  1.737101674079895 2.452434778213501 4.1895365715026855
Loss :  1.7367867231369019 2.6583569049835205 4.395143508911133
Loss :  1.7356491088867188 2.3338162899017334 4.069465637207031
  batch 20 loss: 1.7356491088867188, 2.3338162899017334, 4.069465637207031
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7378920316696167 2.4793012142181396 4.217193126678467
Loss :  1.7390141487121582 2.4915122985839844 4.230526447296143
Loss :  1.7403565645217896 2.1408050060272217 3.881161689758301
Loss :  1.7439926862716675 2.5478336811065674 4.291826248168945
Loss :  1.73904287815094 2.8016676902770996 4.54071044921875
Loss :  1.735268473625183 2.516096830368042 4.2513651847839355
Loss :  1.7422763109207153 1.7871763706207275 3.5294528007507324
Loss :  1.7342973947525024 2.0997331142425537 3.8340306282043457
Loss :  1.7399855852127075 1.9652422666549683 3.705227851867676
Loss :  1.7329875230789185 2.0716848373413086 3.8046722412109375
Loss :  1.7462027072906494 2.7446250915527344 4.490827560424805
Loss :  1.740921974182129 2.3369569778442383 4.077878952026367
Loss :  1.735221028327942 2.774580955505371 4.509801864624023
Loss :  1.735334873199463 2.779952049255371 4.515286922454834
Loss :  1.7420753240585327 2.506462812423706 4.248538017272949
Loss :  1.741904377937317 2.0601918697357178 3.802096366882324
Loss :  1.7394322156906128 2.193096876144409 3.9325289726257324
Loss :  1.736234426498413 2.230727434158325 3.9669618606567383
Loss :  1.7378439903259277 1.905061960220337 3.6429059505462646
Loss :  1.739591121673584 3.3519444465637207 5.091535568237305
  batch 40 loss: 1.739591121673584, 3.3519444465637207, 5.091535568237305
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7374576330184937 2.3463733196258545 4.083830833435059
Loss :  1.736820101737976 2.940845012664795 4.6776652336120605
Loss :  1.7385035753250122 2.534578800201416 4.273082256317139
Loss :  1.7378123998641968 2.195469617843628 3.933281898498535
Loss :  1.7400704622268677 2.4682602882385254 4.2083306312561035
Loss :  1.736807107925415 2.191977024078369 3.928784132003784
Loss :  1.7371439933776855 1.7397059202194214 3.4768500328063965
Loss :  1.7374322414398193 1.7739980220794678 3.511430263519287
Loss :  1.7391096353530884 2.97036075592041 4.709470272064209
Loss :  1.7363284826278687 2.4285073280334473 4.1648359298706055
Loss :  1.7333182096481323 1.8706302642822266 3.6039485931396484
Loss :  1.7370340824127197 2.624131202697754 4.3611650466918945
Loss :  1.741904377937317 1.6743850708007812 3.4162893295288086
Loss :  1.7353795766830444 2.72143816947937 4.456817626953125
Loss :  1.7367249727249146 2.3988983631134033 4.135623455047607
Loss :  1.734468698501587 2.627563238143921 4.362031936645508
Loss :  1.7391972541809082 2.1053881645202637 3.844585418701172
Loss :  1.742112159729004 1.9386203289031982 3.680732488632202
Loss :  1.7421377897262573 2.1779592037200928 3.9200968742370605
Loss :  1.7389298677444458 2.0037827491760254 3.7427124977111816
  batch 60 loss: 1.7389298677444458, 2.0037827491760254, 3.7427124977111816
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7393558025360107 2.727139472961426 4.466495513916016
Loss :  1.7405556440353394 2.4488182067871094 4.189373970031738
Loss :  1.7415348291397095 1.780608892440796 3.522143840789795
Loss :  1.7362703084945679 1.792099118232727 3.528369426727295
Loss :  1.735748291015625 2.1359994411468506 3.8717477321624756
Loss :  1.7794501781463623 4.169222831726074 5.948673248291016
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.777202844619751 4.1937737464904785 5.970976829528809
Loss :  1.7780144214630127 3.936298370361328 5.714312553405762
Loss :  1.7816545963287354 4.041782855987549 5.823437690734863
Total LOSS train 4.080345553618211 valid 5.864350080490112
CE LOSS train 1.7382633741085345 valid 0.44541364908218384
Contrastive LOSS train 2.3420821905136107 valid 1.0104457139968872
EPOCH 237:
Loss :  1.7396748065948486 1.855635166168213 3.5953099727630615
Loss :  1.7385627031326294 4.337485313415527 6.076047897338867
Loss :  1.737682819366455 3.6877288818359375 5.425411701202393
Loss :  1.7403843402862549 3.702406883239746 5.442790985107422
Loss :  1.7418153285980225 3.041264057159424 4.783079147338867
Loss :  1.7414360046386719 3.101417064666748 4.84285306930542
Loss :  1.7408521175384521 2.6043176651000977 4.345170021057129
Loss :  1.737484097480774 1.6704787015914917 3.4079627990722656
Loss :  1.7384849786758423 2.4120442867279053 4.150529384613037
Loss :  1.7291557788848877 1.8509467840194702 3.5801024436950684
Loss :  1.739417552947998 2.2074978351593018 3.9469153881073
Loss :  1.7456389665603638 2.138484239578247 3.8841233253479004
Loss :  1.7405390739440918 1.8119293451309204 3.5524682998657227
Loss :  1.7374109029769897 2.0850491523742676 3.822460174560547
Loss :  1.7419164180755615 1.865140676498413 3.6070570945739746
Loss :  1.7365037202835083 2.0202791690826416 3.7567830085754395
Loss :  1.7411539554595947 2.080202341079712 3.8213562965393066
Loss :  1.7395483255386353 1.9436652660369873 3.683213710784912
Loss :  1.7381809949874878 1.5436921119689941 3.2818732261657715
Loss :  1.737065315246582 2.0535073280334473 3.7905726432800293
  batch 20 loss: 1.737065315246582, 2.0535073280334473, 3.7905726432800293
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7384647130966187 1.7966480255126953 3.5351128578186035
Loss :  1.7394903898239136 2.262514352798462 4.002004623413086
Loss :  1.7403292655944824 2.029451608657837 3.7697808742523193
Loss :  1.7442545890808105 1.600541591644287 3.3447961807250977
Loss :  1.7393333911895752 2.6159868240356445 4.355319976806641
Loss :  1.735669732093811 2.8071742057800293 4.542843818664551
Loss :  1.7429347038269043 2.3876004219055176 4.130535125732422
Loss :  1.7353792190551758 2.213334083557129 3.9487133026123047
Loss :  1.740578055381775 3.1221723556518555 4.86275053024292
Loss :  1.7340959310531616 2.5832314491271973 4.317327499389648
Loss :  1.746010661125183 2.5455410480499268 4.29155158996582
Loss :  1.7402076721191406 2.6443583965301514 4.384566307067871
Loss :  1.7343345880508423 2.2439520359039307 3.9782867431640625
Loss :  1.7337912321090698 2.0031802654266357 3.736971378326416
Loss :  1.740923523902893 2.1952388286590576 3.9361624717712402
Loss :  1.7402355670928955 2.5039896965026855 4.24422550201416
Loss :  1.7377558946609497 2.648064613342285 4.385820388793945
Loss :  1.7336238622665405 2.286345958709717 4.019969940185547
Loss :  1.7361485958099365 2.1115994453430176 3.847748041152954
Loss :  1.7375510931015015 1.8256891965866089 3.5632402896881104
  batch 40 loss: 1.7375510931015015, 1.8256891965866089, 3.5632402896881104
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7359278202056885 2.3143160343170166 4.050243854522705
Loss :  1.733914852142334 1.6261426210403442 3.3600573539733887
Loss :  1.7373212575912476 2.208327531814575 3.945648670196533
Loss :  1.7351298332214355 2.2898330688476562 4.024962902069092
Loss :  1.7383424043655396 1.9884487390518188 3.7267911434173584
Loss :  1.735547423362732 2.3958141803741455 4.131361484527588
Loss :  1.7346059083938599 2.461916923522949 4.1965227127075195
Loss :  1.7363923788070679 1.6734082698822021 3.4098005294799805
Loss :  1.7370723485946655 1.6815185546875 3.418591022491455
Loss :  1.7364691495895386 1.9203238487243652 3.6567931175231934
Loss :  1.7334716320037842 2.011141538619995 3.7446131706237793
Loss :  1.7367427349090576 2.0118844509124756 3.748627185821533
Loss :  1.7410649061203003 1.9310481548309326 3.6721129417419434
Loss :  1.735630989074707 2.0290894508361816 3.7647204399108887
Loss :  1.7374088764190674 1.997573971748352 3.734982967376709
Loss :  1.7332006692886353 2.111060619354248 3.8442611694335938
Loss :  1.7389334440231323 2.1502480506896973 3.889181613922119
Loss :  1.7413036823272705 1.7973066568374634 3.5386104583740234
Loss :  1.7417492866516113 2.2048425674438477 3.946591854095459
Loss :  1.738041877746582 1.817609190940857 3.5556511878967285
  batch 60 loss: 1.738041877746582, 1.817609190940857, 3.5556511878967285
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7379186153411865 1.8907703161239624 3.6286888122558594
Loss :  1.7396721839904785 1.8126318454742432 3.5523040294647217
Loss :  1.739516019821167 1.9276963472366333 3.66721248626709
Loss :  1.7360299825668335 2.014669179916382 3.750699043273926
Loss :  1.7348576784133911 1.2334033250808716 2.9682610034942627
Loss :  1.7890928983688354 4.215091228485107 6.004184246063232
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.7864482402801514 4.202327251434326 5.988775253295898
Loss :  1.7873597145080566 4.102971076965332 5.890330791473389
Loss :  1.7912940979003906 3.9472360610961914 5.738530158996582
Total LOSS train 3.952632295168363 valid 5.905455112457275
CE LOSS train 1.7381582590249869 valid 0.44782352447509766
Contrastive LOSS train 2.214474032475398 valid 0.9868090152740479
EPOCH 238:
Loss :  1.738640308380127 1.9378448724746704 3.676485061645508
Loss :  1.736698031425476 2.130410671234131 3.8671088218688965
Loss :  1.7339929342269897 1.7091069221496582 3.4430999755859375
Loss :  1.7365576028823853 2.104253053665161 3.840810775756836
Loss :  1.740424633026123 1.5603042840957642 3.3007287979125977
Loss :  1.7369705438613892 1.736443042755127 3.4734134674072266
Loss :  1.7392245531082153 1.941829800605774 3.6810543537139893
Loss :  1.7350742816925049 1.8146337270736694 3.5497078895568848
Loss :  1.7368377447128296 1.656650185585022 3.3934879302978516
Loss :  1.7273374795913696 1.6097126007080078 3.337049961090088
Loss :  1.7368357181549072 1.8396726846694946 3.5765085220336914
Loss :  1.7445825338363647 1.7999804019927979 3.544562816619873
Loss :  1.7389320135116577 1.7582615613937378 3.4971935749053955
Loss :  1.736424207687378 2.1901962757110596 3.9266204833984375
Loss :  1.7402186393737793 2.080458164215088 3.820676803588867
Loss :  1.7355704307556152 2.1709916591644287 3.906562089920044
Loss :  1.7387808561325073 1.8372607231140137 3.5760416984558105
Loss :  1.738084316253662 1.8178634643554688 3.555947780609131
Loss :  1.7370128631591797 2.011835813522339 3.7488486766815186
Loss :  1.7347030639648438 1.8526395559310913 3.5873427391052246
  batch 20 loss: 1.7347030639648438, 1.8526395559310913, 3.5873427391052246
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.737358570098877 1.7185993194580078 3.4559578895568848
Loss :  1.7382702827453613 1.6773568391799927 3.4156270027160645
Loss :  1.7398571968078613 2.1893258094787598 3.929183006286621
Loss :  1.7435624599456787 2.0124025344848633 3.755964994430542
Loss :  1.7395362854003906 1.6967145204544067 3.436250686645508
Loss :  1.7351548671722412 1.8246667385101318 3.559821605682373
Loss :  1.7425415515899658 2.185887575149536 3.928429126739502
Loss :  1.734438180923462 2.1967885494232178 3.9312267303466797
Loss :  1.7404500246047974 1.7733033895492554 3.5137534141540527
Loss :  1.7329775094985962 2.0824711322784424 3.815448760986328
Loss :  1.7463089227676392 2.09173583984375 3.8380446434020996
Loss :  1.7403671741485596 2.047183036804199 3.787550210952759
Loss :  1.7342370748519897 1.4662456512451172 3.2004828453063965
Loss :  1.7341324090957642 1.7512221336364746 3.485354423522949
Loss :  1.7409363985061646 2.1900618076324463 3.9309983253479004
Loss :  1.740865707397461 1.8350398540496826 3.5759055614471436
Loss :  1.7386261224746704 2.042202949523926 3.7808289527893066
Loss :  1.7353595495224 1.8869383335113525 3.622297763824463
Loss :  1.7378337383270264 2.1080386638641357 3.845872402191162
Loss :  1.7395480871200562 1.9274784326553345 3.6670265197753906
  batch 40 loss: 1.7395480871200562, 1.9274784326553345, 3.6670265197753906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7375563383102417 2.2032580375671387 3.94081449508667
Loss :  1.7369099855422974 1.8439505100250244 3.5808606147766113
Loss :  1.7393935918807983 2.0914485454559326 3.8308420181274414
Loss :  1.7381529808044434 1.902427077293396 3.640580177307129
Loss :  1.7410132884979248 2.228088855743408 3.969102144241333
Loss :  1.7383723258972168 2.350353717803955 4.088726043701172
Loss :  1.7381881475448608 2.321155548095703 4.0593438148498535
Loss :  1.7390660047531128 1.9280602931976318 3.667126178741455
Loss :  1.740669846534729 2.185020685195923 3.9256906509399414
Loss :  1.7379018068313599 1.9295240640640259 3.6674258708953857
Loss :  1.7347276210784912 1.7185642719268799 3.453291893005371
Loss :  1.7384933233261108 2.0532357692718506 3.791728973388672
Loss :  1.7433068752288818 1.7770836353302002 3.520390510559082
Loss :  1.7367204427719116 1.7352125644683838 3.471932888031006
Loss :  1.7386795282363892 1.7360796928405762 3.474759101867676
Loss :  1.7364660501480103 2.2684834003448486 4.004949569702148
Loss :  1.7409274578094482 1.9810264110565186 3.721953868865967
Loss :  1.743298053741455 1.812675952911377 3.555974006652832
Loss :  1.7433587312698364 2.3866331577301025 4.1299920082092285
Loss :  1.7407509088516235 1.8348331451416016 3.5755839347839355
  batch 60 loss: 1.7407509088516235, 1.8348331451416016, 3.5755839347839355
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7412896156311035 1.7804383039474487 3.521728038787842
Loss :  1.742079257965088 1.923621416091919 3.665700674057007
Loss :  1.7427698373794556 2.06166934967041 3.804439067840576
Loss :  1.737789273262024 2.239902973175049 3.977692127227783
Loss :  1.736767053604126 1.8614120483398438 3.5981791019439697
Loss :  1.8058730363845825 4.141623020172119 5.947495937347412
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8018256425857544 4.066729545593262 5.868555068969727
Loss :  1.8035463094711304 3.906430959701538 5.709977149963379
Loss :  1.8066238164901733 3.793259382247925 5.599883079528809
Total LOSS train 3.6833551516899696 valid 5.7814778089523315
CE LOSS train 1.7384602033174954 valid 0.45165595412254333
Contrastive LOSS train 1.9448949538744413 valid 0.9483148455619812
EPOCH 239:
Loss :  1.740161657333374 1.5988757610321045 3.3390374183654785
Loss :  1.7391583919525146 1.9478161334991455 3.68697452545166
Loss :  1.7360178232192993 2.797128438949585 4.533146381378174
Loss :  1.738089680671692 1.977851390838623 3.7159409523010254
Loss :  1.7414770126342773 2.1830897331237793 3.9245667457580566
Loss :  1.7387429475784302 2.093109607696533 3.831852436065674
Loss :  1.7403651475906372 2.733219861984253 4.47358512878418
Loss :  1.737082600593567 2.9178123474121094 4.654894828796387
Loss :  1.738598346710205 3.5791664123535156 5.317764759063721
Loss :  1.7286792993545532 3.1540379524230957 4.882717132568359
Loss :  1.7384382486343384 2.678947687149048 4.417386054992676
Loss :  1.7452824115753174 2.4574315547943115 4.202713966369629
Loss :  1.7397416830062866 2.4662225246429443 4.205964088439941
Loss :  1.7369890213012695 2.499393939971924 4.236382961273193
Loss :  1.7403372526168823 2.932398557662964 4.672735691070557
Loss :  1.735547661781311 2.240668773651123 3.9762163162231445
Loss :  1.7395933866500854 3.2069246768951416 4.9465179443359375
Loss :  1.737830638885498 1.812371850013733 3.5502023696899414
Loss :  1.7377766370773315 1.9553850889205933 3.693161725997925
Loss :  1.7351696491241455 3.9953348636627197 5.730504512786865
  batch 20 loss: 1.7351696491241455, 3.9953348636627197, 5.730504512786865
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7370542287826538 1.8054322004318237 3.5424864292144775
Loss :  1.7390079498291016 2.7290260791778564 4.468033790588379
Loss :  1.7402986288070679 1.8651889562606812 3.605487585067749
Loss :  1.7444334030151367 1.7693188190460205 3.5137522220611572
Loss :  1.7402781248092651 1.9589828252792358 3.699260950088501
Loss :  1.7362182140350342 2.39292311668396 4.129141330718994
Loss :  1.743265151977539 2.0663769245147705 3.8096420764923096
Loss :  1.7354505062103271 1.7933322191238403 3.528782844543457
Loss :  1.741018295288086 2.0012972354888916 3.7423155307769775
Loss :  1.7339729070663452 2.250000476837158 3.983973503112793
Loss :  1.7465815544128418 2.245774745941162 3.992356300354004
Loss :  1.7412446737289429 1.7969505786895752 3.5381951332092285
Loss :  1.7351351976394653 1.7081984281539917 3.443333625793457
Loss :  1.7356443405151367 1.5207394361495972 3.2563838958740234
Loss :  1.7422608137130737 2.3871400356292725 4.129400730133057
Loss :  1.742505431175232 2.018425941467285 3.7609314918518066
Loss :  1.7396479845046997 1.9629576206207275 3.702605724334717
Loss :  1.735722303390503 1.8524510860443115 3.5881733894348145
Loss :  1.73775315284729 2.43237042427063 4.17012357711792
Loss :  1.739122986793518 2.488332986831665 4.227456092834473
  batch 40 loss: 1.739122986793518, 2.488332986831665, 4.227456092834473
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7368146181106567 2.749809980392456 4.486624717712402
Loss :  1.7364940643310547 2.6177282333374023 4.354222297668457
Loss :  1.7387168407440186 3.0881187915802 4.826835632324219
Loss :  1.7369359731674194 1.8648561239242554 3.601792097091675
Loss :  1.7400075197219849 1.535420298576355 3.27542781829834
Loss :  1.7371597290039062 1.86091148853302 3.5980710983276367
Loss :  1.7363359928131104 2.09421968460083 3.8305556774139404
Loss :  1.7383307218551636 1.9702924489974976 3.708623170852661
Loss :  1.7387421131134033 2.4368205070495605 4.175562858581543
Loss :  1.736977458000183 2.1156532764434814 3.852630615234375
Loss :  1.733359456062317 1.885796308517456 3.6191558837890625
Loss :  1.7375445365905762 1.733231782913208 3.470776319503784
Loss :  1.7418063879013062 1.5547868013381958 3.296593189239502
Loss :  1.7362761497497559 1.9470117092132568 3.6832878589630127
Loss :  1.7377345561981201 1.9853723049163818 3.723106861114502
Loss :  1.7347168922424316 1.9203907251358032 3.6551074981689453
Loss :  1.7401422262191772 2.6603317260742188 4.4004740715026855
Loss :  1.7424043416976929 2.0469517707824707 3.789356231689453
Loss :  1.743260383605957 2.3831183910369873 4.126379013061523
Loss :  1.739882469177246 1.7056390047073364 3.445521354675293
  batch 60 loss: 1.739882469177246, 1.7056390047073364, 3.445521354675293
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.74038827419281 2.2973525524139404 4.037740707397461
Loss :  1.741665005683899 2.5429632663726807 4.284628391265869
Loss :  1.7422990798950195 1.7255733013153076 3.467872381210327
Loss :  1.738900065422058 2.271434783935547 4.0103349685668945
Loss :  1.7378085851669312 1.3406541347503662 3.078462600708008
Loss :  1.7849338054656982 4.19577693939209 5.980710983276367
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7826613187789917 4.18954610824585 5.972207546234131
Loss :  1.7833354473114014 4.015920639038086 5.799256324768066
Loss :  1.7879267930984497 4.050034999847412 5.837961673736572
Total LOSS train 3.963434545810406 valid 5.897534132003784
CE LOSS train 1.7387138274999765 valid 0.4469816982746124
Contrastive LOSS train 2.2247207183104294 valid 1.012508749961853
EPOCH 240:
Loss :  1.741233468055725 1.8440309762954712 3.5852644443511963
Loss :  1.739919662475586 2.0336430072784424 3.7735626697540283
Loss :  1.737183928489685 1.9882832765579224 3.7254672050476074
Loss :  1.739373803138733 2.197057008743286 3.9364309310913086
Loss :  1.743416666984558 1.6918749809265137 3.4352917671203613
Loss :  1.7399886846542358 1.6231372356414795 3.363125801086426
Loss :  1.7426419258117676 2.1300313472747803 3.872673273086548
Loss :  1.738844871520996 2.023094892501831 3.761939764022827
Loss :  1.7399251461029053 2.0357656478881836 3.775690793991089
Loss :  1.7309015989303589 1.831498384475708 3.5623998641967773
Loss :  1.7398016452789307 2.2424378395080566 3.9822394847869873
Loss :  1.7456355094909668 2.227973222732544 3.9736087322235107
Loss :  1.7410032749176025 1.9633406400680542 3.704343795776367
Loss :  1.7379015684127808 1.8136640787124634 3.551565647125244
Loss :  1.7423824071884155 2.1120426654815674 3.8544249534606934
Loss :  1.7372381687164307 1.8008708953857422 3.538109064102173
Loss :  1.740845799446106 1.6816279888153076 3.422473907470703
Loss :  1.7397881746292114 1.8724225759506226 3.612210750579834
Loss :  1.7392858266830444 1.7567877769470215 3.4960737228393555
Loss :  1.737991213798523 1.981998324394226 3.719989538192749
  batch 20 loss: 1.737991213798523, 1.981998324394226, 3.719989538192749
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7397806644439697 1.8598512411117554 3.5996317863464355
Loss :  1.7410227060317993 1.7962857484817505 3.53730845451355
Loss :  1.7421261072158813 1.7450212240219116 3.487147331237793
Loss :  1.7457109689712524 1.7280619144439697 3.4737730026245117
Loss :  1.7406202554702759 2.0194828510284424 3.760103225708008
Loss :  1.7365623712539673 1.815402626991272 3.5519649982452393
Loss :  1.7432845830917358 1.5031434297561646 3.2464280128479004
Loss :  1.7351733446121216 1.478005051612854 3.2131783962249756
Loss :  1.74056077003479 1.6826845407485962 3.423245429992676
Loss :  1.7337663173675537 2.0990288257598877 3.8327951431274414
Loss :  1.7463754415512085 2.2326698303222656 3.9790453910827637
Loss :  1.7408013343811035 2.0173325538635254 3.758133888244629
Loss :  1.7355204820632935 2.0029430389404297 3.7384634017944336
Loss :  1.734878420829773 2.0167574882507324 3.751636028289795
Loss :  1.7427268028259277 2.6225881576538086 4.365314960479736
Loss :  1.7420133352279663 2.011653423309326 3.753666877746582
Loss :  1.739501953125 2.095154047012329 3.834656000137329
Loss :  1.7364542484283447 1.8927571773529053 3.62921142578125
Loss :  1.738086223602295 2.1202282905578613 3.8583145141601562
Loss :  1.7389559745788574 2.0503499507904053 3.7893059253692627
  batch 40 loss: 1.7389559745788574, 2.0503499507904053, 3.7893059253692627
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7377625703811646 1.9181324243545532 3.6558949947357178
Loss :  1.737280249595642 1.5406529903411865 3.277933120727539
Loss :  1.7392218112945557 1.9008939266204834 3.640115737915039
Loss :  1.7384802103042603 2.0176584720611572 3.756138801574707
Loss :  1.740776538848877 1.9572198390960693 3.6979963779449463
Loss :  1.737151026725769 1.6950230598449707 3.4321742057800293
Loss :  1.7374112606048584 1.788942575454712 3.5263538360595703
Loss :  1.7392256259918213 1.7089887857437134 3.448214530944824
Loss :  1.7405335903167725 1.83892023563385 3.579453945159912
Loss :  1.7386474609375 1.6852848529815674 3.4239323139190674
Loss :  1.7355468273162842 2.076385736465454 3.8119325637817383
Loss :  1.7394895553588867 1.9147089719772339 3.65419864654541
Loss :  1.7438186407089233 1.716871738433838 3.460690498352051
Loss :  1.7374743223190308 1.9854061603546143 3.7228803634643555
Loss :  1.7390406131744385 1.8389365673065186 3.577977180480957
Loss :  1.7368052005767822 2.106961727142334 3.843766927719116
Loss :  1.7414283752441406 2.0091090202331543 3.750537395477295
Loss :  1.743829607963562 1.577262043952942 3.321091651916504
Loss :  1.7438759803771973 1.7831529378890991 3.527029037475586
Loss :  1.7409138679504395 1.7981300354003906 3.53904390335083
  batch 60 loss: 1.7409138679504395, 1.7981300354003906, 3.53904390335083
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7410411834716797 2.0217444896698 3.7627856731414795
Loss :  1.7420772314071655 1.7997462749481201 3.541823387145996
Loss :  1.742534875869751 1.45759117603302 3.2001261711120605
Loss :  1.7378400564193726 2.1146934032440186 3.8525333404541016
Loss :  1.7363742589950562 1.512902021408081 3.2492761611938477
Loss :  1.784265160560608 4.017507076263428 5.801772117614746
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.7820011377334595 3.9690184593200684 5.751019477844238
Loss :  1.7827869653701782 3.8817574977874756 5.664544582366943
Loss :  1.7864130735397339 3.8417599201202393 5.628172874450684
Total LOSS train 3.6382170933943527 valid 5.711377263069153
CE LOSS train 1.7396585629536556 valid 0.44660326838493347
Contrastive LOSS train 1.8985585176027737 valid 0.9604399800300598
EPOCH 241:
Loss :  1.7406049966812134 1.6751130819320679 3.4157180786132812
Loss :  1.7394083738327026 1.8643258810043335 3.603734254837036
Loss :  1.7359508275985718 2.028290271759033 3.7642412185668945
Loss :  1.7386051416397095 1.9533699750900269 3.6919751167297363
Loss :  1.7413748502731323 1.688363790512085 3.4297385215759277
Loss :  1.7392410039901733 1.6543066501617432 3.393547534942627
Loss :  1.739822506904602 1.7006962299346924 3.440518856048584
Loss :  1.7368884086608887 2.0262866020202637 3.7631750106811523
Loss :  1.7381770610809326 1.9063323736190796 3.6445093154907227
Loss :  1.7281084060668945 1.8278164863586426 3.555924892425537
Loss :  1.7386870384216309 2.194068670272827 3.932755708694458
Loss :  1.7460519075393677 2.099107265472412 3.8451590538024902
Loss :  1.7403545379638672 1.984636664390564 3.7249913215637207
Loss :  1.7377592325210571 1.632587194442749 3.3703465461730957
Loss :  1.742450475692749 1.6580208539962769 3.4004712104797363
Loss :  1.7366963624954224 1.6848002672195435 3.421496629714966
Loss :  1.7409305572509766 1.7267323732376099 3.467662811279297
Loss :  1.7394061088562012 1.6090930700302124 3.348499298095703
Loss :  1.7396478652954102 1.7967991828918457 3.536447048187256
Loss :  1.7392669916152954 2.165062189102173 3.904329299926758
  batch 20 loss: 1.7392669916152954, 2.165062189102173, 3.904329299926758
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7416119575500488 1.8211067914962769 3.5627188682556152
Loss :  1.742244839668274 2.296679973602295 4.038924694061279
Loss :  1.7450460195541382 2.195939540863037 3.940985679626465
Loss :  1.7466448545455933 2.075521230697632 3.8221659660339355
Loss :  1.7423264980316162 2.3125345706939697 4.054861068725586
Loss :  1.7397795915603638 2.0407464504241943 3.7805261611938477
Loss :  1.7457857131958008 2.5606963634490967 4.306482315063477
Loss :  1.7388274669647217 2.1207807064056396 3.8596081733703613
Loss :  1.7445443868637085 2.1425716876983643 3.887115955352783
Loss :  1.736995816230774 3.0815086364746094 4.818504333496094
Loss :  1.7483536005020142 3.0751707553863525 4.823524475097656
Loss :  1.7437595129013062 2.0259511470794678 3.7697105407714844
Loss :  1.738036870956421 3.432495594024658 5.1705322265625
Loss :  1.7379484176635742 3.333012819290161 5.070960998535156
Loss :  1.7435849905014038 2.298967123031616 4.0425519943237305
Loss :  1.7435975074768066 1.9806877374649048 3.724285125732422
Loss :  1.7404593229293823 1.912930965423584 3.653390407562256
Loss :  1.7373768091201782 1.607972264289856 3.345349073410034
Loss :  1.7385808229446411 1.549381136894226 3.287961959838867
Loss :  1.7401084899902344 1.6183764934539795 3.358484983444214
  batch 40 loss: 1.7401084899902344, 1.6183764934539795, 3.358484983444214
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7381994724273682 2.1534583568573 3.891657829284668
Loss :  1.7378556728363037 1.9290060997009277 3.6668617725372314
Loss :  1.7402210235595703 2.1519863605499268 3.892207384109497
Loss :  1.7394005060195923 2.544095039367676 4.2834954261779785
Loss :  1.7418034076690674 2.768439292907715 4.510242462158203
Loss :  1.7381497621536255 2.8132758140563965 4.551425457000732
Loss :  1.73779296875 2.040517568588257 3.778310537338257
Loss :  1.7393295764923096 2.0413577556610107 3.7806873321533203
Loss :  1.7387855052947998 1.792690634727478 3.5314760208129883
Loss :  1.7382014989852905 1.9183546304702759 3.6565561294555664
Loss :  1.7338169813156128 2.4011194705963135 4.134936332702637
Loss :  1.7378391027450562 2.685256242752075 4.423095226287842
Loss :  1.7423344850540161 1.9456207752227783 3.687955379486084
Loss :  1.7368814945220947 2.1572840213775635 3.894165515899658
Loss :  1.7383698225021362 2.150550127029419 3.8889198303222656
Loss :  1.7358993291854858 2.5117909908294678 4.247690200805664
Loss :  1.7399171590805054 2.032665729522705 3.7725830078125
Loss :  1.7428810596466064 2.8335020542144775 4.576383113861084
Loss :  1.7431130409240723 2.558239221572876 4.301352500915527
Loss :  1.740363597869873 1.8120204210281372 3.5523838996887207
  batch 60 loss: 1.740363597869873, 1.8120204210281372, 3.5523838996887207
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7403037548065186 1.8201255798339844 3.560429334640503
Loss :  1.741593599319458 1.9010484218597412 3.642642021179199
Loss :  1.742557168006897 1.5375900268554688 3.280147075653076
Loss :  1.736966848373413 1.9403902292251587 3.6773571968078613
Loss :  1.7362035512924194 1.9283419847488403 3.6645455360412598
Loss :  1.7810078859329224 3.9926886558532715 5.773696422576904
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.7794568538665771 3.9774794578552246 5.756936073303223
Loss :  1.7789978981018066 3.727316379547119 5.506314277648926
Loss :  1.7843481302261353 3.9121313095092773 5.696479320526123
Total LOSS train 3.843406050021832 valid 5.683356523513794
CE LOSS train 1.739905023574829 valid 0.4460870325565338
Contrastive LOSS train 2.1035010447868934 valid 0.9780328273773193
EPOCH 242:
Loss :  1.7407256364822388 1.812532663345337 3.5532584190368652
Loss :  1.7399334907531738 2.2137835025787354 3.953716993331909
Loss :  1.7373288869857788 2.4215891361236572 4.1589179039001465
Loss :  1.740071415901184 1.700927734375 3.4409990310668945
Loss :  1.7425185441970825 2.1045289039611816 3.8470473289489746
Loss :  1.7408602237701416 2.5603532791137695 4.301213264465332
Loss :  1.7418644428253174 3.185108184814453 4.926972389221191
Loss :  1.7401111125946045 2.7039456367492676 4.444056510925293
Loss :  1.7408714294433594 2.018301248550415 3.7591726779937744
Loss :  1.7319614887237549 2.694692373275757 4.426653861999512
Loss :  1.7415626049041748 2.6311492919921875 4.372712135314941
Loss :  1.747864007949829 1.6587536334991455 3.4066176414489746
Loss :  1.7419346570968628 2.1005196571350098 3.842454433441162
Loss :  1.7397353649139404 1.9169539213180542 3.656689167022705
Loss :  1.744827151298523 1.7675374746322632 3.512364625930786
Loss :  1.7386565208435059 1.9511464834213257 3.689803123474121
Loss :  1.7425037622451782 1.78134024143219 3.523844003677368
Loss :  1.739502191543579 1.7598291635513306 3.499331474304199
Loss :  1.7405656576156616 1.9097236394882202 3.650289297103882
Loss :  1.7392950057983398 1.7039819955825806 3.443276882171631
  batch 20 loss: 1.7392950057983398, 1.7039819955825806, 3.443276882171631
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.74052894115448 1.9846832752227783 3.7252120971679688
Loss :  1.742048978805542 1.8705041408538818 3.612553119659424
Loss :  1.7427167892456055 1.8194104433059692 3.562127113342285
Loss :  1.746150255203247 1.9373804330825806 3.683530807495117
Loss :  1.7420434951782227 2.4197144508361816 4.161757946014404
Loss :  1.7385653257369995 1.827065348625183 3.5656306743621826
Loss :  1.744874358177185 1.9107674360275269 3.655641794204712
Loss :  1.7374342679977417 2.268301486968994 4.005735874176025
Loss :  1.7432801723480225 2.23309326171875 3.9763734340667725
Loss :  1.7372426986694336 2.5246150493621826 4.261857986450195
Loss :  1.7489243745803833 2.6550726890563965 4.40399694442749
Loss :  1.7431843280792236 2.4939639568328857 4.237148284912109
Loss :  1.7378627061843872 1.8179453611373901 3.5558080673217773
Loss :  1.7380421161651611 1.9135468006134033 3.6515889167785645
Loss :  1.7441496849060059 2.1853790283203125 3.9295287132263184
Loss :  1.744251012802124 1.883979320526123 3.628230333328247
Loss :  1.7414436340332031 2.3460631370544434 4.0875067710876465
Loss :  1.7377630472183228 3.622379779815674 5.360142707824707
Loss :  1.7404135465621948 1.754334568977356 3.494748115539551
Loss :  1.7417291402816772 1.6741883754730225 3.41591739654541
  batch 40 loss: 1.7417291402816772, 1.6741883754730225, 3.41591739654541
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7393758296966553 2.6028337478637695 4.342209815979004
Loss :  1.73996102809906 2.6015548706054688 4.341516017913818
Loss :  1.7417463064193726 2.6277339458465576 4.369480133056641
Loss :  1.7392438650131226 1.966858148574829 3.706101894378662
Loss :  1.7418501377105713 1.7787169218063354 3.520566940307617
Loss :  1.7391636371612549 2.6776437759399414 4.416807174682617
Loss :  1.738688588142395 2.075563669204712 3.8142523765563965
Loss :  1.7398914098739624 1.8349909782409668 3.5748825073242188
Loss :  1.7423193454742432 1.9139533042907715 3.6562726497650146
Loss :  1.7396775484085083 2.197542428970337 3.9372200965881348
Loss :  1.7376365661621094 2.565129518508911 4.302765846252441
Loss :  1.7410171031951904 2.3226101398468018 4.063627243041992
Loss :  1.7451841831207275 2.62984561920166 4.375029563903809
Loss :  1.739397644996643 2.0053913593292236 3.7447891235351562
Loss :  1.7410472631454468 2.1272222995758057 3.868269443511963
Loss :  1.7388428449630737 1.9401850700378418 3.679028034210205
Loss :  1.7433838844299316 2.0799784660339355 3.823362350463867
Loss :  1.7451132535934448 1.9355676174163818 3.680680751800537
Loss :  1.745565414428711 2.517991542816162 4.263556957244873
Loss :  1.7427483797073364 1.792563796043396 3.5353121757507324
  batch 60 loss: 1.7427483797073364, 1.792563796043396, 3.5353121757507324
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.742619276046753 2.0829262733459473 3.8255455493927
Loss :  1.743810772895813 2.322688579559326 4.06649923324585
Loss :  1.7434306144714355 2.2155113220214844 3.95894193649292
Loss :  1.7398489713668823 2.6503193378448486 4.390168190002441
Loss :  1.7385096549987793 1.3715782165527344 3.1100878715515137
Loss :  1.7823017835617065 3.958986520767212 5.741288185119629
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7804664373397827 3.8713185787200928 5.651784896850586
Loss :  1.7808946371078491 3.8191170692443848 5.600011825561523
Loss :  1.7846540212631226 3.7951791286468506 5.579833030700684
Total LOSS train 3.903867725225595 valid 5.6432294845581055
CE LOSS train 1.741160246042105 valid 0.44616350531578064
Contrastive LOSS train 2.1627074993573703 valid 0.9487947821617126
EPOCH 243:
Loss :  1.7415250539779663 2.2043685913085938 3.9458937644958496
Loss :  1.7402251958847046 2.3445353507995605 4.084760665893555
Loss :  1.7374387979507446 1.9657469987869263 3.703185796737671
Loss :  1.7394683361053467 2.5774195194244385 4.316887855529785
Loss :  1.7429096698760986 2.201361656188965 3.9442713260650635
Loss :  1.7397077083587646 1.8971595764160156 3.6368672847747803
Loss :  1.7413766384124756 1.8842664957046509 3.625643253326416
Loss :  1.7374168634414673 2.152712345123291 3.8901290893554688
Loss :  1.738561987876892 2.021777868270874 3.7603397369384766
Loss :  1.7295246124267578 1.7402721643447876 3.469796657562256
Loss :  1.7390586137771606 2.7501633167266846 4.489222049713135
Loss :  1.7463951110839844 1.8762954473495483 3.6226906776428223
Loss :  1.740820050239563 1.698652982711792 3.4394731521606445
Loss :  1.7382757663726807 1.7492351531982422 3.487510919570923
Loss :  1.7420971393585205 2.003246784210205 3.7453439235687256
Loss :  1.737565279006958 1.9866125583648682 3.724177837371826
Loss :  1.7412340641021729 2.220930814743042 3.962164878845215
Loss :  1.7399981021881104 1.9733645915985107 3.713362693786621
Loss :  1.7399715185165405 2.636608123779297 4.376579761505127
Loss :  1.7376763820648193 1.5876224040985107 3.32529878616333
  batch 20 loss: 1.7376763820648193, 1.5876224040985107, 3.32529878616333
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7394925355911255 1.863534927368164 3.60302734375
Loss :  1.7410025596618652 1.8753702640533447 3.61637282371521
Loss :  1.7417054176330566 2.1063907146453857 3.8480961322784424
Loss :  1.746090292930603 2.171628475189209 3.9177188873291016
Loss :  1.741776704788208 2.1200947761535645 3.8618714809417725
Loss :  1.7381423711776733 2.0862622261047363 3.824404716491699
Loss :  1.74488365650177 2.0106523036956787 3.7555360794067383
Loss :  1.737304449081421 1.9810220003128052 3.7183265686035156
Loss :  1.7428027391433716 1.9138109683990479 3.656613826751709
Loss :  1.7363255023956299 2.324632167816162 4.060957908630371
Loss :  1.7482150793075562 2.388252019882202 4.136466979980469
Loss :  1.7432169914245605 3.1078124046325684 4.851029396057129
Loss :  1.7379624843597412 2.9130330085754395 4.650995254516602
Loss :  1.7368968725204468 3.79154634475708 5.528443336486816
Loss :  1.7430943250656128 2.298961639404297 4.042056083679199
Loss :  1.742980718612671 2.8338775634765625 4.5768585205078125
Loss :  1.7405701875686646 2.0757479667663574 3.8163180351257324
Loss :  1.7382949590682983 1.9556268453598022 3.6939218044281006
Loss :  1.7396721839904785 1.7284419536590576 3.468114137649536
Loss :  1.7414562702178955 1.629477620124817 3.370934009552002
  batch 40 loss: 1.7414562702178955, 1.629477620124817, 3.370934009552002
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7394919395446777 1.770717740058899 3.510209560394287
Loss :  1.7381868362426758 1.4846889972686768 3.2228758335113525
Loss :  1.7407183647155762 1.7736953496932983 3.514413833618164
Loss :  1.7393938302993774 1.647066354751587 3.386460304260254
Loss :  1.7424694299697876 2.323528528213501 4.065998077392578
Loss :  1.740501880645752 2.8874595165252686 4.627961158752441
Loss :  1.7403870820999146 2.209350824356079 3.949738025665283
Loss :  1.7410334348678589 1.9394125938415527 3.680446147918701
Loss :  1.743518590927124 1.985080599784851 3.7285990715026855
Loss :  1.740211009979248 1.8212326765060425 3.56144380569458
Loss :  1.7377681732177734 2.0441439151763916 3.781912088394165
Loss :  1.7409642934799194 1.937746524810791 3.6787109375
Loss :  1.745423436164856 1.9404693841934204 3.6858928203582764
Loss :  1.739280104637146 2.425774574279785 4.165054798126221
Loss :  1.7412467002868652 2.455707550048828 4.196954250335693
Loss :  1.7389329671859741 2.342395067214966 4.08132791519165
Loss :  1.7427361011505127 1.9306830167770386 3.6734189987182617
Loss :  1.745383381843567 1.6648837327957153 3.4102671146392822
Loss :  1.7449758052825928 2.593838930130005 4.338814735412598
Loss :  1.7420748472213745 1.6215533018112183 3.3636281490325928
  batch 60 loss: 1.7420748472213745, 1.6215533018112183, 3.3636281490325928
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7420614957809448 1.9830857515335083 3.725147247314453
Loss :  1.7432879209518433 1.9108620882034302 3.6541500091552734
Loss :  1.74333655834198 1.6444456577301025 3.387782096862793
Loss :  1.7391881942749023 2.5767760276794434 4.315964221954346
Loss :  1.7383370399475098 1.61660635471344 3.35494327545166
Loss :  1.7842992544174194 3.9973199367523193 5.781619071960449
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.781959056854248 3.9747631549835205 5.756722450256348
Loss :  1.7827045917510986 3.9477131366729736 5.730417728424072
Loss :  1.7865777015686035 3.7661798000335693 5.552757263183594
Total LOSS train 3.851135044831496 valid 5.705379128456116
CE LOSS train 1.7406775786326483 valid 0.4466444253921509
Contrastive LOSS train 2.110457446024968 valid 0.9415449500083923
EPOCH 244:
Loss :  1.7420457601547241 1.8861267566680908 3.6281723976135254
Loss :  1.7399698495864868 2.3946480751037598 4.134617805480957
Loss :  1.738162636756897 2.213536024093628 3.9516987800598145
Loss :  1.740643858909607 1.4541831016540527 3.194827079772949
Loss :  1.743823528289795 1.4165290594100952 3.1603527069091797
Loss :  1.7410759925842285 1.5815808773040771 3.3226568698883057
Loss :  1.742929220199585 2.1396474838256836 3.8825767040252686
Loss :  1.73960542678833 1.9303590059280396 3.66996431350708
Loss :  1.7412155866622925 1.81303870677948 3.5542542934417725
Loss :  1.7324403524398804 2.310432195663452 4.042872428894043
Loss :  1.7414913177490234 2.3505818843841553 4.092073440551758
Loss :  1.748443841934204 2.4186034202575684 4.167047500610352
Loss :  1.7430920600891113 2.953129529953003 4.696221351623535
Loss :  1.7401446104049683 2.2133054733276367 3.9534502029418945
Loss :  1.7440513372421265 2.563082695007324 4.30713415145874
Loss :  1.7391891479492188 2.6758029460906982 4.414992332458496
Loss :  1.7431014776229858 2.63055419921875 4.373655796051025
Loss :  1.7413750886917114 3.0171689987182617 4.758543968200684
Loss :  1.7418290376663208 2.4311811923980713 4.173010349273682
Loss :  1.7393842935562134 2.2381622791290283 3.9775466918945312
  batch 20 loss: 1.7393842935562134, 2.2381622791290283, 3.9775466918945312
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7416640520095825 2.1980509757995605 3.9397149085998535
Loss :  1.7431719303131104 3.425091028213501 5.168262958526611
Loss :  1.744205355644226 2.3005380630493164 4.044743537902832
Loss :  1.7481229305267334 3.2203876972198486 4.968510627746582
Loss :  1.7433345317840576 2.7772486209869385 4.520583152770996
Loss :  1.7401845455169678 2.2550365924835205 3.9952211380004883
Loss :  1.7464348077774048 2.6595122814178467 4.405947208404541
Loss :  1.7394254207611084 1.8468329906463623 3.5862584114074707
Loss :  1.7445549964904785 2.0441370010375977 3.788691997528076
Loss :  1.739030361175537 2.7183890342712402 4.457419395446777
Loss :  1.7491438388824463 2.620652675628662 4.3697967529296875
Loss :  1.7439547777175903 3.1772279739379883 4.921182632446289
Loss :  1.7389211654663086 1.9092671871185303 3.648188352584839
Loss :  1.7380030155181885 1.6521025896072388 3.390105724334717
Loss :  1.7448997497558594 1.9558658599853516 3.700765609741211
Loss :  1.7447746992111206 1.830771803855896 3.5755465030670166
Loss :  1.7425950765609741 1.845152735710144 3.587747812271118
Loss :  1.740224838256836 1.6561435461044312 3.3963685035705566
Loss :  1.7421241998672485 1.997572422027588 3.739696502685547
Loss :  1.743088960647583 1.9268627166748047 3.6699516773223877
  batch 40 loss: 1.743088960647583, 1.9268627166748047, 3.6699516773223877
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7420833110809326 2.2649762630462646 4.007059574127197
Loss :  1.742334008216858 1.8526229858398438 3.594956874847412
Loss :  1.7440851926803589 2.1326024532318115 3.876687526702881
Loss :  1.7431405782699585 1.9893685579299927 3.732509136199951
Loss :  1.7455201148986816 1.8865959644317627 3.6321160793304443
Loss :  1.7430593967437744 2.3894248008728027 4.132484436035156
Loss :  1.7433247566223145 2.2610228061676025 4.004347801208496
Loss :  1.7436130046844482 1.890808343887329 3.6344213485717773
Loss :  1.7455943822860718 2.682755947113037 4.428350448608398
Loss :  1.7422082424163818 2.72047758102417 4.462685585021973
Loss :  1.739050269126892 2.5579919815063477 4.297042369842529
Loss :  1.7419862747192383 2.2224295139312744 3.9644157886505127
Loss :  1.74651038646698 1.8510921001434326 3.597602367401123
Loss :  1.7397277355194092 2.3344717025756836 4.074199676513672
Loss :  1.7408968210220337 2.1144680976867676 3.8553647994995117
Loss :  1.7402514219284058 1.932820439338684 3.67307186126709
Loss :  1.7427489757537842 1.9506663084030151 3.6934151649475098
Loss :  1.7450568675994873 1.8295120000839233 3.574568748474121
Loss :  1.744736909866333 2.4233202934265137 4.168057441711426
Loss :  1.7425254583358765 1.900241732597351 3.6427671909332275
  batch 60 loss: 1.7425254583358765, 1.900241732597351, 3.6427671909332275
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.742273211479187 2.008594512939453 3.7508678436279297
Loss :  1.743465542793274 2.4048473834991455 4.148313045501709
Loss :  1.743965983390808 1.8022289276123047 3.5461950302124023
Loss :  1.7384555339813232 2.152048349380493 3.8905038833618164
Loss :  1.7370951175689697 1.6730632781982422 3.410158395767212
Loss :  1.7883632183074951 3.9140665531158447 5.70242977142334
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7856649160385132 3.913116931915283 5.698781967163086
Loss :  1.7863256931304932 3.8415491580963135 5.627874851226807
Loss :  1.7906666994094849 3.790351629257202 5.581018447875977
Total LOSS train 3.955731307543241 valid 5.652526259422302
CE LOSS train 1.7422397411786592 valid 0.4476666748523712
Contrastive LOSS train 2.2134915388547456 valid 0.9475879073143005
EPOCH 245:
Loss :  1.741764783859253 1.8375722169876099 3.5793371200561523
Loss :  1.740540623664856 2.615359306335449 4.355899810791016
Loss :  1.7374814748764038 2.2640998363494873 4.001581192016602
Loss :  1.7404266595840454 2.608360528945923 4.348787307739258
Loss :  1.7422351837158203 2.440744161605835 4.182979583740234
Loss :  1.7414475679397583 2.623213529586792 4.36466121673584
Loss :  1.7408946752548218 2.3909382820129395 4.131833076477051
Loss :  1.7388955354690552 1.7038248777389526 3.442720413208008
Loss :  1.7399970293045044 1.9645209312438965 3.7045178413391113
Loss :  1.7300083637237549 2.2226614952087402 3.952669858932495
Loss :  1.7409560680389404 2.3929710388183594 4.133927345275879
Loss :  1.7479255199432373 2.10727596282959 3.855201482772827
Loss :  1.7421194314956665 2.2385072708129883 3.9806265830993652
Loss :  1.7392317056655884 2.064237117767334 3.803468704223633
Loss :  1.7433688640594482 2.819744110107422 4.563113212585449
Loss :  1.7376742362976074 2.1826441287994385 3.920318365097046
Loss :  1.7419440746307373 1.9752566814422607 3.717200756072998
Loss :  1.7398598194122314 2.6690642833709717 4.408924102783203
Loss :  1.7398749589920044 1.4780898094177246 3.2179646492004395
Loss :  1.7381491661071777 1.7870734930038452 3.5252227783203125
  batch 20 loss: 1.7381491661071777, 1.7870734930038452, 3.5252227783203125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7406500577926636 1.923531413078308 3.6641814708709717
Loss :  1.742121696472168 2.1286351680755615 3.8707568645477295
Loss :  1.7435671091079712 2.1588406562805176 3.902407646179199
Loss :  1.7470892667770386 2.7377097606658936 4.484798908233643
Loss :  1.74233877658844 2.7014691829681396 4.443808078765869
Loss :  1.7401795387268066 2.1678199768066406 3.9079995155334473
Loss :  1.7466942071914673 2.2479026317596436 3.9945969581604004
Loss :  1.740286111831665 2.3135998249053955 4.0538859367370605
Loss :  1.7452573776245117 3.0452282428741455 4.790485382080078
Loss :  1.7390098571777344 2.0542006492614746 3.793210506439209
Loss :  1.7497731447219849 2.376124858856201 4.1258978843688965
Loss :  1.7450900077819824 2.5194473266601562 4.264537334442139
Loss :  1.7401005029678345 2.454519748687744 4.194620132446289
Loss :  1.739781141281128 1.9804567098617554 3.7202377319335938
Loss :  1.7453950643539429 2.690643310546875 4.436038494110107
Loss :  1.745491623878479 2.267306089401245 4.012797832489014
Loss :  1.7428442239761353 2.6701111793518066 4.412955284118652
Loss :  1.7397712469100952 2.047985315322876 3.7877564430236816
Loss :  1.7409939765930176 1.999066948890686 3.740060806274414
Loss :  1.742177963256836 1.9019436836242676 3.6441216468811035
  batch 40 loss: 1.742177963256836, 1.9019436836242676, 3.6441216468811035
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7399301528930664 2.9280288219451904 4.667959213256836
Loss :  1.7393296957015991 2.2405660152435303 3.97989559173584
Loss :  1.7410956621170044 1.6784042119979858 3.4194998741149902
Loss :  1.7397102117538452 1.70513117313385 3.4448413848876953
Loss :  1.7426427602767944 3.467862606048584 5.210505485534668
Loss :  1.740145206451416 2.566962718963623 4.307107925415039
Loss :  1.7402162551879883 2.1703684329986572 3.9105846881866455
Loss :  1.7406270503997803 2.580219268798828 4.3208465576171875
Loss :  1.7422822713851929 2.5468835830688477 4.28916597366333
Loss :  1.7397090196609497 1.9601935148239136 3.6999025344848633
Loss :  1.7369126081466675 2.2896275520324707 4.026540279388428
Loss :  1.7402228116989136 2.151089668273926 3.891312599182129
Loss :  1.7447962760925293 2.385075569152832 4.129871845245361
Loss :  1.7387324571609497 2.0931551456451416 3.831887722015381
Loss :  1.740354061126709 2.2042582035064697 3.9446122646331787
Loss :  1.7380343675613403 3.0383152961730957 4.7763495445251465
Loss :  1.741255521774292 3.0793726444244385 4.8206281661987305
Loss :  1.7438322305679321 1.6940290927886963 3.437861442565918
Loss :  1.7445170879364014 4.160873889923096 5.905390739440918
Loss :  1.7409573793411255 1.4576276540756226 3.198585033416748
  batch 60 loss: 1.7409573793411255, 1.4576276540756226, 3.198585033416748
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7407538890838623 2.7927842140197754 4.533537864685059
Loss :  1.7442638874053955 3.7310104370117188 5.475274085998535
Loss :  1.7426867485046387 1.7825275659561157 3.525214195251465
Loss :  1.7378602027893066 2.1863057613372803 3.924165964126587
Loss :  1.737187147140503 1.2388088703155518 2.9759960174560547
Loss :  1.7868688106536865 4.1655707359313965 5.952439308166504
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7857078313827515 4.180700778961182 5.966408729553223
Loss :  1.7849622964859009 4.089226722717285 5.8741888999938965
Loss :  1.7900495529174805 4.136981010437012 5.927030563354492
Total LOSS train 4.0628253423250635 valid 5.930016875267029
CE LOSS train 1.7412532861416157 valid 0.4475123882293701
Contrastive LOSS train 2.3215720561834483 valid 1.034245252609253
EPOCH 246:
Loss :  1.7414805889129639 1.9290258884429932 3.670506477355957
Loss :  1.7398680448532104 1.926568627357483 3.6664366722106934
Loss :  1.7370961904525757 1.622987151145935 3.3600833415985107
Loss :  1.7403191328048706 2.069403648376465 3.809722900390625
Loss :  1.7421005964279175 1.7315163612365723 3.4736170768737793
Loss :  1.7408860921859741 2.0472326278686523 3.788118839263916
Loss :  1.7407673597335815 2.2433433532714844 3.9841108322143555
Loss :  1.73882257938385 1.9894238710403442 3.7282464504241943
Loss :  1.7404162883758545 2.3515703678131104 4.091986656188965
Loss :  1.7309813499450684 2.705549955368042 4.436531066894531
Loss :  1.7408981323242188 2.316331386566162 4.057229518890381
Loss :  1.7481529712677002 2.691159963607788 4.439312934875488
Loss :  1.7424044609069824 3.237004280090332 4.9794087409973145
Loss :  1.7398247718811035 2.82317852973938 4.5630035400390625
Loss :  1.74466073513031 3.185763359069824 4.930424213409424
Loss :  1.738395094871521 3.004002094268799 4.742397308349609
Loss :  1.7414904832839966 3.0791099071502686 4.820600509643555
Loss :  1.738663673400879 3.3957879543304443 5.134451866149902
Loss :  1.740661859512329 3.845076084136963 5.585738182067871
Loss :  1.737465739250183 2.73775577545166 4.475221633911133
  batch 20 loss: 1.737465739250183, 2.73775577545166, 4.475221633911133
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7389724254608154 1.934901475906372 3.6738739013671875
Loss :  1.739911437034607 2.062241792678833 3.8021531105041504
Loss :  1.741003394126892 1.6810513734817505 3.4220547676086426
Loss :  1.745548129081726 2.8367230892181396 4.582271099090576
Loss :  1.7426053285598755 1.901233196258545 3.643838405609131
Loss :  1.7385683059692383 2.334052801132202 4.0726213455200195
Loss :  1.7450779676437378 1.9976155757904053 3.7426934242248535
Loss :  1.7380446195602417 1.6712831258773804 3.409327745437622
Loss :  1.7441303730010986 2.3886616230010986 4.132791996002197
Loss :  1.7380610704421997 2.283406972885132 4.021468162536621
Loss :  1.7497389316558838 2.163113594055176 3.9128525257110596
Loss :  1.7451813220977783 1.924243450164795 3.6694247722625732
Loss :  1.7399976253509521 1.8735852241516113 3.6135828495025635
Loss :  1.740457534790039 2.088378667831421 3.82883620262146
Loss :  1.7464359998703003 2.3126776218414307 4.059113502502441
Loss :  1.746545672416687 1.9829961061477661 3.729541778564453
Loss :  1.743868350982666 2.0450503826141357 3.7889187335968018
Loss :  1.7409088611602783 2.2117371559143066 3.952646017074585
Loss :  1.7425771951675415 3.2452943325042725 4.9878716468811035
Loss :  1.7438410520553589 2.3286356925964355 4.072476863861084
  batch 40 loss: 1.7438410520553589, 2.3286356925964355, 4.072476863861084
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7417036294937134 2.839773654937744 4.581477165222168
Loss :  1.7423704862594604 3.227731227874756 4.970101833343506
Loss :  1.7440019845962524 2.059194326400757 3.803196430206299
Loss :  1.7425317764282227 1.922622799873352 3.665154457092285
Loss :  1.7445294857025146 2.8412251472473145 4.58575439453125
Loss :  1.7417856454849243 2.148893117904663 3.890678882598877
Loss :  1.7410215139389038 1.7366254329681396 3.477646827697754
Loss :  1.741990566253662 1.8830559253692627 3.625046491622925
Loss :  1.7426154613494873 1.6899149417877197 3.432530403137207
Loss :  1.7407283782958984 1.654012680053711 3.3947410583496094
Loss :  1.7374615669250488 1.851940393447876 3.589401960372925
Loss :  1.7403223514556885 2.2815582752227783 4.021880626678467
Loss :  1.7438925504684448 1.8774914741516113 3.6213841438293457
Loss :  1.739122986793518 2.0787417888641357 3.8178648948669434
Loss :  1.7401202917099 1.5560050010681152 3.2961254119873047
Loss :  1.7360905408859253 1.818878412246704 3.55496883392334
Loss :  1.7417998313903809 1.7473158836364746 3.4891157150268555
Loss :  1.7442213296890259 1.6729443073272705 3.417165756225586
Loss :  1.744454026222229 2.5158498287200928 4.260303974151611
Loss :  1.740551471710205 1.7610423564910889 3.501593828201294
  batch 60 loss: 1.740551471710205, 1.7610423564910889, 3.501593828201294
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7406055927276611 2.1279497146606445 3.8685553073883057
Loss :  1.74209463596344 1.6549055576324463 3.397000312805176
Loss :  1.7428501844406128 1.5323665142059326 3.275216579437256
Loss :  1.7392487525939941 2.0102384090423584 3.7494871616363525
Loss :  1.7386966943740845 1.2870597839355469 3.025756359100342
Loss :  1.769581913948059 3.8217217922210693 5.591303825378418
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7685238122940063 3.9781174659729004 5.746641159057617
Loss :  1.768709421157837 3.7516603469848633 5.520369529724121
Loss :  1.7732374668121338 3.846331834793091 5.619569301605225
Total LOSS train 3.956425483410175 valid 5.619470953941345
CE LOSS train 1.7414098996382492 valid 0.44330936670303345
Contrastive LOSS train 2.215015559930068 valid 0.9615829586982727
EPOCH 247:
Loss :  1.742435097694397 1.6595768928527832 3.4020118713378906
Loss :  1.740812063217163 2.013869285583496 3.754681348800659
Loss :  1.7387758493423462 1.6574593782424927 3.396235227584839
Loss :  1.7410577535629272 2.2118897438049316 3.9529476165771484
Loss :  1.744570016860962 1.9564645290374756 3.7010345458984375
Loss :  1.741648554801941 1.9115976095199585 3.6532461643218994
Loss :  1.7441500425338745 2.0270097255706787 3.7711596488952637
Loss :  1.7410831451416016 2.677777051925659 4.41886043548584
Loss :  1.7421531677246094 1.9794543981552124 3.7216076850891113
Loss :  1.733741283416748 1.9682542085647583 3.701995372772217
Loss :  1.7428271770477295 2.323418140411377 4.066245079040527
Loss :  1.748408555984497 2.7260165214538574 4.474425315856934
Loss :  1.7440853118896484 2.0297274589538574 3.773812770843506
Loss :  1.7415310144424438 2.0525522232055664 3.7940831184387207
Loss :  1.7458873987197876 2.048271417617798 3.794158935546875
Loss :  1.7405054569244385 1.6190868616104126 3.3595924377441406
Loss :  1.7440820932388306 1.388891339302063 3.1329734325408936
Loss :  1.741986632347107 2.4244184494018555 4.166405200958252
Loss :  1.7410074472427368 2.2412619590759277 3.982269287109375
Loss :  1.7408366203308105 2.2068541049957275 3.947690725326538
  batch 20 loss: 1.7408366203308105, 2.2068541049957275, 3.947690725326538
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7424428462982178 2.728630542755127 4.471073150634766
Loss :  1.7425719499588013 2.9568793773651123 4.699451446533203
Loss :  1.7449103593826294 1.7213393449783325 3.466249704360962
Loss :  1.747023344039917 1.9307019710540771 3.677725315093994
Loss :  1.742696762084961 2.2676339149475098 4.010330677032471
Loss :  1.739932656288147 2.1686742305755615 3.908607006072998
Loss :  1.7480130195617676 2.315434217453003 4.063446998596191
Loss :  1.7392171621322632 2.2221450805664062 3.961362361907959
Loss :  1.7485243082046509 3.6083180904388428 5.356842517852783
Loss :  1.742809772491455 2.3610498905181885 4.103859901428223
Loss :  1.7507847547531128 2.301718235015869 4.0525031089782715
Loss :  1.7472379207611084 2.379425287246704 4.1266632080078125
Loss :  1.7435516119003296 2.213071584701538 3.956623077392578
Loss :  1.741653323173523 2.290863037109375 4.0325164794921875
Loss :  1.7484222650527954 2.3343775272369385 4.082799911499023
Loss :  1.7478218078613281 2.421701192855835 4.169523239135742
Loss :  1.7467072010040283 2.2034926414489746 3.950199842453003
Loss :  1.7455508708953857 2.124864339828491 3.870415210723877
Loss :  1.7455897331237793 2.2469522953033447 3.992542028427124
Loss :  1.7453526258468628 2.1214544773101807 3.866806983947754
  batch 40 loss: 1.7453526258468628, 2.1214544773101807, 3.866806983947754
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7455095052719116 2.64678692817688 4.392296314239502
Loss :  1.745209813117981 1.9245829582214355 3.669792652130127
Loss :  1.7440656423568726 2.3486921787261963 4.092757701873779
Loss :  1.746570348739624 2.650601387023926 4.397171974182129
Loss :  1.7466697692871094 3.490145683288574 5.236815452575684
Loss :  1.743698000907898 4.397451877593994 6.141149997711182
Loss :  1.7443602085113525 3.3634860515594482 5.107846260070801
Loss :  1.7407386302947998 1.731251835823059 3.4719905853271484
Loss :  1.7431230545043945 1.6694355010986328 3.4125585556030273
Loss :  1.7404725551605225 1.5267131328582764 3.267185688018799
Loss :  1.7383639812469482 1.7819498777389526 3.5203137397766113
Loss :  1.7426562309265137 2.172579288482666 3.9152355194091797
Loss :  1.7480850219726562 1.4050524234771729 3.153137445449829
Loss :  1.741780161857605 1.5366947650909424 3.278474807739258
Loss :  1.7435333728790283 1.543211579322815 3.286745071411133
Loss :  1.7429640293121338 2.1575534343719482 3.900517463684082
Loss :  1.7452020645141602 2.776878595352173 4.522080421447754
Loss :  1.747147560119629 2.291922092437744 4.039069652557373
Loss :  1.7466603517532349 2.7303545475006104 4.477015018463135
Loss :  1.7433778047561646 2.3676021099090576 4.110980033874512
  batch 60 loss: 1.7433778047561646, 2.3676021099090576, 4.110980033874512
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7430990934371948 1.9036778211593628 3.6467769145965576
Loss :  1.7441874742507935 2.1187753677368164 3.8629627227783203
Loss :  1.7434555292129517 1.9874144792556763 3.730870008468628
Loss :  1.7401677370071411 2.3246347904205322 4.064802646636963
Loss :  1.7391718626022339 1.5790033340454102 3.3181753158569336
Loss :  1.7813085317611694 3.575549602508545 5.356858253479004
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7808938026428223 3.6136932373046875 5.39458703994751
Loss :  1.7792197465896606 3.441725492477417 5.220945358276367
Loss :  1.7865276336669922 3.365032196044922 5.151559829711914
Total LOSS train 3.966180005440345 valid 5.280987620353699
CE LOSS train 1.7435795197120079 valid 0.44663190841674805
Contrastive LOSS train 2.2226004710564244 valid 0.8412580490112305
EPOCH 248:
Loss :  1.742586612701416 2.2176871299743652 3.9602737426757812
Loss :  1.740527868270874 1.9796347618103027 3.7201626300811768
Loss :  1.738946795463562 2.092867612838745 3.8318142890930176
Loss :  1.7420949935913086 3.125930070877075 4.868024826049805
Loss :  1.7440437078475952 2.2087972164154053 3.952840805053711
Loss :  1.7424414157867432 2.0687947273254395 3.8112361431121826
Loss :  1.743173360824585 2.8212363719940186 4.5644097328186035
Loss :  1.7407596111297607 2.3258936405181885 4.066653251647949
Loss :  1.7420481443405151 2.7348899841308594 4.476938247680664
Loss :  1.7325596809387207 1.8353328704833984 3.567892551422119
Loss :  1.7423069477081299 1.6045536994934082 3.346860647201538
Loss :  1.7487449645996094 1.8853706121444702 3.634115695953369
Loss :  1.7431098222732544 2.393831491470337 4.136941432952881
Loss :  1.740878701210022 1.9985737800598145 3.739452362060547
Loss :  1.7452751398086548 2.104872226715088 3.850147247314453
Loss :  1.7393791675567627 2.0784475803375244 3.817826747894287
Loss :  1.7440881729125977 1.9405367374420166 3.6846249103546143
Loss :  1.7410260438919067 2.4025559425354004 4.143581867218018
Loss :  1.742079734802246 1.8563729524612427 3.598452568054199
Loss :  1.740155816078186 2.2040088176727295 3.944164752960205
  batch 20 loss: 1.740155816078186, 2.2040088176727295, 3.944164752960205
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7416636943817139 3.25113844871521 4.992802143096924
Loss :  1.743261694908142 2.809675931930542 4.5529375076293945
Loss :  1.7435721158981323 1.8941446542739868 3.637716770172119
Loss :  1.7474734783172607 1.904954195022583 3.6524276733398438
Loss :  1.7426490783691406 2.13173770904541 3.874386787414551
Loss :  1.7393537759780884 1.9571692943572998 3.6965231895446777
Loss :  1.7457349300384521 2.4058918952941895 4.1516265869140625
Loss :  1.7386733293533325 2.351741313934326 4.090414524078369
Loss :  1.7436940670013428 2.2038018703460693 3.947495937347412
Loss :  1.7380836009979248 2.014855146408081 3.752938747406006
Loss :  1.7491068840026855 2.0825090408325195 3.831615924835205
Loss :  1.7434381246566772 1.6017897129058838 3.3452277183532715
Loss :  1.73859441280365 1.82710862159729 3.5657029151916504
Loss :  1.7384514808654785 1.943361520767212 3.6818130016326904
Loss :  1.7448337078094482 2.1216812133789062 3.8665149211883545
Loss :  1.7447481155395508 1.890347957611084 3.6350960731506348
Loss :  1.742550253868103 2.2097620964050293 3.952312469482422
Loss :  1.739499807357788 2.049546241760254 3.789046049118042
Loss :  1.741743803024292 2.0108537673950195 3.7525975704193115
Loss :  1.743050217628479 2.136136770248413 3.8791871070861816
  batch 40 loss: 1.743050217628479, 2.136136770248413, 3.8791871070861816
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7412419319152832 2.270825147628784 4.012066841125488
Loss :  1.7405034303665161 2.4052252769470215 4.145728588104248
Loss :  1.742160439491272 2.0227549076080322 3.7649154663085938
Loss :  1.74113929271698 1.678939938545227 3.420079231262207
Loss :  1.7434265613555908 1.6086081266403198 3.352034568786621
Loss :  1.7409906387329102 1.7855411767959595 3.52653169631958
Loss :  1.740633249282837 1.6355491876602173 3.3761825561523438
Loss :  1.741534948348999 1.5438839197158813 3.28541898727417
Loss :  1.7430052757263184 1.7800167798995972 3.523022174835205
Loss :  1.7406612634658813 1.6069093942642212 3.3475706577301025
Loss :  1.7386741638183594 2.0948374271392822 3.8335115909576416
Loss :  1.7419910430908203 1.9935259819030762 3.7355170249938965
Loss :  1.7464263439178467 1.9811028242111206 3.7275290489196777
Loss :  1.740833044052124 2.0439226627349854 3.7847557067871094
Loss :  1.7431124448776245 2.0460588932037354 3.7891712188720703
Loss :  1.7401665449142456 2.0172042846679688 3.757370948791504
Loss :  1.7454557418823242 2.2133591175079346 3.958814859390259
Loss :  1.7479174137115479 2.001629114151001 3.749546527862549
Loss :  1.7481375932693481 2.3401832580566406 4.088320732116699
Loss :  1.7450135946273804 2.011178493499756 3.756192207336426
  batch 60 loss: 1.7450135946273804, 2.011178493499756, 3.756192207336426
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7457493543624878 1.9077869653701782 3.653536319732666
Loss :  1.746970295906067 1.5414010286331177 3.2883713245391846
Loss :  1.7469502687454224 1.8397789001464844 3.586729049682617
Loss :  1.7451924085617065 2.249265432357788 3.994457721710205
Loss :  1.7442750930786133 1.108259916305542 2.8525350093841553
Loss :  1.78542160987854 4.211091995239258 5.996513366699219
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7833921909332275 4.323276996612549 6.1066694259643555
Loss :  1.7841490507125854 4.095138072967529 5.879287242889404
Loss :  1.7867618799209595 4.116094589233398 5.902856349945068
Total LOSS train 3.8103801250457763 valid 5.971331596374512
CE LOSS train 1.742593317765456 valid 0.44669046998023987
Contrastive LOSS train 2.0677868274542 valid 1.0290236473083496
EPOCH 249:
Loss :  1.7465907335281372 2.004178285598755 3.7507691383361816
Loss :  1.7451386451721191 2.141183614730835 3.886322259902954
Loss :  1.7435671091079712 1.4977494478225708 3.241316556930542
Loss :  1.745924711227417 1.638200283050537 3.384124994277954
Loss :  1.7483164072036743 1.7543845176696777 3.5027008056640625
Loss :  1.7454863786697388 2.0157294273376465 3.7612156867980957
Loss :  1.7479709386825562 2.165724515914917 3.9136953353881836
Loss :  1.7448371648788452 1.805418610572815 3.55025577545166
Loss :  1.7456363439559937 1.9434378147125244 3.6890740394592285
Loss :  1.7380857467651367 1.845716118812561 3.583801746368408
Loss :  1.7452642917633057 2.2063026428222656 3.9515669345855713
Loss :  1.7497234344482422 2.0880706310272217 3.837794065475464
Loss :  1.746045708656311 1.886351227760315 3.632396936416626
Loss :  1.7432210445404053 2.0944576263427734 3.8376786708831787
Loss :  1.7477372884750366 1.751441240310669 3.499178409576416
Loss :  1.7423336505889893 1.6055034399032593 3.347836971282959
Loss :  1.7459075450897217 1.7829669713974 3.528874397277832
Loss :  1.743901014328003 2.113884210586548 3.857785224914551
Loss :  1.7437764406204224 1.9876997470855713 3.731476306915283
Loss :  1.7437766790390015 2.5275769233703613 4.271353721618652
  batch 20 loss: 1.7437766790390015, 2.5275769233703613, 4.271353721618652
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7445800304412842 1.9543724060058594 3.6989524364471436
Loss :  1.7451578378677368 1.8337327241897583 3.578890562057495
Loss :  1.747393250465393 1.8653228282928467 3.6127161979675293
Loss :  1.7492612600326538 1.9935767650604248 3.742837905883789
Loss :  1.7452548742294312 1.7627280950546265 3.5079829692840576
Loss :  1.742417573928833 2.214561939239502 3.956979513168335
Loss :  1.7478913068771362 2.634188652038574 4.382080078125
Loss :  1.7416322231292725 1.8273955583572388 3.569027900695801
Loss :  1.7463977336883545 1.87539541721344 3.621793270111084
Loss :  1.7411489486694336 2.268000841140747 4.009149551391602
Loss :  1.7509880065917969 2.034006118774414 3.784994125366211
Loss :  1.746098518371582 1.9798798561096191 3.725978374481201
Loss :  1.7413477897644043 2.2327828407287598 3.974130630493164
Loss :  1.7416205406188965 1.9520517587661743 3.6936721801757812
Loss :  1.7469935417175293 2.3439149856567383 4.090908527374268
Loss :  1.7472237348556519 1.6100995540618896 3.357323169708252
Loss :  1.744987964630127 2.036435127258301 3.7814230918884277
Loss :  1.7425413131713867 1.9809119701385498 3.7234532833099365
Loss :  1.7443729639053345 1.8521003723144531 3.596473217010498
Loss :  1.7459439039230347 1.844753623008728 3.5906975269317627
  batch 40 loss: 1.7459439039230347, 1.844753623008728, 3.5906975269317627
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.744002103805542 2.150739908218384 3.894742012023926
Loss :  1.7443817853927612 1.5847452878952026 3.329127073287964
Loss :  1.745862603187561 2.1029257774353027 3.848788261413574
Loss :  1.7441046237945557 1.8867785930633545 3.63088321685791
Loss :  1.7466193437576294 1.7871170043945312 3.533736228942871
Loss :  1.7447937726974487 2.146428346633911 3.8912220001220703
Loss :  1.744308352470398 1.5731791257858276 3.3174874782562256
Loss :  1.7447518110275269 1.7673696279525757 3.5121214389801025
Loss :  1.7472538948059082 2.1920037269592285 3.9392576217651367
Loss :  1.7440285682678223 1.8250101804733276 3.5690388679504395
Loss :  1.7419272661209106 1.9914181232452393 3.7333455085754395
Loss :  1.7444126605987549 1.959396481513977 3.7038092613220215
Loss :  1.748209834098816 1.7059143781661987 3.4541242122650146
Loss :  1.7426286935806274 1.8029353618621826 3.5455641746520996
Loss :  1.7439934015274048 2.276789426803589 4.020782947540283
Loss :  1.7417776584625244 2.3359594345092773 4.077736854553223
Loss :  1.7455646991729736 2.095689535140991 3.841254234313965
Loss :  1.7477495670318604 1.8355978727340698 3.5833473205566406
Loss :  1.7479743957519531 2.4659037590026855 4.213878154754639
Loss :  1.7450897693634033 1.7239882946014404 3.4690780639648438
  batch 60 loss: 1.7450897693634033, 1.7239882946014404, 3.4690780639648438
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7451730966567993 2.260356903076172 4.005529880523682
Loss :  1.7465828657150269 1.5959720611572266 3.342555046081543
Loss :  1.7463421821594238 1.7318103313446045 3.4781525135040283
Loss :  1.743687629699707 1.9998818635940552 3.7435693740844727
Loss :  1.7425836324691772 1.3512661457061768 3.0938496589660645
Loss :  1.7865976095199585 3.8412349224090576 5.627832412719727
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7845886945724487 3.859736680984497 5.644325256347656
Loss :  1.7851101160049438 3.7421205043792725 5.527230739593506
Loss :  1.788621425628662 3.833740711212158 5.62236213684082
Total LOSS train 3.7004871368408203 valid 5.605437636375427
CE LOSS train 1.7450815200805665 valid 0.4471553564071655
Contrastive LOSS train 1.9554056351001445 valid 0.9584351778030396
EPOCH 250:
Loss :  1.7461265325546265 1.7411097288131714 3.487236261367798
Loss :  1.7448148727416992 2.2872984409332275 4.032113075256348
Loss :  1.742646336555481 2.22634220123291 3.9689884185791016
Loss :  1.744990348815918 2.5526463985443115 4.297636985778809
Loss :  1.7462517023086548 2.8754637241363525 4.621715545654297
Loss :  1.744562029838562 2.72627854347229 4.4708404541015625
Loss :  1.7455188035964966 2.05940842628479 3.804927349090576
Loss :  1.7430000305175781 2.736598253250122 4.479598045349121
Loss :  1.7436895370483398 2.5014381408691406 4.2451276779174805
Loss :  1.7350406646728516 3.929466724395752 5.6645073890686035
Loss :  1.7439078092575073 3.5600273609161377 5.3039350509643555
Loss :  1.7498003244400024 2.336482286453247 4.086282730102539
Loss :  1.7449537515640259 3.0886032581329346 4.83355712890625
Loss :  1.7418911457061768 3.181279420852661 4.923170566558838
Loss :  1.7461248636245728 2.5886404514312744 4.334765434265137
Loss :  1.7411448955535889 1.6532807350158691 3.394425630569458
Loss :  1.7448418140411377 1.597480297088623 3.3423221111297607
Loss :  1.742819905281067 1.5819023847579956 3.3247222900390625
Loss :  1.742566466331482 1.7897993326187134 3.5323657989501953
Loss :  1.74154794216156 2.292013645172119 4.033561706542969
  batch 20 loss: 1.74154794216156, 2.292013645172119, 4.033561706542969
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7428442239761353 2.351954221725464 4.094798564910889
Loss :  1.7436254024505615 2.3942625522613525 4.137887954711914
Loss :  1.74494206905365 2.133326768875122 3.8782687187194824
Loss :  1.7478511333465576 1.8455766439437866 3.5934276580810547
Loss :  1.7438620328903198 2.0986506938934326 3.842512607574463
Loss :  1.7404650449752808 2.1865041255950928 3.926969051361084
Loss :  1.7468981742858887 2.0735011100769043 3.820399284362793
Loss :  1.7404972314834595 2.2377607822418213 3.9782581329345703
Loss :  1.7454644441604614 2.5960354804992676 4.3414998054504395
Loss :  1.738850712776184 2.520817995071411 4.259668827056885
Loss :  1.7499374151229858 1.863893985748291 3.6138315200805664
Loss :  1.745331883430481 1.7450865507125854 3.4904184341430664
Loss :  1.7398933172225952 2.2302896976470947 3.9701828956604004
Loss :  1.739986538887024 2.7176718711853027 4.457658290863037
Loss :  1.7460005283355713 3.0289831161499023 4.7749834060668945
Loss :  1.7454484701156616 2.3197436332702637 4.065192222595215
Loss :  1.743064045906067 2.511748790740967 4.254812717437744
Loss :  1.7400285005569458 2.8282828330993652 4.5683112144470215
Loss :  1.7420464754104614 1.9717470407485962 3.7137935161590576
Loss :  1.7429101467132568 2.2619314193725586 4.0048418045043945
  batch 40 loss: 1.7429101467132568, 2.2619314193725586, 4.0048418045043945
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7416408061981201 2.255173683166504 3.996814489364624
Loss :  1.7417412996292114 1.2869311571121216 3.028672456741333
Loss :  1.7433176040649414 1.6537388563156128 3.3970565795898438
Loss :  1.7424306869506836 1.6883481740951538 3.430778980255127
Loss :  1.7448265552520752 2.389673948287964 4.134500503540039
Loss :  1.7417757511138916 2.406975746154785 4.148751258850098
Loss :  1.7416725158691406 2.7564992904663086 4.498171806335449
Loss :  1.7430305480957031 2.8984808921813965 4.6415114402771
Loss :  1.7433489561080933 1.7979702949523926 3.5413193702697754
Loss :  1.74213445186615 1.6787638664245605 3.4208984375
Loss :  1.7392277717590332 1.980371356010437 3.7195992469787598
Loss :  1.742121696472168 2.376992702484131 4.119114398956299
Loss :  1.7462818622589111 2.681952714920044 4.428234577178955
Loss :  1.741565227508545 2.263235330581665 4.004800796508789
Loss :  1.7432268857955933 2.0972015857696533 3.840428352355957
Loss :  1.740433931350708 3.0603322982788086 4.8007659912109375
Loss :  1.7437753677368164 2.6978518962860107 4.441627502441406
Loss :  1.7465929985046387 1.7002604007720947 3.4468533992767334
Loss :  1.7472654581069946 2.843888998031616 4.5911545753479
Loss :  1.744670033454895 2.256027936935425 4.000698089599609
  batch 60 loss: 1.744670033454895, 2.256027936935425, 4.000698089599609
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7450484037399292 3.5024735927581787 5.247521877288818
Loss :  1.7470420598983765 2.7391440868377686 4.4861860275268555
Loss :  1.7471503019332886 1.8231195211410522 3.570269823074341
Loss :  1.7449842691421509 2.0345876216888428 3.779572010040283
Loss :  1.7443233728408813 1.3689146041870117 3.1132378578186035
Loss :  1.7868387699127197 4.170475482940674 5.957314491271973
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.784533977508545 4.157145023345947 5.941679000854492
Loss :  1.7851778268814087 4.099869728088379 5.885047435760498
Loss :  1.789151906967163 4.1134257316589355 5.9025774002075195
Total LOSS train 4.073816248086783 valid 5.921654582023621
CE LOSS train 1.7436279443594125 valid 0.44728797674179077
Contrastive LOSS train 2.330188301893381 valid 1.0283564329147339
EPOCH 251:
Loss :  1.7479127645492554 2.4923505783081055 4.24026346206665
Loss :  1.7469795942306519 2.276301622390747 4.023281097412109
Loss :  1.7449601888656616 2.5070176124572754 4.251977920532227
Loss :  1.74747896194458 2.2784054279327393 4.025884628295898
Loss :  1.7489506006240845 1.9899101257324219 3.738860607147217
Loss :  1.7474148273468018 1.721591591835022 3.4690065383911133
Loss :  1.7485954761505127 2.218372344970703 3.966967821121216
Loss :  1.7462596893310547 1.611940860748291 3.3582005500793457
Loss :  1.7465897798538208 2.4911699295043945 4.237759590148926
Loss :  1.7390559911727905 1.7725170850753784 3.511573076248169
Loss :  1.7460960149765015 1.9178881645202637 3.6639842987060547
Loss :  1.7505064010620117 1.7257202863693237 3.476226806640625
Loss :  1.7466508150100708 2.382500171661377 4.129150867462158
Loss :  1.743367075920105 2.8068008422851562 4.550168037414551
Loss :  1.7473928928375244 2.938627004623413 4.6860198974609375
Loss :  1.7421128749847412 2.71301531791687 4.455128192901611
Loss :  1.7456377744674683 2.864920139312744 4.610558032989502
Loss :  1.7435309886932373 2.7440946102142334 4.487625598907471
Loss :  1.7430460453033447 1.6895102262496948 3.43255615234375
Loss :  1.741920828819275 2.7315871715545654 4.473507881164551
  batch 20 loss: 1.741920828819275, 2.7315871715545654, 4.473507881164551
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7429968118667603 2.863842725753784 4.606839656829834
Loss :  1.7440439462661743 2.377986431121826 4.122030258178711
Loss :  1.74580717086792 2.471424102783203 4.217231273651123
Loss :  1.7490195035934448 2.3949732780456543 4.143992900848389
Loss :  1.7449089288711548 3.0539050102233887 4.798813819885254
Loss :  1.7416927814483643 1.8071123361587524 3.5488052368164062
Loss :  1.7478654384613037 2.433668613433838 4.1815338134765625
Loss :  1.7416999340057373 2.4222781658172607 4.163978099822998
Loss :  1.746291160583496 2.180272340774536 3.9265635013580322
Loss :  1.7402242422103882 2.346369981765747 4.086594104766846
Loss :  1.7505393028259277 2.6618943214416504 4.412433624267578
Loss :  1.745756983757019 2.1264405250549316 3.8721976280212402
Loss :  1.740797758102417 1.7951432466506958 3.5359411239624023
Loss :  1.7410075664520264 1.647051453590393 3.388059139251709
Loss :  1.7469756603240967 2.3489186763763428 4.0958943367004395
Loss :  1.7472796440124512 2.475461006164551 4.222740650177002
Loss :  1.7457044124603271 2.4572103023529053 4.202914714813232
Loss :  1.7432857751846313 2.5910534858703613 4.334339141845703
Loss :  1.744974970817566 2.055708885192871 3.8006839752197266
Loss :  1.7459532022476196 1.8628515005111694 3.608804702758789
  batch 40 loss: 1.7459532022476196, 1.8628515005111694, 3.608804702758789
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7445460557937622 2.1441802978515625 3.888726234436035
Loss :  1.7444707155227661 1.790582299232483 3.535053014755249
Loss :  1.7450439929962158 2.068122148513794 3.8131661415100098
Loss :  1.7446560859680176 2.589977502822876 4.334633827209473
Loss :  1.7462304830551147 1.8324447870254517 3.5786752700805664
Loss :  1.7433123588562012 1.957645058631897 3.7009572982788086
Loss :  1.7437041997909546 1.8164654970169067 3.5601696968078613
Loss :  1.7438087463378906 1.5917295217514038 3.335538387298584
Loss :  1.7449307441711426 2.0637807846069336 3.808711528778076
Loss :  1.7425408363342285 1.5744799375534058 3.317020893096924
Loss :  1.7395457029342651 1.9863734245300293 3.725919246673584
Loss :  1.742491602897644 2.569000244140625 4.311491966247559
Loss :  1.7469720840454102 2.4276082515716553 4.1745805740356445
Loss :  1.7413934469223022 2.5810177326202393 4.322411060333252
Loss :  1.742895245552063 1.9305427074432373 3.67343807220459
Loss :  1.7402373552322388 1.9849727153778076 3.725210189819336
Loss :  1.7440980672836304 2.424980401992798 4.169078350067139
Loss :  1.7467753887176514 2.806025743484497 4.552801132202148
Loss :  1.7466509342193604 2.797682046890259 4.544332981109619
Loss :  1.7435942888259888 1.9775874614715576 3.721181869506836
  batch 60 loss: 1.7435942888259888, 1.9775874614715576, 3.721181869506836
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7434666156768799 1.7082725763320923 3.4517393112182617
Loss :  1.7448049783706665 1.5334608554840088 3.278265953063965
Loss :  1.7448774576187134 3.9015913009643555 5.646468639373779
Loss :  1.741623044013977 2.177039623260498 3.9186625480651855
Loss :  1.7410653829574585 1.094131350517273 2.8351967334747314
Loss :  1.8038599491119385 4.199177265167236 6.003037452697754
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8005629777908325 4.208493709564209 6.009056568145752
Loss :  1.801242709159851 4.09310245513916 5.894345283508301
Loss :  1.805076003074646 4.148693084716797 5.953769207000732
Total LOSS train 3.9843465181497426 valid 5.965052127838135
CE LOSS train 1.7446926245322594 valid 0.4512690007686615
Contrastive LOSS train 2.2396538734436033 valid 1.0371732711791992
EPOCH 252:
Loss :  1.7445496320724487 2.088742256164551 3.833292007446289
Loss :  1.7434419393539429 1.9492542743682861 3.6926960945129395
Loss :  1.7413504123687744 1.7657588720321655 3.5071091651916504
Loss :  1.7437186241149902 2.593079090118408 4.336797714233398
Loss :  1.7460155487060547 2.9775426387786865 4.72355842590332
Loss :  1.7442423105239868 1.333141565322876 3.0773839950561523
Loss :  1.7454912662506104 2.1542394161224365 3.899730682373047
Loss :  1.7427611351013184 1.5872807502746582 3.3300418853759766
Loss :  1.7439430952072144 2.226971387863159 3.970914363861084
Loss :  1.7350493669509888 2.033151149749756 3.768200397491455
Loss :  1.7444075345993042 2.5533804893493652 4.297788143157959
Loss :  1.7498853206634521 2.138023853302002 3.887909173965454
Loss :  1.7450560331344604 1.7827147245407104 3.527770757675171
Loss :  1.7422497272491455 2.242713689804077 3.9849634170532227
Loss :  1.7470289468765259 2.238349676132202 3.9853787422180176
Loss :  1.741223931312561 2.2408127784729004 3.982036590576172
Loss :  1.744830846786499 3.0109739303588867 4.755805015563965
Loss :  1.7427358627319336 2.3937289714813232 4.136465072631836
Loss :  1.7423840761184692 1.8913148641586304 3.6336989402770996
Loss :  1.7410719394683838 2.67456316947937 4.415635108947754
  batch 20 loss: 1.7410719394683838, 2.67456316947937, 4.415635108947754
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7423542737960815 3.1939265727996826 4.936280727386475
Loss :  1.7434215545654297 3.4555115699768066 5.198933124542236
Loss :  1.7452051639556885 1.7601109743118286 3.5053162574768066
Loss :  1.7476383447647095 2.2240028381347656 3.9716410636901855
Loss :  1.7444573640823364 2.36820912361145 4.112666606903076
Loss :  1.740825891494751 2.0332047939300537 3.7740306854248047
Loss :  1.747135043144226 1.7486108541488647 3.495745897293091
Loss :  1.7406171560287476 2.5752475261688232 4.315864562988281
Loss :  1.7456693649291992 2.165163993835449 3.9108333587646484
Loss :  1.7393529415130615 2.2362141609191895 3.975567102432251
Loss :  1.7507686614990234 2.7548255920410156 4.505594253540039
Loss :  1.745778203010559 2.1913185119628906 3.93709659576416
Loss :  1.7408478260040283 2.562185764312744 4.303033828735352
Loss :  1.7408030033111572 3.348109006881714 5.088912010192871
Loss :  1.7470252513885498 2.9611217975616455 4.708147048950195
Loss :  1.7467751502990723 2.34041428565979 4.087189674377441
Loss :  1.7446039915084839 3.024158000946045 4.768762111663818
Loss :  1.7415926456451416 2.7030107975006104 4.444603443145752
Loss :  1.7436890602111816 2.8108105659484863 4.554499626159668
Loss :  1.7441399097442627 3.476134777069092 5.220274925231934
  batch 40 loss: 1.7441399097442627, 3.476134777069092, 5.220274925231934
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7430615425109863 2.508462429046631 4.251523971557617
Loss :  1.7429741621017456 1.9507638216018677 3.6937379837036133
Loss :  1.7437525987625122 1.7754377126693726 3.5191903114318848
Loss :  1.7428678274154663 1.6441025733947754 3.3869705200195312
Loss :  1.7450122833251953 1.4816144704818726 3.2266268730163574
Loss :  1.7426857948303223 1.5897259712219238 3.332411766052246
Loss :  1.7432799339294434 1.743667721748352 3.486947536468506
Loss :  1.744113802909851 1.864993929862976 3.609107732772827
Loss :  1.747129201889038 2.0394678115844727 3.7865970134735107
Loss :  1.7432692050933838 1.9683431386947632 3.7116122245788574
Loss :  1.7417229413986206 2.043569803237915 3.785292625427246
Loss :  1.7448735237121582 2.3964288234710693 4.141302108764648
Loss :  1.7495745420455933 1.8885427713394165 3.6381173133850098
Loss :  1.7425299882888794 2.316542387008667 4.059072494506836
Loss :  1.7438608407974243 2.4011952877044678 4.145056247711182
Loss :  1.7429689168930054 3.2300171852111816 4.972986221313477
Loss :  1.7468634843826294 2.6392717361450195 4.386135101318359
Loss :  1.7490028142929077 2.7106592655181885 4.459661960601807
Loss :  1.748610258102417 3.5009469985961914 5.2495574951171875
Loss :  1.7462878227233887 3.494110107421875 5.240397930145264
  batch 60 loss: 1.7462878227233887, 3.494110107421875, 5.240397930145264
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.746829628944397 2.801469564437866 4.548299312591553
Loss :  1.7471404075622559 2.5156407356262207 4.262781143188477
Loss :  1.7482627630233765 1.8823407888412476 3.630603551864624
Loss :  1.7444850206375122 2.6116795539855957 4.356164455413818
Loss :  1.7433922290802002 2.1416687965393066 3.885061025619507
Loss :  1.7970950603485107 3.8917059898376465 5.688800811767578
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7946161031723022 3.9195218086242676 5.714138031005859
Loss :  1.795238733291626 3.886075019836426 5.681313514709473
Loss :  1.798993706703186 3.5977776050567627 5.396771430969238
Total LOSS train 4.097344361818754 valid 5.620255947113037
CE LOSS train 1.7442875366944532 valid 0.4497484266757965
Contrastive LOSS train 2.3530568067844095 valid 0.8994444012641907
EPOCH 253:
Loss :  1.746336817741394 2.765200614929199 4.511537551879883
Loss :  1.7461198568344116 2.754727602005005 4.500847339630127
Loss :  1.7434039115905762 1.6783993244171143 3.4218032360076904
Loss :  1.7446388006210327 1.6196396350860596 3.3642783164978027
Loss :  1.7475115060806274 1.7026481628417969 3.4501595497131348
Loss :  1.7444738149642944 1.5276659727096558 3.27213978767395
Loss :  1.7469890117645264 1.9998948574066162 3.7468838691711426
Loss :  1.7433267831802368 2.733729600906372 4.477056503295898
Loss :  1.7440212965011597 2.8358449935913086 4.579866409301758
Loss :  1.7359395027160645 2.686955213546753 4.422894477844238
Loss :  1.7442065477371216 2.4692001342773438 4.213406562805176
Loss :  1.7497961521148682 2.084820508956909 3.8346166610717773
Loss :  1.744943380355835 1.5688400268554688 3.3137834072113037
Loss :  1.742296814918518 2.2631828784942627 4.00547981262207
Loss :  1.746181845664978 1.7040705680847168 3.4502525329589844
Loss :  1.7412737607955933 1.6565358638763428 3.3978095054626465
Loss :  1.7450239658355713 2.5829787254333496 4.3280029296875
Loss :  1.7429884672164917 2.638655662536621 4.381644248962402
Loss :  1.7434649467468262 2.1309218406677246 3.874386787414551
Loss :  1.7419424057006836 2.2592575550079346 4.001199722290039
  batch 20 loss: 1.7419424057006836, 2.2592575550079346, 4.001199722290039
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7432522773742676 1.959449291229248 3.7027015686035156
Loss :  1.7450721263885498 2.0699620246887207 3.8150341510772705
Loss :  1.7454854249954224 2.3463923931121826 4.0918779373168945
Loss :  1.749853253364563 2.8072917461395264 4.557145118713379
Loss :  1.744841456413269 2.3275818824768066 4.072423458099365
Loss :  1.7417316436767578 2.6748783588409424 4.416609764099121
Loss :  1.7478809356689453 2.5818774700164795 4.329758644104004
Loss :  1.7418879270553589 3.786548376083374 5.528436183929443
Loss :  1.746747374534607 2.7004990577697754 4.447246551513672
Loss :  1.7421830892562866 2.362534761428833 4.10471773147583
Loss :  1.7511606216430664 2.456583261489868 4.2077436447143555
Loss :  1.746622920036316 3.0065736770629883 4.753196716308594
Loss :  1.7422784566879272 1.7029385566711426 3.4452171325683594
Loss :  1.7419824600219727 1.9347962141036987 3.676778793334961
Loss :  1.7477283477783203 2.726393699645996 4.474122047424316
Loss :  1.74779212474823 2.720299005508423 4.468091011047363
Loss :  1.7456822395324707 2.215266227722168 3.9609484672546387
Loss :  1.742692470550537 2.6702895164489746 4.412981986999512
Loss :  1.744830846786499 2.4985177516937256 4.243348598480225
Loss :  1.7459124326705933 1.894163966178894 3.6400763988494873
  batch 40 loss: 1.7459124326705933, 1.894163966178894, 3.6400763988494873
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7442547082901 2.145984172821045 3.8902387619018555
Loss :  1.74454665184021 3.6256206035614014 5.370167255401611
Loss :  1.746214509010315 3.408923387527466 5.15513801574707
Loss :  1.74469792842865 3.7358832359313965 5.480581283569336
Loss :  1.7467453479766846 1.4848272800445557 3.2315726280212402
Loss :  1.743998646736145 1.4733237028121948 3.21732234954834
Loss :  1.7435301542282104 1.5661581754684448 3.3096883296966553
Loss :  1.7442272901535034 1.8845983743667603 3.6288256645202637
Loss :  1.7452247142791748 2.1539039611816406 3.8991286754608154
Loss :  1.7433007955551147 1.8368955850601196 3.5801963806152344
Loss :  1.7403937578201294 2.1701741218566895 3.9105677604675293
Loss :  1.7432047128677368 2.076704740524292 3.8199095726013184
Loss :  1.74722421169281 1.8825441598892212 3.6297683715820312
Loss :  1.7417161464691162 2.490962028503418 4.232678413391113
Loss :  1.7431846857070923 2.206862449645996 3.950047016143799
Loss :  1.7408169507980347 2.78332257270813 4.524139404296875
Loss :  1.7447471618652344 2.847229242324829 4.591976165771484
Loss :  1.7471119165420532 2.517456293106079 4.264568328857422
Loss :  1.74818754196167 3.6535518169403076 5.401739120483398
Loss :  1.7461961507797241 3.4330813884735107 5.179277420043945
  batch 60 loss: 1.7461961507797241, 3.4330813884735107, 5.179277420043945
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7474900484085083 2.7489240169525146 4.4964141845703125
Loss :  1.7484686374664307 2.197035551071167 3.9455041885375977
Loss :  1.7478560209274292 1.9806469678878784 3.7285029888153076
Loss :  1.7434065341949463 2.238579273223877 3.9819858074188232
Loss :  1.7427685260772705 2.1587791442871094 3.90154767036438
Loss :  1.782754898071289 3.6877825260162354 5.470537185668945
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.778441071510315 3.6097142696380615 5.388155460357666
Loss :  1.7812705039978027 3.6228575706481934 5.404128074645996
Loss :  1.7802540063858032 3.5400350093841553 5.320289134979248
Total LOSS train 4.111046013465295 valid 5.395777463912964
CE LOSS train 1.7447694118206318 valid 0.4450635015964508
Contrastive LOSS train 2.366276603478652 valid 0.8850087523460388
EPOCH 254:
Loss :  1.7476282119750977 2.0640034675598145 3.811631679534912
Loss :  1.7474470138549805 2.3766820430755615 4.124129295349121
Loss :  1.7451578378677368 2.050527811050415 3.7956857681274414
Loss :  1.7487444877624512 1.7886027097702026 3.5373473167419434
Loss :  1.748733401298523 1.580629587173462 3.3293628692626953
Loss :  1.7495256662368774 1.9672304391860962 3.7167561054229736
Loss :  1.7493315935134888 2.3577332496643066 4.107064723968506
Loss :  1.7488324642181396 2.0565319061279297 3.8053643703460693
Loss :  1.7482227087020874 1.846671462059021 3.5948941707611084
Loss :  1.7414438724517822 2.334061861038208 4.07550573348999
Loss :  1.749739408493042 2.8284292221069336 4.578168869018555
Loss :  1.7532202005386353 2.7521274089813232 4.505347728729248
Loss :  1.7512283325195312 3.0669004917144775 4.81812858581543
Loss :  1.7479251623153687 2.4295172691345215 4.17744255065918
Loss :  1.7531594038009644 2.541215658187866 4.294374942779541
Loss :  1.744808554649353 2.7058098316192627 4.450618267059326
Loss :  1.7486652135849 2.5588090419769287 4.307474136352539
Loss :  1.7472268342971802 1.7342636585235596 3.4814906120300293
Loss :  1.7462055683135986 2.284626007080078 4.030831336975098
Loss :  1.748018503189087 1.5988237857818604 3.3468422889709473
  batch 20 loss: 1.748018503189087, 1.5988237857818604, 3.3468422889709473
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.747564435005188 1.866873025894165 3.6144375801086426
Loss :  1.7485030889511108 2.097839593887329 3.8463425636291504
Loss :  1.7488459348678589 1.777775526046753 3.5266213417053223
Loss :  1.7519586086273193 2.6943702697753906 4.446329116821289
Loss :  1.7462871074676514 2.7264392375946045 4.472726345062256
Loss :  1.743827223777771 2.3208727836608887 4.064700126647949
Loss :  1.7496616840362549 2.543546199798584 4.293208122253418
Loss :  1.7439993619918823 2.3770813941955566 4.1210808753967285
Loss :  1.7477459907531738 1.9248147010803223 3.672560691833496
Loss :  1.742343783378601 2.346132755279541 4.088476657867432
Loss :  1.7516001462936401 2.264760732650757 4.016360759735107
Loss :  1.7474080324172974 2.066232919692993 3.81364107131958
Loss :  1.7428932189941406 1.8209974765777588 3.5638906955718994
Loss :  1.7421667575836182 1.886268973350525 3.6284356117248535
Loss :  1.7480111122131348 2.189117193222046 3.9371283054351807
Loss :  1.7473646402359009 3.637673854827881 5.385038375854492
Loss :  1.7454452514648438 2.231564998626709 3.9770102500915527
Loss :  1.7432233095169067 2.0035083293914795 3.746731758117676
Loss :  1.744585394859314 2.0034830570220947 3.748068332672119
Loss :  1.744926929473877 2.3894426822662354 4.134369850158691
  batch 40 loss: 1.744926929473877, 2.3894426822662354, 4.134369850158691
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7440546751022339 3.568617582321167 5.312672138214111
Loss :  1.7437695264816284 2.083575963973999 3.827345371246338
Loss :  1.7445586919784546 2.0239710807800293 3.7685298919677734
Loss :  1.7443701028823853 2.4508392810821533 4.195209503173828
Loss :  1.7457576990127563 1.4655840396881104 3.2113418579101562
Loss :  1.7427408695220947 1.6005483865737915 3.343289375305176
Loss :  1.7431390285491943 1.600035548210144 3.343174457550049
Loss :  1.74405837059021 2.0481603145599365 3.7922186851501465
Loss :  1.7447330951690674 2.9391157627105713 4.683848857879639
Loss :  1.7433165311813354 3.6356875896453857 5.379004001617432
Loss :  1.7405017614364624 2.2999491691589355 4.0404510498046875
Loss :  1.7435636520385742 2.278341770172119 4.021905422210693
Loss :  1.7481460571289062 3.1648786067962646 4.91302490234375
Loss :  1.7430044412612915 2.7106518745422363 4.453656196594238
Loss :  1.7443290948867798 3.3439717292785645 5.088300704956055
Loss :  1.7428017854690552 2.412320375442505 4.15512228012085
Loss :  1.746091604232788 1.9815956354141235 3.727687358856201
Loss :  1.7483106851577759 2.151843309402466 3.9001541137695312
Loss :  1.7481729984283447 2.1963508129119873 3.944523811340332
Loss :  1.745275855064392 1.6481937170028687 3.3934695720672607
  batch 60 loss: 1.745275855064392, 1.6481937170028687, 3.3934695720672607
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7450443506240845 1.858905553817749 3.603950023651123
Loss :  1.7465741634368896 2.4500086307525635 4.196582794189453
Loss :  1.745809555053711 1.8615511655807495 3.60736083984375
Loss :  1.7425206899642944 2.634798526763916 4.3773193359375
Loss :  1.7414189577102661 2.4041683673858643 4.14558744430542
Loss :  1.8016266822814941 4.131829738616943 5.9334564208984375
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.7977972030639648 4.163113594055176 5.960910797119141
Loss :  1.7989369630813599 3.9806487560272217 5.779585838317871
Loss :  1.8026865720748901 3.995258331298828 5.797945022583008
Total LOSS train 4.037097688821646 valid 5.867974519729614
CE LOSS train 1.7462413952900813 valid 0.45067164301872253
Contrastive LOSS train 2.2908562678557174 valid 0.998814582824707
EPOCH 255:
Loss :  1.745154857635498 1.6405073404312134 3.385662078857422
Loss :  1.7435052394866943 2.2037160396575928 3.947221279144287
Loss :  1.7415422201156616 1.745118498802185 3.4866607189178467
Loss :  1.7450273036956787 3.1150357723236084 4.860063076019287
Loss :  1.7457116842269897 3.0501480102539062 4.7958598136901855
Loss :  1.7449264526367188 2.515022039413452 4.25994873046875
Loss :  1.7455228567123413 3.0237350463867188 4.76925802230835
Loss :  1.7432997226715088 3.2514877319335938 4.994787216186523
Loss :  1.7444921731948853 2.939399242401123 4.683891296386719
Loss :  1.7361390590667725 2.254169225692749 3.9903082847595215
Loss :  1.7447651624679565 2.1126179695129395 3.8573832511901855
Loss :  1.7508889436721802 1.8570563793182373 3.607945442199707
Loss :  1.7466086149215698 1.5736122131347656 3.320220947265625
Loss :  1.7445781230926514 2.0828499794006348 3.827428102493286
Loss :  1.7492704391479492 2.4212327003479004 4.17050313949585
Loss :  1.7436810731887817 1.9240697622299194 3.667750835418701
Loss :  1.7475463151931763 2.278979539871216 4.026525974273682
Loss :  1.7451534271240234 2.8294012546539307 4.574554443359375
Loss :  1.7444183826446533 1.8890084028244019 3.6334266662597656
Loss :  1.7445682287216187 1.6622165441513062 3.406784772872925
  batch 20 loss: 1.7445682287216187, 1.6622165441513062, 3.406784772872925
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7449685335159302 2.2024283409118652 3.947396755218506
Loss :  1.7450459003448486 2.063042402267456 3.8080883026123047
Loss :  1.7464723587036133 1.9144222736358643 3.6608946323394775
Loss :  1.7488141059875488 2.1720118522644043 3.920825958251953
Loss :  1.7451473474502563 2.2436015605926514 3.9887490272521973
Loss :  1.7413495779037476 2.341594696044922 4.082944393157959
Loss :  1.7474020719528198 2.0857365131378174 3.8331384658813477
Loss :  1.7408815622329712 2.087963104248047 3.8288445472717285
Loss :  1.74579656124115 2.2117908000946045 3.957587242126465
Loss :  1.740490436553955 2.234368324279785 3.9748587608337402
Loss :  1.7512402534484863 2.1731374263763428 3.924377679824829
Loss :  1.7461425065994263 2.378751277923584 4.124893665313721
Loss :  1.7419089078903198 2.2889370918273926 4.030846118927002
Loss :  1.741696834564209 2.177337169647217 3.919034004211426
Loss :  1.7479925155639648 3.79887056350708 5.546863079071045
Loss :  1.7472360134124756 2.1677606105804443 3.91499662399292
Loss :  1.745051383972168 2.8522789478302 4.597330093383789
Loss :  1.7419865131378174 2.7378885746002197 4.479875087738037
Loss :  1.7444274425506592 2.3851864337921143 4.129613876342773
Loss :  1.7441821098327637 2.7196044921875 4.463786602020264
  batch 40 loss: 1.7441821098327637, 2.7196044921875, 4.463786602020264
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7435853481292725 2.5178561210632324 4.261441230773926
Loss :  1.74421226978302 1.6862201690673828 3.4304323196411133
Loss :  1.7446959018707275 1.8432971239089966 3.5879931449890137
Loss :  1.7437630891799927 2.0821661949157715 3.8259291648864746
Loss :  1.745376467704773 2.3672616481781006 4.112637996673584
Loss :  1.7427246570587158 2.4556777477264404 4.198402404785156
Loss :  1.7430182695388794 2.5317299365997314 4.2747483253479
Loss :  1.744020700454712 3.6450321674346924 5.389052867889404
Loss :  1.7449309825897217 2.2611470222473145 4.006077766418457
Loss :  1.7431731224060059 2.489835023880005 4.23300838470459
Loss :  1.740269660949707 2.1529502868652344 3.8932199478149414
Loss :  1.743288278579712 2.0939364433288574 3.8372247219085693
Loss :  1.7475589513778687 1.8737719058990479 3.621330738067627
Loss :  1.742277979850769 2.8136179447174072 4.555895805358887
Loss :  1.7440075874328613 2.4722166061401367 4.216224193572998
Loss :  1.7421331405639648 2.2459874153137207 3.9881205558776855
Loss :  1.74591064453125 2.7366180419921875 4.4825286865234375
Loss :  1.7482863664627075 2.0181591510772705 3.7664456367492676
Loss :  1.7482545375823975 2.945814609527588 4.694068908691406
Loss :  1.7456010580062866 2.271568775177002 4.017169952392578
  batch 60 loss: 1.7456010580062866, 2.271568775177002, 4.017169952392578
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.745446801185608 2.690481662750244 4.4359283447265625
Loss :  1.7467759847640991 2.804042100906372 4.550817966461182
Loss :  1.7466719150543213 2.8527400493621826 4.599411964416504
Loss :  1.7434942722320557 2.5344908237457275 4.277985095977783
Loss :  1.7421197891235352 2.621692180633545 4.36381196975708
Loss :  1.7952769994735718 4.320464611053467 6.115741729736328
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7919141054153442 4.363585472106934 6.155499458312988
Loss :  1.7930445671081543 4.299981117248535 6.0930256843566895
Loss :  1.797125220298767 4.082346439361572 5.879471778869629
Total LOSS train 4.123400570796086 valid 6.060934662818909
CE LOSS train 1.7447173998906063 valid 0.4492813050746918
Contrastive LOSS train 2.3786831892453706 valid 1.020586609840393
EPOCH 256:
Loss :  1.7452011108398438 1.6648857593536377 3.4100868701934814
Loss :  1.7441335916519165 2.61141037940979 4.355544090270996
Loss :  1.7420507669448853 2.707770824432373 4.449821472167969
Loss :  1.7444483041763306 2.7230257987976074 4.467473983764648
Loss :  1.746470332145691 1.6835312843322754 3.430001735687256
Loss :  1.7444530725479126 1.883324146270752 3.627777099609375
Loss :  1.745523452758789 2.4993820190429688 4.244905471801758
Loss :  1.7425332069396973 2.067460775375366 3.8099939823150635
Loss :  1.7434461116790771 1.4294284582138062 3.1728744506835938
Loss :  1.7353914976119995 1.6195316314697266 3.3549232482910156
Loss :  1.7437076568603516 1.9779155254364014 3.721623182296753
Loss :  1.7496806383132935 2.0343546867370605 3.7840352058410645
Loss :  1.7450217008590698 1.8967936038970947 3.641815185546875
Loss :  1.7420995235443115 2.7767813205718994 4.518880844116211
Loss :  1.746118426322937 2.270203113555908 4.016321659088135
Loss :  1.740858554840088 2.978339433670044 4.719198226928711
Loss :  1.7442692518234253 1.7194494009017944 3.4637186527252197
Loss :  1.7416449785232544 2.3730199337005615 4.1146650314331055
Loss :  1.742782473564148 1.4435721635818481 3.186354637145996
Loss :  1.7402079105377197 1.9449137449264526 3.685121536254883
  batch 20 loss: 1.7402079105377197, 1.9449137449264526, 3.685121536254883
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7413674592971802 2.3999390602111816 4.141306400299072
Loss :  1.7429640293121338 1.9218021631240845 3.664766311645508
Loss :  1.743617057800293 2.5087058544158936 4.252323150634766
Loss :  1.7474851608276367 2.64353609085083 4.391021251678467
Loss :  1.7445861101150513 2.912423849105835 4.657010078430176
Loss :  1.7395294904708862 2.276336431503296 4.015865802764893
Loss :  1.7454158067703247 2.00313663482666 3.7485523223876953
Loss :  1.7374701499938965 1.7200523614883423 3.457522392272949
Loss :  1.7433650493621826 3.136711359024048 4.8800764083862305
Loss :  1.738418459892273 3.0956146717071533 4.834033012390137
Loss :  1.750264048576355 2.639643669128418 4.3899078369140625
Loss :  1.7433329820632935 2.0263357162475586 3.7696685791015625
Loss :  1.7383966445922852 2.1611452102661133 3.8995418548583984
Loss :  1.7390738725662231 3.670427083969116 5.409501075744629
Loss :  1.746289849281311 3.632582187652588 5.378871917724609
Loss :  1.7452890872955322 2.055968761444092 3.801257848739624
Loss :  1.7424798011779785 2.2825028896331787 4.024982452392578
Loss :  1.7375022172927856 1.9970959424972534 3.734598159790039
Loss :  1.742264747619629 2.0432322025299072 3.785496950149536
Loss :  1.7427711486816406 1.8208181858062744 3.563589334487915
  batch 40 loss: 1.7427711486816406, 1.8208181858062744, 3.563589334487915
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7415097951889038 2.6455373764038086 4.387047290802002
Loss :  1.7424135208129883 1.9137650728225708 3.6561784744262695
Loss :  1.744238018989563 3.609562873840332 5.3538007736206055
Loss :  1.742675542831421 1.9391597509384155 3.681835174560547
Loss :  1.7449272871017456 1.781311273574829 3.526238441467285
Loss :  1.741934061050415 2.0081777572631836 3.7501118183135986
Loss :  1.7415168285369873 2.897361993789673 4.63887882232666
Loss :  1.742961049079895 3.257967233657837 5.0009284019470215
Loss :  1.7437859773635864 3.612809419631958 5.356595516204834
Loss :  1.7425025701522827 3.0983974933624268 4.84089994430542
Loss :  1.7399580478668213 3.2121763229370117 4.952134132385254
Loss :  1.7428938150405884 3.0535504817962646 4.796444416046143
Loss :  1.747319221496582 2.340095281600952 4.087414741516113
Loss :  1.741988182067871 2.3385095596313477 4.080497741699219
Loss :  1.7435481548309326 2.23793363571167 3.9814817905426025
Loss :  1.7417036294937134 2.1023542881011963 3.844058036804199
Loss :  1.745270848274231 2.0454721450805664 3.790742874145508
Loss :  1.7475998401641846 1.9143356084823608 3.661935329437256
Loss :  1.7474327087402344 2.2542388439178467 4.00167179107666
Loss :  1.7451294660568237 1.79550039768219 3.5406298637390137
  batch 60 loss: 1.7451294660568237, 1.79550039768219, 3.5406298637390137
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7448198795318604 2.074084758758545 3.8189046382904053
Loss :  1.7460497617721558 2.119117259979248 3.8651671409606934
Loss :  1.7458919286727905 3.173293352127075 4.919185161590576
Loss :  1.7425143718719482 2.87477970123291 4.6172943115234375
Loss :  1.740931749343872 1.4572559595108032 3.198187828063965
Loss :  1.793339729309082 4.274420261383057 6.067759990692139
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.790116786956787 4.26497745513916 6.055094242095947
Loss :  1.7909526824951172 4.07568883895874 5.866641521453857
Loss :  1.7951605319976807 4.019283771514893 5.814444541931152
Total LOSS train 4.0972814486576965 valid 5.950985074043274
CE LOSS train 1.743283722950862 valid 0.44879013299942017
Contrastive LOSS train 2.353997725706834 valid 1.0048209428787231
EPOCH 257:
Loss :  1.7440595626831055 1.8236315250396729 3.5676910877227783
Loss :  1.7430150508880615 2.397878408432007 4.140893459320068
Loss :  1.7401615381240845 1.6904891729354858 3.4306507110595703
Loss :  1.7432068586349487 2.0213468074798584 3.7645535469055176
Loss :  1.7449142932891846 2.4929115772247314 4.237825870513916
Loss :  1.7440457344055176 2.593151092529297 4.3371968269348145
Loss :  1.743803858757019 2.8386642932891846 4.582468032836914
Loss :  1.7414627075195312 1.6170382499694824 3.3585009574890137
Loss :  1.7422460317611694 1.7856388092041016 3.5278849601745605
Loss :  1.73330557346344 1.639193058013916 3.3724985122680664
Loss :  1.74276602268219 2.075809955596924 3.818575859069824
Loss :  1.7503912448883057 2.17454195022583 3.9249331951141357
Loss :  1.744138479232788 1.8952038288116455 3.6393423080444336
Loss :  1.7417011260986328 2.1208269596099854 3.862528085708618
Loss :  1.7459629774093628 2.0880696773529053 3.8340325355529785
Loss :  1.7404439449310303 1.6604498624801636 3.4008936882019043
Loss :  1.7453768253326416 1.8045886754989624 3.5499653816223145
Loss :  1.7417702674865723 1.779313564300537 3.5210838317871094
Loss :  1.743436336517334 3.359851121902466 5.103287696838379
Loss :  1.7413544654846191 2.9024229049682617 4.643777370452881
  batch 20 loss: 1.7413544654846191, 2.9024229049682617, 4.643777370452881
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7419897317886353 3.22586727142334 4.9678568840026855
Loss :  1.7445461750030518 2.9736554622650146 4.718201637268066
Loss :  1.7434548139572144 1.685381531715393 3.4288363456726074
Loss :  1.7485462427139282 1.7988642454147339 3.547410488128662
Loss :  1.7440800666809082 2.746589183807373 4.490669250488281
Loss :  1.7399994134902954 1.7432639598846436 3.4832634925842285
Loss :  1.7465814352035522 1.9042516946792603 3.6508331298828125
Loss :  1.740257740020752 2.8524248600006104 4.592682838439941
Loss :  1.7456986904144287 2.334132432937622 4.079831123352051
Loss :  1.7410564422607422 3.177823066711426 4.918879508972168
Loss :  1.7513467073440552 3.1183743476867676 4.869720935821533
Loss :  1.7464356422424316 2.243574619293213 3.9900102615356445
Loss :  1.7420341968536377 1.8805125951766968 3.622546672821045
Loss :  1.7423125505447388 1.9689289331436157 3.7112414836883545
Loss :  1.7481586933135986 2.4214539527893066 4.169612884521484
Loss :  1.7483251094818115 2.254173755645752 4.002498626708984
Loss :  1.7461659908294678 1.9065488576889038 3.652714729309082
Loss :  1.7436362504959106 1.9387775659561157 3.6824138164520264
Loss :  1.745266079902649 2.33804988861084 4.083315849304199
Loss :  1.7462960481643677 1.9770245552062988 3.723320484161377
  batch 40 loss: 1.7462960481643677, 1.9770245552062988, 3.723320484161377
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.74508535861969 1.889359712600708 3.6344451904296875
Loss :  1.7448632717132568 1.6936322450637817 3.438495635986328
Loss :  1.7460410594940186 1.8970963954925537 3.6431374549865723
Loss :  1.7447106838226318 2.0969350337982178 3.8416457176208496
Loss :  1.747073769569397 2.2528364658355713 3.999910354614258
Loss :  1.7452844381332397 1.817771553993225 3.563055992126465
Loss :  1.7449588775634766 1.73688805103302 3.481846809387207
Loss :  1.7451837062835693 2.5088117122650146 4.253995418548584
Loss :  1.7472344636917114 2.893108367919922 4.640342712402344
Loss :  1.744104266166687 1.760204553604126 3.5043087005615234
Loss :  1.7420291900634766 2.179875373840332 3.9219045639038086
Loss :  1.744516372680664 1.615604043006897 3.3601202964782715
Loss :  1.7482300996780396 2.485684871673584 4.233914852142334
Loss :  1.7426881790161133 1.704017162322998 3.4467053413391113
Loss :  1.7446742057800293 2.0563974380493164 3.8010716438293457
Loss :  1.7427539825439453 2.250361919403076 3.9931159019470215
Loss :  1.7465600967407227 2.358767032623291 4.105327129364014
Loss :  1.7487528324127197 2.1626827716827393 3.911435604095459
Loss :  1.7486560344696045 2.705268621444702 4.453924655914307
Loss :  1.7459993362426758 2.215043306350708 3.961042642593384
  batch 60 loss: 1.7459993362426758, 2.215043306350708, 3.961042642593384
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7459765672683716 2.0410959720611572 3.7870726585388184
Loss :  1.7473640441894531 1.88674795627594 3.6341118812561035
Loss :  1.7466403245925903 1.4686577320098877 3.2152981758117676
Loss :  1.7438486814498901 2.155484914779663 3.8993334770202637
Loss :  1.7426576614379883 1.6172603368759155 3.3599181175231934
Loss :  1.8197095394134521 4.404113292694092 6.223822593688965
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.8147810697555542 4.38568639755249 6.200467586517334
Loss :  1.8161616325378418 4.305849552154541 6.122011184692383
Loss :  1.8192511796951294 4.0926032066345215 5.911854267120361
Total LOSS train 3.909075773679293 valid 6.114538908004761
CE LOSS train 1.7444559757526104 valid 0.45481279492378235
Contrastive LOSS train 2.164619812598595 valid 1.0231508016586304
EPOCH 258:
Loss :  1.7456103563308716 2.1358301639556885 3.8814406394958496
Loss :  1.7441495656967163 1.9635900259017944 3.7077395915985107
Loss :  1.7420971393585205 1.6897833347320557 3.431880474090576
Loss :  1.7441827058792114 1.7664339542388916 3.5106167793273926
Loss :  1.7463210821151733 2.2891602516174316 4.0354814529418945
Loss :  1.744537591934204 1.7593127489089966 3.5038504600524902
Loss :  1.7461186647415161 2.0337750911712646 3.7798938751220703
Loss :  1.743626594543457 1.7804497480392456 3.524076461791992
Loss :  1.7448787689208984 2.3945517539978027 4.139430522918701
Loss :  1.7366113662719727 1.6261135339736938 3.362724781036377
Loss :  1.7450765371322632 2.4966583251953125 4.241734981536865
Loss :  1.750596284866333 2.2067272663116455 3.9573235511779785
Loss :  1.7460699081420898 2.256564140319824 4.002634048461914
Loss :  1.7432883977890015 2.7094197273254395 4.4527082443237305
Loss :  1.7470426559448242 2.1613383293151855 3.9083809852600098
Loss :  1.7421237230300903 2.0869863033294678 3.8291101455688477
Loss :  1.7458692789077759 1.6764638423919678 3.422333240509033
Loss :  1.7437349557876587 1.6573814153671265 3.401116371154785
Loss :  1.7435400485992432 1.8112983703613281 3.5548384189605713
Loss :  1.7427932024002075 1.8810213804244995 3.623814582824707
  batch 20 loss: 1.7427932024002075, 1.8810213804244995, 3.623814582824707
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.743822693824768 1.7522534132003784 3.4960761070251465
Loss :  1.7445422410964966 1.8723881244659424 3.6169304847717285
Loss :  1.7458157539367676 1.4043408632278442 3.1501564979553223
Loss :  1.7486367225646973 1.509195327758789 3.2578320503234863
Loss :  1.7444089651107788 2.071669578552246 3.8160786628723145
Loss :  1.7413749694824219 2.4965224266052246 4.2378973960876465
Loss :  1.747576355934143 2.4070231914520264 4.154599666595459
Loss :  1.7410602569580078 2.8464226722717285 4.587482929229736
Loss :  1.7459652423858643 2.1500484943389893 3.8960137367248535
Loss :  1.739418864250183 2.1195871829986572 3.859005928039551
Loss :  1.750496745109558 2.5001933574676514 4.25068998336792
Loss :  1.7460941076278687 2.3276774883270264 4.0737714767456055
Loss :  1.7408056259155273 2.0175347328186035 3.758340358734131
Loss :  1.7410459518432617 2.0469954013824463 3.788041353225708
Loss :  1.7467930316925049 2.329524278640747 4.076317310333252
Loss :  1.7468209266662598 2.765103340148926 4.5119242668151855
Loss :  1.7444889545440674 2.8413796424865723 4.585868835449219
Loss :  1.741689920425415 2.8143362998962402 4.556026458740234
Loss :  1.743472933769226 2.5989458560943604 4.342418670654297
Loss :  1.7441068887710571 1.9194802045822144 3.6635870933532715
  batch 40 loss: 1.7441068887710571, 1.9194802045822144, 3.6635870933532715
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.742871880531311 2.4640040397644043 4.206875801086426
Loss :  1.743186593055725 3.6237943172454834 5.366981029510498
Loss :  1.743794560432434 2.0024869441986084 3.746281623840332
Loss :  1.743835687637329 1.852231740951538 3.596067428588867
Loss :  1.7455158233642578 1.7838689088821411 3.5293846130371094
Loss :  1.7427338361740112 2.0198898315429688 3.7626237869262695
Loss :  1.7432500123977661 2.5399088859558105 4.283158779144287
Loss :  1.7440319061279297 1.9606693983078003 3.7047014236450195
Loss :  1.745875358581543 2.0720818042755127 3.8179571628570557
Loss :  1.743879795074463 1.7372286319732666 3.4811084270477295
Loss :  1.7413709163665771 2.006234884262085 3.747605800628662
Loss :  1.7446595430374146 2.5770795345306396 4.321739196777344
Loss :  1.7487425804138184 1.695649266242981 3.4443917274475098
Loss :  1.7428971529006958 1.97312593460083 3.7160229682922363
Loss :  1.744408130645752 1.7983264923095703 3.5427346229553223
Loss :  1.742396354675293 1.8898026943206787 3.6321990489959717
Loss :  1.7457693815231323 1.8141131401062012 3.559882640838623
Loss :  1.7479099035263062 2.148547649383545 3.8964576721191406
Loss :  1.7480872869491577 2.34340238571167 4.091489791870117
Loss :  1.7456390857696533 2.219773292541504 3.9654123783111572
  batch 60 loss: 1.7456390857696533, 2.219773292541504, 3.9654123783111572
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7457295656204224 2.152818441390991 3.898548126220703
Loss :  1.747102975845337 1.283363699913025 3.0304665565490723
Loss :  1.7469868659973145 1.5595221519470215 3.306509017944336
Loss :  1.7439335584640503 2.372711420059204 4.116644859313965
Loss :  1.7425909042358398 1.849475622177124 3.592066526412964
Loss :  1.7953306436538696 4.2888922691345215 6.084222793579102
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.792311429977417 4.2904157638549805 6.082727432250977
Loss :  1.793200969696045 4.137084007263184 5.9302849769592285
Loss :  1.7967267036437988 4.18880033493042 5.985527038574219
Total LOSS train 3.850884613623986 valid 6.020690560340881
CE LOSS train 1.7445216252253606 valid 0.4491816759109497
Contrastive LOSS train 2.106362964556767 valid 1.047200083732605
EPOCH 259:
Loss :  1.7458854913711548 2.1966474056243896 3.942533016204834
Loss :  1.7449971437454224 2.4300642013549805 4.175061225891113
Loss :  1.7420787811279297 1.5115094184875488 3.2535881996154785
Loss :  1.7440211772918701 1.4332464933395386 3.177267551422119
Loss :  1.7464429140090942 2.1178557872772217 3.8642988204956055
Loss :  1.744455337524414 2.0298688411712646 3.7743241786956787
Loss :  1.745986819267273 1.9048444032669067 3.6508312225341797
Loss :  1.7433863878250122 1.6911052465438843 3.4344916343688965
Loss :  1.7444216012954712 1.82062566280365 3.565047264099121
Loss :  1.7359906435012817 1.4674148559570312 3.2034053802490234
Loss :  1.74466872215271 2.4925613403320312 4.23723030090332
Loss :  1.750686526298523 2.04571795463562 3.7964043617248535
Loss :  1.7457026243209839 1.4497904777526855 3.195493221282959
Loss :  1.7432767152786255 2.0681803226470947 3.8114571571350098
Loss :  1.7472529411315918 2.0402655601501465 3.7875185012817383
Loss :  1.7419425249099731 1.9802498817443848 3.7221922874450684
Loss :  1.7454686164855957 1.8484759330749512 3.593944549560547
Loss :  1.7428895235061646 1.9254059791564941 3.668295383453369
Loss :  1.7429646253585815 1.37088942527771 3.113853931427002
Loss :  1.7418256998062134 1.7094249725341797 3.4512505531311035
  batch 20 loss: 1.7418256998062134, 1.7094249725341797, 3.4512505531311035
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7428661584854126 1.7001088857650757 3.4429750442504883
Loss :  1.7438918352127075 2.0160508155822754 3.7599425315856934
Loss :  1.7446411848068237 1.828715205192566 3.5733563899993896
Loss :  1.7482095956802368 2.071162462234497 3.8193721771240234
Loss :  1.7442704439163208 2.297075033187866 4.041345596313477
Loss :  1.740836501121521 1.9252346754074097 3.6660711765289307
Loss :  1.747153878211975 2.0972278118133545 3.844381809234619
Loss :  1.7406725883483887 2.0447020530700684 3.785374641418457
Loss :  1.74604070186615 1.962454080581665 3.7084946632385254
Loss :  1.7397524118423462 2.1490142345428467 3.8887667655944824
Loss :  1.750935673713684 2.0819039344787598 3.8328394889831543
Loss :  1.745976448059082 2.688500165939331 4.434476852416992
Loss :  1.741117238998413 1.7644379138946533 3.5055551528930664
Loss :  1.7410402297973633 2.128413677215576 3.8694539070129395
Loss :  1.747025489807129 2.647616147994995 4.394641876220703
Loss :  1.7471359968185425 2.6263575553894043 4.373493671417236
Loss :  1.7447553873062134 2.151711940765381 3.8964672088623047
Loss :  1.7418570518493652 2.0825693607330322 3.8244264125823975
Loss :  1.7435916662216187 2.246631145477295 3.990222930908203
Loss :  1.7446632385253906 1.6997723579406738 3.4444355964660645
  batch 40 loss: 1.7446632385253906, 1.6997723579406738, 3.4444355964660645
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7429211139678955 2.203850746154785 3.9467718601226807
Loss :  1.7426060438156128 1.570942759513855 3.3135488033294678
Loss :  1.7445578575134277 1.763184666633606 3.507742404937744
Loss :  1.7433894872665405 2.051856279373169 3.79524564743042
Loss :  1.7463815212249756 1.8832056522369385 3.629587173461914
Loss :  1.7446398735046387 2.2856898307800293 4.030329704284668
Loss :  1.7438232898712158 2.1890554428100586 3.9328787326812744
Loss :  1.745269536972046 2.129894495010376 3.875164031982422
Loss :  1.7473524808883667 2.484143018722534 4.231495380401611
Loss :  1.7448545694351196 1.9582966566085815 3.703151226043701
Loss :  1.743227243423462 1.9931533336639404 3.7363805770874023
Loss :  1.7451837062835693 2.1663949489593506 3.91157865524292
Loss :  1.7481708526611328 1.5828219652175903 3.3309926986694336
Loss :  1.7433799505233765 1.9116806983947754 3.6550607681274414
Loss :  1.7451744079589844 2.359365701675415 4.10453987121582
Loss :  1.7424296140670776 2.699540376663208 4.441969871520996
Loss :  1.745898962020874 2.4794681072235107 4.225367069244385
Loss :  1.7478644847869873 1.6233471632003784 3.371211528778076
Loss :  1.7482529878616333 2.725663900375366 4.473917007446289
Loss :  1.7458709478378296 1.6521869897842407 3.3980579376220703
  batch 60 loss: 1.7458709478378296, 1.6521869897842407, 3.3980579376220703
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.74545419216156 2.817783832550049 4.563238143920898
Loss :  1.7471574544906616 2.5680761337280273 4.3152337074279785
Loss :  1.7466554641723633 1.885129451751709 3.6317849159240723
Loss :  1.743412971496582 2.3469362258911133 4.090349197387695
Loss :  1.7423269748687744 1.4140068292617798 3.1563339233398438
Loss :  1.8062328100204468 4.178151607513428 5.984384536743164
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.803053379058838 4.1584906578063965 5.961544036865234
Loss :  1.8032312393188477 3.960125207901001 5.7633562088012695
Loss :  1.8078001737594604 3.932194232940674 5.739994525909424
Total LOSS train 3.7828694380246675 valid 5.862319827079773
CE LOSS train 1.7445697619364813 valid 0.4519500434398651
Contrastive LOSS train 2.0382996742541972 valid 0.9830485582351685
EPOCH 260:
Loss :  1.7457561492919922 1.7976933717727661 3.5434494018554688
Loss :  1.7447298765182495 2.7950618267059326 4.539791584014893
Loss :  1.742365837097168 2.201881170272827 3.944247007369995
Loss :  1.7446731328964233 1.8682464361190796 3.612919569015503
Loss :  1.7470132112503052 2.295121669769287 4.042134761810303
Loss :  1.7449119091033936 2.063664436340332 3.8085763454437256
Loss :  1.7465565204620361 2.89400315284729 4.640559673309326
Loss :  1.743588924407959 2.918823719024658 4.662412643432617
Loss :  1.7446426153182983 2.0528485774993896 3.7974910736083984
Loss :  1.7368190288543701 1.889706015586853 3.6265249252319336
Loss :  1.7449042797088623 3.2223868370056152 4.967290878295898
Loss :  1.7502597570419312 2.837021827697754 4.587281703948975
Loss :  1.7463797330856323 3.289384365081787 5.035764217376709
Loss :  1.7437885999679565 1.9374374151229858 3.6812260150909424
Loss :  1.747641682624817 2.448974370956421 4.196616172790527
Loss :  1.7429156303405762 2.193720579147339 3.936636209487915
Loss :  1.7460711002349854 2.9040603637695312 4.6501312255859375
Loss :  1.7444226741790771 2.5254364013671875 4.269859313964844
Loss :  1.7434241771697998 2.907578706741333 4.651002883911133
Loss :  1.7427420616149902 2.589062213897705 4.331804275512695
  batch 20 loss: 1.7427420616149902, 2.589062213897705, 4.331804275512695
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7438993453979492 3.994353771209717 5.738253116607666
Loss :  1.7442302703857422 2.297346830368042 4.041577339172363
Loss :  1.7461540699005127 2.8828506469726562 4.62900447845459
Loss :  1.7489053010940552 2.1463820934295654 3.89528751373291
Loss :  1.7452893257141113 2.2914586067199707 4.036747932434082
Loss :  1.7424060106277466 2.4533536434173584 4.1957597732543945
Loss :  1.748404860496521 2.6190850734710693 4.367489814758301
Loss :  1.742296814918518 2.6175179481506348 4.359814643859863
Loss :  1.7470935583114624 2.9797351360321045 4.726828575134277
Loss :  1.7401609420776367 3.157925844192505 4.8980865478515625
Loss :  1.7507967948913574 3.3120768070220947 5.062873840332031
Loss :  1.7467398643493652 3.20396089553833 4.950700759887695
Loss :  1.7415465116500854 2.6074981689453125 4.3490447998046875
Loss :  1.7415841817855835 1.6895900964736938 3.4311742782592773
Loss :  1.747115969657898 2.5814292430877686 4.328545093536377
Loss :  1.7473007440567017 1.8648847341537476 3.612185478210449
Loss :  1.745180368423462 2.490278959274292 4.235459327697754
Loss :  1.7428650856018066 3.0607340335845947 4.8035993576049805
Loss :  1.7449039220809937 1.6170316934585571 3.361935615539551
Loss :  1.7457391023635864 2.3504509925842285 4.096189975738525
  batch 40 loss: 1.7457391023635864, 2.3504509925842285, 4.096189975738525
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7440526485443115 2.7495572566986084 4.49360990524292
Loss :  1.7443044185638428 1.9486545324325562 3.6929588317871094
Loss :  1.7452619075775146 2.2566404342651367 4.0019025802612305
Loss :  1.744035243988037 1.933590292930603 3.6776256561279297
Loss :  1.7462624311447144 1.8097978830337524 3.556060314178467
Loss :  1.7436354160308838 2.1939480304718018 3.9375834465026855
Loss :  1.7434022426605225 1.7449933290481567 3.4883956909179688
Loss :  1.7442734241485596 2.0996015071868896 3.843874931335449
Loss :  1.7459367513656616 1.9720791578292847 3.7180159091949463
Loss :  1.7433103322982788 1.9132667779922485 3.6565771102905273
Loss :  1.7406737804412842 2.317565679550171 4.058239459991455
Loss :  1.7439149618148804 2.4163975715637207 4.160312652587891
Loss :  1.747767686843872 2.5145747661590576 4.26234245300293
Loss :  1.7419030666351318 2.775707244873047 4.517610549926758
Loss :  1.7430676221847534 2.513791084289551 4.256858825683594
Loss :  1.7413173913955688 2.8775484561920166 4.618865966796875
Loss :  1.744996190071106 2.252214193344116 3.9972105026245117
Loss :  1.7474141120910645 2.8467888832092285 4.594202995300293
Loss :  1.7474325895309448 2.621509313583374 4.368941783905029
Loss :  1.7448945045471191 2.09383487701416 3.8387293815612793
  batch 60 loss: 1.7448945045471191, 2.09383487701416, 3.8387293815612793
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7448548078536987 2.2298338413238525 3.9746885299682617
Loss :  1.7464677095413208 2.4176647663116455 4.164132595062256
Loss :  1.7471610307693481 2.0028717517852783 3.750032901763916
Loss :  1.7438304424285889 2.901960849761963 4.645791053771973
Loss :  1.7429087162017822 1.669954776763916 3.4128634929656982
Loss :  1.7969589233398438 4.379139423370361 6.176098346710205
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.7935510873794556 4.431474685668945 6.225025653839111
Loss :  1.7945996522903442 4.289622783660889 6.084222316741943
Loss :  1.7988059520721436 4.142693996429443 5.941499710083008
Total LOSS train 4.189749317902785 valid 6.106711506843567
CE LOSS train 1.7447584519019494 valid 0.4497014880180359
Contrastive LOSS train 2.4449908604988684 valid 1.0356734991073608
EPOCH 261:
Loss :  1.7463853359222412 2.0724024772644043 3.8187878131866455
Loss :  1.7458986043930054 2.533959150314331 4.279857635498047
Loss :  1.7434279918670654 2.739528179168701 4.4829559326171875
Loss :  1.7456079721450806 2.7269718647003174 4.4725799560546875
Loss :  1.7478119134902954 1.9881147146224976 3.735926628112793
Loss :  1.745728611946106 2.0794172286987305 3.825145721435547
Loss :  1.7477768659591675 2.2064921855926514 3.9542689323425293
Loss :  1.744974136352539 2.360755205154419 4.105729103088379
Loss :  1.7455250024795532 1.8944265842437744 3.639951705932617
Loss :  1.7383966445922852 2.0656254291534424 3.8040220737457275
Loss :  1.746360421180725 2.166041612625122 3.9124021530151367
Loss :  1.7509387731552124 2.3571178913116455 4.108056545257568
Loss :  1.7476023435592651 2.269479513168335 4.0170817375183105
Loss :  1.7447563409805298 2.299807071685791 4.044563293457031
Loss :  1.7484524250030518 2.272643566131592 4.021096229553223
Loss :  1.7440606355667114 1.5677696466445923 3.3118302822113037
Loss :  1.7471356391906738 1.8008698225021362 3.5480055809020996
Loss :  1.7451484203338623 1.875403642654419 3.6205520629882812
Loss :  1.7447545528411865 1.8657139539718628 3.6104683876037598
Loss :  1.7437384128570557 2.04656982421875 3.7903082370758057
  batch 20 loss: 1.7437384128570557, 2.04656982421875, 3.7903082370758057
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7447161674499512 1.823872447013855 3.5685887336730957
Loss :  1.7454555034637451 1.82114577293396 3.566601276397705
Loss :  1.7466332912445068 1.923555612564087 3.6701889038085938
Loss :  1.7493818998336792 2.036006212234497 3.7853879928588867
Loss :  1.7458312511444092 2.289935827255249 4.035767078399658
Loss :  1.7427136898040771 2.2731521129608154 4.015865802764893
Loss :  1.748311161994934 1.974473476409912 3.7227845191955566
Loss :  1.7416900396347046 2.1404216289520264 3.8821115493774414
Loss :  1.7466298341751099 2.3135838508605957 4.060213565826416
Loss :  1.7460664510726929 2.8133769035339355 4.559443473815918
Loss :  1.7532835006713867 3.433053493499756 5.186336994171143
Loss :  1.7483786344528198 2.5355145931243896 4.28389310836792
Loss :  1.7457085847854614 3.135690927505493 4.881399631500244
Loss :  1.7431672811508179 2.1016428470611572 3.8448100090026855
Loss :  1.7502068281173706 2.3223702907562256 4.072576999664307
Loss :  1.7482857704162598 2.183933973312378 3.9322197437286377
Loss :  1.747118353843689 2.207414388656616 3.9545326232910156
Loss :  1.7445045709609985 2.0911078453063965 3.8356122970581055
Loss :  1.7472481727600098 2.593853235244751 4.34110164642334
Loss :  1.7472470998764038 2.5745718479156494 4.321818828582764
  batch 40 loss: 1.7472470998764038, 2.5745718479156494, 4.321818828582764
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7465571165084839 3.433342933654785 5.179900169372559
Loss :  1.7472262382507324 3.1952199935913086 4.942446231842041
Loss :  1.7477339506149292 2.291895866394043 4.039629936218262
Loss :  1.7490894794464111 2.0098419189453125 3.7589313983917236
Loss :  1.7493853569030762 2.1688435077667236 3.9182288646698
Loss :  1.7461864948272705 2.8756279945373535 4.621814727783203
Loss :  1.7483628988265991 3.167764902114868 4.916127681732178
Loss :  1.747544288635254 2.677952527999878 4.425497055053711
Loss :  1.748681902885437 2.059351682662964 3.8080334663391113
Loss :  1.746576189994812 2.109154462814331 3.8557305335998535
Loss :  1.743620753288269 2.0992298126220703 3.842850685119629
Loss :  1.7465722560882568 2.736250400543213 4.482822418212891
Loss :  1.7512675523757935 2.2987868785858154 4.050054550170898
Loss :  1.7450571060180664 2.879838705062866 4.624896049499512
Loss :  1.745847463607788 2.0804927349090576 3.8263401985168457
Loss :  1.7461800575256348 2.599778175354004 4.345958232879639
Loss :  1.7487292289733887 2.2511305809020996 3.9998598098754883
Loss :  1.750842809677124 1.6207462549209595 3.371589183807373
Loss :  1.7500133514404297 1.9340283870697021 3.684041738510132
Loss :  1.748551607131958 1.7109942436218262 3.459545850753784
  batch 60 loss: 1.748551607131958, 1.7109942436218262, 3.459545850753784
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7480988502502441 1.874101161956787 3.6222000122070312
Loss :  1.7486332654953003 1.7536875009536743 3.5023207664489746
Loss :  1.7489184141159058 2.0804953575134277 3.829413890838623
Loss :  1.7456800937652588 2.2018861770629883 3.947566270828247
Loss :  1.7445706129074097 1.4612265825271606 3.2057971954345703
Loss :  1.7959177494049072 3.727118968963623 5.523036956787109
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7931301593780518 3.8128273487091064 5.605957508087158
Loss :  1.7942728996276855 3.601983070373535 5.396255970001221
Loss :  1.7971019744873047 3.553149700164795 5.3502516746521
Total LOSS train 4.013637564732478 valid 5.468875527381897
CE LOSS train 1.7467228687726535 valid 0.44927549362182617
Contrastive LOSS train 2.266914701461792 valid 0.8882874250411987
EPOCH 262:
Loss :  1.7471109628677368 2.565735340118408 4.3128461837768555
Loss :  1.7453869581222534 2.2496497631073 3.9950366020202637
Loss :  1.7435961961746216 1.8301470279693604 3.5737433433532715
Loss :  1.745427131652832 2.638648748397827 4.384076118469238
Loss :  1.747855544090271 2.7085046768188477 4.456360340118408
Loss :  1.7457941770553589 2.8434641361236572 4.589258193969727
Loss :  1.7473397254943848 3.368557929992676 5.1158976554870605
Loss :  1.7448598146438599 2.373765707015991 4.118625640869141
Loss :  1.745665431022644 2.69351863861084 4.439184188842773
Loss :  1.7374424934387207 1.9860574007034302 3.7234997749328613
Loss :  1.7452956438064575 2.2267537117004395 3.9720492362976074
Loss :  1.7513642311096191 2.419367790222168 4.170732021331787
Loss :  1.7459784746170044 2.3437814712524414 4.089759826660156
Loss :  1.7438971996307373 3.0893425941467285 4.833239555358887
Loss :  1.7474645376205444 1.9343092441558838 3.6817736625671387
Loss :  1.742266058921814 2.164121150970459 3.9063873291015625
Loss :  1.7460130453109741 2.0052008628845215 3.751214027404785
Loss :  1.7436360120773315 2.115335702896118 3.85897159576416
Loss :  1.745482325553894 1.925432562828064 3.670914888381958
Loss :  1.7428513765335083 2.3969507217407227 4.139801979064941
  batch 20 loss: 1.7428513765335083, 2.3969507217407227, 4.139801979064941
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7443466186523438 2.036506175994873 3.780852794647217
Loss :  1.7461943626403809 2.587238311767578 4.333432674407959
Loss :  1.74628484249115 1.618568778038025 3.364853620529175
Loss :  1.7504011392593384 2.1230571269989014 3.8734583854675293
Loss :  1.7463539838790894 2.2139716148376465 3.9603257179260254
Loss :  1.7429677248001099 1.8964124917984009 3.6393802165985107
Loss :  1.7487821578979492 2.086930274963379 3.835712432861328
Loss :  1.7423030138015747 2.0258238315582275 3.768126964569092
Loss :  1.747371792793274 2.4556727409362793 4.203044414520264
Loss :  1.7431727647781372 2.2623629570007324 4.00553560256958
Loss :  1.752459168434143 2.386197805404663 4.138657093048096
Loss :  1.7474533319473267 2.1023519039154053 3.8498053550720215
Loss :  1.7433074712753296 2.2629737854003906 4.00628137588501
Loss :  1.7432446479797363 2.6744184494018555 4.417663097381592
Loss :  1.7487720251083374 3.0833308696746826 4.8321027755737305
Loss :  1.7488276958465576 2.840461015701294 4.589288711547852
Loss :  1.746763825416565 2.5418007373809814 4.288564682006836
Loss :  1.743314266204834 2.4234392642974854 4.166753768920898
Loss :  1.7459948062896729 2.878962755203247 4.62495756149292
Loss :  1.746631383895874 3.497898578643799 5.244529724121094
  batch 40 loss: 1.746631383895874, 3.497898578643799, 5.244529724121094
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7458590269088745 2.624790668487549 4.370649814605713
Loss :  1.7472310066223145 2.2655856609344482 4.012816429138184
Loss :  1.7476272583007812 2.404364824295044 4.151991844177246
Loss :  1.7478433847427368 1.8790274858474731 3.62687087059021
Loss :  1.7489464282989502 1.9205821752548218 3.6695284843444824
Loss :  1.745775818824768 2.666297674179077 4.412073612213135
Loss :  1.7467643022537231 2.058816432952881 3.8055806159973145
Loss :  1.747554063796997 2.087466239929199 3.8350203037261963
Loss :  1.7484241724014282 2.061211109161377 3.8096351623535156
Loss :  1.7467610836029053 2.9755873680114746 4.722348213195801
Loss :  1.7443281412124634 3.1076014041900635 4.851929664611816
Loss :  1.7463353872299194 2.2810962200164795 4.027431488037109
Loss :  1.7508504390716553 2.46439790725708 4.215248107910156
Loss :  1.7456570863723755 2.3991615772247314 4.1448187828063965
Loss :  1.747272253036499 3.132915496826172 4.88018798828125
Loss :  1.7456666231155396 2.354978322982788 4.100645065307617
Loss :  1.7476322650909424 3.061863660812378 4.80949592590332
Loss :  1.7498986721038818 1.7515908479690552 3.5014896392822266
Loss :  1.7498784065246582 2.8746695518493652 4.624547958374023
Loss :  1.7477915287017822 2.1257033348083496 3.873494863510132
  batch 60 loss: 1.7477915287017822, 2.1257033348083496, 3.873494863510132
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.747820496559143 2.269906520843506 4.017726898193359
Loss :  1.7491629123687744 2.7145915031433105 4.463754653930664
Loss :  1.7487457990646362 2.5572903156280518 4.306035995483398
Loss :  1.7461100816726685 3.3570218086242676 5.1031317710876465
Loss :  1.7453083992004395 2.158266544342041 3.9035749435424805
Loss :  1.7987464666366577 4.316573143005371 6.115319728851318
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.796104907989502 4.337057113647461 6.133162021636963
Loss :  1.7970211505889893 4.248773574829102 6.045794486999512
Loss :  1.7978262901306152 4.1457953453063965 5.943621635437012
Total LOSS train 4.168411203531119 valid 6.059474468231201
CE LOSS train 1.7463833753879254 valid 0.4494565725326538
Contrastive LOSS train 2.4220278354791493 valid 1.0364488363265991
EPOCH 263:
Loss :  1.7481714487075806 3.4851126670837402 5.233283996582031
Loss :  1.7468626499176025 2.9060258865356445 4.652888298034668
Loss :  1.7454724311828613 3.35166072845459 5.097133159637451
Loss :  1.748274803161621 3.9261863231658936 5.674461364746094
Loss :  1.7493842840194702 3.852044105529785 5.601428508758545
Loss :  1.7478153705596924 2.09900164604187 3.8468170166015625
Loss :  1.7495037317276 2.80268931388855 4.5521931648254395
Loss :  1.7469820976257324 2.024501085281372 3.7714831829071045
Loss :  1.7476247549057007 1.5729484558105469 3.320573329925537
Loss :  1.740384817123413 1.4492566585540771 3.1896414756774902
Loss :  1.747791051864624 1.822740077972412 3.570531129837036
Loss :  1.75209641456604 1.8579639196395874 3.610060214996338
Loss :  1.74825119972229 1.5707956552505493 3.319046974182129
Loss :  1.7461555004119873 2.004587411880493 3.7507429122924805
Loss :  1.750753402709961 2.0830750465393066 3.8338284492492676
Loss :  1.7450755834579468 2.5875303745269775 4.332605838775635
Loss :  1.7489666938781738 2.2345969676971436 3.9835636615753174
Loss :  1.7464264631271362 1.566164493560791 3.312591075897217
Loss :  1.7466243505477905 1.8026844263076782 3.5493087768554688
Loss :  1.7464109659194946 1.910115361213684 3.6565263271331787
  batch 20 loss: 1.7464109659194946, 1.910115361213684, 3.6565263271331787
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.746924877166748 2.070627450942993 3.817552328109741
Loss :  1.7476886510849 2.108811140060425 3.856499671936035
Loss :  1.7492533922195435 2.2861459255218506 4.035399436950684
Loss :  1.751543402671814 2.258875846862793 4.0104193687438965
Loss :  1.74767005443573 3.066450357437134 4.814120292663574
Loss :  1.7450987100601196 2.3690788745880127 4.114177703857422
Loss :  1.7509675025939941 2.3711907863616943 4.122158050537109
Loss :  1.745415210723877 2.4370737075805664 4.182488918304443
Loss :  1.7497451305389404 2.60494327545166 4.35468864440918
Loss :  1.7436141967773438 2.6384875774383545 4.382102012634277
Loss :  1.753204584121704 2.5558888912200928 4.309093475341797
Loss :  1.74895441532135 2.0635087490081787 3.8124632835388184
Loss :  1.7445746660232544 2.0069668292999268 3.7515416145324707
Loss :  1.7445800304412842 2.0716466903686523 3.8162267208099365
Loss :  1.7496566772460938 2.00095796585083 3.750614643096924
Loss :  1.749577522277832 2.3858449459075928 4.135422706604004
Loss :  1.747680902481079 3.1343581676483154 4.8820390701293945
Loss :  1.7448843717575073 1.493288278579712 3.2381725311279297
Loss :  1.7471091747283936 1.5256527662277222 3.272761821746826
Loss :  1.7477035522460938 1.938080072402954 3.685783624649048
  batch 40 loss: 1.7477035522460938, 1.938080072402954, 3.685783624649048
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7465713024139404 2.4317007064819336 4.178272247314453
Loss :  1.746691107749939 2.6864383220672607 4.43312931060791
Loss :  1.7474430799484253 3.8201487064361572 5.567591667175293
Loss :  1.747668981552124 3.6206257343292236 5.368294715881348
Loss :  1.7485475540161133 4.241866588592529 5.990414142608643
Loss :  1.7457464933395386 3.5182108879089355 5.263957500457764
Loss :  1.7468243837356567 3.079352617263794 4.82617712020874
Loss :  1.7470124959945679 3.0556654930114746 4.802678108215332
Loss :  1.747934103012085 2.7787435054779053 4.52667760848999
Loss :  1.7458056211471558 2.647156238555908 4.3929619789123535
Loss :  1.7433099746704102 2.5740578174591064 4.3173675537109375
Loss :  1.7452900409698486 2.782172203063965 4.527462005615234
Loss :  1.7490944862365723 2.2602386474609375 4.00933313369751
Loss :  1.744342565536499 2.9184980392456055 4.662840843200684
Loss :  1.7457141876220703 3.281127452850342 5.026841640472412
Loss :  1.7435219287872314 2.4781575202941895 4.2216796875
Loss :  1.7468619346618652 2.5993120670318604 4.346174240112305
Loss :  1.74923837184906 1.8073744773864746 3.556612968444824
Loss :  1.7492945194244385 2.296473979949951 4.045768737792969
Loss :  1.7466366291046143 2.04244327545166 3.7890799045562744
  batch 60 loss: 1.7466366291046143, 2.04244327545166, 3.7890799045562744
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7465648651123047 1.9933940172195435 3.7399587631225586
Loss :  1.7478729486465454 1.9270622730255127 3.6749353408813477
Loss :  1.7474631071090698 1.8434603214263916 3.590923309326172
Loss :  1.7447333335876465 2.7866885662078857 4.531421661376953
Loss :  1.7440398931503296 1.5687280893325806 3.31276798248291
Loss :  1.8015196323394775 4.3779191970825195 6.179438591003418
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7978050708770752 4.426765441894531 6.224570274353027
Loss :  1.7989310026168823 4.2394819259643555 6.038413047790527
Loss :  1.8037710189819336 4.226459980010986 6.03023099899292
Total LOSS train 4.198550106928899 valid 6.118163228034973
CE LOSS train 1.7472169069143442 valid 0.4509427547454834
Contrastive LOSS train 2.4513331761726964 valid 1.0566149950027466
EPOCH 264:
Loss :  1.7471002340316772 2.281341791152954 4.028441905975342
Loss :  1.7453641891479492 3.014011859893799 4.759376049041748
Loss :  1.7442083358764648 2.218334913253784 3.962543249130249
Loss :  1.7464829683303833 3.0743515491485596 4.820834636688232
Loss :  1.7483490705490112 1.6621116399765015 3.4104607105255127
Loss :  1.7470935583114624 3.7518231868743896 5.4989166259765625
Loss :  1.7483110427856445 2.5572123527526855 4.30552339553833
Loss :  1.7460381984710693 4.254824161529541 6.000862121582031
Loss :  1.747654914855957 3.287432909011841 5.035087585449219
Loss :  1.7403048276901245 2.8787686824798584 4.619073390960693
Loss :  1.747791051864624 2.470076322555542 4.217867374420166
Loss :  1.7518457174301147 2.2261366844177246 3.977982521057129
Loss :  1.7490524053573608 1.739113211631775 3.4881656169891357
Loss :  1.7466716766357422 1.8877079486846924 3.6343796253204346
Loss :  1.7514311075210571 1.8067958354949951 3.558227062225342
Loss :  1.745622158050537 1.9802193641662598 3.725841522216797
Loss :  1.750207781791687 2.4034323692321777 4.153640270233154
Loss :  1.7477672100067139 2.1180620193481445 3.8658292293548584
Loss :  1.7472611665725708 1.9089573621749878 3.6562185287475586
Loss :  1.74794602394104 2.3850009441375732 4.132946968078613
  batch 20 loss: 1.74794602394104, 2.3850009441375732, 4.132946968078613
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7479053735733032 2.012111186981201 3.760016441345215
Loss :  1.7486950159072876 2.7889761924743652 4.537671089172363
Loss :  1.749845027923584 1.8529115915298462 3.6027565002441406
Loss :  1.7519092559814453 2.174751043319702 3.9266602993011475
Loss :  1.7473201751708984 2.28059458732605 4.027915000915527
Loss :  1.7444703578948975 2.06734561920166 3.8118159770965576
Loss :  1.7496424913406372 3.2578070163726807 5.007449626922607
Loss :  1.7438538074493408 3.070610523223877 4.814464569091797
Loss :  1.7478033304214478 2.8486294746398926 4.596432685852051
Loss :  1.7426280975341797 1.66303288936615 3.405661106109619
Loss :  1.7520484924316406 2.2679924964904785 4.020040988922119
Loss :  1.7476332187652588 1.713376760482788 3.461009979248047
Loss :  1.743458867073059 2.292550563812256 4.036009311676025
Loss :  1.7433282136917114 1.8726457357406616 3.615973949432373
Loss :  1.7490547895431519 2.2311201095581055 3.980175018310547
Loss :  1.7487504482269287 2.3917248249053955 4.140475273132324
Loss :  1.7466151714324951 2.5115370750427246 4.258152008056641
Loss :  1.743952989578247 2.1198198795318604 3.8637728691101074
Loss :  1.7461251020431519 2.101097822189331 3.8472228050231934
Loss :  1.7467279434204102 1.923885703086853 3.6706137657165527
  batch 40 loss: 1.7467279434204102, 1.923885703086853, 3.6706137657165527
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7456759214401245 2.1780097484588623 3.9236855506896973
Loss :  1.7455002069473267 1.6628577709197998 3.408358097076416
Loss :  1.7469079494476318 1.8919111490249634 3.6388192176818848
Loss :  1.745652675628662 1.7700388431549072 3.5156915187835693
Loss :  1.7475829124450684 2.2467474937438965 3.994330406188965
Loss :  1.7455570697784424 2.730809450149536 4.4763665199279785
Loss :  1.745330572128296 1.8954358100891113 3.6407663822174072
Loss :  1.7459429502487183 1.9873998165130615 3.7333426475524902
Loss :  1.7471833229064941 2.059326648712158 3.8065099716186523
Loss :  1.7450426816940308 1.8367193937301636 3.5817620754241943
Loss :  1.7430974245071411 2.040407419204712 3.7835049629211426
Loss :  1.7452322244644165 1.93770170211792 3.682933807373047
Loss :  1.7489745616912842 1.8748323917388916 3.623806953430176
Loss :  1.743927001953125 2.044917345046997 3.788844347000122
Loss :  1.745578646659851 1.968258261680603 3.713836908340454
Loss :  1.742674708366394 1.9511802196502686 3.693854808807373
Loss :  1.7470133304595947 2.034620523452759 3.7816338539123535
Loss :  1.7491834163665771 1.5884257555007935 3.33760929107666
Loss :  1.749367594718933 2.5721278190612793 4.321495532989502
Loss :  1.7466564178466797 3.017573595046997 4.764229774475098
  batch 60 loss: 1.7466564178466797, 3.017573595046997, 4.764229774475098
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7466312646865845 2.3403894901275635 4.0870208740234375
Loss :  1.747737169265747 2.371842622756958 4.119579792022705
Loss :  1.7476881742477417 3.3400120735168457 5.087700366973877
Loss :  1.7450847625732422 2.528146982192993 4.273231506347656
Loss :  1.743953824043274 1.2616772651672363 3.0056309700012207
Loss :  1.8120850324630737 4.365603923797607 6.177689075469971
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.8073012828826904 4.410941123962402 6.218242645263672
Loss :  1.80925714969635 4.352823257446289 6.16208028793335
Loss :  1.8122618198394775 4.0816497802734375 5.893911361694336
Total LOSS train 4.031093135246864 valid 6.112980842590332
CE LOSS train 1.746791516817533 valid 0.4530654549598694
Contrastive LOSS train 2.2843016275992762 valid 1.0204124450683594
EPOCH 265:
Loss :  1.7467840909957886 2.1234846115112305 3.8702688217163086
Loss :  1.7450048923492432 1.8887804746627808 3.6337852478027344
Loss :  1.743393898010254 1.608055591583252 3.351449489593506
Loss :  1.7457780838012695 1.663050889968872 3.4088289737701416
Loss :  1.7478464841842651 1.9662208557128906 3.7140674591064453
Loss :  1.7462472915649414 1.8049896955490112 3.551237106323242
Loss :  1.7479479312896729 2.724010467529297 4.471958160400391
Loss :  1.7456583976745605 1.924360990524292 3.6700193881988525
Loss :  1.7466814517974854 2.1122725009918213 3.8589539527893066
Loss :  1.7394568920135498 2.221670389175415 3.961127281188965
Loss :  1.7473554611206055 2.654268741607666 4.4016242027282715
Loss :  1.7522436380386353 2.483180522918701 4.235424041748047
Loss :  1.748583436012268 2.1190919876098633 3.867675304412842
Loss :  1.7462624311447144 2.168276309967041 3.914538860321045
Loss :  1.7504123449325562 2.3728480339050293 4.123260498046875
Loss :  1.7454618215560913 2.8850953578948975 4.630557060241699
Loss :  1.7491260766983032 2.3729300498962402 4.122056007385254
Loss :  1.74710214138031 3.250704526901245 4.997806549072266
Loss :  1.7469030618667603 2.616867780685425 4.363770961761475
Loss :  1.7469676733016968 1.8521473407745361 3.5991148948669434
  batch 20 loss: 1.7469676733016968, 1.8521473407745361, 3.5991148948669434
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.747339129447937 1.9765543937683105 3.723893642425537
Loss :  1.7475321292877197 2.219193935394287 3.966726064682007
Loss :  1.7487940788269043 1.523252248764038 3.2720463275909424
Loss :  1.7509098052978516 1.7765216827392578 3.5274314880371094
Loss :  1.7472721338272095 2.593632936477661 4.34090518951416
Loss :  1.7442411184310913 2.7082769870758057 4.452517986297607
Loss :  1.7495146989822388 2.213585615158081 3.9631004333496094
Loss :  1.7436821460723877 2.3886663913726807 4.132348537445068
Loss :  1.7485946416854858 2.1119768619537354 3.8605713844299316
Loss :  1.7421770095825195 2.3431556224823 4.085332870483398
Loss :  1.7521663904190063 3.113433599472046 4.865600109100342
Loss :  1.748221755027771 2.761176109313965 4.509397983551025
Loss :  1.7432589530944824 2.6478147506713867 4.391073703765869
Loss :  1.743261456489563 2.2978012561798096 4.041062831878662
Loss :  1.7489482164382935 2.830324411392212 4.579272747039795
Loss :  1.7487825155258179 2.2073895931243896 3.956171989440918
Loss :  1.7463436126708984 1.8595420122146606 3.6058855056762695
Loss :  1.744043231010437 2.2138185501098633 3.95786190032959
Loss :  1.745693564414978 2.630725145339966 4.376418590545654
Loss :  1.7467045783996582 2.239983320236206 3.9866878986358643
  batch 40 loss: 1.7467045783996582, 2.239983320236206, 3.9866878986358643
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.745259165763855 2.274709939956665 4.0199689865112305
Loss :  1.7455681562423706 1.6578043699264526 3.4033725261688232
Loss :  1.7471380233764648 2.177105188369751 3.924243211746216
Loss :  1.7453421354293823 2.2378642559051514 3.983206272125244
Loss :  1.7473946809768677 1.8641422986984253 3.611536979675293
Loss :  1.7450425624847412 1.8111354112625122 3.556178092956543
Loss :  1.7441400289535522 1.8516415357589722 3.5957815647125244
Loss :  1.7460541725158691 1.8148651123046875 3.5609192848205566
Loss :  1.7456889152526855 2.5373988151550293 4.283087730407715
Loss :  1.7452890872955322 2.428467035293579 4.173756122589111
Loss :  1.742322564125061 2.6628527641296387 4.40517520904541
Loss :  1.7449328899383545 2.25400447845459 3.9989373683929443
Loss :  1.7481296062469482 1.7529114484786987 3.5010409355163574
Loss :  1.7439855337142944 2.204683542251587 3.948668956756592
Loss :  1.745259404182434 2.024777412414551 3.7700366973876953
Loss :  1.7428393363952637 1.7459579706192017 3.488797187805176
Loss :  1.7467483282089233 2.189072370529175 3.9358205795288086
Loss :  1.7493044137954712 2.585228443145752 4.334532737731934
Loss :  1.7493139505386353 2.5939548015594482 4.343268871307373
Loss :  1.7466386556625366 1.7472290992736816 3.493867874145508
  batch 60 loss: 1.7466386556625366, 1.7472290992736816, 3.493867874145508
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7467644214630127 1.997854471206665 3.7446188926696777
Loss :  1.7483352422714233 1.7629668712615967 3.5113019943237305
Loss :  1.7483301162719727 1.7217446565628052 3.4700746536254883
Loss :  1.746386170387268 2.292849540710449 4.039235591888428
Loss :  1.745513916015625 2.158831834793091 3.904345750808716
Loss :  1.8063124418258667 4.366469383239746 6.172781944274902
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.801928162574768 4.325246334075928 6.127174377441406
Loss :  1.8036571741104126 4.252220630645752 6.055877685546875
Loss :  1.8059635162353516 4.083219051361084 5.8891825675964355
Total LOSS train 3.9591323925898627 valid 6.061254143714905
CE LOSS train 1.7465295406488273 valid 0.4514908790588379
Contrastive LOSS train 2.2126028647789586 valid 1.020804762840271
EPOCH 266:
Loss :  1.748257040977478 1.6371777057647705 3.385434627532959
Loss :  1.74684476852417 2.021710157394409 3.768554925918579
Loss :  1.7454203367233276 1.789099931716919 3.534520149230957
Loss :  1.7479164600372314 3.313321590423584 5.0612382888793945
Loss :  1.7495133876800537 1.3963558673858643 3.145869255065918
Loss :  1.747524619102478 2.004142999649048 3.7516674995422363
Loss :  1.7493048906326294 1.7913782596588135 3.5406832695007324
Loss :  1.7465721368789673 2.0031418800354004 3.749713897705078
Loss :  1.7476980686187744 1.7410115003585815 3.4887094497680664
Loss :  1.741313099861145 2.011488437652588 3.7528014183044434
Loss :  1.7477084398269653 2.753138542175293 4.500846862792969
Loss :  1.7521430253982544 2.408583641052246 4.160726547241211
Loss :  1.7491976022720337 1.9467581510543823 3.695955753326416
Loss :  1.7466063499450684 2.0462586879730225 3.792865037918091
Loss :  1.75114107131958 2.088965892791748 3.840106964111328
Loss :  1.7463966608047485 1.7650947570800781 3.511491298675537
Loss :  1.7494292259216309 2.935690402984619 4.68511962890625
Loss :  1.747838020324707 2.553938150405884 4.301775932312012
Loss :  1.7472496032714844 2.296971082687378 4.044220924377441
Loss :  1.7471206188201904 2.373459577560425 4.120580196380615
  batch 20 loss: 1.7471206188201904, 2.373459577560425, 4.120580196380615
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7475680112838745 1.8673399686813354 3.61490797996521
Loss :  1.7476786375045776 2.1544339656829834 3.9021124839782715
Loss :  1.749406337738037 2.6559407711029053 4.405346870422363
Loss :  1.7515802383422852 2.0805840492248535 3.8321642875671387
Loss :  1.7478386163711548 2.150240898132324 3.8980793952941895
Loss :  1.7445456981658936 2.6473398208618164 4.391885757446289
Loss :  1.7506316900253296 3.7078280448913574 5.458459854125977
Loss :  1.7455192804336548 2.705291509628296 4.45081090927124
Loss :  1.7492092847824097 3.0441277027130127 4.793336868286133
Loss :  1.7443355321884155 2.6553268432617188 4.399662494659424
Loss :  1.7528780698776245 2.1935179233551025 3.9463958740234375
Loss :  1.7493786811828613 1.740049958229065 3.4894285202026367
Loss :  1.744583010673523 1.7282295227050781 3.4728126525878906
Loss :  1.7448350191116333 1.9338874816894531 3.678722381591797
Loss :  1.750797152519226 2.2591514587402344 4.00994873046875
Loss :  1.7505302429199219 1.905305027961731 3.6558351516723633
Loss :  1.749531865119934 2.329799175262451 4.079330921173096
Loss :  1.7478708028793335 2.1144025325775146 3.8622732162475586
Loss :  1.7492035627365112 2.738499641418457 4.487703323364258
Loss :  1.7491934299468994 3.1638121604919434 4.913005828857422
  batch 40 loss: 1.7491934299468994, 3.1638121604919434, 4.913005828857422
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7477407455444336 2.18679141998291 3.9345321655273438
Loss :  1.7478598356246948 1.8658796548843384 3.613739490509033
Loss :  1.7493473291397095 2.0947933197021484 3.8441405296325684
Loss :  1.7475965023040771 1.967342495918274 3.7149391174316406
Loss :  1.749495506286621 3.0744502544403076 4.823945999145508
Loss :  1.7485790252685547 3.3990728855133057 5.147651672363281
Loss :  1.7485592365264893 2.667226552963257 4.415785789489746
Loss :  1.7486437559127808 1.9376089572906494 3.6862525939941406
Loss :  1.7512809038162231 1.8992688655853271 3.65054988861084
Loss :  1.746806025505066 1.6865143775939941 3.4333205223083496
Loss :  1.7448102235794067 1.8643969297409058 3.6092071533203125
Loss :  1.7462433576583862 1.8782410621643066 3.6244845390319824
Loss :  1.749652624130249 1.6578418016433716 3.40749454498291
Loss :  1.7444294691085815 1.7736316919326782 3.5180611610412598
Loss :  1.7457621097564697 3.280907154083252 5.026669502258301
Loss :  1.743397831916809 2.0244226455688477 3.767820358276367
Loss :  1.7469228506088257 2.9042398929595947 4.651162624359131
Loss :  1.7492984533309937 1.5355918407440186 3.2848901748657227
Loss :  1.7489653825759888 3.6660962104797363 5.4150614738464355
Loss :  1.7461912631988525 1.7769978046417236 3.523189067840576
  batch 60 loss: 1.7461912631988525, 1.7769978046417236, 3.523189067840576
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7461384534835815 2.0736277103424072 3.819766044616699
Loss :  1.747438907623291 2.0969228744506836 3.8443617820739746
Loss :  1.7474561929702759 2.3879103660583496 4.135366439819336
Loss :  1.7435940504074097 2.3676092624664307 4.111203193664551
Loss :  1.7431331872940063 1.5125738382339478 3.255707025527954
Loss :  1.7968937158584595 4.351626396179199 6.148519992828369
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7943556308746338 4.395766258239746 6.190121650695801
Loss :  1.796285629272461 4.268137454986572 6.064423084259033
Loss :  1.7925339937210083 4.095036029815674 5.887569904327393
Total LOSS train 3.9974524351266716 valid 6.072658658027649
CE LOSS train 1.7477177509894737 valid 0.4481334984302521
Contrastive LOSS train 2.249734700643099 valid 1.0237590074539185
EPOCH 267:
Loss :  1.746406078338623 1.6258834600448608 3.3722896575927734
Loss :  1.744591236114502 1.9224363565444946 3.667027473449707
Loss :  1.7432961463928223 2.0477824211120605 3.791078567504883
Loss :  1.7460870742797852 2.3184738159179688 4.064560890197754
Loss :  1.7474133968353271 1.740930438041687 3.4883437156677246
Loss :  1.7464011907577515 1.9831708669662476 3.729572057723999
Loss :  1.7469077110290527 2.1035315990448 3.8504393100738525
Loss :  1.745163917541504 1.8036103248596191 3.548774242401123
Loss :  1.746198058128357 1.9676238298416138 3.7138218879699707
Loss :  1.738385796546936 2.703643560409546 4.4420294761657715
Loss :  1.7469953298568726 2.6029446125030518 4.349939823150635
Loss :  1.7531355619430542 2.377615213394165 4.13075065612793
Loss :  1.748323917388916 2.883434534072876 4.631758689880371
Loss :  1.7459888458251953 3.3998219966888428 5.145811080932617
Loss :  1.750438928604126 4.114962577819824 5.865401268005371
Loss :  1.7445971965789795 2.010573148727417 3.7551703453063965
Loss :  1.7490711212158203 1.9580159187316895 3.7070870399475098
Loss :  1.74552583694458 1.8552849292755127 3.6008107662200928
Loss :  1.7468661069869995 2.2656731605529785 4.012539386749268
Loss :  1.745992660522461 2.7411742210388184 4.487166881561279
  batch 20 loss: 1.745992660522461, 2.7411742210388184, 4.487166881561279
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7459516525268555 1.4257858991622925 3.1717376708984375
Loss :  1.7472922801971436 2.226928234100342 3.9742205142974854
Loss :  1.747848629951477 1.8459686040878296 3.5938172340393066
Loss :  1.7510713338851929 2.0278372764587402 3.7789087295532227
Loss :  1.7473032474517822 2.1970055103302 3.9443087577819824
Loss :  1.7439991235733032 1.8709120750427246 3.6149110794067383
Loss :  1.7493878602981567 2.2073755264282227 3.95676326751709
Loss :  1.7432456016540527 2.0590097904205322 3.802255392074585
Loss :  1.7482259273529053 2.1833596229553223 3.9315855503082275
Loss :  1.7426023483276367 2.0347447395324707 3.7773470878601074
Loss :  1.752884030342102 2.0325162410736084 3.785400390625
Loss :  1.7482194900512695 1.965384602546692 3.713603973388672
Loss :  1.74349045753479 2.6025261878967285 4.346016883850098
Loss :  1.7442340850830078 2.142674446105957 3.886908531188965
Loss :  1.7491792440414429 2.8898518085479736 4.639030933380127
Loss :  1.7494537830352783 2.0150234699249268 3.764477252960205
Loss :  1.7471063137054443 2.653491973876953 4.400598526000977
Loss :  1.7437578439712524 2.2253782749176025 3.9691362380981445
Loss :  1.745962142944336 1.8244693279266357 3.5704314708709717
Loss :  1.746962547302246 1.8823566436767578 3.629319190979004
  batch 40 loss: 1.746962547302246, 1.8823566436767578, 3.629319190979004
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7453235387802124 2.23954701423645 3.984870433807373
Loss :  1.746029019355774 2.0318639278411865 3.77789306640625
Loss :  1.747497320175171 2.3097214698791504 4.057218551635742
Loss :  1.746161937713623 2.5746541023254395 4.3208160400390625
Loss :  1.7480310201644897 3.3144662380218506 5.062497138977051
Loss :  1.7454769611358643 2.255995035171509 4.001471996307373
Loss :  1.74552321434021 2.613344430923462 4.358867645263672
Loss :  1.746893048286438 2.791229724884033 4.538122653961182
Loss :  1.7471328973770142 3.1128010749816895 4.859933853149414
Loss :  1.745825171470642 2.3466224670410156 4.092447757720947
Loss :  1.7435437440872192 2.6376969814300537 4.3812408447265625
Loss :  1.7457293272018433 2.3706908226013184 4.116420269012451
Loss :  1.7498527765274048 1.9670639038085938 3.716916561126709
Loss :  1.745065450668335 1.9781415462493896 3.7232069969177246
Loss :  1.7465431690216064 2.0082972049713135 3.75484037399292
Loss :  1.7444953918457031 2.553454637527466 4.29794979095459
Loss :  1.7477338314056396 2.6046221256256104 4.35235595703125
Loss :  1.7499325275421143 2.1143574714660645 3.8642899990081787
Loss :  1.7498880624771118 2.3921797275543213 4.142067909240723
Loss :  1.7475510835647583 1.6204913854599 3.368042469024658
  batch 60 loss: 1.7475510835647583, 1.6204913854599, 3.368042469024658
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7476434707641602 2.764526844024658 4.512170314788818
Loss :  1.748816967010498 2.4623639583587646 4.211180686950684
Loss :  1.7488425970077515 1.5281696319580078 3.277012348175049
Loss :  1.7460004091262817 2.2895829677581787 4.03558349609375
Loss :  1.745509147644043 2.20701265335083 3.952521800994873
Loss :  1.7832387685775757 3.9269495010375977 5.710188388824463
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7811589241027832 3.855891227722168 5.637050151824951
Loss :  1.7827644348144531 3.8210880756378174 5.603852272033691
Loss :  1.7809633016586304 3.707535982131958 5.488499164581299
Total LOSS train 4.0210013976463905 valid 5.609897494316101
CE LOSS train 1.74672317138085 valid 0.4452408254146576
Contrastive LOSS train 2.2742782244315514 valid 0.9268839955329895
EPOCH 268:
Loss :  1.7479565143585205 1.7900534868240356 3.5380101203918457
Loss :  1.7461843490600586 2.714928150177002 4.4611124992370605
Loss :  1.7444576025009155 1.897464632987976 3.6419222354888916
Loss :  1.7465229034423828 1.9061236381530762 3.652646541595459
Loss :  1.748512625694275 1.7874557971954346 3.53596830368042
Loss :  1.7462356090545654 2.0808637142181396 3.827099323272705
Loss :  1.7476956844329834 2.4133670330047607 4.161062717437744
Loss :  1.7445120811462402 2.82669734954834 4.57120943069458
Loss :  1.74580979347229 2.2172062397003174 3.9630160331726074
Loss :  1.7388896942138672 1.9826127290725708 3.7215023040771484
Loss :  1.7460639476776123 2.1725683212280273 3.9186322689056396
Loss :  1.751427173614502 2.426936626434326 4.178363800048828
Loss :  1.747685194015503 2.1442906856536865 3.8919758796691895
Loss :  1.7455549240112305 2.1407546997070312 3.8863096237182617
Loss :  1.7493960857391357 1.9774256944656372 3.7268218994140625
Loss :  1.744602084159851 2.2813069820404053 4.025908946990967
Loss :  1.7483994960784912 2.178662061691284 3.9270615577697754
Loss :  1.7456653118133545 2.0639264583587646 3.809591770172119
Loss :  1.7468141317367554 1.6093233823776245 3.35613751411438
Loss :  1.7459248304367065 2.3091320991516113 4.055057048797607
  batch 20 loss: 1.7459248304367065, 2.3091320991516113, 4.055057048797607
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7464351654052734 2.4645912647247314 4.211026191711426
Loss :  1.7478185892105103 1.82686185836792 3.5746803283691406
Loss :  1.7482709884643555 1.929598331451416 3.6778693199157715
Loss :  1.7516003847122192 1.903573751449585 3.6551742553710938
Loss :  1.747660517692566 1.813339352607727 3.560999870300293
Loss :  1.7442394495010376 2.2072417736053467 3.951481342315674
Loss :  1.7494664192199707 2.80578351020813 4.55525016784668
Loss :  1.7433695793151855 2.117123603820801 3.8604931831359863
Loss :  1.7480511665344238 2.0313611030578613 3.779412269592285
Loss :  1.7434602975845337 2.935145378112793 4.678605556488037
Loss :  1.7526193857192993 2.3471500873565674 4.099769592285156
Loss :  1.748091220855713 2.4836206436157227 4.2317118644714355
Loss :  1.7436131238937378 2.744274139404297 4.487887382507324
Loss :  1.744201898574829 2.088822841644287 3.833024740219116
Loss :  1.749264121055603 2.172398328781128 3.9216623306274414
Loss :  1.7494765520095825 1.71234130859375 3.461817741394043
Loss :  1.7470811605453491 2.9373703002929688 4.684451580047607
Loss :  1.7437812089920044 1.4716287851333618 3.215409994125366
Loss :  1.7460741996765137 1.5811783075332642 3.3272523880004883
Loss :  1.747162938117981 2.2120091915130615 3.959172248840332
  batch 40 loss: 1.747162938117981, 2.2120091915130615, 3.959172248840332
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.745477318763733 2.5116379261016846 4.257115364074707
Loss :  1.7456257343292236 1.7665306329727173 3.5121564865112305
Loss :  1.7476085424423218 2.2591323852539062 4.006741046905518
Loss :  1.7454211711883545 2.6447932720184326 4.390214443206787
Loss :  1.7478039264678955 2.346489429473877 4.094293594360352
Loss :  1.7456846237182617 2.4171128273010254 4.162797451019287
Loss :  1.744913935661316 2.434603452682495 4.1795172691345215
Loss :  1.7467390298843384 2.1118690967559814 3.8586082458496094
Loss :  1.7472376823425293 1.8125948905944824 3.5598325729370117
Loss :  1.7462756633758545 1.7743957042694092 3.5206713676452637
Loss :  1.7441644668579102 1.9824066162109375 3.7265710830688477
Loss :  1.7461268901824951 1.9415106773376465 3.6876375675201416
Loss :  1.7495715618133545 1.809061884880066 3.558633327484131
Loss :  1.7452481985092163 1.951688528060913 3.69693660736084
Loss :  1.7467756271362305 2.7775802612304688 4.524355888366699
Loss :  1.7444034814834595 2.9654321670532227 4.709835529327393
Loss :  1.7476444244384766 2.204380750656128 3.9520251750946045
Loss :  1.749952793121338 1.7610390186309814 3.5109918117523193
Loss :  1.7501238584518433 1.8566434383392334 3.606767177581787
Loss :  1.7478814125061035 3.6014654636383057 5.349347114562988
  batch 60 loss: 1.7478814125061035, 3.6014654636383057, 5.349347114562988
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7478010654449463 4.008921146392822 5.756722450256348
Loss :  1.7496100664138794 3.825124502182007 5.574734687805176
Loss :  1.749125599861145 2.0504872798919678 3.7996129989624023
Loss :  1.746725082397461 2.621424436569214 4.368149757385254
Loss :  1.745915174484253 1.8514020442962646 3.5973172187805176
Loss :  1.7788077592849731 3.6767053604125977 5.455513000488281
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7761932611465454 3.5650382041931152 5.341231346130371
Loss :  1.7779895067214966 3.5296876430511475 5.307677268981934
Loss :  1.7774735689163208 3.21090030670166 4.988373756408691
Total LOSS train 3.992894590817965 valid 5.273198843002319
CE LOSS train 1.7468293190002442 valid 0.4443683922290802
Contrastive LOSS train 2.24606525347783 valid 0.802725076675415
EPOCH 269:
Loss :  1.7484105825424194 2.1337852478027344 3.8821959495544434
Loss :  1.7463918924331665 2.9226226806640625 4.6690144538879395
Loss :  1.744913935661316 1.8162713050842285 3.561185359954834
Loss :  1.7468851804733276 2.7819511890411377 4.528836250305176
Loss :  1.7497717142105103 2.6742122173309326 4.423984050750732
Loss :  1.7467372417449951 1.9298664331436157 3.6766037940979004
Loss :  1.7480112314224243 2.539379835128784 4.287391185760498
Loss :  1.7455523014068604 2.6500532627105713 4.395605564117432
Loss :  1.7468684911727905 3.9249374866485596 5.6718058586120605
Loss :  1.7393145561218262 3.3413121700286865 5.080626487731934
Loss :  1.7468801736831665 2.323809862136841 4.070690155029297
Loss :  1.7524898052215576 2.0079338550567627 3.7604236602783203
Loss :  1.7481646537780762 2.162780284881592 3.910944938659668
Loss :  1.7460085153579712 1.8132858276367188 3.5592942237854004
Loss :  1.7499884366989136 2.7109785079956055 4.460967063903809
Loss :  1.745070219039917 1.698943018913269 3.4440131187438965
Loss :  1.7487739324569702 2.257704257965088 4.006478309631348
Loss :  1.7463500499725342 2.377014636993408 4.123364448547363
Loss :  1.7468572854995728 2.281862497329712 4.028719902038574
Loss :  1.7457913160324097 3.17254900932312 4.91834020614624
  batch 20 loss: 1.7457913160324097, 3.17254900932312, 4.91834020614624
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7466357946395874 2.734469175338745 4.481104850769043
Loss :  1.7476807832717896 2.7675235271453857 4.515204429626465
Loss :  1.748097538948059 2.6041693687438965 4.352266788482666
Loss :  1.7512446641921997 1.9122235774993896 3.663468360900879
Loss :  1.7474751472473145 2.847010374069214 4.594485282897949
Loss :  1.7445214986801147 2.6692779064178467 4.413799285888672
Loss :  1.7498235702514648 2.8673787117004395 4.617202281951904
Loss :  1.7443840503692627 2.3899455070495605 4.134329795837402
Loss :  1.748893141746521 2.046938180923462 3.7958312034606934
Loss :  1.7439411878585815 2.183655261993408 3.9275965690612793
Loss :  1.7530152797698975 2.274726152420044 4.027741432189941
Loss :  1.748942494392395 1.9049780368804932 3.6539206504821777
Loss :  1.744660496711731 2.1379435062408447 3.8826041221618652
Loss :  1.7447017431259155 2.096989870071411 3.841691493988037
Loss :  1.749991774559021 2.742798089981079 4.4927897453308105
Loss :  1.7497937679290771 3.077578544616699 4.8273725509643555
Loss :  1.7478703260421753 2.420097589492798 4.167967796325684
Loss :  1.745092511177063 2.4067721366882324 4.151864528656006
Loss :  1.746781587600708 3.5901846885681152 5.336966514587402
Loss :  1.7475130558013916 3.583505153656006 5.331018447875977
  batch 40 loss: 1.7475130558013916, 3.583505153656006, 5.331018447875977
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.746297836303711 3.2464520931243896 4.99275016784668
Loss :  1.7461159229278564 1.797447681427002 3.5435636043548584
Loss :  1.7475436925888062 2.645291566848755 4.3928351402282715
Loss :  1.7465506792068481 1.76103675365448 3.507587432861328
Loss :  1.7487928867340088 1.6616020202636719 3.4103949069976807
Loss :  1.7475265264511108 1.9958006143569946 3.7433271408081055
Loss :  1.7479230165481567 2.2898240089416504 4.037746906280518
Loss :  1.7488293647766113 1.9186320304870605 3.667461395263672
Loss :  1.7506800889968872 1.9570704698562622 3.7077505588531494
Loss :  1.7480700016021729 1.6419597864151 3.3900299072265625
Loss :  1.7466144561767578 2.261303186416626 4.007917404174805
Loss :  1.748515009880066 2.0479085445404053 3.7964234352111816
Loss :  1.7519934177398682 1.7901039123535156 3.542097330093384
Loss :  1.7471134662628174 2.173153877258301 3.920267343521118
Loss :  1.749035120010376 2.024808883666992 3.773844003677368
Loss :  1.7471058368682861 2.532224416732788 4.279330253601074
Loss :  1.750253677368164 3.7171390056610107 5.467392921447754
Loss :  1.7524436712265015 2.6028175354003906 4.355261325836182
Loss :  1.7520408630371094 2.3519699573516846 4.104010581970215
Loss :  1.7497860193252563 1.9021395444869995 3.651925563812256
  batch 60 loss: 1.7497860193252563, 1.9021395444869995, 3.651925563812256
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7495143413543701 1.69516122341156 3.4446754455566406
Loss :  1.7508431673049927 1.9248204231262207 3.675663471221924
Loss :  1.7505959272384644 1.7541978359222412 3.504793643951416
Loss :  1.748550534248352 2.3715696334838867 4.120120048522949
Loss :  1.7475675344467163 1.9470860958099365 3.6946535110473633
Loss :  1.814443588256836 4.267544746398926 6.081988334655762
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8104249238967896 4.236224174499512 6.046648979187012
Loss :  1.8115836381912231 4.120067119598389 5.931650638580322
Loss :  1.8141365051269531 3.942023992538452 5.756160736083984
Total LOSS train 4.129254377805269 valid 5.95411217212677
CE LOSS train 1.7478553075056809 valid 0.4535341262817383
Contrastive LOSS train 2.381399077635545 valid 0.985505998134613
EPOCH 270:
Loss :  1.7502453327178955 2.2713093757629395 4.021554946899414
Loss :  1.7487519979476929 2.7426369190216064 4.49138879776001
Loss :  1.7474981546401978 2.5646355152130127 4.3121337890625
Loss :  1.749374270439148 2.2017884254455566 3.951162815093994
Loss :  1.7513720989227295 2.3116376399993896 4.063009738922119
Loss :  1.7493596076965332 2.3799564838409424 4.129316329956055
Loss :  1.7517896890640259 3.962254762649536 5.714044570922852
Loss :  1.7485333681106567 2.055889844894409 3.8044233322143555
Loss :  1.749478816986084 2.2878763675689697 4.037355422973633
Loss :  1.743116021156311 2.6107916831970215 4.353907585144043
Loss :  1.7493833303451538 2.217323064804077 3.9667062759399414
Loss :  1.7525794506072998 2.112194538116455 3.864773988723755
Loss :  1.75045645236969 1.9366888999938965 3.687145233154297
Loss :  1.7479952573776245 1.9560168981552124 3.704012155532837
Loss :  1.7527319192886353 1.9377208948135376 3.690452814102173
Loss :  1.7478291988372803 1.9112051725387573 3.659034252166748
Loss :  1.7509852647781372 1.7719402313232422 3.52292537689209
Loss :  1.7492725849151611 1.9147807359695435 3.664053440093994
Loss :  1.748567819595337 1.7565150260925293 3.505082845687866
Loss :  1.7495450973510742 1.9081250429153442 3.657670021057129
  batch 20 loss: 1.7495450973510742, 1.9081250429153442, 3.657670021057129
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.749613881111145 1.7275358438491821 3.477149724960327
Loss :  1.7499895095825195 2.162172794342041 3.9121623039245605
Loss :  1.7514814138412476 1.890511155128479 3.6419925689697266
Loss :  1.7533307075500488 3.2524425983428955 5.005773544311523
Loss :  1.7493470907211304 2.474120855331421 4.223467826843262
Loss :  1.7466176748275757 2.6245357990264893 4.371153354644775
Loss :  1.7515190839767456 2.5400025844573975 4.2915215492248535
Loss :  1.7464340925216675 3.5643677711486816 5.310801982879639
Loss :  1.7500157356262207 3.051077127456665 4.801093101501465
Loss :  1.7446541786193848 3.3895699977874756 5.134223937988281
Loss :  1.7531601190567017 2.752155303955078 4.50531530380249
Loss :  1.7494691610336304 2.722719430923462 4.472188472747803
Loss :  1.744729995727539 2.257659673690796 4.002389907836914
Loss :  1.7445935010910034 3.1524226665496826 4.8970160484313965
Loss :  1.7499094009399414 2.423766851425171 4.173676490783691
Loss :  1.7492197751998901 2.3074352741241455 4.056654930114746
Loss :  1.7471668720245361 2.2046923637390137 3.95185923576355
Loss :  1.7447973489761353 2.92113995552063 4.665937423706055
Loss :  1.7457393407821655 3.020839214324951 4.766578674316406
Loss :  1.7473541498184204 1.9404476881027222 3.6878018379211426
  batch 40 loss: 1.7473541498184204, 1.9404476881027222, 3.6878018379211426
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7452325820922852 2.4384443759918213 4.183676719665527
Loss :  1.745289921760559 2.0492703914642334 3.794560432434082
Loss :  1.7477209568023682 1.9323608875274658 3.680081844329834
Loss :  1.7455174922943115 2.7132296562194824 4.458746910095215
Loss :  1.7480818033218384 1.5876353979110718 3.33571720123291
Loss :  1.7462868690490723 1.6957061290740967 3.441992998123169
Loss :  1.745717167854309 1.5856757164001465 3.331392765045166
Loss :  1.7474578619003296 2.0101351737976074 3.7575931549072266
Loss :  1.7482327222824097 1.9916324615478516 3.739865303039551
Loss :  1.746842861175537 1.726959466934204 3.473802328109741
Loss :  1.7449727058410645 2.0134687423706055 3.75844144821167
Loss :  1.7468816041946411 2.087273120880127 3.8341546058654785
Loss :  1.7500994205474854 1.9093157052993774 3.6594152450561523
Loss :  1.7460414171218872 1.9549167156219482 3.700958251953125
Loss :  1.7474887371063232 2.3061957359313965 4.053684234619141
Loss :  1.744524359703064 2.208312749862671 3.9528369903564453
Loss :  1.7485907077789307 2.0533559322357178 3.8019466400146484
Loss :  1.7508649826049805 2.034599781036377 3.7854647636413574
Loss :  1.7509136199951172 1.9668843746185303 3.7177979946136475
Loss :  1.7482025623321533 1.542225956916809 3.290428638458252
  batch 60 loss: 1.7482025623321533, 1.542225956916809, 3.290428638458252
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7484562397003174 2.8734638690948486 4.621920108795166
Loss :  1.7493783235549927 2.042975902557373 3.792354106903076
Loss :  1.7493624687194824 1.7988245487213135 3.548187017440796
Loss :  1.7467375993728638 2.8217036724090576 4.568441390991211
Loss :  1.7459114789962769 1.1438469886779785 2.889758586883545
Loss :  1.8098896741867065 4.3601555824279785 6.170045375823975
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.8062849044799805 4.451364040374756 6.257648944854736
Loss :  1.8075382709503174 4.231096267700195 6.038634300231934
Loss :  1.8084136247634888 4.17251443862915 5.98092794418335
Total LOSS train 4.020340486673208 valid 6.1118141412734985
CE LOSS train 1.748351003573491 valid 0.4521034061908722
Contrastive LOSS train 2.271989475763761 valid 1.0431286096572876
EPOCH 271:
Loss :  1.7485768795013428 1.5540539026260376 3.30263090133667
Loss :  1.746918797492981 1.8728753328323364 3.6197941303253174
Loss :  1.7454402446746826 2.256624937057495 4.002065181732178
Loss :  1.7480459213256836 1.9643772840499878 3.712423324584961
Loss :  1.7489384412765503 1.5566823482513428 3.3056206703186035
Loss :  1.7477129697799683 1.3926869630813599 3.140399932861328
Loss :  1.749037742614746 1.9567009210586548 3.7057385444641113
Loss :  1.746663212776184 1.8408739566802979 3.5875372886657715
Loss :  1.7477807998657227 2.2656137943267822 4.013394355773926
Loss :  1.7407528162002563 2.585414171218872 4.326167106628418
Loss :  1.7482272386550903 2.6084203720092773 4.356647491455078
Loss :  1.752493977546692 2.347442626953125 4.099936485290527
Loss :  1.7491616010665894 1.9983744621276855 3.7475361824035645
Loss :  1.7466731071472168 2.3079047203063965 4.054577827453613
Loss :  1.7505794763565063 2.006908655166626 3.757488250732422
Loss :  1.7463321685791016 2.3280797004699707 4.074411869049072
Loss :  1.7489715814590454 2.5541367530822754 4.303108215332031
Loss :  1.7478654384613037 2.6849770545959473 4.432842254638672
Loss :  1.7469638586044312 2.9948153495788574 4.741779327392578
Loss :  1.7463922500610352 2.0267112255096436 3.7731034755706787
  batch 20 loss: 1.7463922500610352, 2.0267112255096436, 3.7731034755706787
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7473748922348022 2.6188173294067383 4.36619234085083
Loss :  1.7473809719085693 2.694559335708618 4.4419403076171875
Loss :  1.749176025390625 2.5460805892944336 4.295256614685059
Loss :  1.751374363899231 2.4950239658355713 4.246398448944092
Loss :  1.7484759092330933 1.8351445198059082 3.583620548248291
Loss :  1.7453991174697876 2.0224852561950684 3.7678842544555664
Loss :  1.750534176826477 2.190507173538208 3.9410414695739746
Loss :  1.7451781034469604 1.785706639289856 3.5308847427368164
Loss :  1.7496098279953003 2.1994943618774414 3.9491043090820312
Loss :  1.7447609901428223 2.4483988285064697 4.193160057067871
Loss :  1.753548502922058 2.388517141342163 4.142065525054932
Loss :  1.7489820718765259 2.107513189315796 3.8564953804016113
Loss :  1.7447223663330078 3.1091339588165283 4.853856086730957
Loss :  1.7442653179168701 2.0285592079162598 3.77282452583313
Loss :  1.749546766281128 2.6453115940093994 4.394858360290527
Loss :  1.7491741180419922 1.9364814758300781 3.6856555938720703
Loss :  1.7468475103378296 2.0647571086883545 3.8116044998168945
Loss :  1.743957281112671 2.3884243965148926 4.132381439208984
Loss :  1.746230959892273 2.374443531036377 4.1206746101379395
Loss :  1.7469868659973145 2.618870258331299 4.365857124328613
  batch 40 loss: 1.7469868659973145, 2.618870258331299, 4.365857124328613
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7459464073181152 2.930476427078247 4.676423072814941
Loss :  1.745952844619751 1.7075079679489136 3.453460693359375
Loss :  1.7478368282318115 1.7967747449874878 3.5446114540100098
Loss :  1.7458397150039673 1.437733769416809 3.1835734844207764
Loss :  1.7480155229568481 1.4914648532867432 3.239480495452881
Loss :  1.7458248138427734 1.8679953813552856 3.6138200759887695
Loss :  1.7447943687438965 2.5458760261535645 4.290670394897461
Loss :  1.7467683553695679 3.027202844619751 4.773971080780029
Loss :  1.746669888496399 2.196805238723755 3.9434752464294434
Loss :  1.7460436820983887 1.5145905017852783 3.260634183883667
Loss :  1.743700623512268 1.5945546627044678 3.3382554054260254
Loss :  1.7456542253494263 2.2293550968170166 3.9750094413757324
Loss :  1.7489478588104248 2.4857709407806396 4.2347187995910645
Loss :  1.7453511953353882 2.350597858428955 4.095949172973633
Loss :  1.7466415166854858 2.4968645572662354 4.243505954742432
Loss :  1.7433509826660156 2.257711887359619 4.001062870025635
Loss :  1.7475407123565674 1.9977937936782837 3.7453346252441406
Loss :  1.7496919631958008 1.372052788734436 3.1217446327209473
Loss :  1.750023365020752 2.7877309322357178 4.537754058837891
Loss :  1.7472933530807495 3.017181396484375 4.764474868774414
  batch 60 loss: 1.7472933530807495, 3.017181396484375, 4.764474868774414
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7470697164535522 2.1685709953308105 3.9156408309936523
Loss :  1.7487144470214844 1.725232481956482 3.473947048187256
Loss :  1.7482795715332031 1.5818148851394653 3.330094337463379
Loss :  1.7458293437957764 2.1015031337738037 3.84733247756958
Loss :  1.7449537515640259 1.2909388542175293 3.0358924865722656
Loss :  1.8096309900283813 4.368843078613281 6.178473949432373
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.8056753873825073 4.414570331573486 6.220245838165283
Loss :  1.8067854642868042 4.2976298332214355 6.104415416717529
Loss :  1.8100231885910034 4.1234636306762695 5.9334869384765625
Total LOSS train 3.925381480730497 valid 6.109155535697937
CE LOSS train 1.7472890725502601 valid 0.45250579714775085
Contrastive LOSS train 2.1780924063462477 valid 1.0308659076690674
EPOCH 272:
Loss :  1.748219609260559 2.132760524749756 3.8809800148010254
Loss :  1.746442437171936 2.0475828647613525 3.794025421142578
Loss :  1.744893193244934 2.4251654148101807 4.170058727264404
Loss :  1.7478426694869995 3.439054250717163 5.186896800994873
Loss :  1.7485783100128174 2.1497488021850586 3.898327112197876
Loss :  1.7477377653121948 2.4275104999542236 4.175248146057129
Loss :  1.7478832006454468 2.2241580486297607 3.972041130065918
Loss :  1.7457643747329712 2.1312994956970215 3.877063751220703
Loss :  1.7469148635864258 1.6260937452316284 3.3730087280273438
Loss :  1.7392340898513794 2.117358446121216 3.8565926551818848
Loss :  1.7470391988754272 2.189863681793213 3.9369029998779297
Loss :  1.7529306411743164 2.0739340782165527 3.826864719390869
Loss :  1.748366117477417 1.942674994468689 3.6910409927368164
Loss :  1.7462987899780273 1.8493735790252686 3.595672369003296
Loss :  1.7502057552337646 1.7481602430343628 3.498365879058838
Loss :  1.7457813024520874 1.8752113580703735 3.620992660522461
Loss :  1.7488197088241577 1.9418753385543823 3.69069504737854
Loss :  1.7469666004180908 1.8938668966293335 3.6408333778381348
Loss :  1.746780276298523 2.194082021713257 3.9408621788024902
Loss :  1.7460471391677856 1.813707947731018 3.5597550868988037
  batch 20 loss: 1.7460471391677856, 1.813707947731018, 3.5597550868988037
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7468383312225342 2.324002742767334 4.070840835571289
Loss :  1.7471481561660767 2.0804250240325928 3.827573299407959
Loss :  1.7490063905715942 1.85170578956604 3.600712299346924
Loss :  1.7513484954833984 1.8688501119613647 3.6201987266540527
Loss :  1.7486704587936401 2.323758125305176 4.0724287033081055
Loss :  1.7447847127914429 2.2383627891540527 3.983147621154785
Loss :  1.749934434890747 2.1543362140655518 3.904270648956299
Loss :  1.7438764572143555 1.9236000776290894 3.6674766540527344
Loss :  1.7488062381744385 1.940039873123169 3.6888461112976074
Loss :  1.743317723274231 2.0345380306243896 3.77785587310791
Loss :  1.7534120082855225 2.0155959129333496 3.769007921218872
Loss :  1.749032735824585 1.8162389993667603 3.5652718544006348
Loss :  1.7444881200790405 1.7336398363113403 3.478127956390381
Loss :  1.7447519302368164 1.790054202079773 3.534806251525879
Loss :  1.7505601644515991 2.903247356414795 4.653807640075684
Loss :  1.7500487565994263 1.8720519542694092 3.622100830078125
Loss :  1.7478020191192627 1.7839115858078003 3.5317134857177734
Loss :  1.7451270818710327 1.8213149309158325 3.5664420127868652
Loss :  1.7467262744903564 2.089520215988159 3.8362464904785156
Loss :  1.7477115392684937 1.9419296979904175 3.689641237258911
  batch 40 loss: 1.7477115392684937, 1.9419296979904175, 3.689641237258911
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7466068267822266 2.211723566055298 3.9583303928375244
Loss :  1.747003197669983 1.859754204750061 3.606757402420044
Loss :  1.7487925291061401 2.1685829162597656 3.9173755645751953
Loss :  1.7472254037857056 2.0753042697906494 3.8225297927856445
Loss :  1.7492929697036743 1.6752450466156006 3.4245381355285645
Loss :  1.7472003698349 1.7239036560058594 3.471104145050049
Loss :  1.7468210458755493 1.5249696969985962 3.2717907428741455
Loss :  1.747667908668518 1.7716456651687622 3.5193135738372803
Loss :  1.7483114004135132 1.6625570058822632 3.4108684062957764
Loss :  1.7466585636138916 1.6021950244903564 3.348853588104248
Loss :  1.7441792488098145 1.7455646991729736 3.489743947982788
Loss :  1.7463569641113281 2.0702130794525146 3.8165700435638428
Loss :  1.7492234706878662 2.466111898422241 4.215335369110107
Loss :  1.745341420173645 2.5266785621643066 4.272019863128662
Loss :  1.746246337890625 2.91705584526062 4.663302421569824
Loss :  1.742329716682434 2.2029004096984863 3.945230007171631
Loss :  1.746863842010498 1.9815858602523804 3.728449821472168
Loss :  1.7493025064468384 3.750483512878418 5.499785900115967
Loss :  1.7496223449707031 3.894533634185791 5.644155979156494
Loss :  1.7461029291152954 3.094609022140503 4.840712070465088
  batch 60 loss: 1.7461029291152954, 3.094609022140503, 4.840712070465088
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7456188201904297 2.9392316341400146 4.684850692749023
Loss :  1.7473224401474 2.7821905612945557 4.529512882232666
Loss :  1.746476411819458 2.0789554119110107 3.8254318237304688
Loss :  1.7444711923599243 2.3054847717285156 4.04995584487915
Loss :  1.7436500787734985 1.2712982892990112 3.0149483680725098
Loss :  1.8065025806427002 4.378110408782959 6.184613227844238
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8029508590698242 4.313692092895508 6.116642951965332
Loss :  1.8032615184783936 4.268532752990723 6.071794509887695
Loss :  1.8077024221420288 4.002457618713379 5.810160160064697
Total LOSS train 3.8864340158609245 valid 6.045802712440491
CE LOSS train 1.7471511694101187 valid 0.4519256055355072
Contrastive LOSS train 2.139282829944904 valid 1.0006144046783447
EPOCH 273:
Loss :  1.7471575736999512 1.5944393873214722 3.341597080230713
Loss :  1.745010495185852 1.7673218250274658 3.5123324394226074
Loss :  1.7443642616271973 1.5395972728729248 3.283961534500122
Loss :  1.7476054430007935 1.9735627174377441 3.721168041229248
Loss :  1.7486646175384521 2.645761728286743 4.394426345825195
Loss :  1.7477322816848755 2.428142547607422 4.175874710083008
Loss :  1.7482600212097168 2.5165035724639893 4.264763832092285
Loss :  1.7462583780288696 2.0035560131073 3.749814510345459
Loss :  1.7476447820663452 2.5369648933410645 4.284609794616699
Loss :  1.7404290437698364 2.256899356842041 3.997328281402588
Loss :  1.7484666109085083 2.888714075088501 4.637180805206299
Loss :  1.7536150217056274 2.8266851902008057 4.580300331115723
Loss :  1.7496469020843506 2.2521564960479736 4.001803398132324
Loss :  1.7474480867385864 2.0906529426574707 3.8381009101867676
Loss :  1.751512050628662 2.3559868335723877 4.107499122619629
Loss :  1.7466171979904175 2.4893717765808105 4.235989093780518
Loss :  1.7500020265579224 2.8699896335601807 4.619991779327393
Loss :  1.747802495956421 3.271834135055542 5.019636631011963
Loss :  1.7493427991867065 1.7193408012390137 3.4686837196350098
Loss :  1.7476013898849487 1.8325072526931763 3.580108642578125
  batch 20 loss: 1.7476013898849487, 1.8325072526931763, 3.580108642578125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.748124361038208 2.3497989177703857 4.097923278808594
Loss :  1.7491176128387451 1.953646183013916 3.702763795852661
Loss :  1.7491371631622314 1.5170875787734985 3.2662248611450195
Loss :  1.7529112100601196 1.8444955348968506 3.5974068641662598
Loss :  1.7487244606018066 1.9871231317520142 3.7358474731445312
Loss :  1.7455514669418335 1.9185149669647217 3.6640663146972656
Loss :  1.7513283491134644 1.9774912595748901 3.7288196086883545
Loss :  1.7462260723114014 1.926808476448059 3.67303466796875
Loss :  1.7504620552062988 2.5362653732299805 4.286727428436279
Loss :  1.7453638315200806 2.266066312789917 4.011430263519287
Loss :  1.7541848421096802 2.171562671661377 3.9257473945617676
Loss :  1.7512568235397339 2.4630632400512695 4.214320182800293
Loss :  1.7469005584716797 2.061830759048462 3.8087313175201416
Loss :  1.7465481758117676 2.144426107406616 3.890974283218384
Loss :  1.7515203952789307 3.4022488594055176 5.153769493103027
Loss :  1.7516710758209229 2.8788158893585205 4.630486965179443
Loss :  1.749453067779541 3.7440600395202637 5.493513107299805
Loss :  1.747459888458252 2.8856663703918457 4.633126258850098
Loss :  1.748328685760498 2.3152098655700684 4.063538551330566
Loss :  1.7501838207244873 2.4049172401428223 4.1551008224487305
  batch 40 loss: 1.7501838207244873, 2.4049172401428223, 4.1551008224487305
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.747787594795227 2.3220040798187256 4.069791793823242
Loss :  1.747660517692566 2.3776984214782715 4.125359058380127
Loss :  1.7496216297149658 2.485387086868286 4.235008716583252
Loss :  1.7473667860031128 2.001875162124634 3.749241828918457
Loss :  1.7499346733093262 2.3866028785705566 4.136537551879883
Loss :  1.7488987445831299 2.6721880435943604 4.42108678817749
Loss :  1.7491215467453003 1.5375659465789795 3.2866873741149902
Loss :  1.7488527297973633 1.5250760316848755 3.273928642272949
Loss :  1.7515393495559692 1.5277706384658813 3.2793099880218506
Loss :  1.7478904724121094 1.7166674137115479 3.4645578861236572
Loss :  1.7463799715042114 3.220902442932129 4.967282295227051
Loss :  1.7481088638305664 2.6840667724609375 4.432175636291504
Loss :  1.7517555952072144 2.2875587940216064 4.039314270019531
Loss :  1.746578335762024 2.3651726245880127 4.111751079559326
Loss :  1.7480213642120361 2.6074392795562744 4.3554606437683105
Loss :  1.7462655305862427 1.9174902439117432 3.6637558937072754
Loss :  1.7492716312408447 1.996988296508789 3.746259927749634
Loss :  1.7515286207199097 2.172534942626953 3.9240636825561523
Loss :  1.7512049674987793 3.8272933959960938 5.578498363494873
Loss :  1.7485196590423584 2.4755985736846924 4.224118232727051
  batch 60 loss: 1.7485196590423584, 2.4755985736846924, 4.224118232727051
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7487671375274658 1.9719815254211426 3.7207486629486084
Loss :  1.7496627569198608 3.0488693714141846 4.798532009124756
Loss :  1.7487469911575317 3.8119895458221436 5.560736656188965
Loss :  1.7466458082199097 3.9386847019195557 5.685330390930176
Loss :  1.746349573135376 2.8953747749328613 4.641724586486816
Loss :  1.7695249319076538 2.897848606109619 4.6673736572265625
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7678775787353516 2.7845654487609863 4.552443027496338
Loss :  1.7680108547210693 2.695667028427124 4.463677883148193
Loss :  1.7717758417129517 2.4013679027557373 4.1731438636779785
Total LOSS train 4.123692090694721 valid 4.464159607887268
CE LOSS train 1.7485556345719557 valid 0.4429439604282379
Contrastive LOSS train 2.3751364341148964 valid 0.6003419756889343
Saved best model. Old loss 4.804861307144165 and new best loss 4.464159607887268
EPOCH 274:
Loss :  1.749060034751892 2.1404898166656494 3.889549732208252
Loss :  1.7473703622817993 2.0343971252441406 3.7817673683166504
Loss :  1.7460148334503174 2.672119617462158 4.418134689331055
Loss :  1.7480578422546387 2.983123540878296 4.7311811447143555
Loss :  1.74994695186615 2.8800337314605713 4.629980564117432
Loss :  1.7476476430892944 2.0978739261627197 3.8455214500427246
Loss :  1.7497236728668213 2.6111881732940674 4.360911846160889
Loss :  1.746564269065857 1.8720961809158325 3.6186604499816895
Loss :  1.7475382089614868 1.5055078268051147 3.2530460357666016
Loss :  1.7413718700408936 1.4088720083236694 3.1502437591552734
Loss :  1.747890591621399 2.1441736221313477 3.892064094543457
Loss :  1.7517900466918945 1.8372431993484497 3.5890331268310547
Loss :  1.7491720914840698 1.8330570459365845 3.5822291374206543
Loss :  1.746504306793213 2.1436564922332764 3.8901607990264893
Loss :  1.7508488893508911 2.086092472076416 3.8369412422180176
Loss :  1.746046543121338 2.7930150032043457 4.539061546325684
Loss :  1.748831868171692 2.7164762020111084 4.46530818939209
Loss :  1.7476705312728882 2.8433678150177 4.591038227081299
Loss :  1.7469651699066162 2.538369655609131 4.285334587097168
Loss :  1.7464101314544678 1.987572431564331 3.733982563018799
  batch 20 loss: 1.7464101314544678, 1.987572431564331, 3.733982563018799
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7473328113555908 2.5411722660064697 4.2885050773620605
Loss :  1.7474679946899414 2.711573362350464 4.459041595458984
Loss :  1.7491523027420044 3.2454535961151123 4.994606018066406
Loss :  1.7518858909606934 2.7656681537628174 4.51755428314209
Loss :  1.748471975326538 2.322222948074341 4.070694923400879
Loss :  1.7448288202285767 2.284667730331421 4.029496669769287
Loss :  1.7503767013549805 1.9315472841262817 3.6819238662719727
Loss :  1.7448296546936035 2.7663564682006836 4.511186122894287
Loss :  1.7493921518325806 2.2097198963165283 3.9591121673583984
Loss :  1.7439825534820557 2.0179548263549805 3.761937379837036
Loss :  1.7535349130630493 2.2887189388275146 4.0422539710998535
Loss :  1.7495747804641724 2.493276357650757 4.242851257324219
Loss :  1.7449612617492676 2.24790358543396 3.9928648471832275
Loss :  1.745488166809082 2.711491823196411 4.456979751586914
Loss :  1.750446081161499 2.287757396697998 4.038203239440918
Loss :  1.7502249479293823 2.4855916500091553 4.235816478729248
Loss :  1.7484289407730103 1.7988499402999878 3.547278881072998
Loss :  1.7456958293914795 2.660719633102417 4.4064154624938965
Loss :  1.7474631071090698 1.9926433563232422 3.7401065826416016
Loss :  1.749220371246338 2.431379795074463 4.180600166320801
  batch 40 loss: 1.749220371246338, 2.431379795074463, 4.180600166320801
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7472401857376099 2.788691759109497 4.5359320640563965
Loss :  1.7468394041061401 2.173363208770752 3.9202027320861816
Loss :  1.7492619752883911 2.201158285140991 3.950420379638672
Loss :  1.7461892366409302 1.4279897212982178 3.1741790771484375
Loss :  1.748781681060791 1.8160014152526855 3.5647830963134766
Loss :  1.747814416885376 2.2136969566345215 3.9615113735198975
Loss :  1.7463459968566895 2.238307476043701 3.9846534729003906
Loss :  1.747537612915039 1.9457765817642212 3.6933140754699707
Loss :  1.7490370273590088 2.0536282062530518 3.8026652336120605
Loss :  1.7467598915100098 1.806136965751648 3.5528969764709473
Loss :  1.7455061674118042 2.6238434314727783 4.369349479675293
Loss :  1.7467687129974365 2.38185715675354 4.128625869750977
Loss :  1.7496094703674316 1.8557168245315552 3.6053261756896973
Loss :  1.745780348777771 1.9987082481384277 3.7444887161254883
Loss :  1.7472234964370728 1.848724603652954 3.5959482192993164
Loss :  1.7431327104568481 2.7372689247131348 4.480401515960693
Loss :  1.747995138168335 2.1010305881500244 3.8490257263183594
Loss :  1.7503273487091064 2.292665958404541 4.042993545532227
Loss :  1.7503399848937988 2.308635711669922 4.058975696563721
Loss :  1.7475941181182861 3.2841947078704834 5.0317888259887695
  batch 60 loss: 1.7475941181182861, 3.2841947078704834, 5.0317888259887695
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.747630000114441 2.067728042602539 3.8153581619262695
Loss :  1.7490050792694092 1.9608787298202515 3.709883689880371
Loss :  1.7490304708480835 2.0331497192382812 3.7821803092956543
Loss :  1.7468409538269043 3.090263605117798 4.837104797363281
Loss :  1.7463829517364502 2.7813003063201904 4.527683258056641
Loss :  1.776828408241272 3.541811227798462 5.318639755249023
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7744895219802856 3.467625141143799 5.242114543914795
Loss :  1.775352954864502 3.2988791465759277 5.07423210144043
Loss :  1.7772024869918823 3.2222800254821777 4.99948263168335
Total LOSS train 4.0455580271207365 valid 5.158617258071899
CE LOSS train 1.7478024537746724 valid 0.4443006217479706
Contrastive LOSS train 2.2977555696780865 valid 0.8055700063705444
EPOCH 275:
Loss :  1.7490907907485962 2.4875941276550293 4.236684799194336
Loss :  1.7474068403244019 2.872281074523926 4.619688034057617
Loss :  1.7462196350097656 3.117269277572632 4.863489151000977
Loss :  1.747941255569458 1.8347573280334473 3.5826985836029053
Loss :  1.749705195426941 1.778825283050537 3.5285305976867676
Loss :  1.7470289468765259 1.8459707498550415 3.5929996967315674
Loss :  1.7493847608566284 1.8747093677520752 3.624094009399414
Loss :  1.7460894584655762 2.1113946437835693 3.8574841022491455
Loss :  1.7474676370620728 2.4531331062316895 4.200600624084473
Loss :  1.740401268005371 2.449225425720215 4.189626693725586
Loss :  1.7473289966583252 2.8338210582733154 4.581150054931641
Loss :  1.7525745630264282 2.3048181533813477 4.057392597198486
Loss :  1.748449683189392 1.966509461402893 3.714959144592285
Loss :  1.7463536262512207 2.221881866455078 3.968235492706299
Loss :  1.750303030014038 2.923332691192627 4.673635482788086
Loss :  1.7459046840667725 3.3447537422180176 5.090658187866211
Loss :  1.7481058835983276 2.54624605178833 4.294352054595947
Loss :  1.7465442419052124 1.8397196531295776 3.58626389503479
Loss :  1.746985912322998 2.883319616317749 4.630305290222168
Loss :  1.7463021278381348 1.9101518392562866 3.656454086303711
  batch 20 loss: 1.7463021278381348, 1.9101518392562866, 3.656454086303711
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7472301721572876 2.693814277648926 4.441044330596924
Loss :  1.7478243112564087 2.226210832595825 3.9740352630615234
Loss :  1.7491298913955688 2.4212594032287598 4.170389175415039
Loss :  1.7519018650054932 2.3860843181610107 4.137986183166504
Loss :  1.7488563060760498 2.1936681270599365 3.9425244331359863
Loss :  1.745404601097107 3.123430013656616 4.868834495544434
Loss :  1.7505491971969604 2.832540988922119 4.583090305328369
Loss :  1.744896411895752 2.4268174171447754 4.171713829040527
Loss :  1.7492138147354126 2.973968505859375 4.723182201385498
Loss :  1.7453439235687256 2.5143344402313232 4.259678363800049
Loss :  1.753682017326355 2.301823139190674 4.055505275726318
Loss :  1.7488399744033813 2.192694902420044 3.941534996032715
Loss :  1.7447494268417358 1.733927845954895 3.478677272796631
Loss :  1.7448158264160156 2.0785951614379883 3.823410987854004
Loss :  1.7504178285598755 2.602292060852051 4.352709770202637
Loss :  1.750133752822876 1.9926477670669556 3.742781639099121
Loss :  1.7483893632888794 2.169235944747925 3.9176254272460938
Loss :  1.744706392288208 3.8845317363739014 5.629238128662109
Loss :  1.7486461400985718 2.909926414489746 4.658572673797607
Loss :  1.7485694885253906 1.5829815864562988 3.3315510749816895
  batch 40 loss: 1.7485694885253906, 1.5829815864562988, 3.3315510749816895
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7472580671310425 1.778188705444336 3.525446891784668
Loss :  1.748518705368042 1.487449288368225 3.2359681129455566
Loss :  1.749735951423645 1.7138086557388306 3.4635446071624756
Loss :  1.747541904449463 3.1939198970794678 4.941461563110352
Loss :  1.7497451305389404 2.1945791244506836 3.944324254989624
Loss :  1.748392105102539 2.658189535140991 4.406581878662109
Loss :  1.748018741607666 1.84769606590271 3.595714807510376
Loss :  1.7494639158248901 2.4993014335632324 4.248765468597412
Loss :  1.7516244649887085 2.903330087661743 4.654954433441162
Loss :  1.749070644378662 2.688037872314453 4.437108516693115
Loss :  1.7486155033111572 2.360308885574341 4.108924388885498
Loss :  1.749768853187561 2.442610740661621 4.192379474639893
Loss :  1.752794623374939 2.7102766036987305 4.463071346282959
Loss :  1.7482587099075317 2.9224603176116943 4.670719146728516
Loss :  1.7498797178268433 2.3458375930786133 4.095717430114746
Loss :  1.747741937637329 2.321498155593872 4.069240093231201
Loss :  1.751204252243042 2.666152000427246 4.417356491088867
Loss :  1.7531862258911133 2.74605131149292 4.499237537384033
Loss :  1.752948522567749 2.3780741691589355 4.1310224533081055
Loss :  1.7507566213607788 1.7690062522888184 3.5197629928588867
  batch 60 loss: 1.7507566213607788, 1.7690062522888184, 3.5197629928588867
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.750593662261963 2.2115070819854736 3.9621007442474365
Loss :  1.7517839670181274 2.262242555618286 4.014026641845703
Loss :  1.7511175870895386 1.4199501276016235 3.171067714691162
Loss :  1.7490382194519043 1.869507074356079 3.6185452938079834
Loss :  1.7481062412261963 1.3922780752182007 3.1403841972351074
Loss :  1.8194575309753418 4.4075775146484375 6.227035045623779
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8150079250335693 4.444937229156494 6.259944915771484
Loss :  1.816415548324585 4.214694499969482 6.031109809875488
Loss :  1.8185579776763916 4.239578723907471 6.058136940002441
Total LOSS train 4.112012536709125 valid 6.144056677818298
CE LOSS train 1.7485854387283326 valid 0.4546394944190979
Contrastive LOSS train 2.3634270924788257 valid 1.0598946809768677
EPOCH 276:
Loss :  1.7504420280456543 1.9289644956588745 3.6794066429138184
Loss :  1.7486931085586548 2.8819491863250732 4.630642414093018
Loss :  1.747581958770752 3.224583864212036 4.972166061401367
Loss :  1.7492778301239014 1.9698423147201538 3.7191200256347656
Loss :  1.7505581378936768 1.427375316619873 3.17793345451355
Loss :  1.7485175132751465 1.5552634000778198 3.303781032562256
Loss :  1.7502609491348267 2.142613172531128 3.892874240875244
Loss :  1.747228741645813 2.6766459941864014 4.423874855041504
Loss :  1.7481597661972046 3.162761926651001 4.910921573638916
Loss :  1.7412559986114502 1.938759684562683 3.6800155639648438
Loss :  1.7479865550994873 2.2501399517059326 3.99812650680542
Loss :  1.752867341041565 2.537370204925537 4.2902374267578125
Loss :  1.749075174331665 2.3918423652648926 4.140917778015137
Loss :  1.746686577796936 2.040714979171753 3.7874016761779785
Loss :  1.7503145933151245 2.5068917274475098 4.257206439971924
Loss :  1.7457383871078491 2.66145920753479 4.40719747543335
Loss :  1.7492908239364624 2.7187576293945312 4.468048572540283
Loss :  1.7471723556518555 3.1819987297058105 4.929171085357666
Loss :  1.7472436428070068 1.5051630735397339 3.252406597137451
Loss :  1.7469133138656616 2.5930655002593994 4.3399786949157715
  batch 20 loss: 1.7469133138656616, 2.5930655002593994, 4.3399786949157715
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7473961114883423 2.0817439556121826 3.8291401863098145
Loss :  1.7481437921524048 1.8703821897506714 3.618525981903076
Loss :  1.7495999336242676 2.8004274368286133 4.550027370452881
Loss :  1.7521387338638306 2.0898561477661133 3.8419947624206543
Loss :  1.7486377954483032 2.8121960163116455 4.560833930969238
Loss :  1.746058702468872 2.0664236545562744 3.8124823570251465
Loss :  1.7514643669128418 2.279122829437256 4.030587196350098
Loss :  1.746701955795288 2.194326877593994 3.9410288333892822
Loss :  1.7507749795913696 2.2660326957702637 4.016807556152344
Loss :  1.7456352710723877 2.186318874359131 3.9319541454315186
Loss :  1.7540189027786255 2.6408071517944336 4.3948259353637695
Loss :  1.7503286600112915 1.9473310708999634 3.697659730911255
Loss :  1.7463011741638184 1.9681974649429321 3.714498519897461
Loss :  1.7461851835250854 2.303769826889038 4.049954891204834
Loss :  1.750893473625183 2.735372304916382 4.486265659332275
Loss :  1.7508471012115479 1.691253423690796 3.4421005249023438
Loss :  1.7491103410720825 1.5790842771530151 3.3281946182250977
Loss :  1.746756672859192 3.0134341716766357 4.760190963745117
Loss :  1.7486249208450317 1.7731269598007202 3.521751880645752
Loss :  1.74947190284729 1.3887184858322144 3.138190269470215
  batch 40 loss: 1.74947190284729, 1.3887184858322144, 3.138190269470215
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7481911182403564 1.8098591566085815 3.5580501556396484
Loss :  1.7483030557632446 1.9874703884124756 3.7357735633850098
Loss :  1.7495179176330566 2.356956958770752 4.106474876403809
Loss :  1.747759461402893 2.718489408493042 4.466248989105225
Loss :  1.749703288078308 2.4292640686035156 4.178967475891113
Loss :  1.7482244968414307 2.304098606109619 4.052323341369629
Loss :  1.7476251125335693 1.8194135427474976 3.5670385360717773
Loss :  1.7481895685195923 2.0027456283569336 3.7509350776672363
Loss :  1.7496273517608643 2.076359272003174 3.825986623764038
Loss :  1.7472001314163208 1.8157037496566772 3.562903881072998
Loss :  1.7455179691314697 2.212698221206665 3.9582161903381348
Loss :  1.74729585647583 2.25323748588562 4.000533103942871
Loss :  1.7507538795471191 2.4516148567199707 4.20236873626709
Loss :  1.7462233304977417 2.3388242721557617 4.085047721862793
Loss :  1.7473515272140503 2.193444013595581 3.940795421600342
Loss :  1.7451164722442627 2.3717312812805176 4.116847991943359
Loss :  1.7488963603973389 2.362727165222168 4.111623764038086
Loss :  1.7507407665252686 1.938388466835022 3.68912935256958
Loss :  1.7506405115127563 2.451005220413208 4.201645851135254
Loss :  1.7481017112731934 2.0266809463500977 3.774782657623291
  batch 60 loss: 1.7481017112731934, 2.0266809463500977, 3.774782657623291
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.747618556022644 2.0341413021087646 3.781759738922119
Loss :  1.748929500579834 1.7809323072433472 3.5298619270324707
Loss :  1.7486799955368042 1.4881508350372314 3.236830711364746
Loss :  1.7460445165634155 1.7059704065322876 3.452014923095703
Loss :  1.7451329231262207 1.545324683189392 3.2904577255249023
Loss :  1.813353419303894 4.383018970489502 6.1963725090026855
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8092273473739624 4.408353805541992 6.217581272125244
Loss :  1.8105701208114624 4.267861366271973 6.078431606292725
Loss :  1.8121888637542725 4.241315841674805 6.053504943847656
Total LOSS train 3.955462488761315 valid 6.136472582817078
CE LOSS train 1.7483960023293128 valid 0.4530472159385681
Contrastive LOSS train 2.207066473594079 valid 1.0603289604187012
EPOCH 277:
Loss :  1.7484029531478882 2.340894937515259 4.089297771453857
Loss :  1.7467023134231567 1.948889136314392 3.695591449737549
Loss :  1.7453681230545044 1.6982500553131104 3.4436182975769043
Loss :  1.747891902923584 2.6294515132904053 4.37734317779541
Loss :  1.7490124702453613 2.124972343444824 3.8739848136901855
Loss :  1.7475746870040894 2.0795698165893555 3.8271446228027344
Loss :  1.748698353767395 2.468987226486206 4.217685699462891
Loss :  1.7460342645645142 2.1524462699890137 3.8984804153442383
Loss :  1.7473607063293457 2.5629396438598633 4.310300350189209
Loss :  1.7403541803359985 1.9311240911483765 3.671478271484375
Loss :  1.747041940689087 2.1066319942474365 3.8536739349365234
Loss :  1.7527676820755005 1.981142282485962 3.733910083770752
Loss :  1.748336672782898 3.313307285308838 5.061644077301025
Loss :  1.7465323209762573 1.9754499197006226 3.72198224067688
Loss :  1.7504374980926514 1.4591354131698608 3.2095727920532227
Loss :  1.7462069988250732 1.432401180267334 3.1786081790924072
Loss :  1.749003529548645 1.5983444452285767 3.3473479747772217
Loss :  1.747115969657898 2.43698787689209 4.184103965759277
Loss :  1.747452974319458 2.505023241043091 4.252476215362549
Loss :  1.7467567920684814 2.3865792751312256 4.133336067199707
  batch 20 loss: 1.7467567920684814, 2.3865792751312256, 4.133336067199707
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7475653886795044 2.819671154022217 4.567236423492432
Loss :  1.7477688789367676 1.5648581981658936 3.312627077102661
Loss :  1.7491899728775024 1.4467719793319702 3.1959619522094727
Loss :  1.7515486478805542 2.1337053775787354 3.88525390625
Loss :  1.7487248182296753 2.6268157958984375 4.375540733337402
Loss :  1.7448692321777344 3.2036657333374023 4.948534965515137
Loss :  1.7499288320541382 2.8917434215545654 4.641672134399414
Loss :  1.743949294090271 1.9936341047286987 3.7375833988189697
Loss :  1.74880051612854 3.5422730445861816 5.291073799133301
Loss :  1.7443056106567383 2.316498279571533 4.0608038902282715
Loss :  1.7536799907684326 2.6098179817199707 4.363497734069824
Loss :  1.7486108541488647 2.2617685794830322 4.010379314422607
Loss :  1.7443643808364868 1.3495548963546753 3.093919277191162
Loss :  1.7451527118682861 1.5666773319244385 3.3118300437927246
Loss :  1.7498749494552612 2.0041329860687256 3.7540078163146973
Loss :  1.749988317489624 2.1655869483947754 3.9155752658843994
Loss :  1.7475720643997192 1.819246768951416 3.5668187141418457
Loss :  1.7442830801010132 1.8477628231048584 3.592045783996582
Loss :  1.7477384805679321 2.0718748569488525 3.819613456726074
Loss :  1.7486685514450073 1.8487703800201416 3.5974388122558594
  batch 40 loss: 1.7486685514450073, 1.8487703800201416, 3.5974388122558594
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7476941347122192 2.383812665939331 4.13150691986084
Loss :  1.7484396696090698 1.7636942863464355 3.512134075164795
Loss :  1.75019109249115 2.917353630065918 4.667544841766357
Loss :  1.7486289739608765 2.5260093212127686 4.2746381759643555
Loss :  1.7510426044464111 3.1396169662475586 4.890659332275391
Loss :  1.7491734027862549 3.945908784866333 5.695082187652588
Loss :  1.749215841293335 2.053703546524048 3.802919387817383
Loss :  1.7504220008850098 2.1656274795532227 3.9160494804382324
Loss :  1.7521246671676636 2.366210460662842 4.118335247039795
Loss :  1.7494672536849976 1.8454095125198364 3.594876766204834
Loss :  1.7489466667175293 2.28243350982666 4.0313801765441895
Loss :  1.7505912780761719 2.1510095596313477 3.9016008377075195
Loss :  1.7540385723114014 2.0325253009796143 3.7865638732910156
Loss :  1.7486947774887085 1.8838269710540771 3.632521629333496
Loss :  1.7507154941558838 1.823944330215454 3.574659824371338
Loss :  1.7487455606460571 1.9022209644317627 3.6509666442871094
Loss :  1.7517162561416626 2.7087833881378174 4.4604997634887695
Loss :  1.7538806200027466 1.9523106813430786 3.706191301345825
Loss :  1.7534575462341309 2.422114133834839 4.175571441650391
Loss :  1.751146674156189 2.003326177597046 3.7544727325439453
  batch 60 loss: 1.751146674156189, 2.003326177597046, 3.7544727325439453
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.751192569732666 1.5702937841415405 3.321486473083496
Loss :  1.7523161172866821 2.1449942588806152 3.897310256958008
Loss :  1.751618504524231 1.992118239402771 3.743736743927002
Loss :  1.7504162788391113 2.1695303916931152 3.9199466705322266
Loss :  1.7492600679397583 1.461232304573059 3.2104923725128174
Loss :  1.813765525817871 4.321426868438721 6.135192394256592
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.810441493988037 4.379361629486084 6.189803123474121
Loss :  1.8111178874969482 4.183575630187988 5.994693756103516
Loss :  1.8136738538742065 4.041998863220215 5.855672836303711
Total LOSS train 3.946063723930946 valid 6.043840527534485
CE LOSS train 1.7487503620294425 valid 0.45341846346855164
Contrastive LOSS train 2.197313372905438 valid 1.0104997158050537
EPOCH 278:
Loss :  1.751185417175293 1.8293119668960571 3.5804972648620605
Loss :  1.749148964881897 2.098721504211426 3.847870349884033
Loss :  1.7480806112289429 2.2186405658721924 3.9667210578918457
Loss :  1.7497050762176514 1.9829570055007935 3.7326622009277344
Loss :  1.7516944408416748 1.671446442604065 3.4231410026550293
Loss :  1.7491945028305054 2.0468099117279053 3.796004295349121
Loss :  1.75165593624115 2.1802072525024414 3.931863307952881
Loss :  1.7484300136566162 1.7885438203811646 3.5369739532470703
Loss :  1.7495347261428833 1.8237231969833374 3.5732579231262207
Loss :  1.743301272392273 2.0732052326202393 3.8165063858032227
Loss :  1.7494230270385742 1.969942331314087 3.719365358352661
Loss :  1.753183126449585 2.0454602241516113 3.7986433506011963
Loss :  1.7505409717559814 2.464416265487671 4.214957237243652
Loss :  1.7480146884918213 1.9913444519042969 3.739359140396118
Loss :  1.7520831823349 2.1000587940216064 3.852141857147217
Loss :  1.7471923828125 2.233234405517578 3.980426788330078
Loss :  1.7504457235336304 2.091574192047119 3.842020034790039
Loss :  1.7486125230789185 1.8395565748214722 3.5881690979003906
Loss :  1.7482529878616333 1.685534119606018 3.4337871074676514
Loss :  1.7484766244888306 1.8155055046081543 3.5639820098876953
  batch 20 loss: 1.7484766244888306, 1.8155055046081543, 3.5639820098876953
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7490350008010864 1.7835628986358643 3.5325980186462402
Loss :  1.7494697570800781 1.846402645111084 3.595872402191162
Loss :  1.750903606414795 2.844898223876953 4.595801830291748
Loss :  1.7530546188354492 1.9011119604110718 3.6541666984558105
Loss :  1.7492451667785645 2.6170554161071777 4.366300582885742
Loss :  1.7464982271194458 2.2186639308929443 3.9651622772216797
Loss :  1.751300573348999 2.0628738403320312 3.8141744136810303
Loss :  1.7457331418991089 2.693345308303833 4.439078330993652
Loss :  1.7498440742492676 2.3910202980041504 4.140864372253418
Loss :  1.7453389167785645 1.9236798286437988 3.6690187454223633
Loss :  1.7538663148880005 2.1951935291290283 3.9490599632263184
Loss :  1.749488353729248 3.053281307220459 4.802769660949707
Loss :  1.7450546026229858 2.8305389881134033 4.5755934715271
Loss :  1.7449201345443726 2.1519410610198975 3.8968610763549805
Loss :  1.7503873109817505 2.901334285736084 4.651721477508545
Loss :  1.7499511241912842 1.8158639669418335 3.565814971923828
Loss :  1.7479174137115479 1.94081711769104 3.688734531402588
Loss :  1.7452716827392578 1.8884577751159668 3.6337294578552246
Loss :  1.747222900390625 1.9585109949111938 3.7057337760925293
Loss :  1.7480499744415283 1.7829563617706299 3.531006336212158
  batch 40 loss: 1.7480499744415283, 1.7829563617706299, 3.531006336212158
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7468397617340088 2.036930799484253 3.7837705612182617
Loss :  1.7469061613082886 2.254596471786499 4.001502513885498
Loss :  1.7485319375991821 1.9222426414489746 3.670774459838867
Loss :  1.7466545104980469 1.705859899520874 3.452514410018921
Loss :  1.7489312887191772 1.6404610872268677 3.389392375946045
Loss :  1.747605323791504 2.6421115398406982 4.389717102050781
Loss :  1.7469472885131836 1.5891238451004028 3.336071014404297
Loss :  1.7481180429458618 1.5994964838027954 3.3476145267486572
Loss :  1.7498054504394531 2.474151611328125 4.223957061767578
Loss :  1.74750554561615 1.6082617044448853 3.355767250061035
Loss :  1.746152400970459 1.9029515981674194 3.649104118347168
Loss :  1.7477941513061523 2.3283121585845947 4.076106071472168
Loss :  1.7508912086486816 1.7732278108596802 3.5241189002990723
Loss :  1.7469006776809692 2.4156038761138916 4.16250467300415
Loss :  1.7481776475906372 2.0973219871520996 3.8454995155334473
Loss :  1.7452902793884277 2.65618896484375 4.401479244232178
Loss :  1.748977541923523 2.296398639678955 4.045376300811768
Loss :  1.7510675191879272 1.9178569316864014 3.668924331665039
Loss :  1.7512837648391724 2.739593505859375 4.490877151489258
Loss :  1.7485243082046509 2.3568921089172363 4.105416297912598
  batch 60 loss: 1.7485243082046509, 2.3568921089172363, 4.105416297912598
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7482216358184814 3.0470170974731445 4.795238494873047
Loss :  1.749440312385559 1.7405354976654053 3.489975929260254
Loss :  1.7487701177597046 1.6031023263931274 3.351872444152832
Loss :  1.7469946146011353 2.2844057083129883 4.031400203704834
Loss :  1.7460733652114868 1.5362153053283691 3.2822885513305664
Loss :  1.810205101966858 4.377804279327393 6.188009262084961
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8061656951904297 4.3777265548706055 6.183892250061035
Loss :  1.807231068611145 4.302342414855957 6.1095733642578125
Loss :  1.8098418712615967 4.154968738555908 5.964810371398926
Total LOSS train 3.855133471122155 valid 6.111571311950684
CE LOSS train 1.7486632915643545 valid 0.45246046781539917
Contrastive LOSS train 2.1064702015656693 valid 1.038742184638977
EPOCH 279:
Loss :  1.7486072778701782 1.8499644994735718 3.59857177734375
Loss :  1.7463855743408203 2.666167974472046 4.412553787231445
Loss :  1.7455521821975708 2.7760345935821533 4.521586894989014
Loss :  1.7475203275680542 2.5126869678497314 4.260207176208496
Loss :  1.7501546144485474 1.8050434589385986 3.5551981925964355
Loss :  1.747045636177063 1.7116148471832275 3.45866060256958
Loss :  1.7498141527175903 2.018197774887085 3.768012046813965
Loss :  1.7461426258087158 1.9137765169143677 3.659919261932373
Loss :  1.7476822137832642 1.8158924579620361 3.56357479095459
Loss :  1.7408709526062012 1.8427999019622803 3.5836708545684814
Loss :  1.7472808361053467 2.0750627517700195 3.822343587875366
Loss :  1.7525674104690552 2.547520399093628 4.300087928771973
Loss :  1.7487359046936035 1.6699453592300415 3.4186811447143555
Loss :  1.7470146417617798 1.8065102100372314 3.553524971008301
Loss :  1.7510367631912231 2.4067747592926025 4.157811641693115
Loss :  1.7465925216674805 2.1655399799346924 3.912132501602173
Loss :  1.7497594356536865 2.090118646621704 3.8398780822753906
Loss :  1.747868537902832 2.730072259902954 4.477940559387207
Loss :  1.7479257583618164 1.9278416633605957 3.675767421722412
Loss :  1.7473580837249756 2.0400846004486084 3.787442684173584
  batch 20 loss: 1.7473580837249756, 2.0400846004486084, 3.787442684173584
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7482331991195679 2.297293186187744 4.045526504516602
Loss :  1.748763084411621 3.024726629257202 4.773489952087402
Loss :  1.7502021789550781 2.9313056468963623 4.6815080642700195
Loss :  1.7522633075714111 2.4165749549865723 4.1688385009765625
Loss :  1.7490777969360352 2.274430990219116 4.0235090255737305
Loss :  1.7461440563201904 3.6721620559692383 5.418306350708008
Loss :  1.7510714530944824 3.9882543087005615 5.739325523376465
Loss :  1.7456769943237305 2.5334784984588623 4.279155731201172
Loss :  1.749902367591858 2.67563533782959 4.425537586212158
Loss :  1.7446428537368774 2.267669439315796 4.012312412261963
Loss :  1.753725290298462 2.354771375656128 4.10849666595459
Loss :  1.7498494386672974 1.7316006422042847 3.481450080871582
Loss :  1.7456648349761963 1.5509803295135498 3.296645164489746
Loss :  1.7457301616668701 1.9135775566101074 3.6593077182769775
Loss :  1.750960111618042 2.7085254192352295 4.4594855308532715
Loss :  1.7509816884994507 2.2788047790527344 4.029786586761475
Loss :  1.7491379976272583 2.119586706161499 3.868724822998047
Loss :  1.7469605207443237 1.6297026872634888 3.3766632080078125
Loss :  1.7487919330596924 2.436749219894409 4.185541152954102
Loss :  1.749759554862976 3.6007516384124756 5.350511074066162
  batch 40 loss: 1.749759554862976, 3.6007516384124756, 5.350511074066162
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7483406066894531 3.4960737228393555 5.244414329528809
Loss :  1.7488387823104858 2.3143789768218994 4.063217639923096
Loss :  1.7497810125350952 1.7320201396942139 3.4818010330200195
Loss :  1.7484619617462158 2.2278029918670654 3.9762649536132812
Loss :  1.7500340938568115 1.767153024673462 3.5171871185302734
Loss :  1.7481176853179932 2.332885980606079 4.081003665924072
Loss :  1.7478822469711304 2.694985866546631 4.442868232727051
Loss :  1.7483513355255127 2.435617208480835 4.183968544006348
Loss :  1.7496473789215088 2.0388786792755127 3.7885260581970215
Loss :  1.747420310974121 1.9371683597564697 3.684588670730591
Loss :  1.7452149391174316 2.009615421295166 3.7548303604125977
Loss :  1.7471240758895874 1.9083219766616821 3.6554460525512695
Loss :  1.75059974193573 2.3482282161712646 4.098827838897705
Loss :  1.74654221534729 2.0626723766326904 3.8092145919799805
Loss :  1.7478781938552856 1.7824369668960571 3.5303151607513428
Loss :  1.7453263998031616 1.829006314277649 3.5743327140808105
Loss :  1.7487095594406128 2.3089022636413574 4.05761194229126
Loss :  1.7511301040649414 1.7286574840545654 3.479787588119507
Loss :  1.7510476112365723 2.345916986465454 4.0969648361206055
Loss :  1.7482473850250244 2.061102867126465 3.8093502521514893
  batch 60 loss: 1.7482473850250244, 2.061102867126465, 3.8093502521514893
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7481186389923096 2.068335771560669 3.8164544105529785
Loss :  1.7494406700134277 1.9822447299957275 3.7316854000091553
Loss :  1.7492271661758423 1.9260743856430054 3.6753015518188477
Loss :  1.747030258178711 2.2499570846557617 3.9969873428344727
Loss :  1.7464300394058228 1.446993112564087 3.193423271179199
Loss :  1.8109407424926758 4.431009292602539 6.241950035095215
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8068444728851318 4.396627426147461 6.203472137451172
Loss :  1.8081127405166626 4.304686069488525 6.112798690795898
Loss :  1.8103644847869873 4.1103515625 5.920716285705566
Total LOSS train 3.991631709612333 valid 6.119734287261963
CE LOSS train 1.7483445644378661 valid 0.4525911211967468
Contrastive LOSS train 2.2432871066606963 valid 1.027587890625
EPOCH 280:
Loss :  1.7494347095489502 1.9382930994033813 3.687727928161621
Loss :  1.747945785522461 2.1413538455963135 3.8892996311187744
Loss :  1.746659755706787 1.6780658960342407 3.4247255325317383
Loss :  1.7492344379425049 1.5574716329574585 3.306705951690674
Loss :  1.7500702142715454 1.4851176738739014 3.2351880073547363
Loss :  1.748932957649231 1.7259639501571655 3.4748969078063965
Loss :  1.7497063875198364 2.058926582336426 3.8086328506469727
Loss :  1.7475998401641846 1.725039005279541 3.4726388454437256
Loss :  1.7487059831619263 1.8837471008300781 3.632452964782715
Loss :  1.7418521642684937 1.8673195838928223 3.6091718673706055
Loss :  1.7489910125732422 2.2795348167419434 4.0285258293151855
Loss :  1.7540334463119507 2.1074025630950928 3.861435890197754
Loss :  1.7499783039093018 1.99479341506958 3.744771718978882
Loss :  1.7478002309799194 1.9887669086456299 3.7365670204162598
Loss :  1.7521183490753174 2.1414849758148193 3.8936033248901367
Loss :  1.7467540502548218 1.971292495727539 3.7180466651916504
Loss :  1.7503485679626465 2.014732599258423 3.7650811672210693
Loss :  1.747178316116333 1.486090064048767 3.2332682609558105
Loss :  1.7485289573669434 1.3279471397399902 3.0764760971069336
Loss :  1.747687816619873 1.5948148965835571 3.3425025939941406
  batch 20 loss: 1.747687816619873, 1.5948148965835571, 3.3425025939941406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7482761144638062 1.7993007898330688 3.547576904296875
Loss :  1.749170184135437 1.6294533014297485 3.3786234855651855
Loss :  1.7501192092895508 1.6948063373565674 3.444925546646118
Loss :  1.752774715423584 1.8356258869171143 3.5884006023406982
Loss :  1.7497330904006958 2.1199028491973877 3.869636058807373
Loss :  1.746416449546814 2.0190062522888184 3.765422821044922
Loss :  1.751509189605713 1.9659104347229004 3.7174196243286133
Loss :  1.7459172010421753 1.937461495399475 3.6833786964416504
Loss :  1.7508373260498047 2.339740753173828 4.090578079223633
Loss :  1.7448714971542358 2.5448408126831055 4.289712429046631
Loss :  1.7546952962875366 2.3687479496002197 4.123443126678467
Loss :  1.750755786895752 2.525393009185791 4.276148796081543
Loss :  1.7461363077163696 2.5684354305267334 4.314571857452393
Loss :  1.7467641830444336 1.5889626741409302 3.335726737976074
Loss :  1.7517329454421997 2.506669521331787 4.258402347564697
Loss :  1.751908540725708 2.210017442703247 3.961925983428955
Loss :  1.749753475189209 1.9460948705673218 3.6958484649658203
Loss :  1.7471706867218018 2.0675721168518066 3.8147428035736084
Loss :  1.7487331628799438 2.0994882583618164 3.8482213020324707
Loss :  1.7501616477966309 2.5249598026275635 4.275121688842773
  batch 40 loss: 1.7501616477966309, 2.5249598026275635, 4.275121688842773
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7484887838363647 3.0855085849761963 4.8339972496032715
Loss :  1.7485018968582153 1.8756139278411865 3.6241159439086914
Loss :  1.750275731086731 2.181701421737671 3.9319772720336914
Loss :  1.7481226921081543 2.592515468597412 4.340638160705566
Loss :  1.7502857446670532 1.5996488332748413 3.3499345779418945
Loss :  1.7487742900848389 2.0022828578948975 3.7510571479797363
Loss :  1.7479485273361206 2.3979146480560303 4.145863056182861
Loss :  1.7487443685531616 1.5746893882751465 3.3234338760375977
Loss :  1.7495362758636475 2.211826801300049 3.9613630771636963
Loss :  1.747829794883728 2.2726666927337646 4.020496368408203
Loss :  1.7459478378295898 1.9066344499588013 3.6525821685791016
Loss :  1.7477703094482422 1.6493394374847412 3.3971097469329834
Loss :  1.7510186433792114 2.0052602291107178 3.7562789916992188
Loss :  1.7470370531082153 2.1723108291625977 3.9193477630615234
Loss :  1.7483069896697998 2.176546335220337 3.9248533248901367
Loss :  1.7455579042434692 2.126171827316284 3.871729850769043
Loss :  1.7492362260818481 2.4007537364959717 4.149990081787109
Loss :  1.7513067722320557 2.128066301345825 3.879373073577881
Loss :  1.7516087293624878 3.149761438369751 4.901370048522949
Loss :  1.748922348022461 2.479794979095459 4.22871732711792
  batch 60 loss: 1.748922348022461, 2.479794979095459, 4.22871732711792
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7487694025039673 2.974874258041382 4.723643779754639
Loss :  1.7502562999725342 2.4677388668060303 4.2179951667785645
Loss :  1.7500159740447998 2.5091936588287354 4.259209632873535
Loss :  1.7478251457214355 3.038863182067871 4.786688327789307
Loss :  1.7469420433044434 1.9831629991531372 3.730104923248291
Loss :  1.803760290145874 4.31417989730835 6.1179399490356445
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.800516963005066 4.341650485992432 6.142167568206787
Loss :  1.8011479377746582 4.33939266204834 6.140540599822998
Loss :  1.8050552606582642 4.246212482452393 6.051267623901367
Total LOSS train 3.8446679592132567 valid 6.112978935241699
CE LOSS train 1.7489235089375423 valid 0.45126381516456604
Contrastive LOSS train 2.0957444521097037 valid 1.0615531206130981
EPOCH 281:
Loss :  1.749979019165039 2.258251905441284 4.008231163024902
Loss :  1.7485381364822388 1.789016842842102 3.537554979324341
Loss :  1.7471121549606323 1.4491826295852661 3.1962947845458984
Loss :  1.7495739459991455 1.6855965852737427 3.4351706504821777
Loss :  1.7504560947418213 1.7524964809417725 3.5029525756835938
Loss :  1.749055027961731 1.8663437366485596 3.61539888381958
Loss :  1.7499723434448242 2.5071072578430176 4.257079601287842
Loss :  1.7471903562545776 1.9132843017578125 3.6604747772216797
Loss :  1.747982382774353 2.34063720703125 4.088619709014893
Loss :  1.7409917116165161 2.6661741733551025 4.407166004180908
Loss :  1.748071551322937 3.1175875663757324 4.865659236907959
Loss :  1.7534745931625366 1.8200033903121948 3.5734779834747314
Loss :  1.749429702758789 1.9353382587432861 3.684767961502075
Loss :  1.7472596168518066 2.3734095096588135 4.120669364929199
Loss :  1.7510961294174194 3.2119433879852295 4.963039398193359
Loss :  1.7469311952590942 2.6914517879486084 4.438383102416992
Loss :  1.7498669624328613 2.878770589828491 4.628637313842773
Loss :  1.7481486797332764 2.2065510749816895 3.954699754714966
Loss :  1.7484829425811768 1.730332851409912 3.478815793991089
Loss :  1.7477080821990967 1.9675804376602173 3.7152886390686035
  batch 20 loss: 1.7477080821990967, 1.9675804376602173, 3.7152886390686035
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7487555742263794 1.790231704711914 3.538987159729004
Loss :  1.7493810653686523 2.133234739303589 3.882615804672241
Loss :  1.7505077123641968 1.8028652667999268 3.553372859954834
Loss :  1.7532932758331299 2.4159367084503174 4.169229984283447
Loss :  1.7495570182800293 2.5010993480682373 4.2506561279296875
Loss :  1.746634602546692 2.0217697620391846 3.768404483795166
Loss :  1.7516752481460571 2.075627565383911 3.827302932739258
Loss :  1.746300220489502 3.1004443168640137 4.846744537353516
Loss :  1.750290870666504 2.0026235580444336 3.7529144287109375
Loss :  1.7461142539978027 2.342334032058716 4.088448524475098
Loss :  1.7542810440063477 2.586089611053467 4.3403706550598145
Loss :  1.750146508216858 1.7600873708724976 3.5102338790893555
Loss :  1.7464277744293213 1.918753743171692 3.6651816368103027
Loss :  1.746145248413086 2.092240571975708 3.838385820388794
Loss :  1.7515802383422852 2.580322742462158 4.331902980804443
Loss :  1.751109004020691 2.6658215522766113 4.416930675506592
Loss :  1.748673439025879 2.125072956085205 3.873746395111084
Loss :  1.7456239461898804 2.150212049484253 3.8958358764648438
Loss :  1.747678279876709 1.7420272827148438 3.4897055625915527
Loss :  1.7481220960617065 1.984769582748413 3.73289155960083
  batch 40 loss: 1.7481220960617065, 1.984769582748413, 3.73289155960083
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7471357583999634 2.5421512126922607 4.289287090301514
Loss :  1.7474133968353271 1.6191158294677734 3.3665292263031006
Loss :  1.749330759048462 2.0998194217681885 3.8491501808166504
Loss :  1.7474309206008911 2.6371042728424072 4.384535312652588
Loss :  1.7495603561401367 1.5861681699752808 3.335728645324707
Loss :  1.747455358505249 1.5402714014053345 3.287726879119873
Loss :  1.7469778060913086 1.4031318426132202 3.1501097679138184
Loss :  1.748811960220337 2.217437267303467 3.9662492275238037
Loss :  1.7490503787994385 1.9171006679534912 3.6661510467529297
Loss :  1.748382329940796 1.795609951019287 3.543992280960083
Loss :  1.7465546131134033 2.2892005443573 4.035755157470703
Loss :  1.7484263181686401 2.057070016860962 3.8054962158203125
Loss :  1.7515394687652588 2.2351059913635254 3.986645460128784
Loss :  1.7480101585388184 2.753278970718384 4.501289367675781
Loss :  1.7494726181030273 2.817664384841919 4.567136764526367
Loss :  1.7466113567352295 2.2725937366485596 4.019205093383789
Loss :  1.7497788667678833 2.2027432918548584 3.9525222778320312
Loss :  1.7519187927246094 2.0088813304901123 3.7608001232147217
Loss :  1.7520778179168701 2.618831157684326 4.370908737182617
Loss :  1.749226450920105 2.4574501514434814 4.206676483154297
  batch 60 loss: 1.749226450920105, 2.4574501514434814, 4.206676483154297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7487341165542603 3.072089910507202 4.820824146270752
Loss :  1.7501250505447388 2.8694348335266113 4.6195597648620605
Loss :  1.749362826347351 1.7294620275497437 3.4788248538970947
Loss :  1.7474842071533203 2.238049268722534 3.9855334758758545
Loss :  1.746358036994934 1.3584349155426025 3.104793071746826
Loss :  1.8038172721862793 4.382306098937988 6.186123371124268
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8010480403900146 4.32818603515625 6.129234313964844
Loss :  1.80097234249115 4.2612786293029785 6.062251091003418
Loss :  1.8055305480957031 4.310999393463135 6.116529941558838
Total LOSS train 3.9378719109755296 valid 6.123534679412842
CE LOSS train 1.7487822734392606 valid 0.4513826370239258
Contrastive LOSS train 2.1890896155284003 valid 1.0777498483657837
EPOCH 282:
Loss :  1.7492785453796387 2.0406556129455566 3.7899341583251953
Loss :  1.7471082210540771 1.5832937955856323 3.33040189743042
Loss :  1.74618399143219 1.2706773281097412 3.0168614387512207
Loss :  1.7489827871322632 1.7709388732910156 3.5199217796325684
Loss :  1.7501006126403809 2.571406126022339 4.321506500244141
Loss :  1.7484347820281982 2.7504382133483887 4.498872756958008
Loss :  1.7496579885482788 2.19673228263855 3.946390151977539
Loss :  1.7469918727874756 2.454404354095459 4.2013959884643555
Loss :  1.7486357688903809 2.523444175720215 4.272079944610596
Loss :  1.74174964427948 1.7738713026046753 3.5156209468841553
Loss :  1.7483909130096436 2.048426389694214 3.7968173027038574
Loss :  1.7536406517028809 2.9726791381835938 4.726319789886475
Loss :  1.7495543956756592 2.0069222450256348 3.756476640701294
Loss :  1.7476576566696167 2.4285380840301514 4.1761956214904785
Loss :  1.7514013051986694 2.439056634902954 4.190457820892334
Loss :  1.7473374605178833 1.9539053440093994 3.7012429237365723
Loss :  1.749866008758545 1.5983202457427979 3.3481862545013428
Loss :  1.7479240894317627 1.844970464706421 3.5928945541381836
Loss :  1.7483692169189453 1.5901240110397339 3.3384933471679688
Loss :  1.746991753578186 2.464089870452881 4.211081504821777
  batch 20 loss: 1.746991753578186, 2.464089870452881, 4.211081504821777
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7479586601257324 2.0718915462493896 3.819850206375122
Loss :  1.748550295829773 2.3777177333831787 4.126267910003662
Loss :  1.74971604347229 2.6691887378692627 4.418904781341553
Loss :  1.7525979280471802 1.9912523031234741 3.7438502311706543
Loss :  1.749666690826416 2.296464204788208 4.046131134033203
Loss :  1.7461084127426147 2.1226000785827637 3.868708610534668
Loss :  1.7513444423675537 2.074383020401001 3.8257274627685547
Loss :  1.7457785606384277 2.692206621170044 4.437985420227051
Loss :  1.750340461730957 1.9807988405227661 3.7311391830444336
Loss :  1.7466487884521484 2.0256001949310303 3.7722489833831787
Loss :  1.7549586296081543 1.9030792713165283 3.6580379009246826
Loss :  1.7503533363342285 1.6511764526367188 3.4015297889709473
Loss :  1.746808409690857 1.5839859247207642 3.330794334411621
Loss :  1.7461602687835693 2.2593882083892822 4.005548477172852
Loss :  1.7518666982650757 2.022387742996216 3.774254322052002
Loss :  1.7512527704238892 1.9873789548873901 3.7386317253112793
Loss :  1.7495545148849487 1.955590009689331 3.7051444053649902
Loss :  1.7470022439956665 2.210878849029541 3.957880973815918
Loss :  1.7496869564056396 1.9557032585144043 3.705390214920044
Loss :  1.7497543096542358 2.166315793991089 3.916069984436035
  batch 40 loss: 1.7497543096542358, 2.166315793991089, 3.916069984436035
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7494056224822998 2.2167418003082275 3.9661474227905273
Loss :  1.7499938011169434 1.9882922172546387 3.738286018371582
Loss :  1.7507662773132324 2.126544713973999 3.8773109912872314
Loss :  1.7497011423110962 2.214067220687866 3.963768482208252
Loss :  1.7514703273773193 2.9604289531707764 4.711899280548096
Loss :  1.7494059801101685 2.3243072032928467 4.073713302612305
Loss :  1.749045729637146 2.268551826477051 4.017597675323486
Loss :  1.7501070499420166 2.1928563117980957 3.9429633617401123
Loss :  1.7511346340179443 1.835053563117981 3.586188316345215
Loss :  1.74918532371521 1.6889866590499878 3.438171863555908
Loss :  1.7478641271591187 1.9331949949264526 3.6810591220855713
Loss :  1.7494491338729858 1.8113036155700684 3.5607528686523438
Loss :  1.7522636651992798 1.9904857873916626 3.7427494525909424
Loss :  1.7484177350997925 2.606044292449951 4.354462146759033
Loss :  1.750060796737671 2.1174051761627197 3.8674659729003906
Loss :  1.7470570802688599 2.912411689758301 4.659468650817871
Loss :  1.7504518032073975 2.123950958251953 3.8744027614593506
Loss :  1.7529689073562622 1.8121930360794067 3.565161943435669
Loss :  1.7529118061065674 2.3671658039093018 4.120077610015869
Loss :  1.7496802806854248 1.925520658493042 3.675200939178467
  batch 60 loss: 1.7496802806854248, 1.925520658493042, 3.675200939178467
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7493550777435303 2.1376311779022217 3.886986255645752
Loss :  1.7506166696548462 2.583407163619995 4.334023952484131
Loss :  1.7497732639312744 2.545258045196533 4.295031547546387
Loss :  1.7479557991027832 2.332322359085083 4.080278396606445
Loss :  1.7472789287567139 1.445057988166809 3.1923370361328125
Loss :  1.8049815893173218 4.378168106079102 6.183149814605713
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.8020122051239014 4.353082180023193 6.155094146728516
Loss :  1.802371859550476 4.280402660369873 6.082774639129639
Loss :  1.8058271408081055 4.195889949798584 6.0017170906066895
Total LOSS train 3.8837038883796104 valid 6.105683922767639
CE LOSS train 1.7492413392433754 valid 0.45145678520202637
Contrastive LOSS train 2.134462545468257 valid 1.048972487449646
EPOCH 283:
Loss :  1.750065803527832 1.7343754768371582 3.4844412803649902
Loss :  1.7478582859039307 2.3661813735961914 4.114039421081543
Loss :  1.747110366821289 1.8861764669418335 3.633286952972412
Loss :  1.7495501041412354 2.2919983863830566 4.041548728942871
Loss :  1.7514880895614624 2.026836633682251 3.778324604034424
Loss :  1.7495025396347046 1.8325549364089966 3.582057476043701
Loss :  1.7515344619750977 2.016652822494507 3.7681872844696045
Loss :  1.7485915422439575 1.8391362428665161 3.5877277851104736
Loss :  1.749592661857605 2.1121463775634766 3.861739158630371
Loss :  1.7432856559753418 1.8574891090393066 3.6007747650146484
Loss :  1.7491779327392578 2.6677281856536865 4.416906356811523
Loss :  1.753490686416626 2.427443027496338 4.180933952331543
Loss :  1.7503831386566162 1.4034615755081177 3.1538448333740234
Loss :  1.7482795715332031 1.6567457914352417 3.4050254821777344
Loss :  1.7518792152404785 1.856536865234375 3.6084160804748535
Loss :  1.7478166818618774 1.688393473625183 3.4362101554870605
Loss :  1.7505570650100708 1.825923204421997 3.5764803886413574
Loss :  1.7488727569580078 2.232814311981201 3.981687068939209
Loss :  1.7486859560012817 2.238036870956421 3.986722946166992
Loss :  1.7479803562164307 2.1309573650360107 3.8789377212524414
  batch 20 loss: 1.7479803562164307, 2.1309573650360107, 3.8789377212524414
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7486647367477417 1.9360289573669434 3.6846938133239746
Loss :  1.7490251064300537 3.0015103816986084 4.750535488128662
Loss :  1.7502905130386353 1.648574709892273 3.398865222930908
Loss :  1.7529380321502686 1.757596492767334 3.5105345249176025
Loss :  1.7499730587005615 2.0757813453674316 3.825754404067993
Loss :  1.7464773654937744 2.2957804203033447 4.042257785797119
Loss :  1.751612901687622 3.4596493244171143 5.211262226104736
Loss :  1.7459796667099 2.711799144744873 4.4577789306640625
Loss :  1.7504661083221436 2.489560604095459 4.240026473999023
Loss :  1.7457301616668701 2.0587196350097656 3.8044497966766357
Loss :  1.754547357559204 2.000542640686035 3.7550899982452393
Loss :  1.7501275539398193 1.784811019897461 3.5349385738372803
Loss :  1.74625825881958 2.1904637813568115 3.9367220401763916
Loss :  1.7463014125823975 2.4253861904144287 4.171687602996826
Loss :  1.7519744634628296 2.534808397293091 4.286782741546631
Loss :  1.7515828609466553 2.272439479827881 4.024022102355957
Loss :  1.749552845954895 4.080776691436768 5.830329418182373
Loss :  1.746683120727539 2.247263193130493 3.9939463138580322
Loss :  1.7493778467178345 1.7824310064315796 3.531808853149414
Loss :  1.7493162155151367 1.7163615226745605 3.4656777381896973
  batch 40 loss: 1.7493162155151367, 1.7163615226745605, 3.4656777381896973
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.748427391052246 2.2073206901550293 3.9557480812072754
Loss :  1.7490673065185547 1.700587511062622 3.4496548175811768
Loss :  1.7502902746200562 2.066009998321533 3.816300392150879
Loss :  1.7483432292938232 1.8524396419525146 3.600782871246338
Loss :  1.7503899335861206 2.1222541332244873 3.8726439476013184
Loss :  1.748314380645752 1.7773669958114624 3.525681495666504
Loss :  1.747417688369751 1.7740236520767212 3.5214414596557617
Loss :  1.7491213083267212 2.0077004432678223 3.756821632385254
Loss :  1.7496428489685059 2.155839681625366 3.905482530593872
Loss :  1.7484883069992065 2.6865339279174805 4.435022354125977
Loss :  1.746609091758728 1.9097529649734497 3.6563620567321777
Loss :  1.7483546733856201 3.1699342727661133 4.9182891845703125
Loss :  1.751280665397644 1.589882493019104 3.341163158416748
Loss :  1.747503638267517 1.7348532676696777 3.4823570251464844
Loss :  1.748860239982605 2.086447238922119 3.8353075981140137
Loss :  1.7457213401794434 1.681937575340271 3.427659034729004
Loss :  1.749608039855957 2.768465280532837 4.518073081970215
Loss :  1.751659631729126 2.562178134918213 4.313838005065918
Loss :  1.752066731452942 3.035881757736206 4.7879486083984375
Loss :  1.749445915222168 2.0433590412139893 3.7928049564361572
  batch 60 loss: 1.749445915222168, 2.0433590412139893, 3.7928049564361572
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.749220371246338 1.9203275442123413 3.6695480346679688
Loss :  1.7508054971694946 1.6897894144058228 3.4405949115753174
Loss :  1.7502601146697998 2.007215738296509 3.7574758529663086
Loss :  1.7478458881378174 2.083667755126953 3.8315136432647705
Loss :  1.7471498250961304 1.954336166381836 3.701486110687256
Loss :  1.80604088306427 4.338381290435791 6.1444220542907715
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.802569031715393 4.356658458709717 6.15922737121582
Loss :  1.803358793258667 4.285286903381348 6.088645935058594
Loss :  1.8068578243255615 4.343752861022949 6.15061092376709
Total LOSS train 3.8899762667142426 valid 6.135726571083069
CE LOSS train 1.7492073352520283 valid 0.4517144560813904
Contrastive LOSS train 2.1407689039523783 valid 1.0859382152557373
EPOCH 284:
Loss :  1.7502143383026123 1.798164963722229 3.548379421234131
Loss :  1.748346209526062 1.8582154512405396 3.6065616607666016
Loss :  1.7472610473632812 2.217602014541626 3.9648630619049072
Loss :  1.749967098236084 1.6146409511566162 3.3646080493927
Loss :  1.7508634328842163 1.5252611637115479 3.2761244773864746
Loss :  1.7496532201766968 2.0318896770477295 3.7815427780151367
Loss :  1.750227689743042 2.098313093185425 3.848540782928467
Loss :  1.7477586269378662 1.1867642402648926 2.934522867202759
Loss :  1.7487448453903198 1.3577722311019897 3.1065170764923096
Loss :  1.74180269241333 1.3722317218780518 3.114034414291382
Loss :  1.748891830444336 1.9090864658355713 3.6579782962799072
Loss :  1.7543123960494995 1.8659402132034302 3.6202526092529297
Loss :  1.750535011291504 1.6139793395996094 3.3645143508911133
Loss :  1.7484474182128906 3.143026828765869 4.89147424697876
Loss :  1.7524696588516235 1.9065444469451904 3.6590142250061035
Loss :  1.748040795326233 1.6346569061279297 3.382697582244873
Loss :  1.7512749433517456 1.9934489727020264 3.7447237968444824
Loss :  1.7491589784622192 2.0239315032958984 3.773090362548828
Loss :  1.7497971057891846 1.3335974216461182 3.0833945274353027
Loss :  1.7491726875305176 1.5977065563201904 3.346879243850708
  batch 20 loss: 1.7491726875305176, 1.5977065563201904, 3.346879243850708
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7499386072158813 1.6721402406692505 3.422078847885132
Loss :  1.7505437135696411 1.954935073852539 3.7054786682128906
Loss :  1.7521308660507202 1.8207576274871826 3.5728883743286133
Loss :  1.7543481588363647 1.923435091972351 3.677783250808716
Loss :  1.751582145690918 2.588648557662964 4.340230941772461
Loss :  1.7488631010055542 1.965912103652954 3.7147750854492188
Loss :  1.7537620067596436 2.4031102657318115 4.156872272491455
Loss :  1.7492543458938599 2.14654803276062 3.8958024978637695
Loss :  1.7533599138259888 1.6236541271209717 3.37701416015625
Loss :  1.7479922771453857 2.388514518737793 4.136507034301758
Loss :  1.7559199333190918 2.272573947906494 4.028493881225586
Loss :  1.7529162168502808 2.183448314666748 3.9363646507263184
Loss :  1.7485970258712769 1.8679172992706299 3.616514205932617
Loss :  1.7491068840026855 1.9832912683486938 3.73239803314209
Loss :  1.7534533739089966 2.378227472305298 4.131680965423584
Loss :  1.7535465955734253 1.8255400657653809 3.5790867805480957
Loss :  1.7514643669128418 2.0415496826171875 3.7930140495300293
Loss :  1.7492419481277466 1.7208292484283447 3.470071315765381
Loss :  1.7503254413604736 1.646416425704956 3.3967418670654297
Loss :  1.7517105340957642 1.5219953060150146 3.2737059593200684
  batch 40 loss: 1.7517105340957642, 1.5219953060150146, 3.2737059593200684
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7498760223388672 1.7400482892990112 3.489924430847168
Loss :  1.7495667934417725 1.4116790294647217 3.161245822906494
Loss :  1.7514172792434692 1.5397557020187378 3.291172981262207
Loss :  1.7484750747680664 1.8948776721954346 3.643352746963501
Loss :  1.751123070716858 1.6997231245040894 3.4508461952209473
Loss :  1.7501001358032227 1.809354305267334 3.5594544410705566
Loss :  1.7489176988601685 1.8496590852737427 3.598576784133911
Loss :  1.7501481771469116 1.834436058998108 3.5845842361450195
Loss :  1.7515226602554321 1.8645846843719482 3.61610746383667
Loss :  1.7492536306381226 1.8777728080749512 3.6270265579223633
Loss :  1.7480313777923584 1.843588948249817 3.591620445251465
Loss :  1.749584436416626 1.8765360116958618 3.6261205673217773
Loss :  1.752205729484558 1.7526408433914185 3.5048465728759766
Loss :  1.7482199668884277 1.8356221914291382 3.5838422775268555
Loss :  1.7496333122253418 2.1116127967834473 3.861246109008789
Loss :  1.746652364730835 2.176551103591919 3.923203468322754
Loss :  1.750829815864563 2.2248470783233643 3.975677013397217
Loss :  1.7528270483016968 2.2703657150268555 4.023192882537842
Loss :  1.7528852224349976 2.144061326980591 3.896946430206299
Loss :  1.7502070665359497 1.8696290254592896 3.6198360919952393
  batch 60 loss: 1.7502070665359497, 1.8696290254592896, 3.6198360919952393
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7500355243682861 2.405010938644409 4.155046463012695
Loss :  1.7511287927627563 1.9725189208984375 3.7236475944519043
Loss :  1.7511199712753296 2.1699719429016113 3.9210920333862305
Loss :  1.7487941980361938 2.138850688934326 3.8876447677612305
Loss :  1.7478948831558228 1.06831693649292 2.816211700439453
Loss :  1.8079503774642944 4.411491394042969 6.219441890716553
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.8044564723968506 4.371230602264404 6.175686836242676
Loss :  1.805317759513855 4.356318950653076 6.161636829376221
Loss :  1.8081772327423096 4.182818412780762 5.990995407104492
Total LOSS train 3.6486105038569523 valid 6.136940240859985
CE LOSS train 1.7502376574736376 valid 0.4520443081855774
Contrastive LOSS train 1.8983728317114024 valid 1.0457046031951904
EPOCH 285:
Loss :  1.7507268190383911 2.0174012184143066 3.768127918243408
Loss :  1.7491618394851685 2.338196277618408 4.087357997894287
Loss :  1.747860312461853 2.5296146869659424 4.277474880218506
Loss :  1.7502920627593994 1.6799982786178589 3.4302902221679688
Loss :  1.7512747049331665 1.598900556564331 3.350175380706787
Loss :  1.7498610019683838 1.7831155061721802 3.5329766273498535
Loss :  1.7514199018478394 2.1777613162994385 3.9291810989379883
Loss :  1.7488073110580444 1.623092770576477 3.3719000816345215
Loss :  1.7499425411224365 1.6981121301651 3.448054790496826
Loss :  1.7435948848724365 2.1071882247924805 3.850783109664917
Loss :  1.7502319812774658 2.523279905319214 4.27351188659668
Loss :  1.7545682191848755 2.1224420070648193 3.8770103454589844
Loss :  1.7518255710601807 3.0819334983825684 4.833759307861328
Loss :  1.749650478363037 2.951411247253418 4.701061725616455
Loss :  1.753372073173523 2.3560774326324463 4.10944938659668
Loss :  1.7490707635879517 2.167541027069092 3.916611671447754
Loss :  1.7521576881408691 2.1213667392730713 3.8735244274139404
Loss :  1.7506897449493408 1.9807125329971313 3.7314023971557617
Loss :  1.7501447200775146 2.2726590633392334 4.022803783416748
Loss :  1.7505509853363037 2.4264063835144043 4.176957130432129
  batch 20 loss: 1.7505509853363037, 2.4264063835144043, 4.176957130432129
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7510892152786255 1.897148847579956 3.648238182067871
Loss :  1.7513059377670288 2.0206573009490967 3.771963119506836
Loss :  1.7526596784591675 1.9541890621185303 3.706848621368408
Loss :  1.7544859647750854 1.6982407569885254 3.4527268409729004
Loss :  1.751065731048584 2.206573724746704 3.957639455795288
Loss :  1.7484086751937866 2.540499687194824 4.2889084815979
Loss :  1.7529736757278442 2.7236974239349365 4.47667121887207
Loss :  1.7481205463409424 2.744276285171509 4.492396831512451
Loss :  1.7519882917404175 1.8401720523834229 3.592160224914551
Loss :  1.747189998626709 2.075667381286621 3.82285737991333
Loss :  1.7553671598434448 2.873166799545288 4.628533840179443
Loss :  1.7516008615493774 1.8714467287063599 3.6230475902557373
Loss :  1.7472875118255615 1.8466705083847046 3.5939579010009766
Loss :  1.7473018169403076 2.2731964588165283 4.020498275756836
Loss :  1.75227952003479 2.201563596725464 3.953843116760254
Loss :  1.7520393133163452 1.9590375423431396 3.7110767364501953
Loss :  1.7501578330993652 2.7110772132873535 4.461235046386719
Loss :  1.7479270696640015 2.0169429779052734 3.7648701667785645
Loss :  1.7489001750946045 2.158710479736328 3.9076106548309326
Loss :  1.7506657838821411 1.7118315696716309 3.4624972343444824
  batch 40 loss: 1.7506657838821411, 1.7118315696716309, 3.4624972343444824
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7488900423049927 2.284335136413574 4.033225059509277
Loss :  1.7482709884643555 1.6699612140655518 3.4182322025299072
Loss :  1.7506341934204102 1.827009916305542 3.577644109725952
Loss :  1.7483646869659424 1.7162705659866333 3.4646353721618652
Loss :  1.7510348558425903 1.9225133657455444 3.6735482215881348
Loss :  1.7498337030410767 2.057040214538574 3.8068737983703613
Loss :  1.7493821382522583 1.8755236864089966 3.624905824661255
Loss :  1.7505332231521606 1.8185336589813232 3.5690670013427734
Loss :  1.7515738010406494 1.8341649770736694 3.5857386589050293
Loss :  1.7499613761901855 2.0487594604492188 3.7987208366394043
Loss :  1.748218059539795 1.9632316827774048 3.71144962310791
Loss :  1.7501497268676758 2.2484381198883057 3.9985878467559814
Loss :  1.7533063888549805 2.0416924953460693 3.79499888420105
Loss :  1.7495169639587402 1.8716439008712769 3.6211609840393066
Loss :  1.750674843788147 1.8225690126419067 3.5732438564300537
Loss :  1.748128056526184 1.712411642074585 3.4605398178100586
Loss :  1.7515472173690796 2.0463600158691406 3.7979073524475098
Loss :  1.7539845705032349 1.9577810764312744 3.711765766143799
Loss :  1.753646731376648 3.015578508377075 4.769225120544434
Loss :  1.7508958578109741 1.889615774154663 3.6405115127563477
  batch 60 loss: 1.7508958578109741, 1.889615774154663, 3.6405115127563477
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7512328624725342 1.683416724205017 3.4346494674682617
Loss :  1.7524170875549316 1.6184955835342407 3.370912551879883
Loss :  1.7521593570709229 1.956830382347107 3.7089896202087402
Loss :  1.750225305557251 2.358438014984131 4.108663558959961
Loss :  1.7493146657943726 1.9023492336273193 3.6516637802124023
Loss :  1.8061821460723877 4.4280195236206055 6.234201431274414
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.802685260772705 4.320584297180176 6.123269557952881
Loss :  1.8034802675247192 4.30946683883667 6.1129469871521
Loss :  1.8073341846466064 4.206236839294434 6.013570785522461
Total LOSS train 3.843182398722722 valid 6.120997190475464
CE LOSS train 1.7504910010557908 valid 0.4518335461616516
Contrastive LOSS train 2.0926914086708654 valid 1.0515592098236084
EPOCH 286:
Loss :  1.751763939857483 2.0969953536987305 3.848759174346924
Loss :  1.749698281288147 1.9092679023742676 3.658966064453125
Loss :  1.7486032247543335 1.6888656616210938 3.437469005584717
Loss :  1.750495433807373 2.36790132522583 4.118396759033203
Loss :  1.7521371841430664 2.012176990509033 3.7643141746520996
Loss :  1.7496039867401123 1.6742112636566162 3.4238152503967285
Loss :  1.7515015602111816 2.4290990829467773 4.180600643157959
Loss :  1.7483696937561035 1.8889775276184082 3.6373472213745117
Loss :  1.7497520446777344 1.7727406024932861 3.5224926471710205
Loss :  1.7435588836669922 1.856294870376587 3.599853754043579
Loss :  1.7498191595077515 2.3185200691223145 4.0683393478393555
Loss :  1.7544033527374268 2.23149037361145 3.985893726348877
Loss :  1.7515504360198975 2.0649008750915527 3.81645131111145
Loss :  1.7492341995239258 1.8783687353134155 3.627603054046631
Loss :  1.7529326677322388 2.2022504806518555 3.9551830291748047
Loss :  1.7486133575439453 2.010540723800659 3.7591540813446045
Loss :  1.7516112327575684 1.8684924840927124 3.6201038360595703
Loss :  1.7499547004699707 1.8927496671676636 3.642704486846924
Loss :  1.7498878240585327 1.4697405099868774 3.21962833404541
Loss :  1.749351143836975 2.0427486896514893 3.792099952697754
  batch 20 loss: 1.749351143836975, 2.0427486896514893, 3.792099952697754
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7500160932540894 2.805541753768921 4.555557727813721
Loss :  1.750427007675171 1.9880154132843018 3.7384424209594727
Loss :  1.7514772415161133 1.892304539680481 3.6437816619873047
Loss :  1.753943920135498 2.2192869186401367 3.9732308387756348
Loss :  1.7507156133651733 3.862440824508667 5.613156318664551
Loss :  1.7473260164260864 2.67779803276062 4.425124168395996
Loss :  1.752155065536499 2.8919320106506348 4.644086837768555
Loss :  1.7461917400360107 3.3864376544952393 5.13262939453125
Loss :  1.751355528831482 2.4401774406433105 4.191533088684082
Loss :  1.7460451126098633 2.166074275970459 3.9121193885803223
Loss :  1.7551851272583008 2.20308780670166 3.958272933959961
Loss :  1.750882863998413 1.8029180765151978 3.5538010597229004
Loss :  1.746518611907959 1.8996096849441528 3.6461281776428223
Loss :  1.7469300031661987 1.7313894033432007 3.4783194065093994
Loss :  1.7521134614944458 2.3591668605804443 4.11128044128418
Loss :  1.752313256263733 1.9091403484344482 3.6614537239074707
Loss :  1.7501856088638306 1.819019079208374 3.569204807281494
Loss :  1.7474373579025269 2.546757459640503 4.29419469833374
Loss :  1.7494091987609863 1.7597765922546387 3.509185791015625
Loss :  1.7501657009124756 2.11251163482666 3.8626773357391357
  batch 40 loss: 1.7501657009124756, 2.11251163482666, 3.8626773357391357
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7486686706542969 1.967617392539978 3.7162861824035645
Loss :  1.7488834857940674 2.3543078899383545 4.103191375732422
Loss :  1.7506194114685059 1.94721257686615 3.6978321075439453
Loss :  1.74846613407135 2.292921543121338 4.041387557983398
Loss :  1.7507414817810059 2.146336078643799 3.8970775604248047
Loss :  1.7487293481826782 2.453744649887085 4.202474117279053
Loss :  1.7477909326553345 1.8869297504425049 3.634720802307129
Loss :  1.7492990493774414 2.9303741455078125 4.679673194885254
Loss :  1.7493261098861694 2.2652206420898438 4.014546871185303
Loss :  1.7484925985336304 2.8568427562713623 4.605335235595703
Loss :  1.7464005947113037 1.9544014930725098 3.7008020877838135
Loss :  1.7485078573226929 2.266956329345703 4.0154643058776855
Loss :  1.751568078994751 2.226703643798828 3.978271722793579
Loss :  1.7479692697525024 2.6369423866271973 4.38491153717041
Loss :  1.7494213581085205 2.4177017211914062 4.167122840881348
Loss :  1.7464122772216797 2.1655080318450928 3.9119203090667725
Loss :  1.7500554323196411 2.6082069873809814 4.358262538909912
Loss :  1.7523142099380493 2.0498650074005127 3.8021793365478516
Loss :  1.7524343729019165 2.4667000770568848 4.219134330749512
Loss :  1.7494643926620483 2.610809087753296 4.360273361206055
  batch 60 loss: 1.7494643926620483, 2.610809087753296, 4.360273361206055
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7494248151779175 2.57995867729187 4.329383373260498
Loss :  1.750648856163025 1.5523724555969238 3.3030214309692383
Loss :  1.7504714727401733 1.3933918476104736 3.1438632011413574
Loss :  1.7483396530151367 1.8684027194976807 3.6167423725128174
Loss :  1.7476526498794556 1.5801072120666504 3.3277597427368164
Loss :  1.80894136428833 4.397243022918701 6.206184387207031
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.805508017539978 4.36289644241333 6.168404579162598
Loss :  1.8061672449111938 4.221398830413818 6.027565956115723
Loss :  1.8093022108078003 4.21268367767334 6.02198600769043
Total LOSS train 3.928692208803617 valid 6.106035232543945
CE LOSS train 1.7497806053895217 valid 0.4523255527019501
Contrastive LOSS train 2.1789116015801064 valid 1.053170919418335
EPOCH 287:
Loss :  1.750659704208374 1.9256024360656738 3.676262140274048
Loss :  1.7489526271820068 2.054025411605835 3.802978038787842
Loss :  1.7477000951766968 1.9740396738052368 3.7217397689819336
Loss :  1.750352382659912 1.7543174028396606 3.504669666290283
Loss :  1.7511008977890015 2.152442693710327 3.903543472290039
Loss :  1.749881386756897 2.0358054637908936 3.78568696975708
Loss :  1.7505927085876465 1.985641598701477 3.736234188079834
Loss :  1.7480896711349487 2.182257890701294 3.930347442626953
Loss :  1.7492239475250244 1.7084356546401978 3.4576597213745117
Loss :  1.7421960830688477 1.326316475868225 3.068512439727783
Loss :  1.7492555379867554 1.5893940925598145 3.3386497497558594
Loss :  1.7544137239456177 1.8251891136169434 3.5796027183532715
Loss :  1.7507904767990112 1.883020043373108 3.633810520172119
Loss :  1.7486176490783691 1.653653621673584 3.402271270751953
Loss :  1.7522627115249634 2.26886248588562 4.021125316619873
Loss :  1.7481130361557007 1.97994065284729 3.728053569793701
Loss :  1.7505730390548706 1.7067954540252686 3.4573683738708496
Loss :  1.7490150928497314 2.11582350730896 3.8648386001586914
Loss :  1.7492225170135498 1.8746412992477417 3.623863697052002
Loss :  1.7479310035705566 1.9241353273391724 3.6720662117004395
  batch 20 loss: 1.7479310035705566, 1.9241353273391724, 3.6720662117004395
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7491995096206665 1.8002760410308838 3.54947566986084
Loss :  1.7499496936798096 2.1497392654418945 3.899688959121704
Loss :  1.7512328624725342 1.5242501497268677 3.2754831314086914
Loss :  1.754107117652893 1.8395061492919922 3.5936131477355957
Loss :  1.750971794128418 2.0677459239959717 3.8187177181243896
Loss :  1.7478971481323242 1.9330649375915527 3.680962085723877
Loss :  1.7528460025787354 1.624887466430664 3.3777334690093994
Loss :  1.7477644681930542 1.7863380908966064 3.534102439880371
Loss :  1.7522146701812744 1.8430266380310059 3.5952413082122803
Loss :  1.747819423675537 2.052217960357666 3.800037384033203
Loss :  1.7559823989868164 2.1502904891967773 3.9062728881835938
Loss :  1.7519054412841797 2.083552598953247 3.8354580402374268
Loss :  1.7483930587768555 2.039654493331909 3.7880475521087646
Loss :  1.748486876487732 1.9925659894943237 3.7410528659820557
Loss :  1.7535474300384521 2.1306235790252686 3.8841710090637207
Loss :  1.7533831596374512 2.0792713165283203 3.8326544761657715
Loss :  1.751481056213379 2.095366954803467 3.8468480110168457
Loss :  1.7489999532699585 1.6907811164855957 3.4397811889648438
Loss :  1.7507412433624268 1.9002703428268433 3.6510114669799805
Loss :  1.7514019012451172 1.6459382772445679 3.3973402976989746
  batch 40 loss: 1.7514019012451172, 1.6459382772445679, 3.3973402976989746
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.750394344329834 2.111595869064331 3.861990213394165
Loss :  1.7506697177886963 1.5097171068191528 3.2603869438171387
Loss :  1.751788854598999 1.7885582447052002 3.540347099304199
Loss :  1.7502201795578003 1.874390721321106 3.6246109008789062
Loss :  1.7521711587905884 1.6121660470962524 3.364337205886841
Loss :  1.7504087686538696 1.7442251443862915 3.494633913040161
Loss :  1.749646782875061 1.6063796281814575 3.3560264110565186
Loss :  1.7506288290023804 1.6804473400115967 3.4310760498046875
Loss :  1.7516858577728271 2.041280508041382 3.792966365814209
Loss :  1.7500452995300293 1.9774855375289917 3.7275309562683105
Loss :  1.7483115196228027 2.048816204071045 3.7971277236938477
Loss :  1.7502202987670898 1.6008278131484985 3.351047992706299
Loss :  1.7531074285507202 1.7742239236831665 3.5273313522338867
Loss :  1.749237298965454 2.2437970638275146 3.9930343627929688
Loss :  1.7507539987564087 2.1542999744415283 3.9050540924072266
Loss :  1.747904658317566 1.8104541301727295 3.558358669281006
Loss :  1.7514294385910034 1.9366161823272705 3.6880455017089844
Loss :  1.7536633014678955 1.992698311805725 3.74636173248291
Loss :  1.753792643547058 2.043062925338745 3.7968554496765137
Loss :  1.751116156578064 2.030806541442871 3.7819228172302246
  batch 60 loss: 1.751116156578064, 2.030806541442871, 3.7819228172302246
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7510625123977661 2.0759456157684326 3.8270082473754883
Loss :  1.752505898475647 1.6303784847259521 3.3828845024108887
Loss :  1.7521998882293701 1.3470134735107422 3.0992133617401123
Loss :  1.7511895895004272 1.7731791734695435 3.5243687629699707
Loss :  1.7505912780761719 1.856202244758606 3.6067934036254883
Loss :  1.80035400390625 4.30625581741333 6.10660982131958
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7979272603988647 4.390284061431885 6.188211441040039
Loss :  1.797792911529541 4.243056774139404 6.040849685668945
Loss :  1.8020501136779785 4.178755283355713 5.980805397033691
Total LOSS train 3.636866015654344 valid 6.079119086265564
CE LOSS train 1.7504925416066097 valid 0.45051252841949463
Contrastive LOSS train 1.8863734813836905 valid 1.0446888208389282
EPOCH 288:
Loss :  1.7530914545059204 2.4276950359344482 4.180786609649658
Loss :  1.7513437271118164 2.460937023162842 4.212280750274658
Loss :  1.7504040002822876 2.00945782661438 3.759861946105957
Loss :  1.7524515390396118 2.2383580207824707 3.990809440612793
Loss :  1.7539342641830444 1.6388884782791138 3.392822742462158
Loss :  1.751887321472168 1.642311692237854 3.3941988945007324
Loss :  1.753501534461975 2.0395007133483887 3.793002128601074
Loss :  1.750503659248352 2.2741305828094482 4.02463436126709
Loss :  1.7513954639434814 2.974332571029663 4.7257280349731445
Loss :  1.7448581457138062 3.1126348972320557 4.857492923736572
Loss :  1.750382661819458 2.6114611625671387 4.361844062805176
Loss :  1.7551077604293823 3.309041738510132 5.064149379730225
Loss :  1.7512809038162231 2.796973943710327 4.54825496673584
Loss :  1.7490687370300293 2.173060417175293 3.9221291542053223
Loss :  1.752632737159729 1.6912572383880615 3.44389009475708
Loss :  1.7483335733413696 1.5679681301116943 3.3163018226623535
Loss :  1.7516582012176514 2.7668962478637695 4.5185546875
Loss :  1.749686360359192 2.3573875427246094 4.107073783874512
Loss :  1.7508957386016846 4.15883207321167 5.909728050231934
Loss :  1.749365210533142 2.5088305473327637 4.258195877075195
  batch 20 loss: 1.749365210533142, 2.5088305473327637, 4.258195877075195
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7507516145706177 1.6805237531661987 3.4312753677368164
Loss :  1.7514078617095947 2.2784464359283447 4.0298542976379395
Loss :  1.7522751092910767 2.5158722400665283 4.2681474685668945
Loss :  1.754970908164978 2.005784034729004 3.7607550621032715
Loss :  1.7512383460998535 2.4952402114868164 4.24647855758667
Loss :  1.7485445737838745 2.2600693702697754 4.0086140632629395
Loss :  1.7535780668258667 2.5268516540527344 4.280429840087891
Loss :  1.7487238645553589 2.187394380569458 3.9361181259155273
Loss :  1.752553105354309 2.6906468868255615 4.44320011138916
Loss :  1.7478854656219482 2.1696553230285645 3.9175407886505127
Loss :  1.756084680557251 2.1481728553771973 3.9042575359344482
Loss :  1.7524093389511108 2.0455222129821777 3.797931671142578
Loss :  1.7482073307037354 2.724090337753296 4.472297668457031
Loss :  1.7484793663024902 2.180549383163452 3.9290287494659424
Loss :  1.753100872039795 3.250791072845459 5.003891944885254
Loss :  1.7531187534332275 2.2289681434631348 3.9820868968963623
Loss :  1.7512879371643066 2.3025319576263428 4.05381965637207
Loss :  1.7487452030181885 2.1397883892059326 3.888533592224121
Loss :  1.7502132654190063 2.142169237136841 3.8923826217651367
Loss :  1.751631498336792 1.3628636598587036 3.114495277404785
  batch 40 loss: 1.751631498336792, 1.3628636598587036, 3.114495277404785
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7495441436767578 1.6073299646377563 3.3568739891052246
Loss :  1.7492973804473877 1.766342282295227 3.5156397819519043
Loss :  1.7508615255355835 1.9916512966156006 3.7425127029418945
Loss :  1.749316692352295 1.8158893585205078 3.5652060508728027
Loss :  1.7512555122375488 2.0213472843170166 3.7726027965545654
Loss :  1.7493256330490112 2.5586793422698975 4.308004856109619
Loss :  1.7498292922973633 2.218658447265625 3.9684877395629883
Loss :  1.7505921125411987 1.8051180839538574 3.5557103157043457
Loss :  1.7508659362792969 2.209826707839966 3.9606926441192627
Loss :  1.749910831451416 1.5034130811691284 3.253324031829834
Loss :  1.7477861642837524 1.7846657037734985 3.532451868057251
Loss :  1.7498937845230103 1.5754812955856323 3.3253750801086426
Loss :  1.753812313079834 1.4605635404586792 3.2143759727478027
Loss :  1.7495580911636353 1.7461256980895996 3.4956836700439453
Loss :  1.7514452934265137 1.8511348962783813 3.6025800704956055
Loss :  1.7498221397399902 2.251420259475708 4.001242637634277
Loss :  1.7519840002059937 2.544766664505005 4.296750545501709
Loss :  1.7546237707138062 2.0634372234344482 3.818060874938965
Loss :  1.7544667720794678 2.403442144393921 4.157908916473389
Loss :  1.7524563074111938 2.662057638168335 4.414514064788818
  batch 60 loss: 1.7524563074111938, 2.662057638168335, 4.414514064788818
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7523123025894165 2.6209309101104736 4.37324333190918
Loss :  1.7537354230880737 2.25144362449646 4.005178928375244
Loss :  1.752954363822937 1.9149482250213623 3.6679024696350098
Loss :  1.7501673698425293 2.8105616569519043 4.560729026794434
Loss :  1.7493627071380615 1.5141922235488892 3.2635550498962402
Loss :  1.7947462797164917 3.9550676345825195 5.749814033508301
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7920575141906738 4.02335262298584 5.815410137176514
Loss :  1.7946258783340454 3.8556735515594482 5.650299549102783
Loss :  1.789917230606079 3.680964469909668 5.470881462097168
Total LOSS train 3.9826382526984583 valid 5.671601295471191
CE LOSS train 1.7511103079869197 valid 0.4474793076515198
Contrastive LOSS train 2.2315279227036697 valid 0.920241117477417
EPOCH 289:
Loss :  1.7525253295898438 1.9368385076522827 3.689363956451416
Loss :  1.7511358261108398 2.261889934539795 4.013025760650635
Loss :  1.7498457431793213 1.8975245952606201 3.6473703384399414
Loss :  1.752657413482666 1.9106463193893433 3.663303852081299
Loss :  1.7522834539413452 1.820923924446106 3.573207378387451
Loss :  1.752327561378479 1.6794582605361938 3.431785821914673
Loss :  1.7520692348480225 2.242217779159546 3.9942870140075684
Loss :  1.7507423162460327 1.9098875522613525 3.6606297492980957
Loss :  1.7515066862106323 1.9612404108047485 3.712747097015381
Loss :  1.7443257570266724 1.881198525428772 3.6255242824554443
Loss :  1.751452922821045 2.3178091049194336 4.0692620277404785
Loss :  1.756303071975708 2.205709934234619 3.962013006210327
Loss :  1.75248384475708 2.559044599533081 4.311528205871582
Loss :  1.750733494758606 3.1567137241363525 4.907447338104248
Loss :  1.7539597749710083 2.437227725982666 4.191187381744385
Loss :  1.750022530555725 1.9625166654586792 3.7125391960144043
Loss :  1.7532941102981567 2.024512767791748 3.7778067588806152
Loss :  1.751286506652832 1.8913010358810425 3.642587661743164
Loss :  1.7509100437164307 1.852257251739502 3.6031672954559326
Loss :  1.750651240348816 1.890559434890747 3.6412105560302734
  batch 20 loss: 1.750651240348816, 1.890559434890747, 3.6412105560302734
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7513647079467773 1.877150058746338 3.6285147666931152
Loss :  1.7516406774520874 2.0822079181671143 3.833848476409912
Loss :  1.7533509731292725 2.021843194961548 3.7751941680908203
Loss :  1.755082368850708 2.380056619644165 4.135138988494873
Loss :  1.751789927482605 1.977615475654602 3.729405403137207
Loss :  1.7491041421890259 2.0199360847473145 3.769040107727051
Loss :  1.7538176774978638 2.624037981033325 4.3778557777404785
Loss :  1.7493808269500732 2.1592962741851807 3.908677101135254
Loss :  1.752929449081421 2.0807087421417236 3.8336381912231445
Loss :  1.7485774755477905 2.1744801998138428 3.9230575561523438
Loss :  1.7563250064849854 3.307645320892334 5.063970565795898
Loss :  1.752996802330017 2.765195369720459 4.518192291259766
Loss :  1.749289870262146 3.2225828170776367 4.971872806549072
Loss :  1.7490700483322144 2.6774537563323975 4.426523685455322
Loss :  1.7538081407546997 2.458115816116333 4.211924076080322
Loss :  1.753601312637329 2.2411983013153076 3.9947996139526367
Loss :  1.7519365549087524 2.340364694595337 4.092301368713379
Loss :  1.7499035596847534 3.0309154987335205 4.780818939208984
Loss :  1.7510422468185425 2.099738121032715 3.850780487060547
Loss :  1.7519611120224 1.8432384729385376 3.5951995849609375
  batch 40 loss: 1.7519611120224, 1.8432384729385376, 3.5951995849609375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7507109642028809 2.629380464553833 4.380091667175293
Loss :  1.7506407499313354 1.8377960920333862 3.5884368419647217
Loss :  1.7513731718063354 2.106292486190796 3.857665538787842
Loss :  1.750339150428772 2.0610733032226562 3.8114123344421387
Loss :  1.7520337104797363 2.0553462505340576 3.807379961013794
Loss :  1.750212550163269 2.252941846847534 4.003154277801514
Loss :  1.7498763799667358 3.206251621246338 4.956128120422363
Loss :  1.7507884502410889 2.806781768798828 4.557570457458496
Loss :  1.7527976036071777 2.48427152633667 4.237069129943848
Loss :  1.7500202655792236 2.0807998180389404 3.830820083618164
Loss :  1.7484897375106812 2.110586404800415 3.8590760231018066
Loss :  1.7503279447555542 2.221921443939209 3.9722495079040527
Loss :  1.7532780170440674 2.450115203857422 4.20339298248291
Loss :  1.74959135055542 1.8553954362869263 3.6049866676330566
Loss :  1.7507150173187256 1.9583911895751953 3.709106206893921
Loss :  1.7484415769577026 1.7879925966262817 3.5364341735839844
Loss :  1.7512444257736206 1.670051097869873 3.421295642852783
Loss :  1.7541786432266235 2.115217924118042 3.869396686553955
Loss :  1.7543213367462158 2.0179715156555176 3.7722928524017334
Loss :  1.75186026096344 1.8940322399139404 3.64589262008667
  batch 60 loss: 1.75186026096344, 1.8940322399139404, 3.64589262008667
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7521471977233887 1.9121801853179932 3.664327383041382
Loss :  1.7534832954406738 1.8986289501190186 3.6521122455596924
Loss :  1.7533944845199585 1.7520575523376465 3.5054521560668945
Loss :  1.7520791292190552 3.2715680599212646 5.023647308349609
Loss :  1.7513890266418457 2.1491973400115967 3.9005863666534424
Loss :  1.79385507106781 3.881927967071533 5.675783157348633
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7911704778671265 3.8311712741851807 5.622341632843018
Loss :  1.7933279275894165 3.6723616123199463 5.465689659118652
Loss :  1.790845513343811 3.7406046390533447 5.531450271606445
Total LOSS train 3.963395782617422 valid 5.573816180229187
CE LOSS train 1.7515572951390193 valid 0.44771137833595276
Contrastive LOSS train 2.2118384783084575 valid 0.9351511597633362
EPOCH 290:
Loss :  1.7536962032318115 2.836974859237671 4.590671062469482
Loss :  1.7522175312042236 2.779953718185425 4.532171249389648
Loss :  1.7514901161193848 2.0812344551086426 3.8327245712280273
Loss :  1.7535603046417236 2.7280824184417725 4.481642723083496
Loss :  1.7546517848968506 2.5959506034851074 4.350602149963379
Loss :  1.7525601387023926 2.2946791648864746 4.047239303588867
Loss :  1.7541288137435913 1.9247210025787354 3.678849697113037
Loss :  1.7515833377838135 2.6492936611175537 4.400876998901367
Loss :  1.7521929740905762 2.8084352016448975 4.5606279373168945
Loss :  1.746800184249878 2.6502816677093506 4.3970818519592285
Loss :  1.7519140243530273 2.9055395126342773 4.657453536987305
Loss :  1.7551349401474 3.0475587844848633 4.802693843841553
Loss :  1.7531274557113647 2.947075366973877 4.700202941894531
Loss :  1.7504408359527588 2.122377634048462 3.8728184700012207
Loss :  1.7542341947555542 2.088589668273926 3.8428239822387695
Loss :  1.749833345413208 3.0474164485931396 4.797249794006348
Loss :  1.7530176639556885 1.9375407695770264 3.690558433532715
Loss :  1.7515064477920532 1.8043948411941528 3.555901288986206
Loss :  1.7517095804214478 1.7768058776855469 3.528515338897705
Loss :  1.752174735069275 2.0989620685577393 3.8511366844177246
  batch 20 loss: 1.752174735069275, 2.0989620685577393, 3.8511366844177246
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7526872158050537 1.9708209037780762 3.72350811958313
Loss :  1.7525681257247925 2.9672956466674805 4.7198638916015625
Loss :  1.7545620203018188 2.2239603996276855 3.978522300720215
Loss :  1.7563997507095337 2.2309226989746094 3.9873223304748535
Loss :  1.7527179718017578 2.454721212387085 4.207439422607422
Loss :  1.7501733303070068 2.7096128463745117 4.459786415100098
Loss :  1.7544916868209839 2.284764528274536 4.0392560958862305
Loss :  1.7499853372573853 2.3304998874664307 4.0804853439331055
Loss :  1.7533150911331177 2.623446464538574 4.376761436462402
Loss :  1.7482576370239258 2.659426689147949 4.407684326171875
Loss :  1.7563177347183228 2.1699016094207764 3.9262194633483887
Loss :  1.7530779838562012 2.010176658630371 3.7632546424865723
Loss :  1.7490520477294922 1.9470274448394775 3.6960794925689697
Loss :  1.7490835189819336 2.1076040267944336 3.856687545776367
Loss :  1.7540479898452759 2.8222134113311768 4.576261520385742
Loss :  1.75367271900177 2.2776715755462646 4.031344413757324
Loss :  1.75180983543396 2.0853159427642822 3.837125778198242
Loss :  1.7497385740280151 1.8647229671478271 3.6144614219665527
Loss :  1.7509479522705078 2.9360883235931396 4.687036514282227
Loss :  1.7520968914031982 1.5201722383499146 3.2722692489624023
  batch 40 loss: 1.7520968914031982, 1.5201722383499146, 3.2722692489624023
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.751215934753418 2.7306106090545654 4.4818267822265625
Loss :  1.7519389390945435 2.2586302757263184 4.010569095611572
Loss :  1.753271222114563 2.437793254852295 4.191064357757568
Loss :  1.7516322135925293 2.2935333251953125 4.045165538787842
Loss :  1.7534680366516113 2.9269726276397705 4.680440902709961
Loss :  1.7521315813064575 2.5786025524139404 4.3307342529296875
Loss :  1.7517552375793457 2.730708360671997 4.482463836669922
Loss :  1.7522022724151611 1.8858190774917603 3.638021469116211
Loss :  1.7543925046920776 2.0109143257141113 3.7653069496154785
Loss :  1.7513940334320068 2.2198996543884277 3.9712936878204346
Loss :  1.7498723268508911 2.505563974380493 4.255436420440674
Loss :  1.7518500089645386 2.7776312828063965 4.529481410980225
Loss :  1.7548147439956665 3.19913911819458 4.953953742980957
Loss :  1.7501275539398193 1.8667006492614746 3.616828203201294
Loss :  1.7511959075927734 1.7009625434875488 3.4521584510803223
Loss :  1.7502601146697998 1.5969161987304688 3.3471763134002686
Loss :  1.752864956855774 1.939822793006897 3.692687749862671
Loss :  1.7551462650299072 2.142974376678467 3.898120641708374
Loss :  1.755050778388977 2.2395219802856445 3.994572639465332
Loss :  1.7529257535934448 2.053154230117798 3.806079864501953
  batch 60 loss: 1.7529257535934448, 2.053154230117798, 3.806079864501953
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7529864311218262 2.1066534519195557 3.859639883041382
Loss :  1.7541499137878418 2.0021870136260986 3.7563369274139404
Loss :  1.7539321184158325 1.503585934638977 3.2575180530548096
Loss :  1.7521823644638062 2.1473538875579834 3.8995361328125
Loss :  1.7513262033462524 1.3690693378448486 3.1203956604003906
Loss :  1.796019196510315 4.018912315368652 5.814931392669678
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.793413519859314 4.1148881912231445 5.908301830291748
Loss :  1.7952555418014526 3.8568472862243652 5.652102947235107
Loss :  1.7930052280426025 3.90735125541687 5.700356483459473
Total LOSS train 4.068461854641254 valid 5.7689231634140015
CE LOSS train 1.7523548529698298 valid 0.44825130701065063
Contrastive LOSS train 2.3161069851655225 valid 0.9768378138542175
EPOCH 291:
Loss :  1.7538713216781616 2.026895761489868 3.7807669639587402
Loss :  1.7524967193603516 2.218649387359619 3.9711461067199707
Loss :  1.7509571313858032 2.812347412109375 4.563304424285889
Loss :  1.7530889511108398 2.9141530990600586 4.667242050170898
Loss :  1.7539678812026978 2.185030698776245 3.9389986991882324
Loss :  1.752329707145691 1.9967864751815796 3.7491161823272705
Loss :  1.7535558938980103 2.195849895477295 3.9494056701660156
Loss :  1.7512513399124146 1.9886338710784912 3.7398853302001953
Loss :  1.752095103263855 2.734954357147217 4.487049579620361
Loss :  1.7459172010421753 1.6997301578521729 3.4456472396850586
Loss :  1.7521040439605713 1.8975030183792114 3.6496071815490723
Loss :  1.7561957836151123 1.7408591508865356 3.4970550537109375
Loss :  1.753198504447937 1.7184484004974365 3.471646785736084
Loss :  1.7511461973190308 1.9743473529815674 3.7254934310913086
Loss :  1.7552776336669922 2.1741185188293457 3.929396152496338
Loss :  1.7503360509872437 2.743222951889038 4.493558883666992
Loss :  1.753869891166687 1.6043959856033325 3.3582658767700195
Loss :  1.7516852617263794 1.5494838953018188 3.3011691570281982
Loss :  1.7518967390060425 1.5510224103927612 3.3029191493988037
Loss :  1.7523088455200195 1.642050862312317 3.394359588623047
  batch 20 loss: 1.7523088455200195, 1.642050862312317, 3.394359588623047
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7525227069854736 2.605492115020752 4.358015060424805
Loss :  1.7529009580612183 2.17419695854187 3.927097797393799
Loss :  1.7537094354629517 2.03574538230896 3.789454936981201
Loss :  1.7559330463409424 2.0958993434906006 3.851832389831543
Loss :  1.7524503469467163 1.9253640174865723 3.677814483642578
Loss :  1.7501420974731445 2.912752866744995 4.662895202636719
Loss :  1.7546195983886719 2.5530993938446045 4.3077192306518555
Loss :  1.7500799894332886 2.6310107707977295 4.3810906410217285
Loss :  1.7534046173095703 2.1915481090545654 3.9449527263641357
Loss :  1.7494028806686401 2.2383947372436523 3.987797737121582
Loss :  1.7568825483322144 2.1424665451049805 3.8993492126464844
Loss :  1.7536669969558716 1.8717623949050903 3.625429391860962
Loss :  1.750095248222351 2.0051655769348145 3.755260944366455
Loss :  1.7504301071166992 1.5461771488189697 3.296607255935669
Loss :  1.754601001739502 2.638558864593506 4.393159866333008
Loss :  1.7547352313995361 2.108145236968994 3.8628804683685303
Loss :  1.7530839443206787 1.909676194190979 3.6627602577209473
Loss :  1.7511091232299805 1.841498851776123 3.5926079750061035
Loss :  1.7522603273391724 1.8739370107650757 3.626197338104248
Loss :  1.753697395324707 2.4759676456451416 4.2296648025512695
  batch 40 loss: 1.753697395324707, 2.4759676456451416, 4.2296648025512695
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7522510290145874 2.3433423042297363 4.095593452453613
Loss :  1.7523828744888306 1.4260461330413818 3.178429126739502
Loss :  1.7538230419158936 1.9982125759124756 3.752035617828369
Loss :  1.751883625984192 1.8191725015640259 3.5710561275482178
Loss :  1.7539350986480713 2.1274573802948 3.881392478942871
Loss :  1.7528715133666992 2.106419801712036 3.8592913150787354
Loss :  1.7524160146713257 1.7466846704483032 3.499100685119629
Loss :  1.7527456283569336 2.1246390342712402 3.877384662628174
Loss :  1.754464864730835 1.8942954540252686 3.6487603187561035
Loss :  1.7520263195037842 2.8489809036254883 4.601007461547852
Loss :  1.7511327266693115 3.840475559234619 5.591608047485352
Loss :  1.7523016929626465 3.389118194580078 5.141419887542725
Loss :  1.754858374595642 3.67154598236084 5.4264044761657715
Loss :  1.7506318092346191 2.252518892288208 4.003150939941406
Loss :  1.7520580291748047 1.8599597215652466 3.6120176315307617
Loss :  1.7498892545700073 2.0459940433502197 3.7958831787109375
Loss :  1.7527289390563965 1.981096625328064 3.73382568359375
Loss :  1.7547370195388794 1.8273742198944092 3.582111358642578
Loss :  1.7546255588531494 2.3754427433013916 4.130068302154541
Loss :  1.7515567541122437 1.9154376983642578 3.666994571685791
  batch 60 loss: 1.7515567541122437, 1.9154376983642578, 3.666994571685791
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7512314319610596 2.4379777908325195 4.189208984375
Loss :  1.7522482872009277 2.027035713195801 3.7792840003967285
Loss :  1.752061128616333 1.940690279006958 3.692751407623291
Loss :  1.7504534721374512 2.271014451980591 4.021468162536621
Loss :  1.7495133876800537 1.3354648351669312 3.0849781036376953
Loss :  1.7944258451461792 3.872518301010132 5.6669440269470215
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7915585041046143 3.813100576400757 5.604659080505371
Loss :  1.793653130531311 3.7505598068237305 5.544212818145752
Loss :  1.7911934852600098 3.6929306983947754 5.484124183654785
Total LOSS train 3.917905341661893 valid 5.574985027313232
CE LOSS train 1.7524939335309542 valid 0.44779837131500244
Contrastive LOSS train 2.1654113897910485 valid 0.9232326745986938
EPOCH 292:
Loss :  1.752018690109253 1.8429467678070068 3.5949654579162598
Loss :  1.7504303455352783 2.3948216438293457 4.145252227783203
Loss :  1.7493573427200317 1.6873382329940796 3.4366955757141113
Loss :  1.7514715194702148 1.9956284761428833 3.7470998764038086
Loss :  1.7527998685836792 1.8789348602294922 3.631734848022461
Loss :  1.7508277893066406 2.1612792015075684 3.912106990814209
Loss :  1.752431869506836 2.975963830947876 4.728395462036133
Loss :  1.749316692352295 1.9268525838851929 3.6761693954467773
Loss :  1.7507827281951904 1.9747339487075806 3.7255167961120605
Loss :  1.7444735765457153 1.8127433061599731 3.5572168827056885
Loss :  1.7504966259002686 2.1971898078918457 3.9476864337921143
Loss :  1.7556966543197632 2.8165879249572754 4.572284698486328
Loss :  1.7520980834960938 1.9110151529312134 3.6631131172180176
Loss :  1.7501024007797241 2.4608829021453857 4.21098518371582
Loss :  1.7541080713272095 1.9398819208145142 3.6939899921417236
Loss :  1.749658465385437 2.117537260055542 3.8671956062316895
Loss :  1.7527213096618652 1.917532205581665 3.6702535152435303
Loss :  1.7506526708602905 2.2584969997406006 4.009149551391602
Loss :  1.7512494325637817 1.4613170623779297 3.212566375732422
Loss :  1.7507139444351196 1.5118948221206665 3.262608766555786
  batch 20 loss: 1.7507139444351196, 1.5118948221206665, 3.262608766555786
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7511729001998901 1.4502874612808228 3.201460361480713
Loss :  1.751800775527954 2.0879909992218018 3.839791774749756
Loss :  1.7531070709228516 2.259535074234009 4.012641906738281
Loss :  1.7553423643112183 2.0349409580230713 3.790283203125
Loss :  1.7522636651992798 1.8712166547775269 3.6234803199768066
Loss :  1.749361515045166 1.8467059135437012 3.596067428588867
Loss :  1.7539150714874268 2.467801570892334 4.22171688079834
Loss :  1.7490875720977783 3.0957400798797607 4.844827651977539
Loss :  1.7527971267700195 1.6016048192977905 3.3544020652770996
Loss :  1.7487781047821045 1.9086250066757202 3.657402992248535
Loss :  1.7564526796340942 2.946275472640991 4.702728271484375
Loss :  1.7527059316635132 1.9393936395645142 3.6920995712280273
Loss :  1.7489806413650513 1.7761212587356567 3.525101900100708
Loss :  1.749318242073059 1.8610810041427612 3.6103992462158203
Loss :  1.7539279460906982 2.3289194107055664 4.082847595214844
Loss :  1.753980040550232 1.9670859575271606 3.7210659980773926
Loss :  1.7521322965621948 2.0747461318969727 3.826878547668457
Loss :  1.7492674589157104 2.17535662651062 3.924623966217041
Loss :  1.7509361505508423 2.0990588665008545 3.8499951362609863
Loss :  1.75229012966156 1.9276716709136963 3.679961681365967
  batch 40 loss: 1.75229012966156, 1.9276716709136963, 3.679961681365967
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7504923343658447 2.6558241844177246 4.406316757202148
Loss :  1.750369906425476 1.628463625907898 3.378833532333374
Loss :  1.7525612115859985 1.967877984046936 3.7204391956329346
Loss :  1.7499808073043823 1.947589635848999 3.697570323944092
Loss :  1.7526170015335083 1.7954775094985962 3.5480945110321045
Loss :  1.7517393827438354 2.8107218742370605 4.5624613761901855
Loss :  1.750662088394165 2.442723274230957 4.193385124206543
Loss :  1.7514711618423462 2.1297903060913086 3.8812613487243652
Loss :  1.7533949613571167 2.3008651733398438 4.05426025390625
Loss :  1.750396966934204 2.314884662628174 4.065281867980957
Loss :  1.7497636079788208 2.8899147510528564 4.639678478240967
Loss :  1.751172661781311 2.521174907684326 4.272347450256348
Loss :  1.7540714740753174 1.365377426147461 3.1194489002227783
Loss :  1.7496230602264404 1.662569522857666 3.4121925830841064
Loss :  1.7510136365890503 1.5636191368103027 3.3146328926086426
Loss :  1.7487354278564453 1.7420741319656372 3.490809440612793
Loss :  1.7524055242538452 2.0697696208953857 3.8221750259399414
Loss :  1.7541630268096924 1.9003390073776245 3.6545019149780273
Loss :  1.75430428981781 2.3713862895965576 4.125690460205078
Loss :  1.751537799835205 1.6364097595214844 3.3879475593566895
  batch 60 loss: 1.751537799835205, 1.6364097595214844, 3.3879475593566895
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7513928413391113 1.6125450134277344 3.3639378547668457
Loss :  1.752537727355957 1.5464118719100952 3.298949718475342
Loss :  1.7524043321609497 1.8578455448150635 3.6102499961853027
Loss :  1.7504253387451172 2.0841431617736816 3.834568500518799
Loss :  1.7494546175003052 2.083552122116089 3.8330068588256836
Loss :  1.795427918434143 3.4010660648345947 5.196494102478027
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.792434811592102 3.34763503074646 5.140069961547852
Loss :  1.7946921586990356 3.1595664024353027 4.954258441925049
Loss :  1.7917803525924683 3.102116584777832 4.89389705657959
Total LOSS train 3.8109354642721325 valid 5.046179890632629
CE LOSS train 1.751472568511963 valid 0.44794508814811707
Contrastive LOSS train 2.059462892092191 valid 0.775529146194458
EPOCH 293:
Loss :  1.7521355152130127 1.5690834522247314 3.321218967437744
Loss :  1.7505027055740356 2.722384452819824 4.47288703918457
Loss :  1.7493582963943481 2.4296634197235107 4.179021835327148
Loss :  1.7516449689865112 3.410386800765991 5.162031650543213
Loss :  1.7530065774917603 2.350114345550537 4.103120803833008
Loss :  1.7513930797576904 2.8381943702697754 4.589587211608887
Loss :  1.7526987791061401 2.276991844177246 4.029690742492676
Loss :  1.750074028968811 2.6314358711242676 4.381509780883789
Loss :  1.7510172128677368 3.284339666366577 5.0353569984436035
Loss :  1.7443164587020874 4.069716453552246 5.814033031463623
Loss :  1.7509684562683105 2.0950944423675537 3.8460628986358643
Loss :  1.7562878131866455 2.0094101428985596 3.765697956085205
Loss :  1.752128005027771 1.7192714214324951 3.4713993072509766
Loss :  1.7505109310150146 2.067497730255127 3.8180086612701416
Loss :  1.7544598579406738 2.2328522205352783 3.987312078475952
Loss :  1.7501356601715088 2.0977113246917725 3.8478469848632812
Loss :  1.7526322603225708 2.8805665969848633 4.6331987380981445
Loss :  1.7507190704345703 2.4328227043151855 4.183541774749756
Loss :  1.7509733438491821 1.9478833675384521 3.698856830596924
Loss :  1.7510724067687988 2.7594757080078125 4.510548114776611
  batch 20 loss: 1.7510724067687988, 2.7594757080078125, 4.510548114776611
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7518372535705566 1.8387075662612915 3.5905447006225586
Loss :  1.7521357536315918 2.003941297531128 3.7560770511627197
Loss :  1.7538790702819824 2.210411310195923 3.9642903804779053
Loss :  1.755690097808838 2.9408886432647705 4.6965789794921875
Loss :  1.7521933317184448 3.648275852203369 5.4004693031311035
Loss :  1.7498468160629272 2.371190071105957 4.121037006378174
Loss :  1.754312515258789 2.4826819896698 4.236994743347168
Loss :  1.7495373487472534 2.5051677227020264 4.25470495223999
Loss :  1.7536040544509888 3.697802782058716 5.451406955718994
Loss :  1.749358892440796 3.7180850505828857 5.467443943023682
Loss :  1.7567452192306519 2.653475522994995 4.410220623016357
Loss :  1.7536334991455078 2.2468762397766113 4.000509738922119
Loss :  1.7495793104171753 1.9061062335968018 3.6556854248046875
Loss :  1.7494025230407715 2.465703010559082 4.2151055335998535
Loss :  1.753869891166687 2.505474328994751 4.259344100952148
Loss :  1.7536587715148926 3.031806230545044 4.785465240478516
Loss :  1.75177800655365 3.648010730743408 5.399788856506348
Loss :  1.7494134902954102 1.7070883512496948 3.4565019607543945
Loss :  1.7510881423950195 1.7826435565948486 3.533731698989868
Loss :  1.752510666847229 1.548156976699829 3.3006677627563477
  batch 40 loss: 1.752510666847229, 1.548156976699829, 3.3006677627563477
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7505747079849243 2.0846381187438965 3.8352127075195312
Loss :  1.7508538961410522 2.4031248092651367 4.1539788246154785
Loss :  1.7526121139526367 1.7670038938522339 3.51961612701416
Loss :  1.7502059936523438 1.5490505695343018 3.2992565631866455
Loss :  1.7522121667861938 1.5929731130599976 3.3451852798461914
Loss :  1.7514843940734863 1.9404613971710205 3.691945791244507
Loss :  1.7508442401885986 1.4537949562072754 3.204639196395874
Loss :  1.751211166381836 1.4641897678375244 3.2154009342193604
Loss :  1.752148151397705 1.4581983089447021 3.2103464603424072
Loss :  1.7501220703125 1.429239273071289 3.179361343383789
Loss :  1.7482326030731201 1.7493776082992554 3.497610092163086
Loss :  1.749944806098938 2.435859441757202 4.18580436706543
Loss :  1.7532777786254883 2.306206226348877 4.059484004974365
Loss :  1.7494957447052002 1.979337453842163 3.7288331985473633
Loss :  1.7510014772415161 3.143866777420044 4.89486837387085
Loss :  1.7493584156036377 2.9237425327301025 4.67310094833374
Loss :  1.7522079944610596 3.2543680667877197 5.006576061248779
Loss :  1.7545799016952515 2.7540855407714844 4.508665561676025
Loss :  1.7541589736938477 2.550199031829834 4.304358005523682
Loss :  1.7514668703079224 1.949156641960144 3.7006235122680664
  batch 60 loss: 1.7514668703079224, 1.949156641960144, 3.7006235122680664
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.751442551612854 2.2062623500823975 3.957705020904541
Loss :  1.7525712251663208 1.766133189201355 3.518704414367676
Loss :  1.7519981861114502 1.6321346759796143 3.3841328620910645
Loss :  1.7496699094772339 2.718869209289551 4.468539237976074
Loss :  1.7486376762390137 2.2752161026000977 4.023853778839111
Loss :  1.7984246015548706 4.120812892913818 5.9192376136779785
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7963831424713135 4.175702095031738 5.972084999084473
Loss :  1.7966499328613281 4.021225452423096 5.817875385284424
Loss :  1.7994381189346313 4.021448612213135 5.820886611938477
Total LOSS train 4.113466200461755 valid 5.882521152496338
CE LOSS train 1.7515449707324688 valid 0.44985952973365784
Contrastive LOSS train 2.361921213223384 valid 1.0053621530532837
EPOCH 294:
Loss :  1.7517359256744385 1.8248403072357178 3.5765762329101562
Loss :  1.7499778270721436 1.9965296983718872 3.7465076446533203
Loss :  1.7487409114837646 2.241722583770752 3.9904634952545166
Loss :  1.7512927055358887 2.750117301940918 4.501410007476807
Loss :  1.7520034313201904 2.085062026977539 3.8370654582977295
Loss :  1.7508524656295776 2.57026743888855 4.321119785308838
Loss :  1.7516014575958252 3.1476635932922363 4.899265289306641
Loss :  1.749282717704773 4.307733058929443 6.057015895843506
Loss :  1.7505934238433838 2.387568473815918 4.138161659240723
Loss :  1.7443318367004395 1.9201416969299316 3.664473533630371
Loss :  1.7506846189498901 2.0983903408050537 3.8490748405456543
Loss :  1.7558467388153076 2.3417370319366455 4.097583770751953
Loss :  1.7522780895233154 2.0817644596099854 3.834042549133301
Loss :  1.7505218982696533 2.668015480041504 4.418537139892578
Loss :  1.7545260190963745 2.6404333114624023 4.394959449768066
Loss :  1.749920129776001 2.284876585006714 4.034796714782715
Loss :  1.7528516054153442 1.743285059928894 3.4961366653442383
Loss :  1.750688076019287 2.1929404735565186 3.9436285495758057
Loss :  1.7506115436553955 1.3957353830337524 3.1463470458984375
Loss :  1.750723123550415 2.264469623565674 4.015192985534668
  batch 20 loss: 1.750723123550415, 2.264469623565674, 4.015192985534668
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7513947486877441 2.10815691947937 3.8595516681671143
Loss :  1.7515084743499756 2.249480962753296 4.0009894371032715
Loss :  1.7530604600906372 2.1616880893707275 3.9147486686706543
Loss :  1.7552804946899414 2.210530996322632 3.9658114910125732
Loss :  1.75212562084198 2.2005791664123535 3.952704906463623
Loss :  1.7492070198059082 2.215078592300415 3.9642856121063232
Loss :  1.7538055181503296 1.9470847845077515 3.700890302658081
Loss :  1.7487707138061523 3.397587537765503 5.146358489990234
Loss :  1.7527860403060913 1.681679368019104 3.4344654083251953
Loss :  1.7480374574661255 1.9476431608200073 3.695680618286133
Loss :  1.7563996315002441 2.090599298477173 3.846998929977417
Loss :  1.7525150775909424 1.909087896347046 3.6616029739379883
Loss :  1.7483720779418945 2.8594024181365967 4.60777473449707
Loss :  1.748380184173584 2.9917001724243164 4.7400803565979
Loss :  1.7531980276107788 2.1517817974090576 3.904979705810547
Loss :  1.7528681755065918 2.0056402683258057 3.7585084438323975
Loss :  1.7516953945159912 3.3417558670043945 5.093451499938965
Loss :  1.7483088970184326 3.8954195976257324 5.643728256225586
Loss :  1.7504940032958984 2.797478437423706 4.547972679138184
Loss :  1.7509641647338867 2.0411412715911865 3.7921054363250732
  batch 40 loss: 1.7509641647338867, 2.0411412715911865, 3.7921054363250732
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7500576972961426 2.1102793216705322 3.860337018966675
Loss :  1.7504444122314453 1.4853397607803345 3.2357840538024902
Loss :  1.7520970106124878 1.701971411705017 3.454068422317505
Loss :  1.7507984638214111 2.36199951171875 4.112797737121582
Loss :  1.7529939413070679 1.4251285791397095 3.1781225204467773
Loss :  1.7520954608917236 3.3491976261138916 5.101293087005615
Loss :  1.752200961112976 3.138324499130249 4.8905253410339355
Loss :  1.753066062927246 2.2819900512695312 4.035056114196777
Loss :  1.7552847862243652 2.301680564880371 4.056965351104736
Loss :  1.752089262008667 1.8394465446472168 3.591535806655884
Loss :  1.7509101629257202 1.8033523559570312 3.554262638092041
Loss :  1.7527329921722412 2.1761233806610107 3.928856372833252
Loss :  1.7559410333633423 2.25594425201416 4.011885166168213
Loss :  1.7516365051269531 2.628262519836426 4.379899024963379
Loss :  1.752977728843689 2.657169818878174 4.410147666931152
Loss :  1.7508776187896729 3.310765266418457 5.061642646789551
Loss :  1.7537089586257935 1.718742847442627 3.472451686859131
Loss :  1.7555644512176514 1.6374019384384155 3.3929662704467773
Loss :  1.7553374767303467 1.9839916229248047 3.7393290996551514
Loss :  1.7532095909118652 1.6507928371429443 3.4040024280548096
  batch 60 loss: 1.7532095909118652, 1.6507928371429443, 3.4040024280548096
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7531640529632568 2.0000829696655273 3.753247022628784
Loss :  1.7539958953857422 2.303497314453125 4.057493209838867
Loss :  1.7538026571273804 1.7470017671585083 3.5008044242858887
Loss :  1.7516889572143555 2.190230369567871 3.9419193267822266
Loss :  1.7504440546035767 2.339562177658081 4.090006351470947
Loss :  1.7858984470367432 3.784806251525879 5.570704460144043
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.785436749458313 3.733208179473877 5.5186448097229
Loss :  1.784769892692566 3.572192668914795 5.35696268081665
Loss :  1.7882068157196045 3.6541638374328613 5.442370414733887
Total LOSS train 4.052468417241023 valid 5.47217059135437
CE LOSS train 1.7517747218792254 valid 0.4470517039299011
Contrastive LOSS train 2.3006936898598305 valid 0.9135409593582153
EPOCH 295:
Loss :  1.7528369426727295 3.953338146209717 5.706174850463867
Loss :  1.7521644830703735 3.2766153812408447 5.028779983520508
Loss :  1.750415325164795 1.907989501953125 3.65840482711792
Loss :  1.7524172067642212 1.830790638923645 3.583207845687866
Loss :  1.7535791397094727 2.139113426208496 3.8926925659179688
Loss :  1.7519787549972534 2.064114809036255 3.8160934448242188
Loss :  1.753342628479004 1.8551390171051025 3.6084816455841064
Loss :  1.7508844137191772 1.6381841897964478 3.389068603515625
Loss :  1.7520759105682373 1.7480309009552002 3.5001068115234375
Loss :  1.746577262878418 1.4866164922714233 3.233193874359131
Loss :  1.7528185844421387 2.236973762512207 3.9897923469543457
Loss :  1.7564555406570435 2.5107035636901855 4.2671589851379395
Loss :  1.7541968822479248 2.2621452808380127 4.0163421630859375
Loss :  1.75210702419281 2.36224365234375 4.11435079574585
Loss :  1.7558377981185913 3.3245463371276855 5.080384254455566
Loss :  1.7513192892074585 2.2595152854919434 4.010834693908691
Loss :  1.754790186882019 3.1037099361419678 4.858500003814697
Loss :  1.7530267238616943 2.982128620147705 4.73515510559082
Loss :  1.752372145652771 2.588041067123413 4.3404130935668945
Loss :  1.7534778118133545 2.6231722831726074 4.376649856567383
  batch 20 loss: 1.7534778118133545, 2.6231722831726074, 4.376649856567383
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7533363103866577 3.314220666885376 5.067556858062744
Loss :  1.753572702407837 2.797074317932129 4.550646781921387
Loss :  1.754616618156433 2.003603935241699 3.758220672607422
Loss :  1.7564035654067993 1.853786587715149 3.6101901531219482
Loss :  1.7527775764465332 1.9901026487350464 3.742880344390869
Loss :  1.750447392463684 1.816806435585022 3.567253828048706
Loss :  1.7550832033157349 1.8151609897613525 3.570244312286377
Loss :  1.7508196830749512 1.8731549978256226 3.6239748001098633
Loss :  1.7540079355239868 2.238283634185791 3.9922914505004883
Loss :  1.7491650581359863 1.9806654453277588 3.729830503463745
Loss :  1.7565861940383911 2.291783571243286 4.048369884490967
Loss :  1.753226637840271 2.9288055896759033 4.682032108306885
Loss :  1.749409556388855 1.8620399236679077 3.6114494800567627
Loss :  1.7488006353378296 2.270447015762329 4.019247531890869
Loss :  1.753970980644226 3.0112626552581787 4.765233516693115
Loss :  1.7532949447631836 2.5329577922821045 4.286252975463867
Loss :  1.751386046409607 2.3662359714508057 4.117621898651123
Loss :  1.7492752075195312 3.194697141647339 4.943972587585449
Loss :  1.7507073879241943 2.113741636276245 3.8644490242004395
Loss :  1.751301884651184 2.326897144317627 4.0781989097595215
  batch 40 loss: 1.751301884651184, 2.326897144317627, 4.0781989097595215
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7507625818252563 2.092053174972534 3.84281587600708
Loss :  1.75114107131958 3.394507884979248 5.145648956298828
Loss :  1.7514257431030273 2.4721298217773438 4.223555564880371
Loss :  1.7511639595031738 2.056673288345337 3.8078372478485107
Loss :  1.7522903680801392 1.5696693658828735 3.3219597339630127
Loss :  1.7493151426315308 1.7099933624267578 3.459308624267578
Loss :  1.749789834022522 1.9548710584640503 3.7046608924865723
Loss :  1.7510173320770264 1.8433226346969604 3.5943398475646973
Loss :  1.7509684562683105 1.897750973701477 3.648719310760498
Loss :  1.7503154277801514 2.3137929439544678 4.064108371734619
Loss :  1.7481492757797241 2.046389102935791 3.7945384979248047
Loss :  1.7500370740890503 2.006047010421753 3.7560839653015137
Loss :  1.7534927129745483 1.893397569656372 3.646890163421631
Loss :  1.7499897480010986 2.107656717300415 3.8576464653015137
Loss :  1.7515069246292114 2.010953664779663 3.762460708618164
Loss :  1.7493395805358887 1.9817944765090942 3.7311339378356934
Loss :  1.7521644830703735 2.0381805896759033 3.7903451919555664
Loss :  1.7546701431274414 2.0665974617004395 3.821267604827881
Loss :  1.7545350790023804 2.5056066513061523 4.260141849517822
Loss :  1.7520407438278198 2.0048251152038574 3.756865978240967
  batch 60 loss: 1.7520407438278198, 2.0048251152038574, 3.756865978240967
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.751586675643921 1.9434360265731812 3.6950225830078125
Loss :  1.7527271509170532 1.895024061203003 3.6477513313293457
Loss :  1.7522419691085815 1.683567762374878 3.43580961227417
Loss :  1.7499979734420776 2.3261101245880127 4.076107978820801
Loss :  1.7493082284927368 1.7964861392974854 3.5457944869995117
Loss :  1.791614055633545 3.238401174545288 5.030014991760254
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7889516353607178 3.2751569747924805 5.064108848571777
Loss :  1.791121482849121 3.216784954071045 5.007906436920166
Loss :  1.7889682054519653 3.1234021186828613 4.912370204925537
Total LOSS train 4.003484894679143 valid 5.003600120544434
CE LOSS train 1.7520129423875075 valid 0.44724205136299133
Contrastive LOSS train 2.2514719596275916 valid 0.7808505296707153
EPOCH 296:
Loss :  1.7523934841156006 1.7534229755401611 3.5058164596557617
Loss :  1.7506340742111206 2.5241305828094482 4.274764537811279
Loss :  1.7496713399887085 1.825716495513916 3.575387954711914
Loss :  1.7521978616714478 1.7848517894744873 3.5370497703552246
Loss :  1.752901315689087 1.8946980237960815 3.647599220275879
Loss :  1.7522214651107788 2.8216629028320312 4.5738844871521
Loss :  1.7528914213180542 2.696835517883301 4.4497270584106445
Loss :  1.7508963346481323 2.8291139602661133 4.580010414123535
Loss :  1.7518784999847412 2.7966229915618896 4.548501491546631
Loss :  1.7459309101104736 2.525195837020874 4.271126747131348
Loss :  1.7521963119506836 3.3416554927825928 5.0938520431518555
Loss :  1.7563358545303345 1.9147186279296875 3.6710543632507324
Loss :  1.75349760055542 1.7967759370803833 3.5502734184265137
Loss :  1.7515183687210083 2.4814534187316895 4.232971668243408
Loss :  1.7551710605621338 2.4481499195098877 4.2033209800720215
Loss :  1.7506649494171143 1.7998875379562378 3.5505523681640625
Loss :  1.7539695501327515 1.8641369342803955 3.6181063652038574
Loss :  1.7520760297775269 1.5367919206619263 3.288867950439453
Loss :  1.751892328262329 1.4847838878631592 3.2366762161254883
Loss :  1.7519726753234863 2.331381320953369 4.0833539962768555
  batch 20 loss: 1.7519726753234863, 2.331381320953369, 4.0833539962768555
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.752217173576355 2.4948174953460693 4.247034549713135
Loss :  1.7525745630264282 2.0382273197174072 3.790802001953125
Loss :  1.7535804510116577 1.9638023376464844 3.7173829078674316
Loss :  1.7559365034103394 1.9203007221221924 3.676237106323242
Loss :  1.7524354457855225 2.1134252548217773 3.8658607006073
Loss :  1.7500332593917847 2.0961554050445557 3.846188545227051
Loss :  1.7548757791519165 2.127070665359497 3.881946563720703
Loss :  1.750734806060791 2.093611478805542 3.844346284866333
Loss :  1.7540861368179321 2.3853704929351807 4.139456748962402
Loss :  1.7493784427642822 2.2853338718414307 4.034712314605713
Loss :  1.7568268775939941 2.3344109058380127 4.091238021850586
Loss :  1.7537158727645874 1.7658370733261108 3.5195529460906982
Loss :  1.7500979900360107 2.086911678314209 3.8370096683502197
Loss :  1.7498722076416016 2.0856547355651855 3.835526943206787
Loss :  1.75439453125 2.3763418197631836 4.130736351013184
Loss :  1.754428744316101 2.174328565597534 3.9287571907043457
Loss :  1.7529184818267822 2.3020052909851074 4.054924011230469
Loss :  1.7502201795578003 2.741921901702881 4.492142200469971
Loss :  1.752240777015686 1.8770323991775513 3.6292731761932373
Loss :  1.752638339996338 1.6665843725204468 3.419222831726074
  batch 40 loss: 1.752638339996338, 1.6665843725204468, 3.419222831726074
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7517627477645874 2.1415047645568848 3.8932676315307617
Loss :  1.7521575689315796 2.4973604679107666 4.249517917633057
Loss :  1.7526531219482422 2.7097551822662354 4.462408065795898
Loss :  1.7517352104187012 1.9156887531280518 3.667423963546753
Loss :  1.75315260887146 1.6896404027938843 3.4427928924560547
Loss :  1.751247763633728 1.9302732944488525 3.681520938873291
Loss :  1.7513549327850342 1.8917244672775269 3.6430792808532715
Loss :  1.7516628503799438 1.9362568855285645 3.6879196166992188
Loss :  1.7528334856033325 2.854393482208252 4.607226848602295
Loss :  1.7510077953338623 1.7628486156463623 3.5138564109802246
Loss :  1.7493228912353516 1.8226889371871948 3.572011947631836
Loss :  1.7511281967163086 2.477656602859497 4.228784561157227
Loss :  1.7544535398483276 1.457739233970642 3.2121927738189697
Loss :  1.7504428625106812 1.7600995302200317 3.510542392730713
Loss :  1.7519491910934448 2.39263653755188 4.144585609436035
Loss :  1.7497012615203857 1.7083896398544312 3.4580907821655273
Loss :  1.7527201175689697 2.0080180168151855 3.7607381343841553
Loss :  1.754882574081421 1.949813723564148 3.7046961784362793
Loss :  1.7550127506256104 2.352811336517334 4.107824325561523
Loss :  1.7530461549758911 1.9479217529296875 3.700967788696289
  batch 60 loss: 1.7530461549758911, 1.9479217529296875, 3.700967788696289
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7531126737594604 2.520491123199463 4.273603916168213
Loss :  1.7541424036026 2.008901596069336 3.7630438804626465
Loss :  1.7542911767959595 1.7149189710617065 3.469210147857666
Loss :  1.7524070739746094 2.2949461936950684 4.047353267669678
Loss :  1.7515958547592163 1.677034616470337 3.4286303520202637
Loss :  1.7961682081222534 3.9199719429016113 5.716140270233154
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7932103872299194 3.71941876411438 5.51262903213501
Loss :  1.795474886894226 3.751497745513916 5.546972751617432
Loss :  1.7925126552581787 3.5894715785980225 5.381984233856201
Total LOSS train 3.887792895390437 valid 5.539431571960449
CE LOSS train 1.7523368431971624 valid 0.4481281638145447
Contrastive LOSS train 2.1354560613632203 valid 0.8973678946495056
EPOCH 297:
Loss :  1.7541567087173462 1.7971614599227905 3.5513181686401367
Loss :  1.753055453300476 2.000718832015991 3.7537741661071777
Loss :  1.7518290281295776 1.7217495441436768 3.473578453063965
Loss :  1.7537410259246826 1.9944911003112793 3.748232126235962
Loss :  1.7547932863235474 1.9113430976867676 3.6661362648010254
Loss :  1.753178358078003 1.8155341148376465 3.5687124729156494
Loss :  1.7547799348831177 2.6265666484832764 4.381346702575684
Loss :  1.7524549961090088 1.7741442918777466 3.526599407196045
Loss :  1.7534048557281494 2.73726487159729 4.4906697273254395
Loss :  1.7478747367858887 1.734874963760376 3.4827497005462646
Loss :  1.7533754110336304 2.088475227355957 3.841850757598877
Loss :  1.7564548254013062 2.919625759124756 4.676080703735352
Loss :  1.7546591758728027 2.3189494609832764 4.0736083984375
Loss :  1.7525995969772339 1.8510860204696655 3.6036856174468994
Loss :  1.7566957473754883 1.9525700807571411 3.70926570892334
Loss :  1.7524158954620361 2.123601198196411 3.8760170936584473
Loss :  1.7551690340042114 2.356647253036499 4.11181640625
Loss :  1.7540172338485718 2.8869402408599854 4.640957355499268
Loss :  1.753171682357788 2.4400410652160645 4.193212509155273
Loss :  1.753717303276062 1.9509742259979248 3.7046914100646973
  batch 20 loss: 1.753717303276062, 1.9509742259979248, 3.7046914100646973
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7538886070251465 1.6507011651992798 3.4045896530151367
Loss :  1.753432035446167 1.8117625713348389 3.565194606781006
Loss :  1.7551590204238892 2.3569695949554443 4.112128734588623
Loss :  1.7566757202148438 2.7846009731292725 4.541276931762695
Loss :  1.7532857656478882 2.4288852214813232 4.182170867919922
Loss :  1.750589370727539 3.7560932636260986 5.506682395935059
Loss :  1.755355715751648 2.088202714920044 3.8435583114624023
Loss :  1.751043438911438 2.020360231399536 3.7714037895202637
Loss :  1.7545493841171265 2.0862855911254883 3.8408350944519043
Loss :  1.7487726211547852 2.1543362140655518 3.903108835220337
Loss :  1.756937861442566 2.214388370513916 3.9713263511657715
Loss :  1.7541627883911133 2.170701503753662 3.9248642921447754
Loss :  1.7498340606689453 1.9325846433639526 3.6824188232421875
Loss :  1.7500407695770264 1.8564435243606567 3.6064844131469727
Loss :  1.7545921802520752 2.024313449859619 3.7789056301116943
Loss :  1.754676342010498 1.9779967069625854 3.732673168182373
Loss :  1.7530936002731323 2.0682787895202637 3.8213725090026855
Loss :  1.7513484954833984 2.0893280506134033 3.8406765460968018
Loss :  1.7521675825119019 2.7470860481262207 4.499253749847412
Loss :  1.753514289855957 1.8601796627044678 3.613693952560425
  batch 40 loss: 1.753514289855957, 1.8601796627044678, 3.613693952560425
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7519437074661255 2.587007761001587 4.338951587677002
Loss :  1.7516944408416748 1.8640674352645874 3.6157617568969727
Loss :  1.7531516551971436 1.8861174583435059 3.6392691135406494
Loss :  1.7515257596969604 1.7978326082229614 3.549358367919922
Loss :  1.7535291910171509 2.2967143058776855 4.050243377685547
Loss :  1.7525019645690918 2.2274253368377686 3.9799273014068604
Loss :  1.7523866891860962 1.5428011417388916 3.2951879501342773
Loss :  1.7525508403778076 1.7460386753082275 3.498589515686035
Loss :  1.7546000480651855 1.6726925373077393 3.427292585372925
Loss :  1.7517400979995728 1.559454321861267 3.31119441986084
Loss :  1.750088095664978 2.166922092437744 3.9170103073120117
Loss :  1.7518055438995361 2.5077154636383057 4.259521007537842
Loss :  1.7546194791793823 2.2452101707458496 3.9998297691345215
Loss :  1.7503025531768799 1.8396732807159424 3.5899758338928223
Loss :  1.7516387701034546 2.127143383026123 3.878782272338867
Loss :  1.7494664192199707 1.8901660442352295 3.6396324634552
Loss :  1.7520910501480103 2.345100164413452 4.097191333770752
Loss :  1.754330039024353 1.694474697113037 3.4488048553466797
Loss :  1.7543203830718994 2.2168872356414795 3.971207618713379
Loss :  1.7518491744995117 1.7223294973373413 3.4741787910461426
  batch 60 loss: 1.7518491744995117, 1.7223294973373413, 3.4741787910461426
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7515435218811035 1.94767165184021 3.6992151737213135
Loss :  1.752636432647705 1.948635458946228 3.7012720108032227
Loss :  1.7525604963302612 1.8073283433914185 3.5598888397216797
Loss :  1.7505085468292236 2.1023292541503906 3.8528378009796143
Loss :  1.7500662803649902 1.512775182723999 3.2628414630889893
Loss :  1.7945541143417358 3.72576642036438 5.520320415496826
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7918928861618042 3.636348009109497 5.428240776062012
Loss :  1.7939751148223877 3.5500757694244385 5.344050884246826
Loss :  1.7914435863494873 3.768857717514038 5.560301303863525
Total LOSS train 3.8503828818981463 valid 5.463228344917297
CE LOSS train 1.7528940787682166 valid 0.4478608965873718
Contrastive LOSS train 2.0974887884580173 valid 0.9422144293785095
EPOCH 298:
Loss :  1.752848505973816 2.4784817695617676 4.231330394744873
Loss :  1.7512223720550537 2.156096935272217 3.9073193073272705
Loss :  1.7500243186950684 1.857979416847229 3.608003616333008
Loss :  1.7519460916519165 2.1714301109313965 3.9233760833740234
Loss :  1.7533245086669922 2.0468740463256836 3.800198554992676
Loss :  1.7515077590942383 1.9499539136886597 3.7014617919921875
Loss :  1.752958059310913 2.3564608097076416 4.109418869018555
Loss :  1.7504652738571167 1.9474273920059204 3.697892665863037
Loss :  1.751842737197876 2.0582756996154785 3.8101184368133545
Loss :  1.7457892894744873 1.9636520147323608 3.7094411849975586
Loss :  1.7520819902420044 2.5421457290649414 4.294227600097656
Loss :  1.7562580108642578 2.458953380584717 4.215211391448975
Loss :  1.7532360553741455 2.3535335063934326 4.106769561767578
Loss :  1.751077651977539 2.075334072113037 3.826411724090576
Loss :  1.755157232284546 2.1034789085388184 3.8586361408233643
Loss :  1.7504171133041382 2.4485762119293213 4.19899320602417
Loss :  1.7537565231323242 1.5301285982131958 3.2838850021362305
Loss :  1.7517422437667847 1.3880946636199951 3.1398367881774902
Loss :  1.75177800655365 1.3350374698638916 3.086815357208252
Loss :  1.7519344091415405 1.4788086414337158 3.230742931365967
  batch 20 loss: 1.7519344091415405, 1.4788086414337158, 3.230742931365967
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7524367570877075 1.578385829925537 3.330822467803955
Loss :  1.752983570098877 1.6095696687698364 3.362553119659424
Loss :  1.7539515495300293 1.6204180717468262 3.3743696212768555
Loss :  1.7563400268554688 1.7187438011169434 3.475083827972412
Loss :  1.7530338764190674 2.616551160812378 4.369585037231445
Loss :  1.7504345178604126 1.7996201515197754 3.5500545501708984
Loss :  1.7549835443496704 1.802333116531372 3.557316780090332
Loss :  1.7503507137298584 2.5260655879974365 4.276416301727295
Loss :  1.754160761833191 3.080587148666382 4.834747791290283
Loss :  1.749916672706604 2.3098628520965576 4.059779644012451
Loss :  1.757596492767334 2.1443631649017334 3.9019596576690674
Loss :  1.7542134523391724 2.4732577800750732 4.227471351623535
Loss :  1.7503975629806519 2.353118419647217 4.103516101837158
Loss :  1.7506417036056519 1.9962648153305054 3.7469065189361572
Loss :  1.7549835443496704 2.3359906673431396 4.0909743309021
Loss :  1.7551599740982056 2.4949121475219727 4.250072002410889
Loss :  1.7533745765686035 1.9522508382797241 3.705625534057617
Loss :  1.75146484375 2.0183839797973633 3.7698488235473633
Loss :  1.7526662349700928 1.873339056968689 3.626005172729492
Loss :  1.7540545463562012 1.951897144317627 3.705951690673828
  batch 40 loss: 1.7540545463562012, 1.951897144317627, 3.705951690673828
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7523616552352905 2.0022454261779785 3.7546072006225586
Loss :  1.7521520853042603 1.6244326829910278 3.376584768295288
Loss :  1.753861904144287 1.842984676361084 3.596846580505371
Loss :  1.7513706684112549 2.0720763206481934 3.8234469890594482
Loss :  1.7538976669311523 1.7946561574935913 3.548553943634033
Loss :  1.7527815103530884 1.8353062868118286 3.588087797164917
Loss :  1.7522631883621216 1.6487658023834229 3.401029109954834
Loss :  1.7532479763031006 2.0614724159240723 3.814720392227173
Loss :  1.7551461458206177 1.8168388605117798 3.5719850063323975
Loss :  1.7526944875717163 1.6838107109069824 3.4365053176879883
Loss :  1.7513487339019775 1.8606064319610596 3.611955165863037
Loss :  1.7531049251556396 1.8379775285720825 3.5910825729370117
Loss :  1.7556333541870117 1.859952688217163 3.615586042404175
Loss :  1.7518017292022705 2.1728708744049072 3.9246726036071777
Loss :  1.7529295682907104 2.145458221435547 3.898387908935547
Loss :  1.7507238388061523 1.9763389825820923 3.727062702178955
Loss :  1.7534143924713135 2.476402759552002 4.2298173904418945
Loss :  1.7558650970458984 2.3483388423919678 4.104204177856445
Loss :  1.755602478981018 2.5772504806518555 4.332852840423584
Loss :  1.753090500831604 1.912477731704712 3.6655683517456055
  batch 60 loss: 1.753090500831604, 1.912477731704712, 3.6655683517456055
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7530347108840942 1.754640817642212 3.5076756477355957
Loss :  1.7541252374649048 1.7316269874572754 3.4857521057128906
Loss :  1.7539420127868652 1.5024398565292358 3.2563819885253906
Loss :  1.7515891790390015 1.8004052639007568 3.5519943237304688
Loss :  1.7506061792373657 1.5116385221481323 3.262244701385498
Loss :  1.79804265499115 3.785761594772339 5.583804130554199
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7951135635375977 3.6757938861846924 5.470907211303711
Loss :  1.7972948551177979 3.6336164474487305 5.430911064147949
Loss :  1.7946399450302124 3.585785150527954 5.380424976348877
Total LOSS train 3.7651808702028715 valid 5.466511845588684
CE LOSS train 1.7527554200245783 valid 0.4486599862575531
Contrastive LOSS train 2.012425446510315 valid 0.8964462876319885
EPOCH 299:
Loss :  1.7533776760101318 1.873471736907959 3.626849412918091
Loss :  1.751920223236084 2.043123960494995 3.795044183731079
Loss :  1.7503411769866943 2.2071239948272705 3.957465171813965
Loss :  1.7525132894515991 2.3831160068511963 4.135629177093506
Loss :  1.753551959991455 2.069762945175171 3.823314905166626
Loss :  1.7523549795150757 2.0322630405426025 3.7846179008483887
Loss :  1.7530945539474487 1.9088695049285889 3.661963939666748
Loss :  1.7507696151733398 1.8667887449264526 3.617558479309082
Loss :  1.7514395713806152 1.8146427869796753 3.56608247756958
Loss :  1.7451887130737305 2.0174012184143066 3.762589931488037
Loss :  1.7521674633026123 2.317985773086548 4.07015323638916
Loss :  1.757218837738037 2.0147488117218018 3.771967649459839
Loss :  1.7539688348770142 2.280575752258301 4.034544467926025
Loss :  1.752179741859436 2.1348764896392822 3.887056350708008
Loss :  1.7564176321029663 1.8263911008834839 3.58280873298645
Loss :  1.751739740371704 1.8630093336105347 3.614748954772949
Loss :  1.755824089050293 1.809548020362854 3.5653719902038574
Loss :  1.753210186958313 1.8350509405136108 3.588261127471924
Loss :  1.754165530204773 1.9338651895523071 3.68803071975708
Loss :  1.7548961639404297 1.8729156255722046 3.627811908721924
  batch 20 loss: 1.7548961639404297, 1.8729156255722046, 3.627811908721924
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7546112537384033 2.4837441444396973 4.23835563659668
Loss :  1.7552858591079712 2.4162018299102783 4.171487808227539
Loss :  1.7558417320251465 3.0041615962982178 4.760003089904785
Loss :  1.7580138444900513 2.2953379154205322 4.053351879119873
Loss :  1.7543232440948486 1.9186376333236694 3.6729607582092285
Loss :  1.7523378133773804 1.765262246131897 3.5176000595092773
Loss :  1.7565417289733887 1.7155824899673462 3.4721240997314453
Loss :  1.7527600526809692 2.1657655239105225 3.9185256958007812
Loss :  1.7559465169906616 2.2928273677825928 4.048773765563965
Loss :  1.7523163557052612 2.513202667236328 4.265519142150879
Loss :  1.7585179805755615 1.9164923429489136 3.6750102043151855
Loss :  1.7554408311843872 1.7262085676193237 3.481649398803711
Loss :  1.7519454956054688 1.6284000873565674 3.380345582962036
Loss :  1.7517744302749634 1.8780176639556885 3.6297922134399414
Loss :  1.7557014226913452 2.3058054447174072 4.061506748199463
Loss :  1.755629301071167 2.084465980529785 3.840095281600952
Loss :  1.7538419961929321 2.251682996749878 4.0055251121521
Loss :  1.7513940334320068 2.336691379547119 4.088085174560547
Loss :  1.7531452178955078 1.7019919157028198 3.455137252807617
Loss :  1.7536015510559082 2.0147674083709717 3.76836895942688
  batch 40 loss: 1.7536015510559082, 2.0147674083709717, 3.76836895942688
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.752748727798462 2.243290901184082 3.996039628982544
Loss :  1.7532594203948975 2.0620455741882324 3.81530499458313
Loss :  1.7538560628890991 3.4337611198425293 5.187617301940918
Loss :  1.7513935565948486 3.417489767074585 5.168883323669434
Loss :  1.754284143447876 1.5995078086853027 3.3537919521331787
Loss :  1.7529065608978271 2.3135483264923096 4.066454887390137
Loss :  1.752585768699646 2.0332906246185303 3.7858762741088867
Loss :  1.753218412399292 2.5649428367614746 4.3181610107421875
Loss :  1.7548770904541016 1.9251312017440796 3.6800084114074707
Loss :  1.7526401281356812 1.5706745386123657 3.323314666748047
Loss :  1.7511459589004517 2.211432695388794 3.962578773498535
Loss :  1.752794623374939 1.8348915576934814 3.587686061859131
Loss :  1.7558622360229492 2.2158548831939697 3.971717119216919
Loss :  1.7518305778503418 2.0741302967071533 3.825960874557495
Loss :  1.753210186958313 1.7957570552825928 3.5489673614501953
Loss :  1.7516268491744995 1.9448761940002441 3.696503162384033
Loss :  1.7540991306304932 2.260251045227051 4.014349937438965
Loss :  1.7562509775161743 2.2406609058380127 3.9969120025634766
Loss :  1.7562452554702759 2.5371246337890625 4.293369770050049
Loss :  1.753696322441101 2.2249855995178223 3.978682041168213
  batch 60 loss: 1.753696322441101, 2.2249855995178223, 3.978682041168213
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.753731369972229 2.14731502532959 3.9010462760925293
Loss :  1.7545933723449707 2.153794050216675 3.9083874225616455
Loss :  1.7543669939041138 2.0826175212860107 3.836984634399414
Loss :  1.7519398927688599 2.8236136436462402 4.5755534172058105
Loss :  1.7508827447891235 1.1567554473876953 2.9076380729675293
Loss :  1.7921347618103027 3.9398422241210938 5.7319769859313965
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.790391445159912 4.019855976104736 5.810247421264648
Loss :  1.7918617725372314 3.8515892028808594 5.643450736999512
Loss :  1.78981614112854 3.7935495376586914 5.583365440368652
Total LOSS train 3.8671981224646936 valid 5.692260146141052
CE LOSS train 1.7535285692948561 valid 0.447454035282135
Contrastive LOSS train 2.1136695605057936 valid 0.9483873844146729
EPOCH 300:
Loss :  1.7537058591842651 1.5933842658996582 3.347090244293213
Loss :  1.7524913549423218 2.112941026687622 3.8654322624206543
Loss :  1.7510454654693604 2.821101665496826 4.572147369384766
Loss :  1.7533427476882935 1.9312846660614014 3.6846275329589844
Loss :  1.7540149688720703 1.9032502174377441 3.6572651863098145
Loss :  1.753067135810852 2.087961435317993 3.8410286903381348
Loss :  1.753851056098938 2.6788244247436523 4.432675361633301
Loss :  1.7516499757766724 1.7215853929519653 3.4732353687286377
Loss :  1.7526437044143677 1.6963051557540894 3.448948860168457
Loss :  1.746715784072876 1.8033658266067505 3.550081729888916
Loss :  1.752987265586853 2.1199097633361816 3.872897148132324
Loss :  1.7572336196899414 1.94273042678833 3.6999640464782715
Loss :  1.754388451576233 1.8909753561019897 3.6453638076782227
Loss :  1.7519301176071167 2.558060884475708 4.309990882873535
Loss :  1.7553859949111938 2.219616413116455 3.9750022888183594
Loss :  1.7511664628982544 1.898174524307251 3.649341106414795
Loss :  1.7541468143463135 2.4572393894195557 4.211386203765869
Loss :  1.7526613473892212 2.339940309524536 4.092601776123047
Loss :  1.7525553703308105 2.0010082721710205 3.753563642501831
Loss :  1.751973032951355 1.9137364625930786 3.6657094955444336
  batch 20 loss: 1.751973032951355, 1.9137364625930786, 3.6657094955444336
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7527107000350952 1.4191464185714722 3.1718571186065674
Loss :  1.7531028985977173 1.9003630876541138 3.653465986251831
Loss :  1.7538870573043823 2.0271246433258057 3.7810115814208984
Loss :  1.7564001083374023 2.077360153198242 3.8337602615356445
Loss :  1.7526907920837402 2.275024890899658 4.027715682983398
Loss :  1.7499967813491821 1.6779744625091553 3.427971363067627
Loss :  1.7548221349716187 1.9986804723739624 3.753502607345581
Loss :  1.7500154972076416 2.00225830078125 3.7522737979888916
Loss :  1.7537394762039185 2.0022482872009277 3.7559876441955566
Loss :  1.7496733665466309 2.2151741981506348 3.9648475646972656
Loss :  1.7572938203811646 2.4063003063201904 4.1635942459106445
Loss :  1.7537132501602173 2.382662296295166 4.136375427246094
Loss :  1.7500102519989014 2.1024672985076904 3.852477550506592
Loss :  1.7495523691177368 2.201402187347412 3.9509544372558594
Loss :  1.7547271251678467 2.447183847427368 4.201910972595215
Loss :  1.754673957824707 2.6121227741241455 4.366796493530273
Loss :  1.752938985824585 2.115985155105591 3.868924140930176
Loss :  1.751157522201538 2.3412556648254395 4.092412948608398
Loss :  1.7525326013565063 2.8969552516937256 4.6494879722595215
Loss :  1.7527066469192505 1.6854019165039062 3.438108444213867
  batch 40 loss: 1.7527066469192505, 1.6854019165039062, 3.438108444213867
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7521724700927734 1.9149813652038574 3.667153835296631
Loss :  1.752232313156128 1.8485015630722046 3.600733757019043
Loss :  1.7525560855865479 2.1821398735046387 3.9346959590911865
Loss :  1.7520731687545776 2.925915241241455 4.677988529205322
Loss :  1.7533385753631592 1.7810074090957642 3.534346103668213
Loss :  1.7510453462600708 1.854148268699646 3.605193614959717
Loss :  1.7509591579437256 2.07405686378479 3.8250160217285156
Loss :  1.752099871635437 2.331470251083374 4.0835700035095215
Loss :  1.7528280019760132 1.7460540533065796 3.4988820552825928
Loss :  1.7514581680297852 2.123492479324341 3.874950647354126
Loss :  1.7492165565490723 2.0717437267303467 3.820960283279419
Loss :  1.7512069940567017 2.3172667026519775 4.068473815917969
Loss :  1.7543421983718872 1.497580885887146 3.251923084259033
Loss :  1.7505558729171753 2.34328293800354 4.093838691711426
Loss :  1.7519270181655884 1.8983548879623413 3.6502819061279297
Loss :  1.750680685043335 2.6142075061798096 4.3648881912231445
Loss :  1.7534228563308716 2.166231393814087 3.919654369354248
Loss :  1.7559049129486084 1.8413958549499512 3.5973007678985596
Loss :  1.755889654159546 2.327888011932373 4.08377742767334
Loss :  1.7536317110061646 1.953218936920166 3.706850528717041
  batch 60 loss: 1.7536317110061646, 1.953218936920166, 3.706850528717041
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7534812688827515 2.177581787109375 3.931063175201416
Loss :  1.7545706033706665 1.8216670751571655 3.576237678527832
Loss :  1.7540476322174072 2.3390276432037354 4.093075275421143
Loss :  1.7519011497497559 2.173231363296509 3.9251325130462646
Loss :  1.7509558200836182 1.3772059679031372 3.128161907196045
Loss :  1.7992299795150757 4.059600830078125 5.85883092880249
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.796656608581543 4.187798500061035 5.984455108642578
Loss :  1.7986112833023071 4.060305118560791 5.858916282653809
Loss :  1.7965915203094482 4.003342628479004 5.799934387207031
Total LOSS train 3.847783682896541 valid 5.875534176826477
CE LOSS train 1.752704614859361 valid 0.44914788007736206
Contrastive LOSS train 2.095079069871169 valid 1.000835657119751
Length of Train dataset : 646 and Test dataset : 35
Length of Train loader : 65 and Test loader : 4
self.partnames :  ['background', 'head', 'neck', 'torso', 'tail', 'legs']
prompts :  [' the background of the cat.', ' the head of the cat.', ' the neck of the cat.', ' the torso of the cat.', ' the tail of the cat.', ' the legs of the cat.']
model device :  cuda
self.prompts : torch.Size([6, 77])
self.prompts : torch.Size([6, 1024])
self.prompts rquires grad :  <built-in method requires_grad_ of Tensor object at 0x7fc2f90db2c0>
layer: relu3_2,relu5_4
weighs: [0.33, 1.0]
/home/harsh21122/.conda/envs/part/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
/home/harsh21122/.conda/envs/part/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Printing parameters and their gradient
gamma True
image_encoder.0.weight False
image_encoder.1.weight False
image_encoder.1.bias False
image_encoder.3.weight False
image_encoder.4.weight False
image_encoder.4.bias False
image_encoder.6.weight False
image_encoder.7.weight False
image_encoder.7.bias False
image_encoder.10.0.conv1.weight False
image_encoder.10.0.bn1.weight False
image_encoder.10.0.bn1.bias False
image_encoder.10.0.conv2.weight False
image_encoder.10.0.bn2.weight False
image_encoder.10.0.bn2.bias False
image_encoder.10.0.conv3.weight False
image_encoder.10.0.bn3.weight False
image_encoder.10.0.bn3.bias False
image_encoder.10.0.downsample.0.weight False
image_encoder.10.0.downsample.1.weight False
image_encoder.10.0.downsample.1.bias False
image_encoder.10.1.conv1.weight False
image_encoder.10.1.bn1.weight False
image_encoder.10.1.bn1.bias False
image_encoder.10.1.conv2.weight False
image_encoder.10.1.bn2.weight False
image_encoder.10.1.bn2.bias False
image_encoder.10.1.conv3.weight False
image_encoder.10.1.bn3.weight False
image_encoder.10.1.bn3.bias False
image_encoder.10.2.conv1.weight False
image_encoder.10.2.bn1.weight False
image_encoder.10.2.bn1.bias False
image_encoder.10.2.conv2.weight False
image_encoder.10.2.bn2.weight False
image_encoder.10.2.bn2.bias False
image_encoder.10.2.conv3.weight False
image_encoder.10.2.bn3.weight False
image_encoder.10.2.bn3.bias False
image_encoder.11.0.conv1.weight False
image_encoder.11.0.bn1.weight False
image_encoder.11.0.bn1.bias False
image_encoder.11.0.conv2.weight False
image_encoder.11.0.bn2.weight False
image_encoder.11.0.bn2.bias False
image_encoder.11.0.conv3.weight False
image_encoder.11.0.bn3.weight False
image_encoder.11.0.bn3.bias False
image_encoder.11.0.downsample.0.weight False
image_encoder.11.0.downsample.1.weight False
image_encoder.11.0.downsample.1.bias False
image_encoder.11.1.conv1.weight False
image_encoder.11.1.bn1.weight False
image_encoder.11.1.bn1.bias False
image_encoder.11.1.conv2.weight False
image_encoder.11.1.bn2.weight False
image_encoder.11.1.bn2.bias False
image_encoder.11.1.conv3.weight False
image_encoder.11.1.bn3.weight False
image_encoder.11.1.bn3.bias False
image_encoder.11.2.conv1.weight False
image_encoder.11.2.bn1.weight False
image_encoder.11.2.bn1.bias False
image_encoder.11.2.conv2.weight False
image_encoder.11.2.bn2.weight False
image_encoder.11.2.bn2.bias False
image_encoder.11.2.conv3.weight False
image_encoder.11.2.bn3.weight False
image_encoder.11.2.bn3.bias False
image_encoder.11.3.conv1.weight False
image_encoder.11.3.bn1.weight False
image_encoder.11.3.bn1.bias False
image_encoder.11.3.conv2.weight False
image_encoder.11.3.bn2.weight False
image_encoder.11.3.bn2.bias False
image_encoder.11.3.conv3.weight False
image_encoder.11.3.bn3.weight False
image_encoder.11.3.bn3.bias False
image_encoder.12.0.conv1.weight False
image_encoder.12.0.bn1.weight False
image_encoder.12.0.bn1.bias False
image_encoder.12.0.conv2.weight False
image_encoder.12.0.bn2.weight False
image_encoder.12.0.bn2.bias False
image_encoder.12.0.conv3.weight False
image_encoder.12.0.bn3.weight False
image_encoder.12.0.bn3.bias False
image_encoder.12.0.downsample.0.weight False
image_encoder.12.0.downsample.1.weight False
image_encoder.12.0.downsample.1.bias False
image_encoder.12.1.conv1.weight False
image_encoder.12.1.bn1.weight False
image_encoder.12.1.bn1.bias False
image_encoder.12.1.conv2.weight False
image_encoder.12.1.bn2.weight False
image_encoder.12.1.bn2.bias False
image_encoder.12.1.conv3.weight False
image_encoder.12.1.bn3.weight False
image_encoder.12.1.bn3.bias False
image_encoder.12.2.conv1.weight False
image_encoder.12.2.bn1.weight False
image_encoder.12.2.bn1.bias False
image_encoder.12.2.conv2.weight False
image_encoder.12.2.bn2.weight False
image_encoder.12.2.bn2.bias False
image_encoder.12.2.conv3.weight False
image_encoder.12.2.bn3.weight False
image_encoder.12.2.bn3.bias False
image_encoder.12.3.conv1.weight False
image_encoder.12.3.bn1.weight False
image_encoder.12.3.bn1.bias False
image_encoder.12.3.conv2.weight False
image_encoder.12.3.bn2.weight False
image_encoder.12.3.bn2.bias False
image_encoder.12.3.conv3.weight False
image_encoder.12.3.bn3.weight False
image_encoder.12.3.bn3.bias False
image_encoder.12.4.conv1.weight False
image_encoder.12.4.bn1.weight False
image_encoder.12.4.bn1.bias False
image_encoder.12.4.conv2.weight False
image_encoder.12.4.bn2.weight False
image_encoder.12.4.bn2.bias False
image_encoder.12.4.conv3.weight False
image_encoder.12.4.bn3.weight False
image_encoder.12.4.bn3.bias False
image_encoder.12.5.conv1.weight False
image_encoder.12.5.bn1.weight False
image_encoder.12.5.bn1.bias False
image_encoder.12.5.conv2.weight False
image_encoder.12.5.bn2.weight False
image_encoder.12.5.bn2.bias False
image_encoder.12.5.conv3.weight False
image_encoder.12.5.bn3.weight False
image_encoder.12.5.bn3.bias False
image_encoder.13.0.conv1.weight False
image_encoder.13.0.bn1.weight False
image_encoder.13.0.bn1.bias False
image_encoder.13.0.conv2.weight False
image_encoder.13.0.bn2.weight False
image_encoder.13.0.bn2.bias False
image_encoder.13.0.conv3.weight False
image_encoder.13.0.bn3.weight False
image_encoder.13.0.bn3.bias False
image_encoder.13.0.downsample.0.weight False
image_encoder.13.0.downsample.1.weight False
image_encoder.13.0.downsample.1.bias False
image_encoder.13.1.conv1.weight False
image_encoder.13.1.bn1.weight False
image_encoder.13.1.bn1.bias False
image_encoder.13.1.conv2.weight False
image_encoder.13.1.bn2.weight False
image_encoder.13.1.bn2.bias False
image_encoder.13.1.conv3.weight False
image_encoder.13.1.bn3.weight False
image_encoder.13.1.bn3.bias False
image_encoder.13.2.conv1.weight False
image_encoder.13.2.bn1.weight False
image_encoder.13.2.bn1.bias False
image_encoder.13.2.conv2.weight False
image_encoder.13.2.bn2.weight False
image_encoder.13.2.bn2.bias False
image_encoder.13.2.conv3.weight False
image_encoder.13.2.bn3.weight False
image_encoder.13.2.bn3.bias False
attnpool.positional_embedding True
attnpool.k_proj.weight True
attnpool.k_proj.bias True
attnpool.q_proj.weight True
attnpool.q_proj.bias True
attnpool.v_proj.weight True
attnpool.v_proj.bias True
attnpool.c_proj.weight True
attnpool.c_proj.bias True
align_context.memory_proj.0.weight True
align_context.memory_proj.0.bias True
align_context.memory_proj.1.weight True
align_context.memory_proj.1.bias True
align_context.memory_proj.2.weight True
align_context.memory_proj.2.bias True
align_context.text_proj.0.weight True
align_context.text_proj.0.bias True
align_context.text_proj.1.weight True
align_context.text_proj.1.bias True
align_context.decoder.0.self_attn.q_proj.weight True
align_context.decoder.0.self_attn.k_proj.weight True
align_context.decoder.0.self_attn.v_proj.weight True
align_context.decoder.0.self_attn.proj.weight True
align_context.decoder.0.self_attn.proj.bias True
align_context.decoder.0.cross_attn.q_proj.weight True
align_context.decoder.0.cross_attn.k_proj.weight True
align_context.decoder.0.cross_attn.v_proj.weight True
align_context.decoder.0.cross_attn.proj.weight True
align_context.decoder.0.cross_attn.proj.bias True
align_context.decoder.0.norm1.weight True
align_context.decoder.0.norm1.bias True
align_context.decoder.0.norm2.weight True
align_context.decoder.0.norm2.bias True
align_context.decoder.0.norm3.weight True
align_context.decoder.0.norm3.bias True
align_context.decoder.0.mlp.0.weight True
align_context.decoder.0.mlp.0.bias True
align_context.decoder.0.mlp.3.weight True
align_context.decoder.0.mlp.3.bias True
align_context.decoder.1.self_attn.q_proj.weight True
align_context.decoder.1.self_attn.k_proj.weight True
align_context.decoder.1.self_attn.v_proj.weight True
align_context.decoder.1.self_attn.proj.weight True
align_context.decoder.1.self_attn.proj.bias True
align_context.decoder.1.cross_attn.q_proj.weight True
align_context.decoder.1.cross_attn.k_proj.weight True
align_context.decoder.1.cross_attn.v_proj.weight True
align_context.decoder.1.cross_attn.proj.weight True
align_context.decoder.1.cross_attn.proj.bias True
align_context.decoder.1.norm1.weight True
align_context.decoder.1.norm1.bias True
align_context.decoder.1.norm2.weight True
align_context.decoder.1.norm2.bias True
align_context.decoder.1.norm3.weight True
align_context.decoder.1.norm3.bias True
align_context.decoder.1.mlp.0.weight True
align_context.decoder.1.mlp.0.bias True
align_context.decoder.1.mlp.3.weight True
align_context.decoder.1.mlp.3.bias True
align_context.decoder.2.self_attn.q_proj.weight True
align_context.decoder.2.self_attn.k_proj.weight True
align_context.decoder.2.self_attn.v_proj.weight True
align_context.decoder.2.self_attn.proj.weight True
align_context.decoder.2.self_attn.proj.bias True
align_context.decoder.2.cross_attn.q_proj.weight True
align_context.decoder.2.cross_attn.k_proj.weight True
align_context.decoder.2.cross_attn.v_proj.weight True
align_context.decoder.2.cross_attn.proj.weight True
align_context.decoder.2.cross_attn.proj.bias True
align_context.decoder.2.norm1.weight True
align_context.decoder.2.norm1.bias True
align_context.decoder.2.norm2.weight True
align_context.decoder.2.norm2.bias True
align_context.decoder.2.norm3.weight True
align_context.decoder.2.norm3.bias True
align_context.decoder.2.mlp.0.weight True
align_context.decoder.2.mlp.0.bias True
align_context.decoder.2.mlp.3.weight True
align_context.decoder.2.mlp.3.bias True
align_context.decoder.3.self_attn.q_proj.weight True
align_context.decoder.3.self_attn.k_proj.weight True
align_context.decoder.3.self_attn.v_proj.weight True
align_context.decoder.3.self_attn.proj.weight True
align_context.decoder.3.self_attn.proj.bias True
align_context.decoder.3.cross_attn.q_proj.weight True
align_context.decoder.3.cross_attn.k_proj.weight True
align_context.decoder.3.cross_attn.v_proj.weight True
align_context.decoder.3.cross_attn.proj.weight True
align_context.decoder.3.cross_attn.proj.bias True
align_context.decoder.3.norm1.weight True
align_context.decoder.3.norm1.bias True
align_context.decoder.3.norm2.weight True
align_context.decoder.3.norm2.bias True
align_context.decoder.3.norm3.weight True
align_context.decoder.3.norm3.bias True
align_context.decoder.3.mlp.0.weight True
align_context.decoder.3.mlp.0.bias True
align_context.decoder.3.mlp.3.weight True
align_context.decoder.3.mlp.3.bias True
align_context.decoder.4.self_attn.q_proj.weight True
align_context.decoder.4.self_attn.k_proj.weight True
align_context.decoder.4.self_attn.v_proj.weight True
align_context.decoder.4.self_attn.proj.weight True
align_context.decoder.4.self_attn.proj.bias True
align_context.decoder.4.cross_attn.q_proj.weight True
align_context.decoder.4.cross_attn.k_proj.weight True
align_context.decoder.4.cross_attn.v_proj.weight True
align_context.decoder.4.cross_attn.proj.weight True
align_context.decoder.4.cross_attn.proj.bias True
align_context.decoder.4.norm1.weight True
align_context.decoder.4.norm1.bias True
align_context.decoder.4.norm2.weight True
align_context.decoder.4.norm2.bias True
align_context.decoder.4.norm3.weight True
align_context.decoder.4.norm3.bias True
align_context.decoder.4.mlp.0.weight True
align_context.decoder.4.mlp.0.bias True
align_context.decoder.4.mlp.3.weight True
align_context.decoder.4.mlp.3.bias True
align_context.decoder.5.self_attn.q_proj.weight True
align_context.decoder.5.self_attn.k_proj.weight True
align_context.decoder.5.self_attn.v_proj.weight True
align_context.decoder.5.self_attn.proj.weight True
align_context.decoder.5.self_attn.proj.bias True
align_context.decoder.5.cross_attn.q_proj.weight True
align_context.decoder.5.cross_attn.k_proj.weight True
align_context.decoder.5.cross_attn.v_proj.weight True
align_context.decoder.5.cross_attn.proj.weight True
align_context.decoder.5.cross_attn.proj.bias True
align_context.decoder.5.norm1.weight True
align_context.decoder.5.norm1.bias True
align_context.decoder.5.norm2.weight True
align_context.decoder.5.norm2.bias True
align_context.decoder.5.norm3.weight True
align_context.decoder.5.norm3.bias True
align_context.decoder.5.mlp.0.weight True
align_context.decoder.5.mlp.0.bias True
align_context.decoder.5.mlp.3.weight True
align_context.decoder.5.mlp.3.bias True
align_context.out_proj.0.weight True
align_context.out_proj.0.bias True
align_context.out_proj.1.weight True
align_context.out_proj.1.bias True
decoder.conv_layers.0.weight True
decoder.conv_layers.0.bias True
decoder.conv_layers.2.weight True
decoder.conv_layers.2.bias True
decoder.conv_layers.3.weight True
decoder.conv_layers.3.bias True
decoder.conv_layers.5.weight True
decoder.conv_layers.5.bias True
EPOCH 1:
Loss :  2.1015102863311768 4.221807956695557 6.3233184814453125
Loss :  2.189436435699463 4.713050842285156 6.902487277984619
Loss :  2.2788262367248535 4.7279510498046875 7.006777286529541
Loss :  2.180185317993164 4.483911514282227 6.664096832275391
Loss :  2.033489465713501 4.528961658477783 6.562451362609863
Loss :  2.126662492752075 4.212642192840576 6.3393049240112305
Loss :  2.1250548362731934 4.495450973510742 6.6205058097839355
Loss :  2.0978660583496094 4.6718316078186035 6.769697666168213
Loss :  2.0871105194091797 4.829835414886475 6.916945934295654
Loss :  2.143913507461548 4.65730619430542 6.801219940185547
Loss :  2.284111976623535 4.698517799377441 6.982629776000977
Loss :  2.1885175704956055 4.543463230133057 6.731980800628662
Loss :  2.1870081424713135 4.799963474273682 6.986971855163574
Loss :  2.2241246700286865 4.547201156616211 6.771326065063477
Loss :  2.230415105819702 4.697972774505615 6.928387641906738
Loss :  2.3159990310668945 4.631724834442139 6.947723865509033
Loss :  2.126448392868042 4.6379714012146 6.7644195556640625
Loss :  2.2053210735321045 4.501597881317139 6.706918716430664
Loss :  2.100414991378784 4.50570011138916 6.606115341186523
Loss :  2.2053020000457764 4.775578022003174 6.980879783630371
  batch 20 loss: 2.2053020000457764, 4.775578022003174, 6.980879783630371
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  2.2587671279907227 4.3979926109313965 6.656759738922119
Loss :  2.140063762664795 4.5821661949157715 6.722229957580566
Loss :  2.1126656532287598 4.646136283874512 6.7588019371032715
Loss :  2.083913564682007 4.564871788024902 6.648785591125488
Loss :  2.2482407093048096 4.656545639038086 6.904786109924316
Loss :  2.1132519245147705 4.594688415527344 6.707940101623535
Loss :  2.2802562713623047 4.612111568450928 6.892367839813232
Loss :  2.16823673248291 4.508761405944824 6.676998138427734
Loss :  2.1067192554473877 4.85435676574707 6.961075782775879
Loss :  2.1253511905670166 4.465085029602051 6.590435981750488
Loss :  2.0179295539855957 4.603845596313477 6.621775150299072
Loss :  2.0650527477264404 4.81154727935791 6.87660026550293
Loss :  2.1514055728912354 4.375472068786621 6.526877403259277
Loss :  2.2101142406463623 4.443491458892822 6.6536054611206055
Loss :  2.06050705909729 4.624322414398193 6.6848297119140625
Loss :  2.110199213027954 4.591211795806885 6.701411247253418
Loss :  2.1330020427703857 4.573330879211426 6.706333160400391
Loss :  2.019895315170288 4.429718971252441 6.449614524841309
Loss :  2.1032257080078125 4.449387550354004 6.552613258361816
Loss :  2.0509533882141113 4.622865676879883 6.673819065093994
  batch 40 loss: 2.0509533882141113, 4.622865676879883, 6.673819065093994
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  2.066283702850342 4.690184593200684 6.756468296051025
Loss :  2.096062660217285 4.447420120239258 6.543482780456543
Loss :  2.103097677230835 4.317779064178467 6.420876502990723
Loss :  2.065540313720703 4.4762773513793945 6.541817665100098
Loss :  2.0932164192199707 4.5123701095581055 6.605586528778076
Loss :  1.977417230606079 4.515436172485352 6.492853164672852
Loss :  2.129042387008667 4.48605489730835 6.6150970458984375
Loss :  2.0210013389587402 4.671943187713623 6.692944526672363
Loss :  2.0390868186950684 4.410150051116943 6.449236869812012
Loss :  1.9582425355911255 4.432539939880371 6.390782356262207
Loss :  1.9965821504592896 4.475686550140381 6.472268581390381
Loss :  1.9979556798934937 4.512256145477295 6.510211944580078
Loss :  2.0038745403289795 4.523962020874023 6.527836799621582
Loss :  1.9647319316864014 4.392175197601318 6.356906890869141
Loss :  2.0397732257843018 4.514108180999756 6.553881645202637
Loss :  1.9423072338104248 4.459996700286865 6.402303695678711
Loss :  1.9663716554641724 4.365208625793457 6.33158016204834
Loss :  1.9728410243988037 4.558132648468018 6.530973434448242
Loss :  2.0026254653930664 4.584884166717529 6.587509632110596
Loss :  1.8896898031234741 4.457623481750488 6.347313404083252
  batch 60 loss: 1.8896898031234741, 4.457623481750488, 6.347313404083252
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  2.0105931758880615 4.598240852355957 6.608834266662598
Loss :  1.93807852268219 4.576029300689697 6.514107704162598
Loss :  2.0217926502227783 4.480556011199951 6.502348899841309
Loss :  2.0283660888671875 4.591082572937012 6.619448661804199
Loss :  1.9269793033599854 4.105078220367432 6.032057762145996
idx :  64
Loss :  2.895893096923828 4.35022497177124 7.246118068695068
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 3, 4, 5], device='cuda:0')
Loss :  2.783259153366089 4.411049842834473 7.194309234619141
Loss :  2.8167171478271484 4.315739631652832 7.1324567794799805
Loss :  2.9247641563415527 4.130197525024414 7.054961681365967
idx :  3
Total LOSS train 6.745133571326733 valid 7.156961441040039
CE LOSS train 2.1287972293794155 valid 0.7311910390853882
Contrastive LOSS train 4.616336338222027 valid 1.0325493812561035
Saved best model. Old loss 1000000.0 and new best loss 7.156961441040039
EPOCH 2:
Loss :  1.9595082998275757 4.774702072143555 6.73421049118042
Loss :  1.9160634279251099 4.7465338706970215 6.662597179412842
Loss :  2.004514694213867 4.618768692016602 6.623283386230469
Loss :  2.005507707595825 4.466561317443848 6.472068786621094
Loss :  1.923396110534668 4.381648063659668 6.305044174194336
Loss :  1.9095900058746338 4.495367527008057 6.4049577713012695
Loss :  1.8353739976882935 4.696591377258301 6.531965255737305
Loss :  1.81063711643219 4.3982648849487305 6.208901882171631
Loss :  2.0065855979919434 4.528555393218994 6.5351409912109375
Loss :  1.9469259977340698 4.479794502258301 6.42672061920166
Loss :  2.0389750003814697 4.650156021118164 6.689130783081055
Loss :  1.978564739227295 4.467013835906982 6.445578575134277
Loss :  2.0100443363189697 4.546142101287842 6.556186676025391
Loss :  2.068089723587036 4.430701732635498 6.498791694641113
Loss :  1.9705361127853394 4.328347682952881 6.29888391494751
Loss :  1.9125850200653076 4.672974586486816 6.585559844970703
Loss :  1.9194653034210205 4.386250972747803 6.305716514587402
Loss :  1.976739525794983 4.5038838386535645 6.480623245239258
Loss :  1.9451394081115723 4.4323625564575195 6.377501964569092
Loss :  1.9650126695632935 4.827138423919678 6.792150974273682
  batch 20 loss: 1.9650126695632935, 4.827138423919678, 6.792150974273682
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8076905012130737 4.41153621673584 6.219226837158203
Loss :  1.9567208290100098 4.4918365478515625 6.448557376861572
Loss :  2.0439305305480957 4.549055099487305 6.5929856300354
Loss :  1.9973912239074707 4.457821369171143 6.455212593078613
Loss :  2.0262749195098877 4.739461421966553 6.7657365798950195
Loss :  1.983825922012329 4.906547546386719 6.890373229980469
Loss :  1.9078654050827026 4.847540378570557 6.755405902862549
Loss :  1.9449617862701416 4.39894962310791 6.343911170959473
Loss :  1.9584405422210693 4.641258716583252 6.599699020385742
Loss :  2.025819778442383 4.493673324584961 6.519493103027344
Loss :  1.9840892553329468 4.568810939788818 6.552900314331055
Loss :  1.969090223312378 4.7493062019348145 6.718396186828613
Loss :  2.0415279865264893 4.52976655960083 6.571294784545898
Loss :  1.9812651872634888 4.591191291809082 6.572456359863281
Loss :  2.0552785396575928 4.6468095779418945 6.702088356018066
Loss :  1.9968634843826294 4.87752103805542 6.87438440322876
Loss :  2.032243251800537 4.763487815856934 6.795731067657471
Loss :  2.0015172958374023 5.250139236450195 7.251656532287598
Loss :  1.9730831384658813 4.504277229309082 6.477360248565674
Loss :  1.9506622552871704 5.032970905303955 6.983633041381836
  batch 40 loss: 1.9506622552871704, 5.032970905303955, 6.983633041381836
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9224985837936401 4.802862644195557 6.725361347198486
Loss :  2.003798723220825 4.778955459594727 6.782753944396973
Loss :  2.0086257457733154 4.472853660583496 6.481479644775391
Loss :  2.007387161254883 4.782493591308594 6.789880752563477
Loss :  2.055050849914551 4.461155414581299 6.51620626449585
Loss :  1.9804292917251587 4.685389995574951 6.66581916809082
Loss :  2.142486810684204 4.26729679107666 6.409783363342285
Loss :  2.030885696411133 4.832961559295654 6.863847255706787
Loss :  2.0898473262786865 4.643186092376709 6.733033180236816
Loss :  1.9720410108566284 4.689652919769287 6.661694049835205
Loss :  1.924938440322876 4.58182430267334 6.506762504577637
Loss :  1.9596221446990967 4.411653518676758 6.371275901794434
Loss :  1.9450623989105225 4.575353145599365 6.520415306091309
Loss :  1.963997483253479 4.6742262840271 6.638223648071289
Loss :  2.019160509109497 4.988020896911621 7.007181167602539
Loss :  2.0279150009155273 4.590753555297852 6.618668556213379
Loss :  1.8976644277572632 4.565093040466309 6.462757587432861
Loss :  1.9512463808059692 4.593306064605713 6.544552326202393
Loss :  1.8423606157302856 4.44327449798584 6.285634994506836
Loss :  2.00366473197937 4.493014812469482 6.496679306030273
  batch 60 loss: 2.00366473197937, 4.493014812469482, 6.496679306030273
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9265270233154297 4.5279130935668945 6.454440116882324
Loss :  2.0060014724731445 4.489399433135986 6.495400905609131
Loss :  2.0118370056152344 4.487563133239746 6.4994001388549805
Loss :  1.9922059774398804 4.780121803283691 6.772327899932861
Loss :  2.0229005813598633 4.227934837341309 6.250835418701172
idx :  64
Loss :  3.43497371673584 4.420945644378662 7.855919361114502
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 3, 4, 5], device='cuda:0')
Loss :  3.263660430908203 4.423497676849365 7.687158107757568
Loss :  3.3692636489868164 4.399664402008057 7.768928050994873
Loss :  3.3416333198547363 4.067434310913086 7.409067630767822
idx :  3
Total LOSS train 6.680936440825462 valid 7.680268287658691
CE LOSS train 2.007030503824353 valid 0.8354083299636841
Contrastive LOSS train 4.6739059537649155 valid 1.0168585777282715
EPOCH 3:
Loss :  1.9840291738510132 4.546648979187012 6.5306782722473145
Loss :  2.0186355113983154 4.706306457519531 6.724942207336426
Loss :  2.094364643096924 4.689115524291992 6.783480167388916
Loss :  2.1134276390075684 4.533478736877441 6.64690637588501
Loss :  2.0188632011413574 4.330606460571289 6.3494696617126465
Loss :  1.9992804527282715 4.70106840133667 6.700348854064941
Loss :  1.999713659286499 4.624505996704102 6.62421989440918
Loss :  2.0168330669403076 4.686675071716309 6.703508377075195
Loss :  1.9643511772155762 4.872054100036621 6.836405277252197
Loss :  1.8997352123260498 4.792295455932617 6.692030906677246
Loss :  1.97483491897583 4.644570827484131 6.619405746459961
Loss :  1.9571958780288696 4.567149639129639 6.524345397949219
Loss :  2.0630452632904053 4.510452747344971 6.573497772216797
Loss :  2.0019423961639404 4.624639987945557 6.626582145690918
Loss :  2.123582601547241 4.6495466232299805 6.773129463195801
Loss :  1.9660719633102417 4.494668006896973 6.460740089416504
Loss :  1.976459264755249 4.811934471130371 6.788393974304199
Loss :  1.9585902690887451 4.498887538909912 6.457477569580078
Loss :  2.074509620666504 4.440793037414551 6.515302658081055
Loss :  2.0109362602233887 4.479413986206055 6.490350246429443
  batch 20 loss: 2.0109362602233887, 4.479413986206055, 6.490350246429443
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  2.0085065364837646 4.537229537963867 6.545736312866211
Loss :  1.9889487028121948 4.55068302154541 6.5396318435668945
Loss :  2.053110122680664 4.390460014343262 6.443570137023926
Loss :  2.0176126956939697 4.5317816734313965 6.549394607543945
Loss :  2.0466701984405518 4.803410053253174 6.850080490112305
Loss :  1.9453359842300415 4.512850284576416 6.458186149597168
Loss :  1.9490472078323364 4.630126953125 6.579174041748047
Loss :  1.98401939868927 4.340492248535156 6.324511528015137
Loss :  1.9297053813934326 4.489548683166504 6.419254302978516
Loss :  2.1236519813537598 4.4842376708984375 6.607889652252197
Loss :  2.0368404388427734 4.446700096130371 6.4835405349731445
Loss :  1.9081637859344482 4.693609714508057 6.601773262023926
Loss :  1.8752421140670776 4.625009536743164 6.500251770019531
Loss :  1.9335672855377197 4.511767387390137 6.445334434509277
Loss :  1.95499849319458 4.502615451812744 6.457613945007324
Loss :  1.898435354232788 4.713881015777588 6.612316131591797
Loss :  2.0150585174560547 4.4485344886779785 6.463593006134033
Loss :  1.9878597259521484 4.5675506591796875 6.555410385131836
Loss :  1.9936603307724 4.48846960067749 6.48213005065918
Loss :  1.87581205368042 4.479416847229004 6.355228900909424
  batch 40 loss: 1.87581205368042, 4.479416847229004, 6.355228900909424
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9052526950836182 4.444058895111084 6.349311828613281
Loss :  1.943787932395935 4.604416847229004 6.5482048988342285
Loss :  1.9723927974700928 4.4341349601745605 6.406527519226074
Loss :  1.9714006185531616 4.528624057769775 6.500024795532227
Loss :  1.9472075700759888 4.444738864898682 6.391946315765381
Loss :  1.9036310911178589 4.550691604614258 6.454322814941406
Loss :  1.855445384979248 4.514240741729736 6.369686126708984
Loss :  1.965157389640808 4.549893379211426 6.515050888061523
Loss :  1.933290719985962 4.370941638946533 6.304232597351074
Loss :  2.0345282554626465 4.683478832244873 6.7180070877075195
Loss :  1.9083985090255737 4.561281204223633 6.469679832458496
Loss :  1.7950375080108643 4.4614386558532715 6.256476402282715
Loss :  1.910938024520874 4.435104846954346 6.346042633056641
Loss :  1.8468141555786133 4.391483783721924 6.238297939300537
Loss :  1.9669193029403687 4.342195510864258 6.309114933013916
Loss :  1.954219937324524 4.782342433929443 6.736562252044678
Loss :  1.9375216960906982 4.699480056762695 6.637001991271973
Loss :  1.9376633167266846 4.531739234924316 6.469402313232422
Loss :  1.9292097091674805 4.680694103240967 6.609903812408447
Loss :  1.8946501016616821 4.648097515106201 6.542747497558594
  batch 60 loss: 1.8946501016616821, 4.648097515106201, 6.542747497558594
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8318438529968262 4.568152904510498 6.399996757507324
Loss :  1.8979103565216064 5.0240960121154785 6.922006607055664
Loss :  1.828923225402832 4.622466087341309 6.451389312744141
Loss :  1.9338597059249878 4.584650993347168 6.518510818481445
Loss :  1.9261277914047241 4.115373134613037 6.041501045227051
idx :  64
Loss :  2.9773693084716797 4.378732204437256 7.3561015129089355
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 5], device='cuda:0')
Loss :  2.9256749153137207 4.394590854644775 7.320265769958496
Loss :  2.933885335922241 4.380808353424072 7.314693450927734
Loss :  2.688110828399658 4.255934238433838 6.944045066833496
idx :  3
Total LOSS train 6.628152899444103 valid 7.2337764501571655
CE LOSS train 1.9949184395372868 valid 0.6720277070999146
Contrastive LOSS train 4.633234426379204 valid 1.0639835596084595
EPOCH 4:
Loss :  1.860694408416748 4.948174476623535 6.808868885040283
Loss :  1.8199341297149658 4.97852897644043 6.798462867736816
Loss :  1.82001793384552 4.44233512878418 6.26235294342041
Loss :  1.7702351808547974 4.118467807769775 5.888702869415283
Loss :  1.772287130355835 4.450819492340088 6.223106384277344
Loss :  1.770504355430603 4.122211456298828 5.892715930938721
Loss :  1.7792900800704956 4.576391220092773 6.355681419372559
Loss :  1.8057085275650024 4.077673435211182 5.8833818435668945
Loss :  1.909813404083252 4.454570293426514 6.364383697509766
Loss :  1.7544169425964355 4.422696590423584 6.1771135330200195
Loss :  1.8216909170150757 4.726447582244873 6.548138618469238
Loss :  1.8985826969146729 4.5587897300720215 6.457372665405273
Loss :  1.885294795036316 4.625009536743164 6.5103044509887695
Loss :  1.7998802661895752 4.447600841522217 6.247481346130371
Loss :  1.9344943761825562 4.628983497619629 6.563477993011475
Loss :  1.8190743923187256 4.596041202545166 6.4151153564453125
Loss :  1.8618342876434326 4.556696891784668 6.41853141784668
Loss :  2.0000481605529785 4.563673496246338 6.563721656799316
Loss :  1.8625866174697876 4.196037769317627 6.058624267578125
Loss :  1.799878478050232 4.5320611000061035 6.331939697265625
  batch 20 loss: 1.799878478050232, 4.5320611000061035, 6.331939697265625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9219470024108887 4.4369893074035645 6.358936309814453
Loss :  1.8455125093460083 4.323551654815674 6.169064044952393
Loss :  1.837067723274231 4.578440189361572 6.415507793426514
Loss :  1.9206467866897583 4.3590312004089355 6.279677867889404
Loss :  1.7939279079437256 4.747689723968506 6.541617393493652
Loss :  1.8004803657531738 4.498687744140625 6.299168109893799
Loss :  1.8095037937164307 4.3949174880981445 6.204421043395996
Loss :  1.8268471956253052 4.41538143157959 6.2422285079956055
Loss :  1.8734534978866577 4.721312046051025 6.594765663146973
Loss :  1.7761497497558594 4.736031532287598 6.512181282043457
Loss :  1.8422448635101318 4.4870285987854 6.329273223876953
Loss :  1.788033366203308 4.481201648712158 6.269235134124756
Loss :  1.8060425519943237 4.4551544189453125 6.261197090148926
Loss :  1.8333837985992432 4.393998146057129 6.227381706237793
Loss :  1.885867714881897 4.180919170379639 6.066786766052246
Loss :  1.8785459995269775 4.556410789489746 6.4349565505981445
Loss :  1.9605857133865356 4.911744594573975 6.872330188751221
Loss :  1.9096587896347046 5.280824184417725 7.190483093261719
Loss :  1.8159762620925903 4.173050880432129 5.98902702331543
Loss :  1.8360881805419922 4.790934085845947 6.6270222663879395
  batch 40 loss: 1.8360881805419922, 4.790934085845947, 6.6270222663879395
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.820441722869873 4.120533466339111 5.940975189208984
Loss :  1.8114840984344482 3.923405647277832 5.734889984130859
Loss :  1.8969714641571045 4.287830829620361 6.184802055358887
Loss :  1.8373006582260132 4.597297191619873 6.434597969055176
Loss :  2.0391366481781006 4.242207050323486 6.281343460083008
Loss :  1.8356069326400757 4.388009548187256 6.223616600036621
Loss :  1.9653204679489136 4.436400890350342 6.401721477508545
Loss :  1.9367297887802124 4.627720355987549 6.564450263977051
Loss :  2.0519180297851562 4.463768005371094 6.51568603515625
Loss :  1.939664363861084 4.544775009155273 6.484439373016357
Loss :  1.886067271232605 4.5830864906311035 6.469153881072998
Loss :  1.9114792346954346 4.544830322265625 6.4563093185424805
Loss :  1.82864248752594 4.590560436248779 6.41920280456543
Loss :  2.015421152114868 4.629746437072754 6.645167350769043
Loss :  2.050522565841675 4.589873313903809 6.6403961181640625
Loss :  2.0481069087982178 4.655470371246338 6.703577041625977
Loss :  1.9674783945083618 4.562685489654541 6.530163764953613
Loss :  1.923183798789978 4.750239372253418 6.6734232902526855
Loss :  1.859171986579895 4.535083293914795 6.3942551612854
Loss :  1.8502057790756226 4.742303848266602 6.592509746551514
  batch 60 loss: 1.8502057790756226, 4.742303848266602, 6.592509746551514
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8538392782211304 4.816598415374756 6.670437812805176
Loss :  1.9163528680801392 4.536186218261719 6.452538967132568
Loss :  1.8138456344604492 4.36668062210083 6.180526256561279
Loss :  1.8972176313400269 4.4270782470703125 6.324295997619629
Loss :  1.7915288209915161 4.225864410400391 6.017393112182617
idx :  64
Loss :  2.776733875274658 4.112342834472656 6.8890767097473145
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 3, 5], device='cuda:0')
Loss :  2.705888271331787 4.175085544586182 6.880973815917969
Loss :  2.6880786418914795 4.045075416564941 6.733154296875
Loss :  2.5967957973480225 4.138294696807861 6.735090255737305
idx :  3
Total LOSS train 6.477978311479092 valid 6.809573769569397
CE LOSS train 1.8977479506283998 valid 0.6491989493370056
Contrastive LOSS train 4.580230385065079 valid 1.0345736742019653
Saved best model. Old loss 7.156961441040039 and new best loss 6.809573769569397
EPOCH 5:
Loss :  1.9582629203796387 4.711808204650879 6.670071125030518
Loss :  1.9173634052276611 4.760660648345947 6.6780242919921875
Loss :  1.99396550655365 4.55674934387207 6.55071496963501
Loss :  1.9801331758499146 4.459486961364746 6.439620018005371
Loss :  1.8497226238250732 4.643624782562256 6.49334716796875
Loss :  1.9050071239471436 4.433711051940918 6.338718414306641
Loss :  1.7881271839141846 4.646191120147705 6.434318542480469
Loss :  1.912461757659912 4.285061836242676 6.197523593902588
Loss :  1.8814605474472046 4.7426838874816895 6.624144554138184
Loss :  1.8268344402313232 4.404215335845947 6.231049537658691
Loss :  1.9804034233093262 4.468469142913818 6.4488725662231445
Loss :  1.8705307245254517 4.561238765716553 6.431769371032715
Loss :  1.909968376159668 4.772507667541504 6.682476043701172
Loss :  1.856316089630127 4.549865245819092 6.406181335449219
Loss :  1.9412710666656494 4.809545993804932 6.75081729888916
Loss :  1.9815797805786133 4.491012096405029 6.472591876983643
Loss :  1.9258347749710083 4.627933025360107 6.553767681121826
Loss :  1.962740421295166 4.629871845245361 6.592612266540527
Loss :  2.0042006969451904 4.414067268371582 6.418268203735352
Loss :  2.077631711959839 4.484342098236084 6.561973571777344
  batch 20 loss: 2.077631711959839, 4.484342098236084, 6.561973571777344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8462145328521729 4.649465084075928 6.49567985534668
Loss :  1.9539169073104858 4.475384712219238 6.429301738739014
Loss :  1.9648942947387695 4.672536849975586 6.6374311447143555
Loss :  1.978184461593628 4.537323951721191 6.515508651733398
Loss :  1.810798168182373 4.483964443206787 6.29476261138916
Loss :  1.9155104160308838 4.400514602661133 6.3160247802734375
Loss :  2.1098170280456543 4.777331829071045 6.887148857116699
Loss :  1.8357182741165161 4.484734535217285 6.320452690124512
Loss :  1.982688069343567 4.533990383148193 6.516678333282471
Loss :  1.9796053171157837 4.652087211608887 6.631692409515381
Loss :  1.9959135055541992 4.432253360748291 6.42816686630249
Loss :  1.8530867099761963 4.8465166091918945 6.699603080749512
Loss :  1.9041742086410522 4.391273498535156 6.295447826385498
Loss :  1.8461709022521973 4.393038272857666 6.239209175109863
Loss :  1.8997467756271362 4.640031337738037 6.539778232574463
Loss :  1.9997944831848145 4.366302013397217 6.366096496582031
Loss :  2.044696092605591 4.75322961807251 6.79792594909668
Loss :  1.9510325193405151 4.678682327270508 6.6297149658203125
Loss :  1.9460480213165283 4.416472434997559 6.362520217895508
Loss :  1.9248156547546387 4.460302352905273 6.385118007659912
  batch 40 loss: 1.9248156547546387, 4.460302352905273, 6.385118007659912
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8506211042404175 4.41365385055542 6.264275074005127
Loss :  2.0966641902923584 4.554327011108398 6.650991439819336
Loss :  1.9284329414367676 4.665170669555664 6.593603610992432
Loss :  1.9371941089630127 4.579988479614258 6.517182350158691
Loss :  1.9363890886306763 4.622883319854736 6.559272289276123
Loss :  1.8829994201660156 4.70646333694458 6.589462757110596
Loss :  1.9232792854309082 4.618738174438477 6.542017459869385
Loss :  1.8560553789138794 4.513444423675537 6.369499683380127
Loss :  2.0180513858795166 4.498599052429199 6.516650199890137
Loss :  1.9919373989105225 4.650681495666504 6.6426191329956055
Loss :  1.8025643825531006 4.4607462882995605 6.263310432434082
Loss :  1.9296016693115234 4.441338539123535 6.370940208435059
Loss :  1.865935206413269 4.307666301727295 6.1736016273498535
Loss :  1.869534969329834 4.605616092681885 6.475151062011719
Loss :  1.9288508892059326 4.606635570526123 6.535486221313477
Loss :  1.9051145315170288 4.5586628913879395 6.463777542114258
Loss :  1.8600648641586304 4.468881607055664 6.328946590423584
Loss :  1.876695990562439 4.645658493041992 6.522354602813721
Loss :  1.813313603401184 4.479971408843994 6.293284893035889
Loss :  1.8346176147460938 4.529022693634033 6.363640308380127
  batch 60 loss: 1.8346176147460938, 4.529022693634033, 6.363640308380127
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7893345355987549 4.460111141204834 6.249445915222168
Loss :  1.8707858324050903 4.799363613128662 6.670149326324463
Loss :  1.728982925415039 4.517479419708252 6.246462345123291
Loss :  1.9150829315185547 4.315708637237549 6.2307915687561035
Loss :  1.8315882682800293 4.15024471282959 5.981832981109619
idx :  64
Loss :  1.9637401103973389 4.594597816467285 6.558338165283203
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9913548231124878 4.451887607574463 6.44324254989624
Loss :  2.053042411804199 4.337820529937744 6.390862941741943
Loss :  2.1253974437713623 4.399217128753662 6.524614334106445
idx :  3
Total LOSS train 6.565310530364513 valid 6.479264497756958
CE LOSS train 1.9454739782959223 valid 0.5313493609428406
Contrastive LOSS train 4.619836546480656 valid 1.0998042821884155
Saved best model. Old loss 6.809573769569397 and new best loss 6.479264497756958
EPOCH 6:
Loss :  1.7965636253356934 4.3302106857299805 6.126774311065674
Loss :  1.7724037170410156 4.4349470138549805 6.207350730895996
Loss :  1.8756012916564941 4.487496852874756 6.36309814453125
Loss :  1.8572431802749634 4.491941928863525 6.349184989929199
Loss :  2.1024229526519775 4.435011386871338 6.5374345779418945
Loss :  1.8359417915344238 4.264736175537109 6.100677967071533
Loss :  1.8140332698822021 4.460692405700684 6.274725914001465
Loss :  1.8385193347930908 4.5788397789001465 6.417359352111816
Loss :  1.8588072061538696 4.442397594451904 6.301204681396484
Loss :  1.9121417999267578 4.210493564605713 6.122635364532471
Loss :  1.837995171546936 4.368196964263916 6.2061920166015625
Loss :  1.8716884851455688 4.557877063751221 6.4295654296875
Loss :  1.8962863683700562 4.371702671051025 6.267989158630371
Loss :  1.8838998079299927 4.398507118225098 6.282406806945801
Loss :  1.935482144355774 4.517080783843994 6.4525628089904785
Loss :  1.8398797512054443 4.8172760009765625 6.657155990600586
Loss :  1.829420804977417 4.501082897186279 6.330503463745117
Loss :  1.8087562322616577 4.377713203430176 6.186469554901123
Loss :  1.9272264242172241 4.62685489654541 6.554081439971924
Loss :  1.9654358625411987 4.565304756164551 6.530740737915039
  batch 20 loss: 1.9654358625411987, 4.565304756164551, 6.530740737915039
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.921109914779663 4.388047695159912 6.309157371520996
Loss :  1.9068681001663208 4.461441516876221 6.368309497833252
Loss :  1.8874298334121704 4.5756964683532715 6.463126182556152
Loss :  1.8611042499542236 4.890528202056885 6.7516326904296875
Loss :  1.7540085315704346 4.71422004699707 6.468228340148926
Loss :  1.9452259540557861 4.444885730743408 6.390111923217773
Loss :  1.9253058433532715 4.718038082122803 6.643343925476074
Loss :  1.9478304386138916 4.505332946777344 6.453163146972656
Loss :  1.8351129293441772 4.451675891876221 6.2867889404296875
Loss :  1.9479260444641113 4.414137840270996 6.362063884735107
Loss :  1.8263273239135742 4.946444034576416 6.77277135848999
Loss :  1.8164303302764893 4.581587791442871 6.398017883300781
Loss :  1.875791072845459 4.567324161529541 6.443115234375
Loss :  1.896711826324463 4.458947658538818 6.355659484863281
Loss :  1.8486524820327759 4.507519721984863 6.35617208480835
Loss :  1.9002655744552612 4.4305806159973145 6.330846309661865
Loss :  1.9017583131790161 4.71351957321167 6.6152777671813965
Loss :  1.85601007938385 4.589775562286377 6.4457855224609375
Loss :  1.8724582195281982 4.581446170806885 6.453904151916504
Loss :  1.8157753944396973 4.609555721282959 6.425331115722656
  batch 40 loss: 1.8157753944396973, 4.609555721282959, 6.425331115722656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8551892042160034 4.581358432769775 6.436547756195068
Loss :  1.9535988569259644 4.741373538970947 6.694972515106201
Loss :  1.866668939590454 4.669821262359619 6.536490440368652
Loss :  1.784914255142212 4.491569519042969 6.276483535766602
Loss :  1.8869940042495728 4.732172012329102 6.619165897369385
Loss :  1.9198107719421387 4.506712913513184 6.426523685455322
Loss :  2.0311546325683594 4.6084699630737305 6.63962459564209
Loss :  1.9474449157714844 5.394783020019531 7.342227935791016
Loss :  1.9380139112472534 4.661513805389404 6.599527835845947
Loss :  2.031787872314453 4.6837921142578125 6.715579986572266
Loss :  1.8693634271621704 4.518548965454102 6.387912273406982
Loss :  1.8196948766708374 4.616791248321533 6.43648624420166
Loss :  1.9439984560012817 4.6091790199279785 6.553177356719971
Loss :  1.9293590784072876 4.585549354553223 6.514908313751221
Loss :  1.8659477233886719 4.563105583190918 6.42905330657959
Loss :  1.9736430644989014 4.574736595153809 6.548379898071289
Loss :  1.8164844512939453 4.597201347351074 6.4136857986450195
Loss :  1.9117971658706665 4.482696533203125 6.394493579864502
Loss :  1.9171879291534424 4.576087474822998 6.4932756423950195
Loss :  1.8356949090957642 4.373100757598877 6.208795547485352
  batch 60 loss: 1.8356949090957642, 4.373100757598877, 6.208795547485352
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8420169353485107 4.548516273498535 6.390533447265625
Loss :  1.8621238470077515 4.761627197265625 6.623751163482666
Loss :  1.936734914779663 4.336422443389893 6.273157119750977
Loss :  1.84727942943573 4.400440692901611 6.247720241546631
Loss :  1.863312005996704 4.36998176574707 6.233293533325195
idx :  64
Loss :  1.9016623497009277 4.34296178817749 6.244624137878418
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([1, 3], device='cuda:0')
Loss :  1.8974413871765137 4.4344377517700195 6.331879138946533
Loss :  1.9062116146087646 4.246230602264404 6.15244197845459
Loss :  1.8706144094467163 4.189482688903809 6.0600972175598145
idx :  3
Total LOSS train 6.534791998565197 valid 6.197260618209839
CE LOSS train 1.9133135508745909 valid 0.4676536023616791
Contrastive LOSS train 4.621478453278542 valid 1.0473706722259521
Saved best model. Old loss 6.479264497756958 and new best loss 6.197260618209839
EPOCH 7:
Loss :  1.9072240591049194 4.405107498168945 6.312331676483154
Loss :  1.9582964181900024 4.519852638244629 6.478148937225342
Loss :  1.910717248916626 4.500786304473877 6.411503791809082
Loss :  1.8136451244354248 4.475207805633545 6.288852691650391
Loss :  1.9172213077545166 4.460110187530518 6.377331733703613
Loss :  1.8815407752990723 4.492588996887207 6.374129772186279
Loss :  1.9021893739700317 4.723977565765381 6.626166820526123
Loss :  1.945796012878418 4.282290935516357 6.228086948394775
Loss :  1.9404069185256958 4.396673202514648 6.337080001831055
Loss :  1.9188116788864136 4.468216896057129 6.387028694152832
Loss :  1.9263019561767578 4.379120826721191 6.305422782897949
Loss :  1.8982070684432983 4.602007865905762 6.50021505355835
Loss :  1.8950446844100952 4.9163360595703125 6.811380863189697
Loss :  1.8426342010498047 5.025345802307129 6.867980003356934
Loss :  1.8663169145584106 4.770280361175537 6.636597156524658
Loss :  1.839959979057312 4.773553848266602 6.613513946533203
Loss :  1.8826414346694946 4.7194695472717285 6.602110862731934
Loss :  1.8682146072387695 4.574618816375732 6.442833423614502
Loss :  1.8909952640533447 4.192408561706543 6.083403587341309
Loss :  1.8504055738449097 4.7852020263671875 6.635607719421387
  batch 20 loss: 1.8504055738449097, 4.7852020263671875, 6.635607719421387
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8101915121078491 4.939840316772461 6.7500319480896
Loss :  1.8599984645843506 4.712574481964111 6.572572708129883
Loss :  1.8455315828323364 4.522922039031982 6.368453502655029
Loss :  1.8540388345718384 4.532740592956543 6.386779308319092
Loss :  1.9968554973602295 4.528441905975342 6.525297164916992
Loss :  1.864372730255127 4.285049915313721 6.149422645568848
Loss :  1.8833765983581543 4.5357866287231445 6.419163227081299
Loss :  1.8442471027374268 4.8824896812438965 6.726737022399902
Loss :  1.8550283908843994 4.371695518493652 6.226723670959473
Loss :  1.9269704818725586 4.240159034729004 6.1671295166015625
Loss :  1.87126624584198 3.9650304317474365 5.836296558380127
Loss :  2.0366859436035156 4.321760654449463 6.3584465980529785
Loss :  1.85369873046875 4.905796527862549 6.759495258331299
Loss :  1.7880772352218628 5.273265361785889 7.061342716217041
Loss :  1.808036208152771 4.476511001586914 6.284547328948975
Loss :  1.7863909006118774 4.43420934677124 6.220600128173828
Loss :  1.7355929613113403 4.332937717437744 6.068530559539795
Loss :  1.8432263135910034 4.466601848602295 6.309828281402588
Loss :  1.7593101263046265 4.609778881072998 6.369089126586914
  batch 40 loss: 1.7593101263046265, 4.609778881072998, 6.369089126586914
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8364617824554443 4.280117511749268 6.116579055786133
Loss :  1.833549976348877 4.406302452087402 6.239852428436279
Loss :  1.8125889301300049 4.239489555358887 6.0520782470703125
Loss :  1.8310387134552002 4.270927429199219 6.10196590423584
Loss :  1.8356661796569824 4.371073246002197 6.20673942565918
Loss :  1.769464135169983 4.451542377471924 6.221006393432617
Loss :  1.7519649267196655 4.477309703826904 6.229274749755859
Loss :  1.791750431060791 4.671354293823242 6.463104724884033
Loss :  1.790751576423645 4.564780235290527 6.355531692504883
Loss :  1.826021432876587 4.672464370727539 6.498485565185547
Loss :  1.8230639696121216 4.551304817199707 6.374368667602539
Loss :  1.8556358814239502 4.887271404266357 6.742907524108887
Loss :  1.8237168788909912 4.500818252563477 6.324535369873047
Loss :  1.861913800239563 4.521818161010742 6.383731842041016
Loss :  1.9000487327575684 4.364235877990723 6.264284610748291
Loss :  1.8426611423492432 4.312827110290527 6.155488014221191
Loss :  1.893754243850708 4.582945823669434 6.4766998291015625
Loss :  1.8572598695755005 4.4302263259887695 6.2874860763549805
Loss :  1.9750430583953857 4.504305362701416 6.479348182678223
Loss :  1.8528605699539185 4.698426723480225 6.5512871742248535
  batch 60 loss: 1.8528605699539185, 4.698426723480225, 6.5512871742248535
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9296448230743408 4.7528977394104 6.68254280090332
Loss :  2.0151519775390625 4.5473408699035645 6.562492847442627
Loss :  1.92051362991333 4.520913600921631 6.441427230834961
Loss :  1.9871968030929565 4.442998886108398 6.4301958084106445
Loss :  1.8616008758544922 4.514875888824463 6.376476764678955
idx :  64
Loss :  1.8732343912124634 4.433760643005371 6.306994915008545
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7905359268188477 4.450085639953613 6.240621566772461
Loss :  1.8666150569915771 4.211990833282471 6.078605651855469
Loss :  1.696227788925171 4.231995105743408 5.92822265625
idx :  3
Total LOSS train 6.509374119341373 valid 6.138611197471619
CE LOSS train 1.8969015050679445 valid 0.4240569472312927
Contrastive LOSS train 4.612472642213106 valid 1.057998776435852
Saved best model. Old loss 6.197260618209839 and new best loss 6.138611197471619
EPOCH 8:
Loss :  2.026064157485962 4.351396560668945 6.377460479736328
Loss :  2.027026891708374 4.792281150817871 6.819308280944824
Loss :  2.0818965435028076 4.535534858703613 6.617431640625
Loss :  1.995391607284546 4.553107738494873 6.54849910736084
Loss :  2.0882163047790527 4.465686321258545 6.553902626037598
Loss :  1.9261394739151 4.314306259155273 6.240445613861084
Loss :  2.072796583175659 4.428557395935059 6.501354217529297
Loss :  2.0554420948028564 4.626549243927002 6.6819915771484375
Loss :  1.9830260276794434 4.36825704574585 6.351283073425293
Loss :  2.012533187866211 4.412323951721191 6.424857139587402
Loss :  2.1160507202148438 4.651190280914307 6.76724100112915
Loss :  1.9750264883041382 4.535415172576904 6.510441780090332
Loss :  1.9048640727996826 4.458220958709717 6.36308479309082
Loss :  1.9233149290084839 4.215433597564697 6.138748645782471
Loss :  1.9452847242355347 4.51469087600708 6.459975719451904
Loss :  1.9449704885482788 4.529573917388916 6.474544525146484
Loss :  1.8631608486175537 4.5828022956848145 6.445962905883789
Loss :  1.9047636985778809 4.234796524047852 6.139560222625732
Loss :  1.9003417491912842 4.382503032684326 6.282844543457031
Loss :  1.987089991569519 4.4488959312438965 6.435986042022705
  batch 20 loss: 1.987089991569519, 4.4488959312438965, 6.435986042022705
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.965754747390747 4.502670764923096 6.468425750732422
Loss :  1.9133338928222656 4.501446723937988 6.414780616760254
Loss :  1.986270546913147 4.34019660949707 6.326467037200928
Loss :  1.8914257287979126 4.469213485717773 6.3606390953063965
Loss :  1.864302635192871 4.67555570602417 6.539858341217041
Loss :  1.936246633529663 4.400815963745117 6.337062835693359
Loss :  1.9600239992141724 4.540482997894287 6.50050687789917
Loss :  1.9060744047164917 4.223153591156006 6.129228115081787
Loss :  1.8570151329040527 4.286395072937012 6.1434102058410645
Loss :  1.7960097789764404 4.455286979675293 6.2512969970703125
Loss :  1.8374717235565186 4.588540077209473 6.42601203918457
Loss :  1.840643286705017 4.554476737976074 6.395120143890381
Loss :  1.8568949699401855 4.62516975402832 6.482064723968506
Loss :  1.9433720111846924 4.533812522888184 6.477184295654297
Loss :  1.8087666034698486 4.572229862213135 6.3809967041015625
Loss :  1.9564383029937744 4.57418966293335 6.530628204345703
Loss :  1.9123197793960571 4.799050807952881 6.711370468139648
Loss :  1.8920475244522095 4.5501508712768555 6.442198276519775
Loss :  1.7976583242416382 4.400132656097412 6.19779109954834
Loss :  1.8559973239898682 4.5022735595703125 6.358270645141602
  batch 40 loss: 1.8559973239898682, 4.5022735595703125, 6.358270645141602
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9635355472564697 4.857326984405518 6.820862770080566
Loss :  1.9941120147705078 4.417060852050781 6.411172866821289
Loss :  1.8912417888641357 4.262054920196533 6.15329647064209
Loss :  1.9234092235565186 4.312467098236084 6.235876083374023
Loss :  1.87978994846344 4.412871360778809 6.292661190032959
Loss :  1.881614327430725 4.434817314147949 6.316431522369385
Loss :  1.8504942655563354 4.288802623748779 6.139297008514404
Loss :  1.8145136833190918 4.721997261047363 6.536510944366455
Loss :  1.7628854513168335 4.820080280303955 6.582965850830078
Loss :  1.8373751640319824 4.506373882293701 6.343749046325684
Loss :  1.8313127756118774 4.00890588760376 5.840218544006348
Loss :  1.8486851453781128 4.006496906280518 5.85518217086792
Loss :  1.8387805223464966 4.5498785972595215 6.3886590003967285
Loss :  1.8446227312088013 4.16471004486084 6.009332656860352
Loss :  1.8543814420700073 4.602359294891357 6.456740856170654
Loss :  1.8286172151565552 4.3746657371521 6.203282833099365
Loss :  1.857895851135254 4.758345603942871 6.616241455078125
Loss :  1.8334633111953735 4.405488967895508 6.238952159881592
Loss :  1.892590880393982 4.302603721618652 6.195194721221924
Loss :  1.855500340461731 3.9130074977874756 5.768507957458496
  batch 60 loss: 1.855500340461731, 3.9130074977874756, 5.768507957458496
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8047239780426025 4.341741561889648 6.146465301513672
Loss :  1.8404897451400757 4.166333198547363 6.0068230628967285
Loss :  1.8326667547225952 4.176884651184082 6.009551525115967
Loss :  1.8370015621185303 4.172717094421387 6.009718894958496
Loss :  1.7831404209136963 4.015641689300537 5.7987823486328125
idx :  64
Loss :  1.6348130702972412 4.262206077575684 5.897019386291504
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6396945714950562 4.3913116455078125 6.031006336212158
Loss :  1.6397135257720947 4.274026870727539 5.913740158081055
Loss :  1.6406450271606445 4.2569661140441895 5.897611141204834
idx :  3
Total LOSS train 6.44351115077734 valid 5.934844255447388
CE LOSS train 1.9358486253768206 valid 0.41016125679016113
Contrastive LOSS train 4.507662508636713 valid 1.0642415285110474
Saved best model. Old loss 6.138611197471619 and new best loss 5.934844255447388
EPOCH 9:
Loss :  1.8379286527633667 4.1861891746521 6.024117946624756
Loss :  1.787508249282837 4.179098606109619 5.966607093811035
Loss :  1.8216280937194824 4.385988712310791 6.207616806030273
Loss :  1.828548789024353 4.254470348358154 6.083019256591797
Loss :  1.7471914291381836 4.169464588165283 5.916656017303467
Loss :  1.86802339553833 4.118185520172119 5.986208915710449
Loss :  1.8175995349884033 4.357475280761719 6.175074577331543
Loss :  1.8569109439849854 4.185038089752197 6.041949272155762
Loss :  1.8509336709976196 4.019885540008545 5.870819091796875
Loss :  1.8382536172866821 3.762261390686035 5.600514888763428
Loss :  1.8825167417526245 3.879380702972412 5.761897563934326
Loss :  1.8786211013793945 4.201849460601807 6.080470561981201
Loss :  1.8679550886154175 3.9545583724975586 5.822513580322266
Loss :  1.8652585744857788 4.042294025421143 5.907552719116211
Loss :  1.8088191747665405 4.372566223144531 6.181385517120361
Loss :  1.9072359800338745 4.498470783233643 6.405706882476807
Loss :  1.947288990020752 4.619760990142822 6.567049980163574
Loss :  1.8850610256195068 4.712635517120361 6.597696304321289
Loss :  2.018308639526367 4.604895114898682 6.623203754425049
Loss :  2.005018472671509 4.497534275054932 6.5025529861450195
  batch 20 loss: 2.005018472671509, 4.497534275054932, 6.5025529861450195
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  2.0140342712402344 4.783560276031494 6.7975945472717285
Loss :  1.9339443445205688 4.509678840637207 6.443623065948486
Loss :  1.9522894620895386 4.651849746704102 6.60413932800293
Loss :  1.9493606090545654 4.354542255401611 6.303902626037598
Loss :  1.958522081375122 4.57532262802124 6.533844947814941
Loss :  1.9298436641693115 4.483870029449463 6.413713455200195
Loss :  1.8311514854431152 4.7245774269104 6.555728912353516
Loss :  1.840683102607727 4.15461540222168 5.995298385620117
Loss :  1.9033982753753662 4.397643089294434 6.301041603088379
Loss :  1.824161410331726 4.443478107452393 6.267639636993408
Loss :  1.8049629926681519 4.903041362762451 6.708004474639893
Loss :  1.9031741619110107 4.530442714691162 6.433616638183594
Loss :  1.878525972366333 4.3244733810424805 6.202999114990234
Loss :  1.8842060565948486 4.794961929321289 6.679167747497559
Loss :  1.8145421743392944 4.282864093780518 6.097406387329102
Loss :  1.8867844343185425 4.389822006225586 6.276606559753418
Loss :  1.9338523149490356 4.520705223083496 6.454557418823242
Loss :  1.8876092433929443 4.147648334503174 6.035257339477539
Loss :  1.89165198802948 4.1989946365356445 6.090646743774414
Loss :  1.8676499128341675 4.399755001068115 6.267405033111572
  batch 40 loss: 1.8676499128341675, 4.399755001068115, 6.267405033111572
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9215893745422363 4.292265892028809 6.213855266571045
Loss :  1.8098368644714355 4.1532087326049805 5.963045597076416
Loss :  1.8542439937591553 4.402237415313721 6.256481170654297
Loss :  1.7858786582946777 4.605771064758301 6.3916497230529785
Loss :  1.9104845523834229 4.626949787139893 6.5374345779418945
Loss :  1.8654530048370361 4.622489929199219 6.487942695617676
Loss :  1.8726625442504883 4.663269519805908 6.5359320640563965
Loss :  1.8729841709136963 4.493830680847168 6.366814613342285
Loss :  1.9095518589019775 4.469228744506836 6.378780364990234
Loss :  1.9071379899978638 4.774892330169678 6.682030200958252
Loss :  1.8309942483901978 4.649133205413818 6.480127334594727
Loss :  1.8215057849884033 4.624197006225586 6.44570255279541
Loss :  1.896912932395935 4.4965128898620605 6.393425941467285
Loss :  1.8131576776504517 4.226117134094238 6.0392746925354
Loss :  1.8797669410705566 4.425995826721191 6.305762767791748
Loss :  1.8186448812484741 4.521382808685303 6.340027809143066
Loss :  1.8491793870925903 4.776658058166504 6.625837326049805
Loss :  1.8729807138442993 4.5795207023620605 6.45250129699707
Loss :  1.8656777143478394 4.653027534484863 6.518705368041992
Loss :  1.8929226398468018 4.516088962554932 6.4090118408203125
  batch 60 loss: 1.8929226398468018, 4.516088962554932, 6.4090118408203125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.811548113822937 4.531161785125732 6.342710018157959
Loss :  1.8693125247955322 4.64093017578125 6.510242462158203
Loss :  1.8838725090026855 4.497376918792725 6.38124942779541
Loss :  1.8628976345062256 4.366389274597168 6.229287147521973
Loss :  1.9494953155517578 4.07316255569458 6.022657871246338
idx :  64
Loss :  1.8543957471847534 4.76556921005249 6.619965076446533
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.89645516872406 4.542680740356445 6.439136028289795
Loss :  1.8713933229446411 4.414069652557373 6.285462856292725
Loss :  1.9383876323699951 4.014098167419434 5.952486038208008
idx :  3
Total LOSS train 6.392114028334618 valid 6.324262499809265
CE LOSS train 1.9037445336580276 valid 0.4845969080924988
Contrastive LOSS train 4.488369502127171 valid 1.0035245418548584
EPOCH 10:
Loss :  1.8777650594711304 4.580619812011719 6.458384990692139
Loss :  1.9624546766281128 4.514066696166992 6.4765214920043945
Loss :  1.9071083068847656 4.646010398864746 6.553118705749512
Loss :  1.9319761991500854 4.444831371307373 6.376807689666748
Loss :  1.8437566757202148 4.446470260620117 6.290226936340332
Loss :  1.8137294054031372 4.185608386993408 5.999337673187256
Loss :  1.9311074018478394 4.646421909332275 6.577529430389404
Loss :  1.8796237707138062 4.580301761627197 6.459925651550293
Loss :  1.890707015991211 4.69407844543457 6.584785461425781
Loss :  1.9179911613464355 4.569516658782959 6.4875078201293945
Loss :  1.8849899768829346 4.680939197540283 6.565929412841797
Loss :  1.9051122665405273 4.782197952270508 6.687310218811035
Loss :  1.9152305126190186 4.542551517486572 6.457781791687012
Loss :  1.772160530090332 4.591499328613281 6.363659858703613
Loss :  1.8739092350006104 4.6607255935668945 6.534634590148926
Loss :  1.8130292892456055 4.5523552894592285 6.365384578704834
Loss :  1.8716329336166382 4.47308349609375 6.344716548919678
Loss :  1.9187779426574707 4.284273624420166 6.203051567077637
Loss :  1.8413971662521362 4.578683376312256 6.420080661773682
Loss :  1.904478907585144 4.528322219848633 6.432801246643066
  batch 20 loss: 1.904478907585144, 4.528322219848633, 6.432801246643066
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9004961252212524 4.359986782073975 6.2604827880859375
Loss :  1.821103572845459 4.471324443817139 6.292428016662598
Loss :  1.8150495290756226 4.323181629180908 6.13823127746582
Loss :  1.8822027444839478 4.256022930145264 6.138225555419922
Loss :  2.0225815773010254 4.526046276092529 6.548627853393555
Loss :  1.8336539268493652 4.50163459777832 6.3352885246276855
Loss :  1.8967885971069336 4.537978172302246 6.43476676940918
Loss :  1.890587329864502 4.55718994140625 6.447777271270752
Loss :  1.8229418992996216 4.538164138793945 6.361105918884277
Loss :  1.8221508264541626 4.453028202056885 6.275178909301758
Loss :  1.8779720067977905 4.604104995727539 6.482077121734619
Loss :  1.832717776298523 4.688687324523926 6.521405220031738
Loss :  1.8641023635864258 4.439281940460205 6.303384304046631
Loss :  1.8268201351165771 4.702645778656006 6.529465675354004
Loss :  1.8763597011566162 4.383978366851807 6.260337829589844
Loss :  1.745143175125122 4.467763900756836 6.212906837463379
Loss :  1.887285828590393 4.487615585327148 6.374901294708252
Loss :  1.9577113389968872 4.2967400550842285 6.254451274871826
Loss :  1.7913050651550293 4.450700283050537 6.242005348205566
Loss :  1.930907130241394 4.483981609344482 6.414888858795166
  batch 40 loss: 1.930907130241394, 4.483981609344482, 6.414888858795166
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8677626848220825 4.247355937957764 6.115118503570557
Loss :  1.7036782503128052 4.4919753074646 6.195653438568115
Loss :  1.8211102485656738 4.262351036071777 6.083461284637451
Loss :  1.8301881551742554 4.660425662994385 6.49061393737793
Loss :  1.8391411304473877 4.1288933753967285 5.968034744262695
Loss :  1.7480093240737915 4.523393154144287 6.271402359008789
Loss :  1.78955078125 4.329469203948975 6.119019985198975
Loss :  1.8657199144363403 4.5475006103515625 6.413220405578613
Loss :  1.8642170429229736 4.3835039138793945 6.247720718383789
Loss :  1.8449002504348755 4.298448085784912 6.143348217010498
Loss :  1.8421850204467773 4.324406147003174 6.166591167449951
Loss :  1.7747076749801636 4.125161170959473 5.899868965148926
Loss :  1.7776738405227661 4.113858699798584 5.8915324211120605
Loss :  1.7635998725891113 4.183018207550049 5.94661808013916
Loss :  1.8067625761032104 4.06984281539917 5.87660551071167
Loss :  1.8385353088378906 4.113740921020508 5.952276229858398
Loss :  1.817655086517334 4.1992506980896 6.016905784606934
Loss :  1.6609042882919312 4.1681718826293945 5.829076290130615
Loss :  1.783600091934204 4.249608993530273 6.033208847045898
Loss :  1.8809762001037598 3.970747470855713 5.851723670959473
  batch 60 loss: 1.8809762001037598, 3.970747470855713, 5.851723670959473
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7487483024597168 4.255876541137695 6.004624843597412
Loss :  1.8994389772415161 4.10474157333374 6.004180431365967
Loss :  1.785918116569519 3.9902608394622803 5.77617883682251
Loss :  1.767338514328003 4.022680759429932 5.7900190353393555
Loss :  1.732230305671692 3.7269091606140137 5.459139347076416
idx :  64
Loss :  1.9504607915878296 4.366359233856201 6.31682014465332
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9797749519348145 4.438155651092529 6.417930603027344
Loss :  1.9996756315231323 4.305021286010742 6.304697036743164
Loss :  2.010030746459961 4.23015022277832 6.240180969238281
idx :  3
Total LOSS train 6.343962125480175 valid 6.319907188415527
CE LOSS train 1.875146422535181 valid 0.5025076866149902
Contrastive LOSS train 4.468815725296736 valid 1.05753755569458
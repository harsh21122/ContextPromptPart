wandb: Currently logged in as: harsh21122. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.3
wandb: Run data is saved locally in /ssd-scratch/harsh21122/ContextPromptPart/wandb/run-20230206_003657-2ipu888q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run radiant-ox-26
wandb: ‚≠êÔ∏è View project at https://wandb.ai/harsh21122/part_segmentation
wandb: üöÄ View run at https://wandb.ai/harsh21122/part_segmentation/runs/2ipu888q
/home/harsh21122/.conda/envs/part/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
/home/harsh21122/.conda/envs/part/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Arguments are :  Namespace(batch_size=10, epochs=300, starting_epoch=1, base_lr=0.03, clip_model='RN50', result_dir='./Results/', model_dir='../ContextPromptPart_model', dataset_dir='/home/harsh21122/tmp/cat_dataset', resume=False, model_name='../ContextPromptPart_model/last_model', calc_accuracy_training=False, multi_step_scheduler=True, lr_decay=0.5, milestones=[50, 100, 150, 200, 250, 300], wandb=True, temperature=0.19, layer_len=-1, ref_layer1='relu3_2', ref_layer2='relu5_4', ref_weight1=0.33, ref_weight2=1.0, lamda_contrastive=5.0, lamda_cross=1.0, weight_decay=0.01)
Using device :  cuda
Length of Train dataset : 646 and Test dataset : 35
Length of Train loader : 65 and Test loader : 4
self.partnames :  ['background', 'head', 'neck', 'torso', 'tail', 'legs']
prompts :  [' the background of the cat.', ' the head of the cat.', ' the neck of the cat.', ' the torso of the cat.', ' the tail of the cat.', ' the legs of the cat.']
model device :  cuda
self.prompts : torch.Size([6, 77])
self.prompts : torch.Size([6, 1024])
self.prompts rquires grad :  <built-in method requires_grad_ of Tensor object at 0x7f74e19494a0>
layer: relu3_2,relu5_4
weighs: [0.33, 1.0]
Printing parameters and their gradient
gamma True
image_encoder.0.weight False
image_encoder.1.weight False
image_encoder.1.bias False
image_encoder.3.weight False
image_encoder.4.weight False
image_encoder.4.bias False
image_encoder.6.weight False
image_encoder.7.weight False
image_encoder.7.bias False
image_encoder.10.0.conv1.weight False
image_encoder.10.0.bn1.weight False
image_encoder.10.0.bn1.bias False
image_encoder.10.0.conv2.weight False
image_encoder.10.0.bn2.weight False
image_encoder.10.0.bn2.bias False
image_encoder.10.0.conv3.weight False
image_encoder.10.0.bn3.weight False
image_encoder.10.0.bn3.bias False
image_encoder.10.0.downsample.0.weight False
image_encoder.10.0.downsample.1.weight False
image_encoder.10.0.downsample.1.bias False
image_encoder.10.1.conv1.weight False
image_encoder.10.1.bn1.weight False
image_encoder.10.1.bn1.bias False
image_encoder.10.1.conv2.weight False
image_encoder.10.1.bn2.weight False
image_encoder.10.1.bn2.bias False
image_encoder.10.1.conv3.weight False
image_encoder.10.1.bn3.weight False
image_encoder.10.1.bn3.bias False
image_encoder.10.2.conv1.weight False
image_encoder.10.2.bn1.weight False
image_encoder.10.2.bn1.bias False
image_encoder.10.2.conv2.weight False
image_encoder.10.2.bn2.weight False
image_encoder.10.2.bn2.bias False
image_encoder.10.2.conv3.weight False
image_encoder.10.2.bn3.weight False
image_encoder.10.2.bn3.bias False
image_encoder.11.0.conv1.weight False
image_encoder.11.0.bn1.weight False
image_encoder.11.0.bn1.bias False
image_encoder.11.0.conv2.weight False
image_encoder.11.0.bn2.weight False
image_encoder.11.0.bn2.bias False
image_encoder.11.0.conv3.weight False
image_encoder.11.0.bn3.weight False
image_encoder.11.0.bn3.bias False
image_encoder.11.0.downsample.0.weight False
image_encoder.11.0.downsample.1.weight False
image_encoder.11.0.downsample.1.bias False
image_encoder.11.1.conv1.weight False
image_encoder.11.1.bn1.weight False
image_encoder.11.1.bn1.bias False
image_encoder.11.1.conv2.weight False
image_encoder.11.1.bn2.weight False
image_encoder.11.1.bn2.bias False
image_encoder.11.1.conv3.weight False
image_encoder.11.1.bn3.weight False
image_encoder.11.1.bn3.bias False
image_encoder.11.2.conv1.weight False
image_encoder.11.2.bn1.weight False
image_encoder.11.2.bn1.bias False
image_encoder.11.2.conv2.weight False
image_encoder.11.2.bn2.weight False
image_encoder.11.2.bn2.bias False
image_encoder.11.2.conv3.weight False
image_encoder.11.2.bn3.weight False
image_encoder.11.2.bn3.bias False
image_encoder.11.3.conv1.weight False
image_encoder.11.3.bn1.weight False
image_encoder.11.3.bn1.bias False
image_encoder.11.3.conv2.weight False
image_encoder.11.3.bn2.weight False
image_encoder.11.3.bn2.bias False
image_encoder.11.3.conv3.weight False
image_encoder.11.3.bn3.weight False
image_encoder.11.3.bn3.bias False
image_encoder.12.0.conv1.weight False
image_encoder.12.0.bn1.weight False
image_encoder.12.0.bn1.bias False
image_encoder.12.0.conv2.weight False
image_encoder.12.0.bn2.weight False
image_encoder.12.0.bn2.bias False
image_encoder.12.0.conv3.weight False
image_encoder.12.0.bn3.weight False
image_encoder.12.0.bn3.bias False
image_encoder.12.0.downsample.0.weight False
image_encoder.12.0.downsample.1.weight False
image_encoder.12.0.downsample.1.bias False
image_encoder.12.1.conv1.weight False
image_encoder.12.1.bn1.weight False
image_encoder.12.1.bn1.bias False
image_encoder.12.1.conv2.weight False
image_encoder.12.1.bn2.weight False
image_encoder.12.1.bn2.bias False
image_encoder.12.1.conv3.weight False
image_encoder.12.1.bn3.weight False
image_encoder.12.1.bn3.bias False
image_encoder.12.2.conv1.weight False
image_encoder.12.2.bn1.weight False
image_encoder.12.2.bn1.bias False
image_encoder.12.2.conv2.weight False
image_encoder.12.2.bn2.weight False
image_encoder.12.2.bn2.bias False
image_encoder.12.2.conv3.weight False
image_encoder.12.2.bn3.weight False
image_encoder.12.2.bn3.bias False
image_encoder.12.3.conv1.weight False
image_encoder.12.3.bn1.weight False
image_encoder.12.3.bn1.bias False
image_encoder.12.3.conv2.weight False
image_encoder.12.3.bn2.weight False
image_encoder.12.3.bn2.bias False
image_encoder.12.3.conv3.weight False
image_encoder.12.3.bn3.weight False
image_encoder.12.3.bn3.bias False
image_encoder.12.4.conv1.weight False
image_encoder.12.4.bn1.weight False
image_encoder.12.4.bn1.bias False
image_encoder.12.4.conv2.weight False
image_encoder.12.4.bn2.weight False
image_encoder.12.4.bn2.bias False
image_encoder.12.4.conv3.weight False
image_encoder.12.4.bn3.weight False
image_encoder.12.4.bn3.bias False
image_encoder.12.5.conv1.weight False
image_encoder.12.5.bn1.weight False
image_encoder.12.5.bn1.bias False
image_encoder.12.5.conv2.weight False
image_encoder.12.5.bn2.weight False
image_encoder.12.5.bn2.bias False
image_encoder.12.5.conv3.weight False
image_encoder.12.5.bn3.weight False
image_encoder.12.5.bn3.bias False
image_encoder.13.0.conv1.weight False
image_encoder.13.0.bn1.weight False
image_encoder.13.0.bn1.bias False
image_encoder.13.0.conv2.weight False
image_encoder.13.0.bn2.weight False
image_encoder.13.0.bn2.bias False
image_encoder.13.0.conv3.weight False
image_encoder.13.0.bn3.weight False
image_encoder.13.0.bn3.bias False
image_encoder.13.0.downsample.0.weight False
image_encoder.13.0.downsample.1.weight False
image_encoder.13.0.downsample.1.bias False
image_encoder.13.1.conv1.weight False
image_encoder.13.1.bn1.weight False
image_encoder.13.1.bn1.bias False
image_encoder.13.1.conv2.weight False
image_encoder.13.1.bn2.weight False
image_encoder.13.1.bn2.bias False
image_encoder.13.1.conv3.weight False
image_encoder.13.1.bn3.weight False
image_encoder.13.1.bn3.bias False
image_encoder.13.2.conv1.weight False
image_encoder.13.2.bn1.weight False
image_encoder.13.2.bn1.bias False
image_encoder.13.2.conv2.weight False
image_encoder.13.2.bn2.weight False
image_encoder.13.2.bn2.bias False
image_encoder.13.2.conv3.weight False
image_encoder.13.2.bn3.weight False
image_encoder.13.2.bn3.bias False
attnpool.positional_embedding True
attnpool.k_proj.weight True
attnpool.k_proj.bias True
attnpool.q_proj.weight True
attnpool.q_proj.bias True
attnpool.v_proj.weight True
attnpool.v_proj.bias True
attnpool.c_proj.weight True
attnpool.c_proj.bias True
align_context.memory_proj.0.weight True
align_context.memory_proj.0.bias True
align_context.memory_proj.1.weight True
align_context.memory_proj.1.bias True
align_context.memory_proj.2.weight True
align_context.memory_proj.2.bias True
align_context.text_proj.0.weight True
align_context.text_proj.0.bias True
align_context.text_proj.1.weight True
align_context.text_proj.1.bias True
align_context.decoder.0.self_attn.q_proj.weight True
align_context.decoder.0.self_attn.k_proj.weight True
align_context.decoder.0.self_attn.v_proj.weight True
align_context.decoder.0.self_attn.proj.weight True
align_context.decoder.0.self_attn.proj.bias True
align_context.decoder.0.cross_attn.q_proj.weight True
align_context.decoder.0.cross_attn.k_proj.weight True
align_context.decoder.0.cross_attn.v_proj.weight True
align_context.decoder.0.cross_attn.proj.weight True
align_context.decoder.0.cross_attn.proj.bias True
align_context.decoder.0.norm1.weight True
align_context.decoder.0.norm1.bias True
align_context.decoder.0.norm2.weight True
align_context.decoder.0.norm2.bias True
align_context.decoder.0.norm3.weight True
align_context.decoder.0.norm3.bias True
align_context.decoder.0.mlp.0.weight True
align_context.decoder.0.mlp.0.bias True
align_context.decoder.0.mlp.3.weight True
align_context.decoder.0.mlp.3.bias True
align_context.decoder.1.self_attn.q_proj.weight True
align_context.decoder.1.self_attn.k_proj.weight True
align_context.decoder.1.self_attn.v_proj.weight True
align_context.decoder.1.self_attn.proj.weight True
align_context.decoder.1.self_attn.proj.bias True
align_context.decoder.1.cross_attn.q_proj.weight True
align_context.decoder.1.cross_attn.k_proj.weight True
align_context.decoder.1.cross_attn.v_proj.weight True
align_context.decoder.1.cross_attn.proj.weight True
align_context.decoder.1.cross_attn.proj.bias True
align_context.decoder.1.norm1.weight True
align_context.decoder.1.norm1.bias True
align_context.decoder.1.norm2.weight True
align_context.decoder.1.norm2.bias True
align_context.decoder.1.norm3.weight True
align_context.decoder.1.norm3.bias True
align_context.decoder.1.mlp.0.weight True
align_context.decoder.1.mlp.0.bias True
align_context.decoder.1.mlp.3.weight True
align_context.decoder.1.mlp.3.bias True
align_context.decoder.2.self_attn.q_proj.weight True
align_context.decoder.2.self_attn.k_proj.weight True
align_context.decoder.2.self_attn.v_proj.weight True
align_context.decoder.2.self_attn.proj.weight True
align_context.decoder.2.self_attn.proj.bias True
align_context.decoder.2.cross_attn.q_proj.weight True
align_context.decoder.2.cross_attn.k_proj.weight True
align_context.decoder.2.cross_attn.v_proj.weight True
align_context.decoder.2.cross_attn.proj.weight True
align_context.decoder.2.cross_attn.proj.bias True
align_context.decoder.2.norm1.weight True
align_context.decoder.2.norm1.bias True
align_context.decoder.2.norm2.weight True
align_context.decoder.2.norm2.bias True
align_context.decoder.2.norm3.weight True
align_context.decoder.2.norm3.bias True
align_context.decoder.2.mlp.0.weight True
align_context.decoder.2.mlp.0.bias True
align_context.decoder.2.mlp.3.weight True
align_context.decoder.2.mlp.3.bias True
align_context.decoder.3.self_attn.q_proj.weight True
align_context.decoder.3.self_attn.k_proj.weight True
align_context.decoder.3.self_attn.v_proj.weight True
align_context.decoder.3.self_attn.proj.weight True
align_context.decoder.3.self_attn.proj.bias True
align_context.decoder.3.cross_attn.q_proj.weight True
align_context.decoder.3.cross_attn.k_proj.weight True
align_context.decoder.3.cross_attn.v_proj.weight True
align_context.decoder.3.cross_attn.proj.weight True
align_context.decoder.3.cross_attn.proj.bias True
align_context.decoder.3.norm1.weight True
align_context.decoder.3.norm1.bias True
align_context.decoder.3.norm2.weight True
align_context.decoder.3.norm2.bias True
align_context.decoder.3.norm3.weight True
align_context.decoder.3.norm3.bias True
align_context.decoder.3.mlp.0.weight True
align_context.decoder.3.mlp.0.bias True
align_context.decoder.3.mlp.3.weight True
align_context.decoder.3.mlp.3.bias True
align_context.decoder.4.self_attn.q_proj.weight True
align_context.decoder.4.self_attn.k_proj.weight True
align_context.decoder.4.self_attn.v_proj.weight True
align_context.decoder.4.self_attn.proj.weight True
align_context.decoder.4.self_attn.proj.bias True
align_context.decoder.4.cross_attn.q_proj.weight True
align_context.decoder.4.cross_attn.k_proj.weight True
align_context.decoder.4.cross_attn.v_proj.weight True
align_context.decoder.4.cross_attn.proj.weight True
align_context.decoder.4.cross_attn.proj.bias True
align_context.decoder.4.norm1.weight True
align_context.decoder.4.norm1.bias True
align_context.decoder.4.norm2.weight True
align_context.decoder.4.norm2.bias True
align_context.decoder.4.norm3.weight True
align_context.decoder.4.norm3.bias True
align_context.decoder.4.mlp.0.weight True
align_context.decoder.4.mlp.0.bias True
align_context.decoder.4.mlp.3.weight True
align_context.decoder.4.mlp.3.bias True
align_context.decoder.5.self_attn.q_proj.weight True
align_context.decoder.5.self_attn.k_proj.weight True
align_context.decoder.5.self_attn.v_proj.weight True
align_context.decoder.5.self_attn.proj.weight True
align_context.decoder.5.self_attn.proj.bias True
align_context.decoder.5.cross_attn.q_proj.weight True
align_context.decoder.5.cross_attn.k_proj.weight True
align_context.decoder.5.cross_attn.v_proj.weight True
align_context.decoder.5.cross_attn.proj.weight True
align_context.decoder.5.cross_attn.proj.bias True
align_context.decoder.5.norm1.weight True
align_context.decoder.5.norm1.bias True
align_context.decoder.5.norm2.weight True
align_context.decoder.5.norm2.bias True
align_context.decoder.5.norm3.weight True
align_context.decoder.5.norm3.bias True
align_context.decoder.5.mlp.0.weight True
align_context.decoder.5.mlp.0.bias True
align_context.decoder.5.mlp.3.weight True
align_context.decoder.5.mlp.3.bias True
align_context.out_proj.0.weight True
align_context.out_proj.0.bias True
align_context.out_proj.1.weight True
align_context.out_proj.1.bias True
decoder.conv_layers.0.weight True
decoder.conv_layers.0.bias True
decoder.conv_layers.2.weight True
decoder.conv_layers.2.bias True
decoder.conv_layers.3.weight True
decoder.conv_layers.3.bias True
decoder.conv_layers.5.weight True
decoder.conv_layers.5.bias True
Total epochs to be executed :  300
EPOCH 1:
Loss :  2.1428468227386475 4.609465599060059 25.190176010131836
Loss :  2.002244234085083 5.742074489593506 30.712617874145508
Loss :  2.288114547729492 5.048223972320557 27.529233932495117
Loss :  2.349520206451416 4.583746910095215 25.26825523376465
Loss :  2.160876512527466 4.821545600891113 26.268604278564453
Loss :  2.27365779876709 4.510699272155762 24.82715606689453
Loss :  2.0258939266204834 4.649302005767822 25.272403717041016
Loss :  2.09563946723938 4.536440372467041 24.777841567993164
Loss :  2.210239887237549 4.745054244995117 25.935510635375977
Loss :  2.1307549476623535 4.398155689239502 24.121532440185547
Loss :  2.217083215713501 4.538315296173096 24.908658981323242
Loss :  2.19953989982605 4.858680248260498 26.49294090270996
Loss :  2.2168776988983154 4.47550106048584 24.594383239746094
Loss :  2.1744704246520996 4.577667713165283 25.062808990478516
Loss :  2.115978240966797 4.532246112823486 24.77720832824707
Loss :  1.977028727531433 4.6351141929626465 25.152599334716797
Loss :  2.0322253704071045 4.560266971588135 24.833559036254883
Loss :  2.119419574737549 4.647603988647461 25.357439041137695
Loss :  2.0711827278137207 4.6894850730896 25.51860809326172
Loss :  2.098771095275879 4.786843299865723 26.03298568725586
  batch 20 loss: 2.098771095275879, 4.786843299865723, 26.03298568725586
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.89203679561615 4.6077117919921875 24.93059539794922
Loss :  2.000615358352661 4.567635536193848 24.83879280090332
Loss :  2.0005669593811035 4.5666680335998535 24.833906173706055
Loss :  2.1386752128601074 4.518023490905762 24.72879409790039
Loss :  1.923238754272461 4.631548881530762 25.080984115600586
Loss :  1.8664852380752563 4.842679023742676 26.079879760742188
Loss :  1.8149274587631226 4.975528240203857 26.692569732666016
Loss :  2.014892339706421 4.544187545776367 24.735830307006836
Loss :  1.992641568183899 4.812071323394775 26.052997589111328
Loss :  2.0461373329162598 4.439214706420898 24.242210388183594
Loss :  1.9338812828063965 4.48621129989624 24.364938735961914
Loss :  2.0045840740203857 4.6029205322265625 25.01918601989746
Loss :  1.9268717765808105 4.74819278717041 25.667835235595703
Loss :  1.935694694519043 4.519593238830566 24.533660888671875
Loss :  1.8062503337860107 4.624435901641846 24.928428649902344
Loss :  1.8321057558059692 4.352873802185059 23.59647560119629
Loss :  1.8537077903747559 4.411037921905518 23.908897399902344
Loss :  1.9412773847579956 4.6785736083984375 25.334144592285156
Loss :  1.8099998235702515 4.297937870025635 23.2996883392334
Loss :  1.8882893323898315 4.320380687713623 23.490192413330078
  batch 40 loss: 1.8882893323898315, 4.320380687713623, 23.490192413330078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8955820798873901 4.413039207458496 23.960779190063477
Loss :  1.8850045204162598 4.510444164276123 24.437225341796875
Loss :  1.8586355447769165 4.679309368133545 25.25518226623535
Loss :  1.8058570623397827 4.375889778137207 23.685304641723633
Loss :  1.785062313079834 4.436007499694824 23.96510124206543
Loss :  1.8708808422088623 4.510019302368164 24.420976638793945
Loss :  1.8261076211929321 4.563780784606934 24.64501190185547
Loss :  1.674017310142517 4.4476470947265625 23.91225242614746
Loss :  1.7736855745315552 4.6095452308654785 24.8214111328125
Loss :  1.8341037034988403 4.518067359924316 24.424440383911133
Loss :  1.8533079624176025 4.557474613189697 24.64068031311035
Loss :  1.8182463645935059 4.386097431182861 23.748733520507812
Loss :  1.755061149597168 4.380650043487549 23.658309936523438
Loss :  1.8202440738677979 4.613075256347656 24.8856201171875
Loss :  1.7036259174346924 4.394801616668701 23.677635192871094
Loss :  1.7310377359390259 4.48110818862915 24.136577606201172
Loss :  1.566867709159851 4.603437423706055 24.584054946899414
Loss :  1.58027982711792 4.390635967254639 23.53346061706543
Loss :  1.6085649728775024 4.582137584686279 24.51925277709961
Loss :  1.7118529081344604 4.502015113830566 24.221927642822266
  batch 60 loss: 1.7118529081344604, 4.502015113830566, 24.221927642822266
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.5858182907104492 4.423257827758789 23.702106475830078
Loss :  1.6010503768920898 4.428112506866455 23.74161148071289
Loss :  1.558902621269226 4.636234760284424 24.740076065063477
Loss :  1.5765137672424316 4.567928791046143 24.416156768798828
Loss :  1.538096308708191 4.117300987243652 22.124601364135742
Loss :  1.6572587490081787 4.412826061248779 23.721389770507812
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4, 5], device='cuda:0')
Loss :  1.6875884532928467 4.429202079772949 23.833599090576172
Loss :  1.6786167621612549 4.359936714172363 23.478300094604492
Loss :  1.7844849824905396 4.380674839019775 23.68785858154297
Total LOSS train 24.813123409564678 valid 23.68028688430786
CE LOSS train 1.9191330946408784 valid 0.4461212456226349
Contrastive LOSS train 4.5787980959965635 valid 1.0951687097549438
Saved best model. Old loss 1000000.0 and new best loss 23.68028688430786
EPOCH 2:
Loss :  1.679298758506775 4.55112886428833 24.4349422454834
Loss :  1.6815474033355713 4.486226558685303 24.112680435180664
Loss :  1.63551664352417 4.414109230041504 23.70606231689453
Loss :  1.6671780347824097 4.448184490203857 23.908100128173828
Loss :  1.6952074766159058 4.476293563842773 24.076675415039062
Loss :  1.6200512647628784 4.452760219573975 23.883852005004883
Loss :  1.7094310522079468 4.403275966644287 23.725811004638672
Loss :  1.688965916633606 4.43214225769043 23.84967803955078
Loss :  1.650551676750183 4.430507183074951 23.80308723449707
Loss :  1.6959213018417358 4.294973373413086 23.170787811279297
Loss :  1.6364330053329468 4.6331892013549805 24.802379608154297
Loss :  1.5462884902954102 4.628117084503174 24.686874389648438
Loss :  1.6172153949737549 4.563920974731445 24.43682098388672
Loss :  1.5292073488235474 4.599393844604492 24.52617645263672
Loss :  1.643180251121521 4.6637468338012695 24.9619140625
Loss :  1.692177414894104 4.832189559936523 25.853124618530273
Loss :  1.5402190685272217 4.305332660675049 23.06688117980957
Loss :  1.6131566762924194 4.308906555175781 23.157690048217773
Loss :  1.5375577211380005 4.357950687408447 23.32731056213379
Loss :  1.73668372631073 4.377767086029053 23.625518798828125
  batch 20 loss: 1.73668372631073, 4.377767086029053, 23.625518798828125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5344345569610596 4.296058177947998 23.014726638793945
Loss :  1.5714831352233887 4.216950416564941 22.656234741210938
Loss :  1.546493649482727 4.49756383895874 24.034313201904297
Loss :  1.5725032091140747 4.520623207092285 24.17561912536621
Loss :  1.6688134670257568 4.731730937957764 25.327468872070312
Loss :  1.551054835319519 4.41310453414917 23.6165771484375
Loss :  1.549739122390747 4.719300746917725 25.146244049072266
Loss :  1.6228969097137451 4.507364749908447 24.159719467163086
Loss :  1.4706122875213623 4.224563121795654 22.593427658081055
Loss :  1.6827163696289062 4.3890228271484375 23.627830505371094
Loss :  1.4820115566253662 4.465660095214844 23.810312271118164
Loss :  1.5808043479919434 4.572781085968018 24.44470977783203
Loss :  1.6388174295425415 4.366551399230957 23.471572875976562
Loss :  1.5881493091583252 4.487118244171143 24.023740768432617
Loss :  1.5296448469161987 4.56838321685791 24.371559143066406
Loss :  1.6269280910491943 4.646921157836914 24.861534118652344
Loss :  1.5385514497756958 4.535101413726807 24.21405792236328
Loss :  1.6617625951766968 4.430296421051025 23.813243865966797
Loss :  1.6232494115829468 4.419294834136963 23.719722747802734
Loss :  1.6150668859481812 4.517552375793457 24.20282745361328
  batch 40 loss: 1.6150668859481812, 4.517552375793457, 24.20282745361328
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.5851110219955444 4.428598880767822 23.728105545043945
Loss :  1.530778408050537 4.5675530433654785 24.36854362487793
Loss :  1.504786729812622 4.688151836395264 24.945547103881836
Loss :  1.5635980367660522 4.474764823913574 23.937423706054688
Loss :  1.506030797958374 4.353273391723633 23.272397994995117
Loss :  1.5455694198608398 4.515928745269775 24.125213623046875
Loss :  1.5251318216323853 4.43501615524292 23.700212478637695
Loss :  1.5418051481246948 4.480401992797852 23.943815231323242
Loss :  1.6020307540893555 4.460637092590332 23.905216217041016
Loss :  1.5440270900726318 4.614800453186035 24.61802864074707
Loss :  1.5550333261489868 4.4571661949157715 23.840864181518555
Loss :  1.5389941930770874 4.3749680519104 23.413833618164062
Loss :  1.5805799961090088 4.561704158782959 24.389101028442383
Loss :  1.6496572494506836 4.343593120574951 23.36762237548828
Loss :  1.5570199489593506 4.405091762542725 23.58247947692871
Loss :  1.6178621053695679 4.38687801361084 23.55225372314453
Loss :  1.441672682762146 4.487020492553711 23.87677574157715
Loss :  1.523743987083435 4.586336135864258 24.455425262451172
Loss :  1.4687813520431519 4.468931674957275 23.813438415527344
Loss :  1.5937601327896118 4.455416679382324 23.8708438873291
  batch 60 loss: 1.5937601327896118, 4.455416679382324, 23.8708438873291
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.4920202493667603 4.503988742828369 24.011962890625
Loss :  1.507960319519043 4.564287185668945 24.329395294189453
Loss :  1.5452297925949097 4.380402088165283 23.44723892211914
Loss :  1.4457484483718872 4.301103115081787 22.951265335083008
Loss :  1.396905541419983 4.159778118133545 22.195796966552734
Loss :  1.8824044466018677 4.43583869934082 24.06159782409668
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([5], device='cuda:0')
Loss :  1.8612194061279297 4.469775199890137 24.21009635925293
Loss :  1.8636000156402588 4.295804977416992 23.34262466430664
Loss :  1.9811887741088867 4.037414073944092 22.168258666992188
Total LOSS train 23.939147861187276 valid 23.44564437866211
CE LOSS train 1.5820824714807364 valid 0.4952971935272217
Contrastive LOSS train 4.471413091512827 valid 1.009353518486023
Saved best model. Old loss 23.68028688430786 and new best loss 23.44564437866211
EPOCH 3:
Loss :  1.5594850778579712 4.2051310539245605 22.585140228271484
Loss :  1.569675326347351 4.68496561050415 24.994503021240234
Loss :  1.4727683067321777 4.336794376373291 23.156740188598633
Loss :  1.504024624824524 4.603297233581543 24.520511627197266
Loss :  1.5199000835418701 4.185406684875488 22.446931838989258
Loss :  1.4502179622650146 4.356301307678223 23.23172378540039
Loss :  1.5160211324691772 4.53402042388916 24.18612289428711
Loss :  1.4680567979812622 4.224663257598877 22.591373443603516
Loss :  1.4870225191116333 4.048008918762207 21.727066040039062
Loss :  1.4880322217941284 4.297667026519775 22.97636604309082
Loss :  1.435327410697937 4.360498428344727 23.23781967163086
Loss :  1.4387696981430054 4.518249034881592 24.03001594543457
Loss :  1.3995498418807983 4.527463436126709 24.036867141723633
Loss :  1.4167958498001099 4.460018634796143 23.716888427734375
Loss :  1.5144906044006348 4.40338659286499 23.531423568725586
Loss :  1.4960205554962158 4.514471054077148 24.068376541137695
Loss :  1.3672593832015991 4.562217712402344 24.178348541259766
Loss :  1.4697424173355103 4.457278728485107 23.756135940551758
Loss :  1.3695718050003052 4.337966442108154 23.059404373168945
Loss :  1.5253716707229614 4.529657363891602 24.17365837097168
  batch 20 loss: 1.5253716707229614, 4.529657363891602, 24.17365837097168
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.4053733348846436 4.3701863288879395 23.256303787231445
Loss :  1.3920488357543945 4.553106784820557 24.157581329345703
Loss :  1.420297622680664 4.562018871307373 24.230392456054688
Loss :  1.4947808980941772 4.533500671386719 24.16228485107422
Loss :  1.4377214908599854 4.541700839996338 24.146224975585938
Loss :  1.4739855527877808 4.365323543548584 23.30060386657715
Loss :  1.48611319065094 4.674032688140869 24.85627555847168
Loss :  1.50381600856781 4.314333438873291 23.075483322143555
Loss :  1.3570255041122437 4.419615268707275 23.455101013183594
Loss :  1.485823392868042 4.449816703796387 23.734907150268555
Loss :  1.3432703018188477 4.357178688049316 23.129161834716797
Loss :  1.4434375762939453 4.551659107208252 24.201732635498047
Loss :  1.364708423614502 4.330984592437744 23.019630432128906
Loss :  1.3602838516235352 4.338045120239258 23.05051040649414
Loss :  1.3292217254638672 4.517172336578369 23.915082931518555
Loss :  1.3512176275253296 4.399099826812744 23.346715927124023
Loss :  1.3620370626449585 4.282079696655273 22.772436141967773
Loss :  1.4933003187179565 4.2989420890808105 22.98801040649414
Loss :  1.5651589632034302 4.307380199432373 23.102060317993164
Loss :  1.4752814769744873 4.302389144897461 22.987226486206055
  batch 40 loss: 1.4752814769744873, 4.302389144897461, 22.987226486206055
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.45856773853302 4.442121982574463 23.669178009033203
Loss :  1.435017704963684 4.494707107543945 23.908554077148438
Loss :  1.400643229484558 4.4339518547058105 23.570402145385742
Loss :  1.461519718170166 4.363809108734131 23.28056526184082
Loss :  1.4359649419784546 4.3459086418151855 23.165508270263672
Loss :  1.5807278156280518 4.386769771575928 23.514577865600586
Loss :  1.6373226642608643 4.449521541595459 23.884931564331055
Loss :  1.435401439666748 4.445055961608887 23.660682678222656
Loss :  1.5158714056015015 4.337148189544678 23.20161247253418
Loss :  1.4538958072662354 4.473206996917725 23.819931030273438
Loss :  1.5782495737075806 4.520228862762451 24.179393768310547
Loss :  1.5404894351959229 4.560638427734375 24.34368133544922
Loss :  1.4757130146026611 4.39679479598999 23.459688186645508
Loss :  1.5242469310760498 4.495049953460693 23.999496459960938
Loss :  1.4762248992919922 4.4253997802734375 23.60322380065918
Loss :  1.5579687356948853 4.450156211853027 23.80875015258789
Loss :  1.3884673118591309 4.658584117889404 24.68138885498047
Loss :  1.4629647731781006 4.439601421356201 23.660972595214844
Loss :  1.4195678234100342 4.652987957000732 24.684507369995117
Loss :  1.5560088157653809 4.424469947814941 23.67835807800293
  batch 60 loss: 1.5560088157653809, 4.424469947814941, 23.67835807800293
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.4040404558181763 4.464816093444824 23.72812271118164
Loss :  1.4462230205535889 4.4919657707214355 23.906051635742188
Loss :  1.48862624168396 4.6454267501831055 24.715761184692383
Loss :  1.374316930770874 4.38886833190918 23.31865882873535
Loss :  1.398891568183899 4.038813591003418 21.592960357666016
Loss :  1.3589518070220947 4.569238662719727 24.20514488220215
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.4235175848007202 4.41859769821167 23.51650619506836
Loss :  1.4475868940353394 4.288314342498779 22.889158248901367
Loss :  1.0565671920776367 4.323543071746826 22.67428207397461
Total LOSS train 23.602001571655272 valid 23.32127285003662
CE LOSS train 1.460306745309096 valid 0.2641417980194092
Contrastive LOSS train 4.428338960500864 valid 1.0808857679367065
Saved best model. Old loss 23.44564437866211 and new best loss 23.32127285003662
EPOCH 4:
Loss :  1.5541476011276245 4.267264366149902 22.890470504760742
Loss :  1.539325475692749 4.547821998596191 24.27843475341797
Loss :  1.5223013162612915 4.4736409187316895 23.890504837036133
Loss :  1.4684865474700928 4.39594030380249 23.44818878173828
Loss :  1.4450761079788208 4.440291881561279 23.646535873413086
Loss :  1.3667972087860107 4.473872661590576 23.736160278320312
Loss :  1.4625673294067383 4.452582359313965 23.725479125976562
Loss :  1.4326766729354858 4.343838691711426 23.15186882019043
Loss :  1.4075809717178345 4.466286659240723 23.739013671875
Loss :  1.4335476160049438 4.156103610992432 22.214065551757812
Loss :  1.3415493965148926 4.417287826538086 23.427988052368164
Loss :  1.3931859731674194 4.6639628410339355 24.713001251220703
Loss :  1.3569650650024414 4.596963405609131 24.341781616210938
Loss :  1.3962175846099854 4.585280418395996 24.322620391845703
Loss :  1.4616421461105347 4.491995334625244 23.92161750793457
Loss :  1.3905587196350098 4.2507243156433105 22.644180297851562
Loss :  1.296526551246643 4.165190696716309 22.122480392456055
Loss :  1.336928129196167 4.208102226257324 22.377439498901367
Loss :  1.254408359527588 4.23729944229126 22.44090461730957
Loss :  1.410710096359253 4.2157511711120605 22.489465713500977
  batch 20 loss: 1.410710096359253, 4.2157511711120605, 22.489465713500977
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.333389401435852 4.006315231323242 21.364965438842773
Loss :  1.261265754699707 4.1450605392456055 21.986568450927734
Loss :  1.3214173316955566 4.292392253875732 22.78337860107422
Loss :  1.3449324369430542 4.211764812469482 22.403757095336914
Loss :  1.4152214527130127 4.290257930755615 22.866512298583984
Loss :  1.3378193378448486 4.012063026428223 21.398134231567383
Loss :  1.350809931755066 4.4836955070495605 23.769287109375
Loss :  1.3884915113449097 4.195993900299072 22.368459701538086
Loss :  1.2632567882537842 4.1508378982543945 22.017444610595703
Loss :  1.4724786281585693 4.1174445152282715 22.059701919555664
Loss :  1.2528570890426636 4.185621738433838 22.180965423583984
Loss :  1.4391241073608398 4.29614782333374 22.919864654541016
Loss :  1.356867790222168 4.13582706451416 22.03600311279297
Loss :  1.400994896888733 4.274205684661865 22.772024154663086
Loss :  1.2849839925765991 4.295114517211914 22.760557174682617
Loss :  1.347129464149475 4.3070220947265625 22.882240295410156
Loss :  1.310660481452942 4.110389232635498 21.862607955932617
Loss :  1.4736250638961792 3.875807523727417 20.852663040161133
Loss :  1.4680378437042236 4.053191661834717 21.733997344970703
Loss :  1.5079615116119385 4.177929878234863 22.39760971069336
  batch 40 loss: 1.5079615116119385, 4.177929878234863, 22.39760971069336
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.3986790180206299 4.172186851501465 22.259614944458008
Loss :  1.3362561464309692 4.272221088409424 22.69736099243164
Loss :  1.30372154712677 4.213006496429443 22.368755340576172
Loss :  1.304094910621643 4.272583484649658 22.667011260986328
Loss :  1.2491050958633423 4.016462326049805 21.331417083740234
Loss :  1.3172451257705688 4.059434413909912 21.614418029785156
Loss :  1.402435302734375 4.132649898529053 22.065685272216797
Loss :  1.2785365581512451 4.203834056854248 22.297706604003906
Loss :  1.4664640426635742 4.20404052734375 22.48666763305664
Loss :  1.2728692293167114 4.229421615600586 22.41997718811035
Loss :  1.4290423393249512 4.489768028259277 23.877883911132812
Loss :  1.425984501838684 4.311987400054932 22.98592185974121
Loss :  1.3572313785552979 4.1843180656433105 22.27882194519043
Loss :  1.4844965934753418 4.3965559005737305 23.46727752685547
Loss :  1.3745896816253662 4.407998085021973 23.414579391479492
Loss :  1.4890203475952148 4.473780632019043 23.857925415039062
Loss :  1.326228141784668 4.469716548919678 23.67481231689453
Loss :  1.3000311851501465 4.199666500091553 22.298364639282227
Loss :  1.3243359327316284 4.180613994598389 22.227405548095703
Loss :  1.4744627475738525 4.350620746612549 23.22756576538086
  batch 60 loss: 1.4744627475738525, 4.350620746612549, 23.22756576538086
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
Loss :  1.2858445644378662 4.391972064971924 23.245704650878906
Loss :  1.352094054222107 4.614518165588379 24.424684524536133
Loss :  1.3368630409240723 4.251945972442627 22.59659194946289
Loss :  1.304659366607666 3.931614637374878 20.962732315063477
Loss :  1.2434521913528442 3.670745849609375 19.59718132019043
Loss :  1.380993127822876 4.409134864807129 23.426666259765625
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 5], device='cuda:0')
Loss :  1.3896048069000244 4.390847682952881 23.343843460083008
Loss :  1.3958425521850586 4.276968955993652 22.780689239501953
Loss :  1.4076007604599 4.197916030883789 22.397180557250977
Total LOSS train 22.727461712176982 valid 22.98709487915039
CE LOSS train 1.3760041035138644 valid 0.351900190114975
Contrastive LOSS train 4.27029149715717 valid 1.0494790077209473
Saved best model. Old loss 23.32127285003662 and new best loss 22.98709487915039
EPOCH 5:
Loss :  1.4052189588546753 3.800978899002075 20.410112380981445
Loss :  1.431221604347229 4.145854473114014 22.160493850708008
Loss :  1.412382960319519 4.525148868560791 24.03812599182129
Loss :  1.4057788848876953 4.400584697723389 23.408702850341797
Loss :  1.478735327720642 4.235602378845215 22.656747817993164
Loss :  1.3412673473358154 4.488461494445801 23.783573150634766
Loss :  1.4715642929077148 4.503865718841553 23.990894317626953
Loss :  1.4269205331802368 4.568466663360596 24.26925277709961
Loss :  1.4467896223068237 4.494754314422607 23.920560836791992
Loss :  1.5627509355545044 4.258049011230469 22.852996826171875
Loss :  1.4767669439315796 4.399230003356934 23.472917556762695
Loss :  1.41865074634552 4.480105876922607 23.819181442260742
Loss :  1.392319917678833 4.523866176605225 24.01165199279785
Loss :  1.4315624237060547 4.629586696624756 24.579496383666992
Loss :  1.5822709798812866 4.433405876159668 23.74930191040039
Loss :  1.5111991167068481 4.608160495758057 24.552001953125
Loss :  1.3678780794143677 4.450057506561279 23.618165969848633
Loss :  1.4610660076141357 4.39755392074585 23.448835372924805
Loss :  1.3771952390670776 4.558694839477539 24.170669555664062
Loss :  1.5203219652175903 4.564604759216309 24.343347549438477
  batch 20 loss: 1.5203219652175903, 4.564604759216309, 24.343347549438477
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.504976511001587 4.550469398498535 24.257322311401367
Loss :  1.464553713798523 4.597189903259277 24.450504302978516
Loss :  1.417318344116211 4.455775737762451 23.696197509765625
Loss :  1.5110828876495361 4.539236068725586 24.207263946533203
Loss :  1.6056857109069824 4.505734443664551 24.134357452392578
Loss :  1.5754438638687134 4.484597206115723 23.998428344726562
Loss :  1.4352927207946777 4.863757133483887 25.754079818725586
Loss :  1.4881898164749146 4.543842792510986 24.2074031829834
Loss :  1.388344645500183 4.537233352661133 24.07451057434082
Loss :  1.5889991521835327 4.4041900634765625 23.609949111938477
Loss :  1.3337538242340088 4.556762218475342 24.117565155029297
Loss :  1.5030279159545898 4.673232555389404 24.869190216064453
Loss :  1.4639499187469482 4.3814849853515625 23.371374130249023
Loss :  1.4882419109344482 4.402206897735596 23.49927520751953
Loss :  1.339053750038147 4.517347812652588 23.925792694091797
Loss :  1.4058525562286377 4.390976905822754 23.360736846923828
Loss :  1.4285026788711548 4.4498162269592285 23.677583694458008
Loss :  1.540298342704773 4.439177989959717 23.736188888549805
Loss :  1.5373437404632568 4.366575717926025 23.370222091674805
Loss :  1.5027680397033691 4.687695026397705 24.941242218017578
  batch 40 loss: 1.5027680397033691, 4.687695026397705, 24.941242218017578
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.5291281938552856 4.2987751960754395 23.02300453186035
Loss :  1.4741733074188232 4.620612144470215 24.577234268188477
Loss :  1.4313955307006836 4.333340644836426 23.098098754882812
Loss :  1.433576226234436 4.474324703216553 23.805200576782227
Loss :  1.4454877376556396 4.330734729766846 23.09916114807129
Loss :  1.4075175523757935 4.525147914886475 24.03325843811035
Loss :  1.6210434436798096 4.515422344207764 24.198156356811523
Loss :  1.4198147058486938 4.570535182952881 24.272491455078125
Loss :  1.5886180400848389 4.492857456207275 24.05290412902832
Loss :  1.422630786895752 4.422690391540527 23.536083221435547
Loss :  1.4837148189544678 4.574253559112549 24.354982376098633
Loss :  1.4569134712219238 4.427237510681152 23.593101501464844
Loss :  1.4565942287445068 4.304762840270996 22.980409622192383
Loss :  1.5128124952316284 4.3504252433776855 23.264938354492188
Loss :  1.3762660026550293 4.530969619750977 24.03111457824707
Loss :  1.6496531963348389 4.431319236755371 23.806249618530273
Loss :  1.4444794654846191 4.566848278045654 24.27872085571289
Loss :  1.4026050567626953 4.522512912750244 24.015169143676758
Loss :  1.3821262121200562 4.624168395996094 24.502967834472656
Loss :  1.539109230041504 4.442997932434082 23.75409698486328
  batch 60 loss: 1.539109230041504, 4.442997932434082, 23.75409698486328
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  1.4011635780334473 4.511816501617432 23.96024513244629
Loss :  1.3749332427978516 4.417129993438721 23.460582733154297
Loss :  1.344632863998413 4.414484977722168 23.41705894470215
Loss :  1.3834198713302612 4.383630752563477 23.301572799682617
Loss :  1.3135532140731812 4.162355422973633 22.125329971313477
Loss :  1.5607737302780151 4.345705986022949 23.289304733276367
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.627593755722046 4.4289655685424805 23.772422790527344
Loss :  1.6173121929168701 4.325597763061523 23.24530029296875
Loss :  1.539474606513977 4.298765182495117 23.033300399780273
Total LOSS train 23.77012848487267 valid 23.335082054138184
CE LOSS train 1.4575369908259466 valid 0.38486865162849426
Contrastive LOSS train 4.462518292206984 valid 1.0746912956237793
EPOCH 6:
Loss :  1.4952197074890137 4.47099494934082 23.850194931030273
Loss :  1.4738590717315674 4.547476768493652 24.211244583129883
Loss :  1.430610179901123 4.373828411102295 23.299753189086914
Loss :  1.4156570434570312 4.461785793304443 23.724586486816406
Loss :  1.5614745616912842 4.3943891525268555 23.53342056274414
Loss :  1.370166301727295 4.266715049743652 22.70374298095703
Loss :  1.4847015142440796 4.565675735473633 24.313079833984375
Loss :  1.38823401927948 3.7282445430755615 20.029455184936523
Loss :  1.3561506271362305 3.8852384090423584 20.78234100341797
Loss :  1.4547430276870728 4.2602314949035645 22.75589942932129
Loss :  1.351216197013855 4.427469730377197 23.488563537597656
Loss :  1.348593831062317 4.400413513183594 23.350662231445312
Loss :  1.3333418369293213 4.190063953399658 22.283660888671875
Loss :  1.3795490264892578 4.330144882202148 23.0302734375
Loss :  1.4990757703781128 4.127345561981201 22.13580322265625
Loss :  1.538711667060852 4.5486650466918945 24.28203582763672
Loss :  1.3472025394439697 3.8264076709747314 20.47924041748047
Loss :  1.4135934114456177 3.841155529022217 20.61937141418457
Loss :  1.3479971885681152 3.8395540714263916 20.54576873779297
Loss :  1.5365992784500122 4.521948337554932 24.14634132385254
  batch 20 loss: 1.5365992784500122, 4.521948337554932, 24.14634132385254
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.4567556381225586 4.3333353996276855 23.123432159423828
Loss :  1.343978762626648 4.633490562438965 24.511432647705078
Loss :  1.4006859064102173 4.49884557723999 23.894914627075195
Loss :  1.4816943407058716 4.56377649307251 24.30057716369629
Loss :  1.5094945430755615 4.633581638336182 24.67740249633789
Loss :  1.353318691253662 4.42111873626709 23.458913803100586
Loss :  1.3132116794586182 4.727120876312256 24.948816299438477
Loss :  1.3624619245529175 4.352721691131592 23.126070022583008
Loss :  1.1992939710617065 4.457692623138428 23.487756729125977
Loss :  1.4640190601348877 4.400184154510498 23.464941024780273
Loss :  1.2012487649917603 4.607004642486572 24.236270904541016
Loss :  1.4203611612319946 4.648741245269775 24.664066314697266
Loss :  1.272386908531189 4.456852436065674 23.55664825439453
Loss :  1.3245607614517212 4.434441566467285 23.496767044067383
Loss :  1.1564399003982544 4.511496067047119 23.71392059326172
Loss :  1.220522403717041 4.501280784606934 23.726926803588867
Loss :  1.19807767868042 4.358647346496582 22.991313934326172
Loss :  1.4436601400375366 4.207573413848877 22.48152732849121
Loss :  1.3915071487426758 3.8039839267730713 20.411426544189453
Loss :  1.4041038751602173 4.103142261505127 21.919815063476562
  batch 40 loss: 1.4041038751602173, 4.103142261505127, 21.919815063476562
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.268559217453003 4.332671642303467 22.931917190551758
Loss :  1.1699399948120117 3.981431722640991 21.077098846435547
Loss :  1.218011736869812 4.143671989440918 21.936372756958008
Loss :  1.2933504581451416 4.203416347503662 22.31043243408203
Loss :  1.1930636167526245 3.959322929382324 20.98967933654785
Loss :  1.2900246381759644 4.273855209350586 22.659299850463867
Loss :  1.461133599281311 4.3555707931518555 23.238988876342773
Loss :  1.233533263206482 4.633797645568848 24.40252113342285
Loss :  1.4534039497375488 4.374337196350098 23.325088500976562
Loss :  1.2371875047683716 4.666433811187744 24.56935691833496
Loss :  1.3804558515548706 4.539082050323486 24.075864791870117
Loss :  1.368060827255249 4.384544849395752 23.29078483581543
Loss :  1.3000420331954956 3.7874298095703125 20.23719024658203
Loss :  1.4280842542648315 3.6941068172454834 19.898616790771484
Loss :  1.1985479593276978 3.970473051071167 21.050912857055664
Loss :  1.4861990213394165 3.5867502689361572 19.419950485229492
Loss :  1.2509217262268066 3.611468553543091 19.308263778686523
Loss :  1.1788623332977295 3.8841254711151123 20.599489212036133
Loss :  1.2572259902954102 4.110138416290283 21.807918548583984
Loss :  1.5339754819869995 4.002416133880615 21.546056747436523
  batch 60 loss: 1.5339754819869995, 4.002416133880615, 21.546056747436523
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.1992650032043457 3.6787843704223633 19.593185424804688
Loss :  1.3319538831710815 3.6744792461395264 19.704349517822266
Loss :  1.2586395740509033 3.7165896892547607 19.841588973999023
Loss :  1.1570818424224854 3.8860318660736084 20.58724021911621
Loss :  1.0729750394821167 3.490675687789917 18.52635383605957
Loss :  1.623814344406128 4.43629264831543 23.80527687072754
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2], device='cuda:0')
Loss :  1.7177858352661133 4.4535393714904785 23.98548126220703
Loss :  1.6978307962417603 4.367396831512451 23.534814834594727
Loss :  1.7063912153244019 4.385940074920654 23.636091232299805
Total LOSS train 22.472106170654296 valid 23.740416049957275
CE LOSS train 1.3486919824893657 valid 0.42659780383110046
Contrastive LOSS train 4.224682855606079 valid 1.0964850187301636
EPOCH 7:
Loss :  1.3706490993499756 3.488316535949707 18.812231063842773
Loss :  1.3912385702133179 3.7334773540496826 20.058626174926758
Loss :  1.281854510307312 4.359381675720215 23.078763961791992
Loss :  1.3045954704284668 3.7202720642089844 19.905956268310547
Loss :  1.3574769496917725 3.5269672870635986 18.992313385009766
Loss :  1.228222370147705 3.512054443359375 18.788494110107422
Loss :  1.355397343635559 3.7680115699768066 20.19545555114746
Loss :  1.244170904159546 3.4564692974090576 18.526517868041992
Loss :  1.2011586427688599 4.081291675567627 21.607616424560547
Loss :  1.3149110078811646 3.544835329055786 19.039087295532227
Loss :  1.1920462846755981 3.755319833755493 19.968647003173828
Loss :  1.1845120191574097 4.212721347808838 22.248117446899414
Loss :  1.162972092628479 3.8432135581970215 20.379039764404297
Loss :  1.1821413040161133 4.057227611541748 21.468280792236328
Loss :  1.4267159700393677 3.8548927307128906 20.70117950439453
Loss :  1.3939746618270874 3.6369104385375977 19.57852554321289
Loss :  1.1565682888031006 3.56862735748291 18.999704360961914
Loss :  1.300000786781311 3.58406662940979 19.220335006713867
Loss :  1.1405532360076904 3.768709897994995 19.984102249145508
Loss :  1.394505262374878 3.8767483234405518 20.77824592590332
  batch 20 loss: 1.394505262374878, 3.8767483234405518, 20.77824592590332
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  1.2380375862121582 3.9209771156311035 20.84292221069336
Loss :  1.1433881521224976 3.9577910900115967 20.932344436645508
Loss :  1.2251110076904297 3.7530007362365723 19.990114212036133
Loss :  1.3252367973327637 3.9945080280303955 21.29777717590332
Loss :  1.4447137117385864 4.157433986663818 22.231884002685547
Loss :  1.2467186450958252 4.188149929046631 22.187469482421875
Loss :  1.291206955909729 3.8598132133483887 20.590272903442383
Loss :  1.2074187994003296 4.221848964691162 22.31666374206543
Loss :  1.0530881881713867 3.667128562927246 19.38873291015625
Loss :  1.3663007020950317 3.9977004528045654 21.35480308532715
Loss :  1.0573413372039795 4.005062103271484 21.082651138305664
Loss :  1.309574842453003 3.892775058746338 20.773448944091797
Loss :  1.194449782371521 3.9992165565490723 21.190532684326172
Loss :  1.195936918258667 4.1877546310424805 22.13471031188965
Loss :  1.0647094249725342 3.872215509414673 20.4257869720459
Loss :  1.132116675376892 3.7591428756713867 19.927831649780273
Loss :  1.1107189655303955 3.6639132499694824 19.430286407470703
Loss :  1.3506619930267334 3.283783435821533 17.76957893371582
Loss :  1.369231939315796 3.359117031097412 18.164817810058594
Loss :  1.4121342897415161 4.193725109100342 22.380760192871094
  batch 40 loss: 1.4121342897415161, 4.193725109100342, 22.380760192871094
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
Loss :  1.2533270120620728 4.01494836807251 21.328067779541016
Loss :  1.1978883743286133 3.4069013595581055 18.23239517211914
Loss :  1.1428766250610352 3.6024060249328613 19.1549072265625
Loss :  1.1957218647003174 3.561326265335083 19.00235366821289
Loss :  1.1166284084320068 3.480724334716797 18.52025032043457
Loss :  1.2321730852127075 3.6040985584259033 19.252666473388672
Loss :  1.367243766784668 3.8964929580688477 20.849708557128906
Loss :  1.1609001159667969 4.319743633270264 22.759618759155273
Loss :  1.4405103921890259 3.808605194091797 20.483535766601562
Loss :  1.1976020336151123 3.5682904720306396 19.039052963256836
Loss :  1.3176231384277344 3.4887733459472656 18.761489868164062
Loss :  1.318544864654541 3.650413990020752 19.570613861083984
Loss :  1.2466964721679688 3.558614492416382 19.03976821899414
Loss :  1.378465175628662 3.5293188095092773 19.025060653686523
Loss :  1.1622384786605835 3.4320175647735596 18.32232666015625
Loss :  1.4511196613311768 3.158973455429077 17.245986938476562
Loss :  1.210884690284729 3.332545757293701 17.873613357543945
Loss :  1.1353943347930908 3.6864287853240967 19.56753921508789
Loss :  1.2056645154953003 3.6561086177825928 19.4862060546875
Loss :  1.4741177558898926 3.283510684967041 17.89167022705078
  batch 60 loss: 1.4741177558898926, 3.283510684967041, 17.89167022705078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.1428849697113037 3.2987914085388184 17.636842727661133
Loss :  1.275923728942871 3.328089952468872 17.91637420654297
Loss :  1.2031786441802979 3.6402947902679443 19.404651641845703
Loss :  1.1343940496444702 3.3641130924224854 17.954959869384766
Loss :  1.0462626218795776 3.0481815338134766 16.28717041015625
Loss :  10.877543449401855 4.486166000366211 33.308372497558594
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
Loss :  11.295788764953613 4.443937301635742 33.51547622680664
Loss :  11.298598289489746 4.398714065551758 33.29216766357422
Loss :  8.89741039276123 4.241690158843994 30.10586166381836
Total LOSS train 19.866945530818057 valid 32.55546951293945
CE LOSS train 1.2512311733686008 valid 2.2243525981903076
Contrastive LOSS train 3.7231428623199463 valid 1.0604225397109985
EPOCH 8:
Loss :  1.3311290740966797 4.116025924682617 21.911258697509766
Loss :  1.3827811479568481 4.669814109802246 24.731853485107422
Loss :  1.2732505798339844 3.6225836277008057 19.38616943359375
Loss :  1.3154700994491577 3.950990915298462 21.070425033569336
Loss :  1.3882627487182617 4.271598815917969 22.746257781982422
Loss :  1.2157739400863647 4.324066162109375 22.836105346679688
Loss :  1.386229157447815 4.304868698120117 22.910572052001953
Loss :  1.2544082403182983 3.7163827419281006 19.836320877075195
Loss :  1.2338184118270874 4.112920761108398 21.79842185974121
Loss :  1.3696597814559937 4.253815174102783 22.638734817504883
Loss :  1.2224448919296265 4.506587505340576 23.755382537841797
Loss :  1.227295160293579 4.521570682525635 23.835147857666016
Loss :  1.1831910610198975 4.377598285675049 23.071182250976562
Loss :  1.2504888772964478 4.5478386878967285 23.989681243896484
Loss :  1.509263277053833 4.472592830657959 23.872228622436523
Loss :  1.4735517501831055 4.5147786140441895 24.047443389892578
Loss :  1.2047935724258423 4.429469108581543 23.352140426635742
Loss :  1.2869282960891724 4.278609752655029 22.679977416992188
Loss :  1.1791999340057373 4.458092212677002 23.46965980529785
Loss :  1.4352227449417114 4.5920000076293945 24.395221710205078
  batch 20 loss: 1.4352227449417114, 4.5920000076293945, 24.395221710205078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.2834502458572388 4.381072044372559 23.188812255859375
Loss :  1.237483024597168 4.8214311599731445 25.34463882446289
Loss :  1.2784631252288818 4.5227274894714355 23.892101287841797
Loss :  1.2983248233795166 4.364901065826416 23.12282943725586
Loss :  1.4260987043380737 4.494191646575928 23.897056579589844
Loss :  1.3445173501968384 4.574037075042725 24.214702606201172
Loss :  1.2894184589385986 4.533452987670898 23.956684112548828
Loss :  1.3172519207000732 4.382786273956299 23.231182098388672
Loss :  1.094283103942871 4.6872076988220215 24.530323028564453
Loss :  1.4497554302215576 4.419593334197998 23.54772186279297
Loss :  1.1094242334365845 4.60882568359375 24.153553009033203
Loss :  1.3880537748336792 4.781538963317871 25.29574966430664
Loss :  1.2435433864593506 4.398592948913574 23.236509323120117
Loss :  1.2572455406188965 4.554429531097412 24.029394149780273
Loss :  1.1753654479980469 4.465268135070801 23.501705169677734
Loss :  1.2223762273788452 4.484964370727539 23.647197723388672
Loss :  1.1674474477767944 4.464596271514893 23.490428924560547
Loss :  1.4224458932876587 4.37868070602417 23.31584930419922
Loss :  1.3726030588150024 4.588942527770996 24.31731605529785
Loss :  1.3629858493804932 4.5707316398620605 24.216644287109375
  batch 40 loss: 1.3629858493804932, 4.5707316398620605, 24.216644287109375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.2656484842300415 4.602205276489258 24.276674270629883
Loss :  1.238845705986023 4.515723705291748 23.81746482849121
Loss :  1.212713599205017 4.406132698059082 23.243375778198242
Loss :  1.2448476552963257 4.569158554077148 24.090641021728516
Loss :  1.1535283327102661 4.3881120681762695 23.094087600708008
Loss :  1.224874496459961 4.539884090423584 23.92429542541504
Loss :  1.3484044075012207 4.450753211975098 23.602169036865234
Loss :  1.2065324783325195 4.545595645904541 23.93450927734375
Loss :  1.4472795724868774 4.315045356750488 23.022504806518555
Loss :  1.2086881399154663 4.398233890533447 23.199857711791992
Loss :  1.3708969354629517 4.535942554473877 24.050609588623047
Loss :  1.2878509759902954 4.45244026184082 23.550052642822266
Loss :  1.2420026063919067 4.389072895050049 23.187366485595703
Loss :  1.3953807353973389 4.677829265594482 24.784526824951172
Loss :  1.2386164665222168 4.421743869781494 23.347335815429688
Loss :  1.457773208618164 4.3859734535217285 23.38763999938965
Loss :  1.204959750175476 4.648116111755371 24.445541381835938
Loss :  1.1492195129394531 4.491456508636475 23.606502532958984
Loss :  1.2205053567886353 4.562774658203125 24.034378051757812
Loss :  1.5066657066345215 4.526896953582764 24.141151428222656
  batch 60 loss: 1.5066657066345215, 4.526896953582764, 24.141151428222656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.113492488861084 4.5190229415893555 23.708608627319336
Loss :  1.2194918394088745 4.482482433319092 23.63190460205078
Loss :  1.2002630233764648 4.5159010887146 23.779769897460938
Loss :  1.1621379852294922 4.409245014190674 23.208362579345703
Loss :  1.0294134616851807 4.088048458099365 21.469655990600586
Loss :  2.9272594451904297 4.415098190307617 25.002750396728516
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  3.099163293838501 4.446256637573242 25.330446243286133
Loss :  3.0392775535583496 4.324330806732178 24.660932540893555
Loss :  2.909547805786133 4.357337474822998 24.69623565673828
Total LOSS train 23.461593393179086 valid 24.92259120941162
CE LOSS train 1.280211272606483 valid 0.7273869514465332
Contrastive LOSS train 4.43627641751216 valid 1.0893343687057495
EPOCH 9:
Loss :  1.3550739288330078 4.330766677856445 23.008907318115234
Loss :  1.418126106262207 4.588924884796143 24.362751007080078
Loss :  1.270570993423462 4.39983606338501 23.269750595092773
Loss :  1.2865936756134033 4.588522434234619 24.229206085205078
Loss :  1.3516340255737305 4.482159614562988 23.762432098388672
Loss :  1.1883726119995117 4.3465256690979 22.921001434326172
Loss :  1.386492371559143 4.674362659454346 24.758304595947266
Loss :  1.2849010229110718 4.396775722503662 23.268779754638672
Loss :  1.2738316059112549 4.5086798667907715 23.817232131958008
Loss :  1.389912724494934 4.396103858947754 23.370431900024414
Loss :  1.2090977430343628 4.494929313659668 23.683744430541992
Loss :  1.2033766508102417 4.4928131103515625 23.667442321777344
Loss :  1.1331298351287842 4.502762317657471 23.646940231323242
Loss :  1.1855337619781494 4.5525641441345215 23.948354721069336
Loss :  1.500009536743164 4.370275497436523 23.35138702392578
Loss :  1.3751267194747925 4.568195819854736 24.21610450744629
Loss :  1.1250656843185425 4.508660316467285 23.668365478515625
Loss :  1.2739732265472412 4.35546875 23.05131721496582
Loss :  1.1702029705047607 4.359353065490723 22.96696662902832
Loss :  1.411329984664917 4.304766654968262 22.935163497924805
  batch 20 loss: 1.411329984664917, 4.304766654968262, 22.935163497924805
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.2777225971221924 4.367959499359131 23.117521286010742
Loss :  1.16985023021698 4.353058338165283 22.93514060974121
Loss :  1.2599126100540161 4.5172858238220215 23.846342086791992
Loss :  1.298426866531372 4.555577278137207 24.076313018798828
Loss :  1.3879371881484985 4.601361274719238 24.394742965698242
Loss :  1.2209253311157227 4.33712100982666 22.90652847290039
Loss :  1.14866042137146 4.760031700134277 24.948820114135742
Loss :  1.248970627784729 4.377758979797363 23.13776397705078
Loss :  1.0020908117294312 4.376536846160889 22.884775161743164
Loss :  1.3943525552749634 4.364139080047607 23.21504783630371
Loss :  1.0258228778839111 4.81131649017334 25.082406997680664
Loss :  1.3362001180648804 4.726712226867676 24.96976089477539
Loss :  1.2216435670852661 4.39035701751709 23.173429489135742
Loss :  1.2398606538772583 4.4849748611450195 23.66473388671875
Loss :  1.0944666862487793 4.629860877990723 24.243770599365234
Loss :  1.1229526996612549 4.470570087432861 23.47580337524414
Loss :  1.1318588256835938 4.4789204597473145 23.526460647583008
Loss :  1.3659212589263916 4.370059490203857 23.216218948364258
Loss :  1.3997602462768555 4.371236324310303 23.255943298339844
Loss :  1.4366251230239868 4.347259998321533 23.172924041748047
  batch 40 loss: 1.4366251230239868, 4.347259998321533, 23.172924041748047
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.255041480064392 4.386542797088623 23.187755584716797
Loss :  1.1639823913574219 4.5448198318481445 23.888080596923828
Loss :  1.1844451427459717 4.447071552276611 23.419801712036133
Loss :  1.2074387073516846 4.455177307128906 23.483325958251953
Loss :  1.1180959939956665 4.313169479370117 22.683942794799805
Loss :  1.1956851482391357 4.60453987121582 24.2183837890625
Loss :  1.3189952373504639 4.561184406280518 24.124916076660156
Loss :  1.1841185092926025 4.433070182800293 23.349470138549805
Loss :  1.4398751258850098 4.393551349639893 23.407630920410156
Loss :  1.219861388206482 4.4358344078063965 23.39903450012207
Loss :  1.3811969757080078 4.527669429779053 24.01954460144043
Loss :  1.325681447982788 4.368354320526123 23.16745376586914
Loss :  1.2319440841674805 4.376468181610107 23.11428451538086
Loss :  1.3363196849822998 4.3635125160217285 23.153881072998047
Loss :  1.2183226346969604 4.429494380950928 23.365795135498047
Loss :  1.4239658117294312 4.495438575744629 23.90115737915039
Loss :  1.2288175821304321 4.56658935546875 24.061763763427734
Loss :  1.1544734239578247 4.436241149902344 23.33568000793457
Loss :  1.185610055923462 4.491383075714111 23.64252471923828
Loss :  1.5044852495193481 4.463698387145996 23.822978973388672
  batch 60 loss: 1.5044852495193481, 4.463698387145996, 23.822978973388672
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.1326130628585815 4.620534420013428 24.23528480529785
Loss :  1.2860196828842163 4.436273097991943 23.46738624572754
Loss :  1.1715120077133179 4.354269027709961 22.94285774230957
Loss :  1.1353691816329956 4.378532886505127 23.028032302856445
Loss :  1.0694875717163086 3.9692981243133545 20.915977478027344
Loss :  1.185528039932251 4.32391881942749 22.80512237548828
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.2455464601516724 4.192934036254883 22.210216522216797
Loss :  1.2303643226623535 4.320218563079834 22.831457138061523
Loss :  1.253015160560608 4.4320387840271 23.413209915161133
Total LOSS train 23.545938081007737 valid 22.815001487731934
CE LOSS train 1.2561488316609308 valid 0.313253790140152
Contrastive LOSS train 4.45795788031358 valid 1.108009696006775
Saved best model. Old loss 22.98709487915039 and new best loss 22.815001487731934
EPOCH 10:
Loss :  1.2765276432037354 4.33388090133667 22.945932388305664
Loss :  1.3329710960388184 4.5486040115356445 24.075990676879883
Loss :  1.2789738178253174 4.313046932220459 22.844209671020508
Loss :  1.233142614364624 4.406938552856445 23.26783561706543
Loss :  1.3750581741333008 4.320643424987793 22.978275299072266
Loss :  1.119387149810791 4.379527568817139 23.017024993896484
Loss :  1.3154162168502808 4.482388973236084 23.72736167907715
Loss :  1.213658094406128 4.288721084594727 22.657262802124023
Loss :  1.2205770015716553 4.416566848754883 23.30341148376465
Loss :  1.3793081045150757 4.405969142913818 23.409154891967773
Loss :  1.178782343864441 4.475268840789795 23.555126190185547
Loss :  1.0844861268997192 4.490832328796387 23.53864860534668
Loss :  1.088037133216858 4.449585437774658 23.33596420288086
Loss :  1.156258463859558 4.488448143005371 23.598499298095703
Loss :  1.4063185453414917 4.395339488983154 23.38301658630371
Loss :  1.3889880180358887 4.605432987213135 24.416152954101562
Loss :  1.1148552894592285 4.362793445587158 22.928821563720703
Loss :  1.2048207521438599 4.394861698150635 23.179128646850586
Loss :  1.1049351692199707 4.419340133666992 23.201635360717773
Loss :  1.370549201965332 4.505341529846191 23.897254943847656
  batch 20 loss: 1.370549201965332, 4.505341529846191, 23.897254943847656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.2556874752044678 4.272381782531738 22.617595672607422
Loss :  1.1194100379943848 4.4652628898620605 23.445724487304688
Loss :  1.2205368280410767 4.455382347106934 23.49744987487793
Loss :  1.2206143140792847 4.455722332000732 23.499225616455078
Loss :  1.4181379079818726 4.717927932739258 25.00777816772461
Loss :  1.2145373821258545 4.534022808074951 23.88465118408203
Loss :  1.2326539754867554 4.729469299316406 24.880001068115234
Loss :  1.2049367427825928 4.424957752227783 23.32972526550293
Loss :  0.9993028044700623 4.3541364669799805 22.76998519897461
Loss :  1.341117262840271 4.439287185668945 23.537553787231445
Loss :  1.0192725658416748 4.5342020988464355 23.690282821655273
Loss :  1.260892391204834 4.715153217315674 24.836658477783203
Loss :  1.1411681175231934 4.577598571777344 24.02916145324707
Loss :  1.1683557033538818 4.4730963706970215 23.533838272094727
Loss :  1.0444718599319458 4.679999351501465 24.444469451904297
Loss :  1.0870440006256104 4.650820732116699 24.341148376464844
Loss :  1.1192467212677002 4.49245548248291 23.581523895263672
Loss :  1.4033349752426147 4.687499523162842 24.84083366394043
Loss :  1.3249554634094238 4.291067123413086 22.780290603637695
Loss :  1.4098643064498901 4.635504722595215 24.58738899230957
  batch 40 loss: 1.4098643064498901, 4.635504722595215, 24.58738899230957
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.2749234437942505 4.499854564666748 23.77419662475586
Loss :  1.1983119249343872 4.4309234619140625 23.352930068969727
Loss :  1.1568876504898071 4.359549045562744 22.954631805419922
Loss :  1.1809765100479126 4.36388635635376 23.000408172607422
Loss :  1.1242533922195435 4.319862365722656 22.72356605529785
Loss :  1.2377731800079346 4.402742385864258 23.25148582458496
Loss :  1.3243587017059326 4.452998161315918 23.5893497467041
Loss :  1.1351264715194702 4.288651943206787 22.578386306762695
Loss :  1.4130877256393433 4.042577743530273 21.6259765625
Loss :  1.1416950225830078 4.119365215301514 21.738521575927734
Loss :  1.285717487335205 4.253927230834961 22.55535316467285
Loss :  1.2690074443817139 4.221095085144043 22.374483108520508
Loss :  1.183709740638733 4.111083984375 21.7391300201416
Loss :  1.279815912246704 4.037454128265381 21.467086791992188
Loss :  1.1337391138076782 4.112742900848389 21.69745445251465
Loss :  1.368151307106018 3.9634933471679688 21.185617446899414
Loss :  1.1372203826904297 3.8596928119659424 20.435684204101562
Loss :  1.103397011756897 4.281655311584473 22.511672973632812
Loss :  1.153764247894287 4.210373878479004 22.20563316345215
Loss :  1.4040955305099487 3.9749884605407715 21.279037475585938
  batch 60 loss: 1.4040955305099487, 3.9749884605407715, 21.279037475585938
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.0841625928878784 3.6842563152313232 19.505443572998047
Loss :  1.2302745580673218 3.6628987789154053 19.544767379760742
Loss :  1.1576308012008667 3.5782923698425293 19.04909324645996
Loss :  1.0870556831359863 3.6951496601104736 19.562803268432617
Loss :  0.997464120388031 3.3927056789398193 17.96099090576172
Loss :  1.256407380104065 4.419247627258301 23.352643966674805
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.2890260219573975 4.426690578460693 23.4224796295166
Loss :  1.2802586555480957 4.32895040512085 22.925010681152344
Loss :  1.2998528480529785 4.253785133361816 22.568777084350586
Total LOSS train 22.8624568939209 valid 23.067227840423584
CE LOSS train 1.2170952576857346 valid 0.32496321201324463
Contrastive LOSS train 4.329072317710289 valid 1.063446283340454
EPOCH 11:
Loss :  1.2772289514541626 4.001221179962158 21.283334732055664
Loss :  1.3142410516738892 3.7160942554473877 19.894712448120117
Loss :  1.1971242427825928 3.3204641342163086 17.79944610595703
Loss :  1.2452789545059204 3.692779064178467 19.70917510986328
Loss :  1.3290296792984009 3.8296523094177246 20.477291107177734
Loss :  1.203222393989563 3.8154566287994385 20.280506134033203
Loss :  1.3323365449905396 4.042397499084473 21.544322967529297
Loss :  1.2368654012680054 3.914170265197754 20.807716369628906
Loss :  1.2394299507141113 3.8625426292419434 20.552143096923828
Loss :  1.2885862588882446 3.9507064819335938 21.042118072509766
Loss :  1.1458320617675781 4.147243976593018 21.882051467895508
Loss :  1.1719080209732056 4.558316707611084 23.963491439819336
Loss :  1.1309138536453247 4.03050422668457 21.283435821533203
Loss :  1.190224528312683 4.072484970092773 21.552648544311523
Loss :  1.446578025817871 3.960090160369873 21.247028350830078
Loss :  1.3973920345306396 4.381052494049072 23.302654266357422
Loss :  1.1801271438598633 4.3774309158325195 23.067279815673828
Loss :  1.298915982246399 4.3716206550598145 23.157018661499023
Loss :  1.1046080589294434 4.072246551513672 21.46584129333496
Loss :  1.3943455219268799 4.524187088012695 24.015281677246094
  batch 20 loss: 1.3943455219268799, 4.524187088012695, 24.015281677246094
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.2877004146575928 4.0547003746032715 21.561203002929688
Loss :  1.106056809425354 3.9124667644500732 20.66839027404785
Loss :  1.2177790403366089 3.93356990814209 20.885629653930664
Loss :  1.2648178339004517 3.925335645675659 20.891496658325195
Loss :  1.3690004348754883 4.172469615936279 22.23134994506836
Loss :  1.2392927408218384 4.232738494873047 22.402984619140625
Loss :  1.3226604461669922 4.12546443939209 21.949983596801758
Loss :  1.2415803670883179 3.786670207977295 20.1749324798584
Loss :  1.045839548110962 3.9346721172332764 20.719200134277344
Loss :  1.4529364109039307 4.279143810272217 22.848655700683594
Loss :  1.07254159450531 4.3161091804504395 22.653087615966797
Loss :  1.3405946493148804 4.179084777832031 22.236019134521484
Loss :  1.2383346557617188 4.21172571182251 22.29696273803711
Loss :  1.261626124382019 4.084932804107666 21.686288833618164
Loss :  1.0762827396392822 3.9538536071777344 20.845550537109375
Loss :  1.1575852632522583 4.236982345581055 22.342496871948242
Loss :  1.1072368621826172 4.068098068237305 21.44772720336914
Loss :  1.3756881952285767 3.993309736251831 21.34223747253418
Loss :  1.4346847534179688 4.144420623779297 22.156787872314453
Loss :  1.4244787693023682 4.03590726852417 21.604015350341797
  batch 40 loss: 1.4244787693023682, 4.03590726852417, 21.604015350341797
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.2327814102172852 4.27722692489624 22.618915557861328
Loss :  1.186841368675232 4.523270130157471 23.803192138671875
Loss :  1.1615248918533325 4.403485298156738 23.178951263427734
Loss :  1.2133506536483765 4.300564765930176 22.71617317199707
Loss :  1.155023455619812 4.300289630889893 22.656471252441406
Loss :  1.2199366092681885 4.461392879486084 23.526901245117188
Loss :  1.4240341186523438 4.259905815124512 22.72356414794922
Loss :  1.2480725049972534 4.486898899078369 23.682565689086914
Loss :  1.4661383628845215 4.502777099609375 23.980024337768555
Loss :  1.2254970073699951 4.4113335609436035 23.282163619995117
Loss :  1.3545562028884888 4.623693943023682 24.473026275634766
Loss :  1.3565396070480347 4.318262577056885 22.947851181030273
Loss :  1.2501840591430664 4.393708229064941 23.21872329711914
Loss :  1.4111250638961792 4.38266658782959 23.324459075927734
Loss :  1.2671200037002563 4.478835582733154 23.661298751831055
Loss :  1.4251614809036255 4.509051322937012 23.97041893005371
Loss :  1.2694655656814575 4.463290214538574 23.585918426513672
Loss :  1.2201429605484009 4.515347003936768 23.796876907348633
Loss :  1.3077397346496582 4.597543239593506 24.295455932617188
Loss :  1.516465187072754 4.370924472808838 23.37108612060547
  batch 60 loss: 1.516465187072754, 4.370924472808838, 23.37108612060547
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.1850911378860474 4.500576972961426 23.68797492980957
Loss :  1.3631587028503418 4.503418445587158 23.880250930786133
Loss :  1.244064211845398 4.393259525299072 23.21036148071289
Loss :  1.2121918201446533 4.517940044403076 23.80189323425293
Loss :  1.1050434112548828 4.190802097320557 22.059053421020508
Loss :  1.6082813739776611 4.339060306549072 23.3035831451416
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4, 5], device='cuda:0')
Loss :  1.3178091049194336 4.477496147155762 23.705291748046875
Loss :  1.4219970703125 4.289409160614014 22.869043350219727
Loss :  1.3571802377700806 4.209752559661865 22.405942916870117
Total LOSS train 22.257324130718526 valid 23.07096529006958
CE LOSS train 1.2643408591930683 valid 0.33929505944252014
Contrastive LOSS train 4.198596660907452 valid 1.0524381399154663
EPOCH 12:
Loss :  1.3623287677764893 4.287445068359375 22.7995548248291
Loss :  1.3978207111358643 4.5279035568237305 24.03734016418457
Loss :  1.2549299001693726 4.406216621398926 23.286012649536133
Loss :  1.317821979522705 4.4269537925720215 23.452590942382812
Loss :  1.4391294717788696 4.321801662445068 23.048137664794922
Loss :  1.2328178882598877 4.399641513824463 23.23102569580078
Loss :  1.4528708457946777 4.499812602996826 23.951934814453125
Loss :  1.2821317911148071 4.322814464569092 22.896203994750977
Loss :  1.2671489715576172 4.38679313659668 23.201114654541016
Loss :  1.3832290172576904 4.330709457397461 23.036775588989258
Loss :  1.2423774003982544 4.532985210418701 23.907304763793945
Loss :  1.2127211093902588 4.469457149505615 23.560007095336914
Loss :  1.1893270015716553 4.438498497009277 23.381820678710938
Loss :  1.265956163406372 4.448442459106445 23.508169174194336
Loss :  1.4394010305404663 4.41070032119751 23.492902755737305
Loss :  1.3773870468139648 4.495466232299805 23.854717254638672
Loss :  1.1732683181762695 4.755048751831055 24.94851303100586
Loss :  1.3036658763885498 4.312985420227051 22.86859130859375
Loss :  1.1758239269256592 4.546872615814209 23.910186767578125
Loss :  1.4274063110351562 4.46512508392334 23.753032684326172
  batch 20 loss: 1.4274063110351562, 4.46512508392334, 23.753032684326172
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.2697583436965942 4.479898452758789 23.66925048828125
Loss :  1.2011609077453613 4.4386138916015625 23.394229888916016
Loss :  1.26487135887146 4.5555500984191895 24.042621612548828
Loss :  1.2641416788101196 4.5186543464660645 23.857412338256836
Loss :  1.385185718536377 4.667178630828857 24.721078872680664
Loss :  1.2647534608840942 4.386608600616455 23.197795867919922
Loss :  1.2542955875396729 4.657600402832031 24.54229736328125
Loss :  1.2754027843475342 4.5449113845825195 23.999958038330078
Loss :  1.0966776609420776 4.384127140045166 23.01731300354004
Loss :  1.416703224182129 4.452871322631836 23.681060791015625
Loss :  1.0852289199829102 4.538656234741211 23.77851104736328
Loss :  1.334054946899414 4.679563045501709 24.731870651245117
Loss :  1.2383878231048584 4.433707237243652 23.406925201416016
Loss :  1.2432048320770264 4.486618518829346 23.67629623413086
Loss :  1.1342440843582153 4.415714740753174 23.212818145751953
Loss :  1.1836485862731934 4.496260643005371 23.664953231811523
Loss :  1.1617939472198486 4.633450508117676 24.32904624938965
Loss :  1.4082791805267334 4.497398853302002 23.895273208618164
Loss :  1.4446732997894287 4.3074822425842285 22.982084274291992
Loss :  1.4630258083343506 4.664853572845459 24.787294387817383
  batch 40 loss: 1.4630258083343506, 4.664853572845459, 24.787294387817383
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.309448003768921 4.51263427734375 23.87261962890625
Loss :  1.2195923328399658 4.483980178833008 23.639493942260742
Loss :  1.211800217628479 4.437982082366943 23.401710510253906
Loss :  1.2613009214401245 4.472301959991455 23.62281036376953
Loss :  1.1906564235687256 4.467596530914307 23.52863883972168
Loss :  1.2649588584899902 4.326541423797607 22.897666931152344
Loss :  1.4481626749038696 4.441565036773682 23.655986785888672
Loss :  1.211246371269226 4.4347052574157715 23.38477325439453
Loss :  1.5132453441619873 4.390159606933594 23.46404266357422
Loss :  1.2215676307678223 4.518689155578613 23.815011978149414
Loss :  1.3859443664550781 4.516975402832031 23.970821380615234
Loss :  1.332571029663086 4.428978443145752 23.477462768554688
Loss :  1.2746636867523193 4.552316188812256 24.036245346069336
Loss :  1.4591917991638184 4.273691177368164 22.827648162841797
Loss :  1.1758540868759155 4.436005592346191 23.355880737304688
Loss :  1.428970217704773 4.3893303871154785 23.375621795654297
Loss :  1.2195008993148804 4.543122291564941 23.93511199951172
Loss :  1.1330397129058838 4.470926284790039 23.4876708984375
Loss :  1.2211074829101562 4.644589424133301 24.444053649902344
Loss :  1.536280632019043 4.452620983123779 23.79938507080078
  batch 60 loss: 1.536280632019043, 4.452620983123779, 23.79938507080078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.1886053085327148 4.629760265350342 24.337406158447266
Loss :  1.3211175203323364 4.318532466888428 22.913780212402344
Loss :  1.2432446479797363 4.629920482635498 24.392847061157227
Loss :  1.1553293466567993 4.643589973449707 24.37327766418457
Loss :  1.1015475988388062 4.12985372543335 21.750816345214844
Loss :  1.5748651027679443 4.450028419494629 23.82500648498535
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.7577388286590576 4.440977573394775 23.96262550354004
Loss :  1.625568151473999 4.33358097076416 23.293472290039062
Loss :  1.693345069885254 4.281205177307129 23.099369049072266
Total LOSS train 23.638074023907002 valid 23.54511833190918
CE LOSS train 1.2864000430473914 valid 0.4233362674713135
Contrastive LOSS train 4.470334801307091 valid 1.0703012943267822
EPOCH 13:
Loss :  1.3407398462295532 4.385501384735107 23.268247604370117
Loss :  1.4129751920700073 4.6236748695373535 24.531349182128906
Loss :  1.253011703491211 4.4284820556640625 23.395421981811523
Loss :  1.3200546503067017 4.418803691864014 23.414073944091797
Loss :  1.4221124649047852 4.569398880004883 24.269107818603516
Loss :  1.2098692655563354 4.397811412811279 23.19892692565918
Loss :  1.3787940740585327 4.497015476226807 23.86387062072754
Loss :  1.282875895500183 4.503857612609863 23.802162170410156
Loss :  1.2396405935287476 4.436434745788574 23.421815872192383
Loss :  1.3838045597076416 4.256590843200684 22.666759490966797
Loss :  1.1834241151809692 4.465135097503662 23.50909996032715
Loss :  1.1908894777297974 4.584975242614746 24.115766525268555
Loss :  1.1755844354629517 4.44354248046875 23.39329719543457
Loss :  1.2486053705215454 4.549460411071777 23.995908737182617
Loss :  1.467456579208374 4.487436294555664 23.904638290405273
Loss :  1.4513559341430664 4.5367631912231445 24.135169982910156
Loss :  1.1906952857971191 4.371801376342773 23.049701690673828
Loss :  1.2977384328842163 4.447775840759277 23.536619186401367
Loss :  1.1739927530288696 4.349206447601318 22.920024871826172
Loss :  1.4472593069076538 4.4391655921936035 23.64308738708496
  batch 20 loss: 1.4472593069076538, 4.4391655921936035, 23.64308738708496
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.2519980669021606 4.371925354003906 23.11162567138672
Loss :  1.1665605306625366 4.632630348205566 24.3297119140625
Loss :  1.234899640083313 4.4826340675354 23.648069381713867
Loss :  1.3278335332870483 4.454904079437256 23.602354049682617
Loss :  1.4884145259857178 4.804643630981445 25.511632919311523
Loss :  1.2482539415359497 4.371901035308838 23.107759475708008
Loss :  1.2128986120224 4.752427577972412 24.97503662109375
Loss :  1.286028265953064 4.334066390991211 22.95635986328125
Loss :  1.051992416381836 4.440548896789551 23.254735946655273
Loss :  1.3981505632400513 4.522653102874756 24.011417388916016
Loss :  1.0470292568206787 4.561896324157715 23.85651206970215
Loss :  1.4262396097183228 4.624028205871582 24.54637908935547
Loss :  1.2385764122009277 4.4926557540893555 23.70185661315918
Loss :  1.2558485269546509 4.47539758682251 23.632835388183594
Loss :  1.1054837703704834 4.428755760192871 23.249263763427734
Loss :  1.140959620475769 4.527932167053223 23.78061866760254
Loss :  1.175140142440796 4.457407474517822 23.462177276611328
Loss :  1.4292747974395752 4.555379390716553 24.206172943115234
Loss :  1.4273154735565186 4.425268650054932 23.55365753173828
Loss :  1.442895531654358 4.543294906616211 24.15937042236328
  batch 40 loss: 1.442895531654358, 4.543294906616211, 24.15937042236328
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.334228277206421 4.5042924880981445 23.855690002441406
Loss :  1.2234121561050415 4.43887186050415 23.417770385742188
Loss :  1.2367242574691772 4.464327812194824 23.558364868164062
Loss :  1.3047773838043213 4.4728522300720215 23.669038772583008
Loss :  1.182906150817871 4.28193998336792 22.592605590820312
Loss :  1.3378770351409912 4.585574150085449 24.265748977661133
Loss :  1.4575879573822021 4.527846336364746 24.096820831298828
Loss :  1.2665791511535645 4.6587605476379395 24.560380935668945
Loss :  1.4573429822921753 4.463654041290283 23.775611877441406
Loss :  1.2494136095046997 4.5820631980896 24.159730911254883
Loss :  1.3680261373519897 4.500067234039307 23.868362426757812
Loss :  1.3789012432098389 4.398018836975098 23.368993759155273
Loss :  1.2732547521591187 4.36320161819458 23.089262008666992
Loss :  1.3893762826919556 4.64035177230835 24.591135025024414
Loss :  1.2859572172164917 4.589628219604492 24.234098434448242
Loss :  1.4958640336990356 4.507623195648193 24.033981323242188
Loss :  1.242881417274475 4.702671527862549 24.75623893737793
Loss :  1.1877368688583374 4.479950428009033 23.587488174438477
Loss :  1.2090522050857544 4.53268575668335 23.872482299804688
Loss :  1.4836294651031494 4.641520023345947 24.69122886657715
  batch 60 loss: 1.4836294651031494, 4.641520023345947, 24.69122886657715
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.1778866052627563 4.45128870010376 23.434329986572266
Loss :  1.28730046749115 4.473716735839844 23.6558837890625
Loss :  1.2397074699401855 4.465414524078369 23.56678009033203
Loss :  1.1702425479888916 4.396938800811768 23.154935836791992
Loss :  1.061358094215393 4.072376251220703 21.42323875427246
Loss :  1.2210867404937744 4.4290900230407715 23.36653709411621
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.2674368619918823 4.37783145904541 23.156593322753906
Loss :  1.2509709596633911 4.298973083496094 22.74583625793457
Loss :  1.286111831665039 4.333226203918457 22.952241897583008
Total LOSS train 23.722658450786884 valid 23.055302143096924
CE LOSS train 1.2881337991127602 valid 0.32152795791625977
Contrastive LOSS train 4.486904921898475 valid 1.0833065509796143
EPOCH 14:
Loss :  1.3452638387680054 4.264721870422363 22.668872833251953
Loss :  1.4155076742172241 4.396398544311523 23.39750099182129
Loss :  1.27729070186615 4.30557918548584 22.805187225341797
Loss :  1.303955316543579 4.442994594573975 23.51892852783203
Loss :  1.3720486164093018 4.258026599884033 22.662181854248047
Loss :  1.2120697498321533 4.137348651885986 21.898813247680664
Loss :  1.373793363571167 4.169025897979736 22.218921661376953
Loss :  1.2480804920196533 4.203351974487305 22.264841079711914
Loss :  1.2339729070663452 4.443824291229248 23.453094482421875
Loss :  1.399868369102478 4.126611709594727 22.032926559448242
Loss :  1.1635037660598755 4.4858527183532715 23.5927677154541
Loss :  1.1845470666885376 4.524703025817871 23.808063507080078
Loss :  1.1529064178466797 4.462126731872559 23.46354103088379
Loss :  1.2435938119888306 4.5139288902282715 23.8132381439209
Loss :  1.432443618774414 4.481308460235596 23.838985443115234
Loss :  1.430762767791748 4.498936176300049 23.925443649291992
Loss :  1.166324496269226 4.49955940246582 23.664121627807617
Loss :  1.2503217458724976 4.463996410369873 23.57030487060547
Loss :  1.1190563440322876 4.576694011688232 24.002527236938477
Loss :  1.3990370035171509 4.361235618591309 23.205215454101562
  batch 20 loss: 1.3990370035171509, 4.361235618591309, 23.205215454101562
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.2806636095046997 4.351244926452637 23.036890029907227
Loss :  1.114120602607727 4.373162746429443 22.979934692382812
Loss :  1.218821406364441 4.554128646850586 23.989463806152344
Loss :  1.293156623840332 4.463022708892822 23.60826873779297
Loss :  1.4120396375656128 4.665585041046143 24.73996353149414
Loss :  1.2271608114242554 4.315344333648682 22.803882598876953
Loss :  1.2739464044570923 4.416428565979004 23.356088638305664
Loss :  1.2872637510299683 4.1758222579956055 22.1663761138916
Loss :  1.021475076675415 4.1815667152404785 21.92930793762207
Loss :  1.4198589324951172 4.165905475616455 22.249385833740234
Loss :  1.0301315784454346 4.337069988250732 22.715482711791992
Loss :  1.3547309637069702 4.407238960266113 23.39092445373535
Loss :  1.2515615224838257 4.494965553283691 23.726388931274414
Loss :  1.2041583061218262 4.382667541503906 23.117496490478516
Loss :  1.0892890691757202 4.585997104644775 24.01927375793457
Loss :  1.1567509174346924 4.541326522827148 23.863384246826172
Loss :  1.1082477569580078 4.328795433044434 22.752225875854492
Loss :  1.3944989442825317 4.3777689933776855 23.283344268798828
Loss :  1.3663996458053589 4.489867687225342 23.815738677978516
Loss :  1.459383487701416 4.4162774085998535 23.540769577026367
  batch 40 loss: 1.459383487701416, 4.4162774085998535, 23.540769577026367
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.249125599861145 4.452773094177246 23.51299285888672
Loss :  1.209905982017517 4.367071628570557 23.045263290405273
Loss :  1.1552094221115112 4.518864631652832 23.749530792236328
Loss :  1.2082288265228271 4.571450233459473 24.065479278564453
Loss :  1.1408438682556152 4.280278205871582 22.542234420776367
Loss :  1.2322614192962646 4.421249866485596 23.338510513305664
Loss :  1.3997631072998047 4.442202568054199 23.610776901245117
Loss :  1.2403910160064697 4.38130521774292 23.14691734313965
Loss :  1.509792685508728 4.764455318450928 25.332069396972656
Loss :  1.261722445487976 4.49603271484375 23.741886138916016
Loss :  1.402745246887207 4.667383670806885 24.739662170410156
Loss :  1.3760164976119995 4.490742206573486 23.829727172851562
Loss :  1.302847146987915 4.354341506958008 23.074554443359375
Loss :  1.3537360429763794 4.440320014953613 23.555335998535156
Loss :  1.2406318187713623 4.415417671203613 23.317718505859375
Loss :  1.4502476453781128 4.48227596282959 23.86162757873535
Loss :  1.2140741348266602 4.539123058319092 23.909690856933594
Loss :  1.1399708986282349 4.34203577041626 22.850149154663086
Loss :  1.1796475648880005 4.569998264312744 24.029638290405273
Loss :  1.4637222290039062 4.3612847328186035 23.270145416259766
  batch 60 loss: 1.4637222290039062, 4.3612847328186035, 23.270145416259766
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.1176973581314087 4.409186363220215 23.16362953186035
Loss :  1.3494699001312256 4.532698631286621 24.012964248657227
Loss :  1.2088552713394165 4.453020095825195 23.473955154418945
Loss :  1.1543344259262085 4.382630825042725 23.067489624023438
Loss :  1.0462034940719604 4.183818817138672 21.96529769897461
Loss :  1.2720661163330078 4.451094627380371 23.52754020690918
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.365878701210022 4.396637439727783 23.34906578063965
Loss :  1.3360795974731445 4.35479736328125 23.110065460205078
Loss :  1.3988982439041138 4.014588832855225 21.471843719482422
Total LOSS train 23.339958689762994 valid 22.864628791809082
CE LOSS train 1.266083864065317 valid 0.34972456097602844
Contrastive LOSS train 4.414774960737962 valid 1.0036472082138062
EPOCH 15:
Loss :  1.3525243997573853 4.3038649559021 22.871849060058594
Loss :  1.3598865270614624 4.5365447998046875 24.04261016845703
Loss :  1.2466005086898804 4.416821479797363 23.330707550048828
Loss :  1.271498203277588 4.41648006439209 23.353899002075195
Loss :  1.344078540802002 4.463467121124268 23.661413192749023
Loss :  1.1736091375350952 4.390504837036133 23.12613296508789
Loss :  1.3650493621826172 4.778875350952148 25.25942611694336
Loss :  1.2217342853546143 4.330984115600586 22.87665557861328
Loss :  1.1799790859222412 4.35737943649292 22.966876983642578
Loss :  1.3032100200653076 4.385858535766602 23.232501983642578
Loss :  1.1267759799957275 4.506178855895996 23.657670974731445
Loss :  1.121520757675171 4.501570224761963 23.629371643066406
Loss :  1.1343291997909546 4.544114112854004 23.85489845275879
Loss :  1.1576892137527466 4.499842643737793 23.656904220581055
Loss :  1.40566885471344 4.341501712799072 23.113176345825195
Loss :  1.4095607995986938 4.558712482452393 24.203123092651367
Loss :  1.1067193746566772 4.408384799957275 23.148643493652344
Loss :  1.2161378860473633 4.3571858406066895 23.00206756591797
Loss :  1.076338768005371 4.5758585929870605 23.955631256103516
Loss :  1.376081109046936 4.332221508026123 23.037189483642578
  batch 20 loss: 1.376081109046936, 4.332221508026123, 23.037189483642578
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.2296229600906372 4.426788330078125 23.36356544494629
Loss :  1.0807729959487915 4.3843092918396 23.0023193359375
Loss :  1.1819992065429688 4.613030910491943 24.247154235839844
Loss :  1.2489829063415527 4.482712745666504 23.662546157836914
Loss :  1.3592944145202637 4.604336738586426 24.380977630615234
Loss :  1.168199062347412 4.313202857971191 22.73421287536621
Loss :  1.2090415954589844 4.737682819366455 24.8974552154541
Loss :  1.1958695650100708 4.340120792388916 22.896472930908203
Loss :  0.9605122208595276 4.360049247741699 22.760759353637695
Loss :  1.3719185590744019 4.4634013175964355 23.68892478942871
Loss :  0.9654380679130554 4.47628116607666 23.346843719482422
Loss :  1.2836427688598633 4.676880836486816 24.668045043945312
Loss :  1.1417351961135864 4.476598739624023 23.524728775024414
Loss :  1.1579779386520386 4.558143138885498 23.948694229125977
Loss :  1.007085919380188 4.57430362701416 23.878602981567383
Loss :  1.0579869747161865 4.506109237670898 23.588533401489258
Loss :  1.0641064643859863 4.603429794311523 24.081254959106445
Loss :  1.3480275869369507 4.260344505310059 22.649751663208008
Loss :  1.347213864326477 4.267126560211182 22.682846069335938
Loss :  1.3986998796463013 4.3365044593811035 23.081222534179688
  batch 40 loss: 1.3986998796463013, 4.3365044593811035, 23.081222534179688
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.225966215133667 4.45466423034668 23.499286651611328
Loss :  1.1466823816299438 4.497550010681152 23.63443374633789
Loss :  1.108885645866394 4.498856067657471 23.603164672851562
Loss :  1.1721031665802002 4.415228366851807 23.248245239257812
Loss :  1.0658833980560303 4.24204158782959 22.276092529296875
Loss :  1.1885355710983276 4.433562278747559 23.356348037719727
Loss :  1.3407809734344482 4.350666522979736 23.094112396240234
Loss :  1.1255152225494385 4.582449436187744 24.037761688232422
Loss :  1.4080387353897095 4.252132415771484 22.668701171875
Loss :  1.143132209777832 4.54829740524292 23.884620666503906
Loss :  1.2770220041275024 4.4099555015563965 23.326799392700195
Loss :  1.2962919473648071 4.418783187866211 23.390207290649414
Loss :  1.2075475454330444 4.439424991607666 23.404672622680664
Loss :  1.3170549869537354 4.429492473602295 23.46451759338379
Loss :  1.1744693517684937 4.396926403045654 23.159101486206055
Loss :  1.4171591997146606 4.386232852935791 23.348323822021484
Loss :  1.1966193914413452 4.424386024475098 23.31854820251465
Loss :  1.092614769935608 4.485791206359863 23.521570205688477
Loss :  1.1952074766159058 4.724160194396973 24.816007614135742
Loss :  1.4276695251464844 4.508959770202637 23.972469329833984
  batch 60 loss: 1.4276695251464844, 4.508959770202637, 23.972469329833984
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.136996865272522 4.378666400909424 23.03032875061035
Loss :  1.2147620916366577 4.385388374328613 23.14170265197754
Loss :  1.1230242252349854 4.335891246795654 22.802480697631836
Loss :  1.082676887512207 4.391716957092285 23.041259765625
Loss :  0.9776994585990906 4.120553970336914 21.580469131469727
Loss :  1.4747586250305176 4.222424030303955 22.586877822875977
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.5570000410079956 4.2713236808776855 22.913618087768555
Loss :  1.4744880199432373 4.173243999481201 22.340707778930664
Loss :  1.59652841091156 4.1829752922058105 22.51140594482422
Total LOSS train 23.441336705134464 valid 22.588152408599854
CE LOSS train 1.2121455293435317 valid 0.39913210272789
Contrastive LOSS train 4.445838253314679 valid 1.0457438230514526
Saved best model. Old loss 22.815001487731934 and new best loss 22.588152408599854
EPOCH 16:
Loss :  1.2766457796096802 4.271860122680664 22.63594627380371
Loss :  1.3150293827056885 4.4709906578063965 23.66998291015625
Loss :  1.239865779876709 4.435549736022949 23.41761589050293
Loss :  1.2788869142532349 4.340648174285889 22.982128143310547
Loss :  1.33720064163208 4.390085697174072 23.287628173828125
Loss :  1.2005970478057861 4.341586112976074 22.90852928161621
Loss :  1.3059197664260864 4.509582996368408 23.85383415222168
Loss :  1.1712605953216553 4.328104496002197 22.811782836914062
Loss :  1.1388531923294067 4.401630878448486 23.14700698852539
Loss :  1.279630422592163 4.363548278808594 23.09737205505371
Loss :  1.1013227701187134 4.358317852020264 22.892911911010742
Loss :  1.1017966270446777 4.3625617027282715 22.91460609436035
Loss :  1.0930848121643066 4.378729343414307 22.986730575561523
Loss :  1.157510757446289 4.5795392990112305 24.055208206176758
Loss :  1.3649276494979858 4.399947166442871 23.36466407775879
Loss :  1.3229707479476929 4.462071895599365 23.633331298828125
Loss :  1.1147472858428955 4.3727126121521 22.97831153869629
Loss :  1.215954065322876 4.3357062339782715 22.894485473632812
Loss :  1.061529278755188 4.17858362197876 21.95444679260254
Loss :  1.3347378969192505 4.282739639282227 22.748435974121094
  batch 20 loss: 1.3347378969192505, 4.282739639282227, 22.748435974121094
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.2121572494506836 4.18001127243042 22.112213134765625
Loss :  1.1011879444122314 4.317240238189697 22.687389373779297
Loss :  1.1694200038909912 4.408823490142822 23.213537216186523
Loss :  1.2113568782806396 4.454294204711914 23.48282814025879
Loss :  1.3038890361785889 4.467600345611572 23.641889572143555
Loss :  1.1797024011611938 4.364026069641113 22.999832153320312
Loss :  1.2540791034698486 4.296850204467773 22.738330841064453
Loss :  1.264343023300171 4.123571872711182 21.8822021484375
Loss :  1.0310217142105103 3.9811036586761475 20.936538696289062
Loss :  1.3547941446304321 4.28517484664917 22.780668258666992
Loss :  1.0173430442810059 4.358839511871338 22.811540603637695
Loss :  1.3686546087265015 4.467288017272949 23.705095291137695
Loss :  1.2396827936172485 4.192601680755615 22.20269203186035
Loss :  1.2028942108154297 4.447868347167969 23.442235946655273
Loss :  1.0573523044586182 4.335496425628662 22.734834671020508
Loss :  1.135825276374817 4.5273284912109375 23.77246856689453
Loss :  1.10448157787323 4.2701239585876465 22.455101013183594
Loss :  1.3087085485458374 4.402959823608398 23.32350730895996
Loss :  1.3389865159988403 4.353200912475586 23.104991912841797
Loss :  1.4142171144485474 4.445054531097412 23.639490127563477
  batch 40 loss: 1.4142171144485474, 4.445054531097412, 23.639490127563477
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.2744754552841187 4.561781406402588 24.08338165283203
Loss :  1.2077468633651733 4.609946250915527 24.257478713989258
Loss :  1.1766078472137451 4.53745698928833 23.8638916015625
Loss :  1.1865003108978271 4.564677715301514 24.009889602661133
Loss :  1.1279646158218384 4.339338779449463 22.824657440185547
Loss :  1.2699395418167114 4.223592758178711 22.387903213500977
Loss :  1.3829431533813477 4.516059398651123 23.963241577148438
Loss :  1.2154924869537354 4.551794052124023 23.974462509155273
Loss :  1.4294651746749878 4.405928611755371 23.459108352661133
Loss :  1.2184388637542725 4.539193153381348 23.914403915405273
Loss :  1.3289434909820557 4.633702278137207 24.497453689575195
Loss :  1.3286679983139038 4.345831394195557 23.057825088500977
Loss :  1.2271814346313477 4.39320707321167 23.193218231201172
Loss :  1.363659143447876 4.477899551391602 23.753156661987305
Loss :  1.218687653541565 4.432610511779785 23.381738662719727
Loss :  1.380070686340332 4.332913398742676 23.044635772705078
Loss :  1.2625048160552979 4.495707988739014 23.741044998168945
Loss :  1.1350823640823364 4.612603187561035 24.198097229003906
Loss :  1.271986961364746 4.546189308166504 24.002933502197266
Loss :  1.508774757385254 4.341442584991455 23.215988159179688
  batch 60 loss: 1.508774757385254, 4.341442584991455, 23.215988159179688
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.1956050395965576 4.4661126136779785 23.526166915893555
Loss :  1.2999227046966553 4.311498641967773 22.8574161529541
Loss :  1.202865719795227 4.479431629180908 23.60002326965332
Loss :  1.1922693252563477 4.3969407081604 23.176971435546875
Loss :  1.1129865646362305 3.9726786613464355 20.97637939453125
Loss :  1.2732497453689575 4.480738639831543 23.676944732666016
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.3288788795471191 4.4208526611328125 23.433141708374023
Loss :  1.2952665090560913 4.330671787261963 22.948625564575195
Loss :  1.320515751838684 4.205291748046875 22.346975326538086
Total LOSS train 23.182520206157978 valid 23.10142183303833
CE LOSS train 1.2338669520158034 valid 0.330128937959671
Contrastive LOSS train 4.389730662565965 valid 1.0513229370117188
EPOCH 17:
Loss :  1.3976889848709106 4.140034198760986 22.09786033630371
Loss :  1.4347587823867798 4.406937122344971 23.469444274902344
Loss :  1.2375673055648804 4.532527446746826 23.900205612182617
Loss :  1.2772465944290161 4.369603633880615 23.12526512145996
Loss :  1.3882564306259155 4.330141067504883 23.03896141052246
Loss :  1.272125244140625 4.40114688873291 23.27785873413086
Loss :  1.328176736831665 4.554860591888428 24.102479934692383
Loss :  1.25306236743927 4.3375444412231445 22.940784454345703
Loss :  1.2164937257766724 4.412439346313477 23.278690338134766
Loss :  1.3423399925231934 4.395073413848877 23.317707061767578
Loss :  1.2030526399612427 4.4734344482421875 23.57022476196289
Loss :  1.2131476402282715 4.500691890716553 23.71660804748535
Loss :  1.2204819917678833 4.440577983856201 23.423372268676758
Loss :  1.3159708976745605 4.6275200843811035 24.453571319580078
Loss :  1.4478158950805664 4.465811729431152 23.776874542236328
Loss :  1.4222806692123413 4.555115699768066 24.197858810424805
Loss :  1.2071746587753296 4.439478397369385 23.404565811157227
Loss :  1.2810901403427124 4.380269527435303 23.182437896728516
Loss :  1.2055526971817017 4.519063472747803 23.800870895385742
Loss :  1.4305907487869263 4.383816242218018 23.349672317504883
  batch 20 loss: 1.4305907487869263, 4.383816242218018, 23.349672317504883
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.300295114517212 4.339744567871094 22.9990177154541
Loss :  1.2188942432403564 4.504228115081787 23.740036010742188
Loss :  1.2953027486801147 4.592649459838867 24.2585506439209
Loss :  1.3702504634857178 4.388235569000244 23.31142807006836
Loss :  1.3214694261550903 4.693543910980225 24.7891902923584
Loss :  1.285969853401184 4.465664386749268 23.61429214477539
Loss :  1.3428723812103271 4.510748863220215 23.896617889404297
Loss :  1.2714163064956665 4.426458358764648 23.40370750427246
Loss :  1.1339659690856934 4.44093132019043 23.338623046875
Loss :  1.4778136014938354 4.488656044006348 23.921092987060547
Loss :  1.1534900665283203 4.457658767700195 23.441783905029297
Loss :  1.4351661205291748 4.701460361480713 24.942466735839844
Loss :  1.3007830381393433 4.557138442993164 24.086475372314453
Loss :  1.2729777097702026 4.494332790374756 23.74464225769043
Loss :  1.152828574180603 4.408882141113281 23.19723892211914
Loss :  1.229479432106018 4.65602970123291 24.509626388549805
Loss :  1.2159631252288818 4.423218727111816 23.332056045532227
Loss :  1.5043387413024902 4.4622955322265625 23.81581687927246
Loss :  1.421344518661499 4.38656759262085 23.354183197021484
Loss :  1.4777204990386963 4.414773464202881 23.55158805847168
  batch 40 loss: 1.4777204990386963, 4.414773464202881, 23.55158805847168
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4, 5], device='cuda:0')
Loss :  1.3668529987335205 4.635080337524414 24.542255401611328
Loss :  1.294726014137268 4.500696659088135 23.798208236694336
Loss :  1.2601635456085205 4.4527459144592285 23.523893356323242
Loss :  1.3297685384750366 4.67828369140625 24.721187591552734
Loss :  1.2404811382293701 4.328361511230469 22.882287979125977
Loss :  1.4009915590286255 4.392535209655762 23.36366844177246
Loss :  1.4554709196090698 4.51995325088501 24.05523681640625
Loss :  1.2188866138458252 4.446477890014648 23.451276779174805
Loss :  1.5264557600021362 4.5064005851745605 24.05845832824707
Loss :  1.2989693880081177 4.459486961364746 23.596405029296875
Loss :  1.48544180393219 4.52879524230957 24.129417419433594
Loss :  1.4390172958374023 4.565530300140381 24.26667022705078
Loss :  1.364278793334961 4.591595649719238 24.322256088256836
Loss :  1.4917125701904297 4.6700334548950195 24.84187889099121
Loss :  1.2977869510650635 4.669577598571777 24.645675659179688
Loss :  1.547666311264038 4.4646124839782715 23.870729446411133
Loss :  1.3114761114120483 4.519115924835205 23.907054901123047
Loss :  1.267225980758667 4.7080183029174805 24.80731773376465
Loss :  1.3164607286453247 4.569311618804932 24.16301918029785
Loss :  1.5637032985687256 4.476836681365967 23.947887420654297
  batch 60 loss: 1.5637032985687256, 4.476836681365967, 23.947887420654297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.2937512397766113 4.365119934082031 23.11935043334961
Loss :  1.418284296989441 4.487502574920654 23.855796813964844
Loss :  1.311810851097107 4.369442462921143 23.15902328491211
Loss :  1.2838091850280762 4.415624141693115 23.36193084716797
Loss :  1.1610751152038574 4.1429243087768555 21.87569808959961
Loss :  1.1902689933776855 4.434442520141602 23.36248207092285
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.2508233785629272 4.473546981811523 23.618558883666992
Loss :  1.2227424383163452 4.375947952270508 23.102481842041016
Loss :  1.2730718851089478 4.184166431427002 22.19390296936035
Total LOSS train 23.706774344811073 valid 23.069356441497803
CE LOSS train 1.3265151243943434 valid 0.31826797127723694
Contrastive LOSS train 4.476051822075477 valid 1.0460416078567505
EPOCH 18:
Loss :  1.4158858060836792 4.45627498626709 23.697261810302734
Loss :  1.4759451150894165 4.576837539672852 24.360132217407227
Loss :  1.3540565967559814 4.381345272064209 23.260784149169922
Loss :  1.376644492149353 4.270881175994873 22.731050491333008
Loss :  1.4728246927261353 4.2671637535095215 22.808643341064453
Loss :  1.2636009454727173 4.326674461364746 22.896974563598633
Loss :  1.4619721174240112 4.325378894805908 23.088865280151367
Loss :  1.3967359066009521 4.250575542449951 22.649614334106445
Loss :  1.3510593175888062 4.259437561035156 22.64824676513672
Loss :  1.4585115909576416 4.193437099456787 22.425697326660156
Loss :  1.2845598459243774 4.851746559143066 25.543291091918945
Loss :  1.2282710075378418 4.376947402954102 23.113008499145508
Loss :  1.2726161479949951 4.433613300323486 23.44068145751953
Loss :  1.3129539489746094 4.5739359855651855 24.182634353637695
Loss :  1.5279488563537598 4.476678848266602 23.91134262084961
Loss :  1.438878059387207 4.517354488372803 24.025650024414062
Loss :  1.2418272495269775 4.523399353027344 23.858823776245117
Loss :  1.3751180171966553 4.387105464935303 23.310646057128906
Loss :  1.2136591672897339 4.375454425811768 23.090930938720703
Loss :  1.5137604475021362 4.361836910247803 23.32294464111328
  batch 20 loss: 1.5137604475021362, 4.361836910247803, 23.32294464111328
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.3034732341766357 4.374807834625244 23.17751121520996
Loss :  1.248342752456665 4.520685195922852 23.851768493652344
Loss :  1.2472918033599854 4.419857978820801 23.346580505371094
Loss :  1.362695574760437 4.2817511558532715 22.771451950073242
Loss :  1.4393439292907715 4.579397201538086 24.33633041381836
Loss :  1.3269975185394287 4.4882941246032715 23.768468856811523
Loss :  1.2886885404586792 4.574073791503906 24.1590576171875
Loss :  1.2959150075912476 4.406461238861084 23.328222274780273
Loss :  1.0945262908935547 4.417553901672363 23.182294845581055
Loss :  1.4330908060073853 4.522726058959961 24.046720504760742
Loss :  1.098536491394043 4.54177713394165 23.807422637939453
Loss :  1.4721835851669312 4.588901042938232 24.416688919067383
Loss :  1.3057126998901367 4.2444024085998535 22.527725219726562
Loss :  1.2871934175491333 4.235966205596924 22.467023849487305
Loss :  1.1400508880615234 4.2649312019348145 22.464706420898438
Loss :  1.1881160736083984 4.412815570831299 23.252193450927734
Loss :  1.203217625617981 4.104495525360107 21.725696563720703
Loss :  1.4775726795196533 4.648223400115967 24.718690872192383
Loss :  1.4991867542266846 4.690184116363525 24.95010757446289
Loss :  1.5048859119415283 4.6056084632873535 24.532928466796875
  batch 40 loss: 1.5048859119415283, 4.6056084632873535, 24.532928466796875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4, 5], device='cuda:0')
Loss :  1.3582731485366821 4.46265983581543 23.671571731567383
Loss :  1.2663792371749878 4.629859924316406 24.415678024291992
Loss :  1.2264384031295776 4.410360336303711 23.278240203857422
Loss :  1.246411919593811 4.440582275390625 23.449323654174805
Loss :  1.1753777265548706 4.411134243011475 23.231048583984375
Loss :  1.2346199750900269 4.503237247467041 23.750804901123047
Loss :  1.3969529867172241 4.431075096130371 23.552330017089844
Loss :  1.1884790658950806 4.570986270904541 24.04340934753418
Loss :  1.4434311389923096 4.363259792327881 23.25973129272461
Loss :  1.2197661399841309 4.466115474700928 23.550344467163086
Loss :  1.3633716106414795 4.490736961364746 23.81705665588379
Loss :  1.4013761281967163 4.376591682434082 23.284334182739258
Loss :  1.277528166770935 4.498653888702393 23.770797729492188
Loss :  1.41889226436615 4.525079727172852 24.04429054260254
Loss :  1.2391072511672974 4.5058417320251465 23.7683162689209
Loss :  1.4600534439086914 4.524139881134033 24.080753326416016
Loss :  1.213383436203003 4.587628364562988 24.15152359008789
Loss :  1.1499154567718506 4.5418314933776855 23.859073638916016
Loss :  1.2368534803390503 4.545943260192871 23.966569900512695
Loss :  1.5373427867889404 4.400533676147461 23.540010452270508
  batch 60 loss: 1.5373427867889404, 4.400533676147461, 23.540010452270508
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.1893272399902344 4.405879020690918 23.21872329711914
Loss :  1.2598018646240234 4.4427690505981445 23.47364616394043
Loss :  1.2302179336547852 4.560719966888428 24.033817291259766
Loss :  1.2055259943008423 4.387667179107666 23.143861770629883
Loss :  1.0710381269454956 4.1143798828125 21.64293670654297
Loss :  99190.2109375 4.4291205406188965 99212.359375
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  64819.77734375 4.486508369445801 64842.2109375
Loss :  1174.7425537109375 4.253098964691162 1196.008056640625
Loss :  235404.171875 4.2559943199157715 235425.453125
Total LOSS train 23.52610781742976 valid 100169.00787353516
CE LOSS train 1.3179033206059383 valid 58851.04296875
Contrastive LOSS train 4.4416409052335295 valid 1.0639985799789429
EPOCH 19:
Loss :  1.3489989042282104 4.285865783691406 22.77832794189453
Loss :  1.381436824798584 4.521583557128906 23.989355087280273
Loss :  1.2770640850067139 4.404877185821533 23.301448822021484
Loss :  1.3424854278564453 4.428848743438721 23.48672866821289
Loss :  1.32723867893219 4.3920722007751465 23.287599563598633
Loss :  1.2619788646697998 4.341867446899414 22.971315383911133
Loss :  1.4059202671051025 4.512672424316406 23.969282150268555
Loss :  1.3350424766540527 4.276837348937988 22.719228744506836
Loss :  1.2170120477676392 4.5483808517456055 23.95891761779785
Loss :  1.3728597164154053 4.338264465332031 23.06418228149414
Loss :  1.2736467123031616 4.565886497497559 24.10308074951172
Loss :  1.1722222566604614 4.500739574432373 23.675920486450195
Loss :  1.1679491996765137 4.4709930419921875 23.52291488647461
Loss :  1.2618277072906494 4.525880336761475 23.8912296295166
Loss :  1.423902988433838 4.332648754119873 23.087146759033203
Loss :  1.389326572418213 4.501212120056152 23.895387649536133
Loss :  1.1754391193389893 4.509073257446289 23.720806121826172
Loss :  1.2548328638076782 4.481051445007324 23.660091400146484
Loss :  1.1286133527755737 4.3088603019714355 22.672914505004883
Loss :  1.4650925397872925 4.583786964416504 24.38402557373047
  batch 20 loss: 1.4650925397872925, 4.583786964416504, 24.38402557373047
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.2951104640960693 4.378058433532715 23.18540382385254
Loss :  1.1651136875152588 4.4622650146484375 23.476438522338867
Loss :  1.2474058866500854 4.531938552856445 23.9070987701416
Loss :  1.2855247259140015 4.551639080047607 24.043720245361328
Loss :  1.4381955862045288 4.6451334953308105 24.663864135742188
Loss :  1.2033740282058716 4.3611884117126465 23.00931739807129
Loss :  1.2562637329101562 4.5090718269348145 23.80162239074707
Loss :  1.2467209100723267 4.410332679748535 23.298383712768555
Loss :  1.0267633199691772 4.432063579559326 23.187082290649414
Loss :  1.3377515077590942 4.468812465667725 23.681814193725586
Loss :  1.034151315689087 4.5039496421813965 23.55389976501465
Loss :  1.3002991676330566 4.69226598739624 24.761629104614258
Loss :  1.202391266822815 4.457803726196289 23.491409301757812
Loss :  1.1945486068725586 4.442959308624268 23.409343719482422
Loss :  1.0281258821487427 4.555988788604736 23.808069229125977
Loss :  1.1313238143920898 4.466978073120117 23.46621322631836
Loss :  1.0859057903289795 4.413731575012207 23.15456199645996
Loss :  1.3770822286605835 4.391627311706543 23.335220336914062
Loss :  1.4016391038894653 4.432136535644531 23.56232261657715
Loss :  1.450121521949768 4.521613121032715 24.05818748474121
  batch 40 loss: 1.450121521949768, 4.521613121032715, 24.05818748474121
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.3027147054672241 4.557796955108643 24.091699600219727
Loss :  1.1645042896270752 4.415594577789307 23.242477416992188
Loss :  1.1456300020217896 4.383572578430176 23.063491821289062
Loss :  1.194018840789795 4.493896007537842 23.66349983215332
Loss :  1.1312733888626099 4.2453694343566895 22.35811996459961
Loss :  1.280657410621643 4.532055854797363 23.940935134887695
Loss :  1.49583101272583 4.43221378326416 23.656898498535156
Loss :  1.2131177186965942 4.341211318969727 22.919174194335938
Loss :  1.523675560951233 4.394616603851318 23.49675941467285
Loss :  1.1915687322616577 4.245728015899658 22.420207977294922
Loss :  1.3702915906906128 4.186623573303223 22.303407669067383
Loss :  1.300937533378601 4.469400882720947 23.64794158935547
Loss :  1.2097660303115845 4.546436786651611 23.94194984436035
Loss :  1.3479511737823486 4.289041042327881 22.79315757751465
Loss :  1.1502517461776733 4.508668899536133 23.69359588623047
Loss :  1.432226300239563 4.448770523071289 23.67607879638672
Loss :  1.1449840068817139 4.468193054199219 23.48594856262207
Loss :  1.1229606866836548 4.541622638702393 23.831073760986328
Loss :  1.16854989528656 4.553285121917725 23.93497657775879
Loss :  1.492912769317627 4.520726203918457 24.096542358398438
  batch 60 loss: 1.492912769317627, 4.520726203918457, 24.096542358398438
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4, 5], device='cuda:0')
Loss :  1.1058145761489868 4.601086139678955 24.111244201660156
Loss :  1.269033432006836 4.501278400421143 23.77542495727539
Loss :  1.144242286682129 4.418271541595459 23.235599517822266
Loss :  1.0935051441192627 4.378171443939209 22.984363555908203
Loss :  1.0182069540023804 4.23500394821167 22.193227767944336
Loss :  9.871051788330078 4.475187301635742 32.246986389160156
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([5], device='cuda:0')
Loss :  9.852045059204102 4.370573043823242 31.704910278320312
Loss :  9.967286109924316 4.374130725860596 31.837940216064453
Loss :  10.837127685546875 4.074203014373779 31.20814323425293
Total LOSS train 23.50082004253681 valid 31.749495029449463
CE LOSS train 1.2570050606360803 valid 2.7092819213867188
Contrastive LOSS train 4.448763003716102 valid 1.0185507535934448
EPOCH 20:
Loss :  1.3709864616394043 4.385514736175537 23.298561096191406
Loss :  1.4179409742355347 4.5994367599487305 24.415124893188477
Loss :  1.2846640348434448 4.356665134429932 23.067989349365234
Loss :  1.3131673336029053 4.318177700042725 22.904056549072266
Loss :  1.391220211982727 4.371676445007324 23.249603271484375
Loss :  1.1912496089935303 4.26885986328125 22.53554916381836
Loss :  1.402674674987793 4.503969669342041 23.922523498535156
Loss :  1.2831512689590454 4.611608505249023 24.34119415283203
Loss :  1.199344515800476 4.353817462921143 22.96843147277832
Loss :  1.3642038106918335 4.187140941619873 22.299909591674805
Loss :  1.216530442237854 4.344913005828857 22.94109535217285
Loss :  1.2391668558120728 4.52717924118042 23.875062942504883
Loss :  1.2629698514938354 4.516894340515137 23.847442626953125
Loss :  1.2778209447860718 4.534303665161133 23.949338912963867
Loss :  1.5059614181518555 4.4562907218933105 23.78741455078125
Loss :  1.3993092775344849 4.4578537940979 23.68857765197754
Loss :  1.207163691520691 4.465444087982178 23.53438377380371
Loss :  1.2823255062103271 4.380003452301025 23.182342529296875
Loss :  1.146053671836853 4.3433356285095215 22.86273193359375
Loss :  1.4661808013916016 4.606966018676758 24.50101089477539
  batch 20 loss: 1.4661808013916016, 4.606966018676758, 24.50101089477539
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.3004169464111328 4.330731391906738 22.954072952270508
Loss :  1.148655891418457 4.343869686126709 22.868003845214844
Loss :  1.2332079410552979 4.477119445800781 23.618804931640625
Loss :  1.309032678604126 4.522696018218994 23.92251205444336
Loss :  1.4563484191894531 4.586874961853027 24.390724182128906
Loss :  1.2036631107330322 4.312743663787842 22.76738166809082
Loss :  1.2268025875091553 4.663895130157471 24.54627799987793
Loss :  1.2534511089324951 4.434475898742676 23.42582893371582
Loss :  1.0430221557617188 4.44174861907959 23.251766204833984
Loss :  1.3817132711410522 4.456140518188477 23.662416458129883
Loss :  1.0061333179473877 4.3109130859375 22.560699462890625
Loss :  1.3131775856018066 4.560153007507324 24.113943099975586
Loss :  1.2091566324234009 4.441330909729004 23.415809631347656
Loss :  1.2356466054916382 4.413015365600586 23.300724029541016
Loss :  1.0451781749725342 4.427259922027588 23.181476593017578
Loss :  1.1419976949691772 4.738955497741699 24.836776733398438
Loss :  1.0884771347045898 4.41173791885376 23.147167205810547
Loss :  1.3907620906829834 4.3044328689575195 22.912925720214844
Loss :  1.3790380954742432 4.36000919342041 23.1790828704834
Loss :  1.4116528034210205 4.474371910095215 23.78351402282715
  batch 40 loss: 1.4116528034210205, 4.474371910095215, 23.78351402282715
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.2765181064605713 4.446558952331543 23.509313583374023
Loss :  1.2190836668014526 4.366133689880371 23.049753189086914
Loss :  1.1587269306182861 4.5455193519592285 23.886323928833008
Loss :  1.1637938022613525 4.539069652557373 23.859142303466797
Loss :  1.1390191316604614 4.5023651123046875 23.65084457397461
Loss :  1.2340857982635498 4.4860053062438965 23.664112091064453
Loss :  1.4390345811843872 4.569525718688965 24.286664962768555
Loss :  1.193916916847229 4.4306182861328125 23.347007751464844
Loss :  1.5236246585845947 4.474323749542236 23.89524269104004
Loss :  1.1842162609100342 4.526607036590576 23.817251205444336
Loss :  1.3240426778793335 4.558695316314697 24.11751937866211
Loss :  1.3800429105758667 4.397465229034424 23.367368698120117
Loss :  1.2825214862823486 4.427938938140869 23.422216415405273
Loss :  1.4441888332366943 4.366563320159912 23.277006149291992
Loss :  1.1653035879135132 4.638405799865723 24.357332229614258
Loss :  1.4358580112457275 4.422036647796631 23.54604148864746
Loss :  1.2795227766036987 4.672807693481445 24.6435604095459
Loss :  1.1339237689971924 4.5220842361450195 23.74434471130371
Loss :  1.2100530862808228 4.718222618103027 24.801166534423828
Loss :  1.4434435367584229 4.508849620819092 23.98769187927246
  batch 60 loss: 1.4434435367584229, 4.508849620819092, 23.98769187927246
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.1121532917022705 4.424497604370117 23.234642028808594
Loss :  1.2182047367095947 4.48921012878418 23.664255142211914
Loss :  1.1870709657669067 4.575450420379639 24.06432342529297
Loss :  1.1026581525802612 4.4491095542907715 23.34820556640625
Loss :  1.057809591293335 4.273746967315674 22.426544189453125
Loss :  1.3199470043182373 4.4346923828125 23.493408203125
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.3596432209014893 4.448084354400635 23.600065231323242
Loss :  1.3051260709762573 4.325103282928467 22.93064308166504
Loss :  1.582730770111084 4.294538497924805 23.055423736572266
Total LOSS train 23.56892503591684 valid 23.269885063171387
CE LOSS train 1.2662836441626915 valid 0.395682692527771
Contrastive LOSS train 4.460528263678918 valid 1.0736346244812012
EPOCH 21:
Loss :  1.356623888015747 4.266720771789551 22.690227508544922
Loss :  1.3355598449707031 4.59096097946167 24.29036521911621
Loss :  1.2017583847045898 4.358712673187256 22.995323181152344
Loss :  1.3212867975234985 4.538125038146973 24.011911392211914
Loss :  1.295119285583496 4.67558479309082 24.67304229736328
Loss :  1.1915820837020874 4.3747735023498535 23.065448760986328
Loss :  1.3683751821517944 4.61655855178833 24.451168060302734
Loss :  1.2818406820297241 4.373380661010742 23.148744583129883
Loss :  1.2799514532089233 4.469922065734863 23.629560470581055
Loss :  1.303706169128418 4.216950416564941 22.388458251953125
Loss :  1.189403772354126 4.632207870483398 24.35044288635254
Loss :  1.1417853832244873 4.510467052459717 23.694120407104492
Loss :  1.1661781072616577 4.535559177398682 23.84397315979004
Loss :  1.2402334213256836 4.6013922691345215 24.247196197509766
Loss :  1.4503308534622192 4.453482627868652 23.717744827270508
Loss :  1.3759143352508545 4.525843143463135 24.005128860473633
Loss :  1.1870434284210205 4.445757865905762 23.415834426879883
Loss :  1.278572678565979 4.709843158721924 24.827787399291992
Loss :  1.1886987686157227 4.353938579559326 22.958393096923828
Loss :  1.4556735754013062 4.354998588562012 23.230667114257812
  batch 20 loss: 1.4556735754013062, 4.354998588562012, 23.230667114257812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.2755241394042969 4.372702598571777 23.1390380859375
Loss :  1.1333038806915283 4.519815921783447 23.732383728027344
Loss :  1.2410438060760498 4.537774562835693 23.929916381835938
Loss :  1.246156930923462 4.497465133666992 23.733482360839844
Loss :  1.4127358198165894 4.6696553230285645 24.761011123657227
Loss :  1.289790153503418 4.636147499084473 24.47052764892578
Loss :  1.4364014863967896 4.5639424324035645 24.256113052368164
Loss :  1.2838068008422852 4.337717056274414 22.972393035888672
Loss :  1.07224440574646 4.4398884773254395 23.271686553955078
Loss :  1.3210771083831787 4.390546798706055 23.27381134033203
Loss :  1.0635466575622559 4.5279765129089355 23.70343017578125
Loss :  1.357621431350708 4.624549865722656 24.480371475219727
Loss :  1.2296169996261597 4.427680969238281 23.36802101135254
Loss :  1.2221546173095703 4.489235877990723 23.668333053588867
Loss :  1.0986790657043457 4.418776035308838 23.19255828857422
Loss :  1.1433302164077759 4.705176830291748 24.669214248657227
Loss :  1.1499662399291992 4.543350696563721 23.866718292236328
Loss :  1.384115219116211 4.44835090637207 23.625869750976562
Loss :  1.3503565788269043 4.384490966796875 23.272811889648438
Loss :  1.4502089023590088 4.811276912689209 25.506593704223633
  batch 40 loss: 1.4502089023590088, 4.811276912689209, 25.506593704223633
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.306970477104187 4.423207759857178 23.423009872436523
Loss :  1.2468734979629517 4.463662624359131 23.565187454223633
Loss :  1.2113044261932373 4.619297504425049 24.307790756225586
Loss :  1.1937165260314941 4.612431526184082 24.25587272644043
Loss :  1.1731101274490356 4.355149745941162 22.94886016845703
Loss :  1.296177864074707 4.607948303222656 24.335918426513672
Loss :  1.3915759325027466 4.475912570953369 23.77113914489746
Loss :  1.2150049209594727 4.604146957397461 24.235740661621094
Loss :  1.4470468759536743 4.637858867645264 24.636341094970703
Loss :  1.2467881441116333 4.439319133758545 23.443384170532227
Loss :  1.348038673400879 4.621537685394287 24.455726623535156
Loss :  1.3883585929870605 4.348968505859375 23.133201599121094
Loss :  1.250974178314209 4.349395275115967 22.99795150756836
Loss :  1.4088518619537354 4.460509777069092 23.711400985717773
Loss :  1.232191801071167 4.452719211578369 23.495786666870117
Loss :  1.4704022407531738 4.714500427246094 25.042903900146484
Loss :  1.2155566215515137 4.225435256958008 22.34273338317871
Loss :  1.1945961713790894 4.352109432220459 22.955142974853516
Loss :  1.2274425029754639 4.232167720794678 22.388280868530273
Loss :  1.4337961673736572 4.224996566772461 22.558778762817383
  batch 60 loss: 1.4337961673736572, 4.224996566772461, 22.558778762817383
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.2008750438690186 4.338831424713135 22.895030975341797
Loss :  1.2714132070541382 4.087075233459473 21.706789016723633
Loss :  1.2566137313842773 4.5016608238220215 23.76491928100586
Loss :  1.2581782341003418 4.351568222045898 23.016019821166992
Loss :  1.1599085330963135 4.151736259460449 21.918590545654297
Loss :  1.8072148561477661 4.475161075592041 24.183019638061523
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.7676458358764648 4.432435035705566 23.929821014404297
Loss :  2.069791078567505 4.346851825714111 23.80405044555664
Loss :  2.2845890522003174 4.260056972503662 23.584875106811523
Total LOSS train 23.62825114910419 valid 23.875441551208496
CE LOSS train 1.2741089985920833 valid 0.5711472630500793
Contrastive LOSS train 4.470828430469219 valid 1.0650142431259155
EPOCH 22:
Loss :  1.387471079826355 4.287102699279785 22.822982788085938
Loss :  1.4316152334213257 4.603273391723633 24.447982788085938
Loss :  1.259637475013733 4.098467826843262 21.751977920532227
Loss :  1.3552895784378052 4.282018184661865 22.765380859375
Loss :  1.3978856801986694 4.369261741638184 23.24419593811035
Loss :  1.3099522590637207 4.391910076141357 23.269502639770508
Loss :  1.3952893018722534 4.6620683670043945 24.705629348754883
Loss :  1.316066861152649 4.327974796295166 22.95594024658203
Loss :  1.2368232011795044 4.571202278137207 24.09283447265625
Loss :  1.4441019296646118 4.305627346038818 22.972238540649414
Loss :  1.2053090333938599 4.343362808227539 22.922122955322266
Loss :  1.1997426748275757 4.0394697189331055 21.397092819213867
Loss :  1.202680230140686 4.210867881774902 22.257020950317383
Loss :  1.2915420532226562 4.228071689605713 22.431900024414062
Loss :  1.422288179397583 4.220015048980713 22.522363662719727
Loss :  1.5360311269760132 4.57919979095459 24.432031631469727
Loss :  1.2810885906219482 4.38100528717041 23.186113357543945
Loss :  1.3709211349487305 4.347806453704834 23.109954833984375
Loss :  1.2333121299743652 4.387682914733887 23.171728134155273
Loss :  1.465624451637268 4.385827541351318 23.39476203918457
  batch 20 loss: 1.465624451637268, 4.385827541351318, 23.39476203918457
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.3340243101119995 4.301533222198486 22.841690063476562
Loss :  1.2740521430969238 4.350991249084473 23.029006958007812
Loss :  1.28757905960083 4.609069347381592 24.33292579650879
Loss :  1.3300060033798218 4.640317916870117 24.53159523010254
Loss :  1.4203530550003052 4.666677474975586 24.753740310668945
Loss :  1.3154683113098145 4.483668804168701 23.73381233215332
Loss :  1.3165501356124878 4.131921291351318 21.97615623474121
Loss :  1.2943341732025146 3.9470913410186768 21.0297908782959
Loss :  1.1257798671722412 4.2728424072265625 22.489992141723633
Loss :  1.4253039360046387 4.46446418762207 23.74762535095215
Loss :  1.1591649055480957 4.4071784019470215 23.195056915283203
Loss :  1.3403385877609253 4.541263103485107 24.046653747558594
Loss :  1.3307181596755981 4.36443567276001 23.152896881103516
Loss :  1.2764780521392822 4.44508171081543 23.50188636779785
Loss :  1.1313928365707397 4.462168216705322 23.44223403930664
Loss :  1.1887801885604858 4.5178093910217285 23.7778263092041
Loss :  1.187753438949585 4.4752607345581055 23.564058303833008
Loss :  1.4383206367492676 4.463433265686035 23.75548553466797
Loss :  1.4046860933303833 4.451486110687256 23.66211700439453
Loss :  1.52305006980896 4.324557781219482 23.14583969116211
  batch 40 loss: 1.52305006980896, 4.324557781219482, 23.14583969116211
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.3202792406082153 4.749663829803467 25.068599700927734
Loss :  1.26278555393219 4.54237174987793 23.97464370727539
Loss :  1.2343333959579468 4.36381196975708 23.05339241027832
Loss :  1.2625691890716553 4.356595993041992 23.045549392700195
Loss :  1.1844929456710815 4.398497104644775 23.176977157592773
Loss :  1.2843317985534668 4.432443618774414 23.446550369262695
Loss :  1.4732469320297241 4.426284313201904 23.60466957092285
Loss :  1.2822645902633667 4.571158409118652 24.138057708740234
Loss :  1.4199591875076294 4.506860256195068 23.954261779785156
Loss :  1.2759207487106323 4.4557366371154785 23.554603576660156
Loss :  1.365675687789917 4.517923831939697 23.955293655395508
Loss :  1.382280945777893 4.370945453643799 23.23700714111328
Loss :  1.315161108970642 4.421167373657227 23.420997619628906
Loss :  1.3821312189102173 4.441840171813965 23.591333389282227
Loss :  1.288716197013855 4.494168758392334 23.759559631347656
Loss :  1.5444697141647339 4.611044406890869 24.59969139099121
Loss :  1.2641466856002808 4.396938800811768 23.24884033203125
Loss :  1.2570419311523438 4.487796306610107 23.69602394104004
Loss :  1.3449262380599976 4.5330281257629395 24.010066986083984
Loss :  1.5176335573196411 4.43433141708374 23.68929100036621
  batch 60 loss: 1.5176335573196411, 4.43433141708374, 23.68929100036621
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.3019789457321167 4.631078720092773 24.457372665405273
Loss :  1.3317973613739014 4.4012250900268555 23.337923049926758
Loss :  1.2662822008132935 4.219442844390869 22.363496780395508
Loss :  1.1884691715240479 4.508813858032227 23.7325382232666
Loss :  1.122882604598999 4.08773136138916 21.561538696289062
Loss :  1.286484956741333 4.331686019897461 22.944915771484375
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.3233577013015747 4.396125316619873 23.303985595703125
Loss :  1.3576231002807617 4.279060363769531 22.752925872802734
Loss :  1.303877353668213 4.231216907501221 22.4599609375
Total LOSS train 23.37296039874737 valid 22.86544704437256
CE LOSS train 1.3187782049179078 valid 0.3259693384170532
Contrastive LOSS train 4.410836428862352 valid 1.0578042268753052
EPOCH 23:
Loss :  1.359082818031311 4.295852184295654 22.83834457397461
Loss :  1.3661221265792847 4.528749465942383 24.009868621826172
Loss :  1.3375215530395508 4.337043285369873 23.02273941040039
Loss :  1.3310431241989136 4.337597370147705 23.01902961730957
Loss :  1.3743140697479248 4.343151569366455 23.090070724487305
Loss :  1.2585805654525757 4.292572021484375 22.7214412689209
Loss :  1.4261269569396973 4.764244556427002 25.24734878540039
Loss :  1.3047236204147339 4.647709846496582 24.543272018432617
Loss :  1.236533284187317 4.403173923492432 23.252403259277344
Loss :  1.3937660455703735 4.441590785980225 23.601720809936523
Loss :  1.2555824518203735 4.581727981567383 24.164222717285156
Loss :  1.2353997230529785 4.541657447814941 23.94368553161621
Loss :  1.2568358182907104 4.5110931396484375 23.812301635742188
Loss :  1.253831148147583 4.596615791320801 24.236909866333008
Loss :  1.476818561553955 4.544920444488525 24.201419830322266
Loss :  1.4330040216445923 4.701278209686279 24.939395904541016
Loss :  1.1890791654586792 4.513823986053467 23.75819969177246
Loss :  1.3353902101516724 4.317323684692383 22.922008514404297
Loss :  1.2146164178848267 4.13172721862793 21.873252868652344
Loss :  1.4494843482971191 4.331878662109375 23.108877182006836
  batch 20 loss: 1.4494843482971191, 4.331878662109375, 23.108877182006836
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.3145664930343628 4.325047016143799 22.939800262451172
Loss :  1.188043475151062 4.543054580688477 23.903316497802734
Loss :  1.26608145236969 4.632057189941406 24.426366806030273
Loss :  1.3819693326950073 4.585448265075684 24.30921173095703
Loss :  1.4886919260025024 4.6561126708984375 24.769254684448242
Loss :  1.2966992855072021 4.412414073944092 23.3587703704834
Loss :  1.2897398471832275 4.5684709548950195 24.13209342956543
Loss :  1.305397391319275 4.376965045928955 23.190221786499023
Loss :  1.140018105506897 4.491497993469238 23.59750747680664
Loss :  1.3816816806793213 4.4154839515686035 23.4591007232666
Loss :  1.0874522924423218 4.551241874694824 23.84366226196289
Loss :  1.3897135257720947 4.633011817932129 24.554771423339844
Loss :  1.2943567037582397 4.3611860275268555 23.10028839111328
Loss :  1.3132221698760986 4.37193489074707 23.172897338867188
Loss :  1.1530424356460571 4.6536970138549805 24.421527862548828
Loss :  1.197927713394165 4.459357738494873 23.49471664428711
Loss :  1.206784963607788 4.377306938171387 23.093320846557617
Loss :  1.5109963417053223 4.486891269683838 23.945451736450195
Loss :  1.5499306917190552 4.334500789642334 23.222434997558594
Loss :  1.5528112649917603 4.562420845031738 24.364913940429688
  batch 40 loss: 1.5528112649917603, 4.562420845031738, 24.364913940429688
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.312942385673523 4.440705299377441 23.516468048095703
Loss :  1.275639533996582 4.650150775909424 24.52639389038086
Loss :  1.254736304283142 4.558036804199219 24.044919967651367
Loss :  1.2975988388061523 4.492271423339844 23.758956909179688
Loss :  1.2472798824310303 4.5494208335876465 23.994384765625
Loss :  1.324129581451416 4.407882213592529 23.363540649414062
Loss :  1.4575304985046387 4.232233047485352 22.618696212768555
Loss :  1.2896136045455933 4.026658535003662 21.42290687561035
Loss :  1.4685165882110596 4.240151405334473 22.669273376464844
Loss :  1.3068265914916992 4.466041088104248 23.63703155517578
Loss :  1.4130736589431763 4.52321195602417 24.02913475036621
Loss :  1.4325445890426636 4.7758331298828125 25.311710357666016
Loss :  1.3389885425567627 4.359328746795654 23.13563346862793
Loss :  1.4674201011657715 4.522124290466309 24.07804298400879
Loss :  1.3168118000030518 4.4764404296875 23.69901466369629
Loss :  1.608715295791626 4.473460674285889 23.97601890563965
Loss :  1.4099160432815552 4.63541841506958 24.587007522583008
Loss :  1.2893733978271484 4.453094482421875 23.554845809936523
Loss :  1.3080281019210815 4.513379096984863 23.874921798706055
Loss :  1.6732912063598633 4.409465312957764 23.720619201660156
  batch 60 loss: 1.6732912063598633, 4.409465312957764, 23.720619201660156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.2932928800582886 4.513747215270996 23.862030029296875
Loss :  1.335777759552002 4.3569769859313965 23.120662689208984
Loss :  1.2995957136154175 4.299582004547119 22.797504425048828
Loss :  1.2593084573745728 4.358645439147949 23.052536010742188
Loss :  1.2022621631622314 4.063705921173096 21.52079200744629
Loss :  1.3285136222839355 4.468594074249268 23.671483993530273
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.362906813621521 4.415492534637451 23.440370559692383
Loss :  1.3220692873001099 4.306821346282959 22.856176376342773
Loss :  1.3272125720977783 4.1408233642578125 22.031330108642578
Total LOSS train 23.62275675260104 valid 22.999840259552002
CE LOSS train 1.3335414868134718 valid 0.3318031430244446
Contrastive LOSS train 4.45784304692195 valid 1.0352058410644531
EPOCH 24:
Loss :  1.3770568370819092 4.27728796005249 22.76349639892578
Loss :  1.4409410953521729 4.4884161949157715 23.88302230834961
Loss :  1.3279235363006592 4.187452793121338 22.265186309814453
Loss :  1.3847205638885498 4.367020606994629 23.21982192993164
Loss :  1.403049349784851 4.10659122467041 21.936004638671875
Loss :  1.264079213142395 4.175448894500732 22.141324996948242
Loss :  1.406319499015808 4.062199592590332 21.717315673828125
Loss :  1.2877697944641113 4.155882358551025 22.067180633544922
Loss :  1.25485098361969 4.27734899520874 22.6415958404541
Loss :  1.3788573741912842 4.097311019897461 21.86541175842285
Loss :  1.2418546676635742 4.255114555358887 22.51742935180664
Loss :  1.2519696950912476 4.33005428314209 22.90224266052246
Loss :  1.2338361740112305 4.252220630645752 22.494937896728516
Loss :  1.2548589706420898 4.207561016082764 22.29266357421875
Loss :  1.4009332656860352 4.227304935455322 22.537456512451172
Loss :  1.424525260925293 4.249152183532715 22.6702880859375
Loss :  1.1951795816421509 4.341022968292236 22.900293350219727
Loss :  1.2712491750717163 4.07990837097168 21.670791625976562
Loss :  1.1734225749969482 3.861332893371582 20.480085372924805
Loss :  1.3594863414764404 3.9172279834747314 20.94562530517578
  batch 20 loss: 1.3594863414764404, 3.9172279834747314, 20.94562530517578
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.26565682888031 4.185917377471924 22.19524383544922
Loss :  1.1571904420852661 3.7913424968719482 20.113903045654297
Loss :  1.2485744953155518 4.5414814949035645 23.955982208251953
Loss :  1.4985443353652954 3.9744725227355957 21.370906829833984
Loss :  1.5854175090789795 4.188205242156982 22.526443481445312
Loss :  1.4263131618499756 4.03121280670166 21.58237648010254
Loss :  1.4316132068634033 4.0262064933776855 21.562646865844727
Loss :  1.4249942302703857 3.791342258453369 20.381704330444336
Loss :  1.060807228088379 4.113296031951904 21.627288818359375
Loss :  1.3270058631896973 3.934231996536255 20.998165130615234
Loss :  1.0871987342834473 4.0229878425598145 21.202136993408203
Loss :  1.2883117198944092 4.038696765899658 21.481794357299805
Loss :  1.2521965503692627 4.188112735748291 22.192760467529297
Loss :  1.273934245109558 3.982689619064331 21.187381744384766
Loss :  1.08820641040802 4.021854400634766 21.197479248046875
Loss :  1.1760221719741821 4.264578342437744 22.498912811279297
Loss :  1.111486792564392 4.001720905303955 21.12009048461914
Loss :  1.2899901866912842 3.7309224605560303 19.94460105895996
Loss :  1.5240150690078735 4.351956367492676 23.283796310424805
Loss :  1.4087462425231934 3.7478156089782715 20.147825241088867
  batch 40 loss: 1.4087462425231934, 3.7478156089782715, 20.147825241088867
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.2299398183822632 3.7409400939941406 19.934640884399414
Loss :  1.257896900177002 3.91076397895813 20.811716079711914
Loss :  1.1786161661148071 4.058523178100586 21.47123146057129
Loss :  1.2861099243164062 4.1762189865112305 22.167205810546875
Loss :  1.165049433708191 3.8201067447662354 20.265583038330078
Loss :  1.3985551595687866 3.788224220275879 20.339675903320312
Loss :  1.6045775413513184 3.9437780380249023 21.323469161987305
Loss :  1.376097559928894 4.186388969421387 22.308042526245117
Loss :  1.6876161098480225 3.9637982845306396 21.506607055664062
Loss :  1.3206838369369507 4.306946754455566 22.855417251586914
Loss :  1.4350336790084839 3.97353458404541 21.302705764770508
Loss :  1.4590445756912231 4.291730880737305 22.917699813842773
Loss :  1.36190927028656 4.3087992668151855 22.905906677246094
Loss :  1.4191405773162842 3.994817018508911 21.393224716186523
Loss :  1.3244564533233643 4.165280342102051 22.15085792541504
Loss :  1.509266972541809 4.381643295288086 23.417484283447266
Loss :  1.3122806549072266 4.204592704772949 22.33524513244629
Loss :  1.2629915475845337 4.287906169891357 22.70252227783203
Loss :  1.3090413808822632 4.127167701721191 21.94487953186035
Loss :  1.499746322631836 4.176754951477051 22.383520126342773
  batch 60 loss: 1.499746322631836, 4.176754951477051, 22.383520126342773
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.2110633850097656 3.9023730754852295 20.722929000854492
Loss :  1.3196654319763184 3.998732089996338 21.313325881958008
Loss :  1.237850308418274 4.269334316253662 22.584522247314453
Loss :  1.2928205728530884 4.68206787109375 24.70315933227539
Loss :  1.1918073892593384 4.058510780334473 21.484359741210938
Loss :  1.3735166788101196 4.314131259918213 22.944171905517578
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.3907133340835571 4.446380138397217 23.62261390686035
Loss :  1.3748607635498047 4.515295505523682 23.951337814331055
Loss :  1.4631743431091309 4.19210958480835 22.423723220825195
Total LOSS train 21.903531470665566 valid 23.235461711883545
CE LOSS train 1.3216980053828313 valid 0.3657935857772827
Contrastive LOSS train 4.116366731203519 valid 1.0480273962020874
EPOCH 25:
Loss :  1.4433085918426514 4.35036039352417 23.195110321044922
Loss :  1.4682849645614624 4.592615127563477 24.431360244750977
Loss :  1.3147310018539429 4.251728057861328 22.57337188720703
Loss :  1.4072185754776 4.430878639221191 23.56161117553711
Loss :  1.4839870929718018 4.26046895980835 22.786333084106445
Loss :  1.260025978088379 4.397070407867432 23.245376586914062
Loss :  1.437137484550476 4.31736421585083 23.023958206176758
Loss :  1.307280421257019 4.267492771148682 22.644742965698242
Loss :  1.326699137687683 4.614522933959961 24.39931297302246
Loss :  1.4762787818908691 4.379415035247803 23.373353958129883
Loss :  1.2552202939987183 4.388298034667969 23.19671058654785
Loss :  1.2853137254714966 4.489371299743652 23.7321720123291
Loss :  1.2665910720825195 4.485771179199219 23.695446014404297
Loss :  1.2855685949325562 4.503035545349121 23.80074691772461
Loss :  1.533205509185791 4.401258945465088 23.539499282836914
Loss :  1.5287556648254395 4.21292781829834 22.593395233154297
Loss :  1.2123031616210938 4.399256229400635 23.20858383178711
Loss :  1.3065322637557983 4.573889255523682 24.17597770690918
Loss :  1.2461655139923096 4.394707679748535 23.219703674316406
Loss :  1.4698724746704102 4.408175945281982 23.510753631591797
  batch 20 loss: 1.4698724746704102, 4.408175945281982, 23.510753631591797
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.3896434307098389 4.436750411987305 23.573394775390625
Loss :  1.2369872331619263 4.386268138885498 23.1683292388916
Loss :  1.285550594329834 4.662360668182373 24.597354888916016
Loss :  1.3624659776687622 4.313995838165283 22.932445526123047
Loss :  1.51730477809906 4.668522357940674 24.85991668701172
Loss :  1.335977554321289 4.526573657989502 23.96884536743164
Loss :  1.3143295049667358 4.6740946769714355 24.684803009033203
Loss :  1.3102939128875732 4.4317474365234375 23.469030380249023
Loss :  1.1068843603134155 4.36640739440918 22.938920974731445
Loss :  1.4633004665374756 4.2173261642456055 22.5499324798584
Loss :  1.1166760921478271 4.146457672119141 21.84896469116211
Loss :  1.3455462455749512 4.357722282409668 23.134159088134766
Loss :  1.3190094232559204 4.255788803100586 22.59795379638672
Loss :  1.3070107698440552 4.479310989379883 23.70356559753418
Loss :  1.1737910509109497 4.038731575012207 21.367448806762695
Loss :  1.216338038444519 4.336309432983398 22.897884368896484
Loss :  1.196201205253601 4.114892959594727 21.770666122436523
Loss :  1.356223225593567 3.9162988662719727 20.93771743774414
Loss :  1.474255919456482 3.8813023567199707 20.880767822265625
Loss :  1.4989982843399048 4.091090202331543 21.954450607299805
  batch 40 loss: 1.4989982843399048, 4.091090202331543, 21.954450607299805
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.323756456375122 4.052209377288818 21.58480453491211
Loss :  1.3741322755813599 4.125298976898193 22.000627517700195
Loss :  1.2358275651931763 4.159145355224609 22.03155517578125
Loss :  1.3488622903823853 4.265451908111572 22.67612075805664
Loss :  1.2060850858688354 3.9863531589508057 21.13785171508789
Loss :  1.4014768600463867 4.037520408630371 21.589080810546875
Loss :  1.5375697612762451 4.059811115264893 21.836624145507812
Loss :  1.2707836627960205 3.7340033054351807 19.9408016204834
Loss :  1.5520457029342651 3.952038049697876 21.312236785888672
Loss :  1.3567378520965576 4.216915607452393 22.441314697265625
Loss :  1.4736483097076416 4.0626702308654785 21.786998748779297
Loss :  1.4647456407546997 4.121082305908203 22.070158004760742
Loss :  1.40901517868042 3.9924356937408447 21.37119483947754
Loss :  1.5301169157028198 4.034570217132568 21.70296859741211
Loss :  1.3295327425003052 4.121899604797363 21.939029693603516
Loss :  1.4850776195526123 4.165078163146973 22.310466766357422
Loss :  1.3317794799804688 4.1955647468566895 22.309602737426758
Loss :  1.301745891571045 3.8116989135742188 20.360240936279297
Loss :  1.3759469985961914 4.129483699798584 22.023365020751953
Loss :  1.6311510801315308 3.7754712104797363 20.508506774902344
  batch 60 loss: 1.6311510801315308, 3.7754712104797363, 20.508506774902344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.389265537261963 3.9368865489959717 21.073698043823242
Loss :  1.4579718112945557 4.068498611450195 21.800464630126953
Loss :  1.3613530397415161 4.2314324378967285 22.51851463317871
Loss :  1.3551082611083984 4.1784772872924805 22.247495651245117
Loss :  1.2700634002685547 3.7250852584838867 19.895490646362305
Loss :  1.418460726737976 4.429656028747559 23.566741943359375
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.4529495239257812 4.383720397949219 23.371551513671875
Loss :  1.4840338230133057 4.216882228851318 22.568445205688477
Loss :  1.4075614213943481 4.33470344543457 23.081079483032227
Total LOSS train 22.55712746840257 valid 23.14695453643799
CE LOSS train 1.3602318121836736 valid 0.35189035534858704
Contrastive LOSS train 4.239379116205069 valid 1.0836758613586426
EPOCH 26:
Loss :  1.498958706855774 3.853834867477417 20.76813316345215
Loss :  1.5214176177978516 4.16676664352417 22.35525131225586
Loss :  1.409923791885376 3.81606388092041 20.49024200439453
Loss :  1.3914316892623901 3.9765403270721436 21.274133682250977
Loss :  1.464005708694458 4.217419147491455 22.551101684570312
Loss :  1.3343690633773804 3.946197271347046 21.06535530090332
Loss :  1.4337952136993408 4.143791198730469 22.152751922607422
Loss :  1.3418927192687988 4.257972240447998 22.63175392150879
Loss :  1.2788348197937012 3.624906301498413 19.403366088867188
Loss :  1.4170295000076294 3.779317617416382 20.313617706298828
Loss :  1.3033822774887085 4.124326705932617 21.925016403198242
Loss :  1.3072330951690674 4.058647632598877 21.60047149658203
Loss :  1.3420188426971436 4.1919846534729 22.30194091796875
Loss :  1.3335425853729248 3.78472900390625 20.257186889648438
Loss :  1.5320128202438354 4.275607585906982 22.910051345825195
Loss :  1.5653996467590332 4.540531635284424 24.268056869506836
Loss :  1.335744023323059 4.409183979034424 23.381664276123047
Loss :  1.3746589422225952 4.245308876037598 22.6012020111084
Loss :  1.3197033405303955 3.9798762798309326 21.219085693359375
Loss :  1.52230703830719 4.264843940734863 22.846525192260742
  batch 20 loss: 1.52230703830719, 4.264843940734863, 22.846525192260742
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.3918801546096802 3.9104256629943848 20.944007873535156
Loss :  1.2623677253723145 4.046847820281982 21.496606826782227
Loss :  1.3197259902954102 4.290626049041748 22.772857666015625
Loss :  1.3790745735168457 3.929288387298584 21.025516510009766
Loss :  1.560550332069397 4.199999809265137 22.560550689697266
Loss :  1.3696985244750977 4.137972354888916 22.059558868408203
Loss :  1.4216545820236206 4.1588640213012695 22.215972900390625
Loss :  1.3914196491241455 4.099277973175049 21.88780975341797
Loss :  1.2191433906555176 4.089475154876709 21.666519165039062
Loss :  1.5523430109024048 3.9349863529205322 21.227275848388672
Loss :  1.1958110332489014 4.263786792755127 22.51474380493164
Loss :  1.4055678844451904 4.399421691894531 23.40267562866211
Loss :  1.3284739255905151 4.536123752593994 24.009092330932617
Loss :  1.3344908952713013 4.125894546508789 21.963964462280273
Loss :  1.1848582029342651 4.2881646156311035 22.625680923461914
Loss :  1.2754582166671753 4.123012065887451 21.890518188476562
Loss :  1.2573919296264648 4.3222737312316895 22.868759155273438
Loss :  1.4545272588729858 4.3802409172058105 23.355731964111328
Loss :  1.5568629503250122 4.397544860839844 23.544588088989258
Loss :  1.5907602310180664 4.327823162078857 23.229877471923828
  batch 40 loss: 1.5907602310180664, 4.327823162078857, 23.229877471923828
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.4407163858413696 4.255490303039551 22.71816635131836
Loss :  1.397057056427002 4.1542253494262695 22.168182373046875
Loss :  1.3346176147460938 3.7761526107788086 20.215381622314453
Loss :  1.4043834209442139 4.2991228103637695 22.899995803833008
Loss :  1.3018909692764282 3.781959056854248 20.211687088012695
Loss :  1.435228943824768 3.934154987335205 21.106002807617188
Loss :  1.5461573600769043 4.11584997177124 22.125408172607422
Loss :  1.3510427474975586 4.0941996574401855 21.822040557861328
Loss :  1.5866937637329102 3.9753644466400146 21.463516235351562
Loss :  1.4320948123931885 4.145089626312256 22.157543182373047
Loss :  1.5440083742141724 4.058685302734375 21.837434768676758
Loss :  1.4690635204315186 3.7766432762145996 20.352279663085938
Loss :  1.4155679941177368 4.076653480529785 21.7988338470459
Loss :  1.5812933444976807 3.5459296703338623 19.310941696166992
Loss :  1.3655678033828735 4.090327262878418 21.81720542907715
Loss :  1.5587785243988037 3.920915365219116 21.163354873657227
Loss :  1.3996984958648682 3.6974055767059326 19.88672637939453
Loss :  1.388796091079712 3.893449544906616 20.856042861938477
Loss :  1.3971372842788696 4.1515583992004395 22.15492820739746
Loss :  1.6049203872680664 3.666168451309204 19.93576431274414
  batch 60 loss: 1.6049203872680664, 3.666168451309204, 19.93576431274414
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.3589673042297363 4.034071445465088 21.52932357788086
Loss :  1.4432286024093628 3.8337290287017822 20.611873626708984
Loss :  1.3609991073608398 3.8667471408843994 20.69473648071289
Loss :  1.3339654207229614 3.892285108566284 20.795391082763672
Loss :  1.2622305154800415 3.3308258056640625 17.916358947753906
Loss :  1.3305774927139282 4.42974328994751 23.479293823242188
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.3808891773223877 4.4098992347717285 23.43038558959961
Loss :  1.374616265296936 4.225059509277344 22.499914169311523
Loss :  1.372552752494812 4.090010643005371 21.822607040405273
Total LOSS train 21.70966670696552 valid 22.80805015563965
CE LOSS train 1.4029819653584408 valid 0.343138188123703
Contrastive LOSS train 4.061336972163274 valid 1.0225026607513428
EPOCH 27:
Loss :  1.483523964881897 3.9347288608551025 21.157169342041016
Loss :  1.5401519536972046 4.251109600067139 22.795700073242188
Loss :  1.4240607023239136 4.18396520614624 22.343887329101562
Loss :  1.4410345554351807 3.6604788303375244 19.74342918395996
Loss :  1.501530408859253 3.837846279144287 20.69076156616211
Loss :  1.3730210065841675 3.974008798599243 21.243064880371094
Loss :  1.4909025430679321 4.150137424468994 22.241588592529297
Loss :  1.3861018419265747 3.744115114212036 20.106678009033203
Loss :  1.3541666269302368 4.003342628479004 21.370878219604492
Loss :  1.4795997142791748 4.146882057189941 22.214008331298828
Loss :  1.3174182176589966 4.178531169891357 22.21007537841797
Loss :  1.3100781440734863 4.212283611297607 22.371496200561523
Loss :  1.3416755199432373 4.137406826019287 22.028709411621094
Loss :  1.349243402481079 3.8631465435028076 20.664976119995117
Loss :  1.5387749671936035 3.7802913188934326 20.440231323242188
Loss :  1.5187047719955444 4.03921365737915 21.714773178100586
Loss :  1.2765653133392334 3.787222385406494 20.212677001953125
Loss :  1.4103238582611084 4.007288932800293 21.44676971435547
Loss :  1.303098201751709 3.5155317783355713 18.88075828552246
Loss :  1.5083523988723755 3.977151393890381 21.39410972595215
  batch 20 loss: 1.5083523988723755, 3.977151393890381, 21.39410972595215
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.3940367698669434 3.979631185531616 21.292192459106445
Loss :  1.3296263217926025 4.4184064865112305 23.421659469604492
Loss :  1.3882805109024048 4.5443010330200195 24.109785079956055
Loss :  1.4941599369049072 3.9081919193267822 21.035120010375977
Loss :  1.5125802755355835 3.9847941398620605 21.436552047729492
Loss :  1.385157823562622 4.467555046081543 23.72293472290039
Loss :  1.4229859113693237 4.148786544799805 22.16691780090332
Loss :  1.429386019706726 3.7897253036499023 20.378013610839844
Loss :  1.2421427965164185 4.129818439483643 21.8912353515625
Loss :  1.5295498371124268 4.257657527923584 22.817838668823242
Loss :  1.22353994846344 3.9817018508911133 21.132047653198242
Loss :  1.470697283744812 3.9054768085479736 20.99808120727539
Loss :  1.384000539779663 3.526454448699951 19.016273498535156
Loss :  1.364261507987976 3.5239052772521973 18.983787536621094
Loss :  1.2301572561264038 3.929151773452759 20.875917434692383
Loss :  1.2669384479522705 4.1716742515563965 22.12531089782715
Loss :  1.271386742591858 4.055805683135986 21.5504150390625
Loss :  1.4980895519256592 3.889465808868408 20.945417404174805
Loss :  1.5005967617034912 3.7324960231781006 20.163076400756836
Loss :  1.5381801128387451 3.3593804836273193 18.335081100463867
  batch 40 loss: 1.5381801128387451, 3.3593804836273193, 18.335081100463867
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.4045226573944092 3.6939330101013184 19.874187469482422
Loss :  1.351193904876709 3.804086923599243 20.37162971496582
Loss :  1.2858843803405762 3.454549551010132 18.558631896972656
Loss :  1.3531947135925293 3.9509246349334717 21.107818603515625
Loss :  1.2554447650909424 3.7694296836853027 20.10259437561035
Loss :  1.3831371068954468 4.168265342712402 22.224464416503906
Loss :  1.516173005104065 4.487433910369873 23.95334243774414
Loss :  1.3113164901733398 4.25199031829834 22.571269989013672
Loss :  1.5594470500946045 4.142040729522705 22.269649505615234
Loss :  1.2998236417770386 3.9582908153533936 21.091278076171875
Loss :  1.4169785976409912 4.223356246948242 22.53376007080078
Loss :  1.4200571775436401 3.7099030017852783 19.969572067260742
Loss :  1.357253074645996 3.9562292098999023 21.13840103149414
Loss :  1.4289411306381226 4.029435157775879 21.57611656188965
Loss :  1.2907130718231201 3.890244483947754 20.741933822631836
Loss :  1.5613882541656494 4.16766881942749 22.39973258972168
Loss :  1.3534572124481201 4.222585678100586 22.466384887695312
Loss :  1.2849516868591309 4.444543361663818 23.50766944885254
Loss :  1.370485782623291 4.403319835662842 23.3870849609375
Loss :  1.5723458528518677 4.368350028991699 23.41409683227539
  batch 60 loss: 1.5723458528518677, 4.368350028991699, 23.41409683227539
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.3286938667297363 4.100882530212402 21.833106994628906
Loss :  1.4048821926116943 4.1120381355285645 21.965072631835938
Loss :  1.3354355096817017 4.027344226837158 21.472156524658203
Loss :  1.263137698173523 4.007773399353027 21.302005767822266
Loss :  1.182145357131958 3.3899199962615967 18.131746292114258
Loss :  1.3390743732452393 4.204665184020996 22.362401962280273
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.372817873954773 4.140870094299316 22.077167510986328
Loss :  1.363065481185913 4.163014888763428 22.17814064025879
Loss :  1.395355224609375 4.075725078582764 21.77398109436035
Total LOSS train 21.378970865102914 valid 22.097922801971436
CE LOSS train 1.3925397946284368 valid 0.34883880615234375
Contrastive LOSS train 3.9972861766815186 valid 1.018931269645691
Saved best model. Old loss 22.588152408599854 and new best loss 22.097922801971436
EPOCH 28:
Loss :  1.4204645156860352 3.7708792686462402 20.274860382080078
Loss :  1.4415810108184814 4.136521339416504 22.124187469482422
Loss :  1.3454691171646118 4.649935245513916 24.595144271850586
Loss :  1.4099658727645874 4.307360649108887 22.94676971435547
Loss :  1.4572699069976807 4.132558345794678 22.12006187438965
Loss :  1.339676022529602 4.078810214996338 21.733726501464844
Loss :  1.5656311511993408 4.050761699676514 21.819440841674805
Loss :  1.4488826990127563 3.843435525894165 20.666061401367188
Loss :  1.3356643915176392 4.0579705238342285 21.625516891479492
Loss :  1.542492151260376 3.4883029460906982 18.984006881713867
Loss :  1.4262722730636597 3.9413084983825684 21.132814407348633
Loss :  1.4041740894317627 4.464005470275879 23.724201202392578
Loss :  1.4207992553710938 4.237210273742676 22.606849670410156
Loss :  1.3940258026123047 4.048309803009033 21.635574340820312
Loss :  1.5652765035629272 4.013028144836426 21.630416870117188
Loss :  1.4303531646728516 4.204866409301758 22.45468521118164
Loss :  1.2421696186065674 4.224005222320557 22.36219596862793
Loss :  1.3011343479156494 4.652800559997559 24.56513786315918
Loss :  1.2929280996322632 4.2854461669921875 22.72015953063965
Loss :  1.5122144222259521 4.42553186416626 23.639873504638672
  batch 20 loss: 1.5122144222259521, 4.42553186416626, 23.639873504638672
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.3281842470169067 4.168401718139648 22.17019271850586
Loss :  1.2877554893493652 3.9732351303100586 21.153932571411133
Loss :  1.3896058797836304 4.341658115386963 23.097896575927734
Loss :  1.434260368347168 4.142301082611084 22.145767211914062
Loss :  1.6175073385238647 4.078564643859863 22.010330200195312
Loss :  1.4362971782684326 3.9428465366363525 21.150529861450195
Loss :  1.4367839097976685 4.196476936340332 22.41916847229004
Loss :  1.3905638456344604 4.027206897735596 21.52659797668457
Loss :  1.1379328966140747 4.264016151428223 22.4580135345459
Loss :  1.4979393482208252 4.058259010314941 21.789234161376953
Loss :  1.1942018270492554 4.208342552185059 22.235916137695312
Loss :  1.4143661260604858 4.141709327697754 22.12291145324707
Loss :  1.3272318840026855 4.044348239898682 21.548973083496094
Loss :  1.2877663373947144 4.00201416015625 21.297836303710938
Loss :  1.2207117080688477 4.042399883270264 21.43271255493164
Loss :  1.2295786142349243 3.909533977508545 20.77724838256836
Loss :  1.1922661066055298 4.337567329406738 22.880102157592773
Loss :  1.5275442600250244 4.254822731018066 22.80165672302246
Loss :  1.5156077146530151 4.123552322387695 22.13336944580078
Loss :  1.5850350856781006 4.088243007659912 22.0262508392334
  batch 40 loss: 1.5850350856781006, 4.088243007659912, 22.0262508392334
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.441081166267395 3.9986841678619385 21.43450355529785
Loss :  1.3682564496994019 4.2936625480651855 22.83656883239746
Loss :  1.2724307775497437 4.351295471191406 23.028907775878906
Loss :  1.2726597785949707 4.23228645324707 22.434091567993164
Loss :  1.1809993982315063 4.120223522186279 21.78211784362793
Loss :  1.2803149223327637 4.128933906555176 21.924983978271484
Loss :  1.4217604398727417 3.9946959018707275 21.395240783691406
Loss :  1.2566145658493042 3.8814215660095215 20.66372299194336
Loss :  1.4765068292617798 4.300967216491699 22.98134422302246
Loss :  1.3062690496444702 4.251864910125732 22.565593719482422
Loss :  1.4138984680175781 4.114235877990723 21.985076904296875
Loss :  1.3848817348480225 4.011809349060059 21.44392967224121
Loss :  1.299762487411499 3.9825527667999268 21.212526321411133
Loss :  1.4248305559158325 3.9273340702056885 21.06150245666504
Loss :  1.3739179372787476 4.186293601989746 22.305387496948242
Loss :  1.6092520952224731 4.346303939819336 23.34077262878418
Loss :  1.3680188655853271 4.429776191711426 23.51689910888672
Loss :  1.2549468278884888 4.309079647064209 22.80034637451172
Loss :  1.355530023574829 4.019066333770752 21.45086097717285
Loss :  1.5467417240142822 3.829585552215576 20.694669723510742
  batch 60 loss: 1.5467417240142822, 3.829585552215576, 20.694669723510742
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 5], device='cuda:0')
Loss :  1.2389986515045166 3.849957227706909 20.488784790039062
Loss :  1.2968015670776367 3.956418037414551 21.07889175415039
Loss :  1.2544887065887451 3.999166250228882 21.25031852722168
Loss :  1.1747685670852661 3.934781789779663 20.848676681518555
Loss :  1.0797061920166016 3.953972816467285 20.84956932067871
Loss :  1.2424099445343018 4.337676525115967 22.93079376220703
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.2655127048492432 4.38072395324707 23.169132232666016
Loss :  1.219570279121399 4.230854511260986 22.373842239379883
Loss :  1.4120911359786987 4.078714847564697 21.8056640625
Total LOSS train 21.967932510375977 valid 22.569858074188232
CE LOSS train 1.370784959426293 valid 0.3530227839946747
Contrastive LOSS train 4.11942949295044 valid 1.0196787118911743
EPOCH 29:
Loss :  1.297823429107666 3.4211432933807373 18.403539657592773
Loss :  1.5112049579620361 3.7781524658203125 20.401968002319336
Loss :  1.3253800868988037 3.913959264755249 20.89517593383789
Loss :  1.342478632926941 3.7700910568237305 20.192934036254883
Loss :  1.401476263999939 4.210227012634277 22.452611923217773
Loss :  1.3096849918365479 3.88095760345459 20.714473724365234
Loss :  1.4441924095153809 4.153563976287842 22.212013244628906
Loss :  1.3521957397460938 4.106739044189453 21.88589096069336
Loss :  1.2540795803070068 4.152246475219727 22.01531219482422
Loss :  1.4727065563201904 4.045407772064209 21.699745178222656
Loss :  1.2449058294296265 3.988255500793457 21.186182022094727
Loss :  1.3078892230987549 4.184563159942627 22.23070526123047
Loss :  1.3691134452819824 3.9072864055633545 20.905546188354492
Loss :  1.403231143951416 3.773329973220825 20.269880294799805
Loss :  1.7030833959579468 4.115089416503906 22.27853012084961
Loss :  1.5117303133010864 4.692111492156982 24.972288131713867
Loss :  1.3146389722824097 4.370663166046143 23.167953491210938
Loss :  1.358136534690857 4.429266452789307 23.50446891784668
Loss :  1.1809138059616089 4.4325456619262695 23.34364128112793
Loss :  1.510290265083313 4.183155059814453 22.42606544494629
  batch 20 loss: 1.510290265083313, 4.183155059814453, 22.42606544494629
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.2990328073501587 4.298544883728027 22.791757583618164
Loss :  1.1707242727279663 4.394158363342285 23.141515731811523
Loss :  1.2340558767318726 4.405359745025635 23.260854721069336
Loss :  1.2860091924667358 4.3347954750061035 22.959985733032227
Loss :  1.4117121696472168 4.586122512817383 24.34232521057129
Loss :  1.2976752519607544 4.485561847686768 23.72548484802246
Loss :  1.529691219329834 4.476844787597656 23.913915634155273
Loss :  1.3384935855865479 4.476926326751709 23.723125457763672
Loss :  1.1327794790267944 4.497593879699707 23.62074851989746
Loss :  1.4021307229995728 4.491078853607178 23.857524871826172
Loss :  1.1666576862335205 4.676469802856445 24.549007415771484
Loss :  1.313955545425415 4.291749000549316 22.7726993560791
Loss :  1.2873625755310059 4.495752811431885 23.76612663269043
Loss :  1.279258370399475 4.231485366821289 22.43668556213379
Loss :  1.1404476165771484 4.338448524475098 22.83268928527832
Loss :  1.2055667638778687 4.040102481842041 21.406078338623047
Loss :  1.154875636100769 4.263150215148926 22.470624923706055
Loss :  1.337826132774353 4.43367862701416 23.50621795654297
Loss :  1.4364001750946045 4.272668361663818 22.799741744995117
Loss :  1.4770146608352661 4.338285446166992 23.168441772460938
  batch 40 loss: 1.4770146608352661, 4.338285446166992, 23.168441772460938
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.2997071743011475 4.114318370819092 21.871299743652344
Loss :  1.27182137966156 3.847996711730957 20.511804580688477
Loss :  1.2313650846481323 4.2348313331604 22.405521392822266
Loss :  1.3236497640609741 4.15768575668335 22.112079620361328
Loss :  1.2026751041412354 3.891136884689331 20.65835952758789
Loss :  1.3544429540634155 3.6974635124206543 19.841760635375977
Loss :  1.4935369491577148 4.210546970367432 22.54627227783203
Loss :  1.251953363418579 3.967301607131958 21.08846092224121
Loss :  1.69552743434906 4.349081516265869 23.440935134887695
Loss :  1.3707480430603027 4.470495700836182 23.72322654724121
Loss :  1.5379445552825928 4.320805072784424 23.141969680786133
Loss :  1.5788272619247437 4.353611469268799 23.34688377380371
Loss :  1.4227322340011597 4.095139980316162 21.8984317779541
Loss :  1.531204342842102 3.695749044418335 20.00994873046875
Loss :  1.266201138496399 4.101263523101807 21.772518157958984
Loss :  1.6363753080368042 4.116411209106445 22.21843147277832
Loss :  1.3138700723648071 3.892138719558716 20.77456283569336
Loss :  1.2174311876296997 4.063704013824463 21.535951614379883
Loss :  1.2908791303634644 4.01991605758667 21.390459060668945
Loss :  1.5333683490753174 4.325918197631836 23.162960052490234
  batch 60 loss: 1.5333683490753174, 4.325918197631836, 23.162960052490234
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.2433640956878662 4.060368061065674 21.545204162597656
Loss :  1.3087266683578491 4.0662665367126465 21.640060424804688
Loss :  1.270308494567871 4.31298303604126 22.835224151611328
Loss :  1.199994683265686 4.168055534362793 22.040273666381836
Loss :  1.1339333057403564 3.8096799850463867 20.182334899902344
Loss :  1.304823398590088 4.383347511291504 23.221559524536133
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.3407062292099 4.485302448272705 23.7672176361084
Loss :  1.2932376861572266 4.331337928771973 22.949926376342773
Loss :  1.3601675033569336 4.097731590270996 21.848827362060547
Total LOSS train 22.24460634084848 valid 22.946882724761963
CE LOSS train 1.346114052259005 valid 0.3400418758392334
Contrastive LOSS train 4.179698467254639 valid 1.024432897567749
EPOCH 30:
Loss :  1.3179385662078857 3.8980607986450195 20.80824089050293
Loss :  1.4747083187103271 3.9913690090179443 21.43155288696289
Loss :  1.3721293210983276 4.137552261352539 22.059890747070312
Loss :  1.3822064399719238 3.781161308288574 20.288013458251953
Loss :  1.4235949516296387 4.0495524406433105 21.671358108520508
Loss :  1.2806602716445923 4.138444900512695 21.972885131835938
Loss :  1.4223982095718384 4.286328315734863 22.85403823852539
Loss :  1.4005943536758423 4.500198841094971 23.901588439941406
Loss :  1.3926537036895752 4.189234733581543 22.338829040527344
Loss :  1.5320205688476562 3.962982416152954 21.346933364868164
Loss :  1.3836355209350586 3.939607858657837 21.081676483154297
Loss :  1.2999526262283325 4.124974727630615 21.924827575683594
Loss :  1.31057608127594 4.2353668212890625 22.487409591674805
Loss :  1.358271598815918 4.13342809677124 22.025413513183594
Loss :  1.5600123405456543 4.280220985412598 22.961116790771484
Loss :  1.5719908475875854 4.124139785766602 22.192689895629883
Loss :  1.3089649677276611 3.872215509414673 20.6700439453125
Loss :  1.4648690223693848 4.0654168128967285 21.79195213317871
Loss :  1.2958824634552002 3.990914821624756 21.250457763671875
Loss :  1.5814642906188965 4.533174514770508 24.247337341308594
  batch 20 loss: 1.5814642906188965, 4.533174514770508, 24.247337341308594
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.3168717622756958 4.026657581329346 21.450159072875977
Loss :  1.2475305795669556 4.039074897766113 21.442903518676758
Loss :  1.2944303750991821 4.552886009216309 24.058860778808594
Loss :  1.340922474861145 4.384458065032959 23.263214111328125
Loss :  1.5489416122436523 4.340632915496826 23.252105712890625
Loss :  1.3931834697723389 4.151610851287842 22.15123748779297
Loss :  1.4594186544418335 4.217293739318848 22.545886993408203
Loss :  1.4095821380615234 4.434366226196289 23.58141326904297
Loss :  1.2664940357208252 4.666652202606201 24.599756240844727
Loss :  1.4820630550384521 4.350823879241943 23.236183166503906
Loss :  1.2652112245559692 4.661277770996094 24.57159996032715
Loss :  1.4640742540359497 4.753255367279053 25.2303524017334
Loss :  1.452498197555542 4.694121360778809 24.923105239868164
Loss :  1.3804188966751099 4.363812446594238 23.199480056762695
Loss :  1.263885736465454 4.311497688293457 22.821372985839844
Loss :  1.319206714630127 4.2991437911987305 22.814926147460938
Loss :  1.2859537601470947 4.325375556945801 22.912830352783203
Loss :  1.5521494150161743 4.3251423835754395 23.177860260009766
Loss :  1.5857652425765991 4.363581657409668 23.403675079345703
Loss :  1.6306438446044922 4.441392421722412 23.83760643005371
  batch 40 loss: 1.6306438446044922, 4.441392421722412, 23.83760643005371
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.362398624420166 4.47135066986084 23.719152450561523
Loss :  1.5382425785064697 4.255273342132568 22.81460952758789
Loss :  1.3890551328659058 4.274683475494385 22.76247215270996
Loss :  1.4481306076049805 4.392370700836182 23.409984588623047
Loss :  1.3533295392990112 4.473599910736084 23.721328735351562
Loss :  1.4551520347595215 3.9200375080108643 21.055339813232422
Loss :  1.5672905445098877 4.198035717010498 22.557470321655273
Loss :  1.3598613739013672 4.358453750610352 23.152130126953125
Loss :  1.5880879163742065 4.312726020812988 23.151716232299805
Loss :  1.3968716859817505 4.259125232696533 22.69249725341797
Loss :  1.5259010791778564 4.266236305236816 22.85708236694336
Loss :  1.4574075937271118 4.217979907989502 22.547306060791016
Loss :  1.4394898414611816 4.094559192657471 21.91228485107422
Loss :  1.5617551803588867 3.978929042816162 21.456401824951172
Loss :  1.3509371280670166 4.202231407165527 22.36209487915039
Loss :  1.5704023838043213 3.9629392623901367 21.385099411010742
Loss :  1.395999789237976 4.227928638458252 22.535642623901367
Loss :  1.3323825597763062 3.9545960426330566 21.105361938476562
Loss :  1.37868332862854 4.124982833862305 22.003597259521484
Loss :  1.5646584033966064 3.977308750152588 21.451202392578125
  batch 60 loss: 1.5646584033966064, 3.977308750152588, 21.451202392578125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.3222427368164062 4.217896938323975 22.411727905273438
Loss :  1.3777211904525757 4.011623859405518 21.435840606689453
Loss :  1.3330045938491821 4.057742118835449 21.621715545654297
Loss :  1.2726107835769653 4.071507930755615 21.630151748657227
Loss :  1.21778404712677 3.7729949951171875 20.082759857177734
Loss :  1.3183306455612183 4.459719181060791 23.616926193237305
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.3703616857528687 4.308725357055664 22.91398811340332
Loss :  1.3697277307510376 4.402096748352051 23.380210876464844
Loss :  1.4060769081115723 4.357213020324707 23.192140579223633
Total LOSS train 22.486396232018105 valid 23.275816440582275
CE LOSS train 1.4100487782404973 valid 0.35151922702789307
Contrastive LOSS train 4.215269466546865 valid 1.0893032550811768
EPOCH 31:
Loss :  1.389642357826233 4.058831691741943 21.683801651000977
Loss :  1.515575647354126 4.071500778198242 21.873079299926758
Loss :  1.347933292388916 3.992082357406616 21.3083438873291
Loss :  1.3902113437652588 4.433933734893799 23.559879302978516
Loss :  1.4447909593582153 4.0173726081848145 21.531654357910156
Loss :  1.2843997478485107 3.9831273555755615 21.200035095214844
Loss :  1.4614250659942627 4.0977959632873535 21.95040512084961
Loss :  1.3719216585159302 4.026838302612305 21.506113052368164
Loss :  1.2957061529159546 3.92830228805542 20.937217712402344
Loss :  1.4463008642196655 3.983938694000244 21.36599349975586
Loss :  1.2911444902420044 4.32022762298584 22.892284393310547
Loss :  1.277307152748108 4.357388496398926 23.06424903869629
Loss :  1.340341329574585 4.289448261260986 22.787582397460938
Loss :  1.2993700504302979 4.331395626068115 22.956348419189453
Loss :  1.5733283758163452 4.508978366851807 24.11821937561035
Loss :  1.6017475128173828 4.382767677307129 23.51558494567871
Loss :  1.2784024477005005 3.9611093997955322 21.08395004272461
Loss :  1.386448860168457 4.3859710693359375 23.316303253173828
Loss :  1.2916659116744995 4.506753921508789 23.825435638427734
Loss :  1.4878923892974854 3.8775389194488525 20.875587463378906
  batch 20 loss: 1.4878923892974854, 3.8775389194488525, 20.875587463378906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.3289988040924072 3.8124399185180664 20.391197204589844
Loss :  1.2469743490219116 4.047964096069336 21.48679542541504
Loss :  1.3115066289901733 4.153103828430176 22.077024459838867
Loss :  1.3923331499099731 3.973757743835449 21.261123657226562
Loss :  1.5202401876449585 4.279823303222656 22.919357299804688
Loss :  1.34592866897583 4.074673652648926 21.719295501708984
Loss :  1.4374605417251587 4.234216690063477 22.608543395996094
Loss :  1.4176098108291626 4.241745471954346 22.6263370513916
Loss :  1.1217198371887207 4.304328918457031 22.64336395263672
Loss :  1.467815637588501 4.342357158660889 23.179601669311523
Loss :  1.155930519104004 4.201808452606201 22.164974212646484
Loss :  1.4354259967803955 4.579178333282471 24.331317901611328
Loss :  1.3058276176452637 4.256097316741943 22.586315155029297
Loss :  1.3089845180511475 4.289891719818115 22.75844383239746
Loss :  1.173340082168579 4.248627662658691 22.41647720336914
Loss :  1.2731400728225708 4.16726016998291 22.109439849853516
Loss :  1.2906765937805176 4.163313865661621 22.10724639892578
Loss :  1.5278555154800415 4.271326065063477 22.884485244750977
Loss :  1.4462534189224243 4.286787509918213 22.880189895629883
Loss :  1.4930580854415894 3.861855983734131 20.802337646484375
  batch 40 loss: 1.4930580854415894, 3.861855983734131, 20.802337646484375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.297810435295105 4.007717132568359 21.336395263671875
Loss :  1.2730246782302856 3.936948537826538 20.957767486572266
Loss :  1.1984678506851196 3.9609363079071045 21.003149032592773
Loss :  1.2917709350585938 3.9017343521118164 20.80044174194336
Loss :  1.1673879623413086 3.8778626918792725 20.55670166015625
Loss :  1.2982338666915894 3.901263952255249 20.80455207824707
Loss :  1.4253509044647217 3.928809881210327 21.069398880004883
Loss :  1.2200793027877808 3.871818780899048 20.579174041748047
Loss :  1.4706515073776245 3.9747326374053955 21.344314575195312
Loss :  1.27144455909729 4.085800647735596 21.70044708251953
Loss :  1.367492437362671 4.152528762817383 22.130136489868164
Loss :  1.3123277425765991 3.9286510944366455 20.955583572387695
Loss :  1.2800438404083252 3.987125873565674 21.215673446655273
Loss :  1.3815444707870483 3.694607734680176 19.854581832885742
Loss :  1.2408889532089233 3.8010404109954834 20.246089935302734
Loss :  1.487593650817871 3.6565909385681152 19.770549774169922
Loss :  1.3224432468414307 4.2106709480285645 22.375797271728516
Loss :  1.217090368270874 3.9358482360839844 20.896331787109375
Loss :  1.3273895978927612 4.071104526519775 21.682910919189453
Loss :  1.5137351751327515 3.9904768466949463 21.46611976623535
  batch 60 loss: 1.5137351751327515, 3.9904768466949463, 21.46611976623535
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.2103734016418457 3.776794195175171 20.094343185424805
Loss :  1.2735892534255981 3.886122703552246 20.704204559326172
Loss :  1.226434350013733 3.849482297897339 20.473846435546875
Loss :  1.169974446296692 3.7905805110931396 20.12287712097168
Loss :  1.0805988311767578 3.8206443786621094 20.183820724487305
Loss :  1.24149751663208 4.155209064483643 22.017541885375977
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.292932152748108 4.214372158050537 22.36479377746582
Loss :  1.2693177461624146 3.981841802597046 21.178525924682617
Loss :  1.2799396514892578 4.073122978210449 21.64555549621582
Total LOSS train 21.748171439537636 valid 21.80160427093506
CE LOSS train 1.340036575610821 valid 0.31998491287231445
Contrastive LOSS train 4.081627005797166 valid 1.0182807445526123
Saved best model. Old loss 22.097922801971436 and new best loss 21.80160427093506
EPOCH 32:
Loss :  1.345447063446045 4.303847789764404 22.864686965942383
Loss :  1.4064462184906006 4.105681419372559 21.93485450744629
Loss :  1.2675976753234863 4.176270008087158 22.14894676208496
Loss :  1.3079752922058105 3.772935390472412 20.172653198242188
Loss :  1.3796664476394653 3.778109550476074 20.27021598815918
Loss :  1.2762317657470703 4.346061706542969 23.006540298461914
Loss :  1.5049444437026978 4.368772029876709 23.348804473876953
Loss :  1.3011655807495117 4.141367435455322 22.00800323486328
Loss :  1.2672775983810425 4.107151031494141 21.80303192138672
Loss :  1.3660542964935303 4.122120380401611 21.976655960083008
Loss :  1.210685133934021 4.4486083984375 23.45372772216797
Loss :  1.2281869649887085 4.459943771362305 23.52790641784668
Loss :  1.2177073955535889 4.0947723388671875 21.69156837463379
Loss :  1.2100697755813599 3.8708159923553467 20.564149856567383
Loss :  1.4842634201049805 3.91880464553833 21.078285217285156
Loss :  1.4398525953292847 4.083436489105225 21.85703468322754
Loss :  1.2277156114578247 4.0228071212768555 21.341753005981445
Loss :  1.3605141639709473 4.177289962768555 22.246963500976562
Loss :  1.1922650337219238 4.345232963562012 22.91843032836914
Loss :  1.394019603729248 3.753471851348877 20.161378860473633
  batch 20 loss: 1.394019603729248, 3.753471851348877, 20.161378860473633
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.2472705841064453 3.6060824394226074 19.27768325805664
Loss :  1.1537752151489258 3.893231153488159 20.619930267333984
Loss :  1.2097927331924438 4.028044700622559 21.350017547607422
Loss :  1.2685222625732422 3.7801506519317627 20.169275283813477
Loss :  1.3942288160324097 4.146687984466553 22.127668380737305
Loss :  1.2155749797821045 4.2250871658325195 22.34100914001465
Loss :  1.2680739164352417 4.3406219482421875 22.97118377685547
Loss :  1.248950719833374 4.198552131652832 22.241710662841797
Loss :  1.1423563957214355 4.147678375244141 21.880748748779297
Loss :  1.48162841796875 4.256555557250977 22.764406204223633
Loss :  1.0842612981796265 4.569075107574463 23.929636001586914
Loss :  1.3920283317565918 4.52249813079834 24.004520416259766
Loss :  1.2308292388916016 4.294921875 22.7054386138916
Loss :  1.3307790756225586 4.267378330230713 22.66767120361328
Loss :  1.1557328701019287 4.054457664489746 21.428022384643555
Loss :  1.229164481163025 4.256717681884766 22.512752532958984
Loss :  1.1757827997207642 4.1805243492126465 22.078405380249023
Loss :  1.4877357482910156 4.349311828613281 23.234294891357422
Loss :  1.5042742490768433 4.19282341003418 22.46839141845703
Loss :  1.5434993505477905 4.3537983894348145 23.312490463256836
  batch 40 loss: 1.5434993505477905, 4.3537983894348145, 23.312490463256836
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.34967839717865 4.489055156707764 23.794954299926758
Loss :  1.246469497680664 4.405942916870117 23.27618408203125
Loss :  1.2194029092788696 4.331080913543701 22.874807357788086
Loss :  1.2719013690948486 4.357130527496338 23.057554244995117
Loss :  1.1975529193878174 4.313220024108887 22.763654708862305
Loss :  1.333596110343933 4.330785274505615 22.98752212524414
Loss :  1.4735815525054932 4.3568549156188965 23.257856369018555
Loss :  1.225600004196167 4.260674953460693 22.528974533081055
Loss :  1.4840805530548096 4.0055460929870605 21.511812210083008
Loss :  1.2650020122528076 4.211545467376709 22.322729110717773
Loss :  1.404723882675171 4.311249732971191 22.96097183227539
Loss :  1.3361046314239502 4.114417552947998 21.908193588256836
Loss :  1.2774430513381958 3.9432992935180664 20.993938446044922
Loss :  1.396281361579895 4.029666423797607 21.544614791870117
Loss :  1.2284780740737915 3.975133180618286 21.104143142700195
Loss :  1.4804086685180664 4.0127854347229 21.544334411621094
Loss :  1.2600325345993042 4.0654096603393555 21.587081909179688
Loss :  1.1977883577346802 4.271186828613281 22.553722381591797
Loss :  1.2602157592773438 4.043509483337402 21.477764129638672
Loss :  1.5283195972442627 4.114147663116455 22.099058151245117
  batch 60 loss: 1.5283195972442627, 4.114147663116455, 22.099058151245117
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.1963601112365723 4.05450963973999 21.468908309936523
Loss :  1.3260778188705444 3.9283838272094727 20.96799659729004
Loss :  1.2180118560791016 4.312716007232666 22.781591415405273
Loss :  1.2031166553497314 4.09188985824585 21.662567138671875
Loss :  1.112170934677124 3.840362071990967 20.313982009887695
Loss :  52.6435546875 4.243708610534668 73.86209869384766
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([3], device='cuda:0')
Loss :  52.80959701538086 4.237220764160156 73.99569702148438
Loss :  51.28144836425781 4.089491367340088 71.7289047241211
Loss :  53.794525146484375 3.9561779499053955 73.5754165649414
Total LOSS train 22.089319463876578 valid 73.29052925109863
CE LOSS train 1.302226864374601 valid 13.448631286621094
Contrastive LOSS train 4.157418493124155 valid 0.9890444874763489
EPOCH 33:
Loss :  1.3913772106170654 3.8688135147094727 20.735443115234375
Loss :  1.4518489837646484 3.9640390872955322 21.272045135498047
Loss :  1.3056690692901611 3.902890920639038 20.82012367248535
Loss :  1.3378562927246094 3.9022247791290283 20.848979949951172
Loss :  1.4040696620941162 3.877662181854248 20.792381286621094
Loss :  1.2515180110931396 4.364105701446533 23.072046279907227
Loss :  1.4104359149932861 3.712425947189331 19.972566604614258
Loss :  1.292869210243225 3.711719512939453 19.85146713256836
Loss :  1.2533514499664307 3.7870194911956787 20.188447952270508
Loss :  1.4117451906204224 3.442866802215576 18.626079559326172
Loss :  1.2177255153656006 3.577063798904419 19.103044509887695
Loss :  1.229433298110962 3.7473511695861816 19.966188430786133
Loss :  1.2215967178344727 3.9825491905212402 21.134342193603516
Loss :  1.2434861660003662 3.9617879390716553 21.052425384521484
Loss :  1.5035889148712158 4.1970906257629395 22.489042282104492
Loss :  1.4623773097991943 4.147161483764648 22.198184967041016
Loss :  1.2043111324310303 4.013684272766113 21.27273178100586
Loss :  1.3317880630493164 4.404049873352051 23.352035522460938
Loss :  1.190745234489441 3.8402912616729736 20.392200469970703
Loss :  1.4530017375946045 4.393481731414795 23.42041015625
  batch 20 loss: 1.4530017375946045, 4.393481731414795, 23.42041015625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.30013108253479 4.156248092651367 22.081371307373047
Loss :  1.176673412322998 4.268592834472656 22.519638061523438
Loss :  1.236606240272522 4.186259746551514 22.167905807495117
Loss :  1.3085086345672607 4.317488193511963 22.89594841003418
Loss :  1.434222936630249 4.53885555267334 24.128501892089844
Loss :  1.2723759412765503 4.400919437408447 23.2769718170166
Loss :  1.3172106742858887 4.657520771026611 24.604814529418945
Loss :  1.296045184135437 4.292574882507324 22.758920669555664
Loss :  1.0783993005752563 4.337516784667969 22.76598358154297
Loss :  1.4148898124694824 4.288019180297852 22.8549861907959
Loss :  1.060866355895996 3.7277145385742188 19.699440002441406
Loss :  1.3359955549240112 4.243264675140381 22.552318572998047
Loss :  1.243823528289795 3.8133466243743896 20.310556411743164
Loss :  1.2126054763793945 3.7936155796051025 20.180683135986328
Loss :  1.0927133560180664 4.105009078979492 21.617759704589844
Loss :  1.141717791557312 3.944361686706543 20.863527297973633
Loss :  1.152884602546692 4.381491661071777 23.060344696044922
Loss :  1.3743997812271118 3.7648732662200928 20.19876480102539
Loss :  1.4201031923294067 4.01688289642334 21.504518508911133
Loss :  1.4469903707504272 3.956549644470215 21.229740142822266
  batch 40 loss: 1.4469903707504272, 3.956549644470215, 21.229740142822266
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.2800544500350952 3.8942742347717285 20.75142478942871
Loss :  1.2782597541809082 3.7670862674713135 20.113691329956055
Loss :  1.2188661098480225 3.9286434650421143 20.862083435058594
Loss :  1.317004919052124 4.150716781616211 22.070589065551758
Loss :  1.1734609603881836 4.031061172485352 21.328765869140625
Loss :  1.4026418924331665 4.231523513793945 22.560258865356445
Loss :  1.4799377918243408 4.1702165603637695 22.33102035522461
Loss :  1.3000775575637817 4.171579837799072 22.157976150512695
Loss :  1.5205790996551514 4.141137599945068 22.226266860961914
Loss :  1.2529575824737549 4.160060405731201 22.053260803222656
Loss :  1.412660837173462 4.630134105682373 24.563331604003906
Loss :  1.365273118019104 4.354526042938232 23.137903213500977
Loss :  1.286376714706421 4.471033573150635 23.641544342041016
Loss :  1.4621944427490234 4.459320545196533 23.75879669189453
Loss :  1.2089117765426636 4.252806663513184 22.472946166992188
Loss :  1.5750291347503662 4.333903789520264 23.244548797607422
Loss :  1.23703932762146 4.0493083000183105 21.48358154296875
Loss :  1.188918948173523 4.405580520629883 23.216821670532227
Loss :  1.301023244857788 4.229947566986084 22.450761795043945
Loss :  1.568741798400879 4.307580947875977 23.106647491455078
  batch 60 loss: 1.568741798400879, 4.307580947875977, 23.106647491455078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.1525561809539795 4.355031490325928 22.92771339416504
Loss :  1.3139277696609497 3.9338252544403076 20.983055114746094
Loss :  1.2980464696884155 4.262964725494385 22.612869262695312
Loss :  1.1499179601669312 4.020351886749268 21.251676559448242
Loss :  1.0616432428359985 3.7039222717285156 19.581254959106445
Loss :  1.2961671352386475 4.352850914001465 23.060422897338867
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.333158016204834 4.539281845092773 24.02956771850586
Loss :  1.3547775745391846 4.0741682052612305 21.72562026977539
Loss :  1.2446094751358032 4.030777454376221 21.398496627807617
Total LOSS train 21.79571803166316 valid 22.553526878356934
CE LOSS train 1.302954759964576 valid 0.3111523687839508
Contrastive LOSS train 4.098552645169772 valid 1.0076943635940552
EPOCH 34:
Loss :  1.3134891986846924 3.7217934131622314 19.922456741333008
Loss :  1.4328285455703735 4.118650436401367 22.026081085205078
Loss :  1.2494207620620728 3.613166570663452 19.31525230407715
Loss :  1.2667853832244873 3.688953399658203 19.711551666259766
Loss :  1.356141209602356 3.729572296142578 20.004003524780273
Loss :  1.214444875717163 3.6568968296051025 19.498929977416992
Loss :  1.4071725606918335 3.8128411769866943 20.471378326416016
Loss :  1.2834450006484985 3.4879508018493652 18.72319984436035
Loss :  1.2221890687942505 3.4945452213287354 18.694915771484375
Loss :  1.379090666770935 3.030219793319702 16.530189514160156
Loss :  1.2136527299880981 3.7584874629974365 20.00609016418457
Loss :  1.2196648120880127 3.619056224822998 19.3149471282959
Loss :  1.1871001720428467 3.5800042152404785 19.087120056152344
Loss :  1.1867260932922363 3.9467508792877197 20.920480728149414
Loss :  1.3944646120071411 3.3339152336120605 18.064041137695312
Loss :  1.4038540124893188 3.6581220626831055 19.69446563720703
Loss :  1.1644296646118164 3.3634064197540283 17.981460571289062
Loss :  1.2915668487548828 3.403695821762085 18.31004524230957
Loss :  1.1417003870010376 3.2813382148742676 17.548391342163086
Loss :  1.415602445602417 3.741865634918213 20.124929428100586
  batch 20 loss: 1.415602445602417, 3.741865634918213, 20.124929428100586
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.2660378217697144 3.742530107498169 19.978687286376953
Loss :  1.147739052772522 3.533752679824829 18.816503524780273
Loss :  1.2253891229629517 3.4729130268096924 18.589954376220703
Loss :  1.284099817276001 3.4185266494750977 18.376731872558594
Loss :  1.412234902381897 3.6901257038116455 19.862863540649414
Loss :  1.2508318424224854 3.3426475524902344 17.964069366455078
Loss :  1.3102775812149048 3.5222790241241455 18.921672821044922
Loss :  1.2670023441314697 3.730198860168457 19.91799545288086
Loss :  1.0794389247894287 3.8511955738067627 20.335416793823242
Loss :  1.3967911005020142 3.7983672618865967 20.388628005981445
Loss :  1.1015138626098633 4.032991409301758 21.26647186279297
Loss :  1.3237210512161255 4.095123767852783 21.799339294433594
Loss :  1.2520300149917603 3.9881527423858643 21.192792892456055
Loss :  1.242737054824829 3.536667823791504 18.926074981689453
Loss :  1.1087590456008911 3.9648308753967285 20.932912826538086
Loss :  1.1841078996658325 3.8129372596740723 20.248794555664062
Loss :  1.1663057804107666 4.1396307945251465 21.864459991455078
Loss :  1.4549667835235596 4.234683036804199 22.62838363647461
Loss :  1.4784868955612183 4.12464714050293 22.101722717285156
Loss :  1.5262106657028198 4.13949728012085 22.223697662353516
  batch 40 loss: 1.5262106657028198, 4.13949728012085, 22.223697662353516
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.3315736055374146 4.379420280456543 23.228675842285156
Loss :  1.2527378797531128 3.792498826980591 20.21523094177246
Loss :  1.1949570178985596 3.9285833835601807 20.837875366210938
Loss :  1.289062738418579 4.59808349609375 24.27947998046875
Loss :  1.181672215461731 4.323337078094482 22.798358917236328
Loss :  1.3467912673950195 4.3844075202941895 23.268829345703125
Loss :  1.45833420753479 4.1111297607421875 22.01398277282715
Loss :  1.2571921348571777 4.294887065887451 22.73162841796875
Loss :  1.5352709293365479 4.123810768127441 22.15432357788086
Loss :  1.250792384147644 4.178409099578857 22.142837524414062
Loss :  1.4400427341461182 4.0868611335754395 21.874347686767578
Loss :  1.4222970008850098 4.3931450843811035 23.38802146911621
Loss :  1.3019665479660034 4.155009746551514 22.077014923095703
Loss :  1.4278756380081177 4.2607951164245605 22.73185157775879
Loss :  1.2358587980270386 3.9910128116607666 21.1909236907959
Loss :  1.4659408330917358 4.006129741668701 21.49658966064453
Loss :  1.261453628540039 3.9465954303741455 20.994430541992188
Loss :  1.1712982654571533 3.710164785385132 19.722122192382812
Loss :  1.2614656686782837 3.6770739555358887 19.646835327148438
Loss :  1.5399718284606934 3.7445759773254395 20.26285171508789
  batch 60 loss: 1.5399718284606934, 3.7445759773254395, 20.26285171508789
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.2462159395217896 3.837918758392334 20.435810089111328
Loss :  1.3727426528930664 3.712040662765503 19.932945251464844
Loss :  1.2741326093673706 4.134909629821777 21.948680877685547
Loss :  1.2376731634140015 3.5122323036193848 18.7988338470459
Loss :  1.1137312650680542 3.381124258041382 18.019351959228516
Loss :  12.82067584991455 4.394373416900635 34.79254150390625
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([5], device='cuda:0')
Loss :  12.391366004943848 4.415826320648193 34.470497131347656
Loss :  12.013678550720215 4.462449073791504 34.325923919677734
Loss :  15.048361778259277 4.286044597625732 36.47858428955078
Total LOSS train 20.438137494600735 valid 35.016886711120605
CE LOSS train 1.293746177966778 valid 3.7620904445648193
Contrastive LOSS train 3.82887826699477 valid 1.071511149406433
EPOCH 35:
Loss :  1.4067195653915405 4.012180805206299 21.467622756958008
Loss :  1.4875102043151855 4.457064151763916 23.772830963134766
Loss :  1.2905666828155518 4.064085960388184 21.610998153686523
Loss :  1.3379552364349365 4.270817279815674 22.692041397094727
Loss :  1.4640498161315918 4.444033145904541 23.684215545654297
Loss :  1.3301061391830444 4.212580680847168 22.39301109313965
Loss :  1.544987678527832 4.251964092254639 22.8048095703125
Loss :  1.4099347591400146 4.086309909820557 21.84148406982422
Loss :  1.343680739402771 4.103930473327637 21.86333465576172
Loss :  1.4518646001815796 4.098787784576416 21.945802688598633
Loss :  1.3083852529525757 4.66943359375 24.655553817749023
Loss :  1.2748534679412842 4.32531213760376 22.901412963867188
Loss :  1.2518483400344849 4.248583793640137 22.494768142700195
Loss :  1.2921476364135742 4.074488162994385 21.664588928222656
Loss :  1.452825665473938 3.8969857692718506 20.937753677368164
Loss :  1.4231231212615967 3.9058332443237305 20.952289581298828
Loss :  1.2082841396331787 3.895394802093506 20.685258865356445
Loss :  1.2858269214630127 3.8655030727386475 20.61334228515625
Loss :  1.1589324474334717 3.7605674266815186 19.961769104003906
Loss :  1.480289340019226 4.329638481140137 23.128482818603516
  batch 20 loss: 1.480289340019226, 4.329638481140137, 23.128482818603516
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.3410028219223022 4.546714782714844 24.07457733154297
Loss :  1.1950433254241943 4.347951889038086 22.934803009033203
Loss :  1.3526406288146973 4.284068584442139 22.77298355102539
Loss :  1.4091120958328247 4.052392482757568 21.67107582092285
Loss :  1.5614511966705322 4.41412878036499 23.632095336914062
Loss :  1.3738195896148682 4.066311836242676 21.70537757873535
Loss :  1.3647701740264893 4.290320873260498 22.816375732421875
Loss :  1.3483680486679077 3.836205244064331 20.529394149780273
Loss :  1.0830051898956299 4.151817321777344 21.842092514038086
Loss :  1.4914803504943848 4.320445537567139 23.093708038330078
Loss :  1.1143147945404053 4.22113037109375 22.219966888427734
Loss :  1.3602120876312256 4.488112449645996 23.8007755279541
Loss :  1.2782036066055298 4.209990501403809 22.328157424926758
Loss :  1.2989728450775146 4.151099681854248 22.054471969604492
Loss :  1.141230583190918 4.171472072601318 21.998592376708984
Loss :  1.197574496269226 4.173960208892822 22.06737518310547
Loss :  1.1917093992233276 4.382936477661133 23.10639190673828
Loss :  1.4049757719039917 4.050577163696289 21.657861709594727
Loss :  1.465031623840332 3.811940908432007 20.524734497070312
Loss :  1.5015692710876465 3.7680253982543945 20.34169578552246
  batch 40 loss: 1.5015692710876465, 3.7680253982543945, 20.34169578552246
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.3403757810592651 4.102935791015625 21.85505485534668
Loss :  1.2756614685058594 3.922532558441162 20.888324737548828
Loss :  1.2453416585922241 4.119516372680664 21.842924118041992
Loss :  1.283854365348816 4.059820175170898 21.58295440673828
Loss :  1.1817258596420288 3.823270559310913 20.298078536987305
Loss :  1.3135898113250732 3.546910285949707 19.048139572143555
Loss :  1.4533991813659668 3.5964839458465576 19.435819625854492
Loss :  1.234508752822876 3.584019184112549 19.154603958129883
Loss :  1.5247076749801636 3.613316059112549 19.59128761291504
Loss :  1.2524534463882446 3.6847994327545166 19.676450729370117
Loss :  1.3994781970977783 3.693859577178955 19.868776321411133
Loss :  1.3703442811965942 3.823152780532837 20.486108779907227
Loss :  1.2931795120239258 3.5091872215270996 18.839115142822266
Loss :  1.4001375436782837 3.841806650161743 20.60917091369629
Loss :  1.2324358224868774 3.755523204803467 20.010051727294922
Loss :  1.497676968574524 3.8655483722686768 20.82541847229004
Loss :  1.2835874557495117 4.075015068054199 21.65866470336914
Loss :  1.2171204090118408 3.75622820854187 19.998262405395508
Loss :  1.2792819738388062 3.9920623302459717 21.239593505859375
Loss :  1.5572490692138672 3.620015859603882 19.65732765197754
  batch 60 loss: 1.5572490692138672, 3.620015859603882, 19.65732765197754
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.2069262266159058 3.654111385345459 19.47748374938965
Loss :  1.3296812772750854 3.62133526802063 19.436357498168945
Loss :  1.2495462894439697 3.393885612487793 18.218975067138672
Loss :  1.1968692541122437 3.7173564434051514 19.78365135192871
Loss :  1.1294511556625366 3.5335683822631836 18.79729461669922
Loss :  1.254712462425232 4.052387714385986 21.516651153564453
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.3068621158599854 4.099739074707031 21.805557250976562
Loss :  1.2899079322814941 3.9426541328430176 21.003177642822266
Loss :  1.3015539646148682 3.9283487796783447 20.94329833984375
Total LOSS train 21.3772887303279 valid 21.317171096801758
CE LOSS train 1.329645586013794 valid 0.32538849115371704
Contrastive LOSS train 4.0095285855806795 valid 0.9820871949195862
Saved best model. Old loss 21.80160427093506 and new best loss 21.317171096801758
EPOCH 36:
Loss :  1.4048396348953247 3.6229937076568604 19.51980972290039
Loss :  1.458081603050232 4.051631450653076 21.71623992919922
Loss :  1.3201128244400024 3.634165048599243 19.490938186645508
Loss :  1.351884126663208 3.690709352493286 19.805431365966797
Loss :  1.4197067022323608 3.6476032733917236 19.65772247314453
Loss :  1.252909779548645 3.5430853366851807 18.968338012695312
Loss :  1.4176381826400757 3.899851083755493 20.916894912719727
Loss :  1.300645351409912 3.588179349899292 19.24154281616211
Loss :  1.2757971286773682 3.512646436691284 18.83902931213379
Loss :  1.4137195348739624 3.620631456375122 19.516876220703125
Loss :  1.2343553304672241 3.7197728157043457 19.833219528198242
Loss :  1.247916340827942 3.73837947845459 19.939815521240234
Loss :  1.2343132495880127 3.406428575515747 18.266456604003906
Loss :  1.2613078355789185 3.7432892322540283 19.977754592895508
Loss :  1.5008594989776611 3.5791239738464355 19.396480560302734
Loss :  1.4563236236572266 3.6984221935272217 19.948434829711914
Loss :  1.2247893810272217 3.545016288757324 18.949871063232422
Loss :  1.3341751098632812 3.5508475303649902 19.08841323852539
Loss :  1.2091916799545288 3.305046319961548 17.734424591064453
Loss :  1.454167127609253 3.5085928440093994 18.99713134765625
  batch 20 loss: 1.454167127609253, 3.5085928440093994, 18.99713134765625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.3095057010650635 3.406510353088379 18.342056274414062
Loss :  1.2094497680664062 3.66426944732666 19.53079605102539
Loss :  1.287439227104187 3.540713310241699 18.99100685119629
Loss :  1.341389536857605 3.477595567703247 18.729366302490234
Loss :  1.4479972124099731 3.771174669265747 20.303871154785156
Loss :  1.2964072227478027 3.5934669971466064 19.263742446899414
Loss :  1.3389612436294556 3.5465216636657715 19.071569442749023
Loss :  1.3044986724853516 3.078988790512085 16.699443817138672
Loss :  1.1180719137191772 3.2306034564971924 17.271089553833008
Loss :  1.4292001724243164 3.7834861278533936 20.346630096435547
Loss :  1.1260576248168945 3.5247461795806885 18.74979019165039
Loss :  1.3782228231430054 3.8503310680389404 20.629878997802734
Loss :  1.2829382419586182 3.730980157852173 19.93783950805664
Loss :  1.2794033288955688 3.445869207382202 18.50874900817871
Loss :  1.151190161705017 3.5867271423339844 19.08482551574707
Loss :  1.2120392322540283 3.131840229034424 16.871240615844727
Loss :  1.199662446975708 3.434807062149048 18.373699188232422
Loss :  1.4287457466125488 3.2282562255859375 17.570026397705078
Loss :  1.4477182626724243 3.4033799171447754 18.464616775512695
Loss :  1.48586905002594 3.2778401374816895 17.87506866455078
  batch 40 loss: 1.48586905002594, 3.2778401374816895, 17.87506866455078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.3253145217895508 3.143949031829834 17.045059204101562
Loss :  1.2809776067733765 3.7243940830230713 19.9029483795166
Loss :  1.2443344593048096 3.302842855453491 17.758548736572266
Loss :  1.2841988801956177 3.259387493133545 17.58113670349121
Loss :  1.2130600214004517 3.21813702583313 17.30374526977539
Loss :  1.3172824382781982 3.3255772590637207 17.945167541503906
Loss :  1.44625985622406 3.211587429046631 17.50419807434082
Loss :  1.2745641469955444 3.327752113342285 17.9133243560791
Loss :  1.502742886543274 3.235177516937256 17.678630828857422
Loss :  1.2934340238571167 3.3489959239959717 18.038414001464844
Loss :  1.408982753753662 3.4272797107696533 18.545381546020508
Loss :  1.3885568380355835 3.4058146476745605 18.417631149291992
Loss :  1.3238179683685303 3.6834399700164795 19.741018295288086
Loss :  1.4111791849136353 3.3878366947174072 18.35036277770996
Loss :  1.2864114046096802 3.3443636894226074 18.008230209350586
Loss :  1.488102912902832 3.173313617706299 17.354671478271484
Loss :  1.3047058582305908 3.597290277481079 19.29115867614746
Loss :  1.2462002038955688 3.1759746074676514 17.126073837280273
Loss :  1.2964038848876953 3.449415922164917 18.54348373413086
Loss :  1.5233218669891357 3.269676685333252 17.8717041015625
  batch 60 loss: 1.5233218669891357, 3.269676685333252, 17.8717041015625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  1.2457574605941772 3.8031504154205322 20.261510848999023
Loss :  1.3486319780349731 3.401402235031128 18.35564422607422
Loss :  1.2809803485870361 3.3960928916931152 18.261445999145508
Loss :  1.2404741048812866 3.5167510509490967 18.824230194091797
Loss :  1.1682918071746826 3.063880681991577 16.487695693969727
Loss :  1.2669867277145386 4.2883148193359375 22.708560943603516
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  1.3086990118026733 4.307574272155762 22.84657096862793
Loss :  1.2914915084838867 4.195258617401123 22.267784118652344
Loss :  1.3124868869781494 4.138267993927002 22.003826141357422
Total LOSS train 18.746639193021334 valid 22.456685543060303
CE LOSS train 1.3229459854272696 valid 0.32812172174453735
Contrastive LOSS train 3.4847385736612173 valid 1.0345669984817505
EPOCH 37:
Loss :  1.3964804410934448 3.1806020736694336 17.299489974975586
Loss :  1.44195556640625 3.5541043281555176 19.21247673034668
Loss :  1.3335143327713013 3.167412519454956 17.170578002929688
Loss :  1.3611903190612793 4.063614368438721 21.679262161254883
Loss :  1.4203574657440186 3.4771950244903564 18.806331634521484
Loss :  1.2875615358352661 3.7589492797851562 20.082307815551758
Loss :  1.423732876777649 3.477617025375366 18.811817169189453
Loss :  1.3242089748382568 3.130403757095337 16.976226806640625
Loss :  1.2946029901504517 3.2595009803771973 17.59210777282715
Loss :  1.4070589542388916 3.210662364959717 17.460371017456055
Loss :  1.2490999698638916 3.9397475719451904 20.947837829589844
Loss :  1.2622413635253906 3.444880723953247 18.486644744873047
Loss :  1.246312141418457 3.6273980140686035 19.38330078125
Loss :  1.2673845291137695 3.539468288421631 18.964725494384766
Loss :  1.477956771850586 3.3153116703033447 18.054515838623047
Loss :  1.4413177967071533 3.6095898151397705 19.489267349243164
Loss :  1.2438709735870361 3.898998498916626 20.73886489868164
Loss :  1.3248529434204102 3.985954761505127 21.254627227783203
Loss :  1.2144590616226196 3.571139335632324 19.07015609741211
Loss :  1.4482526779174805 3.2431299686431885 17.663902282714844
  batch 20 loss: 1.4482526779174805, 3.2431299686431885, 17.663902282714844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.3047963380813599 3.941274881362915 21.011171340942383
Loss :  1.2122596502304077 3.3261830806732178 17.84317398071289
Loss :  1.2841150760650635 3.723475456237793 19.901493072509766
Loss :  1.3286607265472412 3.6821439266204834 19.7393798828125
Loss :  1.4377449750900269 3.7087926864624023 19.981708526611328
Loss :  1.2786904573440552 3.34226131439209 17.98999786376953
Loss :  1.3203189373016357 3.6510932445526123 19.57578468322754
Loss :  1.2889304161071777 3.4512741565704346 18.54530143737793
Loss :  1.1008491516113281 3.864811897277832 20.424907684326172
Loss :  1.4126949310302734 4.0040283203125 21.432836532592773
Loss :  1.1048917770385742 3.5782241821289062 18.996013641357422
Loss :  1.357512354850769 3.570922374725342 19.21212387084961
Loss :  1.2607378959655762 3.43780517578125 18.449764251708984
Loss :  1.2590354681015015 3.3718106746673584 18.118087768554688
Loss :  1.1388723850250244 3.748671531677246 19.882230758666992
Loss :  1.1941285133361816 3.2055251598358154 17.22175407409668
Loss :  1.1859157085418701 3.269922971725464 17.53553009033203
Loss :  1.413928747177124 3.614668369293213 19.48727035522461
Loss :  1.4407345056533813 3.5228281021118164 19.054874420166016
Loss :  1.4747974872589111 3.50991153717041 19.024354934692383
  batch 40 loss: 1.4747974872589111, 3.50991153717041, 19.024354934692383
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.332267165184021 3.3111841678619385 17.8881893157959
Loss :  1.2840360403060913 3.0243427753448486 16.405750274658203
Loss :  1.2507257461547852 3.375880002975464 18.130126953125
Loss :  1.2912830114364624 3.2615537643432617 17.59905242919922
Loss :  1.214558720588684 3.5897791385650635 19.163455963134766
Loss :  1.3179131746292114 3.898128032684326 20.80855369567871
Loss :  1.4428805112838745 3.1545040607452393 17.21540069580078
Loss :  1.264735460281372 3.282886028289795 17.679166793823242
Loss :  1.4953770637512207 3.112976312637329 17.060258865356445
Loss :  1.2838960886001587 3.7854278087615967 20.211034774780273
Loss :  1.3991059064865112 3.3818070888519287 18.30813980102539
Loss :  1.3781330585479736 3.850109100341797 20.628679275512695
Loss :  1.3058158159255981 4.022382736206055 21.4177303314209
Loss :  1.4004806280136108 3.189734697341919 17.349153518676758
Loss :  1.2706464529037476 3.2624030113220215 17.58266258239746
Loss :  1.4767435789108276 3.1690826416015625 17.32215690612793
Loss :  1.277641773223877 3.4061686992645264 18.30848503112793
Loss :  1.2391886711120605 3.2577996253967285 17.528186798095703
Loss :  1.2865811586380005 3.662332057952881 19.598241806030273
Loss :  1.5167748928070068 3.8624227046966553 20.828887939453125
  batch 60 loss: 1.5167748928070068, 3.8624227046966553, 20.828887939453125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.2255526781082153 4.300146579742432 22.726285934448242
Loss :  1.3544197082519531 4.483315944671631 23.770999908447266
Loss :  1.2635107040405273 3.5818517208099365 19.172767639160156
Loss :  1.235308289527893 3.352531909942627 17.997966766357422
Loss :  1.1586135625839233 2.760470151901245 14.96096420288086
Loss :  1.2708089351654053 4.251877784729004 22.530197143554688
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  1.313327431678772 4.3297905921936035 22.9622802734375
Loss :  1.2737314701080322 4.236298561096191 22.455223083496094
Loss :  1.371613621711731 3.9459238052368164 21.101232528686523
Total LOSS train 18.95745952312763 valid 22.2622332572937
CE LOSS train 1.31741872383998 valid 0.34290340542793274
Contrastive LOSS train 3.5280081565563495 valid 0.9864809513092041
EPOCH 38:
Loss :  1.387769341468811 3.1948225498199463 17.361881256103516
Loss :  1.4230690002441406 3.388360023498535 18.3648681640625
Loss :  1.318792462348938 3.146029233932495 17.048938751220703
Loss :  1.3468996286392212 3.681074857711792 19.752273559570312
Loss :  1.4035767316818237 3.674177885055542 19.774465560913086
Loss :  1.2774989604949951 3.633661985397339 19.44580841064453
Loss :  1.4077322483062744 3.3795859813690186 18.305662155151367
Loss :  1.3157742023468018 3.1163575649261475 16.89756202697754
Loss :  1.282246708869934 3.3315365314483643 17.939929962158203
Loss :  1.3913791179656982 3.243643283843994 17.609594345092773
Loss :  1.2523953914642334 3.8069708347320557 20.287250518798828
Loss :  1.2616633176803589 3.626579523086548 19.394561767578125
Loss :  1.2535285949707031 3.2970075607299805 17.738567352294922
Loss :  1.2684415578842163 3.2631983757019043 17.584434509277344
Loss :  1.4550055265426636 3.28621768951416 17.886093139648438
Loss :  1.4481217861175537 3.2590365409851074 17.743305206298828
Loss :  1.2409415245056152 3.2442104816436768 17.461994171142578
Loss :  1.344428539276123 2.9406824111938477 16.047840118408203
Loss :  1.226788878440857 3.183142900466919 17.14250373840332
Loss :  1.4481161832809448 3.575535535812378 19.325794219970703
  batch 20 loss: 1.4481161832809448, 3.575535535812378, 19.325794219970703
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.3115578889846802 3.4056079387664795 18.339597702026367
Loss :  1.2311530113220215 3.202486515045166 17.24358558654785
Loss :  1.30500328540802 3.1314783096313477 16.96239471435547
Loss :  1.3573514223098755 3.220017194747925 17.45743751525879
Loss :  1.460793137550354 3.4561526775360107 18.74155616760254
Loss :  1.3130722045898438 3.0432116985321045 16.529130935668945
Loss :  1.3528903722763062 3.1903231143951416 17.304506301879883
Loss :  1.3235383033752441 2.8830435276031494 15.73875617980957
Loss :  1.1464647054672241 3.4312753677368164 18.302841186523438
Loss :  1.4347554445266724 3.0072715282440186 16.471113204956055
Loss :  1.1455318927764893 3.18715763092041 17.08131980895996
Loss :  1.377015471458435 3.3160293102264404 17.957162857055664
Loss :  1.2877720594406128 3.452155113220215 18.548547744750977
Loss :  1.2841821908950806 3.234844923019409 17.458406448364258
Loss :  1.1656386852264404 3.2716681957244873 17.52397918701172
Loss :  1.2220687866210938 3.0834617614746094 16.63937759399414
Loss :  1.2133896350860596 3.089539051055908 16.66108512878418
Loss :  1.42920982837677 2.9995052814483643 16.42673683166504
Loss :  1.4423236846923828 2.887709617614746 15.880871772766113
Loss :  1.4797745943069458 3.5432522296905518 19.196035385131836
  batch 40 loss: 1.4797745943069458, 3.5432522296905518, 19.196035385131836
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.3398369550704956 3.1100411415100098 16.89004135131836
Loss :  1.2995569705963135 2.816511392593384 15.382113456726074
Loss :  1.2591408491134644 3.3535592555999756 18.026935577392578
Loss :  1.3087480068206787 3.058022975921631 16.59886360168457
Loss :  1.2408840656280518 2.9165585041046143 15.823676109313965
Loss :  1.3443843126296997 3.6741039752960205 19.71490478515625
Loss :  1.4616893529891968 3.799283266067505 20.458105087280273
Loss :  1.2918771505355835 2.8775057792663574 15.67940616607666
Loss :  1.5146360397338867 3.4538142681121826 18.783706665039062
Loss :  1.3214200735092163 2.7511842250823975 15.077341079711914
Loss :  1.419607162475586 3.119969367980957 17.019454956054688
Loss :  1.410720705986023 2.9456675052642822 16.139057159423828
Loss :  1.3470052480697632 3.475088357925415 18.722448348999023
Loss :  1.4348341226577759 3.631904363632202 19.5943546295166
Loss :  1.305851936340332 3.742413282394409 20.01791763305664
Loss :  1.506791591644287 2.886289358139038 15.938238143920898
Loss :  1.3253365755081177 3.211320161819458 17.38193702697754
Loss :  1.2806181907653809 2.8939425945281982 15.75033187866211
Loss :  1.3244880437850952 3.36116623878479 18.130319595336914
Loss :  1.526949167251587 3.1727442741394043 17.390670776367188
  batch 60 loss: 1.526949167251587, 3.1727442741394043, 17.390670776367188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.272109866142273 3.1728827953338623 17.136524200439453
Loss :  1.362273097038269 3.302800178527832 17.876272201538086
Loss :  1.2961722612380981 4.003219127655029 21.31226921081543
Loss :  1.2560516595840454 3.5125582218170166 18.818843841552734
Loss :  1.1842210292816162 2.4757637977600098 13.563039779663086
Loss :  1.4263943433761597 4.137608051300049 22.11443328857422
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.4463224411010742 4.090357780456543 21.898113250732422
Loss :  1.441176176071167 3.955677032470703 21.219560623168945
Loss :  1.5116915702819824 4.095680236816406 21.990093231201172
Total LOSS train 17.643146837674653 valid 21.80555009841919
CE LOSS train 1.3334286267940814 valid 0.3779228925704956
Contrastive LOSS train 3.2619436484116773 valid 1.0239200592041016
EPOCH 39:
Loss :  1.4044431447982788 2.793571949005127 15.372302055358887
Loss :  1.4476532936096191 3.1863796710968018 17.37955093383789
Loss :  1.3502908945083618 2.6423919200897217 14.562251091003418
Loss :  1.3793338537216187 3.3110768795013428 17.934717178344727
Loss :  1.4363161325454712 2.8650786876678467 15.761710166931152
Loss :  1.3210514783859253 2.5390868186950684 14.016486167907715
Loss :  1.4424208402633667 3.062666893005371 16.755754470825195
Loss :  1.3579437732696533 2.67533278465271 14.734607696533203
Loss :  1.3310438394546509 3.0137438774108887 16.399763107299805
Loss :  1.4332128763198853 2.9785547256469727 16.325986862182617
Loss :  1.2924864292144775 3.1065428256988525 16.8252010345459
Loss :  1.3001155853271484 3.049046039581299 16.545345306396484
Loss :  1.2900725603103638 3.019275665283203 16.386451721191406
Loss :  1.309844732284546 2.7127976417541504 14.873832702636719
Loss :  1.4958919286727905 3.1114211082458496 17.052997589111328
Loss :  1.4701085090637207 3.123656988143921 17.088394165039062
Loss :  1.2925106287002563 3.358725070953369 18.086135864257812
Loss :  1.3726105690002441 2.807675838470459 15.410989761352539
Loss :  1.2803455591201782 2.942535877227783 15.993024826049805
Loss :  1.4755991697311401 2.7020716667175293 14.985958099365234
  batch 20 loss: 1.4755991697311401, 2.7020716667175293, 14.985958099365234
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.3548128604888916 2.8095552921295166 15.402588844299316
Loss :  1.2731765508651733 2.718400001525879 14.8651762008667
Loss :  1.3296197652816772 2.5720529556274414 14.189884185791016
Loss :  1.3755125999450684 2.546889066696167 14.10995864868164
Loss :  1.4679921865463257 2.6130669116973877 14.533326148986816
Loss :  1.33574640750885 2.468700885772705 13.679250717163086
Loss :  1.3689285516738892 2.973653554916382 16.23719596862793
Loss :  1.3525782823562622 2.8489158153533936 15.59715747833252
Loss :  1.200931191444397 2.98630690574646 16.132465362548828
Loss :  1.4589557647705078 3.162126064300537 17.26958656311035
Loss :  1.2060242891311646 2.925204038619995 15.83204460144043
Loss :  1.412157654762268 2.6544032096862793 14.684174537658691
Loss :  1.3328274488449097 2.7415926456451416 15.040790557861328
Loss :  1.337188482284546 2.874763011932373 15.711004257202148
Loss :  1.238599419593811 2.8914363384246826 15.695781707763672
Loss :  1.283892273902893 2.941882610321045 15.993306159973145
Loss :  1.2733135223388672 2.8097939491271973 15.322282791137695
Loss :  1.4533956050872803 2.824253797531128 15.574665069580078
Loss :  1.4674991369247437 2.5214476585388184 14.074737548828125
Loss :  1.495564341545105 2.70292329788208 15.010180473327637
  batch 40 loss: 1.495564341545105, 2.70292329788208, 15.010180473327637
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  1.3802040815353394 3.045279026031494 16.606597900390625
Loss :  1.3422744274139404 3.0189149379730225 16.43684959411621
Loss :  1.3195652961730957 3.15031099319458 17.07111930847168
Loss :  1.3506137132644653 3.3196661472320557 17.948945999145508
Loss :  1.2891308069229126 3.14768123626709 17.027536392211914
Loss :  1.378891110420227 2.9731829166412354 16.24480628967285
Loss :  1.4808510541915894 2.469125509262085 13.826478958129883
Loss :  1.3280874490737915 2.308751344680786 12.871844291687012
Loss :  1.5247397422790527 3.0299811363220215 16.674646377563477
Loss :  1.3485177755355835 2.862922430038452 15.663129806518555
Loss :  1.4486726522445679 2.765902280807495 15.278183937072754
Loss :  1.434326410293579 3.1816017627716064 17.342334747314453
Loss :  1.3777109384536743 2.7404935359954834 15.080179214477539
Loss :  1.4562735557556152 2.9044077396392822 15.978311538696289
Loss :  1.347799301147461 2.870692729949951 15.701263427734375
Loss :  1.5178478956222534 2.9145619869232178 16.09065818786621
Loss :  1.3638455867767334 2.8809661865234375 15.7686767578125
Loss :  1.3227522373199463 2.7254958152770996 14.950231552124023
Loss :  1.369320273399353 3.0909149646759033 16.823894500732422
Loss :  1.54495108127594 2.6215627193450928 14.652765274047852
  batch 60 loss: 1.54495108127594, 2.6215627193450928, 14.652765274047852
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.3212212324142456 3.065951108932495 16.650976181030273
Loss :  1.405578851699829 2.8131296634674072 15.471226692199707
Loss :  1.3503414392471313 2.381554365158081 13.258113861083984
Loss :  1.3230012655258179 2.883989095687866 15.74294662475586
Loss :  1.2632813453674316 2.5484611988067627 14.005586624145508
Loss :  1.3769419193267822 4.404572010040283 23.39980125427246
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.4014464616775513 4.413232326507568 23.467609405517578
Loss :  1.3944753408432007 4.279050827026367 22.789730072021484
Loss :  1.4124102592468262 4.1161274909973145 21.9930477142334
Total LOSS train 15.701758810190054 valid 22.91254711151123
CE LOSS train 1.3706432562607986 valid 0.35310256481170654
Contrastive LOSS train 2.8662231041834905 valid 1.0290318727493286
EPOCH 40:
Loss :  1.444779634475708 2.734590768814087 15.117733001708984
Loss :  1.4784774780273438 3.2555549144744873 17.75625228881836
Loss :  1.400011658668518 3.2959465980529785 17.879743576049805
Loss :  1.4267805814743042 2.811871290206909 15.486137390136719
Loss :  1.4663482904434204 2.806882619857788 15.500761985778809
Loss :  1.3721325397491455 2.3543052673339844 13.143658638000488
Loss :  1.470605492591858 2.784388303756714 15.392547607421875
Loss :  1.400355339050293 3.181276559829712 17.306737899780273
Loss :  1.3842480182647705 2.6172587871551514 14.470541954040527
Loss :  1.4701176881790161 2.656552314758301 14.75287914276123
Loss :  1.3486676216125488 2.7292802333831787 14.99506950378418
Loss :  1.3548628091812134 3.225447416305542 17.482099533081055
Loss :  1.348265290260315 3.1297080516815186 16.99680519104004
Loss :  1.3585363626480103 3.128988027572632 17.003477096557617
Loss :  1.5190718173980713 3.082292079925537 16.930532455444336
Loss :  1.4983993768692017 3.0838301181793213 16.91754913330078
Loss :  1.3485599756240845 3.038238525390625 16.539752960205078
Loss :  1.4136314392089844 2.4977481365203857 13.902372360229492
Loss :  1.3351328372955322 2.847872018814087 15.574492454528809
Loss :  1.5052965879440308 2.7665092945098877 15.33784294128418
  batch 20 loss: 1.5052965879440308, 2.7665092945098877, 15.33784294128418
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
Loss :  1.3983728885650635 2.8965399265289307 15.881072044372559
Loss :  1.3355909585952759 3.521538257598877 18.943281173706055
Loss :  1.3886135816574097 2.741142511367798 15.09432601928711
Loss :  1.4268920421600342 3.1282002925872803 17.067893981933594
Loss :  1.4995099306106567 3.1769840717315674 17.384429931640625
Loss :  1.3933519124984741 2.852553367614746 15.656118392944336
Loss :  1.423808217048645 2.9662184715270996 16.254901885986328
Loss :  1.4046412706375122 3.5332531929016113 19.070907592773438
Loss :  1.2715352773666382 2.6799633502960205 14.671351432800293
Loss :  1.4907804727554321 3.4260740280151367 18.621150970458984
Loss :  1.271876573562622 4.445479393005371 23.49927520751953
Loss :  1.4438798427581787 3.639662265777588 19.64219093322754
Loss :  1.3804762363433838 4.214404106140137 22.452497482299805
Loss :  1.3826879262924194 3.7125778198242188 19.94557762145996
Loss :  1.2906768321990967 3.814988374710083 20.365617752075195
Loss :  1.3209102153778076 4.1587982177734375 22.114900588989258
Loss :  1.3129770755767822 3.861760139465332 20.621776580810547
Loss :  1.4765658378601074 3.555063009262085 19.251880645751953
Loss :  1.4809777736663818 3.4836678504943848 18.899316787719727
Loss :  1.5191503763198853 3.868785858154297 20.863079071044922
  batch 40 loss: 1.5191503763198853, 3.868785858154297, 20.863079071044922
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  1.4189645051956177 3.895453929901123 20.8962345123291
Loss :  1.377287745475769 3.502310037612915 18.888837814331055
Loss :  1.3456724882125854 3.6576273441314697 19.63381004333496
Loss :  1.3707175254821777 2.8806638717651367 15.774036407470703
Loss :  1.310484528541565 3.1949081420898438 17.285024642944336
Loss :  1.3936563730239868 3.799947500228882 20.39339256286621
Loss :  1.4903804063796997 2.7781789302825928 15.381275177001953
Loss :  1.3477628231048584 3.7738351821899414 20.216938018798828
Loss :  1.5333740711212158 3.2790110111236572 17.928430557250977
Loss :  1.356026530265808 3.283240556716919 17.772228240966797
Loss :  1.4491469860076904 3.2489593029022217 17.69394302368164
Loss :  1.4264531135559082 3.4454665184020996 18.653785705566406
Loss :  1.3717502355575562 4.2378668785095215 22.561084747314453
Loss :  1.4409570693969727 3.820862054824829 20.545269012451172
Loss :  1.3298375606536865 3.2181448936462402 17.420562744140625
Loss :  1.5050607919692993 3.128394365310669 17.147031784057617
Loss :  1.3446768522262573 3.3373239040374756 18.031295776367188
Loss :  1.3080147504806519 3.293161153793335 17.773818969726562
Loss :  1.3418985605239868 3.4785659313201904 18.73472785949707
Loss :  1.5198404788970947 3.268932580947876 17.864503860473633
  batch 60 loss: 1.5198404788970947, 3.268932580947876, 17.864503860473633
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  1.2886356115341187 3.2841763496398926 17.709516525268555
Loss :  1.373661756515503 2.9838454723358154 16.292888641357422
Loss :  1.3114731311798096 3.385040044784546 18.23667335510254
Loss :  1.2842286825180054 3.5194571018218994 18.881515502929688
Loss :  1.2230356931686401 2.583704948425293 14.141560554504395
Loss :  1.3732613325119019 3.8017046451568604 20.381784439086914
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.4055596590042114 3.7218668460845947 20.014894485473633
Loss :  1.3899953365325928 3.6862573623657227 19.82128143310547
Loss :  1.4440441131591797 3.56221079826355 19.255098342895508
Total LOSS train 17.70226029616136 valid 19.86826467514038
CE LOSS train 1.3957008361816405 valid 0.3610110282897949
Contrastive LOSS train 3.2613119052006647 valid 0.8905526995658875
Saved best model. Old loss 21.317171096801758 and new best loss 19.86826467514038
EPOCH 41:
Loss :  1.4169331789016724 2.9265639781951904 16.049753189086914
Loss :  1.4468746185302734 3.400855541229248 18.451152801513672
Loss :  1.35892653465271 3.0114121437072754 16.415987014770508
Loss :  1.3845083713531494 3.2685117721557617 17.727067947387695
Loss :  1.4248265027999878 3.0001943111419678 16.425798416137695
Loss :  1.3225244283676147 2.9793035984039307 16.21904182434082
Loss :  1.4330976009368896 3.0942165851593018 16.9041805267334
Loss :  1.35798978805542 3.1630990505218506 17.173484802246094
Loss :  1.3329516649246216 3.2908856868743896 17.78738021850586
Loss :  1.4319812059402466 3.067875862121582 16.771360397338867
Loss :  1.302711844444275 3.4401872158050537 18.503646850585938
Loss :  1.3053340911865234 3.1537110805511475 17.073890686035156
Loss :  1.30315101146698 3.015036106109619 16.37833023071289
Loss :  1.3215874433517456 3.1110877990722656 16.877025604248047
Loss :  1.4875555038452148 3.0058979988098145 16.517044067382812
Loss :  1.4631463289260864 3.115675449371338 17.041522979736328
Loss :  1.2967145442962646 3.4323766231536865 18.45859718322754
Loss :  1.3678689002990723 3.221409320831299 17.47491455078125
Loss :  1.2906825542449951 2.9384186267852783 15.982775688171387
Loss :  1.4730523824691772 2.8012382984161377 15.479243278503418
  batch 20 loss: 1.4730523824691772, 2.8012382984161377, 15.479243278503418
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  1.370278239250183 3.3169546127319336 17.95505142211914
Loss :  1.2935781478881836 3.598046064376831 19.283809661865234
Loss :  1.344974398612976 2.8092892169952393 15.391420364379883
Loss :  1.392159342765808 2.9195525646209717 15.989922523498535
Loss :  1.4723734855651855 2.9822397232055664 16.38357162475586
Loss :  1.3521591424942017 2.8257830142974854 15.481074333190918
Loss :  1.3816776275634766 3.1385600566864014 17.074478149414062
Loss :  1.3683077096939087 3.1046547889709473 16.89158058166504
Loss :  1.2337411642074585 3.6160497665405273 19.31399154663086
Loss :  1.4648542404174805 3.3439674377441406 18.1846923828125
Loss :  1.2385177612304688 2.9451074600219727 15.964055061340332
Loss :  1.4210147857666016 3.084601402282715 16.84402084350586
Loss :  1.3566434383392334 3.141484022140503 17.064064025878906
Loss :  1.3588696718215942 2.7577738761901855 15.14773941040039
Loss :  1.272292971611023 2.7733473777770996 15.139030456542969
Loss :  1.3145315647125244 2.966679334640503 16.14792823791504
Loss :  1.310292363166809 3.0643393993377686 16.631990432739258
Loss :  1.4713013172149658 2.8354218006134033 15.648409843444824
Loss :  1.4852802753448486 3.3789072036743164 18.37981605529785
Loss :  1.5114490985870361 3.1481549739837646 17.25222396850586
  batch 40 loss: 1.5114490985870361, 3.1481549739837646, 17.25222396850586
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.40867280960083 3.4683616161346436 18.75048065185547
Loss :  1.3781814575195312 2.7248971462249756 15.002667427062988
Loss :  1.3515468835830688 3.0141079425811768 16.422086715698242
Loss :  1.381608009338379 2.8895676136016846 15.829445838928223
Loss :  1.3277901411056519 3.3241078853607178 17.948328018188477
Loss :  1.4047495126724243 3.2619550228118896 17.714523315429688
Loss :  1.4944747686386108 2.83201003074646 15.654524803161621
Loss :  1.3655306100845337 3.0155463218688965 16.443262100219727
Loss :  1.5275696516036987 2.9491636753082275 16.273387908935547
Loss :  1.3772358894348145 3.2619574069976807 17.687023162841797
Loss :  1.461567997932434 2.7972567081451416 15.447851181030273
Loss :  1.448878288269043 2.760671854019165 15.252237319946289
Loss :  1.3975980281829834 3.254005193710327 17.66762351989746
Loss :  1.4674283266067505 2.7338361740112305 15.136609077453613
Loss :  1.3719676733016968 2.672783136367798 14.735882759094238
Loss :  1.528246521949768 3.506167411804199 19.059083938598633
Loss :  1.3857203722000122 2.758697509765625 15.179207801818848
Loss :  1.351536512374878 4.339292526245117 23.047998428344727
Loss :  1.3898746967315674 2.962653160095215 16.203140258789062
Loss :  1.543393850326538 2.6757795810699463 14.92229175567627
  batch 60 loss: 1.543393850326538, 2.6757795810699463, 14.92229175567627
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  1.3519184589385986 2.9299638271331787 16.001737594604492
Loss :  1.4317947626113892 2.7334437370300293 15.099014282226562
Loss :  1.3786766529083252 4.345606803894043 23.106712341308594
Loss :  1.3640559911727905 3.7502145767211914 20.115127563476562
Loss :  1.3143573999404907 2.6843948364257812 14.736331939697266
Loss :  1.4290409088134766 4.248939514160156 22.673738479614258
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.461985468864441 4.2515034675598145 22.719501495361328
Loss :  1.450956106185913 4.044878959655762 21.675352096557617
Loss :  1.4644992351531982 4.107334613800049 22.001171112060547
Total LOSS train 16.912579213655913 valid 22.267440795898438
CE LOSS train 1.3867860078811645 valid 0.36612480878829956
Contrastive LOSS train 3.1051586591280422 valid 1.0268336534500122
EPOCH 42:
Loss :  1.4733854532241821 3.654350996017456 19.745140075683594
Loss :  1.4991817474365234 3.5110366344451904 19.054365158081055
Loss :  1.424646258354187 3.1423609256744385 17.136449813842773
Loss :  1.4418129920959473 3.36350679397583 18.25934600830078
Loss :  1.477518081665039 3.345024347305298 18.202640533447266
Loss :  1.3783502578735352 3.548884868621826 19.12277603149414
Loss :  1.4833279848098755 3.3581340312957764 18.273998260498047
Loss :  1.410513162612915 3.0325214862823486 16.5731201171875
Loss :  1.3957300186157227 3.066981554031372 16.730636596679688
Loss :  1.488156795501709 3.1086456775665283 17.03138542175293
Loss :  1.3646790981292725 3.076439380645752 16.746875762939453
Loss :  1.364665150642395 3.1729607582092285 17.229469299316406
Loss :  1.3689498901367188 3.734703302383423 20.04246711730957
Loss :  1.3791234493255615 3.4001176357269287 18.379711151123047
Loss :  1.5200828313827515 3.9398577213287354 21.219371795654297
Loss :  1.4961186647415161 3.198944091796875 17.4908390045166
Loss :  1.3605749607086182 3.0626111030578613 16.673629760742188
Loss :  1.4242324829101562 3.1323752403259277 17.086109161376953
Loss :  1.3505209684371948 3.094827175140381 16.824657440185547
Loss :  1.5207502841949463 3.0613291263580322 16.827396392822266
  batch 20 loss: 1.5207502841949463, 3.0613291263580322, 16.827396392822266
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  1.4171348810195923 3.083035707473755 16.832313537597656
Loss :  1.3442007303237915 3.1592886447906494 17.140644073486328
Loss :  1.39545738697052 3.267183542251587 17.73137664794922
Loss :  1.4319087266921997 3.4440512657165527 18.65216636657715
Loss :  1.5026544332504272 3.1979641914367676 17.492475509643555
Loss :  1.3965733051300049 2.8512375354766846 15.65276050567627
Loss :  1.424798607826233 2.877969980239868 15.814648628234863
Loss :  1.4166362285614014 2.966230869293213 16.24778938293457
Loss :  1.2871770858764648 2.707045078277588 14.822402000427246
Loss :  1.5046685934066772 3.163456916809082 17.32195281982422
Loss :  1.29390287399292 3.1890146732330322 17.238975524902344
Loss :  1.4640415906906128 2.9816219806671143 16.372150421142578
Loss :  1.3981504440307617 2.8064591884613037 15.43044662475586
Loss :  1.4011118412017822 3.3413429260253906 18.107826232910156
Loss :  1.314751148223877 3.361557960510254 18.122539520263672
Loss :  1.3501250743865967 3.372925281524658 18.214750289916992
Loss :  1.3467142581939697 3.1821393966674805 17.25741195678711
Loss :  1.4885951280593872 3.0646417140960693 16.811803817749023
Loss :  1.501443862915039 3.204538583755493 17.524137496948242
Loss :  1.5235732793807983 3.2330586910247803 17.688865661621094
  batch 40 loss: 1.5235732793807983, 3.2330586910247803, 17.688865661621094
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.4330952167510986 2.8691070079803467 15.778630256652832
Loss :  1.4024018049240112 2.850329637527466 15.65404987335205
Loss :  1.3781317472457886 2.9421892166137695 16.08907699584961
Loss :  1.405256748199463 2.7307448387145996 15.058980941772461
Loss :  1.3494857549667358 2.5186240673065186 13.942605972290039
Loss :  1.429123878479004 3.4910905361175537 18.88457489013672
Loss :  1.516011118888855 3.3390488624572754 18.211254119873047
Loss :  1.388139009475708 3.5438334941864014 19.10730743408203
Loss :  1.5467908382415771 2.7281339168548584 15.187460899353027
Loss :  1.3922094106674194 2.894582986831665 15.865123748779297
Loss :  1.4801522493362427 3.2759978771209717 17.86014175415039
Loss :  1.456779956817627 2.8910574913024902 15.912067413330078
Loss :  1.4081416130065918 2.6519668102264404 14.667976379394531
Loss :  1.4755616188049316 3.5035526752471924 18.993324279785156
Loss :  1.3857645988464355 3.0654118061065674 16.71282386779785
Loss :  1.5334250926971436 3.288010835647583 17.973478317260742
Loss :  1.3945813179016113 3.4507932662963867 18.648548126220703
Loss :  1.3617299795150757 3.1019606590270996 16.87153434753418
Loss :  1.3933866024017334 3.880455255508423 20.795663833618164
Loss :  1.5540117025375366 3.4604873657226562 18.856449127197266
  batch 60 loss: 1.5540117025375366, 3.4604873657226562, 18.856449127197266
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.3525192737579346 2.998135805130005 16.343198776245117
Loss :  1.4334901571273804 3.033620595932007 16.601593017578125
Loss :  1.381770372390747 3.234832286834717 17.555932998657227
Loss :  1.3561735153198242 3.7188873291015625 19.950611114501953
Loss :  1.2990552186965942 2.6760244369506836 14.679177284240723
Loss :  1.3920559883117676 3.5735671520233154 19.259891510009766
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.4257124662399292 3.4767720699310303 18.809572219848633
Loss :  1.4209688901901245 3.4187796115875244 18.514867782592773
Loss :  1.421978235244751 3.364546060562134 18.244709014892578
Total LOSS train 17.312760118337778 valid 18.707260131835938
CE LOSS train 1.420509673998906 valid 0.35549455881118774
Contrastive LOSS train 3.1784500929025503 valid 0.8411365151405334
Saved best model. Old loss 19.86826467514038 and new best loss 18.707260131835938
EPOCH 43:
Loss :  1.4585868120193481 2.6949093341827393 14.933133125305176
Loss :  1.4882735013961792 3.584681272506714 19.411680221557617
Loss :  1.4153772592544556 3.170116662979126 17.265960693359375
Loss :  1.4358192682266235 3.0634961128234863 16.753299713134766
Loss :  1.4725369215011597 3.1483123302459717 17.21409797668457
Loss :  1.3746951818466187 2.5623481273651123 14.18643569946289
Loss :  1.4760206937789917 3.123225688934326 17.09214973449707
Loss :  1.404867172241211 2.6051974296569824 14.430854797363281
Loss :  1.387794852256775 2.8989145755767822 15.882367134094238
Loss :  1.4708359241485596 2.588346004486084 14.412566184997559
Loss :  1.3512954711914062 3.1093668937683105 16.898130416870117
Loss :  1.3507697582244873 2.579686403274536 14.249201774597168
Loss :  1.3487242460250854 3.3993453979492188 18.34545135498047
Loss :  1.3659303188323975 2.7655394077301025 15.19362735748291
Loss :  1.5188994407653809 2.700822591781616 15.023012161254883
Loss :  1.4846407175064087 3.0279672145843506 16.62447738647461
Loss :  1.3472906351089478 3.4475276470184326 18.584928512573242
Loss :  1.4062126874923706 3.1854286193847656 17.333354949951172
Loss :  1.3341500759124756 2.3763935565948486 13.216117858886719
Loss :  1.5080456733703613 3.417372703552246 18.59490966796875
  batch 20 loss: 1.5080456733703613, 3.417372703552246, 18.59490966796875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.4076625108718872 2.934492826461792 16.080127716064453
Loss :  1.3349887132644653 3.005730390548706 16.3636417388916
Loss :  1.3819963932037354 2.968461513519287 16.22430419921875
Loss :  1.41483736038208 3.094757080078125 16.888622283935547
Loss :  1.4886995553970337 3.0406224727630615 16.69181251525879
Loss :  1.3820464611053467 2.969496250152588 16.22952651977539
Loss :  1.412330150604248 3.038522243499756 16.604942321777344
Loss :  1.4024368524551392 3.1911227703094482 17.358051300048828
Loss :  1.2699050903320312 3.1323163509368896 16.931488037109375
Loss :  1.4864479303359985 2.665783405303955 14.815364837646484
Loss :  1.2683504819869995 3.6999170780181885 19.76793670654297
Loss :  1.441976547241211 3.3913540840148926 18.398746490478516
Loss :  1.3757729530334473 3.0942986011505127 16.847265243530273
Loss :  1.3745601177215576 3.3343114852905273 18.046117782592773
Loss :  1.2932418584823608 3.630262851715088 19.444555282592773
Loss :  1.3333814144134521 3.2827770709991455 17.74726676940918
Loss :  1.332136869430542 2.995976448059082 16.31201934814453
Loss :  1.4751230478286743 2.911129951477051 16.030773162841797
Loss :  1.492424488067627 3.47048282623291 18.844837188720703
Loss :  1.5161364078521729 2.8078906536102295 15.55558967590332
  batch 40 loss: 1.5161364078521729, 2.8078906536102295, 15.55558967590332
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.4177746772766113 2.887051820755005 15.853033065795898
Loss :  1.3825626373291016 3.145375967025757 17.10944366455078
Loss :  1.3596235513687134 3.0118913650512695 16.41908073425293
Loss :  1.385111689567566 3.079726457595825 16.783742904663086
Loss :  1.335065484046936 2.695427417755127 14.812202453613281
Loss :  1.4086543321609497 2.835066080093384 15.583984375
Loss :  1.496367335319519 2.619457721710205 14.593655586242676
Loss :  1.3729114532470703 2.950591802597046 16.125869750976562
Loss :  1.5388636589050293 2.8441741466522217 15.759735107421875
Loss :  1.3803857564926147 2.4774892330169678 13.767831802368164
Loss :  1.4744952917099 2.6916537284851074 14.932764053344727
Loss :  1.4533345699310303 3.0571272373199463 16.738969802856445
Loss :  1.4045699834823608 3.1044538021087646 16.92683982849121
Loss :  1.4751728773117065 3.189814805984497 17.424245834350586
Loss :  1.3796744346618652 3.306593418121338 17.912641525268555
Loss :  1.5357800722122192 3.048792839050293 16.77974510192871
Loss :  1.3945213556289673 3.2517244815826416 17.65314483642578
Loss :  1.3632752895355225 3.1709530353546143 17.218040466308594
Loss :  1.3935798406600952 3.1946074962615967 17.36661720275879
Loss :  1.5485483407974243 3.361015558242798 18.353626251220703
  batch 60 loss: 1.5485483407974243, 3.361015558242798, 18.353626251220703
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.3542463779449463 2.8819832801818848 15.764162063598633
Loss :  1.4287608861923218 2.786878824234009 15.363154411315918
Loss :  1.377405047416687 3.33915638923645 18.07318687438965
Loss :  1.3517974615097046 2.8985936641693115 15.844765663146973
Loss :  1.3016945123672485 2.4298667907714844 13.451028823852539
Loss :  1.3683182001113892 3.892409086227417 20.830364227294922
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.4053093194961548 3.913835287094116 20.974485397338867
Loss :  1.4004738330841064 3.799018383026123 20.395566940307617
Loss :  1.3821134567260742 3.7015016078948975 19.88962173461914
Total LOSS train 16.5144655080942 valid 20.522509574890137
CE LOSS train 1.409221518956698 valid 0.34552836418151855
Contrastive LOSS train 3.0210487952599157 valid 0.9253754019737244
EPOCH 44:
Loss :  1.4626792669296265 3.027043104171753 16.5978946685791
Loss :  1.4899026155471802 3.4293053150177 18.636428833007812
Loss :  1.4215971231460571 2.9972469806671143 16.4078311920166
Loss :  1.4429755210876465 2.812401533126831 15.504983901977539
Loss :  1.4810417890548706 3.1592931747436523 17.277507781982422
Loss :  1.386008381843567 2.9008309841156006 15.89016342163086
Loss :  1.4875819683074951 2.9288887977600098 16.13202476501465
Loss :  1.4190845489501953 2.4447147846221924 13.642658233642578
Loss :  1.401257038116455 2.9889955520629883 16.346235275268555
Loss :  1.4788926839828491 2.8136603832244873 15.547194480895996
Loss :  1.369417428970337 3.1411843299865723 17.07533836364746
Loss :  1.3709897994995117 3.121570587158203 16.978843688964844
Loss :  1.3708429336547852 3.0073845386505127 16.407764434814453
Loss :  1.3880726099014282 2.9304399490356445 16.040271759033203
Loss :  1.532023549079895 2.9571123123168945 16.317584991455078
Loss :  1.5000814199447632 3.190849542617798 17.454328536987305
Loss :  1.3710675239562988 3.0811784267425537 16.776960372924805
Loss :  1.4263373613357544 3.0744118690490723 16.798397064208984
Loss :  1.3555785417556763 2.81893253326416 15.450241088867188
Loss :  1.521837592124939 2.6808462142944336 14.926068305969238
  batch 20 loss: 1.521837592124939, 2.6808462142944336, 14.926068305969238
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.429010272026062 2.7942352294921875 15.400186538696289
Loss :  1.3600363731384277 3.3628153800964355 18.174114227294922
Loss :  1.4051803350448608 2.6634271144866943 14.722315788269043
Loss :  1.442255973815918 3.045973300933838 16.672122955322266
Loss :  1.509482979774475 2.9035768508911133 16.027366638183594
Loss :  1.4097429513931274 3.0611183643341064 16.715333938598633
Loss :  1.437965989112854 2.695826292037964 14.917098045349121
Loss :  1.4296787977218628 3.6657474040985107 19.75841522216797
Loss :  1.3116722106933594 3.177882194519043 17.20108413696289
Loss :  1.508004069328308 2.922760248184204 16.12180519104004
Loss :  1.317937970161438 2.995767831802368 16.296777725219727
Loss :  1.4709455966949463 3.2172038555145264 17.556964874267578
Loss :  1.4131220579147339 3.0795671939849854 16.810956954956055
Loss :  1.4095808267593384 2.661634922027588 14.717755317687988
Loss :  1.331939935684204 2.7902348041534424 15.283113479614258
Loss :  1.3662376403808594 3.0135960578918457 16.43421745300293
Loss :  1.3653738498687744 2.947360038757324 16.102174758911133
Loss :  1.497780680656433 2.801849842071533 15.50702953338623
Loss :  1.515296459197998 2.9309792518615723 16.17019271850586
Loss :  1.5336099863052368 2.772324800491333 15.395234107971191
  batch 40 loss: 1.5336099863052368, 2.772324800491333, 15.395234107971191
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.4418342113494873 3.0508244037628174 16.695955276489258
Loss :  1.4076908826828003 2.7896790504455566 15.356085777282715
Loss :  1.387306571006775 3.013592004776001 16.45526695251465
Loss :  1.4084676504135132 2.702413320541382 14.920534133911133
Loss :  1.3670135736465454 2.670680046081543 14.720414161682129
Loss :  1.435914397239685 2.7459912300109863 15.16586971282959
Loss :  1.5183519124984741 2.7235755920410156 15.136229515075684
Loss :  1.4060331583023071 3.1644811630249023 17.228439331054688
Loss :  1.5528903007507324 3.1982815265655518 17.54429817199707
Loss :  1.4147077798843384 2.886841297149658 15.84891414642334
Loss :  1.497150182723999 2.6262929439544678 14.628615379333496
Loss :  1.4802510738372803 2.6632893085479736 14.796697616577148
Loss :  1.4358500242233276 3.0465762615203857 16.668731689453125
Loss :  1.4967411756515503 3.1951491832733154 17.47248649597168
Loss :  1.4097514152526855 3.155754566192627 17.18852424621582
Loss :  1.550569772720337 3.0310168266296387 16.70565414428711
Loss :  1.4205682277679443 2.818551778793335 15.513327598571777
Loss :  1.391482949256897 2.7698912620544434 15.24094009399414
Loss :  1.4167753458023071 2.7687268257141113 15.260409355163574
Loss :  1.561539649963379 2.9536352157592773 16.329715728759766
  batch 60 loss: 1.561539649963379, 2.9536352157592773, 16.329715728759766
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.3813999891281128 2.9869577884674072 16.31618881225586
Loss :  1.4502925872802734 2.903284788131714 15.966716766357422
Loss :  1.400016188621521 3.036179780960083 16.580915451049805
Loss :  1.3766850233078003 2.9125871658325195 15.939620971679688
Loss :  1.328064203262329 2.533926486968994 13.997695922851562
Loss :  1.3883579969406128 4.2498979568481445 22.637845993041992
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.4255269765853882 4.303411483764648 22.942584991455078
Loss :  1.41469407081604 4.188212871551514 22.355758666992188
Loss :  1.40543532371521 4.149033069610596 22.15060043334961
Total LOSS train 16.151865049508903 valid 22.521697521209717
CE LOSS train 1.4319918907605684 valid 0.3513588309288025
Contrastive LOSS train 2.9439746416532078 valid 1.037258267402649
EPOCH 45:
Loss :  1.480406403541565 2.7294490337371826 15.127652168273926
Loss :  1.5065512657165527 3.5980818271636963 19.49696159362793
Loss :  1.4397251605987549 3.392057180404663 18.40001106262207
Loss :  1.4617160558700562 3.0165908336639404 16.54467010498047
Loss :  1.494376540184021 2.7593119144439697 15.290935516357422
Loss :  1.4155198335647583 2.6543824672698975 14.687432289123535
Loss :  1.4994055032730103 3.073009490966797 16.864452362060547
Loss :  1.4366645812988281 2.7364022731781006 15.11867618560791
Loss :  1.4215710163116455 2.9313929080963135 16.078535079956055
Loss :  1.4974796772003174 3.0938034057617188 16.96649742126465
Loss :  1.392801284790039 3.4759457111358643 18.77252960205078
Loss :  1.3901256322860718 3.285033702850342 17.81529426574707
Loss :  1.3835078477859497 3.2589025497436523 17.678022384643555
Loss :  1.3997998237609863 3.4466917514801025 18.633258819580078
Loss :  1.5417979955673218 3.1548728942871094 17.316162109375
Loss :  1.5053651332855225 3.3745150566101074 18.377941131591797
Loss :  1.3719710111618042 3.14680814743042 17.10601234436035
Loss :  1.4214441776275635 3.1772921085357666 17.307905197143555
Loss :  1.3580485582351685 3.400897741317749 18.362537384033203
Loss :  1.512787103652954 3.5333125591278076 19.179349899291992
  batch 20 loss: 1.512787103652954, 3.5333125591278076, 19.179349899291992
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.4171509742736816 2.963662624359131 16.235464096069336
Loss :  1.3522201776504517 3.4498727321624756 18.60158348083496
Loss :  1.394927978515625 3.08107852935791 16.80031967163086
Loss :  1.4249327182769775 3.4044885635375977 18.44737434387207
Loss :  1.4962071180343628 3.2354612350463867 17.673513412475586
Loss :  1.38429856300354 3.1849942207336426 17.309268951416016
Loss :  1.4080913066864014 3.725794553756714 20.037063598632812
Loss :  1.398842215538025 3.146008253097534 17.128883361816406
Loss :  1.2616571187973022 3.468914031982422 18.60622787475586
Loss :  1.4831637144088745 3.026005506515503 16.613191604614258
Loss :  1.2628813982009888 3.2199082374572754 17.362422943115234
Loss :  1.436030387878418 3.54019832611084 19.13702392578125
Loss :  1.3700474500656128 3.2509005069732666 17.624549865722656
Loss :  1.3668304681777954 2.7606873512268066 15.170267105102539
Loss :  1.2819045782089233 2.976245403289795 16.163131713867188
Loss :  1.3181580305099487 3.2256886959075928 17.44659996032715
Loss :  1.319714069366455 3.4705708026885986 18.67256736755371
Loss :  1.4689264297485352 3.5216217041015625 19.07703399658203
Loss :  1.4870158433914185 3.248501777648926 17.729524612426758
Loss :  1.515324354171753 2.858067035675049 15.805659294128418
  batch 40 loss: 1.515324354171753, 2.858067035675049, 15.805659294128418
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.4051481485366821 2.956779956817627 16.18904685974121
Loss :  1.3679797649383545 2.9822075366973877 16.279016494750977
Loss :  1.3426764011383057 3.3520009517669678 18.102680206298828
Loss :  1.3673561811447144 3.241286039352417 17.57378578186035
Loss :  1.3100666999816895 3.690048933029175 19.760311126708984
Loss :  1.3887253999710083 3.7250571250915527 20.01401138305664
Loss :  1.481533169746399 4.0910115242004395 21.93659019470215
Loss :  1.344423770904541 3.684455156326294 19.766698837280273
Loss :  1.5264493227005005 4.061750411987305 21.835201263427734
Loss :  1.3526368141174316 3.8721792697906494 20.713533401489258
Loss :  1.4525272846221924 3.681377649307251 19.859416961669922
Loss :  1.4327373504638672 3.672691583633423 19.79619598388672
Loss :  1.3789756298065186 3.9759631156921387 21.258790969848633
Loss :  1.4647160768508911 3.7920725345611572 20.425079345703125
Loss :  1.342025876045227 3.7203595638275146 19.943822860717773
Loss :  1.5184690952301025 3.9245026111602783 21.140981674194336
Loss :  1.3527631759643555 3.1869537830352783 17.287532806396484
Loss :  1.3077787160873413 3.2657978534698486 17.636768341064453
Loss :  1.3472951650619507 3.414641857147217 18.42050552368164
Loss :  1.5413011312484741 3.133443593978882 17.208518981933594
  batch 60 loss: 1.5413011312484741, 3.133443593978882, 17.208518981933594
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.3005284070968628 3.1608636379241943 17.104846954345703
Loss :  1.3937634229660034 3.3577616214752197 18.182571411132812
Loss :  1.3240532875061035 2.9000604152679443 15.824356079101562
Loss :  1.2965055704116821 3.112952470779419 16.86126708984375
Loss :  1.2313294410705566 2.346953868865967 12.96609878540039
Loss :  1.4807997941970825 3.8651819229125977 20.80670928955078
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.524002194404602 3.794794797897339 20.497976303100586
Loss :  1.5195516347885132 3.7599267959594727 20.319185256958008
Loss :  1.5230085849761963 3.7968382835388184 20.507200241088867
Total LOSS train 17.89006365262545 valid 20.53276777267456
CE LOSS train 1.4054023816035344 valid 0.3807521462440491
Contrastive LOSS train 3.296932257138766 valid 0.9492095708847046
EPOCH 46:
Loss :  1.4249862432479858 3.2940566539764404 17.8952693939209
Loss :  1.468401551246643 3.168804168701172 17.312421798706055
Loss :  1.3776780366897583 3.0378520488739014 16.566938400268555
Loss :  1.4046354293823242 3.12518310546875 17.03055191040039
Loss :  1.4465899467468262 2.949640989303589 16.194795608520508
Loss :  1.3350768089294434 2.827204942703247 15.471101760864258
Loss :  1.4546384811401367 3.436445951461792 18.63686752319336
Loss :  1.3738586902618408 3.8396198749542236 20.571958541870117
Loss :  1.3540995121002197 2.93113112449646 16.009756088256836
Loss :  1.445504069328308 2.9501941204071045 16.196474075317383
Loss :  1.317879557609558 3.1463356018066406 17.049556732177734
Loss :  1.320784568786621 2.9389376640319824 16.015472412109375
Loss :  1.315635323524475 3.1499135494232178 17.065202713012695
Loss :  1.3377803564071655 3.514892816543579 18.91224479675293
Loss :  1.5068960189819336 3.348702907562256 18.250411987304688
Loss :  1.4768245220184326 3.7448551654815674 20.201099395751953
Loss :  1.3185601234436035 3.406830310821533 18.352710723876953
Loss :  1.3933748006820679 3.539090633392334 19.088829040527344
Loss :  1.3109545707702637 3.0830934047698975 16.726421356201172
Loss :  1.507573127746582 3.433298349380493 18.67406463623047
  batch 20 loss: 1.507573127746582, 3.433298349380493, 18.67406463623047
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.3955003023147583 3.130829095840454 17.049646377563477
Loss :  1.3193496465682983 3.675931692123413 19.699007034301758
Loss :  1.3710969686508179 3.418386697769165 18.463031768798828
Loss :  1.407112956047058 3.1453874111175537 17.134050369262695
Loss :  1.4899369478225708 3.192995309829712 17.454914093017578
Loss :  1.3665231466293335 3.6947712898254395 19.84037971496582
Loss :  1.3969792127609253 3.2871744632720947 17.83285140991211
Loss :  1.3855726718902588 2.8395683765411377 15.583414077758789
Loss :  1.2374359369277954 3.2342536449432373 17.40870475769043
Loss :  1.481676697731018 3.1861627101898193 17.412490844726562
Loss :  1.2398602962493896 3.2207961082458496 17.343841552734375
Loss :  1.4319703578948975 3.6997337341308594 19.930639266967773
Loss :  1.3608472347259521 3.1794803142547607 17.258249282836914
Loss :  1.3547029495239258 2.943387508392334 16.071640014648438
Loss :  1.2601709365844727 3.0250332355499268 16.385337829589844
Loss :  1.3048020601272583 3.3207669258117676 17.90863609313965
Loss :  1.3021987676620483 3.1943085193634033 17.273740768432617
Loss :  1.4725958108901978 3.2488176822662354 17.716684341430664
Loss :  1.4967705011367798 3.2758939266204834 17.876239776611328
Loss :  1.5224806070327759 2.902472972869873 16.03484535217285
  batch 40 loss: 1.5224806070327759, 2.902472972869873, 16.03484535217285
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.409712791442871 3.0704360008239746 16.76189422607422
Loss :  1.3704980611801147 2.932992696762085 16.03546142578125
Loss :  1.3435384035110474 3.1406641006469727 17.046859741210938
Loss :  1.368847370147705 2.965397596359253 16.19583511352539
Loss :  1.3175382614135742 3.054187059402466 16.58847427368164
Loss :  1.4028891324996948 3.0002684593200684 16.404232025146484
Loss :  1.5045700073242188 3.6852517127990723 19.930828094482422
Loss :  1.3643561601638794 2.9139153957366943 15.93393325805664
Loss :  1.5511506795883179 3.297820568084717 18.040254592895508
Loss :  1.3749076128005981 3.3115274906158447 17.932546615600586
Loss :  1.4749994277954102 3.2667205333709717 17.80860137939453
Loss :  1.4520882368087769 3.3622210025787354 18.263193130493164
Loss :  1.3942950963974 3.1136696338653564 16.962642669677734
Loss :  1.4654574394226074 3.085106372833252 16.890989303588867
Loss :  1.35994553565979 2.9342291355133057 16.031091690063477
Loss :  1.5365999937057495 3.030336856842041 16.688283920288086
Loss :  1.3751100301742554 3.348306179046631 18.116641998291016
Loss :  1.3421226739883423 3.2513835430145264 17.599040985107422
Loss :  1.3724631071090698 3.452671766281128 18.635822296142578
Loss :  1.551485300064087 3.649815797805786 19.80056381225586
  batch 60 loss: 1.551485300064087, 3.649815797805786, 19.80056381225586
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.331659197807312 3.454552412033081 18.604421615600586
Loss :  1.4189774990081787 3.3586246967315674 18.212100982666016
Loss :  1.359932541847229 3.5331993103027344 19.025928497314453
Loss :  1.3306972980499268 2.759596824645996 15.128681182861328
Loss :  1.271604299545288 2.5860371589660645 14.201789855957031
Loss :  1.3798489570617676 4.280087471008301 22.780284881591797
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.416722297668457 4.358368873596191 23.20856475830078
Loss :  1.4202665090560913 4.1738691329956055 22.289613723754883
Loss :  1.4050288200378418 4.3520708084106445 23.165382385253906
Total LOSS train 17.48831702012282 valid 22.860961437225342
CE LOSS train 1.392842490856464 valid 0.35125720500946045
Contrastive LOSS train 3.2190948816446157 valid 1.0880177021026611
EPOCH 47:
Loss :  1.4584064483642578 3.367530584335327 18.296058654785156
Loss :  1.4949054718017578 3.6182122230529785 19.585966110229492
Loss :  1.407301902770996 3.7186503410339355 20.000553131103516
Loss :  1.4332947731018066 3.544872999191284 19.15765953063965
Loss :  1.4745451211929321 3.112907648086548 17.03908348083496
Loss :  1.370100975036621 2.910344362258911 15.921822547912598
Loss :  1.4792686700820923 3.7173874378204346 20.066205978393555
Loss :  1.398358702659607 3.4366095066070557 18.58140754699707
Loss :  1.3736375570297241 2.6548352241516113 14.647812843322754
Loss :  1.4544193744659424 2.9614787101745605 16.26181411743164
Loss :  1.3374593257904053 3.585726022720337 19.266090393066406
Loss :  1.3373589515686035 3.309627056121826 17.885494232177734
Loss :  1.331291675567627 3.083940029144287 16.750991821289062
Loss :  1.349879264831543 3.0422232151031494 16.56099510192871
Loss :  1.519486665725708 3.622147798538208 19.630226135253906
Loss :  1.4832992553710938 2.931739568710327 16.141998291015625
Loss :  1.3200485706329346 3.0816032886505127 16.728065490722656
Loss :  1.3870714902877808 3.1302053928375244 17.038097381591797
Loss :  1.302403450012207 2.695504903793335 14.779928207397461
Loss :  1.4986889362335205 3.374267816543579 18.37002944946289
  batch 20 loss: 1.4986889362335205, 3.374267816543579, 18.37002944946289
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.3816436529159546 2.970088481903076 16.232086181640625
Loss :  1.2982581853866577 3.0717012882232666 16.65676498413086
Loss :  1.3529298305511475 3.0365023612976074 16.535442352294922
Loss :  1.3957048654556274 2.839427947998047 15.59284496307373
Loss :  1.4783744812011719 3.2688400745391846 17.822574615478516
Loss :  1.3551608324050903 2.965874433517456 16.184534072875977
Loss :  1.3848159313201904 3.383512258529663 18.30237579345703
Loss :  1.3786994218826294 2.9836432933807373 16.296916961669922
Loss :  1.2217209339141846 3.210395574569702 17.273698806762695
Loss :  1.480261206626892 3.0889666080474854 16.925094604492188
Loss :  1.225424885749817 3.1285693645477295 16.86827278137207
Loss :  1.4321045875549316 2.9445295333862305 16.154752731323242
Loss :  1.3607796430587769 3.072232484817505 16.721940994262695
Loss :  1.3574817180633545 3.2927563190460205 17.82126235961914
Loss :  1.2559244632720947 3.4104573726654053 18.308210372924805
Loss :  1.3025188446044922 3.0880701541900635 16.742870330810547
Loss :  1.301476001739502 3.0655181407928467 16.629066467285156
Loss :  1.4684404134750366 2.9026236534118652 15.981558799743652
Loss :  1.4883148670196533 2.6895852088928223 14.936240196228027
Loss :  1.51357901096344 2.819655418395996 15.611856460571289
  batch 40 loss: 1.51357901096344, 2.819655418395996, 15.611856460571289
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.3984712362289429 3.2196567058563232 17.496755599975586
Loss :  1.3582803010940552 3.2297651767730713 17.50710678100586
Loss :  1.3340001106262207 3.493690252304077 18.80245018005371
Loss :  1.3633149862289429 3.2298672199249268 17.512651443481445
Loss :  1.310145616531372 3.44873309135437 18.55381202697754
Loss :  1.3934694528579712 3.214832305908203 17.46763038635254
Loss :  1.496811032295227 2.839627265930176 15.694947242736816
Loss :  1.3525289297103882 3.2146143913269043 17.425601959228516
Loss :  1.5407447814941406 3.0387966632843018 16.73472785949707
Loss :  1.3639434576034546 3.167841672897339 17.20315170288086
Loss :  1.4650286436080933 3.0171022415161133 16.550539016723633
Loss :  1.443843960762024 2.980448007583618 16.346084594726562
Loss :  1.3889262676239014 3.000711679458618 16.392484664916992
Loss :  1.4639806747436523 3.033301830291748 16.630489349365234
Loss :  1.36454176902771 3.29794979095459 17.854291915893555
Loss :  1.5359479188919067 2.9790282249450684 16.431089401245117
Loss :  1.3748769760131836 3.2594075202941895 17.671913146972656
Loss :  1.3415359258651733 3.1785240173339844 17.234155654907227
Loss :  1.3721052408218384 3.139810085296631 17.071155548095703
Loss :  1.5511308908462524 2.7949647903442383 15.525955200195312
  batch 60 loss: 1.5511308908462524, 2.7949647903442383, 15.525955200195312
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.3287421464920044 2.6230242252349854 14.443862915039062
Loss :  1.4104702472686768 2.9107072353363037 15.964006423950195
Loss :  1.3541063070297241 2.90920090675354 15.900110244750977
Loss :  1.3198645114898682 3.198371648788452 17.311723709106445
Loss :  1.2618826627731323 3.495682716369629 18.74029541015625
Loss :  1.3174936771392822 3.8415377140045166 20.525182723999023
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.3608617782592773 3.9517736434936523 21.119731903076172
Loss :  1.3830780982971191 3.7773349285125732 20.269752502441406
Loss :  1.3147701025009155 3.6640431880950928 19.634984970092773
Total LOSS train 17.088856271597056 valid 20.387413024902344
CE LOSS train 1.3928236062710102 valid 0.3286925256252289
Contrastive LOSS train 3.13920651949369 valid 0.9160107970237732
EPOCH 48:
Loss :  1.4503530263900757 3.4007568359375 18.454137802124023
Loss :  1.492210865020752 4.209412574768066 22.53927230834961
Loss :  1.4048898220062256 3.045278310775757 16.63128089904785
Loss :  1.4276665449142456 2.9584662914276123 16.21999740600586
Loss :  1.4631876945495605 2.988900899887085 16.407691955566406
Loss :  1.3624746799468994 2.9851057529449463 16.28800392150879
Loss :  1.4754973649978638 3.959611415863037 21.273555755615234
Loss :  1.3973711729049683 3.322484016418457 18.009790420532227
Loss :  1.3763543367385864 3.152125597000122 17.136981964111328
Loss :  1.466896891593933 3.1869935989379883 17.401865005493164
Loss :  1.3401844501495361 3.582324266433716 19.251806259155273
Loss :  1.3392552137374878 3.8557674884796143 20.618091583251953
Loss :  1.3414844274520874 4.328500270843506 22.983985900878906
Loss :  1.3798478841781616 4.39769983291626 23.36834716796875
Loss :  1.575269103050232 3.6856377124786377 20.00345802307129
Loss :  1.531484603881836 3.978875160217285 21.425859451293945
Loss :  1.3460856676101685 3.9307563304901123 20.999868392944336
Loss :  1.4072550535202026 4.3639349937438965 23.226930618286133
Loss :  1.3299344778060913 3.9977564811706543 21.31871795654297
Loss :  1.5021706819534302 4.18298864364624 22.4171142578125
  batch 20 loss: 1.5021706819534302, 4.18298864364624, 22.4171142578125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.3891404867172241 3.4222137928009033 18.50020980834961
Loss :  1.2964560985565186 4.432638645172119 23.45964813232422
Loss :  1.3568001985549927 4.501014709472656 23.861873626708984
Loss :  1.3910545110702515 4.5049567222595215 23.91583824157715
Loss :  1.5387712717056274 4.391646385192871 23.49700355529785
Loss :  1.338830828666687 4.438479423522949 23.53122901916504
Loss :  1.384774088859558 4.625523567199707 24.51239013671875
Loss :  1.3804874420166016 4.464932918548584 23.70515251159668
Loss :  1.2092368602752686 4.659192085266113 24.50519561767578
Loss :  1.4903275966644287 4.524106025695801 24.110857009887695
Loss :  1.2049214839935303 4.4395880699157715 23.402862548828125
Loss :  1.3916075229644775 4.626474857330322 24.52398109436035
Loss :  1.328110694885254 4.36502742767334 23.153247833251953
Loss :  1.3146470785140991 4.460835933685303 23.61882781982422
Loss :  1.1607853174209595 4.286259174346924 22.59208106994629
Loss :  1.2113221883773804 4.287272930145264 22.647687911987305
Loss :  1.2227883338928223 4.410386562347412 23.274721145629883
Loss :  1.4097464084625244 4.421061992645264 23.515056610107422
Loss :  1.4364345073699951 4.336042881011963 23.116647720336914
Loss :  1.4719983339309692 4.445799827575684 23.700998306274414
  batch 40 loss: 1.4719983339309692, 4.445799827575684, 23.700998306274414
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.3256323337554932 4.182239532470703 22.23682975769043
Loss :  1.2788982391357422 4.49927282333374 23.7752628326416
Loss :  1.2236512899398804 4.376437187194824 23.105838775634766
Loss :  1.2686519622802734 4.2812018394470215 22.67466163635254
Loss :  1.1887186765670776 4.367517948150635 23.026308059692383
Loss :  1.2968032360076904 4.482749938964844 23.710552215576172
Loss :  1.4122750759124756 4.145669460296631 22.140623092651367
Loss :  1.207784652709961 4.093186378479004 21.673715591430664
Loss :  1.4885889291763306 4.082803726196289 21.902606964111328
Loss :  1.2406898736953735 4.198670387268066 22.234041213989258
Loss :  1.3770756721496582 4.337855339050293 23.06635284423828
Loss :  1.351209282875061 4.093966484069824 21.821043014526367
Loss :  1.2626501321792603 4.190152645111084 22.21341323852539
Loss :  1.3839590549468994 4.133072376251221 22.049320220947266
Loss :  1.2254880666732788 4.0040059089660645 21.24551773071289
Loss :  1.4784138202667236 4.045685291290283 21.70684051513672
Loss :  1.2351126670837402 4.127064228057861 21.870433807373047
Loss :  1.2087315320968628 4.329633712768555 22.85689926147461
Loss :  1.2373425960540771 4.498664379119873 23.73066520690918
Loss :  1.5038658380508423 4.362687587738037 23.317304611206055
  batch 60 loss: 1.5038658380508423, 4.362687587738037, 23.317304611206055
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4, 5], device='cuda:0')
Loss :  1.1831001043319702 4.021900177001953 21.292600631713867
Loss :  1.294087290763855 4.347565174102783 23.031911849975586
Loss :  1.2290822267532349 3.8643391132354736 20.550777435302734
Loss :  1.174817442893982 3.900707960128784 20.67835807800293
Loss :  1.0825977325439453 3.3592960834503174 17.879077911376953
Loss :  3754.185546875 4.3061323165893555 3775.71630859375
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([4], device='cuda:0')
Loss :  3802.210205078125 4.267948627471924 3823.550048828125
Loss :  3774.221435546875 4.166489601135254 3795.053955078125
Loss :  3864.92236328125 4.069966793060303 3885.272216796875
Total LOSS train 21.798664973332333 valid 3819.8981323242188
CE LOSS train 1.3461129683714645 valid 966.2305908203125
Contrastive LOSS train 4.090510401358971 valid 1.0174916982650757
EPOCH 49:
Loss :  1.3540741205215454 3.4770560264587402 18.739355087280273
Loss :  1.409677505493164 4.006208896636963 21.44072151184082
Loss :  1.292161464691162 3.886261224746704 20.723468780517578
Loss :  1.3195860385894775 3.8215250968933105 20.42721176147461
Loss :  1.3917652368545532 3.7338778972625732 20.061155319213867
Loss :  1.2327872514724731 3.3640921115875244 18.05324935913086
Loss :  1.3999305963516235 3.3555333614349365 18.177597045898438
Loss :  1.2895445823669434 3.490124225616455 18.74016571044922
Loss :  1.2536450624465942 3.6277363300323486 19.39232635498047
Loss :  1.3939129114151 3.880690813064575 20.797367095947266
Loss :  1.2081142663955688 3.940047025680542 20.908349990844727
Loss :  1.2106215953826904 3.4174931049346924 18.298086166381836
Loss :  1.2036418914794922 3.714635133743286 19.776817321777344
Loss :  1.2350316047668457 3.723036766052246 19.850215911865234
Loss :  1.478360652923584 3.4579598903656006 18.768159866333008
Loss :  1.4385770559310913 3.6631925106048584 19.754539489746094
Loss :  1.202114462852478 3.6732163429260254 19.568195343017578
Loss :  1.3070507049560547 3.736388921737671 19.988994598388672
Loss :  1.1811403036117554 3.618586301803589 19.274072647094727
Loss :  1.4564480781555176 3.406806230545044 18.490478515625
  batch 20 loss: 1.4564480781555176, 3.406806230545044, 18.490478515625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.2967268228530884 3.895127058029175 20.772361755371094
Loss :  1.1869851350784302 3.7387375831604004 19.880672454833984
Loss :  1.2640349864959717 3.725405216217041 19.89105987548828
Loss :  1.3245456218719482 3.8803603649139404 20.726346969604492
Loss :  1.4460872411727905 4.166909217834473 22.28063201904297
Loss :  1.273215889930725 3.8198647499084473 20.372539520263672
Loss :  1.3140819072723389 3.8683645725250244 20.65590476989746
Loss :  1.2972694635391235 3.963149309158325 21.11301612854004
Loss :  1.0992977619171143 3.635498046875 19.27678871154785
Loss :  1.4262125492095947 3.657416343688965 19.713294982910156
Loss :  1.0998600721359253 3.664585590362549 19.422786712646484
Loss :  1.36447274684906 3.3336024284362793 18.032485961914062
Loss :  1.2675528526306152 3.2585608959198 17.56035804748535
Loss :  1.265263319015503 3.4197182655334473 18.363853454589844
Loss :  1.1316100358963013 3.3909785747528076 18.086503982543945
Loss :  1.1938273906707764 3.6051437854766846 19.219545364379883
Loss :  1.1955900192260742 3.266495943069458 17.52806854248047
Loss :  1.4181700944900513 3.2486205101013184 17.661273956298828
Loss :  1.4450210332870483 3.1779592037200928 17.33481788635254
Loss :  1.479154109954834 3.2860641479492188 17.909475326538086
  batch 40 loss: 1.479154109954834, 3.2860641479492188, 17.909475326538086
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.3273978233337402 3.63192081451416 19.487001419067383
Loss :  1.2713890075683594 3.4964840412139893 18.753808975219727
Loss :  1.2384498119354248 3.5386455059051514 18.931676864624023
Loss :  1.280165433883667 3.608961343765259 19.32497215270996
Loss :  1.2053844928741455 3.2905056476593018 17.657913208007812
Loss :  1.3288540840148926 3.280592918395996 17.73181915283203
Loss :  1.4571222066879272 3.42032527923584 18.55875015258789
Loss :  1.2683801651000977 3.517954111099243 18.858150482177734
Loss :  1.5192418098449707 3.122873067855835 17.133607864379883
Loss :  1.2848460674285889 3.1908297538757324 17.238994598388672
Loss :  1.415720820426941 3.2800440788269043 17.815940856933594
Loss :  1.3886942863464355 3.2316982746124268 17.54718589782715
Loss :  1.3261739015579224 3.423384666442871 18.443098068237305
Loss :  1.4207038879394531 3.488450527191162 18.862957000732422
Loss :  1.2800410985946655 3.2555928230285645 17.55800437927246
Loss :  1.5039163827896118 2.9876978397369385 16.442405700683594
Loss :  1.305540919303894 3.494715690612793 18.77911949157715
Loss :  1.26046621799469 3.2219531536102295 17.37023162841797
Loss :  1.3029282093048096 3.3961329460144043 18.283594131469727
Loss :  1.5306942462921143 3.1375391483306885 17.2183895111084
  batch 60 loss: 1.5306942462921143, 3.1375391483306885, 17.2183895111084
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.2588064670562744 3.3806865215301514 18.16223907470703
Loss :  1.363481879234314 3.0769317150115967 16.748140335083008
Loss :  1.2935905456542969 3.055417060852051 16.570674896240234
Loss :  1.2556217908859253 3.1352484226226807 16.93186378479004
Loss :  1.1902333498001099 2.7692272663116455 15.036369323730469
Loss :  1.3020797967910767 4.278294086456299 22.69355010986328
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.3493437767028809 4.27241849899292 22.711437225341797
Loss :  1.3148926496505737 4.11568021774292 21.893293380737305
Loss :  1.3568658828735352 4.142025470733643 22.066993713378906
Total LOSS train 18.807372665405275 valid 22.341318607330322
CE LOSS train 1.3126924514770508 valid 0.3392164707183838
Contrastive LOSS train 3.4989360405848577 valid 1.0355063676834106
EPOCH 50:
Loss :  1.4187889099121094 3.5686516761779785 19.262046813964844
Loss :  1.4599363803863525 3.4321095943450928 18.6204833984375
Loss :  1.3574187755584717 2.974978446960449 16.232311248779297
Loss :  1.3786576986312866 3.273651599884033 17.746915817260742
Loss :  1.4317412376403809 3.364346504211426 18.25347328186035
Loss :  1.3088165521621704 3.290027618408203 17.758955001831055
Loss :  1.461930751800537 4.250796794891357 22.71591567993164
Loss :  1.3632136583328247 4.283531665802002 22.780872344970703
Loss :  1.3569512367248535 4.152247905731201 22.11819076538086
Loss :  1.454220175743103 3.9887990951538086 21.398216247558594
Loss :  1.3202069997787476 4.100048065185547 21.82044792175293
Loss :  1.3013145923614502 3.9977943897247314 21.290287017822266
Loss :  1.3247803449630737 4.2945404052734375 22.797481536865234
Loss :  1.3080556392669678 4.0261735916137695 21.438922882080078
Loss :  1.4765790700912476 3.6429784297943115 19.691471099853516
Loss :  1.4473822116851807 3.6398565769195557 19.646665573120117
Loss :  1.2771837711334229 3.7447080612182617 20.00072479248047
Loss :  1.359610676765442 4.232497692108154 22.5221004486084
Loss :  1.277105450630188 4.097858905792236 21.766399383544922
Loss :  1.4675325155258179 4.238912582397461 22.66209602355957
  batch 20 loss: 1.4675325155258179, 4.238912582397461, 22.66209602355957
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.3446612358093262 3.8508975505828857 20.599149703979492
Loss :  1.28836190700531 4.436682224273682 23.471773147583008
Loss :  1.321071743965149 4.5415496826171875 24.028820037841797
Loss :  1.377137541770935 4.397017955780029 23.362228393554688
Loss :  1.4904594421386719 4.247140407562256 22.72616195678711
Loss :  1.3286975622177124 4.3431806564331055 23.044601440429688
Loss :  1.2992472648620605 4.723475933074951 24.916627883911133
Loss :  1.369667887687683 4.382607936859131 23.28270721435547
Loss :  1.1695977449417114 4.261882305145264 22.4790096282959
Loss :  1.4341320991516113 4.40629768371582 23.465620040893555
Loss :  1.2038688659667969 4.646008014678955 24.433908462524414
Loss :  1.4291324615478516 4.4834089279174805 23.84617805480957
Loss :  1.3577462434768677 4.758331298828125 25.149402618408203
Loss :  1.316413402557373 4.48817777633667 23.75730323791504
Loss :  1.2224960327148438 4.34943962097168 22.969694137573242
Loss :  1.2499322891235352 4.4861836433410645 23.680850982666016
Loss :  1.2407402992248535 4.330942153930664 22.895450592041016
Loss :  1.4078625440597534 4.330775260925293 23.061738967895508
Loss :  1.4418946504592896 4.420893669128418 23.546363830566406
Loss :  1.5327473878860474 4.4687275886535645 23.876384735107422
  batch 40 loss: 1.5327473878860474, 4.4687275886535645, 23.876384735107422
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.3332327604293823 4.381547451019287 23.240970611572266
Loss :  1.3198741674423218 4.412140369415283 23.38057518005371
Loss :  1.261491298675537 4.272547721862793 22.624231338500977
Loss :  1.3149412870407104 4.40722131729126 23.35104751586914
Loss :  1.2387877702713013 4.440870761871338 23.44314193725586
Loss :  1.3454344272613525 4.6642303466796875 24.66658592224121
Loss :  1.5144269466400146 4.811197757720947 25.570415496826172
Loss :  1.3137089014053345 4.610980033874512 24.368610382080078
Loss :  1.5229333639144897 4.3661956787109375 23.353912353515625
Loss :  1.2824145555496216 4.8644304275512695 25.60456657409668
Loss :  1.4261060953140259 4.482903003692627 23.840620040893555
Loss :  1.3655791282653809 4.506197452545166 23.89656639099121
Loss :  1.2944304943084717 4.367903709411621 23.133949279785156
Loss :  1.450972557067871 4.43220853805542 23.612014770507812
Loss :  1.2677198648452759 4.502297878265381 23.77920913696289
Loss :  1.47763192653656 4.417516231536865 23.565214157104492
Loss :  1.2440185546875 4.570158958435059 24.09481430053711
Loss :  1.2709521055221558 4.647688388824463 24.5093936920166
Loss :  1.2989046573638916 4.682051181793213 24.70915985107422
Loss :  1.5302821397781372 4.372849941253662 23.394533157348633
  batch 60 loss: 1.5302821397781372, 4.372849941253662, 23.394533157348633
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.2287390232086182 4.551063537597656 23.98405647277832
Loss :  1.367292881011963 4.546072006225586 24.097652435302734
Loss :  1.303804636001587 4.418133735656738 23.394472122192383
Loss :  1.2386393547058105 4.4408650398254395 23.442964553833008
Loss :  1.2046210765838623 4.292952537536621 22.669384002685547
Loss :  1.5811710357666016 4.438586711883545 23.774105072021484
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.5452275276184082 4.453957557678223 23.815013885498047
Loss :  1.5499433279037476 4.36062479019165 23.35306739807129
Loss :  1.7285343408584595 4.298563003540039 23.221349716186523
Total LOSS train 22.689938677274263 valid 23.540884017944336
CE LOSS train 1.3506805419921875 valid 0.43213358521461487
Contrastive LOSS train 4.267851598446185 valid 1.0746407508850098
EPOCH 51:
Loss :  1.4328174591064453 4.3742547035217285 23.30409049987793
Loss :  1.4376039505004883 4.5863728523254395 24.369468688964844
Loss :  1.3722350597381592 4.441476821899414 23.579618453979492
Loss :  1.3745039701461792 4.306022644042969 22.904617309570312
Loss :  1.4390047788619995 4.380494594573975 23.34147834777832
Loss :  1.314469337463379 4.424460411071777 23.436771392822266
Loss :  1.476754903793335 4.630472183227539 24.62911605834961
Loss :  1.3928730487823486 4.245866298675537 22.62220573425293
Loss :  1.3439639806747437 4.787015914916992 25.279043197631836
Loss :  1.451167345046997 4.241734981536865 22.65984344482422
Loss :  1.2872127294540405 4.384801387786865 23.211219787597656
Loss :  1.294824242591858 4.560110569000244 24.09537696838379
Loss :  1.3045920133590698 4.565217971801758 24.13068199157715
Loss :  1.327429175376892 4.580478191375732 24.229820251464844
Loss :  1.515663743019104 4.455923080444336 23.795278549194336
Loss :  1.4551185369491577 4.6648054122924805 24.779146194458008
Loss :  1.352615475654602 4.431674480438232 23.510988235473633
Loss :  1.373336672782898 4.365884780883789 23.202760696411133
Loss :  1.2662283182144165 4.303898334503174 22.78571891784668
Loss :  1.4859251976013184 4.55858850479126 24.278867721557617
  batch 20 loss: 1.4859251976013184, 4.55858850479126, 24.278867721557617
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  1.3871134519577026 4.370038032531738 23.237302780151367
Loss :  1.3169480562210083 4.434882640838623 23.491361618041992
Loss :  1.3796617984771729 4.237549304962158 22.567407608032227
Loss :  1.406616449356079 4.694697380065918 24.880104064941406
Loss :  1.4488948583602905 4.093455791473389 21.916173934936523
Loss :  1.3719110488891602 4.302440166473389 22.884113311767578
Loss :  1.3788714408874512 3.8378794193267822 20.568269729614258
Loss :  1.3499212265014648 4.0962090492248535 21.83096694946289
Loss :  1.217833161354065 3.403855085372925 18.23710823059082
Loss :  1.442439317703247 4.376420021057129 23.324539184570312
Loss :  1.2418416738510132 3.764427900314331 20.063982009887695
Loss :  1.402862310409546 4.022241115570068 21.514068603515625
Loss :  1.372616171836853 4.448496341705322 23.615097045898438
Loss :  1.327094554901123 4.570024490356445 24.177217483520508
Loss :  1.284533143043518 4.453075885772705 23.549911499023438
Loss :  1.3386852741241455 4.457921028137207 23.6282901763916
Loss :  1.3411705493927002 4.469686031341553 23.68960189819336
Loss :  1.4462225437164307 4.383593559265137 23.36419105529785
Loss :  1.5184800624847412 4.371140480041504 23.374181747436523
Loss :  1.52887761592865 4.432357311248779 23.690664291381836
  batch 40 loss: 1.52887761592865, 4.432357311248779, 23.690664291381836
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.3976292610168457 4.43561315536499 23.575695037841797
Loss :  1.3520234823226929 4.511094570159912 23.90749740600586
Loss :  1.3004801273345947 4.586030960083008 24.230634689331055
Loss :  1.3372619152069092 4.51788854598999 23.92670440673828
Loss :  1.321333646774292 4.355267524719238 23.09766960144043
Loss :  1.401339054107666 4.323556423187256 23.019121170043945
Loss :  1.4944177865982056 4.390082359313965 23.4448299407959
Loss :  1.2829023599624634 3.5975615978240967 19.270709991455078
Loss :  1.4968149662017822 3.4799575805664062 18.896602630615234
Loss :  1.3030284643173218 3.6534085273742676 19.570070266723633
Loss :  1.3757085800170898 3.3980696201324463 18.366058349609375
Loss :  1.4220722913742065 4.400050163269043 23.42232322692871
Loss :  1.3744113445281982 4.571776390075684 24.233293533325195
Loss :  1.414469599723816 3.6309049129486084 19.568992614746094
Loss :  1.2374873161315918 3.5493667125701904 18.98432159423828
Loss :  1.475646734237671 3.3964922428131104 18.45810890197754
Loss :  1.2979928255081177 3.857491970062256 20.585453033447266
Loss :  1.2368247509002686 4.095492839813232 21.71428871154785
Loss :  1.2930964231491089 4.004858016967773 21.317386627197266
Loss :  1.4995900392532349 3.4716265201568604 18.857723236083984
  batch 60 loss: 1.4995900392532349, 3.4716265201568604, 18.857723236083984
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.2724361419677734 4.3882551193237305 23.213712692260742
Loss :  1.349673867225647 4.43210506439209 23.51020050048828
Loss :  1.2534514665603638 4.817580699920654 25.34135627746582
Loss :  1.2405744791030884 4.65942907333374 24.5377197265625
Loss :  1.1432843208312988 4.384738922119141 23.066978454589844
Loss :  1.2253910303115845 4.38211727142334 23.13597869873047
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.2836498022079468 4.392426490783691 23.24578094482422
Loss :  1.2831761837005615 4.329946041107178 22.932907104492188
Loss :  1.266276240348816 4.32088041305542 22.870677947998047
Total LOSS train 22.7056633582482 valid 23.04633617401123
CE LOSS train 1.365767475274893 valid 0.316569060087204
Contrastive LOSS train 4.267979148718027 valid 1.080220103263855
EPOCH 52:
Loss :  1.397840976715088 4.278100967407227 22.788345336914062
Loss :  1.420592188835144 4.546900272369385 24.155092239379883
Loss :  1.3265204429626465 4.470909595489502 23.681068420410156
Loss :  1.3268674612045288 4.429117202758789 23.472454071044922
Loss :  1.396890640258789 4.414970874786377 23.471744537353516
Loss :  1.2800980806350708 4.492518424987793 23.742691040039062
Loss :  1.4262208938598633 4.406207084655762 23.457256317138672
Loss :  1.27350914478302 3.2752928733825684 17.649974822998047
Loss :  1.2335193157196045 3.372103691101074 18.094038009643555
Loss :  1.3570983409881592 3.4744715690612793 18.729455947875977
Loss :  1.2324422597885132 4.5653815269470215 24.059350967407227
Loss :  1.2422009706497192 4.689870834350586 24.69155502319336
Loss :  1.1913868188858032 4.557840824127197 23.9805908203125
Loss :  1.2432351112365723 4.554532527923584 24.015897750854492
Loss :  1.4517656564712524 4.211154460906982 22.507537841796875
Loss :  1.4184871912002563 3.9804940223693848 21.32095718383789
Loss :  1.1820234060287476 3.88503098487854 20.607179641723633
Loss :  1.2890561819076538 3.764180898666382 20.109960556030273
Loss :  1.16525399684906 3.958984613418579 20.96017837524414
Loss :  1.4002161026000977 4.45725679397583 23.686500549316406
  batch 20 loss: 1.4002161026000977, 4.45725679397583, 23.686500549316406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.2727375030517578 3.7943522930145264 20.24449920654297
Loss :  1.1565473079681396 3.7340500354766846 19.826797485351562
Loss :  1.2283453941345215 3.805878162384033 20.257736206054688
Loss :  1.3010402917861938 3.674182891845703 19.671955108642578
Loss :  1.4069273471832275 4.0313191413879395 21.563522338867188
Loss :  1.2467275857925415 3.49784517288208 18.735952377319336
Loss :  1.292985200881958 4.026258945465088 21.424280166625977
Loss :  1.2678906917572021 4.17507266998291 22.143253326416016
Loss :  1.0667049884796143 3.6040632724761963 19.08702278137207
Loss :  1.396287202835083 4.346574783325195 23.129161834716797
Loss :  1.0892977714538574 4.549849987030029 23.83854866027832
Loss :  1.361197590827942 4.643097877502441 24.57668685913086
Loss :  1.2325828075408936 4.403447151184082 23.24981689453125
Loss :  1.2325137853622437 4.322975158691406 22.847389221191406
Loss :  1.092819333076477 3.9829065799713135 21.007352828979492
Loss :  1.1553000211715698 3.9120383262634277 20.715492248535156
Loss :  1.1421242952346802 3.781320810317993 20.048728942871094
Loss :  1.4020812511444092 3.5410714149475098 19.107437133789062
Loss :  1.3963990211486816 3.8977038860321045 20.884918212890625
Loss :  1.4571205377578735 4.2191853523254395 22.55304718017578
  batch 40 loss: 1.4571205377578735, 4.2191853523254395, 22.55304718017578
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.2858238220214844 4.598029136657715 24.275970458984375
Loss :  1.2338595390319824 4.5029802322387695 23.748760223388672
Loss :  1.2287495136260986 4.4092817306518555 23.27515983581543
Loss :  1.2418925762176514 3.773088216781616 20.107332229614258
Loss :  1.1400636434555054 3.7728769779205322 20.00444984436035
Loss :  1.2517534494400024 3.8378713130950928 20.44110870361328
Loss :  1.3824594020843506 3.8100125789642334 20.43252182006836
Loss :  1.2675539255142212 4.527144432067871 23.903276443481445
Loss :  1.4818464517593384 4.622172832489014 24.592710494995117
Loss :  1.2688391208648682 4.671170711517334 24.624692916870117
Loss :  1.3934663534164429 4.706446647644043 24.925701141357422
Loss :  1.3673052787780762 4.422811031341553 23.481361389160156
Loss :  1.318161964416504 4.407540798187256 23.355865478515625
Loss :  1.376668095588684 4.369248390197754 23.222909927368164
Loss :  1.2691922187805176 4.532132625579834 23.929855346679688
Loss :  1.4329028129577637 4.361780166625977 23.241804122924805
Loss :  1.2670100927352905 4.884948253631592 25.69175148010254
Loss :  1.1912939548492432 4.625866413116455 24.32062530517578
Loss :  1.2532087564468384 4.65418004989624 24.52410888671875
Loss :  1.5105489492416382 4.5935378074646 24.478239059448242
  batch 60 loss: 1.5105489492416382, 4.5935378074646, 24.478239059448242
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.1782569885253906 4.587432384490967 24.115419387817383
Loss :  1.3437442779541016 4.567255973815918 24.180025100708008
Loss :  1.2231868505477905 4.597349643707275 24.20993423461914
Loss :  1.201210856437683 4.399992942810059 23.201175689697266
Loss :  1.136470913887024 4.145898818969727 21.865964889526367
Loss :  1.1980212926864624 4.469282627105713 23.54443359375
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.2571929693222046 4.413361549377441 23.323999404907227
Loss :  1.2525781393051147 4.2469024658203125 22.487091064453125
Loss :  1.2315797805786133 4.229569435119629 22.379425048828125
Total LOSS train 22.373017736581655 valid 22.93373727798462
CE LOSS train 1.2876665372114915 valid 0.3078949451446533
Contrastive LOSS train 4.217070216398973 valid 1.0573923587799072
EPOCH 53:
Loss :  1.3682411909103394 4.3101654052734375 22.9190673828125
Loss :  1.4302608966827393 4.388671398162842 23.373619079589844
Loss :  1.2362415790557861 3.770033359527588 20.086408615112305
Loss :  1.2703955173492432 3.653357982635498 19.537185668945312
Loss :  1.3301202058792114 4.600608825683594 24.33316421508789
Loss :  1.2230037450790405 4.368131637573242 23.063661575317383
Loss :  1.3995118141174316 4.539156913757324 24.09529685974121
Loss :  1.272430181503296 4.285905838012695 22.70195960998535
Loss :  1.2255030870437622 4.484430313110352 23.647655487060547
Loss :  1.331592082977295 4.725030422210693 24.956745147705078
Loss :  1.1700080633163452 4.507497787475586 23.707496643066406
Loss :  1.130987524986267 4.453802108764648 23.39999771118164
Loss :  1.1319156885147095 4.473872184753418 23.501277923583984
Loss :  1.187862515449524 4.451568603515625 23.44570541381836
Loss :  1.4455360174179077 4.345979690551758 23.175434112548828
Loss :  1.3883240222930908 4.469947814941406 23.73806381225586
Loss :  1.1391834020614624 4.511931896209717 23.698843002319336
Loss :  1.2477834224700928 4.445741653442383 23.476491928100586
Loss :  1.1032623052597046 4.389875888824463 23.052640914916992
Loss :  1.3788502216339111 4.6919965744018555 24.838834762573242
  batch 20 loss: 1.3788502216339111, 4.6919965744018555, 24.838834762573242
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.204674243927002 4.118960857391357 21.79947853088379
Loss :  1.0793397426605225 4.118714809417725 21.672914505004883
Loss :  1.1848562955856323 3.9996931552886963 21.18332290649414
Loss :  1.2727020978927612 3.7536966800689697 20.04118537902832
Loss :  1.4088596105575562 3.9622538089752197 21.220129013061523
Loss :  1.1934045553207397 4.427149295806885 23.329151153564453
Loss :  1.248078465461731 4.759567737579346 25.045917510986328
Loss :  1.2348235845565796 4.695216655731201 24.710906982421875
Loss :  0.9845314621925354 4.652694225311279 24.248003005981445
Loss :  1.4108549356460571 4.54633903503418 24.142549514770508
Loss :  0.9825664758682251 4.462638854980469 23.295761108398438
Loss :  1.3421649932861328 4.648599147796631 24.585161209106445
Loss :  1.188065528869629 4.4768524169921875 23.57232666015625
Loss :  1.1923353672027588 4.476665019989014 23.575660705566406
Loss :  1.0541712045669556 4.565247535705566 23.880407333374023
Loss :  1.099013328552246 4.602686882019043 24.112449645996094
Loss :  1.1216368675231934 4.569338321685791 23.96832847595215
Loss :  1.430449366569519 4.316137790679932 23.011137008666992
Loss :  1.4243258237838745 4.451699733734131 23.682825088500977
Loss :  1.4677194356918335 4.391767501831055 23.426557540893555
  batch 40 loss: 1.4677194356918335, 4.391767501831055, 23.426557540893555
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.2627404928207397 4.421794414520264 23.371713638305664
Loss :  1.2094002962112427 4.579977035522461 24.109285354614258
Loss :  1.1500564813613892 4.54589319229126 23.8795223236084
Loss :  1.215889573097229 4.457364559173584 23.50271224975586
Loss :  1.1101086139678955 4.369167804718018 22.955947875976562
Loss :  1.2605446577072144 4.296849727630615 22.744792938232422
Loss :  1.4159997701644897 4.499732494354248 23.914663314819336
Loss :  1.191516637802124 4.497788429260254 23.680458068847656
Loss :  1.5041640996932983 4.3757500648498535 23.38291358947754
Loss :  1.2190523147583008 4.500895977020264 23.723533630371094
Loss :  1.4002918004989624 4.455485820770264 23.67772102355957
Loss :  1.3427977561950684 4.238114356994629 22.533369064331055
Loss :  1.2549841403961182 4.393731594085693 23.223642349243164
Loss :  1.3739514350891113 4.613799571990967 24.442949295043945
Loss :  1.1960092782974243 4.510221004486084 23.747114181518555
Loss :  1.4882303476333618 4.572024345397949 24.348352432250977
Loss :  1.2169417142868042 4.556859970092773 24.00124168395996
Loss :  1.1303586959838867 4.484915733337402 23.55493927001953
Loss :  1.200563907623291 4.6036882400512695 24.219003677368164
Loss :  1.5220736265182495 4.427134990692139 23.65774917602539
  batch 60 loss: 1.5220736265182495, 4.427134990692139, 23.65774917602539
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.144479751586914 4.3945159912109375 23.1170597076416
Loss :  1.2977005243301392 4.445895195007324 23.527177810668945
Loss :  1.181673526763916 4.329145908355713 22.827402114868164
Loss :  1.1379451751708984 4.410427570343018 23.190082550048828
Loss :  1.0407299995422363 4.130496025085449 21.69321060180664
Loss :  1.1945865154266357 4.470781326293945 23.548492431640625
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.241355538368225 4.575348377227783 24.11809730529785
Loss :  1.2304751873016357 4.360600471496582 23.033475875854492
Loss :  1.2388420104980469 4.3569016456604 23.02334976196289
Total LOSS train 23.327388939490685 valid 23.430853843688965
CE LOSS train 1.2523660228802607 valid 0.3097105026245117
Contrastive LOSS train 4.415004550493681 valid 1.0892254114151
EPOCH 54:
Loss :  1.35316801071167 4.361868381500244 23.16250991821289
Loss :  1.416338324546814 4.481570243835449 23.824190139770508
Loss :  1.2609435319900513 4.460812568664551 23.565006256103516
Loss :  1.3039219379425049 4.468139171600342 23.64461898803711
Loss :  1.3708131313323975 4.306210041046143 22.90186309814453
Loss :  1.1972681283950806 4.457906246185303 23.486799240112305
Loss :  1.3925589323043823 4.635145664215088 24.568286895751953
Loss :  1.2660667896270752 4.277776718139648 22.654951095581055
Loss :  1.2237344980239868 4.35141658782959 22.980817794799805
Loss :  1.383316993713379 4.493780136108398 23.852218627929688
Loss :  1.161136507987976 4.447428226470947 23.398277282714844
Loss :  1.1704298257827759 4.559036731719971 23.965612411499023
Loss :  1.160711407661438 4.619410514831543 24.25776481628418
Loss :  1.180917739868164 4.540782451629639 23.884830474853516
Loss :  1.4587708711624146 4.422922134399414 23.573381423950195
Loss :  1.4142428636550903 4.470850944519043 23.76849937438965
Loss :  1.1817859411239624 4.3957085609436035 23.160327911376953
Loss :  1.2813448905944824 4.392427921295166 23.243484497070312
Loss :  1.1520723104476929 4.413520336151123 23.219675064086914
Loss :  1.4481357336044312 4.520567893981934 24.050975799560547
  batch 20 loss: 1.4481357336044312, 4.520567893981934, 24.050975799560547
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.2907317876815796 4.509568691253662 23.83857536315918
Loss :  1.1745505332946777 4.5578460693359375 23.963781356811523
Loss :  1.2476691007614136 4.562260150909424 24.058969497680664
Loss :  1.3294622898101807 4.463263988494873 23.645782470703125
Loss :  1.445228099822998 4.623808860778809 24.564273834228516
Loss :  1.2487910985946655 4.398383617401123 23.24070930480957
Loss :  1.2728922367095947 4.653890609741211 24.54234504699707
Loss :  1.285866141319275 4.365176677703857 23.11174964904785
Loss :  1.0334726572036743 4.454813003540039 23.307537078857422
Loss :  1.46078622341156 4.404970169067383 23.485637664794922
Loss :  1.0536997318267822 4.52453088760376 23.676353454589844
Loss :  1.3763179779052734 4.637346267700195 24.56304931640625
Loss :  1.2671998739242554 4.423472881317139 23.384565353393555
Loss :  1.2392617464065552 4.41085147857666 23.29351806640625
Loss :  1.089084506034851 4.657437324523926 24.376270294189453
Loss :  1.1544592380523682 4.464269638061523 23.475807189941406
Loss :  1.1427476406097412 4.187497138977051 22.080232620239258
Loss :  1.4553287029266357 4.527550220489502 24.09307861328125
Loss :  1.4477578401565552 4.432901859283447 23.612266540527344
Loss :  1.4639331102371216 4.5382561683654785 24.155214309692383
  batch 40 loss: 1.4639331102371216, 4.5382561683654785, 24.155214309692383
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.2960996627807617 4.505759239196777 23.82489776611328
Loss :  1.233310580253601 4.423369407653809 23.35015869140625
Loss :  1.2009954452514648 4.443099021911621 23.416492462158203
Loss :  1.2794822454452515 4.532616138458252 23.942562103271484
Loss :  1.1547420024871826 4.3832688331604 23.07108497619629
Loss :  1.336740255355835 4.392526149749756 23.29937171936035
Loss :  1.440784215927124 4.495307922363281 23.91732406616211
Loss :  1.236014723777771 4.5488762855529785 23.980396270751953
Loss :  1.554726481437683 4.704709053039551 25.078269958496094
Loss :  1.274969220161438 4.46218204498291 23.585878372192383
Loss :  1.4488964080810547 4.607175350189209 24.484773635864258
Loss :  1.3696112632751465 4.780641078948975 25.272817611694336
Loss :  1.3092488050460815 4.5229411125183105 23.923954010009766
Loss :  1.433863878250122 4.353684902191162 23.202289581298828
Loss :  1.258491039276123 4.444796562194824 23.48247528076172
Loss :  1.5158240795135498 4.482081413269043 23.926231384277344
Loss :  1.2651126384735107 4.449851036071777 23.514368057250977
Loss :  1.214652180671692 4.6216630935668945 24.322967529296875
Loss :  1.231229305267334 4.627168655395508 24.36707305908203
Loss :  1.5768605470657349 4.633206367492676 24.742891311645508
  batch 60 loss: 1.5768605470657349, 4.633206367492676, 24.742891311645508
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.2013647556304932 4.6312150955200195 24.357439041137695
Loss :  1.3698976039886475 4.434634685516357 23.543071746826172
Loss :  1.2434314489364624 4.691758155822754 24.702220916748047
Loss :  1.2026145458221436 4.397999286651611 23.192609786987305
Loss :  1.0824049711227417 4.0942487716674805 21.55364990234375
Loss :  1.7282261848449707 4.44480562210083 23.952253341674805
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4], device='cuda:0')
Loss :  1.8438260555267334 4.448353290557861 24.08559226989746
Loss :  1.7518432140350342 4.336905479431152 23.436370849609375
Loss :  1.88845694065094 4.024561405181885 22.011262893676758
Total LOSS train 23.718293498112605 valid 23.3713698387146
CE LOSS train 1.2921274955456072 valid 0.472114235162735
Contrastive LOSS train 4.485233182173509 valid 1.0061403512954712
EPOCH 55:
Loss :  1.4016475677490234 4.288928031921387 22.846288681030273
Loss :  1.4333382844924927 4.632331371307373 24.594995498657227
Loss :  1.2669038772583008 4.343338489532471 22.983596801757812
Loss :  1.3488821983337402 4.4631667137146 23.664716720581055
Loss :  1.4197992086410522 4.3092827796936035 22.96621322631836
Loss :  1.2366009950637817 4.277699947357178 22.62510108947754
Loss :  1.3750228881835938 4.5519537925720215 24.13479232788086
Loss :  1.2913647890090942 4.721101760864258 24.896873474121094
Loss :  1.2871203422546387 4.544957160949707 24.011905670166016
Loss :  1.395566463470459 4.397971153259277 23.38542366027832
Loss :  1.1949607133865356 4.4555439949035645 23.472681045532227
Loss :  1.2098196744918823 4.483744144439697 23.6285400390625
Loss :  1.177591323852539 4.408918857574463 23.222185134887695
Loss :  1.21437668800354 4.511288166046143 23.770816802978516
Loss :  1.4952753782272339 4.435069561004639 23.670623779296875
Loss :  1.4381992816925049 4.540938377380371 24.142892837524414
Loss :  1.2026300430297852 4.473011493682861 23.56768798828125
Loss :  1.289716362953186 4.423294544219971 23.40618896484375
Loss :  1.1795233488082886 4.401381969451904 23.186433792114258
Loss :  1.4433200359344482 4.405947685241699 23.473058700561523
  batch 20 loss: 1.4433200359344482, 4.405947685241699, 23.473058700561523
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4], device='cuda:0')
Loss :  1.3049464225769043 4.509275913238525 23.85132598876953
Loss :  1.221113681793213 4.5169453620910645 23.80583953857422
Loss :  1.2573922872543335 4.60776948928833 24.296239852905273
Loss :  1.3101197481155396 4.465846061706543 23.63935089111328
Loss :  1.4375126361846924 4.595641613006592 24.415721893310547
Loss :  1.2513501644134521 4.461382865905762 23.558265686035156
Loss :  1.3060945272445679 4.709992408752441 24.856056213378906
Loss :  1.2990093231201172 4.32405948638916 22.9193058013916
Loss :  1.0760070085525513 4.425280570983887 23.202411651611328
Loss :  1.4374438524246216 4.423956394195557 23.557226181030273
Loss :  1.079580545425415 4.49398946762085 23.549528121948242
Loss :  1.379766583442688 4.589045524597168 24.324995040893555
Loss :  1.2790637016296387 4.470048904418945 23.629308700561523
Loss :  1.2450530529022217 4.49236536026001 23.706878662109375
Loss :  1.1289782524108887 4.5147624015808105 23.702791213989258
Loss :  1.1912072896957397 4.485054969787598 23.61648178100586
Loss :  1.1967612504959106 4.616426944732666 24.27889633178711
Loss :  1.4276950359344482 4.695713520050049 24.906261444091797
Loss :  1.456270694732666 4.541593551635742 24.16423797607422
Loss :  1.4992269277572632 4.627798080444336 24.63821792602539
  batch 40 loss: 1.4992269277572632, 4.627798080444336, 24.63821792602539
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.3164091110229492 4.556460857391357 24.098712921142578
Loss :  1.2692317962646484 4.486501216888428 23.701738357543945
Loss :  1.217590093612671 4.431195259094238 23.373565673828125
Loss :  1.278186559677124 4.528671741485596 23.921545028686523
Loss :  1.1809622049331665 4.304425239562988 22.703086853027344
Loss :  1.3355172872543335 4.542662143707275 24.048828125
Loss :  1.4460581541061401 4.641855716705322 24.655336380004883
Loss :  1.263521671295166 4.435615062713623 23.44159698486328
Loss :  1.5057814121246338 4.366713523864746 23.3393497467041
Loss :  1.2747085094451904 4.430451393127441 23.426963806152344
Loss :  1.3893609046936035 4.5829548835754395 24.304134368896484
Loss :  1.3747601509094238 4.548547267913818 24.117496490478516
Loss :  1.3118747472763062 4.44169282913208 23.52033805847168
Loss :  1.392232060432434 4.381709575653076 23.30078125
Loss :  1.257028579711914 4.302849292755127 22.77127456665039
Loss :  1.5129082202911377 4.445496082305908 23.740388870239258
Loss :  1.2804806232452393 4.52705717086792 23.915767669677734
Loss :  1.2092915773391724 4.490903377532959 23.663808822631836
Loss :  1.306179404258728 4.445989608764648 23.5361270904541
Loss :  1.4965670108795166 4.460783958435059 23.800487518310547
  batch 60 loss: 1.4965670108795166, 4.460783958435059, 23.800487518310547
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4], device='cuda:0')
Loss :  1.2181435823440552 4.463691234588623 23.53660011291504
Loss :  1.351675033569336 4.4115309715271 23.409330368041992
Loss :  1.2605955600738525 4.443052291870117 23.47585678100586
Loss :  1.2114427089691162 4.396523475646973 23.194059371948242
Loss :  1.125773310661316 4.18419075012207 22.04672622680664
Loss :  1.267356514930725 4.554627895355225 24.040496826171875
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.2901195287704468 4.454546928405762 23.562854766845703
Loss :  1.319379210472107 4.354136943817139 23.090065002441406
Loss :  1.3612474203109741 4.267587184906006 22.69918441772461
Total LOSS train 23.68175782423753 valid 23.3481502532959
CE LOSS train 1.3057312726974488 valid 0.34031185507774353
Contrastive LOSS train 4.475205289400541 valid 1.0668967962265015
EPOCH 56:
Loss :  1.3859540224075317 4.302592754364014 22.89891815185547
Loss :  1.4535218477249146 4.53709602355957 24.139001846313477
Loss :  1.320185661315918 4.413235664367676 23.386363983154297
Loss :  1.3268460035324097 4.829516887664795 25.474430084228516
Loss :  1.4012428522109985 4.525364398956299 24.028064727783203
Loss :  1.269458293914795 4.392219066619873 23.230554580688477
Loss :  1.4217125177383423 4.3972930908203125 23.408178329467773
Loss :  1.3054536581039429 4.673746109008789 24.674184799194336
Loss :  1.2745616436004639 4.693181037902832 24.74046516418457
Loss :  1.399598479270935 4.3864006996154785 23.331602096557617
Loss :  1.2398592233657837 4.503236770629883 23.75604248046875
Loss :  1.2454439401626587 4.581382751464844 24.15235710144043
Loss :  1.2194650173187256 4.5769362449646 24.10414695739746
Loss :  1.258265495300293 4.484684944152832 23.681690216064453
Loss :  1.4879027605056763 4.615649223327637 24.566150665283203
Loss :  1.4273544549942017 4.50715446472168 23.96312713623047
Loss :  1.257381558418274 4.593842506408691 24.226593017578125
Loss :  1.3266727924346924 4.592297554016113 24.28816032409668
Loss :  1.2376452684402466 4.347560405731201 22.975448608398438
Loss :  1.4500073194503784 4.4475626945495605 23.687820434570312
  batch 20 loss: 1.4500073194503784, 4.4475626945495605, 23.687820434570312
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.3070541620254517 4.372137069702148 23.167739868164062
Loss :  1.2249292135238647 4.534790992736816 23.898883819580078
Loss :  1.291092872619629 4.54481315612793 24.015159606933594
Loss :  1.356013536453247 4.469128608703613 23.701656341552734
Loss :  1.4575563669204712 4.733913898468018 25.127124786376953
Loss :  1.293279767036438 4.646243095397949 24.52449607849121
Loss :  1.3263746500015259 4.7247538566589355 24.950143814086914
Loss :  1.3148820400238037 4.441168785095215 23.520727157592773
Loss :  1.1285423040390015 4.438968181610107 23.323383331298828
Loss :  1.4468885660171509 4.422529220581055 23.559534072875977
Loss :  1.1438825130462646 4.619230270385742 24.240034103393555
Loss :  1.3602722883224487 4.7365922927856445 25.043231964111328
Loss :  1.2953081130981445 4.518322944641113 23.886920928955078
Loss :  1.2718515396118164 4.768949508666992 25.116600036621094
Loss :  1.159089207649231 4.473548889160156 23.52683448791504
Loss :  1.2135311365127563 4.550047397613525 23.963768005371094
Loss :  1.2144609689712524 4.51841402053833 23.806529998779297
Loss :  1.3958959579467773 4.502492904663086 23.90835952758789
Loss :  1.4087883234024048 4.412290096282959 23.470239639282227
Loss :  1.459577202796936 4.355177402496338 23.235464096069336
  batch 40 loss: 1.459577202796936, 4.355177402496338, 23.235464096069336
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.3118027448654175 4.426207065582275 23.44283676147461
Loss :  1.288516879081726 4.444267749786377 23.509855270385742
Loss :  1.2521075010299683 4.373855113983154 23.121383666992188
Loss :  1.2945691347122192 4.411081790924072 23.349977493286133
Loss :  1.2244325876235962 4.3027191162109375 22.738027572631836
Loss :  1.3171497583389282 4.6105475425720215 24.369888305664062
Loss :  1.4400126934051514 4.583613872528076 24.358081817626953
Loss :  1.2926890850067139 4.4390034675598145 23.48770523071289
Loss :  1.4859269857406616 4.436126708984375 23.666561126708984
Loss :  1.2928838729858398 4.468465805053711 23.635211944580078
Loss :  1.4266976118087769 4.502725124359131 23.940322875976562
Loss :  1.3857872486114502 4.330807209014893 23.039823532104492
Loss :  1.3099708557128906 4.543968677520752 24.029813766479492
Loss :  1.3878884315490723 4.395312786102295 23.364452362060547
Loss :  1.2615909576416016 4.464378356933594 23.58348274230957
Loss :  1.4543852806091309 4.452224254608154 23.71550750732422
Loss :  1.2780203819274902 4.5633625984191895 24.094833374023438
Loss :  1.239969253540039 4.592085361480713 24.200395584106445
Loss :  1.2671676874160767 4.575079917907715 24.142568588256836
Loss :  1.5076408386230469 4.527232646942139 24.1438045501709
  batch 60 loss: 1.5076408386230469, 4.527232646942139, 24.1438045501709
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.2149696350097656 4.473312854766846 23.581533432006836
Loss :  1.3476216793060303 4.410248279571533 23.398862838745117
Loss :  1.2554415464401245 4.503262519836426 23.771753311157227
Loss :  1.2214945554733276 4.445694446563721 23.449966430664062
Loss :  1.1220064163208008 4.0998854637146 21.62143325805664
Loss :  1.2071253061294556 4.417160511016846 23.292926788330078
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.2536461353302002 4.400093078613281 23.254112243652344
Loss :  1.2462221384048462 4.3091349601745605 22.79189682006836
Loss :  1.245756983757019 4.10604190826416 21.775964736938477
Total LOSS train 23.822434087900014 valid 22.778725147247314
CE LOSS train 1.3178238025078406 valid 0.31143924593925476
Contrastive LOSS train 4.500922071016752 valid 1.02651047706604
EPOCH 57:
Loss :  1.3745999336242676 4.3569254875183105 23.15922737121582
Loss :  1.4237003326416016 4.684989929199219 24.848649978637695
Loss :  1.2988274097442627 4.453766822814941 23.56766128540039
Loss :  1.3368297815322876 4.311193943023682 22.892799377441406
Loss :  1.378267765045166 4.241820812225342 22.587371826171875
Loss :  1.217324137687683 4.4718217849731445 23.576431274414062
Loss :  1.390613079071045 4.588014125823975 24.330684661865234
Loss :  1.2971584796905518 4.304747581481934 22.820898056030273
Loss :  1.2722829580307007 4.539350509643555 23.969036102294922
Loss :  1.372428297996521 4.244287490844727 22.5938663482666
Loss :  1.2097421884536743 4.531266689300537 23.86607551574707
Loss :  1.1850214004516602 4.618632793426514 24.278186798095703
Loss :  1.2005354166030884 4.558253288269043 23.991802215576172
Loss :  1.1893513202667236 4.452963352203369 23.45416831970215
Loss :  1.4676234722137451 4.380005359649658 23.36764907836914
Loss :  1.3841131925582886 4.535612106323242 24.06217384338379
Loss :  1.1921777725219727 4.372564792633057 23.05500030517578
Loss :  1.2825791835784912 4.51135778427124 23.83936882019043
Loss :  1.1709074974060059 4.430393695831299 23.3228759765625
Loss :  1.4516189098358154 4.6484599113464355 24.693918228149414
  batch 20 loss: 1.4516189098358154, 4.6484599113464355, 24.693918228149414
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.2638342380523682 4.428069591522217 23.40418243408203
Loss :  1.169669270515442 4.703911781311035 24.689228057861328
Loss :  1.2402784824371338 4.477752685546875 23.62904167175293
Loss :  1.2855278253555298 4.43627405166626 23.46689796447754
Loss :  1.3844287395477295 4.414100170135498 23.45492935180664
Loss :  1.2243081331253052 4.369627475738525 23.072444915771484
Loss :  1.2691929340362549 4.646271705627441 24.500551223754883
Loss :  1.2797266244888306 4.378136157989502 23.170406341552734
Loss :  1.0380946397781372 4.679265022277832 24.434419631958008
Loss :  1.3815758228302002 4.410488605499268 23.434019088745117
Loss :  1.0497843027114868 4.461630344390869 23.357934951782227
Loss :  1.3236274719238281 4.348205089569092 23.064653396606445
Loss :  1.2020143270492554 4.622506141662598 24.314544677734375
Loss :  1.19866144657135 4.27746057510376 22.58596420288086
Loss :  1.083392858505249 4.239072322845459 22.27875518798828
Loss :  1.146489143371582 4.296750545501709 22.63024139404297
Loss :  1.1326929330825806 4.337327003479004 22.819326400756836
Loss :  1.3748900890350342 4.4546427726745605 23.648103713989258
Loss :  1.3936516046524048 4.322523593902588 23.006269454956055
Loss :  1.4636545181274414 4.461113452911377 23.769222259521484
  batch 40 loss: 1.4636545181274414, 4.461113452911377, 23.769222259521484
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.3061317205429077 4.492685794830322 23.769559860229492
Loss :  1.2302662134170532 4.5214643478393555 23.837589263916016
Loss :  1.2063480615615845 4.542028903961182 23.916492462158203
Loss :  1.250526785850525 4.396218299865723 23.231616973876953
Loss :  1.1781171560287476 4.380780220031738 23.08201789855957
Loss :  1.281782865524292 4.281006813049316 22.68681526184082
Loss :  1.4058982133865356 4.4824604988098145 23.818201065063477
Loss :  1.2200040817260742 4.487055778503418 23.655284881591797
Loss :  1.4749422073364258 4.3483686447143555 23.216785430908203
Loss :  1.224523663520813 4.458197593688965 23.515512466430664
Loss :  1.366129994392395 4.373701095581055 23.234636306762695
Loss :  1.3442480564117432 4.40152645111084 23.35188102722168
Loss :  1.2711983919143677 4.420571327209473 23.374053955078125
Loss :  1.357401728630066 4.519514560699463 23.954973220825195
Loss :  1.2229872941970825 4.427430152893066 23.360137939453125
Loss :  1.4403935670852661 4.565340518951416 24.2670955657959
Loss :  1.207153558731079 4.54164981842041 23.915401458740234
Loss :  1.201786994934082 4.466350078582764 23.533538818359375
Loss :  1.2398885488510132 4.568821907043457 24.08399772644043
Loss :  1.5098626613616943 4.507402420043945 24.046875
  batch 60 loss: 1.5098626613616943, 4.507402420043945, 24.046875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.1893543004989624 4.596044540405273 24.16957664489746
Loss :  1.3026841878890991 4.422583103179932 23.415599822998047
Loss :  1.2210158109664917 4.596072196960449 24.201377868652344
Loss :  1.168921947479248 4.64142370223999 24.376041412353516
Loss :  1.0899910926818848 4.195682525634766 22.068403244018555
Loss :  2.476198196411133 4.473413944244385 24.8432674407959
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2], device='cuda:0')
Loss :  2.449359178543091 4.447222709655762 24.685474395751953
Loss :  2.4294657707214355 4.406640529632568 24.462669372558594
Loss :  2.550734758377075 4.230526924133301 23.703369140625
Total LOSS train 23.555268419705904 valid 24.42369508743286
CE LOSS train 1.275580877524156 valid 0.6376836895942688
Contrastive LOSS train 4.455937517606295 valid 1.0576317310333252
EPOCH 58:
Loss :  1.382976770401001 4.772049903869629 25.24322509765625
Loss :  1.3991509675979614 4.471489429473877 23.7565975189209
Loss :  1.27206289768219 4.477309226989746 23.65860939025879
Loss :  1.337217926979065 4.458708763122559 23.630762100219727
Loss :  1.3257832527160645 4.4007086753845215 23.329326629638672
Loss :  1.2307294607162476 4.403512001037598 23.248289108276367
Loss :  1.3835062980651855 4.420512676239014 23.48607063293457
Loss :  1.2636744976043701 4.284602165222168 22.68668556213379
Loss :  1.2488245964050293 4.425889015197754 23.37826919555664
Loss :  1.3659327030181885 4.369853496551514 23.215200424194336
Loss :  1.2091199159622192 4.4368791580200195 23.39351463317871
Loss :  1.1988928318023682 4.408424377441406 23.24101448059082
Loss :  1.1986589431762695 4.576369762420654 24.080509185791016
Loss :  1.2279452085494995 4.385404109954834 23.154966354370117
Loss :  1.4432125091552734 4.230762004852295 22.597023010253906
Loss :  1.3889567852020264 4.21263313293457 22.45212173461914
Loss :  1.1687827110290527 4.030880451202393 21.323184967041016
Loss :  1.2641628980636597 4.435857772827148 23.443450927734375
Loss :  1.166639804840088 4.576977729797363 24.05152702331543
Loss :  1.400630235671997 4.485868453979492 23.829973220825195
  batch 20 loss: 1.400630235671997, 4.485868453979492, 23.829973220825195
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.2787121534347534 4.284666061401367 22.702041625976562
Loss :  1.152935266494751 4.101578235626221 21.660825729370117
Loss :  1.219818353652954 4.12343692779541 21.83700180053711
Loss :  1.2857122421264648 4.352692127227783 23.049171447753906
Loss :  1.3780860900878906 4.358341693878174 23.1697940826416
Loss :  1.2279850244522095 3.8110134601593018 20.283052444458008
Loss :  1.252994418144226 4.01528263092041 21.32940673828125
Loss :  1.2461049556732178 3.795391082763672 20.223060607910156
Loss :  1.0533995628356934 4.034226417541504 21.224531173706055
Loss :  1.3722481727600098 4.096190452575684 21.853200912475586
Loss :  1.0662919282913208 4.187300682067871 22.002796173095703
Loss :  1.3218220472335815 4.246930122375488 22.55647087097168
Loss :  1.2287629842758179 4.176347732543945 22.110502243041992
Loss :  1.2182368040084839 4.237702369689941 22.406747817993164
Loss :  1.1223955154418945 4.539212703704834 23.818458557128906
Loss :  1.1701446771621704 4.71297025680542 24.734996795654297
Loss :  1.158790111541748 4.499063491821289 23.65410804748535
Loss :  1.379770278930664 4.450875759124756 23.6341495513916
Loss :  1.3812792301177979 4.44101619720459 23.586360931396484
Loss :  1.4545661211013794 4.539344787597656 24.151290893554688
  batch 40 loss: 1.4545661211013794, 4.539344787597656, 24.151290893554688
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.2913750410079956 4.653289318084717 24.55782127380371
Loss :  1.206425666809082 4.322458744049072 22.81871795654297
Loss :  1.208389401435852 4.359196662902832 23.004371643066406
Loss :  1.23445463180542 4.360780715942383 23.038358688354492
Loss :  1.1574465036392212 4.279090881347656 22.552900314331055
Loss :  1.2764357328414917 4.498315334320068 23.76801300048828
Loss :  1.4009056091308594 4.50136661529541 23.907737731933594
Loss :  1.218206524848938 3.7361462116241455 19.898937225341797
Loss :  1.483189344406128 3.679748058319092 19.881929397583008
Loss :  1.2395919561386108 3.620598316192627 19.34258270263672
Loss :  1.3764289617538452 4.374919414520264 23.251026153564453
Loss :  1.305007815361023 4.523730278015137 23.923660278320312
Loss :  1.28229558467865 4.285048484802246 22.707538604736328
Loss :  1.3914090394973755 4.431093692779541 23.546876907348633
Loss :  1.2304037809371948 4.307767868041992 22.769243240356445
Loss :  1.4727059602737427 4.079556941986084 21.87049102783203
Loss :  1.243189811706543 3.8809471130371094 20.647926330566406
Loss :  1.1958789825439453 3.588117837905884 19.1364688873291
Loss :  1.2425721883773804 3.5550079345703125 19.01761245727539
Loss :  1.498013973236084 3.234816074371338 17.672094345092773
  batch 60 loss: 1.498013973236084, 3.234816074371338, 17.672094345092773
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.1826778650283813 3.6292097568511963 19.32872772216797
Loss :  1.296669602394104 3.3906476497650146 18.249906539916992
Loss :  1.212483286857605 3.5747485160827637 19.086225509643555
Loss :  1.1744335889816284 3.5932774543762207 19.140819549560547
Loss :  1.0895777940750122 3.2386412620544434 17.282785415649414
Loss :  5.881293773651123 4.362480640411377 27.693696975708008
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([2], device='cuda:0')
Loss :  5.853435516357422 4.3775835037231445 27.741352081298828
Loss :  5.981186389923096 4.2760186195373535 27.361278533935547
Loss :  6.220626354217529 4.1296162605285645 26.86870765686035
Total LOSS train 22.270631731473483 valid 27.416258811950684
CE LOSS train 1.2731859353872446 valid 1.5551565885543823
Contrastive LOSS train 4.199489179024329 valid 1.0324040651321411
EPOCH 59:
Loss :  1.3575935363769531 3.6114141941070557 19.41466522216797
Loss :  1.4055615663528442 3.7134103775024414 19.972612380981445
Loss :  1.2901570796966553 3.146721124649048 17.023761749267578
Loss :  1.3251519203186035 3.2840490341186523 17.745397567749023
Loss :  1.378912329673767 3.201097011566162 17.384397506713867
Loss :  1.2420438528060913 3.1896491050720215 17.190290451049805
Loss :  1.394745945930481 3.2329986095428467 17.55974006652832
Loss :  1.2904994487762451 2.9040298461914062 15.810648918151855
Loss :  1.2502461671829224 3.5440847873687744 18.970670700073242
Loss :  1.3845890760421753 3.3348562717437744 18.058870315551758
Loss :  1.2079722881317139 3.684750556945801 19.631723403930664
Loss :  1.2155439853668213 3.952601194381714 20.97854995727539
Loss :  1.198302149772644 3.8769118785858154 20.582860946655273
Loss :  1.222523808479309 3.1576876640319824 17.010963439941406
Loss :  1.457823395729065 3.1600847244262695 17.25824737548828
Loss :  1.4177542924880981 3.2846274375915527 17.840892791748047
Loss :  1.1960639953613281 3.272737979888916 17.55975341796875
Loss :  1.2913686037063599 3.1134417057037354 16.858577728271484
Loss :  1.174826979637146 3.3342888355255127 17.846271514892578
Loss :  1.441406011581421 3.239769220352173 17.6402530670166
  batch 20 loss: 1.441406011581421, 3.239769220352173, 17.6402530670166
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.2826396226882935 3.032388210296631 16.444581985473633
Loss :  1.177259087562561 3.2892370223999023 17.623445510864258
Loss :  1.255730152130127 3.5550172328948975 19.03081512451172
Loss :  1.3109780550003052 3.2286720275878906 17.45433807373047
Loss :  1.4267650842666626 3.6974172592163086 19.91385269165039
Loss :  1.2681679725646973 3.6574971675872803 19.555652618408203
Loss :  1.3010395765304565 3.638068675994873 19.491382598876953
Loss :  1.2812306880950928 3.2280399799346924 17.421430587768555
Loss :  1.0861494541168213 3.292577028274536 17.549034118652344
Loss :  1.4175957441329956 3.2997515201568604 17.916353225708008
Loss :  1.0914528369903564 3.519454002380371 18.688724517822266
Loss :  1.3451159000396729 3.9712300300598145 21.201265335083008
Loss :  1.254406452178955 3.1914570331573486 17.21169090270996
Loss :  1.2528200149536133 3.633694648742676 19.42129135131836
Loss :  1.1255303621292114 3.34531569480896 17.852108001708984
Loss :  1.186863660812378 3.3262782096862793 17.818254470825195
Loss :  1.1815767288208008 3.4272196292877197 18.317676544189453
Loss :  1.4008467197418213 3.0051910877227783 16.426801681518555
Loss :  1.4308627843856812 2.9007089138031006 15.934407234191895
Loss :  1.4657220840454102 2.929433584213257 16.112890243530273
  batch 40 loss: 1.4657220840454102, 2.929433584213257, 16.112890243530273
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.3161001205444336 3.2453105449676514 17.542652130126953
Loss :  1.2772808074951172 2.7932465076446533 15.243513107299805
Loss :  1.2296793460845947 3.2419731616973877 17.439544677734375
Loss :  1.2732326984405518 3.131619930267334 16.931333541870117
Loss :  1.1989400386810303 3.1931989192962646 17.164934158325195
Loss :  1.31051766872406 2.9221580028533936 15.921307563781738
Loss :  1.440268635749817 2.8565027713775635 15.722782135009766
Loss :  1.2525745630264282 2.771045207977295 15.10780143737793
Loss :  1.5005003213882446 3.0330145359039307 16.665573120117188
Loss :  1.2746585607528687 2.9188425540924072 15.868870735168457
Loss :  1.3990578651428223 3.1190755367279053 16.994436264038086
Loss :  1.37717604637146 3.0194132328033447 16.474241256713867
Loss :  1.3073726892471313 3.017313241958618 16.393938064575195
Loss :  1.4106730222702026 3.031970977783203 16.570528030395508
Loss :  1.2600579261779785 3.0494778156280518 16.5074462890625
Loss :  1.4903035163879395 2.972551107406616 16.353059768676758
Loss :  1.2894762754440308 3.138962507247925 16.984289169311523
Loss :  1.238690733909607 3.5265614986419678 18.871498107910156
Loss :  1.2886884212493896 3.422513008117676 18.40125274658203
Loss :  1.5125778913497925 3.140864372253418 17.216899871826172
  batch 60 loss: 1.5125778913497925, 3.140864372253418, 17.216899871826172
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.2381449937820435 3.0937111377716064 16.706701278686523
Loss :  1.3406339883804321 3.792374610900879 20.302505493164062
Loss :  1.2691049575805664 3.1775553226470947 17.15688133239746
Loss :  1.2337191104888916 3.5839967727661133 19.153701782226562
Loss :  1.1614196300506592 2.580648422241211 14.064661979675293
Loss :  25.021120071411133 4.101883411407471 45.53053665161133
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
Loss :  25.059629440307617 4.02513313293457 45.18529510498047
Loss :  25.127368927001953 4.003205299377441 45.143394470214844
Loss :  25.424373626708984 3.992863416671753 45.38869094848633
Total LOSS train 17.622853836646446 valid 45.31197929382324
CE LOSS train 1.3011798345125638 valid 6.356093406677246
Contrastive LOSS train 3.264334803361159 valid 0.9982158541679382
EPOCH 60:
Loss :  1.395964503288269 3.1659560203552246 17.225744247436523
Loss :  1.4332462549209595 3.5059802532196045 18.96314811706543
Loss :  1.3329458236694336 3.4730024337768555 18.697959899902344
Loss :  1.3650455474853516 3.242466926574707 17.57737922668457
Loss :  1.4097614288330078 3.0882887840270996 16.851205825805664
Loss :  1.2913771867752075 3.156200885772705 17.0723819732666
Loss :  1.422524094581604 3.326983690261841 18.05744171142578
Loss :  1.3336609601974487 2.92279052734375 15.947613716125488
Loss :  1.3050421476364136 3.1719603538513184 17.164844512939453
Loss :  1.4179446697235107 2.865302324295044 15.74445629119873
Loss :  1.2696812152862549 3.1097426414489746 16.818395614624023
Loss :  1.2713078260421753 3.0440454483032227 16.491535186767578
Loss :  1.259842038154602 3.6210825443267822 19.36525535583496
Loss :  1.2799876928329468 3.402510643005371 18.29254150390625
Loss :  1.4858179092407227 2.92730450630188 16.12234115600586
Loss :  1.453238844871521 2.9386587142944336 16.14653205871582
Loss :  1.2665451765060425 3.1888158321380615 17.21062469482422
Loss :  1.347169041633606 2.7607309818267822 15.150823593139648
Loss :  1.2467334270477295 2.9034082889556885 15.763774871826172
Loss :  1.4720877408981323 2.9448084831237793 16.196130752563477
  batch 20 loss: 1.4720877408981323, 2.9448084831237793, 16.196130752563477
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.3337708711624146 2.603384494781494 14.350692749023438
Loss :  1.245967149734497 2.8657069206237793 15.574501991271973
Loss :  1.3123629093170166 3.298715353012085 17.805938720703125
Loss :  1.357246994972229 3.2525339126586914 17.619915008544922
Loss :  1.4533734321594238 3.2430012226104736 17.668378829956055
Loss :  1.3194440603256226 3.0126049518585205 16.382469177246094
Loss :  1.3447844982147217 3.1938486099243164 17.314027786254883
Loss :  1.330349326133728 2.8675339221954346 15.668018341064453
Loss :  1.1743074655532837 2.879132032394409 15.569968223571777
Loss :  1.4406335353851318 2.9926180839538574 16.403724670410156
Loss :  1.178829312324524 3.162102222442627 16.98933982849121
Loss :  1.3841493129730225 3.14564847946167 17.11239242553711
Loss :  1.3114020824432373 2.966808557510376 16.145444869995117
Loss :  1.3095871210098267 2.933917999267578 15.979177474975586
Loss :  1.2120015621185303 2.9181647300720215 15.802825927734375
Loss :  1.257908582687378 3.209846258163452 17.307138442993164
Loss :  1.2528727054595947 3.3185055255889893 17.845399856567383
Loss :  1.4404289722442627 2.7234947681427 15.057902336120605
Loss :  1.4552783966064453 3.140916109085083 17.15985870361328
Loss :  1.4844344854354858 3.0409247875213623 16.689058303833008
  batch 40 loss: 1.4844344854354858, 3.0409247875213623, 16.689058303833008
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.3639534711837769 2.6791622638702393 14.759764671325684
Loss :  1.3348420858383179 2.83341908454895 15.501936912536621
Loss :  1.2942036390304565 3.319605827331543 17.89223289489746
Loss :  1.3341684341430664 3.526818037033081 18.968257904052734
Loss :  1.2662584781646729 2.69218111038208 14.727163314819336
Loss :  1.3464494943618774 2.7459511756896973 15.076205253601074
Loss :  1.4570616483688354 2.8951210975646973 15.932666778564453
Loss :  1.306197166442871 2.8444631099700928 15.528512954711914
Loss :  1.5014221668243408 2.9556803703308105 16.27982521057129
Loss :  1.3277091979980469 2.6884803771972656 14.770111083984375
Loss :  1.4179878234863281 3.283308267593384 17.834529876708984
Loss :  1.408544659614563 2.775113582611084 15.284112930297852
Loss :  1.350352168083191 3.1058716773986816 16.879709243774414
Loss :  1.4345526695251465 3.2036705017089844 17.452905654907227
Loss :  1.3097208738327026 2.782546043395996 15.222451210021973
Loss :  1.4942576885223389 3.0193121433258057 16.590818405151367
Loss :  1.3283684253692627 2.7008235454559326 14.832486152648926
Loss :  1.2925920486450195 3.0331943035125732 16.45856475830078
Loss :  1.3312996625900269 3.2395849227905273 17.529224395751953
Loss :  1.5150763988494873 2.854404926300049 15.787100791931152
  batch 60 loss: 1.5150763988494873, 2.854404926300049, 15.787100791931152
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.287381887435913 3.3145952224731445 17.8603572845459
Loss :  1.3731046915054321 2.854081392288208 15.643511772155762
Loss :  1.3130539655685425 3.1651647090911865 17.138877868652344
Loss :  1.2773112058639526 3.499800443649292 18.77631378173828
Loss :  1.219199299812317 2.4639430046081543 13.538914680480957
Loss :  1.4051400423049927 3.9698164463043213 21.254222869873047
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.4340379238128662 3.9429612159729004 21.14884376525879
Loss :  1.428856372833252 3.8731229305267334 20.794469833374023
Loss :  1.4458401203155518 3.866746187210083 20.779571533203125
Total LOSS train 16.578043996370756 valid 20.994277000427246
CE LOSS train 1.3468327008760892 valid 0.36146003007888794
Contrastive LOSS train 3.046242251762977 valid 0.9666865468025208
EPOCH 61:
Loss :  1.425426721572876 3.053931713104248 16.695085525512695
Loss :  1.4582529067993164 3.054387331008911 16.73019027709961
Loss :  1.366740345954895 2.9302167892456055 16.017824172973633
Loss :  1.398012399673462 3.026538133621216 16.530702590942383
Loss :  1.4316071271896362 2.721426486968994 15.038739204406738
Loss :  1.3374005556106567 2.7836525440216064 15.25566291809082
Loss :  1.4390602111816406 3.0756101608276367 16.81711196899414
Loss :  1.364389181137085 2.760484457015991 15.1668119430542
Loss :  1.3338297605514526 3.737881898880005 20.023239135742188
Loss :  1.4200628995895386 3.0732579231262207 16.786352157592773
Loss :  1.3062121868133545 3.2632734775543213 17.62257957458496
Loss :  1.3083126544952393 3.197802782058716 17.297327041625977
Loss :  1.2983359098434448 3.2047274112701416 17.32197380065918
Loss :  1.318062663078308 3.0419600009918213 16.527862548828125
Loss :  1.4918406009674072 2.8922173976898193 15.952927589416504
Loss :  1.468385934829712 3.3415632247924805 18.17620277404785
Loss :  1.307394027709961 3.4160356521606445 18.387571334838867
Loss :  1.3822412490844727 3.06256365776062 16.69506072998047
Loss :  1.292570948600769 2.891430139541626 15.74972152709961
Loss :  1.478789210319519 2.7106854915618896 15.032217025756836
  batch 20 loss: 1.478789210319519, 2.7106854915618896, 15.032217025756836
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.361499309539795 2.4839608669281006 13.781303405761719
Loss :  1.2865958213806152 3.0055530071258545 16.314361572265625
Loss :  1.3388680219650269 3.1317331790924072 16.997533798217773
Loss :  1.3774014711380005 2.840737819671631 15.581090927124023
Loss :  1.460637092590332 3.1136200428009033 17.028736114501953
Loss :  1.338489294052124 2.859295129776001 15.634964942932129
Loss :  1.3704540729522705 3.130995273590088 17.02543067932129
Loss :  1.3511093854904175 2.9722824096679688 16.212520599365234
Loss :  1.2137638330459595 3.0367116928100586 16.397321701049805
Loss :  1.4518588781356812 2.9530739784240723 16.217227935791016
Loss :  1.2212889194488525 2.9351930618286133 15.89725399017334
Loss :  1.4068405628204346 2.7847819328308105 15.330750465393066
Loss :  1.3398140668869019 2.8565402030944824 15.622515678405762
Loss :  1.336873173713684 2.8941333293914795 15.807539939880371
Loss :  1.2492748498916626 3.193730354309082 17.217926025390625
Loss :  1.2891234159469604 3.193946599960327 17.25885581970215
Loss :  1.2838249206542969 2.9251484870910645 15.909566879272461
Loss :  1.4510929584503174 3.6042640209198 19.472414016723633
Loss :  1.4659044742584229 3.097987413406372 16.955841064453125
Loss :  1.490288257598877 2.6230015754699707 14.605295181274414
  batch 40 loss: 1.490288257598877, 2.6230015754699707, 14.605295181274414
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.3781133890151978 2.725428342819214 15.005255699157715
Loss :  1.3542726039886475 2.702345848083496 14.866002082824707
Loss :  1.3211296796798706 3.0323359966278076 16.48280906677246
Loss :  1.356406331062317 2.91424298286438 15.927620887756348
Loss :  1.298570156097412 3.0895321369171143 16.746231079101562
Loss :  1.3732517957687378 2.895585775375366 15.851181030273438
Loss :  1.473280668258667 2.901315450668335 15.9798583984375
Loss :  1.3351808786392212 2.7704110145568848 15.187235832214355
Loss :  1.5100491046905518 2.684414863586426 14.932123184204102
Loss :  1.353263020515442 2.6807022094726562 14.756773948669434
Loss :  1.4352169036865234 2.8104050159454346 15.487241744995117
Loss :  1.427080512046814 2.801999807357788 15.437079429626465
Loss :  1.376737117767334 2.841773509979248 15.58560562133789
Loss :  1.4531947374343872 3.135446071624756 17.13042640686035
Loss :  1.338748812675476 2.851261854171753 15.59505844116211
Loss :  1.50806725025177 3.035801410675049 16.687074661254883
Loss :  1.3584250211715698 3.2883336544036865 17.800092697143555
Loss :  1.3256337642669678 2.726752996444702 14.959399223327637
Loss :  1.3621101379394531 3.185988426208496 17.29205322265625
Loss :  1.5277388095855713 2.5855467319488525 14.455471992492676
  batch 60 loss: 1.5277388095855713, 2.5855467319488525, 14.455471992492676
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.3224636316299438 3.0096466541290283 16.370697021484375
Loss :  1.40027916431427 2.8660805225372314 15.730681419372559
Loss :  1.3438441753387451 2.7930359840393066 15.3090238571167
Loss :  1.3136712312698364 3.0192389488220215 16.409866333007812
Loss :  1.259723424911499 2.488225221633911 13.700849533081055
Loss :  1.458785891532898 3.5192809104919434 19.055191040039062
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.4873632192611694 3.492466926574707 18.949697494506836
Loss :  1.4787336587905884 3.4682862758636475 18.82016372680664
Loss :  1.5075263977050781 3.3255088329315186 18.13507080078125
Total LOSS train 16.196605036808894 valid 18.740030765533447
CE LOSS train 1.37259050149184 valid 0.37688159942626953
Contrastive LOSS train 2.9648028997274545 valid 0.8313772082328796
EPOCH 62:
Loss :  1.4402352571487427 2.7542917728424072 15.21169376373291
Loss :  1.4692455530166626 3.010253667831421 16.5205135345459
Loss :  1.3894429206848145 2.687472105026245 14.826803207397461
Loss :  1.4198113679885864 3.4284279346466064 18.56195068359375
Loss :  1.4522849321365356 2.7340645790100098 15.122607231140137
Loss :  1.3654158115386963 2.3623712062835693 13.177271842956543
Loss :  1.4588987827301025 2.538541078567505 14.151603698730469
Loss :  1.3927876949310303 2.684443473815918 14.8150053024292
Loss :  1.366361141204834 2.816981315612793 15.45126724243164
Loss :  1.4463204145431519 2.8084051609039307 15.488346099853516
Loss :  1.3431373834609985 2.9718286991119385 16.202280044555664
Loss :  1.3439812660217285 2.8774938583374023 15.731451034545898
Loss :  1.3327882289886475 2.8848774433135986 15.75717544555664
Loss :  1.3486156463623047 2.8403122425079346 15.550176620483398
Loss :  1.5056259632110596 2.7833101749420166 15.422176361083984
Loss :  1.4822821617126465 2.767822027206421 15.321392059326172
Loss :  1.3388231992721558 2.969024419784546 16.183944702148438
Loss :  1.4054784774780273 3.1697804927825928 17.25438117980957
Loss :  1.3265259265899658 3.2395706176757812 17.52437973022461
Loss :  1.4960765838623047 2.888302803039551 15.937590599060059
  batch 20 loss: 1.4960765838623047, 2.888302803039551, 15.937590599060059
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.391120433807373 2.896153688430786 15.871889114379883
Loss :  1.3241947889328003 3.177172899246216 17.210058212280273
Loss :  1.3732532262802124 2.8724303245544434 15.735404968261719
Loss :  1.4092991352081299 3.03391695022583 16.57888412475586
Loss :  1.4869444370269775 2.853058099746704 15.75223445892334
Loss :  1.3800489902496338 2.508770227432251 13.92389965057373
Loss :  1.4064669609069824 3.1995153427124023 17.404043197631836
Loss :  1.3887853622436523 3.116652488708496 16.972047805786133
Loss :  1.2672960758209229 2.8045241832733154 15.2899169921875
Loss :  1.478300929069519 3.1245672702789307 17.101137161254883
Loss :  1.2704582214355469 3.093076467514038 16.73583984375
Loss :  1.434564232826233 3.2769558429718018 17.81934356689453
Loss :  1.3729231357574463 3.6224005222320557 19.484926223754883
Loss :  1.3678513765335083 3.4163713455200195 18.44970703125
Loss :  1.2916456460952759 3.493180990219116 18.757549285888672
Loss :  1.329755425453186 2.732992649078369 14.994718551635742
Loss :  1.3266390562057495 3.254936695098877 17.601322174072266
Loss :  1.4809284210205078 3.2395708560943604 17.678783416748047
Loss :  1.4965962171554565 2.881455421447754 15.903873443603516
Loss :  1.5123766660690308 2.7710936069488525 15.367844581604004
  batch 40 loss: 1.5123766660690308, 2.7710936069488525, 15.367844581604004
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.408766269683838 3.113130807876587 16.97442054748535
Loss :  1.3800477981567383 2.8505003452301025 15.632549285888672
Loss :  1.3517979383468628 3.014861583709717 16.426105499267578
Loss :  1.3818037509918213 2.8854260444641113 15.80893325805664
Loss :  1.3302395343780518 2.757394313812256 15.11721134185791
Loss :  1.4014097452163696 3.0579428672790527 16.691123962402344
Loss :  1.4931602478027344 2.772315263748169 15.354736328125
Loss :  1.366616129875183 2.8434574604034424 15.583903312683105
Loss :  1.5295333862304688 3.0717039108276367 16.88805389404297
Loss :  1.3835647106170654 2.9774928092956543 16.271028518676758
Loss :  1.4613449573516846 2.9413256645202637 16.1679744720459
Loss :  1.4555946588516235 3.1680312156677246 17.295751571655273
Loss :  1.4101035594940186 2.994493007659912 16.382568359375
Loss :  1.479027271270752 3.0876047611236572 16.917051315307617
Loss :  1.374473214149475 3.2407124042510986 17.578035354614258
Loss :  1.5309643745422363 2.4591891765594482 13.826910018920898
Loss :  1.3909944295883179 2.6947617530822754 14.864802360534668
Loss :  1.3610496520996094 2.9443321228027344 16.08271026611328
Loss :  1.3928643465042114 4.013718128204346 21.461454391479492
Loss :  1.5457689762115479 3.6477389335632324 19.78446388244629
  batch 60 loss: 1.5457689762115479, 3.6477389335632324, 19.78446388244629
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.3588862419128418 3.071549892425537 16.716636657714844
Loss :  1.430081844329834 2.7226052284240723 15.043107986450195
Loss :  1.3740119934082031 2.4466445446014404 13.607234954833984
Loss :  1.3425955772399902 3.1277260780334473 16.981225967407227
Loss :  1.2906986474990845 2.355828046798706 13.069839477539062
Loss :  1.4624743461608887 4.398453235626221 23.454740524291992
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.4964284896850586 4.412679195404053 23.559825897216797
Loss :  1.4862163066864014 4.326846122741699 23.120447158813477
Loss :  1.5204071998596191 4.341301441192627 23.226913452148438
Total LOSS train 16.23651180267334 valid 23.340481758117676
CE LOSS train 1.4021382570266723 valid 0.3801017999649048
Contrastive LOSS train 2.966874727836022 valid 1.0853253602981567
EPOCH 63:
Loss :  1.4634991884231567 2.726496458053589 15.09598159790039
Loss :  1.4897130727767944 2.916546106338501 16.07244300842285
Loss :  1.4111416339874268 2.5281975269317627 14.052128791809082
Loss :  1.4368740320205688 2.8716187477111816 15.794967651367188
Loss :  1.4663689136505127 2.854076623916626 15.736751556396484
Loss :  1.3845553398132324 2.7713701725006104 15.241405487060547
Loss :  1.4718621969223022 2.927932024002075 16.111522674560547
Loss :  1.409943699836731 3.1452224254608154 17.136056900024414
Loss :  1.3861838579177856 2.9342970848083496 16.05767059326172
Loss :  1.4652765989303589 2.8474016189575195 15.702284812927246
Loss :  1.3701146841049194 3.3445770740509033 18.093000411987305
Loss :  1.3725768327713013 3.0876150131225586 16.810651779174805
Loss :  1.3643724918365479 2.9513099193573 16.120922088623047
Loss :  1.3772070407867432 2.903113603591919 15.89277458190918
Loss :  1.524506688117981 2.914348840713501 16.096250534057617
Loss :  1.5040022134780884 3.1824347972869873 17.416175842285156
Loss :  1.363171100616455 2.9938695430755615 16.33251953125
Loss :  1.4287598133087158 2.9754984378814697 16.306251525878906
Loss :  1.3521230220794678 2.6882641315460205 14.79344367980957
Loss :  1.508323073387146 3.338615894317627 18.20140266418457
  batch 20 loss: 1.508323073387146, 3.338615894317627, 18.20140266418457
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.4115841388702393 2.635483741760254 14.58900260925293
Loss :  1.3497164249420166 2.676082134246826 14.730127334594727
Loss :  1.3958474397659302 2.5423693656921387 14.107694625854492
Loss :  1.4299883842468262 2.824510335922241 15.552539825439453
Loss :  1.4969688653945923 3.09112811088562 16.95261001586914
Loss :  1.3984674215316772 2.979618549346924 16.296560287475586
Loss :  1.4253151416778564 3.0103747844696045 16.477190017700195
Loss :  1.4096014499664307 2.9050545692443848 15.934873580932617
Loss :  1.2891032695770264 2.7123329639434814 14.850768089294434
Loss :  1.4885399341583252 3.07568359375 16.866958618164062
Loss :  1.2932956218719482 3.132465362548828 16.95562171936035
Loss :  1.4500763416290283 2.9109060764312744 16.004606246948242
Loss :  1.392374038696289 3.067711114883423 16.73093032836914
Loss :  1.3882942199707031 2.8969717025756836 15.873152732849121
Loss :  1.3146766424179077 2.8607945442199707 15.618648529052734
Loss :  1.3491020202636719 3.006192207336426 16.380062103271484
Loss :  1.3436282873153687 3.0833652019500732 16.760454177856445
Loss :  1.4889007806777954 2.8888485431671143 15.933143615722656
Loss :  1.4980790615081787 2.9704554080963135 16.35035514831543
Loss :  1.5192359685897827 2.7797698974609375 15.418085098266602
  batch 40 loss: 1.5192359685897827, 2.7797698974609375, 15.418085098266602
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.4250941276550293 3.1588408946990967 17.21929931640625
Loss :  1.3992010354995728 2.9381139278411865 16.089771270751953
Loss :  1.37169349193573 3.2465200424194336 17.604293823242188
Loss :  1.401175618171692 2.799945831298828 15.400904655456543
Loss :  1.3520787954330444 2.6969494819641113 14.836825370788574
Loss :  1.4161784648895264 2.8146414756774902 15.489386558532715
Loss :  1.5019084215164185 2.642857313156128 14.716195106506348
Loss :  1.3863800764083862 2.7321786880493164 15.047273635864258
Loss :  1.538694977760315 3.2719063758850098 17.898225784301758
Loss :  1.4013904333114624 2.7645697593688965 15.224239349365234
Loss :  1.4722309112548828 2.7054669857025146 14.999566078186035
Loss :  1.4660619497299194 2.646566152572632 14.698892593383789
Loss :  1.4226969480514526 2.856865644454956 15.707025527954102
Loss :  1.4887514114379883 3.078185796737671 16.879680633544922
Loss :  1.3894466161727905 2.7312264442443848 15.045578002929688
Loss :  1.5405348539352417 2.8357324600219727 15.719197273254395
Loss :  1.405877709388733 3.363272190093994 18.222238540649414
Loss :  1.3779293298721313 2.914069175720215 15.948275566101074
Loss :  1.4087883234024048 2.9983861446380615 16.400718688964844
Loss :  1.5510165691375732 2.777676582336426 15.439399719238281
  batch 60 loss: 1.5510165691375732, 2.777676582336426, 15.439399719238281
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.3751945495605469 2.850189447402954 15.626141548156738
Loss :  1.4420026540756226 3.422759532928467 18.555801391601562
Loss :  1.3945224285125732 2.778212308883667 15.285584449768066
Loss :  1.3666545152664185 2.6327810287475586 14.530559539794922
Loss :  1.3221276998519897 2.7294137477874756 14.969196319580078
Loss :  1.4550186395645142 4.2150702476501465 22.530370712280273
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.4813319444656372 4.25610876083374 22.761877059936523
Loss :  1.4811958074569702 4.152061462402344 22.24150276184082
Loss :  1.4656422138214111 4.208010673522949 22.50569725036621
Total LOSS train 15.984650171720064 valid 22.509861946105957
CE LOSS train 1.4200154286164504 valid 0.3664105534553528
Contrastive LOSS train 2.9129269489875207 valid 1.0520026683807373
EPOCH 64:
Loss :  1.485828161239624 2.708706855773926 15.029362678527832
Loss :  1.508404016494751 3.2525551319122314 17.77117919921875
Loss :  1.4329904317855835 2.8564364910125732 15.71517276763916
Loss :  1.4544517993927002 2.893782377243042 15.92336368560791
Loss :  1.4805113077163696 2.4981253147125244 13.971138000488281
Loss :  1.4040920734405518 2.6779961585998535 14.794072151184082
Loss :  1.4843685626983643 2.8483946323394775 15.726341247558594
Loss :  1.426835298538208 2.732496738433838 15.08931827545166
Loss :  1.404158353805542 2.7300918102264404 15.054617881774902
Loss :  1.4754949808120728 2.6525051593780518 14.738020896911621
Loss :  1.3870760202407837 2.834552526473999 15.559839248657227
Loss :  1.38857901096344 2.950383424758911 16.14049530029297
Loss :  1.381320834159851 2.6606061458587646 14.684351921081543
Loss :  1.393930196762085 2.839285135269165 15.59035587310791
Loss :  1.5301203727722168 2.8466694355010986 15.763467788696289
Loss :  1.5097521543502808 2.628021001815796 14.649857521057129
Loss :  1.3807520866394043 3.143076181411743 17.096132278442383
Loss :  1.4388782978057861 3.0615594387054443 16.746675491333008
Loss :  1.3716695308685303 2.636228322982788 14.552811622619629
Loss :  1.5141007900238037 2.5658905506134033 14.34355354309082
  batch 20 loss: 1.5141007900238037, 2.5658905506134033, 14.34355354309082
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.4271832704544067 2.55263352394104 14.190350532531738
Loss :  1.3694469928741455 2.8401288986206055 15.570091247558594
Loss :  1.4113186597824097 3.090686798095703 16.8647518157959
Loss :  1.446070671081543 3.0233869552612305 16.563005447387695
Loss :  1.5085312128067017 3.233717441558838 17.6771183013916
Loss :  1.4129003286361694 2.614264965057373 14.484225273132324
Loss :  1.437936782836914 2.7733418941497803 15.304646492004395
Loss :  1.4232019186019897 2.6156630516052246 14.501517295837402
Loss :  1.3162509202957153 3.1668894290924072 17.150697708129883
Loss :  1.5021244287490845 2.919971227645874 16.101980209350586
Loss :  1.3247261047363281 3.210263252258301 17.376041412353516
Loss :  1.4687633514404297 2.9857993125915527 16.39776039123535
Loss :  1.4156643152236938 2.929725408554077 16.06429100036621
Loss :  1.4104087352752686 2.9193952083587646 16.00738525390625
Loss :  1.343564748764038 2.927474021911621 15.980935096740723
Loss :  1.3736014366149902 2.760197639465332 15.174589157104492
Loss :  1.3692928552627563 2.6883554458618164 14.811070442199707
Loss :  1.5007777214050293 3.490048885345459 18.95102310180664
Loss :  1.514467477798462 2.723839521408081 15.133665084838867
Loss :  1.5357022285461426 2.875406503677368 15.912734985351562
  batch 40 loss: 1.5357022285461426, 2.875406503677368, 15.912734985351562
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.445184588432312 3.4311904907226562 18.601137161254883
Loss :  1.421968936920166 2.8912477493286133 15.87820816040039
Loss :  1.3939036130905151 2.8498711585998535 15.643259048461914
Loss :  1.4196425676345825 2.6525986194610596 14.682635307312012
Loss :  1.3738521337509155 3.001486301422119 16.381282806396484
Loss :  1.4305858612060547 2.848902463912964 15.675098419189453
Loss :  1.5105336904525757 2.6780357360839844 14.900712013244629
Loss :  1.4099206924438477 3.244704008102417 17.633441925048828
Loss :  1.5532987117767334 3.359072685241699 18.348663330078125
Loss :  1.4277526140213013 2.848341941833496 15.669462203979492
Loss :  1.4938944578170776 3.0473787784576416 16.73078727722168
Loss :  1.4857786893844604 2.9888622760772705 16.430089950561523
Loss :  1.4415031671524048 2.548292875289917 14.182968139648438
Loss :  1.499311923980713 3.76729154586792 20.335769653320312
Loss :  1.4104640483856201 3.510944128036499 18.96518325805664
Loss :  1.5506823062896729 3.288658618927002 17.993974685668945
Loss :  1.4271024465560913 3.0464696884155273 16.65945053100586
Loss :  1.3982962369918823 2.9690160751342773 16.243375778198242
Loss :  1.427220106124878 3.458613395690918 18.720287322998047
Loss :  1.557689905166626 2.7281148433685303 15.198264122009277
  batch 60 loss: 1.557689905166626, 2.7281148433685303, 15.198264122009277
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.3941607475280762 2.618621587753296 14.487268447875977
Loss :  1.45665442943573 2.5207700729370117 14.060504913330078
Loss :  1.4101451635360718 2.979862928390503 16.309459686279297
Loss :  1.3813683986663818 3.151768445968628 17.14021110534668
Loss :  1.3402222394943237 2.83648419380188 15.522643089294434
Loss :  1.4878365993499756 3.9242441654205322 21.109058380126953
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.50621497631073 4.071086406707764 21.86164665222168
Loss :  1.5135855674743652 3.8255951404571533 20.64156150817871
Loss :  1.4701659679412842 3.6472537517547607 19.70643424987793
Total LOSS train 16.023879168583797 valid 20.82967519760132
CE LOSS train 1.4373290171990027 valid 0.36754149198532104
Contrastive LOSS train 2.91731004348168 valid 0.9118134379386902
EPOCH 65:
Loss :  1.4897301197052002 2.863513708114624 15.80729866027832
Loss :  1.5102918148040771 3.112403392791748 17.072309494018555
Loss :  1.4445035457611084 2.6361045837402344 14.62502670288086
Loss :  1.4635759592056274 3.050018310546875 16.713666915893555
Loss :  1.4936394691467285 2.746788263320923 15.227581024169922
Loss :  1.412797451019287 2.679520606994629 14.810400009155273
Loss :  1.4950997829437256 3.1171011924743652 17.08060646057129
Loss :  1.440519094467163 2.741304636001587 15.147042274475098
Loss :  1.4209504127502441 2.904446601867676 15.943183898925781
Loss :  1.4914864301681519 3.2920684814453125 17.951828002929688
Loss :  1.3994526863098145 3.810410976409912 20.451507568359375
Loss :  1.4021058082580566 3.1185007095336914 16.994609832763672
Loss :  1.3975862264633179 3.324474334716797 18.01995849609375
Loss :  1.41059410572052 3.863499402999878 20.728092193603516
Loss :  1.5416879653930664 3.4962687492370605 19.023033142089844
Loss :  1.5121961832046509 3.2890450954437256 17.957420349121094
Loss :  1.3976588249206543 3.648123264312744 19.638275146484375
Loss :  1.4460722208023071 2.844775438308716 15.669949531555176
Loss :  1.3887029886245728 3.1654937267303467 17.216171264648438
Loss :  1.531538724899292 2.9666361808776855 16.36471939086914
  batch 20 loss: 1.531538724899292, 2.9666361808776855, 16.36471939086914
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.4510114192962646 3.0351152420043945 16.6265869140625
Loss :  1.3884254693984985 3.5811610221862793 19.294231414794922
Loss :  1.4284049272537231 2.7663791179656982 15.260300636291504
Loss :  1.4573209285736084 2.7480063438415527 15.19735336303711
Loss :  1.5152842998504639 2.8825836181640625 15.928202629089355
Loss :  1.4251891374588013 3.4097864627838135 18.474123001098633
Loss :  1.445583701133728 3.2421467304229736 17.65631675720215
Loss :  1.4353281259536743 3.230949640274048 17.590076446533203
Loss :  1.3281419277191162 2.883815050125122 15.747217178344727
Loss :  1.5064164400100708 3.175245761871338 17.382644653320312
Loss :  1.3318257331848145 3.626837968826294 19.466014862060547
Loss :  1.4721896648406982 3.5481791496276855 19.213085174560547
Loss :  1.4162002801895142 2.760545015335083 15.218925476074219
Loss :  1.412566065788269 2.8152709007263184 15.488921165466309
Loss :  1.3473408222198486 3.1552133560180664 17.1234073638916
Loss :  1.3783529996871948 2.9978294372558594 16.36750030517578
Loss :  1.3749127388000488 3.0145721435546875 16.447772979736328
Loss :  1.498856782913208 3.1389267444610596 17.193490982055664
Loss :  1.5109646320343018 2.784987211227417 15.435900688171387
Loss :  1.530120849609375 2.7899763584136963 15.480002403259277
  batch 40 loss: 1.530120849609375, 2.7899763584136963, 15.480002403259277
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.4438917636871338 3.274629831314087 17.817041397094727
Loss :  1.4220020771026611 2.7395358085632324 15.119681358337402
Loss :  1.399876356124878 2.677978277206421 14.78976821899414
Loss :  1.4230085611343384 3.034379243850708 16.59490394592285
Loss :  1.383523941040039 2.884730577468872 15.80717658996582
Loss :  1.4456443786621094 2.540247678756714 14.146883010864258
Loss :  1.519255518913269 3.385443687438965 18.446474075317383
Loss :  1.4158096313476562 2.847148895263672 15.651554107666016
Loss :  1.551631212234497 2.824669122695923 15.674976348876953
Loss :  1.4250421524047852 2.300110340118408 12.925593376159668
Loss :  1.4963340759277344 3.703974485397339 20.016206741333008
Loss :  1.4856889247894287 2.573319435119629 14.352286338806152
Loss :  1.4468094110488892 2.9720346927642822 16.306982040405273
Loss :  1.4997276067733765 3.146151304244995 17.230484008789062
Loss :  1.415832757949829 2.6034693717956543 14.43317985534668
Loss :  1.5523606538772583 2.9994242191314697 16.549482345581055
Loss :  1.437605619430542 3.6311874389648438 19.593542098999023
Loss :  1.4071131944656372 2.63765811920166 14.595403671264648
Loss :  1.434712290763855 2.8034377098083496 15.45190143585205
Loss :  1.5604878664016724 2.5070152282714844 14.095563888549805
  batch 60 loss: 1.5604878664016724, 2.5070152282714844, 14.095563888549805
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 5], device='cuda:0')
Loss :  1.3992937803268433 2.615337610244751 14.475981712341309
Loss :  1.46360182762146 2.3805367946624756 13.366286277770996
Loss :  1.42078697681427 2.70158052444458 14.928689002990723
Loss :  1.4032896757125854 3.6548984050750732 19.67778205871582
Loss :  1.3604987859725952 2.2925193309783936 12.823095321655273
Loss :  1.4614338874816895 4.2117390632629395 22.52012825012207
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.482913613319397 4.287306308746338 22.919445037841797
Loss :  1.4782854318618774 4.069106101989746 21.823816299438477
Loss :  1.4677960872650146 4.143916606903076 22.187379837036133
Total LOSS train 16.52162575354943 valid 22.36269235610962
CE LOSS train 1.4455147046309251 valid 0.36694902181625366
Contrastive LOSS train 3.015222200980553 valid 1.035979151725769
EPOCH 66:
Loss :  1.494056224822998 2.505281925201416 14.020465850830078
Loss :  1.5135791301727295 3.0159459114074707 16.593307495117188
Loss :  1.455815076828003 2.7316572666168213 15.11410140991211
Loss :  1.473238468170166 2.4061295986175537 13.503887176513672
Loss :  1.5043771266937256 2.679603099822998 14.902393341064453
Loss :  1.4298622608184814 2.5967085361480713 14.41340446472168
Loss :  1.5061589479446411 3.074795722961426 16.880138397216797
Loss :  1.455573558807373 3.3725340366363525 18.31824493408203
Loss :  1.43821382522583 2.9547464847564697 16.211946487426758
Loss :  1.5063868761062622 2.7481725215911865 15.247249603271484
Loss :  1.423029899597168 2.791220188140869 15.379130363464355
Loss :  1.4236211776733398 2.840984344482422 15.62854290008545
Loss :  1.4186121225357056 3.3270339965820312 18.053781509399414
Loss :  1.4268348217010498 3.5411412715911865 19.132539749145508
Loss :  1.551662564277649 3.0183212757110596 16.643268585205078
Loss :  1.5234366655349731 2.808760643005371 15.567239761352539
Loss :  1.4129101037979126 3.3848395347595215 18.337108612060547
Loss :  1.461856722831726 3.415353536605835 18.538623809814453
Loss :  1.4038563966751099 2.7653322219848633 15.230517387390137
Loss :  1.5372649431228638 2.4912948608398438 13.993739128112793
  batch 20 loss: 1.5372649431228638, 2.4912948608398438, 13.993739128112793
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.4591528177261353 2.766899347305298 15.293649673461914
Loss :  1.404478669166565 3.280471086502075 17.806833267211914
Loss :  1.4443964958190918 2.738694667816162 15.137870788574219
Loss :  1.4730652570724487 2.636666774749756 14.656399726867676
Loss :  1.5280364751815796 3.4713335037231445 18.884702682495117
Loss :  1.4410400390625 2.4704649448394775 13.793364524841309
Loss :  1.4624475240707397 3.118028163909912 17.052589416503906
Loss :  1.449393391609192 3.52290678024292 19.063928604125977
Loss :  1.3479641675949097 3.124861001968384 16.97226905822754
Loss :  1.5166925191879272 2.5608785152435303 14.321084976196289
Loss :  1.3529185056686401 3.3065760135650635 17.885799407958984
Loss :  1.4874258041381836 3.7909936904907227 20.442394256591797
Loss :  1.4353938102722168 3.0771801471710205 16.8212947845459
Loss :  1.4332078695297241 2.7558412551879883 15.212413787841797
Loss :  1.3696895837783813 3.2135136127471924 17.437257766723633
Loss :  1.400198221206665 2.977207660675049 16.286235809326172
Loss :  1.3956587314605713 2.743072748184204 15.111021995544434
Loss :  1.5133477449417114 2.9309794902801514 16.168245315551758
Loss :  1.5261529684066772 2.866743803024292 15.859871864318848
Loss :  1.5463286638259888 4.093382358551025 22.013240814208984
  batch 40 loss: 1.5463286638259888, 4.093382358551025, 22.013240814208984
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.4609012603759766 2.803955554962158 15.48067855834961
Loss :  1.443084716796875 3.767746925354004 20.281818389892578
Loss :  1.4222495555877686 3.8331985473632812 20.588241577148438
Loss :  1.4428908824920654 3.1438751220703125 17.16226577758789
Loss :  1.404761791229248 4.197309494018555 22.39130973815918
Loss :  1.460986614227295 3.640681505203247 19.66439437866211
Loss :  1.5291584730148315 2.1710283756256104 12.384300231933594
Loss :  1.43341863155365 2.618213176727295 14.524484634399414
Loss :  1.5589061975479126 3.8926401138305664 21.022106170654297
Loss :  1.4420535564422607 3.365118980407715 18.267648696899414
Loss :  1.5081099271774292 3.1759283542633057 17.38775062561035
Loss :  1.4963343143463135 3.404109001159668 18.51688003540039
Loss :  1.4608060121536255 2.7384233474731445 15.152922630310059
Loss :  1.5125012397766113 3.029813051223755 16.66156578063965
Loss :  1.4344186782836914 2.741024971008301 15.139543533325195
Loss :  1.559134602546692 2.3423805236816406 13.271037101745605
Loss :  1.4481791257858276 2.729994297027588 15.098150253295898
Loss :  1.42075777053833 2.9729926586151123 16.285720825195312
Loss :  1.4489728212356567 3.1778922080993652 17.33843421936035
Loss :  1.5719397068023682 2.8498432636260986 15.821155548095703
  batch 60 loss: 1.5719397068023682, 2.8498432636260986, 15.821155548095703
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.416569471359253 2.8679702281951904 15.756421089172363
Loss :  1.4758801460266113 2.6363365650177 14.657562255859375
Loss :  1.4351617097854614 3.3412773609161377 18.14154815673828
Loss :  1.4168248176574707 2.815889835357666 15.496273040771484
Loss :  1.3762985467910767 2.5072107315063477 13.912352561950684
Loss :  1.5024992227554321 4.362663269042969 23.315814971923828
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.51175057888031 4.334211349487305 23.18280792236328
Loss :  1.5054787397384644 4.210639476776123 22.55867576599121
Loss :  1.5402886867523193 4.041265487670898 21.74661636352539
Total LOSS train 16.589764081514797 valid 22.700978755950928
CE LOSS train 1.4604251806552593 valid 0.38507217168807983
Contrastive LOSS train 3.025867795944214 valid 1.0103163719177246
EPOCH 67:
Loss :  1.5081599950790405 2.987616777420044 16.446243286132812
Loss :  1.5299935340881348 3.038924217224121 16.7246150970459
Loss :  1.4704217910766602 2.571643114089966 14.32863712310791
Loss :  1.488762378692627 2.618561267852783 14.581567764282227
Loss :  1.5177818536758423 2.6762173175811768 14.898868560791016
Loss :  1.4487087726593018 3.1780521869659424 17.338970184326172
Loss :  1.5248953104019165 2.971621513366699 16.38300323486328
Loss :  1.4726815223693848 2.8461830615997314 15.703596115112305
Loss :  1.4527149200439453 3.4446053504943848 18.67574119567871
Loss :  1.5171289443969727 2.735950469970703 15.196881294250488
Loss :  1.4322497844696045 3.315563917160034 18.010068893432617
Loss :  1.4352200031280518 2.580129384994507 14.335866928100586
Loss :  1.4303628206253052 2.8846609592437744 15.853667259216309
Loss :  1.4401034116744995 2.95101261138916 16.195165634155273
Loss :  1.5626384019851685 2.626431703567505 14.694796562194824
Loss :  1.536746621131897 2.9197616577148438 16.135555267333984
Loss :  1.431728720664978 2.8782474994659424 15.822965621948242
Loss :  1.4775288105010986 2.5694334506988525 14.324695587158203
Loss :  1.421717643737793 2.5441062450408936 14.14224910736084
Loss :  1.5513544082641602 2.6113288402557373 14.607998847961426
  batch 20 loss: 1.5513544082641602, 2.6113288402557373, 14.607998847961426
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.4768065214157104 2.5780210494995117 14.366911888122559
Loss :  1.42145574092865 3.1084628105163574 16.963769912719727
Loss :  1.46274733543396 2.9080731868743896 16.00311279296875
Loss :  1.4899365901947021 2.6933085918426514 14.956480026245117
Loss :  1.541747808456421 3.5033061504364014 19.058279037475586
Loss :  1.460841417312622 2.413578510284424 13.528733253479004
Loss :  1.479533076286316 2.7779035568237305 15.369050979614258
Loss :  1.4698758125305176 2.6654775142669678 14.797264099121094
Loss :  1.3711204528808594 2.206536054611206 12.403800964355469
Loss :  1.5325337648391724 2.3773748874664307 13.419407844543457
Loss :  1.372506856918335 2.9430530071258545 16.087772369384766
Loss :  1.5008050203323364 2.7799577713012695 15.400593757629395
Loss :  1.4541094303131104 2.563084363937378 14.26953125
Loss :  1.4533412456512451 3.2179627418518066 17.543153762817383
Loss :  1.3910901546478271 3.608628749847412 19.434234619140625
Loss :  1.4198057651519775 3.135291814804077 17.09626579284668
Loss :  1.4157904386520386 2.9999327659606934 16.415454864501953
Loss :  1.5276345014572144 2.753753185272217 15.296401023864746
Loss :  1.5422651767730713 2.755941152572632 15.32197093963623
Loss :  1.5593416690826416 2.6194729804992676 14.656705856323242
  batch 40 loss: 1.5593416690826416, 2.6194729804992676, 14.656705856323242
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.4782637357711792 2.6797173023223877 14.876850128173828
Loss :  1.460625171661377 2.967402696609497 16.297637939453125
Loss :  1.4376981258392334 2.9800195693969727 16.33779525756836
Loss :  1.460158109664917 3.2583236694335938 17.75177574157715
Loss :  1.420613169670105 2.9098024368286133 15.969625473022461
Loss :  1.4733119010925293 2.951180934906006 16.229217529296875
Loss :  1.540728211402893 3.032391309738159 16.70268440246582
Loss :  1.450770616531372 2.6266531944274902 14.584036827087402
Loss :  1.5739495754241943 2.7977163791656494 15.562531471252441
Loss :  1.460224986076355 2.5729873180389404 14.325161933898926
Loss :  1.5234942436218262 3.257859706878662 17.812793731689453
Loss :  1.5126025676727295 2.8120710849761963 15.572957992553711
Loss :  1.4766886234283447 2.37819766998291 13.367676734924316
Loss :  1.53045654296875 2.488215208053589 13.971532821655273
Loss :  1.454365611076355 3.3766305446624756 18.33751678466797
Loss :  1.5742911100387573 2.4756760597229004 13.95267105102539
Loss :  1.4702260494232178 3.1665170192718506 17.302810668945312
Loss :  1.4411394596099854 3.016648530960083 16.524381637573242
Loss :  1.4642539024353027 3.037944793701172 16.65397834777832
Loss :  1.5794767141342163 2.991518497467041 16.53706932067871
  batch 60 loss: 1.5794767141342163, 2.991518497467041, 16.53706932067871
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.43466055393219 3.10526442527771 16.960983276367188
Loss :  1.48971426486969 2.852832078933716 15.753874778747559
Loss :  1.4511882066726685 3.3180887699127197 18.04163360595703
Loss :  1.4297500848770142 3.2253191471099854 17.556346893310547
Loss :  1.3929580450057983 3.3862173557281494 18.324045181274414
Loss :  1.4654443264007568 4.14760160446167 22.203453063964844
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.4858624935150146 4.153050899505615 22.251117706298828
Loss :  1.4750030040740967 4.062364101409912 21.786823272705078
Loss :  1.5147032737731934 3.9943318367004395 21.48636245727539
Total LOSS train 15.878425202002893 valid 21.931939125061035
CE LOSS train 1.476550738628094 valid 0.37867581844329834
Contrastive LOSS train 2.880374893775353 valid 0.9985829591751099
EPOCH 68:
Loss :  1.5208152532577515 3.1267690658569336 17.154661178588867
Loss :  1.5440069437026978 3.1787467002868652 17.437740325927734
Loss :  1.484712839126587 3.2232842445373535 17.601133346557617
Loss :  1.4976366758346558 3.3403029441833496 18.19915199279785
Loss :  1.5244771242141724 2.792354106903076 15.486248016357422
Loss :  1.4556324481964111 2.7857394218444824 15.384329795837402
Loss :  1.52470064163208 3.1466798782348633 17.258100509643555
Loss :  1.4753285646438599 3.1179544925689697 17.065101623535156
Loss :  1.4581023454666138 2.400631904602051 13.461261749267578
Loss :  1.5212578773498535 2.9444587230682373 16.24355125427246
Loss :  1.4449764490127563 3.107935905456543 16.984655380249023
Loss :  1.4496601819992065 2.977695941925049 16.338138580322266
Loss :  1.4447898864746094 3.1500630378723145 17.195104598999023
Loss :  1.4544792175292969 3.1585495471954346 17.24722671508789
Loss :  1.5681627988815308 3.131472110748291 17.225522994995117
Loss :  1.54623281955719 3.5076401233673096 19.08443260192871
Loss :  1.4407777786254883 4.258100986480713 22.731281280517578
Loss :  1.4858646392822266 4.036740779876709 21.66956901550293
Loss :  1.4235626459121704 3.4951441287994385 18.89928436279297
Loss :  1.5411309003829956 2.9292585849761963 16.187423706054688
  batch 20 loss: 1.5411309003829956, 2.9292585849761963, 16.187423706054688
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.4688704013824463 3.665062189102173 19.79418182373047
Loss :  1.4163100719451904 3.293917179107666 17.885894775390625
Loss :  1.4508525133132935 2.7722742557525635 15.312223434448242
Loss :  1.4762870073318481 3.298142194747925 17.966999053955078
Loss :  1.5325677394866943 3.203049898147583 17.54781723022461
Loss :  1.4473618268966675 2.847785711288452 15.686290740966797
Loss :  1.4673351049423218 2.669398546218872 14.814327239990234
Loss :  1.455480933189392 2.954357385635376 16.22726821899414
Loss :  1.358700156211853 3.0021870136260986 16.3696346282959
Loss :  1.5266666412353516 3.3067407608032227 18.06036949157715
Loss :  1.36355459690094 3.383206367492676 18.279584884643555
Loss :  1.494018793106079 3.741569995880127 20.201868057250977
Loss :  1.444629430770874 3.017953395843506 16.53439712524414
Loss :  1.4423741102218628 2.8903403282165527 15.894076347351074
Loss :  1.3729383945465088 3.381924629211426 18.282560348510742
Loss :  1.402340054512024 3.282696485519409 17.81582260131836
Loss :  1.3958405256271362 3.18054461479187 17.29856300354004
Loss :  1.5087506771087646 3.139193534851074 17.2047176361084
Loss :  1.5221264362335205 2.9015653133392334 16.029953002929688
Loss :  1.540068507194519 2.71254301071167 15.102784156799316
  batch 40 loss: 1.540068507194519, 2.71254301071167, 15.102784156799316
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.4580973386764526 2.7213540077209473 15.06486701965332
Loss :  1.4380342960357666 2.743048667907715 15.153277397155762
Loss :  1.416637659072876 3.2807555198669434 17.820415496826172
Loss :  1.440245270729065 3.1619861125946045 17.25017547607422
Loss :  1.3997880220413208 3.2939987182617188 17.869781494140625
Loss :  1.4601067304611206 3.2108147144317627 17.514179229736328
Loss :  1.530320167541504 2.7621874809265137 15.34125804901123
Loss :  1.428812861442566 2.7457287311553955 15.157456398010254
Loss :  1.5594998598098755 2.547586679458618 14.297432899475098
Loss :  1.4397163391113281 2.6434824466705322 14.65712833404541
Loss :  1.5065374374389648 2.9985105991363525 16.49909019470215
Loss :  1.4946374893188477 2.7374768257141113 15.182021141052246
Loss :  1.4597339630126953 2.7004780769348145 14.96212387084961
Loss :  1.515014410018921 3.4191348552703857 18.610689163208008
Loss :  1.4331129789352417 2.663743495941162 14.7518310546875
Loss :  1.561132550239563 2.515272617340088 14.137495040893555
Loss :  1.4502832889556885 3.3000588417053223 17.950576782226562
Loss :  1.4200694561004639 3.20548677444458 17.44750213623047
Loss :  1.449035406112671 3.4932568073272705 18.915319442749023
Loss :  1.569061279296875 2.796393394470215 15.55102825164795
  batch 60 loss: 1.569061279296875, 2.796393394470215, 15.55102825164795
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.4121907949447632 3.1428258419036865 17.126319885253906
Loss :  1.4734426736831665 3.441685199737549 18.681867599487305
Loss :  1.426911473274231 2.63832950592041 14.618558883666992
Loss :  1.4096919298171997 2.7256453037261963 15.037918090820312
Loss :  1.3705356121063232 2.923182249069214 15.98644733428955
Loss :  1.4544572830200195 4.237060070037842 22.639759063720703
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 5], device='cuda:0')
Loss :  1.4764877557754517 4.335379123687744 23.153383255004883
Loss :  1.4760011434555054 4.1522955894470215 22.23748016357422
Loss :  1.4448121786117554 4.196240425109863 22.426013946533203
Total LOSS train 16.87304636148306 valid 22.614159107208252
CE LOSS train 1.467938942175645 valid 0.36120304465293884
Contrastive LOSS train 3.081021521641658 valid 1.0490601062774658
EPOCH 69:
Loss :  1.5012174844741821 3.0569911003112793 16.78617286682129
Loss :  1.5241390466690063 3.258906602859497 17.81867218017578
Loss :  1.474303960800171 3.040586233139038 16.677234649658203
Loss :  1.484392762184143 3.773270845413208 20.350746154785156
Loss :  1.514101266860962 2.58373761177063 14.432788848876953
Loss :  1.4440988302230835 2.6894338130950928 14.891267776489258
Loss :  1.5173320770263672 3.092276096343994 16.97871208190918
Loss :  1.4669902324676514 3.396796703338623 18.450973510742188
Loss :  1.4460456371307373 3.7693581581115723 20.292835235595703
Loss :  1.5078814029693604 2.8909754753112793 15.962759017944336
Loss :  1.426832914352417 3.1719796657562256 17.286731719970703
Loss :  1.4285731315612793 3.228541612625122 17.57128143310547
Loss :  1.4198555946350098 2.977677345275879 16.308242797851562
Loss :  1.428290843963623 3.218076467514038 17.518672943115234
Loss :  1.5473484992980957 3.146484613418579 17.27977180480957
Loss :  1.5278328657150269 3.2007901668548584 17.531782150268555
Loss :  1.4143091440200806 3.3810760974884033 18.31968879699707
Loss :  1.4646779298782349 3.480682611465454 18.868091583251953
Loss :  1.4056015014648438 2.7963624000549316 15.387413024902344
Loss :  1.5382120609283447 2.8404312133789062 15.740367889404297
  batch 20 loss: 1.5382120609283447, 2.8404312133789062, 15.740367889404297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.4554458856582642 3.502952814102173 18.970211029052734
Loss :  1.4028443098068237 3.3554301261901855 18.179994583129883
Loss :  1.439546823501587 3.1087565422058105 16.98332977294922
Loss :  1.4642878770828247 2.7646327018737793 15.28745174407959
Loss :  1.5221749544143677 3.084794044494629 16.94614601135254
Loss :  1.435654640197754 2.7950022220611572 15.410665512084961
Loss :  1.4571785926818848 3.0901527404785156 16.907941818237305
Loss :  1.4477207660675049 2.8846628665924072 15.871034622192383
Loss :  1.3479230403900146 2.914320945739746 15.919528007507324
Loss :  1.516951560974121 2.776658535003662 15.40024471282959
Loss :  1.3504045009613037 3.315836191177368 17.92958641052246
Loss :  1.4856469631195068 2.7511301040649414 15.241297721862793
Loss :  1.4379655122756958 2.6310296058654785 14.59311294555664
Loss :  1.43800687789917 2.915705680847168 16.01653480529785
Loss :  1.3696249723434448 3.7898802757263184 20.319026947021484
Loss :  1.400889277458191 2.9895246028900146 16.348512649536133
Loss :  1.3981884717941284 3.4982147216796875 18.88926124572754
Loss :  1.516080379486084 3.490595579147339 18.969058990478516
Loss :  1.5297688245773315 2.925687074661255 16.158203125
Loss :  1.545845866203308 3.18415904045105 17.46664047241211
  batch 40 loss: 1.545845866203308, 3.18415904045105, 17.46664047241211
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.4612833261489868 2.7761905193328857 15.342236518859863
Loss :  1.438519835472107 2.9427599906921387 16.152320861816406
Loss :  1.4169845581054688 3.103828191757202 16.936126708984375
Loss :  1.4360803365707397 3.2294564247131348 17.583362579345703
Loss :  1.3970823287963867 3.392960548400879 18.36188507080078
Loss :  1.4531711339950562 2.749953269958496 15.202937126159668
Loss :  1.521270751953125 2.7798845767974854 15.420693397521973
Loss :  1.4254299402236938 2.9345476627349854 16.098167419433594
Loss :  1.5498684644699097 2.8003909587860107 15.551823616027832
Loss :  1.4318087100982666 2.9278018474578857 16.070817947387695
Loss :  1.5038028955459595 2.6112468242645264 14.560037612915039
Loss :  1.48939847946167 2.733666181564331 15.157730102539062
Loss :  1.4548970460891724 2.751027822494507 15.210036277770996
Loss :  1.5096855163574219 3.155255079269409 17.285961151123047
Loss :  1.431502103805542 2.86977219581604 15.780363082885742
Loss :  1.5644254684448242 2.808932065963745 15.609086036682129
Loss :  1.4486750364303589 3.2780864238739014 17.839107513427734
Loss :  1.4184993505477905 3.089831590652466 16.867656707763672
Loss :  1.4450054168701172 3.015425205230713 16.522130966186523
Loss :  1.5650639533996582 2.3740456104278564 13.435291290283203
  batch 60 loss: 1.5650639533996582, 2.3740456104278564, 13.435291290283203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.4056205749511719 2.4274306297302246 13.542774200439453
Loss :  1.4696012735366821 2.7448785305023193 15.193994522094727
Loss :  1.4247546195983887 2.892488956451416 15.887199401855469
Loss :  1.4059652090072632 2.8099241256713867 15.455585479736328
Loss :  1.3639554977416992 2.612203359603882 14.424972534179688
Loss :  1.5060036182403564 4.396141052246094 23.486709594726562
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.5219814777374268 4.356471538543701 23.304340362548828
Loss :  1.5286147594451904 4.243279457092285 22.745010375976562
Loss :  1.4850529432296753 4.190273761749268 22.436420440673828
Total LOSS train 16.580835195688103 valid 22.993120193481445
CE LOSS train 1.4596390632482676 valid 0.3712632358074188
Contrastive LOSS train 3.0242392283219557 valid 1.047568440437317
EPOCH 70:
Loss :  1.4967187643051147 2.8759357929229736 15.876397132873535
Loss :  1.519364356994629 2.8243234157562256 15.640981674194336
Loss :  1.4610397815704346 2.5474050045013428 14.198064804077148
Loss :  1.4779326915740967 3.0129458904266357 16.542661666870117
Loss :  1.5118943452835083 2.814185380935669 15.582820892333984
Loss :  1.432515263557434 3.477431297302246 18.819673538208008
Loss :  1.5080193281173706 3.5403692722320557 19.20986557006836
Loss :  1.455672264099121 2.916792154312134 16.03963279724121
Loss :  1.4396964311599731 3.1147890090942383 17.013641357421875
Loss :  1.5044862031936646 3.4521853923797607 18.765413284301758
Loss :  1.4257773160934448 3.4497036933898926 18.67429542541504
Loss :  1.4255450963974 3.1443169116973877 17.14712905883789
Loss :  1.4254281520843506 3.1131162643432617 16.991008758544922
Loss :  1.433618187904358 2.6819348335266113 14.843292236328125
Loss :  1.5501160621643066 2.886225461959839 15.981243133544922
Loss :  1.5263915061950684 3.6719141006469727 19.885961532592773
Loss :  1.420268177986145 3.787285327911377 20.3566951751709
Loss :  1.4608118534088135 3.3816819190979004 18.369220733642578
Loss :  1.4083669185638428 2.806874990463257 15.442742347717285
Loss :  1.5411767959594727 3.511986255645752 19.10110855102539
  batch 20 loss: 1.5411767959594727, 3.511986255645752, 19.10110855102539
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 5], device='cuda:0')
Loss :  1.4595946073532104 3.21242618560791 17.521724700927734
Loss :  1.4078338146209717 4.034497261047363 21.580318450927734
Loss :  1.4396710395812988 3.181363105773926 17.346487045288086
Loss :  1.470348596572876 3.029419422149658 16.61744499206543
Loss :  1.5230705738067627 3.113008737564087 17.08811378479004
Loss :  1.4472730159759521 2.699568748474121 14.945116996765137
Loss :  1.4680591821670532 3.384263515472412 18.38937759399414
Loss :  1.4551723003387451 3.789232015609741 20.401330947875977
Loss :  1.3583788871765137 2.829517364501953 15.505966186523438
Loss :  1.5250381231307983 3.067866325378418 16.864370346069336
Loss :  1.358156442642212 3.2283995151519775 17.500154495239258
Loss :  1.4926609992980957 3.0102665424346924 16.54399299621582
Loss :  1.442661166191101 3.25429105758667 17.7141170501709
Loss :  1.4350045919418335 3.373396396636963 18.301986694335938
Loss :  1.367035984992981 3.3637542724609375 18.185808181762695
Loss :  1.3934810161590576 2.8980987071990967 15.8839750289917
Loss :  1.3880352973937988 3.2928924560546875 17.852497100830078
Loss :  1.5080509185791016 3.2620673179626465 17.818387985229492
Loss :  1.5176756381988525 2.5719528198242188 14.377439498901367
Loss :  1.5375783443450928 2.6449856758117676 14.762506484985352
  batch 40 loss: 1.5375783443450928, 2.6449856758117676, 14.762506484985352
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.4519379138946533 2.887014627456665 15.88701057434082
Loss :  1.4258896112442017 3.000328779220581 16.427534103393555
Loss :  1.4082751274108887 2.9175922870635986 15.996236801147461
Loss :  1.4320666790008545 3.9563844203948975 21.213987350463867
Loss :  1.387529969215393 3.4085371494293213 18.43021583557129
Loss :  1.4500596523284912 3.3517072200775146 18.208595275878906
Loss :  1.5237373113632202 2.6588711738586426 14.818092346191406
Loss :  1.4218740463256836 2.4988558292388916 13.916152954101562
Loss :  1.55465567111969 2.9940385818481445 16.52484893798828
Loss :  1.4263771772384644 2.8612444400787354 15.732599258422852
Loss :  1.49983811378479 3.249006986618042 17.744873046875
Loss :  1.4808753728866577 3.172990560531616 17.345829010009766
Loss :  1.4357755184173584 2.9410500526428223 16.14102554321289
Loss :  1.4903442859649658 2.9891226291656494 16.435956954956055
Loss :  1.4080560207366943 3.013047695159912 16.473295211791992
Loss :  1.5363494157791138 2.6685476303100586 14.879087448120117
Loss :  1.4179984331130981 2.8506834506988525 15.671415328979492
Loss :  1.3873261213302612 3.803241729736328 20.403533935546875
Loss :  1.418223261833191 3.362919330596924 18.232818603515625
Loss :  1.553804636001587 2.927711248397827 16.19236183166504
  batch 60 loss: 1.553804636001587, 2.927711248397827, 16.19236183166504
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.382243037223816 3.4625821113586426 18.695152282714844
Loss :  1.4501010179519653 2.692401647567749 14.912109375
Loss :  1.4045047760009766 2.4873249530792236 13.841129302978516
Loss :  1.3819084167480469 2.9255478382110596 16.009647369384766
Loss :  1.339233636856079 2.2459452152252197 12.56895923614502
Loss :  1.5063000917434692 4.426568031311035 23.63913917541504
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.526943325996399 4.413576126098633 23.594823837280273
Loss :  1.5311628580093384 4.3501667976379395 23.28199577331543
Loss :  1.5237703323364258 4.371253967285156 23.38003921508789
Total LOSS train 16.959775910010706 valid 23.473999500274658
CE LOSS train 1.4536708501669078 valid 0.38094258308410645
Contrastive LOSS train 3.101221036911011 valid 1.092813491821289
EPOCH 71:
Loss :  1.4853661060333252 2.9434866905212402 16.202800750732422
Loss :  1.5093390941619873 2.9629464149475098 16.32406997680664
Loss :  1.4467387199401855 2.405499219894409 13.474235534667969
Loss :  1.4649059772491455 2.9335777759552 16.132795333862305
Loss :  1.5002901554107666 3.0495107173919678 16.747844696044922
Loss :  1.4222745895385742 3.395620107650757 18.400375366210938
Loss :  1.503246784210205 2.8172876834869385 15.589685440063477
Loss :  1.4480401277542114 2.186155080795288 12.378815650939941
Loss :  1.4296969175338745 2.2871642112731934 12.865518569946289
Loss :  1.50062096118927 2.9111053943634033 16.056148529052734
Loss :  1.4109783172607422 2.8886423110961914 15.8541898727417
Loss :  1.4095737934112549 3.1209843158721924 17.014495849609375
Loss :  1.4069918394088745 3.111039876937866 16.962190628051758
Loss :  1.4175915718078613 2.8504183292388916 15.669683456420898
Loss :  1.545250415802002 3.1951048374176025 17.520774841308594
Loss :  1.5259199142456055 2.8212716579437256 15.632278442382812
Loss :  1.4078503847122192 2.8276569843292236 15.546134948730469
Loss :  1.4584753513336182 2.608686685562134 14.501908302307129
Loss :  1.398028016090393 2.5152575969696045 13.974316596984863
Loss :  1.535233974456787 2.492323875427246 13.99685287475586
  batch 20 loss: 1.535233974456787, 2.492323875427246, 13.99685287475586
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.455544114112854 3.1035237312316895 16.973161697387695
Loss :  1.3986526727676392 2.733668088912964 15.066993713378906
Loss :  1.438179612159729 2.956223964691162 16.21929931640625
Loss :  1.4666043519973755 2.8767170906066895 15.850189208984375
Loss :  1.5232574939727783 3.2457170486450195 17.751842498779297
Loss :  1.4357434511184692 2.590517997741699 14.388333320617676
Loss :  1.458615779876709 2.7741403579711914 15.329317092895508
Loss :  1.4485596418380737 2.6662096977233887 14.779608726501465
Loss :  1.3466726541519165 2.5686919689178467 14.190133094787598
Loss :  1.5187004804611206 3.14819073677063 17.259653091430664
Loss :  1.3526853322982788 3.164147138595581 17.17342185974121
Loss :  1.4907727241516113 3.7037394046783447 20.009469985961914
Loss :  1.4418898820877075 2.6893203258514404 14.8884916305542
Loss :  1.439974308013916 2.5088226795196533 13.984086990356445
Loss :  1.3758695125579834 3.188363552093506 17.31768798828125
Loss :  1.408028483390808 3.715151786804199 19.983787536621094
Loss :  1.4061930179595947 2.963179111480713 16.222087860107422
Loss :  1.5219395160675049 2.4909863471984863 13.9768705368042
Loss :  1.5343247652053833 2.264817953109741 12.858414649963379
Loss :  1.5528852939605713 2.607793092727661 14.591850280761719
  batch 40 loss: 1.5528852939605713, 2.607793092727661, 14.591850280761719
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.4748785495758057 2.8717901706695557 15.833828926086426
Loss :  1.4505188465118408 2.6865131855010986 14.883084297180176
Loss :  1.4335685968399048 3.269343614578247 17.78028678894043
Loss :  1.4522398710250854 2.665390968322754 14.779194831848145
Loss :  1.4121471643447876 3.0279951095581055 16.552122116088867
Loss :  1.4691193103790283 2.844635009765625 15.692294120788574
Loss :  1.5382463932037354 2.8832552433013916 15.954522132873535
Loss :  1.4472182989120483 2.9254062175750732 16.074249267578125
Loss :  1.569456934928894 2.9971871376037598 16.555391311645508
Loss :  1.4519414901733398 3.0080771446228027 16.492328643798828
Loss :  1.5179206132888794 3.180420160293579 17.420021057128906
Loss :  1.4999306201934814 2.6227452754974365 14.613656997680664
Loss :  1.4601699113845825 2.843177080154419 15.676054954528809
Loss :  1.5107494592666626 3.2231087684631348 17.626293182373047
Loss :  1.4362354278564453 2.473219156265259 13.80233097076416
Loss :  1.5529708862304688 2.4313406944274902 13.709674835205078
Loss :  1.4488576650619507 2.9089231491088867 15.993473052978516
Loss :  1.4233405590057373 2.8003768920898438 15.425225257873535
Loss :  1.4500622749328613 3.5534744262695312 19.21743392944336
Loss :  1.5683650970458984 3.0838515758514404 16.98762321472168
  batch 60 loss: 1.5683650970458984, 3.0838515758514404, 16.98762321472168
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.4188499450683594 2.9120705127716064 15.979202270507812
Loss :  1.4798346757888794 2.546355962753296 14.211614608764648
Loss :  1.4373046159744263 2.5103116035461426 13.988862037658691
Loss :  1.4187763929367065 2.6570041179656982 14.703797340393066
Loss :  1.3792089223861694 2.2377405166625977 12.567911148071289
Loss :  1.4887758493423462 4.32435941696167 23.110572814941406
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  1.5110465288162231 4.371649742126465 23.36929702758789
Loss :  1.5090376138687134 4.373096942901611 23.374521255493164
Loss :  1.5078126192092896 4.241481781005859 22.715221405029297
Total LOSS train 15.725850677490234 valid 23.14240312576294
CE LOSS train 1.4606679788002601 valid 0.3769531548023224
Contrastive LOSS train 2.8530365467071532 valid 1.0603704452514648
EPOCH 72:
Loss :  1.5074557065963745 2.7031126022338867 15.023018836975098
Loss :  1.5309398174285889 3.3776280879974365 18.419078826904297
Loss :  1.47275972366333 3.359990119934082 18.272708892822266
Loss :  1.4891325235366821 2.8796474933624268 15.887370109558105
Loss :  1.5221896171569824 3.372819423675537 18.386287689208984
Loss :  1.4509108066558838 2.5559911727905273 14.230866432189941
Loss :  1.5261573791503906 3.1632626056671143 17.342470169067383
Loss :  1.4722001552581787 2.733388662338257 15.139143943786621
Loss :  1.453822374343872 2.8080203533172607 15.493924140930176
Loss :  1.5153435468673706 2.3443243503570557 13.23696517944336
Loss :  1.4340150356292725 3.763056755065918 20.249300003051758
Loss :  1.4348608255386353 2.7293715476989746 15.081719398498535
Loss :  1.4292861223220825 3.2566380500793457 17.71247673034668
Loss :  1.4394352436065674 3.1655478477478027 17.267175674438477
Loss :  1.5555843114852905 3.5185317993164062 19.148242950439453
Loss :  1.5350096225738525 3.17724871635437 17.421253204345703
Loss :  1.4257938861846924 3.303090810775757 17.941247940063477
Loss :  1.4756802320480347 2.585360527038574 14.402482986450195
Loss :  1.4146238565444946 2.489027261734009 13.859760284423828
Loss :  1.5423305034637451 2.8738977909088135 15.911819458007812
  batch 20 loss: 1.5423305034637451, 2.8738977909088135, 15.911819458007812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.4662785530090332 3.097214460372925 16.952350616455078
Loss :  1.4164342880249023 2.735588312149048 15.094375610351562
Loss :  1.4565283060073853 2.5559840202331543 14.236449241638184
Loss :  1.4836293458938599 2.8274853229522705 15.621055603027344
Loss :  1.5382524728775024 3.137794017791748 17.227222442626953
Loss :  1.459439754486084 2.8799774646759033 15.85932731628418
Loss :  1.479050874710083 3.2773971557617188 17.866037368774414
Loss :  1.4699033498764038 2.7093260288238525 15.016532897949219
Loss :  1.3739813566207886 2.3343732357025146 13.04584789276123
Loss :  1.5336865186691284 2.7617318630218506 15.34234619140625
Loss :  1.376291275024414 3.3626015186309814 18.189298629760742
Loss :  1.5038210153579712 3.041273832321167 16.710189819335938
Loss :  1.4556465148925781 2.913986921310425 16.02558135986328
Loss :  1.4536808729171753 2.953655242919922 16.221956253051758
Loss :  1.3934156894683838 2.9585225582122803 16.1860294342041
Loss :  1.4211418628692627 2.542292833328247 14.13260555267334
Loss :  1.4182528257369995 3.0425665378570557 16.631084442138672
Loss :  1.526349663734436 2.874119758605957 15.89694881439209
Loss :  1.534299373626709 3.2693064212799072 17.88083267211914
Loss :  1.5551012754440308 2.669278621673584 14.901494979858398
  batch 40 loss: 1.5551012754440308, 2.669278621673584, 14.901494979858398
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.4774543046951294 2.5764427185058594 14.359667778015137
Loss :  1.4545774459838867 2.418621301651001 13.547683715820312
Loss :  1.4363677501678467 3.0413599014282227 16.64316749572754
Loss :  1.4589934349060059 2.7760515213012695 15.339250564575195
Loss :  1.4204537868499756 2.5755810737609863 14.298358917236328
Loss :  1.4760615825653076 2.828765392303467 15.619889259338379
Loss :  1.5428414344787598 2.2630794048309326 12.858238220214844
Loss :  1.4512102603912354 2.3913791179656982 13.408105850219727
Loss :  1.5712954998016357 2.709796905517578 15.120280265808105
Loss :  1.4593929052352905 2.7336602210998535 15.127693176269531
Loss :  1.5246784687042236 3.6719086170196533 19.88422203063965
Loss :  1.5124400854110718 2.8835153579711914 15.93001651763916
Loss :  1.4756312370300293 3.095524549484253 16.95325469970703
Loss :  1.5267679691314697 2.760864496231079 15.331089973449707
Loss :  1.4572004079818726 2.7885773181915283 15.400086402893066
Loss :  1.5727733373641968 2.9516541957855225 16.331045150756836
Loss :  1.4660471677780151 2.868983268737793 15.81096363067627
Loss :  1.4399101734161377 3.1805033683776855 17.34242820739746
Loss :  1.4659361839294434 2.668131113052368 14.806591033935547
Loss :  1.5822651386260986 2.8226048946380615 15.695289611816406
  batch 60 loss: 1.5822651386260986, 2.8226048946380615, 15.695289611816406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.4367773532867432 2.5197501182556152 14.035528182983398
Loss :  1.4945729970932007 2.5562806129455566 14.275975227355957
Loss :  1.454689860343933 3.2434496879577637 17.671937942504883
Loss :  1.4376616477966309 3.527695655822754 19.076139450073242
Loss :  1.3979243040084839 2.4222781658172607 13.509315490722656
Loss :  1.4963141679763794 3.582632303237915 19.40947723388672
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5182572603225708 3.675421714782715 19.895366668701172
Loss :  1.517032265663147 3.4734528064727783 18.884296417236328
Loss :  1.524724006652832 3.5438179969787598 19.243812561035156
Total LOSS train 15.966786135160007 valid 19.358238220214844
CE LOSS train 1.475948326404278 valid 0.381181001663208
Contrastive LOSS train 2.898167555148785 valid 0.8859544992446899
EPOCH 73:
Loss :  1.5217136144638062 3.0216927528381348 16.630176544189453
Loss :  1.5458927154541016 3.2473256587982178 17.782520294189453
Loss :  1.490979790687561 3.6238739490509033 19.610349655151367
Loss :  1.506482720375061 2.842200517654419 15.717485427856445
Loss :  1.5371133089065552 2.9432578086853027 16.253402709960938
Loss :  1.4704174995422363 2.4666945934295654 13.803890228271484
Loss :  1.5366121530532837 3.0313775539398193 16.693500518798828
Loss :  1.4870316982269287 2.565290927886963 14.313486099243164
Loss :  1.4687615633010864 2.7152259349823 15.044891357421875
Loss :  1.5285495519638062 2.5580852031707764 14.318975448608398
Loss :  1.4511799812316895 3.155500650405884 17.228683471679688
Loss :  1.4510996341705322 3.054382085800171 16.723011016845703
Loss :  1.4448055028915405 2.962414026260376 16.25687599182129
Loss :  1.4529653787612915 3.12740159034729 17.08997344970703
Loss :  1.565221905708313 2.8829240798950195 15.979842185974121
Loss :  1.5463035106658936 2.4149224758148193 13.620916366577148
Loss :  1.4410147666931152 2.8780553340911865 15.831291198730469
Loss :  1.4869680404663086 2.6402904987335205 14.688420295715332
Loss :  1.4310390949249268 2.2463462352752686 12.66277027130127
Loss :  1.555046558380127 2.911412000656128 16.112106323242188
  batch 20 loss: 1.555046558380127, 2.911412000656128, 16.112106323242188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.4784609079360962 2.477511167526245 13.86601734161377
Loss :  1.4301236867904663 2.6467676162719727 14.663961410522461
Loss :  1.468320608139038 2.755703926086426 15.246840476989746
Loss :  1.495125651359558 2.9388558864593506 16.18940544128418
Loss :  1.5478193759918213 2.9642529487609863 16.369083404541016
Loss :  1.469151496887207 2.661179542541504 14.775049209594727
Loss :  1.4910696744918823 3.114856243133545 17.065351486206055
Loss :  1.4830825328826904 3.0339860916137695 16.653013229370117
Loss :  1.3883470296859741 3.1522176265716553 17.14943504333496
Loss :  1.5473747253417969 3.245980978012085 17.777278900146484
Loss :  1.392641305923462 3.5477023124694824 19.131153106689453
Loss :  1.517058253288269 2.9684128761291504 16.359121322631836
Loss :  1.4682413339614868 2.8718762397766113 15.827622413635254
Loss :  1.4640756845474243 2.9667744636535645 16.29794692993164
Loss :  1.4020707607269287 3.2134857177734375 17.469499588012695
Loss :  1.4283812046051025 2.6629140377044678 14.742951393127441
Loss :  1.4226675033569336 2.978022336959839 16.31277847290039
Loss :  1.53253972530365 2.897583484649658 16.020456314086914
Loss :  1.542221188545227 3.012449026107788 16.604467391967773
Loss :  1.5625699758529663 3.0572259426116943 16.84869956970215
  batch 40 loss: 1.5625699758529663, 3.0572259426116943, 16.84869956970215
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.4841729402542114 3.2093663215637207 17.531003952026367
Loss :  1.4618325233459473 2.872014045715332 15.821903228759766
Loss :  1.4417283535003662 2.9331588745117188 16.10752296447754
Loss :  1.4669384956359863 2.936134099960327 16.14760971069336
Loss :  1.4271643161773682 2.4228339195251465 13.54133415222168
Loss :  1.4809719324111938 3.0458719730377197 16.710330963134766
Loss :  1.5492959022521973 2.9493370056152344 16.29598045349121
Loss :  1.4588737487792969 2.9557034969329834 16.23739242553711
Loss :  1.577876329421997 3.1707301139831543 17.431528091430664
Loss :  1.466364860534668 2.6348156929016113 14.640442848205566
Loss :  1.52668297290802 2.9142656326293945 16.098011016845703
Loss :  1.5127465724945068 3.4142816066741943 18.58415412902832
Loss :  1.475426197052002 2.9050827026367188 16.000839233398438
Loss :  1.5250310897827148 2.630387544631958 14.676968574523926
Loss :  1.453709363937378 3.6574714183807373 19.741065979003906
Loss :  1.5689918994903564 2.3220441341400146 13.17921257019043
Loss :  1.4653542041778564 2.4603381156921387 13.767045021057129
Loss :  1.440384864807129 3.3822829723358154 18.35179901123047
Loss :  1.4671361446380615 3.261183500289917 17.773054122924805
Loss :  1.5848642587661743 2.783076524734497 15.50024700164795
  batch 60 loss: 1.5848642587661743, 2.783076524734497, 15.50024700164795
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  1.4392709732055664 2.4509077072143555 13.693809509277344
Loss :  1.4977155923843384 2.673859119415283 14.867011070251465
Loss :  1.4569767713546753 2.69409441947937 14.927449226379395
Loss :  1.4383890628814697 2.550896644592285 14.192872047424316
Loss :  1.399426817893982 2.1380770206451416 12.089811325073242
Loss :  1.5171865224838257 4.351583003997803 23.275102615356445
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.538252592086792 4.38430118560791 23.45975685119629
Loss :  1.538623332977295 4.201804161071777 22.547645568847656
Loss :  1.5463168621063232 4.161715984344482 22.354896545410156
Total LOSS train 15.932939998920148 valid 22.909350395202637
CE LOSS train 1.4848902739011325 valid 0.3865792155265808
Contrastive LOSS train 2.889609953073355 valid 1.0404289960861206
EPOCH 74:
Loss :  1.5253713130950928 2.682053804397583 14.935640335083008
Loss :  1.5480482578277588 2.9262101650238037 16.179100036621094
Loss :  1.48867666721344 2.5760836601257324 14.369095802307129
Loss :  1.5021060705184937 2.790666341781616 15.455437660217285
Loss :  1.5304279327392578 2.7081525325775146 15.07119083404541
Loss :  1.4626933336257935 2.667053461074829 14.79796028137207
Loss :  1.5313444137573242 2.9513444900512695 16.288066864013672
Loss :  1.4832242727279663 2.5228524208068848 14.097485542297363
Loss :  1.4667065143585205 2.349543333053589 13.214423179626465
Loss :  1.5290988683700562 2.330951690673828 13.183856964111328
Loss :  1.4492895603179932 2.6372334957122803 14.635457038879395
Loss :  1.4513003826141357 3.1631126403808594 17.266862869262695
Loss :  1.4475390911102295 2.5388333797454834 14.141706466674805
Loss :  1.4578511714935303 2.749241590499878 15.204059600830078
Loss :  1.5703994035720825 2.6073660850524902 14.607230186462402
Loss :  1.5532244443893433 2.5711286067962646 14.408867835998535
Loss :  1.4504578113555908 2.93721342086792 16.136526107788086
Loss :  1.4959983825683594 3.250638723373413 17.749191284179688
Loss :  1.4447388648986816 2.8941099643707275 15.915288925170898
Loss :  1.5642951726913452 3.0458314418792725 16.793453216552734
  batch 20 loss: 1.5642951726913452, 3.0458314418792725, 16.793453216552734
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.492049217224121 3.8019261360168457 20.501678466796875
Loss :  1.4464919567108154 3.32645845413208 18.07878303527832
Loss :  1.4844409227371216 2.3439366817474365 13.204124450683594
Loss :  1.5123233795166016 2.908393144607544 16.054288864135742
Loss :  1.5623347759246826 3.153775691986084 17.331212997436523
Loss :  1.4835517406463623 2.876065731048584 15.86388111114502
Loss :  1.5000219345092773 3.122532367706299 17.112682342529297
Loss :  1.4910173416137695 3.234633207321167 17.6641845703125
Loss :  1.3978784084320068 3.536954641342163 19.082651138305664
Loss :  1.5509705543518066 2.8949508666992188 16.025724411010742
Loss :  1.4000366926193237 2.887253761291504 15.836305618286133
Loss :  1.5214223861694336 3.106689691543579 17.05487060546875
Loss :  1.4774370193481445 3.0662145614624023 16.808509826660156
Loss :  1.4771802425384521 3.2615456581115723 17.784908294677734
Loss :  1.42043936252594 3.235377311706543 17.597326278686523
Loss :  1.44413423538208 3.4173481464385986 18.530874252319336
Loss :  1.439234972000122 3.1783597469329834 17.33103370666504
Loss :  1.549561619758606 3.004058837890625 16.569856643676758
Loss :  1.5571640729904175 3.1785566806793213 17.449947357177734
Loss :  1.5720101594924927 2.8115487098693848 15.629753112792969
  batch 40 loss: 1.5720101594924927, 2.8115487098693848, 15.629753112792969
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.4962060451507568 3.1204452514648438 17.098432540893555
Loss :  1.4739564657211304 3.035369634628296 16.65080451965332
Loss :  1.4582918882369995 2.9929449558258057 16.423015594482422
Loss :  1.476400375366211 3.062314033508301 16.78797149658203
Loss :  1.442136287689209 3.156019687652588 17.22223472595215
Loss :  1.4926611185073853 2.7854340076446533 15.419831275939941
Loss :  1.5539593696594238 3.350214958190918 18.305034637451172
Loss :  1.474465012550354 2.98211669921875 16.385047912597656
Loss :  1.582746982574463 2.9511327743530273 16.338411331176758
Loss :  1.479335904121399 2.9668874740600586 16.31377410888672
Loss :  1.5450783967971802 3.675572633743286 19.922941207885742
Loss :  1.527140736579895 3.7159993648529053 20.10713768005371
Loss :  1.495310664176941 3.077394962310791 16.88228416442871
Loss :  1.543745994567871 3.938279151916504 21.23514175415039
Loss :  1.4740182161331177 2.8416247367858887 15.68214225769043
Loss :  1.5853253602981567 2.8030343055725098 15.600496292114258
Loss :  1.4850322008132935 3.023038148880005 16.600223541259766
Loss :  1.4589238166809082 2.703031301498413 14.974081039428711
Loss :  1.4851155281066895 3.1054258346557617 17.012245178222656
Loss :  1.5962374210357666 3.0583581924438477 16.888029098510742
  batch 60 loss: 1.5962374210357666, 3.0583581924438477, 16.888029098510742
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3], device='cuda:0')
Loss :  1.4559277296066284 3.0151312351226807 16.531583786010742
Loss :  1.5137979984283447 2.94893741607666 16.258485794067383
Loss :  1.4711631536483765 3.0829379558563232 16.885852813720703
Loss :  1.455817699432373 3.2192046642303467 17.551841735839844
Loss :  1.416155457496643 2.83766508102417 15.60448169708252
Loss :  1.5251163244247437 4.189983367919922 22.475032806396484
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
Loss :  1.5506808757781982 4.156793594360352 22.33464813232422
Loss :  1.5553163290023804 4.072228908538818 21.916461944580078
Loss :  1.5524564981460571 3.7508018016815186 20.30646514892578
Total LOSS train 16.47146191230187 valid 21.75815200805664
CE LOSS train 1.4949452730325552 valid 0.3881141245365143
Contrastive LOSS train 2.9953033190507155 valid 0.9377004504203796
EPOCH 75:
Loss :  1.5341742038726807 2.549537420272827 14.281861305236816
Loss :  1.55727219581604 2.950968027114868 16.31211280822754
Loss :  1.4981513023376465 2.448046922683716 13.738386154174805
Loss :  1.5113043785095215 3.248105764389038 17.751832962036133
Loss :  1.5375449657440186 2.3874411582946777 13.474751472473145
Loss :  1.4728766679763794 3.075855255126953 16.852153778076172
Loss :  1.538448452949524 2.4491288661956787 13.784092903137207
Loss :  1.4938228130340576 2.193326473236084 12.460455894470215
Loss :  1.4785187244415283 2.7009172439575195 14.983104705810547
Loss :  1.5409175157546997 2.7415266036987305 15.248550415039062
Loss :  1.4638710021972656 2.8805558681488037 15.866650581359863
Loss :  1.46457839012146 2.7399370670318604 15.164263725280762
Loss :  1.4619935750961304 2.563354253768921 14.278764724731445
Loss :  1.4723869562149048 2.5347530841827393 14.14615249633789
Loss :  1.5804194211959839 2.616762638092041 14.66423225402832
Loss :  1.5626411437988281 2.7079648971557617 15.102465629577637
Loss :  1.4628291130065918 3.2463977336883545 17.6948184967041
Loss :  1.505817174911499 2.836627721786499 15.688956260681152
Loss :  1.4535949230194092 2.671694755554199 14.812068939208984
Loss :  1.567439079284668 2.8043789863586426 15.589333534240723
  batch 20 loss: 1.567439079284668, 2.8043789863586426, 15.589333534240723
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.4997118711471558 2.4994957447052 13.997190475463867
Loss :  1.452940583229065 3.343247175216675 18.16917610168457
Loss :  1.4874777793884277 2.449183464050293 13.733394622802734
Loss :  1.510633945465088 2.7404065132141113 15.212665557861328
Loss :  1.5596787929534912 2.9891159534454346 16.505258560180664
Loss :  1.485041618347168 2.4862172603607178 13.916128158569336
Loss :  1.5036026239395142 2.6192402839660645 14.599803924560547
Loss :  1.4952392578125 2.512770652770996 14.05909252166748
Loss :  1.4067775011062622 3.0363173484802246 16.58836555480957
Loss :  1.557025671005249 2.8735568523406982 15.924810409545898
Loss :  1.410365343093872 3.0128986835479736 16.4748592376709
Loss :  1.527488112449646 2.884486675262451 15.949921607971191
Loss :  1.4845545291900635 2.3901820182800293 13.435464859008789
Loss :  1.4832651615142822 2.387765407562256 13.42209243774414
Loss :  1.4287775754928589 2.8164312839508057 15.510933876037598
Loss :  1.4543213844299316 2.5175774097442627 14.042207717895508
Loss :  1.4529896974563599 2.677570104598999 14.840840339660645
Loss :  1.5525418519973755 2.7444634437561035 15.274858474731445
Loss :  1.5636452436447144 2.601980686187744 14.573548316955566
Loss :  1.5763565301895142 2.4859743118286133 14.00622844696045
  batch 40 loss: 1.5763565301895142, 2.4859743118286133, 14.00622844696045
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
Loss :  1.5059666633605957 3.2952158451080322 17.982046127319336
Loss :  1.4885575771331787 2.620041608810425 14.588766098022461
Loss :  1.4706658124923706 2.7451090812683105 15.196211814880371
Loss :  1.4908027648925781 4.2776899337768555 22.879253387451172
Loss :  1.455763816833496 2.5664074420928955 14.287800788879395
Loss :  1.5025631189346313 2.6382603645324707 14.693864822387695
Loss :  1.564485788345337 3.478708028793335 18.958024978637695
Loss :  1.4818795919418335 2.5311496257781982 14.137627601623535
Loss :  1.597021460533142 2.9744975566864014 16.46950912475586
Loss :  1.49181067943573 2.9287424087524414 16.135522842407227
Loss :  1.5470077991485596 3.467350721359253 18.88376235961914
Loss :  1.534818172454834 3.345391035079956 18.26177406311035
Loss :  1.5008587837219238 2.5706710815429688 14.35421371459961
Loss :  1.5508581399917603 2.7185847759246826 15.143782615661621
Loss :  1.4761711359024048 2.9450318813323975 16.201330184936523
Loss :  1.5875316858291626 2.77738356590271 15.47445011138916
Loss :  1.488315224647522 3.325186252593994 18.114246368408203
Loss :  1.4617990255355835 2.795044422149658 15.437020301818848
Loss :  1.486786961555481 3.2833001613616943 17.903287887573242
Loss :  1.5983223915100098 3.022542715072632 16.711036682128906
  batch 60 loss: 1.5983223915100098, 3.022542715072632, 16.711036682128906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  1.4565690755844116 3.039295196533203 16.653045654296875
Loss :  1.5082979202270508 2.4771134853363037 13.893865585327148
Loss :  1.4671530723571777 3.664067506790161 19.787490844726562
Loss :  1.4484492540359497 3.035282850265503 16.62486457824707
Loss :  1.4146405458450317 2.2293717861175537 12.56149959564209
Loss :  1.520504117012024 3.9337806701660156 21.189407348632812
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.541731357574463 3.8960392475128174 21.021926879882812
Loss :  1.5435224771499634 3.7336084842681885 20.211565017700195
Loss :  1.5289489030838013 3.5183842182159424 19.12087059020996
Total LOSS train 15.591786898099459 valid 20.385942459106445
CE LOSS train 1.5019712851597713 valid 0.3822372257709503
Contrastive LOSS train 2.8179630976456864 valid 0.8795960545539856
EPOCH 76:
Loss :  1.53554105758667 2.6392722129821777 14.731903076171875
Loss :  1.5562454462051392 2.734645366668701 15.229473114013672
Loss :  1.5000526905059814 2.800405979156494 15.502081871032715
Loss :  1.5115443468093872 3.0058350563049316 16.540719985961914
Loss :  1.5400042533874512 2.973142385482788 16.405715942382812
Loss :  1.473052978515625 2.787907123565674 15.412588119506836
Loss :  1.542667269706726 3.1224589347839355 17.15496253967285
Loss :  1.4926220178604126 2.7004525661468506 14.994885444641113
Loss :  1.472305178642273 2.7119734287261963 15.032172203063965
Loss :  1.529689073562622 2.991887331008911 16.489126205444336
Loss :  1.45063054561615 3.316484212875366 18.033050537109375
Loss :  1.448931336402893 2.990269899368286 16.400279998779297
Loss :  1.4451396465301514 2.617542266845703 14.532851219177246
Loss :  1.454199194908142 2.835538625717163 15.631892204284668
Loss :  1.5667619705200195 2.848689079284668 15.81020736694336
Loss :  1.5434123277664185 2.9690661430358887 16.388744354248047
Loss :  1.4408490657806396 2.739300489425659 15.137351989746094
Loss :  1.4854683876037598 2.9647879600524902 16.30940818786621
Loss :  1.431644082069397 2.879836320877075 15.830825805664062
Loss :  1.5506908893585205 2.741044759750366 15.255914688110352
  batch 20 loss: 1.5506908893585205, 2.741044759750366, 15.255914688110352
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.4820730686187744 2.3018758296966553 12.99145221710205
Loss :  1.4333525896072388 3.147491216659546 17.170808792114258
Loss :  1.4697699546813965 2.5393013954162598 14.166276931762695
Loss :  1.4992984533309937 2.8072171211242676 15.535383224487305
Loss :  1.5506352186203003 2.858734369277954 15.844306945800781
Loss :  1.4740597009658813 2.7805562019348145 15.376840591430664
Loss :  1.4950213432312012 2.8555140495300293 15.772592544555664
Loss :  1.4853477478027344 2.6851158142089844 14.910926818847656
Loss :  1.3920059204101562 2.817275285720825 15.478382110595703
Loss :  1.545062780380249 2.806190252304077 15.576014518737793
Loss :  1.3944191932678223 2.941441297531128 16.101625442504883
Loss :  1.516702651977539 2.8775815963745117 15.904610633850098
Loss :  1.47206449508667 3.6211774349212646 19.577951431274414
Loss :  1.4710702896118164 2.708134889602661 15.011744499206543
Loss :  1.4136650562286377 2.5893805027008057 14.360567092895508
Loss :  1.4412198066711426 2.8823115825653076 15.852777481079102
Loss :  1.4394804239273071 2.524902820587158 14.063994407653809
Loss :  1.5468056201934814 3.0161633491516113 16.627622604370117
Loss :  1.556751012802124 3.1995797157287598 17.554649353027344
Loss :  1.5737460851669312 3.044461727142334 16.79605484008789
  batch 40 loss: 1.5737460851669312, 3.044461727142334, 16.79605484008789
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  1.501452922821045 3.157927989959717 17.291093826293945
Loss :  1.482204556465149 2.9216015338897705 16.090211868286133
Loss :  1.4660109281539917 2.9837329387664795 16.384675979614258
Loss :  1.4878535270690918 3.5641822814941406 19.308765411376953
Loss :  1.4527416229248047 3.3402774333953857 18.154129028320312
Loss :  1.5059417486190796 3.4823384284973145 18.917633056640625
Loss :  1.5679214000701904 2.903521776199341 16.085529327392578
Loss :  1.4803822040557861 3.0381367206573486 16.671066284179688
Loss :  1.5870866775512695 2.6579489707946777 14.876832008361816
Loss :  1.4839065074920654 2.7451984882354736 15.209898948669434
Loss :  1.5437259674072266 3.008277177810669 16.585111618041992
Loss :  1.5280896425247192 2.800368070602417 15.529930114746094
Loss :  1.48695707321167 2.6365771293640137 14.669843673706055
Loss :  1.5354222059249878 3.068045139312744 16.875646591186523
Loss :  1.462552547454834 3.3836588859558105 18.380847930908203
Loss :  1.5723758935928345 3.124303102493286 17.193891525268555
Loss :  1.4697145223617554 2.9783170223236084 16.361299514770508
Loss :  1.4434995651245117 3.437021493911743 18.62860870361328
Loss :  1.4696272611618042 3.0294995307922363 16.617124557495117
Loss :  1.5872836112976074 2.983644962310791 16.505508422851562
  batch 60 loss: 1.5872836112976074, 2.983644962310791, 16.505508422851562
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.4389573335647583 2.715562343597412 15.016769409179688
Loss :  1.494602084159851 2.824702501296997 15.618114471435547
Loss :  1.4535152912139893 2.7815020084381104 15.361024856567383
Loss :  1.4317165613174438 3.4612836837768555 18.738136291503906
Loss :  1.3934133052825928 2.4582417011260986 13.684621810913086
Loss :  1.4988642930984497 4.325377941131592 23.125755310058594
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
Loss :  1.5218569040298462 4.314723014831543 23.09547233581543
Loss :  1.5207349061965942 4.166362285614014 22.35254669189453
Loss :  1.522099494934082 4.17462158203125 22.395206451416016
Total LOSS train 16.09623157794659 valid 22.742245197296143
CE LOSS train 1.4915532020422129 valid 0.3805248737335205
Contrastive LOSS train 2.920935660142165 valid 1.0436553955078125
EPOCH 77:
Loss :  1.5209829807281494 3.122408866882324 17.133028030395508
Loss :  1.5424425601959229 2.9562671184539795 16.32377815246582
Loss :  1.4828077554702759 2.3660531044006348 13.31307315826416
Loss :  1.4987471103668213 2.5887112617492676 14.442302703857422
Loss :  1.5296456813812256 3.379944324493408 18.429367065429688
Loss :  1.463510513305664 3.090310573577881 16.915063858032227
Loss :  1.5326131582260132 3.0244226455688477 16.654726028442383
Loss :  1.4855151176452637 2.875866413116455 15.864847183227539
Loss :  1.4671984910964966 2.707616090774536 15.005278587341309
Loss :  1.530410647392273 2.6692049503326416 14.876435279846191
Loss :  1.4492801427841187 3.1767449378967285 17.333003997802734
Loss :  1.4491850137710571 2.9449262619018555 16.173816680908203
Loss :  1.443920373916626 2.9307425022125244 16.097633361816406
Loss :  1.455114722251892 2.902881145477295 15.969520568847656
Loss :  1.5656222105026245 2.749351739883423 15.31238079071045
Loss :  1.5493830442428589 3.2410082817077637 17.754425048828125
Loss :  1.4434515237808228 2.8076603412628174 15.4817533493042
Loss :  1.4916144609451294 2.710197687149048 15.0426025390625
Loss :  1.434923529624939 2.4578452110290527 13.724149703979492
Loss :  1.5567569732666016 2.437044858932495 13.741981506347656
  batch 20 loss: 1.5567569732666016, 2.437044858932495, 13.741981506347656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.484214186668396 2.917005777359009 16.069242477416992
Loss :  1.435263752937317 3.57372784614563 19.303903579711914
Loss :  1.4704277515411377 3.236015796661377 17.6505069732666
Loss :  1.4992945194244385 2.9457383155822754 16.227985382080078
Loss :  1.5507936477661133 2.8804285526275635 15.952936172485352
Loss :  1.470820665359497 2.7568063735961914 15.254852294921875
Loss :  1.4908807277679443 3.6896657943725586 19.939210891723633
Loss :  1.4793239831924438 3.1027519702911377 16.993083953857422
Loss :  1.3867151737213135 2.873297691345215 15.753203392028809
Loss :  1.5441697835922241 2.4602532386779785 13.84543514251709
Loss :  1.391408085823059 2.8078207969665527 15.430512428283691
Loss :  1.519697904586792 2.863433837890625 15.836867332458496
Loss :  1.4742400646209717 2.8161072731018066 15.554776191711426
Loss :  1.4726173877716064 2.972877264022827 16.337003707885742
Loss :  1.409640908241272 2.635523557662964 14.587259292602539
Loss :  1.436348795890808 2.561246633529663 14.242582321166992
Loss :  1.4304026365280151 2.5844573974609375 14.352689743041992
Loss :  1.542074203491211 3.0001933574676514 16.543041229248047
Loss :  1.5553330183029175 2.823481559753418 15.672740936279297
Loss :  1.5705065727233887 2.5834972858428955 14.487993240356445
  batch 40 loss: 1.5705065727233887, 2.5834972858428955, 14.487993240356445
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.4926447868347168 2.970167398452759 16.343481063842773
Loss :  1.4728384017944336 2.448070526123047 13.713191032409668
Loss :  1.4523133039474487 2.7629315853118896 15.266971588134766
Loss :  1.4744513034820557 3.089346170425415 16.92118263244629
Loss :  1.4340884685516357 2.2517428398132324 12.692803382873535
Loss :  1.4873943328857422 3.034600257873535 16.660396575927734
Loss :  1.5549321174621582 3.029353141784668 16.701698303222656
Loss :  1.4640140533447266 3.2277140617370605 17.602584838867188
Loss :  1.5868483781814575 3.358205556869507 18.37787628173828
Loss :  1.4754613637924194 2.7906100749969482 15.428511619567871
Loss :  1.5378639698028564 2.6328327655792236 14.702027320861816
Loss :  1.5250694751739502 3.0390188694000244 16.720163345336914
Loss :  1.4880335330963135 3.5224008560180664 19.10003662109375
Loss :  1.5421357154846191 3.111851453781128 17.10139274597168
Loss :  1.4687179327011108 3.2460947036743164 17.699190139770508
Loss :  1.586342215538025 2.720463991165161 15.188661575317383
Loss :  1.486610770225525 2.805692434310913 15.5150728225708
Loss :  1.4582351446151733 2.5372090339660645 14.144279479980469
Loss :  1.4856805801391602 3.526445150375366 19.117904663085938
Loss :  1.5928902626037598 2.992786169052124 16.556821823120117
  batch 60 loss: 1.5928902626037598, 2.992786169052124, 16.556821823120117
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.4494608640670776 3.1614267826080322 17.256593704223633
Loss :  1.5028361082077026 2.77439022064209 15.374787330627441
Loss :  1.4613333940505981 3.121521472930908 17.068941116333008
Loss :  1.4433019161224365 3.02095627784729 16.54808235168457
Loss :  1.4058912992477417 2.1589908599853516 12.200845718383789
Loss :  1.5092099905014038 4.251948833465576 22.76895523071289
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5271825790405273 4.307973384857178 23.06705093383789
Loss :  1.5305653810501099 4.189348220825195 22.477306365966797
Loss :  1.5148820877075195 4.150747299194336 22.268619537353516
Total LOSS train 15.994284497774565 valid 22.645483016967773
CE LOSS train 1.4897952226492075 valid 0.3787205219268799
Contrastive LOSS train 2.900897866029006 valid 1.037686824798584
EPOCH 78:
Loss :  1.52895188331604 3.0177738666534424 16.617820739746094
Loss :  1.5519790649414062 2.718262195587158 15.143289566040039
Loss :  1.4967058897018433 2.26395845413208 12.816497802734375
Loss :  1.506640076637268 3.024115562438965 16.62721824645996
Loss :  1.5383435487747192 2.934490442276001 16.210796356201172
Loss :  1.4689573049545288 3.655226469039917 19.74509048461914
Loss :  1.5368001461029053 2.562401294708252 14.348806381225586
Loss :  1.4867236614227295 2.6143481731414795 14.558465003967285
Loss :  1.469826102256775 2.4962081909179688 13.95086669921875
Loss :  1.5321338176727295 2.405853509902954 13.5614013671875
Loss :  1.4548394680023193 3.7632830142974854 20.271255493164062
Loss :  1.4574987888336182 3.738907814025879 20.152036666870117
Loss :  1.4513471126556396 3.9134521484375 21.01860809326172
Loss :  1.4597837924957275 2.963627576828003 16.277921676635742
Loss :  1.566317081451416 3.0122361183166504 16.62749671936035
Loss :  1.548437476158142 3.278123140335083 17.93905258178711
Loss :  1.437567114830017 2.6425700187683105 14.65041732788086
Loss :  1.4853692054748535 2.8814303874969482 15.892520904541016
Loss :  1.4307854175567627 2.6699204444885254 14.780386924743652
Loss :  1.551540732383728 3.030458927154541 16.703834533691406
  batch 20 loss: 1.551540732383728, 3.030458927154541, 16.703834533691406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.482036828994751 2.846200466156006 15.71303939819336
Loss :  1.431226372718811 2.8848283290863037 15.855368614196777
Loss :  1.4695169925689697 3.0618698596954346 16.778865814208984
Loss :  1.4988747835159302 2.7246809005737305 15.122279167175293
Loss :  1.5538526773452759 3.135333776473999 17.23052215576172
Loss :  1.4687862396240234 3.446946620941162 18.703519821166992
Loss :  1.4900486469268799 2.6926164627075195 14.953130722045898
Loss :  1.4774757623672485 2.340287685394287 13.178915023803711
Loss :  1.3783122301101685 2.4985008239746094 13.870816230773926
Loss :  1.5415420532226562 2.965099334716797 16.36703872680664
Loss :  1.3782423734664917 2.7487833499908447 15.122159004211426
Loss :  1.5094362497329712 2.757857084274292 15.298722267150879
Loss :  1.4607053995132446 2.9996345043182373 16.458877563476562
Loss :  1.4583783149719238 2.939534902572632 16.15605354309082
Loss :  1.3932864665985107 3.1705548763275146 17.246061325073242
Loss :  1.4239250421524048 2.6989119052886963 14.918484687805176
Loss :  1.4191001653671265 2.726980447769165 15.054001808166504
Loss :  1.5329829454421997 2.593285083770752 14.499407768249512
Loss :  1.54449462890625 2.7264811992645264 15.176900863647461
Loss :  1.5607357025146484 3.0218896865844727 16.670185089111328
  batch 40 loss: 1.5607357025146484, 3.0218896865844727, 16.670185089111328
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.4813413619995117 3.5699191093444824 19.330936431884766
Loss :  1.4578324556350708 2.6450581550598145 14.683122634887695
Loss :  1.4365241527557373 2.775172710418701 15.31238842010498
Loss :  1.459787130355835 2.5001296997070312 13.96043586730957
Loss :  1.419999599456787 2.2874348163604736 12.857173919677734
Loss :  1.4801708459854126 2.32159423828125 13.088142395019531
Loss :  1.5480419397354126 2.9800126552581787 16.448104858398438
Loss :  1.452735424041748 2.9742236137390137 16.323854446411133
Loss :  1.5769280195236206 3.0018937587738037 16.586397171020508
Loss :  1.4625886678695679 2.7778992652893066 15.352084159851074
Loss :  1.5271128416061401 2.5990381240844727 14.522303581237793
Loss :  1.5126893520355225 3.6461117267608643 19.743247985839844
Loss :  1.4738224744796753 2.4665327072143555 13.806486129760742
Loss :  1.5266666412353516 2.291188955307007 12.982611656188965
Loss :  1.4528522491455078 2.247382164001465 12.689763069152832
Loss :  1.5765334367752075 3.0462448596954346 16.807758331298828
Loss :  1.4730020761489868 3.279090166091919 17.868452072143555
Loss :  1.4465104341506958 3.5761783123016357 19.327402114868164
Loss :  1.4778891801834106 2.8333401679992676 15.6445894241333
Loss :  1.5954281091690063 2.311070442199707 13.15078067779541
  batch 60 loss: 1.5954281091690063, 2.311070442199707, 13.15078067779541
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.4426591396331787 3.6161949634552 19.52363395690918
Loss :  1.4966166019439697 2.6631057262420654 14.812145233154297
Loss :  1.4560481309890747 3.244528293609619 17.67868995666504
Loss :  1.4354747533798218 2.8539531230926514 15.705240249633789
Loss :  1.3987414836883545 2.903517246246338 15.916327476501465
Loss :  1.4912540912628174 4.032049179077148 21.651500701904297
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 5], device='cuda:0')
Loss :  1.5114777088165283 4.000451564788818 21.513736724853516
Loss :  1.5114552974700928 3.904581308364868 21.03436279296875
Loss :  1.4954948425292969 3.8237273693084717 20.614131927490234
Total LOSS train 15.944464698204627 valid 21.2034330368042
CE LOSS train 1.484638524055481 valid 0.3738737106323242
Contrastive LOSS train 2.891965231528649 valid 0.9559318423271179
EPOCH 79:
Loss :  1.524261236190796 2.7406768798828125 15.227645874023438
Loss :  1.5445599555969238 2.8845036029815674 15.967077255249023
Loss :  1.4884051084518433 3.0166962146759033 16.57188606262207
Loss :  1.5007147789001465 2.8678572177886963 15.84000015258789
Loss :  1.533630609512329 2.711913824081421 15.093199729919434
Loss :  1.463335394859314 3.5331532955169678 19.129100799560547
Loss :  1.5338705778121948 3.000087022781372 16.534305572509766
Loss :  1.4856157302856445 2.7741475105285645 15.356352806091309
Loss :  1.4682031869888306 3.3392419815063477 18.164411544799805
Loss :  1.53044855594635 2.264000654220581 12.850452423095703
Loss :  1.445443034172058 2.6808698177337646 14.84979248046875
Loss :  1.4453076124191284 2.852811336517334 15.709364891052246
Loss :  1.4397218227386475 3.0022401809692383 16.4509220123291
Loss :  1.4500104188919067 2.5711114406585693 14.305567741394043
Loss :  1.563916563987732 2.8988988399505615 16.05841064453125
Loss :  1.5463440418243408 3.106248378753662 17.077587127685547
Loss :  1.4401414394378662 2.9940905570983887 16.410594940185547
Loss :  1.487573266029358 2.908442497253418 16.02978515625
Loss :  1.434801697731018 3.314584970474243 18.007726669311523
Loss :  1.5576626062393188 2.8855650424957275 15.985487937927246
  batch 20 loss: 1.5576626062393188, 2.8855650424957275, 15.985487937927246
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.4873554706573486 2.8378422260284424 15.676566123962402
Loss :  1.437528371810913 2.5829579830169678 14.35231876373291
Loss :  1.4757866859436035 2.4365477561950684 13.658525466918945
Loss :  1.5032542943954468 2.860588550567627 15.806196212768555
Loss :  1.556754469871521 3.3047091960906982 18.08030128479004
Loss :  1.477830410003662 2.8979272842407227 15.967466354370117
Loss :  1.4950543642044067 2.9486517906188965 16.238313674926758
Loss :  1.4878365993499756 2.9205338954925537 16.090505599975586
Loss :  1.390288233757019 3.8138623237609863 20.459598541259766
Loss :  1.551379919052124 2.937788963317871 16.240324020385742
Loss :  1.3917676210403442 2.8707540035247803 15.745537757873535
Loss :  1.5169432163238525 2.6495468616485596 14.764677047729492
Loss :  1.47139573097229 2.6993587017059326 14.968189239501953
Loss :  1.4668508768081665 2.7820494174957275 15.377098083496094
Loss :  1.4020329713821411 2.7097654342651367 14.950860023498535
Loss :  1.429087519645691 3.7827813625335693 20.342992782592773
Loss :  1.4232944250106812 3.095717430114746 16.90188217163086
Loss :  1.5343698263168335 3.754026174545288 20.304500579833984
Loss :  1.54146409034729 2.880091428756714 15.94192123413086
Loss :  1.5601567029953003 2.490832567214966 14.01431941986084
  batch 40 loss: 1.5601567029953003, 2.490832567214966, 14.01431941986084
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.4805879592895508 2.95668888092041 16.2640323638916
Loss :  1.4574357271194458 2.74448561668396 15.179863929748535
Loss :  1.440631628036499 2.4693334102630615 13.787299156188965
Loss :  1.460436463356018 2.587394952774048 14.397411346435547
Loss :  1.423148274421692 2.715266466140747 14.999480247497559
Loss :  1.4808952808380127 2.799443244934082 15.478111267089844
Loss :  1.5478620529174805 2.6496551036834717 14.796137809753418
Loss :  1.4569170475006104 2.9147276878356934 16.030555725097656
Loss :  1.577981948852539 2.744492292404175 15.300443649291992
Loss :  1.4677640199661255 2.6125590801239014 14.530559539794922
Loss :  1.5331785678863525 3.3661789894104004 18.364072799682617
Loss :  1.5200965404510498 2.6743228435516357 14.891711235046387
Loss :  1.482718825340271 2.7346408367156982 15.155922889709473
Loss :  1.5356968641281128 2.848921537399292 15.780304908752441
Loss :  1.4624509811401367 2.7835588455200195 15.380245208740234
Loss :  1.5832854509353638 3.153372049331665 17.35014533996582
Loss :  1.4747018814086914 3.034141778945923 16.645410537719727
Loss :  1.4458993673324585 2.9281458854675293 16.08662986755371
Loss :  1.474792718887329 3.053530216217041 16.742443084716797
Loss :  1.5927549600601196 2.632969379425049 14.757601737976074
  batch 60 loss: 1.5927549600601196, 2.632969379425049, 14.757601737976074
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.4417665004730225 3.016735076904297 16.525442123413086
Loss :  1.5001267194747925 2.465033531188965 13.825294494628906
Loss :  1.459460973739624 2.8681154251098633 15.80003833770752
Loss :  1.4425798654556274 3.034834623336792 16.61675262451172
Loss :  1.4055744409561157 2.5286755561828613 14.048952102661133
Loss :  1.5018422603607178 4.421736240386963 23.610523223876953
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.5258737802505493 4.388001918792725 23.465883255004883
Loss :  1.5323214530944824 4.21049690246582 22.584806442260742
Loss :  1.500501275062561 4.221729278564453 22.609148025512695
Total LOSS train 15.942101977421688 valid 23.06759023666382
CE LOSS train 1.4866637615057139 valid 0.37512531876564026
Contrastive LOSS train 2.891087660422692 valid 1.0554323196411133
EPOCH 80:
Loss :  1.527205228805542 2.687481641769409 14.964613914489746
Loss :  1.5500853061676025 2.8625426292419434 15.862798690795898
Loss :  1.4925708770751953 3.419558048248291 18.590360641479492
Loss :  1.5030995607376099 2.8516366481781006 15.761282920837402
Loss :  1.5337806940078735 2.9896175861358643 16.481868743896484
Loss :  1.465636968612671 3.4976775646209717 18.954025268554688
Loss :  1.534817099571228 2.784031629562378 15.454975128173828
Loss :  1.487499475479126 3.009613275527954 16.535566329956055
Loss :  1.4742436408996582 2.5841848850250244 14.39516830444336
Loss :  1.538393259048462 2.8415133953094482 15.745960235595703
Loss :  1.4559459686279297 2.9401392936706543 16.15664291381836
Loss :  1.4561359882354736 3.0880208015441895 16.896240234375
Loss :  1.4517649412155151 3.4679951667785645 18.79174041748047
Loss :  1.4622985124588013 2.559016704559326 14.2573823928833
Loss :  1.572542667388916 3.192485809326172 17.534971237182617
Loss :  1.5565632581710815 2.4198381900787354 13.655754089355469
Loss :  1.4550588130950928 2.5768802165985107 14.339460372924805
Loss :  1.5028764009475708 2.7323787212371826 15.164770126342773
Loss :  1.4510786533355713 2.606699228286743 14.484574317932129
Loss :  1.5695983171463013 2.710860252380371 15.123899459838867
  batch 20 loss: 1.5695983171463013, 2.710860252380371, 15.123899459838867
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 5], device='cuda:0')
Loss :  1.5008686780929565 2.7640817165374756 15.321277618408203
Loss :  1.4507111310958862 2.9841291904449463 16.371356964111328
Loss :  1.4868139028549194 2.3299355506896973 13.136490821838379
Loss :  1.5109515190124512 2.665919542312622 14.84054946899414
Loss :  1.5617399215698242 2.595303535461426 14.538257598876953
Loss :  1.4825079441070557 2.711801052093506 15.041513442993164
Loss :  1.500916838645935 3.069011688232422 16.845975875854492
Loss :  1.4929614067077637 2.670004367828369 14.84298324584961
Loss :  1.3991038799285889 3.240936517715454 17.60378646850586
Loss :  1.5555332899093628 3.015141248703003 16.63123893737793
Loss :  1.4027453660964966 2.876591682434082 15.785703659057617
Loss :  1.5257700681686401 2.6424777507781982 14.7381591796875
Loss :  1.4799758195877075 3.0227925777435303 16.59393882751465
Loss :  1.477597951889038 2.7697043418884277 15.326120376586914
Loss :  1.4183164834976196 3.2401154041290283 17.618892669677734
Loss :  1.4466495513916016 3.1564245223999023 17.228771209716797
Loss :  1.444564938545227 2.8925020694732666 15.907074928283691
Loss :  1.5495543479919434 2.9212918281555176 16.15601348876953
Loss :  1.560848593711853 2.87587308883667 15.940214157104492
Loss :  1.5752755403518677 2.953974962234497 16.345149993896484
  batch 40 loss: 1.5752755403518677, 2.953974962234497, 16.345149993896484
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.5010052919387817 3.503041982650757 19.01621437072754
Loss :  1.4769128561019897 2.922262668609619 16.088226318359375
Loss :  1.4591656923294067 2.8615036010742188 15.766683578491211
Loss :  1.4812582731246948 2.5855448246002197 14.408982276916504
Loss :  1.4437205791473389 2.8159019947052 15.52323055267334
Loss :  1.4956328868865967 2.751288652420044 15.252076148986816
Loss :  1.558787226676941 3.299823045730591 18.05790138244629
Loss :  1.4690800905227661 2.973222255706787 16.33519172668457
Loss :  1.5841392278671265 3.1434643268585205 17.30146026611328
Loss :  1.4784170389175415 2.9739763736724854 16.348299026489258
Loss :  1.540246605873108 3.086158037185669 16.971036911010742
Loss :  1.530657410621643 3.3420250415802 18.240781784057617
Loss :  1.4929471015930176 2.55165958404541 14.251245498657227
Loss :  1.5444828271865845 2.864696979522705 15.86796760559082
Loss :  1.4720458984375 2.993776559829712 16.440929412841797
Loss :  1.5859601497650146 2.5764403343200684 14.468162536621094
Loss :  1.4839707612991333 2.8686530590057373 15.82723617553711
Loss :  1.4581362009048462 3.5742125511169434 19.329198837280273
Loss :  1.4840902090072632 3.17018985748291 17.335039138793945
Loss :  1.5981025695800781 3.1154327392578125 17.17526626586914
  batch 60 loss: 1.5981025695800781, 3.1154327392578125, 17.17526626586914
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.4529772996902466 2.989431142807007 16.40013313293457
Loss :  1.5073301792144775 3.096742868423462 16.991044998168945
Loss :  1.4622100591659546 2.835418224334717 15.639301300048828
Loss :  1.4446308612823486 2.97434663772583 16.316364288330078
Loss :  1.4051830768585205 2.4617433547973633 13.713899612426758
Loss :  1.5440367460250854 4.4049859046936035 23.568965911865234
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.564150094985962 4.406761646270752 23.597957611083984
Loss :  1.5722419023513794 4.250517845153809 22.824832916259766
Loss :  1.550402283668518 4.353813171386719 23.319467544555664
Total LOSS train 16.077406120300292 valid 23.327805995941162
CE LOSS train 1.4965491258181058 valid 0.3876005709171295
Contrastive LOSS train 2.9161714003636288 valid 1.0884532928466797
EPOCH 81:
Loss :  1.5295863151550293 2.610790729522705 14.583539962768555
Loss :  1.5523314476013184 2.898801326751709 16.04633903503418
Loss :  1.4922782182693481 2.9984216690063477 16.484386444091797
Loss :  1.5043326616287231 3.0228469371795654 16.618568420410156
Loss :  1.534170150756836 2.9762003421783447 16.415172576904297
Loss :  1.4653910398483276 4.04386043548584 21.684694290161133
Loss :  1.5350240468978882 3.2145512104034424 17.60778045654297
Loss :  1.4845590591430664 2.9951772689819336 16.460445404052734
Loss :  1.4667657613754272 2.8716514110565186 15.82502269744873
Loss :  1.5321325063705444 2.6920552253723145 14.99240779876709
Loss :  1.4449816942214966 3.6059188842773438 19.474576950073242
Loss :  1.4441509246826172 3.47214674949646 18.80488395690918
Loss :  1.4405016899108887 3.1370692253112793 17.1258487701416
Loss :  1.448412299156189 2.8300976753234863 15.598899841308594
Loss :  1.5644766092300415 2.617392063140869 14.651436805725098
Loss :  1.5462336540222168 2.817991018295288 15.636188507080078
Loss :  1.435002326965332 3.617685079574585 19.523426055908203
Loss :  1.4835169315338135 2.978585958480835 16.376447677612305
Loss :  1.4267449378967285 2.6378328800201416 14.615909576416016
Loss :  1.55036199092865 2.6520884037017822 14.810803413391113
  batch 20 loss: 1.55036199092865, 2.6520884037017822, 14.810803413391113
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.4778059720993042 2.7601048946380615 15.27833080291748
Loss :  1.4239927530288696 3.074713706970215 16.797561645507812
Loss :  1.4638993740081787 2.506500244140625 13.996400833129883
Loss :  1.4958075284957886 2.8196280002593994 15.593947410583496
Loss :  1.5507832765579224 3.1766247749328613 17.43390655517578
Loss :  1.4650533199310303 3.078758955001831 16.858848571777344
Loss :  1.4837380647659302 3.147838592529297 17.222930908203125
Loss :  1.4731112718582153 2.5024237632751465 13.985230445861816
Loss :  1.3720206022262573 2.62455153465271 14.494778633117676
Loss :  1.5423306226730347 3.0133485794067383 16.609073638916016
Loss :  1.3735148906707764 2.667095899581909 14.70899486541748
Loss :  1.5080279111862183 2.5842294692993164 14.42917537689209
Loss :  1.4603551626205444 3.159242868423462 17.256568908691406
Loss :  1.4577468633651733 2.76444411277771 15.279967308044434
Loss :  1.3933939933776855 3.2841055393218994 17.813922882080078
Loss :  1.4226139783859253 2.9653725624084473 16.249475479125977
Loss :  1.420901894569397 2.878836154937744 15.815082550048828
Loss :  1.540069580078125 2.5461418628692627 14.27077865600586
Loss :  1.5506463050842285 2.549556255340576 14.29842758178711
Loss :  1.5686924457550049 2.6828343868255615 14.982864379882812
  batch 40 loss: 1.5686924457550049, 2.6828343868255615, 14.982864379882812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.4901477098464966 2.7903940677642822 15.442117691040039
Loss :  1.4650537967681885 2.8285892009735107 15.607999801635742
Loss :  1.4458186626434326 2.4956889152526855 13.924263954162598
Loss :  1.468200922012329 2.2897279262542725 12.916840553283691
Loss :  1.429559350013733 2.745285749435425 15.155988693237305
Loss :  1.487438678741455 2.7215707302093506 15.095293045043945
Loss :  1.5546523332595825 2.6030890941619873 14.570097923278809
Loss :  1.461018681526184 2.6553261280059814 14.737648963928223
Loss :  1.5845270156860352 2.4464704990386963 13.816879272460938
Loss :  1.4711482524871826 2.9495394229888916 16.21884536743164
Loss :  1.5370961427688599 2.929513692855835 16.18466567993164
Loss :  1.5212717056274414 2.73598051071167 15.20117473602295
Loss :  1.4842816591262817 3.1225547790527344 17.097055435180664
Loss :  1.5363343954086304 2.9897031784057617 16.48484992980957
Loss :  1.4596457481384277 2.842378854751587 15.671539306640625
Loss :  1.5793540477752686 2.4325528144836426 13.742117881774902
Loss :  1.4720170497894287 2.765843152999878 15.301233291625977
Loss :  1.4459388256072998 2.8999414443969727 15.945646286010742
Loss :  1.474684238433838 2.5101654529571533 14.025510787963867
Loss :  1.5937749147415161 2.7718355655670166 15.45295238494873
  batch 60 loss: 1.5937749147415161, 2.7718355655670166, 15.45295238494873
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.4460668563842773 2.4379892349243164 13.63601303100586
Loss :  1.5033106803894043 2.243983030319214 12.723226547241211
Loss :  1.4613711833953857 2.674046277999878 14.831603050231934
Loss :  1.4441273212432861 2.4445114135742188 13.6666841506958
Loss :  1.406774640083313 2.0100598335266113 11.457073211669922
Loss :  1.4851611852645874 4.49257755279541 23.948047637939453
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.5099633932113647 4.403860092163086 23.529264450073242
Loss :  1.5101473331451416 4.301207065582275 23.01618194580078
Loss :  1.496158242225647 4.082350254058838 21.907909393310547
Total LOSS train 15.624928723848782 valid 23.100350856781006
CE LOSS train 1.4853703828958364 valid 0.37403956055641174
Contrastive LOSS train 2.8279116557194635 valid 1.0205875635147095
EPOCH 82:
Loss :  1.5321369171142578 3.492680311203003 18.99553871154785
Loss :  1.5520570278167725 2.9114832878112793 16.109474182128906
Loss :  1.4976624250411987 2.664024591445923 14.817785263061523
Loss :  1.513621211051941 2.8997318744659424 16.012279510498047
Loss :  1.5420275926589966 2.9791836738586426 16.437946319580078
Loss :  1.4736028909683228 2.6160264015197754 14.55373477935791
Loss :  1.54257333278656 2.3536651134490967 13.310898780822754
Loss :  1.4919342994689941 3.4283931255340576 18.633899688720703
Loss :  1.4737377166748047 2.6780736446380615 14.864106178283691
Loss :  1.5364197492599487 2.477207660675049 13.922457695007324
Loss :  1.455271601676941 2.8123440742492676 15.51699161529541
Loss :  1.454202651977539 2.5937917232513428 14.423161506652832
Loss :  1.4499136209487915 2.603659152984619 14.468209266662598
Loss :  1.4594664573669434 2.9581682682037354 16.250307083129883
Loss :  1.5697637796401978 3.046060800552368 16.800067901611328
Loss :  1.5501768589019775 2.804879665374756 15.574575424194336
Loss :  1.4396439790725708 2.9614107608795166 16.2466983795166
Loss :  1.4873847961425781 3.3880019187927246 18.42739486694336
Loss :  1.4331990480422974 2.9350993633270264 16.10869598388672
Loss :  1.551623821258545 3.0179224014282227 16.6412353515625
  batch 20 loss: 1.551623821258545, 3.0179224014282227, 16.6412353515625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.485204815864563 2.474782705307007 13.859118461608887
Loss :  1.4362337589263916 3.360717535018921 18.23982048034668
Loss :  1.475183129310608 2.31274676322937 13.038917541503906
Loss :  1.5070257186889648 2.7211508750915527 15.112780570983887
Loss :  1.56106698513031 2.565563678741455 14.388884544372559
Loss :  1.4823052883148193 3.0880770683288574 16.922691345214844
Loss :  1.5022774934768677 3.131232500076294 17.15843963623047
Loss :  1.4917889833450317 2.619927167892456 14.591424942016602
Loss :  1.395699381828308 2.5481197834014893 14.136298179626465
Loss :  1.5516488552093506 2.5116171836853027 14.109735488891602
Loss :  1.3957878351211548 2.741452932357788 15.103053092956543
Loss :  1.5203725099563599 2.475882053375244 13.899782180786133
Loss :  1.475203514099121 2.9165680408477783 16.05804443359375
Loss :  1.471234917640686 2.58777117729187 14.410091400146484
Loss :  1.41012704372406 3.0951004028320312 16.885629653930664
Loss :  1.4391136169433594 2.804288387298584 15.460556030273438
Loss :  1.4358971118927002 2.183631658554077 12.354055404663086
Loss :  1.5464084148406982 2.5396533012390137 14.244675636291504
Loss :  1.5577985048294067 3.3118889331817627 18.11724281311035
Loss :  1.5722229480743408 2.4686224460601807 13.915334701538086
  batch 40 loss: 1.5722229480743408, 2.4686224460601807, 13.915334701538086
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 5], device='cuda:0')
Loss :  1.4970015287399292 3.0121066570281982 16.55753517150879
Loss :  1.4753577709197998 2.42643404006958 13.607527732849121
Loss :  1.458910584449768 2.4864416122436523 13.891119003295898
Loss :  1.4815059900283813 2.6373651027679443 14.66833209991455
Loss :  1.446166753768921 2.909733772277832 15.99483585357666
Loss :  1.5011346340179443 3.209517240524292 17.548721313476562
Loss :  1.5617084503173828 2.6623849868774414 14.87363338470459
Loss :  1.4746694564819336 2.979452610015869 16.371932983398438
Loss :  1.5892528295516968 2.5509161949157715 14.343833923339844
Loss :  1.4857845306396484 3.1447019577026367 17.209293365478516
Loss :  1.5494468212127686 3.052635908126831 16.812625885009766
Loss :  1.539331078529358 3.269324779510498 17.885955810546875
Loss :  1.5051383972167969 3.436145067214966 18.685863494873047
Loss :  1.5536648035049438 2.5226614475250244 14.166972160339355
Loss :  1.479945182800293 2.8260703086853027 15.610297203063965
Loss :  1.58975350856781 2.482861280441284 14.004059791564941
Loss :  1.4922107458114624 2.539883613586426 14.191628456115723
Loss :  1.4675109386444092 2.7796127796173096 15.365574836730957
Loss :  1.4958429336547852 2.6247365474700928 14.619525909423828
Loss :  1.60695481300354 2.330296754837036 13.258438110351562
  batch 60 loss: 1.60695481300354, 2.330296754837036, 13.258438110351562
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.4674562215805054 2.847641706466675 15.70566463470459
Loss :  1.5183862447738647 2.7808544635772705 15.42265796661377
Loss :  1.4804954528808594 2.497843027114868 13.969710350036621
Loss :  1.4611462354660034 2.9376261234283447 16.149276733398438
Loss :  1.425095558166504 1.971321940422058 11.281704902648926
Loss :  1.4957647323608398 4.418806076049805 23.589794158935547
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.5185190439224243 4.334285259246826 23.189945220947266
Loss :  1.5157103538513184 4.308427333831787 23.05784797668457
Loss :  1.5121674537658691 4.003696918487549 21.530651092529297
Total LOSS train 15.420288555438702 valid 22.84205961227417
CE LOSS train 1.497275262612563 valid 0.3780418634414673
Contrastive LOSS train 2.7846026512292714 valid 1.0009242296218872
EPOCH 83:
Loss :  1.5420100688934326 2.8928792476654053 16.006406784057617
Loss :  1.5628806352615356 3.51081919670105 19.11697769165039
Loss :  1.510056734085083 2.3578364849090576 13.299239158630371
Loss :  1.5206502676010132 3.5787737369537354 19.414520263671875
Loss :  1.550377368927002 2.5178961753845215 14.13985824584961
Loss :  1.488882303237915 2.5670034885406494 14.323899269104004
Loss :  1.5554229021072388 2.5178654193878174 14.144749641418457
Loss :  1.5132741928100586 2.191685676574707 12.471702575683594
Loss :  1.49794602394104 2.5512499809265137 14.254196166992188
Loss :  1.5579150915145874 2.732917547225952 15.222502708435059
Loss :  1.481824517250061 2.633640766143799 14.650028228759766
Loss :  1.4825016260147095 2.5062034130096436 14.013519287109375
Loss :  1.4804003238677979 2.893172264099121 15.946261405944824
Loss :  1.4902628660202026 3.036059617996216 16.670560836791992
Loss :  1.5909836292266846 2.8987107276916504 16.084537506103516
Loss :  1.5758638381958008 2.87845516204834 15.9681396484375
Loss :  1.4767684936523438 2.482713222503662 13.890335083007812
Loss :  1.5180330276489258 3.2422397136688232 17.729232788085938
Loss :  1.4699645042419434 2.674511194229126 14.842519760131836
Loss :  1.5764763355255127 2.5835464000701904 14.494208335876465
  batch 20 loss: 1.5764763355255127, 2.5835464000701904, 14.494208335876465
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 5], device='cuda:0')
Loss :  1.5160294771194458 2.2019588947296143 12.525823593139648
Loss :  1.4730786085128784 2.8261914253234863 15.604035377502441
Loss :  1.5066437721252441 2.4439644813537598 13.726465225219727
Loss :  1.5310920476913452 2.2912747859954834 12.987465858459473
Loss :  1.5773489475250244 2.798206090927124 15.568379402160645
Loss :  1.5053120851516724 2.7071197032928467 15.040910720825195
Loss :  1.52126944065094 2.8422462940216064 15.732501029968262
Loss :  1.5115573406219482 2.8480207920074463 15.75166130065918
Loss :  1.43012273311615 2.3518524169921875 13.189384460449219
Loss :  1.5741199254989624 2.5112364292144775 14.130301475524902
Loss :  1.437343955039978 3.373915195465088 18.30691909790039
Loss :  1.5502219200134277 2.59273099899292 14.513877868652344
Loss :  1.5101873874664307 2.411440849304199 13.567391395568848
Loss :  1.507651448249817 2.484360456466675 13.92945384979248
Loss :  1.4501962661743164 3.267712354660034 17.78875732421875
Loss :  1.4751534461975098 2.8121635913848877 15.535970687866211
Loss :  1.4722422361373901 2.693128824234009 14.937886238098145
Loss :  1.5713688135147095 2.27363657951355 12.939552307128906
Loss :  1.583471655845642 3.429929256439209 18.733118057250977
Loss :  1.5966923236846924 2.8703126907348633 15.94825553894043
  batch 40 loss: 1.5966923236846924, 2.8703126907348633, 15.94825553894043
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 5], device='cuda:0')
Loss :  1.5271919965744019 2.4943339824676514 13.998862266540527
Loss :  1.5055437088012695 2.3631348609924316 13.32121753692627
Loss :  1.487390160560608 2.6572465896606445 14.7736234664917
Loss :  1.507851004600525 2.9650704860687256 16.33320426940918
Loss :  1.4751038551330566 2.4152724742889404 13.55146598815918
Loss :  1.5244181156158447 3.1665070056915283 17.356952667236328
Loss :  1.581863284111023 2.9334006309509277 16.24886703491211
Loss :  1.4984151124954224 2.3092141151428223 13.044485092163086
Loss :  1.6030713319778442 3.002551555633545 16.615829467773438
Loss :  1.505231261253357 2.7494637966156006 15.25255012512207
Loss :  1.5619046688079834 3.2631494998931885 17.877653121948242
Loss :  1.5475375652313232 2.426518678665161 13.680130958557129
Loss :  1.5146127939224243 2.4126369953155518 13.577797889709473
Loss :  1.5608034133911133 3.1848299503326416 17.484952926635742
Loss :  1.4973485469818115 3.2264316082000732 17.629507064819336
Loss :  1.6020193099975586 3.076355218887329 16.983795166015625
Loss :  1.5113106966018677 2.886486291885376 15.943741798400879
Loss :  1.4856795072555542 2.9094457626342773 16.032907485961914
Loss :  1.5112392902374268 2.9023983478546143 16.023231506347656
Loss :  1.614237904548645 2.56057071685791 14.417091369628906
  batch 60 loss: 1.614237904548645, 2.56057071685791, 14.417091369628906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.4844386577606201 2.3880157470703125 13.424517631530762
Loss :  1.5334982872009277 2.382981061935425 13.448404312133789
Loss :  1.5024267435073853 2.5340969562530518 14.172911643981934
Loss :  1.487845540046692 3.024207592010498 16.608884811401367
Loss :  1.4573915004730225 2.7481603622436523 15.198193550109863
Loss :  1.5431228876113892 4.416611194610596 23.626178741455078
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5636073350906372 4.367584705352783 23.401531219482422
Loss :  1.5581047534942627 4.175032138824463 22.433265686035156
Loss :  1.5591232776641846 4.146969795227051 22.29397201538086
Total LOSS train 15.232958236107459 valid 22.93873691558838
CE LOSS train 1.5204918898068942 valid 0.38978081941604614
Contrastive LOSS train 2.742493258989774 valid 1.0367424488067627
EPOCH 84:
Loss :  1.563828468322754 3.0128912925720215 16.628284454345703
Loss :  1.579254150390625 3.540778398513794 19.283145904541016
Loss :  1.5307741165161133 3.0010647773742676 16.53609848022461
Loss :  1.5425981283187866 3.31082820892334 18.09674072265625
Loss :  1.5696510076522827 2.4994211196899414 14.066756248474121
Loss :  1.5118459463119507 2.809528112411499 15.559486389160156
Loss :  1.5663946866989136 3.128511667251587 17.208951950073242
Loss :  1.5235968828201294 3.272080659866333 17.884000778198242
Loss :  1.5068246126174927 2.89095401763916 15.961594581604004
Loss :  1.5641486644744873 2.60024094581604 14.565353393554688
Loss :  1.497280240058899 2.7578582763671875 15.286571502685547
Loss :  1.498196005821228 3.4133059978485107 18.564725875854492
Loss :  1.4937477111816406 2.8885233402252197 15.93636417388916
Loss :  1.4998385906219482 3.0537869930267334 16.768774032592773
Loss :  1.593092679977417 3.3937182426452637 18.561683654785156
Loss :  1.5815626382827759 2.982792854309082 16.495527267456055
Loss :  1.4859904050827026 3.3236889839172363 18.104434967041016
Loss :  1.5298452377319336 3.1528120040893555 17.29390525817871
Loss :  1.4784190654754639 2.774343967437744 15.350138664245605
Loss :  1.5833606719970703 2.9245524406433105 16.20612335205078
  batch 20 loss: 1.5833606719970703, 2.9245524406433105, 16.20612335205078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 5], device='cuda:0')
Loss :  1.5210034847259521 2.7975575923919678 15.50879192352295
Loss :  1.4748824834823608 2.822795867919922 15.588861465454102
Loss :  1.5061379671096802 2.8958659172058105 15.985467910766602
Loss :  1.5320552587509155 2.8453240394592285 15.758674621582031
Loss :  1.578920841217041 2.987912893295288 16.51848602294922
Loss :  1.5077688694000244 3.113328695297241 17.074413299560547
Loss :  1.5265425443649292 2.631211757659912 14.682601928710938
Loss :  1.5186458826065063 2.5146145820617676 14.091718673706055
Loss :  1.4306039810180664 2.3392317295074463 13.126762390136719
Loss :  1.5717477798461914 2.7263023853302 15.203259468078613
Loss :  1.4309343099594116 3.671639919281006 19.789134979248047
Loss :  1.5442410707473755 2.9871695041656494 16.48008918762207
Loss :  1.5048002004623413 3.2364120483398438 17.686861038208008
Loss :  1.5034252405166626 3.073620080947876 16.871524810791016
Loss :  1.4465551376342773 3.0055739879608154 16.47442626953125
Loss :  1.4719891548156738 2.9090380668640137 16.017179489135742
Loss :  1.4694634675979614 2.9607937335968018 16.2734317779541
Loss :  1.5684407949447632 2.6043877601623535 14.590378761291504
Loss :  1.5759432315826416 2.6120893955230713 14.63638973236084
Loss :  1.5922789573669434 3.056279420852661 16.873676300048828
  batch 40 loss: 1.5922789573669434, 3.056279420852661, 16.873676300048828
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.5229636430740356 2.8107800483703613 15.576863288879395
Loss :  1.5004534721374512 3.014615535736084 16.573532104492188
Loss :  1.483809232711792 2.7123913764953613 15.04576587677002
Loss :  1.5022516250610352 2.816344738006592 15.583975791931152
Loss :  1.469549298286438 2.526068687438965 14.099892616271973
Loss :  1.5211820602416992 2.8070878982543945 15.556621551513672
Loss :  1.579811692237854 2.7781097888946533 15.47036075592041
Loss :  1.4922734498977661 2.470791816711426 13.846232414245605
Loss :  1.6012548208236694 3.066288471221924 16.932697296142578
Loss :  1.501202940940857 3.519630193710327 19.099353790283203
Loss :  1.5555758476257324 3.2310190200805664 17.710670471191406
Loss :  1.550561785697937 2.4051461219787598 13.576292037963867
Loss :  1.5196727514266968 2.7396926879882812 15.218135833740234
Loss :  1.5690734386444092 3.2335383892059326 17.736764907836914
Loss :  1.4931567907333374 3.298351287841797 17.984912872314453
Loss :  1.601865530014038 2.6407389640808105 14.805561065673828
Loss :  1.5094541311264038 3.1863796710968018 17.44135284423828
Loss :  1.481183409690857 2.590715169906616 14.434759140014648
Loss :  1.505049467086792 3.073794364929199 16.874021530151367
Loss :  1.6102170944213867 2.4863951206207275 14.042192459106445
  batch 60 loss: 1.6102170944213867, 2.4863951206207275, 14.042192459106445
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.478979468345642 2.886399030685425 15.910974502563477
Loss :  1.5316369533538818 2.5169103145599365 14.116189002990723
Loss :  1.498847484588623 3.6582682132720947 19.790189743041992
Loss :  1.4761872291564941 3.0286383628845215 16.6193790435791
Loss :  1.4390833377838135 2.746699571609497 15.17258071899414
Loss :  1.537428379058838 4.234072685241699 22.707792282104492
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5567455291748047 4.193510055541992 22.524295806884766
Loss :  1.5593845844268799 4.081381797790527 21.96629524230957
Loss :  1.539251446723938 3.948464870452881 21.28157615661621
Total LOSS train 16.197077897878792 valid 22.11998987197876
CE LOSS train 1.522645038824815 valid 0.3848128616809845
Contrastive LOSS train 2.9348865619072546 valid 0.9871162176132202
EPOCH 85:
Loss :  1.5526319742202759 2.8548877239227295 15.827071189880371
Loss :  1.570494294166565 3.1045608520507812 17.093297958374023
Loss :  1.517419695854187 3.8009727001190186 20.52228355407715
Loss :  1.5299925804138184 3.065859079360962 16.85928726196289
Loss :  1.5572539567947388 3.255836248397827 17.836435317993164
Loss :  1.493757963180542 3.377385139465332 18.38068199157715
Loss :  1.5568418502807617 2.9807374477386475 16.460529327392578
Loss :  1.5116394758224487 2.588742733001709 14.455353736877441
Loss :  1.4918733835220337 2.7854790687561035 15.419268608093262
Loss :  1.5494743585586548 2.5409796237945557 14.254372596740723
Loss :  1.4713327884674072 3.1294748783111572 17.11870765686035
Loss :  1.4688780307769775 3.7140705585479736 20.039230346679688
Loss :  1.4614115953445435 3.03729510307312 16.64788818359375
Loss :  1.4721944332122803 3.1093027591705322 17.018707275390625
Loss :  1.58054518699646 3.117609739303589 17.168594360351562
Loss :  1.5675588846206665 3.205453634262085 17.594825744628906
Loss :  1.457509160041809 3.0896031856536865 16.90552520751953
Loss :  1.5049272775650024 2.856940269470215 15.789628982543945
Loss :  1.4479116201400757 2.8155250549316406 15.52553653717041
Loss :  1.5616464614868164 2.717909097671509 15.151191711425781
  batch 20 loss: 1.5616464614868164, 2.717909097671509, 15.151191711425781
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.493708848953247 3.1243481636047363 17.115449905395508
Loss :  1.442042350769043 3.299365282058716 17.93886947631836
Loss :  1.4779865741729736 3.6970014572143555 19.962995529174805
Loss :  1.507674217224121 2.7648870944976807 15.332109451293945
Loss :  1.5617637634277344 3.150181770324707 17.312671661376953
Loss :  1.481833815574646 2.8190557956695557 15.577112197875977
Loss :  1.5012379884719849 3.3206872940063477 18.104673385620117
Loss :  1.4896290302276611 2.9852616786956787 16.415937423706055
Loss :  1.3903285264968872 3.41268253326416 18.4537410736084
Loss :  1.5507159233093262 3.326289176940918 18.18216323852539
Loss :  1.3861870765686035 3.1664602756500244 17.218488693237305
Loss :  1.512955665588379 2.889112710952759 15.958518981933594
Loss :  1.4647784233093262 2.86136531829834 15.771604537963867
Loss :  1.4601893424987793 2.7364256381988525 15.142316818237305
Loss :  1.3958526849746704 3.033522129058838 16.56346321105957
Loss :  1.4236091375350952 3.275251626968384 17.799867630004883
Loss :  1.4208406209945679 3.069556951522827 16.768625259399414
Loss :  1.5377540588378906 2.4774396419525146 13.924952507019043
Loss :  1.5503051280975342 2.5280582904815674 14.190596580505371
Loss :  1.5688694715499878 2.5876166820526123 14.506953239440918
  batch 40 loss: 1.5688694715499878, 2.5876166820526123, 14.506953239440918
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.4905016422271729 3.088205099105835 16.931528091430664
Loss :  1.4659671783447266 2.967198610305786 16.301959991455078
Loss :  1.4472506046295166 3.1843533515930176 17.369016647338867
Loss :  1.4692738056182861 2.600884437561035 14.473695755004883
Loss :  1.4305932521820068 2.8029589653015137 15.445388793945312
Loss :  1.4882851839065552 2.7521274089813232 15.248922348022461
Loss :  1.5515236854553223 3.0137741565704346 16.620393753051758
Loss :  1.4633479118347168 2.9191789627075195 16.059242248535156
Loss :  1.5879772901535034 3.397958278656006 18.577768325805664
Loss :  1.4808943271636963 3.189175844192505 17.426773071289062
Loss :  1.5445102453231812 3.0273313522338867 16.681167602539062
Loss :  1.5297293663024902 3.5979413986206055 19.519437789916992
Loss :  1.4898602962493896 2.556483268737793 14.272276878356934
Loss :  1.538151741027832 2.5514817237854004 14.295559883117676
Loss :  1.4638938903808594 2.5350167751312256 14.138978004455566
Loss :  1.5768731832504272 2.7773101329803467 15.463423728942871
Loss :  1.4721230268478394 3.2056612968444824 17.500429153442383
Loss :  1.445974588394165 3.031242609024048 16.602188110351562
Loss :  1.473999261856079 3.279839515686035 17.87319564819336
Loss :  1.593436360359192 2.906585216522217 16.12636375427246
  batch 60 loss: 1.593436360359192, 2.906585216522217, 16.12636375427246
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.441894292831421 2.7786977291107178 15.335383415222168
Loss :  1.5025466680526733 2.595731496810913 14.48120403289795
Loss :  1.4633716344833374 3.1269989013671875 17.098365783691406
Loss :  1.443372130393982 2.879316806793213 15.83995532989502
Loss :  1.403891921043396 2.1246917247772217 12.027350425720215
Loss :  1.4984498023986816 4.361435890197754 23.305627822875977
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.5140527486801147 4.3866729736328125 23.447418212890625
Loss :  1.5129259824752808 4.236749172210693 22.696672439575195
Loss :  1.5047024488449097 4.234733581542969 22.678369522094727
Total LOSS train 16.46183841411884 valid 23.03202199935913
CE LOSS train 1.495427709359389 valid 0.3761756122112274
Contrastive LOSS train 2.993282145720262 valid 1.0586833953857422
EPOCH 86:
Loss :  1.529315710067749 2.5567433834075928 14.313033103942871
Loss :  1.5508043766021729 3.061255693435669 16.85708236694336
Loss :  1.493936538696289 2.840445041656494 15.696161270141602
Loss :  1.5074256658554077 2.7556381225585938 15.285615921020508
Loss :  1.5390231609344482 2.686436414718628 14.971205711364746
Loss :  1.4677473306655884 2.842383623123169 15.679665565490723
Loss :  1.536818504333496 2.93646502494812 16.21914291381836
Loss :  1.4919652938842773 2.601087808609009 14.497404098510742
Loss :  1.4793685674667358 2.5583863258361816 14.271299362182617
Loss :  1.5458166599273682 3.0214264392852783 16.6529483795166
Loss :  1.461881399154663 3.2081687450408936 17.50272560119629
Loss :  1.4620131254196167 2.7676424980163574 15.300226211547852
Loss :  1.4554095268249512 2.9582948684692383 16.246883392333984
Loss :  1.4663867950439453 3.1194050312042236 17.063411712646484
Loss :  1.572045922279358 3.0040438175201416 16.59226417541504
Loss :  1.5546404123306274 3.4716365337371826 18.912822723388672
Loss :  1.4506182670593262 3.683521032333374 19.868223190307617
Loss :  1.4947099685668945 3.093494176864624 16.962181091308594
Loss :  1.4446194171905518 2.7699167728424072 15.29420280456543
Loss :  1.5602505207061768 3.3365671634674072 18.243087768554688
  batch 20 loss: 1.5602505207061768, 3.3365671634674072, 18.243087768554688
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.4952925443649292 3.2091689109802246 17.5411376953125
Loss :  1.4410614967346191 3.469038486480713 18.786252975463867
Loss :  1.4743232727050781 3.322108745574951 18.084867477416992
Loss :  1.504119873046875 3.4166712760925293 18.58747673034668
Loss :  1.5563299655914307 3.410691976547241 18.60978889465332
Loss :  1.4722944498062134 2.6474850177764893 14.70971965789795
Loss :  1.4923334121704102 3.0024828910827637 16.504749298095703
Loss :  1.4837485551834106 3.361342191696167 18.29046058654785
Loss :  1.3866554498672485 2.904392719268799 15.908618927001953
Loss :  1.549115777015686 2.87611722946167 15.929702758789062
Loss :  1.3893542289733887 2.9001474380493164 15.890090942382812
Loss :  1.5206797122955322 2.9037461280822754 16.039409637451172
Loss :  1.4725903272628784 3.069634437561035 16.820762634277344
Loss :  1.468928337097168 3.530900716781616 19.123432159423828
Loss :  1.4048811197280884 3.352382183074951 18.166791915893555
Loss :  1.4307482242584229 3.3478195667266846 18.169845581054688
Loss :  1.4273271560668945 2.9029831886291504 15.942242622375488
Loss :  1.5420153141021729 2.6690738201141357 14.887384414672852
Loss :  1.5513761043548584 2.6623921394348145 14.863336563110352
Loss :  1.569270133972168 3.102457284927368 17.08155632019043
  batch 40 loss: 1.569270133972168, 3.102457284927368, 17.08155632019043
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 5], device='cuda:0')
Loss :  1.4913619756698608 3.1719465255737305 17.35109519958496
Loss :  1.4646462202072144 2.9511818885803223 16.22055435180664
Loss :  1.4485267400741577 2.9886972904205322 16.392013549804688
Loss :  1.467406988143921 3.0955095291137695 16.94495391845703
Loss :  1.4295430183410645 2.8112447261810303 15.485767364501953
Loss :  1.4832137823104858 2.9026856422424316 15.996641159057617
Loss :  1.5498863458633423 3.2902286052703857 18.00102996826172
Loss :  1.4620994329452515 3.2064757347106934 17.494478225708008
Loss :  1.5790718793869019 3.1633377075195312 17.39575958251953
Loss :  1.469800353050232 2.758565664291382 15.262628555297852
Loss :  1.5374341011047363 3.1938560009002686 17.5067138671875
Loss :  1.5233532190322876 2.6708145141601562 14.877426147460938
Loss :  1.4839251041412354 2.8490586280822754 15.729217529296875
Loss :  1.5386360883712769 2.995147466659546 16.514373779296875
Loss :  1.4666640758514404 2.9912352561950684 16.422840118408203
Loss :  1.5831043720245361 3.1937553882598877 17.551881790161133
Loss :  1.4742463827133179 2.872255563735962 15.83552360534668
Loss :  1.4456557035446167 2.6880104541778564 14.88570785522461
Loss :  1.4740632772445679 2.9996232986450195 16.472179412841797
Loss :  1.5975520610809326 2.928680896759033 16.240955352783203
  batch 60 loss: 1.5975520610809326, 2.928680896759033, 16.240955352783203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.4454693794250488 2.792652130126953 15.408729553222656
Loss :  1.505950689315796 2.7463366985321045 15.237634658813477
Loss :  1.465500831604004 2.763859987258911 15.28480052947998
Loss :  1.4422041177749634 3.425978899002075 18.572097778320312
Loss :  1.4008464813232422 2.3452415466308594 13.127054214477539
Loss :  1.4509251117706299 3.8095333576202393 20.498592376708984
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.4863083362579346 3.7545013427734375 20.25881576538086
Loss :  1.481978178024292 3.683994770050049 19.90195083618164
Loss :  1.4620721340179443 3.604077100753784 19.482458114624023
Total LOSS train 16.501219588059644 valid 20.035454273223877
CE LOSS train 1.4911908498177162 valid 0.3655180335044861
Contrastive LOSS train 3.002005767822266 valid 0.901019275188446
EPOCH 87:
Loss :  1.5282511711120605 2.6617212295532227 14.836856842041016
Loss :  1.5478602647781372 2.8949508666992188 16.022615432739258
Loss :  1.4854390621185303 2.714909791946411 15.059988021850586
Loss :  1.5005099773406982 2.9835801124572754 16.41840934753418
Loss :  1.5323845148086548 2.620025396347046 14.632512092590332
Loss :  1.4631308317184448 2.6380615234375 14.653438568115234
Loss :  1.5337103605270386 2.8959767818450928 16.013593673706055
Loss :  1.488951325416565 2.6758782863616943 14.868343353271484
Loss :  1.476182460784912 2.6602416038513184 14.77739143371582
Loss :  1.5443286895751953 2.9221372604370117 16.155014038085938
Loss :  1.4589112997055054 3.2465028762817383 17.691425323486328
Loss :  1.4596433639526367 3.356545925140381 18.242374420166016
Loss :  1.4566965103149414 3.1861257553100586 17.387325286865234
Loss :  1.4689620733261108 3.379354953765869 18.36573600769043
Loss :  1.581617832183838 2.9303700923919678 16.233469009399414
Loss :  1.5624979734420776 2.912511110305786 16.12505340576172
Loss :  1.445889949798584 2.7593934535980225 15.242856979370117
Loss :  1.4892114400863647 2.585911512374878 14.418768882751465
Loss :  1.4341353178024292 2.4415745735168457 13.642007827758789
Loss :  1.5553157329559326 2.807973623275757 15.595184326171875
  batch 20 loss: 1.5553157329559326, 2.807973623275757, 15.595184326171875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.4844017028808594 2.2809104919433594 12.888954162597656
Loss :  1.4272159337997437 3.218390703201294 17.519168853759766
Loss :  1.4616302251815796 2.918454647064209 16.053903579711914
Loss :  1.4933059215545654 3.048541307449341 16.736011505126953
Loss :  1.5478554964065552 2.811847448348999 15.60709285736084
Loss :  1.4613581895828247 3.2273051738739014 17.597885131835938
Loss :  1.483782410621643 2.547086238861084 14.21921443939209
Loss :  1.4749139547348022 3.165607213973999 17.302949905395508
Loss :  1.3759931325912476 2.630882740020752 14.53040599822998
Loss :  1.5464730262756348 2.8158698081970215 15.625822067260742
Loss :  1.3801718950271606 2.9872474670410156 16.316410064697266
Loss :  1.5178442001342773 3.404670000076294 18.541194915771484
Loss :  1.4676419496536255 2.935399055480957 16.144638061523438
Loss :  1.462904453277588 3.2319283485412598 17.62254524230957
Loss :  1.3916041851043701 2.8011534214019775 15.397371292114258
Loss :  1.4211807250976562 2.632065773010254 14.581509590148926
Loss :  1.4184004068374634 2.561534881591797 14.226075172424316
Loss :  1.536281704902649 2.7357144355773926 15.214853286743164
Loss :  1.5512837171554565 2.8679187297821045 15.890877723693848
Loss :  1.5671546459197998 2.5944738388061523 14.53952407836914
  batch 40 loss: 1.5671546459197998, 2.5944738388061523, 14.53952407836914
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.4849532842636108 3.2433652877807617 17.701780319213867
Loss :  1.4579064846038818 2.5682222843170166 14.299017906188965
Loss :  1.4405491352081299 4.052428722381592 21.702693939208984
Loss :  1.459914207458496 2.5766777992248535 14.343302726745605
Loss :  1.4243444204330444 3.4083197116851807 18.465944290161133
Loss :  1.4804112911224365 2.70231556892395 14.991989135742188
Loss :  1.5475473403930664 2.487719774246216 13.986145973205566
Loss :  1.4539828300476074 3.732637643814087 20.117172241210938
Loss :  1.5734807252883911 3.0582640171051025 16.86480140686035
Loss :  1.4609875679016113 2.636587619781494 14.643924713134766
Loss :  1.5267493724822998 3.1951467990875244 17.502483367919922
Loss :  1.512142539024353 3.139631509780884 17.21030044555664
Loss :  1.4698840379714966 3.2391695976257324 17.665733337402344
Loss :  1.5242042541503906 2.5054051876068115 14.051230430603027
Loss :  1.4537842273712158 2.4484188556671143 13.695878028869629
Loss :  1.5701991319656372 2.6749143600463867 14.944770812988281
Loss :  1.4643369913101196 3.000178575515747 16.465229034423828
Loss :  1.4352697134017944 2.6521480083465576 14.696009635925293
Loss :  1.4656211137771606 2.5539791584014893 14.235516548156738
Loss :  1.588801622390747 2.7011260986328125 15.09443187713623
  batch 60 loss: 1.588801622390747, 2.7011260986328125, 15.09443187713623
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.4315054416656494 3.8400895595550537 20.6319522857666
Loss :  1.4933382272720337 2.7047784328460693 15.017230987548828
Loss :  1.4491569995880127 2.529534101486206 14.096827507019043
Loss :  1.4292879104614258 3.0623810291290283 16.741191864013672
Loss :  1.3874907493591309 2.830329656600952 15.539138793945312
Loss :  1.4594805240631104 4.192525863647461 22.422109603881836
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.4857099056243896 4.192793846130371 22.44968032836914
Loss :  1.4837862253189087 3.984924793243408 21.408409118652344
Loss :  1.4568113088607788 3.8320815563201904 20.617219924926758
Total LOSS train 15.96374568939209 valid 21.72435474395752
CE LOSS train 1.484167348421537 valid 0.3642028272151947
Contrastive LOSS train 2.8959156586573673 valid 0.9580203890800476
EPOCH 88:
Loss :  1.517978310585022 3.2352230548858643 17.694093704223633
Loss :  1.5440125465393066 4.263053894042969 22.859281539916992
Loss :  1.4912358522415161 2.8868322372436523 15.925396919250488
Loss :  1.5095077753067017 3.4288554191589355 18.653785705566406
Loss :  1.540540099143982 2.8962888717651367 16.021984100341797
Loss :  1.4643042087554932 3.5403287410736084 19.16594696044922
Loss :  1.5318270921707153 2.9471089839935303 16.267372131347656
Loss :  1.4832260608673096 3.7751612663269043 20.359033584594727
Loss :  1.466117262840271 2.820003032684326 15.566132545471191
Loss :  1.5282692909240723 2.9616239070892334 16.336389541625977
Loss :  1.4428166151046753 3.697584629058838 19.93073844909668
Loss :  1.4488826990127563 3.181643486022949 17.357099533081055
Loss :  1.4407083988189697 2.6721465587615967 14.801441192626953
Loss :  1.4538648128509521 2.5391366481781006 14.149548530578613
Loss :  1.566957950592041 3.236450433731079 17.749210357666016
Loss :  1.5554860830307007 2.577526807785034 14.443120002746582
Loss :  1.4422215223312378 2.955604314804077 16.220243453979492
Loss :  1.4946011304855347 2.7221784591674805 15.105493545532227
Loss :  1.4344831705093384 2.5525739192962646 14.19735336303711
Loss :  1.5629476308822632 2.8010406494140625 15.568150520324707
  batch 20 loss: 1.5629476308822632, 2.8010406494140625, 15.568150520324707
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.4868736267089844 2.795475721359253 15.464252471923828
Loss :  1.4331892728805542 3.118037700653076 17.023378372192383
Loss :  1.4668618440628052 3.3107309341430664 18.02051544189453
Loss :  1.4933360815048218 2.8327534198760986 15.657102584838867
Loss :  1.5485831499099731 2.628743886947632 14.692302703857422
Loss :  1.4641624689102173 3.1399834156036377 17.164079666137695
Loss :  1.4861456155776978 3.1079509258270264 17.02589988708496
Loss :  1.4745984077453613 2.7585346698760986 15.267271041870117
Loss :  1.3755226135253906 3.2074127197265625 17.412586212158203
Loss :  1.5408011674880981 3.47670841217041 18.92434310913086
Loss :  1.3782751560211182 2.702249050140381 14.889520645141602
Loss :  1.5109963417053223 2.927908420562744 16.150537490844727
Loss :  1.4589965343475342 3.02597713470459 16.588882446289062
Loss :  1.4563276767730713 3.357058048248291 18.24161720275879
Loss :  1.3926748037338257 3.222968101501465 17.507516860961914
Loss :  1.4225698709487915 2.879692316055298 15.82103157043457
Loss :  1.4206874370574951 2.6650822162628174 14.746098518371582
Loss :  1.5369656085968018 2.4773566722869873 13.923748970031738
Loss :  1.5528303384780884 3.4662749767303467 18.884204864501953
Loss :  1.5688389539718628 3.043896436691284 16.788320541381836
  batch 40 loss: 1.5688389539718628, 3.043896436691284, 16.788320541381836
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.4852581024169922 3.178626775741577 17.37839126586914
Loss :  1.4603397846221924 2.7643728256225586 15.282203674316406
Loss :  1.4391969442367554 2.8756003379821777 15.817198753356934
Loss :  1.4584349393844604 2.9629499912261963 16.273183822631836
Loss :  1.4187976121902466 2.7927420139312744 15.38250732421875
Loss :  1.4736828804016113 2.8062727451324463 15.505046844482422
Loss :  1.537771463394165 2.6707167625427246 14.891355514526367
Loss :  1.4516510963439941 2.5459275245666504 14.18128776550293
Loss :  1.5680114030838013 2.775864362716675 15.447333335876465
Loss :  1.4592037200927734 2.6813557147979736 14.865982055664062
Loss :  1.5267677307128906 2.6090571880340576 14.572053909301758
Loss :  1.5113965272903442 2.960373878479004 16.31326675415039
Loss :  1.4727808237075806 3.023519992828369 16.59037971496582
Loss :  1.5243371725082397 2.8471500873565674 15.760087013244629
Loss :  1.4531574249267578 2.336076021194458 13.133537292480469
Loss :  1.572266697883606 2.9186196327209473 16.16536521911621
Loss :  1.4645931720733643 3.042372226715088 16.676454544067383
Loss :  1.4371957778930664 3.3521575927734375 18.197982788085938
Loss :  1.4669389724731445 3.1402406692504883 17.168142318725586
Loss :  1.5855402946472168 2.2924184799194336 13.047632217407227
  batch 60 loss: 1.5855402946472168, 2.2924184799194336, 13.047632217407227
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.433186411857605 2.9458627700805664 16.162500381469727
Loss :  1.4898161888122559 2.5087509155273438 14.033571243286133
Loss :  1.4497307538986206 3.097531318664551 16.937387466430664
Loss :  1.4296109676361084 2.97265887260437 16.292905807495117
Loss :  1.3908289670944214 2.27939510345459 12.78780460357666
Loss :  1.466218113899231 4.371581554412842 23.324127197265625
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.488641619682312 4.340926647186279 23.193275451660156
Loss :  1.4914569854736328 4.184382438659668 22.41337013244629
Loss :  1.451130747795105 4.375653266906738 23.329395294189453
Total LOSS train 16.26813212174636 valid 23.06504201889038
CE LOSS train 1.4823034048080443 valid 0.36278268694877625
Contrastive LOSS train 2.9571657584263726 valid 1.0939133167266846
EPOCH 89:
Loss :  1.5154870748519897 2.4301986694335938 13.66648006439209
Loss :  1.5373541116714478 2.836639642715454 15.720552444458008
Loss :  1.4791643619537354 2.8750360012054443 15.854344367980957
Loss :  1.4924896955490112 3.353626012802124 18.260618209838867
Loss :  1.526214838027954 3.3142669200897217 18.097549438476562
Loss :  1.4546031951904297 2.7022624015808105 14.96591567993164
Loss :  1.5249464511871338 2.8236083984375 15.642988204956055
Loss :  1.4758743047714233 2.9346530437469482 16.149139404296875
Loss :  1.4605342149734497 2.772101402282715 15.321041107177734
Loss :  1.524012804031372 2.7734832763671875 15.39142894744873
Loss :  1.4454982280731201 3.1629278659820557 17.2601375579834
Loss :  1.4500720500946045 2.8949594497680664 15.924869537353516
Loss :  1.4485018253326416 2.610474109649658 14.500871658325195
Loss :  1.4626226425170898 3.7423694133758545 20.174468994140625
Loss :  1.5744082927703857 3.282930850982666 17.98906135559082
Loss :  1.5553706884384155 3.1485917568206787 17.298330307006836
Loss :  1.4468357563018799 3.618854522705078 19.541109085083008
Loss :  1.4899765253067017 2.7993829250335693 15.486891746520996
Loss :  1.4323961734771729 2.841771364212036 15.641252517700195
Loss :  1.5519754886627197 2.7438414096832275 15.2711820602417
  batch 20 loss: 1.5519754886627197, 2.7438414096832275, 15.2711820602417
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.4787280559539795 2.6895716190338135 14.926586151123047
Loss :  1.4252705574035645 3.802664279937744 20.43859100341797
Loss :  1.4631855487823486 4.1834025382995605 22.380199432373047
Loss :  1.4879603385925293 3.5761327743530273 19.36862564086914
Loss :  1.5559526681900024 3.5660765171051025 19.386335372924805
Loss :  1.4637912511825562 3.2336626052856445 17.632102966308594
Loss :  1.4868930578231812 3.0455877780914307 16.714832305908203
Loss :  1.4687174558639526 3.14750599861145 17.206247329711914
Loss :  1.373620629310608 4.493252277374268 23.839881896972656
Loss :  1.53184974193573 3.177299737930298 17.41834831237793
Loss :  1.3708020448684692 3.135523557662964 17.048419952392578
Loss :  1.5003387928009033 3.127399444580078 17.13733673095703
Loss :  1.448104977607727 2.9150259494781494 16.023235321044922
Loss :  1.4429477453231812 2.8820817470550537 15.85335636138916
Loss :  1.3770614862442017 3.2305495738983154 17.529809951782227
Loss :  1.4038740396499634 3.1707916259765625 17.257831573486328
Loss :  1.3995736837387085 2.8592348098754883 15.695747375488281
Loss :  1.5228221416473389 3.350550413131714 18.27557373046875
Loss :  1.531356692314148 3.0563066005706787 16.812889099121094
Loss :  1.550142765045166 2.9213497638702393 16.156890869140625
  batch 40 loss: 1.550142765045166, 2.9213497638702393, 16.156890869140625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.4646230936050415 2.984851598739624 16.38888168334961
Loss :  1.4354526996612549 3.226569414138794 17.568300247192383
Loss :  1.412318229675293 3.307640314102173 17.950519561767578
Loss :  1.4391388893127441 3.4870717525482178 18.874496459960938
Loss :  1.3937400579452515 3.2416677474975586 17.602079391479492
Loss :  1.453683614730835 3.5600063800811768 19.25371551513672
Loss :  1.5224545001983643 3.283679723739624 17.940853118896484
Loss :  1.4218116998672485 2.8431332111358643 15.63747787475586
Loss :  1.555743932723999 2.8415403366088867 15.763445854187012
Loss :  1.4309043884277344 3.0115792751312256 16.488800048828125
Loss :  1.5004656314849854 2.9252066612243652 16.12649917602539
Loss :  1.4879965782165527 3.0887105464935303 16.931549072265625
Loss :  1.4434064626693726 2.5385491847991943 14.136152267456055
Loss :  1.5018965005874634 3.0048153400421143 16.525972366333008
Loss :  1.4180349111557007 2.778903007507324 15.312549591064453
Loss :  1.54819655418396 3.0045692920684814 16.571043014526367
Loss :  1.4297122955322266 3.4589333534240723 18.72437858581543
Loss :  1.3991823196411133 2.97261905670166 16.262277603149414
Loss :  1.4294869899749756 3.073702573776245 16.79800033569336
Loss :  1.5675283670425415 2.8451666831970215 15.793362617492676
  batch 60 loss: 1.5675283670425415, 2.8451666831970215, 15.793362617492676
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.3930610418319702 2.7131454944610596 14.95878791809082
Loss :  1.4588708877563477 2.7192459106445312 15.055100440979004
Loss :  1.4143145084381104 3.1910855770111084 17.36974334716797
Loss :  1.388290524482727 3.175328493118286 17.26493263244629
Loss :  1.3434370756149292 2.8282933235168457 15.484903335571289
Loss :  1.4243687391281128 4.376537799835205 23.307056427001953
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  1.4666169881820679 4.4449639320373535 23.691436767578125
Loss :  1.452664852142334 4.3912672996521 23.40900230407715
Loss :  1.4319841861724854 4.170743465423584 22.285701751708984
Total LOSS train 16.954536863473745 valid 23.173299312591553
CE LOSS train 1.4674628331111028 valid 0.35799604654312134
Contrastive LOSS train 3.0974148200108456 valid 1.042685866355896
EPOCH 90:
Loss :  1.4910461902618408 3.0403873920440674 16.692983627319336
Loss :  1.5144388675689697 2.878754138946533 15.908208847045898
Loss :  1.4469387531280518 2.6149117946624756 14.52149772644043
Loss :  1.4623111486434937 3.6872036457061768 19.89832878112793
Loss :  1.503209114074707 3.1542134284973145 17.274276733398438
Loss :  1.4216939210891724 2.76444149017334 15.243901252746582
Loss :  1.503539800643921 3.268913507461548 17.848108291625977
Loss :  1.4456567764282227 3.2473440170288086 17.682376861572266
Loss :  1.4284785985946655 2.891728162765503 15.88711929321289
Loss :  1.4950600862503052 2.7094225883483887 15.042173385620117
Loss :  1.405070185661316 2.8866782188415527 15.838461875915527
Loss :  1.4063324928283691 3.8932807445526123 20.87273597717285
Loss :  1.3995767831802368 3.3868939876556396 18.33404541015625
Loss :  1.4139628410339355 2.7407429218292236 15.117677688598633
Loss :  1.5368520021438599 2.678616523742676 14.92993450164795
Loss :  1.5235216617584229 2.772221088409424 15.384626388549805
Loss :  1.3978246450424194 2.9816057682037354 16.30585289001465
Loss :  1.4569299221038818 2.653442621231079 14.724143028259277
Loss :  1.3902162313461304 2.5824055671691895 14.30224323272705
Loss :  1.5243961811065674 2.991901397705078 16.483903884887695
  batch 20 loss: 1.5243961811065674, 2.991901397705078, 16.483903884887695
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.44670569896698 3.2148003578186035 17.520706176757812
Loss :  1.388872504234314 2.631880760192871 14.5482759475708
Loss :  1.4297595024108887 2.6856298446655273 14.857908248901367
Loss :  1.465898871421814 2.5779309272766113 14.355552673339844
Loss :  1.528910517692566 3.250931739807129 17.783567428588867
Loss :  1.43451726436615 2.9301857948303223 16.085445404052734
Loss :  1.4580713510513306 2.9599859714508057 16.25800132751465
Loss :  1.4481191635131836 2.895669460296631 15.926466941833496
Loss :  1.3394359350204468 2.8730430603027344 15.70465087890625
Loss :  1.5215203762054443 3.2731125354766846 17.887083053588867
Loss :  1.3470853567123413 3.0306246280670166 16.500207901000977
Loss :  1.4943583011627197 3.324645519256592 18.117586135864258
Loss :  1.4380193948745728 2.601465940475464 14.44534969329834
Loss :  1.4346792697906494 2.624825954437256 14.558809280395508
Loss :  1.3666843175888062 2.7947354316711426 15.340360641479492
Loss :  1.3995106220245361 3.5687716007232666 19.243370056152344
Loss :  1.3941627740859985 3.3596768379211426 18.192546844482422
Loss :  1.5270293951034546 2.985537052154541 16.454713821411133
Loss :  1.5375316143035889 2.8655009269714355 15.865036964416504
Loss :  1.557552456855774 3.1174986362457275 17.14504623413086
  batch 40 loss: 1.557552456855774, 3.1174986362457275, 17.14504623413086
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.4679220914840698 3.4081521034240723 18.508682250976562
Loss :  1.4403983354568481 2.5194249153137207 14.037522315979004
Loss :  1.4165167808532715 3.3760628700256348 18.296831130981445
Loss :  1.43649423122406 3.151249885559082 17.1927433013916
Loss :  1.3973737955093384 3.077335834503174 16.7840518951416
Loss :  1.458388328552246 3.308863401412964 18.002704620361328
Loss :  1.5257675647735596 3.3194353580474854 18.12294578552246
Loss :  1.4291629791259766 3.990173101425171 21.380027770996094
Loss :  1.5597728490829468 2.718885898590088 15.15420150756836
Loss :  1.4369468688964844 2.8704211711883545 15.789052963256836
Loss :  1.5122989416122437 3.126429319381714 17.144445419311523
Loss :  1.4968175888061523 2.8680317401885986 15.836976051330566
Loss :  1.4569517374038696 2.9071035385131836 15.992469787597656
Loss :  1.514465093612671 3.6226561069488525 19.62774658203125
Loss :  1.434097170829773 3.0238394737243652 16.553295135498047
Loss :  1.5609886646270752 2.9676928520202637 16.39945411682129
Loss :  1.4461978673934937 4.01210880279541 21.50674057006836
Loss :  1.414499044418335 2.834538459777832 15.587191581726074
Loss :  1.446615219116211 3.1947505474090576 17.420368194580078
Loss :  1.5798600912094116 3.2253215312957764 17.70646858215332
  batch 60 loss: 1.5798600912094116, 3.2253215312957764, 17.70646858215332
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.4133764505386353 3.2235207557678223 17.53097915649414
Loss :  1.476466178894043 2.7454824447631836 15.203878402709961
Loss :  1.4316740036010742 2.7901999950408936 15.382674217224121
Loss :  1.4032468795776367 3.1488306522369385 17.14739990234375
Loss :  1.3616818189620972 2.5941848754882812 14.332606315612793
Loss :  1.4183956384658813 4.17206335067749 22.27871322631836
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.4433417320251465 4.143505096435547 22.16086769104004
Loss :  1.4349496364593506 4.119200706481934 22.030954360961914
Loss :  1.438080906867981 4.179920196533203 22.337682723999023
Total LOSS train 16.64191912137545 valid 22.202054500579834
CE LOSS train 1.4560532533205472 valid 0.35952022671699524
Contrastive LOSS train 3.0371731941516584 valid 1.0449800491333008
EPOCH 91:
Loss :  1.5052872896194458 2.7808587551116943 15.409581184387207
Loss :  1.525747537612915 3.168579578399658 17.36864471435547
Loss :  1.4635361433029175 2.6010794639587402 14.468934059143066
Loss :  1.4779777526855469 3.2614376544952393 17.785165786743164
Loss :  1.5163846015930176 3.8020007610321045 20.52638816833496
Loss :  1.440804123878479 3.1444928646087646 17.16326904296875
Loss :  1.5184637308120728 3.432014226913452 18.67853355407715
Loss :  1.4639549255371094 2.9766764640808105 16.34733772277832
Loss :  1.4457720518112183 2.9790198802948 16.340871810913086
Loss :  1.5136699676513672 2.675050973892212 14.888924598693848
Loss :  1.4330064058303833 2.8414907455444336 15.640460014343262
Loss :  1.4322766065597534 2.9599924087524414 16.23223876953125
Loss :  1.4275717735290527 2.8464813232421875 15.659978866577148
Loss :  1.4375243186950684 3.1627395153045654 17.251222610473633
Loss :  1.5554369688034058 3.111816883087158 17.114521026611328
Loss :  1.5456255674362183 3.0474672317504883 16.782960891723633
Loss :  1.416120171546936 3.059047222137451 16.71135711669922
Loss :  1.473228931427002 2.98988676071167 16.42266273498535
Loss :  1.406300663948059 2.9158623218536377 15.985611915588379
Loss :  1.537311315536499 3.0436596870422363 16.7556095123291
  batch 20 loss: 1.537311315536499, 3.0436596870422363, 16.7556095123291
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.4602357149124146 3.183656930923462 17.378520965576172
Loss :  1.4032095670700073 2.6472558975219727 14.63948917388916
Loss :  1.4446022510528564 2.5078542232513428 13.98387336730957
Loss :  1.4804232120513916 3.2386419773101807 17.673633575439453
Loss :  1.5423622131347656 2.815251588821411 15.618619918823242
Loss :  1.4492740631103516 2.831416130065918 15.606354713439941
Loss :  1.4727979898452759 3.2153801918029785 17.549697875976562
Loss :  1.455289602279663 2.824986457824707 15.580222129821777
Loss :  1.3526089191436768 3.2794852256774902 17.750036239624023
Loss :  1.5280683040618896 2.792741537094116 15.491776466369629
Loss :  1.3581230640411377 2.666083335876465 14.688539505004883
Loss :  1.4989423751831055 2.8101439476013184 15.549662590026855
Loss :  1.4454240798950195 2.9530177116394043 16.210514068603516
Loss :  1.4431204795837402 3.3626034259796143 18.25613784790039
Loss :  1.375892996788025 2.9282636642456055 16.0172119140625
Loss :  1.4077367782592773 2.761042356491089 15.2129487991333
Loss :  1.405350685119629 2.552476167678833 14.167731285095215
Loss :  1.5304479598999023 2.457125186920166 13.816073417663574
Loss :  1.542620301246643 2.6845462322235107 14.965352058410645
Loss :  1.5653194189071655 2.8471932411193848 15.801284790039062
  batch 40 loss: 1.5653194189071655, 2.8471932411193848, 15.801284790039062
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.484930157661438 2.945960283279419 16.214731216430664
Loss :  1.461952567100525 2.9036178588867188 15.98004150390625
Loss :  1.4380826950073242 2.949680805206299 16.186485290527344
Loss :  1.4627629518508911 2.932067394256592 16.12310028076172
Loss :  1.419575572013855 2.4718544483184814 13.778847694396973
Loss :  1.4762259721755981 2.8306374549865723 15.629412651062012
Loss :  1.5416901111602783 2.687072277069092 14.977051734924316
Loss :  1.447111964225769 2.652722120285034 14.710722923278809
Loss :  1.5722979307174683 2.885157585144043 15.998085975646973
Loss :  1.4592020511627197 3.0431442260742188 16.674922943115234
Loss :  1.5268090963363647 3.0035221576690674 16.54442024230957
Loss :  1.513322353363037 2.927809238433838 16.152368545532227
Loss :  1.4753375053405762 2.947293519973755 16.21180534362793
Loss :  1.5241585969924927 2.5277669429779053 14.162993431091309
Loss :  1.4550890922546387 2.4006264209747314 13.458221435546875
Loss :  1.569966197013855 2.9434802532196045 16.28736686706543
Loss :  1.4653360843658447 2.7566134929656982 15.248403549194336
Loss :  1.4418092966079712 3.032355785369873 16.603588104248047
Loss :  1.4713361263275146 3.0410826206207275 16.676748275756836
Loss :  1.5874634981155396 3.2189292907714844 17.682109832763672
  batch 60 loss: 1.5874634981155396, 3.2189292907714844, 17.682109832763672
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.439440131187439 3.0134968757629395 16.50692367553711
Loss :  1.5001187324523926 2.8653109073638916 15.82667350769043
Loss :  1.4552146196365356 2.5240392684936523 14.075410842895508
Loss :  1.4331402778625488 3.422665596008301 18.546466827392578
Loss :  1.394659161567688 2.3930680751800537 13.359999656677246
Loss :  1.4730902910232544 4.362547397613525 23.28582763671875
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.4940769672393799 4.378477096557617 23.386463165283203
Loss :  1.4925674200057983 4.355709552764893 23.271114349365234
Loss :  1.4790153503417969 4.219789028167725 22.577960968017578
Total LOSS train 16.047828571613017 valid 23.13034152984619
CE LOSS train 1.4725366702446572 valid 0.3697538375854492
Contrastive LOSS train 2.9150583854088414 valid 1.0549472570419312
EPOCH 92:
Loss :  1.5240341424942017 2.549715757369995 14.272613525390625
Loss :  1.5458625555038452 2.8576102256774902 15.833913803100586
Loss :  1.4852581024169922 2.43178129196167 13.6441650390625
Loss :  1.497668743133545 2.7214303016662598 15.104820251464844
Loss :  1.531205415725708 2.7846319675445557 15.454364776611328
Loss :  1.459777593612671 2.5474791526794434 14.197174072265625
Loss :  1.52888822555542 2.8392789363861084 15.725282669067383
Loss :  1.4804966449737549 2.403892755508423 13.499959945678711
Loss :  1.4641083478927612 3.2254443168640137 17.59132957458496
Loss :  1.5259047746658325 2.9017529487609863 16.034669876098633
Loss :  1.4437205791473389 3.547971248626709 19.183576583862305
Loss :  1.4433670043945312 3.0851526260375977 16.869129180908203
Loss :  1.4383798837661743 2.9748849868774414 16.31280517578125
Loss :  1.4517061710357666 2.505258321762085 13.977997779846191
Loss :  1.5681686401367188 2.691235303878784 15.024345397949219
Loss :  1.550358533859253 2.881791353225708 15.959315299987793
Loss :  1.4359885454177856 2.8474481105804443 15.673229217529297
Loss :  1.4830564260482788 2.7786736488342285 15.376423835754395
Loss :  1.4247523546218872 2.3277015686035156 13.063260078430176
Loss :  1.5491479635238647 2.51495099067688 14.123902320861816
  batch 20 loss: 1.5491479635238647, 2.51495099067688, 14.123902320861816
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.4750739336013794 2.20771861076355 12.513667106628418
Loss :  1.4190353155136108 3.5013303756713867 18.925687789916992
Loss :  1.4573402404785156 2.4231066703796387 13.572874069213867
Loss :  1.486488938331604 2.950207233428955 16.237524032592773
Loss :  1.5453094244003296 3.3303592205047607 18.197105407714844
Loss :  1.4596772193908691 2.8382506370544434 15.650930404663086
Loss :  1.4827073812484741 2.5827691555023193 14.396553039550781
Loss :  1.4761576652526855 2.865253448486328 15.802425384521484
Loss :  1.375861644744873 3.049595355987549 16.623838424682617
Loss :  1.543848991394043 2.8789992332458496 15.93884563446045
Loss :  1.3774713277816772 3.047436475753784 16.614654541015625
Loss :  1.5149983167648315 3.071528911590576 16.872642517089844
Loss :  1.463196039199829 2.6190497875213623 14.55844497680664
Loss :  1.46023690700531 3.851637601852417 20.718425750732422
Loss :  1.392055630683899 3.106411933898926 16.924116134643555
Loss :  1.4197862148284912 2.9624783992767334 16.232177734375
Loss :  1.4147472381591797 2.7864277362823486 15.346885681152344
Loss :  1.5342237949371338 2.721601963043213 15.142232894897461
Loss :  1.5431467294692993 2.7974894046783447 15.530593872070312
Loss :  1.5604990720748901 3.8771398067474365 20.946197509765625
  batch 40 loss: 1.5604990720748901, 3.8771398067474365, 20.946197509765625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.4786916971206665 3.0307459831237793 16.632421493530273
Loss :  1.4458540678024292 4.387794017791748 23.384824752807617
Loss :  1.4262924194335938 4.1809611320495605 22.331098556518555
Loss :  1.4500395059585571 3.879603862762451 20.848058700561523
Loss :  1.4085099697113037 3.7411692142486572 20.114356994628906
Loss :  1.4664828777313232 3.20084285736084 17.4706974029541
Loss :  1.5314784049987793 4.119195461273193 22.127456665039062
Loss :  1.43118417263031 2.755685567855835 15.209611892700195
Loss :  1.562088966369629 4.167312145233154 22.398651123046875
Loss :  1.431319236755371 3.091472625732422 16.888683319091797
Loss :  1.4997678995132446 3.112697124481201 17.06325340270996
Loss :  1.486451506614685 3.211261749267578 17.542760848999023
Loss :  1.4376953840255737 3.0166571140289307 16.520980834960938
Loss :  1.496212363243103 3.071061611175537 16.851520538330078
Loss :  1.406747579574585 2.86910080909729 15.752251625061035
Loss :  1.5452704429626465 3.355250597000122 18.321523666381836
Loss :  1.4138691425323486 3.841884136199951 20.623291015625
Loss :  1.3811286687850952 3.5141589641571045 18.951923370361328
Loss :  1.4137483835220337 3.264727830886841 17.73738670349121
Loss :  1.5568866729736328 2.475269317626953 13.933233261108398
  batch 60 loss: 1.5568866729736328, 2.475269317626953, 13.933233261108398
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.376460075378418 2.7491517066955566 15.122218132019043
Loss :  1.4498544931411743 2.984937906265259 16.374544143676758
Loss :  1.3977901935577393 2.9455647468566895 16.125614166259766
Loss :  1.3734543323516846 3.0809786319732666 16.77834701538086
Loss :  1.3267426490783691 2.590602397918701 14.279754638671875
Loss :  1.4026529788970947 4.472016334533691 23.762733459472656
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.4380924701690674 4.426846981048584 23.572328567504883
Loss :  1.4419159889221191 4.306820392608643 22.976016998291016
Loss :  1.397510290145874 4.253277778625488 22.663898468017578
Total LOSS train 16.66234716268686 valid 23.243744373321533
CE LOSS train 1.46811898121467 valid 0.3493775725364685
Contrastive LOSS train 3.0388456197885367 valid 1.063319444656372
EPOCH 93:
Loss :  1.4801464080810547 2.73052716255188 15.132781982421875
Loss :  1.5042505264282227 3.3371708393096924 18.190105438232422
Loss :  1.4355716705322266 3.040332317352295 16.63723373413086
Loss :  1.455214023590088 3.0015618801116943 16.463024139404297
Loss :  1.4933725595474243 2.567256212234497 14.3296537399292
Loss :  1.4085197448730469 2.677222728729248 14.794633865356445
Loss :  1.497379183769226 3.1683056354522705 17.33890724182129
Loss :  1.435585856437683 3.014875650405884 16.509963989257812
Loss :  1.4176528453826904 3.0260541439056396 16.547924041748047
Loss :  1.4936665296554565 2.739469051361084 15.191012382507324
Loss :  1.3964498043060303 2.9306821823120117 16.04986000061035
Loss :  1.3958687782287598 2.8332841396331787 15.56229019165039
Loss :  1.393794059753418 3.2950727939605713 17.869159698486328
Loss :  1.4080135822296143 2.8864493370056152 15.84026050567627
Loss :  1.5409936904907227 2.6504244804382324 14.793116569519043
Loss :  1.524216651916504 2.91623854637146 16.105409622192383
Loss :  1.3980234861373901 2.906092643737793 15.928486824035645
Loss :  1.4560762643814087 3.1818504333496094 17.365327835083008
Loss :  1.3901981115341187 3.1762335300445557 17.271366119384766
Loss :  1.5337631702423096 2.8858931064605713 15.963228225708008
  batch 20 loss: 1.5337631702423096, 2.8858931064605713, 15.963228225708008
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.4516170024871826 2.753156900405884 15.217401504516602
Loss :  1.3894498348236084 2.9203684329986572 15.991291999816895
Loss :  1.4303478002548218 3.039843797683716 16.629566192626953
Loss :  1.4633034467697144 2.3447787761688232 13.1871976852417
Loss :  1.52712881565094 2.606475353240967 14.5595064163208
Loss :  1.4333813190460205 2.923640727996826 16.051586151123047
Loss :  1.4558898210525513 2.961972951889038 16.26575469970703
Loss :  1.4453237056732178 2.7316038608551025 15.10334300994873
Loss :  1.3373392820358276 2.590092182159424 14.287799835205078
Loss :  1.521659016609192 3.0162880420684814 16.603099822998047
Loss :  1.3447641134262085 3.059398889541626 16.64175796508789
Loss :  1.489675521850586 2.8947291374206543 15.963321685791016
Loss :  1.4377578496932983 2.801234006881714 15.443927764892578
Loss :  1.435014009475708 2.8656837940216064 15.763432502746582
Loss :  1.3677479028701782 2.852146625518799 15.628480911254883
Loss :  1.4016602039337158 2.870189666748047 15.752608299255371
Loss :  1.3990319967269897 3.1572115421295166 17.185089111328125
Loss :  1.5236129760742188 2.4798219203948975 13.922722816467285
Loss :  1.535874605178833 3.3215019702911377 18.14338493347168
Loss :  1.558927059173584 2.8239219188690186 15.678537368774414
  batch 40 loss: 1.558927059173584, 2.8239219188690186, 15.678537368774414
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.4789385795593262 2.956707000732422 16.262474060058594
Loss :  1.4554603099822998 3.9961435794830322 21.43617820739746
Loss :  1.4348336458206177 2.9405391216278076 16.137529373168945
Loss :  1.4576668739318848 3.5908620357513428 19.411975860595703
Loss :  1.4167003631591797 2.6051506996154785 14.442453384399414
Loss :  1.474337100982666 3.1723413467407227 17.336044311523438
Loss :  1.5411949157714844 3.292066812515259 18.001529693603516
Loss :  1.444808006286621 2.6790034770965576 14.839825630187988
Loss :  1.5703740119934082 3.1643013954162598 17.39188003540039
Loss :  1.4534027576446533 3.8300492763519287 20.603649139404297
Loss :  1.524004340171814 3.274757146835327 17.897789001464844
Loss :  1.508743166923523 2.8362276554107666 15.689881324768066
Loss :  1.4651075601577759 2.563072443008423 14.28046989440918
Loss :  1.5193480253219604 2.7279772758483887 15.159235000610352
Loss :  1.4441312551498413 2.7446675300598145 15.167468070983887
Loss :  1.5679750442504883 3.3207833766937256 18.171890258789062
Loss :  1.4589788913726807 2.9609007835388184 16.26348304748535
Loss :  1.429125428199768 2.566593647003174 14.262093544006348
Loss :  1.4613665342330933 2.7970802783966064 15.446767807006836
Loss :  1.5874934196472168 3.0935845375061035 17.055416107177734
  batch 60 loss: 1.5874934196472168, 3.0935845375061035, 17.055416107177734
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.4303854703903198 3.073551654815674 16.79814338684082
Loss :  1.4896312952041626 2.771782398223877 15.348543167114258
Loss :  1.446367859840393 2.7403652667999268 15.148194313049316
Loss :  1.424715280532837 3.037350654602051 16.611469268798828
Loss :  1.3834686279296875 2.2182023525238037 12.474480628967285
Loss :  1.441938042640686 4.0672383308410645 21.77812957763672
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.4651137590408325 4.056441307067871 21.74732208251953
Loss :  1.4650359153747559 3.950885772705078 21.219465255737305
Loss :  1.4555479288101196 4.0483927726745605 21.697511672973633
Total LOSS train 16.146806482168344 valid 21.610607147216797
CE LOSS train 1.4596434153043307 valid 0.3638869822025299
Contrastive LOSS train 2.937432600901677 valid 1.0120981931686401
EPOCH 94:
Loss :  1.5141382217407227 2.4412853717803955 13.720564842224121
Loss :  1.5362169742584229 3.3500285148620605 18.286359786987305
Loss :  1.4765548706054688 2.7976930141448975 15.465020179748535
Loss :  1.4930298328399658 2.4175565242767334 13.580812454223633
Loss :  1.525329828262329 2.249310255050659 12.771881103515625
Loss :  1.452895164489746 2.9333746433258057 16.119768142700195
Loss :  1.5274118185043335 2.964529514312744 16.350059509277344
Loss :  1.4751932621002197 2.551699161529541 14.233688354492188
Loss :  1.458202838897705 2.5540425777435303 14.228416442871094
Loss :  1.5254327058792114 2.788729190826416 15.469078063964844
Loss :  1.4410524368286133 3.2276883125305176 17.57949447631836
Loss :  1.4404493570327759 2.836095094680786 15.620924949645996
Loss :  1.4400196075439453 2.4552128314971924 13.716083526611328
Loss :  1.4523520469665527 2.5479583740234375 14.192144393920898
Loss :  1.569218635559082 3.080153226852417 16.969985961914062
Loss :  1.552411675453186 2.823209524154663 15.66845989227295
Loss :  1.4411824941635132 2.9305741786956787 16.094053268432617
Loss :  1.4923399686813354 2.9049336910247803 16.01700782775879
Loss :  1.4354175329208374 3.1171977519989014 17.021406173706055
Loss :  1.5580075979232788 3.0248003005981445 16.682008743286133
  batch 20 loss: 1.5580075979232788, 3.0248003005981445, 16.682008743286133
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.483152985572815 2.84476375579834 15.706972122192383
Loss :  1.42976975440979 2.8785150051116943 15.822344779968262
Loss :  1.4660923480987549 2.6647684574127197 14.789934158325195
Loss :  1.4948824644088745 3.3304836750030518 18.147300720214844
Loss :  1.5497803688049316 2.965876817703247 16.37916374206543
Loss :  1.470665454864502 2.8732059001922607 15.836694717407227
Loss :  1.4927380084991455 3.2196974754333496 17.59122657775879
Loss :  1.483429193496704 2.6802945137023926 14.88490104675293
Loss :  1.388264775276184 2.7106776237487793 14.94165325164795
Loss :  1.547667145729065 3.2077510356903076 17.586421966552734
Loss :  1.3930250406265259 2.914898157119751 15.96751594543457
Loss :  1.5201996564865112 3.6204798221588135 19.62259864807129
Loss :  1.4728025197982788 3.073707342147827 16.841339111328125
Loss :  1.4689998626708984 2.540621042251587 14.172104835510254
Loss :  1.4097036123275757 2.581310510635376 14.316255569458008
Loss :  1.4381767511367798 2.735947847366333 15.117916107177734
Loss :  1.4367731809616089 3.8475522994995117 20.674535751342773
Loss :  1.5448188781738281 2.8869714736938477 15.979676246643066
Loss :  1.5549575090408325 2.564563751220703 14.377776145935059
Loss :  1.573380470275879 2.5447793006896973 14.297276496887207
  batch 40 loss: 1.573380470275879, 2.5447793006896973, 14.297276496887207
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.5021649599075317 3.4575507640838623 18.789918899536133
Loss :  1.4788682460784912 2.245464563369751 12.706191062927246
Loss :  1.4629480838775635 3.5083932876586914 19.004913330078125
Loss :  1.483083724975586 2.578914165496826 14.377655029296875
Loss :  1.4471526145935059 2.771374225616455 15.304023742675781
Loss :  1.50162935256958 3.325040578842163 18.1268310546875
Loss :  1.561661720275879 2.3755271434783936 13.439297676086426
Loss :  1.478628158569336 2.5695505142211914 14.326380729675293
Loss :  1.5920438766479492 2.4094231128692627 13.639159202575684
Loss :  1.4847744703292847 2.4014291763305664 13.491920471191406
Loss :  1.5492686033248901 3.675140380859375 19.924970626831055
Loss :  1.5339876413345337 2.9453773498535156 16.260873794555664
Loss :  1.497808575630188 2.5062639713287354 14.029128074645996
Loss :  1.544972538948059 2.520542860031128 14.147686958312988
Loss :  1.4780220985412598 2.5686187744140625 14.321115493774414
Loss :  1.5863407850265503 2.8466153144836426 15.819416999816895
Loss :  1.488527536392212 2.9942526817321777 16.45979118347168
Loss :  1.4662349224090576 3.216866970062256 17.550569534301758
Loss :  1.492199182510376 3.175400733947754 17.369203567504883
Loss :  1.600017786026001 2.8786561489105225 15.993298530578613
  batch 60 loss: 1.600017786026001, 2.8786561489105225, 15.993298530578613
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.4604527950286865 2.884765625 15.884281158447266
Loss :  1.5166559219360352 2.68182373046875 14.925774574279785
Loss :  1.4763338565826416 2.633275270462036 14.642709732055664
Loss :  1.4583536386489868 2.84371018409729 15.676904678344727
Loss :  1.423108458518982 2.5197718143463135 14.021966934204102
Loss :  1.4784740209579468 4.450568675994873 23.7313175201416
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5014188289642334 4.439194202423096 23.697389602661133
Loss :  1.5043023824691772 4.313253879547119 23.070571899414062
Loss :  1.4630038738250732 4.244452953338623 22.68526840209961
Total LOSS train 15.73961244729849 valid 23.2961368560791
CE LOSS train 1.4921750215383676 valid 0.3657509684562683
Contrastive LOSS train 2.8494874954223635 valid 1.0611132383346558
EPOCH 95:
Loss :  1.539141058921814 2.71427845954895 15.110532760620117
Loss :  1.5592461824417114 3.1767404079437256 17.442949295043945
Loss :  1.5095415115356445 3.029958963394165 16.65933609008789
Loss :  1.5225963592529297 2.7652394771575928 15.348793983459473
Loss :  1.5515118837356567 3.482527017593384 18.964147567749023
Loss :  1.486695647239685 2.584770441055298 14.410547256469727
Loss :  1.552612066268921 2.730829954147339 15.206762313842773
Loss :  1.5108532905578613 3.0736119747161865 16.87891387939453
Loss :  1.4978907108306885 3.439724922180176 18.696514129638672
Loss :  1.558102011680603 3.1143858432769775 17.13003158569336
Loss :  1.4875823259353638 3.6577842235565186 19.776504516601562
Loss :  1.4874187707901 2.8346850872039795 15.660844802856445
Loss :  1.4839987754821777 3.689051389694214 19.929256439208984
Loss :  1.4917563199996948 2.777452230453491 15.37901782989502
Loss :  1.5907400846481323 3.175112009048462 17.466299057006836
Loss :  1.5767841339111328 3.1417760848999023 17.285663604736328
Loss :  1.481952428817749 3.036440849304199 16.664155960083008
Loss :  1.5206547975540161 3.055264472961426 16.796977996826172
Loss :  1.473369836807251 2.6363420486450195 14.65507984161377
Loss :  1.5796663761138916 3.739104986190796 20.275190353393555
  batch 20 loss: 1.5796663761138916, 3.739104986190796, 20.275190353393555
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5157159566879272 3.4869046211242676 18.950239181518555
Loss :  1.4632030725479126 3.0548088550567627 16.737247467041016
Loss :  1.4954028129577637 3.210890769958496 17.54985809326172
Loss :  1.5197103023529053 2.955192804336548 16.295673370361328
Loss :  1.567569613456726 3.3439106941223145 18.28712272644043
Loss :  1.4919226169586182 3.1811470985412598 17.39765739440918
Loss :  1.5071412324905396 2.9132683277130127 16.073482513427734
Loss :  1.4961521625518799 3.6675314903259277 19.833810806274414
Loss :  1.405680537223816 3.1119039058685303 16.965200424194336
Loss :  1.5535539388656616 2.857919454574585 15.843151092529297
Loss :  1.4025909900665283 3.3797478675842285 18.30133056640625
Loss :  1.5242890119552612 3.5215821266174316 19.132198333740234
Loss :  1.4741326570510864 3.535041093826294 19.149337768554688
Loss :  1.4715813398361206 3.0097601413726807 16.520381927490234
Loss :  1.4118578433990479 3.058555841445923 16.70463752746582
Loss :  1.437564492225647 3.3868179321289062 18.371654510498047
Loss :  1.4368278980255127 2.854743242263794 15.710543632507324
Loss :  1.5422674417495728 3.1255478858947754 17.170005798339844
Loss :  1.5538337230682373 2.913907289505005 16.123369216918945
Loss :  1.5717060565948486 2.851118326187134 15.82729721069336
  batch 40 loss: 1.5717060565948486, 2.851118326187134, 15.82729721069336
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.4955947399139404 2.931586265563965 16.153526306152344
Loss :  1.469773530960083 2.7993569374084473 15.466557502746582
Loss :  1.4540183544158936 3.213287115097046 17.52045249938965
Loss :  1.4724541902542114 2.7438535690307617 15.19172191619873
Loss :  1.4332917928695679 2.7518680095672607 15.192631721496582
Loss :  1.4879264831542969 2.621507406234741 14.595463752746582
Loss :  1.5520344972610474 2.73669171333313 15.235492706298828
Loss :  1.4630111455917358 2.8751139640808105 15.838581085205078
Loss :  1.5851531028747559 3.587275505065918 19.52153205871582
Loss :  1.4712638854980469 3.7108001708984375 20.025264739990234
Loss :  1.5404912233352661 3.056501626968384 16.822999954223633
Loss :  1.5234302282333374 3.4756593704223633 18.90172576904297
Loss :  1.4829190969467163 2.667022943496704 14.818033218383789
Loss :  1.532701849937439 3.0591816902160645 16.828609466552734
Loss :  1.463775873184204 2.926295042037964 16.095251083374023
Loss :  1.5821257829666138 3.0097694396972656 16.63097381591797
Loss :  1.478023886680603 2.8708336353302 15.832191467285156
Loss :  1.4519171714782715 2.9093446731567383 15.998640060424805
Loss :  1.4790371656417847 2.9005534648895264 15.981804847717285
Loss :  1.595950722694397 2.886664867401123 16.02927589416504
  batch 60 loss: 1.595950722694397, 2.886664867401123, 16.02927589416504
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.4465783834457397 2.833374261856079 15.613449096679688
Loss :  1.5060696601867676 2.441063404083252 13.711385726928711
Loss :  1.461806058883667 2.5146822929382324 14.035218238830566
Loss :  1.4444280862808228 2.925395965576172 16.071407318115234
Loss :  1.4053000211715698 1.9943373203277588 11.376986503601074
Loss :  1.5197008848190308 4.369931697845459 23.369359970092773
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5409584045410156 4.324865818023682 23.165287017822266
Loss :  1.5446330308914185 4.276451110839844 22.926889419555664
Loss :  1.5234653949737549 4.132561206817627 22.18627166748047
Total LOSS train 16.71032147040734 valid 22.911952018737793
CE LOSS train 1.5012906954838678 valid 0.3808663487434387
Contrastive LOSS train 3.041806173324585 valid 1.0331403017044067
EPOCH 96:
Loss :  1.5317654609680176 2.378037929534912 13.421955108642578
Loss :  1.5553479194641113 2.769686698913574 15.40378189086914
Loss :  1.4953762292861938 2.8755202293395996 15.872978210449219
Loss :  1.5070300102233887 2.8778045177459717 15.896053314208984
Loss :  1.5352671146392822 2.9551291465759277 16.3109130859375
Loss :  1.459343433380127 2.7152962684631348 15.035823822021484
Loss :  1.5337904691696167 2.5208702087402344 14.138141632080078
Loss :  1.478829264640808 2.5675318241119385 14.316488265991211
Loss :  1.4638639688491821 2.5936203002929688 14.431965827941895
Loss :  1.53150475025177 2.9447426795959473 16.255218505859375
Loss :  1.4443254470825195 3.0236656665802 16.562652587890625
Loss :  1.4416558742523193 3.0222249031066895 16.552780151367188
Loss :  1.4387136697769165 3.310803174972534 17.99272918701172
Loss :  1.4469990730285645 2.615812063217163 14.526060104370117
Loss :  1.567129373550415 2.953436851501465 16.334314346313477
Loss :  1.5465178489685059 2.814152479171753 15.617280960083008
Loss :  1.4322001934051514 2.6692025661468506 14.778213500976562
Loss :  1.481868028640747 2.768988847732544 15.326811790466309
Loss :  1.4258290529251099 2.5970828533172607 14.411243438720703
Loss :  1.5560076236724854 2.5019514560699463 14.065764427185059
  batch 20 loss: 1.5560076236724854, 2.5019514560699463, 14.065764427185059
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.48337721824646 2.667638063430786 14.82156753540039
Loss :  1.4302375316619873 2.796811580657959 15.41429615020752
Loss :  1.4700803756713867 3.351824998855591 18.229206085205078
Loss :  1.497268795967102 2.7460780143737793 15.227659225463867
Loss :  1.5504933595657349 2.667515277862549 14.888069152832031
Loss :  1.4677966833114624 2.5795488357543945 14.365540504455566
Loss :  1.4885520935058594 2.698007345199585 14.978589057922363
Loss :  1.4769623279571533 2.4067044258117676 13.510483741760254
Loss :  1.3739659786224365 2.5838403701782227 14.293168067932129
Loss :  1.5402027368545532 2.59393572807312 14.509881973266602
Loss :  1.3765099048614502 2.6151249408721924 14.452134132385254
Loss :  1.5103089809417725 2.7301621437072754 15.16111946105957
Loss :  1.462085485458374 2.5148284435272217 14.03622817993164
Loss :  1.4603687524795532 3.210822105407715 17.514480590820312
Loss :  1.394372582435608 3.1932342052459717 17.360544204711914
Loss :  1.423091173171997 2.925732374191284 16.051753997802734
Loss :  1.4194880723953247 2.7370262145996094 15.104619026184082
Loss :  1.5335367918014526 2.42825984954834 13.674836158752441
Loss :  1.5452526807785034 2.9341769218444824 16.216136932373047
Loss :  1.564063549041748 3.1644256114959717 17.386192321777344
  batch 40 loss: 1.564063549041748, 3.1644256114959717, 17.386192321777344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.4854490756988525 3.1311538219451904 17.141218185424805
Loss :  1.4617722034454346 2.908545732498169 16.004501342773438
Loss :  1.4461779594421387 2.907498598098755 15.983671188354492
Loss :  1.4668904542922974 2.358462333679199 13.259202003479004
Loss :  1.4289960861206055 2.2174551486968994 12.516271591186523
Loss :  1.484900951385498 3.053387403488159 16.75183868408203
Loss :  1.5509482622146606 2.548760414123535 14.294750213623047
Loss :  1.460216999053955 2.700970411300659 14.965068817138672
Loss :  1.5826466083526611 2.8819291591644287 15.992292404174805
Loss :  1.4675451517105103 2.472182035446167 13.828455924987793
Loss :  1.5334800481796265 2.731226682662964 15.189613342285156
Loss :  1.5193065404891968 2.7254326343536377 15.146469116210938
Loss :  1.4808218479156494 2.2285349369049072 12.623496055603027
Loss :  1.5344971418380737 2.84603214263916 15.764657974243164
Loss :  1.4640607833862305 2.1317880153656006 12.123001098632812
Loss :  1.5825036764144897 2.4654784202575684 13.909895896911621
Loss :  1.4783915281295776 3.1577327251434326 17.26705551147461
Loss :  1.452850103378296 3.037243127822876 16.63906478881836
Loss :  1.482141375541687 3.4753565788269043 18.858924865722656
Loss :  1.5979712009429932 2.3862757682800293 13.529350280761719
  batch 60 loss: 1.5979712009429932, 2.3862757682800293, 13.529350280761719
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.4495810270309448 2.8993983268737793 15.946573257446289
Loss :  1.507685899734497 2.3726141452789307 13.370756149291992
Loss :  1.466762900352478 2.8307011127471924 15.620267868041992
Loss :  1.4465837478637695 2.4570493698120117 13.731830596923828
Loss :  1.408135175704956 2.2437806129455566 12.62703800201416
Loss :  1.4702221155166626 4.380841255187988 23.374427795410156
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.4893267154693604 4.364504337310791 23.311847686767578
Loss :  1.4890968799591064 4.41679048538208 23.573049545288086
Loss :  1.4803439378738403 4.355673789978027 23.25871467590332
Total LOSS train 15.192814489511344 valid 23.379509925842285
CE LOSS train 1.4858722558388344 valid 0.3700859844684601
Contrastive LOSS train 2.7413884272942175 valid 1.0889184474945068
EPOCH 97:
Loss :  1.532362699508667 2.8809866905212402 15.937296867370605
Loss :  1.5529545545578003 2.63065505027771 14.706230163574219
Loss :  1.4995259046554565 2.437225103378296 13.685651779174805
Loss :  1.5154896974563599 2.5610883235931396 14.320931434631348
Loss :  1.5465081930160522 2.6045641899108887 14.569329261779785
Loss :  1.4805424213409424 2.7732765674591064 15.346924781799316
Loss :  1.5490642786026 2.6755874156951904 14.927001953125
Loss :  1.5014641284942627 3.1393394470214844 17.198162078857422
Loss :  1.4840631484985352 2.745285749435425 15.210492134094238
Loss :  1.5463978052139282 2.2730488777160645 12.911642074584961
Loss :  1.462971568107605 2.7587151527404785 15.256546974182129
Loss :  1.4607303142547607 2.673449754714966 14.82797908782959
Loss :  1.457120418548584 2.649773359298706 14.705987930297852
Loss :  1.4673573970794678 3.1059086322784424 16.99690055847168
Loss :  1.5774953365325928 3.3637094497680664 18.396041870117188
Loss :  1.5606623888015747 2.96235990524292 16.37246322631836
Loss :  1.458785891532898 3.1184709072113037 17.05113983154297
Loss :  1.506160855293274 2.5426583290100098 14.219451904296875
Loss :  1.4532172679901123 2.296393394470215 12.935184478759766
Loss :  1.5709266662597656 2.8521316051483154 15.831584930419922
  batch 20 loss: 1.5709266662597656, 2.8521316051483154, 15.831584930419922
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.4990110397338867 2.996523857116699 16.481630325317383
Loss :  1.4478954076766968 3.434605121612549 18.620920181274414
Loss :  1.4839818477630615 2.5255610942840576 14.111787796020508
Loss :  1.5092551708221436 3.2114360332489014 17.566434860229492
Loss :  1.5631449222564697 2.4667019844055176 13.89665412902832
Loss :  1.4908384084701538 3.428745985031128 18.63456916809082
Loss :  1.5086232423782349 3.380584478378296 18.411544799804688
Loss :  1.4997225999832153 2.801022529602051 15.50483512878418
Loss :  1.4059420824050903 2.6526408195495605 14.669146537780762
Loss :  1.5596719980239868 3.4574456214904785 18.846899032592773
Loss :  1.4085609912872314 2.7641661167144775 15.229391098022461
Loss :  1.529127836227417 2.876490354537964 15.911580085754395
Loss :  1.483098030090332 2.6077589988708496 14.521893501281738
Loss :  1.4789842367172241 3.0914018154144287 16.935993194580078
Loss :  1.4185619354248047 3.217416524887085 17.505643844604492
Loss :  1.4454190731048584 3.211257219314575 17.501705169677734
Loss :  1.4425455331802368 2.5840771198272705 14.362931251525879
Loss :  1.5519843101501465 3.056501626968384 16.834491729736328
Loss :  1.561009168624878 3.392528772354126 18.523653030395508
Loss :  1.5820701122283936 2.982269525527954 16.493417739868164
  batch 40 loss: 1.5820701122283936, 2.982269525527954, 16.493417739868164
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.512719750404358 2.8452866077423096 15.739152908325195
Loss :  1.4912176132202148 2.9622597694396973 16.30251693725586
Loss :  1.468152642250061 4.040905952453613 21.67268180847168
Loss :  1.489152431488037 3.021127223968506 16.594789505004883
Loss :  1.4498248100280762 3.0418484210968018 16.659067153930664
Loss :  1.5068126916885376 2.755739688873291 15.285511016845703
Loss :  1.561214804649353 2.591853618621826 14.520483016967773
Loss :  1.471703052520752 3.4415743350982666 18.679574966430664
Loss :  1.583786964416504 3.0292892456054688 16.73023223876953
Loss :  1.4764537811279297 2.8876988887786865 15.914948463439941
Loss :  1.5369157791137695 2.998436689376831 16.529098510742188
Loss :  1.522915005683899 3.1845333576202393 17.445581436157227
Loss :  1.4833283424377441 3.0678937435150146 16.822797775268555
Loss :  1.5351358652114868 3.5891811847686768 19.481040954589844
Loss :  1.456644892692566 2.799298048019409 15.45313549041748
Loss :  1.5754563808441162 2.6303226947784424 14.727069854736328
Loss :  1.4664742946624756 2.8219387531280518 15.576168060302734
Loss :  1.4382328987121582 3.802924871444702 20.452856063842773
Loss :  1.465614914894104 3.48107647895813 18.870996475219727
Loss :  1.5825934410095215 2.9083566665649414 16.12437629699707
  batch 60 loss: 1.5825934410095215, 2.9083566665649414, 16.12437629699707
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.4301015138626099 3.1913671493530273 17.386938095092773
Loss :  1.489262342453003 3.265324831008911 17.815885543823242
Loss :  1.4472914934158325 2.7599525451660156 15.247054100036621
Loss :  1.4230560064315796 3.6803178787231445 19.824644088745117
Loss :  1.3826760053634644 2.391834259033203 13.34184741973877
Loss :  1.4944818019866943 4.421487331390381 23.601919174194336
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5168845653533936 4.415953636169434 23.59665298461914
Loss :  1.5179945230484009 4.334621906280518 23.191102981567383
Loss :  1.4949818849563599 4.1812825202941895 22.40139389038086
Total LOSS train 16.2949309862577 valid 23.19776725769043
CE LOSS train 1.4969228707827054 valid 0.37374547123908997
Contrastive LOSS train 2.959601637033316 valid 1.0453206300735474
EPOCH 98:
Loss :  1.5159087181091309 2.783693552017212 15.434375762939453
Loss :  1.5389546155929565 3.260744333267212 17.842676162719727
Loss :  1.482429027557373 3.770608425140381 20.335472106933594
Loss :  1.4965643882751465 3.248387575149536 17.738502502441406
Loss :  1.528885006904602 3.615048885345459 19.604129791259766
Loss :  1.4558215141296387 2.5020270347595215 13.965957641601562
Loss :  1.5304118394851685 2.6521966457366943 14.79139518737793
Loss :  1.476931095123291 2.351398468017578 13.233922958374023
Loss :  1.4566744565963745 2.3904738426208496 13.40904426574707
Loss :  1.5230414867401123 2.6634905338287354 14.840494155883789
Loss :  1.4349734783172607 2.99250864982605 16.39751625061035
Loss :  1.4312405586242676 2.823834180831909 15.550411224365234
Loss :  1.4262244701385498 3.0966808795928955 16.90962791442871
Loss :  1.4361990690231323 3.189408779144287 17.383243560791016
Loss :  1.55314040184021 3.109851121902466 17.10239601135254
Loss :  1.5328301191329956 2.6311655044555664 14.688657760620117
Loss :  1.4149986505508423 2.605468511581421 14.442341804504395
Loss :  1.466467261314392 3.1581993103027344 17.257463455200195
Loss :  1.4050946235656738 2.739560127258301 15.102895736694336
Loss :  1.5377339124679565 3.0359511375427246 16.71748924255371
  batch 20 loss: 1.5377339124679565, 3.0359511375427246, 16.71748924255371
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.4625308513641357 3.006685495376587 16.49595832824707
Loss :  1.4080396890640259 3.096459150314331 16.890335083007812
Loss :  1.4482643604278564 3.1402981281280518 17.149755477905273
Loss :  1.4797688722610474 2.976428508758545 16.36191177368164
Loss :  1.537681221961975 3.3855574131011963 18.465469360351562
Loss :  1.453108549118042 2.9715309143066406 16.310762405395508
Loss :  1.4738960266113281 2.5490922927856445 14.21935749053955
Loss :  1.46164071559906 2.9948456287384033 16.435869216918945
Loss :  1.3623321056365967 3.680424928665161 19.764455795288086
Loss :  1.5325984954833984 2.6265432834625244 14.665314674377441
Loss :  1.3708401918411255 2.831913948059082 15.530409812927246
Loss :  1.5057916641235352 2.4554531574249268 13.78305721282959
Loss :  1.4594171047210693 2.473405599594116 13.826445579528809
Loss :  1.4552197456359863 3.262554168701172 17.767990112304688
Loss :  1.3889360427856445 3.113741874694824 16.957645416259766
Loss :  1.417142391204834 3.0119640827178955 16.47696304321289
Loss :  1.4138543605804443 2.799865961074829 15.41318416595459
Loss :  1.5320301055908203 2.66982364654541 14.881148338317871
Loss :  1.5416961908340454 2.7445712089538574 15.26455307006836
Loss :  1.560940146446228 2.7641115188598633 15.381497383117676
  batch 40 loss: 1.560940146446228, 2.7641115188598633, 15.381497383117676
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.4828346967697144 3.6731927394866943 19.848796844482422
Loss :  1.4602062702178955 2.7547247409820557 15.233829498291016
Loss :  1.4409090280532837 3.2040956020355225 17.46138572692871
Loss :  1.4631553888320923 3.126356601715088 17.094938278198242
Loss :  1.4215401411056519 2.4244093894958496 13.543587684631348
Loss :  1.4771013259887695 2.8953840732574463 15.954021453857422
Loss :  1.5444176197052002 2.7506182193756104 15.297508239746094
Loss :  1.4505027532577515 3.5595521926879883 19.248262405395508
Loss :  1.576292634010315 2.711345911026001 15.13302230834961
Loss :  1.4597584009170532 2.2431640625 12.675579071044922
Loss :  1.5289117097854614 2.603226661682129 14.545044898986816
Loss :  1.517016887664795 3.485196113586426 18.942996978759766
Loss :  1.478092908859253 3.0027639865875244 16.491912841796875
Loss :  1.5316400527954102 2.745537757873535 15.259328842163086
Loss :  1.4562174081802368 2.5386605262756348 14.149519920349121
Loss :  1.5791276693344116 3.1847195625305176 17.50272560119629
Loss :  1.4705387353897095 3.2052600383758545 17.49683952331543
Loss :  1.4414515495300293 2.5083274841308594 13.983089447021484
Loss :  1.4696046113967896 3.1179392337799072 17.059301376342773
Loss :  1.587956428527832 2.952763557434082 16.351774215698242
  batch 60 loss: 1.587956428527832, 2.952763557434082, 16.351774215698242
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.4379262924194336 2.8178250789642334 15.52705192565918
Loss :  1.4991354942321777 3.4345827102661133 18.672048568725586
Loss :  1.4585564136505127 2.3731980323791504 13.324545860290527
Loss :  1.438625693321228 2.5373849868774414 14.125550270080566
Loss :  1.401452898979187 2.0057499408721924 11.43020248413086
Loss :  1.4719079732894897 4.270913600921631 22.82647705078125
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.494787335395813 4.363620281219482 23.312889099121094
Loss :  1.4919687509536743 4.162457466125488 22.30425453186035
Loss :  1.482960820198059 4.259690761566162 22.781415939331055
Total LOSS train 16.01758402310885 valid 22.806259155273438
CE LOSS train 1.4766650236569918 valid 0.37074020504951477
Contrastive LOSS train 2.908183809427115 valid 1.0649226903915405
EPOCH 99:
Loss :  1.5286346673965454 2.5285415649414062 14.171342849731445
Loss :  1.5493004322052002 3.2242236137390137 17.670419692993164
Loss :  1.4898775815963745 3.7929351329803467 20.454553604125977
Loss :  1.5061311721801758 3.2958908081054688 17.985584259033203
Loss :  1.5340181589126587 2.589501142501831 14.481524467468262
Loss :  1.4664031267166138 3.137343645095825 17.153121948242188
Loss :  1.5367295742034912 2.5711140632629395 14.39229965209961
Loss :  1.4870336055755615 2.9333014488220215 16.153541564941406
Loss :  1.4693065881729126 2.6090779304504395 14.51469612121582
Loss :  1.5315481424331665 2.497864246368408 14.020869255065918
Loss :  1.4522730112075806 2.6365468502044678 14.635007858276367
Loss :  1.4530391693115234 2.762946844100952 15.267773628234863
Loss :  1.4499324560165405 3.0390379428863525 16.645122528076172
Loss :  1.4616117477416992 3.005310297012329 16.488162994384766
Loss :  1.5721954107284546 3.24043607711792 17.774375915527344
Loss :  1.551724910736084 2.899095296859741 16.04720115661621
Loss :  1.4434956312179565 2.892176628112793 15.904378890991211
Loss :  1.4921566247940063 2.761422634124756 15.299270629882812
Loss :  1.4388870000839233 2.8337368965148926 15.60757064819336
Loss :  1.5609720945358276 2.6086106300354004 14.604024887084961
  batch 20 loss: 1.5609720945358276, 2.6086106300354004, 14.604024887084961
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.4887659549713135 2.99318528175354 16.454692840576172
Loss :  1.4351513385772705 2.8396315574645996 15.633309364318848
Loss :  1.4705860614776611 2.9425570964813232 16.183372497558594
Loss :  1.4980281591415405 2.760528564453125 15.300670623779297
Loss :  1.5518975257873535 3.201997995376587 17.561887741088867
Loss :  1.4722604751586914 3.140378713607788 17.17415428161621
Loss :  1.495182991027832 3.1256353855133057 17.12335968017578
Loss :  1.482926607131958 2.4105477333068848 13.535664558410645
Loss :  1.3887251615524292 3.370342254638672 18.240436553955078
Loss :  1.547189712524414 2.615262269973755 14.62350082397461
Loss :  1.3881891965866089 2.8294789791107178 15.535584449768066
Loss :  1.5151681900024414 2.689052104949951 14.960429191589355
Loss :  1.4671378135681152 3.0583033561706543 16.758655548095703
Loss :  1.4633965492248535 3.038689374923706 16.656843185424805
Loss :  1.4026072025299072 2.763599157333374 15.220602989196777
Loss :  1.4313535690307617 3.106083869934082 16.961772918701172
Loss :  1.4285567998886108 2.871856689453125 15.787839889526367
Loss :  1.541702151298523 2.870114326477051 15.892273902893066
Loss :  1.5518805980682373 2.934014320373535 16.221952438354492
Loss :  1.570569634437561 2.8295443058013916 15.718291282653809
  batch 40 loss: 1.570569634437561, 2.8295443058013916, 15.718291282653809
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.4965541362762451 2.4511563777923584 13.752336502075195
Loss :  1.4748562574386597 3.300143241882324 17.97557258605957
Loss :  1.4581685066223145 2.5676445960998535 14.296390533447266
Loss :  1.4779162406921387 2.450037956237793 13.728105545043945
Loss :  1.4397329092025757 2.211390972137451 12.496687889099121
Loss :  1.4927244186401367 3.0138914585113525 16.56218147277832
Loss :  1.5562214851379395 2.417297840118408 13.642709732055664
Loss :  1.4688926935195923 2.3536291122436523 13.237038612365723
Loss :  1.5854289531707764 2.626643657684326 14.718647956848145
Loss :  1.4788670539855957 2.713332176208496 15.045528411865234
Loss :  1.5406380891799927 2.5579922199249268 14.330598831176758
Loss :  1.5301933288574219 2.8408620357513428 15.734503746032715
Loss :  1.4944119453430176 3.006714105606079 16.527982711791992
Loss :  1.544863224029541 3.118992805480957 17.139827728271484
Loss :  1.472547173500061 3.1072909832000732 17.009002685546875
Loss :  1.5899381637573242 3.593202829360962 19.555953979492188
Loss :  1.4853394031524658 3.2129859924316406 17.550270080566406
Loss :  1.4596484899520874 3.0347096920013428 16.633197784423828
Loss :  1.4844772815704346 2.599557399749756 14.482264518737793
Loss :  1.5942635536193848 2.657794713973999 14.883237838745117
  batch 60 loss: 1.5942635536193848, 2.657794713973999, 14.883237838745117
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.4550888538360596 3.124666929244995 17.07842445373535
Loss :  1.509925365447998 3.1124470233917236 17.072160720825195
Loss :  1.4758919477462769 2.582162618637085 14.38670539855957
Loss :  1.4549585580825806 3.352029323577881 18.215105056762695
Loss :  1.418898344039917 3.330970287322998 18.073749542236328
Loss :  1.484837293624878 4.079390048980713 21.881786346435547
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5073764324188232 3.9867217540740967 21.44098472595215
Loss :  1.5064890384674072 3.965184450149536 21.33241081237793
Loss :  1.4944325685501099 3.825517416000366 20.622018814086914
Total LOSS train 15.922220332805926 valid 21.319300174713135
CE LOSS train 1.4924152796085064 valid 0.37360814213752747
Contrastive LOSS train 2.885960975060096 valid 0.9563793540000916
EPOCH 100:
Loss :  1.541627287864685 3.169647216796875 17.389863967895508
Loss :  1.5616559982299805 3.174197196960449 17.432641983032227
Loss :  1.5016175508499146 2.831920623779297 15.66122055053711
Loss :  1.5133576393127441 2.604775905609131 14.537237167358398
Loss :  1.5388870239257812 3.691049337387085 19.99413299560547
Loss :  1.4752588272094727 2.9571359157562256 16.26093864440918
Loss :  1.537872314453125 3.013558864593506 16.605667114257812
Loss :  1.4919023513793945 2.4964897632598877 13.974350929260254
Loss :  1.4699469804763794 2.5557799339294434 14.248847007751465
Loss :  1.5296469926834106 2.9021615982055664 16.040454864501953
Loss :  1.452938437461853 3.243818521499634 17.67203140258789
Loss :  1.452007532119751 2.8704843521118164 15.804429054260254
Loss :  1.4454121589660645 2.855564832687378 15.723236083984375
Loss :  1.4586126804351807 2.6779582500457764 14.848403930664062
Loss :  1.5665749311447144 3.2852537631988525 17.992843627929688
Loss :  1.5536469221115112 3.0495667457580566 16.80147933959961
Loss :  1.4455630779266357 2.9005630016326904 15.948378562927246
Loss :  1.4933840036392212 3.234710216522217 17.666934967041016
Loss :  1.4385212659835815 2.5909669399261475 14.393356323242188
Loss :  1.55819833278656 3.0448453426361084 16.782424926757812
  batch 20 loss: 1.55819833278656, 3.0448453426361084, 16.782424926757812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.4902970790863037 2.522737741470337 14.103985786437988
Loss :  1.4401007890701294 2.71675968170166 15.02389907836914
Loss :  1.475780963897705 3.090000867843628 16.925785064697266
Loss :  1.5023192167282104 3.132587194442749 17.165254592895508
Loss :  1.5561679601669312 3.0546255111694336 16.829296112060547
Loss :  1.4769984483718872 4.132142543792725 22.137712478637695
Loss :  1.496029257774353 3.619832754135132 19.595191955566406
Loss :  1.48621666431427 2.928473949432373 16.12858772277832
Loss :  1.3908131122589111 2.6645100116729736 14.713362693786621
Loss :  1.5512434244155884 3.047968626022339 16.791086196899414
Loss :  1.3926570415496826 3.1754989624023438 17.270151138305664
Loss :  1.518518090248108 3.3417842388153076 18.227439880371094
Loss :  1.4721729755401611 2.9069113731384277 16.006731033325195
Loss :  1.4652225971221924 2.4836196899414062 13.883320808410645
Loss :  1.400822401046753 2.860527992248535 15.703462600708008
Loss :  1.4264018535614014 2.6079869270324707 14.466336250305176
Loss :  1.4219392538070679 3.0409903526306152 16.62689208984375
Loss :  1.5378066301345825 3.6758012771606445 19.916812896728516
Loss :  1.548059105873108 3.320568084716797 18.15089988708496
Loss :  1.5695390701293945 2.59975528717041 14.568315505981445
  batch 40 loss: 1.5695390701293945, 2.59975528717041, 14.568315505981445
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.4888592958450317 2.9206953048706055 16.092336654663086
Loss :  1.4662002325057983 2.74405837059021 15.186491966247559
Loss :  1.4440553188323975 3.084665060043335 16.867380142211914
Loss :  1.468739628791809 3.02655029296875 16.601491928100586
Loss :  1.426388144493103 2.3489890098571777 13.171333312988281
Loss :  1.4837685823440552 2.7571017742156982 15.269277572631836
Loss :  1.550379753112793 3.390702724456787 18.503894805908203
Loss :  1.4553853273391724 3.1721417903900146 17.31609535217285
Loss :  1.5840293169021606 3.0223541259765625 16.69580078125
Loss :  1.4709073305130005 3.066843271255493 16.805124282836914
Loss :  1.535542607307434 3.043778419494629 16.75443458557129
Loss :  1.5246273279190063 3.0148370265960693 16.598812103271484
Loss :  1.484505295753479 3.272555351257324 17.84728240966797
Loss :  1.5401524305343628 3.1097443103790283 17.0888729095459
Loss :  1.4635220766067505 2.6518712043762207 14.722877502441406
Loss :  1.5892759561538696 2.490065813064575 14.039605140686035
Loss :  1.4844311475753784 3.050572156906128 16.73729133605957
Loss :  1.4540432691574097 2.9661481380462646 16.2847843170166
Loss :  1.4837491512298584 3.261474609375 17.791122436523438
Loss :  1.6015442609786987 2.779095411300659 15.497021675109863
  batch 60 loss: 1.6015442609786987, 2.779095411300659, 15.497021675109863
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.4527076482772827 2.679988384246826 14.852649688720703
Loss :  1.5076617002487183 2.874027729034424 15.877799987792969
Loss :  1.470029354095459 2.555612087249756 14.248090744018555
Loss :  1.4532270431518555 2.8373475074768066 15.63996410369873
Loss :  1.417311429977417 2.3524668216705322 13.179645538330078
Loss :  1.4879282712936401 4.257723331451416 22.77654457092285
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5082449913024902 4.200048446655273 22.508487701416016
Loss :  1.5060625076293945 4.206711769104004 22.53961944580078
Loss :  1.488017201423645 4.198367595672607 22.479856491088867
Total LOSS train 16.30281351529635 valid 22.57612705230713
CE LOSS train 1.4914889519031231 valid 0.37200430035591125
Contrastive LOSS train 2.962264893605159 valid 1.0495918989181519
EPOCH 101:
Loss :  1.5427554845809937 2.87562894821167 15.920900344848633
Loss :  1.5644264221191406 3.0826761722564697 16.977806091308594
Loss :  1.5099726915359497 2.5971832275390625 14.495888710021973
Loss :  1.5236784219741821 2.7618606090545654 15.332982063293457
Loss :  1.553122639656067 2.627150774002075 14.688876152038574
Loss :  1.4891546964645386 2.921325206756592 16.095781326293945
Loss :  1.5582093000411987 3.2772722244262695 17.944568634033203
Loss :  1.5099821090698242 2.3730063438415527 13.375014305114746
Loss :  1.4891494512557983 2.295938014984131 12.968839645385742
Loss :  1.5532574653625488 3.0674171447753906 16.890342712402344
Loss :  1.4709361791610718 3.1293301582336426 17.117586135864258
Loss :  1.4681053161621094 2.7752151489257812 15.344181060791016
Loss :  1.462232232093811 2.7778782844543457 15.35162353515625
Loss :  1.4711724519729614 2.955566644668579 16.249006271362305
Loss :  1.5813822746276855 2.650418519973755 14.833475112915039
Loss :  1.5643521547317505 2.5110366344451904 14.119535446166992
Loss :  1.4547996520996094 2.6638314723968506 14.773957252502441
Loss :  1.50517737865448 2.3873250484466553 13.441802978515625
Loss :  1.4471662044525146 2.2842462062835693 12.86839771270752
Loss :  1.5681917667388916 2.9114558696746826 16.125471115112305
  batch 20 loss: 1.5681917667388916, 2.9114558696746826, 16.125471115112305
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.4962966442108154 2.678569793701172 14.889145851135254
Loss :  1.4442849159240723 2.779268980026245 15.340629577636719
Loss :  1.4812672138214111 2.4074292182922363 13.518412590026855
Loss :  1.5086522102355957 2.5852978229522705 14.435140609741211
Loss :  1.5614947080612183 2.863438367843628 15.878686904907227
Loss :  1.484382152557373 3.7770018577575684 20.36939239501953
Loss :  1.503854513168335 4.067916393280029 21.84343719482422
Loss :  1.4962427616119385 2.367619276046753 13.334339141845703
Loss :  1.3990591764450073 2.5416030883789062 14.107074737548828
Loss :  1.5551512241363525 2.882563591003418 15.967968940734863
Loss :  1.399341106414795 3.1427721977233887 17.113203048706055
Loss :  1.5259567499160767 3.008533477783203 16.56862449645996
Loss :  1.4823908805847168 2.7765893936157227 15.365337371826172
Loss :  1.4790056943893433 2.2351953983306885 12.654982566833496
Loss :  1.416182041168213 3.4249064922332764 18.540714263916016
Loss :  1.4435312747955322 2.621821403503418 14.552638053894043
Loss :  1.4394487142562866 2.3174939155578613 13.026917457580566
Loss :  1.551922082901001 2.2811038494110107 12.957441329956055
Loss :  1.561448097229004 2.5787036418914795 14.45496654510498
Loss :  1.579334020614624 2.550969123840332 14.334179878234863
  batch 40 loss: 1.579334020614624, 2.550969123840332, 14.334179878234863
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5011322498321533 2.5110437870025635 14.056350708007812
Loss :  1.4797660112380981 2.2184362411499023 12.57194709777832
Loss :  1.4575726985931396 2.9277970790863037 16.0965576171875
Loss :  1.4818553924560547 4.174254417419434 22.35312843322754
Loss :  1.4412916898727417 2.271052598953247 12.796554565429688
Loss :  1.4948726892471313 2.2420380115509033 12.705062866210938
Loss :  1.55907142162323 2.323025941848755 13.174201011657715
Loss :  1.4659478664398193 2.7762768268585205 15.347332000732422
Loss :  1.586255431175232 2.4064338207244873 13.618424415588379
Loss :  1.4764673709869385 3.3544013500213623 18.24847412109375
Loss :  1.5387816429138184 2.4680638313293457 13.879100799560547
Loss :  1.5281070470809937 2.793707847595215 15.4966459274292
Loss :  1.4909955263137817 2.3412089347839355 13.197040557861328
Loss :  1.5448315143585205 2.6873886585235596 14.98177433013916
Loss :  1.4689147472381592 2.8876078128814697 15.906953811645508
Loss :  1.587850570678711 2.804672956466675 15.611215591430664
Loss :  1.4842220544815063 3.293006181716919 17.94925308227539
Loss :  1.4551070928573608 2.7007412910461426 14.958812713623047
Loss :  1.4830865859985352 3.2540504932403564 17.753337860107422
Loss :  1.5981783866882324 2.7512025833129883 15.354190826416016
  batch 60 loss: 1.5981783866882324, 2.7512025833129883, 15.354190826416016
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.4556609392166138 2.3816580772399902 13.363951683044434
Loss :  1.5100607872009277 2.357570171356201 13.29791259765625
Loss :  1.4749687910079956 2.5441300868988037 14.195619583129883
Loss :  1.4542995691299438 2.837263822555542 15.640619277954102
Loss :  1.4182319641113281 2.2687783241271973 12.762123107910156
Loss :  1.469672679901123 4.20437479019165 22.491546630859375
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.4903632402420044 4.173734188079834 22.35903549194336
Loss :  1.483189582824707 4.060011863708496 21.783248901367188
Loss :  1.4969898462295532 4.07106876373291 21.852333068847656
Total LOSS train 15.222859294597919 valid 22.121541023254395
CE LOSS train 1.5005230848605817 valid 0.3742474615573883
Contrastive LOSS train 2.744467247449435 valid 1.0177671909332275
EPOCH 102:
Loss :  1.5414526462554932 2.7139313220977783 15.111108779907227
Loss :  1.5630613565444946 2.892510414123535 16.02561378479004
Loss :  1.5088305473327637 2.5612175464630127 14.314918518066406
Loss :  1.5224330425262451 2.637521982192993 14.710042953491211
Loss :  1.5505547523498535 2.420724630355835 13.654178619384766
Loss :  1.487674355506897 2.3842194080352783 13.408771514892578
Loss :  1.552930474281311 2.9067273139953613 16.086566925048828
Loss :  1.5058315992355347 2.0925698280334473 11.968680381774902
Loss :  1.485661506652832 2.3557028770446777 13.264176368713379
Loss :  1.5441030263900757 2.3142592906951904 13.115399360656738
Loss :  1.46687650680542 2.4849259853363037 13.89150619506836
Loss :  1.4660779237747192 2.3933677673339844 13.432916641235352
Loss :  1.4619497060775757 2.177034854888916 12.347123146057129
Loss :  1.4729418754577637 2.377224922180176 13.359066009521484
Loss :  1.5779471397399902 3.2572779655456543 17.864337921142578
Loss :  1.5629768371582031 3.0767407417297363 16.946680068969727
Loss :  1.4618875980377197 2.7446823120117188 15.185298919677734
Loss :  1.5083863735198975 2.7253034114837646 15.134903907775879
Loss :  1.456394076347351 2.798736333847046 15.45007610321045
Loss :  1.5678153038024902 2.7600901126861572 15.368265151977539
  batch 20 loss: 1.5678153038024902, 2.7600901126861572, 15.368265151977539
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.5029608011245728 2.314549684524536 13.075709342956543
Loss :  1.4550331830978394 3.7179250717163086 20.044658660888672
Loss :  1.4869446754455566 2.489945888519287 13.936674118041992
Loss :  1.5117899179458618 3.530982494354248 19.166702270507812
Loss :  1.5635943412780762 3.3524394035339355 18.32579231262207
Loss :  1.4889413118362427 2.8622992038726807 15.800436973571777
Loss :  1.5083249807357788 3.0800414085388184 16.908533096313477
Loss :  1.499489665031433 2.7971580028533936 15.48528003692627
Loss :  1.4126451015472412 2.755598306655884 15.19063663482666
Loss :  1.5619360208511353 3.5643703937530518 19.383787155151367
Loss :  1.417660117149353 2.573776960372925 14.286544799804688
Loss :  1.5357937812805176 2.860617160797119 15.838878631591797
Loss :  1.4932671785354614 2.5029425621032715 14.007980346679688
Loss :  1.490571141242981 2.3017828464508057 12.99948501586914
Loss :  1.433571219444275 2.3944544792175293 13.405843734741211
Loss :  1.458597183227539 2.5033822059631348 13.975507736206055
Loss :  1.4554507732391357 3.114250659942627 17.026702880859375
Loss :  1.5588099956512451 3.1702048778533936 17.409833908081055
Loss :  1.5674502849578857 2.3115949630737305 13.125425338745117
Loss :  1.5838425159454346 2.4751017093658447 13.9593505859375
  batch 40 loss: 1.5838425159454346, 2.4751017093658447, 13.9593505859375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.5151022672653198 2.7263829708099365 15.147017478942871
Loss :  1.4942681789398193 2.7237207889556885 15.112872123718262
Loss :  1.4776709079742432 2.7072181701660156 15.013761520385742
Loss :  1.4981260299682617 3.6653382778167725 19.824817657470703
Loss :  1.4636532068252563 2.6280593872070312 14.603950500488281
Loss :  1.514495611190796 3.2279715538024902 17.654354095458984
Loss :  1.5718988180160522 2.3377678394317627 13.260737419128418
Loss :  1.489861011505127 2.500206470489502 13.99089241027832
Loss :  1.5971795320510864 3.018324136734009 16.688800811767578
Loss :  1.4977079629898071 2.177001953125 12.382718086242676
Loss :  1.5573807954788208 2.527618169784546 14.19547176361084
Loss :  1.5459213256835938 2.2888529300689697 12.990185737609863
Loss :  1.5117716789245605 2.6538004875183105 14.78077507019043
Loss :  1.5589618682861328 2.823878526687622 15.678354263305664
Loss :  1.4940710067749023 3.541640520095825 19.202274322509766
Loss :  1.597318410873413 2.6188859939575195 14.69174861907959
Loss :  1.501577377319336 2.76853609085083 15.344257354736328
Loss :  1.4775567054748535 2.7168049812316895 15.061580657958984
Loss :  1.5031088590621948 2.7849111557006836 15.427664756774902
Loss :  1.6086400747299194 2.3521080017089844 13.369179725646973
  batch 60 loss: 1.6086400747299194, 2.3521080017089844, 13.369179725646973
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.4779635667800903 2.990661382675171 16.431270599365234
Loss :  1.5291271209716797 2.830883026123047 15.683542251586914
Loss :  1.4936347007751465 2.9658303260803223 16.322786331176758
Loss :  1.4762417078018188 3.1220335960388184 17.086410522460938
Loss :  1.4418599605560303 1.8770338296890259 10.82702922821045
Loss :  1.4949967861175537 4.358170032501221 23.285846710205078
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5152455568313599 4.300590515136719 23.018198013305664
Loss :  1.5137736797332764 4.31097936630249 23.06867027282715
Loss :  1.499516487121582 4.275636196136475 22.877696990966797
Total LOSS train 15.150243817842924 valid 23.062602996826172
CE LOSS train 1.509962454208961 valid 0.3748791217803955
Contrastive LOSS train 2.7280562749275794 valid 1.0689090490341187
EPOCH 103:
Loss :  1.5525522232055664 2.3017303943634033 13.061203956604004
Loss :  1.5714819431304932 2.516662120819092 14.154792785644531
Loss :  1.5197665691375732 2.240403413772583 12.721783638000488
Loss :  1.533044457435608 2.856497287750244 15.815530776977539
Loss :  1.5601692199707031 2.6295273303985596 14.707805633544922
Loss :  1.5006104707717896 2.6498260498046875 14.749740600585938
Loss :  1.5621931552886963 2.730424404144287 15.214315414428711
Loss :  1.5205800533294678 2.318488121032715 13.113020896911621
Loss :  1.5063743591308594 2.3540329933166504 13.276538848876953
Loss :  1.561061143875122 2.7011404037475586 15.066762924194336
Loss :  1.4919230937957764 3.2523837089538574 17.753841400146484
Loss :  1.4911603927612305 2.5675528049468994 14.328924179077148
Loss :  1.4872279167175293 2.355447292327881 13.26446533203125
Loss :  1.4981732368469238 2.9958279132843018 16.477312088012695
Loss :  1.5937007665634155 2.8627119064331055 15.907259941101074
Loss :  1.5793108940124512 2.7622501850128174 15.390562057495117
Loss :  1.4877936840057373 2.908597469329834 16.030780792236328
Loss :  1.5283578634262085 2.841740369796753 15.737059593200684
Loss :  1.481631875038147 3.0934791564941406 16.94902801513672
Loss :  1.585917353630066 2.143791437149048 12.304874420166016
  batch 20 loss: 1.585917353630066, 2.143791437149048, 12.304874420166016
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.527213215827942 3.4036262035369873 18.545345306396484
Loss :  1.4847263097763062 2.4752213954925537 13.860833168029785
Loss :  1.5157917737960815 2.580655336380005 14.419068336486816
Loss :  1.5392085313796997 2.738257884979248 15.230498313903809
Loss :  1.5847183465957642 3.3912413120269775 18.540925979614258
Loss :  1.5186049938201904 2.7569305896759033 15.303257942199707
Loss :  1.5353126525878906 3.2011919021606445 17.541271209716797
Loss :  1.528666615486145 2.268604040145874 12.871686935424805
Loss :  1.4493951797485352 2.826856851577759 15.58367919921875
Loss :  1.5820344686508179 3.048861026763916 16.826339721679688
Loss :  1.4508154392242432 3.0465521812438965 16.683576583862305
Loss :  1.5556164979934692 2.6694984436035156 14.903108596801758
Loss :  1.5167505741119385 2.449765682220459 13.765579223632812
Loss :  1.5133612155914307 2.4847257137298584 13.936989784240723
Loss :  1.4622700214385986 2.6043221950531006 14.483880996704102
Loss :  1.4856828451156616 2.9232964515686035 16.10216522216797
Loss :  1.481449842453003 2.7330777645111084 15.146839141845703
Loss :  1.5760489702224731 3.1649973392486572 17.40103530883789
Loss :  1.5848610401153564 2.867903470993042 15.924378395080566
Loss :  1.5990949869155884 2.6838390827178955 15.018290519714355
  batch 40 loss: 1.5990949869155884, 2.6838390827178955, 15.018290519714355
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5319832563400269 2.6322736740112305 14.693351745605469
Loss :  1.510184645652771 3.4644625186920166 18.83249855041504
Loss :  1.4951837062835693 3.3518052101135254 18.254209518432617
Loss :  1.514227032661438 2.428252935409546 13.655491828918457
Loss :  1.4817256927490234 2.2184324264526367 12.573887825012207
Loss :  1.5275160074234009 2.8024439811706543 15.5397367477417
Loss :  1.5815025568008423 2.6144747734069824 14.653877258300781
Loss :  1.5072132349014282 2.241053819656372 12.712482452392578
Loss :  1.6066551208496094 2.988081693649292 16.54706382751465
Loss :  1.5136486291885376 4.177056312561035 22.398929595947266
Loss :  1.5671306848526 2.735079288482666 15.24252700805664
Loss :  1.5545607805252075 3.3365883827209473 18.237503051757812
Loss :  1.5222188234329224 3.1431832313537598 17.238134384155273
Loss :  1.566861867904663 3.4339451789855957 18.736587524414062
Loss :  1.5043436288833618 3.032498836517334 16.666837692260742
Loss :  1.6023495197296143 2.627114772796631 14.737923622131348
Loss :  1.5146245956420898 2.7903008460998535 15.4661283493042
Loss :  1.492069125175476 3.0729827880859375 16.856983184814453
Loss :  1.5157285928726196 3.0829319953918457 16.930387496948242
Loss :  1.6130388975143433 2.8222906589508057 15.724492073059082
  batch 60 loss: 1.6130388975143433, 2.8222906589508057, 15.724492073059082
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.4901649951934814 2.744229316711426 15.211311340332031
Loss :  1.539864182472229 2.932898759841919 16.204357147216797
Loss :  1.5055807828903198 2.6739585399627686 14.875373840332031
Loss :  1.4881527423858643 3.305797815322876 18.01714324951172
Loss :  1.4551883935928345 2.2948193550109863 12.929285049438477
Loss :  1.5221859216690063 4.302496910095215 23.034671783447266
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5380529165267944 4.294589042663574 23.01099967956543
Loss :  1.5379489660263062 4.21150541305542 22.595476150512695
Loss :  1.5212584733963013 4.16495418548584 22.346031188964844
Total LOSS train 15.554628577599159 valid 22.74679470062256
CE LOSS train 1.5277903336745042 valid 0.3803146183490753
Contrastive LOSS train 2.80536764218257 valid 1.04123854637146
EPOCH 104:
Loss :  1.5609995126724243 3.0751919746398926 16.93695831298828
Loss :  1.5795435905456543 3.4595534801483154 18.87731170654297
Loss :  1.5309007167816162 2.185588836669922 12.458845138549805
Loss :  1.5419156551361084 2.7508304119110107 15.29606819152832
Loss :  1.5670218467712402 2.3367116451263428 13.250579833984375
Loss :  1.5089781284332275 2.245363712310791 12.735795974731445
Loss :  1.5689198970794678 2.371852397918701 13.428182601928711
Loss :  1.5284221172332764 2.061253547668457 11.83469009399414
Loss :  1.5134451389312744 3.286736011505127 17.947124481201172
Loss :  1.5659993886947632 2.3425116539001465 13.278557777404785
Loss :  1.4989815950393677 2.602724552154541 14.512603759765625
Loss :  1.4992876052856445 2.4228501319885254 13.613537788391113
Loss :  1.4958299398422241 2.1787362098693848 12.389510154724121
Loss :  1.5050272941589355 2.827306032180786 15.641557693481445
Loss :  1.5985074043273926 2.460728883743286 13.902151107788086
Loss :  1.5852311849594116 2.1738831996917725 12.454647064208984
Loss :  1.4969749450683594 2.8570916652679443 15.78243350982666
Loss :  1.5362346172332764 2.7393431663513184 15.232951164245605
Loss :  1.489156723022461 2.688260793685913 14.930460929870605
Loss :  1.5887073278427124 2.2627646923065186 12.902530670166016
  batch 20 loss: 1.5887073278427124, 2.2627646923065186, 12.902530670166016
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.5294631719589233 2.6990280151367188 15.024602890014648
Loss :  1.486802577972412 3.0507121086120605 16.74036407470703
Loss :  1.516929030418396 2.1914947032928467 12.47440242767334
Loss :  1.5414507389068604 2.2561795711517334 12.822348594665527
Loss :  1.586110234260559 3.142876148223877 17.300491333007812
Loss :  1.5233744382858276 2.639291524887085 14.719832420349121
Loss :  1.5404751300811768 3.394314765930176 18.512048721313477
Loss :  1.5321050882339478 2.2234930992126465 12.649571418762207
Loss :  1.451640009880066 2.7812352180480957 15.357815742492676
Loss :  1.581392526626587 3.128701686859131 17.22490119934082
Loss :  1.453991413116455 2.591374158859253 14.41086196899414
Loss :  1.5581796169281006 2.7556755542755127 15.336557388305664
Loss :  1.5204682350158691 2.679082155227661 14.915878295898438
Loss :  1.5166783332824707 2.870542287826538 15.869390487670898
Loss :  1.4660831689834595 2.6460115909576416 14.696141242980957
Loss :  1.4883366823196411 2.401719093322754 13.496932029724121
Loss :  1.485667109489441 2.1175220012664795 12.073277473449707
Loss :  1.5788170099258423 2.1771697998046875 12.464666366577148
Loss :  1.587539792060852 2.667733669281006 14.92620849609375
Loss :  1.6037261486053467 2.6399145126342773 14.803298950195312
  batch 40 loss: 1.6037261486053467, 2.6399145126342773, 14.803298950195312
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.542038917541504 2.584423065185547 14.464154243469238
Loss :  1.523269534111023 2.4703209400177 13.874874114990234
Loss :  1.505979061126709 2.509706974029541 14.054513931274414
Loss :  1.526551365852356 2.4911246299743652 13.98217487335205
Loss :  1.494433045387268 2.236294984817505 12.675908088684082
Loss :  1.5380383729934692 2.8843111991882324 15.9595947265625
Loss :  1.5894232988357544 2.893228054046631 16.055564880371094
Loss :  1.516321063041687 2.9054627418518066 16.04363441467285
Loss :  1.6106311082839966 2.262873888015747 12.925000190734863
Loss :  1.5231176614761353 2.3158669471740723 13.102452278137207
Loss :  1.5736324787139893 2.470653772354126 13.926900863647461
Loss :  1.564395546913147 2.343764305114746 13.283217430114746
Loss :  1.5354825258255005 2.6316659450531006 14.693812370300293
Loss :  1.5765750408172607 2.254971981048584 12.851435661315918
Loss :  1.5174115896224976 2.3502731323242188 13.268776893615723
Loss :  1.6126176118850708 2.7555224895477295 15.390230178833008
Loss :  1.5298588275909424 2.7806026935577393 15.43287181854248
Loss :  1.5082838535308838 2.681610584259033 14.916336059570312
Loss :  1.530814528465271 3.0676259994506836 16.86894416809082
Loss :  1.6239023208618164 2.6590890884399414 14.919347763061523
  batch 60 loss: 1.6239023208618164, 2.6590890884399414, 14.919347763061523
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.509987473487854 2.9139368534088135 16.07967185974121
Loss :  1.5552117824554443 3.224757671356201 17.679000854492188
Loss :  1.5246520042419434 2.4402055740356445 13.725679397583008
Loss :  1.5075745582580566 2.2943506240844727 12.979328155517578
Loss :  1.4776214361190796 1.6686947345733643 9.821094512939453
Loss :  1.5382473468780518 4.338352680206299 23.230010986328125
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5529005527496338 4.2906341552734375 23.006071090698242
Loss :  1.5524868965148926 4.192318439483643 22.51407814025879
Loss :  1.5403929948806763 4.186551570892334 22.47315216064453
Total LOSS train 14.495393987802359 valid 22.805828094482422
CE LOSS train 1.5358021552746113 valid 0.38509824872016907
Contrastive LOSS train 2.591918362103976 valid 1.0466378927230835
EPOCH 105:
Loss :  1.5765007734298706 2.630770206451416 14.730351448059082
Loss :  1.5928772687911987 2.5331947803497314 14.258851051330566
Loss :  1.5489275455474854 2.470137596130371 13.899615287780762
Loss :  1.5605354309082031 2.2692933082580566 12.907001495361328
Loss :  1.5838334560394287 2.224586248397827 12.706765174865723
Loss :  1.5321556329727173 2.65861177444458 14.825214385986328
Loss :  1.5870095491409302 2.402940511703491 13.601712226867676
Loss :  1.5492125749588013 3.0600926876068115 16.84967613220215
Loss :  1.5357556343078613 2.59970760345459 14.534294128417969
Loss :  1.582863688468933 2.75380539894104 15.351890563964844
Loss :  1.5196982622146606 2.714754819869995 15.093472480773926
Loss :  1.5181852579116821 2.780857801437378 15.42247486114502
Loss :  1.5153945684432983 2.489227056503296 13.961529731750488
Loss :  1.525321364402771 2.4592771530151367 13.821706771850586
Loss :  1.6106864213943481 2.8157458305358887 15.68941593170166
Loss :  1.5990629196166992 2.7611165046691895 15.404644966125488
Loss :  1.519191026687622 2.7157790660858154 15.0980863571167
Loss :  1.5538891553878784 2.5311648845672607 14.20971393585205
Loss :  1.512650489807129 3.1432998180389404 17.229148864746094
Loss :  1.6065198183059692 2.823974609375 15.72639274597168
  batch 20 loss: 1.6065198183059692, 2.823974609375, 15.72639274597168
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5503095388412476 3.1611900329589844 17.356260299682617
Loss :  1.5093543529510498 3.5889413356781006 19.454059600830078
Loss :  1.5358699560165405 2.6335370540618896 14.7035551071167
Loss :  1.5565155744552612 3.2720329761505127 17.91667938232422
Loss :  1.5977115631103516 2.7798361778259277 15.496892929077148
Loss :  1.5370807647705078 2.7674379348754883 15.37427043914795
Loss :  1.5508415699005127 2.7316651344299316 15.209166526794434
Loss :  1.544677495956421 2.6716179847717285 14.902767181396484
Loss :  1.471534252166748 3.8722140789031982 20.832605361938477
Loss :  1.592388391494751 2.436018943786621 13.772482872009277
Loss :  1.4724016189575195 2.4961628913879395 13.953215599060059
Loss :  1.5695101022720337 2.4205236434936523 13.672128677368164
Loss :  1.5336891412734985 3.0336945056915283 16.70216178894043
Loss :  1.5311461687088013 2.5138304233551025 14.100297927856445
Loss :  1.4844460487365723 3.139930486679077 17.184099197387695
Loss :  1.5054216384887695 2.698042392730713 14.995633125305176
Loss :  1.5024158954620361 2.2887778282165527 12.946305274963379
Loss :  1.586817741394043 2.5453941822052 14.313788414001465
Loss :  1.5961250066757202 2.704190969467163 15.117079734802246
Loss :  1.608805537223816 3.0957913398742676 17.08776092529297
  batch 40 loss: 1.608805537223816, 3.0957913398742676, 17.08776092529297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5509414672851562 2.450789213180542 13.804887771606445
Loss :  1.534209132194519 2.1520156860351562 12.29428768157959
Loss :  1.5202711820602417 2.7365546226501465 15.203044891357422
Loss :  1.5377285480499268 2.6835596561431885 14.955526351928711
Loss :  1.5072640180587769 3.143294334411621 17.223735809326172
Loss :  1.5489176511764526 2.5364010334014893 14.23092269897461
Loss :  1.5970438718795776 2.281186819076538 13.002978324890137
Loss :  1.5288995504379272 3.3389222621917725 18.2235107421875
Loss :  1.618010401725769 2.942342519760132 16.329723358154297
Loss :  1.5343682765960693 2.340052843093872 13.23463249206543
Loss :  1.580788254737854 3.1987240314483643 17.57440757751465
Loss :  1.5738353729248047 2.478083372116089 13.964252471923828
Loss :  1.5455867052078247 2.1660661697387695 12.375917434692383
Loss :  1.5867805480957031 2.5766959190368652 14.470260620117188
Loss :  1.5305225849151611 4.128556728363037 22.173307418823242
Loss :  1.6206451654434204 3.4049057960510254 18.645174026489258
Loss :  1.5422801971435547 2.6146888732910156 14.615724563598633
Loss :  1.5211204290390015 2.297499656677246 13.008618354797363
Loss :  1.5425662994384766 3.3331475257873535 18.208303451538086
Loss :  1.6287444829940796 2.4230473041534424 13.743980407714844
  batch 60 loss: 1.6287444829940796, 2.4230473041534424, 13.743980407714844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.5205020904541016 2.3074843883514404 13.057924270629883
Loss :  1.562920093536377 3.106666326522827 17.09625244140625
Loss :  1.5324238538742065 3.9548888206481934 21.306867599487305
Loss :  1.5162204504013062 2.646491050720215 14.748675346374512
Loss :  1.4868011474609375 2.0508365631103516 11.740983963012695
Loss :  1.5420358180999756 3.844031810760498 20.762195587158203
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.560275912284851 3.7292773723602295 20.206663131713867
Loss :  1.5588488578796387 3.7052552700042725 20.085124969482422
Loss :  1.5465439558029175 3.594730854034424 19.52019691467285
Total LOSS train 15.317647215036246 valid 20.143545150756836
CE LOSS train 1.5497957688111526 valid 0.38663598895072937
Contrastive LOSS train 2.7535702998821554 valid 0.898682713508606
EPOCH 106:
Loss :  1.5786782503128052 2.8564682006835938 15.861019134521484
Loss :  1.5936288833618164 2.594189405441284 14.564576148986816
Loss :  1.553521752357483 2.837106943130493 15.739056587219238
Loss :  1.5642396211624146 3.780879497528076 20.468637466430664
Loss :  1.5869442224502563 2.4610185623168945 13.892037391662598
Loss :  1.5355174541473389 2.5196030139923096 14.133532524108887
Loss :  1.5890203714370728 3.173189401626587 17.454967498779297
Loss :  1.5528308153152466 3.6691839694976807 19.898752212524414
Loss :  1.5377732515335083 3.5835509300231934 19.455528259277344
Loss :  1.580814242362976 2.521214008331299 14.186883926391602
Loss :  1.5210579633712769 2.821598768234253 15.62905216217041
Loss :  1.52291738986969 2.9362664222717285 16.204248428344727
Loss :  1.517807960510254 2.7497665882110596 15.266640663146973
Loss :  1.5256646871566772 3.1470184326171875 17.260757446289062
Loss :  1.6085799932479858 2.6813313961029053 15.015236854553223
Loss :  1.596711277961731 2.55078387260437 14.350630760192871
Loss :  1.516751766204834 2.946521282196045 16.249359130859375
Loss :  1.5535845756530762 2.9586784839630127 16.34697723388672
Loss :  1.5114829540252686 2.7491724491119385 15.257345199584961
Loss :  1.6022580862045288 2.766936779022217 15.436942100524902
  batch 20 loss: 1.6022580862045288, 2.766936779022217, 15.436942100524902
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.5477373600006104 2.6463611125946045 14.779542922973633
Loss :  1.506450891494751 2.814772367477417 15.580312728881836
Loss :  1.5324022769927979 2.5626254081726074 14.345529556274414
Loss :  1.5522931814193726 2.5170068740844727 14.137327194213867
Loss :  1.5946416854858398 2.6026015281677246 14.607649803161621
Loss :  1.5332245826721191 2.7283403873443604 15.1749267578125
Loss :  1.5492634773254395 2.6181600093841553 14.640064239501953
Loss :  1.5425686836242676 2.6248369216918945 14.666753768920898
Loss :  1.4706754684448242 2.6882781982421875 14.912066459655762
Loss :  1.5918359756469727 2.7188127040863037 15.18589973449707
Loss :  1.4745590686798096 2.9484269618988037 16.216693878173828
Loss :  1.5716888904571533 2.93627667427063 16.25307273864746
Loss :  1.5364760160446167 2.8894946575164795 15.983949661254883
Loss :  1.534803032875061 3.283980369567871 17.9547061920166
Loss :  1.487492322921753 3.5751428604125977 19.363204956054688
Loss :  1.508694052696228 2.4772837162017822 13.895112037658691
Loss :  1.5060685873031616 2.236678123474121 12.689458847045898
Loss :  1.589404821395874 2.6536624431610107 14.857717514038086
Loss :  1.5979220867156982 2.591383695602417 14.554841041564941
Loss :  1.6129255294799805 2.4496588706970215 13.861220359802246
  batch 40 loss: 1.6129255294799805, 2.4496588706970215, 13.861220359802246
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.555907964706421 3.0961673259735107 17.036745071411133
Loss :  1.5399173498153687 3.3128151893615723 18.103992462158203
Loss :  1.5255964994430542 3.7008254528045654 20.02972412109375
Loss :  1.5403778553009033 2.9186995029449463 16.133874893188477
Loss :  1.5104002952575684 2.388632297515869 13.453561782836914
Loss :  1.5510700941085815 2.579428195953369 14.448210716247559
Loss :  1.600046157836914 2.42289137840271 13.714503288269043
Loss :  1.5322668552398682 2.743511438369751 15.249823570251465
Loss :  1.6210236549377441 3.362032175064087 18.431184768676758
Loss :  1.537519931793213 2.2832963466644287 12.954002380371094
Loss :  1.5850496292114258 3.5342092514038086 19.25609588623047
Loss :  1.5749566555023193 2.6881144046783447 15.015528678894043
Loss :  1.5461798906326294 3.0485620498657227 16.788990020751953
Loss :  1.584863543510437 2.8772733211517334 15.971230506896973
Loss :  1.5331039428710938 3.173591136932373 17.401060104370117
Loss :  1.618269443511963 3.16221284866333 17.429332733154297
Loss :  1.541521430015564 2.9059112071990967 16.071077346801758
Loss :  1.5217286348342896 2.434692621231079 13.695191383361816
Loss :  1.5439374446868896 2.771810293197632 15.402989387512207
Loss :  1.6304147243499756 2.653378963470459 14.897310256958008
  batch 60 loss: 1.6304147243499756, 2.653378963470459, 14.897310256958008
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.5207051038742065 2.7376105785369873 15.208758354187012
Loss :  1.5630824565887451 2.847576379776001 15.80096435546875
Loss :  1.5309875011444092 3.3136682510375977 18.099327087402344
Loss :  1.516584038734436 3.010317325592041 16.56817054748535
Loss :  1.4863296747207642 2.1615452766418457 12.294055938720703
Loss :  1.5439729690551758 4.042272567749023 21.75533676147461
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.558807611465454 4.099477767944336 22.056196212768555
Loss :  1.5601108074188232 3.9794631004333496 21.457426071166992
Loss :  1.5445243120193481 3.862320899963379 20.856128692626953
Total LOSS train 15.781352879450871 valid 21.531271934509277
CE LOSS train 1.5508116043530977 valid 0.38613107800483704
Contrastive LOSS train 2.846108238513653 valid 0.9655802249908447
EPOCH 107:
Loss :  1.5812888145446777 2.6919891834259033 15.041234970092773
Loss :  1.5975728034973145 2.869675874710083 15.945951461791992
Loss :  1.5550624132156372 2.6150617599487305 14.63037109375
Loss :  1.5683172941207886 2.6167073249816895 14.651853561401367
Loss :  1.5916121006011963 2.5869603157043457 14.526412963867188
Loss :  1.5430066585540771 2.7710204124450684 15.398109436035156
Loss :  1.5939189195632935 3.1453452110290527 17.320646286010742
Loss :  1.558545470237732 3.0879948139190674 16.998519897460938
Loss :  1.5443035364151 2.969496965408325 16.391788482666016
Loss :  1.589975357055664 2.9709577560424805 16.44476318359375
Loss :  1.527714490890503 2.8755180835723877 15.905304908752441
Loss :  1.5269718170166016 3.0362634658813477 16.708290100097656
Loss :  1.5224658250808716 3.118847608566284 17.1167049407959
Loss :  1.5300853252410889 2.9060699939727783 16.060434341430664
Loss :  1.6135873794555664 2.838794708251953 15.807560920715332
Loss :  1.5983612537384033 2.71004056930542 15.148564338684082
Loss :  1.5180059671401978 2.583263874053955 14.434325218200684
Loss :  1.5527126789093018 2.9540603160858154 16.323015213012695
Loss :  1.5115995407104492 2.3358418941497803 13.19080924987793
Loss :  1.6017321348190308 2.4868109226226807 14.035786628723145
  batch 20 loss: 1.6017321348190308, 2.4868109226226807, 14.035786628723145
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.5491671562194824 3.247337818145752 17.785856246948242
Loss :  1.509331226348877 3.4612934589385986 18.815797805786133
Loss :  1.5362560749053955 3.063900947570801 16.85576057434082
Loss :  1.5576798915863037 2.733508586883545 15.225223541259766
Loss :  1.5984575748443604 2.860612630844116 15.901520729064941
Loss :  1.537431001663208 2.605738878250122 14.56612491607666
Loss :  1.5536291599273682 2.551962375640869 14.313440322875977
Loss :  1.5466238260269165 2.747103452682495 15.28214168548584
Loss :  1.474145770072937 2.838789224624634 15.668091773986816
Loss :  1.5946708917617798 2.563326835632324 14.41130542755127
Loss :  1.4764198064804077 2.4989171028137207 13.971004486083984
Loss :  1.5724869966506958 2.11254620552063 12.135217666625977
Loss :  1.538773775100708 3.08628511428833 16.970199584960938
Loss :  1.5353747606277466 2.48663330078125 13.968541145324707
Loss :  1.4888099431991577 3.294389009475708 17.96075439453125
Loss :  1.5096818208694458 3.788524866104126 20.452306747436523
Loss :  1.506362795829773 2.8607802391052246 15.810264587402344
Loss :  1.5917580127716064 2.6316630840301514 14.750073432922363
Loss :  1.5983275175094604 2.3190226554870605 13.193441390991211
Loss :  1.6127413511276245 2.2814478874206543 13.019981384277344
  batch 40 loss: 1.6127413511276245, 2.2814478874206543, 13.019981384277344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.5543991327285767 3.119096279144287 17.14988136291504
Loss :  1.534780502319336 3.024686098098755 16.65821075439453
Loss :  1.5221880674362183 2.437462568283081 13.709501266479492
Loss :  1.537896752357483 2.6319189071655273 14.697491645812988
Loss :  1.5082893371582031 2.3811240196228027 13.413909912109375
Loss :  1.5516058206558228 2.7601206302642822 15.352209091186523
Loss :  1.5990967750549316 2.4138081073760986 13.668136596679688
Loss :  1.5296669006347656 2.60758900642395 14.567611694335938
Loss :  1.620322346687317 2.804668426513672 15.643664360046387
Loss :  1.5370069742202759 4.450087547302246 23.787445068359375
Loss :  1.5847949981689453 3.827390432357788 20.72174644470215
Loss :  1.5765559673309326 2.724616289138794 15.199637413024902
Loss :  1.5478222370147705 2.632819652557373 14.711920738220215
Loss :  1.5890260934829712 2.98166561126709 16.49735450744629
Loss :  1.5310038328170776 2.6327016353607178 14.694512367248535
Loss :  1.6257160902023315 2.654547691345215 14.898454666137695
Loss :  1.5428729057312012 3.2049648761749268 17.567697525024414
Loss :  1.5218206644058228 2.4256591796875 13.650116920471191
Loss :  1.5420109033584595 3.146843194961548 17.276226043701172
Loss :  1.6296671628952026 2.9437594413757324 16.348464965820312
  batch 60 loss: 1.6296671628952026, 2.9437594413757324, 16.348464965820312
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.520752191543579 2.798374891281128 15.512626647949219
Loss :  1.5631316900253296 2.5503814220428467 14.315038681030273
Loss :  1.5330851078033447 2.2512331008911133 12.789250373840332
Loss :  1.5151642560958862 2.7884106636047363 15.4572172164917
Loss :  1.4864141941070557 2.6523008346557617 14.747918128967285
Loss :  1.5357825756072998 4.353262424468994 23.302093505859375
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5524026155471802 4.310450553894043 23.104656219482422
Loss :  1.549375057220459 4.305852890014648 23.07863998413086
Loss :  1.548043131828308 4.223163604736328 22.663860321044922
Total LOSS train 15.633442145127516 valid 23.037312507629395
CE LOSS train 1.55261630828564 valid 0.387010782957077
Contrastive LOSS train 2.8161651574648343 valid 1.055790901184082
EPOCH 108:
Loss :  1.582206130027771 4.032183647155762 21.743125915527344
Loss :  1.5974417924880981 3.7305474281311035 20.250179290771484
Loss :  1.5554312467575073 2.436373472213745 13.737298965454102
Loss :  1.5660473108291626 2.2840218544006348 12.986156463623047
Loss :  1.589172601699829 2.487982988357544 14.02908706665039
Loss :  1.5381994247436523 3.4525811672210693 18.801105499267578
Loss :  1.5933493375778198 3.667229175567627 19.929494857788086
Loss :  1.5570127964019775 3.090409994125366 17.009063720703125
Loss :  1.5425552129745483 2.275918483734131 12.922147750854492
Loss :  1.5888770818710327 2.2705883979797363 12.941818237304688
Loss :  1.5296165943145752 2.5339152812957764 14.199193000793457
Loss :  1.5296549797058105 3.482119560241699 18.94025421142578
Loss :  1.5256845951080322 2.8103551864624023 15.577460289001465
Loss :  1.5337374210357666 2.7583041191101074 15.325258255004883
Loss :  1.615596055984497 2.6129727363586426 14.680459022521973
Loss :  1.60405695438385 2.5805139541625977 14.506627082824707
Loss :  1.5220955610275269 2.67386794090271 14.891435623168945
Loss :  1.5590747594833374 2.5288264751434326 14.203207015991211
Loss :  1.516737699508667 2.7055463790893555 15.044469833374023
Loss :  1.606906771659851 3.8951544761657715 21.082679748535156
  batch 20 loss: 1.606906771659851, 3.8951544761657715, 21.082679748535156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5559757947921753 3.09625506401062 17.037250518798828
Loss :  1.517669916152954 2.8499574661254883 15.767457008361816
Loss :  1.5448774099349976 2.381955862045288 13.454656600952148
Loss :  1.5657124519348145 2.9342360496520996 16.236892700195312
Loss :  1.604630947113037 2.7840664386749268 15.52496337890625
Loss :  1.5430858135223389 2.70931339263916 15.089653015136719
Loss :  1.5566082000732422 2.708505392074585 15.099135398864746
Loss :  1.5483571290969849 2.7256622314453125 15.176668167114258
Loss :  1.4746955633163452 2.794734001159668 15.448365211486816
Loss :  1.5963270664215088 2.8429322242736816 15.81098747253418
Loss :  1.4757057428359985 2.739366054534912 15.172536849975586
Loss :  1.5718190670013428 2.731397867202759 15.228808403015137
Loss :  1.5355803966522217 2.653371572494507 14.802438735961914
Loss :  1.5330091714859009 3.407576322555542 18.570890426635742
Loss :  1.4865050315856934 3.824932813644409 20.611169815063477
Loss :  1.508211374282837 2.784991502761841 15.433168411254883
Loss :  1.5066449642181396 2.527316093444824 14.14322566986084
Loss :  1.5945916175842285 2.431225061416626 13.750717163085938
Loss :  1.6008471250534058 2.7593111991882324 15.397403717041016
Loss :  1.6158313751220703 2.5322701930999756 14.277182579040527
  batch 40 loss: 1.6158313751220703, 2.5322701930999756, 14.277182579040527
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.5600935220718384 2.9831342697143555 16.475765228271484
Loss :  1.5422983169555664 2.696861505508423 15.026605606079102
Loss :  1.52881920337677 2.3767271041870117 13.412454605102539
Loss :  1.5470304489135742 3.061704397201538 16.855552673339844
Loss :  1.5174914598464966 2.7268807888031006 15.151895523071289
Loss :  1.5598409175872803 2.6242711544036865 14.681197166442871
Loss :  1.6068615913391113 2.5953550338745117 14.583637237548828
Loss :  1.5374515056610107 2.5871667861938477 14.473285675048828
Loss :  1.628032922744751 2.651262044906616 14.884343147277832
Loss :  1.5461102724075317 2.188857316970825 12.490396499633789
Loss :  1.594861388206482 2.930919647216797 16.249460220336914
Loss :  1.5874546766281128 3.0850436687469482 17.012672424316406
Loss :  1.5588293075561523 2.269350528717041 12.9055814743042
Loss :  1.5957412719726562 2.3715028762817383 13.453255653381348
Loss :  1.5393198728561401 2.740591287612915 15.242276191711426
Loss :  1.627177119255066 2.313448429107666 13.194418907165527
Loss :  1.5486427545547485 2.9226298332214355 16.161792755126953
Loss :  1.527604579925537 3.8845672607421875 20.950441360473633
Loss :  1.5480725765228271 2.979072093963623 16.44343376159668
Loss :  1.6343413591384888 2.72767972946167 15.272740364074707
  batch 60 loss: 1.6343413591384888, 2.72767972946167, 15.272740364074707
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.5261988639831543 2.5443789958953857 14.24809455871582
Loss :  1.5686817169189453 2.3951830863952637 13.544597625732422
Loss :  1.5422134399414062 2.560929298400879 14.3468599319458
Loss :  1.527405023574829 2.845151662826538 15.75316333770752
Loss :  1.5007890462875366 2.4403603076934814 13.702589988708496
Loss :  1.5500184297561646 4.344778060913086 23.273908615112305
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5648815631866455 4.333169460296631 23.230730056762695
Loss :  1.5634245872497559 4.145808696746826 22.292469024658203
Loss :  1.556005835533142 4.251321792602539 22.81261444091797
Total LOSS train 15.559240077092097 valid 22.902430534362793
CE LOSS train 1.5567923637536856 valid 0.3890014588832855
Contrastive LOSS train 2.800489517358633 valid 1.0628304481506348
EPOCH 109:
Loss :  1.5936285257339478 2.5764737129211426 14.475996971130371
Loss :  1.609087586402893 2.9338998794555664 16.278587341308594
Loss :  1.5682965517044067 2.7359843254089355 15.248218536376953
Loss :  1.5773866176605225 2.664553165435791 14.900152206420898
Loss :  1.5979065895080566 1.9402416944503784 11.299114227294922
Loss :  1.548298716545105 2.2875428199768066 12.98601245880127
Loss :  1.5987043380737305 3.2093567848205566 17.645488739013672
Loss :  1.5631752014160156 2.816495180130005 15.645650863647461
Loss :  1.5510398149490356 2.8555715084075928 15.828897476196289
Loss :  1.598230242729187 2.3699934482574463 13.448197364807129
Loss :  1.5394794940948486 2.7917532920837402 15.498246192932129
Loss :  1.5375782251358032 2.8615636825561523 15.845396995544434
Loss :  1.5339884757995605 2.660942554473877 14.838701248168945
Loss :  1.5424559116363525 3.653862714767456 19.811769485473633
Loss :  1.6223801374435425 4.018570423126221 21.71523094177246
Loss :  1.612317681312561 2.4188361167907715 13.706499099731445
Loss :  1.532100796699524 2.675605535507202 14.910128593444824
Loss :  1.5692178010940552 2.734105348587036 15.239744186401367
Loss :  1.5265483856201172 2.802941083908081 15.541254043579102
Loss :  1.613662838935852 2.658790111541748 14.907613754272461
  batch 20 loss: 1.613662838935852, 2.658790111541748, 14.907613754272461
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5627985000610352 2.400017023086548 13.562883377075195
Loss :  1.5250294208526611 2.5429883003234863 14.239970207214355
Loss :  1.5506279468536377 2.357773542404175 13.339495658874512
Loss :  1.5713995695114136 2.3676440715789795 13.40962028503418
Loss :  1.6107712984085083 3.033351421356201 16.777528762817383
Loss :  1.5543986558914185 3.0748565196990967 16.928682327270508
Loss :  1.5688482522964478 2.865882396697998 15.898261070251465
Loss :  1.5608680248260498 2.895203113555908 16.036882400512695
Loss :  1.4925116300582886 2.3113934993743896 13.049479484558105
Loss :  1.6070053577423096 2.5047831535339355 14.130921363830566
Loss :  1.4946069717407227 2.43392276763916 13.664220809936523
Loss :  1.586094617843628 3.2781057357788086 17.97662353515625
Loss :  1.5536469221115112 2.58880352973938 14.497664451599121
Loss :  1.5519566535949707 3.462714910507202 18.865530014038086
Loss :  1.507611632347107 3.0010223388671875 16.512723922729492
Loss :  1.5286496877670288 2.4199297428131104 13.628297805786133
Loss :  1.5267236232757568 2.508539915084839 14.06942367553711
Loss :  1.6062458753585815 3.7692224979400635 20.45235824584961
Loss :  1.6139475107192993 2.972853660583496 16.47821617126465
Loss :  1.6278760433197021 3.4489057064056396 18.872404098510742
  batch 40 loss: 1.6278760433197021, 3.4489057064056396, 18.872404098510742
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.575774073600769 2.817034959793091 15.660948753356934
Loss :  1.5589076280593872 2.433013677597046 13.723976135253906
Loss :  1.5444973707199097 2.6363916397094727 14.726455688476562
Loss :  1.5600495338439941 2.461204767227173 13.866073608398438
Loss :  1.531794548034668 2.490396022796631 13.98377513885498
Loss :  1.5699723958969116 2.5348527431488037 14.24423599243164
Loss :  1.6148698329925537 2.799539804458618 15.612568855285645
Loss :  1.54953932762146 2.5015718936920166 14.057398796081543
Loss :  1.6357706785202026 2.5471487045288086 14.371514320373535
Loss :  1.5559940338134766 2.656931161880493 14.840649604797363
Loss :  1.6015537977218628 2.7716104984283447 15.459606170654297
Loss :  1.591454029083252 3.7987630367279053 20.585268020629883
Loss :  1.562330961227417 2.3833725452423096 13.479193687438965
Loss :  1.601225733757019 2.483754873275757 14.020000457763672
Loss :  1.5473802089691162 2.7802302837371826 15.448532104492188
Loss :  1.6357371807098389 2.559072971343994 14.43110179901123
Loss :  1.5591769218444824 3.0756125450134277 16.937240600585938
Loss :  1.5381306409835815 2.5461792945861816 14.269026756286621
Loss :  1.55793035030365 2.6639516353607178 14.87768840789795
Loss :  1.6416884660720825 2.2047946453094482 12.665661811828613
  batch 60 loss: 1.6416884660720825, 2.2047946453094482, 12.665661811828613
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.5365134477615356 2.3381547927856445 13.227287292480469
Loss :  1.577623963356018 2.357959032058716 13.367419242858887
Loss :  1.549403429031372 2.107041597366333 12.084610939025879
Loss :  1.5338046550750732 3.1278371810913086 17.172990798950195
Loss :  1.5074371099472046 2.4169111251831055 13.591992378234863
Loss :  1.5207792520523071 4.426252841949463 23.652042388916016
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5382354259490967 4.312017917633057 23.098323822021484
Loss :  1.5349465608596802 4.3027167320251465 23.04853057861328
Loss :  1.5311018228530884 4.164270877838135 22.352455139160156
Total LOSS train 15.213343165471004 valid 23.037837982177734
CE LOSS train 1.5673178837849544 valid 0.3827754557132721
Contrastive LOSS train 2.72920505633721 valid 1.0410677194595337
EPOCH 110:
Loss :  1.598201036453247 2.7009198665618896 15.102800369262695
Loss :  1.6123956441879272 2.823133945465088 15.72806453704834
Loss :  1.5721358060836792 2.436337471008301 13.753823280334473
Loss :  1.5818229913711548 2.452359914779663 13.843623161315918
Loss :  1.6035934686660767 2.4825439453125 14.016313552856445
Loss :  1.5546000003814697 3.3769900798797607 18.439550399780273
Loss :  1.6061145067214966 2.88152813911438 16.013755798339844
Loss :  1.571344256401062 2.6207590103149414 14.675139427185059
Loss :  1.5581434965133667 2.7514045238494873 15.315166473388672
Loss :  1.6028972864151 2.5736448764801025 14.471121788024902
Loss :  1.5438010692596436 2.9040818214416504 16.064208984375
Loss :  1.541613221168518 2.9492685794830322 16.28795623779297
Loss :  1.5376430749893188 3.555084228515625 19.313064575195312
Loss :  1.5462324619293213 3.255645275115967 17.824459075927734
Loss :  1.6252014636993408 4.280129432678223 23.025848388671875
Loss :  1.6169625520706177 3.8088669776916504 20.661296844482422
Loss :  1.5400623083114624 2.915205717086792 16.116090774536133
Loss :  1.577784538269043 2.9915506839752197 16.535537719726562
Loss :  1.5367902517318726 2.4890260696411133 13.98192024230957
Loss :  1.6233561038970947 2.7251765727996826 15.249238967895508
  batch 20 loss: 1.6233561038970947, 2.7251765727996826, 15.249238967895508
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5720489025115967 3.033510684967041 16.739601135253906
Loss :  1.5343047380447388 2.714977264404297 15.109190940856934
Loss :  1.5602407455444336 2.8078644275665283 15.599562644958496
Loss :  1.5778422355651855 2.5298893451690674 14.227289199829102
Loss :  1.6185095310211182 3.640936851501465 19.82319450378418
Loss :  1.559159517288208 2.8366963863372803 15.74264144897461
Loss :  1.5737344026565552 2.5641121864318848 14.394294738769531
Loss :  1.5655468702316284 2.7389299869537354 15.260196685791016
Loss :  1.4943090677261353 2.744326114654541 15.21593952178955
Loss :  1.611706256866455 3.047032594680786 16.84686851501465
Loss :  1.494641900062561 3.373419761657715 18.36174201965332
Loss :  1.5871766805648804 2.664252758026123 14.908440589904785
Loss :  1.5535612106323242 2.6801769733428955 14.954445838928223
Loss :  1.5499385595321655 2.5978972911834717 14.539424896240234
Loss :  1.5027868747711182 3.2211179733276367 17.60837745666504
Loss :  1.5238929986953735 2.9832980632781982 16.440383911132812
Loss :  1.5209360122680664 3.0042672157287598 16.54227066040039
Loss :  1.6064478158950806 3.7401034832000732 20.306964874267578
Loss :  1.6091735363006592 4.072546482086182 21.971904754638672
Loss :  1.6247271299362183 2.4322264194488525 13.785859107971191
  batch 40 loss: 1.6247271299362183, 2.4322264194488525, 13.785859107971191
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.5645756721496582 3.0648813247680664 16.88898277282715
Loss :  1.5445678234100342 2.950007915496826 16.294607162475586
Loss :  1.52695631980896 2.9693424701690674 16.373668670654297
Loss :  1.544793725013733 3.003723382949829 16.56340980529785
Loss :  1.5143052339553833 3.0040152072906494 16.534381866455078
Loss :  1.5583456754684448 2.8096821308135986 15.606756210327148
Loss :  1.6081559658050537 2.973947286605835 16.47789192199707
Loss :  1.5357158184051514 2.6277108192443848 14.674269676208496
Loss :  1.6308321952819824 2.647139072418213 14.866527557373047
Loss :  1.544336199760437 2.529343605041504 14.191054344177246
Loss :  1.5922958850860596 2.9561407566070557 16.37299919128418
Loss :  1.5845853090286255 2.5430192947387695 14.299681663513184
Loss :  1.5538933277130127 2.8293538093566895 15.700661659240723
Loss :  1.5969496965408325 3.035452365875244 16.774211883544922
Loss :  1.5311534404754639 2.8167529106140137 15.61491870880127
Loss :  1.6295887231826782 2.638044595718384 14.819811820983887
Loss :  1.5446780920028687 2.8430609703063965 15.75998306274414
Loss :  1.5215474367141724 2.5834572315216064 14.438833236694336
Loss :  1.5451712608337402 3.004103660583496 16.565689086914062
Loss :  1.63877534866333 2.544933557510376 14.363443374633789
  batch 60 loss: 1.63877534866333, 2.544933557510376, 14.363443374633789
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5245933532714844 2.901845693588257 16.03382110595703
Loss :  1.5661022663116455 2.427157402038574 13.701889038085938
Loss :  1.5366374254226685 2.6919760704040527 14.9965181350708
Loss :  1.518294334411621 2.7601373195648193 15.318981170654297
Loss :  1.487757682800293 2.491091728210449 13.943216323852539
Loss :  1.5311366319656372 3.9122087955474854 21.092182159423828
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.5498851537704468 3.8766539096832275 20.933155059814453
Loss :  1.5441746711730957 3.8801493644714355 20.944921493530273
Loss :  1.5586745738983154 3.726356029510498 20.190454483032227
Total LOSS train 16.030365899892953 valid 20.790178298950195
CE LOSS train 1.5651690574792716 valid 0.38966864347457886
Contrastive LOSS train 2.8930393842550424 valid 0.9315890073776245
EPOCH 111:
Loss :  1.5861172676086426 2.7756550312042236 15.464391708374023
Loss :  1.6035832166671753 2.7554702758789062 15.380934715270996
Loss :  1.5610463619232178 2.429739475250244 13.70974349975586
Loss :  1.5728192329406738 2.7717642784118652 15.431640625
Loss :  1.5969420671463013 2.5341007709503174 14.26744556427002
Loss :  1.5465301275253296 2.852900266647339 15.811031341552734
Loss :  1.6010754108428955 2.6846776008605957 15.024462699890137
Loss :  1.563504695892334 3.5435492992401123 19.281251907348633
Loss :  1.5497912168502808 3.6046557426452637 19.573070526123047
Loss :  1.5997053384780884 3.0425493717193604 16.81245231628418
Loss :  1.5343739986419678 3.2467422485351562 17.768085479736328
Loss :  1.5324654579162598 2.957355260848999 16.319242477416992
Loss :  1.5289580821990967 2.828366279602051 15.67078971862793
Loss :  1.5371328592300415 2.7768616676330566 15.421441078186035
Loss :  1.6220479011535645 3.1069676876068115 17.15688705444336
Loss :  1.610836386680603 3.021247148513794 16.717071533203125
Loss :  1.526560664176941 2.680917263031006 14.931147575378418
Loss :  1.5658725500106812 2.7996134757995605 15.563940048217773
Loss :  1.520715355873108 2.576416015625 14.402795791625977
Loss :  1.6140249967575073 2.948282480239868 16.355436325073242
  batch 20 loss: 1.6140249967575073, 2.948282480239868, 16.355436325073242
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.5587124824523926 2.8467233180999756 15.792329788208008
Loss :  1.5187309980392456 2.780410051345825 15.420781135559082
Loss :  1.5452381372451782 3.270232677459717 17.89640235900879
Loss :  1.5658750534057617 2.6652987003326416 14.89236831665039
Loss :  1.6075892448425293 3.5158143043518066 19.186660766601562
Loss :  1.5456081628799438 3.4439001083374023 18.76511001586914
Loss :  1.560199499130249 3.900573968887329 21.06307029724121
Loss :  1.549457311630249 2.749614953994751 15.297532081604004
Loss :  1.4726125001907349 2.6921496391296387 14.933361053466797
Loss :  1.5981318950653076 2.42610239982605 13.728644371032715
Loss :  1.473404884338379 3.457519054412842 18.761001586914062
Loss :  1.5742430686950684 3.1518452167510986 17.33346939086914
Loss :  1.5373425483703613 2.7280802726745605 15.177743911743164
Loss :  1.5349806547164917 2.682987928390503 14.949920654296875
Loss :  1.4838299751281738 2.905898094177246 16.013320922851562
Loss :  1.5071996450424194 2.6053640842437744 14.534019470214844
Loss :  1.5050724744796753 2.489931344985962 13.954729080200195
Loss :  1.5961248874664307 3.1028947830200195 17.110599517822266
Loss :  1.604759693145752 2.6266183853149414 14.737852096557617
Loss :  1.6199703216552734 2.6452183723449707 14.846061706542969
  batch 40 loss: 1.6199703216552734, 2.6452183723449707, 14.846061706542969
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.5595943927764893 2.7213351726531982 15.16627025604248
Loss :  1.5420541763305664 2.580620527267456 14.445157051086426
Loss :  1.5243146419525146 2.5540878772735596 14.294754028320312
Loss :  1.5430017709732056 3.1191279888153076 17.138641357421875
Loss :  1.5103387832641602 2.451537847518921 13.768028259277344
Loss :  1.5538352727890015 2.5577027797698975 14.3423490524292
Loss :  1.6043351888656616 2.8872649669647217 16.040660858154297
Loss :  1.529648780822754 2.395979404449463 13.50954532623291
Loss :  1.6261520385742188 2.8668298721313477 15.960301399230957
Loss :  1.5373671054840088 2.6621310710906982 14.8480224609375
Loss :  1.5863720178604126 2.6802146434783936 14.987445831298828
Loss :  1.5793377161026 2.939046621322632 16.27457046508789
Loss :  1.5494779348373413 2.6324703693389893 14.71182918548584
Loss :  1.5925796031951904 2.801703691482544 15.60109806060791
Loss :  1.5310348272323608 2.3679540157318115 13.370804786682129
Loss :  1.6274876594543457 2.1412458419799805 12.333717346191406
Loss :  1.5455340147018433 3.0643463134765625 16.867265701293945
Loss :  1.5240293741226196 2.3777353763580322 13.41270637512207
Loss :  1.5484167337417603 2.756586790084839 15.331351280212402
Loss :  1.6390447616577148 2.8173654079437256 15.725872039794922
  batch 60 loss: 1.6390447616577148, 2.8173654079437256, 15.725872039794922
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.5252647399902344 2.4300925731658936 13.675727844238281
Loss :  1.5661603212356567 3.682260036468506 19.977460861206055
Loss :  1.5361428260803223 3.0325236320495605 16.698760986328125
Loss :  1.5177353620529175 2.7305219173431396 15.170345306396484
Loss :  1.4876924753189087 2.1306703090667725 12.141044616699219
Loss :  1.514944314956665 4.1303911209106445 22.166898727416992
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5331969261169434 4.035197734832764 21.709186553955078
Loss :  1.5270133018493652 4.029516220092773 21.67459487915039
Loss :  1.5440930128097534 3.962857961654663 21.358381271362305
Total LOSS train 15.711568788381724 valid 21.72726535797119
CE LOSS train 1.5572329099361713 valid 0.38602325320243835
Contrastive LOSS train 2.8308671437777004 valid 0.9907144904136658
EPOCH 112:
Loss :  1.586471676826477 2.618579864501953 14.679370880126953
Loss :  1.6028276681900024 3.2668864727020264 17.937259674072266
Loss :  1.5587577819824219 2.7611377239227295 15.364446640014648
Loss :  1.5693254470825195 2.5848844051361084 14.49374771118164
Loss :  1.592466115951538 2.551832437515259 14.351628303527832
Loss :  1.5425379276275635 2.5495359897613525 14.290217399597168
Loss :  1.5943318605422974 3.0069191455841064 16.62892723083496
Loss :  1.5577998161315918 2.4790139198303223 13.952869415283203
Loss :  1.5421476364135742 2.5299596786499023 14.191946029663086
Loss :  1.5908253192901611 2.7491486072540283 15.336567878723145
Loss :  1.530043601989746 3.153517961502075 17.29763412475586
Loss :  1.5300606489181519 2.6225359439849854 14.642740249633789
Loss :  1.5246998071670532 2.437000036239624 13.709700584411621
Loss :  1.5334089994430542 3.7894814014434814 20.480815887451172
Loss :  1.6162385940551758 2.607930898666382 14.655893325805664
Loss :  1.607844591140747 2.5322811603546143 14.26924991607666
Loss :  1.5258116722106934 2.529792070388794 14.174772262573242
Loss :  1.5640466213226318 3.0903706550598145 17.015899658203125
Loss :  1.519748330116272 2.9966623783111572 16.50305938720703
Loss :  1.611069917678833 2.7216684818267822 15.219411849975586
  batch 20 loss: 1.611069917678833, 2.7216684818267822, 15.219411849975586
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5553028583526611 2.698014974594116 15.045377731323242
Loss :  1.5142477750778198 2.7336058616638184 15.18227767944336
Loss :  1.540197491645813 3.8340399265289307 20.710397720336914
Loss :  1.562669038772583 2.952216386795044 16.32375144958496
Loss :  1.6054109334945679 2.8670408725738525 15.940614700317383
Loss :  1.546591877937317 2.586594343185425 14.47956371307373
Loss :  1.5620404481887817 2.746718406677246 15.295632362365723
Loss :  1.5559383630752563 2.7043261528015137 15.077569961547852
Loss :  1.480177640914917 2.7148590087890625 15.054472923278809
Loss :  1.603884220123291 3.2677738666534424 17.942752838134766
Loss :  1.4794373512268066 3.0926711559295654 16.942792892456055
Loss :  1.5779083967208862 3.825073719024658 20.703275680541992
Loss :  1.5408920049667358 3.2087793350219727 17.584787368774414
Loss :  1.538585901260376 2.893688201904297 16.00702667236328
Loss :  1.4873850345611572 2.8638293743133545 15.80653190612793
Loss :  1.5104628801345825 2.7986326217651367 15.503625869750977
Loss :  1.508562684059143 3.162381887435913 17.320472717285156
Loss :  1.5983201265335083 2.6280055046081543 14.738348007202148
Loss :  1.6051466464996338 2.5922036170959473 14.566164016723633
Loss :  1.6191813945770264 2.8001532554626465 15.619948387145996
  batch 40 loss: 1.6191813945770264, 2.8001532554626465, 15.619948387145996
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.5615041255950928 2.518261194229126 14.152810096740723
Loss :  1.5443195104599 3.1068801879882812 17.078720092773438
Loss :  1.5270730257034302 3.1002731323242188 17.028438568115234
Loss :  1.5441632270812988 2.4362308979034424 13.725317001342773
Loss :  1.5105621814727783 2.2499115467071533 12.760119438171387
Loss :  1.552962303161621 3.009587287902832 16.60089874267578
Loss :  1.6015958786010742 2.7046406269073486 15.124798774719238
Loss :  1.5284910202026367 2.353104591369629 13.294013977050781
Loss :  1.622072458267212 2.4692046642303467 13.968095779418945
Loss :  1.5350456237792969 3.9272751808166504 21.17142105102539
Loss :  1.5852333307266235 2.4969048500061035 14.069757461547852
Loss :  1.5767748355865479 2.305607795715332 13.104813575744629
Loss :  1.5471550226211548 2.3335835933685303 13.215073585510254
Loss :  1.5910335779190063 2.7523717880249023 15.352892875671387
Loss :  1.5328290462493896 3.2633867263793945 17.849761962890625
Loss :  1.6269869804382324 2.873380184173584 15.993888854980469
Loss :  1.5431509017944336 2.8851966857910156 15.969134330749512
Loss :  1.5200045108795166 3.419689893722534 18.618453979492188
Loss :  1.5416762828826904 2.784311056137085 15.463232040405273
Loss :  1.6332457065582275 2.42457914352417 13.756141662597656
  batch 60 loss: 1.6332457065582275, 2.42457914352417, 13.756141662597656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5193650722503662 2.894037961959839 15.989555358886719
Loss :  1.564326286315918 2.3755099773406982 13.441876411437988
Loss :  1.5351530313491821 2.3618547916412354 13.344427108764648
Loss :  1.5182892084121704 2.633702039718628 14.686800003051758
Loss :  1.4885234832763672 2.7980642318725586 15.47884464263916
Loss :  1.5518029928207397 3.646761655807495 19.785612106323242
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5683916807174683 3.610445737838745 19.620620727539062
Loss :  1.5717309713363647 3.524259328842163 19.19302749633789
Loss :  1.5495631694793701 3.631401538848877 19.70656967163086
Total LOSS train 15.635089698204627 valid 19.576457500457764
CE LOSS train 1.5560668266736544 valid 0.38739079236984253
Contrastive LOSS train 2.8158045805417573 valid 0.9078503847122192
EPOCH 113:
Loss :  1.5852606296539307 3.0369949340820312 16.770235061645508
Loss :  1.6009892225265503 2.6315901279449463 14.758939743041992
Loss :  1.5568650960922241 2.204941749572754 12.581573486328125
Loss :  1.5676076412200928 2.322161912918091 13.178417205810547
Loss :  1.5909510850906372 2.243880271911621 12.810352325439453
Loss :  1.5384355783462524 2.8038840293884277 15.557856559753418
Loss :  1.5917608737945557 3.412038564682007 18.651952743530273
Loss :  1.5535919666290283 2.0452558994293213 11.779870986938477
Loss :  1.5395748615264893 2.235708475112915 12.718116760253906
Loss :  1.5890597105026245 1.9647891521453857 11.413005828857422
Loss :  1.527358889579773 2.9020602703094482 16.037660598754883
Loss :  1.527733325958252 3.0085668563842773 16.570568084716797
Loss :  1.5242072343826294 2.92872953414917 16.167856216430664
Loss :  1.533215880393982 2.9678122997283936 16.372278213500977
Loss :  1.6167861223220825 2.838085651397705 15.80721378326416
Loss :  1.6061136722564697 2.7677383422851562 15.444805145263672
Loss :  1.525511622428894 2.7724640369415283 15.387831687927246
Loss :  1.5635799169540405 2.841665744781494 15.771907806396484
Loss :  1.5207716226577759 3.638556718826294 19.71355438232422
Loss :  1.6104185581207275 2.757654905319214 15.398693084716797
  batch 20 loss: 1.6104185581207275, 2.757654905319214, 15.398693084716797
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5604559183120728 2.3844223022460938 13.48256778717041
Loss :  1.5221000909805298 2.7599098682403564 15.321649551391602
Loss :  1.55014967918396 2.2518699169158936 12.809499740600586
Loss :  1.5709762573242188 2.7960081100463867 15.551016807556152
Loss :  1.610189437866211 2.895493745803833 16.087657928466797
Loss :  1.5523967742919922 3.184645652770996 17.475624084472656
Loss :  1.5680816173553467 2.563953161239624 14.387847900390625
Loss :  1.5591497421264648 2.4468393325805664 13.793346405029297
Loss :  1.486021876335144 2.560441017150879 14.288227081298828
Loss :  1.6035469770431519 3.0594482421875 16.900787353515625
Loss :  1.4850493669509888 3.0208024978637695 16.589061737060547
Loss :  1.5784255266189575 2.8741261959075928 15.949056625366211
Loss :  1.5437418222427368 2.9544119834899902 16.3158016204834
Loss :  1.540832757949829 2.729750394821167 15.189584732055664
Loss :  1.4945048093795776 3.0280187129974365 16.634597778320312
Loss :  1.5153847932815552 2.4526822566986084 13.778796195983887
Loss :  1.5140572786331177 2.4000954627990723 13.514533996582031
Loss :  1.599929690361023 2.960817575454712 16.404016494750977
Loss :  1.6091413497924805 3.613717794418335 19.677730560302734
Loss :  1.6248915195465088 2.2875053882598877 13.062417984008789
  batch 40 loss: 1.6248915195465088, 2.2875053882598877, 13.062417984008789
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.5676963329315186 2.9284346103668213 16.209869384765625
Loss :  1.5488660335540771 2.4577276706695557 13.837504386901855
Loss :  1.5339411497116089 2.4845378398895264 13.95663070678711
Loss :  1.5503190755844116 2.951448678970337 16.30756187438965
Loss :  1.5185048580169678 2.717194080352783 15.104475021362305
Loss :  1.5594775676727295 3.012911081314087 16.624032974243164
Loss :  1.6089143753051758 3.47969126701355 19.007369995117188
Loss :  1.5401045083999634 3.572068214416504 19.40044403076172
Loss :  1.6289255619049072 2.4074177742004395 13.666013717651367
Loss :  1.547485113143921 2.222500801086426 12.659989356994629
Loss :  1.5946848392486572 2.681577444076538 15.002572059631348
Loss :  1.5865634679794312 2.6716318130493164 14.944722175598145
Loss :  1.558452844619751 2.608724355697632 14.60207462310791
Loss :  1.598348617553711 2.4444291591644287 13.820494651794434
Loss :  1.5429410934448242 2.755093574523926 15.318408966064453
Loss :  1.630728006362915 2.5895934104919434 14.578695297241211
Loss :  1.5508928298950195 3.1949567794799805 17.525676727294922
Loss :  1.5271563529968262 2.5788967609405518 14.421640396118164
Loss :  1.5472261905670166 2.652414083480835 14.809296607971191
Loss :  1.635493516921997 2.3280704021453857 13.275845527648926
  batch 60 loss: 1.635493516921997, 2.3280704021453857, 13.275845527648926
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.524835228919983 2.9970669746398926 16.510169982910156
Loss :  1.5679221153259277 2.281282663345337 12.974334716796875
Loss :  1.538571834564209 2.3136372566223145 13.106758117675781
Loss :  1.5233092308044434 2.7220840454101562 15.133729934692383
Loss :  1.49468994140625 2.4095678329467773 13.542529106140137
Loss :  1.52633798122406 4.413946628570557 23.596071243286133
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5438264608383179 4.454353332519531 23.815593719482422
Loss :  1.5451470613479614 4.376340866088867 23.426851272583008
Loss :  1.5285873413085938 4.2828688621521 22.94293212890625
Total LOSS train 15.176113113990198 valid 23.445362091064453
CE LOSS train 1.5597672535822942 valid 0.38214683532714844
Contrastive LOSS train 2.723269194823045 valid 1.070717215538025
EPOCH 114:
Loss :  1.589550256729126 2.912238121032715 16.150741577148438
Loss :  1.6044901609420776 2.723459482192993 15.221787452697754
Loss :  1.5618966817855835 2.0711562633514404 11.917677879333496
Loss :  1.5718797445297241 3.0639851093292236 16.89180564880371
Loss :  1.5963984727859497 2.2241787910461426 12.717291831970215
Loss :  1.546671748161316 2.2259862422943115 12.676603317260742
Loss :  1.5985127687454224 2.3396172523498535 13.296598434448242
Loss :  1.5642216205596924 2.400081157684326 13.564627647399902
Loss :  1.5511071681976318 2.6074111461639404 14.588163375854492
Loss :  1.5995595455169678 2.1775424480438232 12.487272262573242
Loss :  1.5361895561218262 3.9028708934783936 21.05054473876953
Loss :  1.5357824563980103 2.6407651901245117 14.739608764648438
Loss :  1.533905267715454 3.592129945755005 19.49455451965332
Loss :  1.5422166585922241 2.2442009449005127 12.76322078704834
Loss :  1.6229047775268555 2.4704642295837402 13.975226402282715
Loss :  1.6123218536376953 2.495535135269165 14.089997291564941
Loss :  1.5341713428497314 2.428579092025757 13.677066802978516
Loss :  1.5703026056289673 2.63008713722229 14.720738410949707
Loss :  1.5303254127502441 2.0494158267974854 11.77740478515625
Loss :  1.6168310642242432 2.2485930919647217 12.859796524047852
  batch 20 loss: 1.6168310642242432, 2.2485930919647217, 12.859796524047852
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.566112756729126 2.272974729537964 12.930986404418945
Loss :  1.527344822883606 2.8181939125061035 15.618313789367676
Loss :  1.5541292428970337 2.3729193210601807 13.418725967407227
Loss :  1.573742389678955 2.5103421211242676 14.125452041625977
Loss :  1.6128734350204468 2.7528531551361084 15.3771390914917
Loss :  1.553177833557129 2.569017171859741 14.398263931274414
Loss :  1.5669058561325073 2.4118869304656982 13.626340866088867
Loss :  1.5580322742462158 2.627307891845703 14.694571495056152
Loss :  1.4875990152359009 2.7159814834594727 15.067506790161133
Loss :  1.6059008836746216 3.5877339839935303 19.544570922851562
Loss :  1.4906264543533325 2.6800038814544678 14.890645980834961
Loss :  1.5856410264968872 2.5437536239624023 14.30440902709961
Loss :  1.5514256954193115 2.3837602138519287 13.470227241516113
Loss :  1.5493817329406738 2.644639730453491 14.772581100463867
Loss :  1.5039732456207275 2.980344533920288 16.405696868896484
Loss :  1.5246446132659912 2.9220736026763916 16.135011672973633
Loss :  1.5224288702011108 2.9711861610412598 16.378358840942383
Loss :  1.6051589250564575 2.2451140880584717 12.830729484558105
Loss :  1.61379075050354 2.3246026039123535 13.23680305480957
Loss :  1.627441644668579 2.2191712856292725 12.723298072814941
  batch 40 loss: 1.627441644668579, 2.2191712856292725, 12.723298072814941
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.5719656944274902 2.666187286376953 14.902902603149414
Loss :  1.554882526397705 2.593214750289917 14.520956039428711
Loss :  1.541843295097351 2.377098560333252 13.427335739135742
Loss :  1.559061050415039 3.8566441535949707 20.842281341552734
Loss :  1.530282735824585 3.017606496810913 16.618314743041992
Loss :  1.5694504976272583 2.203056812286377 12.584733963012695
Loss :  1.6143419742584229 2.0829555988311768 12.029119491577148
Loss :  1.5498045682907104 2.7321083545684814 15.210346221923828
Loss :  1.6357405185699463 2.4478063583374023 13.874772071838379
Loss :  1.5581649541854858 2.299316167831421 13.0547456741333
Loss :  1.603325366973877 2.641319990158081 14.809925079345703
Loss :  1.5956017971038818 3.6262969970703125 19.727087020874023
Loss :  1.5671801567077637 2.8714964389801025 15.924661636352539
Loss :  1.6045770645141602 2.4657480716705322 13.933317184448242
Loss :  1.5490565299987793 3.066600799560547 16.882061004638672
Loss :  1.6338777542114258 2.326569080352783 13.266722679138184
Loss :  1.5596318244934082 2.510676383972168 14.113014221191406
Loss :  1.5409927368164062 2.5968480110168457 14.525232315063477
Loss :  1.562269687652588 2.8912644386291504 16.018590927124023
Loss :  1.646708607673645 2.828578472137451 15.78960132598877
  batch 60 loss: 1.646708607673645, 2.828578472137451, 15.78960132598877
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5434370040893555 2.768092632293701 15.38390064239502
Loss :  1.581900954246521 2.254812002182007 12.855960845947266
Loss :  1.5542415380477905 2.1844353675842285 12.476417541503906
Loss :  1.5379832983016968 2.8915205001831055 15.995585441589355
Loss :  1.5109963417053223 2.103306531906128 12.027528762817383
Loss :  1.551892876625061 4.097708702087402 22.040437698364258
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.5663788318634033 4.105928897857666 22.096023559570312
Loss :  1.5634043216705322 3.979753017425537 21.462169647216797
Loss :  1.5648953914642334 3.8372395038604736 20.7510929107666
Total LOSS train 14.667776547945463 valid 21.587430953979492
CE LOSS train 1.5673367555324849 valid 0.39122384786605835
Contrastive LOSS train 2.620087972054115 valid 0.9593098759651184
EPOCH 115:
Loss :  1.5974068641662598 3.0524027347564697 16.859420776367188
Loss :  1.6132285594940186 2.72506046295166 15.238531112670898
Loss :  1.5723263025283813 3.1720809936523438 17.43273162841797
Loss :  1.5825386047363281 2.9649288654327393 16.407182693481445
Loss :  1.606674313545227 2.498457908630371 14.098963737487793
Loss :  1.5607492923736572 2.4661362171173096 13.891429901123047
Loss :  1.6093969345092773 2.7877731323242188 15.548262596130371
Loss :  1.5789895057678223 2.4540138244628906 13.849058151245117
Loss :  1.567018747329712 2.3849399089813232 13.491718292236328
Loss :  1.6125882863998413 2.692854166030884 15.076858520507812
Loss :  1.553623914718628 3.0300674438476562 16.703960418701172
Loss :  1.5495328903198242 2.5330312252044678 14.214689254760742
Loss :  1.545088529586792 2.316265344619751 13.126415252685547
Loss :  1.5511744022369385 2.3695435523986816 13.39889144897461
Loss :  1.6272000074386597 2.7321667671203613 15.288033485412598
Loss :  1.6156892776489258 2.4866013526916504 14.04869556427002
Loss :  1.5390965938568115 2.4487557411193848 13.782875061035156
Loss :  1.5743992328643799 2.5612237453460693 14.380517959594727
Loss :  1.5359902381896973 2.8036444187164307 15.55421257019043
Loss :  1.6195088624954224 2.766558885574341 15.452302932739258
  batch 20 loss: 1.6195088624954224, 2.766558885574341, 15.452302932739258
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.5723365545272827 2.704360008239746 15.094136238098145
Loss :  1.5360782146453857 2.394921064376831 13.5106840133667
Loss :  1.5600535869598389 2.6017091274261475 14.568599700927734
Loss :  1.579232096672058 2.1043107509613037 12.100786209106445
Loss :  1.6170014142990112 3.282264232635498 18.028322219848633
Loss :  1.561257004737854 2.2947804927825928 13.035160064697266
Loss :  1.5757955312728882 2.5382015705108643 14.266802787780762
Loss :  1.5694851875305176 2.6419501304626465 14.77923583984375
Loss :  1.501717448234558 4.128635406494141 22.144893646240234
Loss :  1.612568974494934 2.5209081172943115 14.217109680175781
Loss :  1.5026928186416626 2.848719358444214 15.74629020690918
Loss :  1.5931367874145508 2.410590648651123 13.646090507507324
Loss :  1.5607786178588867 2.444208860397339 13.78182315826416
Loss :  1.5602362155914307 2.40826416015625 13.601556777954102
Loss :  1.5169041156768799 3.133169651031494 17.18275260925293
Loss :  1.5375335216522217 3.5393285751342773 19.234176635742188
Loss :  1.5348807573318481 2.9777979850769043 16.423871994018555
Loss :  1.6120309829711914 2.233997344970703 12.782017707824707
Loss :  1.61819326877594 2.513556480407715 14.185976028442383
Loss :  1.6298754215240479 2.7624351978302 15.44205093383789
  batch 40 loss: 1.6298754215240479, 2.7624351978302, 15.44205093383789
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5773179531097412 2.5286483764648438 14.220560073852539
Loss :  1.5580813884735107 2.223858594894409 12.677374839782715
Loss :  1.5449769496917725 2.464716672897339 13.868560791015625
Loss :  1.5593903064727783 4.017716407775879 21.647972106933594
Loss :  1.5295917987823486 2.330646276473999 13.182823181152344
Loss :  1.5678235292434692 2.4055018424987793 13.595333099365234
Loss :  1.6131044626235962 2.3951575756073 13.588892936706543
Loss :  1.54520583152771 2.3995399475097656 13.542905807495117
Loss :  1.6312510967254639 2.9938604831695557 16.600553512573242
Loss :  1.5530850887298584 3.0362002849578857 16.734086990356445
Loss :  1.5988261699676514 3.556569814682007 19.38167381286621
Loss :  1.589733600616455 2.6146655082702637 14.663061141967773
Loss :  1.5631154775619507 2.4277312755584717 13.70177173614502
Loss :  1.602594017982483 2.573564052581787 14.470415115356445
Loss :  1.54883873462677 3.12827205657959 17.19019889831543
Loss :  1.6347299814224243 3.558730125427246 19.428380966186523
Loss :  1.5589802265167236 2.384138345718384 13.479671478271484
Loss :  1.538320541381836 2.2041635513305664 12.559138298034668
Loss :  1.560612440109253 2.4591052532196045 13.856139183044434
Loss :  1.6477742195129395 2.774089813232422 15.51822280883789
  batch 60 loss: 1.6477742195129395, 2.774089813232422, 15.51822280883789
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.543352723121643 2.6707425117492676 14.897065162658691
Loss :  1.5832659006118774 3.421539306640625 18.690961837768555
Loss :  1.5561199188232422 2.512516498565674 14.118701934814453
Loss :  1.5431079864501953 2.508652925491333 14.086372375488281
Loss :  1.5172618627548218 2.1459052562713623 12.246788024902344
Loss :  1.56927490234375 3.817753314971924 20.65804100036621
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5851699113845825 3.8460710048675537 20.81552505493164
Loss :  1.5855445861816406 3.609339475631714 19.63224220275879
Loss :  1.5759528875350952 3.676623582839966 19.959070205688477
Total LOSS train 15.070226375873272 valid 20.26621961593628
CE LOSS train 1.5727764936593862 valid 0.3939882218837738
Contrastive LOSS train 2.699489978643564 valid 0.9191558957099915
EPOCH 116:
Loss :  1.6026190519332886 3.0768158435821533 16.986698150634766
Loss :  1.618624210357666 3.18245267868042 17.530887603759766
Loss :  1.5787123441696167 2.7919564247131348 15.538494110107422
Loss :  1.5887755155563354 3.0649375915527344 16.913463592529297
Loss :  1.609607458114624 2.978419065475464 16.5017032623291
Loss :  1.5620962381362915 2.594028949737549 14.532240867614746
Loss :  1.6082954406738281 2.6429290771484375 14.822940826416016
Loss :  1.5750051736831665 2.3536620140075684 13.343316078186035
Loss :  1.5621556043624878 2.261481761932373 12.8695650100708
Loss :  1.6077814102172852 2.76250958442688 15.420329093933105
Loss :  1.5491676330566406 2.9614298343658447 16.35631561279297
Loss :  1.5485259294509888 3.14530086517334 17.2750301361084
Loss :  1.5457910299301147 3.3745272159576416 18.418428421020508
Loss :  1.5529221296310425 3.004312753677368 16.574485778808594
Loss :  1.6293028593063354 2.8619771003723145 15.939188003540039
Loss :  1.6165392398834229 2.951939344406128 16.376235961914062
Loss :  1.5382596254348755 2.6435022354125977 14.755770683288574
Loss :  1.5718058347702026 2.7106499671936035 15.125055313110352
Loss :  1.5322903394699097 2.3635644912719727 13.350112915039062
Loss :  1.6177539825439453 3.0487444400787354 16.86147689819336
  batch 20 loss: 1.6177539825439453, 3.0487444400787354, 16.86147689819336
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.5692148208618164 2.3715922832489014 13.427176475524902
Loss :  1.534093976020813 2.6779417991638184 14.923803329467773
Loss :  1.560472011566162 2.518883466720581 14.154890060424805
Loss :  1.5807521343231201 2.5740933418273926 14.451218605041504
Loss :  1.6189950704574585 2.6917338371276855 15.077664375305176
Loss :  1.5613596439361572 2.543087959289551 14.276799201965332
Loss :  1.5755976438522339 2.314892292022705 13.15005874633789
Loss :  1.568529725074768 2.3386423587799072 13.261741638183594
Loss :  1.4988508224487305 2.3769936561584473 13.383818626403809
Loss :  1.6102620363235474 2.4810500144958496 14.015512466430664
Loss :  1.4970533847808838 2.564368724822998 14.318897247314453
Loss :  1.5872441530227661 2.5383822917938232 14.279155731201172
Loss :  1.5522997379302979 2.985934257507324 16.481971740722656
Loss :  1.5516188144683838 3.222519874572754 17.664216995239258
Loss :  1.5076708793640137 2.8760616779327393 15.887979507446289
Loss :  1.5276933908462524 2.6814024448394775 14.93470573425293
Loss :  1.526917815208435 2.5466866493225098 14.260350227355957
Loss :  1.6074225902557373 2.9948012828826904 16.58142852783203
Loss :  1.6157925128936768 2.409207344055176 13.661828994750977
Loss :  1.62895667552948 2.5750412940979004 14.504162788391113
  batch 40 loss: 1.62895667552948, 2.5750412940979004, 14.504162788391113
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5766929388046265 2.495197296142578 14.052679061889648
Loss :  1.5571837425231934 2.3460264205932617 13.287315368652344
Loss :  1.546148419380188 2.571666717529297 14.404481887817383
Loss :  1.5620496273040771 2.5558860301971436 14.341480255126953
Loss :  1.535197138786316 2.4180829524993896 13.625612258911133
Loss :  1.5748478174209595 2.8878610134124756 16.01415252685547
Loss :  1.61923086643219 2.7294211387634277 15.266337394714355
Loss :  1.5546832084655762 2.6092066764831543 14.600717544555664
Loss :  1.6353905200958252 2.625796318054199 14.764371871948242
Loss :  1.5591216087341309 2.7203562259674072 15.16090202331543
Loss :  1.6047745943069458 2.7793612480163574 15.501581192016602
Loss :  1.5955144166946411 2.8806076049804688 15.998552322387695
Loss :  1.567812204360962 3.1898863315582275 17.517244338989258
Loss :  1.6051570177078247 2.9137356281280518 16.17383575439453
Loss :  1.5539674758911133 2.4714176654815674 13.911055564880371
Loss :  1.6372095346450806 2.8695738315582275 15.985078811645508
Loss :  1.5620267391204834 2.8251655101776123 15.687854766845703
Loss :  1.5435363054275513 2.472651481628418 13.906793594360352
Loss :  1.5657082796096802 2.629622220993042 14.71381950378418
Loss :  1.6500247716903687 2.5617895126342773 14.458971977233887
  batch 60 loss: 1.6500247716903687, 2.5617895126342773, 14.458971977233887
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5492011308670044 2.795933246612549 15.5288667678833
Loss :  1.587893009185791 2.559519052505493 14.385488510131836
Loss :  1.5605262517929077 2.347745180130005 13.299251556396484
Loss :  1.5436813831329346 2.6822609901428223 14.954985618591309
Loss :  1.5147590637207031 2.327507972717285 13.152298927307129
Loss :  1.5527740716934204 4.289987564086914 23.00271224975586
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 5], device='cuda:0')
Loss :  1.568078637123108 4.3440704345703125 23.28843116760254
Loss :  1.5657835006713867 4.151006698608398 22.320816040039062
Loss :  1.5608962774276733 4.140707969665527 22.264436721801758
Total LOSS train 15.059736134455754 valid 22.719099044799805
CE LOSS train 1.573217953168429 valid 0.39022406935691833
Contrastive LOSS train 2.697303636257465 valid 1.0351769924163818
EPOCH 117:
Loss :  1.5997053384780884 2.5109968185424805 14.15468978881836
Loss :  1.6143301725387573 3.000361919403076 16.616140365600586
Loss :  1.5739384889602661 2.5234262943267822 14.191069602966309
Loss :  1.5851986408233643 2.5921101570129395 14.545748710632324
Loss :  1.6099412441253662 2.6531171798706055 14.875527381896973
Loss :  1.5641452074050903 2.822953701019287 15.678914070129395
Loss :  1.613581895828247 2.764658212661743 15.436872482299805
Loss :  1.5800496339797974 2.656616449356079 14.863131523132324
Loss :  1.5662341117858887 2.4501664638519287 13.817066192626953
Loss :  1.6107864379882812 2.4321281909942627 13.771427154541016
Loss :  1.5542149543762207 2.5876142978668213 14.492286682128906
Loss :  1.5540525913238525 2.4967362880706787 14.037734031677246
Loss :  1.5515402555465698 2.3654325008392334 13.378703117370605
Loss :  1.5588319301605225 2.409931182861328 13.608488082885742
Loss :  1.633467197418213 2.400869131088257 13.637813568115234
Loss :  1.623892903327942 2.7972512245178223 15.610148429870605
Loss :  1.5456373691558838 2.6140551567077637 14.615913391113281
Loss :  1.579232096672058 2.8628275394439697 15.893369674682617
Loss :  1.5385907888412476 3.139350652694702 17.23534393310547
Loss :  1.620745062828064 2.4706382751464844 13.973936080932617
  batch 20 loss: 1.620745062828064, 2.4706382751464844, 13.973936080932617
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.572041630744934 2.4301693439483643 13.722887992858887
Loss :  1.5376826524734497 2.684098482131958 14.958174705505371
Loss :  1.5645495653152466 2.651028871536255 14.819693565368652
Loss :  1.586032748222351 2.712667942047119 15.149372100830078
Loss :  1.6255569458007812 3.1751701831817627 17.501407623291016
Loss :  1.57045578956604 2.6198387145996094 14.669649124145508
Loss :  1.5830715894699097 2.927394151687622 16.220041275024414
Loss :  1.5744580030441284 3.2396891117095947 17.772903442382812
Loss :  1.5066276788711548 2.3698372840881348 13.355813980102539
Loss :  1.6176254749298096 2.514784812927246 14.191549301147461
Loss :  1.5076767206192017 3.395606517791748 18.48571014404297
Loss :  1.5972120761871338 3.1335792541503906 17.265108108520508
Loss :  1.5640233755111694 3.271310329437256 17.920576095581055
Loss :  1.5617543458938599 2.4148168563842773 13.635838508605957
Loss :  1.5171213150024414 2.9596824645996094 16.315532684326172
Loss :  1.5371990203857422 2.725093126296997 15.162664413452148
Loss :  1.535120964050293 2.6191210746765137 14.63072681427002
Loss :  1.615657925605774 2.3764641284942627 13.497978210449219
Loss :  1.6213959455490112 2.3721699714660645 13.482245445251465
Loss :  1.6347378492355347 2.9627888202667236 16.448680877685547
  batch 40 loss: 1.6347378492355347, 2.9627888202667236, 16.448680877685547
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5808539390563965 2.383361577987671 13.497661590576172
Loss :  1.5624150037765503 2.196584463119507 12.545337677001953
Loss :  1.5498095750808716 2.3578875064849854 13.33924674987793
Loss :  1.564697265625 2.275751829147339 12.943456649780273
Loss :  1.5383611917495728 2.2228128910064697 12.652425765991211
Loss :  1.5788847208023071 2.3057377338409424 13.107573509216309
Loss :  1.624813437461853 2.42390513420105 13.744338989257812
Loss :  1.5619440078735352 2.57257080078125 14.424798011779785
Loss :  1.6451743841171265 2.679053783416748 15.040443420410156
Loss :  1.566680908203125 2.899679660797119 16.065078735351562
Loss :  1.6093698740005493 2.9391493797302246 16.305116653442383
Loss :  1.5980463027954102 2.888925790786743 16.042675018310547
Loss :  1.569370150566101 2.539700984954834 14.267875671386719
Loss :  1.6054775714874268 3.139777898788452 17.304367065429688
Loss :  1.5550346374511719 3.09468412399292 17.02845573425293
Loss :  1.6378064155578613 2.9005088806152344 16.140350341796875
Loss :  1.5651265382766724 3.70526385307312 20.091445922851562
Loss :  1.5464013814926147 2.8206522464752197 15.649662017822266
Loss :  1.5663707256317139 3.1599338054656982 17.366039276123047
Loss :  1.6486842632293701 2.641019105911255 14.853779792785645
  batch 60 loss: 1.6486842632293701, 2.641019105911255, 14.853779792785645
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.5436115264892578 3.29709529876709 18.029088973999023
Loss :  1.5840778350830078 2.3919589519500732 13.543872833251953
Loss :  1.5580822229385376 2.263735771179199 12.876761436462402
Loss :  1.545670747756958 3.467970371246338 18.885522842407227
Loss :  1.520925760269165 1.8760418891906738 10.901134490966797
Loss :  1.5777029991149902 3.9397847652435303 21.276626586914062
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.5928152799606323 3.8974123001098633 21.079875946044922
Loss :  1.5902572870254517 3.7255661487579346 20.218088150024414
Loss :  1.5913658142089844 3.6740095615386963 19.961414337158203
Total LOSS train 15.081344428429237 valid 20.6340012550354
CE LOSS train 1.5774739742279054 valid 0.3978414535522461
Contrastive LOSS train 2.700774104778583 valid 0.9185023903846741
EPOCH 118:
Loss :  1.6064351797103882 2.8419535160064697 15.816202163696289
Loss :  1.6221437454223633 3.141944169998169 17.331863403320312
Loss :  1.5816556215286255 2.4519031047821045 13.841171264648438
Loss :  1.5915974378585815 2.8745579719543457 15.964386940002441
Loss :  1.6133190393447876 2.2166943550109863 12.69679069519043
Loss :  1.5665898323059082 2.9826438426971436 16.479808807373047
Loss :  1.61282479763031 3.3217251300811768 18.221450805664062
Loss :  1.5791419744491577 2.708378314971924 15.12103271484375
Loss :  1.5652498006820679 3.13977313041687 17.264116287231445
Loss :  1.6080306768417358 2.0718698501586914 11.967379570007324
Loss :  1.5507211685180664 2.723085641860962 15.166149139404297
Loss :  1.5504589080810547 2.622276782989502 14.661842346191406
Loss :  1.547500729560852 2.701565980911255 15.055330276489258
Loss :  1.5567445755004883 2.221904754638672 12.666268348693848
Loss :  1.6335067749023438 2.454016923904419 13.90359115600586
Loss :  1.6248245239257812 2.443927526473999 13.844462394714355
Loss :  1.5506154298782349 2.937222719192505 16.23672866821289
Loss :  1.5818201303482056 2.530975103378296 14.236696243286133
Loss :  1.543854832649231 2.2207541465759277 12.647625923156738
Loss :  1.6244573593139648 3.3865950107574463 18.55743408203125
  batch 20 loss: 1.6244573593139648, 3.3865950107574463, 18.55743408203125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5773519277572632 2.483851194381714 13.996607780456543
Loss :  1.5430879592895508 2.493298292160034 14.0095796585083
Loss :  1.5664702653884888 2.2982852458953857 13.057896614074707
Loss :  1.5864852666854858 2.4890692234039307 14.031830787658691
Loss :  1.6241730451583862 2.5047872066497803 14.148109436035156
Loss :  1.5701266527175903 2.4311914443969727 13.726083755493164
Loss :  1.5841777324676514 2.344576120376587 13.307058334350586
Loss :  1.5786148309707642 3.1940157413482666 17.54869270324707
Loss :  1.5128281116485596 3.537968635559082 19.20267105102539
Loss :  1.620644211769104 2.6331517696380615 14.78640365600586
Loss :  1.5127602815628052 2.5393643379211426 14.20958137512207
Loss :  1.6001535654067993 2.46450138092041 13.922660827636719
Loss :  1.568436861038208 3.5243430137634277 19.190153121948242
Loss :  1.5667637586593628 2.730717182159424 15.220349311828613
Loss :  1.5242865085601807 2.5029361248016357 14.03896713256836
Loss :  1.5427429676055908 2.4977455139160156 14.03147029876709
Loss :  1.5385042428970337 2.568316698074341 14.380087852478027
Loss :  1.6174931526184082 2.418163537979126 13.708311080932617
Loss :  1.621280550956726 2.909471273422241 16.168636322021484
Loss :  1.6334941387176514 2.834373950958252 15.805363655090332
  batch 40 loss: 1.6334941387176514, 2.834373950958252, 15.805363655090332
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5817651748657227 2.6407506465911865 14.785518646240234
Loss :  1.5675028562545776 2.688084363937378 15.007925033569336
Loss :  1.5549989938735962 2.9899046421051025 16.5045223236084
Loss :  1.5748149156570435 2.9451582431793213 16.30060577392578
Loss :  1.5483630895614624 2.602349042892456 14.560108184814453
Loss :  1.5856091976165771 2.86275315284729 15.899374961853027
Loss :  1.6303746700286865 2.6343493461608887 14.802122116088867
Loss :  1.5683188438415527 2.982865810394287 16.482648849487305
Loss :  1.6471062898635864 3.1067514419555664 17.180864334106445
Loss :  1.573974370956421 3.1579854488372803 17.363901138305664
Loss :  1.6112556457519531 3.280784845352173 18.015180587768555
Loss :  1.6031237840652466 3.1198575496673584 17.202411651611328
Loss :  1.5765979290008545 2.8600409030914307 15.876802444458008
Loss :  1.6125777959823608 2.9012179374694824 16.118667602539062
Loss :  1.5609488487243652 2.7403228282928467 15.262563705444336
Loss :  1.6428661346435547 2.5137875080108643 14.211803436279297
Loss :  1.5695134401321411 2.9548215866088867 16.3436222076416
Loss :  1.5503549575805664 2.804201602935791 15.571362495422363
Loss :  1.568930983543396 2.911025285720825 16.12405776977539
Loss :  1.650019645690918 2.88964581489563 16.098247528076172
  batch 60 loss: 1.650019645690918, 2.88964581489563, 16.098247528076172
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.549163579940796 2.7751011848449707 15.42466926574707
Loss :  1.5873557329177856 2.3427231311798096 13.300971031188965
Loss :  1.5614186525344849 2.410086154937744 13.611848831176758
Loss :  1.546046495437622 2.6254281997680664 14.673187255859375
Loss :  1.5210357904434204 2.4417734146118164 13.729903221130371
Loss :  1.5723403692245483 4.169631004333496 22.420495986938477
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5873323678970337 4.195310592651367 22.563884735107422
Loss :  1.5868562459945679 4.021123886108398 21.692476272583008
Loss :  1.5737125873565674 3.8994193077087402 21.070810317993164
Total LOSS train 15.240365175100473 valid 21.936916828155518
CE LOSS train 1.580698559834407 valid 0.39342814683914185
Contrastive LOSS train 2.731933322319618 valid 0.9748548269271851
EPOCH 119:
Loss :  1.608232855796814 3.048384666442871 16.850156784057617
Loss :  1.622191309928894 2.3435299396514893 13.33984088897705
Loss :  1.5836318731307983 2.3036787509918213 13.102025032043457
Loss :  1.5928763151168823 2.3612561225891113 13.39915657043457
Loss :  1.6137853860855103 2.078251600265503 12.005043983459473
Loss :  1.5677706003189087 2.3675713539123535 13.405627250671387
Loss :  1.614474892616272 3.4004673957824707 18.616811752319336
Loss :  1.5821830034255981 2.5825085639953613 14.494725227355957
Loss :  1.5695383548736572 2.5995755195617676 14.567415237426758
Loss :  1.6141382455825806 2.4742164611816406 13.985220909118652
Loss :  1.5582529306411743 2.8526110649108887 15.821309089660645
Loss :  1.5581600666046143 2.9885687828063965 16.501005172729492
Loss :  1.5550907850265503 2.1929919719696045 12.520051002502441
Loss :  1.5627418756484985 2.8002514839172363 15.56399917602539
Loss :  1.6358736753463745 2.8726165294647217 15.998956680297852
Loss :  1.624265432357788 2.3852782249450684 13.550657272338867
Loss :  1.5478739738464355 2.2519850730895996 12.80780029296875
Loss :  1.5811493396759033 2.4341790676116943 13.752044677734375
Loss :  1.5421876907348633 2.6719696521759033 14.9020357131958
Loss :  1.6239097118377686 2.2932448387145996 13.090134620666504
  batch 20 loss: 1.6239097118377686, 2.2932448387145996, 13.090134620666504
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.5779589414596558 2.6543638706207275 14.849778175354004
Loss :  1.5429610013961792 2.7601921558380127 15.343921661376953
Loss :  1.5673000812530518 2.613072395324707 14.632661819458008
Loss :  1.5868579149246216 2.7298426628112793 15.236071586608887
Loss :  1.6257668733596802 2.698707342147827 15.119303703308105
Loss :  1.5702701807022095 2.9601430892944336 16.37098503112793
Loss :  1.5842061042785645 2.4010746479034424 13.589578628540039
Loss :  1.576412558555603 3.104271650314331 17.09777069091797
Loss :  1.5103049278259277 2.627631902694702 14.64846420288086
Loss :  1.6206867694854736 2.8455400466918945 15.848386764526367
Loss :  1.5127407312393188 3.329106092453003 18.15827178955078
Loss :  1.5999561548233032 2.391523838043213 13.557575225830078
Loss :  1.5672404766082764 3.450180768966675 18.818143844604492
Loss :  1.56586492061615 2.5491557121276855 14.311643600463867
Loss :  1.5235347747802734 2.867323160171509 15.860150337219238
Loss :  1.5430279970169067 2.9534945487976074 16.310501098632812
Loss :  1.5399855375289917 2.9256787300109863 16.168378829956055
Loss :  1.6172033548355103 2.2704570293426514 12.969489097595215
Loss :  1.623816967010498 2.658242702484131 14.915031433105469
Loss :  1.6365792751312256 3.1368985176086426 17.32107162475586
  batch 40 loss: 1.6365792751312256, 3.1368985176086426, 17.32107162475586
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5842756032943726 2.5106539726257324 14.137545585632324
Loss :  1.5660470724105835 2.3236405849456787 13.184249877929688
Loss :  1.5542049407958984 2.4081099033355713 13.594754219055176
Loss :  1.569959044456482 2.292454719543457 13.032232284545898
Loss :  1.543125033378601 2.5963387489318848 14.524818420410156
Loss :  1.5804535150527954 2.6777350902557373 14.96912956237793
Loss :  1.6248440742492676 2.3988425731658936 13.619056701660156
Loss :  1.563231110572815 2.931302070617676 16.219741821289062
Loss :  1.6441301107406616 2.4485108852386475 13.88668441772461
Loss :  1.5710785388946533 2.703421115875244 15.088183403015137
Loss :  1.6129999160766602 2.296745777130127 13.096728324890137
Loss :  1.606840968132019 3.072394371032715 16.968812942504883
Loss :  1.5820000171661377 3.4965105056762695 19.064552307128906
Loss :  1.619265079498291 2.6509244441986084 14.87388801574707
Loss :  1.5675758123397827 2.657021999359131 14.852685928344727
Loss :  1.6501307487487793 2.377570152282715 13.537981033325195
Loss :  1.5778502225875854 2.7869417667388916 15.512558937072754
Loss :  1.5573419332504272 2.4387009143829346 13.750845909118652
Loss :  1.5770014524459839 3.0543034076690674 16.84851837158203
Loss :  1.655922532081604 2.6518564224243164 14.915205001831055
  batch 60 loss: 1.655922532081604, 2.6518564224243164, 14.915205001831055
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5567591190338135 3.2527925968170166 17.820722579956055
Loss :  1.5937520265579224 2.395052433013916 13.569013595581055
Loss :  1.5681092739105225 2.275696039199829 12.946589469909668
Loss :  1.5543051958084106 3.190049409866333 17.504552841186523
Loss :  1.5307074785232544 2.5110979080200195 14.086196899414062
Loss :  1.5758564472198486 4.0335373878479 21.74354362487793
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.5904619693756104 3.916023015975952 21.170576095581055
Loss :  1.587144374847412 3.821998357772827 20.69713592529297
Loss :  1.5904275178909302 3.6756272315979004 19.968563079833984
Total LOSS train 14.93856069124662 valid 20.894954681396484
CE LOSS train 1.582506379714379 valid 0.39760687947273254
Contrastive LOSS train 2.6712108575380764 valid 0.9189068078994751
EPOCH 120:
Loss :  1.6143028736114502 2.4554731845855713 13.891668319702148
Loss :  1.6302402019500732 2.623152494430542 14.746003150939941
Loss :  1.593164086341858 2.4938509464263916 14.062418937683105
Loss :  1.6017683744430542 2.7166874408721924 15.185205459594727
Loss :  1.6221485137939453 2.974271059036255 16.49350357055664
Loss :  1.5767865180969238 2.2696051597595215 12.924812316894531
Loss :  1.6199276447296143 3.5975334644317627 19.607595443725586
Loss :  1.588320016860962 3.5865671634674072 19.521156311035156
Loss :  1.5745199918746948 2.5193519592285156 14.171279907226562
Loss :  1.6182349920272827 2.564424753189087 14.44035816192627
Loss :  1.5664610862731934 2.982424020767212 16.478580474853516
Loss :  1.5644371509552002 2.5483057498931885 14.305965423583984
Loss :  1.5595802068710327 3.5231709480285645 19.175434112548828
Loss :  1.5658767223358154 2.5306012630462646 14.218883514404297
Loss :  1.638368844985962 3.0871946811676025 17.074342727661133
Loss :  1.6299653053283691 2.607323408126831 14.666582107543945
Loss :  1.556130051612854 2.302424430847168 13.068252563476562
Loss :  1.5895376205444336 2.6994717121124268 15.086895942687988
Loss :  1.5513274669647217 2.225120782852173 12.676931381225586
Loss :  1.6295719146728516 2.133493661880493 12.297039985656738
  batch 20 loss: 1.6295719146728516, 2.133493661880493, 12.297039985656738
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.5817697048187256 2.52775239944458 14.220531463623047
Loss :  1.5466978549957275 2.6392998695373535 14.743196487426758
Loss :  1.569229006767273 2.4227378368377686 13.682918548583984
Loss :  1.5871472358703613 3.260922908782959 17.891761779785156
Loss :  1.6237701177597046 2.689589023590088 15.071714401245117
Loss :  1.5678788423538208 2.7740373611450195 15.438065528869629
Loss :  1.581294059753418 2.446108341217041 13.811835289001465
Loss :  1.573791265487671 2.5565364360809326 14.356473922729492
Loss :  1.5081334114074707 2.6651294231414795 14.833780288696289
Loss :  1.6178282499313354 2.6520206928253174 14.877931594848633
Loss :  1.5101717710494995 3.548114776611328 19.25074577331543
Loss :  1.5974410772323608 2.5674216747283936 14.434549331665039
Loss :  1.5656921863555908 2.451373815536499 13.822561264038086
Loss :  1.5640119314193726 2.7638323307037354 15.383172988891602
Loss :  1.521456003189087 2.823408365249634 15.638497352600098
Loss :  1.5419272184371948 2.5951926708221436 14.517890930175781
Loss :  1.5410412549972534 2.847815752029419 15.780119895935059
Loss :  1.620017647743225 2.702613115310669 15.13308334350586
Loss :  1.625903606414795 2.460919141769409 13.930500030517578
Loss :  1.638593077659607 3.041537046432495 16.84627914428711
  batch 40 loss: 1.638593077659607, 3.041537046432495, 16.84627914428711
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.586161494255066 2.4918770790100098 14.045546531677246
Loss :  1.5680408477783203 2.4427390098571777 13.781736373901367
Loss :  1.5561708211898804 2.7798922061920166 15.455631256103516
Loss :  1.5721579790115356 2.709627866744995 15.1202974319458
Loss :  1.546665072441101 2.2908780574798584 13.001055717468262
Loss :  1.5839868783950806 2.7966740131378174 15.567357063293457
Loss :  1.6270813941955566 2.4028820991516113 13.641490936279297
Loss :  1.5675851106643677 3.6119613647460938 19.627391815185547
Loss :  1.6470931768417358 2.893263101577759 16.1134090423584
Loss :  1.575321078300476 4.213705062866211 22.64384651184082
Loss :  1.6187068223953247 4.054446697235107 21.890941619873047
Loss :  1.6121482849121094 2.6173627376556396 14.698962211608887
Loss :  1.5872856378555298 2.5774810314178467 14.474691390991211
Loss :  1.620593547821045 2.665924310684204 14.950214385986328
Loss :  1.570987343788147 3.0913360118865967 17.027667999267578
Loss :  1.64662766456604 2.4965145587921143 14.129199981689453
Loss :  1.5778908729553223 2.669480800628662 14.925294876098633
Loss :  1.5590364933013916 2.710549831390381 15.111785888671875
Loss :  1.5783841609954834 2.619126319885254 14.674015998840332
Loss :  1.655737042427063 2.408743381500244 13.699453353881836
  batch 60 loss: 1.655737042427063, 2.408743381500244, 13.699453353881836
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.5609338283538818 2.4548213481903076 13.835041046142578
Loss :  1.5973148345947266 3.4244492053985596 18.719560623168945
Loss :  1.570080280303955 2.503807783126831 14.089118957519531
Loss :  1.5559831857681274 2.5927059650421143 14.519513130187988
Loss :  1.5295742750167847 2.236445426940918 12.711801528930664
Loss :  1.5700989961624146 4.333113670349121 23.235668182373047
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5857996940612793 4.352786540985107 23.349733352661133
Loss :  1.5867259502410889 4.245335578918457 22.81340217590332
Loss :  1.5596543550491333 4.05597448348999 21.839527130126953
Total LOSS train 15.326362228393554 valid 22.809582710266113
CE LOSS train 1.5853232805545514 valid 0.3899135887622833
Contrastive LOSS train 2.748207792868981 valid 1.0139936208724976
EPOCH 121:
Loss :  1.6111053228378296 2.346268892288208 13.342449188232422
Loss :  1.625057578086853 2.607659339904785 14.66335391998291
Loss :  1.5862243175506592 2.3945367336273193 13.558908462524414
Loss :  1.5950199365615845 2.6181211471557617 14.685626029968262
Loss :  1.6167396306991577 3.089014768600464 17.061813354492188
Loss :  1.5713509321212769 2.623335123062134 14.688026428222656
Loss :  1.617215633392334 2.678131103515625 15.007871627807617
Loss :  1.585065245628357 2.0302109718322754 11.736119270324707
Loss :  1.5746572017669678 2.391937255859375 13.534343719482422
Loss :  1.6181172132492065 2.5489485263824463 14.362859725952148
Loss :  1.5641900300979614 2.959596872329712 16.36217498779297
Loss :  1.5647428035736084 2.485625982284546 13.992873191833496
Loss :  1.5618889331817627 3.2324204444885254 17.72399139404297
Loss :  1.5712122917175293 2.5683951377868652 14.413188934326172
Loss :  1.6428638696670532 2.679971218109131 15.042720794677734
Loss :  1.6327415704727173 2.450026512145996 13.882874488830566
Loss :  1.5614159107208252 2.9622223377227783 16.372528076171875
Loss :  1.5924705266952515 2.1644279956817627 12.414609909057617
Loss :  1.556027889251709 2.9872703552246094 16.492380142211914
Loss :  1.6325860023498535 2.5360493659973145 14.31283187866211
  batch 20 loss: 1.6325860023498535, 2.5360493659973145, 14.31283187866211
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5875976085662842 2.68442702293396 15.009733200073242
Loss :  1.5555280447006226 2.58321475982666 14.471601486206055
Loss :  1.5794463157653809 2.3917884826660156 13.538389205932617
Loss :  1.597733736038208 2.5200462341308594 14.197964668273926
Loss :  1.6340633630752563 2.720074415206909 15.23443603515625
Loss :  1.582313895225525 2.8874104022979736 16.019365310668945
Loss :  1.5962648391723633 2.564103603363037 14.416783332824707
Loss :  1.5896296501159668 2.587440013885498 14.526830673217773
Loss :  1.5260108709335327 3.5089950561523438 19.070985794067383
Loss :  1.6311142444610596 2.8442575931549072 15.852401733398438
Loss :  1.524692177772522 2.7246782779693604 15.148083686828613
Loss :  1.6077276468276978 2.8140220642089844 15.677838325500488
Loss :  1.5758769512176514 3.2346603870391846 17.749177932739258
Loss :  1.5745489597320557 3.424691915512085 18.698007583618164
Loss :  1.5324270725250244 2.96755313873291 16.370193481445312
Loss :  1.552261233329773 2.8179571628570557 15.642046928405762
Loss :  1.5498590469360352 3.0088448524475098 16.59408187866211
Loss :  1.624480128288269 2.431936025619507 13.784160614013672
Loss :  1.6299902200698853 2.6933088302612305 15.096534729003906
Loss :  1.6398237943649292 3.487550973892212 19.077579498291016
  batch 40 loss: 1.6398237943649292, 3.487550973892212, 19.077579498291016
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5876140594482422 3.211575746536255 17.645492553710938
Loss :  1.5714765787124634 2.045837640762329 11.800664901733398
Loss :  1.5585048198699951 2.7181308269500732 15.14915943145752
Loss :  1.5742707252502441 3.030428647994995 16.72641372680664
Loss :  1.5478192567825317 2.6865603923797607 14.980621337890625
Loss :  1.5848039388656616 2.2630081176757812 12.8998441696167
Loss :  1.627181887626648 3.5618362426757812 19.436363220214844
Loss :  1.5668894052505493 2.496136426925659 14.047572135925293
Loss :  1.6463688611984253 2.6558563709259033 14.925650596618652
Loss :  1.57515549659729 2.325324535369873 13.201778411865234
Loss :  1.6188746690750122 2.5850820541381836 14.54428482055664
Loss :  1.6108853816986084 2.7128424644470215 15.175098419189453
Loss :  1.5842187404632568 2.5719008445739746 14.443723678588867
Loss :  1.6197314262390137 2.857900381088257 15.909233093261719
Loss :  1.5707768201828003 2.426058292388916 13.701067924499512
Loss :  1.648662805557251 2.501650094985962 14.156912803649902
Loss :  1.5795482397079468 3.0601537227630615 16.88031768798828
Loss :  1.5609859228134155 3.2396905422210693 17.759437561035156
Loss :  1.5810562372207642 3.089202880859375 17.027070999145508
Loss :  1.657644271850586 2.4070489406585693 13.692889213562012
  batch 60 loss: 1.657644271850586, 2.4070489406585693, 13.692889213562012
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5601750612258911 2.4545395374298096 13.83287239074707
Loss :  1.5972954034805298 1.9671826362609863 11.433208465576172
Loss :  1.571498155593872 2.3093535900115967 13.118266105651855
Loss :  1.5578581094741821 2.5190861225128174 14.153288841247559
Loss :  1.5334964990615845 2.1918396949768066 12.492694854736328
Loss :  1.588123083114624 4.098057270050049 22.07840919494629
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.6005547046661377 4.025197982788086 21.726545333862305
Loss :  1.600296974182129 3.890293598175049 21.05176544189453
Loss :  1.598218321800232 3.6730780601501465 19.96360969543457
Total LOSS train 15.060948753356934 valid 21.205082416534424
CE LOSS train 1.588690390953651 valid 0.399554580450058
Contrastive LOSS train 2.6944516622103176 valid 0.9182695150375366
EPOCH 122:
Loss :  1.6165350675582886 2.89723801612854 16.102724075317383
Loss :  1.6323440074920654 2.653079032897949 14.89773941040039
Loss :  1.5947322845458984 2.550412893295288 14.346796989440918
Loss :  1.602738380432129 3.020733118057251 16.706403732299805
Loss :  1.6226048469543457 2.8652870655059814 15.949039459228516
Loss :  1.5759261846542358 2.3683226108551025 13.4175386428833
Loss :  1.6200904846191406 2.552098512649536 14.380582809448242
Loss :  1.5879772901535034 2.113198757171631 12.153971672058105
Loss :  1.576456904411316 2.5806884765625 14.479899406433105
Loss :  1.6188745498657227 2.335073947906494 13.294243812561035
Loss :  1.5650851726531982 2.9870731830596924 16.500450134277344
Loss :  1.56607186794281 2.513580560684204 14.133974075317383
Loss :  1.5623462200164795 2.223419189453125 12.679442405700684
Loss :  1.570378303527832 2.8153889179229736 15.647322654724121
Loss :  1.6413218975067139 2.535108804702759 14.316865921020508
Loss :  1.6317975521087646 2.3224666118621826 13.244131088256836
Loss :  1.562238335609436 3.3919408321380615 18.521942138671875
Loss :  1.5937427282333374 2.676331043243408 14.975397109985352
Loss :  1.5576679706573486 2.4699325561523438 13.907330513000488
Loss :  1.635535717010498 2.5424020290374756 14.347545623779297
  batch 20 loss: 1.635535717010498, 2.5424020290374756, 14.347545623779297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5912388563156128 2.8927299976348877 16.054887771606445
Loss :  1.5593562126159668 3.02278208732605 16.673267364501953
Loss :  1.5822898149490356 2.712787628173828 15.146227836608887
Loss :  1.599475622177124 2.501391649246216 14.106433868408203
Loss :  1.636208176612854 2.9838452339172363 16.55543327331543
Loss :  1.5837746858596802 2.70259428024292 15.096746444702148
Loss :  1.594868779182434 3.3707385063171387 18.448562622070312
Loss :  1.5877246856689453 2.4324328899383545 13.749889373779297
Loss :  1.5248080492019653 2.639644145965576 14.723029136657715
Loss :  1.6290040016174316 2.718539237976074 15.221700668334961
Loss :  1.5272858142852783 2.7082359790802 15.068465232849121
Loss :  1.6105891466140747 2.8935365676879883 16.078271865844727
Loss :  1.5828056335449219 2.806159257888794 15.613601684570312
Loss :  1.5809495449066162 2.985377550125122 16.507837295532227
Loss :  1.5413144826889038 3.5770576000213623 19.426603317260742
Loss :  1.5591695308685303 2.7734029293060303 15.42618465423584
Loss :  1.557068109512329 2.7365682125091553 15.239909172058105
Loss :  1.6301548480987549 2.5628838539123535 14.444573402404785
Loss :  1.635872721672058 2.491244316101074 14.092094421386719
Loss :  1.646476149559021 2.411670207977295 13.704827308654785
  batch 40 loss: 1.646476149559021, 2.411670207977295, 13.704827308654785
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.595284104347229 3.2880053520202637 18.035310745239258
Loss :  1.576972484588623 2.402118444442749 13.587564468383789
Loss :  1.5645289421081543 2.78348970413208 15.481977462768555
Loss :  1.5773755311965942 2.7128381729125977 15.141566276550293
Loss :  1.549729585647583 2.2926440238952637 13.01294994354248
Loss :  1.5846620798110962 2.4374144077301025 13.771734237670898
Loss :  1.6273611783981323 3.012219190597534 16.688457489013672
Loss :  1.5672962665557861 2.5456786155700684 14.295689582824707
Loss :  1.6476318836212158 2.6554815769195557 14.925039291381836
Loss :  1.5768340826034546 2.9875285625457764 16.514476776123047
Loss :  1.6204867362976074 2.5938374996185303 14.58967399597168
Loss :  1.6125346422195435 3.476783037185669 18.996450424194336
Loss :  1.5869172811508179 3.3022005558013916 18.09792137145996
Loss :  1.6213417053222656 3.029200792312622 16.767345428466797
Loss :  1.5723564624786377 2.8895976543426514 16.02034568786621
Loss :  1.649742603302002 2.9890475273132324 16.594980239868164
Loss :  1.5805461406707764 2.8738644123077393 15.949868202209473
Loss :  1.562756061553955 3.313478708267212 18.130149841308594
Loss :  1.5818984508514404 2.7398414611816406 15.281105995178223
Loss :  1.6611802577972412 2.8770241737365723 16.046300888061523
  batch 60 loss: 1.6611802577972412, 2.8770241737365723, 16.046300888061523
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5651251077651978 3.2549734115600586 17.83999252319336
Loss :  1.6021074056625366 2.6604583263397217 14.904398918151855
Loss :  1.575013279914856 2.350172519683838 13.325875282287598
Loss :  1.5604448318481445 2.4762303829193115 13.941596984863281
Loss :  1.5334430932998657 2.866248846054077 15.8646879196167
Loss :  1.5668333768844604 4.332559108734131 23.229629516601562
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3], device='cuda:0')
Loss :  1.5812184810638428 4.322510719299316 23.193771362304688
Loss :  1.5779304504394531 4.257717132568359 22.86651611328125
Loss :  1.5778898000717163 4.271235466003418 22.93406867980957
Total LOSS train 15.372113051781287 valid 23.055996417999268
CE LOSS train 1.5915149358602672 valid 0.3944724500179291
Contrastive LOSS train 2.756119625384991 valid 1.0678088665008545
EPOCH 123:
Loss :  1.6140700578689575 2.609483242034912 14.661486625671387
Loss :  1.6306730508804321 2.9355428218841553 16.308387756347656
Loss :  1.5918022394180298 2.990664482116699 16.545124053955078
Loss :  1.6001695394515991 3.732466459274292 20.262502670288086
Loss :  1.6210761070251465 2.163144588470459 12.436800003051758
Loss :  1.5745810270309448 2.651278495788574 14.830973625183105
Loss :  1.618569016456604 2.7634634971618652 15.435887336730957
Loss :  1.5868942737579346 2.344200849533081 13.30789852142334
Loss :  1.57423996925354 2.252506971359253 12.836774826049805
Loss :  1.6175121068954468 3.273740291595459 17.98621368408203
Loss :  1.5616862773895264 2.8568453788757324 15.845913887023926
Loss :  1.5629793405532837 3.086848497390747 16.997220993041992
Loss :  1.559297800064087 2.6124532222747803 14.621563911437988
Loss :  1.56768798828125 2.2435455322265625 12.785415649414062
Loss :  1.639788269996643 2.7239065170288086 15.259321212768555
Loss :  1.6321830749511719 2.508474826812744 14.174556732177734
Loss :  1.559159278869629 2.526606321334839 14.192191123962402
Loss :  1.5915776491165161 2.6040656566619873 14.611906051635742
Loss :  1.5539770126342773 2.9069008827209473 16.088481903076172
Loss :  1.6324492692947388 3.3552894592285156 18.408897399902344
  batch 20 loss: 1.6324492692947388, 3.3552894592285156, 18.408897399902344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5849828720092773 2.512991189956665 14.149938583374023
Loss :  1.551688551902771 2.831746816635132 15.71042251586914
Loss :  1.5751607418060303 2.9318220615386963 16.234270095825195
Loss :  1.5944600105285645 2.5877645015716553 14.533283233642578
Loss :  1.630551815032959 2.750319480895996 15.382148742675781
Loss :  1.5784097909927368 2.488600730895996 14.021413803100586
Loss :  1.5928659439086914 2.5835769176483154 14.510750770568848
Loss :  1.5869336128234863 2.9277398586273193 16.22563362121582
Loss :  1.5242035388946533 2.6441965103149414 14.745185852050781
Loss :  1.6289973258972168 3.89446759223938 21.101335525512695
Loss :  1.524031639099121 2.794133424758911 15.494698524475098
Loss :  1.606305480003357 2.5433738231658936 14.323174476623535
Loss :  1.5762490034103394 2.61228346824646 14.637666702270508
Loss :  1.5742981433868408 2.667569875717163 14.912147521972656
Loss :  1.5345613956451416 3.014773368835449 16.608428955078125
Loss :  1.5531257390975952 2.4121737480163574 13.613994598388672
Loss :  1.5515143871307373 2.7281363010406494 15.192195892333984
Loss :  1.6267426013946533 3.1743264198303223 17.498374938964844
Loss :  1.6336115598678589 2.8017585277557373 15.642404556274414
Loss :  1.6451692581176758 2.51401948928833 14.215266227722168
  batch 40 loss: 1.6451692581176758, 2.51401948928833, 14.215266227722168
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5955748558044434 2.778125762939453 15.486204147338867
Loss :  1.5788171291351318 2.238811492919922 12.77287483215332
Loss :  1.5664342641830444 3.024805784225464 16.69046401977539
Loss :  1.5810372829437256 2.446441173553467 13.813243865966797
Loss :  1.5559298992156982 2.182422399520874 12.468042373657227
Loss :  1.5940808057785034 3.16035795211792 17.395870208740234
Loss :  1.6377674341201782 3.377326250076294 18.524398803710938
Loss :  1.5782757997512817 2.669314384460449 14.924847602844238
Loss :  1.653861165046692 2.735515832901001 15.331439971923828
Loss :  1.5847386121749878 2.406690835952759 13.618192672729492
Loss :  1.625958800315857 2.4797730445861816 14.024823188781738
Loss :  1.6184202432632446 2.2521297931671143 12.879069328308105
Loss :  1.5935190916061401 2.404531955718994 13.616178512573242
Loss :  1.6282835006713867 3.317101001739502 18.213787078857422
Loss :  1.581412434577942 2.444079637527466 13.801810264587402
Loss :  1.6553804874420166 2.771195888519287 15.511360168457031
Loss :  1.5896551609039307 3.2842206954956055 18.010759353637695
Loss :  1.5701396465301514 2.915041923522949 16.145349502563477
Loss :  1.5878881216049194 2.630767345428467 14.741724967956543
Loss :  1.6633871793746948 2.7918078899383545 15.622426986694336
  batch 60 loss: 1.6633871793746948, 2.7918078899383545, 15.622426986694336
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5703929662704468 2.832615375518799 15.733469009399414
Loss :  1.6051886081695557 2.973437786102295 16.47237777709961
Loss :  1.5759665966033936 2.7554943561553955 15.353438377380371
Loss :  1.5625442266464233 2.4183428287506104 13.654257774353027
Loss :  1.5372858047485352 1.8107479810714722 10.591025352478027
Loss :  1.582975149154663 4.298676013946533 23.07635498046875
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5957746505737305 4.336272239685059 23.277137756347656
Loss :  1.5954707860946655 4.223941326141357 22.715177536010742
Loss :  1.5895984172821045 4.224703311920166 22.71311378479004
Total LOSS train 15.257656757648174 valid 22.945446014404297
CE LOSS train 1.5916334904157199 valid 0.3973996043205261
Contrastive LOSS train 2.733204638040983 valid 1.0561758279800415
EPOCH 124:
Loss :  1.6185503005981445 2.9285900592803955 16.26150131225586
Loss :  1.633504033088684 2.677236318588257 15.019685745239258
Loss :  1.593017578125 2.7982890605926514 15.584463119506836
Loss :  1.601625680923462 2.717366933822632 15.188460350036621
Loss :  1.6207547187805176 2.2239773273468018 12.740640640258789
Loss :  1.5756397247314453 2.6710526943206787 14.930903434753418
Loss :  1.6207869052886963 2.963493585586548 16.438255310058594
Loss :  1.591088891029358 2.7105460166931152 15.143819808959961
Loss :  1.5810250043869019 3.0083653926849365 16.622852325439453
Loss :  1.6231086254119873 2.635307550430298 14.799646377563477
Loss :  1.5694869756698608 2.6841206550598145 14.990089416503906
Loss :  1.5703468322753906 3.0026395320892334 16.583545684814453
Loss :  1.5650584697723389 2.4971940517425537 14.051029205322266
Loss :  1.5710153579711914 2.489044427871704 14.016237258911133
Loss :  1.6401361227035522 3.3337326049804688 18.308799743652344
Loss :  1.631472110748291 2.682811975479126 15.0455322265625
Loss :  1.5603381395339966 2.732163429260254 15.221155166625977
Loss :  1.5934985876083374 2.582491874694824 14.50595760345459
Loss :  1.5579744577407837 2.6745126247406006 14.930538177490234
Loss :  1.6354001760482788 2.380195379257202 13.536376953125
  batch 20 loss: 1.6354001760482788, 2.380195379257202, 13.536376953125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5898306369781494 3.0144505500793457 16.66208267211914
Loss :  1.5560885667800903 2.7561442852020264 15.336810111999512
Loss :  1.5785307884216309 3.0771894454956055 16.9644775390625
Loss :  1.5961753129959106 2.7254412174224854 15.223381042480469
Loss :  1.6312620639801025 3.0115647315979004 16.689085006713867
Loss :  1.5772223472595215 2.497689962387085 14.065671920776367
Loss :  1.5908242464065552 2.50788950920105 14.130271911621094
Loss :  1.5847424268722534 2.4570698738098145 13.870091438293457
Loss :  1.5224082469940186 2.9519035816192627 16.281925201416016
Loss :  1.6285916566848755 3.0915417671203613 17.086299896240234
Loss :  1.524227261543274 2.9154012203216553 16.101234436035156
Loss :  1.6066722869873047 2.605740547180176 14.635375022888184
Loss :  1.5767523050308228 2.4978246688842773 14.065876007080078
Loss :  1.5746259689331055 3.0706939697265625 16.928096771240234
Loss :  1.53255033493042 3.339689016342163 18.230995178222656
Loss :  1.55221426486969 3.9830334186553955 21.46738052368164
Loss :  1.5512475967407227 2.7940516471862793 15.521506309509277
Loss :  1.627286672592163 2.8817570209503174 16.03607177734375
Loss :  1.634545922279358 2.8417296409606934 15.843194961547852
Loss :  1.6471426486968994 2.714888095855713 15.221582412719727
  batch 40 loss: 1.6471426486968994, 2.714888095855713, 15.221582412719727
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5968316793441772 3.087986946105957 17.036766052246094
Loss :  1.5791754722595215 3.9133427143096924 21.145889282226562
Loss :  1.564193606376648 2.838054656982422 15.754467010498047
Loss :  1.5779602527618408 2.6920180320739746 15.038050651550293
Loss :  1.5500074625015259 2.5059168338775635 14.079591751098633
Loss :  1.5858803987503052 3.235553026199341 17.76364517211914
Loss :  1.626495599746704 3.176818370819092 17.510587692260742
Loss :  1.5673161745071411 3.0084621906280518 16.60962677001953
Loss :  1.6444603204727173 2.7685353755950928 15.487137794494629
Loss :  1.5753883123397827 2.5763213634490967 14.456995010375977
Loss :  1.6171480417251587 2.6317527294158936 14.775912284851074
Loss :  1.6109575033187866 3.061650514602661 16.91921043395996
Loss :  1.5842643976211548 2.891151189804077 16.040019989013672
Loss :  1.620295524597168 2.884113073348999 16.040861129760742
Loss :  1.5677732229232788 2.7816975116729736 15.4762601852417
Loss :  1.6455692052841187 2.8599133491516113 15.945135116577148
Loss :  1.574159860610962 2.79311203956604 15.539719581604004
Loss :  1.5535811185836792 2.2746307849884033 12.926734924316406
Loss :  1.5719624757766724 3.046588897705078 16.804906845092773
Loss :  1.6512376070022583 2.7854487895965576 15.578481674194336
  batch 60 loss: 1.6512376070022583, 2.7854487895965576, 15.578481674194336
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.5529463291168213 2.986358880996704 16.4847412109375
Loss :  1.5923211574554443 3.4642930030822754 18.913785934448242
Loss :  1.565100073814392 2.1405537128448486 12.267868041992188
Loss :  1.551857352256775 2.695964813232422 15.031681060791016
Loss :  1.5264430046081543 2.1088063716888428 12.070474624633789
Loss :  1.574962854385376 4.233771324157715 22.743820190429688
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5902869701385498 4.289364814758301 23.037109375
Loss :  1.5924491882324219 4.106979846954346 22.127347946166992
Loss :  1.574401617050171 4.109757900238037 22.123191833496094
Total LOSS train 15.691991541935847 valid 22.507867336273193
CE LOSS train 1.5890784061872043 valid 0.3936004042625427
Contrastive LOSS train 2.820582628250122 valid 1.0274394750595093
EPOCH 125:
Loss :  1.6108644008636475 2.3962113857269287 13.59192180633545
Loss :  1.626509189605713 2.4562928676605225 13.907974243164062
Loss :  1.5877677202224731 2.1821095943450928 12.498315811157227
Loss :  1.5963478088378906 3.144676923751831 17.319732666015625
Loss :  1.618888258934021 2.3428094387054443 13.332935333251953
Loss :  1.572489857673645 2.4071731567382812 13.608355522155762
Loss :  1.618933081626892 2.485992670059204 14.048895835876465
Loss :  1.5870729684829712 2.891993284225464 16.047039031982422
Loss :  1.5753108263015747 2.7759015560150146 15.454818725585938
Loss :  1.6188260316848755 2.7024827003479004 15.13123893737793
Loss :  1.5646097660064697 2.9981353282928467 16.555286407470703
Loss :  1.5652494430541992 3.2085275650024414 17.607887268066406
Loss :  1.5597151517868042 2.6071410179138184 14.595420837402344
Loss :  1.5664814710617065 3.080730676651001 16.970134735107422
Loss :  1.6381556987762451 3.367133855819702 18.47382354736328
Loss :  1.6271029710769653 2.4121339321136475 13.687772750854492
Loss :  1.5555832386016846 2.652409076690674 14.817627906799316
Loss :  1.5887250900268555 2.73140287399292 15.245739936828613
Loss :  1.5515161752700806 2.953343391418457 16.318233489990234
Loss :  1.6293110847473145 3.3753809928894043 18.506216049194336
  batch 20 loss: 1.6293110847473145, 3.3753809928894043, 18.506216049194336
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.5840411186218262 2.2205097675323486 12.686590194702148
Loss :  1.5499577522277832 3.8151438236236572 20.62567710876465
Loss :  1.5736382007598877 2.4009087085723877 13.578181266784668
Loss :  1.5928446054458618 2.6678731441497803 14.932210922241211
Loss :  1.6284990310668945 2.5971057415008545 14.614027976989746
Loss :  1.5765388011932373 2.4937798976898193 14.045438766479492
Loss :  1.5887260437011719 2.530771017074585 14.242581367492676
Loss :  1.5821231603622437 2.2113096714019775 12.638670921325684
Loss :  1.5163294076919556 2.2943601608276367 12.988130569458008
Loss :  1.6244614124298096 2.7635483741760254 15.4422025680542
Loss :  1.5169003009796143 3.0055959224700928 16.544879913330078
Loss :  1.6028649806976318 2.907744884490967 16.141590118408203
Loss :  1.5708481073379517 3.265522003173828 17.89845848083496
Loss :  1.5668578147888184 2.44238018989563 13.778759002685547
Loss :  1.5226514339447021 2.9329073429107666 16.18718719482422
Loss :  1.5428653955459595 2.2000458240509033 12.543094635009766
Loss :  1.5412967205047607 2.4390788078308105 13.73669147491455
Loss :  1.6186606884002686 2.4638612270355225 13.937967300415039
Loss :  1.6265959739685059 3.4182095527648926 18.71764373779297
Loss :  1.6386030912399292 3.7396249771118164 20.336727142333984
  batch 40 loss: 1.6386030912399292, 3.7396249771118164, 20.336727142333984
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5867348909378052 3.08048152923584 16.98914337158203
Loss :  1.569665789604187 2.1412413120269775 12.275872230529785
Loss :  1.555850625038147 2.564398765563965 14.37784481048584
Loss :  1.571013331413269 2.4970362186431885 14.056194305419922
Loss :  1.5424121618270874 2.15944766998291 12.33965015411377
Loss :  1.5792601108551025 2.8431642055511475 15.79508113861084
Loss :  1.6233881711959839 2.6972670555114746 15.109724044799805
Loss :  1.5610486268997192 2.6116814613342285 14.619455337524414
Loss :  1.642026424407959 3.2791733741760254 18.037893295288086
Loss :  1.5667253732681274 2.0901238918304443 12.017345428466797
Loss :  1.6098335981369019 2.779679775238037 15.508233070373535
Loss :  1.6019480228424072 2.4393160343170166 13.798527717590332
Loss :  1.5766345262527466 2.381054162979126 13.481904983520508
Loss :  1.6155192852020264 2.7332606315612793 15.28182315826416
Loss :  1.563218593597412 2.8619537353515625 15.872987747192383
Loss :  1.6454671621322632 3.04225492477417 16.85674285888672
Loss :  1.5702580213546753 3.192157506942749 17.53104591369629
Loss :  1.54732084274292 2.5612692832946777 14.353668212890625
Loss :  1.565073847770691 2.9765541553497314 16.447843551635742
Loss :  1.6471909284591675 2.8177826404571533 15.736104011535645
  batch 60 loss: 1.6471909284591675, 2.8177826404571533, 15.736104011535645
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.5448538064956665 2.4704508781433105 13.897109031677246
Loss :  1.5855000019073486 2.316065549850464 13.165827751159668
Loss :  1.5581977367401123 2.234302282333374 12.72970962524414
Loss :  1.5446548461914062 2.9022228717803955 16.055768966674805
Loss :  1.5190377235412598 2.354130506515503 13.289690017700195
Loss :  1.5592939853668213 4.319718837738037 23.157888412475586
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.5754486322402954 4.299393177032471 23.07241439819336
Loss :  1.5728044509887695 4.27981424331665 22.971874237060547
Loss :  1.5750621557235718 4.202719211578369 22.58865737915039
Total LOSS train 15.12248111137977 valid 22.94770860671997
CE LOSS train 1.5833476726825422 valid 0.39376553893089294
Contrastive LOSS train 2.707826673067533 valid 1.0506798028945923
EPOCH 126:
Loss :  1.606911301612854 2.7917978763580322 15.565900802612305
Loss :  1.6228787899017334 2.912630796432495 16.186033248901367
Loss :  1.5819597244262695 2.651656150817871 14.840240478515625
Loss :  1.591288447380066 2.6283748149871826 14.733162879943848
Loss :  1.6120156049728394 2.3201329708099365 13.21268081665039
Loss :  1.5658655166625977 2.4133708477020264 13.632719993591309
Loss :  1.6132229566574097 2.6263461112976074 14.744954109191895
Loss :  1.5820192098617554 2.347973108291626 13.321884155273438
Loss :  1.5712273120880127 2.155211925506592 12.34728717803955
Loss :  1.6174290180206299 2.5708000659942627 14.471428871154785
Loss :  1.5607681274414062 2.864323139190674 15.882383346557617
Loss :  1.5598816871643066 3.190422534942627 17.511993408203125
Loss :  1.5557258129119873 2.699237585067749 15.05191421508789
Loss :  1.5628423690795898 2.896296977996826 16.044326782226562
Loss :  1.6356170177459717 2.615128993988037 14.711262702941895
Loss :  1.626082420349121 2.5588042736053467 14.420104026794434
Loss :  1.5521024465560913 2.448497772216797 13.794590950012207
Loss :  1.587158441543579 2.1010642051696777 12.092479705810547
Loss :  1.5500974655151367 2.6022183895111084 14.561189651489258
Loss :  1.6308727264404297 2.718026876449585 15.221007347106934
  batch 20 loss: 1.6308727264404297, 2.718026876449585, 15.221007347106934
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.583656668663025 2.4331932067871094 13.749622344970703
Loss :  1.5484201908111572 2.460956335067749 13.853201866149902
Loss :  1.5715023279190063 2.6249563694000244 14.696284294128418
Loss :  1.5903425216674805 2.285841226577759 13.019548416137695
Loss :  1.6280313730239868 2.886744260787964 16.061752319335938
Loss :  1.5754700899124146 2.7354986667633057 15.252963066101074
Loss :  1.5903679132461548 2.632009267807007 14.750414848327637
Loss :  1.5842721462249756 2.488548755645752 14.027015686035156
Loss :  1.520122766494751 2.9163830280303955 16.10203742980957
Loss :  1.6281684637069702 2.5246293544769287 14.251315116882324
Loss :  1.5211166143417358 2.7478551864624023 15.260392189025879
Loss :  1.6058179140090942 2.767723321914673 15.44443416595459
Loss :  1.5735571384429932 2.235653877258301 12.751826286315918
Loss :  1.570909023284912 2.3688268661499023 13.415042877197266
Loss :  1.528845191001892 2.6355929374694824 14.706809997558594
Loss :  1.5483555793762207 2.502492666244507 14.060819625854492
Loss :  1.5477783679962158 3.0585241317749023 16.84039878845215
Loss :  1.6248925924301147 2.619598388671875 14.722884178161621
Loss :  1.631831407546997 2.9319796562194824 16.291730880737305
Loss :  1.6434710025787354 2.9260878562927246 16.273910522460938
  batch 40 loss: 1.6434710025787354, 2.9260878562927246, 16.273910522460938
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.593821406364441 2.624023675918579 14.713939666748047
Loss :  1.577530860900879 2.070854425430298 11.931802749633789
Loss :  1.5661870241165161 2.2605159282684326 12.868766784667969
Loss :  1.5810446739196777 2.668649435043335 14.924291610717773
Loss :  1.5564602613449097 1.9760839939117432 11.436880111694336
Loss :  1.5938154458999634 2.851040840148926 15.849020004272461
Loss :  1.6357070207595825 2.3838870525360107 13.555142402648926
Loss :  1.5739742517471313 2.773858070373535 15.443264961242676
Loss :  1.6499894857406616 2.321742057800293 13.258699417114258
Loss :  1.5779484510421753 2.0964250564575195 12.060073852539062
Loss :  1.617416501045227 2.757202386856079 15.403428077697754
Loss :  1.6097025871276855 2.374070882797241 13.480056762695312
Loss :  1.584425926208496 2.444812297821045 13.808487892150879
Loss :  1.6210511922836304 2.774428367614746 15.493192672729492
Loss :  1.5735211372375488 2.2611539363861084 12.879291534423828
Loss :  1.6514568328857422 2.7885942459106445 15.594428062438965
Loss :  1.5833388566970825 2.7866127490997314 15.516402244567871
Loss :  1.5636932849884033 2.206007719039917 12.593731880187988
Loss :  1.5829232931137085 2.940497398376465 16.285409927368164
Loss :  1.658554196357727 2.85612154006958 15.93916130065918
  batch 60 loss: 1.658554196357727, 2.85612154006958, 15.93916130065918
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5613629817962646 3.1635098457336426 17.3789119720459
Loss :  1.5971812009811401 2.931279420852661 16.253578186035156
Loss :  1.5710381269454956 2.5180282592773438 14.161179542541504
Loss :  1.5578951835632324 2.622252941131592 14.669160842895508
Loss :  1.5344184637069702 2.127854347229004 12.173689842224121
Loss :  1.5769122838974 3.995821475982666 21.556018829345703
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.5908918380737305 3.968679189682007 21.434288024902344
Loss :  1.5879000425338745 3.930292844772339 21.239364624023438
Loss :  1.5911753177642822 3.8941826820373535 21.062088012695312
Total LOSS train 14.546952951871432 valid 21.3229398727417
CE LOSS train 1.5873439128582294 valid 0.39779382944107056
Contrastive LOSS train 2.591921810003427 valid 0.9735456705093384
EPOCH 127:
Loss :  1.6161099672317505 3.0823655128479004 17.027936935424805
Loss :  1.6322972774505615 2.489039421081543 14.077494621276855
Loss :  1.5960007905960083 2.7065224647521973 15.128612518310547
Loss :  1.6053386926651 3.2573142051696777 17.891910552978516
Loss :  1.6231380701065063 2.522320508956909 14.2347412109375
Loss :  1.5786924362182617 2.476074695587158 13.959065437316895
Loss :  1.6210055351257324 2.6877293586730957 15.059652328491211
Loss :  1.5894228219985962 2.3645741939544678 13.412294387817383
Loss :  1.5769038200378418 3.906033754348755 21.107072830200195
Loss :  1.617038607597351 2.2776496410369873 13.005287170410156
Loss :  1.562768578529358 3.083930492401123 16.982421875
Loss :  1.5599384307861328 2.9359395503997803 16.239635467529297
Loss :  1.556719422340393 4.062227725982666 21.867856979370117
Loss :  1.5641963481903076 4.163347244262695 22.380931854248047
Loss :  1.636759877204895 3.080990791320801 17.04171371459961
Loss :  1.6276566982269287 3.154604911804199 17.400680541992188
Loss :  1.5539323091506958 3.09291410446167 17.018503189086914
Loss :  1.5884921550750732 2.9320220947265625 16.24860191345215
Loss :  1.5503344535827637 2.8865087032318115 15.982877731323242
Loss :  1.6304821968078613 2.748206853866577 15.371517181396484
  batch 20 loss: 1.6304821968078613, 2.748206853866577, 15.371517181396484
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5810879468917847 2.563803195953369 14.400103569030762
Loss :  1.5448287725448608 2.4497454166412354 13.79355525970459
Loss :  1.5682201385498047 2.3339719772338867 13.238080024719238
Loss :  1.5859254598617554 3.0549466609954834 16.860658645629883
Loss :  1.6236470937728882 2.702693462371826 15.137114524841309
Loss :  1.5703974962234497 2.8226518630981445 15.683656692504883
Loss :  1.5849601030349731 2.933709144592285 16.25350570678711
Loss :  1.5780317783355713 2.7546792030334473 15.35142707824707
Loss :  1.5124142169952393 2.8598012924194336 15.811420440673828
Loss :  1.6217786073684692 2.726789712905884 15.25572681427002
Loss :  1.513655662536621 2.819605827331543 15.611684799194336
Loss :  1.6002967357635498 2.781390428543091 15.507248878479004
Loss :  1.5681822299957275 2.3987436294555664 13.56190013885498
Loss :  1.5649718046188354 3.1667864322662354 17.398902893066406
Loss :  1.5224394798278809 2.5781471729278564 14.413175582885742
Loss :  1.5402330160140991 2.5460875034332275 14.270669937133789
Loss :  1.5373249053955078 2.7348413467407227 15.211531639099121
Loss :  1.6154645681381226 2.453446626663208 13.882697105407715
Loss :  1.6210052967071533 2.273252248764038 12.987266540527344
Loss :  1.6339460611343384 3.057258129119873 16.920236587524414
  batch 40 loss: 1.6339460611343384, 3.057258129119873, 16.920236587524414
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.58291494846344 2.732736825942993 15.246599197387695
Loss :  1.567334532737732 2.9832301139831543 16.48348617553711
Loss :  1.5542362928390503 3.3669776916503906 18.389123916625977
Loss :  1.5715364217758179 2.7281737327575684 15.21240520477295
Loss :  1.543709635734558 2.566039562225342 14.373908042907715
Loss :  1.5818175077438354 2.766892910003662 15.416282653808594
Loss :  1.6246840953826904 3.151597023010254 17.38266944885254
Loss :  1.5610237121582031 2.669764995574951 14.909849166870117
Loss :  1.6407798528671265 2.579444169998169 14.538000106811523
Loss :  1.5672292709350586 3.2742035388946533 17.938247680664062
Loss :  1.6105278730392456 3.4898006916046143 19.05953025817871
Loss :  1.6033962965011597 2.4253172874450684 13.72998332977295
Loss :  1.5772141218185425 2.545766592025757 14.306047439575195
Loss :  1.6131500005722046 2.600338935852051 14.61484432220459
Loss :  1.5665831565856934 2.6651041507720947 14.89210319519043
Loss :  1.6422444581985474 2.582608699798584 14.555288314819336
Loss :  1.572598934173584 2.8815929889678955 15.98056411743164
Loss :  1.553855299949646 2.5684635639190674 14.396172523498535
Loss :  1.572818636894226 2.6958413124084473 15.052024841308594
Loss :  1.6523447036743164 2.4394044876098633 13.849367141723633
  batch 60 loss: 1.6523447036743164, 2.4394044876098633, 13.849367141723633
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5512083768844604 2.7822656631469727 15.462536811828613
Loss :  1.5889947414398193 2.5477170944213867 14.327580451965332
Loss :  1.5604023933410645 2.367453098297119 13.397666931152344
Loss :  1.5457215309143066 3.3189704418182373 18.140573501586914
Loss :  1.5201416015625 2.3212714195251465 13.12649917602539
Loss :  1.5734268426895142 3.4253463745117188 18.700159072875977
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.5878595113754272 3.4263088703155518 18.719404220581055
Loss :  1.5863016843795776 3.2685060501098633 17.928831100463867
Loss :  1.5830974578857422 3.147987127304077 17.32303237915039
Total LOSS train 15.658011157696064 valid 18.167856693267822
CE LOSS train 1.5831155116741473 valid 0.39577436447143555
Contrastive LOSS train 2.8149791460770826 valid 0.7869967818260193
Saved best model. Old loss 18.707260131835938 and new best loss 18.167856693267822
EPOCH 128:
Loss :  1.6049171686172485 2.651996612548828 14.864900588989258
Loss :  1.6208738088607788 2.5151169300079346 14.196457862854004
Loss :  1.581441879272461 2.677384376525879 14.968363761901855
Loss :  1.5924032926559448 2.9123036861419678 16.153921127319336
Loss :  1.6143684387207031 2.2974977493286133 13.10185718536377
Loss :  1.569014310836792 2.8802573680877686 15.970301628112793
Loss :  1.6158703565597534 3.5519185066223145 19.37546157836914
Loss :  1.5855932235717773 2.507974147796631 14.12546443939209
Loss :  1.577739953994751 2.334327220916748 13.24937629699707
Loss :  1.6223913431167603 3.452669858932495 18.885740280151367
Loss :  1.566939115524292 2.8645715713500977 15.88979721069336
Loss :  1.566429615020752 2.49682354927063 14.050546646118164
Loss :  1.5646069049835205 3.2774500846862793 17.951858520507812
Loss :  1.5707772970199585 3.0708417892456055 16.924985885620117
Loss :  1.6425889730453491 2.7114312648773193 15.199745178222656
Loss :  1.6293865442276 4.087826251983643 22.068517684936523
Loss :  1.5605113506317139 2.618870735168457 14.654865264892578
Loss :  1.5905258655548096 3.0553269386291504 16.86716079711914
Loss :  1.5558226108551025 2.5552895069122314 14.332269668579102
Loss :  1.6358726024627686 2.8450024127960205 15.860884666442871
  batch 20 loss: 1.6358726024627686, 2.8450024127960205, 15.860884666442871
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5888855457305908 3.4403886795043945 18.790828704833984
Loss :  1.5541731119155884 2.949181318283081 16.300079345703125
Loss :  1.5782114267349243 2.4671671390533447 13.914047241210938
Loss :  1.5950968265533447 2.32812762260437 13.235734939575195
Loss :  1.6305335760116577 2.7044270038604736 15.152667999267578
Loss :  1.5771411657333374 2.582254648208618 14.48841381072998
Loss :  1.5906959772109985 2.607304811477661 14.627220153808594
Loss :  1.584490180015564 3.586373805999756 19.516359329223633
Loss :  1.5194059610366821 3.0840518474578857 16.939664840698242
Loss :  1.6249570846557617 2.6973445415496826 15.111680030822754
Loss :  1.521266222000122 2.6264607906341553 14.653570175170898
Loss :  1.6069759130477905 3.167642831802368 17.4451904296875
Loss :  1.575217604637146 3.523024082183838 19.190338134765625
Loss :  1.5724273920059204 3.203453779220581 17.589696884155273
Loss :  1.5285660028457642 2.8453404903411865 15.755269050598145
Loss :  1.5454065799713135 2.526031255722046 14.175562858581543
Loss :  1.5413284301757812 2.296809196472168 13.025374412536621
Loss :  1.6168789863586426 2.0859217643737793 12.046487808227539
Loss :  1.6237351894378662 2.6231164932250977 14.739317893981934
Loss :  1.6356282234191895 2.4179844856262207 13.725549697875977
  batch 40 loss: 1.6356282234191895, 2.4179844856262207, 13.725549697875977
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.583748459815979 3.1254255771636963 17.21087646484375
Loss :  1.5677411556243896 2.788128614425659 15.508384704589844
Loss :  1.5540728569030762 2.8663268089294434 15.88570785522461
Loss :  1.5691478252410889 2.720085859298706 15.169577598571777
Loss :  1.5423396825790405 3.321380853652954 18.14924430847168
Loss :  1.5798145532608032 2.6007258892059326 14.583444595336914
Loss :  1.6250680685043335 2.018172025680542 11.715928077697754
Loss :  1.5672317743301392 2.4051239490509033 13.592851638793945
Loss :  1.6477420330047607 2.7749602794647217 15.522543907165527
Loss :  1.5743345022201538 2.875633716583252 15.952502250671387
Loss :  1.6197859048843384 2.727510929107666 15.257340431213379
Loss :  1.610536813735962 2.4709322452545166 13.965197563171387
Loss :  1.5843815803527832 2.635075569152832 14.759759902954102
Loss :  1.6187946796417236 2.796234130859375 15.59996509552002
Loss :  1.5681713819503784 2.7497384548187256 15.316864013671875
Loss :  1.6445112228393555 3.174023389816284 17.514629364013672
Loss :  1.5710785388946533 2.9858903884887695 16.500530242919922
Loss :  1.5519518852233887 3.0071024894714355 16.587465286254883
Loss :  1.5729918479919434 2.5175743103027344 14.160863876342773
Loss :  1.654272198677063 2.6431996822357178 14.870270729064941
  batch 60 loss: 1.654272198677063, 2.6431996822357178, 14.870270729064941
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5554249286651611 2.537222146987915 14.241535186767578
Loss :  1.5940431356430054 2.707282304763794 15.130454063415527
Loss :  1.5642797946929932 2.953317165374756 16.33086585998535
Loss :  1.5495163202285767 2.9135231971740723 16.11713218688965
Loss :  1.5220460891723633 1.768181324005127 10.36295223236084
Loss :  1.5680928230285645 3.9203834533691406 21.17000961303711
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.58273446559906 3.897616386413574 21.070817947387695
Loss :  1.580916166305542 3.7765419483184814 20.463624954223633
Loss :  1.577225685119629 3.5519707202911377 19.337078094482422
Total LOSS train 15.524960268460788 valid 20.510382652282715
CE LOSS train 1.5857249736785888 valid 0.3943064212799072
Contrastive LOSS train 2.7878470530876744 valid 0.8879926800727844
EPOCH 129:
Loss :  1.6051290035247803 2.1752214431762695 12.481236457824707
Loss :  1.6196016073226929 3.017341375350952 16.706308364868164
Loss :  1.5810027122497559 2.685929298400879 15.010648727416992
Loss :  1.5900808572769165 3.0409095287323 16.794628143310547
Loss :  1.612870693206787 2.447667360305786 13.851207733154297
Loss :  1.5655560493469238 2.6101698875427246 14.616405487060547
Loss :  1.6132420301437378 2.942291736602783 16.32469940185547
Loss :  1.5800174474716187 2.47349214553833 13.947477340698242
Loss :  1.5693130493164062 2.4185869693756104 13.662247657775879
Loss :  1.6136056184768677 2.3466804027557373 13.347007751464844
Loss :  1.5585577487945557 2.4831807613372803 13.974461555480957
Loss :  1.5599602460861206 2.5672338008880615 14.396129608154297
Loss :  1.5564830303192139 2.496385097503662 14.038409233093262
Loss :  1.5643665790557861 2.5749404430389404 14.439068794250488
Loss :  1.6374934911727905 2.3829028606414795 13.552007675170898
Loss :  1.6269383430480957 2.94923734664917 16.373125076293945
Loss :  1.5543009042739868 2.5904276371002197 14.506439208984375
Loss :  1.5862104892730713 2.0588650703430176 11.880535125732422
Loss :  1.5488672256469727 1.9498095512390137 11.2979154586792
Loss :  1.6282477378845215 2.4446425437927246 13.851461410522461
  batch 20 loss: 1.6282477378845215, 2.4446425437927246, 13.851461410522461
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5835930109024048 2.6755154132843018 14.961170196533203
Loss :  1.550809621810913 2.2350361347198486 12.725990295410156
Loss :  1.5751399993896484 2.130262613296509 12.226452827453613
Loss :  1.5944660902023315 2.9818410873413086 16.503671646118164
Loss :  1.6305732727050781 3.9590036869049072 21.42559242248535
Loss :  1.5769009590148926 2.3769819736480713 13.461811065673828
Loss :  1.5894352197647095 2.7870070934295654 15.524471282958984
Loss :  1.5829012393951416 2.1181066036224365 12.173434257507324
Loss :  1.5187103748321533 2.223616361618042 12.636792182922363
Loss :  1.6250741481781006 2.5997872352600098 14.62401008605957
Loss :  1.5198968648910522 2.5935184955596924 14.487488746643066
Loss :  1.6060878038406372 2.181056022644043 12.511367797851562
Loss :  1.5747113227844238 2.319187879562378 13.170650482177734
Loss :  1.5720828771591187 2.3501322269439697 13.32274341583252
Loss :  1.5300041437149048 2.526921510696411 14.16461181640625
Loss :  1.5482895374298096 2.616241216659546 14.629495620727539
Loss :  1.545985460281372 2.1713030338287354 12.40250015258789
Loss :  1.6212655305862427 2.151515007019043 12.378840446472168
Loss :  1.6283466815948486 2.777477741241455 15.515734672546387
Loss :  1.6401804685592651 2.9593112468719482 16.436737060546875
  batch 40 loss: 1.6401804685592651, 2.9593112468719482, 16.436737060546875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5910447835922241 2.3942055702209473 13.562071800231934
Loss :  1.5754376649856567 2.112496852874756 12.137922286987305
Loss :  1.5645450353622437 3.1538803577423096 17.333946228027344
Loss :  1.5801185369491577 2.7445497512817383 15.30286693572998
Loss :  1.5547558069229126 4.096087455749512 22.035194396972656
Loss :  1.5927040576934814 2.684933662414551 15.017372131347656
Loss :  1.6356443166732788 2.770756244659424 15.489424705505371
Loss :  1.5745049715042114 2.6114678382873535 14.631843566894531
Loss :  1.6537054777145386 2.9391939640045166 16.349674224853516
Loss :  1.5835398435592651 2.4446566104888916 13.806822776794434
Loss :  1.6244982481002808 2.8962669372558594 16.105833053588867
Loss :  1.6159453392028809 2.702397346496582 15.127931594848633
Loss :  1.5906062126159668 2.4128293991088867 13.654752731323242
Loss :  1.6249226331710815 2.657942771911621 14.914636611938477
Loss :  1.574666976928711 3.121997833251953 17.184656143188477
Loss :  1.6503466367721558 3.4601783752441406 18.95123863220215
Loss :  1.5795979499816895 2.79353404045105 15.54726791381836
Loss :  1.5594313144683838 2.811216115951538 15.615511894226074
Loss :  1.5763347148895264 2.970757246017456 16.43012046813965
Loss :  1.656319499015808 2.2803189754486084 13.057914733886719
  batch 60 loss: 1.656319499015808, 2.2803189754486084, 13.057914733886719
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.559252142906189 2.5648560523986816 14.38353157043457
Loss :  1.595706582069397 2.848060369491577 15.83600902557373
Loss :  1.5703707933425903 2.2358436584472656 12.749588966369629
Loss :  1.556075096130371 2.9248335361480713 16.18024253845215
Loss :  1.5303878784179688 2.34721040725708 13.266439437866211
Loss :  1.5848218202590942 4.091813087463379 22.043886184692383
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.5979551076889038 4.069983005523682 21.9478702545166
Loss :  1.5947717428207397 4.001552104949951 21.6025333404541
Loss :  1.6017478704452515 3.987841844558716 21.540956497192383
Total LOSS train 14.692427708552433 valid 21.783811569213867
CE LOSS train 1.5870271077522864 valid 0.40043696761131287
Contrastive LOSS train 2.6210801418011007 valid 0.996960461139679
EPOCH 130:
Loss :  1.6123027801513672 2.5772156715393066 14.498380661010742
Loss :  1.6270115375518799 2.386681079864502 13.560416221618652
Loss :  1.590074062347412 2.535778522491455 14.268966674804688
Loss :  1.5982731580734253 3.068911075592041 16.942827224731445
Loss :  1.6204535961151123 2.5047237873077393 14.144072532653809
Loss :  1.5746678113937378 2.6081995964050293 14.615666389465332
Loss :  1.6208899021148682 2.3150551319122314 13.196165084838867
Loss :  1.5872820615768433 2.4506826400756836 13.84069538116455
Loss :  1.5742155313491821 2.6447205543518066 14.797818183898926
Loss :  1.6157011985778809 2.753798246383667 15.384693145751953
Loss :  1.560779333114624 2.6245858669281006 14.683709144592285
Loss :  1.5615737438201904 2.500803232192993 14.065589904785156
Loss :  1.5591281652450562 2.226952075958252 12.693887710571289
Loss :  1.568354845046997 2.448859691619873 13.812653541564941
Loss :  1.6407572031021118 2.76849627494812 15.48323917388916
Loss :  1.631843090057373 2.7263708114624023 15.263696670532227
Loss :  1.5585697889328003 2.5266916751861572 14.192028045654297
Loss :  1.590240478515625 2.4441516399383545 13.810998916625977
Loss :  1.5509568452835083 2.13917875289917 12.246850967407227
Loss :  1.628638744354248 2.8488779067993164 15.873027801513672
  batch 20 loss: 1.628638744354248, 2.8488779067993164, 15.873027801513672
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5839959383010864 2.686534881591797 15.016670227050781
Loss :  1.551698088645935 2.3135721683502197 13.119558334350586
Loss :  1.5767710208892822 2.694735527038574 15.050448417663574
Loss :  1.59589421749115 2.6801443099975586 14.996615409851074
Loss :  1.633123517036438 3.4525134563446045 18.89569091796875
Loss :  1.5830087661743164 2.475719928741455 13.961607933044434
Loss :  1.5962978601455688 2.4883627891540527 14.03811264038086
Loss :  1.5889230966567993 2.751307249069214 15.345459938049316
Loss :  1.52652907371521 2.8738856315612793 15.895957946777344
Loss :  1.6260393857955933 3.292609691619873 18.089088439941406
Loss :  1.523307204246521 2.925980567932129 16.153209686279297
Loss :  1.606090784072876 2.703994035720825 15.126060485839844
Loss :  1.5749365091323853 2.9584429264068604 16.367151260375977
Loss :  1.5737361907958984 3.1409225463867188 17.278348922729492
Loss :  1.5337092876434326 3.1379265785217285 17.22334098815918
Loss :  1.5511974096298218 3.533137083053589 19.216882705688477
Loss :  1.5476329326629639 2.955390453338623 16.3245849609375
Loss :  1.6218411922454834 2.534853935241699 14.296111106872559
Loss :  1.626407265663147 2.61659574508667 14.709386825561523
Loss :  1.6358200311660767 2.4537999629974365 13.904820442199707
  batch 40 loss: 1.6358200311660767, 2.4537999629974365, 13.904820442199707
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5846762657165527 2.973780393600464 16.45357894897461
Loss :  1.5706769227981567 2.8799970149993896 15.970662117004395
Loss :  1.5594615936279297 2.564563035964966 14.38227653503418
Loss :  1.5778367519378662 2.6447854042053223 14.801763534545898
Loss :  1.5506457090377808 3.015601396560669 16.628652572631836
Loss :  1.5902842283248901 2.606337308883667 14.621971130371094
Loss :  1.6349610090255737 3.333336114883423 18.3016414642334
Loss :  1.5700223445892334 3.2252492904663086 17.696269989013672
Loss :  1.6486470699310303 3.431874990463257 18.808021545410156
Loss :  1.5740879774093628 2.552954912185669 14.338862419128418
Loss :  1.618052363395691 4.2017364501953125 22.626733779907227
Loss :  1.6088982820510864 2.7592759132385254 15.405277252197266
Loss :  1.5788960456848145 2.7741646766662598 15.449718475341797
Loss :  1.6202536821365356 3.3678412437438965 18.459461212158203
Loss :  1.5580177307128906 2.7400734424591064 15.258384704589844
Loss :  1.6510525941848755 2.969207525253296 16.49709129333496
Loss :  1.5717837810516357 3.116718292236328 17.15537452697754
Loss :  1.5475380420684814 2.9981606006622314 16.538341522216797
Loss :  1.5692495107650757 3.0468461513519287 16.80348014831543
Loss :  1.6539535522460938 2.6951444149017334 15.12967586517334
  batch 60 loss: 1.6539535522460938, 2.6951444149017334, 15.12967586517334
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.545639991760254 2.678530693054199 14.93829345703125
Loss :  1.5831787586212158 2.825302839279175 15.70969295501709
Loss :  1.5571794509887695 2.787142276763916 15.492890357971191
Loss :  1.5385066270828247 2.8324363231658936 15.700688362121582
Loss :  1.5111253261566162 2.903012275695801 16.026185989379883
Loss :  1.5533331632614136 3.425243854522705 18.67955207824707
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5673565864562988 3.5195116996765137 19.164915084838867
Loss :  1.562945008277893 3.399592161178589 18.56090545654297
Loss :  1.576049566268921 3.437683343887329 18.764467239379883
Total LOSS train 15.562761248075045 valid 18.792459964752197
CE LOSS train 1.5862046040021456 valid 0.3940123915672302
Contrastive LOSS train 2.7953113335829514 valid 0.8594208359718323
EPOCH 131:
Loss :  1.605649471282959 3.3637535572052 18.42441749572754
Loss :  1.6200811862945557 3.340071678161621 18.3204402923584
Loss :  1.5759153366088867 2.997035503387451 16.561092376708984
Loss :  1.5827466249465942 3.0137858390808105 16.651676177978516
Loss :  1.604280948638916 3.452070951461792 18.864635467529297
Loss :  1.552667260169983 2.5987937450408936 14.546636581420898
Loss :  1.603606104850769 2.530646562576294 14.25683879852295
Loss :  1.565582275390625 2.5638515949249268 14.38484001159668
Loss :  1.5510252714157104 2.8977584838867188 16.039817810058594
Loss :  1.5975849628448486 2.720808982849121 15.201629638671875
Loss :  1.538366675376892 3.11622953414917 17.11951446533203
Loss :  1.5380245447158813 2.994025707244873 16.508153915405273
Loss :  1.5325472354888916 2.7931761741638184 15.498428344726562
Loss :  1.5387777090072632 2.5247929096221924 14.162741661071777
Loss :  1.6181061267852783 2.6352381706237793 14.794297218322754
Loss :  1.612640619277954 3.586949348449707 19.547386169433594
Loss :  1.5271350145339966 2.48484206199646 13.951345443725586
Loss :  1.5690393447875977 2.346611261367798 13.302095413208008
Loss :  1.523262858390808 2.9289228916168213 16.167877197265625
Loss :  1.6124746799468994 2.9158172607421875 16.191560745239258
  batch 20 loss: 1.6124746799468994, 2.9158172607421875, 16.191560745239258
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.560853362083435 2.5306918621063232 14.214312553405762
Loss :  1.5233557224273682 2.5703306198120117 14.375008583068848
Loss :  1.5511423349380493 2.7035531997680664 15.06890869140625
Loss :  1.5721019506454468 2.712393283843994 15.13406753540039
Loss :  1.6124112606048584 2.794177532196045 15.58329963684082
Loss :  1.5529834032058716 2.920393466949463 16.154951095581055
Loss :  1.565898060798645 2.5150704383850098 14.141249656677246
Loss :  1.5573580265045166 2.3596982955932617 13.355849266052246
Loss :  1.4850257635116577 2.2571072578430176 12.770561218261719
Loss :  1.6058059930801392 2.7162272930145264 15.186943054199219
Loss :  1.4880084991455078 2.7544949054718018 15.260482788085938
Loss :  1.5855214595794678 2.593600034713745 14.553522109985352
Loss :  1.552894949913025 2.9073426723480225 16.089609146118164
Loss :  1.5506454706192017 2.7268624305725098 15.184957504272461
Loss :  1.5023261308670044 3.225144386291504 17.628047943115234
Loss :  1.5236057043075562 2.606060028076172 14.553905487060547
Loss :  1.5201325416564941 2.54447865486145 14.242525100708008
Loss :  1.6073294878005981 2.495591878890991 14.085289001464844
Loss :  1.6125842332839966 2.877096652984619 15.998066902160645
Loss :  1.626985788345337 3.0735890865325928 16.994932174682617
  batch 40 loss: 1.626985788345337, 3.0735890865325928, 16.994932174682617
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5690381526947021 2.587409734725952 14.506087303161621
Loss :  1.5531257390975952 2.6034743785858154 14.570497512817383
Loss :  1.5374692678451538 2.579632043838501 14.435628890991211
Loss :  1.5569381713867188 2.3286523818969727 13.200200080871582
Loss :  1.5274982452392578 2.6197237968444824 14.626117706298828
Loss :  1.569852352142334 2.6664535999298096 14.902120590209961
Loss :  1.6183348894119263 2.750744342803955 15.372056007385254
Loss :  1.5470166206359863 3.075469493865967 16.92436408996582
Loss :  1.6373193264007568 2.2161965370178223 12.718301773071289
Loss :  1.5561050176620483 2.284560203552246 12.97890567779541
Loss :  1.6024863719940186 3.747443675994873 20.339704513549805
Loss :  1.5959161520004272 2.3239705562591553 13.215768814086914
Loss :  1.5677200555801392 2.341055393218994 13.27299690246582
Loss :  1.6094247102737427 2.598693609237671 14.602892875671387
Loss :  1.550294041633606 2.9315905570983887 16.208248138427734
Loss :  1.6381546258926392 2.4042165279388428 13.6592378616333
Loss :  1.5600478649139404 2.2257096767425537 12.688596725463867
Loss :  1.537766933441162 2.269089460372925 12.883214950561523
Loss :  1.5581719875335693 2.835400342941284 15.735174179077148
Loss :  1.6465635299682617 2.606529951095581 14.679213523864746
  batch 60 loss: 1.6465635299682617, 2.606529951095581, 14.679213523864746
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5408029556274414 3.072094678878784 16.901275634765625
Loss :  1.5798423290252686 2.320408344268799 13.181883811950684
Loss :  1.5506941080093384 3.3417444229125977 18.259414672851562
Loss :  1.534010410308838 2.5473527908325195 14.270774841308594
Loss :  1.5054821968078613 1.9596006870269775 11.303485870361328
Loss :  1.530285358428955 4.100447654724121 22.03252410888672
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5452412366867065 4.178167819976807 22.436079025268555
Loss :  1.541507601737976 4.115044116973877 22.116727828979492
Loss :  1.5491440296173096 4.132730960845947 22.212799072265625
Total LOSS train 15.17704734802246 valid 22.199532508850098
CE LOSS train 1.5673316992246187 valid 0.3872860074043274
Contrastive LOSS train 2.721943129025973 valid 1.0331827402114868
EPOCH 132:
Loss :  1.5999821424484253 2.5052082538604736 14.126023292541504
Loss :  1.6207685470581055 3.001858711242676 16.630062103271484
Loss :  1.58053457736969 2.086824893951416 12.01465892791748
Loss :  1.5876067876815796 3.023768663406372 16.706449508666992
Loss :  1.6101408004760742 3.3962368965148926 18.591323852539062
Loss :  1.5611947774887085 3.7321081161499023 20.221736907958984
Loss :  1.6135567426681519 3.1225192546844482 17.226152420043945
Loss :  1.5778603553771973 2.5423922538757324 14.28982162475586
Loss :  1.56407630443573 3.0415024757385254 16.771587371826172
Loss :  1.6121104955673218 3.9304864406585693 21.264541625976562
Loss :  1.5502961874008179 3.0989139080047607 17.04486656188965
Loss :  1.5482591390609741 2.767768144607544 15.387099266052246
Loss :  1.5472016334533691 2.910628318786621 16.100343704223633
Loss :  1.5551457405090332 3.339843988418579 18.254365921020508
Loss :  1.6342763900756836 2.7717208862304688 15.492880821228027
Loss :  1.6268008947372437 2.8727142810821533 15.990371704101562
Loss :  1.5425963401794434 2.722748279571533 15.15633773803711
Loss :  1.5791106224060059 3.2101404666900635 17.62981414794922
Loss :  1.5327574014663696 2.2039759159088135 12.552637100219727
Loss :  1.6201579570770264 2.821038246154785 15.725349426269531
  batch 20 loss: 1.6201579570770264, 2.821038246154785, 15.725349426269531
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5652613639831543 2.505803108215332 14.094276428222656
Loss :  1.5271207094192505 3.6836400032043457 19.94532012939453
Loss :  1.5533366203308105 3.0363149642944336 16.73491096496582
Loss :  1.5749647617340088 2.8423852920532227 15.786890983581543
Loss :  1.6169826984405518 2.9983201026916504 16.608583450317383
Loss :  1.5589405298233032 2.8722434043884277 15.920158386230469
Loss :  1.5731860399246216 3.352083444595337 18.33360481262207
Loss :  1.5645376443862915 3.2323689460754395 17.726381301879883
Loss :  1.4898662567138672 2.654177188873291 14.760751724243164
Loss :  1.6118196249008179 3.3088772296905518 18.156206130981445
Loss :  1.4880586862564087 2.7196714878082275 15.086416244506836
Loss :  1.585008144378662 2.5231270790100098 14.200643539428711
Loss :  1.5515187978744507 2.368191719055176 13.392477035522461
Loss :  1.5504399538040161 2.3982622623443604 13.54175090789795
Loss :  1.501659631729126 2.594193696975708 14.472627639770508
Loss :  1.52531099319458 3.057391881942749 16.812271118164062
Loss :  1.5232419967651367 2.4004733562469482 13.525609016418457
Loss :  1.6109861135482788 3.03489351272583 16.78545379638672
Loss :  1.6184468269348145 2.584801197052002 14.542451858520508
Loss :  1.6322991847991943 2.4310595989227295 13.78759765625
  batch 40 loss: 1.6322991847991943, 2.4310595989227295, 13.78759765625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.572356104850769 3.334319591522217 18.243953704833984
Loss :  1.5552953481674194 2.5996253490448 14.553421974182129
Loss :  1.5357334613800049 3.062427282333374 16.847869873046875
Loss :  1.553723931312561 2.7804925441741943 15.45618724822998
Loss :  1.5228486061096191 2.3954038619995117 13.499868392944336
Loss :  1.5672169923782349 2.96951961517334 16.41481590270996
Loss :  1.6186152696609497 2.123137950897217 12.234305381774902
Loss :  1.5460790395736694 2.432584047317505 13.708998680114746
Loss :  1.6426018476486206 2.9709625244140625 16.497413635253906
Loss :  1.5572415590286255 3.50276517868042 19.071067810058594
Loss :  1.6038569211959839 2.5131688117980957 14.169700622558594
Loss :  1.5978083610534668 3.0720736980438232 16.95817756652832
Loss :  1.5677251815795898 2.5798709392547607 14.467080116271973
Loss :  1.6108112335205078 2.955371856689453 16.387670516967773
Loss :  1.5453906059265137 2.3622915744781494 13.356847763061523
Loss :  1.643500804901123 3.4766323566436768 19.026662826538086
Loss :  1.561869740486145 2.7773783206939697 15.448760986328125
Loss :  1.5369467735290527 2.546567440032959 14.269784927368164
Loss :  1.5581551790237427 2.7930359840393066 15.523334503173828
Loss :  1.6466175317764282 2.528559684753418 14.289416313171387
  batch 60 loss: 1.6466175317764282, 2.528559684753418, 14.289416313171387
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5352506637573242 3.4229490756988525 18.64999771118164
Loss :  1.575214147567749 3.597421169281006 19.562320709228516
Loss :  1.5483604669570923 3.439720630645752 18.746963500976562
Loss :  1.5320731401443481 3.3300163745880127 18.18215560913086
Loss :  1.5046228170394897 2.835359573364258 15.68142032623291
Loss :  1.5317466259002686 3.8987226486206055 21.025360107421875
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.5487945079803467 3.9339334964752197 21.218461990356445
Loss :  1.5423269271850586 3.7697157859802246 20.390907287597656
Loss :  1.5579406023025513 3.796950101852417 20.542692184448242
Total LOSS train 16.040600057748648 valid 20.794355392456055
CE LOSS train 1.5711897868376512 valid 0.3894851505756378
Contrastive LOSS train 2.893882050881019 valid 0.9492375254631042
EPOCH 133:
Loss :  1.6052401065826416 3.1079204082489014 17.14484214782715
Loss :  1.622117519378662 3.4141249656677246 18.6927433013916
Loss :  1.5786807537078857 2.587127447128296 14.514318466186523
Loss :  1.587586522102356 2.298032283782959 13.07774829864502
Loss :  1.6097891330718994 2.5800929069519043 14.51025390625
Loss :  1.5625784397125244 2.9067814350128174 16.096485137939453
Loss :  1.611950159072876 2.8850717544555664 16.037309646606445
Loss :  1.5748224258422852 3.60888671875 19.61925506591797
Loss :  1.559407114982605 3.0412864685058594 16.765838623046875
Loss :  1.6098041534423828 2.4088289737701416 13.653948783874512
Loss :  1.5469309091567993 2.4099252223968506 13.5965576171875
Loss :  1.5454744100570679 2.5189616680145264 14.14028263092041
Loss :  1.5417273044586182 3.204929828643799 17.566375732421875
Loss :  1.5493963956832886 2.8668367862701416 15.883580207824707
Loss :  1.6303542852401733 2.4503467082977295 13.882087707519531
Loss :  1.6242049932479858 2.5748023986816406 14.49821662902832
Loss :  1.5423312187194824 2.5810203552246094 14.447433471679688
Loss :  1.5810303688049316 2.6631646156311035 14.896852493286133
Loss :  1.5375776290893555 2.8867874145507812 15.971514701843262
Loss :  1.6260480880737305 3.3672688007354736 18.462390899658203
  batch 20 loss: 1.6260480880737305, 3.3672688007354736, 18.462390899658203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5691007375717163 3.2822530269622803 17.980365753173828
Loss :  1.5302037000656128 2.4893441200256348 13.976923942565918
Loss :  1.557304859161377 2.291971445083618 13.017162322998047
Loss :  1.5773910284042358 2.5427181720733643 14.29098129272461
Loss :  1.6191524267196655 2.6455118656158447 14.846711158752441
Loss :  1.5645498037338257 2.6587882041931152 14.858490943908691
Loss :  1.5792235136032104 3.013981580734253 16.649131774902344
Loss :  1.5714465379714966 3.7288870811462402 20.215883255004883
Loss :  1.497362732887268 2.8409500122070312 15.702113151550293
Loss :  1.6149312257766724 2.8254594802856445 15.742228507995605
Loss :  1.4950459003448486 2.8918826580047607 15.954459190368652
Loss :  1.5892914533615112 2.5108914375305176 14.14374828338623
Loss :  1.5561679601669312 3.1395137310028076 17.25373649597168
Loss :  1.5542993545532227 2.381425619125366 13.461427688598633
Loss :  1.5066807270050049 3.133164405822754 17.172502517700195
Loss :  1.5290764570236206 2.8830466270446777 15.944310188293457
Loss :  1.5274443626403809 2.763270139694214 15.343795776367188
Loss :  1.6135532855987549 2.642599582672119 14.826550483703613
Loss :  1.6210284233093262 2.679375410079956 15.017906188964844
Loss :  1.6351368427276611 2.2738888263702393 13.0045804977417
  batch 40 loss: 1.6351368427276611, 2.2738888263702393, 13.0045804977417
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5805290937423706 2.5709238052368164 14.435148239135742
Loss :  1.5655238628387451 3.136812925338745 17.249588012695312
Loss :  1.5494141578674316 3.5994999408721924 19.546913146972656
Loss :  1.5670665502548218 2.465019464492798 13.892163276672363
Loss :  1.5360324382781982 3.1262969970703125 17.167516708374023
Loss :  1.575784683227539 3.5128839015960693 19.14020347595215
Loss :  1.6231929063796997 2.2882654666900635 13.064519882202148
Loss :  1.5522149801254272 2.3374621868133545 13.23952579498291
Loss :  1.6435989141464233 3.0314979553222656 16.801088333129883
Loss :  1.5625274181365967 2.7305960655212402 15.215508460998535
Loss :  1.6076114177703857 3.008223295211792 16.648727416992188
Loss :  1.6016674041748047 2.708057165145874 15.141953468322754
Loss :  1.5731292963027954 2.439177989959717 13.769020080566406
Loss :  1.6128901243209839 2.954216718673706 16.383974075317383
Loss :  1.5536267757415771 2.9838342666625977 16.472797393798828
Loss :  1.641573190689087 2.8018274307250977 15.650710105895996
Loss :  1.5622187852859497 2.9048917293548584 16.08667755126953
Loss :  1.5415126085281372 2.6759297847747803 14.921161651611328
Loss :  1.561926007270813 2.956313371658325 16.34349250793457
Loss :  1.6501176357269287 2.744084119796753 15.370538711547852
  batch 60 loss: 1.6501176357269287, 2.744084119796753, 15.370538711547852
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.544594168663025 2.726572275161743 15.177454948425293
Loss :  1.5812550783157349 3.3119735717773438 18.141122817993164
Loss :  1.554140329360962 2.5212340354919434 14.160310745239258
Loss :  1.5378217697143555 2.773519992828369 15.405421257019043
Loss :  1.5102990865707397 2.0673224925994873 11.846911430358887
Loss :  1.5441995859146118 3.7531042098999023 20.309720993041992
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5596961975097656 3.6882033348083496 20.000713348388672
Loss :  1.5562149286270142 3.4949533939361572 19.030982971191406
Loss :  1.5630238056182861 3.5821878910064697 19.47396469116211
Total LOSS train 15.60205375964825 valid 19.703845500946045
CE LOSS train 1.574549414561345 valid 0.39075595140457153
Contrastive LOSS train 2.805500885156485 valid 0.8955469727516174
EPOCH 134:
Loss :  1.602376103401184 3.111401081085205 17.159381866455078
Loss :  1.6215301752090454 3.293116569519043 18.087114334106445
Loss :  1.5770069360733032 2.5084593296051025 14.119303703308105
Loss :  1.585038661956787 2.9287524223327637 16.228801727294922
Loss :  1.6059136390686035 3.3522913455963135 18.36737060546875
Loss :  1.5540833473205566 2.8086559772491455 15.597362518310547
Loss :  1.6067752838134766 3.336069345474243 18.28712272644043
Loss :  1.5684995651245117 2.7923240661621094 15.530119895935059
Loss :  1.5554636716842651 2.734382390975952 15.227375984191895
Loss :  1.605111002922058 2.6227529048919678 14.718875885009766
Loss :  1.5442163944244385 3.049349784851074 16.790966033935547
Loss :  1.5435502529144287 4.256607532501221 22.826587677001953
Loss :  1.537808895111084 2.7753472328186035 15.414545059204102
Loss :  1.5445927381515503 2.6162421703338623 14.62580394744873
Loss :  1.625286340713501 2.24997878074646 12.8751802444458
Loss :  1.615831732749939 3.42071270942688 18.71939468383789
Loss :  1.5332976579666138 2.333981513977051 13.203205108642578
Loss :  1.572291612625122 2.536811590194702 14.256349563598633
Loss :  1.5279728174209595 3.0085418224334717 16.570682525634766
Loss :  1.6195335388183594 2.541940450668335 14.329236030578613
  batch 20 loss: 1.6195335388183594, 2.541940450668335, 14.329236030578613
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5659773349761963 2.7509045600891113 15.320499420166016
Loss :  1.5265400409698486 3.132540464401245 17.18924331665039
Loss :  1.553279995918274 2.34775447845459 13.292052268981934
Loss :  1.5729197263717651 2.327515125274658 13.210494995117188
Loss :  1.6134376525878906 2.638730764389038 14.80709171295166
Loss :  1.5544853210449219 2.408627986907959 13.597625732421875
Loss :  1.568888545036316 3.7688660621643066 20.413217544555664
Loss :  1.560135841369629 3.1010403633117676 17.065338134765625
Loss :  1.4865590333938599 2.623440742492676 14.60376262664795
Loss :  1.6092123985290527 3.4106791019439697 18.662609100341797
Loss :  1.4866100549697876 2.5631325244903564 14.30227279663086
Loss :  1.5845869779586792 2.7836880683898926 15.503026962280273
Loss :  1.5515875816345215 3.0621278285980225 16.862226486206055
Loss :  1.5487573146820068 2.6798253059387207 14.947883605957031
Loss :  1.4988523721694946 2.6291580200195312 14.64464282989502
Loss :  1.5210767984390259 2.6162219047546387 14.602187156677246
Loss :  1.520389199256897 2.3363826274871826 13.202302932739258
Loss :  1.6089750528335571 2.558258295059204 14.400266647338867
Loss :  1.6175782680511475 2.596407651901245 14.599617004394531
Loss :  1.6322054862976074 2.444000720977783 13.852209091186523
  batch 40 loss: 1.6322054862976074, 2.444000720977783, 13.852209091186523
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.574146032333374 3.273880958557129 17.94355010986328
Loss :  1.5559157133102417 2.2509193420410156 12.81051254272461
Loss :  1.534695029258728 2.4104959964752197 13.587174415588379
Loss :  1.5535082817077637 2.3299832344055176 13.203424453735352
Loss :  1.5219148397445679 2.2865583896636963 12.954706192016602
Loss :  1.565494418144226 2.77376127243042 15.434301376342773
Loss :  1.616355538368225 3.3392245769500732 18.31247901916504
Loss :  1.5442650318145752 3.1335980892181396 17.212255477905273
Loss :  1.6361674070358276 3.200031280517578 17.636323928833008
Loss :  1.5513898134231567 3.0094521045684814 16.598649978637695
Loss :  1.5974019765853882 2.400867462158203 13.601738929748535
Loss :  1.5904427766799927 2.296116828918457 13.071026802062988
Loss :  1.5605777502059937 2.0648186206817627 11.88467025756836
Loss :  1.6048122644424438 2.430835723876953 13.758991241455078
Loss :  1.542756199836731 3.2172975540161133 17.629243850708008
Loss :  1.6397143602371216 3.0559771060943604 16.919599533081055
Loss :  1.5572021007537842 2.426619291305542 13.690299034118652
Loss :  1.533409595489502 2.8782050609588623 15.924434661865234
Loss :  1.554071068763733 3.1232612133026123 17.170377731323242
Loss :  1.6440176963806152 2.3816275596618652 13.552156448364258
  batch 60 loss: 1.6440176963806152, 2.3816275596618652, 13.552156448364258
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5329996347427368 2.34918475151062 13.278923988342285
Loss :  1.5745478868484497 2.207336902618408 12.611231803894043
Loss :  1.5464808940887451 3.071047067642212 16.901716232299805
Loss :  1.5294969081878662 2.927107572555542 16.165035247802734
Loss :  1.4999412298202515 2.0371737480163574 11.685810089111328
Loss :  1.5401551723480225 4.065403938293457 21.86717414855957
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.555916666984558 4.035562038421631 21.733726501464844
Loss :  1.550535798072815 3.8893015384674072 20.99704360961914
Loss :  1.5647523403167725 4.083737850189209 21.983442306518555
Total LOSS train 15.408492058974046 valid 21.645346641540527
CE LOSS train 1.5675378432640663 valid 0.3911880850791931
Contrastive LOSS train 2.7681908204005317 valid 1.0209344625473022
EPOCH 135:
Loss :  1.5939958095550537 3.2028696537017822 17.60834503173828
Loss :  1.6116676330566406 2.507200002670288 14.14766788482666
Loss :  1.5702484846115112 2.486034631729126 14.000421524047852
Loss :  1.5805838108062744 2.6863205432891846 15.012186050415039
Loss :  1.6028982400894165 2.33144211769104 13.260108947753906
Loss :  1.5532281398773193 2.591188669204712 14.509171485900879
Loss :  1.6042941808700562 2.760080575942993 15.404696464538574
Loss :  1.568719744682312 2.8111371994018555 15.624405860900879
Loss :  1.554903268814087 2.9748902320861816 16.429353713989258
Loss :  1.6054842472076416 2.382307529449463 13.517021179199219
Loss :  1.5443737506866455 2.614999532699585 14.61937141418457
Loss :  1.5442863702774048 2.3986496925354004 13.537534713745117
Loss :  1.542240858078003 2.2862188816070557 12.973335266113281
Loss :  1.5494410991668701 2.8400075435638428 15.749479293823242
Loss :  1.630238652229309 3.483942985534668 19.049955368041992
Loss :  1.6213635206222534 3.039537191390991 16.819049835205078
Loss :  1.5365500450134277 2.3220601081848145 13.1468505859375
Loss :  1.574345350265503 2.2606282234191895 12.877486228942871
Loss :  1.5287401676177979 2.1779887676239014 12.418684005737305
Loss :  1.6195666790008545 2.634721040725708 14.793171882629395
  batch 20 loss: 1.6195666790008545, 2.634721040725708, 14.793171882629395
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5651113986968994 3.1807539463043213 17.468881607055664
Loss :  1.527806282043457 2.673771619796753 14.8966646194458
Loss :  1.5538617372512817 2.4438204765319824 13.772964477539062
Loss :  1.5741382837295532 2.980208158493042 16.47517967224121
Loss :  1.6134252548217773 2.700562000274658 15.11623477935791
Loss :  1.5553237199783325 2.8514721393585205 15.812684059143066
Loss :  1.5698466300964355 3.273887872695923 17.939287185668945
Loss :  1.5627092123031616 2.8445332050323486 15.785374641418457
Loss :  1.4899531602859497 3.201763391494751 17.49877166748047
Loss :  1.6102800369262695 3.0421626567840576 16.821094512939453
Loss :  1.4891849756240845 3.3634097576141357 18.30623435974121
Loss :  1.5854933261871338 3.069251537322998 16.931751251220703
Loss :  1.551145076751709 2.812563419342041 15.613962173461914
Loss :  1.5509310960769653 2.3147451877593994 13.124656677246094
Loss :  1.5039422512054443 2.456881284713745 13.788349151611328
Loss :  1.5263280868530273 2.8517038822174072 15.784847259521484
Loss :  1.5245637893676758 3.0862510204315186 16.95581817626953
Loss :  1.60971999168396 2.868837594985962 15.95390796661377
Loss :  1.6156970262527466 2.4935338497161865 14.083366394042969
Loss :  1.6294310092926025 2.3034250736236572 13.14655590057373
  batch 40 loss: 1.6294310092926025, 2.3034250736236572, 13.14655590057373
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.571539044380188 2.6448965072631836 14.796021461486816
Loss :  1.5545705556869507 2.3587393760681152 13.348267555236816
Loss :  1.5374364852905273 2.9309346675872803 16.192110061645508
Loss :  1.5572011470794678 2.489020347595215 14.002303123474121
Loss :  1.526017427444458 1.9683480262756348 11.367756843566895
Loss :  1.5687263011932373 2.2217447757720947 12.677450180053711
Loss :  1.6193499565124512 2.420011281967163 13.719406127929688
Loss :  1.5490702390670776 2.9442687034606934 16.270414352416992
Loss :  1.6440699100494385 2.786595582962036 15.577047348022461
Loss :  1.5605753660202026 2.223008394241333 12.675617218017578
Loss :  1.6061524152755737 2.260114908218384 12.906726837158203
Loss :  1.6001255512237549 2.1480965614318848 12.340607643127441
Loss :  1.5710208415985107 5.2859063148498535 28.000551223754883
Loss :  1.6117117404937744 2.327648162841797 13.24995231628418
Loss :  1.5488375425338745 2.5644452571868896 14.371064186096191
Loss :  1.642110824584961 2.3731770515441895 13.50799560546875
Loss :  1.5616276264190674 2.916247606277466 16.142866134643555
Loss :  1.5402792692184448 4.587132930755615 24.47594451904297
Loss :  1.5638389587402344 2.850266695022583 15.81517219543457
Loss :  1.6527209281921387 2.0788168907165527 12.046806335449219
  batch 60 loss: 1.6527209281921387, 2.0788168907165527, 12.046806335449219
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5444676876068115 2.2971010208129883 13.029973030090332
Loss :  1.5804836750030518 4.272047519683838 22.94072151184082
Loss :  1.553200125694275 2.954477071762085 16.325586318969727
Loss :  1.5357441902160645 3.090876340866089 16.99012565612793
Loss :  1.509714126586914 2.9714133739471436 16.36678123474121
Loss :  1.555893063545227 4.004974842071533 21.580766677856445
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5693309307098389 3.9191296100616455 21.16497802734375
Loss :  1.5658546686172485 3.7538352012634277 20.335031509399414
Loss :  1.578668236732483 3.8395159244537354 20.776248931884766
Total LOSS train 15.383263881389912 valid 20.964256286621094
CE LOSS train 1.5701023743702816 valid 0.3946670591831207
Contrastive LOSS train 2.76263228563162 valid 0.9598789811134338
EPOCH 136:
Loss :  1.6050117015838623 2.6679880619049072 14.944952011108398
Loss :  1.6223466396331787 2.6979267597198486 15.111980438232422
Loss :  1.5794180631637573 2.2031705379486084 12.595271110534668
Loss :  1.5881342887878418 2.714121103286743 15.15873908996582
Loss :  1.6099367141723633 2.5460667610168457 14.340270042419434
Loss :  1.5617923736572266 2.325856924057007 13.19107723236084
Loss :  1.6117008924484253 2.5375497341156006 14.299449920654297
Loss :  1.576423168182373 2.3621551990509033 13.387199401855469
Loss :  1.5619944334030151 2.29870867729187 13.055538177490234
Loss :  1.611909031867981 2.9713144302368164 16.468481063842773
Loss :  1.5510518550872803 3.0038156509399414 16.57012939453125
Loss :  1.5485796928405762 2.5003652572631836 14.050405502319336
Loss :  1.5453547239303589 2.3914167881011963 13.50243854522705
Loss :  1.5529372692108154 2.3202428817749023 13.154151916503906
Loss :  1.6323347091674805 2.45695161819458 13.917092323303223
Loss :  1.6254827976226807 2.732713460922241 15.289050102233887
Loss :  1.5446559190750122 2.513768434524536 14.113497734069824
Loss :  1.5839111804962158 2.477856159210205 13.973191261291504
Loss :  1.5391769409179688 2.640099048614502 14.73967170715332
Loss :  1.6275118589401245 2.8301053047180176 15.778038024902344
  batch 20 loss: 1.6275118589401245, 2.8301053047180176, 15.778038024902344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.574475884437561 2.7936980724334717 15.542966842651367
Loss :  1.5359946489334106 2.9331655502319336 16.20182228088379
Loss :  1.5610857009887695 2.5771408081054688 14.446789741516113
Loss :  1.578460454940796 2.505293846130371 14.10492992401123
Loss :  1.6175607442855835 2.767662286758423 15.45587158203125
Loss :  1.5609257221221924 2.4710683822631836 13.916267395019531
Loss :  1.5749783515930176 2.6393840312957764 14.77189826965332
Loss :  1.567906379699707 2.565418004989624 14.394996643066406
Loss :  1.4986275434494019 2.9597818851470947 16.297536849975586
Loss :  1.615029215812683 2.981229066848755 16.52117347717285
Loss :  1.5016977787017822 3.009793996810913 16.550668716430664
Loss :  1.5942816734313965 3.1002182960510254 17.095373153686523
Loss :  1.5627590417861938 2.597151756286621 14.548518180847168
Loss :  1.5619138479232788 2.6578757762908936 14.851292610168457
Loss :  1.515019416809082 2.9041104316711426 16.035572052001953
Loss :  1.5370965003967285 2.73205828666687 15.1973876953125
Loss :  1.5350263118743896 2.4530320167541504 13.800186157226562
Loss :  1.617876410484314 2.4215612411499023 13.725682258605957
Loss :  1.623580813407898 2.994339942932129 16.595279693603516
Loss :  1.6378856897354126 2.657378911972046 14.92478084564209
  batch 40 loss: 1.6378856897354126, 2.657378911972046, 14.92478084564209
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5817285776138306 2.5665290355682373 14.414374351501465
Loss :  1.5662919282913208 1.9176496267318726 11.154540061950684
Loss :  1.5496889352798462 2.0964243412017822 12.031810760498047
Loss :  1.5694798231124878 2.638688564300537 14.762923240661621
Loss :  1.5414233207702637 2.4183616638183594 13.633232116699219
Loss :  1.5854660272598267 2.6297364234924316 14.734148025512695
Loss :  1.6325093507766724 2.7950363159179688 15.607690811157227
Loss :  1.563952922821045 2.779151439666748 15.459711074829102
Loss :  1.6510316133499146 2.2677175998687744 12.989619255065918
Loss :  1.5696791410446167 2.2433252334594727 12.78630542755127
Loss :  1.6106410026550293 2.4767746925354004 13.994514465332031
Loss :  1.6042964458465576 3.367554187774658 18.442066192626953
Loss :  1.5758240222930908 2.7134830951690674 15.14323902130127
Loss :  1.6158726215362549 2.787148952484131 15.551617622375488
Loss :  1.5563583374023438 2.9931435585021973 16.522075653076172
Loss :  1.6441504955291748 2.6331164836883545 14.809733390808105
Loss :  1.5679247379302979 2.847294807434082 15.804398536682129
Loss :  1.546504020690918 2.6222121715545654 14.657565116882324
Loss :  1.5672379732131958 2.8787150382995605 15.960813522338867
Loss :  1.6522014141082764 2.8033878803253174 15.669140815734863
  batch 60 loss: 1.6522014141082764, 2.8033878803253174, 15.669140815734863
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5479940176010132 2.7535998821258545 15.315993309020996
Loss :  1.5840882062911987 3.1028971672058105 17.098573684692383
Loss :  1.5549460649490356 2.7397804260253906 15.2538480758667
Loss :  1.5370655059814453 3.498457193374634 19.02935218811035
Loss :  1.5080353021621704 2.145972490310669 12.237897872924805
Loss :  1.5420013666152954 4.434280872344971 23.71340560913086
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.558164119720459 4.332952976226807 23.222929000854492
Loss :  1.5542259216308594 4.176068305969238 22.434566497802734
Loss :  1.5558713674545288 4.198975563049316 22.550748825073242
Total LOSS train 14.856689291733963 valid 22.980412483215332
CE LOSS train 1.5774805875924918 valid 0.3889678418636322
Contrastive LOSS train 2.655841748531048 valid 1.049743890762329
EPOCH 137:
Loss :  1.6010982990264893 2.635910987854004 14.78065299987793
Loss :  1.6198723316192627 2.57242488861084 14.481996536254883
Loss :  1.5785701274871826 2.783390998840332 15.495525360107422
Loss :  1.587289571762085 2.681063175201416 14.992605209350586
Loss :  1.6086145639419556 2.5220448970794678 14.218839645385742
Loss :  1.5583218336105347 3.2161359786987305 17.639001846313477
Loss :  1.610079050064087 3.3297629356384277 18.258893966674805
Loss :  1.5734046697616577 2.385014772415161 13.498477935791016
Loss :  1.5612913370132446 2.4087889194488525 13.605236053466797
Loss :  1.6098015308380127 2.252725839614868 12.873430252075195
Loss :  1.5499390363693237 2.726832866668701 15.184103965759277
Loss :  1.5502054691314697 2.6376874446868896 14.738642692565918
Loss :  1.5465912818908691 2.613847255706787 14.615827560424805
Loss :  1.5530704259872437 3.7713065147399902 20.409603118896484
Loss :  1.6332728862762451 2.8769638538360596 16.018091201782227
Loss :  1.6274847984313965 3.6363914012908936 19.8094425201416
Loss :  1.5430768728256226 3.2408993244171143 17.747573852539062
Loss :  1.5818514823913574 3.5420329570770264 19.292016983032227
Loss :  1.5360032320022583 2.495063066482544 14.01131820678711
Loss :  1.624442458152771 2.6468968391418457 14.858925819396973
  batch 20 loss: 1.624442458152771, 2.6468968391418457, 14.858925819396973
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5697832107543945 2.7795040607452393 15.467303276062012
Loss :  1.5314403772354126 2.673762321472168 14.900252342224121
Loss :  1.5588136911392212 2.4424214363098145 13.770920753479004
Loss :  1.5783599615097046 2.4786837100982666 13.97177791595459
Loss :  1.6188464164733887 3.2214932441711426 17.7263126373291
Loss :  1.5644521713256836 2.72774338722229 15.203168869018555
Loss :  1.5772112607955933 2.4546351432800293 13.850387573242188
Loss :  1.5711201429367065 2.3003926277160645 13.07308292388916
Loss :  1.4998588562011719 2.222477436065674 12.612245559692383
Loss :  1.616589903831482 2.9068360328674316 16.15077018737793
Loss :  1.498620867729187 2.852155923843384 15.759400367736816
Loss :  1.5924934148788452 2.525657892227173 14.220782279968262
Loss :  1.560377597808838 2.6353495121002197 14.737125396728516
Loss :  1.5593222379684448 2.341569185256958 13.267168045043945
Loss :  1.5105935335159302 3.0555648803710938 16.78841781616211
Loss :  1.53325355052948 2.7316246032714844 15.191376686096191
Loss :  1.5310070514678955 2.9855644702911377 16.458829879760742
Loss :  1.6157605648040771 2.6365952491760254 14.798736572265625
Loss :  1.621294617652893 2.8049371242523193 15.645980834960938
Loss :  1.6363364458084106 2.581880807876587 14.545740127563477
  batch 40 loss: 1.6363364458084106, 2.581880807876587, 14.545740127563477
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.577420711517334 3.114865303039551 17.15174674987793
Loss :  1.5615593194961548 2.767016887664795 15.396644592285156
Loss :  1.5454859733581543 2.3419461250305176 13.255216598510742
Loss :  1.5652544498443604 2.7051568031311035 15.09103775024414
Loss :  1.5336230993270874 2.3979172706604004 13.523208618164062
Loss :  1.5767810344696045 2.9277737140655518 16.215648651123047
Loss :  1.6253817081451416 2.8523008823394775 15.886885643005371
Loss :  1.5527106523513794 2.885284423828125 15.979132652282715
Loss :  1.64464271068573 3.0207183361053467 16.748233795166016
Loss :  1.5644731521606445 3.1328625679016113 17.22878646850586
Loss :  1.6081507205963135 2.4514453411102295 13.865377426147461
Loss :  1.6041935682296753 2.4643924236297607 13.926156044006348
Loss :  1.5757067203521729 2.5790858268737793 14.471136093139648
Loss :  1.6161696910858154 2.9798688888549805 16.515514373779297
Loss :  1.5568686723709106 3.7702622413635254 20.408180236816406
Loss :  1.6479136943817139 2.233821392059326 12.817021369934082
Loss :  1.5676770210266113 2.5958940982818604 14.547147750854492
Loss :  1.5451916456222534 2.4894256591796875 13.99232006072998
Loss :  1.564871907234192 2.945128917694092 16.290517807006836
Loss :  1.6530628204345703 2.167879343032837 12.492459297180176
  batch 60 loss: 1.6530628204345703, 2.167879343032837, 12.492459297180176
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5466351509094238 3.4344804286956787 18.719036102294922
Loss :  1.583116888999939 2.5973289012908936 14.569761276245117
Loss :  1.5557832717895508 2.504966974258423 14.080617904663086
Loss :  1.5385425090789795 2.5979461669921875 14.528273582458496
Loss :  1.5117789506912231 2.128138542175293 12.152471542358398
Loss :  1.5491530895233154 4.130117416381836 22.199739456176758
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.564610242843628 4.0853376388549805 21.99129867553711
Loss :  1.5599687099456787 4.186222076416016 22.491079330444336
Loss :  1.5727416276931763 4.120941638946533 22.17745018005371
Total LOSS train 15.300346433199369 valid 22.21489191055298
CE LOSS train 1.5757355873401349 valid 0.39318540692329407
Contrastive LOSS train 2.7449221757742075 valid 1.0302354097366333
EPOCH 138:
Loss :  1.605547547340393 2.7930500507354736 15.57079792022705
Loss :  1.6237939596176147 2.5649919509887695 14.448753356933594
Loss :  1.582099437713623 2.2115774154663086 12.639986038208008
Loss :  1.590753436088562 3.4122514724731445 18.652009963989258
Loss :  1.6139990091323853 4.086023330688477 22.04411506652832
Loss :  1.566316843032837 2.457289218902588 13.852762222290039
Loss :  1.6169657707214355 2.3907322883605957 13.570627212524414
Loss :  1.5816689729690552 2.1727757453918457 12.445547103881836
Loss :  1.5684059858322144 2.3772263526916504 13.454537391662598
Loss :  1.6179828643798828 2.3539156913757324 13.387561798095703
Loss :  1.5571074485778809 2.775545835494995 15.434837341308594
Loss :  1.5553038120269775 3.0856375694274902 16.983491897583008
Loss :  1.5506445169448853 2.7831406593322754 15.466347694396973
Loss :  1.5557687282562256 2.7188820838928223 15.150178909301758
Loss :  1.6337909698486328 3.3773202896118164 18.5203914642334
Loss :  1.6263086795806885 4.138443470001221 22.318525314331055
Loss :  1.5425918102264404 2.710263967514038 15.093912124633789
Loss :  1.5818182229995728 2.3798112869262695 13.480875015258789
Loss :  1.536007046699524 2.059826374053955 11.835138320922852
Loss :  1.6255849599838257 2.826143741607666 15.756302833557129
  batch 20 loss: 1.6255849599838257, 2.826143741607666, 15.756302833557129
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5738909244537354 2.4101245403289795 13.624513626098633
Loss :  1.5389484167099 3.522681713104248 19.15235710144043
Loss :  1.566699504852295 2.8064582347869873 15.598991394042969
Loss :  1.5856297016143799 2.8997981548309326 16.08462142944336
Loss :  1.6247676610946655 2.919875383377075 16.224143981933594
Loss :  1.5678672790527344 2.9737401008605957 16.436567306518555
Loss :  1.5794447660446167 2.828563690185547 15.72226333618164
Loss :  1.5720057487487793 2.6936984062194824 15.040498733520508
Loss :  1.501220941543579 2.7378857135772705 15.190649032592773
Loss :  1.6195896863937378 3.255333185195923 17.896255493164062
Loss :  1.5019724369049072 2.9666173458099365 16.335060119628906
Loss :  1.5959570407867432 2.5408549308776855 14.30023193359375
Loss :  1.5637186765670776 2.8032989501953125 15.58021354675293
Loss :  1.5630742311477661 2.7797999382019043 15.462074279785156
Loss :  1.5168417692184448 2.872650146484375 15.88009262084961
Loss :  1.53847074508667 2.828961133956909 15.683277130126953
Loss :  1.5363072156906128 2.8181748390197754 15.627181053161621
Loss :  1.620026707649231 2.7983736991882324 15.611895561218262
Loss :  1.6248944997787476 2.6384644508361816 14.817215919494629
Loss :  1.6386597156524658 2.5785953998565674 14.531636238098145
  batch 40 loss: 1.6386597156524658, 2.5785953998565674, 14.531636238098145
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5825296640396118 3.072535753250122 16.945207595825195
Loss :  1.5678409337997437 2.648592233657837 14.81080150604248
Loss :  1.5479017496109009 2.5467164516448975 14.281484603881836
Loss :  1.5682860612869263 2.811255931854248 15.624566078186035
Loss :  1.5376638174057007 2.373076915740967 13.403048515319824
Loss :  1.5811820030212402 3.0251166820526123 16.70676612854004
Loss :  1.630655288696289 2.701566457748413 15.138487815856934
Loss :  1.560505747795105 3.5786733627319336 19.453872680664062
Loss :  1.6536245346069336 3.7405030727386475 20.35614013671875
Loss :  1.572074055671692 3.3491876125335693 18.318012237548828
Loss :  1.6153935194015503 3.3173043727874756 18.201913833618164
Loss :  1.6092274188995361 2.8833038806915283 16.025747299194336
Loss :  1.5801702737808228 2.822251081466675 15.691426277160645
Loss :  1.620234489440918 2.588134527206421 14.560907363891602
Loss :  1.5606880187988281 2.96225905418396 16.37198257446289
Loss :  1.650389313697815 2.58038592338562 14.552319526672363
Loss :  1.5670201778411865 2.5698320865631104 14.416180610656738
Loss :  1.5432090759277344 2.6908700466156006 14.997559547424316
Loss :  1.5626617670059204 3.1507060527801514 17.316192626953125
Loss :  1.6513301134109497 3.037607431411743 16.839366912841797
  batch 60 loss: 1.6513301134109497, 3.037607431411743, 16.839366912841797
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5409040451049805 3.003720283508301 16.559505462646484
Loss :  1.582707405090332 3.3060109615325928 18.112762451171875
Loss :  1.5533859729766846 2.313443899154663 13.12060546875
Loss :  1.536336898803711 2.743020534515381 15.251440048217773
Loss :  1.506224513053894 3.023818016052246 16.625314712524414
Loss :  1.5475150346755981 3.7354044914245605 20.224538803100586
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.562838077545166 3.643326997756958 19.77947235107422
Loss :  1.5583683252334595 3.6814687252044678 19.96571159362793
Loss :  1.571806788444519 3.505955457687378 19.10158348083496
Total LOSS train 15.82443155141977 valid 19.767826557159424
CE LOSS train 1.5791476084635807 valid 0.39295169711112976
Contrastive LOSS train 2.849056790425227 valid 0.8764888644218445
EPOCH 139:
Loss :  1.6011919975280762 3.017117977142334 16.686782836914062
Loss :  1.6179354190826416 4.071226596832275 21.97406768798828
Loss :  1.5743776559829712 2.2204842567443848 12.676798820495605
Loss :  1.5835254192352295 2.3023531436920166 13.095291137695312
Loss :  1.6075900793075562 2.5352022647857666 14.283600807189941
Loss :  1.5601446628570557 2.876716375350952 15.943726539611816
Loss :  1.611318588256836 3.7743330001831055 20.48298454284668
Loss :  1.576232671737671 3.1877329349517822 17.514896392822266
Loss :  1.5603164434432983 2.6225409507751465 14.67302131652832
Loss :  1.6070867776870728 2.6632843017578125 14.923508644104004
Loss :  1.545488953590393 2.89727520942688 16.031864166259766
Loss :  1.5450224876403809 2.571208953857422 14.401067733764648
Loss :  1.5411784648895264 2.547961473464966 14.280985832214355
Loss :  1.5507975816726685 2.778548002243042 15.443537712097168
Loss :  1.632290005683899 2.9437108039855957 16.35084342956543
Loss :  1.625298023223877 3.0855154991149902 17.052875518798828
Loss :  1.543993353843689 2.7667236328125 15.37761116027832
Loss :  1.5809301137924194 2.885444164276123 16.00815200805664
Loss :  1.5374842882156372 2.587395191192627 14.474459648132324
Loss :  1.6244086027145386 2.6251025199890137 14.749921798706055
  batch 20 loss: 1.6244086027145386, 2.6251025199890137, 14.749921798706055
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5735427141189575 2.579969644546509 14.473390579223633
Loss :  1.5377273559570312 2.4084293842315674 13.579874038696289
Loss :  1.5648903846740723 2.7256083488464355 15.19293212890625
Loss :  1.5862499475479126 2.707951307296753 15.126007080078125
Loss :  1.6254377365112305 2.9084715843200684 16.167797088623047
Loss :  1.568064570426941 2.672051429748535 14.928321838378906
Loss :  1.5813065767288208 2.9613730907440186 16.388172149658203
Loss :  1.5746281147003174 2.6972849369049072 15.061052322387695
Loss :  1.5040266513824463 2.9319052696228027 16.16355323791504
Loss :  1.6201978921890259 3.200165033340454 17.621023178100586
Loss :  1.503645896911621 2.7182693481445312 15.094992637634277
Loss :  1.5976574420928955 3.0242838859558105 16.719078063964844
Loss :  1.5647590160369873 2.5324370861053467 14.226944923400879
Loss :  1.5632716417312622 2.5870461463928223 14.498501777648926
Loss :  1.5167521238327026 2.560577869415283 14.31964111328125
Loss :  1.5384775400161743 3.2554855346679688 17.81590461730957
Loss :  1.537635326385498 2.7938129901885986 15.50670051574707
Loss :  1.621529459953308 3.48918080329895 19.067432403564453
Loss :  1.628144383430481 2.84818696975708 15.869078636169434
Loss :  1.6384105682373047 2.6361260414123535 14.819040298461914
  batch 40 loss: 1.6384105682373047, 2.6361260414123535, 14.819040298461914
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5842896699905396 2.901782989501953 16.093204498291016
Loss :  1.5670047998428345 2.3356354236602783 13.245182037353516
Loss :  1.5512794256210327 2.5590155124664307 14.346356391906738
Loss :  1.5712982416152954 2.7806286811828613 15.474441528320312
Loss :  1.541137456893921 2.11936092376709 12.13794231414795
Loss :  1.5819560289382935 2.675779104232788 14.960851669311523
Loss :  1.6296814680099487 2.4354026317596436 13.806694984436035
Loss :  1.5612040758132935 2.3000478744506836 13.061443328857422
Loss :  1.6523582935333252 2.5774896144866943 14.539806365966797
Loss :  1.5695216655731201 2.5862507820129395 14.500775337219238
Loss :  1.6142032146453857 3.134923219680786 17.288818359375
Loss :  1.6064610481262207 3.087934970855713 17.04613494873047
Loss :  1.578011155128479 2.884073495864868 15.99837875366211
Loss :  1.616395354270935 2.452714443206787 13.87996768951416
Loss :  1.5570178031921387 3.055743932723999 16.835737228393555
Loss :  1.6482425928115845 2.5641703605651855 14.469095230102539
Loss :  1.5718358755111694 2.720599412918091 15.174832344055176
Loss :  1.5516188144683838 2.74104380607605 15.256837844848633
Loss :  1.5739414691925049 3.1867332458496094 17.50760841369629
Loss :  1.6597093343734741 2.260946750640869 12.964442253112793
  batch 60 loss: 1.6597093343734741, 2.260946750640869, 12.964442253112793
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.554516077041626 2.496826171875 14.038646697998047
Loss :  1.5911304950714111 2.3768508434295654 13.475384712219238
Loss :  1.5641108751296997 2.238986015319824 12.759040832519531
Loss :  1.5464177131652832 2.6324169635772705 14.708501815795898
Loss :  1.519714117050171 2.55534029006958 14.296415328979492
Loss :  1.5526989698410034 3.953876256942749 21.322078704833984
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2], device='cuda:0')
Loss :  1.567976713180542 3.978609800338745 21.46102523803711
Loss :  1.5624480247497559 3.7592132091522217 20.3585147857666
Loss :  1.5793431997299194 3.8387300968170166 20.772994995117188
Total LOSS train 15.337415034954365 valid 20.97865343093872
CE LOSS train 1.5790157538193923 valid 0.39483579993247986
Contrastive LOSS train 2.7516798679645245 valid 0.9596825242042542
EPOCH 140:
Loss :  1.6116480827331543 2.5952649116516113 14.587972640991211
Loss :  1.6287436485290527 3.190441370010376 17.580949783325195
Loss :  1.586890697479248 2.57625675201416 14.46817398071289
Loss :  1.5957248210906982 2.493527412414551 14.063362121582031
Loss :  1.6173229217529297 2.869678497314453 15.965715408325195
Loss :  1.5716930627822876 2.8918187618255615 16.030786514282227
Loss :  1.6199240684509277 2.6458077430725098 14.848962783813477
Loss :  1.5847797393798828 2.310105085372925 13.135305404663086
Loss :  1.5704437494277954 2.343653678894043 13.288712501525879
Loss :  1.6193671226501465 3.3610148429870605 18.424442291259766
Loss :  1.5590742826461792 3.6361541748046875 19.739845275878906
Loss :  1.5564872026443481 2.8762435913085938 15.937705039978027
Loss :  1.5526067018508911 2.453442096710205 13.819816589355469
Loss :  1.558882236480713 2.7407708168029785 15.262735366821289
Loss :  1.6371480226516724 3.084122657775879 17.057762145996094
Loss :  1.6306816339492798 3.6242446899414062 19.75190544128418
Loss :  1.5482035875320435 2.40985369682312 13.597472190856934
Loss :  1.5859758853912354 2.7227494716644287 15.199723243713379
Loss :  1.5415587425231934 2.191080331802368 12.496959686279297
Loss :  1.6288076639175415 2.0729002952575684 11.99330997467041
  batch 20 loss: 1.6288076639175415, 2.0729002952575684, 11.99330997467041
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.577424168586731 2.4213218688964844 13.684033393859863
Loss :  1.5417083501815796 3.028552770614624 16.684473037719727
Loss :  1.568938136100769 2.302109718322754 13.079486846923828
Loss :  1.5892268419265747 3.485182285308838 19.015138626098633
Loss :  1.6282130479812622 3.1862688064575195 17.55955696105957
Loss :  1.5735695362091064 2.700998544692993 15.078561782836914
Loss :  1.585904836654663 2.8190793991088867 15.681302070617676
Loss :  1.5771980285644531 2.6381380558013916 14.767888069152832
Loss :  1.5053528547286987 2.6199886798858643 14.60529613494873
Loss :  1.620849609375 2.787837266921997 15.560035705566406
Loss :  1.50608491897583 2.4139530658721924 13.575849533081055
Loss :  1.5985212326049805 2.8609812259674072 15.903427124023438
Loss :  1.5659559965133667 2.993715763092041 16.534534454345703
Loss :  1.5649361610412598 2.469954013824463 13.914705276489258
Loss :  1.5178906917572021 2.8002231121063232 15.519006729125977
Loss :  1.5398706197738647 2.5969185829162598 14.524462699890137
Loss :  1.5373766422271729 3.0670063495635986 16.872407913208008
Loss :  1.6197314262390137 2.6830978393554688 15.035221099853516
Loss :  1.624462366104126 2.5902435779571533 14.575679779052734
Loss :  1.63714599609375 2.8483660221099854 15.878975868225098
  batch 40 loss: 1.63714599609375, 2.8483660221099854, 15.878975868225098
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5801852941513062 3.1475064754486084 17.317718505859375
Loss :  1.5641000270843506 2.1275370121002197 12.20178508758545
Loss :  1.5488982200622559 2.831861734390259 15.708206176757812
Loss :  1.570471167564392 2.6194233894348145 14.667587280273438
Loss :  1.541822910308838 2.427299737930298 13.678321838378906
Loss :  1.5845682621002197 3.2177321910858154 17.673229217529297
Loss :  1.631796956062317 3.2354769706726074 17.80918312072754
Loss :  1.5622414350509644 2.6013829708099365 14.569156646728516
Loss :  1.6527578830718994 2.5710456371307373 14.507986068725586
Loss :  1.5731676816940308 2.38503098487854 13.498322486877441
Loss :  1.617142915725708 2.6041135787963867 14.637710571289062
Loss :  1.611189842224121 2.8901801109313965 16.062091827392578
Loss :  1.582087755203247 2.653735637664795 14.8507661819458
Loss :  1.622151255607605 2.7432799339294434 15.33855152130127
Loss :  1.561213493347168 2.3215372562408447 13.168899536132812
Loss :  1.651206612586975 2.26229190826416 12.962666511535645
Loss :  1.5742247104644775 2.394422769546509 13.546338081359863
Loss :  1.5518466234207153 2.960418939590454 16.353940963745117
Loss :  1.5734502077102661 2.990844488143921 16.527673721313477
Loss :  1.6597620248794556 2.429835081100464 13.808938026428223
  batch 60 loss: 1.6597620248794556, 2.429835081100464, 13.808938026428223
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5556442737579346 2.472423791885376 13.917762756347656
Loss :  1.5910391807556152 2.552278757095337 14.352432250976562
Loss :  1.565953016281128 2.4298226833343506 13.715066909790039
Loss :  1.5478131771087646 3.3220701217651367 18.158164978027344
Loss :  1.5220891237258911 2.0557143688201904 11.800661087036133
Loss :  1.5557200908660889 4.34049654006958 23.258201599121094
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.5708433389663696 4.375646114349365 23.449073791503906
Loss :  1.5649532079696655 4.2424163818359375 22.777034759521484
Loss :  1.581247329711914 4.388531684875488 23.52390480041504
Total LOSS train 15.171304966853215 valid 23.25205373764038
CE LOSS train 1.5828176828531118 valid 0.3953118324279785
Contrastive LOSS train 2.717697451664851 valid 1.097132921218872
EPOCH 141:
Loss :  1.6158902645111084 2.256654977798462 12.899165153503418
Loss :  1.63191819190979 2.5971615314483643 14.617725372314453
Loss :  1.5911409854888916 2.549407958984375 14.338180541992188
Loss :  1.5990995168685913 2.8787224292755127 15.992711067199707
Loss :  1.619131088256836 2.315265417098999 13.19545841217041
Loss :  1.5731545686721802 2.7433741092681885 15.290024757385254
Loss :  1.618695855140686 2.950303554534912 16.370214462280273
Loss :  1.5849794149398804 1.985780119895935 11.513879776000977
Loss :  1.5726057291030884 2.5463876724243164 14.304544448852539
Loss :  1.6219228506088257 2.603726387023926 14.640554428100586
Loss :  1.566282033920288 2.865662097930908 15.89459228515625
Loss :  1.5663617849349976 3.04960560798645 16.814390182495117
Loss :  1.5636882781982422 2.490806818008423 14.017722129821777
Loss :  1.5698891878128052 2.614733934402466 14.643558502197266
Loss :  1.6450040340423584 2.7545359134674072 15.417683601379395
Loss :  1.6422868967056274 2.8167784214019775 15.726179122924805
Loss :  1.5636725425720215 2.7975926399230957 15.5516357421875
Loss :  1.6009409427642822 2.5176403522491455 14.189142227172852
Loss :  1.5582009553909302 4.132235527038574 22.219379425048828
Loss :  1.643815040588379 3.590987205505371 19.598751068115234
  batch 20 loss: 1.643815040588379, 3.590987205505371, 19.598751068115234
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5915300846099854 3.257605791091919 17.879558563232422
Loss :  1.5552492141723633 2.7356514930725098 15.233506202697754
Loss :  1.580058217048645 2.4439878463745117 13.799997329711914
Loss :  1.5980437994003296 2.6964917182922363 15.080501556396484
Loss :  1.635129690170288 2.828303098678589 15.77664566040039
Loss :  1.5828226804733276 3.275203227996826 17.958839416503906
Loss :  1.594020962715149 3.2376015186309814 17.782028198242188
Loss :  1.587982416152954 2.756899833679199 15.372481346130371
Loss :  1.5197358131408691 2.8015921115875244 15.52769660949707
Loss :  1.6308015584945679 2.803133726119995 15.646470069885254
Loss :  1.5185554027557373 2.5437777042388916 14.237443923950195
Loss :  1.6080938577651978 2.624892234802246 14.732555389404297
Loss :  1.5777461528778076 3.4181807041168213 18.668649673461914
Loss :  1.576772928237915 2.75712513923645 15.362398147583008
Loss :  1.528395414352417 3.451324462890625 18.785017013549805
Loss :  1.549259066581726 2.505533456802368 14.076926231384277
Loss :  1.5464822053909302 2.661686658859253 14.854915618896484
Loss :  1.6289715766906738 2.505385637283325 14.155899047851562
Loss :  1.6329638957977295 2.602447509765625 14.645201683044434
Loss :  1.6441199779510498 3.275820255279541 18.02322006225586
  batch 40 loss: 1.6441199779510498, 3.275820255279541, 18.02322006225586
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5867806673049927 2.478256940841675 13.978065490722656
Loss :  1.57027268409729 2.567601442337036 14.408279418945312
Loss :  1.5530850887298584 2.5135865211486816 14.121017456054688
Loss :  1.5745726823806763 2.4045052528381348 13.597098350524902
Loss :  1.545264720916748 3.1398305892944336 17.244417190551758
Loss :  1.587304949760437 2.662665367126465 14.90063190460205
Loss :  1.634888768196106 3.3597376346588135 18.433578491210938
Loss :  1.5659703016281128 2.7725486755371094 15.42871379852295
Loss :  1.6567761898040771 2.3777763843536377 13.545658111572266
Loss :  1.5750936269760132 2.094701051712036 12.048598289489746
Loss :  1.6191555261611938 2.330949544906616 13.273903846740723
Loss :  1.6138513088226318 2.3101627826690674 13.164665222167969
Loss :  1.5867371559143066 2.2867014408111572 13.020244598388672
Loss :  1.6286685466766357 2.8689699172973633 15.973518371582031
Loss :  1.5680676698684692 2.556314706802368 14.349640846252441
Loss :  1.6602673530578613 2.3857507705688477 13.589021682739258
Loss :  1.5814294815063477 4.312013149261475 23.141494750976562
Loss :  1.5578171014785767 2.803524971008301 15.57544231414795
Loss :  1.579463243484497 3.4560461044311523 18.859695434570312
Loss :  1.665016531944275 2.633692741394043 14.833479881286621
  batch 60 loss: 1.665016531944275, 2.633692741394043, 14.833479881286621
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5627315044403076 2.2163381576538086 12.64442253112793
Loss :  1.598168969154358 2.6228339672088623 14.712339401245117
Loss :  1.5722118616104126 2.4426004886627197 13.7852144241333
Loss :  1.554115891456604 2.8707492351531982 15.907862663269043
Loss :  1.5248396396636963 2.255254030227661 12.801109313964844
Loss :  1.5558075904846191 3.50308895111084 19.071252822875977
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5695616006851196 3.5648393630981445 19.393756866455078
Loss :  1.5673668384552002 3.5026209354400635 19.080472946166992
Loss :  1.5770851373672485 3.383176565170288 18.49296760559082
Total LOSS train 15.356516280541053 valid 19.009612560272217
CE LOSS train 1.5901225621883686 valid 0.39427128434181213
Contrastive LOSS train 2.753278748805706 valid 0.845794141292572
EPOCH 142:
Loss :  1.6141505241394043 3.766596555709839 20.447134017944336
Loss :  1.628425121307373 2.908618927001953 16.171520233154297
Loss :  1.588778018951416 3.7584309577941895 20.380931854248047
Loss :  1.5972453355789185 3.2139852046966553 17.667171478271484
Loss :  1.619344711303711 2.246624708175659 12.852468490600586
Loss :  1.5731680393218994 2.6334660053253174 14.740497589111328
Loss :  1.6227658987045288 2.900740146636963 16.126466751098633
Loss :  1.5888415575027466 2.679666757583618 14.987174987792969
Loss :  1.576308012008667 2.7551145553588867 15.35188102722168
Loss :  1.62510347366333 2.3590307235717773 13.420257568359375
Loss :  1.5657622814178467 2.545961618423462 14.295570373535156
Loss :  1.5644665956497192 2.464425563812256 13.886594772338867
Loss :  1.559611201286316 2.671015977859497 14.914690971374512
Loss :  1.5658152103424072 2.5491766929626465 14.311698913574219
Loss :  1.6423536539077759 2.624441623687744 14.764561653137207
Loss :  1.637825608253479 3.0179638862609863 16.727643966674805
Loss :  1.5566388368606567 2.639061450958252 14.751945495605469
Loss :  1.5948936939239502 3.1788129806518555 17.48895835876465
Loss :  1.5508263111114502 2.6223270893096924 14.662461280822754
Loss :  1.6339495182037354 3.0103790760040283 16.68584442138672
  batch 20 loss: 1.6339495182037354, 3.0103790760040283, 16.68584442138672
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.5838336944580078 2.424410343170166 13.70588493347168
Loss :  1.5498626232147217 2.752264976501465 15.311187744140625
Loss :  1.575490951538086 2.9107606410980225 16.129295349121094
Loss :  1.593692421913147 3.8170969486236572 20.67917823791504
Loss :  1.63115394115448 2.8566250801086426 15.914278984069824
Loss :  1.5760829448699951 2.6099705696105957 14.625935554504395
Loss :  1.586515188217163 2.744781494140625 15.310422897338867
Loss :  1.5772383213043213 2.8459503650665283 15.806989669799805
Loss :  1.5095001459121704 4.520855903625488 24.113779067993164
Loss :  1.6168078184127808 3.050673484802246 16.870174407958984
Loss :  1.509537696838379 3.089322090148926 16.956148147583008
Loss :  1.6062272787094116 4.745067119598389 25.33156394958496
Loss :  1.5676683187484741 3.5478620529174805 19.30698013305664
Loss :  1.5669866800308228 3.3705108165740967 18.419540405273438
Loss :  1.526611328125 3.417102575302124 18.612123489379883
Loss :  1.5476655960083008 3.431802988052368 18.706680297851562
Loss :  1.5475927591323853 3.413078784942627 18.612985610961914
Loss :  1.625219702720642 3.2283623218536377 17.767030715942383
Loss :  1.6473137140274048 2.9754655361175537 16.524641036987305
Loss :  1.6594575643539429 3.1285760402679443 17.302337646484375
  batch 40 loss: 1.6594575643539429, 3.1285760402679443, 17.302337646484375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 5], device='cuda:0')
Loss :  1.5972678661346436 2.7491726875305176 15.343131065368652
Loss :  1.5735607147216797 3.8506264686584473 20.826692581176758
Loss :  1.5557355880737305 3.9716343879699707 21.41390609741211
Loss :  1.5734632015228271 3.9339993000030518 21.243459701538086
Loss :  1.540352702140808 4.627344608306885 24.677074432373047
Loss :  1.579919695854187 4.568276882171631 24.42130470275879
Loss :  1.6218979358673096 4.648056983947754 24.8621826171875
Loss :  1.554593563079834 3.9607768058776855 21.358478546142578
Loss :  1.639947533607483 3.968268632888794 21.481290817260742
Loss :  1.5462982654571533 4.343501091003418 23.263805389404297
Loss :  1.5959511995315552 4.62514066696167 24.721654891967773
Loss :  1.5826799869537354 4.595885276794434 24.56210708618164
Loss :  1.5411982536315918 4.366099834442139 23.3716983795166
Loss :  1.6053309440612793 4.2782511711120605 22.9965877532959
Loss :  1.5204405784606934 4.533717155456543 24.189027786254883
Loss :  1.6118968725204468 4.528204441070557 24.252918243408203
Loss :  1.52668297290802 4.5211100578308105 24.132234573364258
Loss :  1.5146187543869019 4.367166996002197 23.350452423095703
Loss :  1.5248364210128784 3.791905403137207 20.48436164855957
Loss :  1.6256604194641113 4.276166915893555 23.006494522094727
  batch 60 loss: 1.6256604194641113, 4.276166915893555, 23.006494522094727
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4, 5], device='cuda:0')
Loss :  1.4942517280578613 3.7970991134643555 20.479747772216797
Loss :  1.5436033010482788 3.8117854595184326 20.60253143310547
Loss :  1.5085951089859009 3.9370172023773193 21.193679809570312
Loss :  1.4969950914382935 4.319893836975098 23.096464157104492
Loss :  1.4557503461837769 3.769519329071045 20.303346633911133
Loss :  1.6459898948669434 4.445387363433838 23.872926712036133
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.6974936723709106 4.411275386810303 23.75387191772461
Loss :  1.715692162513733 4.352339744567871 23.477392196655273
Loss :  1.5221970081329346 4.251504421234131 22.779720306396484
Total LOSS train 18.92734254690317 valid 23.470977783203125
CE LOSS train 1.5760347898189837 valid 0.38054925203323364
Contrastive LOSS train 3.4702615591195913 valid 1.0628761053085327
EPOCH 143:
Loss :  1.5521972179412842 4.362686634063721 23.365629196166992
Loss :  1.564605474472046 4.0172858238220215 21.65103530883789
Loss :  1.5239819288253784 4.037519454956055 21.711578369140625
Loss :  1.5376919507980347 3.512279987335205 19.099090576171875
Loss :  1.559733510017395 4.166879653930664 22.394132614135742
Loss :  1.4891839027404785 4.478248119354248 23.88042449951172
Loss :  1.5505728721618652 4.4657697677612305 23.879423141479492
Loss :  1.5047495365142822 4.0787882804870605 21.898691177368164
Loss :  1.492153286933899 3.146711587905884 17.225711822509766
Loss :  1.5470207929611206 3.02420711517334 16.66805648803711
Loss :  1.4697562456130981 3.0958809852600098 16.949161529541016
Loss :  1.4714112281799316 3.6552882194519043 19.747852325439453
Loss :  1.4675489664077759 3.664400577545166 19.78955078125
Loss :  1.484887957572937 3.532078504562378 19.145280838012695
Loss :  1.5895864963531494 3.4653708934783936 18.916440963745117
Loss :  1.578499674797058 3.9574358463287354 21.365678787231445
Loss :  1.4614195823669434 3.3046815395355225 17.984827041625977
Loss :  1.508239984512329 4.522730827331543 24.12189483642578
Loss :  1.4550367593765259 3.7933781147003174 20.421926498413086
Loss :  1.564745545387268 3.6034696102142334 19.58209228515625
  batch 20 loss: 1.564745545387268, 3.6034696102142334, 19.58209228515625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.4988640546798706 3.4078612327575684 18.538169860839844
Loss :  1.4468852281570435 4.074886798858643 21.821319580078125
Loss :  1.4816889762878418 3.9111313819885254 21.03734588623047
Loss :  1.5135465860366821 3.693840742111206 19.982749938964844
Loss :  1.572851538658142 3.66127872467041 19.879243850708008
Loss :  1.4898014068603516 3.399695634841919 18.488279342651367
Loss :  1.506677269935608 3.338242769241333 18.197891235351562
Loss :  1.4920958280563354 2.8085665702819824 15.534929275512695
Loss :  1.3911304473876953 2.915677785873413 15.96951961517334
Loss :  1.557210922241211 2.936554193496704 16.23998260498047
Loss :  1.391294240951538 3.052976369857788 16.65617561340332
Loss :  1.5278041362762451 3.1775152683258057 17.415380477905273
Loss :  1.4771417379379272 2.7500827312469482 15.227555274963379
Loss :  1.4721581935882568 3.1718695163726807 17.331504821777344
Loss :  1.40597403049469 3.3884685039520264 18.348316192626953
Loss :  1.4344619512557983 3.099623918533325 16.932580947875977
Loss :  1.4316508769989014 2.6151232719421387 14.507267951965332
Loss :  1.5479007959365845 3.3667500019073486 18.381650924682617
Loss :  1.558650016784668 2.781572103500366 15.466510772705078
Loss :  1.577397346496582 2.2848095893859863 13.001444816589355
  batch 40 loss: 1.577397346496582, 2.2848095893859863, 13.001444816589355
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.4961345195770264 2.5936391353607178 14.464330673217773
Loss :  1.4661670923233032 2.6955502033233643 14.943918228149414
Loss :  1.447770118713379 2.5850703716278076 14.373122215270996
Loss :  1.4720617532730103 2.812406301498413 15.534093856811523
Loss :  1.4336472749710083 2.4815797805786133 13.841546058654785
Loss :  1.4932355880737305 3.355292558670044 18.269699096679688
Loss :  1.5560252666473389 3.309067964553833 18.101364135742188
Loss :  1.4675871133804321 3.291342258453369 17.924297332763672
Loss :  1.5872541666030884 3.250654935836792 17.84052848815918
Loss :  1.4744303226470947 2.7703025341033936 15.325942993164062
Loss :  1.5428966283798218 2.8775699138641357 15.930746078491211
Loss :  1.5311092138290405 2.8159148693084717 15.61068344116211
Loss :  1.4909849166870117 3.143550157546997 17.208736419677734
Loss :  1.5510613918304443 3.160360097885132 17.352861404418945
Loss :  1.4729865789413452 2.9978671073913574 16.462322235107422
Loss :  1.5993597507476807 2.7136716842651367 15.167717933654785
Loss :  1.4912749528884888 3.266051769256592 17.821535110473633
Loss :  1.4583631753921509 3.075791358947754 16.83732032775879
Loss :  1.4941730499267578 3.2996764183044434 17.992555618286133
Loss :  1.6214576959609985 2.8918488025665283 16.08070182800293
  batch 60 loss: 1.6214576959609985, 2.8918488025665283, 16.08070182800293
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 5], device='cuda:0')
Loss :  1.4676952362060547 3.0477001667022705 16.706195831298828
Loss :  1.5290305614471436 3.586812734603882 19.463092803955078
Loss :  1.4877564907073975 2.5537142753601074 14.256328582763672
Loss :  1.4657697677612305 3.1723201274871826 17.327369689941406
Loss :  1.4279855489730835 2.5319595336914062 14.087782859802246
Loss :  1.4842400550842285 4.006834506988525 21.51841163635254
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5094321966171265 3.976529598236084 21.392080307006836
Loss :  1.5019207000732422 3.752861738204956 20.2662296295166
Loss :  1.5124614238739014 3.8996200561523438 21.010560989379883
Total LOSS train 17.964324481670673 valid 21.046820640563965
CE LOSS train 1.5026834873052743 valid 0.37811535596847534
Contrastive LOSS train 3.2923282109774075 valid 0.9749050140380859
EPOCH 144:
Loss :  1.5600473880767822 2.667046070098877 14.89527702331543
Loss :  1.5847952365875244 3.175724744796753 17.46341896057129
Loss :  1.5239498615264893 2.7849247455596924 15.448573112487793
Loss :  1.535367727279663 3.5041725635528564 19.056230545043945
Loss :  1.5718587636947632 2.761976718902588 15.381741523742676
Loss :  1.5001981258392334 2.3990538120269775 13.495467185974121
Loss :  1.5693470239639282 2.456343173980713 13.851062774658203
Loss :  1.5216103792190552 2.3017044067382812 13.030132293701172
Loss :  1.502703309059143 2.3232674598693848 13.119040489196777
Loss :  1.5730087757110596 2.6683433055877686 14.914725303649902
Loss :  1.4876233339309692 2.839322090148926 15.684233665466309
Loss :  1.4873476028442383 2.67997145652771 14.887205123901367
Loss :  1.4801851511001587 2.821289300918579 15.586631774902344
Loss :  1.494622826576233 2.7970876693725586 15.480061531066895
Loss :  1.5992120504379272 3.9873759746551514 21.53609275817871
Loss :  1.594884991645813 2.6813933849334717 15.001852035522461
Loss :  1.475712537765503 2.967635154724121 16.313888549804688
Loss :  1.5323266983032227 2.6330502033233643 14.697577476501465
Loss :  1.4724719524383545 2.2831833362579346 12.888388633728027
Loss :  1.587629795074463 2.929959774017334 16.237428665161133
  batch 20 loss: 1.587629795074463, 2.929959774017334, 16.237428665161133
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.5203137397766113 2.1719093322753906 12.379859924316406
Loss :  1.4682660102844238 2.928929090499878 16.112911224365234
Loss :  1.5040587186813354 2.2899887561798096 12.954002380371094
Loss :  1.5355807542800903 2.7186126708984375 15.128643989562988
Loss :  1.5939052104949951 3.6074678897857666 19.631244659423828
Loss :  1.5115959644317627 2.5187270641326904 14.105231285095215
Loss :  1.530867576599121 3.0317864418029785 16.689800262451172
Loss :  1.5191588401794434 2.699028491973877 15.014301300048828
Loss :  1.4182429313659668 3.1761794090270996 17.29914093017578
Loss :  1.5843687057495117 3.157209873199463 17.370418548583984
Loss :  1.4201263189315796 3.191256523132324 17.37640953063965
Loss :  1.5552396774291992 3.8935086727142334 21.022781372070312
Loss :  1.5069844722747803 2.865771532058716 15.83584213256836
Loss :  1.503217339515686 2.9339334964752197 16.172883987426758
Loss :  1.4361575841903687 2.681286573410034 14.84259033203125
Loss :  1.4664356708526611 2.4678685665130615 13.805778503417969
Loss :  1.4660862684249878 2.4253153800964355 13.592663764953613
Loss :  1.584249496459961 2.8884992599487305 16.026744842529297
Loss :  1.59409499168396 2.8282365798950195 15.735278129577637
Loss :  1.6117560863494873 2.5149142742156982 14.186327934265137
  batch 40 loss: 1.6117560863494873, 2.5149142742156982, 14.186327934265137
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
Loss :  1.5378540754318237 2.716212034225464 15.118914604187012
Loss :  1.508466362953186 2.4179322719573975 13.598128318786621
Loss :  1.490560531616211 2.3606839179992676 13.29397964477539
Loss :  1.5165212154388428 2.3234426975250244 13.133734703063965
Loss :  1.475857138633728 2.069467067718506 11.823192596435547
Loss :  1.5308133363723755 2.8837172985076904 15.949399948120117
Loss :  1.591719150543213 2.812835216522217 15.655895233154297
Loss :  1.5043917894363403 2.3128974437713623 13.068879127502441
Loss :  1.618241310119629 2.148590326309204 12.36119270324707
Loss :  1.514729619026184 2.2360339164733887 12.694899559020996
Loss :  1.5773288011550903 2.3370649814605713 13.262653350830078
Loss :  1.5665472745895386 2.3939805030822754 13.536449432373047
Loss :  1.5257865190505981 2.5868146419525146 14.459859848022461
Loss :  1.5810312032699585 2.6072728633880615 14.617395401000977
Loss :  1.5034468173980713 2.14866042137146 12.246748924255371
Loss :  1.6214544773101807 3.0984716415405273 17.113813400268555
Loss :  1.5142282247543335 2.604935884475708 14.538907051086426
Loss :  1.4817034006118774 2.444699764251709 13.70520305633545
Loss :  1.5093001127243042 2.858678102493286 15.802690505981445
Loss :  1.6274162530899048 2.283611536026001 13.0454740524292
  batch 60 loss: 1.6274162530899048, 2.283611536026001, 13.0454740524292
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
Loss :  1.47945237159729 2.1884024143218994 12.421463966369629
Loss :  1.5364203453063965 2.416109800338745 13.61697006225586
Loss :  1.4973506927490234 2.0586533546447754 11.790616989135742
Loss :  1.475851058959961 2.7154946327209473 15.053323745727539
Loss :  1.4385699033737183 2.336871862411499 13.122929573059082
Loss :  1.50644850730896 3.7622218132019043 20.31755828857422
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 4, 5], device='cuda:0')
Loss :  1.5308703184127808 3.82198429107666 20.640790939331055
Loss :  1.5221456289291382 3.587801694869995 19.46115493774414
Loss :  1.540109395980835 3.511484384536743 19.097532272338867
Total LOSS train 14.912070773198055 valid 19.87925910949707
CE LOSS train 1.5263177211468035 valid 0.38502734899520874
Contrastive LOSS train 2.677150612611037 valid 0.8778710961341858
EPOCH 145:
Loss :  1.5591802597045898 3.0005462169647217 16.561912536621094
Loss :  1.5829808712005615 2.360147714614868 13.383719444274902
Loss :  1.5271061658859253 2.016605854034424 11.610135078430176
Loss :  1.5381033420562744 2.5638270378112793 14.35723876953125
Loss :  1.5707881450653076 2.4437990188598633 13.789783477783203
Loss :  1.5022627115249634 2.4678866863250732 13.841696739196777
Loss :  1.5687799453735352 2.3408918380737305 13.273239135742188
Loss :  1.5216856002807617 2.253154754638672 12.787459373474121
Loss :  1.504035472869873 2.4038240909576416 13.523155212402344
Loss :  1.5683915615081787 2.005347728729248 11.595130920410156
Loss :  1.4883861541748047 2.5615689754486084 14.296231269836426
Loss :  1.4884108304977417 2.5749828815460205 14.363325119018555
Loss :  1.4841434955596924 2.2935400009155273 12.95184326171875
Loss :  1.4992643594741821 2.8578989505767822 15.788759231567383
Loss :  1.6017770767211914 2.2093429565429688 12.648491859436035
Loss :  1.5940788984298706 2.253812551498413 12.863142013549805
Loss :  1.4873734712600708 2.1963396072387695 12.469071388244629
Loss :  1.5370821952819824 2.4861197471618652 13.967681884765625
Loss :  1.4845750331878662 2.604492664337158 14.507038116455078
Loss :  1.5957577228546143 2.592146635055542 14.556490898132324
  batch 20 loss: 1.5957577228546143, 2.592146635055542, 14.556490898132324
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
Loss :  1.531200647354126 2.5069732666015625 14.06606674194336
Loss :  1.4813834428787231 2.085953950881958 11.911152839660645
Loss :  1.513175368309021 2.3445160388946533 13.23575496673584
Loss :  1.5412629842758179 2.4601898193359375 13.842211723327637
Loss :  1.594649314880371 2.2943227291107178 13.066263198852539
Loss :  1.5172111988067627 2.481720447540283 13.925812721252441
Loss :  1.5344988107681274 2.632098913192749 14.69499397277832
Loss :  1.5229053497314453 2.5722970962524414 14.384390830993652
Loss :  1.4277005195617676 2.649099588394165 14.673198699951172
Loss :  1.581928014755249 3.761967420578003 20.391765594482422
Loss :  1.4284926652908325 2.3378679752349854 13.11783218383789
Loss :  1.5517210960388184 2.344728946685791 13.275365829467773
Loss :  1.506813406944275 2.5141122341156006 14.077374458312988
Loss :  1.5038524866104126 2.5731353759765625 14.369529724121094
Loss :  1.4423798322677612 2.699582815170288 14.94029426574707
Loss :  1.4699739217758179 2.6634414196014404 14.78718090057373
Loss :  1.469316840171814 2.5242631435394287 14.090632438659668
Loss :  1.579813003540039 2.305039405822754 13.105010032653809
Loss :  1.5895119905471802 2.3469700813293457 13.324361801147461
Loss :  1.6083647012710571 2.7391445636749268 15.30408763885498
  batch 40 loss: 1.6083647012710571, 2.7391445636749268, 15.30408763885498
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
Loss :  1.539977788925171 2.6155314445495605 14.617635726928711
Loss :  1.5177632570266724 1.8205304145812988 10.620414733886719
Loss :  1.5034112930297852 2.4644927978515625 13.825875282287598
Loss :  1.5279593467712402 2.033348321914673 11.694700241088867
Loss :  1.4892172813415527 1.9894664287567139 11.43655014038086
Loss :  1.5407497882843018 2.491210460662842 13.99680233001709
Loss :  1.5988671779632568 2.660435676574707 14.901045799255371
Loss :  1.514673113822937 2.4508845806121826 13.769096374511719
Loss :  1.6238558292388916 2.100395917892456 12.125835418701172
Loss :  1.5255123376846313 1.9215400218963623 11.13321304321289
Loss :  1.5840970277786255 2.116816282272339 12.16817855834961
Loss :  1.5711078643798828 3.835522413253784 20.748720169067383
Loss :  1.5336263179779053 2.1182188987731934 12.12472152709961
Loss :  1.583653450012207 2.8097259998321533 15.632283210754395
Loss :  1.5121670961380005 2.293395757675171 12.979146003723145
Loss :  1.623658299446106 2.3435213565826416 13.341264724731445
Loss :  1.5237771272659302 2.444584608078003 13.746700286865234
Loss :  1.4953255653381348 2.1307294368743896 12.14897346496582
Loss :  1.522464632987976 2.3515241146087646 13.280085563659668
Loss :  1.632521629333496 1.9201513528823853 11.233278274536133
  batch 60 loss: 1.632521629333496, 1.9201513528823853, 11.233278274536133
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4, 5], device='cuda:0')
Loss :  1.4973416328430176 2.1679697036743164 12.337190628051758
Loss :  1.552058219909668 2.3627381324768066 13.365748405456543
Loss :  1.5162231922149658 2.329068899154663 13.161567687988281
Loss :  1.496458649635315 2.608423948287964 14.538578987121582
Loss :  1.4612360000610352 1.9233438968658447 11.07795524597168
Loss :  1.515353798866272 4.1695146560668945 22.362926483154297
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.5331369638442993 4.276885986328125 22.917566299438477
Loss :  1.5374351739883423 4.144421577453613 22.25954246520996
Loss :  1.513203740119934 4.070103645324707 21.86372184753418
Total LOSS train 13.657298278808593 valid 22.35093927383423
CE LOSS train 1.5321234281246479 valid 0.3783009350299835
Contrastive LOSS train 2.4250349539976854 valid 1.0175259113311768
EPOCH 146:
Loss :  1.5751149654388428 2.0899159908294678 12.02469539642334
Loss :  1.5964725017547607 2.5820605754852295 14.506775856018066
Loss :  1.5427682399749756 2.7810165882110596 15.447851181030273
Loss :  1.5533918142318726 2.940976858139038 16.258275985717773
Loss :  1.585506558418274 2.1768412590026855 12.46971321105957
Loss :  1.5209945440292358 2.221069574356079 12.626341819763184
Loss :  1.5835040807724 1.9885478019714355 11.526243209838867
Loss :  1.5399147272109985 2.5955114364624023 14.517472267150879
Loss :  1.5221949815750122 2.235454559326172 12.699467658996582
Loss :  1.5827783346176147 1.9422580003738403 11.294068336486816
Loss :  1.5056099891662598 2.1908822059631348 12.460020065307617
Loss :  1.5023545026779175 2.2457995414733887 12.731352806091309
Loss :  1.4971818923950195 3.9799461364746094 21.39691162109375
Loss :  1.5084216594696045 2.620260715484619 14.609724998474121
Loss :  1.6079320907592773 2.8810958862304688 16.013412475585938
Loss :  1.596580982208252 2.584167957305908 14.517419815063477
Loss :  1.4919339418411255 2.358407735824585 13.28397274017334
Loss :  1.5373742580413818 2.912964105606079 16.10219383239746
Loss :  1.4874200820922852 2.9937243461608887 16.456043243408203
Loss :  1.5955677032470703 3.1224708557128906 17.207921981811523
  batch 20 loss: 1.5955677032470703, 3.1224708557128906, 17.207921981811523
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
Loss :  1.5364516973495483 2.2395541667938232 12.734222412109375
Loss :  1.491660475730896 2.5868306159973145 14.425812721252441
Loss :  1.524036169052124 2.6969058513641357 15.008565902709961
Loss :  1.5526461601257324 2.8357532024383545 15.731412887573242
Loss :  1.602649211883545 2.77305006980896 15.467899322509766
Loss :  1.5283900499343872 2.3698580265045166 13.377679824829102
Loss :  1.5429677963256836 2.0351579189300537 11.718757629394531
Loss :  1.5323984622955322 2.20424485206604 12.553622245788574
Loss :  1.442879319190979 2.1883814334869385 12.384786605834961
Loss :  1.5898101329803467 2.16252064704895 12.402413368225098
Loss :  1.4446823596954346 2.5368733406066895 14.129048347473145
Loss :  1.56417715549469 2.009725332260132 11.612804412841797
Loss :  1.5226693153381348 2.0507118701934814 11.776227951049805
Loss :  1.5198769569396973 3.2893903255462646 17.966827392578125
Loss :  1.4616203308105469 2.366673469543457 13.294987678527832
Loss :  1.4874528646469116 2.839651584625244 15.685709953308105
Loss :  1.486396312713623 2.897658348083496 15.974687576293945
Loss :  1.5915627479553223 2.673612117767334 14.959623336791992
Loss :  1.6012678146362305 2.737508535385132 15.288810729980469
Loss :  1.6176676750183105 2.4942212104797363 14.088773727416992
  batch 40 loss: 1.6176676750183105, 2.4942212104797363, 14.088773727416992
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
Loss :  1.5489530563354492 2.5642950534820557 14.370428085327148
Loss :  1.522712230682373 2.123878002166748 12.14210319519043
Loss :  1.5047285556793213 2.4383890628814697 13.696673393249512
Loss :  1.5260019302368164 2.1115121841430664 12.083562850952148
Loss :  1.489254355430603 1.8315627574920654 10.64706802368164
Loss :  1.5379693508148193 2.3399658203125 13.237798690795898
Loss :  1.5943565368652344 2.2623302936553955 12.906007766723633
Loss :  1.5173193216323853 2.545703649520874 14.245838165283203
Loss :  1.6219223737716675 2.092979669570923 12.086820602416992
Loss :  1.5274039506912231 1.9727885723114014 11.39134693145752
Loss :  1.5866819620132446 2.6231396198272705 14.702380180358887
Loss :  1.5748741626739502 2.9466943740844727 16.308345794677734
Loss :  1.5361689329147339 3.090115785598755 16.98674774169922
Loss :  1.5837392807006836 2.153233289718628 12.349905967712402
Loss :  1.5156962871551514 2.379730463027954 13.414348602294922
Loss :  1.6230918169021606 1.974335789680481 11.494771003723145
Loss :  1.5292048454284668 2.8482413291931152 15.77041244506836
Loss :  1.5025863647460938 2.5816473960876465 14.410823822021484
Loss :  1.5288866758346558 3.09222149848938 16.989994049072266
Loss :  1.635091781616211 3.9387712478637695 21.328947067260742
  batch 60 loss: 1.635091781616211, 3.9387712478637695, 21.328947067260742
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 2, 3, 4, 5], device='cuda:0')
Loss :  1.5015792846679688 2.4196107387542725 13.59963321685791
Loss :  1.5543197393417358 2.0476484298706055 11.792561531066895
Loss :  1.521101951599121 2.066118001937866 11.851692199707031
Loss :  1.5018675327301025 2.8034331798553467 15.519033432006836
Loss :  1.4696935415267944 1.953093409538269 11.235159873962402
Loss :  1.4850503206253052 4.197079181671143 22.47044563293457
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.5051428079605103 4.230528354644775 22.65778350830078
Loss :  1.5060731172561646 4.046522617340088 21.738685607910156
Loss :  1.485868215560913 3.968248128890991 21.32710838317871
Total LOSS train 14.050691648629995 valid 22.048505783081055
CE LOSS train 1.5404536412312435 valid 0.37146705389022827
Contrastive LOSS train 2.502047610282898 valid 0.9920620322227478
EPOCH 147:
Loss :  1.5793629884719849 4.029078483581543 21.724756240844727
Loss :  1.6033620834350586 3.3272268772125244 18.239498138427734
Loss :  1.5544582605361938 2.737165927886963 15.240287780761719
Loss :  1.5631531476974487 2.8369882106781006 15.74809455871582
Loss :  1.5926012992858887 2.8285791873931885 15.735496520996094
Loss :  1.5284860134124756 2.4603734016418457 13.830352783203125
Loss :  1.5876988172531128 2.5075440406799316 14.125418663024902
Loss :  1.5463758707046509 2.210761547088623 12.600184440612793
Loss :  1.532173991203308 2.001723289489746 11.540790557861328
Loss :  1.5916881561279297 2.3639981746673584 13.4116792678833
Loss :  1.5154659748077393 2.4099318981170654 13.565125465393066
Loss :  1.5149863958358765 2.430537700653076 13.667675018310547
Loss :  1.5115200281143188 3.483330726623535 18.928173065185547
Loss :  1.521329402923584 2.8990352153778076 16.01650619506836
Loss :  1.615606427192688 2.7821245193481445 15.526228904724121
Loss :  1.6056677103042603 2.8641061782836914 15.926198959350586
Loss :  1.5039913654327393 3.0202674865722656 16.605329513549805
Loss :  1.5498861074447632 2.8263983726501465 15.681878089904785
Loss :  1.4986469745635986 2.29392671585083 12.968279838562012
Loss :  1.6052181720733643 2.7615602016448975 15.413019180297852
  batch 20 loss: 1.6052181720733643, 2.7615602016448975, 15.413019180297852
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.5411268472671509 2.211653232574463 12.599392890930176
Loss :  1.4972500801086426 2.4025955200195312 13.51022720336914
Loss :  1.529228687286377 2.6814727783203125 14.936592102050781
Loss :  1.555450201034546 2.088146209716797 11.99618148803711
Loss :  1.6067416667938232 2.765469789505005 15.434090614318848
Loss :  1.5334285497665405 2.357896566390991 13.322911262512207
Loss :  1.5483638048171997 2.786998987197876 15.483358383178711
Loss :  1.5377098321914673 2.024967670440674 11.662548065185547
Loss :  1.4505369663238525 2.4494805335998535 13.697938919067383
Loss :  1.5973793268203735 2.504126787185669 14.118013381958008
Loss :  1.4542862176895142 2.68210768699646 14.864825248718262
Loss :  1.5691988468170166 2.992922067642212 16.533809661865234
Loss :  1.528769612312317 2.3928439617156982 13.492989540100098
Loss :  1.5244152545928955 2.034820079803467 11.698515892028809
Loss :  1.4672598838806152 2.7341818809509277 15.13817024230957
Loss :  1.4916354417800903 2.926596164703369 16.124616622924805
Loss :  1.4898921251296997 2.468456745147705 13.832175254821777
Loss :  1.5932499170303345 2.5188210010528564 14.187355041503906
Loss :  1.6029253005981445 2.4855573177337646 14.030712127685547
Loss :  1.6189967393875122 2.8093087673187256 15.66554069519043
  batch 40 loss: 1.6189967393875122, 2.8093087673187256, 15.66554069519043
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4, 5], device='cuda:0')
Loss :  1.5511106252670288 2.854116201400757 15.821691513061523
Loss :  1.5301421880722046 2.141824245452881 12.239263534545898
Loss :  1.513474702835083 2.283439874649048 12.930673599243164
Loss :  1.5347678661346436 2.4261698722839355 13.665617942810059
Loss :  1.4995170831680298 2.509226083755493 14.045647621154785
Loss :  1.5506155490875244 2.3584322929382324 13.342777252197266
Loss :  1.604790210723877 2.1659343242645264 12.43446159362793
Loss :  1.5258909463882446 2.5670902729034424 14.361342430114746
Loss :  1.6262335777282715 2.5414350032806396 14.33340835571289
Loss :  1.5340558290481567 2.404021739959717 13.55416488647461
Loss :  1.58732271194458 2.3452773094177246 13.313709259033203
Loss :  1.5760515928268433 3.626345634460449 19.707780838012695
Loss :  1.5409765243530273 2.1880385875701904 12.481169700622559
Loss :  1.5914571285247803 2.4718596935272217 13.950756072998047
Loss :  1.5237573385238647 2.9227402210235596 16.13745880126953
Loss :  1.6290816068649292 3.545673131942749 19.357446670532227
Loss :  1.537371277809143 3.06367826461792 16.855762481689453
Loss :  1.5119317770004272 2.344062566757202 13.232244491577148
Loss :  1.537420630455017 2.543529987335205 14.255069732666016
Loss :  1.6408405303955078 2.4034740924835205 13.658210754394531
  batch 60 loss: 1.6408405303955078, 2.4034740924835205, 13.658210754394531
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.5140348672866821 3.433479070663452 18.681428909301758
Loss :  1.5616241693496704 2.3889503479003906 13.506376266479492
Loss :  1.5299999713897705 1.7219890356063843 10.139945030212402
Loss :  1.5093767642974854 2.1928014755249023 12.473383903503418
Loss :  1.4756877422332764 2.0643959045410156 11.797667503356934
Loss :  1.523432731628418 3.790038824081421 20.47362518310547
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.543188452720642 3.758042335510254 20.333398818969727
Loss :  1.537278413772583 3.6089963912963867 19.58226203918457
Loss :  1.5484670400619507 3.4830751419067383 18.963842391967773
Total LOSS train 14.54003687638503 valid 19.838282108306885
CE LOSS train 1.5476470415408794 valid 0.38711676001548767
Contrastive LOSS train 2.5984779559648956 valid 0.8707687854766846
EPOCH 148:
Loss :  1.5859063863754272 2.4365639686584473 13.768725395202637
Loss :  1.6083132028579712 2.941256523132324 16.31459617614746
Loss :  1.5566351413726807 2.4910926818847656 14.01209831237793
Loss :  1.563631534576416 2.586395740509033 14.495609283447266
Loss :  1.5921404361724854 2.312387704849243 13.154078483581543
Loss :  1.5329595804214478 2.4598488807678223 13.83220386505127
Loss :  1.5913746356964111 2.391871929168701 13.550734519958496
Loss :  1.5514997243881226 2.215094804763794 12.626973152160645
Loss :  1.5368287563323975 2.273893117904663 12.906294822692871
Loss :  1.5964429378509521 2.501711368560791 14.104999542236328
Loss :  1.5248647928237915 2.2472543716430664 12.761137008666992
Loss :  1.5228759050369263 2.379324436187744 13.4194974899292
Loss :  1.519248366355896 2.137221336364746 12.205354690551758
Loss :  1.5276200771331787 2.309370994567871 13.074475288391113
Loss :  1.6188268661499023 2.74573016166687 15.347477912902832
Loss :  1.6112569570541382 2.060307502746582 11.91279411315918
Loss :  1.5139870643615723 2.6895666122436523 14.961820602416992
Loss :  1.556563138961792 2.0646042823791504 11.879584312438965
Loss :  1.5081665515899658 1.9480060338974 11.248196601867676
Loss :  1.6101579666137695 2.2023680210113525 12.621997833251953
  batch 20 loss: 1.6101579666137695, 2.2023680210113525, 12.621997833251953
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.548450231552124 2.12727689743042 12.184835433959961
Loss :  1.5064620971679688 2.691697120666504 14.964947700500488
Loss :  1.53546142578125 2.3027596473693848 13.049259185791016
Loss :  1.560470461845398 2.648651123046875 14.803726196289062
Loss :  1.6089715957641602 2.849196672439575 15.854954719543457
Loss :  1.5398629903793335 3.6826980113983154 19.953353881835938
Loss :  1.5569220781326294 2.676041841506958 14.93713092803955
Loss :  1.5486533641815186 2.16097092628479 12.353507995605469
Loss :  1.4674381017684937 2.2278800010681152 12.60683822631836
Loss :  1.605151653289795 2.5396015644073486 14.303159713745117
Loss :  1.4710899591445923 3.1158370971679688 17.050275802612305
Loss :  1.580401062965393 2.9377377033233643 16.269088745117188
Loss :  1.5409090518951416 2.5545952320098877 14.313884735107422
Loss :  1.5362391471862793 2.6339354515075684 14.705917358398438
Loss :  1.4789373874664307 2.5680935382843018 14.319404602050781
Loss :  1.5017144680023193 2.2778756618499756 12.891093254089355
Loss :  1.4992008209228516 2.3350203037261963 13.174302101135254
Loss :  1.5987964868545532 2.6549601554870605 14.873598098754883
Loss :  1.6054770946502686 2.5288949012756348 14.249951362609863
Loss :  1.6229846477508545 2.325932264328003 13.252646446228027
  batch 40 loss: 1.6229846477508545, 2.325932264328003, 13.252646446228027
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.5588788986206055 2.4987170696258545 14.052464485168457
Loss :  1.5378116369247437 2.359549045562744 13.335556030273438
Loss :  1.5216867923736572 2.4818081855773926 13.930727005004883
Loss :  1.5406298637390137 2.1813583374023438 12.44742202758789
Loss :  1.5057412385940552 2.108224630355835 12.04686450958252
Loss :  1.5555881261825562 2.8012990951538086 15.56208324432373
Loss :  1.6083682775497437 2.574122428894043 14.47898006439209
Loss :  1.5284159183502197 2.3881912231445312 13.469371795654297
Loss :  1.6337758302688599 2.4454362392425537 13.860957145690918
Loss :  1.53655207157135 2.257082223892212 12.8219633102417
Loss :  1.588815450668335 2.5228350162506104 14.202990531921387
Loss :  1.5790971517562866 2.5414865016937256 14.286529541015625
Loss :  1.5439562797546387 2.5550692081451416 14.31930160522461
Loss :  1.5925774574279785 3.0592451095581055 16.888803482055664
Loss :  1.524864673614502 2.6752231121063232 14.900979995727539
Loss :  1.6303449869155884 3.0155937671661377 16.70831298828125
Loss :  1.5426477193832397 2.918792486190796 16.13661003112793
Loss :  1.5144727230072021 3.0832102298736572 16.930522918701172
Loss :  1.5423392057418823 3.241185426712036 17.748266220092773
Loss :  1.6459581851959229 2.982365608215332 16.55778694152832
  batch 60 loss: 1.6459581851959229, 2.982365608215332, 16.55778694152832
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.5156242847442627 2.845395803451538 15.742603302001953
Loss :  1.5588209629058838 2.6279873847961426 14.69875717163086
Loss :  1.528891682624817 2.446925401687622 13.763518333435059
Loss :  1.507859230041504 2.8033647537231445 15.524682998657227
Loss :  1.4737887382507324 2.8708903789520264 15.828241348266602
Loss :  1.52505624294281 4.080486297607422 21.927488327026367
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 3, 4, 5], device='cuda:0')
Loss :  1.5348296165466309 3.9175195693969727 21.122426986694336
Loss :  1.5284754037857056 3.9949915409088135 21.503433227539062
Loss :  1.5773950815200806 4.041865825653076 21.786724090576172
Total LOSS train 14.285458814180814 valid 21.585018157958984
CE LOSS train 1.5532369466928335 valid 0.39434877038002014
Contrastive LOSS train 2.5464443885363064 valid 1.010466456413269
EPOCH 149:
Loss :  1.5816813707351685 2.8675787448883057 15.919574737548828
Loss :  1.6028364896774292 3.1209897994995117 17.20778465270996
Loss :  1.551330804824829 2.4444541931152344 13.773601531982422
Loss :  1.5615507364273071 2.719568967819214 15.159396171569824
Loss :  1.5908055305480957 2.5966999530792236 14.574304580688477
Loss :  1.5331538915634155 2.5374417304992676 14.220361709594727
Loss :  1.5898140668869019 2.688272714614868 15.031177520751953
Loss :  1.5508506298065186 3.1807973384857178 17.454837799072266
Loss :  1.534812092781067 3.071181535720825 16.89072036743164
Loss :  1.5905033349990845 3.368567943572998 18.4333438873291
Loss :  1.5213534832000732 3.2164714336395264 17.603710174560547
Loss :  1.521101474761963 2.8175668716430664 15.608936309814453
Loss :  1.5167185068130493 2.398555278778076 13.509495735168457
Loss :  1.5280251502990723 2.774538993835449 15.400720596313477
Loss :  1.6158028841018677 2.6703474521636963 14.96753978729248
Loss :  1.6125295162200928 2.8101627826690674 15.66334342956543
Loss :  1.5139694213867188 3.4185266494750977 18.60660171508789
Loss :  1.5573902130126953 2.711615562438965 15.11546802520752
Loss :  1.5107100009918213 2.449512004852295 13.758270263671875
Loss :  1.607735276222229 2.506495237350464 14.140212059020996
  batch 20 loss: 1.607735276222229, 2.506495237350464, 14.140212059020996
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4, 5], device='cuda:0')
Loss :  1.5477811098098755 2.9845542907714844 16.470552444458008
Loss :  1.503690481185913 2.3832437992095947 13.419909477233887
Loss :  1.533381700515747 2.283389091491699 12.950326919555664
Loss :  1.5591216087341309 2.4986143112182617 14.052192687988281
Loss :  1.6067912578582764 2.673699140548706 14.975287437438965
Loss :  1.536787748336792 3.4882657527923584 18.97811508178711
Loss :  1.553782343864441 2.2404584884643555 12.756074905395508
Loss :  1.5441808700561523 1.9589771032333374 11.339066505432129
Loss :  1.4613183736801147 2.5651018619537354 14.286827087402344
Loss :  1.601250171661377 2.6125197410583496 14.663848876953125
Loss :  1.4633429050445557 2.6621713638305664 14.774199485778809
Loss :  1.5745168924331665 2.6642239093780518 14.895636558532715
Loss :  1.535178303718567 2.3601396083831787 13.33587646484375
Loss :  1.5317659378051758 2.680417060852051 14.93385124206543
Loss :  1.4757797718048096 2.5066962242126465 14.009261131286621
Loss :  1.499630331993103 2.7395617961883545 15.197439193725586
Loss :  1.4977104663848877 2.819633960723877 15.595879554748535
Loss :  1.5953364372253418 3.419860363006592 18.694639205932617
Loss :  1.604090929031372 2.5623281002044678 14.415731430053711
Loss :  1.6199619770050049 2.750998020172119 15.374951362609863
  batch 40 loss: 1.6199619770050049, 2.750998020172119, 15.374951362609863
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.5523748397827148 2.4258482456207275 13.681615829467773
Loss :  1.5293419361114502 2.6936869621276855 14.997776985168457
Loss :  1.5134023427963257 2.276015520095825 12.893479347229004
Loss :  1.5333369970321655 2.2056570053100586 12.56162166595459
Loss :  1.4981396198272705 1.9549394845962524 11.272836685180664
Loss :  1.54891037940979 2.5691275596618652 14.394548416137695
Loss :  1.6050575971603394 2.390084981918335 13.555482864379883
Loss :  1.525687575340271 3.2430357933044434 17.740867614746094
Loss :  1.6281367540359497 2.198077440261841 12.618523597717285
Loss :  1.5366394519805908 2.0490620136260986 11.781949043273926
Loss :  1.5920805931091309 2.287039279937744 13.027276992797852
Loss :  1.58204984664917 2.4200332164764404 13.68221664428711
Loss :  1.5471107959747314 1.9614036083221436 11.35412883758545
Loss :  1.5932313203811646 2.8912734985351562 16.049598693847656
Loss :  1.5268231630325317 2.101820707321167 12.035926818847656
Loss :  1.6309336423873901 2.262206792831421 12.941967964172363
Loss :  1.5381979942321777 2.6084938049316406 14.580667495727539
Loss :  1.5105390548706055 2.641167640686035 14.716377258300781
Loss :  1.5350760221481323 2.6986424922943115 15.028288841247559
Loss :  1.6405200958251953 2.5095765590667725 14.188403129577637
  batch 60 loss: 1.6405200958251953, 2.5095765590667725, 14.188403129577637
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.511130452156067 2.1839303970336914 12.430782318115234
Loss :  1.5598503351211548 2.5374298095703125 14.246999740600586
Loss :  1.5276027917861938 2.3903374671936035 13.479290008544922
Loss :  1.5087577104568481 2.7458317279815674 15.237915992736816
Loss :  1.4760291576385498 1.884244680404663 10.897253036499023
Loss :  1.479313611984253 4.381409645080566 23.38636016845703
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.5016344785690308 4.294850826263428 22.975889205932617
Loss :  1.5014921426773071 4.184454441070557 22.423763275146484
Loss :  1.4813735485076904 4.259305953979492 22.777902603149414
Total LOSS train 14.577767137380746 valid 22.890978813171387
CE LOSS train 1.5506005378869863 valid 0.3703433871269226
Contrastive LOSS train 2.6054333209991456 valid 1.064826488494873
EPOCH 150:
Loss :  1.5824757814407349 2.341024160385132 13.287596702575684
Loss :  1.6041213274002075 3.4038150310516357 18.623197555541992
Loss :  1.5531870126724243 2.0626280307769775 11.866327285766602
Loss :  1.562136173248291 2.7684714794158936 15.40449333190918
Loss :  1.5929372310638428 3.50142765045166 19.100074768066406
Loss :  1.533856987953186 2.4470067024230957 13.768890380859375
Loss :  1.591665267944336 2.8885385990142822 16.034358978271484
Loss :  1.5504028797149658 2.7220957279205322 15.160881042480469
Loss :  1.5347075462341309 2.0866057872772217 11.967737197875977
Loss :  1.5920295715332031 2.596795082092285 14.576004981994629
Loss :  1.5221790075302124 2.769602060317993 15.37018871307373
Loss :  1.5209344625473022 3.019526243209839 16.618566513061523
Loss :  1.5172606706619263 2.5080764293670654 14.057642936706543
Loss :  1.5264853239059448 3.581925630569458 19.436113357543945
Loss :  1.6198266744613647 2.392664909362793 13.583150863647461
Loss :  1.612849235534668 2.371246814727783 13.469082832336426
Loss :  1.5134143829345703 2.8779804706573486 15.903316497802734
Loss :  1.5585097074508667 2.3354806900024414 13.235913276672363
Loss :  1.5094654560089111 2.116603136062622 12.092480659484863
Loss :  1.6077202558517456 4.1777753829956055 22.496597290039062
  batch 20 loss: 1.6077202558517456, 4.1777753829956055, 22.496597290039062
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.5489832162857056 3.7764594554901123 20.4312801361084
Loss :  1.506809949874878 2.6667745113372803 14.840682983398438
Loss :  1.535480260848999 2.327883720397949 13.174899101257324
Loss :  1.5609296560287476 2.3579013347625732 13.350436210632324
Loss :  1.6079967021942139 2.989712953567505 16.556560516357422
Loss :  1.5377519130706787 2.507909059524536 14.07729721069336
Loss :  1.5521177053451538 2.5442423820495605 14.273329734802246
Loss :  1.542554259300232 3.133596897125244 17.210538864135742
Loss :  1.456355333328247 2.823424816131592 15.573479652404785
Loss :  1.5990452766418457 2.899123191833496 16.094661712646484
Loss :  1.4577562808990479 2.8954741954803467 15.935127258300781
Loss :  1.5710170269012451 2.4040236473083496 13.59113597869873
Loss :  1.5293914079666138 2.587357759475708 14.466179847717285
Loss :  1.5250643491744995 3.0634143352508545 16.84213638305664
Loss :  1.4679421186447144 2.603865623474121 14.48727035522461
Loss :  1.4920624494552612 2.2186098098754883 12.585111618041992
Loss :  1.4903477430343628 2.40055251121521 13.493110656738281
Loss :  1.594099998474121 2.918470621109009 16.186452865600586
Loss :  1.6035693883895874 2.9178497791290283 16.19281768798828
Loss :  1.621160626411438 2.5118753910064697 14.180537223815918
  batch 40 loss: 1.621160626411438, 2.5118753910064697, 14.180537223815918
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.5547646284103394 2.8786542415618896 15.948036193847656
Loss :  1.533400535583496 2.6106982231140137 14.586892127990723
Loss :  1.5165895223617554 2.861671209335327 15.824945449829102
Loss :  1.5370508432388306 3.77046537399292 20.38937759399414
Loss :  1.501757025718689 2.8303635120391846 15.653573989868164
Loss :  1.5525895357131958 3.2292609214782715 17.698894500732422
Loss :  1.6080578565597534 2.2490997314453125 12.853556632995605
Loss :  1.5287445783615112 2.05511212348938 11.804305076599121
Loss :  1.6296976804733276 2.661072015762329 14.935057640075684
Loss :  1.5375094413757324 2.3436319828033447 13.255668640136719
Loss :  1.592119574546814 2.210944652557373 12.646842956542969
Loss :  1.5803186893463135 2.814317226409912 15.651905059814453
Loss :  1.5455992221832275 2.482652425765991 13.958861351013184
Loss :  1.592134714126587 2.162713050842285 12.405699729919434
Loss :  1.52450692653656 2.534710168838501 14.198057174682617
Loss :  1.6303273439407349 2.1269359588623047 12.265007019042969
Loss :  1.5389679670333862 2.812239170074463 15.600163459777832
Loss :  1.511811375617981 2.843977451324463 15.731698036193848
Loss :  1.5381070375442505 2.90923810005188 16.08429718017578
Loss :  1.6422901153564453 1.955903172492981 11.421806335449219
  batch 60 loss: 1.6422901153564453, 1.955903172492981, 11.421806335449219
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.5175329446792603 2.0915091037750244 11.975078582763672
Loss :  1.5644043684005737 2.2506392002105713 12.81760025024414
Loss :  1.5351024866104126 2.2040855884552 12.555530548095703
Loss :  1.5172066688537598 3.976532459259033 21.39986801147461
Loss :  1.4856619834899902 1.7927216291427612 10.449270248413086
Loss :  1.4843814373016357 4.392535209655762 23.447057723999023
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.5058283805847168 4.380277156829834 23.407215118408203
Loss :  1.5044053792953491 4.330310344696045 23.15595817565918
Loss :  1.5082262754440308 4.078914642333984 21.902799606323242
Total LOSS train 14.949348537738507 valid 22.978257656097412
CE LOSS train 1.5511208259142362 valid 0.3770565688610077
Contrastive LOSS train 2.6796455493340123 valid 1.019728660583496
EPOCH 151:
Loss :  1.5894891023635864 2.341265916824341 13.295818328857422
Loss :  1.6104586124420166 2.508382558822632 14.152371406555176
Loss :  1.5599271059036255 1.8997600078582764 11.058727264404297
Loss :  1.5671718120574951 2.6213467121124268 14.673905372619629
Loss :  1.5954011678695679 2.189913034439087 12.544965744018555
Loss :  1.5377869606018066 2.445079803466797 13.763185501098633
Loss :  1.5947459936141968 2.5950710773468018 14.570100784301758
Loss :  1.5546759366989136 2.2473435401916504 12.791393280029297
Loss :  1.5385174751281738 2.0181784629821777 11.629409790039062
Loss :  1.5935832262039185 2.5023303031921387 14.10523509979248
Loss :  1.5239813327789307 2.6474640369415283 14.761301040649414
Loss :  1.5220134258270264 2.3465545177459717 13.254786491394043
Loss :  1.5182887315750122 2.704254627227783 15.03956127166748
Loss :  1.527130365371704 3.015815019607544 16.606204986572266
Loss :  1.6168179512023926 3.543088436126709 19.332260131835938
Loss :  1.611717700958252 3.872392177581787 20.973678588867188
Loss :  1.5148873329162598 2.5360233783721924 14.195003509521484
Loss :  1.5587095022201538 2.142878293991089 12.273100852966309
Loss :  1.509792447090149 1.9426226615905762 11.222906112670898
Loss :  1.6112667322158813 2.6569664478302 14.896099090576172
  batch 20 loss: 1.6112667322158813, 2.6569664478302, 14.896099090576172
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.551292061805725 2.5825729370117188 14.464157104492188
Loss :  1.509188175201416 2.9395017623901367 16.206697463989258
Loss :  1.5404003858566284 2.4531397819519043 13.806099891662598
Loss :  1.564475417137146 2.0888521671295166 12.008735656738281
Loss :  1.6108545064926147 3.0361666679382324 16.791688919067383
Loss :  1.545332670211792 2.339920997619629 13.244937896728516
Loss :  1.5617871284484863 2.2021381855010986 12.572477340698242
Loss :  1.5536586046218872 2.2811026573181152 12.959172248840332
Loss :  1.4715781211853027 2.4796829223632812 13.869993209838867
Loss :  1.6071380376815796 2.738457202911377 15.299423217773438
Loss :  1.4728988409042358 2.637953996658325 14.662668228149414
Loss :  1.5813941955566406 2.5993399620056152 14.578094482421875
Loss :  1.5430446863174438 2.506086587905884 14.073477745056152
Loss :  1.5394723415374756 2.4860074520111084 13.969510078430176
Loss :  1.4828994274139404 2.4664571285247803 13.815185546875
Loss :  1.506386160850525 2.5023179054260254 14.017974853515625
Loss :  1.5038502216339111 2.3092052936553955 13.04987621307373
Loss :  1.601528525352478 2.086412191390991 12.033589363098145
Loss :  1.6099145412445068 2.1631226539611816 12.425527572631836
Loss :  1.6256881952285767 2.530475378036499 14.27806568145752
  batch 40 loss: 1.6256881952285767, 2.530475378036499, 14.27806568145752
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.5595693588256836 2.1143884658813477 12.131511688232422
Loss :  1.5383228063583374 2.3734092712402344 13.40536880493164
Loss :  1.522329568862915 3.0866310596466064 16.95548439025879
Loss :  1.5420902967453003 2.834667205810547 15.715426445007324
Loss :  1.5072460174560547 1.755728840827942 10.285890579223633
Loss :  1.55597722530365 1.9984906911849976 11.548430442810059
Loss :  1.609431266784668 2.0892457962036133 12.055660247802734
Loss :  1.5315009355545044 2.0362370014190674 11.712685585021973
Loss :  1.6320499181747437 2.205883026123047 12.66146469116211
Loss :  1.5413745641708374 2.272815465927124 12.905451774597168
Loss :  1.5947399139404297 2.437553644180298 13.78250789642334
Loss :  1.585342526435852 2.391209602355957 13.541390419006348
Loss :  1.5518771409988403 3.5585293769836426 19.344524383544922
Loss :  1.597283959388733 2.0765669345855713 11.980118751525879
Loss :  1.5328257083892822 1.8909252882003784 10.987451553344727
Loss :  1.6341595649719238 2.184047222137451 12.55439567565918
Loss :  1.545686960220337 3.1432061195373535 17.261716842651367
Loss :  1.5201752185821533 3.0142011642456055 16.5911808013916
Loss :  1.544511318206787 3.3773419857025146 18.43122100830078
Loss :  1.6453428268432617 2.1743533611297607 12.517109870910645
  batch 60 loss: 1.6453428268432617, 2.1743533611297607, 12.517109870910645
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.5216127634048462 1.9240586757659912 11.14190673828125
Loss :  1.5668175220489502 2.9884393215179443 16.509014129638672
Loss :  1.53648042678833 1.7205538749694824 10.139249801635742
Loss :  1.5181134939193726 2.2017247676849365 12.526737213134766
Loss :  1.4867455959320068 1.6591936349868774 9.782713890075684
Loss :  1.5162709951400757 3.8626294136047363 20.829418182373047
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 5], device='cuda:0')
Loss :  1.5338106155395508 3.7666165828704834 20.366893768310547
Loss :  1.5317742824554443 3.682607889175415 19.944814682006836
Loss :  1.5341442823410034 3.539451837539673 19.231403350830078
Total LOSS train 13.842092015193058 valid 20.093132495880127
CE LOSS train 1.556319262431218 valid 0.38353607058525085
Contrastive LOSS train 2.4571545637570895 valid 0.8848629593849182
EPOCH 152:
Loss :  1.5897517204284668 2.227588415145874 12.727693557739258
Loss :  1.611053705215454 2.7280805110931396 15.251456260681152
Loss :  1.5623995065689087 3.991358757019043 21.519193649291992
Loss :  1.5708459615707397 1.950412392616272 11.322907447814941
Loss :  1.599787950515747 1.9416747093200684 11.308161735534668
Loss :  1.5434272289276123 3.9087185859680176 21.087018966674805
Loss :  1.599146842956543 2.3801863193511963 13.500078201293945
Loss :  1.5595309734344482 2.1646604537963867 12.382833480834961
Loss :  1.5448611974716187 2.111663579940796 12.103178977966309
Loss :  1.5996863842010498 2.204719305038452 12.623283386230469
Loss :  1.5314610004425049 2.6205339431762695 14.634130477905273
Loss :  1.5303717851638794 3.5824992656707764 19.442869186401367
Loss :  1.5276598930358887 2.5343027114868164 14.199172973632812
Loss :  1.5361019372940063 2.7970547676086426 15.52137565612793
Loss :  1.6218231916427612 2.298219919204712 13.112922668457031
Loss :  1.6166874170303345 2.808147430419922 15.657424926757812
Loss :  1.5230575799942017 2.4408464431762695 13.727290153503418
Loss :  1.564778208732605 2.498072862625122 14.055142402648926
Loss :  1.5168591737747192 2.3964691162109375 13.499204635620117
Loss :  1.6142154932022095 2.361544609069824 13.4219388961792
  batch 20 loss: 1.6142154932022095, 2.361544609069824, 13.4219388961792
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.554508090019226 2.383450746536255 13.471761703491211
Loss :  1.5133765935897827 2.832545757293701 15.676105499267578
Loss :  1.5422450304031372 2.379896879196167 13.441729545593262
Loss :  1.566664695739746 2.236142158508301 12.74737548828125
Loss :  1.6119030714035034 2.98828387260437 16.553321838378906
Loss :  1.5461931228637695 2.639070749282837 14.741546630859375
Loss :  1.562231421470642 2.178269624710083 12.45357894897461
Loss :  1.5528150796890259 2.07851505279541 11.945390701293945
Loss :  1.4718705415725708 2.1737241744995117 12.34049129486084
Loss :  1.6065975427627563 2.363438606262207 13.42379093170166
Loss :  1.4742188453674316 2.876232385635376 15.85538101196289
Loss :  1.5817053318023682 3.1266279220581055 17.214845657348633
Loss :  1.544089436531067 2.928978681564331 16.188983917236328
Loss :  1.5414067506790161 2.5025136470794678 14.053975105285645
Loss :  1.4890183210372925 2.4043939113616943 13.510988235473633
Loss :  1.511281967163086 2.278571367263794 12.904138565063477
Loss :  1.5098978281021118 2.1473019123077393 12.246407508850098
Loss :  1.6042791604995728 2.318272113800049 13.195639610290527
Loss :  1.6118084192276 2.5159757137298584 14.19168758392334
Loss :  1.627396583557129 1.893910527229309 11.096949577331543
  batch 40 loss: 1.627396583557129, 1.893910527229309, 11.096949577331543
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.5641177892684937 2.283611536026001 12.98217487335205
Loss :  1.543200969696045 2.144399404525757 12.26519775390625
Loss :  1.5281424522399902 2.038313865661621 11.719711303710938
Loss :  1.5483027696609497 2.30615234375 13.07906436920166
Loss :  1.5146011114120483 1.8542155027389526 10.785677909851074
Loss :  1.5620231628417969 2.5767252445220947 14.445649147033691
Loss :  1.6137713193893433 1.8351562023162842 10.789552688598633
Loss :  1.5375463962554932 2.026116371154785 11.66812801361084
Loss :  1.6346206665039062 2.0078346729278564 11.67379379272461
Loss :  1.5462960004806519 1.8277180194854736 10.68488597869873
Loss :  1.598095417022705 1.9147688150405884 11.171939849853516
Loss :  1.588814616203308 2.6835944652557373 15.006787300109863
Loss :  1.5574523210525513 1.9986754655838013 11.550829887390137
Loss :  1.601044774055481 1.890838384628296 11.05523681640625
Loss :  1.53924560546875 1.9882372617721558 11.48043155670166
Loss :  1.6364432573318481 3.2407519817352295 17.8402042388916
Loss :  1.5505231618881226 2.7666525840759277 15.38378620147705
Loss :  1.5251542329788208 3.2406046390533447 17.728178024291992
Loss :  1.5483801364898682 3.7408337593078613 20.252548217773438
Loss :  1.6452984809875488 2.935419797897339 16.322397232055664
  batch 60 loss: 1.6452984809875488, 2.935419797897339, 16.322397232055664
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.5254305601119995 2.3542327880859375 13.296594619750977
Loss :  1.5696163177490234 2.2212436199188232 12.675834655761719
Loss :  1.5397160053253174 2.069655418395996 11.887992858886719
Loss :  1.5213879346847534 2.816772222518921 15.605249404907227
Loss :  1.4903391599655151 1.9311256408691406 11.145967483520508
Loss :  1.5353304147720337 4.1387410163879395 22.229034423828125
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.5515644550323486 4.255345821380615 22.82829475402832
Loss :  1.5471136569976807 4.028738021850586 21.69080352783203
Loss :  1.5636521577835083 4.013937473297119 21.633338928222656
Total LOSS train 13.859218171926646 valid 22.095367908477783
CE LOSS train 1.5602550708330594 valid 0.3909130394458771
Contrastive LOSS train 2.4597926139831543 valid 1.0034843683242798
EPOCH 153:
Loss :  1.5914905071258545 1.977504014968872 11.479010581970215
Loss :  1.6119766235351562 2.3962059020996094 13.593006134033203
Loss :  1.5645450353622437 1.849676251411438 10.812926292419434
Loss :  1.5731749534606934 2.701694965362549 15.081649780273438
Loss :  1.6007494926452637 2.1291422843933105 12.246461868286133
Loss :  1.5458234548568726 2.5499777793884277 14.2957124710083
Loss :  1.6016125679016113 3.2086033821105957 17.644628524780273
Loss :  1.563381552696228 2.263549327850342 12.881128311157227
Loss :  1.5491961240768433 2.347228765487671 13.285340309143066
Loss :  1.603044033050537 1.8505988121032715 10.856039047241211
Loss :  1.5360596179962158 2.1375815868377686 12.223967552185059
Loss :  1.5347338914871216 2.4472596645355225 13.771032333374023
Loss :  1.5316895246505737 2.4139199256896973 13.601288795471191
Loss :  1.5406473875045776 2.464054822921753 13.860921859741211
Loss :  1.6254745721817017 2.8675131797790527 15.963041305541992
Loss :  1.6180732250213623 2.7731711864471436 15.483929634094238
Loss :  1.5280967950820923 2.5874264240264893 14.465229034423828
Loss :  1.5679436922073364 2.709428310394287 15.11508560180664
Loss :  1.5207695960998535 1.9422441720962524 11.231990814208984
Loss :  1.616271734237671 2.2295074462890625 12.763809204101562
  batch 20 loss: 1.616271734237671, 2.2295074462890625, 12.763809204101562
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.558051586151123 2.204479932785034 12.580451965332031
Loss :  1.5180168151855469 2.592139720916748 14.478715896606445
Loss :  1.5471552610397339 1.8649933338165283 10.872121810913086
Loss :  1.5711305141448975 2.2474708557128906 12.80848503112793
Loss :  1.6161094903945923 2.47995662689209 14.01589298248291
Loss :  1.5518503189086914 2.273750066757202 12.920600891113281
Loss :  1.567355990409851 2.9255080223083496 16.194896697998047
Loss :  1.5584957599639893 2.616225004196167 14.639620780944824
Loss :  1.4803433418273926 2.1596827507019043 12.278757095336914
Loss :  1.610693335533142 2.1177098751068115 12.19924259185791
Loss :  1.4832804203033447 3.0924599170684814 16.945579528808594
Loss :  1.58750581741333 2.635446786880493 14.764739990234375
Loss :  1.5509321689605713 2.731564521789551 15.208754539489746
Loss :  1.5474036931991577 2.7711782455444336 15.403294563293457
Loss :  1.4950451850891113 2.265864372253418 12.82436752319336
Loss :  1.5167654752731323 2.386826515197754 13.450898170471191
Loss :  1.514505386352539 2.092198610305786 11.97549819946289
Loss :  1.6071827411651611 2.206069231033325 12.637528419494629
Loss :  1.6146963834762573 1.9132715463638306 11.18105411529541
Loss :  1.6301100254058838 2.187335729598999 12.566788673400879
  batch 40 loss: 1.6301100254058838, 2.187335729598999, 12.566788673400879
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.5675771236419678 2.4595372676849365 13.865263938903809
Loss :  1.5462610721588135 2.470654010772705 13.899530410766602
Loss :  1.53144109249115 2.6488289833068848 14.775585174560547
Loss :  1.551073431968689 2.3784565925598145 13.443355560302734
Loss :  1.5182991027832031 1.7941372394561768 10.488985061645508
Loss :  1.5646142959594727 2.151071310043335 12.319971084594727
Loss :  1.6157464981079102 2.2792131900787354 13.011812210083008
Loss :  1.541553020477295 1.9980846643447876 11.531976699829102
Loss :  1.6357165575027466 2.3729748725891113 13.500590324401855
Loss :  1.5513439178466797 2.2594449520111084 12.8485689163208
Loss :  1.6025692224502563 2.4942822456359863 14.073980331420898
Loss :  1.5931326150894165 2.0876049995422363 12.031157493591309
Loss :  1.5622409582138062 3.284691333770752 17.98569679260254
Loss :  1.6045629978179932 2.120736598968506 12.208246231079102
Loss :  1.5433481931686401 3.105067491531372 17.06868553161621
Loss :  1.6377848386764526 2.3637046813964844 13.456308364868164
Loss :  1.5534772872924805 2.4490671157836914 13.798812866210938
Loss :  1.5291810035705566 2.1461892127990723 12.260126113891602
Loss :  1.5523135662078857 2.637876033782959 14.741694450378418
Loss :  1.6477421522140503 3.0046920776367188 16.671201705932617
  batch 60 loss: 1.6477421522140503, 3.0046920776367188, 16.671201705932617
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.5301141738891602 2.180114507675171 12.430686950683594
Loss :  1.5739725828170776 2.2849531173706055 12.998738288879395
Loss :  1.54475736618042 2.7246408462524414 15.167961120605469
Loss :  1.5263909101486206 2.3445165157318115 13.248973846435547
Loss :  1.496939778327942 1.8877696990966797 10.93578815460205
Loss :  1.5267454385757446 3.8055222034454346 20.55435562133789
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 5], device='cuda:0')
Loss :  1.5441534519195557 3.913280963897705 21.110557556152344
Loss :  1.5410035848617554 3.744269609451294 20.262351989746094
Loss :  1.5464073419570923 3.6410012245178223 19.751413345336914
Total LOSS train 13.52872594686655 valid 20.41966962814331
CE LOSS train 1.5642082746212298 valid 0.38660183548927307
Contrastive LOSS train 2.392903529680692 valid 0.9102503061294556
EPOCH 154:
Loss :  1.5965040922164917 1.9123785495758057 11.15839672088623
Loss :  1.6166117191314697 2.0505564212799072 11.869393348693848
Loss :  1.5700322389602661 2.1872270107269287 12.5061674118042
Loss :  1.5776536464691162 2.645468235015869 14.804994583129883
Loss :  1.6039555072784424 3.2288498878479004 17.748205184936523
Loss :  1.5500997304916382 2.208786964416504 12.594034194946289
Loss :  1.6036726236343384 2.470982313156128 13.958584785461426
Loss :  1.566475510597229 2.156848907470703 12.350720405578613
Loss :  1.5527540445327759 2.700148820877075 15.053498268127441
Loss :  1.6051836013793945 2.617748260498047 14.693924903869629
Loss :  1.539331316947937 2.346132516860962 13.269993782043457
Loss :  1.5383191108703613 3.0385780334472656 16.73120880126953
Loss :  1.5340596437454224 2.9322447776794434 16.195283889770508
Loss :  1.5424667596817017 2.781315565109253 15.449045181274414
Loss :  1.6271995306015015 2.053067684173584 11.892538070678711
Loss :  1.6201645135879517 2.5789730548858643 14.515029907226562
Loss :  1.532146692276001 2.8146567344665527 15.605430603027344
Loss :  1.5712655782699585 2.7353427410125732 15.247979164123535
Loss :  1.5263680219650269 1.862567663192749 10.83920669555664
Loss :  1.6194543838500977 2.1124327182769775 12.181617736816406
  batch 20 loss: 1.6194543838500977, 2.1124327182769775, 12.181617736816406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.5627022981643677 1.9067915678024292 11.096659660339355
Loss :  1.5228170156478882 2.1440844535827637 12.243239402770996
Loss :  1.550978422164917 2.1936514377593994 12.519235610961914
Loss :  1.574310302734375 2.909287214279175 16.120746612548828
Loss :  1.6190277338027954 3.0694525241851807 16.966289520263672
Loss :  1.5553016662597656 3.0890915393829346 17.00075912475586
Loss :  1.5699416399002075 3.2543578147888184 17.841732025146484
Loss :  1.56211256980896 2.0500118732452393 11.812171936035156
Loss :  1.4850424528121948 2.125128746032715 12.110686302185059
Loss :  1.6122841835021973 2.3190038204193115 13.207304000854492
Loss :  1.4867660999298096 3.0156989097595215 16.565261840820312
Loss :  1.5882983207702637 2.414194345474243 13.659269332885742
Loss :  1.5520974397659302 2.124993324279785 12.177063941955566
Loss :  1.5487911701202393 2.2611160278320312 12.854371070861816
Loss :  1.4974522590637207 2.8182384967803955 15.588644027709961
Loss :  1.5196373462677002 2.39046573638916 13.471965789794922
Loss :  1.517911434173584 2.1349828243255615 12.192825317382812
Loss :  1.6097832918167114 2.3765251636505127 13.492408752441406
Loss :  1.6175137758255005 2.495701551437378 14.09602165222168
Loss :  1.6322057247161865 2.412687063217163 13.69564151763916
  batch 40 loss: 1.6322057247161865, 2.412687063217163, 13.69564151763916
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.5705074071884155 2.5181853771209717 14.161434173583984
Loss :  1.5494434833526611 2.4939465522766113 14.01917552947998
Loss :  1.5338969230651855 1.8746370077133179 10.907081604003906
Loss :  1.553194522857666 3.044705390930176 16.776721954345703
Loss :  1.5208632946014404 1.92466402053833 11.144183158874512
Loss :  1.5664304494857788 2.024707078933716 11.68996524810791
Loss :  1.617397427558899 1.943350911140442 11.334152221679688
Loss :  1.544647455215454 1.8782389163970947 10.93584156036377
Loss :  1.6386542320251465 1.9932899475097656 11.605104446411133
Loss :  1.5538557767868042 2.6972172260284424 15.039941787719727
Loss :  1.603948950767517 2.606642007827759 14.637158393859863
Loss :  1.594590425491333 2.5118353366851807 14.153766632080078
Loss :  1.5636181831359863 2.3587567806243896 13.357402801513672
Loss :  1.6058588027954102 2.5827674865722656 14.519696235656738
Loss :  1.5452834367752075 2.87402606010437 15.915413856506348
Loss :  1.640521764755249 2.4315998554229736 13.798521041870117
Loss :  1.5583699941635132 2.0360429286956787 11.738584518432617
Loss :  1.5343635082244873 1.9786418676376343 11.427573204040527
Loss :  1.5583603382110596 2.6917812824249268 15.017266273498535
Loss :  1.6523808240890503 1.93899405002594 11.34735107421875
  batch 60 loss: 1.6523808240890503, 1.93899405002594, 11.34735107421875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.53788161277771 1.8090026378631592 10.582895278930664
Loss :  1.5799179077148438 1.9821330308914185 11.490583419799805
Loss :  1.5512384176254272 1.8924405574798584 11.01344108581543
Loss :  1.5338289737701416 2.5018341541290283 14.042999267578125
Loss :  1.503875970840454 2.0261082649230957 11.634416580200195
Loss :  1.5383704900741577 3.7592148780822754 20.334444046020508
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 5], device='cuda:0')
Loss :  1.5550305843353271 3.5311455726623535 19.210758209228516
Loss :  1.5514767169952393 3.5624303817749023 19.363630294799805
Loss :  1.5655485391616821 3.2835018634796143 17.983057022094727
Total LOSS train 13.533357268113356 valid 19.22297239303589
CE LOSS train 1.5675634384155273 valid 0.39138713479042053
Contrastive LOSS train 2.393158769607544 valid 0.8208754658699036
EPOCH 155:
Loss :  1.59976065158844 1.9353426694869995 11.276473999023438
Loss :  1.6189669370651245 2.2370681762695312 12.80430793762207
Loss :  1.5728085041046143 1.7989368438720703 10.567492485046387
Loss :  1.5804413557052612 2.2276611328125 12.71874713897705
Loss :  1.606745719909668 2.9001097679138184 16.107295989990234
Loss :  1.5545719861984253 2.2927939891815186 13.018542289733887
Loss :  1.6075345277786255 2.025895595550537 11.73701286315918
Loss :  1.5705758333206177 1.7376240491867065 10.258695602416992
Loss :  1.5563467741012573 2.312798500061035 13.120339393615723
Loss :  1.6080470085144043 2.3781895637512207 13.498994827270508
Loss :  1.5440717935562134 2.984508514404297 16.46661376953125
Loss :  1.5429952144622803 2.4040071964263916 13.563031196594238
Loss :  1.5397000312805176 2.379098653793335 13.43519401550293
Loss :  1.5483359098434448 2.3802168369293213 13.449419975280762
Loss :  1.6289299726486206 2.34293794631958 13.343619346618652
Loss :  1.6238758563995361 2.0231077671051025 11.73941421508789
Loss :  1.5362581014633179 2.0606279373168945 11.839397430419922
Loss :  1.5760226249694824 2.1369786262512207 12.260915756225586
Loss :  1.5314103364944458 1.7976093292236328 10.51945686340332
Loss :  1.6220449209213257 1.919202446937561 11.218056678771973
  batch 20 loss: 1.6220449209213257, 1.919202446937561, 11.218056678771973
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.5681453943252563 1.9721388816833496 11.428840637207031
Loss :  1.530033826828003 2.881108522415161 15.935576438903809
Loss :  1.556985855102539 2.3506836891174316 13.310403823852539
Loss :  1.5796092748641968 2.736987352371216 15.264545440673828
Loss :  1.6222426891326904 2.9834578037261963 16.539531707763672
Loss :  1.5609897375106812 2.407418727874756 13.59808349609375
Loss :  1.5755947828292847 2.2848727703094482 12.999958992004395
Loss :  1.5667132139205933 2.4270529747009277 13.70197868347168
Loss :  1.4927443265914917 2.0376083850860596 11.6807861328125
Loss :  1.6169828176498413 2.4220712184906006 13.727338790893555
Loss :  1.4952516555786133 2.665626049041748 14.823382377624512
Loss :  1.5939589738845825 2.062290906906128 11.905413627624512
Loss :  1.5586466789245605 2.423651933670044 13.67690658569336
Loss :  1.5558384656906128 1.980319619178772 11.457436561584473
Loss :  1.5057286024093628 2.4147443771362305 13.579450607299805
Loss :  1.5277189016342163 2.5127999782562256 14.091718673706055
Loss :  1.5259885787963867 2.357644557952881 13.31421184539795
Loss :  1.6145902872085571 2.2207024097442627 12.71810245513916
Loss :  1.6223196983337402 1.8761231899261475 11.002935409545898
Loss :  1.6374948024749756 1.8672338724136353 10.973664283752441
  batch 40 loss: 1.6374948024749756, 1.8672338724136353, 10.973664283752441
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.5790573358535767 1.9505633115768433 11.331873893737793
Loss :  1.5605428218841553 2.2617225646972656 12.869155883789062
Loss :  1.5457884073257446 3.042888879776001 16.76023292541504
Loss :  1.5646787881851196 2.7088377475738525 15.108867645263672
Loss :  1.5329254865646362 2.5578410625457764 14.322131156921387
Loss :  1.5771865844726562 2.520549774169922 14.179935455322266
Loss :  1.6252175569534302 2.0115792751312256 11.683114051818848
Loss :  1.5547605752944946 2.5776543617248535 14.443032264709473
Loss :  1.6445974111557007 1.8991520404815674 11.14035701751709
Loss :  1.5630438327789307 2.3667142391204834 13.396615028381348
Loss :  1.6092727184295654 2.2861108779907227 13.039827346801758
Loss :  1.600636601448059 2.258282423019409 12.892048835754395
Loss :  1.5710827112197876 2.3780298233032227 13.46123218536377
Loss :  1.6107289791107178 2.4969911575317383 14.095685005187988
Loss :  1.554268479347229 2.77886962890625 15.448616981506348
Loss :  1.643699288368225 2.2822468280792236 13.054933547973633
Loss :  1.5650383234024048 2.2926230430603027 13.028154373168945
Loss :  1.5415947437286377 2.381775140762329 13.450469970703125
Loss :  1.5642505884170532 2.985488176345825 16.49169158935547
Loss :  1.655319333076477 1.7701703310012817 10.506171226501465
  batch 60 loss: 1.655319333076477, 1.7701703310012817, 10.506171226501465
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.543249487876892 1.9493029117584229 11.289763450622559
Loss :  1.584261417388916 1.8941588401794434 11.055055618286133
Loss :  1.5562821626663208 1.9965993165969849 11.539278984069824
Loss :  1.5398136377334595 2.6367709636688232 14.723669052124023
Loss :  1.5120207071304321 2.091531991958618 11.969680786132812
Loss :  1.5575928688049316 4.085504531860352 21.98511505126953
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.574434518814087 4.042277812957764 21.785823822021484
Loss :  1.5685065984725952 4.08373498916626 21.987180709838867
Loss :  1.58638596534729 3.8008108139038086 20.59044075012207
Total LOSS train 13.076228963411772 valid 21.58714008331299
CE LOSS train 1.573482163135822 valid 0.3965964913368225
Contrastive LOSS train 2.300549345750075 valid 0.9502027034759521
EPOCH 156:
Loss :  1.6053019762039185 2.251105546951294 12.86082935333252
Loss :  1.6256139278411865 2.7257814407348633 15.254521369934082
Loss :  1.5818040370941162 2.875241279602051 15.95801067352295
Loss :  1.5892733335494995 3.416388750076294 18.67121696472168
Loss :  1.6149535179138184 2.478991746902466 14.009912490844727
Loss :  1.5643173456192017 1.8120592832565308 10.624613761901855
Loss :  1.6147387027740479 2.129164457321167 12.260560989379883
Loss :  1.5789577960968018 1.738012433052063 10.269020080566406
Loss :  1.5651863813400269 2.2231247425079346 12.68080997467041
Loss :  1.6149157285690308 2.2398064136505127 12.813947677612305
Loss :  1.5531816482543945 2.676053524017334 14.933449745178223
Loss :  1.5524488687515259 2.6897246837615967 15.001072883605957
Loss :  1.5493295192718506 3.059133768081665 16.84499740600586
Loss :  1.5569509267807007 2.0654313564300537 11.88410758972168
Loss :  1.6351016759872437 2.727851390838623 15.274358749389648
Loss :  1.629128336906433 1.8200592994689941 10.729424476623535
Loss :  1.5443553924560547 2.229767084121704 12.693190574645996
Loss :  1.5824095010757446 2.416857957839966 13.666699409484863
Loss :  1.5393638610839844 3.7888453006744385 20.483591079711914
Loss :  1.6274898052215576 1.8701884746551514 10.978432655334473
  batch 20 loss: 1.6274898052215576, 1.8701884746551514, 10.978432655334473
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.5748493671417236 1.9569320678710938 11.359509468078613
Loss :  1.5381923913955688 3.127316474914551 17.174774169921875
Loss :  1.5647140741348267 1.948639154434204 11.307909965515137
Loss :  1.586670160293579 1.9094732999801636 11.13403606414795
Loss :  1.6283801794052124 2.384038209915161 13.54857063293457
Loss :  1.5695401430130005 2.263190269470215 12.885491371154785
Loss :  1.58384370803833 2.4013373851776123 13.590530395507812
Loss :  1.57646644115448 2.1178793907165527 12.165863990783691
Loss :  1.5038810968399048 2.454279899597168 13.775280952453613
Loss :  1.6228710412979126 2.376620054244995 13.505971908569336
Loss :  1.5041924715042114 2.072406768798828 11.866226196289062
Loss :  1.5996756553649902 2.449770212173462 13.848526000976562
Loss :  1.5655877590179443 2.2813425064086914 12.97230052947998
Loss :  1.5629589557647705 2.263533353805542 12.88062572479248
Loss :  1.515316367149353 2.289639711380005 12.96351432800293
Loss :  1.5361851453781128 2.5546250343322754 14.309309959411621
Loss :  1.535021424293518 2.5887842178344727 14.47894287109375
Loss :  1.6205660104751587 2.1804628372192383 12.522880554199219
Loss :  1.6279789209365845 2.3535406589508057 13.395682334899902
Loss :  1.6420133113861084 2.1082332134246826 12.18317985534668
  batch 40 loss: 1.6420133113861084, 2.1082332134246826, 12.18317985534668
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.584521770477295 2.265888214111328 12.913963317871094
Loss :  1.565556287765503 3.3579952716827393 18.355531692504883
Loss :  1.5508909225463867 2.4421091079711914 13.761436462402344
Loss :  1.5690559148788452 2.4012115001678467 13.575113296508789
Loss :  1.5384516716003418 2.165236473083496 12.364633560180664
Loss :  1.582147479057312 2.4505674839019775 13.83498477935791
Loss :  1.630104899406433 2.6528167724609375 14.89418888092041
Loss :  1.5616475343704224 1.9220315217971802 11.171805381774902
Loss :  1.6493054628372192 2.247276782989502 12.885688781738281
Loss :  1.5694218873977661 1.9846757650375366 11.49280071258545
Loss :  1.6149183511734009 2.132721185684204 12.278524398803711
Loss :  1.6062849760055542 2.334376335144043 13.278166770935059
Loss :  1.5768941640853882 2.4883410930633545 14.018599510192871
Loss :  1.616603970527649 2.6698646545410156 14.965927124023438
Loss :  1.5578361749649048 2.4561874866485596 13.838773727416992
Loss :  1.6492877006530762 2.7188024520874023 15.24329948425293
Loss :  1.5704381465911865 3.5707385540008545 19.424131393432617
Loss :  1.547261118888855 3.6571202278137207 19.832860946655273
Loss :  1.5698633193969727 3.35807204246521 18.36022186279297
Loss :  1.6602650880813599 2.792357921600342 15.622055053710938
  batch 60 loss: 1.6602650880813599, 2.792357921600342, 15.622055053710938
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.5503103733062744 2.1660618782043457 12.380619049072266
Loss :  1.5901135206222534 2.3608484268188477 13.394355773925781
Loss :  1.5624929666519165 2.443885087966919 13.7819185256958
Loss :  1.545699954032898 3.0027108192443848 16.559253692626953
Loss :  1.517834186553955 2.354606866836548 13.290868759155273
Loss :  1.55482816696167 4.312507152557373 23.11736488342285
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 5], device='cuda:0')
Loss :  1.573486566543579 4.281258583068848 22.979778289794922
Loss :  1.5694916248321533 4.223989486694336 22.68943977355957
Loss :  1.5827362537384033 3.9766252040863037 21.465862274169922
Total LOSS train 13.865871047973632 valid 22.563111305236816
CE LOSS train 1.5803220730561476 valid 0.39568406343460083
Contrastive LOSS train 2.457109808921814 valid 0.9941563010215759
EPOCH 157:
Loss :  1.6097075939178467 2.174070119857788 12.480058670043945
Loss :  1.629337191581726 3.14483380317688 17.353506088256836
Loss :  1.585465669631958 2.612767457962036 14.64930248260498
Loss :  1.5928250551223755 2.4550631046295166 13.86814022064209
Loss :  1.6177717447280884 1.9631098508834839 11.433320999145508
Loss :  1.568621039390564 2.0760741233825684 11.948991775512695
Loss :  1.6179981231689453 2.4442741870880127 13.83936882019043
Loss :  1.5837979316711426 2.286059856414795 13.014097213745117
Loss :  1.5706679821014404 2.5462639331817627 14.301987648010254
Loss :  1.6188791990280151 3.1287734508514404 17.262746810913086
Loss :  1.5581188201904297 2.8607704639434814 15.861970901489258
Loss :  1.556439757347107 2.5875165462493896 14.494022369384766
Loss :  1.553206443786621 2.1221354007720947 12.163883209228516
Loss :  1.5611563920974731 2.3768951892852783 13.445631980895996
Loss :  1.6371285915374756 2.4854135513305664 14.064196586608887
Loss :  1.6319478750228882 2.353184938430786 13.397871971130371
Loss :  1.5496407747268677 3.116398811340332 17.131635665893555
Loss :  1.586914300918579 2.956084728240967 16.367338180541992
Loss :  1.5448482036590576 3.0439486503601074 16.764591217041016
Loss :  1.629198431968689 2.4984633922576904 14.121515274047852
  batch 20 loss: 1.629198431968689, 2.4984633922576904, 14.121515274047852
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.5797951221466064 2.3021395206451416 13.090492248535156
Loss :  1.5431039333343506 2.996629238128662 16.5262508392334
Loss :  1.5689976215362549 2.088373899459839 12.01086711883545
Loss :  1.591437816619873 2.384335517883301 13.513114929199219
Loss :  1.6316817998886108 2.6183695793151855 14.723529815673828
Loss :  1.5742696523666382 2.1620030403137207 12.384284019470215
Loss :  1.5884114503860474 2.253338098526001 12.855101585388184
Loss :  1.580658197402954 2.115957736968994 12.160446166992188
Loss :  1.5123428106307983 2.462083339691162 13.822759628295898
Loss :  1.6282483339309692 3.174757480621338 17.50203514099121
Loss :  1.5155316591262817 3.999277353286743 21.511919021606445
Loss :  1.6067888736724854 3.3169779777526855 18.191679000854492
Loss :  1.5749156475067139 2.4271273612976074 13.710553169250488
Loss :  1.5716301202774048 2.416836738586426 13.655814170837402
Loss :  1.5253994464874268 2.484475612640381 13.94777774810791
Loss :  1.5450282096862793 2.3073108196258545 13.081583023071289
Loss :  1.542923927307129 2.3383657932281494 13.234752655029297
Loss :  1.6238749027252197 3.223445177078247 17.741100311279297
Loss :  1.6317367553710938 2.2321503162384033 12.792488098144531
Loss :  1.6448310613632202 2.0213377475738525 11.751519203186035
  batch 40 loss: 1.6448310613632202, 2.0213377475738525, 11.751519203186035
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.5893514156341553 2.4008734226226807 13.593718528747559
Loss :  1.5690369606018066 2.046208143234253 11.800077438354492
Loss :  1.5554965734481812 2.605539083480835 14.583191871643066
Loss :  1.5723752975463867 2.5096499919891357 14.120625495910645
Loss :  1.5429537296295166 2.049734354019165 11.791625022888184
Loss :  1.5856441259384155 2.123114585876465 12.201216697692871
Loss :  1.6307967901229858 2.2872314453125 13.066953659057617
Loss :  1.565246820449829 2.8697562217712402 15.91402816772461
Loss :  1.649368405342102 2.4675796031951904 13.987266540527344
Loss :  1.5728509426116943 2.6585311889648438 14.865507125854492
Loss :  1.6177350282669067 2.3665382862091064 13.45042610168457
Loss :  1.609879493713379 2.824716806411743 15.733463287353516
Loss :  1.5812244415283203 2.3110125064849854 13.136286735534668
Loss :  1.619582176208496 2.5388102531433105 14.313633918762207
Loss :  1.5645407438278198 2.0478429794311523 11.803755760192871
Loss :  1.6507841348648071 2.2841150760650635 13.071359634399414
Loss :  1.5751004219055176 2.3354756832122803 13.252479553222656
Loss :  1.552738904953003 2.7275185585021973 15.19033145904541
Loss :  1.5738438367843628 3.2196900844573975 17.672292709350586
Loss :  1.6603294610977173 2.3455822467803955 13.388240814208984
  batch 60 loss: 1.6603294610977173, 2.3455822467803955, 13.388240814208984
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.5538040399551392 2.3891167640686035 13.499387741088867
Loss :  1.593009114265442 2.3961379528045654 13.573698997497559
Loss :  1.5664174556732178 2.5100083351135254 14.116458892822266
Loss :  1.5506961345672607 3.8795218467712402 20.948305130004883
Loss :  1.5244983434677124 2.2868077754974365 12.958537101745605
Loss :  1.5838197469711304 4.169547080993652 22.431556701660156
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.5993967056274414 4.219531059265137 22.697052001953125
Loss :  1.5958337783813477 4.026469707489014 21.72818374633789
Loss :  1.6121060848236084 3.906696081161499 21.145586013793945
Total LOSS train 14.280078682532677 valid 22.00059461593628
CE LOSS train 1.5848089731656587 valid 0.4030265212059021
Contrastive LOSS train 2.5390539554449227 valid 0.9766740202903748
EPOCH 158:
Loss :  1.6136945486068726 2.343895435333252 13.333170890808105
Loss :  1.6322988271713257 2.269979953765869 12.982197761535645
Loss :  1.5904309749603271 1.714536428451538 10.163113594055176
Loss :  1.5976498126983643 2.204115390777588 12.618226051330566
Loss :  1.6224199533462524 2.294543504714966 13.095137596130371
Loss :  1.5740129947662354 2.3863437175750732 13.505731582641602
Loss :  1.6219956874847412 3.155029058456421 17.397140502929688
Loss :  1.5889387130737305 2.1076505184173584 12.127191543579102
Loss :  1.5765529870986938 3.2984931468963623 18.069019317626953
Loss :  1.6247029304504395 2.549905300140381 14.374229431152344
Loss :  1.565995216369629 2.4161410331726074 13.646700859069824
Loss :  1.5647592544555664 2.1914007663726807 12.52176284790039
Loss :  1.5614453554153442 2.312375068664551 13.123320579528809
Loss :  1.5693087577819824 2.426135540008545 13.699987411499023
Loss :  1.6432156562805176 2.3598415851593018 13.442422866821289
Loss :  1.6376734972000122 2.475288152694702 14.014114379882812
Loss :  1.558842658996582 2.0472233295440674 11.79495906829834
Loss :  1.5948262214660645 3.4095962047576904 18.642807006835938
Loss :  1.5540074110031128 1.8951504230499268 11.029759407043457
Loss :  1.6371872425079346 1.935022234916687 11.312297821044922
  batch 20 loss: 1.6371872425079346, 1.935022234916687, 11.312297821044922
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.5886080265045166 2.355700731277466 13.367111206054688
Loss :  1.553601861000061 2.969726085662842 16.402233123779297
Loss :  1.5773556232452393 2.602456569671631 14.589638710021973
Loss :  1.596647024154663 2.3163235187530518 13.178264617919922
Loss :  1.6350651979446411 2.7275798320770264 15.272964477539062
Loss :  1.5795575380325317 2.9449245929718018 16.304180145263672
Loss :  1.5924402475357056 4.3651442527771 23.418161392211914
Loss :  1.5852222442626953 2.7067949771881104 15.119196891784668
Loss :  1.5182820558547974 3.185994863510132 17.448257446289062
Loss :  1.6304852962493896 2.585387945175171 14.557425498962402
Loss :  1.51951003074646 2.481048583984375 13.924753189086914
Loss :  1.608292818069458 2.4315779209136963 13.766181945800781
Loss :  1.5777153968811035 2.5155868530273438 14.155649185180664
Loss :  1.5752805471420288 2.26654052734375 12.90798282623291
Loss :  1.529409408569336 2.548060894012451 14.26971435546875
Loss :  1.5493613481521606 3.0954790115356445 17.026756286621094
Loss :  1.5472921133041382 2.8052544593811035 15.573563575744629
Loss :  1.627563714981079 2.798938751220703 15.622257232666016
Loss :  1.6346315145492554 2.3433897495269775 13.351579666137695
Loss :  1.6477465629577637 2.2523183822631836 12.909337997436523
  batch 40 loss: 1.6477465629577637, 2.2523183822631836, 12.909337997436523
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.592788815498352 3.5489556789398193 19.337566375732422
Loss :  1.574357509613037 2.674285650253296 14.945785522460938
Loss :  1.560239553451538 2.524742841720581 14.183954238891602
Loss :  1.5775513648986816 3.2860677242279053 18.007888793945312
Loss :  1.5487804412841797 3.186974048614502 17.48365020751953
Loss :  1.5891063213348389 2.9278218746185303 16.22821617126465
Loss :  1.6336572170257568 2.514326810836792 14.205291748046875
Loss :  1.5702799558639526 2.6587440967559814 14.86400032043457
Loss :  1.6538643836975098 2.8631346225738525 15.969537734985352
Loss :  1.5790460109710693 2.245452642440796 12.806309700012207
Loss :  1.6232959032058716 2.1743130683898926 12.494860649108887
Loss :  1.6154191493988037 3.5189096927642822 19.20996856689453
Loss :  1.5881719589233398 2.537318468093872 14.274764060974121
Loss :  1.6250345706939697 2.1624855995178223 12.437461853027344
Loss :  1.5725337266921997 2.3083150386810303 13.11410903930664
Loss :  1.6547045707702637 2.0590107440948486 11.949758529663086
Loss :  1.581034779548645 1.9232985973358154 11.197527885437012
Loss :  1.5586377382278442 1.8750499486923218 10.933887481689453
Loss :  1.5799005031585693 3.194805383682251 17.553926467895508
Loss :  1.6630525588989258 2.2932941913604736 13.129523277282715
  batch 60 loss: 1.6630525588989258, 2.2932941913604736, 13.129523277282715
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.5594674348831177 2.392451286315918 13.521723747253418
Loss :  1.5978485345840454 3.1653523445129395 17.424610137939453
Loss :  1.5707104206085205 2.03023624420166 11.721891403198242
Loss :  1.5541868209838867 2.374429225921631 13.4263334274292
Loss :  1.5276445150375366 1.747460961341858 10.264948844909668
Loss :  1.5616549253463745 4.350358486175537 23.313447952270508
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3], device='cuda:0')
Loss :  1.5822794437408447 4.33926248550415 23.27859115600586
Loss :  1.5783075094223022 4.355015277862549 23.353384017944336
Loss :  1.569042682647705 4.296679496765137 23.052440643310547
Total LOSS train 14.380707638080304 valid 23.249465942382812
CE LOSS train 1.590082185085003 valid 0.39226067066192627
Contrastive LOSS train 2.5581251089389507 valid 1.0741698741912842
EPOCH 159:
Loss :  1.615374207496643 1.9471967220306396 11.351358413696289
Loss :  1.6330554485321045 2.8814187049865723 16.04014778137207
Loss :  1.5922547578811646 2.294201612472534 13.063262939453125
Loss :  1.5999845266342163 2.433157444000244 13.76577091217041
Loss :  1.624921202659607 3.097269296646118 17.11126708984375
Loss :  1.5772162675857544 2.1404125690460205 12.279278755187988
Loss :  1.624301791191101 2.4662153720855713 13.955378532409668
Loss :  1.591480016708374 2.127091407775879 12.226937294006348
Loss :  1.5789073705673218 2.6585092544555664 14.871453285217285
Loss :  1.6243418455123901 3.193235158920288 17.590517044067383
Loss :  1.5668212175369263 2.3521640300750732 13.327641487121582
Loss :  1.5656534433364868 2.2964978218078613 13.048142433166504
Loss :  1.5630924701690674 2.105635643005371 12.091270446777344
Loss :  1.5711419582366943 2.5473339557647705 14.307811737060547
Loss :  1.644417643547058 1.9654972553253174 11.471903800964355
Loss :  1.6392017602920532 2.658162832260132 14.93001651763916
Loss :  1.562117338180542 2.725212574005127 15.188179969787598
Loss :  1.59689462184906 1.855316400527954 10.873476028442383
Loss :  1.557361364364624 1.7479711771011353 10.29721736907959
Loss :  1.6381298303604126 3.027618169784546 16.776220321655273
  batch 20 loss: 1.6381298303604126, 3.027618169784546, 16.776220321655273
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.5897191762924194 2.3219480514526367 13.199459075927734
Loss :  1.5553476810455322 2.1883273124694824 12.496984481811523
Loss :  1.5798577070236206 1.872578501701355 10.942749977111816
Loss :  1.5998615026474 2.442925453186035 13.814488410949707
Loss :  1.638564944267273 3.4379842281341553 18.8284854888916
Loss :  1.58371901512146 2.4912478923797607 14.039958953857422
Loss :  1.5965940952301025 2.028026819229126 11.736727714538574
Loss :  1.5895328521728516 2.1177449226379395 12.17825698852539
Loss :  1.5219677686691284 3.3314454555511475 18.1791934967041
Loss :  1.6332377195358276 2.2649643421173096 12.958059310913086
Loss :  1.5240744352340698 2.207139492034912 12.559772491455078
Loss :  1.6119862794876099 2.0936384201049805 12.080178260803223
Loss :  1.5811816453933716 3.5903029441833496 19.532697677612305
Loss :  1.5787503719329834 2.400691032409668 13.582205772399902
Loss :  1.533144474029541 2.3796422481536865 13.431356430053711
Loss :  1.5523350238800049 2.250877618789673 12.806722640991211
Loss :  1.5509345531463623 2.605600118637085 14.578935623168945
Loss :  1.630000114440918 2.1602628231048584 12.431314468383789
Loss :  1.637377381324768 2.473005771636963 14.002406120300293
Loss :  1.650072693824768 2.2159457206726074 12.729802131652832
  batch 40 loss: 1.650072693824768, 2.2159457206726074, 12.729802131652832
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.5969786643981934 2.761627435684204 15.405115127563477
Loss :  1.5789133310317993 2.039783000946045 11.77782917022705
Loss :  1.5657609701156616 2.2473061084747314 12.802290916442871
Loss :  1.581929087638855 2.2831931114196777 12.997895240783691
Loss :  1.5535542964935303 1.808118224143982 10.594145774841309
Loss :  1.5934919118881226 3.6204535961151123 19.69576072692871
Loss :  1.6370257139205933 2.1729421615600586 12.501736640930176
Loss :  1.5742343664169312 2.3309242725372314 13.22885513305664
Loss :  1.6557269096374512 2.105119466781616 12.181324005126953
Loss :  1.5819919109344482 2.2648403644561768 12.906193733215332
Loss :  1.6251672506332397 2.2571802139282227 12.911067962646484
Loss :  1.6176098585128784 3.004568576812744 16.640451431274414
Loss :  1.5905500650405884 2.2566239833831787 12.87367057800293
Loss :  1.6271811723709106 2.4057464599609375 13.655913352966309
Loss :  1.5748813152313232 2.8019371032714844 15.584567070007324
Loss :  1.6575411558151245 2.3393449783325195 13.354266166687012
Loss :  1.5856220722198486 2.0974459648132324 12.07285213470459
Loss :  1.564448356628418 1.9663777351379395 11.396336555480957
Loss :  1.5848414897918701 2.3714118003845215 13.441901206970215
Loss :  1.667336106300354 2.2136027812957764 12.735350608825684
  batch 60 loss: 1.667336106300354, 2.2136027812957764, 12.735350608825684
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.5656490325927734 3.201921224594116 17.575254440307617
Loss :  1.6033650636672974 1.8202732801437378 10.704730987548828
Loss :  1.5774480104446411 2.2116644382476807 12.635769844055176
Loss :  1.5600104331970215 2.7257635593414307 15.188827514648438
Loss :  1.5347009897232056 1.8016643524169922 10.543023109436035
Loss :  1.5873554944992065 4.075448989868164 21.964599609375
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.6023870706558228 4.098406791687012 22.09442138671875
Loss :  1.5986090898513794 4.0594072341918945 21.895645141601562
Loss :  1.6133993864059448 3.8254525661468506 20.74066162109375
Total LOSS train 13.632032878582294 valid 21.673831939697266
CE LOSS train 1.5932444315690262 valid 0.4033498466014862
Contrastive LOSS train 2.4077577040745664 valid 0.9563631415367126
EPOCH 160:
Loss :  1.6191062927246094 2.1629104614257812 12.433658599853516
Loss :  1.635956048965454 2.732569456100464 15.298803329467773
Loss :  1.5953428745269775 2.1623966693878174 12.407325744628906
Loss :  1.6023200750350952 2.433427095413208 13.769454956054688
Loss :  1.62541925907135 1.9379117488861084 11.31497859954834
Loss :  1.5782139301300049 2.926091432571411 16.20867156982422
Loss :  1.624808669090271 3.07653546333313 17.00748634338379
Loss :  1.5919264554977417 1.9519776105880737 11.351814270019531
Loss :  1.580055832862854 2.655940055847168 14.859756469726562
Loss :  1.625382423400879 3.4235615730285645 18.74319076538086
Loss :  1.5694301128387451 2.632549285888672 14.732176780700684
Loss :  1.5692095756530762 2.978757381439209 16.462997436523438
Loss :  1.5661122798919678 2.3104708194732666 13.1184663772583
Loss :  1.5736517906188965 2.6124346256256104 14.635824203491211
Loss :  1.6455758810043335 2.1941256523132324 12.616204261779785
Loss :  1.640406847000122 1.8931584358215332 11.10619831085205
Loss :  1.5640151500701904 2.4948983192443848 14.038506507873535
Loss :  1.5982638597488403 2.3372418880462646 13.284473419189453
Loss :  1.5593452453613281 1.9484726190567017 11.301708221435547
Loss :  1.6397969722747803 2.2552473545074463 12.916033744812012
  batch 20 loss: 1.6397969722747803, 2.2552473545074463, 12.916033744812012
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.5916504859924316 2.0842721462249756 12.013011932373047
Loss :  1.55777907371521 2.985236644744873 16.483963012695312
Loss :  1.5819839239120483 3.7875473499298096 20.51972007751465
Loss :  1.6017476320266724 2.6172242164611816 14.687868118286133
Loss :  1.639875888824463 2.8854620456695557 16.06718635559082
Loss :  1.5851750373840332 2.2310948371887207 12.74064826965332
Loss :  1.5983730554580688 2.12634539604187 12.230100631713867
Loss :  1.5918192863464355 2.5938796997070312 14.56121826171875
Loss :  1.526047945022583 2.73840594291687 15.218077659606934
Loss :  1.6352593898773193 2.6759862899780273 15.015191078186035
Loss :  1.526867389678955 2.6849241256713867 14.951488494873047
Loss :  1.6139715909957886 2.435214042663574 13.79004192352295
Loss :  1.5825179815292358 2.4642605781555176 13.903820037841797
Loss :  1.5796973705291748 2.211604356765747 12.63771915435791
Loss :  1.5349833965301514 2.3159210681915283 13.114588737487793
Loss :  1.5548138618469238 2.3612639904022217 13.361133575439453
Loss :  1.5529217720031738 2.5174570083618164 14.140207290649414
Loss :  1.6318624019622803 2.3016533851623535 13.140129089355469
Loss :  1.6387548446655273 2.5042014122009277 14.159762382507324
Loss :  1.651923656463623 2.485018253326416 14.077014923095703
  batch 40 loss: 1.651923656463623, 2.485018253326416, 14.077014923095703
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.5995159149169922 2.509730577468872 14.148168563842773
Loss :  1.582560658454895 2.586750030517578 14.516310691833496
Loss :  1.5687079429626465 2.4304323196411133 13.720869064331055
Loss :  1.5852292776107788 2.4115452766418457 13.64295482635498
Loss :  1.5565903186798096 2.0819356441497803 11.966268539428711
Loss :  1.5962255001068115 3.278745412826538 17.989952087402344
Loss :  1.639051914215088 2.7177700996398926 15.227901458740234
Loss :  1.575007438659668 2.3026838302612305 13.08842658996582
Loss :  1.6570546627044678 3.0033652782440186 16.67388153076172
Loss :  1.5824133157730103 2.022423505783081 11.694531440734863
Loss :  1.6252573728561401 2.30903697013855 13.170442581176758
Loss :  1.618260145187378 2.08229398727417 12.029730796813965
Loss :  1.591646671295166 2.331550359725952 13.249399185180664
Loss :  1.6283879280090332 2.5749104022979736 14.502939224243164
Loss :  1.5762275457382202 2.768960475921631 15.421030044555664
Loss :  1.6582608222961426 2.4911651611328125 14.114086151123047
Loss :  1.5864108800888062 2.8464834690093994 15.818827629089355
Loss :  1.5652079582214355 2.8707847595214844 15.919132232666016
Loss :  1.5853935480117798 2.631394624710083 14.742366790771484
Loss :  1.6671907901763916 2.3498528003692627 13.416454315185547
  batch 60 loss: 1.6671907901763916, 2.3498528003692627, 13.416454315185547
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.5667065382003784 3.7035410404205322 20.08441162109375
Loss :  1.6035135984420776 2.2329535484313965 12.768281936645508
Loss :  1.5782804489135742 2.3094420433044434 13.12549114227295
Loss :  1.5629974603652954 2.469372272491455 13.909858703613281
Loss :  1.5380198955535889 1.7029353380203247 10.05269718170166
Loss :  1.5898178815841675 4.31251335144043 23.15238380432129
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 5], device='cuda:0')
Loss :  1.6050008535385132 4.2904052734375 23.05702781677246
Loss :  1.604974389076233 4.147478103637695 22.342365264892578
Loss :  1.5950120687484741 3.9801816940307617 21.495922088623047
Total LOSS train 14.144846696120043 valid 22.511924743652344
CE LOSS train 1.595022832430326 valid 0.39875301718711853
Contrastive LOSS train 2.5099647687031674 valid 0.9950454235076904
EPOCH 161:
Loss :  1.622186541557312 2.056408166885376 11.904227256774902
Loss :  1.6392451524734497 2.272881269454956 13.00365161895752
Loss :  1.5994302034378052 1.7038131952285767 10.11849594116211
Loss :  1.6064642667770386 2.9190878868103027 16.201904296875
Loss :  1.6289634704589844 2.37870192527771 13.522473335266113
Loss :  1.5828912258148193 2.015397787094116 11.659880638122559
Loss :  1.6278960704803467 2.4756431579589844 14.006112098693848
Loss :  1.5957216024398804 3.066087484359741 16.926158905029297
Loss :  1.584270715713501 2.067196846008301 11.920254707336426
Loss :  1.628498911857605 2.089033365249634 12.073665618896484
Loss :  1.5729975700378418 2.3310627937316895 13.228311538696289
Loss :  1.571900486946106 2.6645591259002686 14.894696235656738
Loss :  1.5688508749008179 2.1389286518096924 12.263493537902832
Loss :  1.5763142108917236 2.3248066902160645 13.200346946716309
Loss :  1.6475396156311035 2.507075548171997 14.182916641235352
Loss :  1.6420013904571533 1.9245383739471436 11.264693260192871
Loss :  1.5665010213851929 2.550555944442749 14.319280624389648
Loss :  1.6010431051254272 1.877766728401184 10.989876747131348
Loss :  1.5628979206085205 1.8511455059051514 10.818625450134277
Loss :  1.6420758962631226 2.0348544120788574 11.8163480758667
  batch 20 loss: 1.6420758962631226, 2.0348544120788574, 11.8163480758667
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.5960898399353027 1.8511525392532349 10.851852416992188
Loss :  1.5631064176559448 2.1710236072540283 12.418224334716797
Loss :  1.5867931842803955 2.4943418502807617 14.058502197265625
Loss :  1.6064417362213135 3.2734644412994385 17.973764419555664
Loss :  1.642240047454834 3.104318141937256 17.16383171081543
Loss :  1.5894767045974731 2.9587361812591553 16.38315773010254
Loss :  1.6020091772079468 2.4094173908233643 13.64909553527832
Loss :  1.5937836170196533 2.658252000808716 14.885043144226074
Loss :  1.529118537902832 2.7428736686706543 15.243487358093262
Loss :  1.635855793952942 2.9677224159240723 16.474468231201172
Loss :  1.530134916305542 2.6711933612823486 14.886101722717285
Loss :  1.6162348985671997 2.131521701812744 12.273842811584473
Loss :  1.5858545303344727 2.0832903385162354 12.00230598449707
Loss :  1.583452582359314 2.602226734161377 14.594585418701172
Loss :  1.540541410446167 2.600275993347168 14.541921615600586
Loss :  1.558890700340271 2.639697313308716 14.757376670837402
Loss :  1.5570012331008911 3.2317001819610596 17.71550178527832
Loss :  1.6325433254241943 2.5536623001098633 14.40085506439209
Loss :  1.639674425125122 2.6500627994537354 14.88998794555664
Loss :  1.6520580053329468 2.0288138389587402 11.796127319335938
  batch 40 loss: 1.6520580053329468, 2.0288138389587402, 11.796127319335938
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.6008707284927368 2.5170607566833496 14.186175346374512
Loss :  1.5838302373886108 2.299455404281616 13.081107139587402
Loss :  1.5724494457244873 2.427018404006958 13.707541465759277
Loss :  1.5886175632476807 2.5127577781677246 14.152406692504883
Loss :  1.5622283220291138 2.670427083969116 14.914363861083984
Loss :  1.5992412567138672 2.7472121715545654 15.335302352905273
Loss :  1.640737771987915 2.792224407196045 15.601860046386719
Loss :  1.5812824964523315 2.1236836910247803 12.199701309204102
Loss :  1.6582626104354858 2.012425661087036 11.720390319824219
Loss :  1.5877156257629395 1.9484398365020752 11.329914093017578
Loss :  1.6292492151260376 2.548586368560791 14.372180938720703
Loss :  1.6224679946899414 2.822441577911377 15.734675407409668
Loss :  1.5963854789733887 1.9598748683929443 11.395759582519531
Loss :  1.6314244270324707 2.3125078678131104 13.193964004516602
Loss :  1.5817240476608276 2.5425915718078613 14.294681549072266
Loss :  1.66064453125 2.3843095302581787 13.582192420959473
Loss :  1.5908955335617065 2.0895934104919434 12.038863182067871
Loss :  1.5705939531326294 3.4281725883483887 18.711458206176758
Loss :  1.589847445487976 2.441025733947754 13.794976234436035
Loss :  1.669686198234558 1.8979350328445435 11.159361839294434
  batch 60 loss: 1.669686198234558, 1.8979350328445435, 11.159361839294434
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.571033000946045 2.1372435092926025 12.25724983215332
Loss :  1.6068028211593628 2.4901657104492188 14.057631492614746
Loss :  1.5825937986373901 2.2547247409820557 12.856217384338379
Loss :  1.5680570602416992 2.5062789916992188 14.099452018737793
Loss :  1.5432662963867188 2.5162696838378906 14.124614715576172
Loss :  1.587754726409912 4.1255083084106445 22.215295791625977
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 5], device='cuda:0')
Loss :  1.6020840406417847 4.190702438354492 22.55559539794922
Loss :  1.5983169078826904 4.066714763641357 21.9318904876709
Loss :  1.6076854467391968 3.940493583679199 21.31015396118164
Total LOSS train 13.710422897338868 valid 22.003233909606934
CE LOSS train 1.5984446030396682 valid 0.4019213616847992
Contrastive LOSS train 2.42239566216102 valid 0.9851233959197998
EPOCH 162:
Loss :  1.6249539852142334 2.466552972793579 13.957718849182129
Loss :  1.641737937927246 3.2241196632385254 17.76233673095703
Loss :  1.6032356023788452 2.16352915763855 12.420881271362305
Loss :  1.6098928451538086 2.2845282554626465 13.0325345993042
Loss :  1.6322237253189087 2.0602166652679443 11.933307647705078
Loss :  1.5880770683288574 2.231659412384033 12.746374130249023
Loss :  1.6320931911468506 2.331418037414551 13.289183616638184
Loss :  1.6019068956375122 2.0577988624572754 11.890900611877441
Loss :  1.5902764797210693 1.7087956666946411 10.134255409240723
Loss :  1.6334844827651978 1.882935643196106 11.048163414001465
Loss :  1.5796122550964355 2.309051990509033 13.124872207641602
Loss :  1.57843816280365 2.1203601360321045 12.180238723754883
Loss :  1.5751477479934692 4.067200660705566 21.911149978637695
Loss :  1.5813329219818115 2.54756498336792 14.319158554077148
Loss :  1.6503047943115234 2.0572640895843506 11.936625480651855
Loss :  1.645052433013916 2.413982629776001 13.7149658203125
Loss :  1.571374535560608 2.0462374687194824 11.802562713623047
Loss :  1.6046044826507568 2.4233779907226562 13.721494674682617
Loss :  1.5678478479385376 3.1104586124420166 17.120140075683594
Loss :  1.6453043222427368 2.540581464767456 14.348212242126465
  batch 20 loss: 1.6453043222427368, 2.540581464767456, 14.348212242126465
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.599568486213684 2.2608039379119873 12.90358829498291
Loss :  1.567891240119934 2.043260097503662 11.784192085266113
Loss :  1.590193271636963 3.0998432636260986 17.08940887451172
Loss :  1.6090478897094727 3.475146770477295 18.984783172607422
Loss :  1.6441909074783325 2.683373212814331 15.061057090759277
Loss :  1.5921685695648193 2.3406388759613037 13.295363426208496
Loss :  1.6044738292694092 2.291114330291748 13.060046195983887
Loss :  1.596895456314087 2.0605032444000244 11.89941120147705
Loss :  1.5335828065872192 3.2629010677337646 17.848087310791016
Loss :  1.6393110752105713 2.9486241340637207 16.382431030273438
Loss :  1.5349581241607666 2.058955192565918 11.829733848571777
Loss :  1.6197031736373901 2.0836527347564697 12.03796672821045
Loss :  1.5899107456207275 2.2391152381896973 12.785486221313477
Loss :  1.5873397588729858 2.948699474334717 16.33083724975586
Loss :  1.544625163078308 3.4046638011932373 18.567943572998047
Loss :  1.563004970550537 2.1036508083343506 12.081258773803711
Loss :  1.5616339445114136 2.005124092102051 11.587254524230957
Loss :  1.637013554573059 2.165065050125122 12.4623384475708
Loss :  1.643530011177063 2.563805341720581 14.462556838989258
Loss :  1.6562820672988892 2.4312760829925537 13.812663078308105
  batch 40 loss: 1.6562820672988892, 2.4312760829925537, 13.812663078308105
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.606264352798462 2.3237593173980713 13.22506046295166
Loss :  1.5896133184432983 1.7109335660934448 10.144280433654785
Loss :  1.5771100521087646 2.0167548656463623 11.660884857177734
Loss :  1.592848539352417 2.5409743785858154 14.297720909118652
Loss :  1.5661113262176514 2.014798164367676 11.64010238647461
Loss :  1.6037585735321045 3.054370880126953 16.875612258911133
Loss :  1.6455594301223755 2.730970859527588 15.300413131713867
Loss :  1.5860767364501953 3.0037550926208496 16.6048526763916
Loss :  1.6624587774276733 3.016201972961426 16.74346923828125
Loss :  1.5935190916061401 2.1143922805786133 12.165480613708496
Loss :  1.6345999240875244 2.322622776031494 13.247713088989258
Loss :  1.626607894897461 2.4593117237091064 13.923166275024414
Loss :  1.601283073425293 2.3139500617980957 13.171032905578613
Loss :  1.6358009576797485 2.541929006576538 14.345446586608887
Loss :  1.587114691734314 2.518526315689087 14.1797456741333
Loss :  1.66329026222229 1.8435903787612915 10.881241798400879
Loss :  1.5955392122268677 2.255293846130371 12.872008323669434
Loss :  1.5755796432495117 3.84171462059021 20.78415298461914
Loss :  1.5942684412002563 2.816746711730957 15.67800235748291
Loss :  1.6718262434005737 1.9869471788406372 11.606562614440918
  batch 60 loss: 1.6718262434005737, 1.9869471788406372, 11.606562614440918
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.5763131380081177 2.129232406616211 12.222475051879883
Loss :  1.6115624904632568 1.9773741960525513 11.498434066772461
Loss :  1.5862332582473755 1.7988498210906982 10.580482482910156
Loss :  1.5712631940841675 2.2131476402282715 12.637001991271973
Loss :  1.5469388961791992 2.1286020278930664 12.189949035644531
Loss :  1.5865447521209717 3.9265894889831543 21.219491958618164
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 5], device='cuda:0')
Loss :  1.6020342111587524 3.8469021320343018 20.836544036865234
Loss :  1.5970920324325562 3.7177512645721436 20.185848236083984
Loss :  1.6077722311019897 3.580768346786499 19.511613845825195
Total LOSS train 13.771335029602051 valid 20.438374519348145
CE LOSS train 1.6026733581836408 valid 0.40194305777549744
Contrastive LOSS train 2.43373232621413 valid 0.8951920866966248
EPOCH 163:
Loss :  1.6281025409698486 2.8977274894714355 16.116741180419922
Loss :  1.6444162130355835 2.8290438652038574 15.78963565826416
Loss :  1.6073510646820068 2.382204532623291 13.518373489379883
Loss :  1.6142351627349854 2.4617295265197754 13.922882080078125
Loss :  1.6355940103530884 2.112168073654175 12.19643497467041
Loss :  1.5921896696090698 2.2927823066711426 13.056100845336914
Loss :  1.6356557607650757 3.3934926986694336 18.603120803833008
Loss :  1.6060452461242676 2.693081855773926 15.071455001831055
Loss :  1.5937490463256836 3.2253482341766357 17.720489501953125
Loss :  1.6359659433364868 2.9377472400665283 16.3247013092041
Loss :  1.58274507522583 2.5205178260803223 14.185333251953125
Loss :  1.5816888809204102 2.8407552242279053 15.785465240478516
Loss :  1.5791492462158203 2.6729748249053955 14.944023132324219
Loss :  1.5850056409835815 2.5115275382995605 14.142643928527832
Loss :  1.6514652967453003 2.367891550064087 13.490922927856445
Loss :  1.6482096910476685 2.8301446437835693 15.798933029174805
Loss :  1.5746164321899414 2.8308987617492676 15.729109764099121
Loss :  1.6071796417236328 2.561041831970215 14.412388801574707
Loss :  1.5707831382751465 3.0888302326202393 17.014934539794922
Loss :  1.6463230848312378 2.4013280868530273 13.652963638305664
  batch 20 loss: 1.6463230848312378, 2.4013280868530273, 13.652963638305664
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.6009521484375 2.746096134185791 15.331432342529297
Loss :  1.568740963935852 2.8633716106414795 15.885599136352539
Loss :  1.5921920537948608 2.3660097122192383 13.422240257263184
Loss :  1.610877513885498 2.3343663215637207 13.282709121704102
Loss :  1.6465762853622437 2.243089437484741 12.86202335357666
Loss :  1.5961772203445435 2.801319122314453 15.60277271270752
Loss :  1.6085262298583984 2.0931026935577393 12.074039459228516
Loss :  1.601991057395935 1.9435182809829712 11.319581985473633
Loss :  1.5396831035614014 3.230404853820801 17.69170570373535
Loss :  1.6434152126312256 2.4657325744628906 13.972078323364258
Loss :  1.5413655042648315 2.5177314281463623 14.130023002624512
Loss :  1.624147891998291 2.4468345642089844 13.858320236206055
Loss :  1.5952308177947998 2.3008272647857666 13.099367141723633
Loss :  1.5925037860870361 3.3833155632019043 18.509082794189453
Loss :  1.5510982275009155 2.2951223850250244 13.02670955657959
Loss :  1.567737102508545 2.0489325523376465 11.812400817871094
Loss :  1.5656758546829224 2.3186683654785156 13.159017562866211
Loss :  1.6394383907318115 2.865872859954834 15.968803405761719
Loss :  1.6456612348556519 3.4803738594055176 19.047529220581055
Loss :  1.657615065574646 2.1396491527557373 12.355860710144043
  batch 40 loss: 1.657615065574646, 2.1396491527557373, 12.355860710144043
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.6083909273147583 2.4397494792938232 13.807138442993164
Loss :  1.5916327238082886 3.5144262313842773 19.16376495361328
Loss :  1.5795185565948486 2.5774967670440674 14.467001914978027
Loss :  1.595193862915039 2.04714298248291 11.83090877532959
Loss :  1.5685302019119263 1.9934226274490356 11.535643577575684
Loss :  1.6063541173934937 2.6137423515319824 14.675065994262695
Loss :  1.6472262144088745 1.8393287658691406 10.843870162963867
Loss :  1.5890512466430664 2.5677270889282227 14.42768669128418
Loss :  1.6647381782531738 2.414311408996582 13.736295700073242
Loss :  1.595742106437683 2.3778507709503174 13.48499584197998
Loss :  1.6368637084960938 2.6242268085479736 14.757997512817383
Loss :  1.6286954879760742 2.391383647918701 13.585614204406738
Loss :  1.6035412549972534 2.1426568031311035 12.316824913024902
Loss :  1.637292504310608 2.176826000213623 12.52142333984375
Loss :  1.5877902507781982 2.725632667541504 15.215953826904297
Loss :  1.665238618850708 2.488661527633667 14.108546257019043
Loss :  1.5970698595046997 2.2608258724212646 12.901199340820312
Loss :  1.5769388675689697 3.5938761234283447 19.54631996154785
Loss :  1.5952436923980713 2.332968235015869 13.26008415222168
Loss :  1.672005534172058 2.014754056930542 11.745776176452637
  batch 60 loss: 1.672005534172058, 2.014754056930542, 11.745776176452637
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.5771037340164185 2.326235055923462 13.20827865600586
Loss :  1.61164128780365 2.0089104175567627 11.656192779541016
Loss :  1.587765097618103 1.9123876094818115 11.149703025817871
Loss :  1.5737886428833008 2.743046998977661 15.289023399353027
Loss :  1.5497922897338867 2.052564859390259 11.812616348266602
Loss :  1.59250807762146 4.282136917114258 23.003192901611328
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.6056411266326904 4.282528400421143 23.018281936645508
Loss :  1.6009995937347412 4.246147632598877 22.831737518310547
Loss :  1.6160305738449097 4.210040092468262 22.666231155395508
Total LOSS train 14.291321167579063 valid 22.879860877990723
CE LOSS train 1.6054957756629358 valid 0.4040076434612274
Contrastive LOSS train 2.5371650805840127 valid 1.0525100231170654
EPOCH 164:
Loss :  1.629754900932312 2.832285165786743 15.791180610656738
Loss :  1.645492672920227 2.5303955078125 14.297470092773438
Loss :  1.6083210706710815 2.0805768966674805 12.011205673217773
Loss :  1.614275574684143 2.43652081489563 13.796879768371582
Loss :  1.6366627216339111 2.2032394409179688 12.652859687805176
Loss :  1.5934170484542847 2.289592742919922 13.041380882263184
Loss :  1.6357556581497192 2.7893824577331543 15.58266830444336
Loss :  1.6055359840393066 2.376220941543579 13.486640930175781
Loss :  1.5945875644683838 3.2571358680725098 17.880266189575195
Loss :  1.6372820138931274 2.6836774349212646 15.055669784545898
Loss :  1.5846562385559082 2.5890486240386963 14.529899597167969
Loss :  1.5840353965759277 2.6240394115448 14.704233169555664
Loss :  1.5811537504196167 2.457228660583496 13.867297172546387
Loss :  1.5881234407424927 2.6459884643554688 14.818065643310547
Loss :  1.6546673774719238 2.437281608581543 13.841075897216797
Loss :  1.6489284038543701 2.4975907802581787 14.136882781982422
Loss :  1.5785112380981445 3.2837953567504883 17.997486114501953
Loss :  1.61037015914917 2.2690253257751465 12.955497741699219
Loss :  1.5734481811523438 2.199223756790161 12.56956672668457
Loss :  1.647619366645813 2.4023351669311523 13.659295082092285
  batch 20 loss: 1.647619366645813, 2.4023351669311523, 13.659295082092285
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.604333519935608 2.282773494720459 13.01820182800293
Loss :  1.5728065967559814 2.8657262325286865 15.901437759399414
Loss :  1.5961414575576782 2.043865203857422 11.815467834472656
Loss :  1.6146520376205444 2.300152540206909 13.1154146194458
Loss :  1.6500155925750732 3.221463918685913 17.757333755493164
Loss :  1.6001732349395752 2.1254007816314697 12.227176666259766
Loss :  1.6116926670074463 2.233795404434204 12.780669212341309
Loss :  1.605122685432434 3.2279088497161865 17.744667053222656
Loss :  1.5447765588760376 3.164076328277588 17.365158081054688
Loss :  1.6445720195770264 3.34028959274292 18.346019744873047
Loss :  1.545218825340271 3.1077680587768555 17.08405876159668
Loss :  1.6250032186508179 2.9408669471740723 16.32933807373047
Loss :  1.596224069595337 2.2035365104675293 12.613906860351562
Loss :  1.5936121940612793 2.254150867462158 12.86436653137207
Loss :  1.5523877143859863 2.7004895210266113 15.054834365844727
Loss :  1.570162296295166 2.574803590774536 14.44417953491211
Loss :  1.5689194202423096 2.8144423961639404 15.641131401062012
Loss :  1.6412818431854248 2.201930284500122 12.650933265686035
Loss :  1.6478588581085205 2.200587034225464 12.65079402923584
Loss :  1.6599034070968628 2.858513116836548 15.952468872070312
  batch 40 loss: 1.6599034070968628, 2.858513116836548, 15.952468872070312
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.6114190816879272 2.800809383392334 15.615466117858887
Loss :  1.5948665142059326 2.0176854133605957 11.683293342590332
Loss :  1.5825419425964355 2.301121711730957 13.088150024414062
Loss :  1.5979673862457275 2.1662421226501465 12.429178237915039
Loss :  1.57169508934021 2.2550437450408936 12.846914291381836
Loss :  1.6074192523956299 2.086548328399658 12.040160179138184
Loss :  1.6475932598114014 2.34714412689209 13.38331413269043
Loss :  1.590444564819336 2.5776357650756836 14.478623390197754
Loss :  1.6649423837661743 2.0792272090911865 12.061079025268555
Loss :  1.5980768203735352 1.7471436262130737 10.333794593811035
Loss :  1.637621283531189 2.253774404525757 12.906493186950684
Loss :  1.6307322978973389 2.486050844192505 14.060986518859863
Loss :  1.605936884880066 2.6023013591766357 14.617444038391113
Loss :  1.638909935951233 2.5700647830963135 14.48923397064209
Loss :  1.5912127494812012 3.070098638534546 16.94170570373535
Loss :  1.6655104160308838 2.2649967670440674 12.990493774414062
Loss :  1.5993813276290894 2.029247522354126 11.74561882019043
Loss :  1.5803183317184448 1.9512418508529663 11.336527824401855
Loss :  1.5987077951431274 2.533952474594116 14.268470764160156
Loss :  1.6739451885223389 2.0329530239105225 11.83871078491211
  batch 60 loss: 1.6739451885223389, 2.0329530239105225, 11.83871078491211
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.5813772678375244 2.1451539993286133 12.307147026062012
Loss :  1.6158910989761353 2.137611150741577 12.303947448730469
Loss :  1.5924718379974365 2.758004665374756 15.382495880126953
Loss :  1.5783696174621582 2.395005464553833 13.553396224975586
Loss :  1.5545909404754639 2.4365243911743164 13.737213134765625
Loss :  1.591406226158142 3.7533469200134277 20.35814094543457
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 5], device='cuda:0')
Loss :  1.6060553789138794 3.5838215351104736 19.525163650512695
Loss :  1.601105809211731 3.4747607707977295 18.974910736083984
Loss :  1.6122065782546997 3.4699933528900146 18.962173461914062
Total LOSS train 14.038045208270733 valid 19.455097198486328
CE LOSS train 1.6078369269004236 valid 0.4030516445636749
Contrastive LOSS train 2.486041659575242 valid 0.8674983382225037
EPOCH 165:
Loss :  1.6320981979370117 2.180337429046631 12.533785820007324
Loss :  1.6482514142990112 2.1385884284973145 12.341193199157715
Loss :  1.6109265089035034 2.505324125289917 14.137547492980957
Loss :  1.617184042930603 2.5224528312683105 14.229448318481445
Loss :  1.6380094289779663 2.0762827396392822 12.01942253112793
Loss :  1.5959250926971436 2.0201661586761475 11.696756362915039
Loss :  1.6375926733016968 2.402804136276245 13.651613235473633
Loss :  1.6085262298583984 2.1472408771514893 12.344730377197266
Loss :  1.5977431535720825 1.7703964710235596 10.449725151062012
Loss :  1.6393649578094482 2.4746744632720947 14.012737274169922
Loss :  1.587526798248291 2.6797561645507812 14.986307144165039
Loss :  1.5870128870010376 2.5586578845977783 14.380302429199219
Loss :  1.5849133729934692 2.425586223602295 13.712844848632812
Loss :  1.591567039489746 2.7699763774871826 15.441449165344238
Loss :  1.6567643880844116 2.825265645980835 15.783092498779297
Loss :  1.6526687145233154 2.692234754562378 15.113842964172363
Loss :  1.5831644535064697 2.955688238143921 16.36160659790039
Loss :  1.6140141487121582 2.749690294265747 15.362464904785156
Loss :  1.5787346363067627 2.2304954528808594 12.73121166229248
Loss :  1.6509443521499634 2.4209558963775635 13.75572395324707
  batch 20 loss: 1.6509443521499634, 2.4209558963775635, 13.75572395324707
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.6093796491622925 1.8591505289077759 10.905132293701172
Loss :  1.5786815881729126 2.748173713684082 15.319550514221191
Loss :  1.6010879278182983 3.133857250213623 17.270374298095703
Loss :  1.6185357570648193 2.2240309715270996 12.738691329956055
Loss :  1.6524349451065063 2.559941530227661 14.452142715454102
Loss :  1.6029226779937744 2.522165536880493 14.213749885559082
Loss :  1.6145886182785034 2.674175500869751 14.985466003417969
Loss :  1.6077927350997925 2.461480140686035 13.915193557739258
Loss :  1.5478733777999878 3.7756330966949463 20.42603874206543
Loss :  1.648210048675537 2.408294200897217 13.689682006835938
Loss :  1.5502437353134155 2.4596428871154785 13.848457336425781
Loss :  1.6295870542526245 2.1322522163391113 12.290847778320312
Loss :  1.6019175052642822 2.1484687328338623 12.344261169433594
Loss :  1.5995897054672241 2.3200840950012207 13.2000093460083
Loss :  1.5590994358062744 2.5260586738586426 14.18939208984375
Loss :  1.5749887228012085 2.3361196517944336 13.255586624145508
Loss :  1.5728257894515991 2.1096272468566895 12.12096118927002
Loss :  1.6431686878204346 2.5534393787384033 14.410365104675293
Loss :  1.6493399143218994 3.041987419128418 16.859277725219727
Loss :  1.660766363143921 2.2761762142181396 13.041647911071777
  batch 40 loss: 1.660766363143921, 2.2761762142181396, 13.041647911071777
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.6116052865982056 2.2505455017089844 12.864333152770996
Loss :  1.5949451923370361 3.235398530960083 17.77193832397461
Loss :  1.5825421810150146 2.5875637531280518 14.520360946655273
Loss :  1.5975998640060425 2.430492877960205 13.7500638961792
Loss :  1.5726364850997925 2.715075731277466 15.148015022277832
Loss :  1.6078095436096191 2.2585203647613525 12.900411605834961
Loss :  1.6472179889678955 2.262733221054077 12.960884094238281
Loss :  1.5908489227294922 2.124098777770996 12.211342811584473
Loss :  1.6656948328018188 2.089073896408081 12.111064910888672
Loss :  1.5987327098846436 2.1417393684387207 12.307429313659668
Loss :  1.6373672485351562 2.604381799697876 14.659276008605957
Loss :  1.6311246156692505 2.8507707118988037 15.884978294372559
Loss :  1.6067131757736206 2.4380760192871094 13.797093391418457
Loss :  1.6397294998168945 2.6188745498657227 14.734102249145508
Loss :  1.5923126935958862 2.5701189041137695 14.442907333374023
Loss :  1.6675993204116821 2.417192220687866 13.753561019897461
Loss :  1.6014021635055542 2.4876134395599365 14.039469718933105
Loss :  1.5820192098617554 2.5016353130340576 14.090195655822754
Loss :  1.6011050939559937 3.463468313217163 18.918445587158203
Loss :  1.6757922172546387 2.245382070541382 12.902702331542969
  batch 60 loss: 1.6757922172546387, 2.245382070541382, 12.902702331542969
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.5837422609329224 2.490553855895996 14.036511421203613
Loss :  1.618302345275879 1.9828990697860718 11.532797813415527
Loss :  1.5953537225723267 3.5599615573883057 19.39516258239746
Loss :  1.582577109336853 2.114210844039917 12.153631210327148
Loss :  1.5603803396224976 2.5872716903686523 14.49673843383789
Loss :  1.5997689962387085 4.1339921951293945 22.269729614257812
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 5], device='cuda:0')
Loss :  1.6139448881149292 4.133614540100098 22.28201675415039
Loss :  1.613476276397705 3.9635679721832275 21.431316375732422
Loss :  1.6120589971542358 3.997995615005493 21.60203742980957
Total LOSS train 14.060093087416428 valid 21.89627504348755
CE LOSS train 1.6104787808198195 valid 0.40301474928855896
Contrastive LOSS train 2.489922860952524 valid 0.9994989037513733
EPOCH 166:
Loss :  1.6359484195709229 2.0784382820129395 12.028139114379883
Loss :  1.6515061855316162 2.5790984630584717 14.546998977661133
Loss :  1.6157077550888062 1.9381896257400513 11.306655883789062
Loss :  1.6217659711837769 2.2167162895202637 12.705348014831543
Loss :  1.6430591344833374 2.0796453952789307 12.041285514831543
Loss :  1.6013026237487793 2.6944146156311035 15.073375701904297
Loss :  1.6414989233016968 2.41967511177063 13.739873886108398
Loss :  1.611995816230774 2.2728588581085205 12.976289749145508
Loss :  1.6014072895050049 2.4182145595550537 13.692480087280273
Loss :  1.6401724815368652 2.47416353225708 14.010990142822266
Loss :  1.5899100303649902 2.677259922027588 14.97620964050293
Loss :  1.5891175270080566 2.5109832286834717 14.144033432006836
Loss :  1.5857045650482178 2.6932122707366943 15.051766395568848
Loss :  1.592566967010498 2.9545435905456543 16.365285873413086
Loss :  1.6573387384414673 2.7407848834991455 15.361263275146484
Loss :  1.6529046297073364 2.590956687927246 14.607687950134277
Loss :  1.5833399295806885 2.3206088542938232 13.186384201049805
Loss :  1.6153929233551025 2.2401652336120605 12.816219329833984
Loss :  1.5796564817428589 1.9389662742614746 11.27448844909668
Loss :  1.652209997177124 2.076099395751953 12.032707214355469
  batch 20 loss: 1.652209997177124, 2.076099395751953, 12.032707214355469
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.609034538269043 2.10380482673645 12.128058433532715
Loss :  1.5791670083999634 2.6250624656677246 14.704480171203613
Loss :  1.600027322769165 2.078381299972534 11.991933822631836
Loss :  1.6183335781097412 2.212146759033203 12.679067611694336
Loss :  1.6521971225738525 2.3678829669952393 13.49161148071289
Loss :  1.6031116247177124 2.5146663188934326 14.176443099975586
Loss :  1.6151193380355835 2.3945419788360596 13.587828636169434
Loss :  1.607612133026123 2.7216367721557617 15.215795516967773
Loss :  1.5485864877700806 2.5798041820526123 14.44760799407959
Loss :  1.6486238241195679 2.2386631965637207 12.841938972473145
Loss :  1.5500046014785767 2.7884714603424072 15.492362022399902
Loss :  1.628950834274292 2.417078971862793 13.714345932006836
Loss :  1.600572109222412 2.5779635906219482 14.49039077758789
Loss :  1.5978732109069824 2.614915370941162 14.67245101928711
Loss :  1.5572909116744995 2.7945752143859863 15.530166625976562
Loss :  1.5736504793167114 1.9308816194534302 11.228058815002441
Loss :  1.571537971496582 1.961188554763794 11.377480506896973
Loss :  1.6430281400680542 2.2046821117401123 12.666439056396484
Loss :  1.6491484642028809 2.1030166149139404 12.16423225402832
Loss :  1.661294937133789 2.0309040546417236 11.815814971923828
  batch 40 loss: 1.661294937133789, 2.0309040546417236, 11.815814971923828
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.6137888431549072 2.1356189250946045 12.29188346862793
Loss :  1.598017930984497 1.6464909315109253 9.830471992492676
Loss :  1.5862505435943604 2.4748802185058594 13.960651397705078
Loss :  1.602116584777832 2.269777774810791 12.951004981994629
Loss :  1.57734215259552 2.4418327808380127 13.786505699157715
Loss :  1.6136060953140259 2.3088457584381104 13.157835006713867
Loss :  1.65299654006958 2.099910259246826 12.152547836303711
Loss :  1.5962107181549072 2.0171446800231934 11.681934356689453
Loss :  1.6697479486465454 2.5226385593414307 14.282940864562988
Loss :  1.602691411972046 2.8157827854156494 15.681605339050293
Loss :  1.64272940158844 2.635901927947998 14.822239875793457
Loss :  1.6348241567611694 3.342249870300293 18.3460750579834
Loss :  1.6106945276260376 2.4650726318359375 13.936058044433594
Loss :  1.6424388885498047 3.4191229343414307 18.738054275512695
Loss :  1.5978411436080933 2.45666241645813 13.881153106689453
Loss :  1.6691054105758667 3.2349867820739746 17.844039916992188
Loss :  1.6048686504364014 2.5184555053710938 14.19714641571045
Loss :  1.5855780839920044 2.4735794067382812 13.953474998474121
Loss :  1.6032333374023438 2.788865804672241 15.547562599182129
Loss :  1.6771459579467773 2.226884365081787 12.811568260192871
  batch 60 loss: 1.6771459579467773, 2.226884365081787, 12.811568260192871
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.5862298011779785 2.592418909072876 14.548324584960938
Loss :  1.61991548538208 2.194443702697754 12.592134475708008
Loss :  1.596341848373413 2.484410047531128 14.018392562866211
Loss :  1.5822676420211792 2.7418622970581055 15.291579246520996
Loss :  1.5581772327423096 1.9104188680648804 11.110271453857422
Loss :  1.6040735244750977 4.257922649383545 22.893688201904297
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.616869568824768 4.213257312774658 22.683155059814453
Loss :  1.6131558418273926 4.066613674163818 21.946224212646484
Loss :  1.6293816566467285 4.179345607757568 22.52610969543457
Total LOSS train 13.719529944199783 valid 22.51229429244995
CE LOSS train 1.6123358671481793 valid 0.40734541416168213
Contrastive LOSS train 2.421438793035654 valid 1.044836401939392
EPOCH 167:
Loss :  1.634927749633789 2.449305534362793 13.881455421447754
Loss :  1.65053129196167 2.3485710620880127 13.393386840820312
Loss :  1.614479899406433 1.973983645439148 11.484397888183594
Loss :  1.6209300756454468 2.5931663513183594 14.586761474609375
Loss :  1.6422231197357178 2.596072196960449 14.622584342956543
Loss :  1.6011457443237305 2.677374839782715 14.988019943237305
Loss :  1.6425790786743164 2.8367886543273926 15.826521873474121
Loss :  1.6144037246704102 1.9981015920639038 11.604911804199219
Loss :  1.603165626525879 2.3101727962493896 13.154029846191406
Loss :  1.6432222127914429 1.9757128953933716 11.5217866897583
Loss :  1.5930448770523071 2.1426808834075928 12.306449890136719
Loss :  1.5922836065292358 2.1420469284057617 12.302517890930176
Loss :  1.5897505283355713 3.5567851066589355 19.373676300048828
Loss :  1.596242070198059 2.179680824279785 12.494646072387695
Loss :  1.6604127883911133 2.437497138977051 13.847898483276367
Loss :  1.6545612812042236 2.3543736934661865 13.426429748535156
Loss :  1.5854089260101318 2.096992254257202 12.0703706741333
Loss :  1.615445613861084 2.7822256088256836 15.526573181152344
Loss :  1.5803401470184326 2.3652167320251465 13.406424522399902
Loss :  1.6521309614181519 2.2812774181365967 13.058518409729004
  batch 20 loss: 1.6521309614181519, 2.2812774181365967, 13.058518409729004
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.6097018718719482 3.020026683807373 16.709835052490234
Loss :  1.5796160697937012 3.311068534851074 18.134960174560547
Loss :  1.6002897024154663 2.2098093032836914 12.649335861206055
Loss :  1.6186245679855347 2.433920383453369 13.788226127624512
Loss :  1.6536201238632202 3.317739725112915 18.242319107055664
Loss :  1.603342890739441 2.8888564109802246 16.047624588012695
Loss :  1.6155543327331543 2.5194640159606934 14.212875366210938
Loss :  1.608878254890442 2.208408832550049 12.650921821594238
Loss :  1.551145076751709 2.776984930038452 15.43606948852539
Loss :  1.649174690246582 2.7793450355529785 15.545899391174316
Loss :  1.5529496669769287 2.6789231300354004 14.947565078735352
Loss :  1.631495475769043 2.532160758972168 14.292299270629883
Loss :  1.6034252643585205 2.4842641353607178 14.02474594116211
Loss :  1.6007064580917358 2.43979811668396 13.799696922302246
Loss :  1.561292052268982 2.44333815574646 13.777982711791992
Loss :  1.5789213180541992 2.845715045928955 15.807496070861816
Loss :  1.5775725841522217 2.841377019882202 15.78445816040039
Loss :  1.6478441953659058 2.4806199073791504 14.050943374633789
Loss :  1.6538269519805908 2.115673542022705 12.232193946838379
Loss :  1.665615439414978 1.968955636024475 11.510393142700195
  batch 40 loss: 1.665615439414978, 1.968955636024475, 11.510393142700195
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.6187694072723389 1.9979439973831177 11.608489990234375
Loss :  1.6024982929229736 2.6292455196380615 14.748725891113281
Loss :  1.5912835597991943 2.6862566471099854 15.022566795349121
Loss :  1.6041553020477295 2.7394113540649414 15.301212310791016
Loss :  1.5788896083831787 2.0487332344055176 11.822555541992188
Loss :  1.6140416860580444 2.0844945907592773 12.036514282226562
Loss :  1.6531037092208862 2.486504316329956 14.085625648498535
Loss :  1.5960772037506104 2.549734354019165 14.344748497009277
Loss :  1.6676244735717773 2.9608993530273438 16.472122192382812
Loss :  1.60288667678833 2.3768441677093506 13.48710823059082
Loss :  1.6418962478637695 2.955573320388794 16.419761657714844
Loss :  1.6339973211288452 3.1033284664154053 17.1506404876709
Loss :  1.6103440523147583 2.4458436965942383 13.83956241607666
Loss :  1.6432174444198608 2.5144331455230713 14.21538257598877
Loss :  1.5972646474838257 3.489161968231201 19.043075561523438
Loss :  1.6684964895248413 2.4531772136688232 13.934382438659668
Loss :  1.6051534414291382 3.825841188430786 20.734359741210938
Loss :  1.5861245393753052 2.3300280570983887 13.236265182495117
Loss :  1.6042014360427856 3.588738203048706 19.547893524169922
Loss :  1.6769297122955322 1.9310061931610107 11.331960678100586
  batch 60 loss: 1.6769297122955322, 1.9310061931610107, 11.331960678100586
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.5869410037994385 2.443282127380371 13.803351402282715
Loss :  1.620705008506775 2.722355842590332 15.232483863830566
Loss :  1.5981448888778687 2.4011025428771973 13.603656768798828
Loss :  1.5855047702789307 2.8328697681427 15.749853134155273
Loss :  1.5621085166931152 2.181462287902832 12.469419479370117
Loss :  1.5929173231124878 4.213396072387695 22.659896850585938
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 4, 5], device='cuda:0')
Loss :  1.6061440706253052 4.204977512359619 22.631031036376953
Loss :  1.6016252040863037 4.089524745941162 22.04924964904785
Loss :  1.6140731573104858 4.009789943695068 21.663022994995117
Total LOSS train 14.396383402897762 valid 22.250800132751465
CE LOSS train 1.6138643961686354 valid 0.40351828932762146
Contrastive LOSS train 2.556503800245432 valid 1.002447485923767
EPOCH 168:
Loss :  1.6367509365081787 2.0313854217529297 11.793678283691406
Loss :  1.6521127223968506 2.6048977375030518 14.67660140991211
Loss :  1.617301106452942 2.889202356338501 16.063312530517578
Loss :  1.623296856880188 2.2082552909851074 12.664573669433594
Loss :  1.6444261074066162 2.828946352005005 15.78915786743164
Loss :  1.6029095649719238 2.0697221755981445 11.951520919799805
Loss :  1.6444754600524902 3.3282558917999268 18.285755157470703
Loss :  1.615567922592163 3.6653871536254883 19.942502975463867
Loss :  1.6047049760818481 2.232612371444702 12.767766952514648
Loss :  1.644174337387085 2.658254384994507 14.935446739196777
Loss :  1.5942540168762207 2.86726713180542 15.93058967590332
Loss :  1.5934243202209473 3.744572877883911 20.316287994384766
Loss :  1.5912232398986816 2.4817488193511963 13.999967575073242
Loss :  1.5975559949874878 2.5624570846557617 14.409841537475586
Loss :  1.660959243774414 2.5444138050079346 14.383028030395508
Loss :  1.6563583612442017 2.8578171730041504 15.945444107055664
Loss :  1.5882911682128906 3.5019896030426025 19.09823989868164
Loss :  1.6195120811462402 2.7781670093536377 15.510347366333008
Loss :  1.584912657737732 2.514418840408325 14.15700626373291
Loss :  1.6563814878463745 1.958047866821289 11.44662094116211
  batch 20 loss: 1.6563814878463745, 1.958047866821289, 11.44662094116211
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.614526629447937 2.037229061126709 11.80067253112793
Loss :  1.584710955619812 2.990766763687134 16.538543701171875
Loss :  1.6052188873291016 2.6123650074005127 14.667043685913086
Loss :  1.6223031282424927 2.9483556747436523 16.36408233642578
Loss :  1.655356764793396 3.865447759628296 20.982595443725586
Loss :  1.6074409484863281 2.270524501800537 12.960063934326172
Loss :  1.6188383102416992 2.4066946506500244 13.652311325073242
Loss :  1.611581563949585 3.0457534790039062 16.840349197387695
Loss :  1.5535356998443604 2.4608917236328125 13.857994079589844
Loss :  1.6496562957763672 2.611821413040161 14.708763122558594
Loss :  1.5548619031906128 2.6715309619903564 14.912516593933105
Loss :  1.6319974660873413 2.4791274070739746 14.027634620666504
Loss :  1.6044716835021973 2.4684829711914062 13.94688606262207
Loss :  1.6023666858673096 2.4838719367980957 14.02172565460205
Loss :  1.5634620189666748 2.696392774581909 15.045426368713379
Loss :  1.5800856351852417 2.7140305042266846 15.150238037109375
Loss :  1.5793660879135132 3.4086766242980957 18.62274932861328
Loss :  1.6482676267623901 2.064978837966919 11.973161697387695
Loss :  1.6541035175323486 2.0021183490753174 11.664694786071777
Loss :  1.6653990745544434 2.158740997314453 12.459104537963867
  batch 40 loss: 1.6653990745544434, 2.158740997314453, 12.459104537963867
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.6192371845245361 2.172623872756958 12.482356071472168
Loss :  1.603448748588562 2.021682024002075 11.711858749389648
Loss :  1.592045545578003 2.172370433807373 12.453898429870605
Loss :  1.6059318780899048 2.2435085773468018 12.823474884033203
Loss :  1.5811131000518799 3.365079402923584 18.406511306762695
Loss :  1.6156476736068726 3.4047353267669678 18.639324188232422
Loss :  1.6539617776870728 2.0262129306793213 11.785026550292969
Loss :  1.5978116989135742 1.842094898223877 10.8082857131958
Loss :  1.6683851480484009 2.4875552654266357 14.106162071228027
Loss :  1.604480266571045 3.2227606773376465 17.718284606933594
Loss :  1.6428823471069336 2.094050407409668 12.113134384155273
Loss :  1.6362545490264893 2.3140103816986084 13.206306457519531
Loss :  1.612404227256775 3.369425058364868 18.459529876708984
Loss :  1.6441683769226074 2.3997323513031006 13.642829895019531
Loss :  1.5996403694152832 2.4655752182006836 13.92751693725586
Loss :  1.671074390411377 2.6437454223632812 14.889801025390625
Loss :  1.6078815460205078 3.8555588722229004 20.88567543029785
Loss :  1.5893930196762085 2.281226873397827 12.995527267456055
Loss :  1.6070382595062256 2.6585259437561035 14.899667739868164
Loss :  1.6796287298202515 3.2360711097717285 17.859983444213867
  batch 60 loss: 1.6796287298202515, 3.2360711097717285, 17.859983444213867
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.5907208919525146 2.4774281978607178 13.977862358093262
Loss :  1.6236193180084229 1.9079221487045288 11.163229942321777
Loss :  1.6010969877243042 1.731339693069458 10.257795333862305
Loss :  1.587783694267273 2.164288282394409 12.409225463867188
Loss :  1.5653743743896484 3.1773040294647217 17.451894760131836
Loss :  1.5804195404052734 3.79052734375 20.533056259155273
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.5939891338348389 3.759782075881958 20.392898559570312
Loss :  1.5930192470550537 3.737346887588501 20.279754638671875
Loss :  1.5942003726959229 3.563004493713379 19.409221649169922
Total LOSS train 14.728298612741323 valid 20.153732776641846
CE LOSS train 1.6159564238328201 valid 0.3985500931739807
Contrastive LOSS train 2.622468433013329 valid 0.8907511234283447
EPOCH 169:
Loss :  1.6391512155532837 3.5139846801757812 19.209074020385742
Loss :  1.654623031616211 2.166996955871582 12.489607810974121
Loss :  1.6193243265151978 1.7054572105407715 10.146611213684082
Loss :  1.6256121397018433 2.1038894653320312 12.145059585571289
Loss :  1.6461563110351562 1.9546107053756714 11.419209480285645
Loss :  1.6055314540863037 1.8437778949737549 10.824420928955078
Loss :  1.645646572113037 2.8307390213012695 15.799341201782227
Loss :  1.617170810699463 2.5207889080047607 14.221115112304688
Loss :  1.6061267852783203 2.037585973739624 11.79405689239502
Loss :  1.6445207595825195 2.3766016960144043 13.5275297164917
Loss :  1.595085620880127 2.380889415740967 13.499532699584961
Loss :  1.5941962003707886 2.675560712814331 14.972000122070312
Loss :  1.5918835401535034 2.6542608737945557 14.863187789916992
Loss :  1.598687767982483 2.8222708702087402 15.710042953491211
Loss :  1.6615660190582275 2.955630302429199 16.43971824645996
Loss :  1.656803011894226 2.579127311706543 14.55243968963623
Loss :  1.59050452709198 2.8892953395843506 16.0369815826416
Loss :  1.6198348999023438 2.2275078296661377 12.757373809814453
Loss :  1.5852042436599731 1.701442837715149 10.092418670654297
Loss :  1.654842734336853 2.5135269165039062 14.222476959228516
  batch 20 loss: 1.654842734336853, 2.5135269165039062, 14.222476959228516
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.6124517917633057 2.0468695163726807 11.84679889678955
Loss :  1.5830236673355103 3.5374181270599365 19.270112991333008
Loss :  1.603875994682312 2.4233522415161133 13.720637321472168
Loss :  1.6219688653945923 2.38189959526062 13.53146743774414
Loss :  1.6548129320144653 2.69321346282959 15.120880126953125
Loss :  1.607401967048645 2.6339449882507324 14.777127265930176
Loss :  1.6182911396026611 2.790174722671509 15.569164276123047
Loss :  1.611249327659607 2.37479829788208 13.48523998260498
Loss :  1.5526998043060303 2.6236257553100586 14.670828819274902
Loss :  1.6502891778945923 2.1930432319641113 12.61550521850586
Loss :  1.5546207427978516 3.2091171741485596 17.60020637512207
Loss :  1.6321991682052612 2.5070974826812744 14.167686462402344
Loss :  1.6054061651229858 2.0001273155212402 11.606042861938477
Loss :  1.603489875793457 2.3506994247436523 13.356986999511719
Loss :  1.5651509761810303 2.3665974140167236 13.398138046264648
Loss :  1.5819323062896729 2.4337916374206543 13.750890731811523
Loss :  1.5806009769439697 4.3184895515441895 23.17304801940918
Loss :  1.6482967138290405 2.5323445796966553 14.310019493103027
Loss :  1.6545754671096802 2.273184299468994 13.020496368408203
Loss :  1.6657246351242065 2.488142728805542 14.106438636779785
  batch 40 loss: 1.6657246351242065, 2.488142728805542, 14.106438636779785
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.6200133562088013 2.482743501663208 14.033730506896973
Loss :  1.604361653327942 2.580890655517578 14.508814811706543
Loss :  1.5932347774505615 2.325012445449829 13.218297004699707
Loss :  1.606440544128418 2.214707136154175 12.679976463317871
Loss :  1.5809426307678223 2.110262155532837 12.132253646850586
Loss :  1.6149685382843018 2.6441538333892822 14.835737228393555
Loss :  1.6530108451843262 2.6246399879455566 14.77621078491211
Loss :  1.5976675748825073 2.6878204345703125 15.03676986694336
Loss :  1.6689566373825073 2.805208444595337 15.694998741149902
Loss :  1.604502558708191 2.078354835510254 11.99627685546875
Loss :  1.6423503160476685 2.0937917232513428 12.111309051513672
Loss :  1.6351876258850098 2.343430995941162 13.35234260559082
Loss :  1.6116509437561035 2.6689741611480713 14.956521987915039
Loss :  1.6438672542572021 2.661639928817749 14.952067375183105
Loss :  1.598112940788269 3.1323721408843994 17.259973526000977
Loss :  1.6702507734298706 2.178379774093628 12.562150001525879
Loss :  1.6065610647201538 2.5718297958374023 14.465709686279297
Loss :  1.5879466533660889 2.1328659057617188 12.252276420593262
Loss :  1.60587477684021 2.227806806564331 12.744909286499023
Loss :  1.6783487796783447 3.7451364994049072 20.40403175354004
  batch 60 loss: 1.6783487796783447, 3.7451364994049072, 20.40403175354004
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.5893667936325073 3.7425456047058105 20.302095413208008
Loss :  1.6226670742034912 2.2744803428649902 12.99506950378418
Loss :  1.6006170511245728 2.254040002822876 12.870817184448242
Loss :  1.5865864753723145 2.3046090602874756 13.10963249206543
Loss :  1.563770055770874 1.7595272064208984 10.361406326293945
Loss :  1.57895028591156 3.3561408519744873 18.359655380249023
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.5921458005905151 3.1127090454101562 17.155691146850586
Loss :  1.5894755125045776 3.1523892879486084 17.351421356201172
Loss :  1.5964140892028809 2.9690604209899902 16.44171714782715
Total LOSS train 14.175896820655236 valid 17.327121257781982
CE LOSS train 1.6161198671047503 valid 0.3991035223007202
Contrastive LOSS train 2.5119553822737473 valid 0.7422651052474976
Saved best model. Old loss 18.167856693267822 and new best loss 17.327121257781982
EPOCH 170:
Loss :  1.6385332345962524 1.9582486152648926 11.429776191711426
Loss :  1.653652310371399 2.069645404815674 12.00187873840332
Loss :  1.6182644367218018 2.0838499069213867 12.037513732910156
Loss :  1.6240650415420532 2.572558641433716 14.486858367919922
Loss :  1.6442279815673828 2.380397319793701 13.546215057373047
Loss :  1.6033918857574463 2.543642520904541 14.321603775024414
Loss :  1.6437623500823975 2.4731199741363525 14.00936222076416
Loss :  1.6158641576766968 2.2431352138519287 12.83154010772705
Loss :  1.6047871112823486 2.5261189937591553 14.235382080078125
Loss :  1.6449230909347534 2.2066805362701416 12.678325653076172
Loss :  1.5959675312042236 2.508483648300171 14.138385772705078
Loss :  1.595910668373108 2.203827142715454 12.615046501159668
Loss :  1.5934348106384277 3.155900239944458 17.372936248779297
Loss :  1.6007282733917236 2.4590067863464355 13.89576244354248
Loss :  1.662589192390442 2.366551637649536 13.495347023010254
Loss :  1.6573642492294312 2.4324913024902344 13.819820404052734
Loss :  1.5912930965423584 2.6387174129486084 14.784880638122559
Loss :  1.6206138134002686 2.6175222396850586 14.70822525024414
Loss :  1.585950493812561 2.056727170944214 11.869586944580078
Loss :  1.6564185619354248 2.510890483856201 14.210871696472168
  batch 20 loss: 1.6564185619354248, 2.510890483856201, 14.210871696472168
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.6144514083862305 2.2758169174194336 12.993535995483398
Loss :  1.5858579874038696 2.5472252368927 14.32198429107666
Loss :  1.607235074043274 2.0070974826812744 11.642722129821777
Loss :  1.6246345043182373 3.4515199661254883 18.882232666015625
Loss :  1.6569371223449707 3.021305561065674 16.763463973999023
Loss :  1.6100740432739258 2.0536611080169678 11.878379821777344
Loss :  1.6208993196487427 2.06371808052063 11.939489364624023
Loss :  1.6139473915100098 1.9928909540176392 11.578401565551758
Loss :  1.5559569597244263 2.1766998767852783 12.43945598602295
Loss :  1.6514586210250854 2.5693016052246094 14.497966766357422
Loss :  1.5569136142730713 2.499155282974243 14.052689552307129
Loss :  1.6331617832183838 2.4719064235687256 13.992693901062012
Loss :  1.605688214302063 2.480050802230835 14.005942344665527
Loss :  1.6031956672668457 2.3343167304992676 13.274778366088867
Loss :  1.5643033981323242 2.5578420162200928 14.353513717651367
Loss :  1.580935001373291 2.355759859085083 13.359733581542969
Loss :  1.5796147584915161 2.356402635574341 13.361627578735352
Loss :  1.648810863494873 2.60756516456604 14.686635971069336
Loss :  1.6551090478897095 2.0249037742614746 11.77962875366211
Loss :  1.6664260625839233 1.9851164817810059 11.592008590698242
  batch 40 loss: 1.6664260625839233, 1.9851164817810059, 11.592008590698242
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.62043297290802 2.3869597911834717 13.555232048034668
Loss :  1.6051008701324463 2.8073818683624268 15.642009735107422
Loss :  1.5938342809677124 2.733633279800415 15.26200008392334
Loss :  1.6083126068115234 2.3884665966033936 13.55064582824707
Loss :  1.5838711261749268 2.608200788497925 14.62487506866455
Loss :  1.618727684020996 3.0239245891571045 16.73834991455078
Loss :  1.6564686298370361 2.8024981021881104 15.66895866394043
Loss :  1.601356029510498 2.167447566986084 12.438594818115234
Loss :  1.6719316244125366 2.391745090484619 13.630656242370605
Loss :  1.608797311782837 2.2119858264923096 12.668725967407227
Loss :  1.6460866928100586 2.3916831016540527 13.60450267791748
Loss :  1.6394529342651367 1.9888346195220947 11.583625793457031
Loss :  1.616581916809082 2.2185919284820557 12.709541320800781
Loss :  1.6477482318878174 2.3228201866149902 13.261849403381348
Loss :  1.6011229753494263 2.101182699203491 12.107036590576172
Loss :  1.6729004383087158 2.4110267162323 13.728034019470215
Loss :  1.6094130277633667 2.357804775238037 13.3984375
Loss :  1.590378999710083 2.3908727169036865 13.544742584228516
Loss :  1.6077884435653687 2.616266965866089 14.689123153686523
Loss :  1.6794261932373047 2.3280584812164307 13.319718360900879
  batch 60 loss: 1.6794261932373047, 2.3280584812164307, 13.319718360900879
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.5921703577041626 2.3289082050323486 13.236711502075195
Loss :  1.6241282224655151 2.2174456119537354 12.711356163024902
Loss :  1.6025553941726685 2.0180087089538574 11.692599296569824
Loss :  1.5889637470245361 2.2469255924224854 12.823591232299805
Loss :  1.5675394535064697 1.6656839847564697 9.89595890045166
Loss :  1.5935088396072388 3.705582618713379 20.121421813964844
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.6056708097457886 3.5839223861694336 19.525283813476562
Loss :  1.6035562753677368 3.5577545166015625 19.3923282623291
Loss :  1.6079882383346558 3.3818278312683105 18.517127990722656
Total LOSS train 13.538047394385705 valid 19.38904047012329
CE LOSS train 1.6175760507583619 valid 0.40199705958366394
Contrastive LOSS train 2.384094291466933 valid 0.8454569578170776
EPOCH 171:
Loss :  1.641473412513733 1.9741190671920776 11.512068748474121
Loss :  1.65714430809021 2.6430375576019287 14.872332572937012
Loss :  1.6223117113113403 2.803982734680176 15.64222526550293
Loss :  1.628291368484497 2.0702905654907227 11.979743957519531
Loss :  1.6481328010559082 1.937379240989685 11.335029602050781
Loss :  1.608200192451477 1.9586182832717896 11.401291847229004
Loss :  1.6468505859375 2.4191153049468994 13.742426872253418
Loss :  1.6193267107009888 2.009638786315918 11.667520523071289
Loss :  1.6079738140106201 2.4077048301696777 13.646498680114746
Loss :  1.6468926668167114 2.1040241718292236 12.167013168334961
Loss :  1.5982716083526611 2.4110329151153564 13.653435707092285
Loss :  1.597036600112915 2.31557297706604 13.174901008605957
Loss :  1.5946533679962158 2.5227880477905273 14.208593368530273
Loss :  1.600203037261963 2.2601990699768066 12.90119743347168
Loss :  1.6624118089675903 2.827221393585205 15.798518180847168
Loss :  1.6584373712539673 3.05137038230896 16.9152889251709
Loss :  1.593265414237976 2.2709949016571045 12.948240280151367
Loss :  1.6230696439743042 2.0830271244049072 12.03820514678955
Loss :  1.5903501510620117 2.683274745941162 15.00672435760498
Loss :  1.6598094701766968 2.481724500656128 14.068431854248047
  batch 20 loss: 1.6598094701766968, 2.481724500656128, 14.068431854248047
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.6188631057739258 2.374323606491089 13.49048137664795
Loss :  1.5897637605667114 2.5296950340270996 14.238239288330078
Loss :  1.6097158193588257 2.2504892349243164 12.862161636352539
Loss :  1.626309871673584 3.2177512645721436 17.71506690979004
Loss :  1.6588678359985352 2.611241102218628 14.715073585510254
Loss :  1.6121728420257568 3.0478734970092773 16.851539611816406
Loss :  1.6228631734848022 2.7186944484710693 15.21633529663086
Loss :  1.6165046691894531 2.4926419258117676 14.079713821411133
Loss :  1.5602933168411255 1.9124892950057983 11.122739791870117
Loss :  1.6543253660202026 3.068444013595581 16.996545791625977
Loss :  1.561723232269287 2.8132989406585693 15.628217697143555
Loss :  1.6364346742630005 2.706993579864502 15.171401977539062
Loss :  1.6107641458511353 2.841130256652832 15.816415786743164
Loss :  1.6083111763000488 2.863206148147583 15.924341201782227
Loss :  1.5698180198669434 2.552370309829712 14.331668853759766
Loss :  1.5859110355377197 2.5413033962249756 14.292428016662598
Loss :  1.5840822458267212 2.947646141052246 16.32231330871582
Loss :  1.650689721107483 2.131213665008545 12.306758880615234
Loss :  1.6568348407745361 2.171595573425293 12.514812469482422
Loss :  1.6677430868148804 1.805726408958435 10.696374893188477
  batch 40 loss: 1.6677430868148804, 1.805726408958435, 10.696374893188477
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.622628092765808 1.8344601392745972 10.794928550720215
Loss :  1.6077123880386353 2.005537509918213 11.63539981842041
Loss :  1.5967137813568115 3.0314443111419678 16.753934860229492
Loss :  1.6109631061553955 3.402193784713745 18.621932983398438
Loss :  1.587234616279602 2.0075080394744873 11.624774932861328
Loss :  1.6212550401687622 2.3538858890533447 13.390684127807617
Loss :  1.6568881273269653 1.8379043340682983 10.846409797668457
Loss :  1.6036635637283325 2.013735294342041 11.67233943939209
Loss :  1.6728442907333374 2.1392219066619873 12.368953704833984
Loss :  1.610542893409729 1.7246900796890259 10.233993530273438
Loss :  1.6473637819290161 1.8406740427017212 10.850733757019043
Loss :  1.640527367591858 3.2841317653656006 18.061185836791992
Loss :  1.6174837350845337 2.046107530593872 11.848021507263184
Loss :  1.6493866443634033 2.302448034286499 13.161626815795898
Loss :  1.6063438653945923 2.2397003173828125 12.804845809936523
Loss :  1.6747615337371826 2.2068331241607666 12.708927154541016
Loss :  1.6139516830444336 1.9766418933868408 11.497160911560059
Loss :  1.5959718227386475 2.224841833114624 12.720181465148926
Loss :  1.6132827997207642 2.716754198074341 15.197053909301758
Loss :  1.682112693786621 1.7241510152816772 10.302867889404297
  batch 60 loss: 1.682112693786621, 1.7241510152816772, 10.302867889404297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.5975905656814575 1.766744613647461 10.431313514709473
Loss :  1.6297415494918823 1.924373984336853 11.251611709594727
Loss :  1.6080894470214844 2.1807355880737305 12.511767387390137
Loss :  1.5953567028045654 3.3577568531036377 18.384140014648438
Loss :  1.5734740495681763 1.5956265926361084 9.551607131958008
Loss :  1.6106199026107788 4.193946838378906 22.580354690551758
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.6228818893432617 4.226912498474121 22.7574462890625
Loss :  1.6186615228652954 4.071645736694336 21.976890563964844
Loss :  1.6311025619506836 3.9023895263671875 21.143051147460938
Total LOSS train 13.510749450096718 valid 22.11443567276001
CE LOSS train 1.6206458788651688 valid 0.4077756404876709
Contrastive LOSS train 2.3780207248834464 valid 0.9755973815917969
EPOCH 172:
Loss :  1.6444653272628784 2.246628761291504 12.877609252929688
Loss :  1.6592750549316406 2.3525571823120117 13.4220609664917
Loss :  1.6242250204086304 2.0902798175811768 12.075623512268066
Loss :  1.629417896270752 2.254765033721924 12.903242111206055
Loss :  1.648398756980896 2.5306355953216553 14.301576614379883
Loss :  1.6095638275146484 1.8342583179473877 10.780855178833008
Loss :  1.648205041885376 2.067089319229126 11.983651161193848
Loss :  1.6220083236694336 2.4629974365234375 13.936995506286621
Loss :  1.6119580268859863 2.114896297454834 12.186439514160156
Loss :  1.6508573293685913 2.130631685256958 12.304015159606934
Loss :  1.60313081741333 2.225130319595337 12.728782653808594
Loss :  1.602090835571289 2.155888319015503 12.381532669067383
Loss :  1.5998107194900513 2.0439939498901367 11.819780349731445
Loss :  1.6057525873184204 3.2515487670898438 17.863496780395508
Loss :  1.6659563779830933 2.381024122238159 13.571077346801758
Loss :  1.661755919456482 2.6407625675201416 14.865568161010742
Loss :  1.5959457159042358 2.401127338409424 13.601581573486328
Loss :  1.6256370544433594 1.9383952617645264 11.31761360168457
Loss :  1.592490792274475 2.137547731399536 12.280229568481445
Loss :  1.659964680671692 2.5023906230926514 14.171917915344238
  batch 20 loss: 1.659964680671692, 2.5023906230926514, 14.171917915344238
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.620113730430603 2.2266502380371094 12.753364562988281
Loss :  1.5909777879714966 2.238149881362915 12.781726837158203
Loss :  1.6106451749801636 2.360896348953247 13.41512680053711
Loss :  1.6277518272399902 1.997910737991333 11.617305755615234
Loss :  1.660243272781372 2.262195348739624 12.971220016479492
Loss :  1.61398184299469 2.5676138401031494 14.452051162719727
Loss :  1.62461519241333 2.7906992435455322 15.57811164855957
Loss :  1.6183327436447144 2.863032341003418 15.933494567871094
Loss :  1.5628777742385864 2.0404775142669678 11.765265464782715
Loss :  1.6556705236434937 2.8340024948120117 15.825682640075684
Loss :  1.5641111135482788 2.2402608394622803 12.76541519165039
Loss :  1.6379836797714233 2.3806793689727783 13.541379928588867
Loss :  1.612898349761963 2.7966115474700928 15.595956802368164
Loss :  1.6105622053146362 2.357813596725464 13.399630546569824
Loss :  1.5738102197647095 3.1067183017730713 17.10740089416504
Loss :  1.5900427103042603 3.112807273864746 17.15407943725586
Loss :  1.5889450311660767 3.5452446937561035 19.315168380737305
Loss :  1.6542942523956299 2.472724437713623 14.017916679382324
Loss :  1.6605459451675415 1.9805047512054443 11.563070297241211
Loss :  1.671181559562683 1.8818250894546509 11.080307006835938
  batch 40 loss: 1.671181559562683, 1.8818250894546509, 11.080307006835938
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.626443862915039 2.4350075721740723 13.801481246948242
Loss :  1.6122002601623535 2.199392080307007 12.609161376953125
Loss :  1.6009093523025513 2.4222476482391357 13.71214771270752
Loss :  1.6150771379470825 2.432936191558838 13.779757499694824
Loss :  1.5906805992126465 2.3544397354125977 13.362878799438477
Loss :  1.6244803667068481 2.0980427265167236 12.114693641662598
Loss :  1.660705327987671 2.40946102142334 13.70801067352295
Loss :  1.607478141784668 1.9207737445831299 11.211346626281738
Loss :  1.6756575107574463 2.113353729248047 12.242425918579102
Loss :  1.6143419742584229 2.611222743988037 14.670455932617188
Loss :  1.6497102975845337 2.5743155479431152 14.521288871765137
Loss :  1.6435035467147827 2.3946220874786377 13.616613388061523
Loss :  1.619969367980957 2.9608218669891357 16.42407989501953
Loss :  1.651144027709961 2.741581678390503 15.359052658081055
Loss :  1.6070365905761719 2.64031720161438 14.808622360229492
Loss :  1.6752862930297852 2.7463126182556152 15.40684986114502
Loss :  1.6143885850906372 2.2320053577423096 12.774415016174316
Loss :  1.5965639352798462 2.376267910003662 13.477904319763184
Loss :  1.6132941246032715 2.742572784423828 15.32615852355957
Loss :  1.6823015213012695 2.64327335357666 14.89866828918457
  batch 60 loss: 1.6823015213012695, 2.64327335357666, 14.89866828918457
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.59721839427948 2.6242120265960693 14.718278884887695
Loss :  1.6289260387420654 1.971376895904541 11.485810279846191
Loss :  1.6075738668441772 1.9090219736099243 11.15268325805664
Loss :  1.5944899320602417 2.24967098236084 12.84284496307373
Loss :  1.5728741884231567 2.5923144817352295 14.534446716308594
Loss :  1.6165401935577393 4.0028815269470215 21.630949020385742
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 5], device='cuda:0')
Loss :  1.6298272609710693 4.091739177703857 22.088523864746094
Loss :  1.6246715784072876 3.9149539470672607 21.19944190979004
Loss :  1.6390382051467896 3.83402943611145 20.809185028076172
Total LOSS train 13.639498475881723 valid 21.43202495574951
CE LOSS train 1.6229653890316302 valid 0.4097595512866974
Contrastive LOSS train 2.403306619937603 valid 0.9585073590278625
EPOCH 173:
Loss :  1.6439884901046753 3.753939628601074 20.413686752319336
Loss :  1.6586298942565918 2.846787929534912 15.892570495605469
Loss :  1.625089406967163 2.119790554046631 12.224042892456055
Loss :  1.6305687427520752 2.3463785648345947 13.36246109008789
Loss :  1.6499031782150269 3.032681465148926 16.813310623168945
Loss :  1.6111022233963013 2.230724573135376 12.764724731445312
Loss :  1.6498358249664307 2.496741533279419 14.133543014526367
Loss :  1.623486042022705 1.901559829711914 11.131284713745117
Loss :  1.613021731376648 1.8911945819854736 11.068994522094727
Loss :  1.6514465808868408 1.814520239830017 10.724047660827637
Loss :  1.6035935878753662 2.9432475566864014 16.31983184814453
Loss :  1.6023952960968018 2.044546604156494 11.825127601623535
Loss :  1.6001636981964111 1.8172739744186401 10.686532974243164
Loss :  1.6059435606002808 1.9129130840301514 11.170509338378906
Loss :  1.6662101745605469 2.593964099884033 14.636030197143555
Loss :  1.6614880561828613 2.911043405532837 16.216705322265625
Loss :  1.5971579551696777 4.034731388092041 21.770814895629883
Loss :  1.6259183883666992 1.968240737915039 11.467122077941895
Loss :  1.59266996383667 1.8503766059875488 10.844552993774414
Loss :  1.6598654985427856 2.044792652130127 11.883828163146973
  batch 20 loss: 1.6598654985427856, 2.044792652130127, 11.883828163146973
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.6194307804107666 2.4195382595062256 13.717122077941895
Loss :  1.5915108919143677 2.180295705795288 12.492989540100098
Loss :  1.6117067337036133 2.1784210205078125 12.503811836242676
Loss :  1.6290053129196167 2.2838947772979736 13.048479080200195
Loss :  1.660629153251648 2.4496991634368896 13.909125328063965
Loss :  1.6159517765045166 2.501260995864868 14.1222562789917
Loss :  1.6265729665756226 2.6275506019592285 14.764325141906738
Loss :  1.6206563711166382 1.8270752429962158 10.75603199005127
Loss :  1.5651723146438599 1.9113649129867554 11.121996879577637
Loss :  1.6564711332321167 2.573216676712036 14.522554397583008
Loss :  1.5656535625457764 2.292217969894409 13.02674388885498
Loss :  1.6388872861862183 2.2231621742248535 12.754697799682617
Loss :  1.6135303974151611 2.4055745601654053 13.641403198242188
Loss :  1.6113373041152954 2.544426441192627 14.33346939086914
Loss :  1.5740940570831299 2.357304096221924 13.360613822937012
Loss :  1.590879201889038 2.3391952514648438 13.286855697631836
Loss :  1.5901252031326294 2.3274881839752197 13.22756576538086
Loss :  1.6549922227859497 2.3390769958496094 13.350377082824707
Loss :  1.6617017984390259 2.083350896835327 12.07845687866211
Loss :  1.671844244003296 1.9460057020187378 11.401872634887695
  batch 40 loss: 1.671844244003296, 1.9460057020187378, 11.401872634887695
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.6284151077270508 1.975564956665039 11.506239891052246
Loss :  1.6142772436141968 2.5620758533477783 14.42465591430664
Loss :  1.6031700372695923 2.159397602081299 12.400157928466797
Loss :  1.616668939590454 1.693419098854065 10.08376407623291
Loss :  1.592866063117981 1.778206467628479 10.483898162841797
Loss :  1.6262519359588623 2.430032968521118 13.776416778564453
Loss :  1.6615208387374878 2.12056040763855 12.264323234558105
Loss :  1.6097941398620605 2.4231135845184326 13.725362777709961
Loss :  1.6754740476608276 2.5096426010131836 14.223687171936035
Loss :  1.6159754991531372 2.7682371139526367 15.457160949707031
Loss :  1.6506110429763794 2.6248741149902344 14.774981498718262
Loss :  1.6443849802017212 2.2608566284179688 12.948668479919434
Loss :  1.6217553615570068 2.26810622215271 12.962286949157715
Loss :  1.6538763046264648 1.8156492710113525 10.732122421264648
Loss :  1.6104700565338135 2.362978458404541 13.425361633300781
Loss :  1.6783088445663452 2.2815611362457275 13.086113929748535
Loss :  1.618973731994629 1.957363247871399 11.405790328979492
Loss :  1.6017615795135498 1.9370322227478027 11.2869234085083
Loss :  1.6182076930999756 2.1672921180725098 12.454668045043945
Loss :  1.6847776174545288 1.6307945251464844 9.838749885559082
  batch 60 loss: 1.6847776174545288, 1.6307945251464844, 9.838749885559082
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.6023783683776855 1.8853743076324463 11.02924919128418
Loss :  1.6329371929168701 1.8687065839767456 10.976469993591309
Loss :  1.6119886636734009 1.641940712928772 9.82169246673584
Loss :  1.5993081331253052 2.0211000442504883 11.704808235168457
Loss :  1.5782134532928467 1.5025204420089722 9.090815544128418
Loss :  1.6099265813827515 4.048844814300537 21.854150772094727
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.6206852197647095 4.086244583129883 22.051908493041992
Loss :  1.616662621498108 3.9056191444396973 21.144758224487305
Loss :  1.6260442733764648 3.805155038833618 20.65182113647461
Total LOSS train 12.9331513771644 valid 21.425659656524658
CE LOSS train 1.6245384289668157 valid 0.4065110683441162
Contrastive LOSS train 2.261722605045025 valid 0.9512887597084045
EPOCH 174:
Loss :  1.6474870443344116 1.8490437269210815 10.892704963684082
Loss :  1.6616928577423096 2.2892279624938965 13.107832908630371
Loss :  1.6289139986038208 1.4906176328659058 9.082002639770508
Loss :  1.6349369287490845 2.1675474643707275 12.472674369812012
Loss :  1.6535139083862305 2.497387170791626 14.140449523925781
Loss :  1.616188645362854 2.0088987350463867 11.660682678222656
Loss :  1.653281331062317 1.9643080234527588 11.474821090698242
Loss :  1.6278562545776367 1.937497615814209 11.31534481048584
Loss :  1.6174204349517822 1.7608026266098022 10.421433448791504
Loss :  1.6544740200042725 1.7933982610702515 10.621465682983398
Loss :  1.608875036239624 2.601198434829712 14.614867210388184
Loss :  1.6079442501068115 2.145984649658203 12.337867736816406
Loss :  1.6056655645370483 2.0764732360839844 11.988031387329102
Loss :  1.6114394664764404 1.9121381044387817 11.172130584716797
Loss :  1.669332504272461 2.0267865657806396 11.803265571594238
Loss :  1.6648225784301758 2.35178542137146 13.423749923706055
Loss :  1.6026074886322021 2.343925952911377 13.322237014770508
Loss :  1.6307802200317383 2.3210668563842773 13.236114501953125
Loss :  1.5991942882537842 1.8954497575759888 11.076443672180176
Loss :  1.6643363237380981 2.1833252906799316 12.580962181091309
  batch 20 loss: 1.6643363237380981, 2.1833252906799316, 12.580962181091309
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.6260343790054321 2.2462990283966064 12.857529640197754
Loss :  1.5982997417449951 2.35359525680542 13.366276741027832
Loss :  1.6168885231018066 2.020880937576294 11.721292495727539
Loss :  1.6344983577728271 1.8365873098373413 10.817435264587402
Loss :  1.6657851934432983 2.1176586151123047 12.254077911376953
Loss :  1.6221226453781128 2.1411707401275635 12.32797622680664
Loss :  1.6321299076080322 2.141648769378662 12.340373992919922
Loss :  1.6259254217147827 2.4953205585479736 14.102527618408203
Loss :  1.572935700416565 2.568007469177246 14.412973403930664
Loss :  1.661729335784912 2.797945976257324 15.651458740234375
Loss :  1.5750064849853516 2.473940134048462 13.944706916809082
Loss :  1.645285725593567 2.541264295578003 14.351607322692871
Loss :  1.6212085485458374 2.125917911529541 12.250797271728516
Loss :  1.6196650266647339 2.0822107791900635 12.030718803405762
Loss :  1.5836942195892334 2.680741786956787 14.987403869628906
Loss :  1.599261999130249 2.3061366081237793 13.129945755004883
Loss :  1.598523497581482 2.222429037094116 12.710668563842773
Loss :  1.6598820686340332 2.1822495460510254 12.571128845214844
Loss :  1.6655974388122559 2.1149983406066895 12.240589141845703
Loss :  1.6754772663116455 2.2161014080047607 12.75598430633545
  batch 40 loss: 1.6754772663116455, 2.2161014080047607, 12.75598430633545
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.6335148811340332 2.261406421661377 12.940546035766602
Loss :  1.620066523551941 2.132751226425171 12.283823013305664
Loss :  1.6089789867401123 2.5555427074432373 14.386693000793457
Loss :  1.6223888397216797 2.0432937145233154 11.838857650756836
Loss :  1.6001174449920654 2.0359723567962646 11.779979705810547
Loss :  1.6325657367706299 2.289761543273926 13.08137321472168
Loss :  1.6664518117904663 2.3474154472351074 13.403529167175293
Loss :  1.6165542602539062 2.287464141845703 13.053874969482422
Loss :  1.6799712181091309 2.473752737045288 14.048734664916992
Loss :  1.6222506761550903 2.2841908931732178 13.043205261230469
Loss :  1.656132698059082 2.618163585662842 14.74695110321045
Loss :  1.6500380039215088 2.070518732070923 12.002631187438965
Loss :  1.6276150941848755 1.9472482204437256 11.363856315612793
Loss :  1.6580960750579834 1.9719117879867554 11.517655372619629
Loss :  1.6167881488800049 1.9926635026931763 11.580105781555176
Loss :  1.6808984279632568 2.4531352519989014 13.946575164794922
Loss :  1.6245914697647095 2.2098703384399414 12.673943519592285
Loss :  1.6084158420562744 2.087341785430908 12.045124053955078
Loss :  1.6245661973953247 2.288459300994873 13.066863059997559
Loss :  1.6886988878250122 1.7801203727722168 10.589301109313965
  batch 60 loss: 1.6886988878250122, 1.7801203727722168, 10.589301109313965
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.6088879108428955 1.9240851402282715 11.229313850402832
Loss :  1.637945294380188 1.8981961011886597 11.128925323486328
Loss :  1.6179261207580566 1.752251386642456 10.379182815551758
Loss :  1.6057889461517334 2.234881639480591 12.780197143554688
Loss :  1.5862394571304321 2.312671184539795 13.149596214294434
Loss :  1.6178176403045654 3.5287156105041504 19.261394500732422
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.6297781467437744 3.391075849533081 18.58515739440918
Loss :  1.6259785890579224 3.3567566871643066 18.409761428833008
Loss :  1.6330881118774414 3.185894727706909 17.56256103515625
Total LOSS train 12.517406052809495 valid 18.454718589782715
CE LOSS train 1.63009543969081 valid 0.40827202796936035
Contrastive LOSS train 2.1774621156545786 valid 0.7964736819267273
EPOCH 175:
Loss :  1.6528507471084595 2.2566754817962646 12.93622875213623
Loss :  1.6673367023468018 2.2027220726013184 12.680947303771973
Loss :  1.6360219717025757 1.81671142578125 10.719578742980957
Loss :  1.6414787769317627 2.375556707382202 13.519262313842773
Loss :  1.6589890718460083 1.9205186367034912 11.261582374572754
Loss :  1.622572898864746 1.8758717775344849 11.001932144165039
Loss :  1.6589308977127075 2.3806159496307373 13.562010765075684
Loss :  1.6345906257629395 1.8286330699920654 10.777755737304688
Loss :  1.6244758367538452 1.8858847618103027 11.053899765014648
Loss :  1.6597692966461182 1.7865469455718994 10.592503547668457
Loss :  1.6155318021774292 2.690056800842285 15.065815925598145
Loss :  1.6142829656600952 2.5929765701293945 14.5791654586792
Loss :  1.6121821403503418 2.5009870529174805 14.117116928100586
Loss :  1.617626667022705 2.107499837875366 12.155126571655273
Loss :  1.674357295036316 2.5804085731506348 14.576399803161621
Loss :  1.6700149774551392 2.61696720123291 14.754851341247559
Loss :  1.6105625629425049 1.959227442741394 11.406699180603027
Loss :  1.6370383501052856 2.3334105014801025 13.30409049987793
Loss :  1.6074436902999878 1.817525029182434 10.695069313049316
Loss :  1.6698944568634033 1.9348294734954834 11.34404182434082
  batch 20 loss: 1.6698944568634033, 1.9348294734954834, 11.34404182434082
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.632431149482727 1.772822618484497 10.496543884277344
Loss :  1.6065783500671387 2.045200824737549 11.832582473754883
Loss :  1.6248033046722412 1.778396487236023 10.516785621643066
Loss :  1.6409416198730469 1.9291309118270874 11.286596298217773
Loss :  1.669270634651184 2.484592914581299 14.09223461151123
Loss :  1.6279348134994507 2.792846441268921 15.592166900634766
Loss :  1.6382312774658203 2.3017733097076416 13.14709758758545
Loss :  1.6325650215148926 2.26369309425354 12.951030731201172
Loss :  1.5803357362747192 1.815050482749939 10.655588150024414
Loss :  1.6662061214447021 2.952693223953247 16.429672241210938
Loss :  1.5814517736434937 2.645186424255371 14.80738353729248
Loss :  1.6499664783477783 2.4225218296051025 13.762575149536133
Loss :  1.6249748468399048 2.1016952991485596 12.133451461791992
Loss :  1.6232727766036987 2.1302292346954346 12.274418830871582
Loss :  1.5893884897232056 2.246886968612671 12.823823928833008
Loss :  1.6042460203170776 2.0316526889801025 11.7625093460083
Loss :  1.6038048267364502 2.467698097229004 13.94229507446289
Loss :  1.6640938520431519 2.023691415786743 11.782550811767578
Loss :  1.6698839664459229 1.9915732145309448 11.6277494430542
Loss :  1.6785887479782104 2.6614530086517334 14.985854148864746
  batch 40 loss: 1.6785887479782104, 2.6614530086517334, 14.985854148864746
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.6388616561889648 2.6581671237945557 14.929697036743164
Loss :  1.6253769397735596 2.135233163833618 12.301542282104492
Loss :  1.6143405437469482 2.066480875015259 11.946744918823242
Loss :  1.6280080080032349 2.2223422527313232 12.73971939086914
Loss :  1.6046802997589111 2.1045968532562256 12.127664566040039
Loss :  1.6359726190567017 2.617500066757202 14.72347354888916
Loss :  1.6689367294311523 2.0101802349090576 11.71983814239502
Loss :  1.6200010776519775 2.4806792736053467 14.023397445678711
Loss :  1.6821696758270264 2.319845676422119 13.281397819519043
Loss :  1.6261370182037354 2.091726541519165 12.084769248962402
Loss :  1.659305214881897 2.7884445190429688 15.60152816772461
Loss :  1.6533795595169067 2.2690021991729736 12.998390197753906
Loss :  1.6324666738510132 2.000432252883911 11.634627342224121
Loss :  1.662996768951416 2.497889518737793 14.152444839477539
Loss :  1.6208751201629639 2.3487977981567383 13.364864349365234
Loss :  1.6861275434494019 2.59920334815979 14.682144165039062
Loss :  1.6307262182235718 2.805842876434326 15.659940719604492
Loss :  1.6129209995269775 2.6835148334503174 15.030494689941406
Loss :  1.6298444271087646 2.725729465484619 15.258491516113281
Loss :  1.6929099559783936 2.2028214931488037 12.70701789855957
  batch 60 loss: 1.6929099559783936, 2.2028214931488037, 12.70701789855957
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.614974856376648 2.0183982849121094 11.706966400146484
Loss :  1.6419339179992676 2.1020028591156006 12.151948928833008
Loss :  1.623022198677063 2.123525857925415 12.24065113067627
Loss :  1.6101192235946655 2.4408748149871826 13.814493179321289
Loss :  1.5899149179458618 1.5302764177322388 9.241296768188477
Loss :  1.6286145448684692 4.295641899108887 23.10682487487793
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.6350573301315308 4.226682186126709 22.768468856811523
Loss :  1.6304771900177002 4.125185966491699 22.25640869140625
Loss :  1.6656569242477417 4.066086292266846 21.9960880279541
Total LOSS train 12.878931280282828 valid 22.53194761276245
CE LOSS train 1.6353988262323234 valid 0.4164142310619354
Contrastive LOSS train 2.248706498512855 valid 1.0165215730667114
EPOCH 176:
Loss :  1.6561914682388306 2.392287015914917 13.617627143859863
Loss :  1.6703521013259888 2.026421308517456 11.802458763122559
Loss :  1.6401995420455933 2.189608335494995 12.588241577148438
Loss :  1.646705985069275 1.9571994543075562 11.432703018188477
Loss :  1.6654196977615356 2.6507630348205566 14.919234275817871
Loss :  1.6288625001907349 2.7829103469848633 15.543414115905762
Loss :  1.6651784181594849 3.6358911991119385 19.844635009765625
Loss :  1.6377052068710327 2.641080617904663 14.843108177185059
Loss :  1.6253600120544434 2.3768043518066406 13.509382247924805
Loss :  1.6599524021148682 2.2080395221710205 12.700149536132812
Loss :  1.614498257637024 2.8464272022247314 15.846633911132812
Loss :  1.6132097244262695 3.0033113956451416 16.6297664642334
Loss :  1.6103554964065552 2.7908592224121094 15.564651489257812
Loss :  1.614729642868042 2.5889813899993896 14.559637069702148
Loss :  1.6699186563491821 2.863677740097046 15.98830795288086
Loss :  1.6673541069030762 2.379133462905884 13.563020706176758
Loss :  1.6055711507797241 2.0167171955108643 11.689156532287598
Loss :  1.6346585750579834 2.768704652786255 15.478181838989258
Loss :  1.6026556491851807 3.719660758972168 20.200960159301758
Loss :  1.6667641401290894 3.1997079849243164 17.66530418395996
  batch 20 loss: 1.6667641401290894, 3.1997079849243164, 17.66530418395996
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.6283351182937622 2.9218313694000244 16.237491607666016
Loss :  1.601122498512268 2.605520486831665 14.628725051879883
Loss :  1.621063232421875 2.19953989982605 12.618762969970703
Loss :  1.6368743181228638 2.0924530029296875 12.099139213562012
Loss :  1.6676397323608398 2.4615960121154785 13.975619316101074
Loss :  1.6247130632400513 3.7306973934173584 20.278200149536133
Loss :  1.634319543838501 3.554723024368286 19.407934188842773
Loss :  1.6276707649230957 2.339264392852783 13.323991775512695
Loss :  1.5732319355010986 2.1284279823303223 12.215371131896973
Loss :  1.6609135866165161 2.2457504272460938 12.889665603637695
Loss :  1.5721538066864014 2.187023639678955 12.507271766662598
Loss :  1.6432745456695557 2.9610345363616943 16.448448181152344
Loss :  1.6167722940444946 2.8624112606048584 15.928829193115234
Loss :  1.6148416996002197 2.604933261871338 14.639507293701172
Loss :  1.5774762630462646 2.790389060974121 15.52942180633545
Loss :  1.5936315059661865 3.035954713821411 16.773405075073242
Loss :  1.5916324853897095 2.3846635818481445 13.5149507522583
Loss :  1.656267523765564 2.7362241744995117 15.337388038635254
Loss :  1.6610325574874878 2.463869571685791 13.980380058288574
Loss :  1.6719980239868164 3.2294516563415527 17.819255828857422
  batch 40 loss: 1.6719980239868164, 3.2294516563415527, 17.819255828857422
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.6296203136444092 3.6452853679656982 19.856046676635742
Loss :  1.6159355640411377 2.3325157165527344 13.27851390838623
Loss :  1.6026538610458374 2.654334306716919 14.874324798583984
Loss :  1.617090106010437 2.5030369758605957 14.132274627685547
Loss :  1.591878890991211 2.4981374740600586 14.082566261291504
Loss :  1.6241192817687988 2.2497382164001465 12.872810363769531
Loss :  1.6587586402893066 2.5368263721466064 14.342889785766602
Loss :  1.6055155992507935 2.1495237350463867 12.353134155273438
Loss :  1.6734148263931274 2.419804334640503 13.77243709564209
Loss :  1.6114879846572876 2.3987181186676025 13.60507869720459
Loss :  1.6471061706542969 3.1716408729553223 17.50531005859375
Loss :  1.6413325071334839 3.018601894378662 16.734342575073242
Loss :  1.618384838104248 2.8908474445343018 16.072622299194336
Loss :  1.650963306427002 2.2441468238830566 12.871696472167969
Loss :  1.605517029762268 2.6072540283203125 14.6417875289917
Loss :  1.6750853061676025 2.4435112476348877 13.892641067504883
Loss :  1.6146013736724854 2.318962812423706 13.209415435791016
Loss :  1.5962300300598145 2.706721067428589 15.12983512878418
Loss :  1.6137603521347046 3.0699195861816406 16.96335792541504
Loss :  1.6825370788574219 2.327235698699951 13.318716049194336
  batch 60 loss: 1.6825370788574219, 2.327235698699951, 13.318716049194336
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.5978511571884155 2.5738327503204346 14.46701431274414
Loss :  1.6292957067489624 2.3023910522460938 13.141250610351562
Loss :  1.6081711053848267 2.6876749992370605 15.046546936035156
Loss :  1.5957640409469604 2.377262592315674 13.482076644897461
Loss :  1.5748190879821777 2.1187448501586914 12.168542861938477
Loss :  1.621803879737854 4.398584365844727 23.61472511291504
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6367530822753906 4.3741254806518555 23.507381439208984
Loss :  1.6312941312789917 4.3026814460754395 23.14470100402832
Loss :  1.6103216409683228 4.285093307495117 23.03578758239746
Total LOSS train 14.768547160808856 valid 23.32564878463745
CE LOSS train 1.6285000214209924 valid 0.4025804102420807
Contrastive LOSS train 2.6280094458506658 valid 1.0712733268737793
EPOCH 177:
Loss :  1.6459136009216309 2.4680073261260986 13.985950469970703
Loss :  1.6598683595657349 2.8578436374664307 15.94908618927002
Loss :  1.6268845796585083 2.1879360675811768 12.566564559936523
Loss :  1.6331225633621216 2.3150672912597656 13.20845890045166
Loss :  1.6515122652053833 3.3037073612213135 18.1700496673584
Loss :  1.6121282577514648 2.298510789871216 13.104681968688965
Loss :  1.6511056423187256 2.361168622970581 13.456949234008789
Loss :  1.6234159469604492 2.0261662006378174 11.754246711730957
Loss :  1.6132794618606567 1.882880687713623 11.02768325805664
Loss :  1.65058434009552 2.0275676250457764 11.788422584533691
Loss :  1.603848934173584 2.489658832550049 14.052143096923828
Loss :  1.6027617454528809 2.458116292953491 13.893342971801758
Loss :  1.6009018421173096 2.7508959770202637 15.355381965637207
Loss :  1.6070717573165894 2.600806713104248 14.611105918884277
Loss :  1.6665476560592651 2.1716723442077637 12.524909973144531
Loss :  1.6612273454666138 2.1536481380462646 12.429468154907227
Loss :  1.5969102382659912 2.695746660232544 15.075643539428711
Loss :  1.6260801553726196 2.1661314964294434 12.456738471984863
Loss :  1.592801809310913 2.417672872543335 13.681166648864746
Loss :  1.6599314212799072 3.320380449295044 18.26183319091797
  batch 20 loss: 1.6599314212799072, 3.320380449295044, 18.26183319091797
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.6204254627227783 2.6311516761779785 14.776183128356934
Loss :  1.592415452003479 1.895409345626831 11.069462776184082
Loss :  1.6125948429107666 1.7559369802474976 10.392279624938965
Loss :  1.6305419206619263 3.095364570617676 17.107364654541016
Loss :  1.6617339849472046 2.1861929893493652 12.59269905090332
Loss :  1.6172733306884766 2.327164888381958 13.253097534179688
Loss :  1.6272737979888916 2.3718438148498535 13.486492156982422
Loss :  1.6213752031326294 2.5330934524536133 14.286842346191406
Loss :  1.5653220415115356 3.650707244873047 19.818859100341797
Loss :  1.6564861536026 3.116718292236328 17.24007797241211
Loss :  1.5661628246307373 2.3230767250061035 13.181546211242676
Loss :  1.6398977041244507 2.5871734619140625 14.575764656066895
Loss :  1.6134816408157349 2.4560904502868652 13.89393424987793
Loss :  1.6121392250061035 3.0855417251586914 17.03984832763672
Loss :  1.5748677253723145 2.4797275066375732 13.973505020141602
Loss :  1.5910521745681763 2.2787210941314697 12.984657287597656
Loss :  1.5895174741744995 3.4589571952819824 18.88430404663086
Loss :  1.6544535160064697 2.347839832305908 13.393651962280273
Loss :  1.6605583429336548 2.2591660022735596 12.956388473510742
Loss :  1.6710766553878784 2.6412014961242676 14.877083778381348
  batch 40 loss: 1.6710766553878784, 2.6412014961242676, 14.877083778381348
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.6277228593826294 2.0010783672332764 11.6331148147583
Loss :  1.6136139631271362 2.131601333618164 12.271620750427246
Loss :  1.6023876667022705 2.428054094314575 13.742657661437988
Loss :  1.6168583631515503 2.5817651748657227 14.525684356689453
Loss :  1.5929906368255615 1.9801521301269531 11.493751525878906
Loss :  1.6256511211395264 2.5747618675231934 14.49946117401123
Loss :  1.6602468490600586 2.960784673690796 16.464170455932617
Loss :  1.6081005334854126 3.524583101272583 19.231016159057617
Loss :  1.6753355264663696 2.3638288974761963 13.49448013305664
Loss :  1.6138843297958374 2.0108468532562256 11.668118476867676
Loss :  1.6482938528060913 2.326552629470825 13.28105640411377
Loss :  1.6425845623016357 2.711212635040283 15.198647499084473
Loss :  1.6200406551361084 3.108506202697754 17.16257095336914
Loss :  1.6520272493362427 2.2339069843292236 12.821561813354492
Loss :  1.606127142906189 2.000349283218384 11.60787296295166
Loss :  1.6755362749099731 2.000124454498291 11.67615795135498
Loss :  1.6157560348510742 2.456397771835327 13.897745132446289
Loss :  1.5978405475616455 2.532048225402832 14.258081436157227
Loss :  1.614827275276184 2.734386920928955 15.286761283874512
Loss :  1.6825984716415405 2.543426036834717 14.399728775024414
  batch 60 loss: 1.6825984716415405, 2.543426036834717, 14.399728775024414
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.5993547439575195 2.002049446105957 11.609601974487305
Loss :  1.6297115087509155 1.87776780128479 11.018549919128418
Loss :  1.6089167594909668 3.339888572692871 18.308361053466797
Loss :  1.5958678722381592 2.720935821533203 15.200547218322754
Loss :  1.5748811960220337 1.7525200843811035 10.337481498718262
Loss :  1.5776435136795044 4.028388023376465 21.719585418701172
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 5], device='cuda:0')
Loss :  1.5895196199417114 4.032294750213623 21.750993728637695
Loss :  1.5840284824371338 3.9163551330566406 21.165803909301758
Loss :  1.5917783975601196 3.77600359916687 20.4717960357666
Total LOSS train 14.03425648029034 valid 21.277044773101807
CE LOSS train 1.6240877444927508 valid 0.3979445993900299
Contrastive LOSS train 2.482033746059124 valid 0.9440008997917175
EPOCH 178:
Loss :  1.645666480064392 3.127302408218384 17.28217887878418
Loss :  1.6590986251831055 3.0212819576263428 16.7655086517334
Loss :  1.626842737197876 1.8398491144180298 10.826087951660156
Loss :  1.6333539485931396 2.238633871078491 12.826523780822754
Loss :  1.6521546840667725 2.026620626449585 11.785258293151855
Loss :  1.6139174699783325 2.8286945819854736 15.757390022277832
Loss :  1.652428150177002 2.5879762172698975 14.592309951782227
Loss :  1.6251260042190552 2.1785871982574463 12.518061637878418
Loss :  1.615153431892395 2.0880813598632812 12.055560111999512
Loss :  1.6513969898223877 2.2044122219085693 12.673458099365234
Loss :  1.6055294275283813 2.6190617084503174 14.700838088989258
Loss :  1.604965090751648 2.8365399837493896 15.787665367126465
Loss :  1.6024340391159058 2.0973918437957764 12.089393615722656
Loss :  1.6086763143539429 2.822676658630371 15.72205924987793
Loss :  1.6673941612243652 2.4608280658721924 13.971534729003906
Loss :  1.6628313064575195 2.3880417346954346 13.603039741516113
Loss :  1.5993884801864624 2.1996071338653564 12.597423553466797
Loss :  1.6277294158935547 2.1687324047088623 12.471391677856445
Loss :  1.5941745042800903 2.0692975521087646 11.940662384033203
Loss :  1.6608710289001465 3.1364409923553467 17.343076705932617
  batch 20 loss: 1.6608710289001465, 3.1364409923553467, 17.343076705932617
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.620445728302002 3.6926779747009277 20.08383560180664
Loss :  1.5930925607681274 2.3352317810058594 13.269251823425293
Loss :  1.6125903129577637 2.4957191944122314 14.0911865234375
Loss :  1.6295981407165527 1.9566516876220703 11.412857055664062
Loss :  1.6617136001586914 2.2537460327148438 12.93044376373291
Loss :  1.616715669631958 1.8384243249893188 10.808836936950684
Loss :  1.6272944211959839 2.6815240383148193 15.03491497039795
Loss :  1.621130108833313 2.6276257038116455 14.759258270263672
Loss :  1.5666381120681763 1.9445990324020386 11.289632797241211
Loss :  1.6574389934539795 2.234473943710327 12.829809188842773
Loss :  1.5684484243392944 1.936090111732483 11.24889850616455
Loss :  1.6415650844573975 2.5536890029907227 14.41001033782959
Loss :  1.6156927347183228 2.2295379638671875 12.763382911682129
Loss :  1.6141995191574097 2.032348871231079 11.775943756103516
Loss :  1.5768582820892334 2.669090509414673 14.922310829162598
Loss :  1.5929609537124634 2.377061605453491 13.478269577026367
Loss :  1.5912076234817505 1.8953750133514404 11.068082809448242
Loss :  1.6559561491012573 2.428285837173462 13.797385215759277
Loss :  1.6610747575759888 2.27227520942688 13.02245044708252
Loss :  1.6716316938400269 2.4018619060516357 13.680941581726074
  batch 40 loss: 1.6716316938400269, 2.4018619060516357, 13.680941581726074
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.628503680229187 2.197723388671875 12.617120742797852
Loss :  1.6142997741699219 1.877564787864685 11.002123832702637
Loss :  1.602525234222412 1.9827523231506348 11.516286849975586
Loss :  1.6176409721374512 1.8637592792510986 10.936437606811523
Loss :  1.5940542221069336 1.6521605253219604 9.854856491088867
Loss :  1.6268669366836548 2.0770859718322754 12.012296676635742
Loss :  1.6624068021774292 1.9026468992233276 11.175641059875488
Loss :  1.6102173328399658 1.9599096775054932 11.409765243530273
Loss :  1.677482008934021 2.367440700531006 13.51468563079834
Loss :  1.6163963079452515 2.0970160961151123 12.101476669311523
Loss :  1.6510547399520874 2.0117239952087402 11.709674835205078
Loss :  1.6449072360992432 2.2601819038391113 12.945816040039062
Loss :  1.6223217248916626 2.2328481674194336 12.7865629196167
Loss :  1.6533194780349731 2.778785228729248 15.547245979309082
Loss :  1.6088459491729736 2.278271198272705 13.000201225280762
Loss :  1.6763490438461304 2.418368339538574 13.768190383911133
Loss :  1.6171835660934448 2.474658489227295 13.990476608276367
Loss :  1.5997874736785889 2.387680768966675 13.538191795349121
Loss :  1.6163616180419922 2.5221521854400635 14.22712230682373
Loss :  1.6831097602844238 2.114671230316162 12.256465911865234
  batch 60 loss: 1.6831097602844238, 2.114671230316162, 12.256465911865234
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6005399227142334 2.124488592147827 12.222983360290527
Loss :  1.6307893991470337 1.9710053205490112 11.48581600189209
Loss :  1.6097602844238281 2.6427154541015625 14.82333755493164
Loss :  1.5967071056365967 2.945249319076538 16.322954177856445
Loss :  1.5756769180297852 1.6757075786590576 9.954215049743652
Loss :  1.5710246562957764 3.745710849761963 20.299577713012695
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.5860739946365356 3.539090633392334 19.28152847290039
Loss :  1.5843428373336792 3.656061887741089 19.864652633666992
Loss :  1.5779310464859009 3.38787579536438 18.517309188842773
Total LOSS train 13.21087800539457 valid 19.490767002105713
CE LOSS train 1.6252691177221446 valid 0.3944827616214752
Contrastive LOSS train 2.3171217661637526 valid 0.846968948841095
EPOCH 179:
Loss :  1.6453235149383545 1.9915177822113037 11.602912902832031
Loss :  1.6595591306686401 2.25034236907959 12.911271095275879
Loss :  1.626969337463379 2.071254253387451 11.983241081237793
Loss :  1.6334172487258911 1.9947203397750854 11.60701847076416
Loss :  1.6525274515151978 2.1650826930999756 12.477941513061523
Loss :  1.614163875579834 2.685559034347534 15.041959762573242
Loss :  1.653311848640442 2.229508399963379 12.800853729248047
Loss :  1.6271451711654663 1.723670244216919 10.245495796203613
Loss :  1.617261290550232 2.5929970741271973 14.582245826721191
Loss :  1.6534723043441772 2.0831172466278076 12.069058418273926
Loss :  1.6076136827468872 2.755000591278076 15.382616996765137
Loss :  1.6064304113388062 2.184493064880371 12.528895378112793
Loss :  1.60414719581604 2.6051077842712402 14.62968635559082
Loss :  1.6097424030303955 2.0482587814331055 11.851036071777344
Loss :  1.6678210496902466 2.432053327560425 13.82808780670166
Loss :  1.6634504795074463 2.415369987487793 13.740300178527832
Loss :  1.6013104915618896 2.615267038345337 14.677645683288574
Loss :  1.6296335458755493 2.6395254135131836 14.827260971069336
Loss :  1.5970368385314941 2.007131814956665 11.632696151733398
Loss :  1.6626074314117432 2.108675479888916 12.205984115600586
  batch 20 loss: 1.6626074314117432, 2.108675479888916, 12.205984115600586
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.6232956647872925 1.8825832605361938 11.036211967468262
Loss :  1.595894455909729 2.058566093444824 11.888725280761719
Loss :  1.6153208017349243 1.770254135131836 10.466591835021973
Loss :  1.632181167602539 1.9257432222366333 11.260897636413574
Loss :  1.663360595703125 2.041192054748535 11.8693208694458
Loss :  1.6192891597747803 2.505542516708374 14.147002220153809
Loss :  1.6302907466888428 2.4545059204101562 13.902820587158203
Loss :  1.6244006156921387 2.3083255290985107 13.16602897644043
Loss :  1.5700397491455078 2.341188669204712 13.275982856750488
Loss :  1.660452961921692 2.636728525161743 14.844095230102539
Loss :  1.5718742609024048 2.8799960613250732 15.971855163574219
Loss :  1.6430314779281616 2.9398345947265625 16.342205047607422
Loss :  1.617081642150879 2.6312685012817383 14.77342414855957
Loss :  1.6153329610824585 2.2684872150421143 12.957768440246582
Loss :  1.5789543390274048 2.288050651550293 13.019207954406738
Loss :  1.5943917036056519 2.1567912101745605 12.378348350524902
Loss :  1.5934020280838013 1.9857078790664673 11.521941184997559
Loss :  1.6574395895004272 1.9849063158035278 11.581971168518066
Loss :  1.6636412143707275 2.2184243202209473 12.755762100219727
Loss :  1.6741102933883667 2.126755952835083 12.307889938354492
  batch 40 loss: 1.6741102933883667, 2.126755952835083, 12.307889938354492
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.6307713985443115 2.7824866771698 15.543205261230469
Loss :  1.6167386770248413 1.996480941772461 11.599143028259277
Loss :  1.6056605577468872 2.412275552749634 13.667037963867188
Loss :  1.6188435554504395 2.360347032546997 13.420578002929688
Loss :  1.5955407619476318 2.1791625022888184 12.491353988647461
Loss :  1.6275765895843506 2.466355085372925 13.959352493286133
Loss :  1.6618962287902832 2.2871267795562744 13.097530364990234
Loss :  1.6112433671951294 2.00836443901062 11.65306568145752
Loss :  1.6764109134674072 2.4066648483276367 13.709734916687012
Loss :  1.617448091506958 1.8418115377426147 10.826505661010742
Loss :  1.6531686782836914 2.2097978591918945 12.702157974243164
Loss :  1.6464152336120605 2.555858850479126 14.425708770751953
Loss :  1.6242344379425049 1.8174026012420654 10.711247444152832
Loss :  1.6543089151382446 2.560025930404663 14.454439163208008
Loss :  1.6134377717971802 2.0964839458465576 12.095857620239258
Loss :  1.6780527830123901 2.2567012310028076 12.961559295654297
Loss :  1.6202068328857422 2.035393714904785 11.797175407409668
Loss :  1.6031087636947632 2.0650858879089355 11.92853832244873
Loss :  1.6193116903305054 2.3563344478607178 13.400983810424805
Loss :  1.6851584911346436 1.9460750818252563 11.415534019470215
  batch 60 loss: 1.6851584911346436, 1.9460750818252563, 11.415534019470215
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4], device='cuda:0')
Loss :  1.6023263931274414 2.2549331188201904 12.876992225646973
Loss :  1.6322021484375 2.0286777019500732 11.775590896606445
Loss :  1.6116752624511719 2.6400558948516846 14.811954498291016
Loss :  1.5996731519699097 2.2172162532806396 12.685754776000977
Loss :  1.5791940689086914 1.6844555139541626 10.001471519470215
Loss :  1.6027337312698364 3.588555097579956 19.545509338378906
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6165298223495483 3.5201470851898193 19.21726417541504
Loss :  1.6136088371276855 3.4297282695770264 18.762250900268555
Loss :  1.613940954208374 3.221257209777832 17.720226287841797
Total LOSS train 12.89394972874568 valid 18.811312675476074
CE LOSS train 1.62709745993981 valid 0.4034852385520935
Contrastive LOSS train 2.2533704427572396 valid 0.805314302444458
EPOCH 180:
Loss :  1.6479147672653198 2.008291006088257 11.689370155334473
Loss :  1.661775827407837 2.046320676803589 11.893379211425781
Loss :  1.6289790868759155 1.9773707389831543 11.515832901000977
Loss :  1.6352148056030273 2.2166380882263184 12.718405723571777
Loss :  1.6532766819000244 2.0077576637268066 11.69206428527832
Loss :  1.615397334098816 2.2821319103240967 13.026057243347168
Loss :  1.6532337665557861 2.683478355407715 15.070625305175781
Loss :  1.6272944211959839 1.8408910036087036 10.83174991607666
Loss :  1.6174156665802002 1.8617223501205444 10.926027297973633
Loss :  1.6532882452011108 1.7510544061660767 10.408559799194336
Loss :  1.6085398197174072 2.6115105152130127 14.666091918945312
Loss :  1.6082748174667358 2.3216397762298584 13.216473579406738
Loss :  1.6056629419326782 2.0205228328704834 11.708277702331543
Loss :  1.611345648765564 2.569861888885498 14.460655212402344
Loss :  1.6690301895141602 2.0509071350097656 11.923565864562988
Loss :  1.6650081872940063 2.099118232727051 12.160599708557129
Loss :  1.6036174297332764 2.5527868270874023 14.367551803588867
Loss :  1.6321250200271606 2.355628490447998 13.41026782989502
Loss :  1.6002414226531982 2.0922632217407227 12.06155776977539
Loss :  1.6649914979934692 1.9891602993011475 11.610793113708496
  batch 20 loss: 1.6649914979934692, 1.9891602993011475, 11.610793113708496
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.6255499124526978 2.305910348892212 13.155101776123047
Loss :  1.5984890460968018 2.4321115016937256 13.75904655456543
Loss :  1.6171925067901611 2.1461102962493896 12.34774398803711
Loss :  1.6334165334701538 2.05147385597229 11.890785217285156
Loss :  1.6640797853469849 2.151226758956909 12.42021369934082
Loss :  1.6201139688491821 2.171980857849121 12.480018615722656
Loss :  1.6305406093597412 1.8510396480560303 10.88573932647705
Loss :  1.6240402460098267 2.798029661178589 15.614189147949219
Loss :  1.5700348615646362 2.5375616550445557 14.257843017578125
Loss :  1.6601009368896484 1.955396056175232 11.437081336975098
Loss :  1.5713112354278564 2.2520182132720947 12.831401824951172
Loss :  1.643262267112732 1.9717518091201782 11.502020835876465
Loss :  1.6172232627868652 2.7549426555633545 15.391937255859375
Loss :  1.615435242652893 2.6905581951141357 15.06822681427002
Loss :  1.5793917179107666 2.4455745220184326 13.80726432800293
Loss :  1.5954393148422241 2.3877570629119873 13.534224510192871
Loss :  1.5939276218414307 2.071845531463623 11.953155517578125
Loss :  1.6583305597305298 1.8845593929290771 11.081128120422363
Loss :  1.663771629333496 1.8512736558914185 10.920140266418457
Loss :  1.674297571182251 1.765077829360962 10.499686241149902
  batch 40 loss: 1.674297571182251, 1.765077829360962, 10.499686241149902
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6315046548843384 1.9538681507110596 11.400845527648926
Loss :  1.617775797843933 1.6938368082046509 10.086959838867188
Loss :  1.6059561967849731 1.9305111169815063 11.258511543273926
Loss :  1.619925618171692 1.9486562013626099 11.36320686340332
Loss :  1.596745252609253 2.240116596221924 12.797327995300293
Loss :  1.6285176277160645 2.107006311416626 12.163549423217773
Loss :  1.663041114807129 2.556131601333618 14.44369888305664
Loss :  1.6120415925979614 1.664304256439209 9.933563232421875
Loss :  1.6786214113235474 2.133373260498047 12.345487594604492
Loss :  1.619323492050171 1.9259006977081299 11.24882698059082
Loss :  1.6541084051132202 2.1221022605895996 12.264619827270508
Loss :  1.6487538814544678 2.2791969776153564 13.04473876953125
Loss :  1.6271687746047974 1.8139210939407349 10.69677448272705
Loss :  1.6573618650436401 2.067040205001831 11.992563247680664
Loss :  1.6149808168411255 2.258208751678467 12.906024932861328
Loss :  1.679630994796753 2.0001096725463867 11.680179595947266
Loss :  1.6217297315597534 2.605193614959717 14.647698402404785
Loss :  1.6047946214675903 2.361781358718872 13.413701057434082
Loss :  1.621049404144287 2.3971407413482666 13.606752395629883
Loss :  1.6864168643951416 1.844756007194519 10.910196304321289
  batch 60 loss: 1.6864168643951416, 1.844756007194519, 10.910196304321289
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6055972576141357 2.4006223678588867 13.608709335327148
Loss :  1.6352070569992065 2.235775947570801 12.8140869140625
Loss :  1.6147994995117188 2.177304267883301 12.501320838928223
Loss :  1.6024487018585205 2.03904128074646 11.79765510559082
Loss :  1.5819567441940308 2.142503499984741 12.294474601745605
Loss :  1.5851556062698364 3.6605844497680664 19.888076782226562
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.5975486040115356 3.569105863571167 19.443078994750977
Loss :  1.5941604375839233 3.484147787094116 19.0148983001709
Loss :  1.6000865697860718 3.5512049198150635 19.356111526489258
Total LOSS train 12.452558898925782 valid 19.425541400909424
CE LOSS train 1.6284312890126156 valid 0.40002164244651794
Contrastive LOSS train 2.1648255073107205 valid 0.8878012299537659
EPOCH 181:
Loss :  1.64963960647583 2.0163917541503906 11.731597900390625
Loss :  1.6638306379318237 2.630068063735962 14.814170837402344
Loss :  1.6311720609664917 1.5648918151855469 9.455631256103516
Loss :  1.6373510360717773 1.7848690748214722 10.56169605255127
Loss :  1.6557259559631348 1.964288592338562 11.477169036865234
Loss :  1.6185263395309448 2.6397368907928467 14.817211151123047
Loss :  1.6561774015426636 2.398224353790283 13.647298812866211
Loss :  1.631055474281311 1.7776919603347778 10.519515991210938
Loss :  1.6217869520187378 2.511707067489624 14.180322647094727
Loss :  1.6581584215164185 2.187614679336548 12.596231460571289
Loss :  1.6137042045593262 2.4519379138946533 13.873394012451172
Loss :  1.6125227212905884 2.423823356628418 13.731639862060547
Loss :  1.6101264953613281 2.1832122802734375 12.526187896728516
Loss :  1.6144160032272339 2.320033311843872 13.214582443237305
Loss :  1.6719082593917847 2.257518768310547 12.959502220153809
Loss :  1.666426420211792 2.1725621223449707 12.529236793518066
Loss :  1.6050083637237549 2.628009796142578 14.745057106018066
Loss :  1.632280945777893 2.596813917160034 14.616351127624512
Loss :  1.6002148389816284 2.533616542816162 14.268298149108887
Loss :  1.6642131805419922 2.576277256011963 14.545598983764648
  batch 20 loss: 1.6642131805419922, 2.576277256011963, 14.545598983764648
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6255439519882202 1.977877140045166 11.514928817749023
Loss :  1.59980046749115 2.1688714027404785 12.444156646728516
Loss :  1.6187211275100708 2.510593891143799 14.171689987182617
Loss :  1.6354846954345703 2.7036783695220947 15.153876304626465
Loss :  1.6655685901641846 2.6959829330444336 15.145483016967773
Loss :  1.6218632459640503 2.087125062942505 12.057488441467285
Loss :  1.6322962045669556 2.3791449069976807 13.528020858764648
Loss :  1.626093864440918 2.1093993186950684 12.173090934753418
Loss :  1.5731066465377808 2.1028876304626465 12.087545394897461
Loss :  1.6625397205352783 2.4262099266052246 13.79358959197998
Loss :  1.5754750967025757 2.6351706981658936 14.751328468322754
Loss :  1.6452566385269165 1.9982106685638428 11.636310577392578
Loss :  1.6198933124542236 1.9330612421035767 11.285199165344238
Loss :  1.617854118347168 1.9314289093017578 11.274998664855957
Loss :  1.5828804969787598 2.6396145820617676 14.780952453613281
Loss :  1.5976489782333374 2.092738628387451 12.061342239379883
Loss :  1.5959599018096924 2.670720100402832 14.949560165405273
Loss :  1.6587353944778442 2.1528096199035645 12.422782897949219
Loss :  1.6641037464141846 2.5204339027404785 14.26627254486084
Loss :  1.6754194498062134 2.4984652996063232 14.167746543884277
  batch 40 loss: 1.6754194498062134, 2.4984652996063232, 14.167746543884277
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6338121891021729 2.0881335735321045 12.074480056762695
Loss :  1.6206855773925781 1.7969790697097778 10.605581283569336
Loss :  1.6094154119491577 2.4102320671081543 13.660575866699219
Loss :  1.6223962306976318 2.21695613861084 12.70717716217041
Loss :  1.599755883216858 2.2985880374908447 13.092696189880371
Loss :  1.630886435508728 2.307685613632202 13.16931438446045
Loss :  1.6651002168655396 1.8198052644729614 10.764126777648926
Loss :  1.6147738695144653 2.0507755279541016 11.868651390075684
Loss :  1.6799765825271606 2.2269179821014404 12.814566612243652
Loss :  1.6208328008651733 1.913230299949646 11.186984062194824
Loss :  1.6542937755584717 2.2032713890075684 12.67065143585205
Loss :  1.6472724676132202 2.4656014442443848 13.975278854370117
Loss :  1.625145435333252 2.5280544757843018 14.265417098999023
Loss :  1.6559960842132568 2.1278417110443115 12.295205116271973
Loss :  1.6124293804168701 2.210860252380371 12.666730880737305
Loss :  1.6786625385284424 2.2040152549743652 12.698739051818848
Loss :  1.6212162971496582 2.190690755844116 12.574670791625977
Loss :  1.6048731803894043 1.914907455444336 11.179410934448242
Loss :  1.621361255645752 2.613931894302368 14.691020965576172
Loss :  1.6868922710418701 2.2015273571014404 12.69452953338623
  batch 60 loss: 1.6868922710418701, 2.2015273571014404, 12.69452953338623
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6066045761108398 2.3311827182769775 13.262517929077148
Loss :  1.6366102695465088 2.6363728046417236 14.818473815917969
Loss :  1.6157951354980469 2.0177297592163086 11.70444393157959
Loss :  1.603909969329834 2.543278217315674 14.320301055908203
Loss :  1.5828652381896973 1.9694170951843262 11.429950714111328
Loss :  1.6107922792434692 4.205380916595459 22.637697219848633
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6210182905197144 4.217306613922119 22.707550048828125
Loss :  1.6190217733383179 4.041956901550293 21.828807830810547
Loss :  1.6318080425262451 4.032392501831055 21.79376983642578
Total LOSS train 12.94874697465163 valid 22.24195623397827
CE LOSS train 1.6301546775377713 valid 0.4079520106315613
Contrastive LOSS train 2.2637184601563676 valid 1.0080981254577637
EPOCH 182:
Loss :  1.6504219770431519 2.138897657394409 12.344910621643066
Loss :  1.6640881299972534 2.477787971496582 14.053028106689453
Loss :  1.6317681074142456 2.6572554111480713 14.918045043945312
Loss :  1.6380623579025269 1.9989607334136963 11.632865905761719
Loss :  1.6559139490127563 1.92505943775177 11.281211853027344
Loss :  1.6187399625778198 1.914041519165039 11.188947677612305
Loss :  1.6555185317993164 2.392841100692749 13.61972427368164
Loss :  1.6297622919082642 2.669567823410034 14.977602005004883
Loss :  1.6200190782546997 2.6419191360473633 14.829614639282227
Loss :  1.6553765535354614 2.1704304218292236 12.507528305053711
Loss :  1.6104615926742554 2.265795946121216 12.939440727233887
Loss :  1.6092780828475952 2.708263874053955 15.150596618652344
Loss :  1.6068273782730103 2.711901903152466 15.166337013244629
Loss :  1.610906720161438 1.999889612197876 11.61035442352295
Loss :  1.669840931892395 1.9865864515304565 11.60277271270752
Loss :  1.6650028228759766 2.396597385406494 13.647989273071289
Loss :  1.60345458984375 2.4642271995544434 13.924591064453125
Loss :  1.631330966949463 2.3232502937316895 13.247581481933594
Loss :  1.600643277168274 2.208409070968628 12.642688751220703
Loss :  1.6648274660110474 2.0468344688415527 11.89900016784668
  batch 20 loss: 1.6648274660110474, 2.0468344688415527, 11.89900016784668
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.626306176185608 1.928401231765747 11.268312454223633
Loss :  1.5996567010879517 2.156749725341797 12.383405685424805
Loss :  1.6184535026550293 2.16496205329895 12.44326400756836
Loss :  1.634927749633789 2.1061084270477295 12.165470123291016
Loss :  1.6649389266967773 2.3828139305114746 13.579009056091309
Loss :  1.620644211769104 2.2114834785461426 12.678061485290527
Loss :  1.6311640739440918 2.277632236480713 13.019325256347656
Loss :  1.6241858005523682 2.1342246532440186 12.295309066772461
Loss :  1.5707684755325317 2.429245710372925 13.716997146606445
Loss :  1.659699559211731 2.4224114418029785 13.771756172180176
Loss :  1.5728763341903687 2.3015358448028564 13.080554962158203
Loss :  1.6440620422363281 2.681523084640503 15.051677703857422
Loss :  1.61956787109375 2.1244659423828125 12.241897583007812
Loss :  1.6178927421569824 2.0903608798980713 12.069696426391602
Loss :  1.5819593667984009 2.2006003856658936 12.584961891174316
Loss :  1.5975227355957031 1.9823285341262817 11.50916576385498
Loss :  1.5963351726531982 2.0742549896240234 11.967610359191895
Loss :  1.6588796377182007 2.3419299125671387 13.368529319763184
Loss :  1.6643179655075073 1.9907475709915161 11.618056297302246
Loss :  1.67445969581604 1.9687057733535767 11.517988204956055
  batch 40 loss: 1.67445969581604, 1.9687057733535767, 11.517988204956055
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6318628787994385 1.9707658290863037 11.485692024230957
Loss :  1.6179643869400024 1.5989937782287598 9.612933158874512
Loss :  1.6067173480987549 1.9115849733352661 11.164642333984375
Loss :  1.6196855306625366 1.8731482028961182 10.98542594909668
Loss :  1.5970478057861328 2.1088738441467285 12.141416549682617
Loss :  1.6291980743408203 2.1666548252105713 12.462471961975098
Loss :  1.6635255813598633 1.9931057691574097 11.629054069519043
Loss :  1.6137428283691406 2.499887228012085 14.113179206848145
Loss :  1.6789311170578003 2.543360948562622 14.395735740661621
Loss :  1.620863676071167 1.8877660036087036 11.059694290161133
Loss :  1.6556519269943237 2.0776865482330322 12.044084548950195
Loss :  1.6487364768981934 1.9748404026031494 11.522937774658203
Loss :  1.62608802318573 2.4722278118133545 13.987227439880371
Loss :  1.656055212020874 1.934334397315979 11.327727317810059
Loss :  1.6136943101882935 1.926590085029602 11.246644973754883
Loss :  1.6787055730819702 2.4902918338775635 14.13016414642334
Loss :  1.6214704513549805 2.267366886138916 12.958304405212402
Loss :  1.6055850982666016 1.8622498512268066 10.916833877563477
Loss :  1.6219325065612793 2.9803011417388916 16.5234375
Loss :  1.6869829893112183 1.8843255043029785 11.108610153198242
  batch 60 loss: 1.6869829893112183, 1.8843255043029785, 11.108610153198242
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6065211296081543 2.2453393936157227 12.83321762084961
Loss :  1.636155605316162 2.151749849319458 12.394905090332031
Loss :  1.6161996126174927 1.68130362033844 10.022717475891113
Loss :  1.603643774986267 2.3896267414093018 13.551776885986328
Loss :  1.5831934213638306 1.517824411392212 9.17231559753418
Loss :  1.58737051486969 4.318759441375732 23.181167602539062
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.5996880531311035 4.282681941986084 23.013097763061523
Loss :  1.596628189086914 4.180629253387451 22.499774932861328
Loss :  1.6032264232635498 4.102834224700928 22.11739730834961
Total LOSS train 12.589338918832633 valid 22.70285940170288
CE LOSS train 1.6294002899756799 valid 0.40080660581588745
Contrastive LOSS train 2.191987740076505 valid 1.025708556175232
EPOCH 183:
Loss :  1.6505239009857178 2.2215280532836914 12.758164405822754
Loss :  1.663382887840271 2.205369710922241 12.690231323242188
Loss :  1.631880283355713 2.284799098968506 13.055875778198242
Loss :  1.638044834136963 2.6708061695098877 14.992074966430664
Loss :  1.655564546585083 1.8269932270050049 10.79053020477295
Loss :  1.618591070175171 2.257465124130249 12.905917167663574
Loss :  1.655452847480774 1.9454389810562134 11.382647514343262
Loss :  1.6303914785385132 1.7081656455993652 10.171219825744629
Loss :  1.6210205554962158 2.4731531143188477 13.986785888671875
Loss :  1.6573081016540527 2.1839945316314697 12.577280044555664
Loss :  1.6139812469482422 2.4586451053619385 13.907206535339355
Loss :  1.613956332206726 2.0644726753234863 11.936319351196289
Loss :  1.6111903190612793 1.8614639043807983 10.918510437011719
Loss :  1.6164988279342651 1.987216591835022 11.552581787109375
Loss :  1.6718547344207764 2.231253147125244 12.828120231628418
Loss :  1.6662497520446777 2.5864953994750977 14.598726272583008
Loss :  1.605364203453064 2.6138391494750977 14.674559593200684
Loss :  1.632617473602295 2.5745298862457275 14.505266189575195
Loss :  1.601172924041748 2.2776846885681152 12.98959732055664
Loss :  1.6653791666030884 2.5117647647857666 14.224203109741211
  batch 20 loss: 1.6653791666030884, 2.5117647647857666, 14.224203109741211
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.626977562904358 2.357984781265259 13.416901588439941
Loss :  1.60004723072052 2.4191091060638428 13.695592880249023
Loss :  1.6177126169204712 2.229672431945801 12.766075134277344
Loss :  1.6341066360473633 2.107060432434082 12.169408798217773
Loss :  1.6649137735366821 2.438641309738159 13.858120918273926
Loss :  1.6211386919021606 2.209909200668335 12.670684814453125
Loss :  1.6318186521530151 2.1742918491363525 12.503277778625488
Loss :  1.6254574060440063 2.158768892288208 12.419301986694336
Loss :  1.5729387998580933 2.6781015396118164 14.963446617126465
Loss :  1.6619254350662231 2.383624792098999 13.580049514770508
Loss :  1.575593113899231 2.148845672607422 12.31982135772705
Loss :  1.6454099416732788 2.0944554805755615 12.117687225341797
Loss :  1.6208871603012085 2.144543409347534 12.34360408782959
Loss :  1.6193820238113403 2.223432779312134 12.73654556274414
Loss :  1.5841058492660522 2.4758219718933105 13.963215827941895
Loss :  1.5987526178359985 2.325739622116089 13.22745132446289
Loss :  1.5981289148330688 2.6022708415985107 14.60948371887207
Loss :  1.659827709197998 2.4832494258880615 14.076074600219727
Loss :  1.6668819189071655 2.5173325538635254 14.253543853759766
Loss :  1.6779099702835083 2.734879970550537 15.352310180664062
  batch 40 loss: 1.6779099702835083, 2.734879970550537, 15.352310180664062
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.6359246969223022 2.676978588104248 15.020817756652832
Loss :  1.6219017505645752 2.6930673122406006 15.087238311767578
Loss :  1.6094424724578857 2.9253695011138916 16.236289978027344
Loss :  1.6229387521743774 2.122655153274536 12.236214637756348
Loss :  1.5989916324615479 1.8646037578582764 10.92201042175293
Loss :  1.6297999620437622 2.518460512161255 14.222102165222168
Loss :  1.6630185842514038 2.5656068325042725 14.491052627563477
Loss :  1.6139259338378906 2.7167534828186035 15.19769287109375
Loss :  1.677459478378296 2.883223056793213 16.09357452392578
Loss :  1.6185519695281982 2.055548906326294 11.896296501159668
Loss :  1.65326726436615 2.263178586959839 12.969160079956055
Loss :  1.6469752788543701 2.5385172367095947 14.339561462402344
Loss :  1.625329852104187 2.0660665035247803 11.955662727355957
Loss :  1.6560566425323486 2.5650949478149414 14.481531143188477
Loss :  1.6144344806671143 2.5090677738189697 14.159772872924805
Loss :  1.6795940399169922 1.9379386901855469 11.369287490844727
Loss :  1.622158169746399 2.3131983280181885 13.188149452209473
Loss :  1.6049317121505737 2.1333250999450684 12.271557807922363
Loss :  1.6207606792449951 2.8759069442749023 16.000295639038086
Loss :  1.6859461069107056 2.0305919647216797 11.838906288146973
  batch 60 loss: 1.6859461069107056, 2.0305919647216797, 11.838906288146973
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.6037898063659668 2.3514275550842285 13.36092758178711
Loss :  1.6339164972305298 2.09244966506958 12.09616470336914
Loss :  1.6129392385482788 1.7908778190612793 10.567328453063965
Loss :  1.6003695726394653 2.2293877601623535 12.747307777404785
Loss :  1.5793912410736084 2.401176691055298 13.585274696350098
Loss :  1.6449452638626099 4.1312127113342285 22.301008224487305
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.6522321701049805 4.1112518310546875 22.208492279052734
Loss :  1.6580373048782349 4.10066556930542 22.161365509033203
Loss :  1.6328983306884766 4.0406599044799805 21.836198806762695
Total LOSS train 13.243424518291766 valid 22.126766204833984
CE LOSS train 1.630094728103051 valid 0.40822458267211914
Contrastive LOSS train 2.322665964640104 valid 1.0101649761199951
EPOCH 184:
Loss :  1.648480772972107 1.9333443641662598 11.315201759338379
Loss :  1.6619259119033813 2.1692962646484375 12.508407592773438
Loss :  1.6293340921401978 1.8868647813796997 11.063658714294434
Loss :  1.6357101202011108 2.5000510215759277 14.135965347290039
Loss :  1.6540980339050293 2.009455919265747 11.701377868652344
Loss :  1.6164438724517822 2.110952138900757 12.171204566955566
Loss :  1.6543782949447632 2.4214112758636475 13.761434555053711
Loss :  1.6287323236465454 2.1624677181243896 12.441071510314941
Loss :  1.6196377277374268 1.9585273265838623 11.412274360656738
Loss :  1.6560735702514648 1.8960906267166138 11.136527061462402
Loss :  1.6097948551177979 2.092367172241211 12.071630477905273
Loss :  1.6087130308151245 2.7804718017578125 15.511072158813477
Loss :  1.6061484813690186 2.1840405464172363 12.526350975036621
Loss :  1.6111376285552979 2.2437026500701904 12.82965087890625
Loss :  1.6698627471923828 2.3442394733428955 13.391059875488281
Loss :  1.6649333238601685 2.502429485321045 14.177081108093262
Loss :  1.6033611297607422 2.2062714099884033 12.63471794128418
Loss :  1.6324410438537598 2.072988271713257 11.997383117675781
Loss :  1.5998125076293945 2.4460489749908447 13.830057144165039
Loss :  1.6646969318389893 2.2125847339630127 12.727620124816895
  batch 20 loss: 1.6646969318389893, 2.2125847339630127, 12.727620124816895
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.625653624534607 2.858809232711792 15.919699668884277
Loss :  1.5987436771392822 3.156141757965088 17.379451751708984
Loss :  1.6165595054626465 2.443283796310425 13.832979202270508
Loss :  1.6318496465682983 2.002242088317871 11.643059730529785
Loss :  1.6633899211883545 2.1879723072052 12.603251457214355
Loss :  1.6184985637664795 2.2146036624908447 12.691516876220703
Loss :  1.6297180652618408 1.9874670505523682 11.567052841186523
Loss :  1.6230833530426025 2.3794450759887695 13.520308494567871
Loss :  1.5687031745910645 3.612509250640869 19.631248474121094
Loss :  1.6603071689605713 4.108806610107422 22.2043399810791
Loss :  1.5701316595077515 2.3322057723999023 13.231160163879395
Loss :  1.642564296722412 2.289064884185791 13.087888717651367
Loss :  1.617016315460205 2.214491605758667 12.689474105834961
Loss :  1.6147257089614868 2.5970816612243652 14.60013484954834
Loss :  1.5778696537017822 3.1723554134368896 17.439647674560547
Loss :  1.5935152769088745 2.696418046951294 15.075605392456055
Loss :  1.592404842376709 2.027285575866699 11.728832244873047
Loss :  1.6578785181045532 2.5533080101013184 14.424419403076172
Loss :  1.6638671159744263 2.535473585128784 14.341235160827637
Loss :  1.6747020483016968 2.197944164276123 12.664422988891602
  batch 40 loss: 1.6747020483016968, 2.197944164276123, 12.664422988891602
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.6304234266281128 2.4834847450256348 14.047846794128418
Loss :  1.6160672903060913 3.5519397258758545 19.37576675415039
Loss :  1.6043366193771362 3.285174608230591 18.030208587646484
Loss :  1.6178181171417236 3.3885600566864014 18.560619354248047
Loss :  1.5946251153945923 2.9079477787017822 16.134363174438477
Loss :  1.6261088848114014 2.7171831130981445 15.212024688720703
Loss :  1.6620612144470215 2.4830946922302246 14.077535629272461
Loss :  1.6116775274276733 2.3779001235961914 13.501177787780762
Loss :  1.6774134635925293 3.68353009223938 20.095064163208008
Loss :  1.6193773746490479 1.7874557971954346 10.556655883789062
Loss :  1.654754877090454 2.4757943153381348 14.03372573852539
Loss :  1.6471004486083984 2.2670419216156006 12.98231029510498
Loss :  1.6247813701629639 3.057307481765747 16.911317825317383
Loss :  1.655160665512085 3.1028826236724854 17.169572830200195
Loss :  1.6123871803283691 2.968432664871216 16.45454978942871
Loss :  1.6785826683044434 2.1109657287597656 12.23341178894043
Loss :  1.619532823562622 2.989551067352295 16.567289352416992
Loss :  1.6023763418197632 2.440441608428955 13.804583549499512
Loss :  1.6186723709106445 2.269561529159546 12.966480255126953
Loss :  1.6859945058822632 1.8054248094558716 10.713118553161621
  batch 60 loss: 1.6859945058822632, 1.8054248094558716, 10.713118553161621
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4], device='cuda:0')
Loss :  1.604030966758728 2.3568902015686035 13.388481140136719
Loss :  1.6340105533599854 2.155860424041748 12.413312911987305
Loss :  1.6134775876998901 2.127239942550659 12.249677658081055
Loss :  1.6016700267791748 2.9423539638519287 16.313440322875977
Loss :  1.5803449153900146 1.8459924459457397 10.810307502746582
Loss :  1.6159253120422363 4.19711446762085 22.601497650146484
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.6273341178894043 4.227602481842041 22.76534652709961
Loss :  1.622610092163086 4.063545227050781 21.940336227416992
Loss :  1.6429721117019653 3.9683547019958496 21.4847469329834
Total LOSS train 14.034189517681416 valid 22.19798183441162
CE LOSS train 1.627841305732727 valid 0.41074302792549133
Contrastive LOSS train 2.481269645690918 valid 0.9920886754989624
EPOCH 185:
Loss :  1.6491676568984985 2.361182689666748 13.455081939697266
Loss :  1.6624122858047485 2.3189923763275146 13.25737476348877
Loss :  1.6287304162979126 1.8071223497390747 10.664342880249023
Loss :  1.6346608400344849 2.0019001960754395 11.644161224365234
Loss :  1.6525710821151733 1.913152813911438 11.218335151672363
Loss :  1.6141585111618042 2.3075788021087646 13.152052879333496
Loss :  1.6520811319351196 2.904717445373535 16.175668716430664
Loss :  1.6257187128067017 2.4529662132263184 13.89055061340332
Loss :  1.6157699823379517 2.1281325817108154 12.256433486938477
Loss :  1.65238356590271 2.1916086673736572 12.610426902770996
Loss :  1.6062827110290527 3.186411142349243 17.53833770751953
Loss :  1.606010913848877 2.461940050125122 13.91571044921875
Loss :  1.603256344795227 3.0479257106781006 16.842885971069336
Loss :  1.6091736555099487 2.6804769039154053 15.011558532714844
Loss :  1.6694694757461548 3.4024205207824707 18.68157196044922
Loss :  1.662437081336975 2.387336254119873 13.599119186401367
Loss :  1.6001063585281372 2.4894001483917236 14.047106742858887
Loss :  1.6279377937316895 2.239128828048706 12.82358169555664
Loss :  1.5946826934814453 2.974937677383423 16.469371795654297
Loss :  1.6603050231933594 2.2551345825195312 12.935977935791016
  batch 20 loss: 1.6603050231933594, 2.2551345825195312, 12.935977935791016
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6203057765960693 2.359570026397705 13.418155670166016
Loss :  1.5929149389266968 2.7597312927246094 15.391571044921875
Loss :  1.6133536100387573 2.1851272583007812 12.538990020751953
Loss :  1.6298365592956543 2.5808229446411133 14.533950805664062
Loss :  1.662381887435913 2.3368289470672607 13.346527099609375
Loss :  1.6177138090133667 2.22589111328125 12.747169494628906
Loss :  1.628068208694458 2.5666329860687256 14.461233139038086
Loss :  1.6227360963821411 2.192521333694458 12.585342407226562
Loss :  1.5674831867218018 2.711387872695923 15.124422073364258
Loss :  1.6587268114089966 2.735706090927124 15.337257385253906
Loss :  1.5692557096481323 2.2716760635375977 12.92763614654541
Loss :  1.641445517539978 1.9971339702606201 11.627115249633789
Loss :  1.6150673627853394 2.1007304191589355 12.118720054626465
Loss :  1.613083839416504 2.123811960220337 12.23214340209961
Loss :  1.5761713981628418 2.291431427001953 13.033329010009766
Loss :  1.591792345046997 2.0874269008636475 12.028926849365234
Loss :  1.5901048183441162 3.3115577697753906 18.14789390563965
Loss :  1.655196189880371 2.166435718536377 12.487374305725098
Loss :  1.6597086191177368 3.05707049369812 16.94506072998047
Loss :  1.6712390184402466 1.8101516962051392 10.721997261047363
  batch 40 loss: 1.6712390184402466, 1.8101516962051392, 10.721997261047363
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6284064054489136 2.525681495666504 14.256814002990723
Loss :  1.6148823499679565 2.639552116394043 14.812643051147461
Loss :  1.6041524410247803 3.0432140827178955 16.820222854614258
Loss :  1.6182211637496948 2.2821028232574463 13.028735160827637
Loss :  1.5950117111206055 2.075251579284668 11.971269607543945
Loss :  1.6265416145324707 2.6809353828430176 15.031217575073242
Loss :  1.6604474782943726 3.6283109188079834 19.802001953125
Loss :  1.6103979349136353 2.7028307914733887 15.124552726745605
Loss :  1.6757361888885498 2.4320321083068848 13.835896492004395
Loss :  1.6164637804031372 2.086652994155884 12.049728393554688
Loss :  1.651867389678955 1.9883034229278564 11.5933837890625
Loss :  1.6460493803024292 2.225071430206299 12.771406173706055
Loss :  1.623967170715332 2.2956204414367676 13.102068901062012
Loss :  1.6549293994903564 2.9293813705444336 16.301836013793945
Loss :  1.6124211549758911 2.748828887939453 15.356565475463867
Loss :  1.6774789094924927 2.44242787361145 13.889617919921875
Loss :  1.6192667484283447 3.0710551738739014 16.97454261779785
Loss :  1.6031419038772583 2.5170981884002686 14.18863296508789
Loss :  1.6200000047683716 2.8009045124053955 15.62452220916748
Loss :  1.6876901388168335 2.672934055328369 15.052359580993652
  batch 60 loss: 1.6876901388168335, 2.672934055328369, 15.052359580993652
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.6056301593780518 2.132089376449585 12.266077041625977
Loss :  1.6355316638946533 3.137169599533081 17.321380615234375
Loss :  1.614890456199646 1.905266284942627 11.141221046447754
Loss :  1.6017810106277466 2.1867315769195557 12.535438537597656
Loss :  1.5807418823242188 1.7041215896606445 10.101349830627441
Loss :  1.7075551748275757 4.41057825088501 23.760446548461914
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.7060710191726685 4.477401256561279 24.09307861328125
Loss :  1.7012760639190674 4.29123067855835 23.15743064880371
Loss :  1.740432620048523 4.057109355926514 22.02597999572754
Total LOSS train 13.952276171170748 valid 23.259233951568604
CE LOSS train 1.6263007750877967 valid 0.43510815501213074
Contrastive LOSS train 2.465195081784175 valid 1.0142773389816284
EPOCH 186:
Loss :  1.6491062641143799 2.2392561435699463 12.845386505126953
Loss :  1.6619657278060913 2.2831802368164062 13.077866554260254
Loss :  1.629690170288086 2.572751045227051 14.49344539642334
Loss :  1.636248230934143 2.5120370388031006 14.196434020996094
Loss :  1.6546179056167603 1.940700888633728 11.358122825622559
Loss :  1.6176154613494873 3.0479533672332764 16.85738182067871
Loss :  1.6557989120483398 3.901815176010132 21.164875030517578
Loss :  1.6302000284194946 3.8603289127349854 20.93184471130371
Loss :  1.6207246780395508 3.352522850036621 18.383338928222656
Loss :  1.6569889783859253 1.9734656810760498 11.524317741394043
Loss :  1.611190915107727 2.479686975479126 14.009625434875488
Loss :  1.6100844144821167 2.4966444969177246 14.093307495117188
Loss :  1.606389045715332 2.824976682662964 15.73127269744873
Loss :  1.611161708831787 2.57065486907959 14.464435577392578
Loss :  1.6707838773727417 3.479431390762329 19.067941665649414
Loss :  1.6661311388015747 2.81774640083313 15.754862785339355
Loss :  1.604278802871704 2.586479663848877 14.536676406860352
Loss :  1.6318737268447876 3.259418487548828 17.928966522216797
Loss :  1.5993717908859253 3.119940996170044 17.19907569885254
Loss :  1.6644809246063232 2.1787261962890625 12.558112144470215
  batch 20 loss: 1.6644809246063232, 2.1787261962890625, 12.558112144470215
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.623572826385498 2.224747896194458 12.747312545776367
Loss :  1.5956902503967285 2.6940383911132812 15.065881729125977
Loss :  1.614309549331665 4.173252105712891 22.48056983947754
Loss :  1.6307612657546997 3.7989797592163086 20.625661849975586
Loss :  1.66197669506073 2.513579845428467 14.229876518249512
Loss :  1.6170047521591187 2.596325397491455 14.598630905151367
Loss :  1.6278246641159058 2.9578440189361572 16.417043685913086
Loss :  1.6203582286834717 2.5526182651519775 14.38344955444336
Loss :  1.565891146659851 3.339390516281128 18.26284408569336
Loss :  1.6564089059829712 2.782649278640747 15.569655418395996
Loss :  1.5672634840011597 3.4319043159484863 18.726783752441406
Loss :  1.6401680707931519 2.550598382949829 14.393159866333008
Loss :  1.6140520572662354 2.4166553020477295 13.697328567504883
Loss :  1.612129807472229 2.215036153793335 12.687311172485352
Loss :  1.5761924982070923 2.895836114883423 16.05537223815918
Loss :  1.59292471408844 2.015866994857788 11.672260284423828
Loss :  1.5920166969299316 2.7424278259277344 15.304155349731445
Loss :  1.6584054231643677 1.9306795597076416 11.311802864074707
Loss :  1.66361665725708 1.842659831047058 10.876914978027344
Loss :  1.6740739345550537 2.016751766204834 11.757833480834961
  batch 40 loss: 1.6740739345550537, 2.016751766204834, 11.757833480834961
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.63105309009552 2.041358709335327 11.837846755981445
Loss :  1.6172661781311035 1.789971113204956 10.567121505737305
Loss :  1.6042746305465698 1.9853527545928955 11.531038284301758
Loss :  1.6176894903182983 3.498897075653076 19.11217498779297
Loss :  1.5936784744262695 2.451691150665283 13.852133750915527
Loss :  1.6250004768371582 2.485121488571167 14.050607681274414
Loss :  1.6594374179840088 3.093398332595825 17.126428604125977
Loss :  1.608215570449829 2.020745038986206 11.71194076538086
Loss :  1.673711895942688 2.2850420475006104 13.098921775817871
Loss :  1.6150285005569458 2.116220712661743 12.196131706237793
Loss :  1.6497113704681396 2.6664798259735107 14.982110977172852
Loss :  1.6439577341079712 3.1578528881073 17.4332218170166
Loss :  1.6217577457427979 2.6217188835144043 14.730352401733398
Loss :  1.653409719467163 2.7753121852874756 15.5299711227417
Loss :  1.6099746227264404 2.9982223510742188 16.601085662841797
Loss :  1.675528883934021 2.367244243621826 13.511750221252441
Loss :  1.6180009841918945 2.688460111618042 15.060301780700684
Loss :  1.600904107093811 2.568920850753784 14.44550895690918
Loss :  1.6182401180267334 2.8282392024993896 15.75943660736084
Loss :  1.6852158308029175 2.421905994415283 13.794745445251465
  batch 60 loss: 1.6852158308029175, 2.421905994415283, 13.794745445251465
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.602017879486084 2.2504379749298096 12.854207992553711
Loss :  1.631868600845337 3.643846035003662 19.851099014282227
Loss :  1.6099185943603516 3.354236364364624 18.381099700927734
Loss :  1.5975645780563354 2.0919229984283447 12.05717945098877
Loss :  1.5767053365707397 1.8695945739746094 10.924677848815918
Loss :  1.639105200767517 3.967872381210327 21.478466033935547
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 5], device='cuda:0')
Loss :  1.6488600969314575 3.9355242252349854 21.32648277282715
Loss :  1.6500985622406006 3.9029667377471924 21.164932250976562
Loss :  1.6440013647079468 3.7057416439056396 20.17270851135254
Total LOSS train 14.954495899493878 valid 21.03564739227295
CE LOSS train 1.6266688640301044 valid 0.4110003411769867
Contrastive LOSS train 2.6655654173630934 valid 0.9264354109764099
EPOCH 187:
Loss :  1.645933985710144 2.1850051879882812 12.57096004486084
Loss :  1.6597539186477661 2.228635311126709 12.80293083190918
Loss :  1.6272087097167969 1.8757883310317993 11.006150245666504
Loss :  1.6333874464035034 2.3574764728546143 13.420769691467285
Loss :  1.6526256799697876 2.6000070571899414 14.652661323547363
Loss :  1.613401174545288 2.5745251178741455 14.486026763916016
Loss :  1.6525485515594482 2.1860954761505127 12.583025932312012
Loss :  1.6263844966888428 2.0094704627990723 11.673736572265625
Loss :  1.616016149520874 1.7975319623947144 10.603675842285156
Loss :  1.652469515800476 2.653855562210083 14.921747207641602
Loss :  1.6060750484466553 2.816047430038452 15.686312675476074
Loss :  1.6061547994613647 2.6054954528808594 14.633631706237793
Loss :  1.6029688119888306 2.3996613025665283 13.601275444030762
Loss :  1.608198881149292 2.505981922149658 14.138108253479004
Loss :  1.667313575744629 2.7066686153411865 15.20065689086914
Loss :  1.6628515720367432 2.2983031272888184 13.154367446899414
Loss :  1.6000393629074097 2.173635721206665 12.468217849731445
Loss :  1.6292093992233276 2.513950824737549 14.198963165283203
Loss :  1.596742868423462 2.4727842807769775 13.960663795471191
Loss :  1.6628855466842651 2.66998553276062 15.012813568115234
  batch 20 loss: 1.6628855466842651, 2.66998553276062, 15.012813568115234
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.623681664466858 2.809753656387329 15.672450065612793
Loss :  1.5975532531738281 2.1788105964660645 12.491605758666992
Loss :  1.6171815395355225 1.8157150745391846 10.695756912231445
Loss :  1.6336334943771362 1.9009363651275635 11.138315200805664
Loss :  1.66423499584198 2.1574442386627197 12.451456069946289
Loss :  1.6204851865768433 2.539499044418335 14.317980766296387
Loss :  1.6316406726837158 2.2853991985321045 13.058636665344238
Loss :  1.6255030632019043 2.4222493171691895 13.736749649047852
Loss :  1.5722593069076538 2.0475451946258545 11.809985160827637
Loss :  1.661543607711792 2.4743025302886963 14.033056259155273
Loss :  1.5737714767456055 2.7467041015625 15.307291984558105
Loss :  1.6450588703155518 2.303313732147217 13.161627769470215
Loss :  1.6193209886550903 1.805595874786377 10.647299766540527
Loss :  1.6172468662261963 1.898488163948059 11.109687805175781
Loss :  1.5810478925704956 2.1280338764190674 12.221217155456543
Loss :  1.5958014726638794 2.3781089782714844 13.486346244812012
Loss :  1.59446382522583 3.115058183670044 17.169754028320312
Loss :  1.6587342023849487 3.11861252784729 17.25179672241211
Loss :  1.6644415855407715 2.525383949279785 14.291360855102539
Loss :  1.6754357814788818 2.386254072189331 13.606706619262695
  batch 40 loss: 1.6754357814788818, 2.386254072189331, 13.606706619262695
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.6330323219299316 2.5118215084075928 14.192140579223633
Loss :  1.6196277141571045 2.0210604667663574 11.724930763244629
Loss :  1.6082853078842163 2.5330276489257812 14.273423194885254
Loss :  1.6220178604125977 2.82822585105896 15.763147354125977
Loss :  1.5994294881820679 1.8183544874191284 10.691201210021973
Loss :  1.6314455270767212 2.14854097366333 12.374150276184082
Loss :  1.6654396057128906 2.8629150390625 15.98001480102539
Loss :  1.6162002086639404 2.187771797180176 12.555059432983398
Loss :  1.6795997619628906 2.6279783248901367 14.819491386413574
Loss :  1.6221116781234741 2.589425802230835 14.56924057006836
Loss :  1.6567133665084839 2.972083806991577 16.517131805419922
Loss :  1.64933443069458 2.6900479793548584 15.09957504272461
Loss :  1.6270228624343872 2.4719438552856445 13.98674201965332
Loss :  1.656400203704834 2.32867431640625 13.299772262573242
Loss :  1.6138840913772583 2.2755532264709473 12.991649627685547
Loss :  1.6789095401763916 2.425180196762085 13.804810523986816
Loss :  1.620971918106079 2.6428520679473877 14.83523178100586
Loss :  1.6033835411071777 2.452805280685425 13.867410659790039
Loss :  1.6205805540084839 2.378840446472168 13.514782905578613
Loss :  1.686033844947815 2.6395716667175293 14.883893013000488
  batch 60 loss: 1.686033844947815, 2.6395716667175293, 14.883893013000488
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4], device='cuda:0')
Loss :  1.6043163537979126 2.17375111579895 12.473072052001953
Loss :  1.6348748207092285 2.3306326866149902 13.28803825378418
Loss :  1.6145621538162231 2.180058240890503 12.514853477478027
Loss :  1.6029820442199707 2.377075433731079 13.488359451293945
Loss :  1.5822243690490723 2.6223249435424805 14.693849563598633
Loss :  1.6256427764892578 3.591916799545288 19.58522605895996
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6357053518295288 3.589613199234009 19.583772659301758
Loss :  1.6311713457107544 3.5339555740356445 19.300949096679688
Loss :  1.6492688655853271 3.365525484085083 18.476896286010742
Total LOSS train 13.609811518742488 valid 19.236711025238037
CE LOSS train 1.6283783509181096 valid 0.4123172163963318
Contrastive LOSS train 2.396286630630493 valid 0.8413813710212708
EPOCH 188:
Loss :  1.6490331888198853 1.9656258821487427 11.477163314819336
Loss :  1.6630818843841553 2.3033499717712402 13.179832458496094
Loss :  1.6301279067993164 1.5396438837051392 9.328347206115723
Loss :  1.6367415189743042 2.6446940898895264 14.860212326049805
Loss :  1.6553089618682861 2.52185320854187 14.264575004577637
Loss :  1.617063283920288 2.1657164096832275 12.445645332336426
Loss :  1.654175877571106 2.4518842697143555 13.913597106933594
Loss :  1.627774715423584 2.7944982051849365 15.600265502929688
Loss :  1.6180613040924072 1.7468012571334839 10.352066993713379
Loss :  1.6537303924560547 2.6668550968170166 14.988005638122559
Loss :  1.6093287467956543 2.7560412883758545 15.389535903930664
Loss :  1.609334111213684 1.9406706094741821 11.312686920166016
Loss :  1.606776475906372 2.9298157691955566 16.255855560302734
Loss :  1.6118131875991821 2.595714569091797 14.590386390686035
Loss :  1.6710630655288696 2.8286399841308594 15.814263343811035
Loss :  1.6649819612503052 2.5489206314086914 14.409584999084473
Loss :  1.603750467300415 2.3387298583984375 13.297399520874023
Loss :  1.6296708583831787 2.0516791343688965 11.888067245483398
Loss :  1.5989829301834106 2.5860166549682617 14.52906608581543
Loss :  1.6623740196228027 2.3501319885253906 13.413034439086914
  batch 20 loss: 1.6623740196228027, 2.3501319885253906, 13.413034439086914
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6244970560073853 1.9820715188980103 11.534854888916016
Loss :  1.598144769668579 2.514784574508667 14.172067642211914
Loss :  1.6167082786560059 1.7412128448486328 10.322772979736328
Loss :  1.6330592632293701 2.103468894958496 12.15040397644043
Loss :  1.6641229391098022 2.3009748458862305 13.168996810913086
Loss :  1.6194283962249756 2.6034743785858154 14.636800765991211
Loss :  1.63014817237854 2.7296254634857178 15.278275489807129
Loss :  1.6238280534744263 2.633807897567749 14.792867660522461
Loss :  1.571028232574463 2.8775532245635986 15.958793640136719
Loss :  1.6590484380722046 2.5120091438293457 14.219093322753906
Loss :  1.572592854499817 2.606102466583252 14.603104591369629
Loss :  1.6435039043426514 2.5531599521636963 14.409303665161133
Loss :  1.6181153059005737 2.593921661376953 14.587723731994629
Loss :  1.6168867349624634 2.2774078845977783 13.003926277160645
Loss :  1.5823242664337158 2.850527048110962 15.834959030151367
Loss :  1.5972784757614136 2.475477457046509 13.974665641784668
Loss :  1.5963807106018066 2.8455071449279785 15.823915481567383
Loss :  1.6583360433578491 2.145766258239746 12.387166976928711
Loss :  1.6634771823883057 2.250929355621338 12.918123245239258
Loss :  1.6738868951797485 2.634162664413452 14.844700813293457
  batch 40 loss: 1.6738868951797485, 2.634162664413452, 14.844700813293457
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.631759524345398 2.633625030517578 14.799884796142578
Loss :  1.6171940565109253 2.2622570991516113 12.928479194641113
Loss :  1.6063292026519775 2.0854549407958984 12.03360366821289
Loss :  1.6199941635131836 2.009695291519165 11.66847038269043
Loss :  1.5963423252105713 2.7465527057647705 15.329105377197266
Loss :  1.6287907361984253 2.255659818649292 12.907090187072754
Loss :  1.6623141765594482 2.01704478263855 11.747538566589355
Loss :  1.6124976873397827 2.8848180770874023 16.036588668823242
Loss :  1.676594853401184 2.8628430366516113 15.990809440612793
Loss :  1.6185414791107178 2.1217432022094727 12.22725772857666
Loss :  1.6532663106918335 2.5650033950805664 14.478282928466797
Loss :  1.6464003324508667 2.2127559185028076 12.710180282592773
Loss :  1.624734878540039 2.143831729888916 12.343893051147461
Loss :  1.6551624536514282 2.3059706687927246 13.185016632080078
Loss :  1.6126594543457031 2.044552803039551 11.835423469543457
Loss :  1.6781774759292603 2.4489691257476807 13.923023223876953
Loss :  1.6207584142684937 3.2268762588500977 17.755138397216797
Loss :  1.603973627090454 3.015810251235962 16.683025360107422
Loss :  1.621054768562317 2.2249672412872314 12.745890617370605
Loss :  1.6872484683990479 1.8126730918884277 10.750614166259766
  batch 60 loss: 1.6872484683990479, 1.8126730918884277, 10.750614166259766
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.606441617012024 3.6888961791992188 20.050922393798828
Loss :  1.635625958442688 1.8594932556152344 10.93309211730957
Loss :  1.6159608364105225 1.9160393476486206 11.196157455444336
Loss :  1.6026170253753662 2.481227159500122 14.008752822875977
Loss :  1.5824397802352905 2.5516436100006104 14.340657234191895
Loss :  1.6151058673858643 4.078001499176025 22.00511360168457
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6279163360595703 4.058263778686523 21.919235229492188
Loss :  1.622787594795227 4.032090187072754 21.78323745727539
Loss :  1.635416865348816 3.735290765762329 20.311870574951172
Total LOSS train 13.73140009366549 valid 21.50486421585083
CE LOSS train 1.6285053913409893 valid 0.408854216337204
Contrastive LOSS train 2.4205789456000697 valid 0.9338226914405823
EPOCH 189:
Loss :  1.650967001914978 3.0414347648620605 16.85814094543457
Loss :  1.6649490594863892 2.8770711421966553 16.050304412841797
Loss :  1.6327005624771118 2.1565821170806885 12.415611267089844
Loss :  1.639017939567566 2.373155355453491 13.50479507446289
Loss :  1.6559375524520874 2.7359938621520996 15.335906982421875
Loss :  1.6197105646133423 2.3506722450256348 13.373071670532227
Loss :  1.6564319133758545 3.3884851932525635 18.598857879638672
Loss :  1.6311864852905273 2.313746929168701 13.199921607971191
Loss :  1.6203114986419678 2.0950634479522705 12.09562873840332
Loss :  1.655372142791748 1.9742804765701294 11.526775360107422
Loss :  1.6111516952514648 2.2796483039855957 13.009392738342285
Loss :  1.6109312772750854 2.201083183288574 12.616347312927246
Loss :  1.607625126838684 2.0256004333496094 11.735627174377441
Loss :  1.612983226776123 1.8961149454116821 11.093557357788086
Loss :  1.6714203357696533 2.533496379852295 14.338902473449707
Loss :  1.666788935661316 2.5044355392456055 14.188966751098633
Loss :  1.6065959930419922 2.697941303253174 15.096302032470703
Loss :  1.6342601776123047 2.992386817932129 16.596195220947266
Loss :  1.6026098728179932 2.6227035522460938 14.716127395629883
Loss :  1.6667146682739258 2.713167905807495 15.23255443572998
  batch 20 loss: 1.6667146682739258, 2.713167905807495, 15.23255443572998
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6268141269683838 2.3215548992156982 13.234588623046875
Loss :  1.6002644300460815 2.516409158706665 14.182310104370117
Loss :  1.618963360786438 2.2526519298553467 12.882223129272461
Loss :  1.6343237161636353 2.3333208560943604 13.300928115844727
Loss :  1.6642950773239136 2.8394484519958496 15.86153793334961
Loss :  1.6212372779846191 1.9795738458633423 11.519105911254883
Loss :  1.632114052772522 2.743769645690918 15.35096263885498
Loss :  1.6260173320770264 2.628887891769409 14.77045726776123
Loss :  1.5728025436401367 2.1265103816986084 12.205354690551758
Loss :  1.6614112854003906 2.4470362663269043 13.89659309387207
Loss :  1.5744317770004272 3.4056830406188965 18.602848052978516
Loss :  1.6444039344787598 2.6431946754455566 14.860376358032227
Loss :  1.6197787523269653 1.9758503437042236 11.499030113220215
Loss :  1.6184495687484741 3.180305004119873 17.519975662231445
Loss :  1.5822956562042236 2.8989574909210205 16.077083587646484
Loss :  1.5983713865280151 2.622344970703125 14.71009635925293
Loss :  1.5973434448242188 1.949571132659912 11.345199584960938
Loss :  1.6611226797103882 2.329848527908325 13.310364723205566
Loss :  1.6660691499710083 2.3121228218078613 13.226682662963867
Loss :  1.6766341924667358 2.230579137802124 12.829529762268066
  batch 40 loss: 1.6766341924667358, 2.230579137802124, 12.829529762268066
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6342297792434692 2.3349480628967285 13.308969497680664
Loss :  1.6208579540252686 2.030120611190796 11.771461486816406
Loss :  1.6088780164718628 2.3729753494262695 13.4737548828125
Loss :  1.6227692365646362 2.1598079204559326 12.421809196472168
Loss :  1.599615216255188 1.950076699256897 11.349998474121094
Loss :  1.6310784816741943 1.9821722507476807 11.541939735412598
Loss :  1.665056824684143 3.048830270767212 16.909208297729492
Loss :  1.6140433549880981 2.0825817584991455 12.026951789855957
Loss :  1.6793066263198853 2.164823293685913 12.503423690795898
Loss :  1.619985580444336 1.9515604972839355 11.377788543701172
Loss :  1.653946876525879 2.083022117614746 12.06905746459961
Loss :  1.6470890045166016 2.5656142234802246 14.475160598754883
Loss :  1.6256014108657837 2.051104784011841 11.881125450134277
Loss :  1.6558680534362793 2.867694854736328 15.994342803955078
Loss :  1.6140998601913452 2.717512607574463 15.201662063598633
Loss :  1.6778309345245361 2.3047170639038086 13.201416015625
Loss :  1.6214089393615723 3.1883840560913086 17.563329696655273
Loss :  1.605221152305603 2.6669769287109375 14.940105438232422
Loss :  1.6216509342193604 2.359886884689331 13.421085357666016
Loss :  1.6868096590042114 2.12185001373291 12.296059608459473
  batch 60 loss: 1.6868096590042114, 2.12185001373291, 12.296059608459473
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6060738563537598 2.0442559719085693 11.827354431152344
Loss :  1.636329174041748 1.9704968929290771 11.488813400268555
Loss :  1.6164348125457764 3.5058979988098145 19.145923614501953
Loss :  1.6064518690109253 2.3878896236419678 13.545900344848633
Loss :  1.5857475996017456 2.133772850036621 12.25461196899414
Loss :  1.6249878406524658 4.181309223175049 22.53153419494629
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.6360995769500732 4.186774253845215 22.569971084594727
Loss :  1.6344140768051147 4.094113349914551 22.10498046875
Loss :  1.6417127847671509 3.869786024093628 20.990642547607422
Total LOSS train 13.82706906245305 valid 22.04928207397461
CE LOSS train 1.6303260766542875 valid 0.4104281961917877
Contrastive LOSS train 2.4393485839550313 valid 0.967446506023407
EPOCH 190:
Loss :  1.6514251232147217 2.25607967376709 12.93182373046875
Loss :  1.6656807661056519 2.536522388458252 14.348292350769043
Loss :  1.6342666149139404 2.7305471897125244 15.287002563476562
Loss :  1.640681266784668 2.3934972286224365 13.60816764831543
Loss :  1.6583406925201416 2.489750385284424 14.107091903686523
Loss :  1.6224313974380493 2.1240363121032715 12.242613792419434
Loss :  1.6582121849060059 2.422044038772583 13.7684326171875
Loss :  1.632986068725586 1.8499579429626465 10.882776260375977
Loss :  1.6229842901229858 1.8997021913528442 11.121495246887207
Loss :  1.6582591533660889 2.16508412361145 12.48367977142334
Loss :  1.6138792037963867 2.5323896408081055 14.275827407836914
Loss :  1.6133495569229126 2.2987310886383057 13.10700511932373
Loss :  1.6109873056411743 2.9413740634918213 16.31785774230957
Loss :  1.6149736642837524 2.126819610595703 12.249072074890137
Loss :  1.6726499795913696 2.020651340484619 11.775906562805176
Loss :  1.6665794849395752 2.62327241897583 14.782940864562988
Loss :  1.6058931350708008 2.0457093715667725 11.834440231323242
Loss :  1.6332353353500366 2.1351428031921387 12.30894947052002
Loss :  1.6016817092895508 1.787619709968567 10.539780616760254
Loss :  1.666755199432373 2.4961395263671875 14.147453308105469
  batch 20 loss: 1.666755199432373, 2.4961395263671875, 14.147453308105469
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6273646354675293 2.436866044998169 13.811695098876953
Loss :  1.6021227836608887 2.2646641731262207 12.925443649291992
Loss :  1.6207938194274902 1.8551000356674194 10.896293640136719
Loss :  1.6355712413787842 3.054701805114746 16.909080505371094
Loss :  1.6663450002670288 2.4598116874694824 13.96540355682373
Loss :  1.623306155204773 2.5668461322784424 14.457536697387695
Loss :  1.6347579956054688 2.2627451419830322 12.94848346710205
Loss :  1.6296055316925049 2.544736385345459 14.353287696838379
Loss :  1.5759811401367188 2.578392267227173 14.467942237854004
Loss :  1.6640015840530396 3.1097261905670166 17.21263313293457
Loss :  1.5773084163665771 3.1027719974517822 17.091167449951172
Loss :  1.6467289924621582 2.244128704071045 12.867372512817383
Loss :  1.621468424797058 2.3059134483337402 13.151036262512207
Loss :  1.620141863822937 2.203474760055542 12.637516021728516
Loss :  1.5851308107376099 2.4018452167510986 13.594356536865234
Loss :  1.6002684831619263 1.9973355531692505 11.586946487426758
Loss :  1.5994954109191895 2.6336638927459717 14.767814636230469
Loss :  1.6630361080169678 2.5376901626586914 14.351487159729004
Loss :  1.6676918268203735 2.589881420135498 14.61709976196289
Loss :  1.6783626079559326 2.902902364730835 16.192874908447266
  batch 40 loss: 1.6783626079559326, 2.902902364730835, 16.192874908447266
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6346246004104614 3.4357705116271973 18.8134765625
Loss :  1.6214935779571533 2.7759909629821777 15.501448631286621
Loss :  1.6095330715179443 2.3726425170898438 13.472745895385742
Loss :  1.6234301328659058 2.6662285327911377 14.954572677612305
Loss :  1.6000237464904785 2.340158700942993 13.300817489624023
Loss :  1.6314915418624878 2.4340250492095947 13.801616668701172
Loss :  1.6656932830810547 1.8953074216842651 11.142230033874512
Loss :  1.6161988973617554 1.7272108793258667 10.252252578735352
Loss :  1.6810383796691895 2.42975115776062 13.829793930053711
Loss :  1.6226060390472412 1.9522401094436646 11.383807182312012
Loss :  1.6563256978988647 3.242626428604126 17.86945915222168
Loss :  1.6496777534484863 2.342576265335083 13.362558364868164
Loss :  1.6278494596481323 1.8750762939453125 11.003231048583984
Loss :  1.6580467224121094 2.1319870948791504 12.317981719970703
Loss :  1.6157677173614502 2.4972469806671143 14.102002143859863
Loss :  1.6794781684875488 2.17636775970459 12.561317443847656
Loss :  1.6228256225585938 2.7469117641448975 15.35738468170166
Loss :  1.6068272590637207 2.0936319828033447 12.074987411499023
Loss :  1.6225827932357788 2.663754463195801 14.941354751586914
Loss :  1.6872845888137817 2.197749614715576 12.676033020019531
  batch 60 loss: 1.6872845888137817, 2.197749614715576, 12.676033020019531
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6077064275741577 2.2598071098327637 12.906742095947266
Loss :  1.637114405632019 2.597763776779175 14.625933647155762
Loss :  1.6170796155929565 2.3834805488586426 13.5344820022583
Loss :  1.6054458618164062 2.5457088947296143 14.333990097045898
Loss :  1.5842819213867188 1.92246413230896 11.196602821350098
Loss :  1.623116374015808 4.265650272369385 22.951366424560547
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6321438550949097 4.276005268096924 23.012168884277344
Loss :  1.6308835744857788 4.143680095672607 22.349285125732422
Loss :  1.6456904411315918 4.178480625152588 22.53809356689453
Total LOSS train 13.603736965472882 valid 22.71272850036621
CE LOSS train 1.631833265377925 valid 0.41142261028289795
Contrastive LOSS train 2.3943807290150567 valid 1.044620156288147
EPOCH 191:
Loss :  1.651739239692688 2.189444065093994 12.598958969116211
Loss :  1.665556788444519 3.3845314979553223 18.588212966918945
Loss :  1.6322721242904663 2.516240119934082 14.213472366333008
Loss :  1.637923002243042 2.176309823989868 12.519472122192383
Loss :  1.6556013822555542 2.016277313232422 11.736988067626953
Loss :  1.6178263425827026 2.241936683654785 12.827509880065918
Loss :  1.6551862955093384 2.900540590286255 16.157888412475586
Loss :  1.6297504901885986 2.0956130027770996 12.107815742492676
Loss :  1.6206552982330322 2.567627191543579 14.45879077911377
Loss :  1.6572538614273071 2.172255516052246 12.518531799316406
Loss :  1.6121264696121216 2.4674324989318848 13.949288368225098
Loss :  1.6117602586746216 2.0959033966064453 12.091277122497559
Loss :  1.6093552112579346 1.9126794338226318 11.172752380371094
Loss :  1.6142516136169434 2.430011034011841 13.764307022094727
Loss :  1.6722235679626465 2.627410650253296 14.809276580810547
Loss :  1.666029453277588 1.8217551708221436 10.774805068969727
Loss :  1.605574607849121 1.9197314977645874 11.204232215881348
Loss :  1.6331669092178345 2.834956407546997 15.80794906616211
Loss :  1.6014394760131836 2.630568504333496 14.754281997680664
Loss :  1.6668345928192139 2.0316381454467773 11.82502555847168
  batch 20 loss: 1.6668345928192139, 2.0316381454467773, 11.82502555847168
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.626924991607666 1.630596399307251 9.7799072265625
Loss :  1.6012866497039795 2.1552376747131348 12.377474784851074
Loss :  1.6204240322113037 2.6785166263580322 15.013007164001465
Loss :  1.6358451843261719 1.8959568738937378 11.115629196166992
Loss :  1.6667883396148682 2.9641060829162598 16.48731803894043
Loss :  1.623365879058838 2.0471248626708984 11.858989715576172
Loss :  1.6346409320831299 2.0155224800109863 11.712252616882324
Loss :  1.6291861534118652 1.8829349279403687 11.043861389160156
Loss :  1.5756293535232544 1.9262442588806152 11.2068510055542
Loss :  1.665440320968628 2.329298496246338 13.311932563781738
Loss :  1.5778765678405762 2.316885471343994 13.162303924560547
Loss :  1.647829294204712 2.6680245399475098 14.987951278686523
Loss :  1.6220816373825073 1.9460396766662598 11.352279663085938
Loss :  1.62034010887146 1.99141263961792 11.577404022216797
Loss :  1.5836904048919678 2.362063407897949 13.394007682800293
Loss :  1.5976471900939941 2.6958489418029785 15.07689094543457
Loss :  1.5953245162963867 2.668694496154785 14.938796997070312
Loss :  1.659117341041565 2.6424341201782227 14.871288299560547
Loss :  1.6633087396621704 2.4787557125091553 14.057087898254395
Loss :  1.6742467880249023 2.1133792400360107 12.241143226623535
  batch 40 loss: 1.6742467880249023, 2.1133792400360107, 12.241143226623535
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6319533586502075 2.6953625679016113 15.108765602111816
Loss :  1.6188400983810425 2.834571599960327 15.791698455810547
Loss :  1.608451008796692 2.8741886615753174 15.97939395904541
Loss :  1.6223961114883423 3.208209991455078 17.6634464263916
Loss :  1.5991233587265015 2.810033082962036 15.649288177490234
Loss :  1.6314352750778198 2.7012765407562256 15.137818336486816
Loss :  1.6651124954223633 2.0435304641723633 11.88276481628418
Loss :  1.613033652305603 2.0464892387390137 11.845479965209961
Loss :  1.6773878335952759 2.534839630126953 14.35158634185791
Loss :  1.6186877489089966 3.008800983428955 16.66269302368164
Loss :  1.6534507274627686 2.689511299133301 15.101007461547852
Loss :  1.6472127437591553 3.8358030319213867 20.826229095458984
Loss :  1.6253865957260132 2.1771903038024902 12.511338233947754
Loss :  1.6565144062042236 3.455798387527466 18.93550682067871
Loss :  1.6144545078277588 1.9778751134872437 11.503829956054688
Loss :  1.6785905361175537 1.9051225185394287 11.204203605651855
Loss :  1.6216284036636353 2.402080535888672 13.632031440734863
Loss :  1.6040995121002197 2.289294481277466 13.05057144165039
Loss :  1.6204004287719727 2.225306510925293 12.746932983398438
Loss :  1.6855422258377075 2.206714630126953 12.719115257263184
  batch 60 loss: 1.6855422258377075, 2.206714630126953, 12.719115257263184
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6036231517791748 2.380533456802368 13.506290435791016
Loss :  1.6338080167770386 3.302182912826538 18.14472198486328
Loss :  1.613336205482483 2.2145683765411377 12.686178207397461
Loss :  1.6014384031295776 2.614741086959839 14.67514419555664
Loss :  1.5804451704025269 1.9213142395019531 11.187016487121582
Loss :  1.6299660205841064 3.9643313884735107 21.451623916625977
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6400789022445679 3.9831008911132812 21.555583953857422
Loss :  1.6373565196990967 3.8263754844665527 20.76923370361328
Loss :  1.6530128717422485 3.6322920322418213 19.81447410583496
Total LOSS train 13.691542566739596 valid 20.89772891998291
CE LOSS train 1.6302134367135854 valid 0.41325321793556213
Contrastive LOSS train 2.4122658326075626 valid 0.9080730080604553
EPOCH 192:
Loss :  1.6494722366333008 2.227888822555542 12.78891658782959
Loss :  1.6630128622055054 2.4280805587768555 13.803415298461914
Loss :  1.6297223567962646 2.1876981258392334 12.56821346282959
Loss :  1.6360341310501099 2.3293991088867188 13.283029556274414
Loss :  1.6546390056610107 2.07051157951355 12.007197380065918
Loss :  1.6175527572631836 3.777745246887207 20.50627899169922
Loss :  1.6550469398498535 2.2983641624450684 13.146867752075195
Loss :  1.6294653415679932 1.7290668487548828 10.274799346923828
Loss :  1.6205501556396484 2.105428457260132 12.147692680358887
Loss :  1.657189965248108 1.9986319541931152 11.650350570678711
Loss :  1.61116623878479 3.477747678756714 18.99990463256836
Loss :  1.6109740734100342 2.3704583644866943 13.463266372680664
Loss :  1.6089637279510498 3.331282138824463 18.26537322998047
Loss :  1.6143686771392822 3.1621766090393066 17.425251007080078
Loss :  1.6730353832244873 2.8591184616088867 15.9686279296875
Loss :  1.6667628288269043 2.338115692138672 13.357341766357422
Loss :  1.6058892011642456 2.3192644119262695 13.202211380004883
Loss :  1.631826400756836 2.666269540786743 14.963173866271973
Loss :  1.6003650426864624 1.6212400197982788 9.706564903259277
Loss :  1.6642557382583618 2.268547534942627 13.006993293762207
  batch 20 loss: 1.6642557382583618, 2.268547534942627, 13.006993293762207
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6259685754776 2.1028971672058105 12.14045524597168
Loss :  1.5992716550827026 2.471736431121826 13.957954406738281
Loss :  1.6181972026824951 1.7771449089050293 10.503922462463379
Loss :  1.633996844291687 2.4267847537994385 13.76792049407959
Loss :  1.6652547121047974 2.411315441131592 13.721832275390625
Loss :  1.6209440231323242 2.5092523097991943 14.167205810546875
Loss :  1.6312037706375122 2.5505948066711426 14.384177207946777
Loss :  1.6248281002044678 2.4217453002929688 13.73355484008789
Loss :  1.5709680318832397 2.9596896171569824 16.369417190551758
Loss :  1.6602551937103271 1.9460891485214233 11.390701293945312
Loss :  1.5724705457687378 2.072765588760376 11.936298370361328
Loss :  1.6438134908676147 2.1939427852630615 12.613527297973633
Loss :  1.6182810068130493 2.3229708671569824 13.233136177062988
Loss :  1.6164829730987549 2.580378770828247 14.518376350402832
Loss :  1.581255555152893 3.824507713317871 20.703794479370117
Loss :  1.5958603620529175 3.5739641189575195 19.465679168701172
Loss :  1.594624638557434 3.2427780628204346 17.808515548706055
Loss :  1.658223032951355 2.7850747108459473 15.583596229553223
Loss :  1.663440227508545 2.4326937198638916 13.826908111572266
Loss :  1.6748318672180176 2.799229383468628 15.670978546142578
  batch 40 loss: 1.6748318672180176, 2.799229383468628, 15.670978546142578
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6323130130767822 3.9281206130981445 21.27291488647461
Loss :  1.618118166923523 3.095810890197754 17.097171783447266
Loss :  1.6073235273361206 2.324507474899292 13.22986125946045
Loss :  1.6203306913375854 3.12264347076416 17.23354721069336
Loss :  1.597182035446167 2.2481741905212402 12.838053703308105
Loss :  1.6283255815505981 2.488744020462036 14.07204532623291
Loss :  1.6622759103775024 2.3544111251831055 13.434331893920898
Loss :  1.6134254932403564 2.4162755012512207 13.694802284240723
Loss :  1.676160454750061 2.605679988861084 14.704561233520508
Loss :  1.6188793182373047 3.4391820430755615 18.814788818359375
Loss :  1.6541191339492798 2.800525665283203 15.656747817993164
Loss :  1.6466509103775024 2.5067250728607178 14.180276870727539
Loss :  1.6235779523849487 2.871075391769409 15.978955268859863
Loss :  1.653340458869934 3.1248323917388916 17.277502059936523
Loss :  1.6119866371154785 2.1298716068267822 12.261344909667969
Loss :  1.6770884990692139 1.9660327434539795 11.50725269317627
Loss :  1.6199607849121094 2.0633492469787598 11.93670654296875
Loss :  1.6035679578781128 2.481116533279419 14.009150505065918
Loss :  1.6208635568618774 3.5967636108398438 19.60468101501465
Loss :  1.6872479915618896 2.790703058242798 15.640763282775879
  batch 60 loss: 1.6872479915618896, 2.790703058242798, 15.640763282775879
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6048918962478638 2.4444737434387207 13.82726001739502
Loss :  1.635125756263733 2.564974546432495 14.459999084472656
Loss :  1.614094614982605 1.953991413116455 11.384051322937012
Loss :  1.6019166707992554 2.275757074356079 12.980701446533203
Loss :  1.5805574655532837 2.4959516525268555 14.06031608581543
Loss :  1.607276439666748 4.220870494842529 22.71162986755371
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.6180415153503418 4.115877628326416 22.197429656982422
Loss :  1.6143453121185303 4.136761665344238 22.298152923583984
Loss :  1.6283164024353027 3.998084783554077 21.61874008178711
Total LOSS train 14.479864751375638 valid 22.206488132476807
CE LOSS train 1.6289198361910306 valid 0.4070791006088257
Contrastive LOSS train 2.570188984504113 valid 0.9995211958885193
EPOCH 193:
Loss :  1.6493992805480957 2.214467763900757 12.721738815307617
Loss :  1.6636619567871094 2.4117817878723145 13.722570419311523
Loss :  1.6307281255722046 1.8576068878173828 10.91876220703125
Loss :  1.637054681777954 2.2353546619415283 12.813827514648438
Loss :  1.6554782390594482 3.008155107498169 16.696252822875977
Loss :  1.6185113191604614 2.6921567916870117 15.07929515838623
Loss :  1.6567633152008057 2.7260842323303223 15.28718376159668
Loss :  1.6307556629180908 3.2238404750823975 17.749958038330078
Loss :  1.6205885410308838 3.3454906940460205 18.348041534423828
Loss :  1.6582560539245605 3.097996234893799 17.148237228393555
Loss :  1.6122536659240723 2.498814105987549 14.1063232421875
Loss :  1.6125179529190063 2.523069143295288 14.227864265441895
Loss :  1.6099140644073486 2.6152162551879883 14.685995101928711
Loss :  1.6140645742416382 2.6573774814605713 14.900951385498047
Loss :  1.673062801361084 2.4210474491119385 13.778299331665039
Loss :  1.667138695716858 2.4937541484832764 14.135910034179688
Loss :  1.6035677194595337 2.613793134689331 14.672533988952637
Loss :  1.6316721439361572 2.318498134613037 13.224163055419922
Loss :  1.5975289344787598 2.048457622528076 11.83981704711914
Loss :  1.6628830432891846 2.227593183517456 12.800848960876465
  batch 20 loss: 1.6628830432891846, 2.227593183517456, 12.800848960876465
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6218934059143066 2.248507261276245 12.864429473876953
Loss :  1.5944249629974365 3.6777029037475586 19.982940673828125
Loss :  1.6141269207000732 2.5830230712890625 14.529242515563965
Loss :  1.631335973739624 2.098799228668213 12.12533187866211
Loss :  1.6622086763381958 2.4655566215515137 13.989992141723633
Loss :  1.6164803504943848 2.683519124984741 15.034076690673828
Loss :  1.6276917457580566 2.796492099761963 15.610151290893555
Loss :  1.6210719347000122 2.723393440246582 15.238039016723633
Loss :  1.5672603845596313 2.5435945987701416 14.285233497619629
Loss :  1.6586939096450806 3.217423915863037 17.745813369750977
Loss :  1.5694841146469116 2.453407049179077 13.836519241333008
Loss :  1.6422014236450195 2.218585968017578 12.73513126373291
Loss :  1.6164907217025757 2.1482961177825928 12.35797119140625
Loss :  1.6145834922790527 2.1102983951568604 12.166074752807617
Loss :  1.5784358978271484 2.5111186504364014 14.134029388427734
Loss :  1.5941413640975952 2.13291597366333 12.258720397949219
Loss :  1.5931761264801025 2.0993237495422363 12.089794158935547
Loss :  1.6581182479858398 2.0021605491638184 11.66892147064209
Loss :  1.6630116701126099 2.713137626647949 15.228699684143066
Loss :  1.6732687950134277 2.9768223762512207 16.55738067626953
  batch 40 loss: 1.6732687950134277, 2.9768223762512207, 16.55738067626953
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6296789646148682 2.0945513248443604 12.102435111999512
Loss :  1.6166330575942993 2.15793776512146 12.406322479248047
Loss :  1.6048448085784912 2.2613611221313477 12.911650657653809
Loss :  1.6199790239334106 2.2316079139709473 12.7780179977417
Loss :  1.5964328050613403 2.1015703678131104 12.104284286499023
Loss :  1.6292006969451904 2.644662618637085 14.852514266967773
Loss :  1.6643898487091064 1.909753680229187 11.213157653808594
Loss :  1.612594723701477 1.7743722200393677 10.484456062316895
Loss :  1.677283763885498 1.6629548072814941 9.992057800292969
Loss :  1.6170284748077393 2.133869171142578 12.28637409210205
Loss :  1.651563048362732 2.093747854232788 12.120302200317383
Loss :  1.6453558206558228 2.9223482608795166 16.257097244262695
Loss :  1.6221864223480225 2.6981170177459717 15.112771987915039
Loss :  1.653226375579834 2.0414624214172363 11.860538482666016
Loss :  1.6092677116394043 2.0977416038513184 12.097976684570312
Loss :  1.6765193939208984 3.3374924659729004 18.363981246948242
Loss :  1.6176326274871826 2.6969895362854004 15.102580070495605
Loss :  1.6006499528884888 2.739177703857422 15.296538352966309
Loss :  1.6174147129058838 3.1391727924346924 17.313278198242188
Loss :  1.6847647428512573 2.856457471847534 15.967052459716797
  batch 60 loss: 1.6847647428512573, 2.856457471847534, 15.967052459716797
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.601696491241455 2.33646297454834 13.284011840820312
Loss :  1.6318049430847168 2.5274643898010254 14.269126892089844
Loss :  1.612017273902893 3.395206928253174 18.588050842285156
Loss :  1.5990960597991943 2.6221468448638916 14.709830284118652
Loss :  1.578292965888977 1.9559807777404785 11.358196258544922
Loss :  1.610442876815796 4.267484664916992 22.947866439819336
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.6234391927719116 4.221407413482666 22.73047637939453
Loss :  1.6200307607650757 4.197215557098389 22.606109619140625
Loss :  1.625382423400879 4.088008403778076 22.065425872802734
Total LOSS train 14.094302617586576 valid 22.587469577789307
CE LOSS train 1.6275916246267466 valid 0.4063456058502197
Contrastive LOSS train 2.493342216198261 valid 1.022002100944519
EPOCH 194:
Loss :  1.6464823484420776 1.9327125549316406 11.31004524230957
Loss :  1.6603825092315674 2.496495008468628 14.142857551574707
Loss :  1.6267731189727783 2.0258123874664307 11.755834579467773
Loss :  1.6331803798675537 2.513493776321411 14.20064926147461
Loss :  1.651318907737732 2.69834566116333 15.143046379089355
Loss :  1.6121280193328857 2.9322946071624756 16.273601531982422
Loss :  1.6508370637893677 2.867070198059082 15.986187934875488
Loss :  1.6243455410003662 2.2848384380340576 13.048538208007812
Loss :  1.6143337488174438 2.3189759254455566 13.209213256835938
Loss :  1.6510789394378662 1.870092749595642 11.001543045043945
Loss :  1.605530023574829 2.4702847003936768 13.956953048706055
Loss :  1.6056814193725586 2.0022406578063965 11.6168851852417
Loss :  1.6031150817871094 3.1385579109191895 17.2959041595459
Loss :  1.6090203523635864 2.749695301055908 15.35749626159668
Loss :  1.6681729555130005 1.8109155893325806 10.722750663757324
Loss :  1.6629738807678223 2.527461528778076 14.300281524658203
Loss :  1.6015206575393677 1.9495720863342285 11.349380493164062
Loss :  1.629830002784729 2.603847026824951 14.649065971374512
Loss :  1.5982451438903809 1.744892954826355 10.322710037231445
Loss :  1.664519190788269 2.1858770847320557 12.593904495239258
  batch 20 loss: 1.664519190788269, 2.1858770847320557, 12.593904495239258
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6251397132873535 1.6568185091018677 9.909233093261719
Loss :  1.5975853204727173 2.108468770980835 12.13992977142334
Loss :  1.6158274412155151 1.8475260734558105 10.853458404541016
Loss :  1.6323091983795166 1.7152224779129028 10.20842170715332
Loss :  1.6640537977218628 2.4082701206207275 13.705404281616211
Loss :  1.6193430423736572 2.777238607406616 15.505536079406738
Loss :  1.6299134492874146 2.4002668857574463 13.631247520446777
Loss :  1.6234437227249146 2.809648275375366 15.671685218811035
Loss :  1.5694659948349 2.782099962234497 15.479965209960938
Loss :  1.6601288318634033 2.215785026550293 12.739053726196289
Loss :  1.5701385736465454 2.289252996444702 13.016404151916504
Loss :  1.6419355869293213 3.599255084991455 19.63821029663086
Loss :  1.6154816150665283 1.9853432178497314 11.542197227478027
Loss :  1.6135812997817993 2.070296287536621 11.965063095092773
Loss :  1.5771139860153198 3.103158473968506 17.092906951904297
Loss :  1.5926764011383057 2.434767484664917 13.76651382446289
Loss :  1.5913221836090088 2.214388132095337 12.663262367248535
Loss :  1.6557244062423706 1.9288122653961182 11.299785614013672
Loss :  1.6615983247756958 2.606186628341675 14.69253158569336
Loss :  1.672424554824829 2.5573084354400635 14.458966255187988
  batch 40 loss: 1.672424554824829, 2.5573084354400635, 14.458966255187988
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.629023551940918 2.8437960147857666 15.848003387451172
Loss :  1.6152527332305908 1.9686869382858276 11.458686828613281
Loss :  1.6031826734542847 2.18623948097229 12.534379959106445
Loss :  1.6169424057006836 2.0103137493133545 11.668511390686035
Loss :  1.5937798023223877 1.5761338472366333 9.474449157714844
Loss :  1.6266415119171143 2.5409669876098633 14.331476211547852
Loss :  1.661799669265747 1.950676441192627 11.415181159973145
Loss :  1.6098471879959106 2.821068048477173 15.715187072753906
Loss :  1.6771314144134521 2.4394237995147705 13.874250411987305
Loss :  1.6169577836990356 3.5952563285827637 19.59324073791504
Loss :  1.6523021459579468 2.1494641304016113 12.399621963500977
Loss :  1.6468011140823364 1.93354332447052 11.314517974853516
Loss :  1.624437928199768 1.7172973155975342 10.210925102233887
Loss :  1.6548988819122314 2.6305642127990723 14.807719230651855
Loss :  1.6103441715240479 2.2500815391540527 12.86075210571289
Loss :  1.6764899492263794 2.193354368209839 12.643261909484863
Loss :  1.6172845363616943 2.7854387760162354 15.544478416442871
Loss :  1.5998046398162842 1.8130605220794678 10.665107727050781
Loss :  1.6167539358139038 2.828246831893921 15.757987976074219
Loss :  1.6844593286514282 2.4395530223846436 13.882225036621094
  batch 60 loss: 1.6844593286514282, 2.4395530223846436, 13.882225036621094
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.601605772972107 1.8383358716964722 10.79328441619873
Loss :  1.6322027444839478 3.1809334754943848 17.536869049072266
Loss :  1.612243413925171 2.554058790206909 14.382537841796875
Loss :  1.6007062196731567 2.7885706424713135 15.543559074401855
Loss :  1.5793792009353638 1.7017124891281128 10.08794116973877
Loss :  1.6196714639663696 4.327909469604492 23.259218215942383
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6305615901947021 4.416346073150635 23.712291717529297
Loss :  1.6264790296554565 4.1752214431762695 22.50258445739746
Loss :  1.643104076385498 4.287505149841309 23.080631256103516
Total LOSS train 13.424012008080116 valid 23.138681411743164
CE LOSS train 1.6262915299488947 valid 0.4107760190963745
Contrastive LOSS train 2.3595441047961896 valid 1.0718762874603271
EPOCH 195:
Loss :  1.6482516527175903 2.46669602394104 13.981731414794922
Loss :  1.6618906259536743 2.6226015090942383 14.774898529052734
Loss :  1.6292999982833862 2.734281301498413 15.30070686340332
Loss :  1.6352499723434448 2.814707040786743 15.708785057067871
Loss :  1.6533418893814087 2.2366089820861816 12.836386680603027
Loss :  1.6155246496200562 2.548670530319214 14.358877182006836
Loss :  1.6541742086410522 2.798348903656006 15.645918846130371
Loss :  1.6282050609588623 2.4543251991271973 13.89983081817627
Loss :  1.6171107292175293 2.917050838470459 16.20236587524414
Loss :  1.653603196144104 1.9517147541046143 11.412177085876465
Loss :  1.607116460800171 2.322589159011841 13.220062255859375
Loss :  1.6065739393234253 2.1475861072540283 12.344504356384277
Loss :  1.6039903163909912 2.5927274227142334 14.567627906799316
Loss :  1.6098815202713013 2.6854207515716553 15.036985397338867
Loss :  1.6683961153030396 2.633904457092285 14.837918281555176
Loss :  1.6637613773345947 2.5800395011901855 14.563959121704102
Loss :  1.6022999286651611 4.301784038543701 23.111221313476562
Loss :  1.6302121877670288 3.4068214893341064 18.66431999206543
Loss :  1.5992865562438965 1.6107048988342285 9.652811050415039
Loss :  1.6652213335037231 2.143604040145874 12.383241653442383
  batch 20 loss: 1.6652213335037231, 2.143604040145874, 12.383241653442383
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6258494853973389 2.014134407043457 11.696521759033203
Loss :  1.5982497930526733 2.692789316177368 15.062195777893066
Loss :  1.6165680885314941 2.1896514892578125 12.564825057983398
Loss :  1.6324962377548218 2.565528392791748 14.460138320922852
Loss :  1.6639008522033691 3.031153440475464 16.81966781616211
Loss :  1.620173454284668 2.4801785945892334 14.021066665649414
Loss :  1.6304453611373901 2.1554415225982666 12.407652854919434
Loss :  1.6238069534301758 2.072824716567993 11.987930297851562
Loss :  1.567856788635254 2.0298268795013428 11.716991424560547
Loss :  1.6597667932510376 2.3988428115844727 13.65398120880127
Loss :  1.5681401491165161 2.7447509765625 15.291894912719727
Loss :  1.6411985158920288 2.809539318084717 15.688895225524902
Loss :  1.6145695447921753 2.1512062549591064 12.370600700378418
Loss :  1.612316608428955 2.9035768508911133 16.13020133972168
Loss :  1.5759162902832031 3.0808186531066895 16.980009078979492
Loss :  1.5922136306762695 2.8543331623077393 15.863879203796387
Loss :  1.5909385681152344 2.603806972503662 14.609973907470703
Loss :  1.6555535793304443 2.6346540451049805 14.828824043273926
Loss :  1.6608186960220337 2.5988669395446777 14.65515422821045
Loss :  1.6718939542770386 2.5978410243988037 14.661099433898926
  batch 40 loss: 1.6718939542770386, 2.5978410243988037, 14.661099433898926
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6292016506195068 2.589595317840576 14.577178955078125
Loss :  1.6156558990478516 1.8213248252868652 10.722280502319336
Loss :  1.6038728952407837 2.012958288192749 11.668664932250977
Loss :  1.6182008981704712 2.163578748703003 12.436095237731934
Loss :  1.5941293239593506 1.9249240159988403 11.21875
Loss :  1.6268596649169922 1.9763503074645996 11.508611679077148
Loss :  1.661667823791504 2.086458444595337 12.09395980834961
Loss :  1.6093820333480835 2.0773329734802246 11.996047019958496
Loss :  1.6766554117202759 2.145482301712036 12.404067039489746
Loss :  1.6162147521972656 1.8429591655731201 10.831010818481445
Loss :  1.651322364807129 2.640334129333496 14.85299301147461
Loss :  1.6444249153137207 1.8307571411132812 10.798210144042969
Loss :  1.621762752532959 2.0047805309295654 11.645666122436523
Loss :  1.6525787115097046 2.0418779850006104 11.861968040466309
Loss :  1.6098006963729858 1.95887291431427 11.404165267944336
Loss :  1.675139307975769 1.8527697324752808 10.938987731933594
Loss :  1.6170070171356201 2.694085121154785 15.087432861328125
Loss :  1.6003046035766602 1.9792689085006714 11.496648788452148
Loss :  1.617479920387268 2.254796266555786 12.891461372375488
Loss :  1.6851825714111328 2.041159152984619 11.89097785949707
  batch 60 loss: 1.6851825714111328, 2.041159152984619, 11.89097785949707
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6022194623947144 2.6982743740081787 15.093591690063477
Loss :  1.6325033903121948 1.874261736869812 11.003811836242676
Loss :  1.611567497253418 1.7662372589111328 10.442753791809082
Loss :  1.5991014242172241 2.3359270095825195 13.278736114501953
Loss :  1.5780961513519287 2.1006650924682617 12.081421852111816
Loss :  1.6310697793960571 3.98740816116333 21.5681095123291
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6425021886825562 3.9298081398010254 21.291542053222656
Loss :  1.6414752006530762 3.8902804851531982 21.092878341674805
Loss :  1.6459962129592896 3.7693302631378174 20.492647171020508
Total LOSS train 13.572328083331769 valid 21.111294269561768
CE LOSS train 1.6265599415852474 valid 0.4114990532398224
Contrastive LOSS train 2.389153607075031 valid 0.9423325657844543
EPOCH 196:
Loss :  1.6469272375106812 2.2341771125793457 12.817811965942383
Loss :  1.6611851453781128 2.0423848628997803 11.873109817504883
Loss :  1.627684473991394 1.932179570198059 11.288582801818848
Loss :  1.6339068412780762 2.223689079284668 12.752351760864258
Loss :  1.6524879932403564 1.9627494812011719 11.466235160827637
Loss :  1.6147916316986084 1.9237242937088013 11.233413696289062
Loss :  1.6532156467437744 2.4397668838500977 13.852049827575684
Loss :  1.6276907920837402 2.408890724182129 13.672143936157227
Loss :  1.6174687147140503 2.490633726119995 14.070637702941895
Loss :  1.6536731719970703 2.3298912048339844 13.303129196166992
Loss :  1.607009768486023 2.4282634258270264 13.748327255249023
Loss :  1.605978012084961 2.1263153553009033 12.237554550170898
Loss :  1.6031622886657715 2.0184764862060547 11.695545196533203
Loss :  1.6078202724456787 2.0383388996124268 11.799514770507812
Loss :  1.6681922674179077 2.5872297286987305 14.604340553283691
Loss :  1.6632028818130493 2.626730442047119 14.796854972839355
Loss :  1.6012674570083618 2.5164284706115723 14.183409690856934
Loss :  1.628649115562439 2.5246522426605225 14.251910209655762
Loss :  1.5976321697235107 2.4673492908477783 13.934378623962402
Loss :  1.6630332469940186 2.5389530658721924 14.35779857635498
  batch 20 loss: 1.6630332469940186, 2.5389530658721924, 14.35779857635498
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6236721277236938 1.7060799598693848 10.154071807861328
Loss :  1.597190022468567 2.1337900161743164 12.26613998413086
Loss :  1.616968035697937 1.9618151187896729 11.426043510437012
Loss :  1.6332846879959106 2.009448289871216 11.680525779724121
Loss :  1.663899540901184 2.2953438758850098 13.140618324279785
Loss :  1.6195399761199951 2.0737738609313965 11.988409996032715
Loss :  1.6303199529647827 1.973778486251831 11.499212265014648
Loss :  1.6239935159683228 2.159008264541626 12.419034957885742
Loss :  1.5695548057556152 2.265690565109253 12.898008346557617
Loss :  1.6590454578399658 2.222386360168457 12.770977020263672
Loss :  1.5705547332763672 2.389653205871582 13.518820762634277
Loss :  1.6421024799346924 2.7368083000183105 15.326144218444824
Loss :  1.6160862445831299 2.827178716659546 15.75197982788086
Loss :  1.6139744520187378 2.235649347305298 12.792221069335938
Loss :  1.5774043798446655 2.311079978942871 13.132803916931152
Loss :  1.5927356481552124 2.5573058128356934 14.379264831542969
Loss :  1.5909602642059326 2.260394334793091 12.892931938171387
Loss :  1.6557661294937134 2.2258799076080322 12.785165786743164
Loss :  1.6600568294525146 2.341663122177124 13.368372917175293
Loss :  1.6702988147735596 2.239643096923828 12.868514060974121
  batch 40 loss: 1.6702988147735596, 2.239643096923828, 12.868514060974121
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6266834735870361 2.508700132369995 14.170184135437012
Loss :  1.6132111549377441 2.176934242248535 12.497882843017578
Loss :  1.6024495363235474 2.2070131301879883 12.6375150680542
Loss :  1.6172130107879639 2.0346739292144775 11.790582656860352
Loss :  1.594034194946289 1.7199954986572266 10.194011688232422
Loss :  1.6271915435791016 1.78605318069458 10.557456970214844
Loss :  1.662568211555481 1.880601167678833 11.065573692321777
Loss :  1.6107710599899292 1.6974263191223145 10.097902297973633
Loss :  1.6768701076507568 1.9325069189071655 11.339405059814453
Loss :  1.6178334951400757 2.1936118602752686 12.585892677307129
Loss :  1.653259038925171 1.9458885192871094 11.382701873779297
Loss :  1.6466906070709229 1.9730061292648315 11.511720657348633
Loss :  1.624727725982666 1.8428966999053955 10.839210510253906
Loss :  1.6554067134857178 2.3890256881713867 13.60053539276123
Loss :  1.612122893333435 2.448594331741333 13.855093955993652
Loss :  1.678597331047058 2.07775616645813 12.067378044128418
Loss :  1.6197667121887207 2.2127346992492676 12.683439254760742
Loss :  1.6027261018753052 2.4676027297973633 13.940739631652832
Loss :  1.6195634603500366 2.2665560245513916 12.952342987060547
Loss :  1.6864093542099 1.672951340675354 10.051165580749512
  batch 60 loss: 1.6864093542099, 1.672951340675354, 10.051165580749512
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6039326190948486 1.8112643957138062 10.66025447845459
Loss :  1.6344622373580933 2.579488515853882 14.531905174255371
Loss :  1.6137304306030273 2.4819178581237793 14.023320198059082
Loss :  1.6017862558364868 2.257085084915161 12.887211799621582
Loss :  1.5808422565460205 1.7996147871017456 10.5789155960083
Loss :  1.6036490201950073 4.183498382568359 22.521141052246094
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.614432454109192 4.140657901763916 22.31772232055664
Loss :  1.610641598701477 4.121549606323242 22.2183895111084
Loss :  1.626570224761963 3.862401247024536 20.938575744628906
Total LOSS train 12.638472366333009 valid 21.99895715713501
CE LOSS train 1.6268497962218065 valid 0.4066425561904907
Contrastive LOSS train 2.202324527960557 valid 0.965600311756134
EPOCH 197:
Loss :  1.650010108947754 2.196430206298828 12.632161140441895
Loss :  1.6640985012054443 2.7451813220977783 15.390005111694336
Loss :  1.6319564580917358 2.017134189605713 11.717626571655273
Loss :  1.6384010314941406 2.584398031234741 14.560391426086426
Loss :  1.6560434103012085 2.325737714767456 13.2847318649292
Loss :  1.6187829971313477 1.7200472354888916 10.219018936157227
Loss :  1.6563968658447266 1.7708868980407715 10.510831832885742
Loss :  1.6302721500396729 1.5975737571716309 9.618141174316406
Loss :  1.619845986366272 1.7167667150497437 10.203680038452148
Loss :  1.656072735786438 1.6373752355575562 9.842948913574219
Loss :  1.6107776165008545 1.7553926706314087 10.387741088867188
Loss :  1.6101584434509277 2.8580636978149414 15.900476455688477
Loss :  1.6077892780303955 2.1720192432403564 12.46788501739502
Loss :  1.6129752397537231 3.046009063720703 16.843021392822266
Loss :  1.6706395149230957 2.747504711151123 15.408163070678711
Loss :  1.6649526357650757 2.4919369220733643 14.12463665008545
Loss :  1.6034512519836426 2.171189785003662 12.459400177001953
Loss :  1.6313035488128662 2.3076605796813965 13.169607162475586
Loss :  1.5985122919082642 1.646000623703003 9.828516006469727
Loss :  1.6637747287750244 1.8786591291427612 11.057069778442383
  batch 20 loss: 1.6637747287750244, 1.8786591291427612, 11.057069778442383
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4], device='cuda:0')
Loss :  1.6242362260818481 1.900028109550476 11.12437629699707
Loss :  1.59812593460083 1.9865210056304932 11.530731201171875
Loss :  1.6180700063705444 1.6599844694137573 9.917991638183594
Loss :  1.6344915628433228 1.907606840133667 11.172526359558105
Loss :  1.6653975248336792 2.9282562732696533 16.306678771972656
Loss :  1.6228811740875244 2.777874708175659 15.51225471496582
Loss :  1.6332026720046997 1.8959401845932007 11.112903594970703
Loss :  1.6276191473007202 2.1886823177337646 12.571030616760254
Loss :  1.5737963914871216 2.2982497215270996 13.065045356750488
Loss :  1.662554383277893 2.665672779083252 14.990918159484863
Loss :  1.5750720500946045 2.3654110431671143 13.402127265930176
Loss :  1.6454226970672607 1.9288824796676636 11.289834976196289
Loss :  1.619579792022705 2.002612829208374 11.632644653320312
Loss :  1.6180437803268433 2.5984721183776855 14.610404968261719
Loss :  1.5825896263122559 2.251391887664795 12.839550018310547
Loss :  1.597707986831665 2.4530420303344727 13.86291790008545
Loss :  1.5971901416778564 2.500814914703369 14.101263999938965
Loss :  1.660503625869751 2.3716862201690674 13.51893424987793
Loss :  1.6659268140792847 2.129662036895752 12.314236640930176
Loss :  1.6763660907745361 1.7489553689956665 10.421142578125
  batch 40 loss: 1.6763660907745361, 1.7489553689956665, 10.421142578125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6343358755111694 2.0564591884613037 11.916631698608398
Loss :  1.620193600654602 2.451195478439331 13.876171112060547
Loss :  1.6096848249435425 2.6797258853912354 15.00831413269043
Loss :  1.6224530935287476 1.8012243509292603 10.62857437133789
Loss :  1.5991028547286987 2.158705711364746 12.392631530761719
Loss :  1.630749225616455 1.974141001701355 11.501453399658203
Loss :  1.6649649143218994 2.0201079845428467 11.765504837036133
Loss :  1.6146479845046997 2.4017534255981445 13.623414993286133
Loss :  1.6787171363830566 2.7492830753326416 15.425132751464844
Loss :  1.6208031177520752 2.3192427158355713 13.217016220092773
Loss :  1.6557599306106567 2.023749589920044 11.774507522583008
Loss :  1.6492635011672974 2.2524526119232178 12.911526679992676
Loss :  1.6269025802612305 2.041276216506958 11.833283424377441
Loss :  1.6564894914627075 2.0245792865753174 11.779385566711426
Loss :  1.614192247390747 2.2191696166992188 12.710040092468262
Loss :  1.6779392957687378 2.115175724029541 12.253817558288574
Loss :  1.6201525926589966 2.2301530838012695 12.770917892456055
Loss :  1.6032516956329346 1.9075464010238647 11.140983581542969
Loss :  1.6197105646133423 2.1633310317993164 12.436366081237793
Loss :  1.685667634010315 2.445939779281616 13.915367126464844
  batch 60 loss: 1.685667634010315, 2.445939779281616, 13.915367126464844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6044859886169434 2.6111881732940674 14.66042709350586
Loss :  1.6343162059783936 2.4490487575531006 13.879560470581055
Loss :  1.6139023303985596 2.4852616786956787 14.040210723876953
Loss :  1.6014541387557983 2.299514055252075 13.099023818969727
Loss :  1.5808428525924683 1.7887589931488037 10.524638175964355
Loss :  1.5888988971710205 4.388524055480957 23.531518936157227
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6027076244354248 4.353448390960693 23.369949340820312
Loss :  1.596150517463684 4.1656365394592285 22.424333572387695
Loss :  1.615118384361267 4.005722999572754 21.64373207092285
Total LOSS train 12.67705336350661 valid 22.74238348007202
CE LOSS train 1.629768863091102 valid 0.4037795960903168
Contrastive LOSS train 2.2094569059518667 valid 1.0014307498931885
EPOCH 198:
Loss :  1.6491849422454834 1.9864490032196045 11.581430435180664
Loss :  1.6628397703170776 2.064286231994629 11.984271049499512
Loss :  1.6302047967910767 2.0714216232299805 11.987313270568848
Loss :  1.636546015739441 2.3608577251434326 13.440834999084473
Loss :  1.654866337776184 2.1065256595611572 12.187494277954102
Loss :  1.6182526350021362 2.1230404376983643 12.233454704284668
Loss :  1.6554943323135376 2.1092631816864014 12.201810836791992
Loss :  1.6295182704925537 2.1388909816741943 12.323973655700684
Loss :  1.6193337440490723 2.4594218730926514 13.91644287109375
Loss :  1.6546279191970825 2.255326986312866 12.931262969970703
Loss :  1.6103451251983643 2.582270860671997 14.521698951721191
Loss :  1.6094893217086792 2.3371031284332275 13.295004844665527
Loss :  1.6072027683258057 1.9300192594528198 11.257298469543457
Loss :  1.6127151250839233 1.8495690822601318 10.860560417175293
Loss :  1.6701242923736572 1.8592582941055298 10.966415405273438
Loss :  1.666572093963623 2.8036012649536133 15.684577941894531
Loss :  1.606005311012268 2.773472547531128 15.473368644714355
Loss :  1.634138822555542 2.8403642177581787 15.835960388183594
Loss :  1.6021322011947632 1.7679730653762817 10.441997528076172
Loss :  1.6669974327087402 1.761962652206421 10.476810455322266
  batch 20 loss: 1.6669974327087402, 1.761962652206421, 10.476810455322266
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6262445449829102 2.0324628353118896 11.788558959960938
Loss :  1.5996984243392944 1.933708667755127 11.268240928649902
Loss :  1.6184368133544922 1.6717697381973267 9.977285385131836
Loss :  1.6346415281295776 2.4302258491516113 13.785770416259766
Loss :  1.6649081707000732 1.9686031341552734 11.50792407989502
Loss :  1.6217607259750366 2.254366636276245 12.893593788146973
Loss :  1.632728099822998 2.7712738513946533 15.489097595214844
Loss :  1.6263617277145386 2.6598432064056396 14.925578117370605
Loss :  1.573915719985962 2.178398609161377 12.46590805053711
Loss :  1.661512851715088 2.7510135173797607 15.416580200195312
Loss :  1.575061559677124 2.0932092666625977 12.041108131408691
Loss :  1.645111083984375 2.014592170715332 11.718071937561035
Loss :  1.6200155019760132 2.2953689098358154 13.0968599319458
Loss :  1.6177988052368164 2.3374853134155273 13.305225372314453
Loss :  1.582482933998108 3.023371696472168 16.6993408203125
Loss :  1.59735906124115 2.641873359680176 14.80672550201416
Loss :  1.5956830978393555 1.8052687644958496 10.622027397155762
Loss :  1.6591417789459229 1.831846833229065 10.818375587463379
Loss :  1.663957953453064 2.426299571990967 13.795455932617188
Loss :  1.6746366024017334 2.33685302734375 13.358901977539062
  batch 40 loss: 1.6746366024017334, 2.33685302734375, 13.358901977539062
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4], device='cuda:0')
Loss :  1.6322357654571533 2.5679285526275635 14.471878051757812
Loss :  1.6191304922103882 1.6673541069030762 9.955901145935059
Loss :  1.6077865362167358 2.1862778663635254 12.539175033569336
Loss :  1.6223944425582886 2.2539002895355225 12.89189624786377
Loss :  1.5995604991912842 1.8168270587921143 10.683695793151855
Loss :  1.6315184831619263 1.7081496715545654 10.172266960144043
Loss :  1.6657897233963013 2.5613973140716553 14.472776412963867
Loss :  1.6163713932037354 1.6271966695785522 9.752354621887207
Loss :  1.6813400983810425 1.6444354057312012 9.903517723083496
Loss :  1.6229214668273926 1.6257247924804688 9.751544952392578
Loss :  1.656737208366394 2.641719341278076 14.865334510803223
Loss :  1.6504372358322144 2.6058833599090576 14.679854393005371
Loss :  1.6290637254714966 1.9929569959640503 11.59384822845459
Loss :  1.6590596437454224 2.015090227127075 11.73451042175293
Loss :  1.6169975996017456 2.5700137615203857 14.467066764831543
Loss :  1.6811312437057495 1.7002372741699219 10.182317733764648
Loss :  1.6239285469055176 1.8393950462341309 10.820903778076172
Loss :  1.6067003011703491 1.7457480430603027 10.335440635681152
Loss :  1.6230597496032715 2.4364843368530273 13.80548095703125
Loss :  1.6879281997680664 2.003727912902832 11.706567764282227
  batch 60 loss: 1.6879281997680664, 2.003727912902832, 11.706567764282227
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.60783851146698 2.0771405696868896 11.993541717529297
Loss :  1.637176513671875 2.0041263103485107 11.657808303833008
Loss :  1.617775797843933 1.6451127529144287 9.843339920043945
Loss :  1.605855941772461 2.3685412406921387 13.448562622070312
Loss :  1.5859887599945068 2.3443310260772705 13.30764389038086
Loss :  1.5916866064071655 3.960127115249634 21.392322540283203
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6025168895721436 3.9258291721343994 21.23166275024414
Loss :  1.5994868278503418 3.8552086353302 20.875530242919922
Loss :  1.6074515581130981 3.8165173530578613 20.690038681030273
Total LOSS train 12.498766781733586 valid 21.047388553619385
CE LOSS train 1.6304119403545674 valid 0.40186288952827454
Contrastive LOSS train 2.1736709686426017 valid 0.9541293382644653
EPOCH 199:
Loss :  1.652392029762268 2.3935964107513428 13.62037467956543
Loss :  1.6665900945663452 2.6802608966827393 15.067893981933594
Loss :  1.6345157623291016 1.496692180633545 9.117977142333984
Loss :  1.640102744102478 1.99508535861969 11.61552906036377
Loss :  1.658268928527832 1.5298731327056885 9.307634353637695
Loss :  1.6212279796600342 2.351954936981201 13.381003379821777
Loss :  1.6582847833633423 1.8528493642807007 10.922532081604004
Loss :  1.6332467794418335 2.036383628845215 11.815164566040039
Loss :  1.6240555047988892 2.1646132469177246 12.447122573852539
Loss :  1.6596794128417969 2.4948456287384033 14.133907318115234
Loss :  1.6151516437530518 2.517575979232788 14.203031539916992
Loss :  1.61408269405365 1.9690022468566895 11.45909309387207
Loss :  1.6115623712539673 2.031615734100342 11.769641876220703
Loss :  1.6155123710632324 2.1807034015655518 12.51902961730957
Loss :  1.6729302406311035 2.2813222408294678 13.07954216003418
Loss :  1.668485164642334 2.8095407485961914 15.716188430786133
Loss :  1.6083322763442993 2.5658118724823 14.437392234802246
Loss :  1.635564923286438 1.8322198390960693 10.796664237976074
Loss :  1.6054105758666992 1.6367086172103882 9.78895378112793
Loss :  1.6671435832977295 1.9688193798065186 11.51124095916748
  batch 20 loss: 1.6671435832977295, 1.9688193798065186, 11.51124095916748
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6298738718032837 1.8364228010177612 10.81198787689209
Loss :  1.6042944192886353 2.703789710998535 15.12324333190918
Loss :  1.622351050376892 1.7895938158035278 10.570320129394531
Loss :  1.6369450092315674 2.3302595615386963 13.28824234008789
Loss :  1.666046380996704 2.539531707763672 14.363704681396484
Loss :  1.6216316223144531 2.1846203804016113 12.544733047485352
Loss :  1.6315433979034424 2.205850839614868 12.660797119140625
Loss :  1.6248208284378052 2.437452793121338 13.812084197998047
Loss :  1.5712708234786987 2.622234582901001 14.682443618774414
Loss :  1.6597539186477661 2.3940536975860596 13.630022048950195
Loss :  1.5727198123931885 2.798222541809082 15.56383228302002
Loss :  1.6437852382659912 2.514965534210205 14.218612670898438
Loss :  1.6173419952392578 1.984947681427002 11.54207992553711
Loss :  1.615670084953308 2.4539151191711426 13.885245323181152
Loss :  1.5802428722381592 2.642627716064453 14.793381690979004
Loss :  1.5957800149917603 2.2705254554748535 12.948407173156738
Loss :  1.594307541847229 2.006072521209717 11.62467098236084
Loss :  1.6581826210021973 2.3964600563049316 13.640481948852539
Loss :  1.6632647514343262 2.4207746982574463 13.76713752746582
Loss :  1.6743191480636597 2.4543986320495605 13.94631290435791
  batch 40 loss: 1.6743191480636597, 2.4543986320495605, 13.94631290435791
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.6318752765655518 2.574739694595337 14.505573272705078
Loss :  1.6187905073165894 1.7116295099258423 10.1769380569458
Loss :  1.6089915037155151 2.459770441055298 13.907843589782715
Loss :  1.6227123737335205 1.928882360458374 11.26712417602539
Loss :  1.6003350019454956 1.6619898080825806 9.910284042358398
Loss :  1.6318671703338623 2.0575640201568604 11.919687271118164
Loss :  1.6652110815048218 2.007413387298584 11.702278137207031
Loss :  1.6158064603805542 1.9675170183181763 11.453392028808594
Loss :  1.680450201034546 2.1984944343566895 12.672922134399414
Loss :  1.6231402158737183 2.186540126800537 12.555841445922852
Loss :  1.6586507558822632 1.891991376876831 11.118607521057129
Loss :  1.6528526544570923 2.5530917644500732 14.418312072753906
Loss :  1.630610466003418 1.844050645828247 10.850863456726074
Loss :  1.66009521484375 2.514946699142456 14.23482894897461
Loss :  1.6180895566940308 2.593503475189209 14.585607528686523
Loss :  1.681941032409668 2.5707921981811523 14.53590202331543
Loss :  1.6243908405303955 3.357872247695923 18.413753509521484
Loss :  1.6085542440414429 2.6951897144317627 15.084502220153809
Loss :  1.6249538660049438 2.075772285461426 12.003815650939941
Loss :  1.689473032951355 1.9448453187942505 11.413700103759766
  batch 60 loss: 1.689473032951355, 1.9448453187942505, 11.413700103759766
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6098040342330933 1.8140236139297485 10.679922103881836
Loss :  1.6398470401763916 1.8182369470596313 10.73103141784668
Loss :  1.6193289756774902 2.3242034912109375 13.240346908569336
Loss :  1.6068326234817505 2.8344168663024902 15.77891731262207
Loss :  1.5861263275146484 1.3065111637115479 8.118681907653809
Loss :  1.6138405799865723 4.203154563903809 22.629613876342773
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
Loss :  1.6243399381637573 4.14950704574585 22.371875762939453
Loss :  1.6243525743484497 4.157805919647217 22.41338348388672
Loss :  1.614652156829834 4.015643119812012 21.692869186401367
Total LOSS train 12.76012822664701 valid 22.276935577392578
CE LOSS train 1.6316525495969332 valid 0.4036630392074585
Contrastive LOSS train 2.225695127707261 valid 1.003910779953003
EPOCH 200:
Loss :  1.6524662971496582 2.0913894176483154 12.109413146972656
Loss :  1.6659637689590454 1.8263146877288818 10.797537803649902
Loss :  1.6347965002059937 4.210207939147949 22.685836791992188
Loss :  1.6414812803268433 2.540830135345459 14.345632553100586
Loss :  1.659808874130249 1.9849401712417603 11.58450984954834
Loss :  1.623536229133606 1.960124135017395 11.42415714263916
Loss :  1.6597336530685425 1.8628993034362793 10.974230766296387
Loss :  1.6349674463272095 3.457767963409424 18.92380714416504
Loss :  1.6260145902633667 2.4325523376464844 13.788776397705078
Loss :  1.660752296447754 3.634072780609131 19.83111572265625
Loss :  1.6171050071716309 2.3688161373138428 13.461185455322266
Loss :  1.6160448789596558 2.0380146503448486 11.80611801147461
Loss :  1.6133345365524292 2.2946159839630127 13.086414337158203
Loss :  1.6172480583190918 3.183271646499634 17.533605575561523
Loss :  1.674263834953308 2.611809730529785 14.733312606811523
Loss :  1.6685763597488403 3.019692897796631 16.76704216003418
Loss :  1.6096670627593994 2.184748888015747 12.533411026000977
Loss :  1.6352096796035767 2.4146180152893066 13.70829963684082
Loss :  1.6054691076278687 2.7606089115142822 15.408513069152832
Loss :  1.668946385383606 2.211324453353882 12.725568771362305
  batch 20 loss: 1.668946385383606, 2.211324453353882, 12.725568771362305
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6306723356246948 2.091641664505005 12.08888053894043
Loss :  1.6039284467697144 2.159015655517578 12.399006843566895
Loss :  1.6216700077056885 1.809123158454895 10.667285919189453
Loss :  1.6373028755187988 2.6708765029907227 14.99168586730957
Loss :  1.667325735092163 2.6672017574310303 15.003334999084473
Loss :  1.6245036125183105 2.7014994621276855 15.132001876831055
Loss :  1.6352349519729614 2.5563175678253174 14.41682243347168
Loss :  1.6301677227020264 2.7509610652923584 15.384973526000977
Loss :  1.5788068771362305 1.8375986814498901 10.766799926757812
Loss :  1.6652319431304932 1.961890459060669 11.47468376159668
Loss :  1.5792020559310913 1.9086849689483643 11.122626304626465
Loss :  1.6478776931762695 2.093132972717285 12.113542556762695
Loss :  1.6228426694869995 2.858191967010498 15.913803100585938
Loss :  1.620390772819519 2.129870891571045 12.269745826721191
Loss :  1.5851666927337646 2.4162254333496094 13.66629409790039
Loss :  1.6002018451690674 1.8061940670013428 10.631172180175781
Loss :  1.5988353490829468 2.8438403606414795 15.818037033081055
Loss :  1.6605103015899658 2.8587236404418945 15.95412826538086
Loss :  1.6664520502090454 2.1300270557403564 12.316587448120117
Loss :  1.6769393682479858 2.981790781021118 16.585893630981445
  batch 40 loss: 1.6769393682479858, 2.981790781021118, 16.585893630981445
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.635551929473877 2.561131477355957 14.44120979309082
Loss :  1.621898889541626 2.0466392040252686 11.855094909667969
Loss :  1.6114896535873413 2.8513877391815186 15.868428230285645
Loss :  1.624212622642517 1.9507344961166382 11.377884864807129
Loss :  1.601679801940918 1.632074236869812 9.76205062866211
Loss :  1.6318836212158203 3.4551000595092773 18.907384872436523
Loss :  1.6647008657455444 2.0337705612182617 11.833553314208984
Loss :  1.616182804107666 1.7581852674484253 10.407108306884766
Loss :  1.6790176630020142 1.8720364570617676 11.039199829101562
Loss :  1.6226073503494263 1.6140999794006348 9.693106651306152
Loss :  1.6566567420959473 2.474684476852417 14.030078887939453
Loss :  1.6512260437011719 2.476318359375 14.032817840576172
Loss :  1.6304582357406616 3.2329912185668945 17.795413970947266
Loss :  1.6593910455703735 3.704761028289795 20.183197021484375
Loss :  1.6188039779663086 2.734290599822998 15.290257453918457
Loss :  1.681787133216858 2.1323390007019043 12.343482971191406
Loss :  1.6255466938018799 2.3429906368255615 13.340499877929688
Loss :  1.6090551614761353 3.2760329246520996 17.989219665527344
Loss :  1.6252583265304565 3.110751152038574 17.179014205932617
Loss :  1.6895406246185303 2.1699671745300293 12.539377212524414
  batch 60 loss: 1.6895406246185303, 2.1699671745300293, 12.539377212524414
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.61050546169281 2.379218339920044 13.506596565246582
Loss :  1.6394474506378174 2.149082899093628 12.384861946105957
Loss :  1.6199308633804321 2.2786409854888916 13.01313591003418
Loss :  1.6081904172897339 2.5680320262908936 14.44835090637207
Loss :  1.58778715133667 1.5803154706954956 9.489364624023438
Loss :  1.596841812133789 3.546114683151245 19.327415466308594
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.608854055404663 3.3756263256073 18.48698616027832
Loss :  1.6048706769943237 3.289660692214966 18.053173065185547
Loss :  1.6165410280227661 3.099905252456665 17.11606788635254
Total LOSS train 13.841484377934382 valid 18.24591064453125
CE LOSS train 1.6332532259134145 valid 0.40413525700569153
Contrastive LOSS train 2.4416462164658768 valid 0.7749763131141663
EPOCH 201:
Loss :  1.6531331539154053 2.079165458679199 12.04896068572998
Loss :  1.666810154914856 2.4512524604797363 13.92307186126709
Loss :  1.635704517364502 2.5180511474609375 14.225959777832031
Loss :  1.6414589881896973 2.194826364517212 12.615591049194336
Loss :  1.6594927310943604 1.8558706045150757 10.93884563446045
Loss :  1.6227761507034302 3.383988857269287 18.542720794677734
Loss :  1.659417986869812 2.7203876972198486 15.261356353759766
Loss :  1.6344358921051025 2.9197683334350586 16.233278274536133
Loss :  1.6246188879013062 2.3226892948150635 13.238064765930176
Loss :  1.6589159965515137 2.063244104385376 11.975135803222656
Loss :  1.615300178527832 2.4092226028442383 13.661413192749023
Loss :  1.6140531301498413 2.954627275466919 16.387189865112305
Loss :  1.6116337776184082 3.0156545639038086 16.68990707397461
Loss :  1.6169397830963135 2.543328285217285 14.33358097076416
Loss :  1.6724908351898193 2.394219398498535 13.643588066101074
Loss :  1.6691746711730957 2.1830594539642334 12.58447265625
Loss :  1.609183669090271 2.198237657546997 12.600371360778809
Loss :  1.6362372636795044 2.803523302078247 15.653853416442871
Loss :  1.6057628393173218 2.239044427871704 12.800984382629395
Loss :  1.6678661108016968 2.3543097972869873 13.439414978027344
  batch 20 loss: 1.6678661108016968, 2.3543097972869873, 13.439414978027344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6308962106704712 2.5393385887145996 14.327589988708496
Loss :  1.6046035289764404 2.617938995361328 14.69429874420166
Loss :  1.6228525638580322 2.206448554992676 12.655095100402832
Loss :  1.6384683847427368 1.9588932991027832 11.432934761047363
Loss :  1.6687225103378296 2.2832651138305664 13.085047721862793
Loss :  1.6255483627319336 2.094451904296875 12.097807884216309
Loss :  1.6355371475219727 2.456055164337158 13.915812492370605
Loss :  1.628367304801941 2.233238697052002 12.794560432434082
Loss :  1.5759785175323486 2.19281268119812 12.54004192352295
Loss :  1.662792444229126 2.8990588188171387 16.1580867767334
Loss :  1.5775827169418335 2.55659818649292 14.360573768615723
Loss :  1.646775245666504 2.2762911319732666 13.028230667114258
Loss :  1.6215614080429077 2.096012830734253 12.101625442504883
Loss :  1.619553565979004 2.6308553218841553 14.77383041381836
Loss :  1.5847667455673218 2.778815984725952 15.478846549987793
Loss :  1.599959135055542 2.9677228927612305 16.438573837280273
Loss :  1.5986204147338867 1.9570671319961548 11.383955955505371
Loss :  1.6599916219711304 2.1744635105133057 12.532308578491211
Loss :  1.6664427518844604 2.027949571609497 11.806190490722656
Loss :  1.6766337156295776 2.419529914855957 13.774283409118652
  batch 40 loss: 1.6766337156295776, 2.419529914855957, 13.774283409118652
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.6350600719451904 2.57527494430542 14.511435508728027
Loss :  1.6211920976638794 1.6948161125183105 10.0952730178833
Loss :  1.6111810207366943 2.000983476638794 11.616098403930664
Loss :  1.6242280006408691 2.112945795059204 12.188957214355469
Loss :  1.602691650390625 2.320383310317993 13.204607963562012
Loss :  1.6332637071609497 1.8618563413619995 10.942544937133789
Loss :  1.66629159450531 2.298701047897339 13.159796714782715
Loss :  1.6180906295776367 1.9891114234924316 11.563647270202637
Loss :  1.6802181005477905 2.282388210296631 13.092159271240234
Loss :  1.6236203908920288 1.608591079711914 9.66657543182373
Loss :  1.657265305519104 2.223708152770996 12.775806427001953
Loss :  1.650976538658142 2.0974392890930176 12.138172149658203
Loss :  1.6298096179962158 2.475823402404785 14.008926391601562
Loss :  1.6581158638000488 3.0300583839416504 16.808406829833984
Loss :  1.6194093227386475 2.9370453357696533 16.304636001586914
Loss :  1.679882526397705 1.8413798809051514 10.886781692504883
Loss :  1.6245754957199097 2.0100393295288086 11.674772262573242
Loss :  1.609202265739441 2.602837324142456 14.62338924407959
Loss :  1.6242924928665161 3.012742280960083 16.688003540039062
Loss :  1.6879900693893433 2.5347354412078857 14.36166763305664
  batch 60 loss: 1.6879900693893433, 2.5347354412078857, 14.36166763305664
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6090370416641235 2.1091866493225098 12.154970169067383
Loss :  1.6385951042175293 2.099074602127075 12.133968353271484
Loss :  1.6186546087265015 2.413834810256958 13.687828063964844
Loss :  1.6063401699066162 2.647902727127075 14.845853805541992
Loss :  1.5870200395584106 1.5263216495513916 9.2186279296875
Loss :  1.6024779081344604 3.720104694366455 20.203001022338867
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.614385724067688 3.521787405014038 19.22332191467285
Loss :  1.6117591857910156 3.5187745094299316 19.205631256103516
Loss :  1.612497329711914 3.33229923248291 18.27399253845215
Total LOSS train 13.423544032757098 valid 19.226486682891846
CE LOSS train 1.6328933037244358 valid 0.4031243324279785
Contrastive LOSS train 2.3581301597448494 valid 0.8330748081207275
EPOCH 202:
Loss :  1.652953028678894 2.221895456314087 12.762430191040039
Loss :  1.6665875911712646 1.882008671760559 11.076631546020508
Loss :  1.6359171867370605 1.4984887838363647 9.128360748291016
Loss :  1.642146110534668 2.379474639892578 13.539519309997559
Loss :  1.659666657447815 1.7600739002227783 10.460036277770996
Loss :  1.6235884428024292 2.1052358150482178 12.149767875671387
Loss :  1.6597387790679932 2.5809452533721924 14.564464569091797
Loss :  1.6351739168167114 2.5789918899536133 14.530133247375488
Loss :  1.6255706548690796 2.5607593059539795 14.429367065429688
Loss :  1.660077691078186 1.8900341987609863 11.110248565673828
Loss :  1.6167188882827759 3.2030997276306152 17.632217407226562
Loss :  1.6159197092056274 2.1500232219696045 12.366036415100098
Loss :  1.61347496509552 2.8039448261260986 15.633198738098145
Loss :  1.6185835599899292 2.4363181591033936 13.800174713134766
Loss :  1.6740752458572388 2.2954981327056885 13.151565551757812
Loss :  1.6692763566970825 2.0814430713653564 12.076491355895996
Loss :  1.6102769374847412 3.575803518295288 19.489294052124023
Loss :  1.6366231441497803 2.9820656776428223 16.546951293945312
Loss :  1.6066100597381592 1.6899464130401611 10.056342124938965
Loss :  1.6689456701278687 2.3610663414001465 13.47427749633789
  batch 20 loss: 1.6689456701278687, 2.3610663414001465, 13.47427749633789
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.631460428237915 1.8758050203323364 11.010485649108887
Loss :  1.6056426763534546 2.3941879272460938 13.576581954956055
Loss :  1.6235729455947876 2.088182210922241 12.064484596252441
Loss :  1.639815092086792 1.8957741260528564 11.118685722351074
Loss :  1.670278549194336 2.2069079875946045 12.704818725585938
Loss :  1.62726891040802 2.2849276065826416 13.05190658569336
Loss :  1.637712001800537 2.0285041332244873 11.780233383178711
Loss :  1.630858063697815 2.200828790664673 12.635002136230469
Loss :  1.5793122053146362 2.6969192028045654 15.063908576965332
Loss :  1.6652823686599731 3.40470290184021 18.688796997070312
Loss :  1.580654263496399 2.241767406463623 12.789491653442383
Loss :  1.6491599082946777 2.518289089202881 14.240606307983398
Loss :  1.6240440607070923 2.07627272605896 12.00540828704834
Loss :  1.6220767498016357 2.846449851989746 15.854326248168945
Loss :  1.5871412754058838 2.7264702320098877 15.219491958618164
Loss :  1.6020859479904175 2.2696454524993896 12.950313568115234
Loss :  1.6007143259048462 2.038928508758545 11.795357704162598
Loss :  1.662657618522644 2.0714478492736816 12.019896507263184
Loss :  1.6682062149047852 1.97716224193573 11.554017066955566
Loss :  1.6776893138885498 2.323519229888916 13.29528522491455
  batch 40 loss: 1.6776893138885498, 2.323519229888916, 13.29528522491455
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.6362285614013672 2.7998907566070557 15.635682106018066
Loss :  1.6221846342086792 2.3052988052368164 13.14867877960205
Loss :  1.6120092868804932 3.0049936771392822 16.636978149414062
Loss :  1.6254957914352417 1.8278911113739014 10.764951705932617
Loss :  1.6039624214172363 1.5490059852600098 9.348992347717285
Loss :  1.6344877481460571 2.078942060470581 12.02919864654541
Loss :  1.6677747964859009 3.832615613937378 20.830852508544922
Loss :  1.619432806968689 1.8801021575927734 11.019943237304688
Loss :  1.6819535493850708 1.8715440034866333 11.039673805236816
Loss :  1.6252576112747192 1.693311333656311 10.091814041137695
Loss :  1.658703088760376 2.486555337905884 14.091479301452637
Loss :  1.6523377895355225 3.880427837371826 21.05447769165039
Loss :  1.6318026781082153 1.922377109527588 11.243687629699707
Loss :  1.6609917879104614 2.9097471237182617 16.209728240966797
Loss :  1.621103048324585 2.4599859714508057 13.921032905578613
Loss :  1.6826640367507935 4.8509202003479 25.937265396118164
Loss :  1.6275291442871094 4.870083332061768 25.97794532775879
Loss :  1.6123288869857788 4.316041469573975 23.192537307739258
Loss :  1.6272903680801392 3.331954002380371 18.28706169128418
Loss :  1.6902475357055664 2.3515894412994385 13.44819450378418
  batch 60 loss: 1.6902475357055664, 2.3515894412994385, 13.44819450378418
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6128653287887573 2.325624942779541 13.240989685058594
Loss :  1.6415296792984009 2.201042413711548 12.64674186706543
Loss :  1.622828483581543 2.1477060317993164 12.361358642578125
Loss :  1.6098856925964355 2.5212693214416504 14.216232299804688
Loss :  1.590320348739624 2.4256176948547363 13.718408584594727
Loss :  1.6366671323776245 3.8841552734375 21.057443618774414
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6469967365264893 3.9279580116271973 21.286787033081055
Loss :  1.642250895500183 3.8110671043395996 20.697586059570312
Loss :  1.6615229845046997 3.8688137531280518 21.005592346191406
Total LOSS train 14.022930981562688 valid 21.011852264404297
CE LOSS train 1.6345965018639197 valid 0.4153807461261749
Contrastive LOSS train 2.477666880534245 valid 0.9672034382820129
EPOCH 203:
Loss :  1.6562598943710327 2.8018076419830322 15.665297508239746
Loss :  1.6685112714767456 2.349457025527954 13.415796279907227
Loss :  1.637728214263916 1.9344303607940674 11.309879302978516
Loss :  1.6429243087768555 2.1078217029571533 12.182032585144043
Loss :  1.6598021984100342 2.5938379764556885 14.628992080688477
Loss :  1.6240180730819702 2.2791030406951904 13.019533157348633
Loss :  1.6597305536270142 2.749307155609131 15.406267166137695
Loss :  1.6349221467971802 2.621150493621826 14.74067497253418
Loss :  1.6251280307769775 2.493990182876587 14.095078468322754
Loss :  1.6590064764022827 2.6270370483398438 14.794191360473633
Loss :  1.616469144821167 2.586803913116455 14.550488471984863
Loss :  1.6154674291610718 2.591003894805908 14.570486068725586
Loss :  1.6124235391616821 2.500633716583252 14.115592002868652
Loss :  1.617809534072876 3.004134178161621 16.63848114013672
Loss :  1.6731384992599487 2.2925102710723877 13.135689735412598
Loss :  1.6699360609054565 2.353612184524536 13.437996864318848
Loss :  1.6091593503952026 2.340033531188965 13.309327125549316
Loss :  1.6365083456039429 3.175971508026123 17.516366958618164
Loss :  1.6062148809432983 2.3188204765319824 13.2003173828125
Loss :  1.6681658029556274 2.4724488258361816 14.030409812927246
  batch 20 loss: 1.6681658029556274, 2.4724488258361816, 14.030409812927246
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.6318198442459106 2.7886276245117188 15.574957847595215
Loss :  1.6062976121902466 2.4421777725219727 13.81718635559082
Loss :  1.6246674060821533 1.8015304803848267 10.632319450378418
Loss :  1.6405876874923706 2.0111660957336426 11.696417808532715
Loss :  1.6702512502670288 2.604027032852173 14.690385818481445
Loss :  1.6295583248138428 2.726372480392456 15.261421203613281
Loss :  1.6393338441848755 2.6065473556518555 14.672070503234863
Loss :  1.634173035621643 2.4771008491516113 14.01967716217041
Loss :  1.582045316696167 3.8279764652252197 20.721927642822266
Loss :  1.668074607849121 2.9401400089263916 16.3687744140625
Loss :  1.5826828479766846 2.8581326007843018 15.873345375061035
Loss :  1.6506718397140503 2.131866693496704 12.310005187988281
Loss :  1.6258926391601562 2.5036017894744873 14.143901824951172
Loss :  1.6231462955474854 2.589970350265503 14.572998046875
Loss :  1.5884381532669067 2.8583285808563232 15.880081176757812
Loss :  1.6028234958648682 2.7428126335144043 15.316886901855469
Loss :  1.601306676864624 2.7287347316741943 15.244980812072754
Loss :  1.6621156930923462 2.99668025970459 16.645517349243164
Loss :  1.6677336692810059 2.9490692615509033 16.4130802154541
Loss :  1.6771738529205322 2.477550983428955 14.06492805480957
  batch 40 loss: 1.6771738529205322, 2.477550983428955, 14.06492805480957
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6365455389022827 2.7730369567871094 15.501729965209961
Loss :  1.6231595277786255 2.362978219985962 13.438050270080566
Loss :  1.613067865371704 2.667325019836426 14.949692726135254
Loss :  1.626029372215271 2.792672872543335 15.589393615722656
Loss :  1.6041879653930664 1.8516361713409424 10.8623685836792
Loss :  1.6343673467636108 2.079320192337036 12.030967712402344
Loss :  1.6670973300933838 1.9010764360427856 11.172479629516602
Loss :  1.6193950176239014 3.542982578277588 19.334306716918945
Loss :  1.6806941032409668 2.748699903488159 15.4241943359375
Loss :  1.624985933303833 2.098731279373169 12.11864185333252
Loss :  1.6577603816986084 2.4172921180725098 13.744220733642578
Loss :  1.6511307954788208 1.9852594137191772 11.577427864074707
Loss :  1.6300184726715088 1.9738495349884033 11.499265670776367
Loss :  1.6585414409637451 2.670736074447632 15.012222290039062
Loss :  1.6182454824447632 2.888267755508423 16.05958366394043
Loss :  1.6815937757492065 2.439913511276245 13.8811616897583
Loss :  1.6255522966384888 2.0722246170043945 11.986675262451172
Loss :  1.608534812927246 1.9664509296417236 11.440789222717285
Loss :  1.624956488609314 2.6179089546203613 14.714500427246094
Loss :  1.6892600059509277 2.362765312194824 13.50308609008789
  batch 60 loss: 1.6892600059509277, 2.362765312194824, 13.50308609008789
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.6091164350509644 2.519509792327881 14.206665992736816
Loss :  1.6381175518035889 2.8716492652893066 15.996363639831543
Loss :  1.618565559387207 1.9435549974441528 11.33634090423584
Loss :  1.6065534353256226 3.2557857036590576 17.885482788085938
Loss :  1.5869239568710327 2.246323585510254 12.818541526794434
Loss :  1.6181491613388062 4.273590087890625 22.986099243164062
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.6300380229949951 4.257570743560791 22.917890548706055
Loss :  1.625616192817688 4.1880669593811035 22.565950393676758
Loss :  1.637487530708313 4.1718573570251465 22.496774673461914
Total LOSS train 14.273352534954364 valid 22.741678714752197
CE LOSS train 1.6344079806254461 valid 0.40937188267707825
Contrastive LOSS train 2.5277889288388766 valid 1.0429643392562866
EPOCH 204:
Loss :  1.6524935960769653 2.260817050933838 12.956578254699707
Loss :  1.6658422946929932 2.68094801902771 15.070582389831543
Loss :  1.6350502967834473 1.5729719400405884 9.499910354614258
Loss :  1.6410382986068726 1.7894970178604126 10.588522911071777
Loss :  1.6590501070022583 1.8108652830123901 10.71337604522705
Loss :  1.6227599382400513 3.16351580619812 17.440340042114258
Loss :  1.6592936515808105 2.247713565826416 12.89786148071289
Loss :  1.634184718132019 1.9971874952316284 11.620121955871582
Loss :  1.624552845954895 2.5172834396362305 14.210969924926758
Loss :  1.658900499343872 1.96211576461792 11.46947956085205
Loss :  1.6152124404907227 2.275055408477783 12.99048900604248
Loss :  1.6145362854003906 2.710087776184082 15.1649751663208
Loss :  1.6123095750808716 3.1742665767669678 17.483642578125
Loss :  1.6177772283554077 3.28322172164917 18.033885955810547
Loss :  1.6745681762695312 2.4573683738708496 13.961410522460938
Loss :  1.669288158416748 2.0145559310913086 11.742067337036133
Loss :  1.610361933708191 1.8347042798995972 10.783883094787598
Loss :  1.6364240646362305 1.7031619548797607 10.152234077453613
Loss :  1.6060855388641357 1.5278640985488892 9.245406150817871
Loss :  1.6699856519699097 2.6346027851104736 14.842999458312988
  batch 20 loss: 1.6699856519699097, 2.6346027851104736, 14.842999458312988
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6313540935516357 2.130469560623169 12.28370189666748
Loss :  1.6055790185928345 1.8278005123138428 10.744582176208496
Loss :  1.624106526374817 1.900645136833191 11.127331733703613
Loss :  1.6393108367919922 2.6184194087982178 14.73140811920166
Loss :  1.6693373918533325 2.2767274379730225 13.052974700927734
Loss :  1.6267781257629395 2.6071505546569824 14.662530899047852
Loss :  1.6367013454437256 2.8350443840026855 15.81192398071289
Loss :  1.6306513547897339 2.524766683578491 14.254485130310059
Loss :  1.5784435272216797 2.795888662338257 15.557887077331543
Loss :  1.664770245552063 3.2827351093292236 18.078445434570312
Loss :  1.5800777673721313 3.312830686569214 18.14423179626465
Loss :  1.6488076448440552 3.020037889480591 16.74899673461914
Loss :  1.6243655681610107 2.529269218444824 14.270711898803711
Loss :  1.6226886510849 2.2584965229034424 12.915170669555664
Loss :  1.58827543258667 2.8280882835388184 15.728717803955078
Loss :  1.602825403213501 1.9969898462295532 11.587774276733398
Loss :  1.601812481880188 2.691356897354126 15.05859661102295
Loss :  1.6626499891281128 2.266170024871826 12.993500709533691
Loss :  1.6678731441497803 2.467620611190796 14.005976676940918
Loss :  1.6777796745300293 2.259547710418701 12.975519180297852
  batch 40 loss: 1.6777796745300293, 2.259547710418701, 12.975519180297852
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6366831064224243 3.121685266494751 17.24510955810547
Loss :  1.622918725013733 1.8989840745925903 11.117839813232422
Loss :  1.6125452518463135 2.3145806789398193 13.18544864654541
Loss :  1.6248189210891724 2.083857536315918 12.044106483459473
Loss :  1.602692723274231 1.96941077709198 11.449746131896973
Loss :  1.6331111192703247 3.0473639965057373 16.869932174682617
Loss :  1.666347861289978 2.033698558807373 11.834840774536133
Loss :  1.6178638935089111 2.6514952182769775 14.87533950805664
Loss :  1.6809918880462646 2.8682634830474854 16.022308349609375
Loss :  1.6235103607177734 3.1621041297912598 17.434030532836914
Loss :  1.657973051071167 2.4366517066955566 13.841231346130371
Loss :  1.6514904499053955 1.7320854663848877 10.311917304992676
Loss :  1.630228877067566 2.7181179523468018 15.220818519592285
Loss :  1.659746527671814 2.4243874549865723 13.781682968139648
Loss :  1.6187282800674438 2.1409192085266113 12.323324203491211
Loss :  1.681679368019104 1.7021353244781494 10.19235610961914
Loss :  1.625895380973816 2.5743396282196045 14.497593879699707
Loss :  1.6100081205368042 1.8433986902236938 10.827001571655273
Loss :  1.6255594491958618 2.244951009750366 12.85031509399414
Loss :  1.6886144876480103 3.0427730083465576 16.90247917175293
  batch 60 loss: 1.6886144876480103, 3.0427730083465576, 16.90247917175293
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6105248928070068 1.9932440519332886 11.57674503326416
Loss :  1.640197992324829 1.8949207067489624 11.114801406860352
Loss :  1.620090365409851 2.61928391456604 14.716509819030762
Loss :  1.608217477798462 2.7977805137634277 15.59712028503418
Loss :  1.5885660648345947 2.1676716804504395 12.426923751831055
Loss :  1.612633228302002 3.830258846282959 20.763927459716797
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.6254428625106812 3.8473355770111084 20.862119674682617
Loss :  1.620465636253357 3.6863768100738525 20.052350997924805
Loss :  1.6302164793014526 3.322334051132202 18.241886138916016
Total LOSS train 13.597888095562274 valid 19.98007106781006
CE LOSS train 1.6338908947431123 valid 0.40755411982536316
Contrastive LOSS train 2.3927994379630455 valid 0.8305835127830505
EPOCH 205:
Loss :  1.6536582708358765 3.0827176570892334 17.06724739074707
Loss :  1.6664810180664062 2.357678174972534 13.454872131347656
Loss :  1.6350915431976318 2.634394407272339 14.807064056396484
Loss :  1.6411621570587158 2.2619433403015137 12.950879096984863
Loss :  1.659006953239441 2.1158885955810547 12.238450050354004
Loss :  1.6227877140045166 2.195538282394409 12.600479125976562
Loss :  1.6584135293960571 2.385653257369995 13.58668041229248
Loss :  1.6332489252090454 2.2117063999176025 12.691781044006348
Loss :  1.6232125759124756 2.004377603530884 11.645100593566895
Loss :  1.6574530601501465 2.0241096019744873 11.77800178527832
Loss :  1.6149497032165527 2.683506727218628 15.03248405456543
Loss :  1.6141407489776611 2.4320199489593506 13.774240493774414
Loss :  1.6121025085449219 2.3619654178619385 13.421929359436035
Loss :  1.6183245182037354 3.0305685997009277 16.771167755126953
Loss :  1.6736544370651245 2.4924120903015137 14.13571548461914
Loss :  1.6681329011917114 2.326171875 13.298992156982422
Loss :  1.6097394227981567 2.278564691543579 13.002562522888184
Loss :  1.6355773210525513 2.762340545654297 15.447279930114746
Loss :  1.6057435274124146 2.2503855228424072 12.857670783996582
Loss :  1.6668344736099243 2.740035057067871 15.367010116577148
  batch 20 loss: 1.6668344736099243, 2.740035057067871, 15.367010116577148
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6298433542251587 1.8186336755752563 10.72301197052002
Loss :  1.6033111810684204 2.0843758583068848 12.025190353393555
Loss :  1.6208585500717163 1.8674134016036987 10.957924842834473
Loss :  1.6373157501220703 2.3866817951202393 13.570724487304688
Loss :  1.6675515174865723 2.2828335762023926 13.081718444824219
Loss :  1.6243245601654053 2.1738243103027344 12.493446350097656
Loss :  1.635084867477417 2.0531208515167236 11.900689125061035
Loss :  1.6285313367843628 2.458116292953491 13.919113159179688
Loss :  1.5773491859436035 2.4846227169036865 14.000463485717773
Loss :  1.6634657382965088 2.6796209812164307 15.061570167541504
Loss :  1.57950758934021 2.7729666233062744 15.444340705871582
Loss :  1.6482255458831787 2.6636695861816406 14.966573715209961
Loss :  1.623784065246582 2.446233034133911 13.854948997497559
Loss :  1.621747612953186 2.608980655670166 14.666650772094727
Loss :  1.5873857736587524 2.218640089035034 12.680586814880371
Loss :  1.6021788120269775 1.950226902961731 11.353313446044922
Loss :  1.6009656190872192 2.434290885925293 13.772419929504395
Loss :  1.6619893312454224 1.8950368165969849 11.137173652648926
Loss :  1.667625904083252 1.848395824432373 10.909605026245117
Loss :  1.6776615381240845 1.794062852859497 10.64797592163086
  batch 40 loss: 1.6776615381240845, 1.794062852859497, 10.64797592163086
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.637007236480713 1.9467871189117432 11.370943069458008
Loss :  1.6232032775878906 1.8714253902435303 10.980330467224121
Loss :  1.612960934638977 2.1363601684570312 12.294761657714844
Loss :  1.6259740591049194 1.829583764076233 10.773892402648926
Loss :  1.6040273904800415 1.5624321699142456 9.41618824005127
Loss :  1.6344788074493408 2.2516062259674072 12.892509460449219
Loss :  1.6671403646469116 1.8340890407562256 10.83758544921875
Loss :  1.6201695203781128 2.291262149810791 13.0764799118042
Loss :  1.6812093257904053 2.798743486404419 15.6749267578125
Loss :  1.6257489919662476 1.917635202407837 11.213924407958984
Loss :  1.6594101190567017 2.006521701812744 11.692018508911133
Loss :  1.6526217460632324 3.096674919128418 17.135995864868164
Loss :  1.6317514181137085 2.1615376472473145 12.439438819885254
Loss :  1.6607654094696045 2.0064237117767334 11.69288444519043
Loss :  1.6200766563415527 2.1534013748168945 12.387083053588867
Loss :  1.683101773262024 2.1388165950775146 12.377184867858887
Loss :  1.6268823146820068 2.1977694034576416 12.615729331970215
Loss :  1.610484004020691 2.5342719554901123 14.281844139099121
Loss :  1.626555323600769 3.114410161972046 17.198606491088867
Loss :  1.6898022890090942 2.0497593879699707 11.9385986328125
  batch 60 loss: 1.6898022890090942, 2.0497593879699707, 11.9385986328125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6111280918121338 3.701510429382324 20.118680953979492
Loss :  1.6411879062652588 3.931938409805298 21.300880432128906
Loss :  1.6207153797149658 2.4782724380493164 14.012077331542969
Loss :  1.6094505786895752 1.8929826021194458 11.074363708496094
Loss :  1.5897599458694458 1.387848973274231 8.52900505065918
Loss :  1.617734432220459 3.6728568077087402 19.982019424438477
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.627943992614746 3.7040560245513916 20.148223876953125
Loss :  1.6253669261932373 3.545262336730957 19.35167694091797
Loss :  1.6355184316635132 3.4528825283050537 18.899930953979492
Total LOSS train 13.23727666414701 valid 19.595462799072266
CE LOSS train 1.6337544001065767 valid 0.4088796079158783
Contrastive LOSS train 2.3207044454721304 valid 0.8632206320762634
EPOCH 206:
Loss :  1.655470371246338 2.1541593074798584 12.426267623901367
Loss :  1.6687124967575073 2.386287212371826 13.600149154663086
Loss :  1.637681007385254 1.8225573301315308 10.750467300415039
Loss :  1.6432523727416992 2.8085989952087402 15.686247825622559
Loss :  1.660583734512329 2.3483924865722656 13.402545928955078
Loss :  1.6248561143875122 2.43306040763855 13.79015827178955
Loss :  1.660400152206421 2.0255892276763916 11.788346290588379
Loss :  1.6358916759490967 3.0021872520446777 16.646827697753906
Loss :  1.6262880563735962 2.223952293395996 12.746049880981445
Loss :  1.6605421304702759 1.9284757375717163 11.302921295166016
Loss :  1.6170789003372192 2.118443489074707 12.209296226501465
Loss :  1.6159924268722534 2.2432215213775635 12.832099914550781
Loss :  1.6134390830993652 1.9026665687561035 11.126771926879883
Loss :  1.618272304534912 2.397540807723999 13.605976104736328
Loss :  1.6744976043701172 4.094325065612793 22.1461238861084
Loss :  1.669517993927002 3.480208158493042 19.070558547973633
Loss :  1.610344409942627 2.2475054264068604 12.847871780395508
Loss :  1.6367213726043701 2.0600364208221436 11.936903953552246
Loss :  1.6063255071640015 1.5321764945983887 9.267208099365234
Loss :  1.6684852838516235 3.0652923583984375 16.99494743347168
  batch 20 loss: 1.6684852838516235, 3.0652923583984375, 16.99494743347168
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4], device='cuda:0')
Loss :  1.6310608386993408 2.622858762741089 14.745354652404785
Loss :  1.6049535274505615 2.2049601078033447 12.629754066467285
Loss :  1.6225874423980713 2.3035058975219727 13.140116691589355
Loss :  1.6383988857269287 2.4870314598083496 14.073556900024414
Loss :  1.6682260036468506 2.104747772216797 12.191965103149414
Loss :  1.6256027221679688 2.0302724838256836 11.776965141296387
Loss :  1.6360743045806885 1.9896149635314941 11.584148406982422
Loss :  1.6295440196990967 3.092111349105835 17.09010124206543
Loss :  1.578765869140625 2.418673038482666 13.672130584716797
Loss :  1.6644991636276245 2.191270351409912 12.620851516723633
Loss :  1.5802980661392212 2.0367796421051025 11.764196395874023
Loss :  1.6491248607635498 1.846254587173462 10.88039779663086
Loss :  1.6245434284210205 1.8078515529632568 10.663801193237305
Loss :  1.6228333711624146 2.061994791030884 11.932806968688965
Loss :  1.5892095565795898 2.727735757827759 15.227888107299805
Loss :  1.6038628816604614 2.6469764709472656 14.8387451171875
Loss :  1.602942943572998 2.8005640506744385 15.605762481689453
Loss :  1.6631050109863281 2.5415070056915283 14.37063980102539
Loss :  1.66808021068573 2.2380826473236084 12.85849380493164
Loss :  1.6778435707092285 2.441697120666504 13.886329650878906
  batch 40 loss: 1.6778435707092285, 2.441697120666504, 13.886329650878906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6373695135116577 3.361863613128662 18.446687698364258
Loss :  1.623550295829773 2.6396498680114746 14.821800231933594
Loss :  1.6140124797821045 2.452878713607788 13.878406524658203
Loss :  1.6265658140182495 2.207228183746338 12.66270637512207
Loss :  1.604854702949524 2.4680988788604736 13.945348739624023
Loss :  1.63467538356781 1.9724971055984497 11.497160911560059
Loss :  1.6673521995544434 1.966567873954773 11.500190734863281
Loss :  1.6193363666534424 2.281816005706787 13.028416633605957
Loss :  1.6807432174682617 2.405184745788574 13.706666946411133
Loss :  1.624894380569458 1.8761508464813232 11.005648612976074
Loss :  1.6580621004104614 2.753603219985962 15.426077842712402
Loss :  1.6514856815338135 2.526179552078247 14.28238296508789
Loss :  1.6305235624313354 2.319610118865967 13.228574752807617
Loss :  1.6592644453048706 2.545279026031494 14.385659217834473
Loss :  1.620060682296753 2.863292694091797 15.936524391174316
Loss :  1.6821931600570679 2.8026230335235596 15.695307731628418
Loss :  1.6269820928573608 1.9012821912765503 11.133392333984375
Loss :  1.6113232374191284 1.9744576215744019 11.483611106872559
Loss :  1.6270573139190674 2.4292314052581787 13.773214340209961
Loss :  1.6903313398361206 1.9505149126052856 11.442906379699707
  batch 60 loss: 1.6903313398361206, 1.9505149126052856, 11.442906379699707
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.611669659614563 2.0129973888397217 11.676656723022461
Loss :  1.6412289142608643 2.3366620540618896 13.324539184570312
Loss :  1.6213650703430176 2.392631769180298 13.584524154663086
Loss :  1.6094106435775757 2.698413848876953 15.101479530334473
Loss :  1.5899807214736938 2.109541177749634 12.137686729431152
Loss :  1.6238670349121094 4.149153232574463 22.369632720947266
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6356744766235352 4.182971477508545 22.550533294677734
Loss :  1.6310373544692993 4.0089616775512695 21.675844192504883
Loss :  1.6435878276824951 4.095028877258301 22.118730545043945
Total LOSS train 13.489804854759804 valid 22.178685188293457
CE LOSS train 1.6346185023968036 valid 0.4108969569206238
Contrastive LOSS train 2.371037264970633 valid 1.0237572193145752
EPOCH 207:
Loss :  1.655335545539856 2.5929441452026367 14.62005615234375
Loss :  1.6678146123886108 2.3070247173309326 13.202938079833984
Loss :  1.6373445987701416 1.7285864353179932 10.28027629852295
Loss :  1.643044352531433 2.2136497497558594 12.71129322052002
Loss :  1.6602957248687744 1.8595421314239502 10.958005905151367
Loss :  1.6243515014648438 2.18038010597229 12.526251792907715
Loss :  1.659886121749878 2.589386463165283 14.606818199157715
Loss :  1.6353974342346191 2.1957778930664062 12.614286422729492
Loss :  1.6259056329727173 2.108147382736206 12.166643142700195
Loss :  1.6601768732070923 2.1673004627227783 12.496679306030273
Loss :  1.617383599281311 3.0006470680236816 16.62061882019043
Loss :  1.6164406538009644 3.5427889823913574 19.330385208129883
Loss :  1.6138417720794678 2.258923292160034 12.908458709716797
Loss :  1.6200379133224487 2.3580801486968994 13.410438537597656
Loss :  1.6749502420425415 2.011314868927002 11.731524467468262
Loss :  1.6697611808776855 2.0356452465057373 11.84798812866211
Loss :  1.6116873025894165 2.321497678756714 13.219176292419434
Loss :  1.6373720169067383 2.0260586738586426 11.767664909362793
Loss :  1.6078526973724365 2.670210361480713 14.958904266357422
Loss :  1.670049786567688 2.125699520111084 12.298547744750977
  batch 20 loss: 1.670049786567688, 2.125699520111084, 12.298547744750977
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6334152221679688 2.1941778659820557 12.604304313659668
Loss :  1.6079334020614624 2.655726194381714 14.886564254760742
Loss :  1.6255865097045898 1.8066953420639038 10.659063339233398
Loss :  1.6408536434173584 2.628849744796753 14.785102844238281
Loss :  1.6701613664627075 2.338292121887207 13.361621856689453
Loss :  1.6278157234191895 2.2845265865325928 13.05044937133789
Loss :  1.6377674341201782 2.2053678035736084 12.664607048034668
Loss :  1.631659984588623 2.458632230758667 13.924821853637695
Loss :  1.5808401107788086 3.411654233932495 18.639110565185547
Loss :  1.6655654907226562 2.1685304641723633 12.508217811584473
Loss :  1.5823811292648315 1.9539802074432373 11.352282524108887
Loss :  1.649802327156067 1.9679566621780396 11.489585876464844
Loss :  1.6253670454025269 1.9752548933029175 11.501641273498535
Loss :  1.6234415769577026 1.992265224456787 11.584768295288086
Loss :  1.5893727540969849 2.8840012550354004 16.00937843322754
Loss :  1.6039944887161255 2.1167044639587402 12.187517166137695
Loss :  1.6027629375457764 2.43479585647583 13.776741981506348
Loss :  1.6629682779312134 2.4534223079681396 13.93008041381836
Loss :  1.6685787439346313 2.434000253677368 13.838580131530762
Loss :  1.6787245273590088 2.4617393016815186 13.987421035766602
  batch 40 loss: 1.6787245273590088, 2.4617393016815186, 13.987421035766602
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6384830474853516 2.3801536560058594 13.539251327514648
Loss :  1.6250725984573364 2.4608774185180664 13.929459571838379
Loss :  1.6156017780303955 2.2859795093536377 13.045498847961426
Loss :  1.628571629524231 3.7631471157073975 20.444307327270508
Loss :  1.6074200868606567 2.5704643726348877 14.459741592407227
Loss :  1.6371277570724487 2.3734636306762695 13.504446029663086
Loss :  1.6702146530151367 2.119229793548584 12.266364097595215
Loss :  1.6222755908966064 2.0474021434783936 11.859286308288574
Loss :  1.6835885047912598 1.8538118600845337 10.952648162841797
Loss :  1.6270486116409302 1.7024933099746704 10.139514923095703
Loss :  1.6601029634475708 1.7992172241210938 10.65618896484375
Loss :  1.6529762744903564 2.3692996501922607 13.49947452545166
Loss :  1.6314308643341064 3.9140193462371826 21.201528549194336
Loss :  1.659690022468567 2.525965452194214 14.289517402648926
Loss :  1.6207300424575806 2.907111883163452 16.15629005432129
Loss :  1.6815885305404663 2.345337152481079 13.408273696899414
Loss :  1.627279281616211 2.4070281982421875 13.662420272827148
Loss :  1.6119569540023804 2.5610556602478027 14.417235374450684
Loss :  1.627611517906189 3.4164397716522217 18.709810256958008
Loss :  1.689947485923767 2.1121504306793213 12.250699043273926
  batch 60 loss: 1.689947485923767, 2.1121504306793213, 12.250699043273926
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6122914552688599 2.4633383750915527 13.928983688354492
Loss :  1.642159104347229 2.3621888160705566 13.453103065490723
Loss :  1.622354507446289 2.9486894607543945 16.365802764892578
Loss :  1.6118537187576294 2.888638973236084 16.055049896240234
Loss :  1.5928304195404053 2.1219351291656494 12.202506065368652
Loss :  1.6232935190200806 4.096030235290527 22.103445053100586
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6345354318618774 4.146017074584961 22.364620208740234
Loss :  1.6331478357315063 4.002139091491699 21.643844604492188
Loss :  1.6339610815048218 3.9479968547821045 21.373945236206055
Total LOSS train 13.621787966214693 valid 21.871463775634766
CE LOSS train 1.6356635332107543 valid 0.40849027037620544
Contrastive LOSS train 2.3972248719288753 valid 0.9869992136955261
EPOCH 208:
Loss :  1.6560412645339966 2.5873289108276367 14.59268569946289
Loss :  1.6685948371887207 2.6770005226135254 15.053596496582031
Loss :  1.6386678218841553 2.0329298973083496 11.80331802368164
Loss :  1.6448074579238892 2.166276216506958 12.476188659667969
Loss :  1.6620972156524658 3.5036675930023193 19.180435180664062
Loss :  1.6261224746704102 1.9723728895187378 11.48798656463623
Loss :  1.6611361503601074 2.501546621322632 14.168869018554688
Loss :  1.6376409530639648 2.794086217880249 15.608072280883789
Loss :  1.6281057596206665 2.0977723598480225 12.116968154907227
Loss :  1.6617146730422974 2.035167694091797 11.837553024291992
Loss :  1.6187001466751099 2.365929126739502 13.448345184326172
Loss :  1.6173712015151978 2.16502046585083 12.442473411560059
Loss :  1.6153589487075806 2.0659804344177246 11.94526195526123
Loss :  1.6208457946777344 2.2452633380889893 12.847162246704102
Loss :  1.6751720905303955 3.8835439682006836 21.092893600463867
Loss :  1.6701209545135498 2.5775015354156494 14.557628631591797
Loss :  1.6128541231155396 2.548569917678833 14.355703353881836
Loss :  1.6384971141815186 1.6966516971588135 10.121755599975586
Loss :  1.6096503734588623 1.7112897634506226 10.166099548339844
Loss :  1.6698826551437378 1.9951012134552002 11.64538860321045
  batch 20 loss: 1.6698826551437378, 1.9951012134552002, 11.64538860321045
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6346956491470337 2.1040146350860596 12.154768943786621
Loss :  1.60897958278656 2.023599863052368 11.726978302001953
Loss :  1.626460313796997 3.872649669647217 20.989709854125977
Loss :  1.6416906118392944 2.3947789669036865 13.615585327148438
Loss :  1.6703925132751465 2.5902345180511475 14.621564865112305
Loss :  1.6287330389022827 3.8979623317718506 21.11854362487793
Loss :  1.6386052370071411 3.926914691925049 21.273178100585938
Loss :  1.632380723953247 1.9011764526367188 11.138262748718262
Loss :  1.5821760892868042 1.9849872589111328 11.507112503051758
Loss :  1.6658923625946045 2.0951921939849854 12.141853332519531
Loss :  1.5833269357681274 2.4874086380004883 14.020370483398438
Loss :  1.6504440307617188 2.8027563095092773 15.664225578308105
Loss :  1.6266142129898071 2.2831056118011475 13.042142868041992
Loss :  1.624771237373352 2.6051247119903564 14.650394439697266
Loss :  1.5910370349884033 2.653735876083374 14.859716415405273
Loss :  1.6058100461959839 3.374695301055908 18.479286193847656
Loss :  1.6047179698944092 2.0597822666168213 11.903629302978516
Loss :  1.6637033224105835 2.1547083854675293 12.43724536895752
Loss :  1.6689581871032715 2.523404598236084 14.285982131958008
Loss :  1.6781609058380127 2.153303384780884 12.444677352905273
  batch 40 loss: 1.6781609058380127, 2.153303384780884, 12.444677352905273
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6379362344741821 2.31765079498291 13.226190567016602
Loss :  1.6237245798110962 2.416247606277466 13.704962730407715
Loss :  1.6142632961273193 2.1640799045562744 12.434662818908691
Loss :  1.6262801885604858 2.0108120441436768 11.680339813232422
Loss :  1.605189323425293 2.581244707107544 14.511412620544434
Loss :  1.635064721107483 3.0257112979888916 16.763620376586914
Loss :  1.6679309606552124 2.153944730758667 12.437654495239258
Loss :  1.6206191778182983 1.7034357786178589 10.137797355651855
Loss :  1.6821599006652832 1.7833772897720337 10.59904670715332
Loss :  1.6266406774520874 2.349353075027466 13.373405456542969
Loss :  1.6599817276000977 2.822009563446045 15.77003002166748
Loss :  1.6533933877944946 3.5425655841827393 19.366220474243164
Loss :  1.632513165473938 1.6737806797027588 10.001416206359863
Loss :  1.6607258319854736 2.3342411518096924 13.331931114196777
Loss :  1.6221891641616821 2.0682976245880127 11.963677406311035
Loss :  1.683087706565857 2.024712085723877 11.806647300720215
Loss :  1.6286952495574951 2.1425743103027344 12.341567039489746
Loss :  1.6130375862121582 1.81523597240448 10.689216613769531
Loss :  1.6287704706192017 2.3938164710998535 13.59785270690918
Loss :  1.691404104232788 2.0964088439941406 12.17344856262207
  batch 60 loss: 1.691404104232788, 2.0964088439941406, 12.17344856262207
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.6137263774871826 2.3510255813598633 13.368854522705078
Loss :  1.6424005031585693 1.9283851385116577 11.284326553344727
Loss :  1.6223196983337402 1.5030275583267212 9.137456893920898
Loss :  1.6108267307281494 2.056577205657959 11.893712997436523
Loss :  1.5910840034484863 1.437029480934143 8.77623176574707
Loss :  1.626102328300476 4.254144191741943 22.89682388305664
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6374800205230713 4.283787727355957 23.05641746520996
Loss :  1.6331545114517212 4.1764044761657715 22.51517677307129
Loss :  1.6458454132080078 4.194947242736816 22.620580673217773
Total LOSS train 13.498389170720028 valid 22.772249698638916
CE LOSS train 1.6362291812896728 valid 0.41146135330200195
Contrastive LOSS train 2.3724320081564096 valid 1.048736810684204
EPOCH 209:
Loss :  1.6556525230407715 1.6856186389923096 10.083745956420898
Loss :  1.6689168214797974 2.5173933506011963 14.25588321685791
Loss :  1.638240098953247 1.685141921043396 10.063949584960938
Loss :  1.6440119743347168 1.6883622407913208 10.085823059082031
Loss :  1.6615052223205566 2.0238826274871826 11.78091812133789
Loss :  1.6257575750350952 2.2404909133911133 12.828211784362793
Loss :  1.6615015268325806 2.8082730770111084 15.70286750793457
Loss :  1.636948823928833 1.5666155815124512 9.470026969909668
Loss :  1.6275911331176758 1.85562002658844 10.905691146850586
Loss :  1.6621549129486084 2.141629457473755 12.370302200317383
Loss :  1.6193454265594482 2.3936564922332764 13.587628364562988
Loss :  1.618515133857727 2.1908066272735596 12.572547912597656
Loss :  1.6167354583740234 2.1753053665161133 12.49326229095459
Loss :  1.622025728225708 2.738199472427368 15.31302261352539
Loss :  1.6775697469711304 2.612205982208252 14.738598823547363
Loss :  1.672352910041809 2.1877732276916504 12.611218452453613
Loss :  1.6145539283752441 1.7724106311798096 10.476606369018555
Loss :  1.6404436826705933 2.362348794937134 13.452187538146973
Loss :  1.6105859279632568 2.426926612854004 13.745219230651855
Loss :  1.6718072891235352 3.738071918487549 20.362167358398438
  batch 20 loss: 1.6718072891235352, 3.738071918487549, 20.362167358398438
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.635075569152832 1.9139487743377686 11.204819679260254
Loss :  1.6104650497436523 1.7007323503494263 10.114127159118652
Loss :  1.6287695169448853 2.202150583267212 12.639522552490234
Loss :  1.643609881401062 1.7502280473709106 10.394750595092773
Loss :  1.6726113557815552 2.1717782020568848 12.531501770019531
Loss :  1.6314195394515991 1.7378393411636353 10.320615768432617
Loss :  1.6415791511535645 2.356119394302368 13.422176361083984
Loss :  1.6363937854766846 2.2197840213775635 12.735313415527344
Loss :  1.5852017402648926 2.607398748397827 14.622196197509766
Loss :  1.6686890125274658 2.621297597885132 14.775177001953125
Loss :  1.5853967666625977 2.181408405303955 12.492438316345215
Loss :  1.651875615119934 2.102297067642212 12.163360595703125
Loss :  1.6279335021972656 2.10253643989563 12.140615463256836
Loss :  1.6257405281066895 2.041496753692627 11.833223342895508
Loss :  1.5914582014083862 2.356470823287964 13.373812675476074
Loss :  1.605849266052246 2.3171961307525635 13.191829681396484
Loss :  1.6050137281417847 2.2285449504852295 12.7477388381958
Loss :  1.6649830341339111 3.0576071739196777 16.953020095825195
Loss :  1.6696628332138062 2.656494140625 14.952133178710938
Loss :  1.6801140308380127 2.5725581645965576 14.5429048538208
  batch 40 loss: 1.6801140308380127, 2.5725581645965576, 14.5429048538208
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.6396872997283936 2.265230894088745 12.965842247009277
Loss :  1.6270713806152344 1.9775875806808472 11.515008926391602
Loss :  1.6163630485534668 2.5452775955200195 14.342750549316406
Loss :  1.6293609142303467 2.3581953048706055 13.420337677001953
Loss :  1.607141375541687 2.64440655708313 14.829174041748047
Loss :  1.6367753744125366 2.679094076156616 15.032245635986328
Loss :  1.669060230255127 3.535752773284912 19.347824096679688
Loss :  1.6215965747833252 2.122270345687866 12.232948303222656
Loss :  1.6829769611358643 2.2185709476470947 12.77583122253418
Loss :  1.6272170543670654 2.081662654876709 12.035531044006348
Loss :  1.6599253416061401 2.839812994003296 15.858990669250488
Loss :  1.6535873413085938 2.647937536239624 14.893275260925293
Loss :  1.632989525794983 2.5449113845825195 14.35754680633545
Loss :  1.661554217338562 2.6161789894104004 14.742448806762695
Loss :  1.621566653251648 2.492448091506958 14.083806991577148
Loss :  1.6836076974868774 1.9413803815841675 11.390509605407715
Loss :  1.6291227340698242 2.032985210418701 11.794049263000488
Loss :  1.613444209098816 2.0151174068450928 11.689031600952148
Loss :  1.6289026737213135 1.9561883211135864 11.409844398498535
Loss :  1.6910492181777954 1.918778657913208 11.284942626953125
  batch 60 loss: 1.6910492181777954, 1.918778657913208, 11.284942626953125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4], device='cuda:0')
Loss :  1.6144670248031616 2.11914324760437 12.210183143615723
Loss :  1.64324152469635 2.3635823726654053 13.461153984069824
Loss :  1.6237415075302124 2.063624620437622 11.941864013671875
Loss :  1.6122905015945435 1.9171223640441895 11.197901725769043
Loss :  1.5929436683654785 1.4642157554626465 8.914022445678711
Loss :  1.6033179759979248 3.7722084522247314 20.464359283447266
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
Loss :  1.6154658794403076 3.7156383991241455 20.19365692138672
Loss :  1.6120843887329102 3.578197479248047 19.503070831298828
Loss :  1.6200577020645142 3.4164376258850098 18.702245712280273
Total LOSS train 12.950434171236479 valid 19.71583318710327
CE LOSS train 1.6373498769906851 valid 0.40501442551612854
Contrastive LOSS train 2.2626168636175303 valid 0.8541094064712524
EPOCH 210:
Loss :  1.6568982601165771 1.84418785572052 10.877838134765625
Loss :  1.6701716184616089 1.9412705898284912 11.376524925231934
Loss :  1.6396863460540771 2.359637975692749 13.43787670135498
Loss :  1.645409107208252 1.9554368257522583 11.42259407043457
Loss :  1.6624222993850708 2.0848240852355957 12.086542129516602
Loss :  1.6270980834960938 1.66286301612854 9.941412925720215
Loss :  1.6621310710906982 2.1415798664093018 12.370030403137207
Loss :  1.6382932662963867 1.964753270149231 11.46205997467041
Loss :  1.629257321357727 2.1180777549743652 12.219646453857422
Loss :  1.662414312362671 1.6718586683273315 10.021707534790039
Loss :  1.6205883026123047 3.8739206790924072 20.990192413330078
Loss :  1.6199679374694824 1.8760511875152588 11.000223159790039
Loss :  1.6176986694335938 1.709875226020813 10.167075157165527
Loss :  1.6229033470153809 2.252865791320801 12.887231826782227
Loss :  1.6775636672973633 3.0261423587799072 16.80827522277832
Loss :  1.6727076768875122 3.333164930343628 18.338533401489258
Loss :  1.615351915359497 1.8088665008544922 10.659684181213379
Loss :  1.6409884691238403 1.9732369184494019 11.507172584533691
Loss :  1.6115198135375977 1.5108705759048462 9.165872573852539
Loss :  1.6713738441467285 2.401531457901001 13.679031372070312
  batch 20 loss: 1.6713738441467285, 2.401531457901001, 13.679031372070312
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6349228620529175 2.5249290466308594 14.259568214416504
Loss :  1.6099869012832642 2.4621548652648926 13.920761108398438
Loss :  1.6272315979003906 2.3005943298339844 13.130203247070312
Loss :  1.6429656744003296 3.31011962890625 18.19356346130371
Loss :  1.6715729236602783 2.1120898723602295 12.232022285461426
Loss :  1.6305201053619385 1.780362606048584 10.532333374023438
Loss :  1.640628457069397 2.3161094188690186 13.221176147460938
Loss :  1.6342295408248901 2.147590398788452 12.37218189239502
Loss :  1.5846306085586548 1.847171664237976 10.820488929748535
Loss :  1.6678440570831299 2.449962615966797 13.917656898498535
Loss :  1.586118459701538 2.7973411083221436 15.572824478149414
Loss :  1.652880311012268 2.2795207500457764 13.050484657287598
Loss :  1.628909945487976 2.9476537704467773 16.367177963256836
Loss :  1.6268969774246216 1.866313099861145 10.958462715148926
Loss :  1.5935571193695068 2.0079827308654785 11.63347053527832
Loss :  1.6076475381851196 1.7237823009490967 10.22655963897705
Loss :  1.6065337657928467 2.0686917304992676 11.949992179870605
Loss :  1.666316032409668 2.0620996952056885 11.976814270019531
Loss :  1.6712833642959595 2.239853620529175 12.870552062988281
Loss :  1.6808701753616333 1.9106401205062866 11.234070777893066
  batch 40 loss: 1.6808701753616333, 1.9106401205062866, 11.234070777893066
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6408343315124512 2.7516229152679443 15.398948669433594
Loss :  1.6277447938919067 1.8755956888198853 11.005722999572754
Loss :  1.6178538799285889 2.5342869758605957 14.289288520812988
Loss :  1.6308180093765259 2.8525984287261963 15.893810272216797
Loss :  1.6099478006362915 1.4840043783187866 9.029970169067383
Loss :  1.6388828754425049 2.5648465156555176 14.463114738464355
Loss :  1.6710866689682007 2.3559422492980957 13.450797080993652
Loss :  1.6245067119598389 2.040585517883301 11.827434539794922
Loss :  1.6844446659088135 1.9406542778015137 11.387716293334961
Loss :  1.6303071975708008 1.7754290103912354 10.507452011108398
Loss :  1.6625018119812012 1.9430949687957764 11.37797737121582
Loss :  1.6561782360076904 2.8522424697875977 15.917390823364258
Loss :  1.6362049579620361 2.716094732284546 15.216678619384766
Loss :  1.6635818481445312 2.7130942344665527 15.229053497314453
Loss :  1.6248375177383423 2.862790107727051 15.938788414001465
Loss :  1.684730052947998 2.7969653606414795 15.669557571411133
Loss :  1.6307036876678467 2.811375617980957 15.687582015991211
Loss :  1.6148500442504883 2.1543116569519043 12.386408805847168
Loss :  1.6298651695251465 2.457663059234619 13.918180465698242
Loss :  1.6912049055099487 2.1108524799346924 12.245467185974121
  batch 60 loss: 1.6912049055099487, 2.1108524799346924, 12.245467185974121
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4], device='cuda:0')
Loss :  1.6151113510131836 2.1467058658599854 12.348640441894531
Loss :  1.6434929370880127 1.8771579265594482 11.029282569885254
Loss :  1.6250523328781128 1.8099987506866455 10.67504596710205
Loss :  1.6138867139816284 2.3355801105499268 13.291787147521973
Loss :  1.5947365760803223 1.5993518829345703 9.591495513916016
Loss :  1.637202501296997 4.24120569229126 22.843231201171875
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.648252010345459 4.269073009490967 22.99361801147461
Loss :  1.645166277885437 3.9800758361816406 21.54554557800293
Loss :  1.653300166130066 4.070685863494873 22.006729125976562
Total LOSS train 12.871345872145433 valid 22.347280979156494
CE LOSS train 1.6383593357526338 valid 0.4133250415325165
Contrastive LOSS train 2.2465972937070404 valid 1.0176714658737183
EPOCH 211:
Loss :  1.6582269668579102 1.9363545179367065 11.339999198913574
Loss :  1.6710693836212158 2.8457648754119873 15.899893760681152
Loss :  1.6412773132324219 1.663841962814331 9.960487365722656
Loss :  1.6465376615524292 2.463090419769287 13.961990356445312
Loss :  1.6634628772735596 1.769673228263855 10.511828422546387
Loss :  1.6285265684127808 1.8713010549545288 10.985032081604004
Loss :  1.6628868579864502 1.883333683013916 11.079554557800293
Loss :  1.6392449140548706 2.1083602905273438 12.181046485900879
Loss :  1.6293734312057495 3.0655181407928467 16.95696449279785
Loss :  1.6632694005966187 2.068530321121216 12.00592041015625
Loss :  1.6217963695526123 2.249309539794922 12.8683443069458
Loss :  1.6211469173431396 2.075899362564087 12.000643730163574
Loss :  1.6190379858016968 2.5377273559570312 14.307674407958984
Loss :  1.624719262123108 2.4701528549194336 13.975483894348145
Loss :  1.6789429187774658 2.3496322631835938 13.427103996276855
Loss :  1.6737873554229736 2.565310478210449 14.50033950805664
Loss :  1.6174639463424683 2.8612618446350098 15.923772811889648
Loss :  1.6425085067749023 1.7985650300979614 10.635334014892578
Loss :  1.613793969154358 1.6746845245361328 9.98721694946289
Loss :  1.6732624769210815 1.8863188028335571 11.104856491088867
  batch 20 loss: 1.6732624769210815, 1.8863188028335571, 11.104856491088867
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4], device='cuda:0')
Loss :  1.6373695135116577 1.8999354839324951 11.137046813964844
Loss :  1.612284541130066 3.3279879093170166 18.25222396850586
Loss :  1.6300091743469238 2.3180994987487793 13.22050666809082
Loss :  1.6451592445373535 2.5877809524536133 14.584064483642578
Loss :  1.673404574394226 2.32267689704895 13.286788940429688
Loss :  1.6334009170532227 2.5825273990631104 14.546037673950195
Loss :  1.6433444023132324 2.4817099571228027 14.051895141601562
Loss :  1.6377073526382446 1.999536395072937 11.63538932800293
Loss :  1.5886625051498413 2.0879507064819336 12.02841567993164
Loss :  1.6704037189483643 2.653167963027954 14.936243057250977
Loss :  1.5895799398422241 2.1377015113830566 12.27808666229248
Loss :  1.654581904411316 2.484186887741089 14.075516700744629
Loss :  1.6313520669937134 3.1520347595214844 17.391525268554688
Loss :  1.6301038265228271 2.313225507736206 13.196231842041016
Loss :  1.5973868370056152 2.341710329055786 13.305938720703125
Loss :  1.6113108396530151 1.8770326375961304 10.996474266052246
Loss :  1.6104962825775146 2.3752024173736572 13.4865083694458
Loss :  1.6676459312438965 2.0777175426483154 12.056234359741211
Loss :  1.6727681159973145 1.892560362815857 11.135570526123047
Loss :  1.6817312240600586 1.8400137424468994 10.881799697875977
  batch 40 loss: 1.6817312240600586, 1.8400137424468994, 10.881799697875977
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6428178548812866 1.8844166994094849 11.064901351928711
Loss :  1.6290152072906494 1.7993727922439575 10.625879287719727
Loss :  1.6198934316635132 1.8601500988006592 10.92064380645752
Loss :  1.6317243576049805 3.3132870197296143 18.198158264160156
Loss :  1.611613154411316 1.6629765033721924 9.926495552062988
Loss :  1.6404694318771362 1.982806921005249 11.55450439453125
Loss :  1.6717458963394165 2.110018253326416 12.221837043762207
Loss :  1.626436710357666 2.5682568550109863 14.467720031738281
Loss :  1.68573796749115 2.097588300704956 12.17367935180664
Loss :  1.6321312189102173 1.8963297605514526 11.11378002166748
Loss :  1.6643321514129639 2.132002353668213 12.32434368133545
Loss :  1.6585859060287476 2.018505811691284 11.751114845275879
Loss :  1.6388821601867676 1.9015034437179565 11.146398544311523
Loss :  1.6657487154006958 2.881242275238037 16.07196044921875
Loss :  1.6290251016616821 2.678311347961426 15.02058219909668
Loss :  1.6870720386505127 1.7328307628631592 10.351225852966309
Loss :  1.6347715854644775 2.2757742404937744 13.013642311096191
Loss :  1.6198487281799316 2.1223127841949463 12.231412887573242
Loss :  1.6344773769378662 2.4496657848358154 13.882806777954102
Loss :  1.6944630146026611 1.97760808467865 11.582503318786621
  batch 60 loss: 1.6944630146026611, 1.97760808467865, 11.582503318786621
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6199222803115845 2.637038230895996 14.805113792419434
Loss :  1.6477171182632446 2.247929334640503 12.887364387512207
Loss :  1.6284465789794922 2.1799252033233643 12.528072357177734
Loss :  1.6173334121704102 2.4708681106567383 13.971673965454102
Loss :  1.5980827808380127 1.7584017515182495 10.390090942382812
Loss :  1.6148507595062256 3.877197742462158 21.000839233398438
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.626489281654358 3.634371757507324 19.798349380493164
Loss :  1.623892068862915 3.6062934398651123 19.655359268188477
Loss :  1.6289921998977661 3.6114566326141357 19.686275482177734
Total LOSS train 12.835721397399903 valid 20.035205841064453
CE LOSS train 1.6406051103885357 valid 0.40724804997444153
Contrastive LOSS train 2.239023263637836 valid 0.9028641581535339
EPOCH 212:
Loss :  1.6599845886230469 1.76227867603302 10.471378326416016
Loss :  1.672432780265808 2.579517364501953 14.570019721984863
Loss :  1.6428202390670776 2.4636361598968506 13.9610013961792
Loss :  1.648646593093872 2.595618486404419 14.626738548278809
Loss :  1.6654655933380127 2.395779848098755 13.644364356994629
Loss :  1.631237268447876 1.6973869800567627 10.118171691894531
Loss :  1.665487289428711 1.8744479417800903 11.037727355957031
Loss :  1.642316222190857 1.7890678644180298 10.587655067443848
Loss :  1.6336148977279663 1.7140644788742065 10.203936576843262
Loss :  1.6666734218597412 2.1788527965545654 12.560937881469727
Loss :  1.6256965398788452 2.4831788539886475 14.041590690612793
Loss :  1.625044345855713 2.5283043384552 14.266565322875977
Loss :  1.6230489015579224 1.8792221546173096 11.019159317016602
Loss :  1.6283985376358032 2.4299092292785645 13.777944564819336
Loss :  1.6812357902526855 2.026808023452759 11.815275192260742
Loss :  1.6765685081481934 1.7609610557556152 10.481374740600586
Loss :  1.621323823928833 2.071840524673462 11.980525970458984
Loss :  1.6460604667663574 2.362806558609009 13.460092544555664
Loss :  1.6174311637878418 2.5086569786071777 14.160717010498047
Loss :  1.6760499477386475 1.8245959281921387 10.799030303955078
  batch 20 loss: 1.6760499477386475, 1.8245959281921387, 10.799030303955078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6407395601272583 2.1712143421173096 12.496810913085938
Loss :  1.6165581941604614 1.7841089963912964 10.537102699279785
Loss :  1.6334482431411743 1.521605372428894 9.241475105285645
Loss :  1.6478395462036133 2.404526710510254 13.670473098754883
Loss :  1.6758580207824707 1.851298451423645 10.932350158691406
Loss :  1.6361346244812012 2.7374396324157715 15.323333740234375
Loss :  1.6455659866333008 2.386178731918335 13.576459884643555
Loss :  1.6399048566818237 1.6002006530761719 9.640908241271973
Loss :  1.5907968282699585 2.1718015670776367 12.449804306030273
Loss :  1.6720683574676514 2.217144727706909 12.757792472839355
Loss :  1.5920534133911133 2.021995782852173 11.702032089233398
Loss :  1.6567771434783936 2.2675106525421143 12.994330406188965
Loss :  1.6333808898925781 1.7981915473937988 10.624338150024414
Loss :  1.63186776638031 2.1958913803100586 12.611324310302734
Loss :  1.599364161491394 2.734658718109131 15.272658348083496
Loss :  1.613120436668396 2.3776233196258545 13.501236915588379
Loss :  1.6122288703918457 2.3200113773345947 13.212285995483398
Loss :  1.6700201034545898 2.255469799041748 12.947369575500488
Loss :  1.6749396324157715 1.7642179727554321 10.496028900146484
Loss :  1.684802532196045 2.547116756439209 14.420387268066406
  batch 40 loss: 1.684802532196045, 2.547116756439209, 14.420387268066406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6462466716766357 1.6853655576705933 10.073074340820312
Loss :  1.6335530281066895 1.5189025402069092 9.228065490722656
Loss :  1.6236933469772339 2.433316469192505 13.790275573730469
Loss :  1.6362842321395874 1.695143461227417 10.112001419067383
Loss :  1.6155256032943726 1.5049114227294922 9.140082359313965
Loss :  1.6442323923110962 1.7612172365188599 10.450319290161133
Loss :  1.675451636314392 2.622002124786377 14.78546142578125
Loss :  1.6296530961990356 2.275170087814331 13.00550365447998
Loss :  1.68893563747406 1.8007712364196777 10.692791938781738
Loss :  1.6357593536376953 1.7753974199295044 10.512746810913086
Loss :  1.6674376726150513 2.6602783203125 14.968829154968262
Loss :  1.661458969116211 2.5712881088256836 14.517899513244629
Loss :  1.6414079666137695 2.0129544734954834 11.706180572509766
Loss :  1.6690268516540527 2.633331775665283 14.835685729980469
Loss :  1.6302403211593628 2.2120792865753174 12.69063663482666
Loss :  1.6896119117736816 2.0831611156463623 12.105417251586914
Loss :  1.6366158723831177 1.8141918182373047 10.707574844360352
Loss :  1.621220588684082 2.175374746322632 12.49809455871582
Loss :  1.6362409591674805 2.5520150661468506 14.396316528320312
Loss :  1.6959922313690186 2.152594566345215 12.458965301513672
  batch 60 loss: 1.6959922313690186, 2.152594566345215, 12.458965301513672
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6218713521957397 1.7784143686294556 10.51394271850586
Loss :  1.6492470502853394 1.5862679481506348 9.580586433410645
Loss :  1.6306650638580322 2.0458908081054688 11.860118865966797
Loss :  1.619715690612793 2.403744697570801 13.638439178466797
Loss :  1.600999116897583 1.9152522087097168 11.177260398864746
Loss :  1.6040136814117432 3.9990923404693604 21.599475860595703
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6153104305267334 3.9401819705963135 21.316221237182617
Loss :  1.6124190092086792 3.7763516902923584 20.494176864624023
Loss :  1.6208438873291016 3.734282970428467 20.292259216308594
Total LOSS train 12.23752275613638 valid 20.925533294677734
CE LOSS train 1.6433552723664504 valid 0.4052109718322754
Contrastive LOSS train 2.118833501522358 valid 0.9335707426071167
EPOCH 213:
Loss :  1.6623567342758179 1.8183237314224243 10.753974914550781
Loss :  1.6747628450393677 1.9482146501541138 11.415836334228516
Loss :  1.64557683467865 1.5647780895233154 9.469467163085938
Loss :  1.651328206062317 1.8075724840164185 10.689190864562988
Loss :  1.6677579879760742 1.4759666919708252 9.047591209411621
Loss :  1.6339892148971558 2.055558681488037 11.911783218383789
Loss :  1.667702317237854 1.9109586477279663 11.222496032714844
Loss :  1.6444984674453735 1.7471200227737427 10.380099296569824
Loss :  1.6355339288711548 1.8762097358703613 11.016582489013672
Loss :  1.667826533317566 1.8123730421066284 10.729691505432129
Loss :  1.6270980834960938 2.6701033115386963 14.977614402770996
Loss :  1.6263337135314941 1.7257788181304932 10.255228042602539
Loss :  1.624175786972046 2.072213888168335 11.985245704650879
Loss :  1.6290702819824219 2.1247408390045166 12.252774238586426
Loss :  1.6817278861999512 1.8189465999603271 10.776460647583008
Loss :  1.6774851083755493 1.8847389221191406 11.101180076599121
Loss :  1.6222683191299438 2.590355634689331 14.574047088623047
Loss :  1.6472266912460327 2.366506338119507 13.479758262634277
Loss :  1.618766188621521 1.6794687509536743 10.016109466552734
Loss :  1.677054524421692 1.576893925666809 9.561524391174316
  batch 20 loss: 1.677054524421692, 1.576893925666809, 9.561524391174316
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6417438983917236 2.5290372371673584 14.286930084228516
Loss :  1.6174933910369873 2.264655590057373 12.94077205657959
Loss :  1.634281873703003 2.210129499435425 12.684929847717285
Loss :  1.6487115621566772 2.0944759845733643 12.12109088897705
Loss :  1.6765938997268677 2.020892858505249 11.781058311462402
Loss :  1.6371568441390991 2.0552196502685547 11.913254737854004
Loss :  1.6468359231948853 2.443558931350708 13.864630699157715
Loss :  1.640844464302063 2.749483823776245 15.388263702392578
Loss :  1.5927884578704834 2.0782289505004883 11.983933448791504
Loss :  1.672985553741455 2.2717063426971436 13.031517028808594
Loss :  1.5942834615707397 1.8303817510604858 10.74619197845459
Loss :  1.6582310199737549 1.861999273300171 10.96822738647461
Loss :  1.6354920864105225 1.9731483459472656 11.50123405456543
Loss :  1.6336774826049805 2.7351295948028564 15.309325218200684
Loss :  1.6014230251312256 2.4137508869171143 13.670177459716797
Loss :  1.6153284311294556 1.9403642416000366 11.317150115966797
Loss :  1.614337682723999 1.7269104719161987 10.248889923095703
Loss :  1.670907735824585 2.260117292404175 12.971494674682617
Loss :  1.6756408214569092 2.500068187713623 14.175982475280762
Loss :  1.6847649812698364 2.4205756187438965 13.787643432617188
  batch 40 loss: 1.6847649812698364, 2.4205756187438965, 13.787643432617188
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.646540641784668 1.9706406593322754 11.499743461608887
Loss :  1.6340972185134888 1.7414803504943848 10.341498374938965
Loss :  1.6241258382797241 1.834546685218811 10.796858787536621
Loss :  1.6364496946334839 2.3668675422668457 13.470787048339844
Loss :  1.6159451007843018 1.8006384372711182 10.619136810302734
Loss :  1.644252896308899 1.9527573585510254 11.408039093017578
Loss :  1.6746418476104736 2.299384355545044 13.171563148498535
Loss :  1.6298465728759766 2.123291015625 12.246301651000977
Loss :  1.6875942945480347 2.202014207839966 12.697665214538574
Loss :  1.6349239349365234 2.018578052520752 11.727813720703125
Loss :  1.6662287712097168 2.539898157119751 14.365718841552734
Loss :  1.6603044271469116 2.062220811843872 11.971407890319824
Loss :  1.6411594152450562 1.8496540784835815 10.889429092407227
Loss :  1.6682206392288208 2.1035633087158203 12.186037063598633
Loss :  1.6298917531967163 2.0057363510131836 11.658573150634766
Loss :  1.689491868019104 1.9881511926651 11.630248069763184
Loss :  1.6371726989746094 2.7168898582458496 15.221622467041016
Loss :  1.6216201782226562 2.545820951461792 14.350725173950195
Loss :  1.636141061782837 2.0137948989868164 11.70511531829834
Loss :  1.694995641708374 1.634154200553894 9.865766525268555
  batch 60 loss: 1.694995641708374, 1.634154200553894, 9.865766525268555
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6217374801635742 1.8107707500457764 10.675591468811035
Loss :  1.6487300395965576 2.0411407947540283 11.8544340133667
Loss :  1.6304717063903809 2.0532872676849365 11.896907806396484
Loss :  1.6194568872451782 1.9217925071716309 11.22842025756836
Loss :  1.6010098457336426 2.0594019889831543 11.898019790649414
Loss :  1.6152864694595337 3.588111162185669 19.55584144592285
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6258149147033691 3.5955145359039307 19.6033878326416
Loss :  1.622637152671814 3.4394991397857666 18.820133209228516
Loss :  1.6304746866226196 3.3121190071105957 18.191068649291992
Total LOSS train 11.995181186382586 valid 19.04260778427124
CE LOSS train 1.6441709646811853 valid 0.4076186716556549
Contrastive LOSS train 2.0702020480082584 valid 0.8280297517776489
EPOCH 214:
Loss :  1.6622262001037598 1.6162934303283691 9.743692398071289
Loss :  1.6747015714645386 2.5779333114624023 14.56436824798584
Loss :  1.6460450887680054 2.1829588413238525 12.56083869934082
Loss :  1.6516505479812622 1.85002863407135 10.901793479919434
Loss :  1.6678909063339233 1.7139813899993896 10.237797737121582
Loss :  1.634422779083252 1.6869165897369385 10.069005966186523
Loss :  1.6678528785705566 2.6642465591430664 14.989086151123047
Loss :  1.6449170112609863 2.0305325984954834 11.79758071899414
Loss :  1.636088490486145 2.0320987701416016 11.796582221984863
Loss :  1.6679670810699463 1.8882344961166382 11.109139442443848
Loss :  1.6280384063720703 2.1923367977142334 12.589722633361816
Loss :  1.6273878812789917 2.052795886993408 11.891366958618164
Loss :  1.6250427961349487 2.008399486541748 11.667040824890137
Loss :  1.6302461624145508 2.0263009071350098 11.761750221252441
Loss :  1.6819648742675781 2.6776535511016846 15.070232391357422
Loss :  1.6771793365478516 2.132476329803467 12.339561462402344
Loss :  1.622614860534668 1.8895835876464844 11.07053279876709
Loss :  1.646941900253296 1.7042629718780518 10.168256759643555
Loss :  1.6193585395812988 2.4646124839782715 13.942420959472656
Loss :  1.6765564680099487 2.5082733631134033 14.217923164367676
  batch 20 loss: 1.6765564680099487, 2.5082733631134033, 14.217923164367676
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6422059535980225 2.3182623386383057 13.23351764678955
Loss :  1.6182262897491455 1.9396648406982422 11.316550254821777
Loss :  1.634507179260254 1.629346489906311 9.78123950958252
Loss :  1.6493996381759644 2.0702905654907227 12.000852584838867
Loss :  1.6769813299179077 2.3007431030273438 13.180696487426758
Loss :  1.6372921466827393 2.125290870666504 12.26374626159668
Loss :  1.6469671726226807 2.163097858428955 12.462455749511719
Loss :  1.6408334970474243 2.043701171875 11.859339714050293
Loss :  1.5932621955871582 2.2120766639709473 12.653644561767578
Loss :  1.6727491617202759 2.5275068283081055 14.310283660888672
Loss :  1.5942316055297852 2.2812983989715576 13.000723838806152
Loss :  1.6580421924591064 2.213230609893799 12.724194526672363
Loss :  1.6352287530899048 1.7098305225372314 10.184381484985352
Loss :  1.6333214044570923 1.859560489654541 10.931123733520508
Loss :  1.601699709892273 2.8437530994415283 15.820465087890625
Loss :  1.615282654762268 2.186274290084839 12.54665470123291
Loss :  1.6142222881317139 2.2988674640655518 13.108559608459473
Loss :  1.6711031198501587 2.2183163166046143 12.76268482208252
Loss :  1.6759750843048096 2.601444959640503 14.683199882507324
Loss :  1.6850247383117676 1.879723072052002 11.083639144897461
  batch 40 loss: 1.6850247383117676, 1.879723072052002, 11.083639144897461
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.6465736627578735 1.7596113681793213 10.44463062286377
Loss :  1.633912205696106 1.5053690671920776 9.160758018493652
Loss :  1.6241008043289185 2.09334659576416 12.09083366394043
Loss :  1.6364890336990356 2.6171023845672607 14.722001075744629
Loss :  1.6163370609283447 2.3359498977661133 13.296086311340332
Loss :  1.6450728178024292 2.3775060176849365 13.53260326385498
Loss :  1.6759272813796997 2.0836668014526367 12.094261169433594
Loss :  1.6309431791305542 2.018023729324341 11.721061706542969
Loss :  1.6895869970321655 2.0312132835388184 11.845653533935547
Loss :  1.6366605758666992 1.7204444408416748 10.238883018493652
Loss :  1.6672070026397705 1.9002090692520142 11.168251991271973
Loss :  1.661154866218567 2.001253366470337 11.667421340942383
Loss :  1.641567349433899 1.8433880805969238 10.85850715637207
Loss :  1.668376088142395 1.9602748155593872 11.46975040435791
Loss :  1.6300526857376099 2.5094683170318604 14.177393913269043
Loss :  1.688914179801941 2.621537923812866 14.79660415649414
Loss :  1.6367095708847046 2.7575230598449707 15.424324035644531
Loss :  1.621343731880188 2.409897565841675 13.670831680297852
Loss :  1.6358678340911865 2.635897159576416 14.815353393554688
Loss :  1.6948763132095337 1.890923023223877 11.149491310119629
  batch 60 loss: 1.6948763132095337, 1.890923023223877, 11.149491310119629
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.6216511726379395 1.9138493537902832 11.190896987915039
Loss :  1.6485693454742432 2.0250957012176514 11.7740478515625
Loss :  1.6304748058319092 1.936213731765747 11.311543464660645
Loss :  1.6194308996200562 2.279346227645874 13.016161918640137
Loss :  1.601232886314392 2.286452293395996 13.033493995666504
Loss :  1.6104925870895386 4.19736385345459 22.597312927246094
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6213853359222412 4.024104595184326 21.74190902709961
Loss :  1.6181983947753906 4.05690336227417 21.9027156829834
Loss :  1.6248363256454468 3.942836284637451 21.339017868041992
Total LOSS train 12.324115269000714 valid 21.895238876342773
CE LOSS train 1.6444412653262799 valid 0.4062090814113617
Contrastive LOSS train 2.13593481870798 valid 0.9857090711593628
EPOCH 215:
Loss :  1.6617538928985596 2.2054529190063477 12.689018249511719
Loss :  1.674267292022705 2.3691189289093018 13.519861221313477
Loss :  1.6458256244659424 1.9150335788726807 11.220993041992188
Loss :  1.6518220901489258 2.6330676078796387 14.817160606384277
Loss :  1.6682028770446777 2.460860252380371 13.972503662109375
Loss :  1.6347850561141968 2.4345786571502686 13.80767822265625
Loss :  1.6678828001022339 2.6920197010040283 15.127981185913086
Loss :  1.6452230215072632 1.571360468864441 9.502025604248047
Loss :  1.6361290216445923 2.2537426948547363 12.904842376708984
Loss :  1.6676561832427979 2.0676214694976807 12.005763053894043
Loss :  1.6280484199523926 1.9312517642974854 11.284307479858398
Loss :  1.627365231513977 1.9254343509674072 11.254536628723145
Loss :  1.6249912977218628 1.9247969388961792 11.24897575378418
Loss :  1.6300570964813232 1.9505842924118042 11.382978439331055
Loss :  1.6818420886993408 1.9571497440338135 11.46759033203125
Loss :  1.6774474382400513 1.9479806423187256 11.417350769042969
Loss :  1.6225050687789917 1.8773722648620605 11.009366989135742
Loss :  1.6470446586608887 1.7438812255859375 10.366451263427734
Loss :  1.6191123723983765 2.0345709323883057 11.791966438293457
Loss :  1.6763266324996948 2.6659839153289795 15.006246566772461
  batch 20 loss: 1.6763266324996948, 2.6659839153289795, 15.006246566772461
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.642107367515564 1.9274842739105225 11.279528617858887
Loss :  1.618259310722351 1.9460848569869995 11.34868335723877
Loss :  1.6346904039382935 1.6525667905807495 9.897523880004883
Loss :  1.6496038436889648 1.730064034461975 10.29992389678955
Loss :  1.6770539283752441 2.510612964630127 14.230117797851562
Loss :  1.637965202331543 1.8536278009414673 10.90610408782959
Loss :  1.6477000713348389 1.6886959075927734 10.091179847717285
Loss :  1.6422864198684692 2.06160831451416 11.95032787322998
Loss :  1.594826340675354 2.8305466175079346 15.747559547424316
Loss :  1.6744887828826904 2.9466302394866943 16.40764045715332
Loss :  1.596136212348938 2.617701292037964 14.684642791748047
Loss :  1.658984899520874 2.4368066787719727 13.843018531799316
Loss :  1.636193871498108 1.9843864440917969 11.558126449584961
Loss :  1.6341441869735718 2.0207414627075195 11.7378511428833
Loss :  1.6025519371032715 2.8646340370178223 15.925722122192383
Loss :  1.6159218549728394 2.362361431121826 13.427729606628418
Loss :  1.6146501302719116 2.3983685970306396 13.60649299621582
Loss :  1.6706739664077759 2.336003065109253 13.350689888000488
Loss :  1.6755955219268799 1.8447997570037842 10.8995943069458
Loss :  1.6846339702606201 2.376331090927124 13.566289901733398
  batch 40 loss: 1.6846339702606201, 2.376331090927124, 13.566289901733398
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6470025777816772 2.5027871131896973 14.160937309265137
Loss :  1.6343257427215576 2.2705612182617188 12.98713207244873
Loss :  1.6245756149291992 2.5708420276641846 14.478785514831543
Loss :  1.6367775201797485 2.319592237472534 13.234739303588867
Loss :  1.616778016090393 2.052374839782715 11.878652572631836
Loss :  1.6450611352920532 2.2358474731445312 12.824298858642578
Loss :  1.67527174949646 2.045661211013794 11.90357780456543
Loss :  1.6306583881378174 1.92827308177948 11.27202320098877
Loss :  1.689140796661377 1.8295849561691284 10.837064743041992
Loss :  1.6364153623580933 2.0092389583587646 11.682610511779785
Loss :  1.6670825481414795 2.3014769554138184 13.174468040466309
Loss :  1.661976933479309 1.8635613918304443 10.97978401184082
Loss :  1.6427992582321167 2.309039354324341 13.187995910644531
Loss :  1.6694422960281372 2.4259238243103027 13.79906177520752
Loss :  1.6321383714675903 2.1448261737823486 12.356268882751465
Loss :  1.6899861097335815 2.4325454235076904 13.852713584899902
Loss :  1.638885498046875 2.455397129058838 13.915870666503906
Loss :  1.6245238780975342 2.1000289916992188 12.124669075012207
Loss :  1.6381553411483765 2.556098699569702 14.418648719787598
Loss :  1.6962484121322632 1.6629985570907593 10.01124095916748
  batch 60 loss: 1.6962484121322632, 1.6629985570907593, 10.01124095916748
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.624223232269287 2.1744461059570312 12.496454238891602
Loss :  1.6506340503692627 1.6684181690216064 9.992724418640137
Loss :  1.6323796510696411 1.6492334604263306 9.878546714782715
Loss :  1.6211717128753662 1.7701225280761719 10.471784591674805
Loss :  1.6027737855911255 1.452165126800537 8.86359977722168
Loss :  1.5987547636032104 4.223788738250732 22.71769905090332
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.6112531423568726 4.2074689865112305 22.64859962463379
Loss :  1.6094837188720703 4.097268104553223 22.095823287963867
Loss :  1.6131188869476318 4.014828681945801 21.6872615814209
Total LOSS train 12.389846126849834 valid 22.28734588623047
CE LOSS train 1.6450028364474958 valid 0.40327972173690796
Contrastive LOSS train 2.148968661748446 valid 1.0037071704864502
EPOCH 216:
Loss :  1.6640050411224365 2.0362489223480225 11.845250129699707
Loss :  1.6764146089553833 2.412614345550537 13.739486694335938
Loss :  1.6478270292282104 1.7110739946365356 10.203197479248047
Loss :  1.6534348726272583 2.240971565246582 12.858292579650879
Loss :  1.6697229146957397 1.5998011827468872 9.668728828430176
Loss :  1.6364920139312744 1.8171346187591553 10.72216510772705
Loss :  1.6696988344192505 2.442171812057495 13.880558013916016
Loss :  1.6466436386108398 1.7302087545394897 10.297687530517578
Loss :  1.6379157304763794 1.7200499773025513 10.238165855407715
Loss :  1.6695247888565063 1.7050710916519165 10.194880485534668
Loss :  1.629748821258545 1.780282974243164 10.531164169311523
Loss :  1.6292706727981567 1.6809871196746826 10.03420639038086
Loss :  1.6269828081130981 1.61332106590271 9.693588256835938
Loss :  1.6317259073257446 2.1110613346099854 12.187032699584961
Loss :  1.683536171913147 2.3661625385284424 13.514348983764648
Loss :  1.6797409057617188 2.314615488052368 13.25281810760498
Loss :  1.6248255968093872 1.904264211654663 11.146146774291992
Loss :  1.6495563983917236 1.7220019102096558 10.259565353393555
Loss :  1.6216460466384888 2.1597447395324707 12.420369148254395
Loss :  1.6785688400268555 2.3776655197143555 13.566896438598633
  batch 20 loss: 1.6785688400268555, 2.3776655197143555, 13.566896438598633
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6446677446365356 2.0944840908050537 12.117088317871094
Loss :  1.6210112571716309 2.386401891708374 13.553020477294922
Loss :  1.6375856399536133 1.5524524450302124 9.399847984313965
Loss :  1.6516666412353516 1.5822685956954956 9.563009262084961
Loss :  1.678956389427185 2.339465379714966 13.376282691955566
Loss :  1.6402829885482788 2.6755714416503906 15.018139839172363
Loss :  1.6495862007141113 1.886195421218872 11.080562591552734
Loss :  1.6441787481307983 1.8670449256896973 10.979402542114258
Loss :  1.5968154668807983 1.964495301246643 11.419291496276855
Loss :  1.675673007965088 2.4947428703308105 14.14938735961914
Loss :  1.5980706214904785 1.7752017974853516 10.474079132080078
Loss :  1.6607756614685059 1.681484341621399 10.068197250366211
Loss :  1.6387033462524414 1.608738660812378 9.68239688873291
Loss :  1.6371854543685913 2.303494453430176 13.154657363891602
Loss :  1.6056876182556152 2.5360167026519775 14.285770416259766
Loss :  1.619266152381897 2.2698349952697754 12.968441009521484
Loss :  1.618141531944275 1.7414684295654297 10.325483322143555
Loss :  1.6738884449005127 1.8193877935409546 10.770827293395996
Loss :  1.6784251928329468 2.7502620220184326 15.42973518371582
Loss :  1.6876091957092285 2.6716854572296143 15.046035766601562
  batch 40 loss: 1.6876091957092285, 2.6716854572296143, 15.046035766601562
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6501080989837646 2.201982021331787 12.660018920898438
Loss :  1.6379040479660034 2.7238337993621826 15.257073402404785
Loss :  1.6284997463226318 1.7765223979949951 10.511112213134766
Loss :  1.6406397819519043 1.738227128982544 10.331775665283203
Loss :  1.6202173233032227 2.009927988052368 11.669857025146484
Loss :  1.6482757329940796 2.1813387870788574 12.554969787597656
Loss :  1.678731083869934 2.615971088409424 14.758585929870605
Loss :  1.6346193552017212 1.4617377519607544 8.943307876586914
Loss :  1.6920253038406372 1.6621850728988647 10.002950668334961
Loss :  1.6401556730270386 1.5983164310455322 9.63173770904541
Loss :  1.6707254648208618 1.750443696975708 10.422944068908691
Loss :  1.6649500131607056 1.7120378017425537 10.225139617919922
Loss :  1.6455379724502563 2.2242958545684814 12.767017364501953
Loss :  1.6720035076141357 2.4146485328674316 13.745245933532715
Loss :  1.635123372077942 2.2139108180999756 12.70467758178711
Loss :  1.6919108629226685 2.018947124481201 11.786646842956543
Loss :  1.6410245895385742 2.089102268218994 12.086535453796387
Loss :  1.6261112689971924 2.128361225128174 12.267916679382324
Loss :  1.6404356956481934 2.2997148036956787 13.139009475708008
Loss :  1.6980946063995361 1.5510222911834717 9.453206062316895
  batch 60 loss: 1.6980946063995361, 1.5510222911834717, 9.453206062316895
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6267273426055908 2.7064626216888428 15.159040451049805
Loss :  1.653260588645935 2.7070212364196777 15.188366889953613
Loss :  1.6350269317626953 1.881638765335083 11.043220520019531
Loss :  1.6246001720428467 2.074838161468506 11.998791694641113
Loss :  1.6063429117202759 1.3644648790359497 8.428667068481445
Loss :  1.6173650026321411 3.8032591342926025 20.6336612701416
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.62733793258667 3.7773845195770264 20.51426124572754
Loss :  1.6257623434066772 3.7385244369506836 20.31838607788086
Loss :  1.6350849866867065 3.6345531940460205 19.807849884033203
Total LOSS train 11.84390800182636 valid 20.3185396194458
CE LOSS train 1.6475155445245595 valid 0.40877124667167664
Contrastive LOSS train 2.039278503564688 valid 0.9086382985115051
EPOCH 217:
Loss :  1.6657692193984985 2.1442618370056152 12.387079238891602
Loss :  1.6778078079223633 2.2381186485290527 12.868401527404785
Loss :  1.6493107080459595 1.5024691820144653 9.161656379699707
Loss :  1.6549127101898193 2.285571336746216 13.082769393920898
Loss :  1.6709916591644287 1.5586791038513184 9.464386940002441
Loss :  1.6382204294204712 1.63657808303833 9.821110725402832
Loss :  1.6709716320037842 1.654666781425476 9.944305419921875
Loss :  1.6484506130218506 1.5519673824310303 9.40828800201416
Loss :  1.6400283575057983 2.2551863193511963 12.915959358215332
Loss :  1.6716511249542236 2.226262092590332 12.802961349487305
Loss :  1.6318515539169312 2.227527379989624 12.769488334655762
Loss :  1.6311781406402588 2.1758809089660645 12.510581970214844
Loss :  1.6289339065551758 1.6076306104660034 9.667086601257324
Loss :  1.6336313486099243 1.8365854024887085 10.816558837890625
Loss :  1.6843394041061401 1.7649234533309937 10.508956909179688
Loss :  1.6804064512252808 2.3333489894866943 13.347151756286621
Loss :  1.6268303394317627 2.307481288909912 13.164237022399902
Loss :  1.6510591506958008 2.2773561477661133 13.037839889526367
Loss :  1.623610258102417 2.2013134956359863 12.63017749786377
Loss :  1.680397629737854 1.9534294605255127 11.447545051574707
  batch 20 loss: 1.680397629737854, 1.9534294605255127, 11.447545051574707
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6462007761001587 2.2471060752868652 12.881731986999512
Loss :  1.6228135824203491 2.4366648197174072 13.806137084960938
Loss :  1.6391831636428833 1.5810080766677856 9.54422378540039
Loss :  1.6528128385543823 2.0648550987243652 11.977088928222656
Loss :  1.6795457601547241 2.410151481628418 13.730302810668945
Loss :  1.6410950422286987 1.7044832706451416 10.163511276245117
Loss :  1.6503689289093018 1.699452519416809 10.147631645202637
Loss :  1.6450313329696655 2.1905245780944824 12.597654342651367
Loss :  1.5982509851455688 1.7145329713821411 10.170916557312012
Loss :  1.6764410734176636 1.8347759246826172 10.850320816040039
Loss :  1.5994175672531128 2.18646240234375 12.531729698181152
Loss :  1.6615831851959229 2.6529994010925293 14.926580429077148
Loss :  1.6393901109695435 2.3863437175750732 13.5711088180542
Loss :  1.6377092599868774 1.8943649530410767 11.10953426361084
Loss :  1.6065404415130615 1.9118199348449707 11.165639877319336
Loss :  1.6200966835021973 2.1958816051483154 12.599504470825195
Loss :  1.619187831878662 2.0160956382751465 11.699666976928711
Loss :  1.6745855808258057 2.0496914386749268 11.923042297363281
Loss :  1.679321527481079 1.4622822999954224 8.99073314666748
Loss :  1.6885725259780884 1.8732044696807861 11.054594993591309
  batch 40 loss: 1.6885725259780884, 1.8732044696807861, 11.054594993591309
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.651524543762207 1.9818336963653564 11.56069278717041
Loss :  1.6394476890563965 1.9851261377334595 11.565078735351562
Loss :  1.6296919584274292 2.8150017261505127 15.704700469970703
Loss :  1.6416882276535034 2.702423334121704 15.153804779052734
Loss :  1.6216439008712769 1.4218406677246094 8.730847358703613
Loss :  1.6490840911865234 2.6037094593048096 14.667631149291992
Loss :  1.6789822578430176 2.2016313076019287 12.687139511108398
Loss :  1.6354187726974487 1.977639079093933 11.523613929748535
Loss :  1.6920721530914307 1.5977427959442139 9.6807861328125
Loss :  1.640872597694397 1.6473388671875 9.877567291259766
Loss :  1.6712305545806885 1.9801502227783203 11.571981430053711
Loss :  1.6654689311981201 2.3521931171417236 13.426434516906738
Loss :  1.6464786529541016 2.4493215084075928 13.893086433410645
Loss :  1.67305326461792 2.7343738079071045 15.34492301940918
Loss :  1.636576533317566 2.11563777923584 12.214765548706055
Loss :  1.6928828954696655 1.9997233152389526 11.691498756408691
Loss :  1.6427778005599976 2.6384246349334717 14.834900856018066
Loss :  1.6284675598144531 2.4156367778778076 13.70665168762207
Loss :  1.642848253250122 2.512421131134033 14.20495319366455
Loss :  1.7000609636306763 2.084075689315796 12.120439529418945
  batch 60 loss: 1.7000609636306763, 2.084075689315796, 12.120439529418945
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6296006441116333 1.9712748527526855 11.48597526550293
Loss :  1.6558411121368408 1.8875272274017334 11.093477249145508
Loss :  1.6373326778411865 1.8602756261825562 10.938711166381836
Loss :  1.6268908977508545 2.6267333030700684 14.760558128356934
Loss :  1.6085354089736938 1.8359767198562622 10.788419723510742
Loss :  1.6232728958129883 4.165961742401123 22.453083038330078
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.6318836212158203 4.192234516143799 22.593055725097656
Loss :  1.6304972171783447 4.131521224975586 22.288103103637695
Loss :  1.6433660984039307 3.9484105110168457 21.385417938232422
Total LOSS train 12.00656669323261 valid 22.179914951324463
CE LOSS train 1.648876507465656 valid 0.41084152460098267
Contrastive LOSS train 2.071538021014287 valid 0.9871026277542114
EPOCH 218:
Loss :  1.667206883430481 1.7413884401321411 10.374149322509766
Loss :  1.6790616512298584 2.1042721271514893 12.200422286987305
Loss :  1.6509321928024292 2.1755406856536865 12.52863597869873
Loss :  1.6565083265304565 2.1281139850616455 12.297078132629395
Loss :  1.6724849939346313 2.1663496494293213 12.504233360290527
Loss :  1.640089511871338 2.195507764816284 12.61762809753418
Loss :  1.6724873781204224 2.648470878601074 14.914841651916504
Loss :  1.650282621383667 2.056164264678955 11.931103706359863
Loss :  1.6415159702301025 2.329204797744751 13.2875394821167
Loss :  1.6724071502685547 2.3361828327178955 13.353321075439453
Loss :  1.632787823677063 1.9518280029296875 11.391927719116211
Loss :  1.6317391395568848 2.512047052383423 14.191974639892578
Loss :  1.6295511722564697 1.806946873664856 10.664285659790039
Loss :  1.6339682340621948 2.157667875289917 12.422307968139648
Loss :  1.6845192909240723 2.1137876510620117 12.253458023071289
Loss :  1.6800763607025146 2.049987316131592 11.930013656616211
Loss :  1.6269738674163818 1.805963397026062 10.656790733337402
Loss :  1.6509928703308105 2.1342036724090576 12.322011947631836
Loss :  1.6239674091339111 2.0639514923095703 11.943724632263184
Loss :  1.680102825164795 2.3024251461029053 13.192228317260742
  batch 20 loss: 1.680102825164795, 2.3024251461029053, 13.192228317260742
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.646557092666626 1.8567804098129272 10.930459022521973
Loss :  1.6235098838806152 1.9428037405014038 11.337528228759766
Loss :  1.6401015520095825 1.8599926233291626 10.940064430236816
Loss :  1.6536997556686401 1.86117684841156 10.95958423614502
Loss :  1.6802823543548584 2.379744291305542 13.579004287719727
Loss :  1.6423110961914062 2.3089988231658936 13.187305450439453
Loss :  1.6512444019317627 2.0336191654205322 11.819339752197266
Loss :  1.645961046218872 1.9685367345809937 11.48864459991455
Loss :  1.599658489227295 1.9461930990219116 11.330623626708984
Loss :  1.6764593124389648 2.145920753479004 12.406063079833984
Loss :  1.600422739982605 2.1509580612182617 12.355213165283203
Loss :  1.6618179082870483 1.8968068361282349 11.145852088928223
Loss :  1.6401252746582031 2.123290538787842 12.25657844543457
Loss :  1.6385387182235718 2.0682318210601807 11.979697227478027
Loss :  1.6080358028411865 2.7379648685455322 15.297860145568848
Loss :  1.6214128732681274 2.361417531967163 13.42850112915039
Loss :  1.6206992864608765 1.9756355285644531 11.498876571655273
Loss :  1.6748871803283691 1.9603191614151 11.476482391357422
Loss :  1.679593801498413 2.1527674198150635 12.44343090057373
Loss :  1.6885976791381836 2.0518271923065186 11.947733879089355
  batch 40 loss: 1.6885976791381836, 2.0518271923065186, 11.947733879089355
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.6518099308013916 2.4602043628692627 13.952831268310547
Loss :  1.6394166946411133 2.3351988792419434 13.315411567687988
Loss :  1.6297345161437988 2.112391233444214 12.191690444946289
Loss :  1.6415013074874878 1.7983720302581787 10.63336181640625
Loss :  1.621852993965149 2.033522844314575 11.789466857910156
Loss :  1.6492969989776611 1.9378503561019897 11.33854866027832
Loss :  1.67885422706604 2.0305237770080566 11.831472396850586
Loss :  1.6359754800796509 2.412804126739502 13.699995994567871
Loss :  1.6915441751480103 2.4707248210906982 14.04516887664795
Loss :  1.6412508487701416 2.2715768814086914 12.99913501739502
Loss :  1.6714892387390137 2.0451085567474365 11.897031784057617
Loss :  1.6660491228103638 2.17922043800354 12.562150955200195
Loss :  1.6473045349121094 2.102907419204712 12.16184139251709
Loss :  1.6732217073440552 2.0234105587005615 11.790274620056152
Loss :  1.6377620697021484 1.9204697608947754 11.240110397338867
Loss :  1.6929823160171509 2.088665723800659 12.136311531066895
Loss :  1.6437660455703735 2.1589112281799316 12.438322067260742
Loss :  1.6299585103988647 2.1091513633728027 12.175715446472168
Loss :  1.6439096927642822 2.8622939586639404 15.955379486083984
Loss :  1.6998385190963745 2.233530282974243 12.8674898147583
  batch 60 loss: 1.6998385190963745, 2.233530282974243, 12.8674898147583
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6301323175430298 2.3407225608825684 13.333745956420898
Loss :  1.6561020612716675 1.7346829175949097 10.329516410827637
Loss :  1.6377969980239868 1.6761443614959717 10.018519401550293
Loss :  1.6271030902862549 2.2575039863586426 12.91462230682373
Loss :  1.6092734336853027 2.1184308528900146 12.201427459716797
Loss :  1.6322270631790161 4.210533618927002 22.684894561767578
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.642423152923584 4.159341812133789 22.439132690429688
Loss :  1.64044189453125 4.113879203796387 22.2098388671875
Loss :  1.6486310958862305 4.023404121398926 21.76565170288086
Total LOSS train 12.286247092026931 valid 22.274879455566406
CE LOSS train 1.6495307500545795 valid 0.4121577739715576
Contrastive LOSS train 2.127343271328853 valid 1.0058510303497314
EPOCH 219:
Loss :  1.6678087711334229 1.8961116075515747 11.148366928100586
Loss :  1.6796127557754517 2.2890992164611816 13.12510871887207
Loss :  1.6516579389572144 2.3912110328674316 13.607712745666504
Loss :  1.6570717096328735 2.5730462074279785 14.522302627563477
Loss :  1.6726800203323364 2.4239346981048584 13.792353630065918
Loss :  1.6403746604919434 2.319847345352173 13.23961067199707
Loss :  1.6723732948303223 2.7271013259887695 15.307880401611328
Loss :  1.6503233909606934 2.3420698642730713 13.360671997070312
Loss :  1.6418436765670776 2.3581342697143555 13.432515144348145
Loss :  1.6727347373962402 2.3717217445373535 13.531343460083008
Loss :  1.633707880973816 2.438832998275757 13.827873229980469
Loss :  1.633123755455017 2.5402066707611084 14.33415699005127
Loss :  1.631348967552185 1.923799753189087 11.250347137451172
Loss :  1.6362690925598145 2.464961290359497 13.961074829101562
Loss :  1.686806321144104 1.97689688205719 11.571290969848633
Loss :  1.6821297407150269 2.2418787479400635 12.891523361206055
Loss :  1.629312515258789 2.0647506713867188 11.953065872192383
Loss :  1.6526694297790527 2.03525972366333 11.828968048095703
Loss :  1.6259980201721191 2.5150115489959717 14.201055526733398
Loss :  1.6813560724258423 2.604274034500122 14.702726364135742
  batch 20 loss: 1.6813560724258423, 2.604274034500122, 14.702726364135742
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
Loss :  1.648455262184143 2.3739991188049316 13.518450736999512
Loss :  1.625732660293579 2.4855849742889404 14.053657531738281
Loss :  1.6421916484832764 2.149240255355835 12.38839340209961
Loss :  1.655547857284546 2.7733166217803955 15.522130966186523
Loss :  1.681736946105957 2.627560615539551 14.819540023803711
Loss :  1.6442680358886719 2.66131591796875 14.950847625732422
Loss :  1.6529802083969116 1.9584742784500122 11.445351600646973
Loss :  1.6474822759628296 2.464939832687378 13.97218132019043
Loss :  1.6012221574783325 2.0998315811157227 12.100379943847656
Loss :  1.6775774955749512 2.3449676036834717 13.402416229248047
Loss :  1.6023973226547241 2.0437028408050537 11.820911407470703
Loss :  1.6631797552108765 2.0942862033843994 12.134610176086426
Loss :  1.641595721244812 1.9961373805999756 11.622282981872559
Loss :  1.6395708322525024 2.3440709114074707 13.359925270080566
Loss :  1.6090903282165527 2.591783285140991 14.56800651550293
Loss :  1.6218068599700928 2.52382755279541 14.240944862365723
Loss :  1.6208146810531616 2.0486738681793213 11.86418342590332
Loss :  1.675777554512024 2.0031232833862305 11.691393852233887
Loss :  1.6800323724746704 2.257265567779541 12.966360092163086
Loss :  1.6893184185028076 2.1873526573181152 12.626082420349121
  batch 40 loss: 1.6893184185028076, 2.1873526573181152, 12.626082420349121
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.653307557106018 1.9135396480560303 11.221006393432617
Loss :  1.6415520906448364 2.356950283050537 13.42630386352539
Loss :  1.633069634437561 2.5246517658233643 14.256328582763672
Loss :  1.6448874473571777 2.0578248500823975 11.934011459350586
Loss :  1.6253869533538818 1.8501931428909302 10.87635326385498
Loss :  1.6519880294799805 1.929437279701233 11.299174308776855
Loss :  1.6813995838165283 2.3111202716827393 13.237000465393066
Loss :  1.6382631063461304 2.232100486755371 12.798765182495117
Loss :  1.6930890083312988 2.6860201358795166 15.123189926147461
Loss :  1.6435989141464233 2.151709794998169 12.40214729309082
Loss :  1.6733858585357666 2.4259467124938965 13.803119659423828
Loss :  1.667441725730896 2.2683639526367188 13.009261131286621
Loss :  1.64866304397583 2.2676734924316406 12.987030029296875
Loss :  1.6736674308776855 2.405867338180542 13.703004837036133
Loss :  1.6395450830459595 2.0516176223754883 11.89763355255127
Loss :  1.6938145160675049 1.866532802581787 11.02647876739502
Loss :  1.6449249982833862 2.5063223838806152 14.17653751373291
Loss :  1.6303153038024902 2.510422706604004 14.182428359985352
Loss :  1.644170880317688 2.8711678981781006 16.000011444091797
Loss :  1.7001476287841797 2.1151812076568604 12.276053428649902
  batch 60 loss: 1.7001476287841797, 2.1151812076568604, 12.276053428649902
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6296379566192627 1.9163471460342407 11.211373329162598
Loss :  1.6554964780807495 1.9341323375701904 11.32615852355957
Loss :  1.63748037815094 1.7972991466522217 10.623976707458496
Loss :  1.6275714635849 2.358579397201538 13.4204683303833
Loss :  1.6100587844848633 1.8748645782470703 10.984381675720215
Loss :  1.6270661354064941 4.006645679473877 21.660293579101562
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6359670162200928 3.930436611175537 21.288150787353516
Loss :  1.6350682973861694 3.9107000827789307 21.188570022583008
Loss :  1.6391304731369019 3.7135579586029053 20.206918716430664
Total LOSS train 13.013233801034781 valid 21.085983276367188
CE LOSS train 1.65081300001878 valid 0.40978261828422546
Contrastive LOSS train 2.2724841594696046 valid 0.9283894896507263
EPOCH 220:
Loss :  1.6677660942077637 2.208686590194702 12.711198806762695
Loss :  1.6796817779541016 2.184211015701294 12.600736618041992
Loss :  1.6522308588027954 1.5542668104171753 9.423564910888672
Loss :  1.6576255559921265 1.977965235710144 11.54745101928711
Loss :  1.6736881732940674 2.1014914512634277 12.181145668029785
Loss :  1.6416242122650146 2.3123600482940674 13.203424453735352
Loss :  1.6736286878585815 2.458361864089966 13.965437889099121
Loss :  1.6515722274780273 2.40659761428833 13.68455982208252
Loss :  1.6429567337036133 1.8959109783172607 11.122511863708496
Loss :  1.6736104488372803 1.8695719242095947 11.021470069885254
Loss :  1.634717345237732 2.3451995849609375 13.36071491241455
Loss :  1.6337926387786865 2.267397880554199 12.970782279968262
Loss :  1.6321779489517212 2.2132568359375 12.69846248626709
Loss :  1.6365976333618164 2.2847023010253906 13.06010913848877
Loss :  1.6864955425262451 2.1940832138061523 12.656911849975586
Loss :  1.6820012331008911 2.1696841716766357 12.53042221069336
Loss :  1.6296738386154175 2.2660202980041504 12.9597749710083
Loss :  1.6537457704544067 1.9357798099517822 11.33264446258545
Loss :  1.6263340711593628 1.8497928380966187 10.875298500061035
Loss :  1.6814380884170532 2.455265998840332 13.957768440246582
  batch 20 loss: 1.6814380884170532, 2.455265998840332, 13.957768440246582
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.648015022277832 1.8003807067871094 10.649918556213379
Loss :  1.6256372928619385 2.0857465267181396 12.054369926452637
Loss :  1.6419504880905151 2.0308609008789062 11.796255111694336
Loss :  1.655533790588379 2.038081407546997 11.845940589904785
Loss :  1.6816524267196655 2.167515277862549 12.519227981567383
Loss :  1.6442532539367676 1.986240267753601 11.575454711914062
Loss :  1.6534571647644043 2.19806170463562 12.643766403198242
Loss :  1.6484647989273071 1.9646633863449097 11.471781730651855
Loss :  1.6027522087097168 2.0050878524780273 11.628190994262695
Loss :  1.6790401935577393 2.9710540771484375 16.534311294555664
Loss :  1.604539155960083 2.8298349380493164 15.753713607788086
Loss :  1.6648571491241455 2.835566759109497 15.842690467834473
Loss :  1.6435675621032715 2.478692054748535 14.037027359008789
Loss :  1.6418994665145874 2.631983518600464 14.801816940307617
Loss :  1.611462116241455 2.3907065391540527 13.564994812011719
Loss :  1.6242499351501465 2.4642081260681152 13.945291519165039
Loss :  1.6232900619506836 2.4936869144439697 14.091724395751953
Loss :  1.6773103475570679 2.25683331489563 12.96147632598877
Loss :  1.6815361976623535 2.10964298248291 12.229751586914062
Loss :  1.6904330253601074 2.209843397140503 12.73965072631836
  batch 40 loss: 1.6904330253601074, 2.209843397140503, 12.73965072631836
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6543266773223877 2.6336817741394043 14.822735786437988
Loss :  1.6422405242919922 2.362595319747925 13.455217361450195
Loss :  1.6331268548965454 2.4481136798858643 13.873695373535156
Loss :  1.6445924043655396 2.2499032020568848 12.894107818603516
Loss :  1.6248430013656616 2.1908071041107178 12.578878402709961
Loss :  1.6518166065216064 2.012371778488159 11.713675498962402
Loss :  1.681076169013977 1.9903401136398315 11.632776260375977
Loss :  1.638132929801941 1.9735450744628906 11.505858421325684
Loss :  1.6937053203582764 1.8783773183822632 11.085592269897461
Loss :  1.643399953842163 2.3712077140808105 13.499439239501953
Loss :  1.6729142665863037 2.7037594318389893 15.19171142578125
Loss :  1.667154312133789 2.497462511062622 14.15446662902832
Loss :  1.6484379768371582 2.4335556030273438 13.816215515136719
Loss :  1.6745408773422241 2.5173182487487793 14.26113224029541
Loss :  1.638667106628418 1.8800904750823975 11.039119720458984
Loss :  1.6940374374389648 2.2126591205596924 12.757332801818848
Loss :  1.6449331045150757 2.390352725982666 13.596695899963379
Loss :  1.6310112476348877 1.977297306060791 11.517497062683105
Loss :  1.6448498964309692 2.0609054565429688 11.949377059936523
Loss :  1.70066237449646 1.6555315256118774 9.978320121765137
  batch 60 loss: 1.70066237449646, 1.6555315256118774, 9.978320121765137
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.631840467453003 1.9184943437576294 11.224312782287598
Loss :  1.6577491760253906 1.7369581460952759 10.34253978729248
Loss :  1.639849305152893 1.6051894426345825 9.665797233581543
Loss :  1.6301438808441162 2.3997576236724854 13.628931999206543
Loss :  1.6125987768173218 1.4600296020507812 8.91274642944336
Loss :  1.6307705640792847 3.8431882858276367 20.846712112426758
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6405631303787231 3.8269777297973633 20.77545166015625
Loss :  1.639605164527893 3.7078983783721924 20.179096221923828
Loss :  1.6449511051177979 3.468909740447998 18.989500045776367
Total LOSS train 12.609998747018667 valid 20.1976900100708
CE LOSS train 1.6515678644180298 valid 0.41123777627944946
Contrastive LOSS train 2.1916861809217014 valid 0.8672274351119995
EPOCH 221:
Loss :  1.6698637008666992 1.6909699440002441 10.124712944030762
Loss :  1.6818854808807373 2.3143632411956787 13.253702163696289
Loss :  1.654104471206665 2.203639507293701 12.67230224609375
Loss :  1.6591346263885498 1.7968881130218506 10.643575668334961
Loss :  1.6743412017822266 2.4472200870513916 13.910441398620605
Loss :  1.6423949003219604 2.276245355606079 13.023621559143066
Loss :  1.6742933988571167 2.1947951316833496 12.648269653320312
Loss :  1.6520730257034302 1.5427966117858887 9.366056442260742
Loss :  1.6438159942626953 1.6764034032821655 10.025833129882812
Loss :  1.6740585565567017 2.357428550720215 13.461201667785645
Loss :  1.6354072093963623 2.210341215133667 12.687113761901855
Loss :  1.63482666015625 2.5196826457977295 14.233240127563477
Loss :  1.6329482793807983 2.1989526748657227 12.627711296081543
Loss :  1.6375364065170288 2.2789366245269775 13.032218933105469
Loss :  1.6873366832733154 2.5044732093811035 14.209702491760254
Loss :  1.6823413372039795 2.3800437450408936 13.582560539245605
Loss :  1.630773663520813 2.269951820373535 12.9805326461792
Loss :  1.6538434028625488 2.0672476291656494 11.990081787109375
Loss :  1.6272481679916382 2.0401766300201416 11.828130722045898
Loss :  1.681545376777649 1.8903592824935913 11.133341789245605
  batch 20 loss: 1.681545376777649, 1.8903592824935913, 11.133341789245605
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6489291191101074 1.6783610582351685 10.040735244750977
Loss :  1.626497745513916 2.4514288902282715 13.883642196655273
Loss :  1.6428203582763672 2.3639578819274902 13.462610244750977
Loss :  1.6563693284988403 1.8171192407608032 10.741965293884277
Loss :  1.682510256767273 2.097585678100586 12.170438766479492
Loss :  1.645385980606079 1.8675178289413452 10.982975006103516
Loss :  1.6544215679168701 2.020493507385254 11.756889343261719
Loss :  1.6491787433624268 2.427659749984741 13.787477493286133
Loss :  1.6035654544830322 2.1526498794555664 12.366814613342285
Loss :  1.6789636611938477 2.1989152431488037 12.673540115356445
Loss :  1.6047624349594116 2.151470899581909 12.362116813659668
Loss :  1.6645084619522095 2.27441668510437 13.036592483520508
Loss :  1.6430069208145142 2.307187795639038 13.178946495056152
Loss :  1.6414788961410522 1.793099284172058 10.606974601745605
Loss :  1.6113022565841675 2.1426377296447754 12.324490547180176
Loss :  1.6241424083709717 1.9061442613601685 11.154864311218262
Loss :  1.6230522394180298 1.9558727741241455 11.402416229248047
Loss :  1.6764627695083618 2.7158732414245605 15.255829811096191
Loss :  1.6812143325805664 2.087714433670044 12.119786262512207
Loss :  1.6902251243591309 2.442610740661621 13.903278350830078
  batch 40 loss: 1.6902251243591309, 2.442610740661621, 13.903278350830078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6543627977371216 2.5188090801239014 14.248408317565918
Loss :  1.6425124406814575 2.366795301437378 13.476489067077637
Loss :  1.6339221000671387 2.5654995441436768 14.461420059204102
Loss :  1.6450417041778564 2.3834197521209717 13.562140464782715
Loss :  1.6259466409683228 2.207873821258545 12.665316581726074
Loss :  1.6525739431381226 2.236048936843872 12.832818031311035
Loss :  1.6810001134872437 2.255211591720581 12.95705795288086
Loss :  1.639088749885559 1.9170464277267456 11.224320411682129
Loss :  1.6929577589035034 1.9665576219558716 11.52574634552002
Loss :  1.643843650817871 1.9314903020858765 11.301295280456543
Loss :  1.673602819442749 2.238246440887451 12.864835739135742
Loss :  1.6680115461349487 2.001127004623413 11.673646926879883
Loss :  1.64957857131958 1.9533287286758423 11.416221618652344
Loss :  1.6751939058303833 1.9341715574264526 11.346051216125488
Loss :  1.6395422220230103 1.9164658784866333 11.221872329711914
Loss :  1.694471836090088 1.7468774318695068 10.42885971069336
Loss :  1.6455374956130981 1.950011134147644 11.39559268951416
Loss :  1.6313661336898804 2.118638277053833 12.224556922912598
Loss :  1.6448522806167603 2.418034791946411 13.735026359558105
Loss :  1.7003244161605835 2.0688674449920654 12.044661521911621
  batch 60 loss: 1.7003244161605835, 2.0688674449920654, 12.044661521911621
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6318800449371338 2.8987419605255127 16.12558937072754
Loss :  1.6568838357925415 2.1511178016662598 12.41247272491455
Loss :  1.639822006225586 1.932064414024353 11.30014419555664
Loss :  1.62936532497406 1.9484121799468994 11.37142562866211
Loss :  1.6121114492416382 1.3458720445632935 8.341471672058105
Loss :  1.6156699657440186 3.925706386566162 21.24420166015625
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
Loss :  1.6284595727920532 3.9705262184143066 21.481090545654297
Loss :  1.6244232654571533 3.832062005996704 20.78473472595215
Loss :  1.630074143409729 3.8910515308380127 21.085330963134766
Total LOSS train 12.320064251239483 valid 21.148839473724365
CE LOSS train 1.6520363752658551 valid 0.40751853585243225
Contrastive LOSS train 2.133605564557589 valid 0.9727628827095032
EPOCH 222:
Loss :  1.6697295904159546 1.7462607622146606 10.401033401489258
Loss :  1.6814627647399902 1.8663787841796875 11.013357162475586
Loss :  1.653989315032959 1.938234567642212 11.345161437988281
Loss :  1.6592997312545776 1.7094979286193848 10.206789016723633
Loss :  1.6746684312820435 1.5698155164718628 9.523746490478516
Loss :  1.6432939767837524 1.674522876739502 10.015908241271973
Loss :  1.6750844717025757 1.930612325668335 11.328145980834961
Loss :  1.6534830331802368 1.9767357110977173 11.537161827087402
Loss :  1.6451308727264404 2.098345994949341 12.136860847473145
Loss :  1.6746559143066406 2.6504828929901123 14.927070617675781
Loss :  1.6372414827346802 2.1476621627807617 12.3755521774292
Loss :  1.636781930923462 2.04197096824646 11.846636772155762
Loss :  1.634374976158142 2.2915639877319336 13.092194557189941
Loss :  1.638998031616211 2.1906847953796387 12.592422485351562
Loss :  1.6875656843185425 2.0742502212524414 12.058816909790039
Loss :  1.684029221534729 2.010737895965576 11.737719535827637
Loss :  1.6317307949066162 1.9675790071487427 11.469626426696777
Loss :  1.655349612236023 1.779636263847351 10.5535306930542
Loss :  1.6283352375030518 1.6798137426376343 10.027403831481934
Loss :  1.682255506515503 1.9661298990249634 11.51290512084961
  batch 20 loss: 1.682255506515503, 1.9661298990249634, 11.51290512084961
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6499358415603638 1.9690749645233154 11.49531078338623
Loss :  1.6278884410858154 2.5272130966186523 14.263954162597656
Loss :  1.6440683603286743 2.071368455886841 12.000910758972168
Loss :  1.6582220792770386 1.7934601306915283 10.62552261352539
Loss :  1.6840603351593018 2.08111310005188 12.089625358581543
Loss :  1.6472080945968628 2.042341709136963 11.858916282653809
Loss :  1.655944585800171 2.458662509918213 13.949256896972656
Loss :  1.650553584098816 1.7926048040390015 10.613577842712402
Loss :  1.605444073677063 2.586456060409546 14.537724494934082
Loss :  1.680208444595337 2.218045949935913 12.770438194274902
Loss :  1.6062594652175903 2.1527259349823 12.369889259338379
Loss :  1.665574073791504 2.2406022548675537 12.868585586547852
Loss :  1.6441643238067627 2.596297264099121 14.625650405883789
Loss :  1.6424167156219482 1.885434627532959 11.06959056854248
Loss :  1.6122015714645386 2.2995996475219727 13.110199928283691
Loss :  1.6251193284988403 1.9563573598861694 11.406906127929688
Loss :  1.6238784790039062 2.404956579208374 13.648661613464355
Loss :  1.677433729171753 2.021127462387085 11.783071517944336
Loss :  1.6818991899490356 2.1056413650512695 12.210105895996094
Loss :  1.6906410455703735 1.9572861194610596 11.477071762084961
  batch 40 loss: 1.6906410455703735, 1.9572861194610596, 11.477071762084961
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6548662185668945 2.029721260070801 11.803472518920898
Loss :  1.6436082124710083 2.432490348815918 13.806059837341309
Loss :  1.6343419551849365 2.362959861755371 13.449141502380371
Loss :  1.645956039428711 1.8271275758743286 10.781594276428223
Loss :  1.6267931461334229 1.824116826057434 10.747377395629883
Loss :  1.6532617807388306 1.9389251470565796 11.347887992858887
Loss :  1.6816965341567993 1.7178409099578857 10.270901679992676
Loss :  1.6391358375549316 2.0524961948394775 11.901617050170898
Loss :  1.6937994956970215 2.539388418197632 14.390741348266602
Loss :  1.6441351175308228 1.7660173177719116 10.474222183227539
Loss :  1.6732763051986694 1.9444059133529663 11.395305633544922
Loss :  1.6677918434143066 1.9025894403457642 11.18073844909668
Loss :  1.6497573852539062 2.328827381134033 13.293893814086914
Loss :  1.674986720085144 2.567352533340454 14.511749267578125
Loss :  1.6402137279510498 1.8831884860992432 11.056156158447266
Loss :  1.694546103477478 2.0129144191741943 11.75911808013916
Loss :  1.6462273597717285 2.1077046394348145 12.184749603271484
Loss :  1.6326051950454712 1.9117332696914673 11.191271781921387
Loss :  1.6459383964538574 2.31322979927063 13.212087631225586
Loss :  1.701351523399353 1.720370888710022 10.303205490112305
  batch 60 loss: 1.701351523399353, 1.720370888710022, 10.303205490112305
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6327564716339111 2.2708685398101807 12.987098693847656
Loss :  1.6577774286270142 2.15733003616333 12.444427490234375
Loss :  1.64064621925354 2.2066891193389893 12.674091339111328
Loss :  1.6303319931030273 2.35202693939209 13.390466690063477
Loss :  1.6135796308517456 1.683520793914795 10.031184196472168
Loss :  1.6209990978240967 4.141737937927246 22.329689025878906
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.632444143295288 4.224419116973877 22.754539489746094
Loss :  1.6295183897018433 3.9401907920837402 21.330472946166992
Loss :  1.6307435035705566 3.935059070587158 21.30603790283203
Total LOSS train 11.985624210651105 valid 21.930184841156006
CE LOSS train 1.652922507432791 valid 0.40768587589263916
Contrastive LOSS train 2.066540334774898 valid 0.9837647676467896
EPOCH 223:
Loss :  1.6700416803359985 1.8692554235458374 11.016319274902344
Loss :  1.6818820238113403 2.031752109527588 11.840641975402832
Loss :  1.654938817024231 1.8841664791107178 11.07577133178711
Loss :  1.6605067253112793 1.7126984596252441 10.2239990234375
Loss :  1.6756198406219482 1.8316590785980225 10.833915710449219
Loss :  1.644695520401001 2.3644769191741943 13.467080116271973
Loss :  1.675875186920166 2.523613452911377 14.293941497802734
Loss :  1.654802680015564 2.5438568592071533 14.374086380004883
Loss :  1.6458595991134644 2.489610195159912 14.093911170959473
Loss :  1.6749519109725952 1.9760090112686157 11.554996490478516
Loss :  1.6373885869979858 2.032604455947876 11.800410270690918
Loss :  1.6367114782333374 2.2498104572296143 12.885763168334961
Loss :  1.634112000465393 1.9918363094329834 11.593294143676758
Loss :  1.6384825706481934 2.296475887298584 13.12086296081543
Loss :  1.6867305040359497 2.1934995651245117 12.654228210449219
Loss :  1.682796597480774 1.9780266284942627 11.572929382324219
Loss :  1.631156325340271 1.9817259311676025 11.539785385131836
Loss :  1.6549718379974365 2.4885637760162354 14.097790718078613
Loss :  1.628283143043518 2.6061291694641113 14.658928871154785
Loss :  1.6827188730239868 2.692964792251587 15.147542953491211
  batch 20 loss: 1.6827188730239868, 2.692964792251587, 15.147542953491211
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6499335765838623 2.706526517868042 15.18256664276123
Loss :  1.628002405166626 2.4446945190429688 13.85147476196289
Loss :  1.6442234516143799 2.3990960121154785 13.639702796936035
Loss :  1.658232569694519 2.357726812362671 13.446866989135742
Loss :  1.683547854423523 2.699232578277588 15.179710388183594
Loss :  1.646966814994812 2.561434268951416 14.454137802124023
Loss :  1.656140685081482 2.468285083770752 13.997565269470215
Loss :  1.6510272026062012 1.9847900867462158 11.57497787475586
Loss :  1.6069024801254272 2.560570240020752 14.40975284576416
Loss :  1.6814546585083008 2.4153664112091064 13.758286476135254
Loss :  1.6079881191253662 2.0799906253814697 12.007941246032715
Loss :  1.6672849655151367 1.8582597970962524 10.95858383178711
Loss :  1.6460740566253662 2.2886369228363037 13.089259147644043
Loss :  1.6442348957061768 2.4776108264923096 14.032288551330566
Loss :  1.614145040512085 2.079184055328369 12.010065078735352
Loss :  1.6269007921218872 1.9880144596099854 11.566972732543945
Loss :  1.625845193862915 2.236384153366089 12.80776596069336
Loss :  1.6790728569030762 1.9088499546051025 11.223321914672852
Loss :  1.6834080219268799 1.916245698928833 11.264636039733887
Loss :  1.6917818784713745 2.403592348098755 13.70974349975586
  batch 40 loss: 1.6917818784713745, 2.403592348098755, 13.70974349975586
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6560076475143433 2.513411521911621 14.223065376281738
Loss :  1.6443403959274292 2.137328624725342 12.330984115600586
Loss :  1.6342477798461914 2.0786616802215576 12.027556419372559
Loss :  1.646071195602417 2.043058156967163 11.86136245727539
Loss :  1.627212405204773 2.1375668048858643 12.315046310424805
Loss :  1.6535704135894775 2.286105155944824 13.08409595489502
Loss :  1.6824780702590942 2.148106813430786 12.423011779785156
Loss :  1.640590786933899 2.3880116939544678 13.580649375915527
Loss :  1.6962995529174805 2.4844677448272705 14.118638038635254
Loss :  1.646676778793335 2.2092857360839844 12.693105697631836
Loss :  1.6750872135162354 2.4309160709381104 13.829667091369629
Loss :  1.670184850692749 2.344130039215088 13.39083480834961
Loss :  1.651872158050537 2.1688239574432373 12.495992660522461
Loss :  1.6772507429122925 1.9773136377334595 11.56381893157959
Loss :  1.6406726837158203 2.3959152698516846 13.620248794555664
Loss :  1.6955660581588745 2.1831917762756348 12.61152458190918
Loss :  1.6465928554534912 2.6259090900421143 14.776138305664062
Loss :  1.6327515840530396 2.2440216541290283 12.852859497070312
Loss :  1.6460641622543335 2.3308138847351074 13.30013370513916
Loss :  1.7014764547348022 2.2719926834106445 13.061439514160156
  batch 60 loss: 1.7014764547348022, 2.2719926834106445, 13.061439514160156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.6325875520706177 2.0214719772338867 11.739947319030762
Loss :  1.6574751138687134 2.6373305320739746 14.844128608703613
Loss :  1.6404474973678589 2.034123659133911 11.811065673828125
Loss :  1.6301591396331787 2.103693962097168 12.148629188537598
Loss :  1.6134425401687622 1.9029947519302368 11.128416061401367
Loss :  1.6403741836547852 4.0934247970581055 22.107498168945312
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.651563286781311 3.9775567054748535 21.53934669494629
Loss :  1.6472352743148804 3.932447910308838 21.30947494506836
Loss :  1.6529784202575684 3.98453950881958 21.57567596435547
Total LOSS train 12.859141217745268 valid 21.632998943328857
CE LOSS train 1.6536126008400551 valid 0.4132446050643921
Contrastive LOSS train 2.2411057417209332 valid 0.996134877204895
EPOCH 224:
Loss :  1.669848918914795 2.43973970413208 13.868547439575195
Loss :  1.6815422773361206 2.0646984577178955 12.005034446716309
Loss :  1.6549170017242432 1.6312826871871948 9.81132984161377
Loss :  1.6606727838516235 2.164163827896118 12.481492042541504
Loss :  1.6754213571548462 1.8806909322738647 11.078876495361328
Loss :  1.6443982124328613 1.943379521369934 11.361295700073242
Loss :  1.675515055656433 2.1602964401245117 12.476997375488281
Loss :  1.6541812419891357 1.8961964845657349 11.135164260864258
Loss :  1.6455271244049072 2.3590872287750244 13.440962791442871
Loss :  1.6750353574752808 2.2980146408081055 13.165108680725098
Loss :  1.6378546953201294 2.650731325149536 14.891510963439941
Loss :  1.6376482248306274 2.0872087478637695 12.073692321777344
Loss :  1.63528311252594 1.8875911235809326 11.07323932647705
Loss :  1.6400607824325562 2.077355146408081 12.026836395263672
Loss :  1.6892807483673096 2.1565895080566406 12.472228050231934
Loss :  1.6845002174377441 2.147226095199585 12.420631408691406
Loss :  1.6339409351348877 2.695361614227295 15.110749244689941
Loss :  1.656844973564148 2.619605541229248 14.754873275756836
Loss :  1.6305675506591797 2.3377292156219482 13.3192138671875
Loss :  1.6837332248687744 2.5293688774108887 14.330577850341797
  batch 20 loss: 1.6837332248687744, 2.5293688774108887, 14.330577850341797
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.651902437210083 2.0023062229156494 11.663433074951172
Loss :  1.6295499801635742 2.0816304683685303 12.037702560424805
Loss :  1.6449328660964966 1.952485203742981 11.40735912322998
Loss :  1.6578940153121948 2.422790288925171 13.771845817565918
Loss :  1.6830872297286987 2.734942674636841 15.357800483703613
Loss :  1.6463496685028076 2.468043088912964 13.986565589904785
Loss :  1.655426263809204 2.44842529296875 13.897552490234375
Loss :  1.6498044729232788 2.4901766777038574 14.100687980651855
Loss :  1.604993462562561 2.5830423831939697 14.5202054977417
Loss :  1.679727554321289 2.7777364253997803 15.56840991973877
Loss :  1.605722427368164 2.953080415725708 16.371124267578125
Loss :  1.665617823600769 2.7182717323303223 15.256976127624512
Loss :  1.643875002861023 2.3552355766296387 13.420053482055664
Loss :  1.6418509483337402 2.485816240310669 14.070932388305664
Loss :  1.612294316291809 2.3953893184661865 13.589241027832031
Loss :  1.6247535943984985 2.0639169216156006 11.94433879852295
Loss :  1.6237835884094238 2.0337655544281006 11.792612075805664
Loss :  1.6773079633712769 1.9946151971817017 11.650383949279785
Loss :  1.681317925453186 2.2148916721343994 12.755776405334473
Loss :  1.6899306774139404 2.0861246585845947 12.120553970336914
  batch 40 loss: 1.6899306774139404, 2.0861246585845947, 12.120553970336914
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
Loss :  1.6543196439743042 2.0039117336273193 11.67387866973877
Loss :  1.6428391933441162 2.3695716857910156 13.490697860717773
Loss :  1.6339424848556519 2.2461061477661133 12.864473342895508
Loss :  1.6458024978637695 1.9180158376693726 11.235881805419922
Loss :  1.6270958185195923 1.7215551137924194 10.234871864318848
Loss :  1.6540049314498901 1.9475300312042236 11.391654968261719
Loss :  1.6830962896347046 1.8339282274246216 10.852737426757812
Loss :  1.640615701675415 2.2262442111968994 12.771836280822754
Loss :  1.6955703496932983 1.9771615266799927 11.581377983093262
Loss :  1.6458954811096191 1.780465841293335 10.548225402832031
Loss :  1.6747984886169434 1.9189890623092651 11.269742965698242
Loss :  1.669018030166626 2.6747994422912598 15.043014526367188
Loss :  1.6508408784866333 2.648578643798828 14.893733978271484
Loss :  1.6764887571334839 2.1196272373199463 12.274624824523926
Loss :  1.6406677961349487 2.3757855892181396 13.519596099853516
Loss :  1.695812702178955 2.294847249984741 13.170049667358398
Loss :  1.6473898887634277 2.133524179458618 12.315010070800781
Loss :  1.633118748664856 1.752963662147522 10.397936820983887
Loss :  1.6469789743423462 2.2184879779815674 12.739418983459473
Loss :  1.7022405862808228 2.41304087638855 13.76744556427002
  batch 60 loss: 1.7022405862808228, 2.41304087638855, 13.76744556427002
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.6336182355880737 2.209442377090454 12.680830001831055
Loss :  1.6583852767944336 1.9608920812606812 11.462845802307129
Loss :  1.6407227516174316 1.7907001972198486 10.594223022460938
Loss :  1.6303898096084595 2.3537487983703613 13.399133682250977
Loss :  1.612661361694336 1.846192717552185 10.84362506866455
Loss :  1.6248892545700073 4.356104850769043 23.405414581298828
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.6329418420791626 4.335978031158447 23.31283187866211
Loss :  1.6309987306594849 4.230049133300781 22.7812442779541
Loss :  1.6431236267089844 4.012766361236572 21.706954956054688
Total LOSS train 12.732288976816031 valid 22.80161142349243
CE LOSS train 1.6534340106523955 valid 0.4107809066772461
Contrastive LOSS train 2.2157709781940165 valid 1.003191590309143
EPOCH 225:
Loss :  1.6695454120635986 2.19185471534729 12.62881851196289
Loss :  1.6811286211013794 2.7947921752929688 15.655089378356934
Loss :  1.6545238494873047 1.5216090679168701 9.262569427490234
Loss :  1.6601554155349731 1.8723427057266235 11.021868705749512
Loss :  1.675568699836731 1.616295337677002 9.757044792175293
Loss :  1.6446211338043213 2.026546001434326 11.777351379394531
Loss :  1.6759365797042847 2.167273998260498 12.512307167053223
Loss :  1.6548707485198975 2.384932041168213 13.579530715942383
Loss :  1.6457195281982422 1.7714029550552368 10.502734184265137
Loss :  1.675917148590088 2.5092453956604004 14.222143173217773
Loss :  1.6379859447479248 2.9171204566955566 16.223587036132812
Loss :  1.6374201774597168 1.9832955598831177 11.553897857666016
Loss :  1.6348243951797485 1.781307339668274 10.541361808776855
Loss :  1.6391913890838623 2.085974931716919 12.069066047668457
Loss :  1.6879311800003052 2.004160165786743 11.708731651306152
Loss :  1.6837013959884644 1.9330158233642578 11.348780632019043
Loss :  1.632633090019226 1.8866734504699707 11.065999984741211
Loss :  1.6560447216033936 2.1103532314300537 12.20781135559082
Loss :  1.6299020051956177 1.9422518014907837 11.341160774230957
Loss :  1.6840521097183228 1.7450566291809082 10.409335136413574
  batch 20 loss: 1.6840521097183228, 1.7450566291809082, 10.409335136413574
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.6517702341079712 1.7686136960983276 10.49483871459961
Loss :  1.6296441555023193 1.7974846363067627 10.617067337036133
Loss :  1.6454071998596191 2.319823980331421 13.244527816772461
Loss :  1.6586931943893433 1.7351206541061401 10.334296226501465
Loss :  1.6840757131576538 2.191549062728882 12.641820907592773
Loss :  1.647884726524353 2.5873231887817383 14.584500312805176
Loss :  1.6570110321044922 2.6787993907928467 15.051008224487305
Loss :  1.6523776054382324 2.2584879398345947 12.944816589355469
Loss :  1.6080728769302368 1.9399302005767822 11.307723999023438
Loss :  1.68235182762146 2.09978985786438 12.18130111694336
Loss :  1.6087291240692139 2.456900119781494 13.893229484558105
Loss :  1.6672065258026123 1.7902334928512573 10.61837387084961
Loss :  1.6456272602081299 1.8795615434646606 11.043435096740723
Loss :  1.6437270641326904 2.242581367492676 12.856634140014648
Loss :  1.6136971712112427 2.5851492881774902 14.539443969726562
Loss :  1.6265650987625122 2.3418354988098145 13.335741996765137
Loss :  1.6253488063812256 2.417480945587158 13.712753295898438
Loss :  1.6781708002090454 2.7210257053375244 15.283299446105957
Loss :  1.6824980974197388 2.0447874069213867 11.906435012817383
Loss :  1.6911557912826538 2.2128984928131104 12.755647659301758
  batch 40 loss: 1.6911557912826538, 2.2128984928131104, 12.755647659301758
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6559141874313354 2.310844898223877 13.210138320922852
Loss :  1.6446681022644043 1.741406798362732 10.351701736450195
Loss :  1.6354632377624512 1.8161951303482056 10.716438293457031
Loss :  1.6469858884811401 2.4761977195739746 14.027975082397461
Loss :  1.6284571886062622 2.3967645168304443 13.612279891967773
Loss :  1.6550252437591553 2.579134225845337 14.55069637298584
Loss :  1.6830332279205322 2.427419424057007 13.820130348205566
Loss :  1.6417467594146729 2.390022039413452 13.591856956481934
Loss :  1.6958781480789185 2.4776272773742676 14.084013938903809
Loss :  1.6473448276519775 2.439302921295166 13.84385871887207
Loss :  1.6764434576034546 2.470094919204712 14.026917457580566
Loss :  1.6707725524902344 2.52695369720459 14.305541038513184
Loss :  1.6532222032546997 2.164499044418335 12.475717544555664
Loss :  1.6774009466171265 2.0849523544311523 12.10216236114502
Loss :  1.6426784992218018 2.139600992202759 12.340682983398438
Loss :  1.6957433223724365 1.8981941938400269 11.186714172363281
Loss :  1.6479297876358032 2.1301960945129395 12.298910140991211
Loss :  1.6335923671722412 1.9296447038650513 11.281816482543945
Loss :  1.647072434425354 2.3257181644439697 13.275663375854492
Loss :  1.701340675354004 2.4973349571228027 14.188015937805176
  batch 60 loss: 1.701340675354004, 2.4973349571228027, 14.188015937805176
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.633586049079895 2.110513210296631 12.186152458190918
Loss :  1.658372163772583 2.5916547775268555 14.616645812988281
Loss :  1.6416512727737427 2.59022855758667 14.592794418334961
Loss :  1.6316074132919312 2.7574169635772705 15.418691635131836
Loss :  1.614547848701477 1.700065016746521 10.114872932434082
Loss :  1.638325572013855 4.031343460083008 21.795042037963867
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.647632360458374 4.0648980140686035 21.972122192382812
Loss :  1.646672010421753 3.90596866607666 21.176513671875
Loss :  1.6491702795028687 3.7558329105377197 20.428335189819336
Total LOSS train 12.599330359238845 valid 21.343003273010254
CE LOSS train 1.6541871639398429 valid 0.41229256987571716
Contrastive LOSS train 2.18902865923368 valid 0.9389582276344299
EPOCH 226:
Loss :  1.6703400611877441 2.101691722869873 12.17879867553711
Loss :  1.6818832159042358 2.22761607170105 12.819963455200195
Loss :  1.6557978391647339 2.217890739440918 12.745251655578613
Loss :  1.6610625982284546 2.7202634811401367 15.26237964630127
Loss :  1.6762093305587769 1.7699650526046753 10.526034355163574
Loss :  1.6449474096298218 1.9140657186508179 11.215275764465332
Loss :  1.6759670972824097 2.0517609119415283 11.934771537780762
Loss :  1.6552330255508423 1.989056944847107 11.600518226623535
Loss :  1.6470611095428467 1.9277631044387817 11.285877227783203
Loss :  1.6762503385543823 1.6869120597839355 10.110811233520508
Loss :  1.6392747163772583 2.7054762840270996 15.166656494140625
Loss :  1.639046549797058 2.6300151348114014 14.789122581481934
Loss :  1.636644721031189 2.510788917541504 14.19058895111084
Loss :  1.6414783000946045 2.133479595184326 12.308876991271973
Loss :  1.6891679763793945 2.414844036102295 13.763388633728027
Loss :  1.6842178106307983 2.376303195953369 13.565732955932617
Loss :  1.6342624425888062 2.2441864013671875 12.855194091796875
Loss :  1.656530499458313 2.2396762371063232 12.854911804199219
Loss :  1.6303519010543823 2.1396172046661377 12.328437805175781
Loss :  1.6844182014465332 2.3174755573272705 13.271795272827148
  batch 20 loss: 1.6844182014465332, 2.3174755573272705, 13.271795272827148
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.652145266532898 1.7787667512893677 10.545979499816895
Loss :  1.6301876306533813 2.0448081493377686 11.854228973388672
Loss :  1.6458516120910645 1.707399606704712 10.182849884033203
Loss :  1.6593787670135498 1.7529582977294922 10.42417049407959
Loss :  1.684708833694458 2.376051664352417 13.564967155456543
Loss :  1.6481776237487793 2.315446376800537 13.225410461425781
Loss :  1.656762719154358 2.455674648284912 13.935136795043945
Loss :  1.6512757539749146 1.853490948677063 10.918730735778809
Loss :  1.6074162721633911 2.324535608291626 13.230093955993652
Loss :  1.680684208869934 2.4591050148010254 13.976208686828613
Loss :  1.6088204383850098 2.3867573738098145 13.542606353759766
Loss :  1.6671401262283325 2.2920267581939697 13.127273559570312
Loss :  1.6462517976760864 1.9605586528778076 11.449045181274414
Loss :  1.644876480102539 1.911545991897583 11.202606201171875
Loss :  1.6155259609222412 2.301525831222534 13.12315559387207
Loss :  1.6279529333114624 2.2316057682037354 12.785981178283691
Loss :  1.6269429922103882 2.797863006591797 15.616257667541504
Loss :  1.6790512800216675 2.6879420280456543 15.118762016296387
Loss :  1.683525562286377 2.3405354022979736 13.386201858520508
Loss :  1.6918326616287231 2.264331102371216 13.013487815856934
  batch 40 loss: 1.6918326616287231, 2.264331102371216, 13.013487815856934
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.6565978527069092 2.0838825702667236 12.076010704040527
Loss :  1.6451091766357422 2.606670618057251 14.678462028503418
Loss :  1.6368149518966675 2.6840007305145264 15.056818962097168
Loss :  1.6477649211883545 2.6503751277923584 14.899641036987305
Loss :  1.629194736480713 2.0344185829162598 11.801286697387695
Loss :  1.6554533243179321 2.07670259475708 12.038966178894043
Loss :  1.6838788986206055 1.9140113592147827 11.253935813903809
Loss :  1.642191767692566 1.9785014390945435 11.534699440002441
Loss :  1.6956019401550293 2.6781771183013916 15.08648681640625
Loss :  1.647310495376587 2.4042837619781494 13.668728828430176
Loss :  1.6761258840560913 2.059267044067383 11.972460746765137
Loss :  1.6705741882324219 1.8878438472747803 11.109793663024902
Loss :  1.652462124824524 1.9580321311950684 11.442623138427734
Loss :  1.6776632070541382 2.2963922023773193 13.159624099731445
Loss :  1.643202781677246 2.7058675289154053 15.172540664672852
Loss :  1.6973669528961182 2.6585824489593506 14.990279197692871
Loss :  1.65044367313385 2.262883186340332 12.964859962463379
Loss :  1.636539340019226 2.2299952507019043 12.786516189575195
Loss :  1.6505221128463745 2.31313419342041 13.216193199157715
Loss :  1.704411506652832 2.056504011154175 11.986931800842285
  batch 60 loss: 1.704411506652832, 2.056504011154175, 11.986931800842285
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.6364245414733887 2.4902937412261963 14.087892532348633
Loss :  1.6608437299728394 2.3728723526000977 13.525205612182617
Loss :  1.6443028450012207 1.7400867938995361 10.344736099243164
Loss :  1.6349356174468994 2.2153658866882324 12.71176528930664
Loss :  1.6184523105621338 1.491586685180664 9.076385498046875
Loss :  1.6169463396072388 4.347234725952148 23.353120803833008
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.6266552209854126 4.4630818367004395 23.94206428527832
Loss :  1.6234277486801147 4.234845161437988 22.797653198242188
Loss :  1.6435497999191284 4.195287227630615 22.619985580444336
Total LOSS train 12.763698240426871 valid 23.178205966949463
CE LOSS train 1.6551206607085007 valid 0.4108874499797821
Contrastive LOSS train 2.221715516310472 valid 1.0488218069076538
EPOCH 227:
Loss :  1.6749045848846436 2.166764736175537 12.508728981018066
Loss :  1.6859074831008911 2.9728176593780518 16.54999542236328
Loss :  1.6591886281967163 2.5158424377441406 14.23840045928955
Loss :  1.6636945009231567 2.417635679244995 13.751873016357422
Loss :  1.6779980659484863 1.7442443370819092 10.399219512939453
Loss :  1.6466480493545532 1.6183027029037476 9.73816204071045
Loss :  1.6770836114883423 2.2409613132476807 12.881890296936035
Loss :  1.6556396484375 1.5988218784332275 9.649748802185059
Loss :  1.6477560997009277 2.241980791091919 12.857660293579102
Loss :  1.6769107580184937 1.7938036918640137 10.645929336547852
Loss :  1.640095591545105 2.3967819213867188 13.624005317687988
Loss :  1.6399173736572266 2.385693073272705 13.568382263183594
Loss :  1.6381539106369019 2.437568426132202 13.825996398925781
Loss :  1.6429409980773926 2.8858842849731445 16.072362899780273
Loss :  1.6910637617111206 2.7993879318237305 15.688003540039062
Loss :  1.6864310503005981 2.7129430770874023 15.25114631652832
Loss :  1.6364277601242065 2.380631446838379 13.53958511352539
Loss :  1.6589502096176147 2.0469796657562256 11.893848419189453
Loss :  1.633685827255249 1.9523470401763916 11.395421028137207
Loss :  1.6870843172073364 2.1734001636505127 12.554084777832031
  batch 20 loss: 1.6870843172073364, 2.1734001636505127, 12.554084777832031
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.654604196548462 2.0097715854644775 11.703461647033691
Loss :  1.6329878568649292 2.6907620429992676 15.086797714233398
Loss :  1.6489582061767578 2.27242112159729 13.011063575744629
Loss :  1.6618481874465942 2.1524925231933594 12.424310684204102
Loss :  1.686383843421936 3.555715799331665 19.464963912963867
Loss :  1.650060772895813 2.128704309463501 12.29358196258545
Loss :  1.6587693691253662 1.877402424812317 11.045782089233398
Loss :  1.6534512042999268 1.8240504264831543 10.773703575134277
Loss :  1.60850989818573 2.4668939113616943 13.94297981262207
Loss :  1.6825190782546997 2.502192974090576 14.19348430633545
Loss :  1.6094133853912354 2.7111361026763916 15.165093421936035
Loss :  1.6680235862731934 2.1643049716949463 12.489547729492188
Loss :  1.6464086771011353 2.1290111541748047 12.291464805603027
Loss :  1.6444717645645142 3.1274571418762207 17.281757354736328
Loss :  1.6152409315109253 3.9858129024505615 21.54430389404297
Loss :  1.6272326707839966 2.154017210006714 12.397318840026855
Loss :  1.6263427734375 2.3989133834838867 13.620909690856934
Loss :  1.679280161857605 2.096590518951416 12.162232398986816
Loss :  1.6834492683410645 1.9761770963668823 11.564334869384766
Loss :  1.6919276714324951 1.9280036687850952 11.33194637298584
  batch 40 loss: 1.6919276714324951, 1.9280036687850952, 11.33194637298584
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6575138568878174 2.2831602096557617 13.073314666748047
Loss :  1.6461269855499268 2.890676259994507 16.09950828552246
Loss :  1.6385940313339233 2.6481714248657227 14.879450798034668
Loss :  1.6495022773742676 2.493170738220215 14.1153564453125
Loss :  1.6312912702560425 2.247824192047119 12.87041187286377
Loss :  1.6568858623504639 2.4948909282684326 14.131340980529785
Loss :  1.6841957569122314 1.7814680337905884 10.591535568237305
Loss :  1.6441233158111572 1.8907160758972168 11.09770393371582
Loss :  1.6966458559036255 2.3523714542388916 13.458502769470215
Loss :  1.648432970046997 2.00258207321167 11.661343574523926
Loss :  1.677625298500061 2.972872734069824 16.541988372802734
Loss :  1.672558307647705 2.956615924835205 16.455636978149414
Loss :  1.653625726699829 1.9836838245391846 11.572044372558594
Loss :  1.6781988143920898 2.721055507659912 15.283476829528809
Loss :  1.6437408924102783 1.9376490116119385 11.331985473632812
Loss :  1.69669508934021 2.357363700866699 13.483513832092285
Loss :  1.6493263244628906 2.1160686016082764 12.229669570922852
Loss :  1.6355394124984741 1.9273009300231934 11.27204418182373
Loss :  1.6492257118225098 2.6974904537200928 15.136678695678711
Loss :  1.7031683921813965 2.6289587020874023 14.84796142578125
  batch 60 loss: 1.7031683921813965, 2.6289587020874023, 14.84796142578125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.635807752609253 2.5229170322418213 14.25039291381836
Loss :  1.661116600036621 2.4028172492980957 13.675202369689941
Loss :  1.6437036991119385 2.376377820968628 13.525592803955078
Loss :  1.6336630582809448 2.6680636405944824 14.973981857299805
Loss :  1.6165204048156738 1.613400936126709 9.683525085449219
Loss :  1.6461782455444336 4.260815620422363 22.95025634765625
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.6557246446609497 4.324740409851074 23.279428482055664
Loss :  1.6527737379074097 4.227829933166504 22.791921615600586
Loss :  1.6580569744110107 4.031669616699219 21.816404342651367
Total LOSS train 13.394856100815993 valid 22.709502696990967
CE LOSS train 1.6562183600205642 valid 0.4145142436027527
Contrastive LOSS train 2.347727553661053 valid 1.0079174041748047
EPOCH 228:
Loss :  1.6727125644683838 2.080306053161621 12.07424259185791
Loss :  1.6843606233596802 2.2709617614746094 13.039169311523438
Loss :  1.6576241254806519 1.7581015825271606 10.448132514953613
Loss :  1.6626853942871094 2.642383337020874 14.874602317810059
Loss :  1.6780809164047241 1.9576375484466553 11.466268539428711
Loss :  1.6470450162887573 2.98915958404541 16.59284210205078
Loss :  1.678065299987793 2.430830717086792 13.832219123840332
Loss :  1.6574046611785889 2.5435409545898438 14.375109672546387
Loss :  1.6495225429534912 2.1140480041503906 12.219762802124023
Loss :  1.678407907485962 2.3166792392730713 13.26180362701416
Loss :  1.6415122747421265 2.6595847606658936 14.939435958862305
Loss :  1.6410658359527588 1.9680449962615967 11.481290817260742
Loss :  1.6388260126113892 2.93650484085083 16.32135009765625
Loss :  1.6435329914093018 1.932516098022461 11.306113243103027
Loss :  1.6916753053665161 1.9685300588607788 11.53432559967041
Loss :  1.686945915222168 1.9504587650299072 11.439239501953125
Loss :  1.637391209602356 2.2366859912872314 12.820820808410645
Loss :  1.6592841148376465 2.0254883766174316 11.786725997924805
Loss :  1.633949637413025 2.1772782802581787 12.520340919494629
Loss :  1.6863293647766113 2.351196050643921 13.442310333251953
  batch 20 loss: 1.6863293647766113, 2.351196050643921, 13.442310333251953
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6545894145965576 2.070507287979126 12.007125854492188
Loss :  1.6332228183746338 1.852652668952942 10.896486282348633
Loss :  1.6486338376998901 1.8654693365097046 10.975980758666992
Loss :  1.6620397567749023 1.9679006338119507 11.501543045043945
Loss :  1.686888337135315 2.6830337047576904 15.102057456970215
Loss :  1.651190161705017 2.4761359691619873 14.031869888305664
Loss :  1.6596009731292725 2.3960986137390137 13.640094757080078
Loss :  1.6549162864685059 2.3764729499816895 13.537281036376953
Loss :  1.6110813617706299 2.6456363201141357 14.839262962341309
Loss :  1.6839548349380493 2.9957258701324463 16.66258430480957
Loss :  1.6120041608810425 2.0370631217956543 11.797320365905762
Loss :  1.6695513725280762 1.9402941465377808 11.371021270751953
Loss :  1.6486260890960693 2.071965217590332 12.008452415466309
Loss :  1.646867036819458 2.1541011333465576 12.417372703552246
Loss :  1.618139386177063 2.320783853530884 13.222058296203613
Loss :  1.6304951906204224 2.044642210006714 11.853706359863281
Loss :  1.629940390586853 1.9989392757415771 11.62463665008545
Loss :  1.6813151836395264 2.3117423057556152 13.24002742767334
Loss :  1.6856752634048462 1.8506072759628296 10.938712120056152
Loss :  1.694149374961853 2.0598933696746826 11.993616104125977
  batch 40 loss: 1.694149374961853, 2.0598933696746826, 11.993616104125977
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6597367525100708 1.8726904392242432 11.023188591003418
Loss :  1.648744821548462 1.6579865217208862 9.938676834106445
Loss :  1.6400229930877686 1.9217002391815186 11.24852466583252
Loss :  1.650704264640808 2.3542115688323975 13.421762466430664
Loss :  1.6325838565826416 2.143683433532715 12.351000785827637
Loss :  1.6577216386795044 2.017561435699463 11.745528221130371
Loss :  1.6850244998931885 1.757893443107605 10.474491119384766
Loss :  1.6447113752365112 1.8345242738723755 10.817333221435547
Loss :  1.6968169212341309 1.933343768119812 11.363534927368164
Loss :  1.6489449739456177 1.705270528793335 10.175297737121582
Loss :  1.677522897720337 2.3897833824157715 13.626440048217773
Loss :  1.671779751777649 1.776214599609375 10.552852630615234
Loss :  1.654222011566162 1.7960448265075684 10.63444709777832
Loss :  1.6783429384231567 2.364712715148926 13.501906394958496
Loss :  1.6457633972167969 2.3760805130004883 13.526165962219238
Loss :  1.6972655057907104 2.6192924976348877 14.79372787475586
Loss :  1.651210069656372 2.0924360752105713 12.11338996887207
Loss :  1.6379303932189941 1.9877911806106567 11.576887130737305
Loss :  1.6513192653656006 2.3877341747283936 13.589990615844727
Loss :  1.7044881582260132 1.666877269744873 10.038874626159668
  batch 60 loss: 1.7044881582260132, 1.666877269744873, 10.038874626159668
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6383968591690063 1.889482855796814 11.085811614990234
Loss :  1.6629949808120728 2.121622323989868 12.271106719970703
Loss :  1.6462125778198242 1.5612345933914185 9.452384948730469
Loss :  1.637001633644104 2.006237268447876 11.668188095092773
Loss :  1.6203117370605469 1.465196132659912 8.946292877197266
Loss :  1.6460447311401367 4.122437953948975 22.258235931396484
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6545603275299072 4.040521621704102 21.857168197631836
Loss :  1.6533961296081543 4.044736862182617 21.8770809173584
Loss :  1.657537579536438 3.8179800510406494 20.747438430786133
Total LOSS train 12.359617247948279 valid 21.684980869293213
CE LOSS train 1.6573704187686626 valid 0.4143843948841095
Contrastive LOSS train 2.140449358866765 valid 0.9544950127601624
EPOCH 229:
Loss :  1.6739245653152466 1.7397593259811401 10.372720718383789
Loss :  1.6849972009658813 2.762195348739624 15.49597454071045
Loss :  1.6584497690200806 1.9985252618789673 11.651076316833496
Loss :  1.663527011871338 2.095508337020874 12.141069412231445
Loss :  1.6783448457717896 1.701585292816162 10.186271667480469
Loss :  1.6477181911468506 2.4308319091796875 13.801877975463867
Loss :  1.67800772190094 2.1937549114227295 12.646782875061035
Loss :  1.657225489616394 1.9907646179199219 11.611048698425293
Loss :  1.6494722366333008 1.703356146812439 10.166253089904785
Loss :  1.6784456968307495 1.5438694953918457 9.39779281616211
Loss :  1.642158031463623 2.1691572666168213 12.487943649291992
Loss :  1.6419124603271484 2.208059787750244 12.682210922241211
Loss :  1.6402497291564941 1.833385944366455 10.807178497314453
Loss :  1.6448396444320679 1.9690239429473877 11.489958763122559
Loss :  1.6924632787704468 1.784082055091858 10.612873077392578
Loss :  1.6871652603149414 2.488752841949463 14.130928993225098
Loss :  1.6386563777923584 2.499763011932373 14.137472152709961
Loss :  1.6601418256759644 2.008150815963745 11.700896263122559
Loss :  1.6351126432418823 1.56234872341156 9.446856498718262
Loss :  1.6875073909759521 2.5859034061431885 14.617024421691895
  batch 20 loss: 1.6875073909759521, 2.5859034061431885, 14.617024421691895
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6552619934082031 2.339782238006592 13.35417366027832
Loss :  1.634545922279358 2.1168341636657715 12.218717575073242
Loss :  1.6494905948638916 2.058821678161621 11.943598747253418
Loss :  1.6625856161117554 2.01834774017334 11.754323959350586
Loss :  1.686977505683899 2.917886972427368 16.276412963867188
Loss :  1.651185154914856 2.68992018699646 15.100786209106445
Loss :  1.6599204540252686 2.686610221862793 15.092971801757812
Loss :  1.654632329940796 2.358933687210083 13.449300765991211
Loss :  1.611344814300537 1.8674503564834595 10.948596954345703
Loss :  1.6838366985321045 2.2472758293151855 12.92021656036377
Loss :  1.6125273704528809 2.270066261291504 12.962858200073242
Loss :  1.6700135469436646 1.8682843446731567 11.011435508728027
Loss :  1.649696707725525 2.5743026733398438 14.521209716796875
Loss :  1.6479482650756836 2.7010722160339355 15.15330982208252
Loss :  1.6198381185531616 2.8386969566345215 15.813323020935059
Loss :  1.6317899227142334 2.6440930366516113 14.852254867553711
Loss :  1.6314098834991455 1.8565058708190918 10.913939476013184
Loss :  1.681817650794983 1.974289059638977 11.553263664245605
Loss :  1.6861960887908936 1.844891905784607 10.910655975341797
Loss :  1.6942520141601562 2.12434458732605 12.315975189208984
  batch 40 loss: 1.6942520141601562, 2.12434458732605, 12.315975189208984
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6604039669036865 1.9655803442001343 11.488306045532227
Loss :  1.649548053741455 1.8387372493743896 10.84323501586914
Loss :  1.6413955688476562 2.109046459197998 12.186628341674805
Loss :  1.6518218517303467 2.5205442905426025 14.25454330444336
Loss :  1.6340076923370361 1.8672120571136475 10.970067977905273
Loss :  1.6588537693023682 2.5624568462371826 14.471138000488281
Loss :  1.6859341859817505 2.0059311389923096 11.71558952331543
Loss :  1.6463305950164795 1.9812030792236328 11.552346229553223
Loss :  1.6974928379058838 2.3607890605926514 13.50143814086914
Loss :  1.6507539749145508 1.9588342905044556 11.444925308227539
Loss :  1.6792281866073608 2.166694164276123 12.512699127197266
Loss :  1.6733779907226562 1.8537946939468384 10.942351341247559
Loss :  1.6556631326675415 1.6872361898422241 10.09184455871582
Loss :  1.6794794797897339 1.7467864751815796 10.413412094116211
Loss :  1.646098017692566 2.1603176593780518 12.447686195373535
Loss :  1.6973685026168823 1.8161154985427856 10.777946472167969
Loss :  1.651187777519226 2.224937677383423 12.77587604522705
Loss :  1.6377424001693726 1.8207080364227295 10.74128246307373
Loss :  1.6509597301483154 2.156360626220703 12.43276309967041
Loss :  1.704315423965454 2.0535287857055664 11.971959114074707
  batch 60 loss: 1.704315423965454, 2.0535287857055664, 11.971959114074707
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6382267475128174 2.7643587589263916 15.460020065307617
Loss :  1.6625627279281616 2.527865409851074 14.301889419555664
Loss :  1.6456096172332764 2.1268279552459717 12.279749870300293
Loss :  1.6360418796539307 2.5821945667266846 14.547014236450195
Loss :  1.619547963142395 1.44369375705719 8.838016510009766
Loss :  1.6326686143875122 4.2849884033203125 23.0576114654541
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.643686294555664 4.226471900939941 22.776044845581055
Loss :  1.641556978225708 4.086122989654541 22.072172164916992
Loss :  1.6448445320129395 4.051846981048584 21.90407943725586
Total LOSS train 12.39406560751108 valid 22.452476978302002
CE LOSS train 1.6579929553545438 valid 0.41121113300323486
Contrastive LOSS train 2.147214515392597 valid 1.012961745262146
EPOCH 230:
Loss :  1.673899531364441 1.7757670879364014 10.552735328674316
Loss :  1.6849358081817627 2.181942939758301 12.594650268554688
Loss :  1.6588810682296753 1.8153787851333618 10.735774993896484
Loss :  1.6644399166107178 1.9152429103851318 11.240654945373535
Loss :  1.6787517070770264 1.6086807250976562 9.722155570983887
Loss :  1.6487994194030762 1.7367507219314575 10.33255386352539
Loss :  1.6791242361068726 1.9058277606964111 11.20826244354248
Loss :  1.6588108539581299 2.53399920463562 14.32880687713623
Loss :  1.6508517265319824 2.024456262588501 11.77313232421875
Loss :  1.6795896291732788 1.8689111471176147 11.024145126342773
Loss :  1.6432623863220215 2.289612054824829 13.09132194519043
Loss :  1.6427969932556152 2.3236172199249268 13.260883331298828
Loss :  1.6408967971801758 1.6680357456207275 9.981075286865234
Loss :  1.6454248428344727 1.8753132820129395 11.021990776062012
Loss :  1.6922991275787354 1.7668818235397339 10.526707649230957
Loss :  1.6867458820343018 1.8005681037902832 10.68958568572998
Loss :  1.6386362314224243 2.0699353218078613 11.988312721252441
Loss :  1.6599982976913452 2.387564182281494 13.597818374633789
Loss :  1.6354893445968628 2.0928688049316406 12.099833488464355
Loss :  1.6869802474975586 2.15625262260437 12.468243598937988
  batch 20 loss: 1.6869802474975586, 2.15625262260437, 12.468243598937988
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6556497812271118 1.7136728763580322 10.224014282226562
Loss :  1.6343986988067627 1.6990776062011719 10.129786491394043
Loss :  1.6490665674209595 1.9779034852981567 11.53858470916748
Loss :  1.6625062227249146 1.7717134952545166 10.521073341369629
Loss :  1.6870862245559692 1.9160856008529663 11.2675142288208
Loss :  1.6514443159103394 2.1281323432922363 12.292105674743652
Loss :  1.660520315170288 2.721593141555786 15.268486022949219
Loss :  1.6558253765106201 2.1121127605438232 12.216389656066895
Loss :  1.6131056547164917 2.3110857009887695 13.168534278869629
Loss :  1.6854239702224731 2.7547607421875 15.459227561950684
Loss :  1.6148977279663086 2.654468297958374 14.887239456176758
Loss :  1.6715885400772095 2.558582305908203 14.464500427246094
Loss :  1.6514065265655518 2.627835512161255 14.790583610534668
Loss :  1.6497267484664917 2.0678324699401855 11.988889694213867
Loss :  1.6214262247085571 2.28446364402771 13.043745040893555
Loss :  1.6328257322311401 2.0526227951049805 11.895939826965332
Loss :  1.6316604614257812 1.8392399549484253 10.827859878540039
Loss :  1.6820521354675293 2.9525258541107178 16.44468116760254
Loss :  1.6857244968414307 2.624579906463623 14.808624267578125
Loss :  1.6939305067062378 2.243712902069092 12.912495613098145
  batch 40 loss: 1.6939305067062378, 2.243712902069092, 12.912495613098145
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.659941554069519 2.6158127784729004 14.739005088806152
Loss :  1.6480783224105835 2.04555344581604 11.875844955444336
Loss :  1.639654278755188 2.203796863555908 12.658638000488281
Loss :  1.649828553199768 2.033534526824951 11.81750202178955
Loss :  1.6316735744476318 1.896898627281189 11.116167068481445
Loss :  1.656927466392517 2.061603307723999 11.964943885803223
Loss :  1.684586763381958 1.7603492736816406 10.486332893371582
Loss :  1.6449668407440186 2.2760488986968994 13.025211334228516
Loss :  1.6972962617874146 2.2274203300476074 12.83439826965332
Loss :  1.6502602100372314 2.347011089324951 13.385315895080566
Loss :  1.6787824630737305 2.710362195968628 15.23059368133545
Loss :  1.6728156805038452 2.256054162979126 12.953085899353027
Loss :  1.6554441452026367 2.0855536460876465 12.083212852478027
Loss :  1.6794567108154297 2.300152063369751 13.180216789245605
Loss :  1.647058129310608 2.7797067165374756 15.545592308044434
Loss :  1.6977005004882812 2.1711487770080566 12.553443908691406
Loss :  1.652182698249817 1.9203218221664429 11.253791809082031
Loss :  1.6386315822601318 1.7532957792282104 10.405110359191895
Loss :  1.651472568511963 2.225231409072876 12.777629852294922
Loss :  1.7042697668075562 1.975364089012146 11.581089973449707
  batch 60 loss: 1.7042697668075562, 1.975364089012146, 11.581089973449707
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6384755373001099 2.214771270751953 12.712331771850586
Loss :  1.6627384424209595 1.7304564714431763 10.315021514892578
Loss :  1.6458286046981812 1.6181741952896118 9.736699104309082
Loss :  1.6363633871078491 2.095219612121582 12.11246109008789
Loss :  1.6196807622909546 1.5418843030929565 9.329102516174316
Loss :  1.641995906829834 4.243714332580566 22.860567092895508
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6510472297668457 4.271684169769287 23.00946807861328
Loss :  1.6494566202163696 4.143588542938232 22.367399215698242
Loss :  1.6565213203430176 4.058828353881836 21.95066261291504
Total LOSS train 12.247102502676157 valid 22.547024250030518
CE LOSS train 1.6582306935237003 valid 0.4141303300857544
Contrastive LOSS train 2.1177743654984695 valid 1.014707088470459
EPOCH 231:
Loss :  1.673856258392334 2.0039238929748535 11.693475723266602
Loss :  1.684932827949524 2.273259162902832 13.051228523254395
Loss :  1.6591322422027588 1.8595529794692993 10.956896781921387
Loss :  1.664743423461914 2.000507116317749 11.667279243469238
Loss :  1.6791962385177612 2.5502498149871826 14.430445671081543
Loss :  1.6492959260940552 2.7116942405700684 15.207767486572266
Loss :  1.67933988571167 1.9606328010559082 11.482503890991211
Loss :  1.6587563753128052 1.7864654064178467 10.591083526611328
Loss :  1.6509023904800415 1.6905186176300049 10.103495597839355
Loss :  1.678965449333191 1.9403269290924072 11.380599975585938
Loss :  1.6428110599517822 2.269641637802124 12.991019248962402
Loss :  1.642246961593628 2.1174964904785156 12.229729652404785
Loss :  1.640105962753296 2.246143341064453 12.87082290649414
Loss :  1.644153356552124 1.9221147298812866 11.254727363586426
Loss :  1.6914535760879517 2.1354422569274902 12.36866569519043
Loss :  1.6869713068008423 2.203465223312378 12.70429801940918
Loss :  1.6380685567855835 2.2440342903137207 12.85823917388916
Loss :  1.6602517366409302 2.035313129425049 11.836816787719727
Loss :  1.635270595550537 2.2480764389038086 12.875652313232422
Loss :  1.687362790107727 1.8938661813735962 11.156693458557129
  batch 20 loss: 1.687362790107727, 1.8938661813735962, 11.156693458557129
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6557304859161377 1.8244690895080566 10.778075218200684
Loss :  1.6346089839935303 2.2204368114471436 12.736793518066406
Loss :  1.650052547454834 1.9742145538330078 11.521125793457031
Loss :  1.6626720428466797 2.047977924346924 11.90256118774414
Loss :  1.687101125717163 2.546299695968628 14.418600082397461
Loss :  1.651627779006958 1.9672223329544067 11.487739562988281
Loss :  1.6604573726654053 2.0484120845794678 11.902518272399902
Loss :  1.6555309295654297 1.9823983907699585 11.567523002624512
Loss :  1.6126000881195068 1.9717220067977905 11.471210479736328
Loss :  1.6844046115875244 2.244885206222534 12.908830642700195
Loss :  1.6134350299835205 1.9413856267929077 11.32036304473877
Loss :  1.670607089996338 1.9046037197113037 11.193626403808594
Loss :  1.6502079963684082 2.0502912998199463 11.901664733886719
Loss :  1.6487185955047607 2.1316041946411133 12.306739807128906
Loss :  1.6195591688156128 2.5329275131225586 14.284196853637695
Loss :  1.6318867206573486 2.4223825931549072 13.743799209594727
Loss :  1.6306968927383423 2.2033464908599854 12.647429466247559
Loss :  1.6819493770599365 2.1888437271118164 12.626168251037598
Loss :  1.6857308149337769 2.1108007431030273 12.239734649658203
Loss :  1.6942389011383057 2.0493886470794678 11.941182136535645
  batch 40 loss: 1.6942389011383057, 2.0493886470794678, 11.941182136535645
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6598742008209229 2.5313398838043213 14.316573143005371
Loss :  1.6490501165390015 1.8130295276641846 10.714197158813477
Loss :  1.6393705606460571 2.1295576095581055 12.287158966064453
Loss :  1.6507658958435059 1.8529183864593506 10.91535758972168
Loss :  1.6323758363723755 1.8402457237243652 10.83360481262207
Loss :  1.6580686569213867 2.301973819732666 13.167937278747559
Loss :  1.6855852603912354 2.3552980422973633 13.462075233459473
Loss :  1.6448568105697632 1.957340955734253 11.431561470031738
Loss :  1.6983003616333008 1.985573410987854 11.626167297363281
Loss :  1.6504695415496826 1.9995225667953491 11.648082733154297
Loss :  1.6782876253128052 1.9633328914642334 11.494952201843262
Loss :  1.6729246377944946 1.9041366577148438 11.193608283996582
Loss :  1.655250906944275 2.0605051517486572 11.957776069641113
Loss :  1.6797436475753784 2.0804951190948486 12.082219123840332
Loss :  1.6453149318695068 2.1340107917785645 12.31536865234375
Loss :  1.6977211236953735 2.3265695571899414 13.33056926727295
Loss :  1.6515318155288696 1.9947398900985718 11.625231742858887
Loss :  1.638063669204712 1.7739945650100708 10.508036613464355
Loss :  1.651314616203308 2.218587636947632 12.744253158569336
Loss :  1.7046071290969849 1.6468926668167114 9.939070701599121
  batch 60 loss: 1.7046071290969849, 1.6468926668167114, 9.939070701599121
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6391123533248901 2.0574545860290527 11.926385879516602
Loss :  1.6632513999938965 1.8545013666152954 10.935758590698242
Loss :  1.64678156375885 2.3099989891052246 13.19677734375
Loss :  1.6369690895080566 2.9441070556640625 16.35750389099121
Loss :  1.6203205585479736 1.5918067693710327 9.579354286193848
Loss :  1.6237581968307495 3.6921465396881104 20.084491729736328
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.633326768875122 3.567204713821411 19.469350814819336
Loss :  1.6294386386871338 3.5514237880706787 19.38655662536621
Loss :  1.6410242319107056 3.3791701793670654 18.536874771118164
Total LOSS train 12.126167766864484 valid 19.36931848526001
CE LOSS train 1.658146858215332 valid 0.4102560579776764
Contrastive LOSS train 2.093604168525109 valid 0.8447925448417664
EPOCH 232:
Loss :  1.6749601364135742 1.9166735410690308 11.25832748413086
Loss :  1.6858092546463013 2.1716811656951904 12.544215202331543
Loss :  1.6597354412078857 1.6954476833343506 10.136974334716797
Loss :  1.664901614189148 2.283078193664551 13.080292701721191
Loss :  1.6792645454406738 1.8612022399902344 10.985275268554688
Loss :  1.6495020389556885 2.0027682781219482 11.66334342956543
Loss :  1.6796822547912598 2.243281602859497 12.896089553833008
Loss :  1.6593339443206787 2.217967987060547 12.749174118041992
Loss :  1.6511913537979126 2.3066375255584717 13.184379577636719
Loss :  1.6796152591705322 2.2674667835235596 13.016948699951172
Loss :  1.6438868045806885 2.023711681365967 11.762445449829102
Loss :  1.6436628103256226 2.3136134147644043 13.211730003356934
Loss :  1.6412094831466675 2.209918260574341 12.690800666809082
Loss :  1.6456876993179321 2.068145513534546 11.98641586303711
Loss :  1.6924924850463867 1.8496071100234985 10.94052791595459
Loss :  1.689049243927002 2.4497146606445312 13.9376220703125
Loss :  1.639860987663269 2.6816439628601074 15.048081398010254
Loss :  1.6625401973724365 2.2493250370025635 12.909165382385254
Loss :  1.6368035078048706 1.9791144132614136 11.53237533569336
Loss :  1.6883981227874756 2.493128538131714 14.154041290283203
  batch 20 loss: 1.6883981227874756, 2.493128538131714, 14.154041290283203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6568500995635986 2.3848657608032227 13.581178665161133
Loss :  1.6362196207046509 2.0362191200256348 11.817315101623535
Loss :  1.65103280544281 1.8528498411178589 10.915281295776367
Loss :  1.6642794609069824 2.220215320587158 12.765356063842773
Loss :  1.6888014078140259 2.4093782901763916 13.735692977905273
Loss :  1.653974175453186 1.8006945848464966 10.657447814941406
Loss :  1.6625834703445435 1.7827117443084717 10.576142311096191
Loss :  1.6574640274047852 2.4704267978668213 14.009597778320312
Loss :  1.6142929792404175 2.7490286827087402 15.359436988830566
Loss :  1.68608820438385 2.791320562362671 15.642691612243652
Loss :  1.614987850189209 2.708585262298584 15.157915115356445
Loss :  1.6717275381088257 2.040613889694214 11.874796867370605
Loss :  1.6510448455810547 2.045107364654541 11.876581192016602
Loss :  1.6493935585021973 2.545414686203003 14.376466751098633
Loss :  1.6209722757339478 2.7930173873901367 15.5860595703125
Loss :  1.6334432363510132 2.5889673233032227 14.578279495239258
Loss :  1.6325907707214355 2.64011549949646 14.833168029785156
Loss :  1.684477686882019 2.317455291748047 13.271754264831543
Loss :  1.6886255741119385 2.335209846496582 13.36467456817627
Loss :  1.6971886157989502 2.2757883071899414 13.076129913330078
  batch 40 loss: 1.6971886157989502, 2.2757883071899414, 13.076129913330078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6631542444229126 2.1748368740081787 12.537339210510254
Loss :  1.6526683568954468 1.7892518043518066 10.598926544189453
Loss :  1.642735242843628 2.199518918991089 12.64033031463623
Loss :  1.6539762020111084 2.193434476852417 12.621149063110352
Loss :  1.6356886625289917 1.5898722410202026 9.585049629211426
Loss :  1.660839557647705 2.3748345375061035 13.535011291503906
Loss :  1.6883069276809692 2.173017740249634 12.55339527130127
Loss :  1.6476877927780151 2.4449522495269775 13.872448921203613
Loss :  1.7003034353256226 2.473243474960327 14.066520690917969
Loss :  1.6528452634811401 1.9595917463302612 11.450803756713867
Loss :  1.6802629232406616 2.142845392227173 12.394489288330078
Loss :  1.6751370429992676 2.6040148735046387 14.695211410522461
Loss :  1.6569068431854248 2.6578032970428467 14.945923805236816
Loss :  1.6812117099761963 2.6824886798858643 15.09365463256836
Loss :  1.6470563411712646 2.5103914737701416 14.199013710021973
Loss :  1.698851466178894 2.5803232192993164 14.600467681884766
Loss :  1.6529312133789062 2.7019951343536377 15.162906646728516
Loss :  1.6396232843399048 2.529470920562744 14.286977767944336
Loss :  1.6525381803512573 3.3046133518218994 18.17560577392578
Loss :  1.706044316291809 2.7743921279907227 15.578004837036133
  batch 60 loss: 1.706044316291809, 2.7743921279907227, 15.578004837036133
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6403825283050537 2.475519895553589 14.017982482910156
Loss :  1.6638693809509277 1.8017292022705078 10.672515869140625
Loss :  1.6470146179199219 1.7529866695404053 10.411948204040527
Loss :  1.6363111734390259 2.9622886180877686 16.44775390625
Loss :  1.6196494102478027 1.9833747148513794 11.536523818969727
Loss :  1.6443711519241333 4.232729434967041 22.80801773071289
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.6539244651794434 4.288037300109863 23.0941104888916
Loss :  1.6510826349258423 4.310338020324707 23.20277214050293
Loss :  1.660500407218933 4.101548671722412 22.168243408203125
Total LOSS train 13.11421764080341 valid 22.818285942077637
CE LOSS train 1.6596864847036508 valid 0.4151251018047333
Contrastive LOSS train 2.290906227551974 valid 1.025387167930603
EPOCH 233:
Loss :  1.6749268770217896 1.9431699514389038 11.390776634216309
Loss :  1.6859337091445923 3.0853335857391357 17.11260223388672
Loss :  1.6599572896957397 1.5211031436920166 9.265472412109375
Loss :  1.6652781963348389 2.529287099838257 14.311714172363281
Loss :  1.6796989440917969 1.7656577825546265 10.507987976074219
Loss :  1.6496148109436035 2.0051825046539307 11.675527572631836
Loss :  1.6802268028259277 2.9091427326202393 16.225940704345703
Loss :  1.6596802473068237 2.1465139389038086 12.392250061035156
Loss :  1.651366114616394 2.299900531768799 13.15086841583252
Loss :  1.679918885231018 1.6123058795928955 9.741448402404785
Loss :  1.6435331106185913 1.9933949708938599 11.61050796508789
Loss :  1.6431766748428345 2.123751401901245 12.261934280395508
Loss :  1.6405662298202515 1.8426769971847534 10.853950500488281
Loss :  1.644476294517517 3.003058433532715 16.65976905822754
Loss :  1.6914862394332886 3.574204206466675 19.56250762939453
Loss :  1.6879528760910034 2.911041498184204 16.243160247802734
Loss :  1.6387478113174438 2.508054256439209 14.179019927978516
Loss :  1.661912202835083 1.9228309392929077 11.276066780090332
Loss :  1.6362155675888062 1.8483036756515503 10.87773323059082
Loss :  1.689011573791504 1.8725554943084717 11.051789283752441
  batch 20 loss: 1.689011573791504, 1.8725554943084717, 11.051789283752441
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6575785875320435 1.7815717458724976 10.565437316894531
Loss :  1.6365879774093628 2.154287576675415 12.408025741577148
Loss :  1.6517996788024902 2.738368511199951 15.343643188476562
Loss :  1.6642673015594482 2.650688886642456 14.917712211608887
Loss :  1.6886056661605835 2.8979341983795166 16.17827606201172
Loss :  1.653232455253601 2.1710052490234375 12.508258819580078
Loss :  1.6613454818725586 1.930322527885437 11.312957763671875
Loss :  1.6559278964996338 1.8099086284637451 10.70547103881836
Loss :  1.6117361783981323 1.787707805633545 10.550275802612305
Loss :  1.6839617490768433 4.045292854309082 21.910425186157227
Loss :  1.6119524240493774 2.4770736694335938 13.997321128845215
Loss :  1.6698600053787231 2.5581791400909424 14.460755348205566
Loss :  1.6484451293945312 2.215975046157837 12.728320121765137
Loss :  1.646701693534851 2.327409505844116 13.2837495803833
Loss :  1.6178051233291626 2.3790462017059326 13.513036727905273
Loss :  1.6302769184112549 2.4421613216400146 13.841083526611328
Loss :  1.62924063205719 2.2507646083831787 12.883064270019531
Loss :  1.6813493967056274 1.78837251663208 10.623211860656738
Loss :  1.6856098175048828 2.3628897666931152 13.500059127807617
Loss :  1.6943825483322144 2.7653005123138428 15.520885467529297
  batch 40 loss: 1.6943825483322144, 2.7653005123138428, 15.520885467529297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6601730585098267 2.63334321975708 14.826889038085938
Loss :  1.6495109796524048 2.7549757957458496 15.42439079284668
Loss :  1.6404272317886353 3.018836259841919 16.734607696533203
Loss :  1.6516447067260742 1.7339622974395752 10.321455955505371
Loss :  1.6333256959915161 1.9720596075057983 11.493623733520508
Loss :  1.6591410636901855 2.051443099975586 11.916357040405273
Loss :  1.6869325637817383 2.416501522064209 13.769440650939941
Loss :  1.645890474319458 1.9501556158065796 11.396668434143066
Loss :  1.6993368864059448 2.5018627643585205 14.208650588989258
Loss :  1.651636004447937 2.446575403213501 13.884512901306152
Loss :  1.679448127746582 2.1228630542755127 12.293763160705566
Loss :  1.6746666431427002 1.7871543169021606 10.610438346862793
Loss :  1.6570100784301758 1.713201642036438 10.223018646240234
Loss :  1.6811763048171997 1.8174971342086792 10.768661499023438
Loss :  1.6467177867889404 2.7920448780059814 15.606942176818848
Loss :  1.6981992721557617 2.6450610160827637 14.923504829406738
Loss :  1.6522421836853027 2.8105998039245605 15.705242156982422
Loss :  1.6385736465454102 2.5535261631011963 14.406204223632812
Loss :  1.6516386270523071 2.257870674133301 12.94099235534668
Loss :  1.7044888734817505 4.387876033782959 23.643869400024414
  batch 60 loss: 1.7044888734817505, 4.387876033782959, 23.643869400024414
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6388088464736938 1.876415729522705 11.02088737487793
Loss :  1.6626598834991455 1.7627931833267212 10.476625442504883
Loss :  1.6460154056549072 1.6734777688980103 10.01340389251709
Loss :  1.6363826990127563 2.322542428970337 13.24909496307373
Loss :  1.6197113990783691 1.614833950996399 9.69388198852539
Loss :  1.642127513885498 4.036763668060303 21.825946807861328
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.6496220827102661 4.098470687866211 22.14197540283203
Loss :  1.6486958265304565 4.179805755615234 22.5477237701416
Loss :  1.6542813777923584 3.6549293994903564 19.92892837524414
Total LOSS train 13.24086346259484 valid 21.611143589019775
CE LOSS train 1.6586170086493859 valid 0.4135703444480896
Contrastive LOSS train 2.316449271715604 valid 0.9137323498725891
EPOCH 234:
Loss :  1.6747812032699585 2.999687433242798 16.6732177734375
Loss :  1.6859591007232666 2.804081916809082 15.706368446350098
Loss :  1.6606923341751099 3.0597341060638428 16.95936393737793
Loss :  1.665726661682129 2.371457576751709 13.523015022277832
Loss :  1.6807470321655273 2.051600217819214 11.938748359680176
Loss :  1.6505144834518433 2.1353237628936768 12.327133178710938
Loss :  1.6807715892791748 2.512152910232544 14.241536140441895
Loss :  1.6603848934173584 2.5801117420196533 14.560943603515625
Loss :  1.652414321899414 3.3389198780059814 18.347013473510742
Loss :  1.6811773777008057 2.6493735313415527 14.928045272827148
Loss :  1.6445083618164062 2.7008605003356934 15.148811340332031
Loss :  1.6440117359161377 2.165001392364502 12.46901798248291
Loss :  1.641172170639038 2.200511932373047 12.643732070922852
Loss :  1.644943356513977 2.2540876865386963 12.91538143157959
Loss :  1.6922529935836792 2.1441519260406494 12.413012504577637
Loss :  1.68801748752594 2.1990292072296143 12.6831636428833
Loss :  1.6378368139266968 2.7720069885253906 15.497871398925781
Loss :  1.6604957580566406 2.680248498916626 15.061738014221191
Loss :  1.6347651481628418 2.510761260986328 14.18857192993164
Loss :  1.6876416206359863 1.8092458248138428 10.733871459960938
  batch 20 loss: 1.6876416206359863, 1.8092458248138428, 10.733871459960938
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6551198959350586 2.098493814468384 12.147588729858398
Loss :  1.633802056312561 2.2395520210266113 12.831562042236328
Loss :  1.6491503715515137 2.078773021697998 12.04301643371582
Loss :  1.6619452238082886 1.6964938640594482 10.144414901733398
Loss :  1.687084436416626 2.5949182510375977 14.661675453186035
Loss :  1.6515285968780518 2.2894673347473145 13.098864555358887
Loss :  1.6604204177856445 1.8806655406951904 11.063748359680176
Loss :  1.6555525064468384 2.3806991577148438 13.559048652648926
Loss :  1.6122446060180664 2.4480087757110596 13.852288246154785
Loss :  1.685180902481079 2.5399301052093506 14.384831428527832
Loss :  1.6138086318969727 2.687962532043457 15.053621292114258
Loss :  1.6713865995407104 2.7121872901916504 15.232322692871094
Loss :  1.6509124040603638 2.773902416229248 15.520424842834473
Loss :  1.6492105722427368 1.9842389822006226 11.570405960083008
Loss :  1.6201153993606567 2.8337783813476562 15.789007186889648
Loss :  1.632460117340088 2.452589511871338 13.895406723022461
Loss :  1.6314865350723267 2.1705281734466553 12.48412799835205
Loss :  1.6831129789352417 1.7195477485656738 10.280851364135742
Loss :  1.6868622303009033 2.1235315799713135 12.304519653320312
Loss :  1.695451021194458 1.8118854761123657 10.754878044128418
  batch 40 loss: 1.695451021194458, 1.8118854761123657, 10.754878044128418
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6610474586486816 2.66302752494812 14.976184844970703
Loss :  1.6502503156661987 2.5387558937072754 14.344029426574707
Loss :  1.6411436796188354 2.084463357925415 12.063460350036621
Loss :  1.6522754430770874 1.99620521068573 11.63330078125
Loss :  1.6340399980545044 1.7462879419326782 10.365479469299316
Loss :  1.659798264503479 2.671342611312866 15.016511917114258
Loss :  1.6874046325683594 2.977569341659546 16.575252532958984
Loss :  1.6464941501617432 2.2251386642456055 12.772187232971191
Loss :  1.6986925601959229 1.9787825345993042 11.592604637145996
Loss :  1.6513704061508179 2.17366361618042 12.519688606262207
Loss :  1.6791082620620728 2.3508012294769287 13.433115005493164
Loss :  1.6738477945327759 2.51448655128479 14.246280670166016
Loss :  1.6561524868011475 2.358790874481201 13.45010757446289
Loss :  1.680639386177063 2.542348623275757 14.392382621765137
Loss :  1.6466425657272339 2.2328712940216064 12.810998916625977
Loss :  1.6989262104034424 2.044642686843872 11.922139167785645
Loss :  1.652437686920166 2.5110368728637695 14.207622528076172
Loss :  1.6390022039413452 2.2796883583068848 13.037443161010742
Loss :  1.6520248651504517 2.0664174556732178 11.984112739562988
Loss :  1.7048041820526123 1.622856616973877 9.819087028503418
  batch 60 loss: 1.7048041820526123, 1.622856616973877, 9.819087028503418
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.639399528503418 2.681187868118286 15.04533863067627
Loss :  1.6636039018630981 2.605121374130249 14.689210891723633
Loss :  1.6471027135849 2.152362823486328 12.408916473388672
Loss :  1.6371467113494873 2.6269500255584717 14.771897315979004
Loss :  1.6202516555786133 1.989606261253357 11.568283081054688
Loss :  1.6564693450927734 4.357917308807373 23.446056365966797
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6658053398132324 4.496552467346191 24.14856719970703
Loss :  1.6621201038360596 4.224935054779053 22.78679656982422
Loss :  1.6750198602676392 4.372049808502197 23.535268783569336
Total LOSS train 13.43505841768705 valid 23.479172229766846
CE LOSS train 1.6589116463294395 valid 0.4187549650669098
Contrastive LOSS train 2.3552293520707352 valid 1.0930124521255493
EPOCH 235:
Loss :  1.6755379438400269 2.4133474826812744 13.74227523803711
Loss :  1.6866751909255981 2.5256640911102295 14.314995765686035
Loss :  1.6604571342468262 1.8470269441604614 10.895591735839844
Loss :  1.6659351587295532 2.078030824661255 12.056089401245117
Loss :  1.6804834604263306 2.442671775817871 13.893842697143555
Loss :  1.6509153842926025 2.393998861312866 13.620909690856934
Loss :  1.6811820268630981 2.0708858966827393 12.035611152648926
Loss :  1.6607506275177002 1.7566771507263184 10.444136619567871
Loss :  1.6528469324111938 2.0036704540252686 11.671199798583984
Loss :  1.6813457012176514 2.379936933517456 13.58103084564209
Loss :  1.6456574201583862 1.8752070665359497 11.021693229675293
Loss :  1.6453101634979248 1.9548718929290771 11.419670104980469
Loss :  1.6429564952850342 4.181384563446045 22.54987907409668
Loss :  1.647288203239441 2.7509610652923584 15.402093887329102
Loss :  1.693904995918274 2.0143473148345947 11.765641212463379
Loss :  1.6890181303024292 2.3312735557556152 13.345386505126953
Loss :  1.639742136001587 2.315720558166504 13.218344688415527
Loss :  1.6621843576431274 2.6472971439361572 14.898670196533203
Loss :  1.6357953548431396 2.2426209449768066 12.848899841308594
Loss :  1.687894344329834 3.085716962814331 17.116479873657227
  batch 20 loss: 1.687894344329834, 3.085716962814331, 17.116479873657227
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6551824808120728 2.221278667449951 12.761576652526855
Loss :  1.6339085102081299 2.8880527019500732 16.074172973632812
Loss :  1.649143934249878 2.802684783935547 15.662568092346191
Loss :  1.6621685028076172 2.541008472442627 14.367210388183594
Loss :  1.6871908903121948 2.6816134452819824 15.095258712768555
Loss :  1.651484489440918 2.017850160598755 11.740735054016113
Loss :  1.6603178977966309 2.4357919692993164 13.839277267456055
Loss :  1.6554901599884033 2.2877817153930664 13.094398498535156
Loss :  1.6121437549591064 3.0349783897399902 16.787036895751953
Loss :  1.6853586435317993 2.701476573944092 15.192742347717285
Loss :  1.6140674352645874 2.448507785797119 13.856605529785156
Loss :  1.6716598272323608 2.46195125579834 13.981415748596191
Loss :  1.6514650583267212 1.9377983808517456 11.34045696258545
Loss :  1.6498075723648071 3.167078733444214 17.485200881958008
Loss :  1.6207945346832275 2.546553373336792 14.353561401367188
Loss :  1.632880687713623 1.9760442972183228 11.513101577758789
Loss :  1.631421685218811 1.9465506076812744 11.364174842834473
Loss :  1.6825920343399048 2.333822011947632 13.351702690124512
Loss :  1.6861546039581299 2.226714611053467 12.819727897644043
Loss :  1.694893717765808 2.242945909500122 12.909623146057129
  batch 40 loss: 1.694893717765808, 2.242945909500122, 12.909623146057129
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6599324941635132 2.2329049110412598 12.824456214904785
Loss :  1.6490983963012695 1.6497844457626343 9.89802074432373
Loss :  1.6394932270050049 2.4192097187042236 13.735541343688965
Loss :  1.651132345199585 1.955884575843811 11.43055534362793
Loss :  1.632591724395752 2.3665919303894043 13.465551376342773
Loss :  1.65834641456604 1.9289538860321045 11.303115844726562
Loss :  1.6860358715057373 1.79873788356781 10.679725646972656
Loss :  1.645646095275879 3.6808855533599854 20.05007553100586
Loss :  1.6987402439117432 1.7692354917526245 10.544917106628418
Loss :  1.6513978242874146 1.767601728439331 10.48940658569336
Loss :  1.6795549392700195 2.349250555038452 13.42580795288086
Loss :  1.674581527709961 2.5069470405578613 14.20931625366211
Loss :  1.6569814682006836 2.4485857486724854 13.899909973144531
Loss :  1.6816703081130981 2.1619362831115723 12.491351127624512
Loss :  1.6471608877182007 1.9589954614639282 11.442137718200684
Loss :  1.6991759538650513 1.7806459665298462 10.602405548095703
Loss :  1.6526241302490234 2.1075408458709717 12.190328598022461
Loss :  1.6389135122299194 1.8198332786560059 10.738080024719238
Loss :  1.6521399021148682 3.6415328979492188 19.859804153442383
Loss :  1.704900860786438 2.170362710952759 12.556714057922363
  batch 60 loss: 1.704900860786438, 2.170362710952759, 12.556714057922363
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6396052837371826 2.298043966293335 13.129825592041016
Loss :  1.6639021635055542 2.2190630435943604 12.759217262268066
Loss :  1.6472686529159546 2.179544687271118 12.544991493225098
Loss :  1.6372625827789307 2.5005781650543213 14.140152931213379
Loss :  1.620566487312317 1.5334278345108032 9.287705421447754
Loss :  1.637637972831726 3.6057019233703613 19.666147232055664
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.6475098133087158 3.570518732070923 19.500104904174805
Loss :  1.6448289155960083 3.411266803741455 18.701162338256836
Loss :  1.6518449783325195 3.1931958198547363 17.61782455444336
Total LOSS train 13.309724661020132 valid 18.871309757232666
CE LOSS train 1.6591189366120558 valid 0.4129612445831299
Contrastive LOSS train 2.330121137545659 valid 0.7982989549636841
EPOCH 236:
Loss :  1.6754292249679565 2.2202422618865967 12.776640892028809
Loss :  1.6864715814590454 1.975062608718872 11.561784744262695
Loss :  1.660309076309204 1.5661671161651611 9.491144180297852
Loss :  1.6657060384750366 2.1168177127838135 12.249794006347656
Loss :  1.6800720691680908 1.9974994659423828 11.667569160461426
Loss :  1.650152564048767 2.963221311569214 16.466259002685547
Loss :  1.680292010307312 2.2383077144622803 12.871830940246582
Loss :  1.6597257852554321 1.766850233078003 10.493977546691895
Loss :  1.6514688730239868 1.7861446142196655 10.582192420959473
Loss :  1.6799653768539429 2.2475638389587402 12.917784690856934
Loss :  1.6440391540527344 2.925220251083374 16.2701416015625
Loss :  1.6437679529190063 2.476541757583618 14.026476860046387
Loss :  1.6414042711257935 2.4393539428710938 13.838173866271973
Loss :  1.6454248428344727 2.43102765083313 13.800562858581543
Loss :  1.6923125982284546 2.429347038269043 13.8390474319458
Loss :  1.6884065866470337 2.4117236137390137 13.747025489807129
Loss :  1.6387107372283936 2.6562530994415283 14.919976234436035
Loss :  1.6615962982177734 2.347581148147583 13.39950180053711
Loss :  1.635873794555664 2.565753698348999 14.464642524719238
Loss :  1.6881635189056396 2.632341146469116 14.849869728088379
  batch 20 loss: 1.6881635189056396, 2.632341146469116, 14.849869728088379
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6564017534255981 2.3017125129699707 13.164963722229004
Loss :  1.6355289220809937 2.6158647537231445 14.714852333068848
Loss :  1.6505850553512573 1.7485262155532837 10.393216133117676
Loss :  1.6633955240249634 1.9776359796524048 11.551575660705566
Loss :  1.687997579574585 2.0569846630096436 11.972921371459961
Loss :  1.653038501739502 2.332623243331909 13.316154479980469
Loss :  1.6617438793182373 2.1542091369628906 12.43278980255127
Loss :  1.6568535566329956 3.1393179893493652 17.353443145751953
Loss :  1.6138331890106201 3.996992826461792 21.598796844482422
Loss :  1.68561589717865 2.601773500442505 14.694482803344727
Loss :  1.615298867225647 2.5229268074035645 14.22993278503418
Loss :  1.672231674194336 2.1101672649383545 12.223068237304688
Loss :  1.6516087055206299 1.962078332901001 11.461999893188477
Loss :  1.6502189636230469 2.237116813659668 12.835803031921387
Loss :  1.621585726737976 2.703199625015259 15.13758373260498
Loss :  1.6335171461105347 2.504605293273926 14.156543731689453
Loss :  1.632401704788208 2.4612085819244385 13.938444137573242
Loss :  1.68357515335083 2.49116587638855 14.139404296875
Loss :  1.687251329421997 2.201228618621826 12.693394660949707
Loss :  1.6955347061157227 1.9165750741958618 11.278409957885742
  batch 40 loss: 1.6955347061157227, 1.9165750741958618, 11.278409957885742
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6614830493927002 1.9612812995910645 11.467888832092285
Loss :  1.65083909034729 1.7427676916122437 10.364677429199219
Loss :  1.642141342163086 3.2310001850128174 17.797142028808594
Loss :  1.6527677774429321 2.5426220893859863 14.365878105163574
Loss :  1.634507656097412 1.6236923933029175 9.752969741821289
Loss :  1.6597955226898193 2.251296043395996 12.916275978088379
Loss :  1.687292456626892 2.382925510406494 13.601919174194336
Loss :  1.646043300628662 2.9510574340820312 16.401330947875977
Loss :  1.6979392766952515 2.5082790851593018 14.239334106445312
Loss :  1.6508373022079468 2.601006031036377 14.655866622924805
Loss :  1.6788243055343628 2.6680235862731934 15.018942832946777
Loss :  1.6737829446792603 2.794232130050659 15.644944190979004
Loss :  1.6560688018798828 2.6202352046966553 14.757245063781738
Loss :  1.6802843809127808 2.0926594734191895 12.14358139038086
Loss :  1.647159218788147 1.962857961654663 11.46144962310791
Loss :  1.6980478763580322 2.214820146560669 12.772148132324219
Loss :  1.6523184776306152 2.374847888946533 13.526557922363281
Loss :  1.6390100717544556 2.198364019393921 12.630830764770508
Loss :  1.6519207954406738 2.569490909576416 14.499374389648438
Loss :  1.7036372423171997 2.340178966522217 13.404532432556152
  batch 60 loss: 1.7036372423171997, 2.340178966522217, 13.404532432556152
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6387335062026978 2.450977325439453 13.893620491027832
Loss :  1.6635574102401733 2.1305317878723145 12.316215515136719
Loss :  1.6466271877288818 2.0742132663726807 12.017693519592285
Loss :  1.6378886699676514 2.8957817554473877 16.116796493530273
Loss :  1.6215674877166748 1.91652512550354 11.204193115234375
Loss :  1.6519718170166016 4.333243370056152 23.31818962097168
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.661034107208252 4.311052322387695 23.21629524230957
Loss :  1.6617441177368164 4.239920139312744 22.861343383789062
Loss :  1.660021424293518 4.218317031860352 22.751605987548828
Total LOSS train 13.453747470562275 valid 23.036858558654785
CE LOSS train 1.6592397744839007 valid 0.4150053560733795
Contrastive LOSS train 2.35890154838562 valid 1.054579257965088
EPOCH 237:
Loss :  1.6752676963806152 3.716531991958618 20.2579288482666
Loss :  1.6866892576217651 2.922187089920044 16.297624588012695
Loss :  1.6611260175704956 1.7121243476867676 10.221747398376465
Loss :  1.6660611629486084 1.7656232118606567 10.49417781829834
Loss :  1.680134654045105 1.6115572452545166 9.737920761108398
Loss :  1.6496638059616089 1.9926553964614868 11.612940788269043
Loss :  1.6796576976776123 2.0135552883148193 11.747434616088867
Loss :  1.6589053869247437 1.6827143430709839 10.072476387023926
Loss :  1.6508197784423828 1.8378090858459473 10.839864730834961
Loss :  1.67876136302948 2.1494295597076416 12.425909042358398
Loss :  1.6425203084945679 3.49543833732605 19.119712829589844
Loss :  1.642223596572876 2.291729211807251 13.100869178771973
Loss :  1.6398018598556519 2.1861801147460938 12.57070255279541
Loss :  1.6438770294189453 2.7352511882781982 15.320133209228516
Loss :  1.6909846067428589 2.648484706878662 14.933408737182617
Loss :  1.6864783763885498 2.5730693340301514 14.551825523376465
Loss :  1.6374980211257935 2.4298789501190186 13.786892890930176
Loss :  1.6595525741577148 2.295362949371338 13.136366844177246
Loss :  1.634743332862854 1.7142292261123657 10.205889701843262
Loss :  1.6870896816253662 2.6608352661132812 14.991266250610352
  batch 20 loss: 1.6870896816253662, 2.6608352661132812, 14.991266250610352
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6551896333694458 1.7476476430892944 10.393427848815918
Loss :  1.6341410875320435 2.0717875957489014 11.99307918548584
Loss :  1.6493017673492432 1.902931571006775 11.163959503173828
Loss :  1.6624656915664673 1.9447576999664307 11.38625431060791
Loss :  1.6869330406188965 2.0410850048065186 11.892358779907227
Loss :  1.6519161462783813 2.0341622829437256 11.822728157043457
Loss :  1.660969614982605 1.9929184913635254 11.625561714172363
Loss :  1.6565372943878174 1.9136303663253784 11.224688529968262
Loss :  1.6138921976089478 2.136476755142212 12.296276092529297
Loss :  1.6858047246932983 2.5490376949310303 14.43099308013916
Loss :  1.6153777837753296 2.5635409355163574 14.433082580566406
Loss :  1.6720653772354126 2.461982488632202 13.981978416442871
Loss :  1.6516419649124146 2.353654146194458 13.419912338256836
Loss :  1.6499607563018799 2.3103373050689697 13.20164680480957
Loss :  1.6214673519134521 2.2245893478393555 12.744414329528809
Loss :  1.6333597898483276 2.1189215183258057 12.227967262268066
Loss :  1.6321364641189575 2.710326671600342 15.183770179748535
Loss :  1.682620882987976 2.477266550064087 14.068953514099121
Loss :  1.6865699291229248 2.2736353874206543 13.054747581481934
Loss :  1.6945401430130005 1.7356908321380615 10.372994422912598
  batch 40 loss: 1.6945401430130005, 1.7356908321380615, 10.372994422912598
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.659897804260254 2.4497151374816895 13.908473014831543
Loss :  1.6490014791488647 2.45827579498291 13.940380096435547
Loss :  1.6397826671600342 2.8402440547943115 15.84100341796875
Loss :  1.6509114503860474 2.624905586242676 14.775439262390137
Loss :  1.6323660612106323 2.7347400188446045 15.306066513061523
Loss :  1.6579883098602295 1.9945900440216064 11.630938529968262
Loss :  1.685636281967163 1.8235341310501099 10.80330753326416
Loss :  1.6452009677886963 2.0064148902893066 11.677274703979492
Loss :  1.6979485750198364 2.190545082092285 12.650673866271973
Loss :  1.650503158569336 2.480224847793579 14.051627159118652
Loss :  1.6782506704330444 2.0268123149871826 11.812312126159668
Loss :  1.673172950744629 2.083528518676758 12.090815544128418
Loss :  1.6557151079177856 1.9387024641036987 11.349226951599121
Loss :  1.6805158853530884 2.599280834197998 14.676920890808105
Loss :  1.6469048261642456 2.5791733264923096 14.542771339416504
Loss :  1.6988461017608643 1.709637999534607 10.24703598022461
Loss :  1.652984380722046 2.0422263145446777 11.864116668701172
Loss :  1.6393842697143555 1.9534969329833984 11.406868934631348
Loss :  1.6522284746170044 2.6798219680786133 15.051338195800781
Loss :  1.7052510976791382 2.1901047229766846 12.655774116516113
  batch 60 loss: 1.7052510976791382, 2.1901047229766846, 12.655774116516113
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.639528751373291 2.124177932739258 12.260417938232422
Loss :  1.663375735282898 1.9032893180847168 11.17982292175293
Loss :  1.646583914756775 1.9777723550796509 11.535445213317871
Loss :  1.636827826499939 2.5277600288391113 14.275627136230469
Loss :  1.6199216842651367 1.6022629737854004 9.63123607635498
Loss :  1.652161717414856 4.1164870262146 22.23459815979004
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.6615239381790161 4.157857894897461 22.45081329345703
Loss :  1.6581891775131226 4.057494163513184 21.945661544799805
Loss :  1.669065237045288 4.108026027679443 22.209196090698242
Total LOSS train 12.85398156092717 valid 22.21006727218628
CE LOSS train 1.6585765581864578 valid 0.417266309261322
Contrastive LOSS train 2.239080995779771 valid 1.0270065069198608
EPOCH 238:
Loss :  1.6747856140136719 2.1318774223327637 12.334173202514648
Loss :  1.6861560344696045 2.1792407035827637 12.58236026763916
Loss :  1.6596368551254272 2.330850124359131 13.313887596130371
Loss :  1.6646355390548706 2.219951868057251 12.764394760131836
Loss :  1.6792495250701904 1.829444169998169 10.826470375061035
Loss :  1.6486753225326538 2.1863644123077393 12.580496788024902
Loss :  1.6789106130599976 2.8463711738586426 15.910765647888184
Loss :  1.6578336954116821 3.0100016593933105 16.707841873168945
Loss :  1.6501179933547974 1.8005954027175903 10.653095245361328
Loss :  1.679166555404663 1.5872074365615845 9.615203857421875
Loss :  1.6426221132278442 1.8816983699798584 11.051114082336426
Loss :  1.642254114151001 2.634068489074707 14.812596321105957
Loss :  1.6400123834609985 2.3094606399536133 13.187315940856934
Loss :  1.6440812349319458 3.539370536804199 19.34093475341797
Loss :  1.6914408206939697 1.857656478881836 10.97972297668457
Loss :  1.6867072582244873 1.95047926902771 11.439104080200195
Loss :  1.6377966403961182 2.410830497741699 13.691948890686035
Loss :  1.6596004962921143 1.978471279144287 11.551957130432129
Loss :  1.634499430656433 1.7938079833984375 10.60353946685791
Loss :  1.6864407062530518 1.916711688041687 11.269998550415039
  batch 20 loss: 1.6864407062530518, 1.916711688041687, 11.269998550415039
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.654464602470398 1.8475373983383179 10.892151832580566
Loss :  1.633131504058838 2.9872376918792725 16.569320678710938
Loss :  1.648349404335022 2.5223896503448486 14.260297775268555
Loss :  1.6616427898406982 3.1960721015930176 17.64200210571289
Loss :  1.6869776248931885 1.8661518096923828 11.017736434936523
Loss :  1.6512514352798462 2.3637876510620117 13.470190048217773
Loss :  1.6603505611419678 2.5321950912475586 14.32132625579834
Loss :  1.655179500579834 2.734668016433716 15.328519821166992
Loss :  1.6120954751968384 2.8744544982910156 15.984368324279785
Loss :  1.6846143007278442 2.797621965408325 15.672723770141602
Loss :  1.6137369871139526 2.456155300140381 13.894514083862305
Loss :  1.671261191368103 2.0827207565307617 12.084864616394043
Loss :  1.6509166955947876 2.07114315032959 12.006632804870605
Loss :  1.6495338678359985 2.4264209270477295 13.781639099121094
Loss :  1.620221495628357 2.643822431564331 14.839333534240723
Loss :  1.6322636604309082 2.01391339302063 11.70182991027832
Loss :  1.6307286024093628 1.797645926475525 10.618958473205566
Loss :  1.682369351387024 2.552255392074585 14.443646430969238
Loss :  1.6861203908920288 2.799009084701538 15.68116569519043
Loss :  1.6944390535354614 3.170701026916504 17.547945022583008
  batch 40 loss: 1.6944390535354614, 3.170701026916504, 17.547945022583008
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.659928560256958 2.563178062438965 14.475818634033203
Loss :  1.6493372917175293 2.357719659805298 13.437934875488281
Loss :  1.640151023864746 2.5703370571136475 14.491836547851562
Loss :  1.6519818305969238 2.6961183547973633 15.132574081420898
Loss :  1.6339834928512573 2.5192854404449463 14.2304105758667
Loss :  1.6596424579620361 2.670724391937256 15.013264656066895
Loss :  1.6875029802322388 2.5277788639068604 14.326396942138672
Loss :  1.6468334197998047 2.6585581302642822 14.939623832702637
Loss :  1.6997917890548706 2.468611001968384 14.0428466796875
Loss :  1.6517244577407837 1.8148362636566162 10.725906372070312
Loss :  1.6789476871490479 2.529862403869629 14.328259468078613
Loss :  1.6741727590560913 2.230692148208618 12.827632904052734
Loss :  1.6568846702575684 1.9637882709503174 11.475826263427734
Loss :  1.68143630027771 2.2076754570007324 12.71981430053711
Loss :  1.6469260454177856 2.726914882659912 15.281500816345215
Loss :  1.6997840404510498 2.413679838180542 13.768183708190918
Loss :  1.653576135635376 2.4850893020629883 14.079022407531738
Loss :  1.64016592502594 2.4136500358581543 13.708416938781738
Loss :  1.653260350227356 2.8995461463928223 16.150991439819336
Loss :  1.7064725160598755 2.3462703227996826 13.437824249267578
  batch 60 loss: 1.7064725160598755, 2.3462703227996826, 13.437824249267578
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6411433219909668 2.469794273376465 13.990114212036133
Loss :  1.6645097732543945 2.8291566371917725 15.810293197631836
Loss :  1.6483138799667358 2.550933837890625 14.402982711791992
Loss :  1.6381803750991821 2.671586036682129 14.996110916137695
Loss :  1.6217893362045288 1.6283535957336426 9.763556480407715
Loss :  1.6406463384628296 3.9083688259124756 21.1824893951416
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.6507853269577026 3.9177091121673584 21.239330291748047
Loss :  1.6470867395401 3.754836320877075 20.421268463134766
Loss :  1.6569545269012451 3.7144129276275635 20.229019165039062
Total LOSS train 13.608203565157377 valid 20.76802682876587
CE LOSS train 1.658626336317796 valid 0.4142386317253113
Contrastive LOSS train 2.3899154351307796 valid 0.9286032319068909
EPOCH 239:
Loss :  1.67622709274292 2.1226980686187744 12.289716720581055
Loss :  1.687186360359192 2.611814022064209 14.746256828308105
Loss :  1.662060260772705 3.3313817977905273 18.3189697265625
Loss :  1.667651891708374 2.5967166423797607 14.651235580444336
Loss :  1.6818538904190063 2.284907341003418 13.106390953063965
Loss :  1.6524686813354492 2.2885234355926514 13.095086097717285
Loss :  1.6823458671569824 2.7457704544067383 15.411197662353516
Loss :  1.6619369983673096 1.9125274419784546 11.224574089050293
Loss :  1.653254747390747 1.9257994890213013 11.282252311706543
Loss :  1.6815736293792725 1.6220749616622925 9.791948318481445
Loss :  1.6456836462020874 2.0672175884246826 11.981771469116211
Loss :  1.6450225114822388 2.0386228561401367 11.838136672973633
Loss :  1.6420516967773438 3.268214702606201 17.983125686645508
Loss :  1.6460648775100708 2.1501259803771973 12.39669418334961
Loss :  1.6924247741699219 2.2094902992248535 12.739875793457031
Loss :  1.6884801387786865 2.2251062393188477 12.814011573791504
Loss :  1.6390129327774048 2.4179842472076416 13.728934288024902
Loss :  1.6614152193069458 2.141258716583252 12.367708206176758
Loss :  1.6357192993164062 2.131007671356201 12.29075813293457
Loss :  1.6868878602981567 2.8285489082336426 15.829631805419922
  batch 20 loss: 1.6868878602981567, 2.8285489082336426, 15.829631805419922
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6560790538787842 2.295384407043457 13.133001327514648
Loss :  1.6352113485336304 3.8042025566101074 20.656225204467773
Loss :  1.6501051187515259 2.029595136642456 11.798081398010254
Loss :  1.663283348083496 2.0622689723968506 11.974628448486328
Loss :  1.6883597373962402 2.363198757171631 13.504354476928711
Loss :  1.6533018350601196 2.085315704345703 12.079880714416504
Loss :  1.6616469621658325 3.3729333877563477 18.52631378173828
Loss :  1.6572023630142212 2.633228302001953 14.823344230651855
Loss :  1.6146317720413208 3.1610772609710693 17.420019149780273
Loss :  1.6855703592300415 3.314709186553955 18.25911521911621
Loss :  1.6153311729431152 2.388042449951172 13.555543899536133
Loss :  1.6713972091674805 2.7253692150115967 15.298243522644043
Loss :  1.6515356302261353 2.364536762237549 13.47421932220459
Loss :  1.6497464179992676 2.4719860553741455 14.009675979614258
Loss :  1.6208372116088867 3.309424638748169 18.16796112060547
Loss :  1.6329625844955444 2.6360580921173096 14.813252449035645
Loss :  1.6320489645004272 2.9040980339050293 16.15254020690918
Loss :  1.682291030883789 2.73047137260437 15.334648132324219
Loss :  1.6875487565994263 2.4130899906158447 13.752998352050781
Loss :  1.6953836679458618 2.7135093212127686 15.262930870056152
  batch 40 loss: 1.6953836679458618, 2.7135093212127686, 15.262930870056152
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4, 5], device='cuda:0')
Loss :  1.6611216068267822 2.24184513092041 12.870347023010254
Loss :  1.6500496864318848 1.903705358505249 11.168577194213867
Loss :  1.6405612230300903 2.0766074657440186 12.023598670959473
Loss :  1.6519618034362793 1.8743164539337158 11.023544311523438
Loss :  1.6332117319107056 2.1868224143981934 12.5673246383667
Loss :  1.659071683883667 2.137505292892456 12.346598625183105
Loss :  1.6867021322250366 2.0610196590423584 11.991800308227539
Loss :  1.6456996202468872 2.731123208999634 15.301315307617188
Loss :  1.6986265182495117 2.315596103668213 13.276606559753418
Loss :  1.6511962413787842 2.0555546283721924 11.928969383239746
Loss :  1.6786270141601562 2.9274401664733887 16.315828323364258
Loss :  1.6736605167388916 2.924041271209717 16.293867111206055
Loss :  1.6559251546859741 2.489025592803955 14.101052284240723
Loss :  1.680379867553711 2.308932065963745 13.225040435791016
Loss :  1.6458570957183838 1.922847032546997 11.260091781616211
Loss :  1.6984634399414062 2.3551435470581055 13.474181175231934
Loss :  1.6525896787643433 2.6241345405578613 14.773262023925781
Loss :  1.6389840841293335 2.519841432571411 14.238190650939941
Loss :  1.6522446870803833 2.136868715286255 12.336587905883789
Loss :  1.7048453092575073 1.9129443168640137 11.269567489624023
  batch 60 loss: 1.7048453092575073, 1.9129443168640137, 11.269567489624023
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6403125524520874 3.222501039505005 17.752817153930664
Loss :  1.6638177633285522 1.7857937812805176 10.592785835266113
Loss :  1.648110270500183 1.5313552618026733 9.304886817932129
Loss :  1.638105034828186 2.2953271865844727 13.114741325378418
Loss :  1.6215002536773682 2.1714212894439697 12.478606224060059
Loss :  1.655728816986084 3.794163227081299 20.626544952392578
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.6643071174621582 3.7108423709869385 20.21851921081543
Loss :  1.6608144044876099 3.6054320335388184 19.68797492980957
Loss :  1.6735455989837646 3.619924306869507 19.77316665649414
Total LOSS train 13.767929884103628 valid 20.07655143737793
CE LOSS train 1.6594680290955763 valid 0.41838639974594116
Contrastive LOSS train 2.421692360364474 valid 0.9049810767173767
EPOCH 240:
Loss :  1.6756622791290283 2.7360055446624756 15.355690002441406
Loss :  1.6867438554763794 2.8263816833496094 15.818652153015137
Loss :  1.66144859790802 2.4887773990631104 14.105335235595703
Loss :  1.6664149761199951 2.147738218307495 12.405106544494629
Loss :  1.680881381034851 2.2150604724884033 12.756183624267578
Loss :  1.65153169631958 2.2743961811065674 13.02351188659668
Loss :  1.6815404891967773 1.9927551746368408 11.645316123962402
Loss :  1.6615955829620361 1.7012035846710205 10.16761302947998
Loss :  1.6534956693649292 1.7573630809783936 10.440311431884766
Loss :  1.681916356086731 1.6341114044189453 9.852473258972168
Loss :  1.645889163017273 2.165301561355591 12.472396850585938
Loss :  1.6456046104431152 1.854642629623413 10.918817520141602
Loss :  1.6428523063659668 1.7960288524627686 10.622997283935547
Loss :  1.6471331119537354 1.8409287929534912 10.851777076721191
Loss :  1.6930756568908691 2.760981559753418 15.497983932495117
Loss :  1.6893234252929688 2.2264866828918457 12.821756362915039
Loss :  1.6411759853363037 2.1518290042877197 12.400321006774902
Loss :  1.6632142066955566 2.642664909362793 14.87653923034668
Loss :  1.6381460428237915 2.6448819637298584 14.862556457519531
Loss :  1.6891716718673706 2.5942959785461426 14.660651206970215
  batch 20 loss: 1.6891716718673706, 2.5942959785461426, 14.660651206970215
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6582341194152832 1.9825116395950317 11.570793151855469
Loss :  1.6370950937271118 2.084498643875122 12.059588432312012
Loss :  1.6516870260238647 1.8150452375411987 10.726912498474121
Loss :  1.6645015478134155 2.062880277633667 11.978902816772461
Loss :  1.6887853145599365 2.404569149017334 13.711631774902344
Loss :  1.6540940999984741 2.716752529144287 15.2378568649292
Loss :  1.6625288724899292 2.752251148223877 15.423784255981445
Loss :  1.6574920415878296 2.1020615100860596 12.16779899597168
Loss :  1.6147513389587402 2.188019275665283 12.554847717285156
Loss :  1.6858245134353638 2.2302730083465576 12.837189674377441
Loss :  1.6160415410995483 2.1531903743743896 12.381993293762207
Loss :  1.6723089218139648 2.1407971382141113 12.376294136047363
Loss :  1.6521060466766357 1.9887970685958862 11.596091270446777
Loss :  1.650947093963623 2.77677321434021 15.534812927246094
Loss :  1.622578501701355 2.965275287628174 16.44895362854004
Loss :  1.6350539922714233 1.9987987279891968 11.629047393798828
Loss :  1.6339788436889648 1.9453014135360718 11.360486030578613
Loss :  1.6843253374099731 2.0228095054626465 11.798373222351074
Loss :  1.6885011196136475 2.4891276359558105 14.134140014648438
Loss :  1.6965980529785156 2.0449893474578857 11.921545028686523
  batch 40 loss: 1.6965980529785156, 2.0449893474578857, 11.921545028686523
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6627306938171387 2.19234299659729 12.624444961547852
Loss :  1.651971459388733 2.3802602291107178 13.55327320098877
Loss :  1.642600655555725 2.5624682903289795 14.45494270324707
Loss :  1.6537593603134155 2.6173975467681885 14.74074649810791
Loss :  1.6356240510940552 2.1548986434936523 12.410117149353027
Loss :  1.6609809398651123 1.7666856050491333 10.494409561157227
Loss :  1.6882693767547607 1.7154372930526733 10.265456199645996
Loss :  1.6480722427368164 2.4528510570526123 13.912327766418457
Loss :  1.7000744342803955 2.5103232860565186 14.251690864562988
Loss :  1.653214454650879 2.3991475105285645 13.648951530456543
Loss :  1.6802868843078613 2.4927546977996826 14.144060134887695
Loss :  1.6753339767456055 2.7225799560546875 15.288233757019043
Loss :  1.6580173969268799 2.210902452468872 12.712529182434082
Loss :  1.6819661855697632 2.415114641189575 13.757538795471191
Loss :  1.6482352018356323 1.834283471107483 10.819652557373047
Loss :  1.6999315023422241 1.6853430271148682 10.126646041870117
Loss :  1.6546555757522583 2.1027944087982178 12.168627738952637
Loss :  1.6417574882507324 2.6313858032226562 14.798686981201172
Loss :  1.6548573970794678 2.977216958999634 16.54094123840332
Loss :  1.7070099115371704 2.3871335983276367 13.642678260803223
  batch 60 loss: 1.7070099115371704, 2.3871335983276367, 13.642678260803223
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6428132057189941 2.5833520889282227 14.559574127197266
Loss :  1.6664927005767822 1.7708805799484253 10.520895004272461
Loss :  1.6504746675491333 1.6140340566635132 9.7206449508667
Loss :  1.6407146453857422 2.463433027267456 13.957880020141602
Loss :  1.624224066734314 1.8250391483306885 10.749419212341309
Loss :  1.6445776224136353 4.301209926605225 23.15062713623047
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.652768611907959 4.31404447555542 23.222991943359375
Loss :  1.6502845287322998 4.218955993652344 22.74506378173828
Loss :  1.6602613925933838 4.172412872314453 22.52232551574707
Total LOSS train 12.874944642873912 valid 22.9102520942688
CE LOSS train 1.660743368588961 valid 0.41506534814834595
Contrastive LOSS train 2.2428402643937333 valid 1.0431032180786133
EPOCH 241:
Loss :  1.6780297756195068 2.6980111598968506 15.168086051940918
Loss :  1.6888333559036255 2.1786787509918213 12.582226753234863
Loss :  1.6642796993255615 2.4336788654327393 13.832674026489258
Loss :  1.669360637664795 2.798478126525879 15.661750793457031
Loss :  1.6831562519073486 1.6047700643539429 9.707006454467773
Loss :  1.6538159847259521 2.2922520637512207 13.115076065063477
Loss :  1.6830765008926392 2.577186346054077 14.569008827209473
Loss :  1.6628872156143188 1.6451752185821533 9.888763427734375
Loss :  1.6551538705825806 1.7460516691207886 10.385412216186523
Loss :  1.6832584142684937 1.601639986038208 9.691457748413086
Loss :  1.647424578666687 1.8478187322616577 10.886518478393555
Loss :  1.6467876434326172 1.8621633052825928 10.95760440826416
Loss :  1.6448256969451904 1.8179588317871094 10.734620094299316
Loss :  1.6493284702301025 2.055305242538452 11.925854682922363
Loss :  1.6947743892669678 1.689989686012268 10.144722938537598
Loss :  1.6910487413406372 2.31561541557312 13.269125938415527
Loss :  1.6436139345169067 1.95564603805542 11.421844482421875
Loss :  1.6653451919555664 1.7366727590560913 10.348709106445312
Loss :  1.6405489444732666 2.4210457801818848 13.745777130126953
Loss :  1.6909149885177612 2.235442638397217 12.868128776550293
  batch 20 loss: 1.6909149885177612, 2.235442638397217, 12.868128776550293
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6604441404342651 1.9304989576339722 11.312938690185547
Loss :  1.6394225358963013 1.836667537689209 10.822760581970215
Loss :  1.6540195941925049 1.6848597526550293 10.07831859588623
Loss :  1.666152834892273 1.8114229440689087 10.723267555236816
Loss :  1.6900397539138794 1.9781330823898315 11.580704689025879
Loss :  1.6561131477355957 1.8611867427825928 10.962047576904297
Loss :  1.6643307209014893 2.470893383026123 14.018797874450684
Loss :  1.659424901008606 1.6325080394744873 9.821965217590332
Loss :  1.617358684539795 2.1777594089508057 12.506155014038086
Loss :  1.687258005142212 2.3097939491271973 13.236227035522461
Loss :  1.6183538436889648 2.4020230770111084 13.628469467163086
Loss :  1.6741185188293457 2.3701701164245605 13.524969100952148
Loss :  1.654219388961792 2.67340350151062 15.02123737335205
Loss :  1.6528058052062988 1.7361270189285278 10.333440780639648
Loss :  1.6247644424438477 2.4095053672790527 13.67229175567627
Loss :  1.636584997177124 2.140681505203247 12.33999252319336
Loss :  1.6354100704193115 2.489302635192871 14.081923484802246
Loss :  1.685214877128601 1.8888620138168335 11.129525184631348
Loss :  1.6893820762634277 2.39577054977417 13.668235778808594
Loss :  1.6973414421081543 1.4995830059051514 9.195257186889648
  batch 40 loss: 1.6973414421081543, 1.4995830059051514, 9.195257186889648
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6635676622390747 2.1535496711730957 12.431315422058105
Loss :  1.6526589393615723 2.6018338203430176 14.661827087402344
Loss :  1.6441749334335327 2.868713617324829 15.98774242401123
Loss :  1.655044674873352 2.435925006866455 13.83466911315918
Loss :  1.6370073556900024 1.9434912204742432 11.354463577270508
Loss :  1.6617608070373535 2.000871181488037 11.666116714477539
Loss :  1.688750147819519 1.6276822090148926 9.827160835266113
Loss :  1.6493806838989258 1.892303466796875 11.1108980178833
Loss :  1.700764775276184 2.667109489440918 15.036312103271484
Loss :  1.6546565294265747 1.8296997547149658 10.803154945373535
Loss :  1.6817306280136108 2.6199326515197754 14.781393051147461
Loss :  1.6767983436584473 2.122631311416626 12.289955139160156
Loss :  1.6596245765686035 1.9708082675933838 11.513666152954102
Loss :  1.6833117008209229 1.8928554058074951 11.147588729858398
Loss :  1.649993658065796 2.0303080081939697 11.801533699035645
Loss :  1.7007272243499756 2.105750322341919 12.22947883605957
Loss :  1.6553869247436523 2.1349422931671143 12.330098152160645
Loss :  1.6423276662826538 1.7441431283950806 10.363042831420898
Loss :  1.6549851894378662 2.0233492851257324 11.771732330322266
Loss :  1.7065290212631226 1.585518717765808 9.634122848510742
  batch 60 loss: 1.7065290212631226, 1.585518717765808, 9.634122848510742
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6428327560424805 2.6302387714385986 14.794026374816895
Loss :  1.6662230491638184 1.796416997909546 10.648307800292969
Loss :  1.6501370668411255 1.9424986839294434 11.362630844116211
Loss :  1.640515685081482 2.261809825897217 12.949564933776855
Loss :  1.6244176626205444 2.228278160095215 12.76580810546875
Loss :  1.636480689048767 3.3894102573394775 18.583532333374023
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.645535945892334 3.1241652965545654 17.2663631439209
Loss :  1.6443676948547363 3.32688307762146 18.27878189086914
Loss :  1.6452453136444092 2.8303215503692627 15.796853065490723
Total LOSS train 12.148607752873348 valid 17.481382608413696
CE LOSS train 1.6621927958268385 valid 0.4113113284111023
Contrastive LOSS train 2.097282992876493 valid 0.7075803875923157
EPOCH 242:
Loss :  1.6772973537445068 1.9806846380233765 11.580720901489258
Loss :  1.6882617473602295 1.8381521701812744 10.879022598266602
Loss :  1.6628284454345703 1.4620438814163208 8.973047256469727
Loss :  1.6678575277328491 2.0281660556793213 11.808687210083008
Loss :  1.6821320056915283 1.8792445659637451 11.078354835510254
Loss :  1.652759313583374 1.7639405727386475 10.47246265411377
Loss :  1.6821588277816772 2.222142219543457 12.792869567871094
Loss :  1.6624031066894531 1.8952616453170776 11.138710975646973
Loss :  1.6547640562057495 2.0219812393188477 11.764670372009277
Loss :  1.683125376701355 1.750353455543518 10.434892654418945
Loss :  1.6475237607955933 2.2836105823516846 13.065576553344727
Loss :  1.6470335721969604 2.215590476989746 12.72498607635498
Loss :  1.645004391670227 2.088507652282715 12.087542533874512
Loss :  1.6489704847335815 1.9899941682815552 11.598941802978516
Loss :  1.6945886611938477 2.18833589553833 12.63626766204834
Loss :  1.6903375387191772 1.7487400770187378 10.434037208557129
Loss :  1.6428927183151245 2.165299654006958 12.469390869140625
Loss :  1.6643258333206177 2.135781764984131 12.34323501586914
Loss :  1.6395008563995361 1.8263165950775146 10.77108383178711
Loss :  1.6902647018432617 1.7765021324157715 10.572775840759277
  batch 20 loss: 1.6902647018432617, 1.7765021324157715, 10.572775840759277
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6594923734664917 1.5995421409606934 9.65720272064209
Loss :  1.63857102394104 2.051457166671753 11.895856857299805
Loss :  1.6531835794448853 1.9426647424697876 11.366507530212402
Loss :  1.6656208038330078 2.4572229385375977 13.951735496520996
Loss :  1.6894084215164185 2.7504689693450928 15.441753387451172
Loss :  1.6550476551055908 2.556006669998169 14.435080528259277
Loss :  1.663497805595398 2.5076358318328857 14.201677322387695
Loss :  1.6587516069412231 2.4181032180786133 13.749267578125
Loss :  1.6166982650756836 1.9075146913528442 11.154272079467773
Loss :  1.6869924068450928 2.281052350997925 13.092254638671875
Loss :  1.6181269884109497 1.957874059677124 11.40749740600586
Loss :  1.6735979318618774 2.203970193862915 12.693449020385742
Loss :  1.6534366607666016 2.6102452278137207 14.704662322998047
Loss :  1.6520402431488037 2.644487142562866 14.874476432800293
Loss :  1.6244436502456665 2.2846500873565674 13.047694206237793
Loss :  1.6365445852279663 1.946473240852356 11.368910789489746
Loss :  1.6360901594161987 1.8028281927108765 10.65023136138916
Loss :  1.6857342720031738 2.3613131046295166 13.492300033569336
Loss :  1.689865231513977 1.801170825958252 10.695718765258789
Loss :  1.6980855464935303 1.7129417657852173 10.262794494628906
  batch 40 loss: 1.6980855464935303, 1.7129417657852173, 10.262794494628906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6654452085494995 2.085026264190674 12.090576171875
Loss :  1.6542428731918335 2.8596999645233154 15.952742576599121
Loss :  1.6467019319534302 2.759563446044922 15.44451904296875
Loss :  1.6571063995361328 2.600346326828003 14.658838272094727
Loss :  1.6388380527496338 1.5309360027313232 9.29351806640625
Loss :  1.6631646156311035 2.132462739944458 12.325477600097656
Loss :  1.6891974210739136 2.628993272781372 14.834163665771484
Loss :  1.6494579315185547 2.0445215702056885 11.872065544128418
Loss :  1.7000571489334106 2.46252179145813 14.012665748596191
Loss :  1.653729796409607 2.344757556915283 13.377516746520996
Loss :  1.6809419393539429 1.8071414232254028 10.716649055480957
Loss :  1.6756306886672974 3.032275438308716 16.837007522583008
Loss :  1.6580508947372437 2.6397736072540283 14.856918334960938
Loss :  1.6819736957550049 2.815868616104126 15.761316299438477
Loss :  1.6491583585739136 2.8974459171295166 16.13638687133789
Loss :  1.6998529434204102 2.594703435897827 14.673370361328125
Loss :  1.6549007892608643 2.3504891395568848 13.40734577178955
Loss :  1.6419987678527832 1.9034539461135864 11.159269332885742
Loss :  1.6550506353378296 2.867130994796753 15.990705490112305
Loss :  1.706390619277954 1.679731011390686 10.105045318603516
  batch 60 loss: 1.706390619277954, 1.679731011390686, 10.105045318603516
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6427282094955444 1.884743571281433 11.066445350646973
Loss :  1.6664068698883057 1.8071588277816772 10.702200889587402
Loss :  1.6502902507781982 1.8169426918029785 10.735003471374512
Loss :  1.6410096883773804 3.806471109390259 20.67336654663086
Loss :  1.6251380443572998 2.2880985736846924 13.065630912780762
Loss :  1.6474640369415283 3.5355160236358643 19.325044631958008
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.6565805673599243 3.4129137992858887 18.721149444580078
Loss :  1.6543989181518555 3.2429168224334717 17.86898422241211
Loss :  1.661393165588379 3.1010890007019043 17.166839599609375
Total LOSS train 12.638759451646072 valid 18.270504474639893
CE LOSS train 1.6619495887022753 valid 0.4153482913970947
Contrastive LOSS train 2.195361988361065 valid 0.7752722501754761
EPOCH 243:
Loss :  1.678373098373413 2.4955646991729736 14.156196594238281
Loss :  1.68941330909729 2.624159097671509 14.810208320617676
Loss :  1.663685917854309 1.8534157276153564 10.930764198303223
Loss :  1.668418288230896 2.84171724319458 15.87700366973877
Loss :  1.6826469898223877 2.8832881450653076 16.099088668823242
Loss :  1.6535389423370361 2.2663896083831787 12.98548698425293
Loss :  1.6826640367507935 2.3078579902648926 13.221953392028809
Loss :  1.6628477573394775 1.9913971424102783 11.619832992553711
Loss :  1.6550703048706055 2.5570712089538574 14.44042682647705
Loss :  1.683002233505249 2.3648674488067627 13.507339477539062
Loss :  1.6474077701568604 2.753390073776245 15.414358139038086
Loss :  1.6466726064682007 2.347998857498169 13.386666297912598
Loss :  1.6449304819107056 2.0583183765411377 11.936522483825684
Loss :  1.649085521697998 1.9530291557312012 11.41423225402832
Loss :  1.6945500373840332 4.296984672546387 23.179473876953125
Loss :  1.6903501749038696 2.007143020629883 11.726065635681152
Loss :  1.6429142951965332 1.9434071779251099 11.35995101928711
Loss :  1.66451096534729 2.02925705909729 11.810795783996582
Loss :  1.639845609664917 1.8834320306777954 11.057005882263184
Loss :  1.690710425376892 2.033952236175537 11.860471725463867
  batch 20 loss: 1.690710425376892, 2.033952236175537, 11.860471725463867
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6598327159881592 1.9562567472457886 11.441116333007812
Loss :  1.6395339965820312 2.0765058994293213 12.022063255310059
Loss :  1.6540776491165161 3.930617570877075 21.307165145874023
Loss :  1.6667249202728271 1.8229708671569824 10.781579971313477
Loss :  1.6903654336929321 1.9170068502426147 11.275400161743164
Loss :  1.6559761762619019 2.310218572616577 13.207069396972656
Loss :  1.6644706726074219 2.199094295501709 12.659942626953125
Loss :  1.659461498260498 2.4506187438964844 13.912555694580078
Loss :  1.6175142526626587 2.308711528778076 13.161072731018066
Loss :  1.687608003616333 2.6763527393341064 15.069371223449707
Loss :  1.618701696395874 2.0201258659362793 11.719331741333008
Loss :  1.674238681793213 1.9370826482772827 11.359651565551758
Loss :  1.6544228792190552 2.352769136428833 13.418268203735352
Loss :  1.6528109312057495 2.6621313095092773 14.963467597961426
Loss :  1.6255346536636353 2.520930290222168 14.230186462402344
Loss :  1.6372642517089844 2.466935873031616 13.971943855285645
Loss :  1.6365694999694824 3.3395960330963135 18.334550857543945
Loss :  1.6857768297195435 2.1908013820648193 12.63978385925293
Loss :  1.6896295547485352 2.378410816192627 13.581683158874512
Loss :  1.6976004838943481 2.2936084270477295 13.165642738342285
  batch 40 loss: 1.6976004838943481, 2.2936084270477295, 13.165642738342285
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
Loss :  1.6642488241195679 2.4196367263793945 13.762432098388672
Loss :  1.653399109840393 1.8989181518554688 11.147990226745605
Loss :  1.644590973854065 2.477072238922119 14.029952049255371
Loss :  1.6551703214645386 2.4115729331970215 13.713035583496094
Loss :  1.6371638774871826 3.298731803894043 18.130823135375977
Loss :  1.6620872020721436 2.9508326053619385 16.416250228881836
Loss :  1.6891975402832031 2.2026526927948 12.702461242675781
Loss :  1.6495153903961182 2.161970853805542 12.459369659423828
Loss :  1.7003886699676514 2.2706797122955322 13.053787231445312
Loss :  1.6544790267944336 2.5898330211639404 14.603644371032715
Loss :  1.6822808980941772 2.798215627670288 15.673358917236328
Loss :  1.6765836477279663 3.414945125579834 18.751310348510742
Loss :  1.659498691558838 2.6087286472320557 14.703142166137695
Loss :  1.6829718351364136 2.329909324645996 13.332518577575684
Loss :  1.6505300998687744 2.2690300941467285 12.99567985534668
Loss :  1.7002240419387817 1.9492427110671997 11.44643783569336
Loss :  1.6558399200439453 1.9709020853042603 11.510350227355957
Loss :  1.6431419849395752 2.5788679122924805 14.537481307983398
Loss :  1.6558529138565063 2.551734447479248 14.414525985717773
Loss :  1.7067095041275024 1.806592583656311 10.739672660827637
  batch 60 loss: 1.7067095041275024, 1.806592583656311, 10.739672660827637
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6435418128967285 2.312269926071167 13.204891204833984
Loss :  1.666953682899475 2.540635585784912 14.370132446289062
Loss :  1.6504980325698853 2.520089626312256 14.250946998596191
Loss :  1.6406371593475342 3.318768262863159 18.234477996826172
Loss :  1.6248117685317993 2.7971351146698 15.610487937927246
Loss :  1.6438764333724976 3.7038991451263428 20.163372039794922
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.6525253057479858 3.648374080657959 19.89439582824707
Loss :  1.6505287885665894 3.466576337814331 18.983409881591797
Loss :  1.6588290929794312 3.3646581172943115 18.482118606567383
Total LOSS train 13.797090016878569 valid 19.380824089050293
CE LOSS train 1.6622933919613179 valid 0.4147072732448578
Contrastive LOSS train 2.4269592982072097 valid 0.8411645293235779
EPOCH 244:
Loss :  1.6773310899734497 2.666516065597534 15.00991153717041
Loss :  1.6882119178771973 2.507628917694092 14.226356506347656
Loss :  1.6625717878341675 2.4120376110076904 13.722760200500488
Loss :  1.6673041582107544 2.3266234397888184 13.300421714782715
Loss :  1.6819343566894531 2.3337271213531494 13.350569725036621
Loss :  1.652437448501587 2.33294415473938 13.317157745361328
Loss :  1.6817799806594849 2.02176570892334 11.790608406066895
Loss :  1.661962628364563 2.4366562366485596 13.845243453979492
Loss :  1.6549721956253052 2.595128059387207 14.63061237335205
Loss :  1.6830768585205078 2.4866347312927246 14.116250991821289
Loss :  1.6479402780532837 2.603391170501709 14.664896965026855
Loss :  1.647143840789795 2.9390623569488525 16.34245491027832
Loss :  1.645410418510437 3.4594101905822754 18.942461013793945
Loss :  1.6488196849822998 2.6538050174713135 14.917844772338867
Loss :  1.6948639154434204 2.696613311767578 15.17793083190918
Loss :  1.6904648542404175 2.58974552154541 14.639192581176758
Loss :  1.6424771547317505 2.4390549659729004 13.837751388549805
Loss :  1.6640923023223877 1.8089226484298706 10.708704948425293
Loss :  1.6393967866897583 2.2844738960266113 13.061765670776367
Loss :  1.690068006515503 2.527175188064575 14.325943946838379
  batch 20 loss: 1.690068006515503, 2.527175188064575, 14.325943946838379
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6589922904968262 1.7926899194717407 10.622442245483398
Loss :  1.6387245655059814 2.7416608333587646 15.347028732299805
Loss :  1.6537542343139648 2.3276381492614746 13.291945457458496
Loss :  1.66628098487854 2.4124512672424316 13.728536605834961
Loss :  1.6900562047958374 2.8044939041137695 15.712525367736816
Loss :  1.655305027961731 2.1728854179382324 12.519732475280762
Loss :  1.663913607597351 2.1467862129211426 12.397844314575195
Loss :  1.6589521169662476 2.5817835330963135 14.567869186401367
Loss :  1.6169720888137817 2.7288317680358887 15.261131286621094
Loss :  1.6868525743484497 2.6333611011505127 14.853657722473145
Loss :  1.6184440851211548 2.5353758335113525 14.295323371887207
Loss :  1.6741074323654175 2.388381004333496 13.616012573242188
Loss :  1.65432608127594 4.073147296905518 22.020061492919922
Loss :  1.65282142162323 1.8855383396148682 11.080513000488281
Loss :  1.6250704526901245 2.4557981491088867 13.904061317443848
Loss :  1.6366169452667236 2.325108528137207 13.26215934753418
Loss :  1.6357569694519043 2.8533313274383545 15.902414321899414
Loss :  1.6859108209609985 2.202533483505249 12.698578834533691
Loss :  1.689326524734497 2.247051239013672 12.924582481384277
Loss :  1.6973692178726196 2.3703646659851074 13.549193382263184
  batch 40 loss: 1.6973692178726196, 2.3703646659851074, 13.549193382263184
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6639430522918701 2.44754695892334 13.901678085327148
Loss :  1.6532933712005615 1.7761417627334595 10.534002304077148
Loss :  1.644156575202942 3.6251938343048096 19.770126342773438
Loss :  1.6554926633834839 1.8645728826522827 10.978357315063477
Loss :  1.6370854377746582 2.0928874015808105 12.101522445678711
Loss :  1.6619645357131958 2.5560803413391113 14.442365646362305
Loss :  1.688485026359558 3.4522347450256348 18.949657440185547
Loss :  1.6502751111984253 2.4398903846740723 13.849726676940918
Loss :  1.7014153003692627 2.4874184131622314 14.138506889343262
Loss :  1.6554614305496216 2.306061029434204 13.185766220092773
Loss :  1.6827260255813599 2.372999668121338 13.547723770141602
Loss :  1.676729440689087 2.3297572135925293 13.325515747070312
Loss :  1.6591092348098755 1.9705071449279785 11.51164436340332
Loss :  1.6825140714645386 2.440394639968872 13.88448715209961
Loss :  1.6488853693008423 2.274031400680542 13.01904296875
Loss :  1.699540376663208 2.3637216091156006 13.518148422241211
Loss :  1.6541181802749634 2.3223180770874023 13.265708923339844
Loss :  1.6409374475479126 2.0145246982574463 11.713561058044434
Loss :  1.6539806127548218 3.113835096359253 17.223155975341797
Loss :  1.7059437036514282 2.147059202194214 12.441240310668945
  batch 60 loss: 1.7059437036514282, 2.147059202194214, 12.441240310668945
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6421949863433838 1.7558064460754395 10.421226501464844
Loss :  1.666213035583496 1.8117220401763916 10.724822998046875
Loss :  1.649855136871338 2.550340175628662 14.401556015014648
Loss :  1.6405433416366577 2.028550863265991 11.783297538757324
Loss :  1.6240921020507812 1.5718456506729126 9.483320236206055
Loss :  1.6598031520843506 4.129878520965576 22.30919647216797
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6691532135009766 4.0995683670043945 22.166994094848633
Loss :  1.6659938097000122 4.01432991027832 21.73764419555664
Loss :  1.6755765676498413 3.892564535140991 21.138399124145508
Total LOSS train 13.809240223811223 valid 21.838058471679688
CE LOSS train 1.661857982782217 valid 0.4188941419124603
Contrastive LOSS train 2.4294764610437247 valid 0.9731411337852478
EPOCH 245:
Loss :  1.6775881052017212 2.461862087249756 13.986899375915527
Loss :  1.6885055303573608 2.044438600540161 11.910697937011719
Loss :  1.663209319114685 1.8994585275650024 11.160501480102539
Loss :  1.6682677268981934 2.425504446029663 13.79578971862793
Loss :  1.6824686527252197 2.0183122158050537 11.774029731750488
Loss :  1.6536310911178589 1.8077130317687988 10.692195892333984
Loss :  1.6829559803009033 2.295815944671631 13.162035942077637
Loss :  1.6626675128936768 2.655134916305542 14.938342094421387
Loss :  1.654570460319519 2.5903499126434326 14.60632038116455
Loss :  1.682518482208252 2.3825387954711914 13.595212936401367
Loss :  1.6467993259429932 2.4693000316619873 13.99329948425293
Loss :  1.646230697631836 2.1650915145874023 12.471688270568848
Loss :  1.6438275575637817 2.202517509460449 12.656414985656738
Loss :  1.6479460000991821 2.1698734760284424 12.497313499450684
Loss :  1.6939395666122437 2.0364255905151367 11.876067161560059
Loss :  1.6899746656417847 2.1513638496398926 12.446793556213379
Loss :  1.6423472166061401 2.134629011154175 12.315492630004883
Loss :  1.664467453956604 1.7396924495697021 10.362930297851562
Loss :  1.6395487785339355 1.6171183586120605 9.725141525268555
Loss :  1.6904000043869019 2.6010003089904785 14.695401191711426
  batch 20 loss: 1.6904000043869019, 2.6010003089904785, 14.695401191711426
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.6594325304031372 2.6757025718688965 15.037945747375488
Loss :  1.6385715007781982 2.0736334323883057 12.006738662719727
Loss :  1.653908610343933 2.408200979232788 13.694913864135742
Loss :  1.6663905382156372 2.0497231483459473 11.915005683898926
Loss :  1.6902797222137451 2.18597674369812 12.620163917541504
Loss :  1.6565814018249512 1.9732426404953003 11.522794723510742
Loss :  1.6649274826049805 2.171771764755249 12.523786544799805
Loss :  1.660378098487854 2.01943302154541 11.757543563842773
Loss :  1.6189007759094238 2.5286669731140137 14.262235641479492
Loss :  1.6880871057510376 2.4700863361358643 14.038518905639648
Loss :  1.6199723482131958 2.166562795639038 12.452786445617676
Loss :  1.6749491691589355 2.1017463207244873 12.18368148803711
Loss :  1.6551892757415771 2.034362316131592 11.827001571655273
Loss :  1.653917670249939 2.1370444297790527 12.339139938354492
Loss :  1.6259011030197144 2.495988368988037 14.105843544006348
Loss :  1.6381068229675293 2.1573240756988525 12.424726486206055
Loss :  1.637041449546814 2.505531072616577 14.16469669342041
Loss :  1.6859264373779297 1.8839837312698364 11.10584545135498
Loss :  1.6898341178894043 1.950620412826538 11.442935943603516
Loss :  1.6980444192886353 1.930320382118225 11.34964656829834
  batch 40 loss: 1.6980444192886353, 1.930320382118225, 11.34964656829834
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6647694110870361 2.511369228363037 14.2216157913208
Loss :  1.6539114713668823 1.7540855407714844 10.424339294433594
Loss :  1.6446492671966553 2.0472798347473145 11.881048202514648
Loss :  1.6555708646774292 2.0410590171813965 11.86086654663086
Loss :  1.637967824935913 2.185988187789917 12.567909240722656
Loss :  1.6636664867401123 2.2529876232147217 12.928605079650879
Loss :  1.6909412145614624 2.2220687866210938 12.801284790039062
Loss :  1.6512963771820068 2.4991350173950195 14.146971702575684
Loss :  1.7029591798782349 2.007636547088623 11.741142272949219
Loss :  1.6567726135253906 1.7288538217544556 10.301041603088379
Loss :  1.6838271617889404 2.0314414501190186 11.841034889221191
Loss :  1.6786881685256958 2.001044750213623 11.68391227722168
Loss :  1.6617202758789062 1.7740100622177124 10.531770706176758
Loss :  1.685085415840149 2.2089040279388428 12.729605674743652
Loss :  1.6513038873672485 2.2438745498657227 12.87067699432373
Loss :  1.7017486095428467 2.6419270038604736 14.911383628845215
Loss :  1.6567643880844116 2.5268332958221436 14.29093074798584
Loss :  1.6434142589569092 1.9870814085006714 11.578821182250977
Loss :  1.6562919616699219 2.2562708854675293 12.937646865844727
Loss :  1.7073789834976196 1.6434376239776611 9.924567222595215
  batch 60 loss: 1.7073789834976196, 1.6434376239776611, 9.924567222595215
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.64397394657135 2.1133720874786377 12.210834503173828
Loss :  1.6673437356948853 1.928737759590149 11.311033248901367
Loss :  1.6515880823135376 2.424130439758301 13.77224063873291
Loss :  1.6422226428985596 2.870711088180542 15.99577808380127
Loss :  1.6260660886764526 2.072916030883789 11.990646362304688
Loss :  1.6502822637557983 4.334747791290283 23.324020385742188
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.658272385597229 4.331984043121338 23.318191528320312
Loss :  1.6575071811676025 4.169341564178467 22.504215240478516
Loss :  1.6651484966278076 4.060272216796875 21.966508865356445
Total LOSS train 12.567603492736817 valid 22.778234004974365
CE LOSS train 1.6627404157931989 valid 0.4162871241569519
Contrastive LOSS train 2.1809725871452916 valid 1.0150680541992188
EPOCH 246:
Loss :  1.6789071559906006 1.792132019996643 10.639567375183105
Loss :  1.6898517608642578 1.9762355089187622 11.571029663085938
Loss :  1.665099024772644 1.520399808883667 9.267098426818848
Loss :  1.66987943649292 2.501148223876953 14.175621032714844
Loss :  1.683849811553955 1.7445164918899536 10.40643310546875
Loss :  1.6546573638916016 1.8691691160202026 11.000502586364746
Loss :  1.683833122253418 1.9680330753326416 11.523998260498047
Loss :  1.6639728546142578 1.634574294090271 9.836844444274902
Loss :  1.656506061553955 1.6661969423294067 9.987491607666016
Loss :  1.6845868825912476 1.9057549238204956 11.213360786437988
Loss :  1.6499489545822144 2.146862745285034 12.384263038635254
Loss :  1.6496026515960693 2.0622124671936035 11.960664749145508
Loss :  1.647821307182312 1.8222821950912476 10.759232521057129
Loss :  1.6521776914596558 1.9198405742645264 11.251380920410156
Loss :  1.696928858757019 1.6673634052276611 10.033745765686035
Loss :  1.6924819946289062 1.6673178672790527 10.029071807861328
Loss :  1.6455267667770386 2.0354573726654053 11.822813987731934
Loss :  1.6665558815002441 1.6955069303512573 10.14409065246582
Loss :  1.6418095827102661 2.236585855484009 12.824738502502441
Loss :  1.6913834810256958 2.453171730041504 13.957242012023926
  batch 20 loss: 1.6913834810256958, 2.453171730041504, 13.957242012023926
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.661203145980835 1.6093388795852661 9.707898139953613
Loss :  1.6405640840530396 2.4814767837524414 14.047947883605957
Loss :  1.6553559303283691 1.561858892440796 9.464651107788086
Loss :  1.667636752128601 1.6563441753387451 9.949357986450195
Loss :  1.6914111375808716 2.694990873336792 15.166365623474121
Loss :  1.657580852508545 2.502929210662842 14.17222785949707
Loss :  1.665849208831787 1.699886679649353 10.165283203125
Loss :  1.661002278327942 1.6492611169815063 9.907307624816895
Loss :  1.6193263530731201 2.1188254356384277 12.213454246520996
Loss :  1.6883440017700195 2.32383131980896 13.307500839233398
Loss :  1.620484471321106 2.420729637145996 13.724132537841797
Loss :  1.675700306892395 2.301555871963501 13.183479309082031
Loss :  1.6560803651809692 2.664578437805176 14.978972434997559
Loss :  1.654802680015564 2.569542169570923 14.50251293182373
Loss :  1.6274728775024414 1.9869434833526611 11.562190055847168
Loss :  1.6391558647155762 2.4716250896453857 13.997282028198242
Loss :  1.6383140087127686 2.365154266357422 13.464085578918457
Loss :  1.6871676445007324 1.820281744003296 10.788576126098633
Loss :  1.6908042430877686 1.7842540740966797 10.612074851989746
Loss :  1.6989678144454956 1.6401530504226685 9.899733543395996
  batch 40 loss: 1.6989678144454956, 1.6401530504226685, 9.899733543395996
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6662256717681885 1.7619879245758057 10.476164817810059
Loss :  1.6555452346801758 1.8161588907241821 10.736339569091797
Loss :  1.6470978260040283 2.5093917846679688 14.194056510925293
Loss :  1.6579902172088623 1.9597047567367554 11.456514358520508
Loss :  1.6402610540390015 1.6943098306655884 10.111809730529785
Loss :  1.6647000312805176 2.192754030227661 12.628469467163086
Loss :  1.6909914016723633 2.2805287837982178 13.093635559082031
Loss :  1.652637004852295 2.057474374771118 11.940008163452148
Loss :  1.7024511098861694 1.777418851852417 10.589545249938965
Loss :  1.6571934223175049 1.7367572784423828 10.34097957611084
Loss :  1.6837714910507202 1.8662364482879639 11.01495361328125
Loss :  1.6786631345748901 2.0951547622680664 12.154437065124512
Loss :  1.6616631746292114 1.8760343790054321 11.041834831237793
Loss :  1.6849416494369507 2.019191265106201 11.780898094177246
Loss :  1.652681827545166 2.0649659633636475 11.97751235961914
Loss :  1.7023566961288452 2.1219356060028076 12.312034606933594
Loss :  1.6582392454147339 2.0508365631103516 11.912422180175781
Loss :  1.645389199256897 2.150517225265503 12.39797592163086
Loss :  1.657745599746704 2.5209546089172363 14.262517929077148
Loss :  1.7080680131912231 2.128251791000366 12.349327087402344
  batch 60 loss: 1.7080680131912231, 2.128251791000366, 12.349327087402344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6454977989196777 2.184464931488037 12.56782341003418
Loss :  1.6683701276779175 1.9746646881103516 11.541693687438965
Loss :  1.652348279953003 2.312006711959839 13.212382316589355
Loss :  1.642604947090149 1.910895586013794 11.19708251953125
Loss :  1.6265398263931274 1.5398417711257935 9.325748443603516
Loss :  1.637331485748291 3.4290411472320557 18.78253746032715
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6465235948562622 3.261383295059204 17.953441619873047
Loss :  1.6452693939208984 3.0795507431030273 17.04302215576172
Loss :  1.6473182439804077 2.910231113433838 16.19847297668457
Total LOSS train 11.757236803494967 valid 17.49436855316162
CE LOSS train 1.6641012100073007 valid 0.41182956099510193
Contrastive LOSS train 2.0186271007244403 valid 0.7275577783584595
EPOCH 247:
Loss :  1.6790212392807007 2.324942111968994 13.303730964660645
Loss :  1.6894867420196533 2.3244316577911377 13.311644554138184
Loss :  1.6645863056182861 1.8860231637954712 11.094701766967773
Loss :  1.6693611145019531 1.9660624265670776 11.499672889709473
Loss :  1.68374502658844 2.0324835777282715 11.846163749694824
Loss :  1.655050277709961 1.8989871740341187 11.149986267089844
Loss :  1.6839962005615234 1.7643226385116577 10.505609512329102
Loss :  1.6644437313079834 2.3585870265960693 13.457379341125488
Loss :  1.656809687614441 2.382628917694092 13.569954872131348
Loss :  1.6843211650848389 2.226322650909424 12.815934181213379
Loss :  1.6497036218643188 2.179966449737549 12.549535751342773
Loss :  1.649114966392517 2.2472329139709473 12.885278701782227
Loss :  1.6471943855285645 2.4778521060943604 14.036455154418945
Loss :  1.651187777519226 2.130694627761841 12.30466079711914
Loss :  1.696141242980957 2.5388357639312744 14.39031982421875
Loss :  1.6921272277832031 2.102898597717285 12.206620216369629
Loss :  1.6456660032272339 2.137699842453003 12.334165573120117
Loss :  1.666938304901123 2.6750168800354004 15.042022705078125
Loss :  1.642757534980774 2.3605988025665283 13.445751190185547
Loss :  1.6923414468765259 1.9074114561080933 11.229398727416992
  batch 20 loss: 1.6923414468765259, 1.9074114561080933, 11.229398727416992
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.661657691001892 1.741161584854126 10.367465019226074
Loss :  1.641301155090332 1.9384188652038574 11.333395957946777
Loss :  1.6556535959243774 1.9000346660614014 11.155827522277832
Loss :  1.668038010597229 2.4923689365386963 14.1298828125
Loss :  1.6913843154907227 2.6376893520355225 14.879831314086914
Loss :  1.6578532457351685 2.0116841793060303 11.71627426147461
Loss :  1.6665781736373901 1.9464952945709229 11.399054527282715
Loss :  1.661973237991333 1.845751404762268 10.890729904174805
Loss :  1.6205945014953613 2.4107742309570312 13.67446517944336
Loss :  1.6897814273834229 2.5669000148773193 14.52428150177002
Loss :  1.6219040155410767 2.6086602210998535 14.665205001831055
Loss :  1.6765857934951782 2.815802812576294 15.755599975585938
Loss :  1.656965732574463 2.4760613441467285 14.037271499633789
Loss :  1.655098557472229 2.060539722442627 11.957797050476074
Loss :  1.6279840469360352 2.2446324825286865 12.851146697998047
Loss :  1.6387219429016113 2.1561455726623535 12.419448852539062
Loss :  1.6375806331634521 2.671416997909546 14.99466609954834
Loss :  1.6870036125183105 2.0920960903167725 12.147483825683594
Loss :  1.6906437873840332 2.271965265274048 13.050470352172852
Loss :  1.6988393068313599 2.182518720626831 12.611433029174805
  batch 40 loss: 1.6988393068313599, 2.182518720626831, 12.611433029174805
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6662485599517822 2.3506176471710205 13.419336318969727
Loss :  1.6558343172073364 2.5451860427856445 14.38176441192627
Loss :  1.6478294134140015 2.8902223110198975 16.098941802978516
Loss :  1.6585369110107422 2.256415843963623 12.940616607666016
Loss :  1.6408820152282715 2.1530561447143555 12.40616226196289
Loss :  1.6650105714797974 1.9642585515975952 11.486303329467773
Loss :  1.691145420074463 1.9318161010742188 11.350225448608398
Loss :  1.652506947517395 1.8494458198547363 10.899735450744629
Loss :  1.7024072408676147 2.2952234745025635 13.178524017333984
Loss :  1.6571518182754517 2.258493661880493 12.949620246887207
Loss :  1.6837379932403564 2.573768377304077 14.552579879760742
Loss :  1.6786506175994873 1.9033902883529663 11.195602416992188
Loss :  1.6621898412704468 1.7292416095733643 10.30839729309082
Loss :  1.685293197631836 1.978582501411438 11.578206062316895
Loss :  1.6530709266662598 1.9628568887710571 11.467355728149414
Loss :  1.7020628452301025 1.83991539478302 10.901639938354492
Loss :  1.6578466892242432 2.0919008255004883 12.117350578308105
Loss :  1.6453520059585571 2.501915454864502 14.154929161071777
Loss :  1.6575697660446167 2.826385021209717 15.789495468139648
Loss :  1.7078416347503662 2.432709217071533 13.871387481689453
  batch 60 loss: 1.7078416347503662, 2.432709217071533, 13.871387481689453
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6454448699951172 1.9255937337875366 11.27341365814209
Loss :  1.6686416864395142 1.807501196861267 10.706148147583008
Loss :  1.6526219844818115 1.9840004444122314 11.572624206542969
Loss :  1.6435863971710205 2.308562755584717 13.186400413513184
Loss :  1.627745509147644 1.6605303287506104 9.930397033691406
Loss :  1.6418107748031616 3.8499650955200195 20.89163589477539
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6504294872283936 3.8503522872924805 20.902191162109375
Loss :  1.6487542390823364 3.739331007003784 20.345409393310547
Loss :  1.654443621635437 3.542794942855835 19.368417739868164
Total LOSS train 12.665505761366624 valid 20.37691354751587
CE LOSS train 1.664297630236699 valid 0.41361090540885925
Contrastive LOSS train 2.2002416335619412 valid 0.8856987357139587
EPOCH 248:
Loss :  1.6797232627868652 1.9693968296051025 11.52670669555664
Loss :  1.6903797388076782 2.04060435295105 11.893402099609375
Loss :  1.6653658151626587 1.8401397466659546 10.86606502532959
Loss :  1.670249104499817 2.617318630218506 14.756842613220215
Loss :  1.684037685394287 1.9050633907318115 11.209354400634766
Loss :  1.6558595895767212 1.9844977855682373 11.578349113464355
Loss :  1.6844520568847656 1.7741367816925049 10.555135726928711
Loss :  1.6648728847503662 1.6608166694641113 9.968955993652344
Loss :  1.6571489572525024 1.9198951721191406 11.256625175476074
Loss :  1.6843823194503784 2.368243932723999 13.525602340698242
Loss :  1.6497822999954224 2.119028091430664 12.244922637939453
Loss :  1.6493005752563477 2.1282713413238525 12.290657043457031
Loss :  1.647207498550415 1.8233377933502197 10.763895988464355
Loss :  1.6513746976852417 2.678757905960083 15.045164108276367
Loss :  1.6959998607635498 2.3965256214141846 13.678627967834473
Loss :  1.6921896934509277 2.285372734069824 13.11905288696289
Loss :  1.6455497741699219 2.1708362102508545 12.499731063842773
Loss :  1.6665910482406616 1.8280612230300903 10.806897163391113
Loss :  1.6423773765563965 2.4101650714874268 13.69320297241211
Loss :  1.6916652917861938 2.05076003074646 11.945466041564941
  batch 20 loss: 1.6916652917861938, 2.05076003074646, 11.945466041564941
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.6616510152816772 2.297271490097046 13.148008346557617
Loss :  1.6414673328399658 2.203970432281494 12.6613187789917
Loss :  1.655950665473938 1.7821141481399536 10.566521644592285
Loss :  1.6684156656265259 1.7389024496078491 10.36292839050293
Loss :  1.6917853355407715 2.2160372734069824 12.77197265625
Loss :  1.6579537391662598 2.578221082687378 14.54905891418457
Loss :  1.6665207147598267 2.164869546890259 12.49086856842041
Loss :  1.6616674661636353 2.6054470539093018 14.688902854919434
Loss :  1.6207996606826782 2.748236656188965 15.361983299255371
Loss :  1.6891378164291382 2.6547114849090576 14.962695121765137
Loss :  1.6221191883087158 2.64039945602417 14.824116706848145
Loss :  1.6763038635253906 1.9460361003875732 11.406484603881836
Loss :  1.6567423343658447 1.8100045919418335 10.706765174865723
Loss :  1.655087947845459 1.8942984342575073 11.126579284667969
Loss :  1.6280814409255981 2.0641419887542725 11.94879150390625
Loss :  1.6395037174224854 1.9936808347702026 11.60790729522705
Loss :  1.6388540267944336 2.634044885635376 14.809078216552734
Loss :  1.6876298189163208 1.9785076379776 11.580167770385742
Loss :  1.6917903423309326 2.39040470123291 13.643814086914062
Loss :  1.6998019218444824 2.299795627593994 13.198780059814453
  batch 40 loss: 1.6998019218444824, 2.299795627593994, 13.198780059814453
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6676607131958008 2.263831377029419 12.986817359924316
Loss :  1.6570848226547241 1.8252443075180054 10.783306121826172
Loss :  1.6489464044570923 1.8942198753356934 11.120046615600586
Loss :  1.659040093421936 2.0682578086853027 12.000329971313477
Loss :  1.6417604684829712 1.8589670658111572 10.936595916748047
Loss :  1.6657603979110718 2.0970849990844727 12.151185035705566
Loss :  1.6919474601745605 2.2202348709106445 12.793121337890625
Loss :  1.6535197496414185 2.186434268951416 12.58569049835205
Loss :  1.7031581401824951 2.440953016281128 13.907923698425293
Loss :  1.6583836078643799 2.0624496936798096 11.97063159942627
Loss :  1.6848403215408325 2.4437103271484375 13.90339183807373
Loss :  1.6793240308761597 2.2729110717773438 13.043879508972168
Loss :  1.6623508930206299 1.9280463457107544 11.302582740783691
Loss :  1.6851876974105835 2.638820171356201 14.879288673400879
Loss :  1.6534126996994019 2.511711835861206 14.2119722366333
Loss :  1.702160120010376 1.7372223138809204 10.38827133178711
Loss :  1.6588959693908691 1.7805167436599731 10.561479568481445
Loss :  1.6465339660644531 2.0371203422546387 11.832136154174805
Loss :  1.6590003967285156 2.1963417530059814 12.640708923339844
Loss :  1.709005355834961 2.2369582653045654 12.893796920776367
  batch 60 loss: 1.709005355834961, 2.2369582653045654, 12.893796920776367
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6467642784118652 2.6021840572357178 14.657684326171875
Loss :  1.669724941253662 2.4959092140197754 14.149271011352539
Loss :  1.653795838356018 2.5134737491607666 14.22116470336914
Loss :  1.6448019742965698 2.564777135848999 14.468688011169434
Loss :  1.6292392015457153 2.09713077545166 12.114892959594727
Loss :  1.6517304182052612 3.7810864448547363 20.557161331176758
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.660658597946167 3.6871225833892822 20.096271514892578
Loss :  1.6586675643920898 3.578509569168091 19.55121612548828
Loss :  1.6655367612838745 3.3421759605407715 18.37641716003418
Total LOSS train 12.556096267700195 valid 19.64526653289795
CE LOSS train 1.6648011244260348 valid 0.41638419032096863
Contrastive LOSS train 2.1782590242532582 valid 0.8355439901351929
EPOCH 249:
Loss :  1.6799391508102417 2.45015811920166 13.930729866027832
Loss :  1.6908634901046753 2.0703606605529785 12.0426664352417
Loss :  1.6660053730010986 1.881498098373413 11.073495864868164
Loss :  1.6704516410827637 2.354883909225464 13.44487190246582
Loss :  1.684424877166748 2.851409435272217 15.941473007202148
Loss :  1.655448079109192 1.97788667678833 11.544880867004395
Loss :  1.6838589906692505 2.1175029277801514 12.271373748779297
Loss :  1.6644850969314575 2.9909403324127197 16.619186401367188
Loss :  1.6572760343551636 1.9470727443695068 11.392640113830566
Loss :  1.6849132776260376 2.556168556213379 14.4657564163208
Loss :  1.6508100032806396 2.5621883869171143 14.461751937866211
Loss :  1.6505767107009888 1.917414903640747 11.237650871276855
Loss :  1.6489362716674805 1.9351779222488403 11.32482624053955
Loss :  1.6530839204788208 2.232429027557373 12.815229415893555
Loss :  1.6981786489486694 2.5791726112365723 14.594040870666504
Loss :  1.693049669265747 2.0350210666656494 11.868154525756836
Loss :  1.6470465660095215 2.3679208755493164 13.486650466918945
Loss :  1.667314052581787 2.118425130844116 12.259439468383789
Loss :  1.6434276103973389 1.5902557373046875 9.594706535339355
Loss :  1.6919019222259521 1.7592661380767822 10.488232612609863
  batch 20 loss: 1.6919019222259521, 1.7592661380767822, 10.488232612609863
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6624524593353271 1.6357743740081787 9.841324806213379
Loss :  1.6422005891799927 2.3222851753234863 13.253625869750977
Loss :  1.656577229499817 1.757049560546875 10.441824913024902
Loss :  1.6690635681152344 1.884006142616272 11.089094161987305
Loss :  1.6924453973770142 2.0206525325775146 11.795708656311035
Loss :  1.6588910818099976 1.8058174848556519 10.68797779083252
Loss :  1.6671147346496582 2.0173845291137695 11.754037857055664
Loss :  1.6624146699905396 1.9959274530410767 11.642051696777344
Loss :  1.6215713024139404 1.7582554817199707 10.412848472595215
Loss :  1.6896424293518066 2.0385522842407227 11.882404327392578
Loss :  1.6226296424865723 1.9914495944976807 11.579877853393555
Loss :  1.676519513130188 1.9437637329101562 11.39533805847168
Loss :  1.657288908958435 1.9905146360397339 11.609861373901367
Loss :  1.6555864810943604 2.0352728366851807 11.831950187683105
Loss :  1.628503441810608 2.877666473388672 16.016836166381836
Loss :  1.6400538682937622 1.8517452478408813 10.89877986907959
Loss :  1.6392451524734497 2.040820598602295 11.843348503112793
Loss :  1.6873188018798828 1.952767014503479 11.451153755187988
Loss :  1.6911996603012085 2.688506841659546 15.133733749389648
Loss :  1.69894540309906 2.503194570541382 14.21491813659668
  batch 40 loss: 1.69894540309906, 2.503194570541382, 14.21491813659668
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6668072938919067 2.1399922370910645 12.366767883300781
Loss :  1.6562221050262451 1.589011549949646 9.601280212402344
Loss :  1.6479824781417847 2.021918773651123 11.757576942443848
Loss :  1.6582149267196655 2.0829615592956543 12.073022842407227
Loss :  1.6409507989883423 2.4428114891052246 13.855009078979492
Loss :  1.6651873588562012 2.320387363433838 13.26712417602539
Loss :  1.6908711194992065 1.8725380897521973 11.053561210632324
Loss :  1.653082251548767 1.9705796241760254 11.505979537963867
Loss :  1.702260136604309 1.8365145921707153 10.884833335876465
Loss :  1.6577074527740479 1.6503945589065552 9.909680366516113
Loss :  1.6845308542251587 2.5719377994537354 14.544219970703125
Loss :  1.6793034076690674 2.4978294372558594 14.168450355529785
Loss :  1.6626065969467163 2.047105073928833 11.898131370544434
Loss :  1.6850626468658447 1.7395066022872925 10.38259506225586
Loss :  1.6536487340927124 2.532193183898926 14.314614295959473
Loss :  1.70211923122406 2.5655198097229004 14.529717445373535
Loss :  1.6588859558105469 2.267409324645996 12.995932579040527
Loss :  1.6464316844940186 1.7679089307785034 10.485976219177246
Loss :  1.6587907075881958 2.0843987464904785 12.08078384399414
Loss :  1.7090033292770386 1.5663764476776123 9.540885925292969
  batch 60 loss: 1.7090033292770386, 1.5663764476776123, 9.540885925292969
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.646743655204773 2.4010744094848633 13.652115821838379
Loss :  1.6693097352981567 1.7032825946807861 10.185722351074219
Loss :  1.653671383857727 1.6327944993972778 9.817644119262695
Loss :  1.644437313079834 2.423511505126953 13.761995315551758
Loss :  1.6287888288497925 1.2655328512191772 7.956453323364258
Loss :  1.648368239402771 4.162716865539551 22.461952209472656
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6559336185455322 4.061190128326416 21.961883544921875
Loss :  1.6549532413482666 3.8939144611358643 21.12452507019043
Loss :  1.6617133617401123 3.9300689697265625 21.312057495117188
Total LOSS train 12.157300420907827 valid 21.715104579925537
CE LOSS train 1.6649888570492084 valid 0.4154283404350281
Contrastive LOSS train 2.0984623212080735 valid 0.9825172424316406
EPOCH 250:
Loss :  1.680376648902893 1.594173789024353 9.651246070861816
Loss :  1.691452145576477 1.6911729574203491 10.147316932678223
Loss :  1.66692316532135 2.04487681388855 11.891307830810547
Loss :  1.6716233491897583 2.35378360748291 13.44054126739502
Loss :  1.6856051683425903 1.685750126838684 10.11435604095459
Loss :  1.656916856765747 1.6239562034606934 9.776698112487793
Loss :  1.6855239868164062 2.115333080291748 12.262189865112305
Loss :  1.665751576423645 2.1720173358917236 12.525837898254395
Loss :  1.6581392288208008 2.3040003776550293 13.178141593933105
Loss :  1.6854137182235718 1.3957135677337646 8.663981437683105
Loss :  1.6512253284454346 1.8524020910263062 10.913235664367676
Loss :  1.6508336067199707 1.8438761234283447 10.870214462280273
Loss :  1.6490681171417236 2.240128517150879 12.849710464477539
Loss :  1.6537151336669922 2.047584295272827 11.891636848449707
Loss :  1.6980170011520386 2.915161609649658 16.27382469177246
Loss :  1.6933863162994385 2.042337417602539 11.905073165893555
Loss :  1.6468737125396729 2.5656416416168213 14.475081443786621
Loss :  1.6672744750976562 2.162795066833496 12.481249809265137
Loss :  1.642883062362671 2.3096725940704346 13.191246032714844
Loss :  1.691849946975708 2.5253236293792725 14.31846809387207
  batch 20 loss: 1.691849946975708, 2.5253236293792725, 14.31846809387207
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6614983081817627 2.5774993896484375 14.548995018005371
Loss :  1.641230821609497 2.0175893306732178 11.729177474975586
Loss :  1.6560741662979126 1.6205395460128784 9.758771896362305
Loss :  1.6685062646865845 2.2449069023132324 12.893041610717773
Loss :  1.6921550035476685 3.2268197536468506 17.82625389099121
Loss :  1.6585403680801392 3.2556376457214355 17.936729431152344
Loss :  1.66664719581604 3.388618230819702 18.609737396240234
Loss :  1.6623260974884033 2.118946075439453 12.25705623626709
Loss :  1.621477723121643 2.327529191970825 13.259123802185059
Loss :  1.6897939443588257 2.2383477687835693 12.881532669067383
Loss :  1.622490406036377 2.064640998840332 11.945695877075195
Loss :  1.6763701438903809 3.167748212814331 17.515111923217773
Loss :  1.6567597389221191 2.823211193084717 15.772815704345703
Loss :  1.6550014019012451 2.4286398887634277 13.798201560974121
Loss :  1.6281089782714844 2.3195955753326416 13.226086616516113
Loss :  1.6397099494934082 2.1457290649414062 12.368354797363281
Loss :  1.6385682821273804 2.095278739929199 12.114961624145508
Loss :  1.6869616508483887 1.999132752418518 11.682624816894531
Loss :  1.690891146659851 1.910620093345642 11.24399185180664
Loss :  1.6988950967788696 4.059742450714111 21.99760627746582
  batch 40 loss: 1.6988950967788696, 4.059742450714111, 21.99760627746582
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6665884256362915 2.490431070327759 14.118743896484375
Loss :  1.6566534042358398 2.2476303577423096 12.894804954528809
Loss :  1.6487265825271606 2.3702802658081055 13.500127792358398
Loss :  1.6587920188903809 1.8797821998596191 11.057703018188477
Loss :  1.641969084739685 1.9943671226501465 11.613804817199707
Loss :  1.6659797430038452 2.2894222736358643 13.113090515136719
Loss :  1.6918206214904785 3.0267858505249023 16.82575035095215
Loss :  1.653922200202942 1.9259049892425537 11.283447265625
Loss :  1.7026524543762207 2.7712037563323975 15.558671951293945
Loss :  1.6582285165786743 3.274125337600708 18.028854370117188
Loss :  1.6847172975540161 2.577718496322632 14.573309898376465
Loss :  1.6792309284210205 2.431938409805298 13.838922500610352
Loss :  1.6617846488952637 1.9978783130645752 11.651176452636719
Loss :  1.684419870376587 2.320998191833496 13.289410591125488
Loss :  1.6525442600250244 2.249221086502075 12.898649215698242
Loss :  1.7005196809768677 2.4385130405426025 13.893084526062012
Loss :  1.6573994159698486 2.4646475315093994 13.980636596679688
Loss :  1.6450932025909424 2.298271656036377 13.13645076751709
Loss :  1.657387614250183 2.9541032314300537 16.42790412902832
Loss :  1.7079200744628906 2.6929614543914795 15.172727584838867
  batch 60 loss: 1.7079200744628906, 2.6929614543914795, 15.172727584838867
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 5], device='cuda:0')
Loss :  1.6460189819335938 2.3295552730560303 13.293795585632324
Loss :  1.6693581342697144 2.534741163253784 14.343064308166504
Loss :  1.6535108089447021 2.3311452865600586 13.309237480163574
Loss :  1.6445022821426392 2.5904717445373535 14.596860885620117
Loss :  1.628691554069519 2.2368569374084473 12.812975883483887
Loss :  1.654105544090271 4.295248508453369 23.130348205566406
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 5], device='cuda:0')
Loss :  1.6619943380355835 4.220688819885254 22.765438079833984
Loss :  1.660622477531433 4.24769401550293 22.899091720581055
Loss :  1.6653156280517578 4.015731334686279 21.743972778320312
Total LOSS train 13.375391285236065 valid 22.63471269607544
CE LOSS train 1.6649737082994902 valid 0.41632890701293945
Contrastive LOSS train 2.3420835183216977 valid 1.0039328336715698
EPOCH 251:
Loss :  1.6799123287200928 2.797088384628296 15.66535472869873
Loss :  1.6906063556671143 2.884498357772827 16.11309814453125
Loss :  1.6658530235290527 2.145486354827881 12.393285751342773
Loss :  1.6705785989761353 2.3241631984710693 13.29139518737793
Loss :  1.6846383810043335 1.8499037027359009 10.93415641784668
Loss :  1.6562055349349976 1.884710669517517 11.079758644104004
Loss :  1.6843791007995605 1.9395872354507446 11.382314682006836
Loss :  1.6650960445404053 1.703833818435669 10.18426513671875
Loss :  1.6576415300369263 1.820271611213684 10.758999824523926
Loss :  1.6846948862075806 1.7220207452774048 10.294798851013184
Loss :  1.650507926940918 2.462014675140381 13.96058177947998
Loss :  1.649911642074585 2.510460615158081 14.202215194702148
Loss :  1.6480129957199097 2.3942580223083496 13.619303703308105
Loss :  1.6519626379013062 2.0595669746398926 11.949796676635742
Loss :  1.696581482887268 1.9952867031097412 11.673015594482422
Loss :  1.6921294927597046 2.0779500007629395 12.081878662109375
Loss :  1.6460880041122437 2.0725347995758057 12.008761405944824
Loss :  1.6666539907455444 1.7827800512313843 10.580554008483887
Loss :  1.6430584192276 1.6186414957046509 9.736266136169434
Loss :  1.6919769048690796 2.459172248840332 13.987837791442871
  batch 20 loss: 1.6919769048690796, 2.459172248840332, 13.987837791442871
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6623433828353882 1.9005813598632812 11.165249824523926
Loss :  1.6423386335372925 1.8784114122390747 11.034396171569824
Loss :  1.6568024158477783 1.7651618719100952 10.482611656188965
Loss :  1.6691877841949463 1.8647973537445068 10.99317455291748
Loss :  1.692243218421936 2.0869078636169434 12.12678337097168
Loss :  1.6586626768112183 1.8928848505020142 11.123086929321289
Loss :  1.6670104265213013 1.9021114110946655 11.177567481994629
Loss :  1.6622925996780396 2.274761438369751 13.036099433898926
Loss :  1.6217455863952637 2.446032762527466 13.851909637451172
Loss :  1.6895602941513062 2.272590398788452 13.052512168884277
Loss :  1.622986078262329 1.9989856481552124 11.617914199829102
Loss :  1.676693320274353 1.9279112815856934 11.31624984741211
Loss :  1.657330870628357 1.7822544574737549 10.568602561950684
Loss :  1.6556711196899414 1.8901044130325317 11.106193542480469
Loss :  1.6285011768341064 2.306929588317871 13.163148880004883
Loss :  1.6399608850479126 2.0684239864349365 11.982081413269043
Loss :  1.6388977766036987 1.8081786632537842 10.679791450500488
Loss :  1.6868699789047241 1.8257757425308228 10.81574821472168
Loss :  1.6910443305969238 1.8224987983703613 10.803537368774414
Loss :  1.6989631652832031 1.7108051776885986 10.252988815307617
  batch 40 loss: 1.6989631652832031, 1.7108051776885986, 10.252988815307617
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.666611909866333 2.5132477283477783 14.232850074768066
Loss :  1.6564701795578003 2.2812340259552 13.062640190124512
Loss :  1.6481094360351562 1.8747305870056152 11.02176284790039
Loss :  1.6588155031204224 2.00968599319458 11.707244873046875
Loss :  1.6414721012115479 2.137636423110962 12.3296537399292
Loss :  1.6658148765563965 1.8859468698501587 11.095548629760742
Loss :  1.691702961921692 1.7576451301574707 10.479928016662598
Loss :  1.65341317653656 2.7243359088897705 15.275092124938965
Loss :  1.7027426958084106 1.8857802152633667 11.131643295288086
Loss :  1.6579011678695679 1.9173380136489868 11.244590759277344
Loss :  1.6841709613800049 2.4496333599090576 13.932337760925293
Loss :  1.6790038347244263 2.2910842895507812 13.134425163269043
Loss :  1.6624654531478882 2.2806284427642822 13.065607070922852
Loss :  1.6851049661636353 1.9992977380752563 11.681593894958496
Loss :  1.6535003185272217 1.9239096641540527 11.273049354553223
Loss :  1.702435851097107 1.9592194557189941 11.49853229522705
Loss :  1.6590209007263184 2.0908427238464355 12.113235473632812
Loss :  1.6463030576705933 1.5612519979476929 9.452563285827637
Loss :  1.6588804721832275 1.906234860420227 11.190054893493652
Loss :  1.7088122367858887 1.3985265493392944 8.701444625854492
  batch 60 loss: 1.7088122367858887, 1.3985265493392944, 8.701444625854492
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.646916151046753 1.6289610862731934 9.791722297668457
Loss :  1.6698906421661377 2.66055965423584 14.972688674926758
Loss :  1.6542223691940308 2.656048059463501 14.934462547302246
Loss :  1.6451057195663452 1.9809104204177856 11.549657821655273
Loss :  1.6294636726379395 2.02023983001709 11.730663299560547
Loss :  1.649886131286621 4.034132480621338 21.82054901123047
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.65791654586792 4.059123992919922 21.953536987304688
Loss :  1.6565909385681152 3.9423060417175293 21.368122100830078
Loss :  1.6624116897583008 3.8162789344787598 20.743804931640625
Total LOSS train 11.951542751605695 valid 21.471503257751465
CE LOSS train 1.6649837787334736 valid 0.4156029224395752
Contrastive LOSS train 2.057311802643996 valid 0.9540697336196899
EPOCH 252:
Loss :  1.6808443069458008 2.27481746673584 13.054931640625
Loss :  1.6915791034698486 2.1907804012298584 12.64548110961914
Loss :  1.6672275066375732 1.7487313747406006 10.410884857177734
Loss :  1.6719528436660767 1.9684500694274902 11.514204025268555
Loss :  1.685664176940918 1.7121164798736572 10.246246337890625
Loss :  1.6573008298873901 1.684396505355835 10.079283714294434
Loss :  1.6855818033218384 1.8249074220657349 10.81011962890625
Loss :  1.6664440631866455 1.7668921947479248 10.50090503692627
Loss :  1.6589858531951904 2.084789514541626 12.08293342590332
Loss :  1.6860754489898682 2.065854787826538 12.015349388122559
Loss :  1.651572585105896 2.464317560195923 13.973159790039062
Loss :  1.6510341167449951 2.5770556926727295 14.5363130569458
Loss :  1.6489496231079102 2.5633432865142822 14.465665817260742
Loss :  1.6528493165969849 2.046933650970459 11.887517929077148
Loss :  1.6971571445465088 1.7777631282806396 10.585972785949707
Loss :  1.6927145719528198 1.8526374101638794 10.955902099609375
Loss :  1.6461251974105835 3.5294461250305176 19.29335594177246
Loss :  1.6670688390731812 3.910183906555176 21.217987060546875
Loss :  1.643293857574463 3.320708751678467 18.246837615966797
Loss :  1.692203164100647 2.507246255874634 14.228434562683105
  batch 20 loss: 1.692203164100647, 2.507246255874634, 14.228434562683105
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.662477970123291 2.1027004718780518 12.175979614257812
Loss :  1.6424587965011597 4.2873005867004395 23.078960418701172
Loss :  1.656976580619812 2.513989210128784 14.226922988891602
Loss :  1.66962468624115 2.3225927352905273 13.282588005065918
Loss :  1.6928783655166626 2.585785150527954 14.621804237365723
Loss :  1.659104347229004 2.4057576656341553 13.68789291381836
Loss :  1.6673638820648193 2.368250608444214 13.508617401123047
Loss :  1.662881851196289 1.888659954071045 11.106182098388672
Loss :  1.6226242780685425 2.8059298992156982 15.652274131774902
Loss :  1.6900428533554077 2.4227776527404785 13.803930282592773
Loss :  1.6239454746246338 2.1105778217315674 12.176834106445312
Loss :  1.6769018173217773 2.318260908126831 13.268206596374512
Loss :  1.6577857732772827 2.1288774013519287 12.302172660827637
Loss :  1.6561486721038818 2.5714592933654785 14.513444900512695
Loss :  1.6295784711837769 2.2567737102508545 12.913447380065918
Loss :  1.6408181190490723 2.114865779876709 12.215147018432617
Loss :  1.6397467851638794 2.4644432067871094 13.961962699890137
Loss :  1.6868435144424438 2.2680585384368896 13.02713680267334
Loss :  1.6909854412078857 1.860800862312317 10.994990348815918
Loss :  1.6989392042160034 1.7445909976959229 10.421894073486328
  batch 40 loss: 1.6989392042160034, 1.7445909976959229, 10.421894073486328
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.666916847229004 1.905954122543335 11.196687698364258
Loss :  1.656427025794983 1.68865168094635 10.099685668945312
Loss :  1.6484220027923584 1.9922938346862793 11.609891891479492
Loss :  1.658581256866455 2.461348056793213 13.965320587158203
Loss :  1.6415385007858276 2.4953129291534424 14.11810302734375
Loss :  1.6657100915908813 2.6368956565856934 14.850189208984375
Loss :  1.6916983127593994 1.6674211025238037 10.028803825378418
Loss :  1.6535595655441284 1.9931968450546265 11.61954402923584
Loss :  1.702376127243042 1.6386264562606812 9.895508766174316
Loss :  1.6582266092300415 1.498453140258789 9.150492668151855
Loss :  1.6845836639404297 2.2836897373199463 13.103032112121582
Loss :  1.6795775890350342 1.7306458950042725 10.332807540893555
Loss :  1.6629858016967773 1.747787594795227 10.401924133300781
Loss :  1.6852648258209229 1.8143311738967896 10.75692081451416
Loss :  1.6543800830841064 2.631037950515747 14.809569358825684
Loss :  1.7022497653961182 2.407714366912842 13.740821838378906
Loss :  1.659437656402588 2.1213748455047607 12.266311645507812
Loss :  1.6470099687576294 1.6250640153884888 9.772330284118652
Loss :  1.6592038869857788 2.3925023078918457 13.62171459197998
Loss :  1.7086446285247803 2.315603494644165 13.286662101745605
  batch 60 loss: 1.7086446285247803, 2.315603494644165, 13.286662101745605
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6471774578094482 2.4919660091400146 14.10700798034668
Loss :  1.6702955961227417 2.41949200630188 13.767755508422852
Loss :  1.6543902158737183 2.0954790115356445 12.13178539276123
Loss :  1.645598292350769 2.2875447273254395 13.083321571350098
Loss :  1.6297607421875 1.8756898641586304 11.008210182189941
Loss :  1.654322862625122 4.395401954650879 23.631332397460938
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6620378494262695 4.328619003295898 23.305133819580078
Loss :  1.6622000932693481 4.209100246429443 22.70770263671875
Loss :  1.6623823642730713 4.321099281311035 23.26787757873535
Total LOSS train 12.867942722027118 valid 23.22801160812378
CE LOSS train 1.6654887346120981 valid 0.4155955910682678
Contrastive LOSS train 2.2404907886798564 valid 1.0802748203277588
EPOCH 253:
Loss :  1.6805967092514038 2.239309787750244 12.877144813537598
Loss :  1.691207766532898 2.6252896785736084 14.817656517028809
Loss :  1.6668434143066406 1.7483323812484741 10.4085054397583
Loss :  1.6717159748077393 1.9403716325759888 11.373574256896973
Loss :  1.6856449842453003 1.4894847869873047 9.133069038391113
Loss :  1.6572624444961548 1.9247205257415771 11.280865669250488
Loss :  1.6855108737945557 2.3106701374053955 13.238861083984375
Loss :  1.6666206121444702 2.223785877227783 12.78554916381836
Loss :  1.659422516822815 2.402243137359619 13.670638084411621
Loss :  1.6865620613098145 2.266383171081543 13.018478393554688
Loss :  1.6522938013076782 2.5196714401245117 14.250651359558105
Loss :  1.6519339084625244 2.016828775405884 11.736077308654785
Loss :  1.6501471996307373 1.872307300567627 11.011683464050293
Loss :  1.6541569232940674 1.998486876487732 11.646591186523438
Loss :  1.6984223127365112 2.3760690689086914 13.578767776489258
Loss :  1.6939096450805664 2.4374635219573975 13.881227493286133
Loss :  1.6485451459884644 2.181838035583496 12.557735443115234
Loss :  1.6687326431274414 1.6613858938217163 9.975662231445312
Loss :  1.6456342935562134 1.7278445959091187 10.284857749938965
Loss :  1.6947332620620728 2.2899489402770996 13.144478797912598
  batch 20 loss: 1.6947332620620728, 2.2899489402770996, 13.144478797912598
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6645891666412354 2.3037514686584473 13.183345794677734
Loss :  1.6447725296020508 1.977560043334961 11.532572746276855
Loss :  1.6586517095565796 2.124629497528076 12.28179931640625
Loss :  1.6705310344696045 2.2246267795562744 12.793664932250977
Loss :  1.6935056447982788 2.7740845680236816 15.56392765045166
Loss :  1.6601629257202148 2.001864194869995 11.66948413848877
Loss :  1.6682302951812744 2.0588743686676025 11.962601661682129
Loss :  1.6636382341384888 1.8229690790176392 10.778483390808105
Loss :  1.6230268478393555 1.8787180185317993 11.016616821289062
Loss :  1.6903413534164429 1.9147064685821533 11.263873100280762
Loss :  1.6239511966705322 1.9599803686141968 11.423852920532227
Loss :  1.677274227142334 1.9259041547775269 11.306795120239258
Loss :  1.657955288887024 1.9182307720184326 11.249109268188477
Loss :  1.6563433408737183 1.932778239250183 11.320234298706055
Loss :  1.629669427871704 2.65354061126709 14.897372245788574
Loss :  1.6409879922866821 2.4740447998046875 14.011212348937988
Loss :  1.6402076482772827 2.4189746379852295 13.73508071899414
Loss :  1.6882059574127197 2.2462728023529053 12.919569969177246
Loss :  1.692259669303894 2.275113821029663 13.067829132080078
Loss :  1.7002735137939453 1.597536325454712 9.687954902648926
  batch 40 loss: 1.7002735137939453, 1.597536325454712, 9.687954902648926
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.668285846710205 2.1853888034820557 12.595230102539062
Loss :  1.6580575704574585 1.6843551397323608 10.079833030700684
Loss :  1.6500364542007446 2.216898202896118 12.734527587890625
Loss :  1.6602563858032227 2.3922650814056396 13.62158203125
Loss :  1.6432888507843018 2.0835559368133545 12.061068534851074
Loss :  1.6671324968338013 2.659534215927124 14.964803695678711
Loss :  1.6930499076843262 2.55708646774292 14.478483200073242
Loss :  1.655545711517334 2.1872732639312744 12.591911315917969
Loss :  1.7044399976730347 1.6564892530441284 9.986886024475098
Loss :  1.660288691520691 2.0836846828460693 12.078712463378906
Loss :  1.6865085363388062 2.188537836074829 12.629197120666504
Loss :  1.6813478469848633 2.0184569358825684 11.773633003234863
Loss :  1.6645644903182983 1.9336870908737183 11.332999229431152
Loss :  1.6871349811553955 2.161008358001709 12.49217700958252
Loss :  1.655632495880127 2.039881706237793 11.85504150390625
Loss :  1.7038151025772095 2.0133447647094727 11.770539283752441
Loss :  1.6607669591903687 2.09136700630188 12.11760139465332
Loss :  1.6483209133148193 1.6412413120269775 9.854527473449707
Loss :  1.6604185104370117 2.4132659435272217 13.7267484664917
Loss :  1.7095755338668823 1.5150078535079956 9.284614562988281
  batch 60 loss: 1.7095755338668823, 1.5150078535079956, 9.284614562988281
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6484063863754272 2.3009679317474365 13.15324592590332
Loss :  1.6711286306381226 1.674494981765747 10.04360294342041
Loss :  1.6555514335632324 1.5315845012664795 9.313474655151367
Loss :  1.6464792490005493 2.3664238452911377 13.478598594665527
Loss :  1.6309365034103394 1.9053435325622559 11.157654762268066
Loss :  1.6413006782531738 4.057759761810303 21.930099487304688
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.6493394374847412 4.091954231262207 22.10910987854004
Loss :  1.6481252908706665 3.978407144546509 21.5401611328125
Loss :  1.655761480331421 3.8876070976257324 21.09379768371582
Total LOSS train 12.146371533320501 valid 21.66829204559326
CE LOSS train 1.6665452920473538 valid 0.4139403700828552
Contrastive LOSS train 2.0959652497218206 valid 0.9719017744064331
EPOCH 254:
Loss :  1.6821457147598267 2.00952410697937 11.729766845703125
Loss :  1.6925421953201294 2.225175380706787 12.818419456481934
Loss :  1.6682645082473755 1.7293249368667603 10.314888954162598
Loss :  1.6730533838272095 2.6402363777160645 14.874235153198242
Loss :  1.6867926120758057 2.0022974014282227 11.69827938079834
Loss :  1.6587616205215454 1.895099401473999 11.134259223937988
Loss :  1.6869785785675049 2.1293625831604004 12.33379077911377
Loss :  1.6677383184432983 2.3835673332214355 13.585575103759766
Loss :  1.6603987216949463 1.7662087678909302 10.491442680358887
Loss :  1.6874595880508423 1.76785147190094 10.526717185974121
Loss :  1.6535106897354126 2.0989599227905273 12.148310661315918
Loss :  1.652960181236267 2.5251922607421875 14.278921127319336
Loss :  1.651151418685913 1.7816452980041504 10.559377670288086
Loss :  1.6550933122634888 1.9697917699813843 11.50405216217041
Loss :  1.6989202499389648 2.0974795818328857 12.186318397521973
Loss :  1.6948717832565308 1.7360421419143677 10.375082969665527
Loss :  1.649145483970642 1.7050340175628662 10.174315452575684
Loss :  1.669844150543213 1.476270318031311 9.05119514465332
Loss :  1.6461460590362549 1.958167552947998 11.436984062194824
Loss :  1.6946243047714233 2.2224011421203613 12.806629180908203
  batch 20 loss: 1.6946243047714233, 2.2224011421203613, 12.806629180908203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6650532484054565 2.0248398780822754 11.789252281188965
Loss :  1.6453367471694946 1.64390230178833 9.864848136901855
Loss :  1.6595293283462524 1.5401215553283691 9.360136985778809
Loss :  1.671592354774475 1.6173408031463623 9.758296966552734
Loss :  1.6945618391036987 3.0587644577026367 16.988384246826172
Loss :  1.6617809534072876 2.2386507987976074 12.855035781860352
Loss :  1.6698700189590454 2.2265655994415283 12.802698135375977
Loss :  1.6653410196304321 1.9631961584091187 11.481322288513184
Loss :  1.6249170303344727 1.747783899307251 10.363836288452148
Loss :  1.6921309232711792 2.1887974739074707 12.636117935180664
Loss :  1.6260507106781006 1.912499189376831 11.188547134399414
Loss :  1.6792455911636353 2.0221986770629883 11.790239334106445
Loss :  1.6601691246032715 2.5586133003234863 14.453235626220703
Loss :  1.658774733543396 2.6264607906341553 14.791078567504883
Loss :  1.632054328918457 2.2870283126831055 13.067195892333984
Loss :  1.6436114311218262 1.8420122861862183 10.85367202758789
Loss :  1.6427819728851318 2.5150153636932373 14.217859268188477
Loss :  1.6902861595153809 2.829216241836548 15.836366653442383
Loss :  1.6939940452575684 2.547553062438965 14.431758880615234
Loss :  1.7019152641296387 1.9347479343414307 11.375654220581055
  batch 40 loss: 1.7019152641296387, 1.9347479343414307, 11.375654220581055
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6701916456222534 2.0867409706115723 12.103896141052246
Loss :  1.6600879430770874 1.806425929069519 10.692216873168945
Loss :  1.6514075994491577 2.0146079063415527 11.724447250366211
Loss :  1.6620075702667236 2.1516544818878174 12.420279502868652
Loss :  1.6446648836135864 1.969372034072876 11.491524696350098
Loss :  1.668351411819458 2.2236926555633545 12.78681468963623
Loss :  1.6939268112182617 2.143841505050659 12.413134574890137
Loss :  1.6562827825546265 2.1297433376312256 12.304999351501465
Loss :  1.7051336765289307 1.5230464935302734 9.320365905761719
Loss :  1.661102056503296 1.420351266860962 8.762858390808105
Loss :  1.6868798732757568 1.9565929174423218 11.469844818115234
Loss :  1.6819238662719727 2.1419694423675537 12.39177131652832
Loss :  1.6654447317123413 1.9688494205474854 11.50969123840332
Loss :  1.6880488395690918 2.2115492820739746 12.745796203613281
Loss :  1.65658438205719 2.055300235748291 11.933085441589355
Loss :  1.7048583030700684 2.1704330444335938 12.557024002075195
Loss :  1.6619305610656738 2.328064203262329 13.302251815795898
Loss :  1.64959716796875 2.484039545059204 14.069794654846191
Loss :  1.6617984771728516 2.642247438430786 14.873035430908203
Loss :  1.710847020149231 1.621669054031372 9.819191932678223
  batch 60 loss: 1.710847020149231, 1.621669054031372, 9.819191932678223
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.650193214416504 1.8128540515899658 10.714463233947754
Loss :  1.6726640462875366 2.4238901138305664 13.7921142578125
Loss :  1.657238245010376 2.2709884643554688 13.01218032836914
Loss :  1.6483312845230103 2.1955668926239014 12.626166343688965
Loss :  1.632841944694519 1.6471806764602661 9.868745803833008
Loss :  1.6380469799041748 4.1688151359558105 22.48212242126465
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6463021039962769 4.221396446228027 22.753284454345703
Loss :  1.6447452306747437 4.061509609222412 21.952293395996094
Loss :  1.6520005464553833 3.9693782329559326 21.498891830444336
Total LOSS train 12.040612191420335 valid 22.171648025512695
CE LOSS train 1.6678728928932778 valid 0.4130001366138458
Contrastive LOSS train 2.074547864840581 valid 0.9923445582389832
EPOCH 255:
Loss :  1.6834255456924438 1.9724692106246948 11.545771598815918
Loss :  1.6939390897750854 2.339693784713745 13.39240837097168
Loss :  1.6697890758514404 2.3365321159362793 13.352450370788574
Loss :  1.6745119094848633 1.8480738401412964 10.914880752563477
Loss :  1.687755823135376 1.508155345916748 9.228532791137695
Loss :  1.6598173379898071 1.6349880695343018 9.834757804870605
Loss :  1.6875699758529663 1.888867735862732 11.131908416748047
Loss :  1.668390154838562 1.9151479005813599 11.24413013458252
Loss :  1.6609989404678345 1.867990255355835 11.000950813293457
Loss :  1.6875364780426025 2.4707090854644775 14.041081428527832
Loss :  1.653812050819397 1.743066430091858 10.369144439697266
Loss :  1.6532505750656128 1.9180406332015991 11.243453979492188
Loss :  1.6513266563415527 1.7734960317611694 10.518806457519531
Loss :  1.6552003622055054 2.3503382205963135 13.406890869140625
Loss :  1.698810338973999 2.2074577808380127 12.736099243164062
Loss :  1.6950128078460693 2.044156074523926 11.915793418884277
Loss :  1.649449110031128 1.664581537246704 9.972356796264648
Loss :  1.6700338125228882 1.5497462749481201 9.4187650680542
Loss :  1.6467182636260986 1.6240534782409668 9.766985893249512
Loss :  1.694760799407959 2.225619077682495 12.822856903076172
  batch 20 loss: 1.694760799407959, 2.225619077682495, 12.822856903076172
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.665641188621521 1.8775906562805176 11.053593635559082
Loss :  1.6461769342422485 2.5378267765045166 14.335310935974121
Loss :  1.6602163314819336 2.2176499366760254 12.748465538024902
Loss :  1.6721866130828857 2.3048386573791504 13.196379661560059
Loss :  1.694934368133545 1.9900766611099243 11.645317077636719
Loss :  1.6620700359344482 2.1115517616271973 12.219828605651855
Loss :  1.6702040433883667 2.098916530609131 12.164787292480469
Loss :  1.665549874305725 2.371331214904785 13.52220630645752
Loss :  1.6256868839263916 2.1651697158813477 12.45153522491455
Loss :  1.6921536922454834 2.2163846492767334 12.774077415466309
Loss :  1.62678861618042 1.8288440704345703 10.77100944519043
Loss :  1.6794317960739136 1.681638479232788 10.087624549865723
Loss :  1.6604582071304321 1.901705265045166 11.168984413146973
Loss :  1.6588376760482788 2.089890241622925 12.108288764953613
Loss :  1.6323987245559692 2.3658668994903564 13.461732864379883
Loss :  1.6435621976852417 1.986887812614441 11.578001022338867
Loss :  1.642789363861084 1.9798643589019775 11.542110443115234
Loss :  1.689734935760498 1.8857367038726807 11.118417739868164
Loss :  1.6935830116271973 2.204803943634033 12.717601776123047
Loss :  1.7012348175048828 2.165544033050537 12.528955459594727
  batch 40 loss: 1.7012348175048828, 2.165544033050537, 12.528955459594727
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6698827743530273 2.3130099773406982 13.234932899475098
Loss :  1.6597028970718384 1.9835915565490723 11.57766056060791
Loss :  1.651951551437378 2.2597544193267822 12.950723648071289
Loss :  1.6620464324951172 2.16916561126709 12.507874488830566
Loss :  1.6451818943023682 1.8409985303878784 10.850173950195312
Loss :  1.668677806854248 2.5683600902557373 14.510478973388672
Loss :  1.694015383720398 1.61323881149292 9.760210037231445
Loss :  1.65699303150177 1.69761323928833 10.145058631896973
Loss :  1.7047672271728516 1.682228922843933 10.115911483764648
Loss :  1.6612533330917358 1.5974235534667969 9.648370742797852
Loss :  1.6868141889572144 1.9109625816345215 11.24162769317627
Loss :  1.6815311908721924 1.9174997806549072 11.26902961730957
Loss :  1.6652522087097168 1.830045461654663 10.815479278564453
Loss :  1.6872248649597168 2.4188246726989746 13.781349182128906
Loss :  1.6570119857788086 2.4360947608947754 13.837485313415527
Loss :  1.7040477991104126 2.116118907928467 12.284643173217773
Loss :  1.6620328426361084 2.262474298477173 12.974404335021973
Loss :  1.650255560874939 2.2277188301086426 12.788848876953125
Loss :  1.6620988845825195 2.4107491970062256 13.715845108032227
Loss :  1.7106707096099854 1.996563196182251 11.693486213684082
  batch 60 loss: 1.7106707096099854, 1.996563196182251, 11.693486213684082
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6502735614776611 1.980413794517517 11.552342414855957
Loss :  1.6727489233016968 1.8767532110214233 11.056514739990234
Loss :  1.657130479812622 1.7807382345199585 10.560821533203125
Loss :  1.6482162475585938 2.4189209938049316 13.742820739746094
Loss :  1.63302481174469 1.8669071197509766 10.967560768127441
Loss :  1.6472766399383545 3.955082654953003 21.42268943786621
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6562296152114868 3.8281514644622803 20.796985626220703
Loss :  1.6550352573394775 3.700935125350952 20.159709930419922
Loss :  1.6563782691955566 3.450528860092163 18.909021377563477
Total LOSS train 11.825167817335862 valid 20.322101593017578
CE LOSS train 1.6681316155653734 valid 0.41409456729888916
Contrastive LOSS train 2.0314072462228627 valid 0.8626322150230408
EPOCH 256:
Loss :  1.6829452514648438 2.2436211109161377 12.901050567626953
Loss :  1.6934446096420288 2.131627321243286 12.351580619812012
Loss :  1.6695313453674316 1.7964823246002197 10.65194320678711
Loss :  1.6740981340408325 1.87889564037323 11.068575859069824
Loss :  1.6873843669891357 1.6489741802215576 9.932255744934082
Loss :  1.6597291231155396 1.757926344871521 10.449360847473145
Loss :  1.6873995065689087 2.040504217147827 11.889921188354492
Loss :  1.6685235500335693 1.4553080797195435 8.945063591003418
Loss :  1.661208987236023 1.802288293838501 10.672650337219238
Loss :  1.6874135732650757 1.618446707725525 9.779646873474121
Loss :  1.6541730165481567 2.3396661281585693 13.352503776550293
Loss :  1.6538149118423462 2.0207934379577637 11.757782936096191
Loss :  1.651913046836853 1.7463265657424927 10.383545875549316
Loss :  1.6559096574783325 2.588097095489502 14.596394538879395
Loss :  1.6991873979568481 2.087003231048584 12.134203910827637
Loss :  1.6952390670776367 1.68993079662323 10.144892692565918
Loss :  1.6497763395309448 1.9474928379058838 11.387240409851074
Loss :  1.6700843572616577 1.7816959619522095 10.578563690185547
Loss :  1.6469249725341797 1.5386477708816528 9.340164184570312
Loss :  1.6946910619735718 1.923221230506897 11.310796737670898
  batch 20 loss: 1.6946910619735718, 1.923221230506897, 11.310796737670898
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6656911373138428 1.9273221492767334 11.302302360534668
Loss :  1.6463004350662231 1.8876725435256958 11.084663391113281
Loss :  1.6601107120513916 1.7572219371795654 10.446220397949219
Loss :  1.6722948551177979 2.5807106494903564 14.575847625732422
Loss :  1.695037603378296 2.6369524002075195 14.879799842834473
Loss :  1.6622940301895142 1.7849795818328857 10.58719253540039
Loss :  1.6703323125839233 1.7151762247085571 10.24621295928955
Loss :  1.6655969619750977 1.9906537532806396 11.618865966796875
Loss :  1.62601900100708 2.1320793628692627 12.286415100097656
Loss :  1.6920357942581177 2.503535747528076 14.209714889526367
Loss :  1.6270040273666382 1.984645962715149 11.550233840942383
Loss :  1.679392695426941 1.7574721574783325 10.466753959655762
Loss :  1.660547137260437 1.8656038045883179 10.988566398620605
Loss :  1.6589816808700562 1.8702588081359863 11.010274887084961
Loss :  1.6327015161514282 2.0571799278259277 11.918601989746094
Loss :  1.6438062191009521 1.8203164339065552 10.745388984680176
Loss :  1.6430864334106445 2.530726671218872 14.296719551086426
Loss :  1.6899200677871704 2.4045321941375732 13.712581634521484
Loss :  1.6939384937286377 2.3114638328552246 13.25125789642334
Loss :  1.7014727592468262 2.1386919021606445 12.39493179321289
  batch 40 loss: 1.7014727592468262, 2.1386919021606445, 12.39493179321289
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.67011559009552 1.677894115447998 10.059586524963379
Loss :  1.6600173711776733 1.4653784036636353 8.986908912658691
Loss :  1.6523747444152832 2.4755043983459473 14.029895782470703
Loss :  1.662235140800476 2.3499722480773926 13.41209602355957
Loss :  1.6457326412200928 2.0980677604675293 12.136072158813477
Loss :  1.6689211130142212 2.1118133068084717 12.227988243103027
Loss :  1.6942086219787598 2.054009437561035 11.964256286621094
Loss :  1.6572614908218384 2.307915687561035 13.196840286254883
Loss :  1.7049697637557983 1.7063721418380737 10.23682975769043
Loss :  1.661725640296936 1.7652592658996582 10.488021850585938
Loss :  1.6874492168426514 1.9659518003463745 11.517208099365234
Loss :  1.6824572086334229 1.828213095664978 10.823522567749023
Loss :  1.6663349866867065 1.6556631326675415 9.944650650024414
Loss :  1.6882857084274292 2.3399975299835205 13.388273239135742
Loss :  1.6581099033355713 1.872589111328125 11.021055221557617
Loss :  1.7047301530838013 1.81309974193573 10.770228385925293
Loss :  1.6627320051193237 2.104501485824585 12.185239791870117
Loss :  1.650837779045105 2.09763240814209 12.138999938964844
Loss :  1.662604808807373 2.3075039386749268 13.200124740600586
Loss :  1.7110135555267334 1.7432385683059692 10.427206993103027
  batch 60 loss: 1.7110135555267334, 1.7432385683059692, 10.427206993103027
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6509472131729126 2.0811541080474854 12.056717872619629
Loss :  1.673357605934143 1.9713406562805176 11.530060768127441
Loss :  1.657725214958191 1.8386509418487549 10.850979804992676
Loss :  1.6487488746643066 2.203613758087158 12.666816711425781
Loss :  1.6334819793701172 1.6055270433425903 9.661117553710938
Loss :  1.6411099433898926 4.054928302764893 21.91575050354004
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.649814486503601 3.9847500324249268 21.573564529418945
Loss :  1.64748215675354 3.7985079288482666 20.64002227783203
Loss :  1.6535687446594238 3.6216883659362793 19.76201057434082
Total LOSS train 11.601867411686824 valid 20.97283697128296
CE LOSS train 1.6684051458652203 valid 0.41339218616485596
Contrastive LOSS train 1.986692452430725 valid 0.9054220914840698
EPOCH 257:
Loss :  1.6834964752197266 1.928584337234497 11.326417922973633
Loss :  1.6937793493270874 1.8773053884506226 11.080306053161621
Loss :  1.6699472665786743 1.4301291704177856 8.820592880249023
Loss :  1.6746776103973389 1.887756109237671 11.113458633422852
Loss :  1.6879613399505615 1.991239070892334 11.644157409667969
Loss :  1.6604537963867188 2.604090452194214 14.680906295776367
Loss :  1.6881253719329834 2.086710214614868 12.121676445007324
Loss :  1.6692941188812256 1.663002610206604 9.984307289123535
Loss :  1.6618632078170776 2.4109702110290527 13.716714859008789
Loss :  1.6879780292510986 2.1837775707244873 12.606865882873535
Loss :  1.6548012495040894 2.3531906604766846 13.420754432678223
Loss :  1.654301643371582 1.8994897603988647 11.151750564575195
Loss :  1.6522806882858276 2.2249960899353027 12.777261734008789
Loss :  1.656421184539795 2.545710325241089 14.384973526000977
Loss :  1.6991820335388184 2.450690984725952 13.95263671875
Loss :  1.6954318284988403 2.023627281188965 11.813568115234375
Loss :  1.6503700017929077 2.5196173191070557 14.248456001281738
Loss :  1.6707978248596191 2.347421407699585 13.407905578613281
Loss :  1.6473864316940308 2.321617364883423 13.255473136901855
Loss :  1.694740891456604 1.9817461967468262 11.603472709655762
  batch 20 loss: 1.694740891456604, 1.9817461967468262, 11.603472709655762
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6659846305847168 1.8758312463760376 11.045141220092773
Loss :  1.6466583013534546 2.2931015491485596 13.112165451049805
Loss :  1.6603152751922607 1.8116626739501953 10.718628883361816
Loss :  1.6724830865859985 1.7611232995986938 10.478099822998047
Loss :  1.6952266693115234 2.0251269340515137 11.82086181640625
Loss :  1.6628468036651611 2.275080680847168 13.038249969482422
Loss :  1.6710375547409058 2.24865984916687 12.914337158203125
Loss :  1.6661173105239868 2.186143398284912 12.596835136413574
Loss :  1.6267173290252686 2.4835004806518555 14.044219970703125
Loss :  1.6926451921463013 2.2502715587615967 12.944003105163574
Loss :  1.6278001070022583 2.145519971847534 12.355400085449219
Loss :  1.6800801753997803 2.1188254356384277 12.274208068847656
Loss :  1.661367654800415 2.6186697483062744 14.754715919494629
Loss :  1.659866452217102 1.9434807300567627 11.377269744873047
Loss :  1.6335980892181396 2.125049352645874 12.258845329284668
Loss :  1.6447622776031494 1.7924346923828125 10.606935501098633
Loss :  1.6439263820648193 2.292149543762207 13.104674339294434
Loss :  1.690530776977539 1.8628123998641968 11.004592895507812
Loss :  1.6947147846221924 1.7901005744934082 10.645216941833496
Loss :  1.7020617723464966 1.7152736186981201 10.278429985046387
  batch 40 loss: 1.7020617723464966, 1.7152736186981201, 10.278429985046387
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6705036163330078 2.019019842147827 11.765603065490723
Loss :  1.660388708114624 1.7878735065460205 10.599756240844727
Loss :  1.6523890495300293 2.6528961658477783 14.9168701171875
Loss :  1.6624685525894165 2.4059319496154785 13.69212818145752
Loss :  1.6460598707199097 2.1559813022613525 12.425966262817383
Loss :  1.6692965030670166 2.27712345123291 13.054913520812988
Loss :  1.6942211389541626 2.4987502098083496 14.187973022460938
Loss :  1.6574589014053345 2.5391836166381836 14.353377342224121
Loss :  1.705150842666626 2.366957902908325 13.539939880371094
Loss :  1.6619269847869873 1.8362960815429688 10.84340763092041
Loss :  1.6870964765548706 2.00752329826355 11.724713325500488
Loss :  1.6825026273727417 1.8634908199310303 10.999957084655762
Loss :  1.6664273738861084 1.824500322341919 10.788928985595703
Loss :  1.6885794401168823 1.915305256843567 11.265106201171875
Loss :  1.6577880382537842 1.853351354598999 10.924545288085938
Loss :  1.7053338289260864 1.7201039791107178 10.305853843688965
Loss :  1.6632664203643799 2.0696916580200195 12.011724472045898
Loss :  1.6513136625289917 1.8149164915084839 10.725895881652832
Loss :  1.6629709005355835 2.0907459259033203 12.116700172424316
Loss :  1.7113929986953735 1.7542911767959595 10.48284912109375
  batch 60 loss: 1.7113929986953735, 1.7542911767959595, 10.48284912109375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6516501903533936 2.029829263687134 11.800796508789062
Loss :  1.673738718032837 1.888297200202942 11.115224838256836
Loss :  1.658620834350586 1.7509533166885376 10.413387298583984
Loss :  1.6495722532272339 2.418780565261841 13.743474960327148
Loss :  1.6346189975738525 2.0432252883911133 11.85074520111084
Loss :  1.6429331302642822 4.142807960510254 22.356971740722656
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6522669792175293 4.1933159828186035 22.618846893310547
Loss :  1.6493726968765259 3.978348970413208 21.54111671447754
Loss :  1.654383897781372 4.063196182250977 21.970365524291992
Total LOSS train 12.125143476632925 valid 22.121825218200684
CE LOSS train 1.6688733522708599 valid 0.413595974445343
Contrastive LOSS train 2.091254003231342 valid 1.0157990455627441
EPOCH 258:
Loss :  1.68415367603302 1.7219783067703247 10.294045448303223
Loss :  1.6941232681274414 1.9091496467590332 11.23987102508545
Loss :  1.6708600521087646 1.5016885995864868 9.179303169250488
Loss :  1.6754875183105469 2.2271742820739746 12.811359405517578
Loss :  1.6885569095611572 1.8374230861663818 10.875672340393066
Loss :  1.6612896919250488 1.9717376232147217 11.519977569580078
Loss :  1.6886568069458008 2.1154215335845947 12.265764236450195
Loss :  1.6700235605239868 2.1498351097106934 12.41919994354248
Loss :  1.6626282930374146 2.2726144790649414 13.025700569152832
Loss :  1.688640832901001 1.892962098121643 11.153450965881348
Loss :  1.6558570861816406 2.403419256210327 13.672953605651855
Loss :  1.6554657220840454 2.7770161628723145 15.540546417236328
Loss :  1.6535263061523438 2.124823570251465 12.277644157409668
Loss :  1.65776789188385 1.9317125082015991 11.316330909729004
Loss :  1.7000784873962402 2.264138698577881 13.020772933959961
Loss :  1.6960080862045288 2.3800294399261475 13.596155166625977
Loss :  1.651275396347046 2.4928576946258545 14.115564346313477
Loss :  1.671354055404663 1.7139039039611816 10.240873336791992
Loss :  1.6481573581695557 1.6195307970046997 9.745811462402344
Loss :  1.6952718496322632 2.2309961318969727 12.850252151489258
  batch 20 loss: 1.6952718496322632, 2.2309961318969727, 12.850252151489258
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6668157577514648 1.865142583847046 10.992528915405273
Loss :  1.647387981414795 2.5045790672302246 14.170284271240234
Loss :  1.6610795259475708 2.032132148742676 11.82174015045166
Loss :  1.673331618309021 2.1911141872406006 12.628902435302734
Loss :  1.6958277225494385 2.7413246631622314 15.402450561523438
Loss :  1.6637777090072632 2.4048361778259277 13.687958717346191
Loss :  1.6716505289077759 2.5017895698547363 14.180598258972168
Loss :  1.6671758890151978 1.7977708578109741 10.656030654907227
Loss :  1.6283693313598633 2.0627353191375732 11.942046165466309
Loss :  1.6935919523239136 2.32281494140625 13.307666778564453
Loss :  1.6293237209320068 2.2427704334259033 12.843175888061523
Loss :  1.680925726890564 2.1258232593536377 12.310041427612305
Loss :  1.662345051765442 2.1844968795776367 12.584829330444336
Loss :  1.6608171463012695 2.58778715133667 14.599753379821777
Loss :  1.6346455812454224 2.513143301010132 14.200362205505371
Loss :  1.6459120512008667 1.8319047689437866 10.805436134338379
Loss :  1.6451144218444824 1.8504502773284912 10.89736557006836
Loss :  1.6911334991455078 2.529637098312378 14.339319229125977
Loss :  1.6951526403427124 2.3555402755737305 13.472853660583496
Loss :  1.7025210857391357 2.486783504486084 14.136439323425293
  batch 40 loss: 1.7025210857391357, 2.486783504486084, 14.136439323425293
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6712332963943481 2.145643711090088 12.39945125579834
Loss :  1.6612098217010498 1.9084304571151733 11.203362464904785
Loss :  1.6529871225357056 2.1176395416259766 12.241185188293457
Loss :  1.6628868579864502 2.6170339584350586 14.748056411743164
Loss :  1.6464953422546387 3.2606873512268066 17.949932098388672
Loss :  1.6696860790252686 2.810401439666748 15.721693992614746
Loss :  1.694568395614624 2.031010150909424 11.849618911743164
Loss :  1.6578481197357178 1.784005880355835 10.57787799835205
Loss :  1.7053910493850708 1.6426056623458862 9.918418884277344
Loss :  1.6622264385223389 2.1369588375091553 12.347021102905273
Loss :  1.6874700784683228 2.3111720085144043 13.243330955505371
Loss :  1.6824548244476318 2.207413673400879 12.719523429870605
Loss :  1.6661827564239502 2.530425548553467 14.318310737609863
Loss :  1.6881521940231323 2.30366587638855 13.20648193359375
Loss :  1.657302737236023 2.381605386734009 13.565329551696777
Loss :  1.704821228981018 1.955003023147583 11.479836463928223
Loss :  1.6624794006347656 2.7330446243286133 15.327702522277832
Loss :  1.6504056453704834 2.9842000007629395 16.5714054107666
Loss :  1.6622999906539917 2.93911075592041 16.357852935791016
Loss :  1.7108674049377441 2.274662971496582 13.084182739257812
  batch 60 loss: 1.7108674049377441, 2.274662971496582, 13.084182739257812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.6506098508834839 2.0149850845336914 11.72553539276123
Loss :  1.6728551387786865 1.8941504955291748 11.143608093261719
Loss :  1.6578242778778076 1.9599988460540771 11.457818984985352
Loss :  1.6489015817642212 2.555610179901123 14.426953315734863
Loss :  1.6338036060333252 2.053738594055176 11.902496337890625
Loss :  1.6633732318878174 3.7589128017425537 20.457937240600586
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6723121404647827 3.9255692958831787 21.30015754699707
Loss :  1.6690953969955444 3.669050931930542 20.01435089111328
Loss :  1.6784230470657349 3.8130218982696533 20.743532180786133
Total LOSS train 12.763507989736704 valid 20.628994464874268
CE LOSS train 1.6693391543168288 valid 0.4196057617664337
Contrastive LOSS train 2.218833745442904 valid 0.9532554745674133
EPOCH 259:
Loss :  1.6839154958724976 2.1016921997070312 12.192376136779785
Loss :  1.6939611434936523 3.092438220977783 17.156150817871094
Loss :  1.6704694032669067 2.9678094387054443 16.509517669677734
Loss :  1.6749120950698853 2.972447156906128 16.537147521972656
Loss :  1.6881426572799683 1.7738016843795776 10.557150840759277
Loss :  1.6607328653335571 2.1219747066497803 12.270606994628906
Loss :  1.6882990598678589 2.5898401737213135 14.637499809265137
Loss :  1.6695654392242432 2.1907296180725098 12.623212814331055
Loss :  1.662248969078064 2.0027687549591064 11.676092147827148
Loss :  1.6884822845458984 1.60562264919281 9.716595649719238
Loss :  1.6556065082550049 1.832339882850647 10.817305564880371
Loss :  1.6551461219787598 2.7123632431030273 15.216962814331055
Loss :  1.6529675722122192 1.9593886137008667 11.449910163879395
Loss :  1.6571704149246216 2.0607190132141113 11.96076488494873
Loss :  1.7001945972442627 2.087538957595825 12.13788890838623
Loss :  1.696319818496704 2.161463737487793 12.50363826751709
Loss :  1.6517008543014526 2.667067527770996 14.987038612365723
Loss :  1.6718871593475342 2.5595242977142334 14.46950912475586
Loss :  1.6489453315734863 2.2397000789642334 12.84744644165039
Loss :  1.696274995803833 1.7563244104385376 10.477896690368652
  batch 20 loss: 1.696274995803833, 1.7563244104385376, 10.477896690368652
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.6674985885620117 1.9482648372650146 11.408823013305664
Loss :  1.6483864784240723 1.9654656648635864 11.475715637207031
Loss :  1.6622343063354492 2.040337085723877 11.863919258117676
Loss :  1.6739708185195923 2.318807363510132 13.2680082321167
Loss :  1.6963064670562744 2.0169503688812256 11.781058311462402
Loss :  1.664320468902588 2.233546257019043 12.832052230834961
Loss :  1.6722770929336548 2.244077444076538 12.892664909362793
Loss :  1.6678152084350586 2.467684030532837 14.006235122680664
Loss :  1.6287226676940918 2.7881548404693604 15.569496154785156
Loss :  1.6941397190093994 2.145380973815918 12.42104434967041
Loss :  1.6295855045318604 1.8326901197433472 10.793035507202148
Loss :  1.6813856363296509 1.9502002000808716 11.432387351989746
Loss :  1.662699580192566 2.0953853130340576 12.139626502990723
Loss :  1.6612898111343384 1.860329270362854 10.962936401367188
Loss :  1.6350481510162354 2.255023241043091 12.910163879394531
Loss :  1.6463721990585327 2.1623146533966064 12.457944869995117
Loss :  1.6455752849578857 1.7484612464904785 10.3878812789917
Loss :  1.692006230354309 1.835289716720581 10.868454933166504
Loss :  1.695908546447754 1.7061781883239746 10.226799964904785
Loss :  1.7034757137298584 1.5578068494796753 9.492509841918945
  batch 40 loss: 1.7034757137298584, 1.5578068494796753, 9.492509841918945
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6726244688034058 2.590738534927368 14.626317024230957
Loss :  1.662495732307434 1.912827968597412 11.226635932922363
Loss :  1.654541254043579 2.9803035259246826 16.556058883666992
Loss :  1.6644747257232666 2.7071585655212402 15.200267791748047
Loss :  1.6473422050476074 1.57177734375 9.506229400634766
Loss :  1.6702512502670288 2.550933599472046 14.424919128417969
Loss :  1.6956896781921387 1.91720712184906 11.28172492980957
Loss :  1.6583006381988525 1.8059931993484497 10.68826675415039
Loss :  1.7060115337371826 2.686046838760376 15.136245727539062
Loss :  1.6627134084701538 1.7789627313613892 10.557526588439941
Loss :  1.687760829925537 1.9481958150863647 11.428739547729492
Loss :  1.6832597255706787 2.4315273761749268 13.840896606445312
Loss :  1.6671743392944336 3.144892454147339 17.39163589477539
Loss :  1.6895861625671387 2.707444429397583 15.226808547973633
Loss :  1.6586514711380005 2.4713242053985596 14.01527214050293
Loss :  1.7061407566070557 2.062774896621704 12.020014762878418
Loss :  1.6640257835388184 2.6635429859161377 14.981740951538086
Loss :  1.6521764993667603 2.302696466445923 13.165658950805664
Loss :  1.6639052629470825 2.51554536819458 14.241631507873535
Loss :  1.712100863456726 2.549466609954834 14.459434509277344
  batch 60 loss: 1.712100863456726, 2.549466609954834, 14.459434509277344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.652490496635437 2.7149581909179688 15.22728157043457
Loss :  1.6742910146713257 1.8756661415100098 11.052620887756348
Loss :  1.659401774406433 2.0880448818206787 12.099626541137695
Loss :  1.6503673791885376 2.509626865386963 14.198501586914062
Loss :  1.6354131698608398 1.6145812273025513 9.708319664001465
Loss :  1.6445568799972534 4.087978363037109 22.084447860717773
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6530948877334595 4.061262607574463 21.959407806396484
Loss :  1.6503084897994995 3.9682939052581787 21.491777420043945
Loss :  1.658829689025879 3.9945061206817627 21.631359100341797
Total LOSS train 12.803043776292068 valid 21.791748046875
CE LOSS train 1.6699561797655547 valid 0.4147074222564697
Contrastive LOSS train 2.226617528842046 valid 0.9986265301704407
EPOCH 260:
Loss :  1.685204029083252 1.5874789953231812 9.622598648071289
Loss :  1.6952883005142212 1.718927264213562 10.289924621582031
Loss :  1.6720839738845825 1.4201596975326538 8.772882461547852
Loss :  1.676698088645935 3.074000358581543 17.04669952392578
Loss :  1.6896768808364868 1.695784091949463 10.168597221374512
Loss :  1.662414312362671 1.7047899961471558 10.18636417388916
Loss :  1.689850926399231 2.382460117340088 13.602150917053223
Loss :  1.6711100339889526 2.4700939655303955 14.02157974243164
Loss :  1.6636836528778076 2.441157102584839 13.86946964263916
Loss :  1.689728856086731 1.9449275732040405 11.414366722106934
Loss :  1.6564990282058716 2.2141363620758057 12.727180480957031
Loss :  1.6559107303619385 1.9055752754211426 11.183786392211914
Loss :  1.6536972522735596 1.834795594215393 10.827674865722656
Loss :  1.6577262878417969 1.938717246055603 11.351312637329102
Loss :  1.7001640796661377 2.164900541305542 12.524666786193848
Loss :  1.6966756582260132 2.3411052227020264 13.402201652526855
Loss :  1.6514798402786255 2.7958197593688965 15.630578994750977
Loss :  1.671968936920166 2.5907809734344482 14.625873565673828
Loss :  1.648628830909729 1.7741577625274658 10.519417762756348
Loss :  1.696166753768921 2.602609872817993 14.709216117858887
  batch 20 loss: 1.696166753768921, 2.602609872817993, 14.709216117858887
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.6673632860183716 1.5483719110488892 9.409222602844238
Loss :  1.6480679512023926 1.7127480506896973 10.211807250976562
Loss :  1.6616990566253662 1.5543233156204224 9.43331527709961
Loss :  1.6734565496444702 1.9301705360412598 11.324308395385742
Loss :  1.6960793733596802 2.1151177883148193 12.271668434143066
Loss :  1.6639213562011719 2.439588785171509 13.861865043640137
Loss :  1.6719499826431274 2.3315622806549072 13.329761505126953
Loss :  1.6673953533172607 2.01037859916687 11.71928882598877
Loss :  1.6282752752304077 1.9040237665176392 11.148393630981445
Loss :  1.6939053535461426 1.8454489707946777 10.921150207519531
Loss :  1.6293206214904785 1.963006615638733 11.444353103637695
Loss :  1.681312084197998 1.9391634464263916 11.377128601074219
Loss :  1.6626722812652588 1.7721726894378662 10.52353572845459
Loss :  1.6612284183502197 1.9089057445526123 11.205757141113281
Loss :  1.6348892450332642 2.4164493083953857 13.71713638305664
Loss :  1.6460753679275513 2.611555337905884 14.703851699829102
Loss :  1.6450906991958618 2.8424072265625 15.85712718963623
Loss :  1.6914292573928833 2.7405917644500732 15.394388198852539
Loss :  1.6952990293502808 1.5225785970687866 9.308192253112793
Loss :  1.7028549909591675 2.5525906085968018 14.465807914733887
  batch 40 loss: 1.7028549909591675, 2.5525906085968018, 14.465807914733887
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.6720597743988037 2.6323904991149902 14.834012985229492
Loss :  1.6623225212097168 2.6494829654693604 14.909736633300781
Loss :  1.6542131900787354 2.781791925430298 15.563172340393066
Loss :  1.6643117666244507 2.6189539432525635 14.75908088684082
Loss :  1.6475615501403809 1.7731060981750488 10.513092041015625
Loss :  1.6706764698028564 2.991425037384033 16.6278018951416
Loss :  1.6956889629364014 2.3106863498687744 13.249120712280273
Loss :  1.6585309505462646 1.58859384059906 9.601500511169434
Loss :  1.7060551643371582 2.069735527038574 12.054733276367188
Loss :  1.662777304649353 2.090252637863159 12.11404037475586
Loss :  1.687861442565918 2.4869813919067383 14.12276840209961
Loss :  1.6831963062286377 2.4589791297912598 13.9780912399292
Loss :  1.6672601699829102 1.7876070737838745 10.605295181274414
Loss :  1.6894344091415405 2.606987953186035 14.724373817443848
Loss :  1.6586037874221802 2.5627946853637695 14.472577095031738
Loss :  1.7059766054153442 2.5398688316345215 14.40532112121582
Loss :  1.6641796827316284 2.697852611541748 15.153443336486816
Loss :  1.6520612239837646 2.607952117919922 14.691822052001953
Loss :  1.6641790866851807 2.9653782844543457 16.491069793701172
Loss :  1.7123281955718994 2.4700369834899902 14.06251335144043
  batch 60 loss: 1.7123281955718994, 2.4700369834899902, 14.06251335144043
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6530126333236694 2.3451735973358154 13.378880500793457
Loss :  1.6750893592834473 2.2247560024261475 12.798870086669922
Loss :  1.6599498987197876 2.4886066913604736 14.102983474731445
Loss :  1.651445746421814 2.5186126232147217 14.244508743286133
Loss :  1.636082410812378 1.4785782098770142 9.028973579406738
Loss :  1.6694669723510742 4.139019966125488 22.364566802978516
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6783370971679688 4.22298002243042 22.793237686157227
Loss :  1.6749681234359741 3.98649263381958 21.607431411743164
Loss :  1.6839697360992432 3.940349817276001 21.385719299316406
Total LOSS train 12.74788285768949 valid 22.037738800048828
CE LOSS train 1.6701815476784339 valid 0.4209924340248108
Contrastive LOSS train 2.215540278874911 valid 0.9850874543190002
EPOCH 261:
Loss :  1.6853561401367188 2.368617296218872 13.5284423828125
Loss :  1.6952192783355713 2.148120880126953 12.435823440551758
Loss :  1.6719577312469482 1.9356136322021484 11.35002613067627
Loss :  1.6764665842056274 2.656937599182129 14.96115493774414
Loss :  1.689657211303711 2.6081438064575195 14.730376243591309
Loss :  1.6626627445220947 1.82781183719635 10.801721572875977
Loss :  1.690097689628601 1.7982871532440186 10.681533813476562
Loss :  1.6716068983078003 1.6776076555252075 10.059645652770996
Loss :  1.6644558906555176 2.473036050796509 14.02963638305664
Loss :  1.6908212900161743 2.2371585369110107 12.876614570617676
Loss :  1.657859206199646 2.029261827468872 11.804167747497559
Loss :  1.6574773788452148 1.9629415273666382 11.472185134887695
Loss :  1.6554919481277466 2.48738431930542 14.092413902282715
Loss :  1.6593983173370361 2.56793212890625 14.499058723449707
Loss :  1.701530933380127 2.2798452377319336 13.100757598876953
Loss :  1.6973429918289185 2.3080711364746094 13.237698554992676
Loss :  1.6529275178909302 1.6866940259933472 10.086397171020508
Loss :  1.6729226112365723 1.9731563329696655 11.538703918457031
Loss :  1.6494686603546143 1.889535665512085 11.097146987915039
Loss :  1.6962568759918213 2.1350557804107666 12.371535301208496
  batch 20 loss: 1.6962568759918213, 2.1350557804107666, 12.371535301208496
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.667603850364685 2.287763833999634 13.106422424316406
Loss :  1.6481338739395142 1.79339599609375 10.615114212036133
Loss :  1.661766529083252 2.621047019958496 14.76700210571289
Loss :  1.673677921295166 2.2245728969573975 12.79654312133789
Loss :  1.6961922645568848 2.388512372970581 13.638753890991211
Loss :  1.6640887260437012 1.8413089513778687 10.870634078979492
Loss :  1.672085165977478 2.051412582397461 11.929147720336914
Loss :  1.6675641536712646 2.5451321601867676 14.393224716186523
Loss :  1.628822922706604 2.9160189628601074 16.20891761779785
Loss :  1.6939308643341064 2.9799647331237793 16.5937557220459
Loss :  1.6301043033599854 3.2029480934143066 17.64484405517578
Loss :  1.6814903020858765 5.328434467315674 28.32366180419922
Loss :  1.6630823612213135 3.828706979751587 20.806617736816406
Loss :  1.661608338356018 2.629573106765747 14.809473991394043
Loss :  1.6358720064163208 2.628692150115967 14.779333114624023
Loss :  1.6469810009002686 2.0289714336395264 11.791838645935059
Loss :  1.6461142301559448 2.3320720195770264 13.306474685668945
Loss :  1.6914753913879395 2.0071873664855957 11.727411270141602
Loss :  1.6959192752838135 2.683209180831909 15.11196517944336
Loss :  1.7029823064804077 2.261806011199951 13.012012481689453
  batch 40 loss: 1.7029823064804077, 2.261806011199951, 13.012012481689453
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.672196388244629 2.1707921028137207 12.526156425476074
Loss :  1.6622267961502075 1.9469102621078491 11.396778106689453
Loss :  1.654470443725586 2.2188210487365723 12.748575210571289
Loss :  1.6640558242797852 1.7473210096359253 10.400660514831543
Loss :  1.6478149890899658 1.790860652923584 10.602118492126465
Loss :  1.6707247495651245 1.98880934715271 11.614771842956543
Loss :  1.6952812671661377 1.9066773653030396 11.228668212890625
Loss :  1.6588155031204224 2.558255195617676 14.450091361999512
Loss :  1.7060216665267944 2.2986648082733154 13.199345588684082
Loss :  1.6634142398834229 1.8818351030349731 11.072589874267578
Loss :  1.6883107423782349 2.4684934616088867 14.030777931213379
Loss :  1.6835238933563232 2.745345115661621 15.410249710083008
Loss :  1.6674507856369019 2.5435595512390137 14.385249137878418
Loss :  1.6893147230148315 3.583613395690918 19.60738182067871
Loss :  1.65874183177948 2.1968600749969482 12.64304256439209
Loss :  1.7059733867645264 1.8770798444747925 11.0913724899292
Loss :  1.6644176244735718 2.6655354499816895 14.992094039916992
Loss :  1.6520190238952637 2.271040916442871 13.007223129272461
Loss :  1.664155125617981 2.571566104888916 14.521985054016113
Loss :  1.7118736505508423 2.2976748943328857 13.200248718261719
  batch 60 loss: 1.7118736505508423, 2.2976748943328857, 13.200248718261719
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.6530241966247559 2.5092828273773193 14.199438095092773
Loss :  1.6746551990509033 3.264005422592163 17.99468231201172
Loss :  1.6604530811309814 2.925333261489868 16.287118911743164
Loss :  1.651780605316162 3.54599666595459 19.381765365600586
Loss :  1.6370388269424438 2.758214235305786 15.428110122680664
Loss :  1.6623440980911255 4.281749248504639 23.071090698242188
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.6711372137069702 4.357855796813965 23.460416793823242
Loss :  1.6686252355575562 4.134842872619629 22.342838287353516
Loss :  1.6770623922348022 4.206913471221924 22.71162986755371
Total LOSS train 13.698595135028546 valid 22.896493911743164
CE LOSS train 1.6705573577147264 valid 0.41926559805870056
Contrastive LOSS train 2.4056075517947857 valid 1.051728367805481
EPOCH 262:
Loss :  1.6863901615142822 3.921450138092041 21.29364013671875
Loss :  1.6961358785629272 3.514503002166748 19.268651962280273
Loss :  1.6731749773025513 2.441596508026123 13.881157875061035
Loss :  1.6773489713668823 2.570185661315918 14.528277397155762
Loss :  1.690266728401184 2.2681639194488525 13.031085968017578
Loss :  1.6628118753433228 2.4273667335510254 13.79964542388916
Loss :  1.689824104309082 2.7097203731536865 15.238426208496094
Loss :  1.6714144945144653 2.273881196975708 13.040820121765137
Loss :  1.664499044418335 2.050173044204712 11.915364265441895
Loss :  1.6900700330734253 2.028902769088745 11.83458423614502
Loss :  1.6572871208190918 2.829334020614624 15.803956985473633
Loss :  1.6573196649551392 2.504751682281494 14.18107795715332
Loss :  1.6550462245941162 1.9189715385437012 11.24990463256836
Loss :  1.6591858863830566 2.4319050312042236 13.818710327148438
Loss :  1.7013881206512451 2.3863766193389893 13.633271217346191
Loss :  1.6970646381378174 2.0690531730651855 12.042330741882324
Loss :  1.6530252695083618 2.482684373855591 14.066447257995605
Loss :  1.6724706888198853 2.145045518875122 12.397698402404785
Loss :  1.6500214338302612 2.3010382652282715 13.155213356018066
Loss :  1.6968584060668945 1.8595008850097656 10.994362831115723
  batch 20 loss: 1.6968584060668945, 1.8595008850097656, 10.994362831115723
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6683835983276367 1.7491868734359741 10.414318084716797
Loss :  1.6493520736694336 2.0803866386413574 12.051285743713379
Loss :  1.662864327430725 1.7509607076644897 10.417668342590332
Loss :  1.6747074127197266 2.1793019771575928 12.57121753692627
Loss :  1.6968125104904175 1.9465045928955078 11.429335594177246
Loss :  1.6648705005645752 1.8252971172332764 10.791356086730957
Loss :  1.672601580619812 1.8398462533950806 10.871832847595215
Loss :  1.6685088872909546 2.5505332946777344 14.421175003051758
Loss :  1.6298279762268066 2.6466286182403564 14.862970352172852
Loss :  1.6945273876190186 2.416193962097168 13.775497436523438
Loss :  1.6308943033218384 2.392881155014038 13.595300674438477
Loss :  1.6818747520446777 2.2695608139038086 13.029678344726562
Loss :  1.6637048721313477 2.9347386360168457 16.337398529052734
Loss :  1.6621688604354858 2.401580333709717 13.67007064819336
Loss :  1.6362870931625366 3.083130121231079 17.051937103271484
Loss :  1.6472883224487305 3.1061856746673584 17.1782169342041
Loss :  1.6462291479110718 3.0493247509002686 16.892852783203125
Loss :  1.6917389631271362 2.239858865737915 12.891033172607422
Loss :  1.6957472562789917 2.262507200241089 13.008283615112305
Loss :  1.7032328844070435 2.0934064388275146 12.170265197753906
  batch 40 loss: 1.7032328844070435, 2.0934064388275146, 12.170265197753906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.67277193069458 1.8309005498886108 10.827274322509766
Loss :  1.6632559299468994 2.17633318901062 12.544921875
Loss :  1.6555603742599487 2.6799280643463135 15.055200576782227
Loss :  1.6651606559753418 2.479710578918457 14.063713073730469
Loss :  1.6491613388061523 3.1571121215820312 17.434722900390625
Loss :  1.671998381614685 2.911151885986328 16.227758407592773
Loss :  1.6963069438934326 2.8636207580566406 16.0144100189209
Loss :  1.6604228019714355 2.900620937347412 16.163528442382812
Loss :  1.707132339477539 3.026115655899048 16.837711334228516
Loss :  1.6646549701690674 2.3206803798675537 13.268056869506836
Loss :  1.6893508434295654 1.8386458158493042 10.882579803466797
Loss :  1.6849510669708252 1.9801808595657349 11.585855484008789
Loss :  1.6694122552871704 2.1160483360290527 12.249654769897461
Loss :  1.6908396482467651 2.8258650302886963 15.820164680480957
Loss :  1.6613010168075562 2.29701566696167 13.146379470825195
Loss :  1.7070817947387695 2.0668721199035645 12.041441917419434
Loss :  1.666133999824524 1.990963339805603 11.620950698852539
Loss :  1.6544039249420166 1.8464909791946411 10.886858940124512
Loss :  1.6657501459121704 2.1680855751037598 12.50617790222168
Loss :  1.7127798795700073 2.6048083305358887 14.736822128295898
  batch 60 loss: 1.7127798795700073, 2.6048083305358887, 14.736822128295898
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6538195610046387 2.411135673522949 13.709497451782227
Loss :  1.675187587738037 2.316147804260254 13.255926132202148
Loss :  1.6601959466934204 3.862259864807129 20.971494674682617
Loss :  1.651332139968872 2.5693142414093018 14.497902870178223
Loss :  1.636312484741211 2.013278007507324 11.702702522277832
Loss :  1.664706826210022 3.91511869430542 21.24030113220215
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6735146045684814 3.91888165473938 21.26792335510254
Loss :  1.6702247858047485 3.8667280673980713 21.00386619567871
Loss :  1.6797512769699097 3.67559552192688 20.057727813720703
Total LOSS train 13.763969670809232 valid 20.892454624176025
CE LOSS train 1.6712077599305373 valid 0.4199378192424774
Contrastive LOSS train 2.418552373005794 valid 0.91889888048172
EPOCH 263:
Loss :  1.685255527496338 2.2539992332458496 12.955251693725586
Loss :  1.6949305534362793 2.419407606124878 13.791969299316406
Loss :  1.6719563007354736 2.1657423973083496 12.5006685256958
Loss :  1.676652193069458 2.6011123657226562 14.68221378326416
Loss :  1.68967604637146 2.4011194705963135 13.695273399353027
Loss :  1.6624633073806763 2.5357885360717773 14.341405868530273
Loss :  1.6893473863601685 2.535290002822876 14.36579704284668
Loss :  1.6713497638702393 1.9535330533981323 11.439014434814453
Loss :  1.6642593145370483 2.0845155715942383 12.086836814880371
Loss :  1.6899610757827759 2.0585296154022217 11.982609748840332
Loss :  1.6573753356933594 2.4779248237609863 14.046998977661133
Loss :  1.6573145389556885 2.753448486328125 15.424556732177734
Loss :  1.6552804708480835 2.082425594329834 12.067408561706543
Loss :  1.6593738794326782 2.422579288482666 13.772270202636719
Loss :  1.700770616531372 2.4835762977600098 14.118651390075684
Loss :  1.696524739265442 2.011535406112671 11.754201889038086
Loss :  1.6529631614685059 1.9609922170639038 11.457923889160156
Loss :  1.672540307044983 1.8050330877304077 10.69770622253418
Loss :  1.649914264678955 1.8009188175201416 10.654508590698242
Loss :  1.6965124607086182 2.095010280609131 12.171564102172852
  batch 20 loss: 1.6965124607086182, 2.095010280609131, 12.171564102172852
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.6684627532958984 2.819913387298584 15.768030166625977
Loss :  1.6492908000946045 2.6170616149902344 14.734599113464355
Loss :  1.663253664970398 2.063995838165283 11.983232498168945
Loss :  1.67519211769104 2.2864012718200684 13.107198715209961
Loss :  1.6973700523376465 2.9209985733032227 16.3023624420166
Loss :  1.6655244827270508 2.556443214416504 14.44774055480957
Loss :  1.6732040643692017 1.9782962799072266 11.564685821533203
Loss :  1.6687325239181519 2.1513261795043945 12.425363540649414
Loss :  1.630298376083374 2.1606569290161133 12.43358325958252
Loss :  1.6944146156311035 2.4429473876953125 13.909151077270508
Loss :  1.631250262260437 2.9078636169433594 16.170568466186523
Loss :  1.6822327375411987 2.472956418991089 14.047015190124512
Loss :  1.663944125175476 1.9615437984466553 11.471663475036621
Loss :  1.662500023841858 1.9160094261169434 11.242547988891602
Loss :  1.6368694305419922 2.1077067852020264 12.175403594970703
Loss :  1.6479836702346802 2.285928249359131 13.077625274658203
Loss :  1.6469626426696777 3.110745906829834 17.200693130493164
Loss :  1.6923545598983765 2.8299174308776855 15.841941833496094
Loss :  1.696494460105896 2.2111525535583496 12.752257347106934
Loss :  1.703702688217163 2.12479567527771 12.327681541442871
  batch 40 loss: 1.703702688217163, 2.12479567527771, 12.327681541442871
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.672871470451355 2.393697500228882 13.641359329223633
Loss :  1.6630194187164307 2.3470263481140137 13.398151397705078
Loss :  1.6548610925674438 1.8661190271377563 10.985456466674805
Loss :  1.6648309230804443 1.7851635217666626 10.590648651123047
Loss :  1.6486644744873047 1.5132635831832886 9.214982986450195
Loss :  1.6715987920761108 1.9434916973114014 11.389057159423828
Loss :  1.6961514949798584 1.5221372842788696 9.306838035583496
Loss :  1.6599067449569702 1.5043009519577026 9.181411743164062
Loss :  1.7069084644317627 1.9618631601333618 11.516223907470703
Loss :  1.6644924879074097 2.153919219970703 12.434088706970215
Loss :  1.6893417835235596 2.325951337814331 13.319098472595215
Loss :  1.6851001977920532 2.2279396057128906 12.824798583984375
Loss :  1.6695858240127563 1.960307002067566 11.471120834350586
Loss :  1.691405177116394 2.380272388458252 13.592766761779785
Loss :  1.661049246788025 2.3043527603149414 13.182812690734863
Loss :  1.7080790996551514 2.0861523151397705 12.138840675354004
Loss :  1.6665964126586914 2.5859389305114746 14.596291542053223
Loss :  1.6542894840240479 2.831974983215332 15.814164161682129
Loss :  1.6662623882293701 2.035639762878418 11.844461441040039
Loss :  1.7136727571487427 1.7589432001113892 10.50838851928711
  batch 60 loss: 1.7136727571487427, 1.7589432001113892, 10.50838851928711
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.6549873352050781 1.742106318473816 10.365518569946289
Loss :  1.6763702630996704 1.6616755723953247 9.984748840332031
Loss :  1.6618537902832031 1.8481688499450684 10.902698516845703
Loss :  1.653122067451477 2.96760630607605 16.491153717041016
Loss :  1.6386382579803467 2.0298402309417725 11.787839889526367
Loss :  1.6544748544692993 3.9331438541412354 21.320194244384766
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.6626442670822144 3.965484857559204 21.490068435668945
Loss :  1.6608514785766602 3.9546825885772705 21.43426513671875
Loss :  1.664292812347412 3.8525402545928955 20.92699432373047
Total LOSS train 12.791893812326284 valid 21.292880535125732
CE LOSS train 1.6713556729830228 valid 0.416073203086853
Contrastive LOSS train 2.224107608428368 valid 0.9631350636482239
EPOCH 264:
Loss :  1.6871604919433594 1.5130246877670288 9.252284049987793
Loss :  1.6968684196472168 1.608357548713684 9.738656997680664
Loss :  1.6742721796035767 1.324955701828003 8.299051284790039
Loss :  1.6786861419677734 1.7219902276992798 10.288637161254883
Loss :  1.6915279626846313 2.6449453830718994 14.916254997253418
Loss :  1.6649717092514038 1.7140412330627441 10.235177040100098
Loss :  1.6917644739151 1.9315365552902222 11.349447250366211
Loss :  1.6735155582427979 2.259263038635254 12.969830513000488
Loss :  1.6663135290145874 1.5051462650299072 9.192044258117676
Loss :  1.6918174028396606 2.2181918621063232 12.782776832580566
Loss :  1.6597726345062256 1.8176552057266235 10.748048782348633
Loss :  1.6593798398971558 1.6937180757522583 10.127970695495605
Loss :  1.6574170589447021 1.8823860883712769 11.069347381591797
Loss :  1.6614445447921753 2.7195932865142822 15.259410858154297
Loss :  1.7027558088302612 2.6233112812042236 14.81931209564209
Loss :  1.6990654468536377 2.5846829414367676 14.622479438781738
Loss :  1.6556181907653809 2.434511423110962 13.828174591064453
Loss :  1.6753355264663696 2.2647957801818848 12.999314308166504
Loss :  1.652651309967041 1.8886568546295166 11.095935821533203
Loss :  1.698486566543579 2.276057720184326 13.078775405883789
  batch 20 loss: 1.698486566543579, 2.276057720184326, 13.078775405883789
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6708030700683594 1.6856673955917358 10.099140167236328
Loss :  1.6519403457641602 2.42284893989563 13.76618480682373
Loss :  1.6655546426773071 1.9160321950912476 11.245716094970703
Loss :  1.6771810054779053 2.1311025619506836 12.332694053649902
Loss :  1.6990727186203003 1.9536508321762085 11.467327117919922
Loss :  1.6679682731628418 1.7791059017181396 10.563497543334961
Loss :  1.6754862070083618 2.297175168991089 13.161362648010254
Loss :  1.6711077690124512 1.958901286125183 11.465614318847656
Loss :  1.6330502033233643 1.8443461656570435 10.854781150817871
Loss :  1.6964863538742065 1.901415467262268 11.203563690185547
Loss :  1.6338235139846802 2.6417107582092285 14.842376708984375
Loss :  1.6842029094696045 2.5853188037872314 14.610796928405762
Loss :  1.6659674644470215 2.5611720085144043 14.47182846069336
Loss :  1.6644997596740723 2.0440640449523926 11.884819030761719
Loss :  1.6388170719146729 2.5106725692749023 14.192179679870605
Loss :  1.6497259140014648 2.612269163131714 14.711071968078613
Loss :  1.648586392402649 2.0064404010772705 11.680788040161133
Loss :  1.6940717697143555 2.3584187030792236 13.486165046691895
Loss :  1.697689414024353 2.3384745121002197 13.390061378479004
Loss :  1.7050695419311523 2.301547050476074 13.212804794311523
  batch 40 loss: 1.7050695419311523, 2.301547050476074, 13.212804794311523
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.6745877265930176 2.384324789047241 13.596212387084961
Loss :  1.6647604703903198 2.5745842456817627 14.537681579589844
Loss :  1.6564804315567017 2.56142520904541 14.463606834411621
Loss :  1.6666563749313354 1.6438140869140625 9.885726928710938
Loss :  1.6505869626998901 1.4630990028381348 8.966081619262695
Loss :  1.6732491254806519 1.6196011304855347 9.771254539489746
Loss :  1.6976326704025269 1.5071498155593872 9.233381271362305
Loss :  1.6617116928100586 1.4609841108322144 8.966632843017578
Loss :  1.7086821794509888 2.103377342224121 12.225568771362305
Loss :  1.6665713787078857 1.897538423538208 11.154263496398926
Loss :  1.691028356552124 1.669219732284546 10.037127494812012
Loss :  1.6869871616363525 1.6401342153549194 9.88765811920166
Loss :  1.671714186668396 1.604826807975769 9.695847511291504
Loss :  1.6933523416519165 2.066357374191284 12.025139808654785
Loss :  1.6631762981414795 2.0820980072021484 12.0736665725708
Loss :  1.7098095417022705 2.0551154613494873 11.985386848449707
Loss :  1.6687333583831787 2.158292531967163 12.460196495056152
Loss :  1.6567875146865845 2.5095975399017334 14.2047758102417
Loss :  1.6681486368179321 2.8181889057159424 15.759093284606934
Loss :  1.7147098779678345 2.1395182609558105 12.412302017211914
  batch 60 loss: 1.7147098779678345, 2.1395182609558105, 12.412302017211914
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6569480895996094 2.1550791263580322 12.432343482971191
Loss :  1.6777704954147339 1.8598817586898804 10.977179527282715
Loss :  1.6633614301681519 2.349531888961792 13.41102123260498
Loss :  1.6543058156967163 2.8189899921417236 15.749255180358887
Loss :  1.6397714614868164 1.7971076965332031 10.625309944152832
Loss :  1.6443712711334229 4.180880069732666 22.548770904541016
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.6533266305923462 4.099203586578369 22.149343490600586
Loss :  1.6500657796859741 4.270976543426514 23.00494956970215
Loss :  1.658966064453125 3.8637969493865967 20.977951049804688
Total LOSS train 12.090037184495193 valid 22.17025375366211
CE LOSS train 1.6733454264127292 valid 0.41474151611328125
Contrastive LOSS train 2.0833383468481212 valid 0.9659492373466492
EPOCH 265:
Loss :  1.6879661083221436 1.528908133506775 9.332507133483887
Loss :  1.6975769996643066 1.839311122894287 10.894132614135742
Loss :  1.675032615661621 1.5327919721603394 9.338993072509766
Loss :  1.6795209646224976 2.190462589263916 12.63183307647705
Loss :  1.6923195123672485 2.6015784740448 14.700212478637695
Loss :  1.6658916473388672 2.512920379638672 14.230493545532227
Loss :  1.692468285560608 2.565497875213623 14.51995849609375
Loss :  1.6743413209915161 2.0926530361175537 12.137606620788574
Loss :  1.667110800743103 2.3230185508728027 13.282203674316406
Loss :  1.6922411918640137 1.465222716331482 9.018354415893555
Loss :  1.6605393886566162 2.2479302883148193 12.900191307067871
Loss :  1.6602174043655396 2.2259674072265625 12.790054321289062
Loss :  1.658088207244873 2.078967571258545 12.052927017211914
Loss :  1.6621164083480835 2.824310779571533 15.783669471740723
Loss :  1.7030619382858276 1.7181370258331299 10.293746948242188
Loss :  1.6998941898345947 2.1347784996032715 12.373786926269531
Loss :  1.6563371419906616 2.1508920192718506 12.410797119140625
Loss :  1.676252841949463 1.6672923564910889 10.012714385986328
Loss :  1.653619408607483 1.6027045249938965 9.667142868041992
Loss :  1.6992472410202026 2.0134341716766357 11.76641845703125
  batch 20 loss: 1.6992472410202026, 2.0134341716766357, 11.76641845703125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.671949863433838 1.973656177520752 11.540229797363281
Loss :  1.653247356414795 2.0073165893554688 11.689830780029297
Loss :  1.6663286685943604 2.059809923171997 11.965377807617188
Loss :  1.6778020858764648 1.473189115524292 9.043747901916504
Loss :  1.699410080909729 2.245051145553589 12.924666404724121
Loss :  1.668588399887085 2.1234259605407715 12.28571891784668
Loss :  1.6761152744293213 2.220407724380493 12.778153419494629
Loss :  1.6714802980422974 1.9433823823928833 11.388392448425293
Loss :  1.6338413953781128 2.1531779766082764 12.399731636047363
Loss :  1.6966077089309692 1.9545559883117676 11.46938705444336
Loss :  1.6346529722213745 1.9252736568450928 11.261021614074707
Loss :  1.6847097873687744 2.1364691257476807 12.36705493927002
Loss :  1.6667671203613281 2.199368953704834 12.663612365722656
Loss :  1.6654632091522217 2.3770229816436768 13.550578117370605
Loss :  1.6402205228805542 3.1765387058258057 17.522912979125977
Loss :  1.6510919332504272 2.887655019760132 16.089366912841797
Loss :  1.6502726078033447 2.879647970199585 16.048513412475586
Loss :  1.6947888135910034 2.402966260910034 13.709620475769043
Loss :  1.6986722946166992 1.6372939348220825 9.88514232635498
Loss :  1.7056186199188232 1.6201140880584717 9.80618953704834
  batch 40 loss: 1.7056186199188232, 1.6201140880584717, 9.80618953704834
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.6751067638397217 2.619988441467285 14.775049209594727
Loss :  1.6651562452316284 1.8801859617233276 11.066085815429688
Loss :  1.6568385362625122 2.1353654861450195 12.33366584777832
Loss :  1.6668003797531128 2.400723934173584 13.67042064666748
Loss :  1.6505752801895142 1.4155515432357788 8.728333473205566
Loss :  1.6731318235397339 1.9479278326034546 11.412771224975586
Loss :  1.6975631713867188 2.0869507789611816 12.132316589355469
Loss :  1.661686897277832 1.7888405323028564 10.605889320373535
Loss :  1.7082666158676147 1.7713876962661743 10.565204620361328
Loss :  1.6662503480911255 1.9133304357528687 11.232902526855469
Loss :  1.6908730268478394 1.8859057426452637 11.120402336120605
Loss :  1.6862831115722656 1.9298759698867798 11.335662841796875
Loss :  1.6708945035934448 1.8429728746414185 10.885759353637695
Loss :  1.692461609840393 2.1184890270233154 12.284907341003418
Loss :  1.6620535850524902 1.8922924995422363 11.123516082763672
Loss :  1.7085260152816772 1.8722264766693115 11.069658279418945
Loss :  1.6672722101211548 2.0635604858398438 11.985074996948242
Loss :  1.6551330089569092 2.455693244934082 13.933599472045898
Loss :  1.6668134927749634 2.803793430328369 15.68578052520752
Loss :  1.7136900424957275 2.4231324195861816 13.829351425170898
  batch 60 loss: 1.7136900424957275, 2.4231324195861816, 13.829351425170898
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.6555767059326172 1.9178575277328491 11.244864463806152
Loss :  1.677034854888916 1.8041304349899292 10.697687149047852
Loss :  1.6622531414031982 1.7285956144332886 10.305231094360352
Loss :  1.6537318229675293 2.926753282546997 16.287498474121094
Loss :  1.6388342380523682 2.29803466796875 13.129007339477539
Loss :  1.6684644222259521 4.207934379577637 22.70813751220703
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.677111029624939 4.26027774810791 22.978498458862305
Loss :  1.6739944219589233 4.1619553565979 22.4837703704834
Loss :  1.6829650402069092 4.044968605041504 21.907806396484375
Total LOSS train 12.184117434575008 valid 22.519553184509277
CE LOSS train 1.673604308641874 valid 0.4207412600517273
Contrastive LOSS train 2.1021026079471294 valid 1.011242151260376
EPOCH 266:
Loss :  1.6869666576385498 1.9871023893356323 11.622478485107422
Loss :  1.6966959238052368 1.812472939491272 10.759060859680176
Loss :  1.6740533113479614 1.3914035558700562 8.631071090698242
Loss :  1.678423285484314 3.263453722000122 17.995691299438477
Loss :  1.6914944648742676 1.7639105319976807 10.51104736328125
Loss :  1.6647661924362183 2.586686134338379 14.598196983337402
Loss :  1.6915104389190674 2.6393966674804688 14.888493537902832
Loss :  1.6734609603881836 2.388578176498413 13.616352081298828
Loss :  1.666736125946045 2.0204505920410156 11.768989562988281
Loss :  1.6921855211257935 2.4429612159729004 13.906991004943848
Loss :  1.6597299575805664 2.5300557613372803 14.310009002685547
Loss :  1.6594574451446533 2.1225523948669434 12.27221965789795
Loss :  1.657647728919983 2.6929116249084473 15.12220573425293
Loss :  1.6615686416625977 2.154268980026245 12.432913780212402
Loss :  1.7030161619186401 2.423051357269287 13.818273544311523
Loss :  1.6985489130020142 2.6863033771514893 15.13006591796875
Loss :  1.6555978059768677 2.4627294540405273 13.969244956970215
Loss :  1.6745784282684326 2.365482807159424 13.501992225646973
Loss :  1.6527796983718872 1.751024603843689 10.407902717590332
Loss :  1.6985806226730347 2.001934766769409 11.70825481414795
  batch 20 loss: 1.6985806226730347, 2.001934766769409, 11.70825481414795
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6711246967315674 2.591150999069214 14.626879692077637
Loss :  1.6524440050125122 2.278850793838501 13.046697616577148
Loss :  1.6659526824951172 2.164679765701294 12.489351272583008
Loss :  1.677187204360962 2.0911805629730225 12.133090019226074
Loss :  1.698742389678955 1.9652048349380493 11.52476692199707
Loss :  1.6674914360046387 1.7977638244628906 10.65631103515625
Loss :  1.674888014793396 2.534179210662842 14.345784187316895
Loss :  1.6706132888793945 3.3813323974609375 18.577274322509766
Loss :  1.6322627067565918 2.606370210647583 14.664113998413086
Loss :  1.6956056356430054 2.315793752670288 13.274574279785156
Loss :  1.633333444595337 2.020453691482544 11.735601425170898
Loss :  1.6837246417999268 2.0685698986053467 12.02657413482666
Loss :  1.6653938293457031 2.5889394283294678 14.610091209411621
Loss :  1.6640151739120483 2.7763593196868896 15.545811653137207
Loss :  1.6391255855560303 2.857663869857788 15.927445411682129
Loss :  1.649929404258728 3.601435899734497 19.657108306884766
Loss :  1.6493088006973267 2.1413261890411377 12.355939865112305
Loss :  1.694130778312683 2.513077974319458 14.259520530700684
Loss :  1.6977288722991943 2.192033290863037 12.657896041870117
Loss :  1.704868197441101 1.9679685831069946 11.544711112976074
  batch 40 loss: 1.704868197441101, 1.9679685831069946, 11.544711112976074
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.67495596408844 2.4467437267303467 13.908675193786621
Loss :  1.6652746200561523 2.185871124267578 12.594630241394043
Loss :  1.6579201221466064 2.0074164867401123 11.695002555847168
Loss :  1.6673779487609863 1.6223374605178833 9.77906608581543
Loss :  1.6516722440719604 1.8844516277313232 11.073930740356445
Loss :  1.6739592552185059 2.2223751544952393 12.785835266113281
Loss :  1.6981500387191772 2.4385008811950684 13.890654563903809
Loss :  1.6630913019180298 3.1375925540924072 17.35105323791504
Loss :  1.7087385654449463 3.5661368370056152 19.5394229888916
Loss :  1.667646884918213 2.291245460510254 13.12387466430664
Loss :  1.6922603845596313 2.321396827697754 13.29924488067627
Loss :  1.6873184442520142 3.5032949447631836 19.203794479370117
Loss :  1.6720283031463623 2.2081844806671143 12.712950706481934
Loss :  1.6926915645599365 4.394545555114746 23.665420532226562
Loss :  1.664110541343689 3.5976462364196777 19.652341842651367
Loss :  1.7092760801315308 2.344116449356079 13.429858207702637
Loss :  1.6690421104431152 2.499490976333618 14.166496276855469
Loss :  1.657471776008606 2.405879259109497 13.686867713928223
Loss :  1.6687794923782349 2.52083683013916 14.272963523864746
Loss :  1.7146005630493164 1.7154887914657593 10.292044639587402
  batch 60 loss: 1.7146005630493164, 1.7154887914657593, 10.292044639587402
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6576645374298096 1.8547272682189941 10.931300163269043
Loss :  1.6788623332977295 1.816167950630188 10.759702682495117
Loss :  1.6642206907272339 1.7554527521133423 10.441484451293945
Loss :  1.6558148860931396 2.8392155170440674 15.851892471313477
Loss :  1.6414047479629517 2.1183505058288574 12.233158111572266
Loss :  1.668264627456665 4.149203300476074 22.414281845092773
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.6764520406723022 4.07766580581665 22.064781188964844
Loss :  1.674930453300476 4.048643589019775 21.918148040771484
Loss :  1.6807806491851807 4.092254638671875 22.142053604125977
Total LOSS train 13.645733290452224 valid 22.13481616973877
CE LOSS train 1.6735384996120746 valid 0.42019516229629517
Contrastive LOSS train 2.3944389416621283 valid 1.0230636596679688
EPOCH 267:
Loss :  1.688661813735962 2.5349435806274414 14.36337947845459
Loss :  1.6980093717575073 2.58896803855896 14.642849922180176
Loss :  1.6754733324050903 1.87671959400177 11.05907154083252
Loss :  1.6797293424606323 1.9621108770370483 11.490283966064453
Loss :  1.6924248933792114 1.773720145225525 10.561025619506836
Loss :  1.6660723686218262 1.9487513303756714 11.409828186035156
Loss :  1.6921724081039429 2.1189002990722656 12.286673545837402
Loss :  1.6743249893188477 2.0633130073547363 11.990889549255371
Loss :  1.6673096418380737 2.571869373321533 14.526656150817871
Loss :  1.6921578645706177 2.4092447757720947 13.738381385803223
Loss :  1.6604948043823242 2.7085256576538086 15.203123092651367
Loss :  1.6600157022476196 2.2805657386779785 13.062844276428223
Loss :  1.6583343744277954 1.7178879976272583 10.247775077819824
Loss :  1.6623880863189697 1.9779402017593384 11.552088737487793
Loss :  1.7031192779541016 1.7786785364151 10.596511840820312
Loss :  1.6993266344070435 2.460244655609131 14.000550270080566
Loss :  1.6566368341445923 2.5334105491638184 14.323690414428711
Loss :  1.6757498979568481 2.4409217834472656 13.880358695983887
Loss :  1.6538692712783813 1.8304903507232666 10.806321144104004
Loss :  1.6993262767791748 1.5993274450302124 9.695963859558105
  batch 20 loss: 1.6993262767791748, 1.5993274450302124, 9.695963859558105
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.6720144748687744 1.5164945125579834 9.254487037658691
Loss :  1.6532539129257202 2.335191249847412 13.32921028137207
Loss :  1.6663973331451416 2.2582013607025146 12.957404136657715
Loss :  1.6779539585113525 1.8964130878448486 11.160018920898438
Loss :  1.6995331048965454 2.3985610008239746 13.692338943481445
Loss :  1.6688820123672485 1.7618205547332764 10.477985382080078
Loss :  1.6763916015625 1.6557193994522095 9.954988479614258
Loss :  1.6722235679626465 2.013282060623169 11.73863410949707
Loss :  1.6346840858459473 1.9285998344421387 11.27768325805664
Loss :  1.697353720664978 1.9818583726882935 11.606645584106445
Loss :  1.6356008052825928 1.9413468837738037 11.34233570098877
Loss :  1.6852697134017944 1.5929468870162964 9.650004386901855
Loss :  1.6674858331680298 2.579318046569824 14.56407642364502
Loss :  1.6659048795700073 2.6723692417144775 15.027750968933105
Loss :  1.6406649351119995 1.8781791925430298 11.031560897827148
Loss :  1.65142822265625 2.4147026538848877 13.72494125366211
Loss :  1.650471806526184 2.2050251960754395 12.675597190856934
Loss :  1.6946877241134644 2.8237123489379883 15.813249588012695
Loss :  1.6983920335769653 2.7055015563964844 15.225899696350098
Loss :  1.7055259943008423 1.811521053314209 10.763132095336914
  batch 40 loss: 1.7055259943008423, 1.811521053314209, 10.763132095336914
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6758267879486084 2.115509510040283 12.253374099731445
Loss :  1.6661268472671509 1.9167343378067017 11.249798774719238
Loss :  1.6583932638168335 2.2213425636291504 12.765105247497559
Loss :  1.668039083480835 2.1273105144500732 12.30459213256836
Loss :  1.6523776054382324 1.55655038356781 9.435129165649414
Loss :  1.674919605255127 2.435103178024292 13.850435256958008
Loss :  1.6990292072296143 2.1954891681671143 12.676474571228027
Loss :  1.6637009382247925 2.3902974128723145 13.615187644958496
Loss :  1.7095812559127808 2.583209276199341 14.625627517700195
Loss :  1.6681232452392578 2.364014148712158 13.48819351196289
Loss :  1.692399501800537 2.3289260864257812 13.337030410766602
Loss :  1.6879459619522095 1.970094084739685 11.538416862487793
Loss :  1.6729297637939453 1.9251072406768799 11.298465728759766
Loss :  1.693803071975708 2.615678548812866 14.772195816040039
Loss :  1.6645644903182983 2.3966050148010254 13.647588729858398
Loss :  1.7095839977264404 2.0579466819763184 11.99931812286377
Loss :  1.6695646047592163 2.338440179824829 13.361764907836914
Loss :  1.657976508140564 1.9152990579605103 11.234471321105957
Loss :  1.6692981719970703 2.3256030082702637 13.297313690185547
Loss :  1.7151331901550293 1.9380691051483154 11.405479431152344
  batch 60 loss: 1.7151331901550293, 1.9380691051483154, 11.405479431152344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6578861474990845 2.1110408306121826 12.213090896606445
Loss :  1.678850531578064 2.492680311203003 14.142251968383789
Loss :  1.6642465591430664 2.3789050579071045 13.558772087097168
Loss :  1.6554938554763794 2.6740028858184814 15.025507926940918
Loss :  1.6410391330718994 2.0757830142974854 12.019953727722168
Loss :  1.655135989189148 3.999228000640869 21.651275634765625
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6634255647659302 3.978780508041382 21.557327270507812
Loss :  1.6617209911346436 3.788264274597168 20.603042602539062
Loss :  1.666817307472229 3.816009998321533 20.74686622619629
Total LOSS train 12.520334625244141 valid 21.139627933502197
CE LOSS train 1.6744084651653584 valid 0.41670432686805725
Contrastive LOSS train 2.1691852312821607 valid 0.9540024995803833
EPOCH 268:
Loss :  1.6881722211837769 2.274562358856201 13.06098461151123
Loss :  1.6974376440048218 2.3894248008728027 13.644561767578125
Loss :  1.6754789352416992 2.342310905456543 13.387033462524414
Loss :  1.6800541877746582 1.9276957511901855 11.318532943725586
Loss :  1.692882776260376 1.7641364336013794 10.513565063476562
Loss :  1.6668285131454468 1.8265010118484497 10.799333572387695
Loss :  1.6928294897079468 2.0638747215270996 12.012203216552734
Loss :  1.6754578351974487 1.8369535207748413 10.860225677490234
Loss :  1.6686170101165771 2.368361234664917 13.51042366027832
Loss :  1.6933882236480713 2.1186671257019043 12.286724090576172
Loss :  1.6621308326721191 2.6712419986724854 15.018341064453125
Loss :  1.6617538928985596 1.8931249380111694 11.127378463745117
Loss :  1.659861445426941 1.9257285594940186 11.288504600524902
Loss :  1.6638163328170776 2.1005454063415527 12.166543960571289
Loss :  1.703553318977356 2.2452428340911865 12.929767608642578
Loss :  1.700002670288086 2.0683770179748535 12.041887283325195
Loss :  1.6579458713531494 2.137401580810547 12.344953536987305
Loss :  1.6763445138931274 1.9344706535339355 11.348698616027832
Loss :  1.6547425985336304 2.22391414642334 12.774312973022461
Loss :  1.69991934299469 2.5203404426574707 14.301621437072754
  batch 20 loss: 1.69991934299469, 2.5203404426574707, 14.301621437072754
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6730705499649048 3.076199531555176 17.054067611694336
Loss :  1.654084324836731 2.390822410583496 13.608196258544922
Loss :  1.66704523563385 2.1125781536102295 12.229936599731445
Loss :  1.6786681413650513 1.8645528554916382 11.001432418823242
Loss :  1.7001233100891113 2.370509624481201 13.552671432495117
Loss :  1.6694375276565552 1.971052885055542 11.524702072143555
Loss :  1.6764775514602661 1.9025859832763672 11.189407348632812
Loss :  1.6719229221343994 2.577678918838501 14.560317039489746
Loss :  1.6343932151794434 2.85139536857605 15.89137077331543
Loss :  1.6964229345321655 2.7268550395965576 15.330698013305664
Loss :  1.6354228258132935 2.4691460132598877 13.981152534484863
Loss :  1.6847026348114014 2.44746994972229 13.922052383422852
Loss :  1.6667410135269165 2.4358267784118652 13.84587574005127
Loss :  1.6655367612838745 2.851424217224121 15.92265796661377
Loss :  1.6403863430023193 2.6584973335266113 14.932872772216797
Loss :  1.6510589122772217 2.443993330001831 13.871026039123535
Loss :  1.6501694917678833 2.4235711097717285 13.768024444580078
Loss :  1.6941235065460205 2.4987998008728027 14.188122749328613
Loss :  1.6979957818984985 2.5630645751953125 14.51331901550293
Loss :  1.7047688961029053 1.9701271057128906 11.555404663085938
  batch 40 loss: 1.7047688961029053, 1.9701271057128906, 11.555404663085938
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6749178171157837 2.240797281265259 12.878904342651367
Loss :  1.6647804975509644 1.81768000125885 10.753180503845215
Loss :  1.6572297811508179 2.4128289222717285 13.721373558044434
Loss :  1.66672682762146 2.2210731506347656 12.772092819213867
Loss :  1.6510992050170898 1.658686876296997 9.944533348083496
Loss :  1.6735481023788452 2.5830681324005127 14.588888168334961
Loss :  1.6979373693466187 1.8225395679473877 10.81063461303711
Loss :  1.6624772548675537 1.9351080656051636 11.338017463684082
Loss :  1.7087899446487427 1.9338154792785645 11.377866744995117
Loss :  1.667143702507019 1.8118476867675781 10.7263822555542
Loss :  1.6917258501052856 2.0385684967041016 11.884568214416504
Loss :  1.6872038841247559 1.949361801147461 11.434013366699219
Loss :  1.672169804573059 2.109056234359741 12.217451095581055
Loss :  1.692987322807312 2.329334259033203 13.339658737182617
Loss :  1.6638025045394897 1.8645310401916504 10.986456871032715
Loss :  1.7093199491500854 1.9492549896240234 11.455595016479492
Loss :  1.6687967777252197 2.1883671283721924 12.610631942749023
Loss :  1.657006859779358 1.865431547164917 10.98416519165039
Loss :  1.6682710647583008 2.258089303970337 12.958717346191406
Loss :  1.7146172523498535 2.6199352741241455 14.814292907714844
  batch 60 loss: 1.7146172523498535, 2.6199352741241455, 14.814292907714844
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6572833061218262 2.748438835144043 15.399477005004883
Loss :  1.6782050132751465 2.046405792236328 11.910234451293945
Loss :  1.664214015007019 1.9598478078842163 11.46345329284668
Loss :  1.655521273612976 3.002650737762451 16.66877555847168
Loss :  1.6413192749023438 2.451481819152832 13.898728370666504
Loss :  1.6803858280181885 4.235584735870361 22.858308792114258
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.688697338104248 4.3313679695129395 23.345537185668945
Loss :  1.685515284538269 4.176621437072754 22.568620681762695
Loss :  1.694979190826416 4.07161808013916 22.053068161010742
Total LOSS train 12.832569210345929 valid 22.70638370513916
CE LOSS train 1.674320987554697 valid 0.423744797706604
Contrastive LOSS train 2.231649640890268 valid 1.01790452003479
EPOCH 269:
Loss :  1.6887524127960205 1.8134629726409912 10.756067276000977
Loss :  1.6983054876327515 1.8675695657730103 11.036152839660645
Loss :  1.6761853694915771 1.750834345817566 10.430356979370117
Loss :  1.680507779121399 2.035362482070923 11.857319831848145
Loss :  1.693066120147705 1.7385598421096802 10.385866165161133
Loss :  1.6667276620864868 1.9403787851333618 11.368621826171875
Loss :  1.6928590536117554 2.2101099491119385 12.743408203125
Loss :  1.674655795097351 1.8195654153823853 10.772482872009277
Loss :  1.6674809455871582 2.300114154815674 13.168050765991211
Loss :  1.692514419555664 2.240795373916626 12.896491050720215
Loss :  1.6610571146011353 3.3342397212982178 18.33225440979004
Loss :  1.6606621742248535 2.325718879699707 13.289257049560547
Loss :  1.658905029296875 2.3680081367492676 13.498945236206055
Loss :  1.6631985902786255 2.9692296981811523 16.509347915649414
Loss :  1.7035653591156006 2.9178123474121094 16.292627334594727
Loss :  1.6997504234313965 3.4228031635284424 18.813766479492188
Loss :  1.6570719480514526 2.4591567516326904 13.952856063842773
Loss :  1.6765751838684082 2.248224973678589 12.917699813842773
Loss :  1.65426766872406 2.1994588375091553 12.651561737060547
Loss :  1.6988909244537354 2.491063356399536 14.154207229614258
  batch 20 loss: 1.6988909244537354, 2.491063356399536, 14.154207229614258
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6724364757537842 2.0062222480773926 11.703547477722168
Loss :  1.654085636138916 2.1685726642608643 12.4969482421875
Loss :  1.667172908782959 2.0058112144470215 11.696229934692383
Loss :  1.678390622138977 2.467714548110962 14.016963005065918
Loss :  1.6996713876724243 2.729597330093384 15.347658157348633
Loss :  1.6693819761276245 2.96232533454895 16.481008529663086
Loss :  1.6767876148223877 3.2004995346069336 17.67928695678711
Loss :  1.6722530126571655 2.5464589595794678 14.404547691345215
Loss :  1.635556697845459 2.658557415008545 14.9283447265625
Loss :  1.6973415613174438 2.7982687950134277 15.68868637084961
Loss :  1.6365779638290405 3.025160789489746 16.76238250732422
Loss :  1.685634732246399 2.519514799118042 14.283208847045898
Loss :  1.6679595708847046 1.9952423572540283 11.644170761108398
Loss :  1.6665220260620117 1.991158366203308 11.622313499450684
Loss :  1.6413823366165161 2.350369453430176 13.393229484558105
Loss :  1.6520586013793945 2.2853963375091553 13.07904052734375
Loss :  1.651219367980957 2.133223056793213 12.317334175109863
Loss :  1.695012092590332 2.0218331813812256 11.804178237915039
Loss :  1.699126124382019 2.315537691116333 13.276814460754395
Loss :  1.7061351537704468 2.2933509349823 13.172889709472656
  batch 40 loss: 1.7061351537704468, 2.2933509349823, 13.172889709472656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6768018007278442 2.2178196907043457 12.765899658203125
Loss :  1.6668806076049805 2.127809762954712 12.305929183959961
Loss :  1.6595710515975952 2.658026933670044 14.949705123901367
Loss :  1.6687419414520264 2.512037992477417 14.22893238067627
Loss :  1.6533589363098145 2.440757989883423 13.857149124145508
Loss :  1.675150990486145 2.433377742767334 13.842040061950684
Loss :  1.6987181901931763 1.7441771030426025 10.41960334777832
Loss :  1.6640210151672363 2.2017080783843994 12.672561645507812
Loss :  1.7089370489120483 1.842564344406128 10.921758651733398
Loss :  1.6680736541748047 2.583653211593628 14.586339950561523
Loss :  1.6920185089111328 1.934227705001831 11.363157272338867
Loss :  1.6876466274261475 2.0561788082122803 11.968541145324707
Loss :  1.6724908351898193 1.9921373128890991 11.633177757263184
Loss :  1.693231463432312 2.7878124713897705 15.632293701171875
Loss :  1.6640398502349854 2.1488397121429443 12.408238410949707
Loss :  1.7092145681381226 1.9827724695205688 11.623076438903809
Loss :  1.6693109273910522 2.058487892150879 11.961750030517578
Loss :  1.6578054428100586 1.9109554290771484 11.2125825881958
Loss :  1.669180989265442 2.6790988445281982 15.064675331115723
Loss :  1.7150934934616089 2.5552635192871094 14.491411209106445
  batch 60 loss: 1.7150934934616089, 2.5552635192871094, 14.491411209106445
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.658238172531128 2.7664835453033447 15.490655899047852
Loss :  1.6790475845336914 2.6120545864105225 14.739320755004883
Loss :  1.6648257970809937 1.9164440631866455 11.247045516967773
Loss :  1.655859351158142 2.434199571609497 13.82685661315918
Loss :  1.6413666009902954 1.6884369850158691 10.083551406860352
Loss :  1.6569459438323975 4.261334419250488 22.9636173248291
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6650856733322144 4.329870223999023 23.314435958862305
Loss :  1.6625170707702637 4.220808506011963 22.766559600830078
Loss :  1.6718366146087646 4.197484493255615 22.659259796142578
Total LOSS train 13.306498116713303 valid 22.925968170166016
CE LOSS train 1.6747589349746703 valid 0.41795915365219116
Contrastive LOSS train 2.326347838915311 valid 1.0493711233139038
EPOCH 270:
Loss :  1.688894271850586 2.1895859241485596 12.636823654174805
Loss :  1.6981360912322998 2.6568593978881836 14.982433319091797
Loss :  1.675965666770935 2.3863863945007324 13.607897758483887
Loss :  1.6805552244186401 2.305095911026001 13.206034660339355
Loss :  1.6930034160614014 1.8880000114440918 11.133004188537598
Loss :  1.667125940322876 2.1735241413116455 12.534746170043945
Loss :  1.6930912733078003 2.5920071601867676 14.65312671661377
Loss :  1.6753450632095337 3.509889841079712 19.224794387817383
Loss :  1.6681185960769653 2.362560272216797 13.48091983795166
Loss :  1.6924513578414917 1.7246650457382202 10.315776824951172
Loss :  1.6615099906921387 2.1368868350982666 12.345943450927734
Loss :  1.6609565019607544 2.776332378387451 15.542618751525879
Loss :  1.658891201019287 2.679332733154297 15.05555534362793
Loss :  1.662904143333435 2.7923617362976074 15.624712944030762
Loss :  1.7035198211669922 2.5295796394348145 14.351417541503906
Loss :  1.6997610330581665 1.7113025188446045 10.256274223327637
Loss :  1.6573387384414673 2.0330677032470703 11.822677612304688
Loss :  1.676693320274353 1.8332526683807373 10.84295654296875
Loss :  1.6546962261199951 1.896255373954773 11.13597297668457
Loss :  1.700003743171692 2.2135989665985107 12.767998695373535
  batch 20 loss: 1.700003743171692, 2.2135989665985107, 12.767998695373535
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6727590560913086 2.544877290725708 14.39714527130127
Loss :  1.6543760299682617 2.735126256942749 15.330007553100586
Loss :  1.667288899421692 1.7211424112319946 10.273000717163086
Loss :  1.6785218715667725 1.8297861814498901 10.827452659606934
Loss :  1.699951171875 2.4692111015319824 14.04600715637207
Loss :  1.6696418523788452 1.9547650814056396 11.443467140197754
Loss :  1.677013635635376 1.6814332008361816 10.084178924560547
Loss :  1.672648310661316 2.262845277786255 12.9868745803833
Loss :  1.635396957397461 2.3010454177856445 13.140624046325684
Loss :  1.6971732378005981 2.1546480655670166 12.470413208007812
Loss :  1.636088490486145 2.124570369720459 12.258940696716309
Loss :  1.685453176498413 2.111654758453369 12.24372673034668
Loss :  1.6676772832870483 2.3831748962402344 13.583551406860352
Loss :  1.666534185409546 1.8242957592010498 10.788013458251953
Loss :  1.6415210962295532 2.452242374420166 13.902732849121094
Loss :  1.6524654626846313 1.9911798238754272 11.608365058898926
Loss :  1.6517820358276367 2.4081075191497803 13.692319869995117
Loss :  1.6957635879516602 2.1278271675109863 12.334898948669434
Loss :  1.6996146440505981 1.9078413248062134 11.238821029663086
Loss :  1.7067424058914185 1.8530714511871338 10.972099304199219
  batch 40 loss: 1.7067424058914185, 1.8530714511871338, 10.972099304199219
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6773215532302856 1.857776403427124 10.966203689575195
Loss :  1.6674729585647583 1.6243091821670532 9.789018630981445
Loss :  1.6596447229385376 2.5037381649017334 14.178336143493652
Loss :  1.6693732738494873 2.3024327754974365 13.181537628173828
Loss :  1.6535940170288086 1.7239387035369873 10.273287773132324
Loss :  1.6755881309509277 2.044381618499756 11.897497177124023
Loss :  1.6993398666381836 1.9474152326583862 11.436415672302246
Loss :  1.6642285585403442 1.8546944856643677 10.937701225280762
Loss :  1.7097747325897217 2.1077699661254883 12.248624801635742
Loss :  1.6686289310455322 1.8476566076278687 10.906911849975586
Loss :  1.6924363374710083 1.929205060005188 11.338461875915527
Loss :  1.6883001327514648 1.7821794748306274 10.599197387695312
Loss :  1.6732964515686035 2.2378270626068115 12.862432479858398
Loss :  1.6943131685256958 1.7727514505386353 10.558070182800293
Loss :  1.6650892496109009 1.9570435285568237 11.45030689239502
Loss :  1.7102611064910889 2.001605272293091 11.718287467956543
Loss :  1.6703063249588013 2.7477667331695557 15.409139633178711
Loss :  1.658974051475525 1.640758752822876 9.862767219543457
Loss :  1.6700713634490967 1.9905349016189575 11.622746467590332
Loss :  1.7157423496246338 1.6194595098495483 9.813039779663086
  batch 60 loss: 1.7157423496246338, 1.6194595098495483, 9.813039779663086
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6591615676879883 1.8973838090896606 11.14608097076416
Loss :  1.6797906160354614 1.8994876146316528 11.177228927612305
Loss :  1.6656076908111572 1.563431978225708 9.482767105102539
Loss :  1.6567323207855225 2.4807662963867188 14.060564041137695
Loss :  1.6425366401672363 1.7688285112380981 10.486679077148438
Loss :  1.6539620161056519 4.007686138153076 21.692392349243164
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6629605293273926 4.068872451782227 22.007322311401367
Loss :  1.6600254774093628 3.8391823768615723 20.85593605041504
Loss :  1.6660001277923584 3.9448132514953613 21.390066146850586
Total LOSS train 12.316117389385516 valid 21.48642921447754
CE LOSS train 1.6751532481266902 valid 0.4165000319480896
Contrastive LOSS train 2.128192822749798 valid 0.9862033128738403
EPOCH 271:
Loss :  1.6895147562026978 1.7527402639389038 10.453216552734375
Loss :  1.698830246925354 1.8897202014923096 11.147431373596191
Loss :  1.676944613456726 1.688344955444336 10.118669509887695
Loss :  1.6813960075378418 2.0068612098693848 11.715702056884766
Loss :  1.6937397718429565 2.3183069229125977 13.285274505615234
Loss :  1.6676976680755615 2.3400094509124756 13.367745399475098
Loss :  1.6933706998825073 2.4395830631256104 13.89128589630127
Loss :  1.675642967224121 2.3656816482543945 13.504051208496094
Loss :  1.66860032081604 2.4116320610046387 13.726760864257812
Loss :  1.692966341972351 2.337944984436035 13.382691383361816
Loss :  1.6621044874191284 2.7834036350250244 15.579122543334961
Loss :  1.6616617441177368 1.953380823135376 11.428565979003906
Loss :  1.6597652435302734 1.8642637729644775 10.981083869934082
Loss :  1.6639492511749268 2.071171283721924 12.019804954528809
Loss :  1.7039395570755005 2.1201939582824707 12.304908752441406
Loss :  1.7005596160888672 2.185055732727051 12.625838279724121
Loss :  1.6584560871124268 1.987886667251587 11.597888946533203
Loss :  1.6774609088897705 1.839118242263794 10.873051643371582
Loss :  1.6559547185897827 2.4147727489471436 13.729818344116211
Loss :  1.7004835605621338 2.5025782585144043 14.213375091552734
  batch 20 loss: 1.7004835605621338, 2.5025782585144043, 14.213375091552734
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.674096703529358 2.422731637954712 13.787755012512207
Loss :  1.6556921005249023 2.53816294670105 14.34650707244873
Loss :  1.6683130264282227 1.7986291646957397 10.661458969116211
Loss :  1.6796696186065674 1.9420400857925415 11.389869689941406
Loss :  1.7007585763931274 2.2713067531585693 13.057292938232422
Loss :  1.6708102226257324 2.368926525115967 13.515443801879883
Loss :  1.6780959367752075 2.529815435409546 14.327173233032227
Loss :  1.6739826202392578 1.7491137981414795 10.419551849365234
Loss :  1.637241244316101 2.231564998626709 12.795066833496094
Loss :  1.6983836889266968 2.3446762561798096 13.421764373779297
Loss :  1.6375945806503296 2.0177958011627197 11.72657299041748
Loss :  1.6863682270050049 2.4197959899902344 13.785347938537598
Loss :  1.6685843467712402 1.8435261249542236 10.886215209960938
Loss :  1.66693115234375 2.000049591064453 11.667179107666016
Loss :  1.642077088356018 2.212345600128174 12.703804969787598
Loss :  1.6526933908462524 1.9087897539138794 11.196642875671387
Loss :  1.651690125465393 2.496828317642212 14.135831832885742
Loss :  1.6957130432128906 2.278040885925293 13.085917472839355
Loss :  1.6994755268096924 2.608407497406006 14.7415132522583
Loss :  1.706750750541687 1.9737075567245483 11.575288772583008
  batch 40 loss: 1.706750750541687, 1.9737075567245483, 11.575288772583008
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4], device='cuda:0')
Loss :  1.677310585975647 2.1204426288604736 12.279523849487305
Loss :  1.6676464080810547 1.878664255142212 11.060967445373535
Loss :  1.6601123809814453 2.176238536834717 12.541305541992188
Loss :  1.669667363166809 2.12916898727417 12.315512657165527
Loss :  1.6542500257492065 2.3232390880584717 13.270445823669434
Loss :  1.6763383150100708 2.0955898761749268 12.154287338256836
Loss :  1.7002942562103271 1.6162848472595215 9.781719207763672
Loss :  1.6654701232910156 1.573930263519287 9.53512191772461
Loss :  1.710774540901184 1.6184539794921875 9.803044319152832
Loss :  1.6700282096862793 2.1401960849761963 12.371007919311523
Loss :  1.6937870979309082 2.340129852294922 13.39443588256836
Loss :  1.6893458366394043 2.480733633041382 14.093013763427734
Loss :  1.6742666959762573 2.324176549911499 13.295149803161621
Loss :  1.6950711011886597 2.1846389770507812 12.618266105651855
Loss :  1.665958285331726 2.2030789852142334 12.681353569030762
Loss :  1.710994839668274 1.5847138166427612 9.634564399719238
Loss :  1.6711348295211792 2.149658441543579 12.419426918029785
Loss :  1.6596299409866333 2.4100422859191895 13.709840774536133
Loss :  1.6708550453186035 1.8962738513946533 11.152223587036133
Loss :  1.7165017127990723 1.8186143636703491 10.809574127197266
  batch 60 loss: 1.7165017127990723, 1.8186143636703491, 10.809574127197266
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.660225749015808 2.1730668544769287 12.52556037902832
Loss :  1.680734634399414 2.037429094314575 11.867879867553711
Loss :  1.6665725708007812 1.9879515171051025 11.606329917907715
Loss :  1.6580177545547485 3.0265071392059326 16.79055404663086
Loss :  1.643797516822815 2.320073366165161 13.24416446685791
Loss :  1.6496281623840332 3.9864044189453125 21.581649780273438
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.6582659482955933 3.900749444961548 21.16201400756836
Loss :  1.6552140712738037 3.8057618141174316 20.684022903442383
Loss :  1.6649454832077026 3.6517364978790283 19.923627853393555
Total LOSS train 12.432734753535344 valid 20.837828636169434
CE LOSS train 1.6759499439826377 valid 0.41623637080192566
Contrastive LOSS train 2.151356952007 valid 0.9129341244697571
EPOCH 272:
Loss :  1.6907919645309448 2.3554558753967285 13.468070983886719
Loss :  1.7001862525939941 1.8113459348678589 10.756916046142578
Loss :  1.6780098676681519 1.7916946411132812 10.636483192443848
Loss :  1.6823954582214355 1.8846288919448853 11.105539321899414
Loss :  1.6947393417358398 2.5252339839935303 14.32090950012207
Loss :  1.669005274772644 2.176407814025879 12.551044464111328
Loss :  1.6948139667510986 1.719921588897705 10.294421195983887
Loss :  1.676952838897705 1.4107941389083862 8.730923652648926
Loss :  1.6699246168136597 2.1630749702453613 12.485299110412598
Loss :  1.6945796012878418 2.5232415199279785 14.310787200927734
Loss :  1.6635836362838745 2.8563485145568848 15.94532585144043
Loss :  1.6631089448928833 2.801109552383423 15.668656349182129
Loss :  1.6612985134124756 2.6340172290802 14.831384658813477
Loss :  1.6650829315185547 2.486464738845825 14.097406387329102
Loss :  1.7050625085830688 2.609076499938965 14.750445365905762
Loss :  1.7017743587493896 1.7897804975509644 10.650676727294922
Loss :  1.659013271331787 1.7763698101043701 10.540863037109375
Loss :  1.6784824132919312 1.676979660987854 10.063380241394043
Loss :  1.6560440063476562 1.9669080972671509 11.490584373474121
Loss :  1.7005972862243652 2.1047251224517822 12.224222183227539
  batch 20 loss: 1.7005972862243652, 2.1047251224517822, 12.224222183227539
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.673606276512146 2.1360561847686768 12.353886604309082
Loss :  1.6552664041519165 2.763855218887329 15.474542617797852
Loss :  1.6685853004455566 2.673609495162964 15.036632537841797
Loss :  1.6798471212387085 1.5919898748397827 9.639796257019043
Loss :  1.7012628316879272 1.710463047027588 10.25357723236084
Loss :  1.671282410621643 1.9405455589294434 11.374011039733887
Loss :  1.6787208318710327 2.243635892868042 12.896900177001953
Loss :  1.6745854616165161 2.014946222305298 11.749316215515137
Loss :  1.6374502182006836 2.0479960441589355 11.87743091583252
Loss :  1.6992459297180176 2.6218974590301514 14.808732986450195
Loss :  1.6384944915771484 2.161797523498535 12.447482109069824
Loss :  1.68746018409729 2.0002548694610596 11.68873405456543
Loss :  1.6700609922409058 2.037024736404419 11.855184555053711
Loss :  1.6688563823699951 1.9980196952819824 11.658955574035645
Loss :  1.6438775062561035 2.8134171962738037 15.71096420288086
Loss :  1.654593825340271 2.6445891857147217 14.87753963470459
Loss :  1.6537482738494873 2.556291103363037 14.43520450592041
Loss :  1.697706699371338 1.675604224205017 10.075727462768555
Loss :  1.7011935710906982 2.3898773193359375 13.650580406188965
Loss :  1.7082990407943726 2.2469708919525146 12.943153381347656
  batch 40 loss: 1.7082990407943726, 2.2469708919525146, 12.943153381347656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6790069341659546 2.7893028259277344 15.625520706176758
Loss :  1.6695455312728882 2.5489988327026367 14.414539337158203
Loss :  1.661270260810852 2.834078311920166 15.831661224365234
Loss :  1.671159267425537 1.8866831064224243 11.104574203491211
Loss :  1.6551339626312256 1.740964651107788 10.359957695007324
Loss :  1.6772736310958862 2.1294190883636475 12.324369430541992
Loss :  1.7010464668273926 1.7218021154403687 10.310056686401367
Loss :  1.6658424139022827 1.6209182739257812 9.77043342590332
Loss :  1.711646556854248 1.7188607454299927 10.305950164794922
Loss :  1.6708416938781738 1.5398707389831543 9.370195388793945
Loss :  1.6943680047988892 1.8462830781936646 10.92578411102295
Loss :  1.6903671026229858 1.9352608919143677 11.366671562194824
Loss :  1.6753066778182983 2.292720317840576 13.138908386230469
Loss :  1.6961398124694824 2.4080588817596436 13.736434936523438
Loss :  1.6664091348648071 2.2721521854400635 13.027170181274414
Loss :  1.7113873958587646 2.20045804977417 12.713678359985352
Loss :  1.6713956594467163 2.943006753921509 16.386428833007812
Loss :  1.6595356464385986 2.665201425552368 14.985542297363281
Loss :  1.6705998182296753 2.0093445777893066 11.71732234954834
Loss :  1.715877890586853 2.1481850147247314 12.456802368164062
  batch 60 loss: 1.715877890586853, 2.1481850147247314, 12.456802368164062
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6597198247909546 2.3029799461364746 13.174619674682617
Loss :  1.6801156997680664 1.8763848543167114 11.062040328979492
Loss :  1.666237711906433 1.76561439037323 10.494309425354004
Loss :  1.6575862169265747 1.944098711013794 11.378079414367676
Loss :  1.6433286666870117 1.3082889318466187 8.184773445129395
Loss :  1.6517072916030884 4.17045259475708 22.503969192504883
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.6606271266937256 4.173466205596924 22.527957916259766
Loss :  1.6588324308395386 3.966947317123413 21.493568420410156
Loss :  1.6638009548187256 3.941741943359375 21.37251091003418
Total LOSS train 12.429192557701697 valid 21.974501609802246
CE LOSS train 1.6767805044467632 valid 0.4159502387046814
Contrastive LOSS train 2.1504824234889104 valid 0.9854354858398438
EPOCH 273:
Loss :  1.6902574300765991 2.00484561920166 11.714485168457031
Loss :  1.6999797821044922 2.1783506870269775 12.5917329788208
Loss :  1.6777963638305664 1.470862865447998 9.032110214233398
Loss :  1.6821365356445312 1.943139910697937 11.397835731506348
Loss :  1.694740891456604 2.107442617416382 12.231954574584961
Loss :  1.6690267324447632 1.7520873546600342 10.429463386535645
Loss :  1.6948130130767822 2.1186940670013428 12.288283348083496
Loss :  1.677193284034729 1.6856709718704224 10.105548858642578
Loss :  1.670324444770813 1.739003300666809 10.365341186523438
Loss :  1.6949142217636108 2.0997776985168457 12.193801879882812
Loss :  1.6641191244125366 2.069171667098999 12.009977340698242
Loss :  1.6638251543045044 1.7048896551132202 10.188273429870605
Loss :  1.6620562076568604 2.0723462104797363 12.023786544799805
Loss :  1.6657525300979614 2.1777589321136475 12.554547309875488
Loss :  1.7056437730789185 1.9390580654144287 11.400934219360352
Loss :  1.7020580768585205 2.213155508041382 12.76783561706543
Loss :  1.660266399383545 1.9186211824417114 11.253372192382812
Loss :  1.6789804697036743 2.0997910499572754 12.177935600280762
Loss :  1.6575430631637573 2.055342674255371 11.934256553649902
Loss :  1.7018028497695923 2.1131932735443115 12.267769813537598
  batch 20 loss: 1.7018028497695923, 2.1131932735443115, 12.267769813537598
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6752551794052124 2.162283182144165 12.48667049407959
Loss :  1.65719473361969 1.912274718284607 11.218568801879883
Loss :  1.6700828075408936 1.7372511625289917 10.356338500976562
Loss :  1.6808247566223145 2.0307929515838623 11.834789276123047
Loss :  1.7016990184783936 1.9732388257980347 11.567893028259277
Loss :  1.6716564893722534 1.8874874114990234 11.10909366607666
Loss :  1.6787800788879395 2.1725337505340576 12.541448593139648
Loss :  1.674409031867981 2.6634697914123535 14.9917573928833
Loss :  1.6375020742416382 2.725416898727417 15.264586448669434
Loss :  1.6987740993499756 2.2486393451690674 12.941970825195312
Loss :  1.638331651687622 1.9067906141281128 11.172284126281738
Loss :  1.687071442604065 1.8302643299102783 10.838393211364746
Loss :  1.6694214344024658 2.510669708251953 14.222769737243652
Loss :  1.66815185546875 2.4802944660186768 14.069623947143555
Loss :  1.6436588764190674 2.9771411418914795 16.52936553955078
Loss :  1.6542408466339111 1.873792052268982 11.023200988769531
Loss :  1.653450608253479 1.9886246919631958 11.596574783325195
Loss :  1.6968488693237305 1.7367764711380005 10.380731582641602
Loss :  1.700473427772522 2.5432448387145996 14.416698455810547
Loss :  1.7075507640838623 2.4895846843719482 14.155474662780762
  batch 40 loss: 1.7075507640838623, 2.4895846843719482, 14.155474662780762
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6786446571350098 2.0229902267456055 11.793596267700195
Loss :  1.669273853302002 2.447660207748413 13.907575607299805
Loss :  1.661864995956421 1.8887536525726318 11.105633735656738
Loss :  1.6714146137237549 1.696165680885315 10.152242660522461
Loss :  1.6558138132095337 1.7346652746200562 10.329140663146973
Loss :  1.6775383949279785 1.9996329545974731 11.675703048706055
Loss :  1.70085871219635 2.0989058017730713 12.195387840270996
Loss :  1.6665185689926147 2.4371604919433594 13.852320671081543
Loss :  1.710976481437683 2.4858932495117188 14.140442848205566
Loss :  1.6707934141159058 1.7570698261260986 10.45614242553711
Loss :  1.6945604085922241 2.0336077213287354 11.862598419189453
Loss :  1.6901870965957642 1.8324668407440186 10.852521896362305
Loss :  1.6754652261734009 1.9260905981063843 11.30591869354248
Loss :  1.6958273649215698 2.3869056701660156 13.630355834960938
Loss :  1.6673429012298584 1.8229095935821533 10.781890869140625
Loss :  1.7110702991485596 2.163177251815796 12.526956558227539
Loss :  1.6717686653137207 2.233609676361084 12.83981704711914
Loss :  1.6603813171386719 2.064023733139038 11.980500221252441
Loss :  1.6713953018188477 2.4134843349456787 13.73881721496582
Loss :  1.7160804271697998 1.998669147491455 11.709425926208496
  batch 60 loss: 1.7160804271697998, 1.998669147491455, 11.709425926208496
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6604547500610352 2.585970640182495 14.59030818939209
Loss :  1.6811741590499878 2.4515726566314697 13.939037322998047
Loss :  1.6669995784759521 1.8759897947311401 11.046948432922363
Loss :  1.658725619316101 2.0460028648376465 11.888740539550781
Loss :  1.6446572542190552 1.4989649057388306 9.139481544494629
Loss :  1.6621695756912231 3.836770534515381 20.846023559570312
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.670100212097168 3.8735249042510986 21.037723541259766
Loss :  1.6684900522232056 3.715123176574707 20.244104385375977
Loss :  1.6735037565231323 3.5872771739959717 19.60988998413086
Total LOSS train 12.078292069068322 valid 20.43443536758423
CE LOSS train 1.6770522502752452 valid 0.4183759391307831
Contrastive LOSS train 2.080247956055861 valid 0.8968192934989929
EPOCH 274:
Loss :  1.690908670425415 1.8567556142807007 10.974686622619629
Loss :  1.70021390914917 2.6388821601867676 14.894624710083008
Loss :  1.6783978939056396 2.1074745655059814 12.215770721435547
Loss :  1.6828114986419678 2.206623077392578 12.715927124023438
Loss :  1.695157766342163 2.3799374103546143 13.594844818115234
Loss :  1.6696395874023438 2.4441311359405518 13.890295028686523
Loss :  1.6949981451034546 2.1382031440734863 12.38601303100586
Loss :  1.6777293682098389 1.5732340812683105 9.543899536132812
Loss :  1.671063780784607 1.99037504196167 11.622939109802246
Loss :  1.6955100297927856 1.5164862871170044 9.277941703796387
Loss :  1.664840817451477 2.1574525833129883 12.452103614807129
Loss :  1.6644178628921509 2.6297450065612793 14.813143730163574
Loss :  1.6627086400985718 2.425116539001465 13.788290977478027
Loss :  1.6666648387908936 2.1157944202423096 12.245636940002441
Loss :  1.7065353393554688 2.197279930114746 12.6929349899292
Loss :  1.70254385471344 1.540743112564087 9.406259536743164
Loss :  1.6612365245819092 1.671924352645874 10.020858764648438
Loss :  1.6796650886535645 1.6084297895431519 9.721813201904297
Loss :  1.6582177877426147 1.875096082687378 11.033698081970215
Loss :  1.702385425567627 2.088263988494873 12.143705368041992
  batch 20 loss: 1.702385425567627, 2.088263988494873, 12.143705368041992
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.675570011138916 1.913915991783142 11.245149612426758
Loss :  1.657562017440796 2.474660634994507 14.030865669250488
Loss :  1.6705232858657837 1.8143876791000366 10.742462158203125
Loss :  1.681296706199646 1.8282033205032349 10.82231330871582
Loss :  1.7022444009780884 2.3388240337371826 13.39636516571045
Loss :  1.6725281476974487 2.088887929916382 12.116968154907227
Loss :  1.6797220706939697 1.6851906776428223 10.105674743652344
Loss :  1.6757256984710693 2.0168509483337402 11.759981155395508
Loss :  1.639097809791565 1.952069640159607 11.399446487426758
Loss :  1.6998900175094604 2.2256462574005127 12.828121185302734
Loss :  1.6399292945861816 1.8800486326217651 11.040172576904297
Loss :  1.6881927251815796 1.7385987043380737 10.381185531616211
Loss :  1.6709121465682983 2.0266337394714355 11.804080963134766
Loss :  1.6697853803634644 2.106235980987549 12.20096492767334
Loss :  1.645350694656372 2.1606318950653076 12.44851016998291
Loss :  1.6558845043182373 1.9490805864334106 11.401288032531738
Loss :  1.6551196575164795 1.7752234935760498 10.531237602233887
Loss :  1.698390245437622 2.3417770862579346 13.407275199890137
Loss :  1.70168936252594 2.388322114944458 13.64330005645752
Loss :  1.7087815999984741 1.9306700229644775 11.362131118774414
  batch 40 loss: 1.7087815999984741, 1.9306700229644775, 11.362131118774414
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6799733638763428 2.205878257751465 12.709364891052246
Loss :  1.6707147359848022 1.8655799627304077 10.998614311218262
Loss :  1.6627930402755737 2.114020586013794 12.232895851135254
Loss :  1.6726064682006836 1.5608857870101929 9.477035522460938
Loss :  1.6567909717559814 1.508034110069275 9.196961402893066
Loss :  1.6780451536178589 2.3214809894561768 13.285449981689453
Loss :  1.7010791301727295 2.410274028778076 13.752449989318848
Loss :  1.6666491031646729 1.985540509223938 11.594351768493652
Loss :  1.7110483646392822 2.102757215499878 12.224834442138672
Loss :  1.6708390712738037 2.328465700149536 13.313167572021484
Loss :  1.6941651105880737 2.5142838954925537 14.265584945678711
Loss :  1.6899478435516357 2.5914523601531982 14.647210121154785
Loss :  1.6749719381332397 2.4046292304992676 13.69811725616455
Loss :  1.695812463760376 2.343686580657959 13.41424560546875
Loss :  1.6668529510498047 2.2029001712799072 12.681353569030762
Loss :  1.711336374282837 2.1002140045166016 12.212406158447266
Loss :  1.672359824180603 2.1226584911346436 12.285652160644531
Loss :  1.661010980606079 2.700308084487915 15.162550926208496
Loss :  1.6722239255905151 2.2493014335632324 12.918731689453125
Loss :  1.7171379327774048 1.796777606010437 10.70102596282959
  batch 60 loss: 1.7171379327774048, 1.796777606010437, 10.70102596282959
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.661929965019226 2.3900146484375 13.612003326416016
Loss :  1.6825155019760132 2.183624505996704 12.600637435913086
Loss :  1.6684637069702148 2.083808422088623 12.087506294250488
Loss :  1.660235047340393 2.409365177154541 13.707060813903809
Loss :  1.646108627319336 2.0996577739715576 12.144397735595703
Loss :  1.6594774723052979 3.27838397026062 18.0513973236084
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6677632331848145 3.208984613418579 17.71268653869629
Loss :  1.6659321784973145 3.0274581909179688 16.80322265625
Loss :  1.6715871095657349 2.6882996559143066 15.11308479309082
Total LOSS train 12.169638325617864 valid 16.920097827911377
CE LOSS train 1.6778377569638765 valid 0.4178967773914337
Contrastive LOSS train 2.098360111163213 valid 0.6720749139785767
Saved best model. Old loss 17.327121257781982 and new best loss 16.920097827911377
EPOCH 275:
Loss :  1.6920450925827026 1.695417046546936 10.169130325317383
Loss :  1.701343297958374 2.062063455581665 12.0116605758667
Loss :  1.6795803308486938 2.383589744567871 13.597529411315918
Loss :  1.6838414669036865 2.5732014179229736 14.549848556518555
Loss :  1.6959779262542725 1.932127594947815 11.356616020202637
Loss :  1.6709612607955933 1.8678406476974487 11.010164260864258
Loss :  1.6961860656738281 1.7444591522216797 10.418481826782227
Loss :  1.679112434387207 1.4601325988769531 8.979775428771973
Loss :  1.6721996068954468 1.9140031337738037 11.242215156555176
Loss :  1.6964178085327148 2.5274477005004883 14.333656311035156
Loss :  1.6659928560256958 2.7652831077575684 15.492408752441406
Loss :  1.6655076742172241 2.6664907932281494 14.997961044311523
Loss :  1.663780689239502 2.1126039028167725 12.226800918579102
Loss :  1.667680263519287 1.7716705799102783 10.526033401489258
Loss :  1.7067509889602661 2.324230909347534 13.327905654907227
Loss :  1.702973484992981 2.513247489929199 14.269210815429688
Loss :  1.6615698337554932 2.195404529571533 12.638591766357422
Loss :  1.6800719499588013 2.5656402111053467 14.508273124694824
Loss :  1.658502221107483 2.4626500606536865 13.971753120422363
Loss :  1.7018625736236572 2.0520823001861572 11.962273597717285
  batch 20 loss: 1.7018625736236572, 2.0520823001861572, 11.962273597717285
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6756173372268677 1.5822252035140991 9.586743354797363
Loss :  1.657374620437622 2.0126261711120605 11.720505714416504
Loss :  1.6701300144195557 1.896324634552002 11.151752471923828
Loss :  1.6812881231307983 1.8441731929779053 10.902153968811035
Loss :  1.7021445035934448 2.377887487411499 13.591582298278809
Loss :  1.6726431846618652 1.9886466264724731 11.615877151489258
Loss :  1.6799908876419067 1.8855547904968262 11.107765197753906
Loss :  1.6758705377578735 2.040875196456909 11.880247116088867
Loss :  1.6399774551391602 2.3093228340148926 13.186591148376465
Loss :  1.7001142501831055 2.0359981060028076 11.880105018615723
Loss :  1.6410061120986938 1.771767020225525 10.499841690063477
Loss :  1.6888794898986816 1.7184481620788574 10.281120300292969
Loss :  1.6715549230575562 2.509666681289673 14.219887733459473
Loss :  1.6702455282211304 2.693493366241455 15.137711524963379
Loss :  1.645652174949646 2.4821810722351074 14.056557655334473
Loss :  1.6561365127563477 2.248538017272949 12.898826599121094
Loss :  1.6552037000656128 1.933312177658081 11.321764945983887
Loss :  1.6980314254760742 2.0312440395355225 11.854251861572266
Loss :  1.7015694379806519 2.2097203731536865 12.750171661376953
Loss :  1.7085257768630981 2.273068904876709 13.073870658874512
  batch 40 loss: 1.7085257768630981, 2.273068904876709, 13.073870658874512
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6797704696655273 2.340773105621338 13.383635520935059
Loss :  1.6703288555145264 2.4052693843841553 13.696676254272461
Loss :  1.6627304553985596 2.39972186088562 13.66133975982666
Loss :  1.6721101999282837 1.9594730138778687 11.469475746154785
Loss :  1.6568268537521362 1.7943342924118042 10.628498077392578
Loss :  1.6782835721969604 2.261995553970337 12.988261222839355
Loss :  1.7013657093048096 1.800910472869873 10.705918312072754
Loss :  1.6674127578735352 2.604611873626709 14.690472602844238
Loss :  1.7115588188171387 1.9900177717208862 11.66164779663086
Loss :  1.6717698574066162 1.8697640895843506 11.020590782165527
Loss :  1.69509756565094 1.6816736459732056 10.103466033935547
Loss :  1.6908254623413086 1.935910701751709 11.370379447937012
Loss :  1.676300048828125 1.9150738716125488 11.251668930053711
Loss :  1.6964972019195557 2.76806902885437 15.536842346191406
Loss :  1.6681571006774902 2.4500977993011475 13.918645858764648
Loss :  1.71172034740448 2.116457462310791 12.294007301330566
Loss :  1.6729235649108887 2.0871522426605225 12.108684539794922
Loss :  1.6617588996887207 1.8610789775848389 10.967153549194336
Loss :  1.6726653575897217 2.1917567253112793 12.631449699401855
Loss :  1.7169697284698486 2.177677869796753 12.605359077453613
  batch 60 loss: 1.7169697284698486, 2.177677869796753, 12.605359077453613
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6621125936508179 2.3287384510040283 13.305804252624512
Loss :  1.6824305057525635 1.7393169403076172 10.37901496887207
Loss :  1.6685869693756104 1.8009085655212402 10.67313003540039
Loss :  1.6602433919906616 2.2847867012023926 13.084176063537598
Loss :  1.6464799642562866 1.5447416305541992 9.370187759399414
Loss :  1.6561005115509033 3.216074228286743 17.736473083496094
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6643598079681396 2.973982572555542 16.534273147583008
Loss :  1.6622240543365479 2.982226848602295 16.5733585357666
Loss :  1.6674153804779053 2.6964683532714844 15.149757385253906
Total LOSS train 12.274063139695388 valid 16.498465538024902
CE LOSS train 1.6782960011408878 valid 0.4168538451194763
Contrastive LOSS train 2.1191534225757307 valid 0.6741170883178711
Saved best model. Old loss 16.920097827911377 and new best loss 16.498465538024902
EPOCH 276:
Loss :  1.6921474933624268 2.108431577682495 12.234305381774902
Loss :  1.7012789249420166 2.0855839252471924 12.12919807434082
Loss :  1.679926872253418 1.7365878820419312 10.362866401672363
Loss :  1.6843531131744385 1.8884974718093872 11.126840591430664
Loss :  1.6964114904403687 1.6499111652374268 9.945966720581055
Loss :  1.6715080738067627 1.9892657995224 11.617836952209473
Loss :  1.6963917016983032 1.8551900386810303 10.972342491149902
Loss :  1.6793699264526367 1.8525032997131348 10.941885948181152
Loss :  1.672430157661438 2.400444507598877 13.674652099609375
Loss :  1.6964406967163086 1.9286638498306274 11.339759826660156
Loss :  1.666372299194336 2.0633018016815186 11.982881546020508
Loss :  1.6659811735153198 2.5320003032684326 14.325983047485352
Loss :  1.6642943620681763 2.5339105129241943 14.333847045898438
Loss :  1.668447494506836 1.9575135707855225 11.456015586853027
Loss :  1.7070579528808594 1.8638451099395752 11.026283264160156
Loss :  1.7036089897155762 2.3008036613464355 13.20762825012207
Loss :  1.6626920700073242 2.3391098976135254 13.358241081237793
Loss :  1.6810338497161865 2.1551618576049805 12.456843376159668
Loss :  1.6599063873291016 1.7483925819396973 10.40186882019043
Loss :  1.7029424905776978 1.8604331016540527 11.005108833312988
  batch 20 loss: 1.7029424905776978, 1.8604331016540527, 11.005108833312988
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6771032810211182 1.8025344610214233 10.689775466918945
Loss :  1.6592077016830444 2.2491843700408936 12.905129432678223
Loss :  1.6715025901794434 2.055025100708008 11.94662857055664
Loss :  1.68253755569458 1.7876513004302979 10.620794296264648
Loss :  1.7030731439590454 2.0164506435394287 11.785326957702637
Loss :  1.6737090349197388 1.9918547868728638 11.632983207702637
Loss :  1.6807823181152344 1.9792392253875732 11.57697868347168
Loss :  1.6765239238739014 2.3380236625671387 13.366642951965332
Loss :  1.6406899690628052 2.4647984504699707 13.964681625366211
Loss :  1.7001391649246216 2.6586339473724365 14.993309020996094
Loss :  1.6414438486099243 2.6647486686706543 14.965188026428223
Loss :  1.6887811422348022 2.665510892868042 15.016335487365723
Loss :  1.6714880466461182 2.3018100261688232 13.180538177490234
Loss :  1.6702184677124023 2.235522985458374 12.847833633422852
Loss :  1.6461505889892578 2.4476118087768555 13.884209632873535
Loss :  1.6565552949905396 2.1266887187957764 12.289999008178711
Loss :  1.6557726860046387 1.9339171648025513 11.325359344482422
Loss :  1.6982464790344238 1.8042852878570557 10.719673156738281
Loss :  1.7022966146469116 2.231239080429077 12.858491897583008
Loss :  1.7092663049697876 2.2470273971557617 12.944403648376465
  batch 40 loss: 1.7092663049697876, 2.2470273971557617, 12.944403648376465
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6808360815048218 2.4506194591522217 13.93393325805664
Loss :  1.6713368892669678 2.2036147117614746 12.689411163330078
Loss :  1.664014220237732 2.5770509243011475 14.54926872253418
Loss :  1.673161268234253 2.6686253547668457 15.016287803649902
Loss :  1.6582176685333252 1.549180507659912 9.404120445251465
Loss :  1.6793882846832275 1.775007724761963 10.554426193237305
Loss :  1.7021260261535645 1.6737209558486938 10.070730209350586
Loss :  1.668668508529663 1.708304524421692 10.21019172668457
Loss :  1.712191104888916 1.9462337493896484 11.443359375
Loss :  1.6728028059005737 1.7430952787399292 10.38827896118164
Loss :  1.6958919763565063 1.9114359617233276 11.253071784973145
Loss :  1.6917258501052856 2.5130093097686768 14.2567720413208
Loss :  1.6774532794952393 2.292849540710449 13.141700744628906
Loss :  1.6974163055419922 2.514751434326172 14.271173477172852
Loss :  1.669433832168579 2.0142221450805664 11.740544319152832
Loss :  1.7129629850387573 1.8706468343734741 11.066197395324707
Loss :  1.6744205951690674 2.3709137439727783 13.5289888381958
Loss :  1.6634256839752197 2.1876704692840576 12.601778030395508
Loss :  1.6741307973861694 2.4289567470550537 13.818914413452148
Loss :  1.7180054187774658 1.9520434141159058 11.478221893310547
  batch 60 loss: 1.7180054187774658, 1.9520434141159058, 11.478221893310547
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6636332273483276 2.1686089038848877 12.506677627563477
Loss :  1.6835194826126099 2.353019952774048 13.44861888885498
Loss :  1.6697648763656616 2.0985159873962402 12.162344932556152
Loss :  1.6611261367797852 2.1072239875793457 12.197245597839355
Loss :  1.6473064422607422 1.3894439935684204 8.594526290893555
Loss :  1.6533334255218506 4.167343616485596 22.49005126953125
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
Loss :  1.6628156900405884 4.1080474853515625 22.203052520751953
Loss :  1.6604280471801758 4.026273250579834 21.791793823242188
Loss :  1.6632752418518066 3.9640018939971924 21.48328399658203
Total LOSS train 12.242175718454215 valid 21.992045402526855
CE LOSS train 1.6790929757631743 valid 0.41581881046295166
Contrastive LOSS train 2.112616546337421 valid 0.9910004734992981
EPOCH 277:
Loss :  1.6926993131637573 1.7136822938919067 10.26111125946045
Loss :  1.7018718719482422 2.079460859298706 12.099176406860352
Loss :  1.680694580078125 2.1852009296417236 12.606698989868164
Loss :  1.684984803199768 2.6477150917053223 14.92356014251709
Loss :  1.6971460580825806 2.4370601177215576 13.882447242736816
Loss :  1.6722043752670288 2.5193915367126465 14.26916217803955
Loss :  1.697312593460083 2.3869082927703857 13.631854057312012
Loss :  1.6801623106002808 2.211456298828125 12.737443923950195
Loss :  1.673539161682129 2.2730212211608887 13.03864574432373
Loss :  1.6974122524261475 2.4082958698272705 13.7388916015625
Loss :  1.6677463054656982 2.5936391353607178 14.635942459106445
Loss :  1.6674070358276367 2.5815553665161133 14.575183868408203
Loss :  1.6657543182373047 2.270075798034668 13.016133308410645
Loss :  1.669917106628418 2.50518798828125 14.195857048034668
Loss :  1.7080879211425781 1.744202971458435 10.429102897644043
Loss :  1.7049485445022583 2.322748899459839 13.318693161010742
Loss :  1.6642311811447144 2.3443305492401123 13.385884284973145
Loss :  1.6825534105300903 2.532341480255127 14.344260215759277
Loss :  1.6613517999649048 2.1099953651428223 12.211328506469727
Loss :  1.7039916515350342 1.8504606485366821 10.956295013427734
  batch 20 loss: 1.7039916515350342, 1.8504606485366821, 10.956295013427734
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6781288385391235 2.1733856201171875 12.54505729675293
Loss :  1.6600197553634644 1.852337121963501 10.92170524597168
Loss :  1.6721186637878418 1.5354939699172974 9.349588394165039
Loss :  1.6831982135772705 2.4765968322753906 14.066182136535645
Loss :  1.7038530111312866 2.2597739696502686 13.00272274017334
Loss :  1.6745463609695435 2.55826997756958 14.465895652770996
Loss :  1.681719422340393 1.8088114261627197 10.725776672363281
Loss :  1.6774780750274658 1.7589871883392334 10.472414016723633
Loss :  1.6420834064483643 2.11612606048584 12.222713470458984
Loss :  1.7015067338943481 2.297905683517456 13.191035270690918
Loss :  1.6432452201843262 2.1888315677642822 12.58740234375
Loss :  1.6905674934387207 2.0502066612243652 11.941600799560547
Loss :  1.673539638519287 2.443302631378174 13.890052795410156
Loss :  1.6722255945205688 2.4762775897979736 14.053613662719727
Loss :  1.648192048072815 2.691727638244629 15.106830596923828
Loss :  1.6582947969436646 2.603330135345459 14.674945831298828
Loss :  1.6573634147644043 2.082505464553833 12.069890975952148
Loss :  1.6994973421096802 1.6510144472122192 9.954569816589355
Loss :  1.7030293941497803 1.6189006567001343 9.79753303527832
Loss :  1.7099671363830566 2.0020623207092285 11.720277786254883
  batch 40 loss: 1.7099671363830566, 2.0020623207092285, 11.720277786254883
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6815727949142456 2.684873342514038 15.105939865112305
Loss :  1.6722497940063477 2.4185526371002197 13.765012741088867
Loss :  1.6648210287094116 2.48717999458313 14.100720405578613
Loss :  1.6740106344223022 1.8112472295761108 10.730246543884277
Loss :  1.6588765382766724 1.5855404138565063 9.586578369140625
Loss :  1.6799695491790771 1.7572104930877686 10.466022491455078
Loss :  1.7028443813323975 1.5897746086120605 9.651717185974121
Loss :  1.6692988872528076 2.3328542709350586 13.33357048034668
Loss :  1.7126762866973877 2.3379569053649902 13.402461051940918
Loss :  1.6735889911651611 2.125868558883667 12.302931785583496
Loss :  1.696685791015625 2.1339569091796875 12.366470336914062
Loss :  1.692603588104248 1.6449246406555176 9.917226791381836
Loss :  1.6782511472702026 2.342315435409546 13.3898286819458
Loss :  1.698114037513733 2.2865078449249268 13.130653381347656
Loss :  1.6704179048538208 1.8332446813583374 10.836641311645508
Loss :  1.7135956287384033 1.7058206796646118 10.242698669433594
Loss :  1.6750329732894897 2.4143929481506348 13.746996879577637
Loss :  1.663892388343811 1.6781667470932007 10.054726600646973
Loss :  1.6744886636734009 2.1538209915161133 12.443593978881836
Loss :  1.7183138132095337 1.4700905084609985 9.068766593933105
  batch 60 loss: 1.7183138132095337, 1.4700905084609985, 9.068766593933105
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6639394760131836 1.6362128257751465 9.845004081726074
Loss :  1.6838085651397705 2.294290065765381 13.155259132385254
Loss :  1.67025887966156 2.150129556655884 12.420906066894531
Loss :  1.6620627641677856 2.560164213180542 14.462883949279785
Loss :  1.6484565734863281 1.3116027116775513 8.206470489501953
Loss :  1.6579563617706299 4.091856479644775 22.117238998413086
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.666798710823059 4.12130069732666 22.27330207824707
Loss :  1.6641299724578857 3.953932285308838 21.43379020690918
Loss :  1.6695445775985718 3.7425851821899414 20.382469177246094
Total LOSS train 12.380781672551082 valid 21.551700115203857
CE LOSS train 1.6800680343921368 valid 0.41738614439964294
Contrastive LOSS train 2.1401427213962263 valid 0.9356462955474854
EPOCH 278:
Loss :  1.6937255859375 2.47419810295105 14.064716339111328
Loss :  1.702910304069519 2.3116753101348877 13.261286735534668
Loss :  1.6816685199737549 2.4097166061401367 13.73025131225586
Loss :  1.68595552444458 1.989108920097351 11.631500244140625
Loss :  1.697838306427002 2.4558966159820557 13.97732162475586
Loss :  1.6730170249938965 2.3257625102996826 13.301830291748047
Loss :  1.6979472637176514 2.1284632682800293 12.340264320373535
Loss :  1.6808725595474243 2.443697214126587 13.899358749389648
Loss :  1.674296498298645 2.491776943206787 14.13318157196045
Loss :  1.6981861591339111 1.5327414274215698 9.361893653869629
Loss :  1.6685147285461426 2.197939395904541 12.658210754394531
Loss :  1.668148159980774 2.125032663345337 12.29331111907959
Loss :  1.666426420211792 1.7988982200622559 10.660918235778809
Loss :  1.6704680919647217 1.9702174663543701 11.52155590057373
Loss :  1.7084617614746094 2.1261050701141357 12.338987350463867
Loss :  1.705031394958496 1.996904969215393 11.689556121826172
Loss :  1.664432168006897 2.0928633213043213 12.128748893737793
Loss :  1.6826882362365723 1.6295543909072876 9.830459594726562
Loss :  1.6615327596664429 1.5856882333755493 9.589973449707031
Loss :  1.7041223049163818 1.7909046411514282 10.658645629882812
  batch 20 loss: 1.7041223049163818, 1.7909046411514282, 10.658645629882812
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6784557104110718 1.8396174907684326 10.876543045043945
Loss :  1.6606987714767456 2.1557700634002686 12.439549446105957
Loss :  1.6732083559036255 1.8948943614959717 11.147680282592773
Loss :  1.6838997602462769 1.662867784500122 9.998238563537598
Loss :  1.7043509483337402 2.0989043712615967 12.198873519897461
Loss :  1.6754117012023926 2.565779685974121 14.504310607910156
Loss :  1.6823561191558838 2.6680500507354736 15.022605895996094
Loss :  1.6782456636428833 2.1676154136657715 12.51632308959961
Loss :  1.6426316499710083 1.7756874561309814 10.521068572998047
Loss :  1.7019239664077759 2.479810953140259 14.10097885131836
Loss :  1.6434814929962158 2.083901882171631 12.06299114227295
Loss :  1.6907354593276978 1.678817868232727 10.08482551574707
Loss :  1.6738438606262207 1.6665772199630737 10.006729125976562
Loss :  1.672659158706665 1.7216334342956543 10.280826568603516
Loss :  1.6486997604370117 1.9992396831512451 11.644898414611816
Loss :  1.6589752435684204 2.1904971599578857 12.611461639404297
Loss :  1.6581727266311646 1.6448094844818115 9.882220268249512
Loss :  1.7004917860031128 2.2448904514312744 12.924943923950195
Loss :  1.7038497924804688 2.016016960144043 11.783934593200684
Loss :  1.7106362581253052 1.8569552898406982 10.995412826538086
  batch 40 loss: 1.7106362581253052, 1.8569552898406982, 10.995412826538086
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6823232173919678 1.821285605430603 10.788751602172852
Loss :  1.6730458736419678 1.6626302003860474 9.986197471618652
Loss :  1.6654846668243408 2.034806966781616 11.839519500732422
Loss :  1.6747139692306519 1.5887391567230225 9.618410110473633
Loss :  1.6595708131790161 1.434849500656128 8.833818435668945
Loss :  1.6806087493896484 2.586458683013916 14.61290168762207
Loss :  1.7033716440200806 1.6183998584747314 9.795371055603027
Loss :  1.6699548959732056 1.5838696956634521 9.589303970336914
Loss :  1.7133103609085083 1.6240919828414917 9.833769798278809
Loss :  1.674224615097046 1.7415422201156616 10.381936073303223
Loss :  1.6972596645355225 1.8463034629821777 10.928777694702148
Loss :  1.6931477785110474 1.7847347259521484 10.6168212890625
Loss :  1.6787748336791992 1.7819983959197998 10.588767051696777
Loss :  1.6986620426177979 2.2433013916015625 12.915168762207031
Loss :  1.6709413528442383 2.3451883792877197 13.396883010864258
Loss :  1.7139711380004883 1.7580914497375488 10.504427909851074
Loss :  1.6757144927978516 2.6977155208587646 15.164292335510254
Loss :  1.6646811962127686 1.5993120670318604 9.66124153137207
Loss :  1.6752994060516357 1.9033832550048828 11.192215919494629
Loss :  1.7188003063201904 1.4234739542007446 8.836170196533203
  batch 60 loss: 1.7188003063201904, 1.4234739542007446, 8.836170196533203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.664747953414917 1.9974863529205322 11.652179718017578
Loss :  1.6845471858978271 2.0469253063201904 11.919174194335938
Loss :  1.6707736253738403 2.053887128829956 11.94020938873291
Loss :  1.6626173257827759 2.94630765914917 16.394155502319336
Loss :  1.648956298828125 1.3378335237503052 8.338123321533203
Loss :  1.6637386083602905 3.48012113571167 19.06434440612793
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6720237731933594 3.2131669521331787 17.737857818603516
Loss :  1.6701154708862305 3.228640556335449 17.81332015991211
Loss :  1.6753597259521484 3.02356219291687 16.793170928955078
Total LOSS train 11.661615004906288 valid 17.852173328399658
CE LOSS train 1.68068423637977 valid 0.4188399314880371
Contrastive LOSS train 1.996186135365413 valid 0.7558905482292175
EPOCH 279:
Loss :  1.694064974784851 2.1615922451019287 12.502026557922363
Loss :  1.7032737731933594 2.7874677181243896 15.640612602233887
Loss :  1.6819630861282349 2.4836132526397705 14.100028991699219
Loss :  1.6861751079559326 2.4564545154571533 13.9684476852417
Loss :  1.698002576828003 1.8330445289611816 10.863224983215332
Loss :  1.6732412576675415 2.354849100112915 13.447486877441406
Loss :  1.6980020999908447 2.0890181064605713 12.143092155456543
Loss :  1.6810048818588257 1.839416742324829 10.878087997436523
Loss :  1.6744357347488403 2.0396132469177246 11.872502326965332
Loss :  1.6984193325042725 1.8408771753311157 10.90280532836914
Loss :  1.6686087846755981 2.3225576877593994 13.281396865844727
Loss :  1.6681703329086304 2.2289505004882812 12.812922477722168
Loss :  1.6664974689483643 2.5375723838806152 14.35435962677002
Loss :  1.6702821254730225 2.5834922790527344 14.587743759155273
Loss :  1.7083356380462646 2.509063243865967 14.253652572631836
Loss :  1.7046325206756592 1.8509955406188965 10.959610939025879
Loss :  1.6641793251037598 2.3131189346313477 13.229774475097656
Loss :  1.6824434995651245 2.116569995880127 12.26529312133789
Loss :  1.6613423824310303 1.437843680381775 8.850561141967773
Loss :  1.7039575576782227 1.9589108228683472 11.49851131439209
  batch 20 loss: 1.7039575576782227, 1.9589108228683472, 11.49851131439209
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6783145666122437 1.8375898599624634 10.866263389587402
Loss :  1.6608600616455078 1.759132981300354 10.456524848937988
Loss :  1.6734898090362549 1.6226986646652222 9.786982536315918
Loss :  1.6842039823532104 1.740025281906128 10.384330749511719
Loss :  1.704633355140686 2.840956449508667 15.909416198730469
Loss :  1.6759525537490845 2.6835827827453613 15.093866348266602
Loss :  1.6828677654266357 2.755263328552246 15.459184646606445
Loss :  1.6790759563446045 3.170206308364868 17.530107498168945
Loss :  1.643545150756836 2.1928024291992188 12.60755729675293
Loss :  1.702312707901001 2.3319625854492188 13.362125396728516
Loss :  1.6436959505081177 2.6542298793792725 14.91484546661377
Loss :  1.6903427839279175 2.268407106399536 13.032378196716309
Loss :  1.6730899810791016 2.212297201156616 12.734576225280762
Loss :  1.6716225147247314 2.4227089881896973 13.78516674041748
Loss :  1.6474599838256836 2.4470949172973633 13.8829345703125
Loss :  1.657440423965454 2.5956223011016846 14.635551452636719
Loss :  1.6564197540283203 2.58430552482605 14.577947616577148
Loss :  1.698707938194275 2.2975480556488037 13.186448097229004
Loss :  1.7022478580474854 2.2421557903289795 12.913026809692383
Loss :  1.7089186906814575 2.4456875324249268 13.937355995178223
  batch 40 loss: 1.7089186906814575, 2.4456875324249268, 13.937355995178223
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6806285381317139 2.8143985271453857 15.7526216506958
Loss :  1.6712987422943115 2.1701669692993164 12.522133827209473
Loss :  1.6641325950622559 2.0124990940093994 11.726627349853516
Loss :  1.6734130382537842 3.0263137817382812 16.804981231689453
Loss :  1.6586024761199951 1.759865641593933 10.457930564880371
Loss :  1.6799019575119019 2.0243020057678223 11.801411628723145
Loss :  1.702818751335144 2.0893921852111816 12.149779319763184
Loss :  1.6696540117263794 3.00469970703125 16.693153381347656
Loss :  1.7130461931228638 2.2569897174835205 12.997994422912598
Loss :  1.6741245985031128 2.7236597537994385 15.292423248291016
Loss :  1.6969629526138306 2.5139241218566895 14.266583442687988
Loss :  1.6926541328430176 2.2820231914520264 13.10276985168457
Loss :  1.6782431602478027 2.6519856452941895 14.93817138671875
Loss :  1.6982098817825317 3.6625518798828125 20.010969161987305
Loss :  1.6704647541046143 2.543689012527466 14.388909339904785
Loss :  1.7136377096176147 2.239241600036621 12.909845352172852
Loss :  1.6749376058578491 2.107039451599121 12.210134506225586
Loss :  1.6638742685317993 1.9751782417297363 11.539765357971191
Loss :  1.6742503643035889 3.647948980331421 19.91399383544922
Loss :  1.718217134475708 1.687772512435913 10.157079696655273
  batch 60 loss: 1.718217134475708, 1.687772512435913, 10.157079696655273
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6634477376937866 3.107442617416382 17.200660705566406
Loss :  1.6830695867538452 3.1247429847717285 17.30678367614746
Loss :  1.6693363189697266 1.6592121124267578 9.965396881103516
Loss :  1.6604701280593872 2.4786148071289062 14.053544044494629
Loss :  1.6468205451965332 2.7303781509399414 15.298711776733398
Loss :  1.6715055704116821 3.99332594871521 21.638134002685547
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6797504425048828 4.016292095184326 21.761211395263672
Loss :  1.6764904260635376 3.816556692123413 20.759273529052734
Loss :  1.6846834421157837 3.6110661029815674 19.740013122558594
Total LOSS train 13.460478577247033 valid 20.974658012390137
CE LOSS train 1.680222329726586 valid 0.4211708605289459
Contrastive LOSS train 2.3560512671103844 valid 0.9027665257453918
EPOCH 280:
Loss :  1.6925768852233887 2.3879714012145996 13.632434844970703
Loss :  1.7013188600540161 2.3710434436798096 13.556535720825195
Loss :  1.6801698207855225 2.261690616607666 12.988622665405273
Loss :  1.684564471244812 2.2309443950653076 12.839286804199219
Loss :  1.6967649459838867 1.7791669368743896 10.592599868774414
Loss :  1.6718995571136475 1.8981586694717407 11.16269302368164
Loss :  1.6970338821411133 1.9621535539627075 11.50780200958252
Loss :  1.68010413646698 3.3406808376312256 18.383506774902344
Loss :  1.6735397577285767 3.469689130783081 19.02198600769043
Loss :  1.6975016593933105 2.28108286857605 13.102916717529297
Loss :  1.668120265007019 2.8100626468658447 15.718433380126953
Loss :  1.6676781177520752 2.423661947250366 13.785987854003906
Loss :  1.665852427482605 2.454542398452759 13.93856430053711
Loss :  1.6699575185775757 2.368062734603882 13.510271072387695
Loss :  1.7081472873687744 2.2729194164276123 13.072744369506836
Loss :  1.7042101621627808 2.205064535140991 12.729533195495605
Loss :  1.663935661315918 2.228235960006714 12.805115699768066
Loss :  1.6821476221084595 2.427133798599243 13.817816734313965
Loss :  1.6610502004623413 1.9087989330291748 11.205044746398926
Loss :  1.703218698501587 2.528684139251709 14.346639633178711
  batch 20 loss: 1.703218698501587, 2.528684139251709, 14.346639633178711
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.678254246711731 2.140742540359497 12.381966590881348
Loss :  1.6601262092590332 2.1490252017974854 12.405252456665039
Loss :  1.6722912788391113 1.8160136938095093 10.752359390258789
Loss :  1.6834760904312134 1.9815086126327515 11.591019630432129
Loss :  1.703964114189148 2.61397385597229 14.773833274841309
Loss :  1.6747890710830688 2.4063398838043213 13.706488609313965
Loss :  1.6819905042648315 2.365490198135376 13.509441375732422
Loss :  1.6770793199539185 2.697829484939575 15.166226387023926
Loss :  1.6420754194259644 2.1731626987457275 12.507888793945312
Loss :  1.700536847114563 2.4906067848205566 14.153570175170898
Loss :  1.6429394483566284 2.1360318660736084 12.323099136352539
Loss :  1.689961314201355 2.318148612976074 13.280704498291016
Loss :  1.6727838516235352 2.329073429107666 13.318150520324707
Loss :  1.6714645624160767 2.514676094055176 14.244845390319824
Loss :  1.6477822065353394 2.6741487979888916 15.018526077270508
Loss :  1.6580756902694702 2.3784432411193848 13.550291061401367
Loss :  1.657142996788025 3.1186599731445312 17.250442504882812
Loss :  1.69913649559021 2.895449638366699 16.17638397216797
Loss :  1.7027052640914917 2.310244083404541 13.253925323486328
Loss :  1.7093433141708374 2.0777924060821533 12.098304748535156
  batch 40 loss: 1.7093433141708374, 2.0777924060821533, 12.098304748535156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6808290481567383 2.149371862411499 12.427688598632812
Loss :  1.6711076498031616 1.8035244941711426 10.688729286193848
Loss :  1.6632294654846191 3.175584554672241 17.541152954101562
Loss :  1.6727070808410645 2.5366742610931396 14.3560791015625
Loss :  1.6573665142059326 1.9700207710266113 11.50747013092041
Loss :  1.678548812866211 2.2195417881011963 12.776257514953613
Loss :  1.7015689611434937 1.7858821153640747 10.630979537963867
Loss :  1.6678202152252197 1.9517688751220703 11.426664352416992
Loss :  1.7120082378387451 3.0032224655151367 16.728120803833008
Loss :  1.672453761100769 2.5031042098999023 14.18797492980957
Loss :  1.6952139139175415 4.312300205230713 23.2567138671875
Loss :  1.6913862228393555 1.9459052085876465 11.420912742614746
Loss :  1.676776647567749 1.8109697103500366 10.7316255569458
Loss :  1.6974828243255615 2.495314836502075 14.174057006835938
Loss :  1.6685510873794556 2.1677868366241455 12.507485389709473
Loss :  1.7126041650772095 1.6433579921722412 9.929394721984863
Loss :  1.6741939783096313 1.9192695617675781 11.27054214477539
Loss :  1.6631823778152466 1.676764726638794 10.047005653381348
Loss :  1.673925757408142 2.358485698699951 13.466354370117188
Loss :  1.7178622484207153 2.905914783477783 16.2474365234375
  batch 60 loss: 1.7178622484207153, 2.905914783477783, 16.2474365234375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6642063856124878 3.4214601516723633 18.77150535583496
Loss :  1.6841150522232056 2.8541839122772217 15.955035209655762
Loss :  1.6706773042678833 2.3496010303497314 13.418682098388672
Loss :  1.661939024925232 2.2549448013305664 12.936662673950195
Loss :  1.6482800245285034 1.638659954071045 9.841580390930176
Loss :  1.6739224195480347 4.0326433181762695 21.83713722229004
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6819206476211548 4.107399940490723 22.21891975402832
Loss :  1.679008960723877 3.9552621841430664 21.455318450927734
Loss :  1.686829686164856 3.763657331466675 20.505117416381836
Total LOSS train 13.529651788564829 valid 21.504123210906982
CE LOSS train 1.6795961068226741 valid 0.421707421541214
Contrastive LOSS train 2.370011142583994 valid 0.9409143328666687
EPOCH 281:
Loss :  1.693868637084961 1.9321026802062988 11.354381561279297
Loss :  1.702982783317566 2.5554494857788086 14.480230331420898
Loss :  1.681823492050171 1.8140318393707275 10.751982688903809
Loss :  1.6858328580856323 2.421149969100952 13.791583061218262
Loss :  1.6979373693466187 2.07705020904541 12.0831880569458
Loss :  1.6733081340789795 2.0916693210601807 12.131654739379883
Loss :  1.6983633041381836 2.1502034664154053 12.449380874633789
Loss :  1.681545376777649 1.876371145248413 11.063401222229004
Loss :  1.6747667789459229 2.559199810028076 14.470766067504883
Loss :  1.6990091800689697 2.33555269241333 13.376771926879883
Loss :  1.6690107583999634 2.6546671390533447 14.942346572875977
Loss :  1.6681568622589111 1.970909595489502 11.522704124450684
Loss :  1.6664613485336304 2.3219387531280518 13.276154518127441
Loss :  1.6697403192520142 2.0032777786254883 11.686129570007324
Loss :  1.707958698272705 1.8631553649902344 11.023735046386719
Loss :  1.7046986818313599 1.8716925382614136 11.06316089630127
Loss :  1.663926362991333 2.814481258392334 15.736332893371582
Loss :  1.6827294826507568 2.170945167541504 12.537455558776855
Loss :  1.6612292528152466 2.2275238037109375 12.798848152160645
Loss :  1.7042418718338013 2.1134352684020996 12.271418571472168
  batch 20 loss: 1.7042418718338013, 2.1134352684020996, 12.271418571472168
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6781243085861206 2.0392956733703613 11.874602317810059
Loss :  1.6603448390960693 1.9792062044143677 11.556376457214355
Loss :  1.6732169389724731 1.832098126411438 10.833707809448242
Loss :  1.6839567422866821 1.811777949333191 10.742846488952637
Loss :  1.704737663269043 3.0309596061706543 16.859535217285156
Loss :  1.6756494045257568 3.024425506591797 16.79777717590332
Loss :  1.6825876235961914 2.9867758750915527 16.616466522216797
Loss :  1.6782976388931274 2.62859845161438 14.821290016174316
Loss :  1.64229416847229 1.8817094564437866 11.050841331481934
Loss :  1.7018247842788696 2.179443120956421 12.599040985107422
Loss :  1.6426624059677124 1.8589038848876953 10.93718147277832
Loss :  1.6902332305908203 2.4477455615997314 13.928960800170898
Loss :  1.6731233596801758 2.409301519393921 13.71963119506836
Loss :  1.6717413663864136 2.4018030166625977 13.680756568908691
Loss :  1.64772367477417 3.183960437774658 17.56752586364746
Loss :  1.6580275297164917 2.143312692642212 12.374590873718262
Loss :  1.6571567058563232 2.1376821994781494 12.34556770324707
Loss :  1.699918270111084 3.1329469680786133 17.364652633666992
Loss :  1.7034469842910767 1.7465401887893677 10.436148643493652
Loss :  1.7104521989822388 2.3619353771209717 13.520129203796387
  batch 40 loss: 1.7104521989822388, 2.3619353771209717, 13.520129203796387
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6821389198303223 3.140272378921509 17.383501052856445
Loss :  1.6725636720657349 3.145827293395996 17.401700973510742
Loss :  1.6649142503738403 2.0066189765930176 11.69800853729248
Loss :  1.674517273902893 2.0523946285247803 11.936491012573242
Loss :  1.6592156887054443 1.8551944494247437 10.935188293457031
Loss :  1.6804882287979126 1.770215630531311 10.531566619873047
Loss :  1.7033840417861938 1.6469968557357788 9.938368797302246
Loss :  1.669888973236084 1.6030678749084473 9.68522834777832
Loss :  1.7133052349090576 1.6339592933654785 9.883101463317871
Loss :  1.674162745475769 1.4670554399490356 9.009439468383789
Loss :  1.697058081626892 2.3609423637390137 13.50177001953125
Loss :  1.6930149793624878 2.2520835399627686 12.9534330368042
Loss :  1.6785738468170166 1.580952525138855 9.58333683013916
Loss :  1.6988525390625 2.3968496322631836 13.683100700378418
Loss :  1.6707481145858765 2.546361207962036 14.40255355834961
Loss :  1.7144135236740112 2.3812766075134277 13.620797157287598
Loss :  1.675942063331604 1.794998288154602 10.650934219360352
Loss :  1.6647237539291382 1.6172744035720825 9.75109577178955
Loss :  1.6755659580230713 3.0360941886901855 16.856037139892578
Loss :  1.7194424867630005 2.046762466430664 11.953254699707031
  batch 60 loss: 1.7194424867630005, 2.046762466430664, 11.953254699707031
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6649060249328613 2.98816180229187 16.605714797973633
Loss :  1.6844139099121094 2.598433494567871 14.676581382751465
Loss :  1.670727252960205 2.1765332221984863 12.55339241027832
Loss :  1.662195086479187 2.443600654602051 13.88019847869873
Loss :  1.6484571695327759 1.3906424045562744 8.601669311523438
Loss :  1.6795638799667358 4.3235273361206055 23.29720115661621
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6871830224990845 4.368447303771973 23.5294189453125
Loss :  1.6847753524780273 4.266794204711914 23.01874542236328
Loss :  1.6904221773147583 4.265204429626465 23.01644515991211
Total LOSS train 12.832549535311186 valid 23.215452671051025
CE LOSS train 1.6805650032483614 valid 0.4226055443286896
Contrastive LOSS train 2.2303969034781823 valid 1.0663011074066162
EPOCH 282:
Loss :  1.693837285041809 2.739185333251953 15.389763832092285
Loss :  1.7028188705444336 2.75246000289917 15.465119361877441
Loss :  1.6817106008529663 2.4512388706207275 13.937904357910156
Loss :  1.6860971450805664 2.5226380825042725 14.299287796020508
Loss :  1.6981850862503052 2.422313928604126 13.809754371643066
Loss :  1.6737741231918335 2.5549376010894775 14.448461532592773
Loss :  1.6988537311553955 2.3702597618103027 13.550152778625488
Loss :  1.6818504333496094 2.1494009494781494 12.428854942321777
Loss :  1.6748383045196533 2.259000062942505 12.96983814239502
Loss :  1.6988214254379272 2.1715447902679443 12.55654525756836
Loss :  1.6686182022094727 2.601811647415161 14.6776762008667
Loss :  1.6680492162704468 2.256394863128662 12.950023651123047
Loss :  1.6661137342453003 1.7648053169250488 10.490139961242676
Loss :  1.669859528541565 1.91178560256958 11.228787422180176
Loss :  1.7076148986816406 2.3992409706115723 13.703819274902344
Loss :  1.7049108743667603 2.5165038108825684 14.287430763244629
Loss :  1.6633332967758179 1.8783329725265503 11.054997444152832
Loss :  1.6823042631149292 1.5852006673812866 9.608307838439941
Loss :  1.6604825258255005 2.010521173477173 11.713088035583496
Loss :  1.7031904458999634 2.2136428356170654 12.771405220031738
  batch 20 loss: 1.7031904458999634, 2.2136428356170654, 12.771405220031738
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 4], device='cuda:0')
Loss :  1.6774253845214844 2.066409111022949 12.00947093963623
Loss :  1.659353256225586 2.43934965133667 13.856101989746094
Loss :  1.6718511581420898 2.571169853210449 14.527700424194336
Loss :  1.6831594705581665 2.318108081817627 13.273699760437012
Loss :  1.703783631324768 3.1009957790374756 17.208763122558594
Loss :  1.674580693244934 2.620009183883667 14.774626731872559
Loss :  1.6816591024398804 2.964089870452881 16.50210952758789
Loss :  1.6773360967636108 2.3633313179016113 13.49399185180664
Loss :  1.641983985900879 2.7761971950531006 15.522970199584961
Loss :  1.70094895362854 2.8570287227630615 15.986092567443848
Loss :  1.6430209875106812 2.5972137451171875 14.62908935546875
Loss :  1.6903619766235352 2.4532575607299805 13.956649780273438
Loss :  1.6735918521881104 1.906594157218933 11.206562042236328
Loss :  1.67251718044281 2.043388605117798 11.889459609985352
Loss :  1.6485931873321533 2.3801047801971436 13.549117088317871
Loss :  1.6590626239776611 1.9122931957244873 11.220528602600098
Loss :  1.6584035158157349 1.8798757791519165 11.057782173156738
Loss :  1.7003028392791748 1.9243911504745483 11.322258949279785
Loss :  1.7037876844406128 1.7995655536651611 10.701615333557129
Loss :  1.7105059623718262 2.3114569187164307 13.267789840698242
  batch 40 loss: 1.7105059623718262, 2.3114569187164307, 13.267789840698242
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6823149919509888 2.1756324768066406 12.560477256774902
Loss :  1.6730307340621948 1.573797583580017 9.54201889038086
Loss :  1.6657421588897705 2.187401294708252 12.602747917175293
Loss :  1.674964427947998 2.07102632522583 12.030096054077148
Loss :  1.6597247123718262 2.7465944290161133 15.392696380615234
Loss :  1.680953025817871 2.265770196914673 13.009803771972656
Loss :  1.7035375833511353 2.402466297149658 13.715868949890137
Loss :  1.6698623895645142 1.8856391906738281 11.098058700561523
Loss :  1.7133418321609497 1.8464910984039307 10.945796966552734
Loss :  1.6741559505462646 1.6425377130508423 9.886844635009766
Loss :  1.6966638565063477 2.690295457839966 15.148140907287598
Loss :  1.6926862001419067 2.519700765609741 14.291190147399902
Loss :  1.6782726049423218 2.1160027980804443 12.258286476135254
Loss :  1.6982777118682861 1.804829716682434 10.722426414489746
Loss :  1.670271396636963 1.7428804636001587 10.384674072265625
Loss :  1.7133328914642334 2.6244094371795654 14.835380554199219
Loss :  1.6752675771713257 2.6371750831604004 14.8611421585083
Loss :  1.6643229722976685 2.5403947830200195 14.366296768188477
Loss :  1.6749268770217896 2.315464973449707 13.252251625061035
Loss :  1.7181569337844849 2.226478338241577 12.85054874420166
  batch 60 loss: 1.7181569337844849, 2.226478338241577, 12.85054874420166
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.664453387260437 2.7134881019592285 15.231893539428711
Loss :  1.684192180633545 2.831552028656006 15.84195327758789
Loss :  1.6704236268997192 2.8836400508880615 16.088624954223633
Loss :  1.6619545221328735 2.2942352294921875 13.13313102722168
Loss :  1.648296594619751 1.441932201385498 8.85795783996582
Loss :  1.6779603958129883 4.303513050079346 23.195526123046875
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6859190464019775 4.3790764808654785 23.581300735473633
Loss :  1.6824713945388794 4.219637393951416 22.780658721923828
Loss :  1.6939167976379395 4.206218242645264 22.725008010864258
Total LOSS train 13.14163147852971 valid 23.07062339782715
CE LOSS train 1.6804095488328201 valid 0.42347919940948486
Contrastive LOSS train 2.2922443921749407 valid 1.051554560661316
EPOCH 283:
Loss :  1.693219780921936 2.0266969203948975 11.826704978942871
Loss :  1.7020245790481567 2.400022268295288 13.702136039733887
Loss :  1.6807498931884766 2.308053731918335 13.22101879119873
Loss :  1.6848427057266235 2.6758828163146973 15.06425666809082
Loss :  1.6968719959259033 2.0107944011688232 11.75084400177002
Loss :  1.6717299222946167 2.494051933288574 14.141989707946777
Loss :  1.696494698524475 3.3775582313537598 18.584285736083984
Loss :  1.6796976327896118 2.9337477684020996 16.34843635559082
Loss :  1.6732916831970215 2.2871501445770264 13.10904312133789
Loss :  1.6970422267913818 1.7178614139556885 10.286349296569824
Loss :  1.6676361560821533 2.533700704574585 14.336139678955078
Loss :  1.6677297353744507 2.8784267902374268 16.059864044189453
Loss :  1.6658014059066772 2.420893430709839 13.770268440246582
Loss :  1.669528841972351 2.4874606132507324 14.106832504272461
Loss :  1.7077523469924927 2.657203197479248 14.993768692016602
Loss :  1.7041574716567993 2.000227212905884 11.705293655395508
Loss :  1.6636236906051636 1.9978318214416504 11.652782440185547
Loss :  1.681777000427246 2.052691698074341 11.945235252380371
Loss :  1.6607903242111206 1.871906042098999 11.020320892333984
Loss :  1.703334927558899 1.9420417547225952 11.413543701171875
  batch 20 loss: 1.703334927558899, 1.9420417547225952, 11.413543701171875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.677958369255066 2.1450419425964355 12.403168678283691
Loss :  1.6602308750152588 1.9476240873336792 11.398350715637207
Loss :  1.6723850965499878 1.818662166595459 10.76569652557373
Loss :  1.6836344003677368 1.9027326107025146 11.197298049926758
Loss :  1.7040272951126099 2.594313144683838 14.675592422485352
Loss :  1.674926996231079 1.9335882663726807 11.342867851257324
Loss :  1.6820770502090454 2.6847989559173584 15.106072425842285
Loss :  1.6778335571289062 1.9211550951004028 11.283609390258789
Loss :  1.642656683921814 2.3197526931762695 13.241419792175293
Loss :  1.7016180753707886 2.376269817352295 13.582967758178711
Loss :  1.6438169479370117 2.70544695854187 15.171051979064941
Loss :  1.6906194686889648 2.5552546977996826 14.466893196105957
Loss :  1.673768162727356 2.495255947113037 14.15004825592041
Loss :  1.6725046634674072 2.25221848487854 12.93359661102295
Loss :  1.6488640308380127 2.4346604347229004 13.822165489196777
Loss :  1.65896737575531 2.0554630756378174 11.93628215789795
Loss :  1.6579564809799194 2.0893478393554688 12.104695320129395
Loss :  1.699626088142395 2.4962594509124756 14.180923461914062
Loss :  1.703303337097168 2.287559986114502 13.14110279083252
Loss :  1.710013747215271 1.9916483163833618 11.668254852294922
  batch 40 loss: 1.710013747215271, 1.9916483163833618, 11.668254852294922
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6817781925201416 1.7827718257904053 10.595637321472168
Loss :  1.672404408454895 1.953213095664978 11.438469886779785
Loss :  1.6651030778884888 2.630720853805542 14.818707466125488
Loss :  1.6743234395980835 1.970556378364563 11.527105331420898
Loss :  1.6594370603561401 1.5562962293624878 9.44091796875
Loss :  1.6804405450820923 1.9453275203704834 11.407078742980957
Loss :  1.7030622959136963 1.7388547658920288 10.39733600616455
Loss :  1.6697443723678589 4.07783842086792 22.058937072753906
Loss :  1.7132774591445923 3.887772560119629 21.15213966369629
Loss :  1.6742241382598877 2.0029304027557373 11.688876152038574
Loss :  1.696847677230835 2.0708320140838623 12.051008224487305
Loss :  1.6928515434265137 1.955040454864502 11.468053817749023
Loss :  1.6785590648651123 1.7618275880813599 10.48769760131836
Loss :  1.6984549760818481 2.3201394081115723 13.299151420593262
Loss :  1.6707526445388794 2.26145339012146 12.978019714355469
Loss :  1.7135426998138428 3.3181467056274414 18.304275512695312
Loss :  1.6755422353744507 2.4753875732421875 14.05247974395752
Loss :  1.6650034189224243 1.9536457061767578 11.433232307434082
Loss :  1.675514578819275 2.7513718605041504 15.432373046875
Loss :  1.7189233303070068 2.3673617839813232 13.555732727050781
  batch 60 loss: 1.7189233303070068, 2.3673617839813232, 13.555732727050781
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6654088497161865 2.46142578125 13.972537994384766
Loss :  1.6850897073745728 2.1370527744293213 12.370353698730469
Loss :  1.6715290546417236 2.0796027183532715 12.06954288482666
Loss :  1.6629270315170288 2.705267906188965 15.189266204833984
Loss :  1.6495484113693237 2.304534912109375 13.172223091125488
Loss :  1.6653426885604858 3.9397027492523193 21.363855361938477
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1], device='cuda:0')
Loss :  1.673775553703308 3.9123048782348633 21.23529815673828
Loss :  1.670597791671753 3.866997480392456 21.005584716796875
Loss :  1.6780039072036743 3.6724207401275635 20.04010772705078
Total LOSS train 13.26111269730788 valid 20.911211490631104
CE LOSS train 1.6802950143814086 valid 0.4195009768009186
Contrastive LOSS train 2.3161635307165294 valid 0.9181051850318909
EPOCH 284:
Loss :  1.6943683624267578 1.9205108880996704 11.29692268371582
Loss :  1.703062653541565 2.937499761581421 16.390562057495117
Loss :  1.6817854642868042 2.206458330154419 12.71407699584961
Loss :  1.685882568359375 2.2507293224334717 12.939529418945312
Loss :  1.697603702545166 2.176178216934204 12.578495025634766
Loss :  1.672925591468811 1.9451119899749756 11.398486137390137
Loss :  1.6974953413009644 2.2497987747192383 12.946489334106445
Loss :  1.6804370880126953 2.2986650466918945 13.173762321472168
Loss :  1.6733876466751099 2.569183349609375 14.519304275512695
Loss :  1.6967408657073975 2.0316929817199707 11.855205535888672
Loss :  1.6675137281417847 2.2622084617614746 12.978556632995605
Loss :  1.6671515703201294 2.1523756980895996 12.429030418395996
Loss :  1.6651420593261719 2.0482819080352783 11.906551361083984
Loss :  1.6690047979354858 2.111926555633545 12.2286376953125
Loss :  1.7071633338928223 1.9403539896011353 11.408933639526367
Loss :  1.7044140100479126 2.464249610900879 14.025662422180176
Loss :  1.6642082929611206 2.3805620670318604 13.567018508911133
Loss :  1.6829872131347656 2.2591326236724854 12.978650093078613
Loss :  1.6621557474136353 2.9771268367767334 16.54779052734375
Loss :  1.7046984434127808 2.5143065452575684 14.27623176574707
  batch 20 loss: 1.7046984434127808, 2.5143065452575684, 14.27623176574707
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6795130968093872 2.4890074729919434 14.124550819396973
Loss :  1.661916732788086 2.405150890350342 13.687671661376953
Loss :  1.6740362644195557 2.534877300262451 14.34842300415039
Loss :  1.6848148107528687 1.947500228881836 11.42231559753418
Loss :  1.704617977142334 2.306368112564087 13.236457824707031
Loss :  1.6761209964752197 2.173814296722412 12.54519271850586
Loss :  1.6830590963363647 1.8626197576522827 10.9961576461792
Loss :  1.6785838603973389 2.0889432430267334 12.123300552368164
Loss :  1.643515944480896 2.374812126159668 13.517576217651367
Loss :  1.7017426490783691 2.5172948837280273 14.288217544555664
Loss :  1.6440844535827637 2.2790744304656982 13.039457321166992
Loss :  1.6908659934997559 2.1772234439849854 12.576982498168945
Loss :  1.6740299463272095 2.0966029167175293 12.157045364379883
Loss :  1.6728112697601318 2.827359199523926 15.80960750579834
Loss :  1.6491484642028809 2.5123977661132812 14.211137771606445
Loss :  1.6592813730239868 2.228879451751709 12.803679466247559
Loss :  1.658478856086731 2.196847677230835 12.642717361450195
Loss :  1.7001796960830688 2.280041456222534 13.100387573242188
Loss :  1.703780174255371 2.029733180999756 11.852446556091309
Loss :  1.7103855609893799 1.962139368057251 11.521081924438477
  batch 40 loss: 1.7103855609893799, 1.962139368057251, 11.521081924438477
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.6820564270019531 1.995388150215149 11.658997535705566
Loss :  1.6727690696716309 1.6524555683135986 9.935047149658203
Loss :  1.6651562452316284 2.015695333480835 11.743633270263672
Loss :  1.6746398210525513 2.0424320697784424 11.886799812316895
Loss :  1.6593645811080933 4.147799968719482 22.398365020751953
Loss :  1.6805049180984497 4.040000915527344 21.880510330200195
Loss :  1.7031667232513428 1.9777718782424927 11.592026710510254
Loss :  1.6698538064956665 1.983319878578186 11.586453437805176
Loss :  1.7135257720947266 1.8054815530776978 10.740933418273926
Loss :  1.674375295639038 1.8888137340545654 11.118444442749023
Loss :  1.6970815658569336 2.717827320098877 15.28621768951416
Loss :  1.6928927898406982 2.571599245071411 14.550889015197754
Loss :  1.678733229637146 1.886705994606018 11.112262725830078
Loss :  1.6987154483795166 1.8955130577087402 11.176280975341797
Loss :  1.6708446741104126 2.540653705596924 14.374113082885742
Loss :  1.7133561372756958 2.736300230026245 15.394857406616211
Loss :  1.675822377204895 4.199227333068848 22.671958923339844
Loss :  1.6651676893234253 3.336099863052368 18.345666885375977
Loss :  1.6756596565246582 2.2493159770965576 12.922239303588867
Loss :  1.7192600965499878 1.813234567642212 10.785432815551758
  batch 60 loss: 1.7192600965499878, 1.813234567642212, 10.785432815551758
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.6656391620635986 1.932381272315979 11.327545166015625
Loss :  1.6852564811706543 1.84415602684021 10.906036376953125
Loss :  1.6718535423278809 2.4182865619659424 13.763286590576172
Loss :  1.6635236740112305 2.657646417617798 14.95175552368164
Loss :  1.6498868465423584 1.9834094047546387 11.566934585571289
Loss :  1.6829602718353271 4.3238630294799805 23.302276611328125
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.691131830215454 4.260031223297119 22.991287231445312
Loss :  1.6883381605148315 4.1834306716918945 22.60548973083496
Loss :  1.6959251165390015 4.026854991912842 21.8302001953125
Total LOSS train 13.320630645751953 valid 22.682313442230225
CE LOSS train 1.6807415650441095 valid 0.42398127913475037
Contrastive LOSS train 2.327977787531339 valid 1.0067137479782104
EPOCH 285:
Loss :  1.6947689056396484 2.073124408721924 12.06039047241211
Loss :  1.7034229040145874 2.7952284812927246 15.6795654296875
Loss :  1.6824557781219482 2.141923189163208 12.392071723937988
Loss :  1.686454176902771 2.5503089427948 14.43799877166748
Loss :  1.6983096599578857 2.054065465927124 11.968637466430664
Loss :  1.6738336086273193 2.0749282836914062 12.04847526550293
Loss :  1.6984350681304932 2.320952892303467 13.303199768066406
Loss :  1.6819030046463013 2.34130597114563 13.388432502746582
Loss :  1.6754300594329834 2.3320538997650146 13.335700035095215
Loss :  1.698936104774475 1.5192171335220337 9.295022010803223
Loss :  1.669625163078308 2.0769546031951904 12.054398536682129
Loss :  1.6692709922790527 1.967779517173767 11.508169174194336
Loss :  1.6673675775527954 2.5764575004577637 14.54965591430664
Loss :  1.671231746673584 2.061086893081665 11.976665496826172
Loss :  1.7088370323181152 2.6624608039855957 15.021141052246094
Loss :  1.7051236629486084 2.150989055633545 12.46006965637207
Loss :  1.6650142669677734 2.2438387870788574 12.884208679199219
Loss :  1.683031439781189 1.9114562273025513 11.240312576293945
Loss :  1.661966323852539 1.7757418155670166 10.540675163269043
Loss :  1.7040694952011108 1.8451447486877441 10.929792404174805
  batch 20 loss: 1.7040694952011108, 1.8451447486877441, 10.929792404174805
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6790380477905273 1.7830376625061035 10.594225883483887
Loss :  1.6615192890167236 2.5271761417388916 14.297399520874023
Loss :  1.6738181114196777 2.061353921890259 11.980587005615234
Loss :  1.6845853328704834 3.3364973068237305 18.36707305908203
Loss :  1.7045944929122925 3.7729949951171875 20.569568634033203
Loss :  1.6761623620986938 1.912060260772705 11.23646354675293
Loss :  1.683085560798645 2.2836639881134033 13.101405143737793
Loss :  1.6789382696151733 2.2980895042419434 13.16938591003418
Loss :  1.6439640522003174 2.024543285369873 11.766680717468262
Loss :  1.702051043510437 1.981940746307373 11.61175537109375
Loss :  1.644682765007019 2.0418450832366943 11.85390853881836
Loss :  1.6911934614181519 1.8079545497894287 10.730966567993164
Loss :  1.6743675470352173 2.266622304916382 13.007479667663574
Loss :  1.6732685565948486 2.131329298019409 12.329915046691895
Loss :  1.6496756076812744 2.1122264862060547 12.210807800292969
Loss :  1.6600725650787354 2.8099961280822754 15.710052490234375
Loss :  1.6593040227890015 2.061556577682495 11.967086791992188
Loss :  1.7009934186935425 1.965480089187622 11.528393745422363
Loss :  1.7044360637664795 1.9072097539901733 11.240485191345215
Loss :  1.7110928297042847 1.8992048501968384 11.207117080688477
  batch 40 loss: 1.7110928297042847, 1.8992048501968384, 11.207117080688477
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6833003759384155 1.9843907356262207 11.605253219604492
Loss :  1.6736520528793335 1.835569977760315 10.85150146484375
Loss :  1.6658883094787598 2.040247917175293 11.867128372192383
Loss :  1.6751381158828735 1.948705792427063 11.418667793273926
Loss :  1.6601797342300415 2.0980045795440674 12.150202751159668
Loss :  1.6812634468078613 3.2697346210479736 18.029935836791992
Loss :  1.7036834955215454 2.886183500289917 16.134601593017578
Loss :  1.6704384088516235 2.67901349067688 15.065505981445312
Loss :  1.7133169174194336 2.7525107860565186 15.475871086120605
Loss :  1.6746389865875244 2.609858751296997 14.723932266235352
Loss :  1.697137475013733 3.384289026260376 18.61858367919922
Loss :  1.6930890083312988 2.9055886268615723 16.221031188964844
Loss :  1.6789336204528809 1.6674978733062744 10.016422271728516
Loss :  1.6988279819488525 2.267970323562622 13.038679122924805
Loss :  1.6706289052963257 2.2131905555725098 12.736580848693848
Loss :  1.713875651359558 2.4706242084503174 14.066996574401855
Loss :  1.675891637802124 2.565612554550171 14.503954887390137
Loss :  1.6650019884109497 2.074551582336426 12.037759780883789
Loss :  1.6755249500274658 2.42975115776062 13.824280738830566
Loss :  1.7181848287582397 2.3426785469055176 13.4315767288208
  batch 60 loss: 1.7181848287582397, 2.3426785469055176, 13.4315767288208
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6651544570922852 2.523454189300537 14.282425880432129
Loss :  1.6852107048034668 2.284182071685791 13.106121063232422
Loss :  1.6718188524246216 2.2021090984344482 12.682364463806152
Loss :  1.6638826131820679 1.9776948690414429 11.552356719970703
Loss :  1.6505472660064697 1.5579283237457275 9.44018840789795
Loss :  1.6693655252456665 4.098443508148193 22.161582946777344
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6782625913619995 4.157544136047363 22.46598243713379
Loss :  1.6775329113006592 4.076131343841553 22.058189392089844
Loss :  1.6741880178451538 3.89943528175354 21.17136573791504
Total LOSS train 13.022111731309158 valid 21.964280128479004
CE LOSS train 1.6812544639293965 valid 0.41854700446128845
Contrastive LOSS train 2.2681714571439304 valid 0.974858820438385
EPOCH 286:
Loss :  1.6946077346801758 1.8661715984344482 11.025465965270996
Loss :  1.7037373781204224 2.088327169418335 12.145373344421387
Loss :  1.6831048727035522 1.9913756847381592 11.639983177185059
Loss :  1.687180995941162 2.1598856449127197 12.486608505249023
Loss :  1.6985896825790405 2.4031264781951904 13.714221954345703
Loss :  1.6739085912704468 2.614788055419922 14.747848510742188
Loss :  1.6981573104858398 1.855334758758545 10.974831581115723
Loss :  1.681241512298584 1.7620576620101929 10.49152946472168
Loss :  1.6744780540466309 1.7746363878250122 10.547660827636719
Loss :  1.697511076927185 1.700461983680725 10.199820518493652
Loss :  1.6686346530914307 2.6176414489746094 14.756841659545898
Loss :  1.6684008836746216 2.6173770427703857 14.75528621673584
Loss :  1.666420817375183 2.351356029510498 13.423201560974121
Loss :  1.6703705787658691 2.5966758728027344 14.653749465942383
Loss :  1.7075693607330322 2.3413777351379395 13.414457321166992
Loss :  1.7044965028762817 2.2100229263305664 12.754611015319824
Loss :  1.6642786264419556 2.1215555667877197 12.272056579589844
Loss :  1.6825454235076904 1.9485474824905396 11.425283432006836
Loss :  1.6614272594451904 1.8345370292663574 10.834113121032715
Loss :  1.7036393880844116 1.998795747756958 11.697617530822754
  batch 20 loss: 1.7036393880844116, 1.998795747756958, 11.697617530822754
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.6781954765319824 1.8785260915756226 11.070825576782227
Loss :  1.6604124307632446 2.4552483558654785 13.936654090881348
Loss :  1.672701120376587 3.2057530879974365 17.701465606689453
Loss :  1.6838470697402954 2.6452527046203613 14.910110473632812
Loss :  1.704272747039795 2.534989833831787 14.379222869873047
Loss :  1.6755701303482056 2.8913800716400146 16.132471084594727
Loss :  1.6826648712158203 2.813424587249756 15.749788284301758
Loss :  1.6783875226974487 2.782743215560913 15.592103958129883
Loss :  1.6431820392608643 2.448899745941162 13.887681007385254
Loss :  1.7016189098358154 2.1262645721435547 12.332942008972168
Loss :  1.6442043781280518 3.1200709342956543 17.24456024169922
Loss :  1.6911022663116455 2.3976852893829346 13.67952823638916
Loss :  1.674264669418335 2.756049633026123 15.454513549804688
Loss :  1.6732351779937744 2.5262460708618164 14.304465293884277
Loss :  1.6496350765228271 2.705308198928833 15.176176071166992
Loss :  1.659908652305603 1.847145915031433 10.895637512207031
Loss :  1.6589196920394897 2.124363660812378 12.28073787689209
Loss :  1.70081627368927 1.9579848051071167 11.490739822387695
Loss :  1.704223394393921 2.5868217945098877 14.63833236694336
Loss :  1.7109023332595825 2.108839273452759 12.255098342895508
  batch 40 loss: 1.7109023332595825, 2.108839273452759, 12.255098342895508
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2], device='cuda:0')
Loss :  1.6824748516082764 1.8923485279083252 11.144217491149902
Loss :  1.6729810237884521 2.2836029529571533 13.090995788574219
Loss :  1.6652536392211914 3.5307095050811768 19.318801879882812
Loss :  1.6746935844421387 2.5946273803710938 14.647830963134766
Loss :  1.6598764657974243 1.9136732816696167 11.228242874145508
Loss :  1.6807303428649902 2.306856870651245 13.215015411376953
Loss :  1.703499436378479 2.3239006996154785 13.323002815246582
Loss :  1.6705520153045654 2.420412540435791 13.772614479064941
Loss :  1.7136828899383545 2.20029354095459 12.715150833129883
Loss :  1.6749792098999023 2.082307815551758 12.086518287658691
Loss :  1.6974906921386719 2.3034415245056152 13.214698791503906
Loss :  1.6936875581741333 1.8634440898895264 11.010908126831055
Loss :  1.6795401573181152 1.9247006177902222 11.303043365478516
Loss :  1.6996270418167114 2.161404609680176 12.5066499710083
Loss :  1.6711732149124146 2.766773223876953 15.50503921508789
Loss :  1.714468002319336 2.685882091522217 15.143878936767578
Loss :  1.6762006282806396 2.0487616062164307 11.920008659362793
Loss :  1.6651793718338013 1.9186238050460815 11.25829792022705
Loss :  1.6758227348327637 2.226560354232788 12.808624267578125
Loss :  1.7194364070892334 1.80422842502594 10.740578651428223
  batch 60 loss: 1.7194364070892334, 1.80422842502594, 10.740578651428223
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6656605005264282 2.632765293121338 14.829486846923828
Loss :  1.6853337287902832 2.5339648723602295 14.355157852172852
Loss :  1.6720292568206787 2.7849552631378174 15.596805572509766
Loss :  1.6636501550674438 2.843006134033203 15.878681182861328
Loss :  1.6501009464263916 1.9770407676696777 11.53530502319336
Loss :  1.6866353750228882 4.363279819488525 23.503034591674805
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0], device='cuda:0')
Loss :  1.6944022178649902 4.434863567352295 23.86872100830078
Loss :  1.6918067932128906 4.372628688812256 23.554950714111328
Loss :  1.6988359689712524 4.216942310333252 22.783546447753906
Total LOSS train 13.28035648052509 valid 23.427563190460205
CE LOSS train 1.6810229044694167 valid 0.4247089922428131
Contrastive LOSS train 2.319866706774785 valid 1.054235577583313
EPOCH 287:
Loss :  1.6952016353607178 2.348426342010498 13.437334060668945
Loss :  1.7039631605148315 2.1307969093322754 12.35794734954834
Loss :  1.6831049919128418 2.2225868701934814 12.796039581298828
Loss :  1.687285304069519 2.6901488304138184 15.138030052185059
Loss :  1.6988719701766968 2.7246220111846924 15.321981430053711
Loss :  1.6745320558547974 2.5163397789001465 14.256231307983398
Loss :  1.6992725133895874 3.6726536750793457 20.06254005432129
Loss :  1.6826229095458984 2.5661768913269043 14.513507843017578
Loss :  1.676164150238037 2.1249685287475586 12.301006317138672
Loss :  1.6997159719467163 2.0657594203948975 12.028512954711914
Loss :  1.670339584350586 3.338963747024536 18.365158081054688
Loss :  1.6699345111846924 1.9562729597091675 11.451298713684082
Loss :  1.668015718460083 1.812414288520813 10.730087280273438
Loss :  1.672063946723938 2.049875259399414 11.921440124511719
Loss :  1.709387183189392 1.9578849077224731 11.498811721801758
Loss :  1.7059550285339355 1.9235769510269165 11.32383918762207
Loss :  1.6659877300262451 2.3963124752044678 13.647550582885742
Loss :  1.6840596199035645 3.25582218170166 17.96316909790039
Loss :  1.6632122993469238 2.576507329940796 14.54574966430664
Loss :  1.7048072814941406 2.544041872024536 14.425016403198242
  batch 20 loss: 1.7048072814941406, 2.544041872024536, 14.425016403198242
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6797454357147217 2.328800916671753 13.323750495910645
Loss :  1.6619541645050049 2.4475173950195312 13.899540901184082
Loss :  1.6738665103912354 1.9625855684280396 11.486794471740723
Loss :  1.684470534324646 1.9110504388809204 11.23972225189209
Loss :  1.7047393321990967 2.0964667797088623 12.187073707580566
Loss :  1.6761562824249268 1.9827473163604736 11.589892387390137
Loss :  1.6831728219985962 1.9236830472946167 11.30158805847168
Loss :  1.6788992881774902 2.255859375 12.958196640014648
Loss :  1.6438838243484497 2.3311116695404053 13.299442291259766
Loss :  1.7025275230407715 2.8009696006774902 15.707376480102539
Loss :  1.6449452638626099 2.6924753189086914 15.107321739196777
Loss :  1.6917541027069092 2.726240634918213 15.322957038879395
Loss :  1.6748212575912476 2.787583112716675 15.612736701965332
Loss :  1.6737650632858276 2.79365873336792 15.642059326171875
Loss :  1.6501024961471558 2.8587279319763184 15.943742752075195
Loss :  1.6604417562484741 2.6773922443389893 15.047402381896973
Loss :  1.6596012115478516 2.5191891193389893 14.255546569824219
Loss :  1.7010328769683838 1.8215372562408447 10.80871868133545
Loss :  1.7045655250549316 2.096435308456421 12.186742782592773
Loss :  1.7113628387451172 1.6615607738494873 10.019166946411133
  batch 40 loss: 1.7113628387451172, 1.6615607738494873, 10.019166946411133
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6837149858474731 2.1795690059661865 12.581560134887695
Loss :  1.6746792793273926 1.9856940507888794 11.6031494140625
Loss :  1.6674699783325195 2.455392360687256 13.944432258605957
Loss :  1.67664635181427 2.296668767929077 13.159990310668945
Loss :  1.6619882583618164 2.2786409854888916 13.055192947387695
Loss :  1.6824662685394287 2.530735969543457 14.336146354675293
Loss :  1.7049670219421387 1.9679110050201416 11.54452133178711
Loss :  1.6721765995025635 2.2794387340545654 13.06937026977539
Loss :  1.7146886587142944 1.7788689136505127 10.60903263092041
Loss :  1.6763249635696411 1.6135650873184204 9.744150161743164
Loss :  1.6984955072402954 1.776375651359558 10.580373764038086
Loss :  1.6944557428359985 1.8654736280441284 11.02182388305664
Loss :  1.6803410053253174 1.7892060279846191 10.626370429992676
Loss :  1.6997188329696655 2.553992748260498 14.469682693481445
Loss :  1.6722078323364258 2.1957955360412598 12.651185035705566
Loss :  1.714443564414978 1.7462471723556519 10.4456787109375
Loss :  1.6767605543136597 2.1836066246032715 12.594794273376465
Loss :  1.6658903360366821 1.8824304342269897 11.078042984008789
Loss :  1.676397442817688 2.275296688079834 13.052881240844727
Loss :  1.7191121578216553 2.0062267780303955 11.750246047973633
  batch 60 loss: 1.7191121578216553, 2.0062267780303955, 11.750246047973633
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4], device='cuda:0')
Loss :  1.6661441326141357 1.8878964185714722 11.105626106262207
Loss :  1.685956597328186 1.8133902549743652 10.752908706665039
Loss :  1.67262864112854 1.7479413747787476 10.412335395812988
Loss :  1.6646658182144165 2.0501761436462402 11.915547370910645
Loss :  1.6514015197753906 1.4550341367721558 8.926572799682617
Loss :  1.6663684844970703 3.971147298812866 21.522104263305664
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6745871305465698 3.8635542392730713 20.992359161376953
Loss :  1.6724120378494263 3.806147813796997 20.70315170288086
Loss :  1.675357460975647 3.656609058380127 19.958402633666992
Total LOSS train 12.923948302635779 valid 20.794004440307617
CE LOSS train 1.682000734255864 valid 0.41883936524391174
Contrastive LOSS train 2.248389511841994 valid 0.9141522645950317
EPOCH 288:
Loss :  1.6956366300582886 2.2232325077056885 12.811799049377441
Loss :  1.7045921087265015 1.7938019037246704 10.673601150512695
Loss :  1.6838585138320923 1.5658341646194458 9.513029098510742
Loss :  1.688108205795288 1.8982919454574585 11.17956829071045
Loss :  1.6995975971221924 2.052158832550049 11.9603910446167
Loss :  1.6753556728363037 2.403778553009033 13.69424819946289
Loss :  1.6996424198150635 1.7477507591247559 10.438396453857422
Loss :  1.683251976966858 1.786770224571228 10.617103576660156
Loss :  1.6768735647201538 2.1884374618530273 12.619060516357422
Loss :  1.6999866962432861 1.5442017316818237 9.420995712280273
Loss :  1.671036958694458 2.363985776901245 13.490965843200684
Loss :  1.670572280883789 1.7371370792388916 10.356257438659668
Loss :  1.6686910390853882 1.7538630962371826 10.438006401062012
Loss :  1.6725386381149292 2.187427043914795 12.609674453735352
Loss :  1.709543228149414 2.3227949142456055 13.323517799377441
Loss :  1.7062628269195557 1.7790433168411255 10.601479530334473
Loss :  1.6666382551193237 1.8719749450683594 11.02651309967041
Loss :  1.684730052947998 2.0928759574890137 12.149110794067383
Loss :  1.6641052961349487 2.0785698890686035 12.056954383850098
Loss :  1.7056097984313965 2.57930326461792 14.602127075195312
  batch 20 loss: 1.7056097984313965, 2.57930326461792, 14.602127075195312
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6807407140731812 2.4294629096984863 13.828054428100586
Loss :  1.6633392572402954 2.420520305633545 13.765941619873047
Loss :  1.6753466129302979 2.225919246673584 12.804943084716797
Loss :  1.6859962940216064 2.3340823650360107 13.35640811920166
Loss :  1.7059636116027832 2.255585193634033 12.983888626098633
Loss :  1.6776378154754639 2.0263922214508057 11.809598922729492
Loss :  1.6845920085906982 2.184603691101074 12.607610702514648
Loss :  1.6804214715957642 2.0913398265838623 12.137121200561523
Loss :  1.646091103553772 2.1389167308807373 12.340675354003906
Loss :  1.7033522129058838 2.3830041885375977 13.618372917175293
Loss :  1.6468778848648071 2.479931592941284 14.046536445617676
Loss :  1.6927138566970825 1.8180593252182007 10.783010482788086
Loss :  1.6761374473571777 2.4154393672943115 13.753334045410156
Loss :  1.6749821901321411 1.8453803062438965 10.901884078979492
Loss :  1.6517373323440552 2.30441951751709 13.173834800720215
Loss :  1.6616852283477783 2.041128396987915 11.867326736450195
Loss :  1.6608351469039917 1.7945305109024048 10.633487701416016
Loss :  1.7018334865570068 2.273818016052246 13.070923805236816
Loss :  1.7053738832473755 1.8151664733886719 10.781206130981445
Loss :  1.7120146751403809 1.7167751789093018 10.295890808105469
  batch 40 loss: 1.7120146751403809, 1.7167751789093018, 10.295890808105469
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6844757795333862 1.8297829627990723 10.833390235900879
Loss :  1.675486445426941 1.6371781826019287 9.861377716064453
Loss :  1.6683207750320435 2.044027805328369 11.888459205627441
Loss :  1.6773250102996826 1.9963526725769043 11.659089088439941
Loss :  1.662631630897522 2.34425950050354 13.383929252624512
Loss :  1.6830404996871948 2.7559404373168945 15.462742805480957
Loss :  1.7052147388458252 2.063781976699829 12.024124145507812
Loss :  1.6725647449493408 1.8973625898361206 11.159377098083496
Loss :  1.714884877204895 1.8147070407867432 10.788419723510742
Loss :  1.6766716241836548 1.652269721031189 9.938020706176758
Loss :  1.6990110874176025 1.7954069375991821 10.676045417785645
Loss :  1.6948856115341187 1.877174973487854 11.08076000213623
Loss :  1.6808689832687378 2.1115262508392334 12.238500595092773
Loss :  1.7002627849578857 1.95543372631073 11.477431297302246
Loss :  1.6733897924423218 2.5824925899505615 14.58585262298584
Loss :  1.7151713371276855 2.6135847568511963 14.78309440612793
Loss :  1.677894949913025 1.9629985094070435 11.492887496948242
Loss :  1.6671360731124878 1.8591187000274658 10.962729454040527
Loss :  1.677525520324707 2.022305727005005 11.789053916931152
Loss :  1.7199676036834717 1.534217357635498 9.391054153442383
  batch 60 loss: 1.7199676036834717, 1.534217357635498, 9.391054153442383
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6672600507736206 2.2366607189178467 12.850564002990723
Loss :  1.6867892742156982 1.6537443399429321 9.955511093139648
Loss :  1.6733540296554565 2.1067588329315186 12.207148551940918
Loss :  1.66550874710083 2.4264116287231445 13.797567367553711
Loss :  1.652209997177124 1.7721407413482666 10.512913703918457
Loss :  1.6696357727050781 3.5206871032714844 19.2730712890625
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6775290966033936 3.2935023307800293 18.14504051208496
Loss :  1.6759045124053955 3.217437744140625 17.763093948364258
Loss :  1.6795756816864014 3.1271493434906006 17.315322875976562
Total LOSS train 11.952967599722056 valid 18.12413215637207
CE LOSS train 1.6828639378914467 valid 0.41989392042160034
Contrastive LOSS train 2.0540207294317394 valid 0.7817873358726501
EPOCH 289:
Loss :  1.6960833072662354 1.9694583415985107 11.543375015258789
Loss :  1.7050135135650635 2.2895522117614746 13.152774810791016
Loss :  1.6843360662460327 2.0087080001831055 11.727875709533691
Loss :  1.688527226448059 1.8697036504745483 11.0370454788208
Loss :  1.699970006942749 1.6109672784805298 9.754806518554688
Loss :  1.6759427785873413 1.888292908668518 11.117406845092773
Loss :  1.6998796463012695 1.9505269527435303 11.4525146484375
Loss :  1.6835300922393799 1.873451590538025 11.050787925720215
Loss :  1.677110195159912 1.9849417209625244 11.601818084716797
Loss :  1.7001104354858398 1.834699034690857 10.873605728149414
Loss :  1.6710484027862549 2.1154887676239014 12.248492240905762
Loss :  1.670602560043335 1.9486685991287231 11.413946151733398
Loss :  1.6690112352371216 1.778563141822815 10.561826705932617
Loss :  1.6727737188339233 2.0831356048583984 12.088451385498047
Loss :  1.7102932929992676 1.9814056158065796 11.617321014404297
Loss :  1.7067879438400269 1.867612361907959 11.04485034942627
Loss :  1.6675479412078857 2.4078261852264404 13.706679344177246
Loss :  1.685067892074585 1.798846960067749 10.679303169250488
Loss :  1.6651535034179688 1.7519612312316895 10.424959182739258
Loss :  1.7065398693084717 2.624234437942505 14.827712059020996
  batch 20 loss: 1.7065398693084717, 2.624234437942505, 14.827712059020996
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6817296743392944 1.9507008790969849 11.435234069824219
Loss :  1.6645618677139282 2.5558104515075684 14.443614959716797
Loss :  1.6764494180679321 1.919837236404419 11.275635719299316
Loss :  1.6868759393692017 1.9291144609451294 11.332448959350586
Loss :  1.7065842151641846 2.1846086978912354 12.629627227783203
Loss :  1.6783020496368408 1.8694568872451782 11.025586128234863
Loss :  1.685172438621521 1.887149691581726 11.120920181274414
Loss :  1.6810998916625977 2.570237398147583 14.532286643981934
Loss :  1.6468485593795776 2.240253448486328 12.848115921020508
Loss :  1.704149842262268 2.5220491886138916 14.314395904541016
Loss :  1.6477254629135132 2.3625407218933105 13.460429191589355
Loss :  1.6933929920196533 2.3969738483428955 13.678261756896973
Loss :  1.6769779920578003 2.401160955429077 13.682783126831055
Loss :  1.675639033317566 2.572683811187744 14.539057731628418
Loss :  1.652561068534851 2.8523671627044678 15.914397239685059
Loss :  1.662289023399353 1.833196997642517 10.82827377319336
Loss :  1.6615464687347412 2.168304681777954 12.503069877624512
Loss :  1.7021933794021606 2.529205799102783 14.348221778869629
Loss :  1.7056920528411865 2.187621593475342 12.643800735473633
Loss :  1.7122633457183838 2.1145730018615723 12.285127639770508
  batch 40 loss: 1.7122633457183838, 2.1145730018615723, 12.285127639770508
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.684932827949524 2.327807903289795 13.323972702026367
Loss :  1.6759839057922363 1.7266602516174316 10.309284210205078
Loss :  1.6688483953475952 2.5169050693511963 14.253373146057129
Loss :  1.67780601978302 2.196045160293579 12.658031463623047
Loss :  1.663328766822815 1.897327184677124 11.149965286254883
Loss :  1.6835527420043945 1.9392601251602173 11.379853248596191
Loss :  1.7055801153182983 1.8465346097946167 10.938252449035645
Loss :  1.6734192371368408 1.9268286228179932 11.307561874389648
Loss :  1.7151068449020386 2.0106801986694336 11.768507957458496
Loss :  1.6774147748947144 1.4897971153259277 9.126399993896484
Loss :  1.6995795965194702 1.7831084728240967 10.615121841430664
Loss :  1.695418119430542 1.8570175170898438 10.98050594329834
Loss :  1.6814019680023193 2.5313847064971924 14.338325500488281
Loss :  1.7003908157348633 2.8187015056610107 15.793898582458496
Loss :  1.6735986471176147 2.2880570888519287 13.113883972167969
Loss :  1.7147973775863647 2.091277599334717 12.171185493469238
Loss :  1.6777445077896118 2.0827574729919434 12.091532707214355
Loss :  1.667222023010254 1.9662632942199707 11.49853801727295
Loss :  1.6774671077728271 2.8702027797698975 16.028480529785156
Loss :  1.7196694612503052 2.4566476345062256 14.002907752990723
  batch 60 loss: 1.7196694612503052, 2.4566476345062256, 14.002907752990723
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6673122644424438 2.1909022331237793 12.621824264526367
Loss :  1.6869008541107178 1.785680890083313 10.61530590057373
Loss :  1.6735968589782715 1.665490984916687 10.00105094909668
Loss :  1.6657875776290894 1.915044903755188 11.241012573242188
Loss :  1.6525887250900269 1.6155563592910767 9.73037052154541
Loss :  1.669839859008789 3.7480738162994385 20.41020965576172
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  1.677720546722412 3.692314386367798 20.139293670654297
Loss :  1.6765284538269043 3.613182306289673 19.742441177368164
Loss :  1.6783323287963867 3.4458885192871094 18.90777587890625
Total LOSS train 12.181938274090106 valid 19.799930095672607
CE LOSS train 1.6833359058086688 valid 0.4195830821990967
Contrastive LOSS train 2.0997204798918503 valid 0.8614721298217773
EPOCH 290:
Loss :  1.696344256401062 1.6956062316894531 10.174375534057617
Loss :  1.7051944732666016 2.158602237701416 12.498205184936523
Loss :  1.684753656387329 1.7578872442245483 10.474189758300781
Loss :  1.6890345811843872 2.2307093143463135 12.842580795288086
Loss :  1.7005280256271362 1.6918469667434692 10.15976333618164
Loss :  1.6767125129699707 2.026576519012451 11.809595108032227
Loss :  1.7004846334457397 2.1335744857788086 12.368356704711914
Loss :  1.6844521760940552 1.8810017108917236 11.089460372924805
Loss :  1.6779260635375977 2.162980079650879 12.492826461791992
Loss :  1.700749397277832 1.8198754787445068 10.800127029418945
Loss :  1.6718052625656128 2.105985641479492 12.201733589172363
Loss :  1.6713000535964966 1.9095481634140015 11.219040870666504
Loss :  1.669756293296814 1.8282736539840698 10.811123847961426
Loss :  1.6733124256134033 2.5207278728485107 14.276951789855957
Loss :  1.710615873336792 2.2802352905273438 13.11179256439209
Loss :  1.706896185874939 1.9595685005187988 11.504737854003906
Loss :  1.667965292930603 2.78912091255188 15.613569259643555
Loss :  1.6854040622711182 2.0941922664642334 12.156365394592285
Loss :  1.6655062437057495 2.0549051761627197 11.940032005310059
Loss :  1.706937551498413 2.6680452823638916 15.047163963317871
  batch 20 loss: 1.706937551498413, 2.6680452823638916, 15.047163963317871
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.681901216506958 1.6475096940994263 9.919449806213379
Loss :  1.664709448814392 2.225541830062866 12.792418479919434
Loss :  1.6766647100448608 1.6626060009002686 9.989694595336914
Loss :  1.6869401931762695 1.9693783521652222 11.533831596374512
Loss :  1.7066255807876587 2.206740140914917 12.740326881408691
Loss :  1.6783198118209839 2.6453287601470947 14.904963493347168
Loss :  1.6852036714553833 1.8773366212844849 11.071887016296387
Loss :  1.6812467575073242 1.767236590385437 10.51742935180664
Loss :  1.6470950841903687 1.9091172218322754 11.192680358886719
Loss :  1.7043085098266602 2.609530448913574 14.751960754394531
Loss :  1.6480917930603027 2.7209386825561523 15.252784729003906
Loss :  1.6935087442398071 2.1592226028442383 12.489622116088867
Loss :  1.677275538444519 2.3142433166503906 13.248492240905762
Loss :  1.676093578338623 2.5647902488708496 14.500045776367188
Loss :  1.6531902551651 2.1003873348236084 12.15512752532959
Loss :  1.6631100177764893 1.7436089515686035 10.38115406036377
Loss :  1.6624293327331543 2.531586170196533 14.32036018371582
Loss :  1.7028872966766357 1.9607926607131958 11.506851196289062
Loss :  1.7064878940582275 2.1643834114074707 12.528404235839844
Loss :  1.7130799293518066 1.433254361152649 8.879351615905762
  batch 40 loss: 1.7130799293518066, 1.433254361152649, 8.879351615905762
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6860127449035645 1.9465019702911377 11.418521881103516
Loss :  1.6770209074020386 2.2296605110168457 12.825323104858398
Loss :  1.6701576709747314 2.381732225418091 13.578818321228027
Loss :  1.678710699081421 2.7166945934295654 15.262184143066406
Loss :  1.6642321348190308 1.5247323513031006 9.287894248962402
Loss :  1.6842786073684692 2.2764346599578857 13.066452026367188
Loss :  1.70613431930542 2.0996322631835938 12.204296112060547
Loss :  1.6740212440490723 1.9766629934310913 11.557336807250977
Loss :  1.71555495262146 2.0828919410705566 12.130014419555664
Loss :  1.6779558658599854 1.4526500701904297 8.941205978393555
Loss :  1.700343132019043 1.850996494293213 10.95532512664795
Loss :  1.6962133646011353 1.7086083889007568 10.239255905151367
Loss :  1.6822049617767334 2.4857263565063477 14.11083698272705
Loss :  1.7012852430343628 2.1175596714019775 12.289083480834961
Loss :  1.6747397184371948 2.2121055126190186 12.735267639160156
Loss :  1.7157686948776245 1.7297780513763428 10.364659309387207
Loss :  1.679068922996521 1.868826150894165 11.023199081420898
Loss :  1.668619990348816 1.8228068351745605 10.782654762268066
Loss :  1.6789119243621826 2.6658565998077393 15.008194923400879
Loss :  1.7207385301589966 2.0795276165008545 12.118376731872559
  batch 60 loss: 1.7207385301589966, 2.0795276165008545, 12.118376731872559
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6690431833267212 2.1176724433898926 12.257405281066895
Loss :  1.6886696815490723 1.7620571851730347 10.498954772949219
Loss :  1.6751630306243896 1.6336292028427124 9.84330940246582
Loss :  1.667494535446167 1.997117280960083 11.653080940246582
Loss :  1.65428626537323 1.2574381828308105 7.941477298736572
Loss :  1.6680529117584229 4.074549674987793 22.040802001953125
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6758008003234863 4.098469257354736 22.16814613342285
Loss :  1.6752690076828003 4.021104335784912 21.780790328979492
Loss :  1.676831841468811 3.8898611068725586 21.12613868713379
Total LOSS train 11.990183940300575 valid 21.778969287872314
CE LOSS train 1.684022749387301 valid 0.41920796036720276
Contrastive LOSS train 2.061232245885409 valid 0.9724652767181396
EPOCH 291:
Loss :  1.6973793506622314 1.6371277570724487 9.883017539978027
Loss :  1.7061265707015991 2.3567054271698 13.489653587341309
Loss :  1.6859211921691895 1.5050679445266724 9.211260795593262
Loss :  1.689921498298645 2.0567917823791504 11.97387981414795
Loss :  1.7012574672698975 1.920698642730713 11.304750442504883
Loss :  1.6772189140319824 2.4538347721099854 13.946392059326172
Loss :  1.700730323791504 2.4724843502044678 14.063152313232422
Loss :  1.6844640970230103 1.8254553079605103 10.81174087524414
Loss :  1.678180456161499 1.9440280199050903 11.398321151733398
Loss :  1.7007075548171997 2.3749513626098633 13.575464248657227
Loss :  1.6720203161239624 2.1231720447540283 12.287879943847656
Loss :  1.67165207862854 2.032949209213257 11.836398124694824
Loss :  1.6701385974884033 2.3656301498413086 13.498289108276367
Loss :  1.6739578247070312 2.6945717334747314 15.14681625366211
Loss :  1.7112873792648315 2.1527955532073975 12.475265502929688
Loss :  1.707743525505066 1.8336061239242554 10.875774383544922
Loss :  1.668914556503296 1.8212521076202393 10.775175094604492
Loss :  1.6860815286636353 1.7055848836898804 10.214006423950195
Loss :  1.6663309335708618 2.399472951889038 13.6636962890625
Loss :  1.707400918006897 1.92477548122406 11.331278800964355
  batch 20 loss: 1.707400918006897, 1.92477548122406, 11.331278800964355
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.683069109916687 1.9149806499481201 11.257972717285156
Loss :  1.6660151481628418 2.3144407272338867 13.238218307495117
Loss :  1.6777355670928955 2.208865165710449 12.722061157226562
Loss :  1.6881054639816284 2.4109137058258057 13.742673873901367
Loss :  1.707800030708313 1.9851680994033813 11.63364028930664
Loss :  1.680112361907959 1.7952255010604858 10.656240463256836
Loss :  1.6866167783737183 1.8616129159927368 10.994681358337402
Loss :  1.6827014684677124 2.1056082248687744 12.210741996765137
Loss :  1.6485649347305298 2.5785441398620605 14.54128646850586
Loss :  1.7049983739852905 2.644970417022705 14.929849624633789
Loss :  1.6494207382202148 2.435689687728882 13.827869415283203
Loss :  1.6941901445388794 2.005671262741089 11.722546577453613
Loss :  1.6781374216079712 2.279836416244507 13.077320098876953
Loss :  1.6769847869873047 2.631974220275879 14.8368558883667
Loss :  1.6546274423599243 2.8974673748016357 16.141963958740234
Loss :  1.6640011072158813 2.8690900802612305 16.009450912475586
Loss :  1.6633003950119019 2.639911651611328 14.862858772277832
Loss :  1.702617883682251 2.434159517288208 13.873414993286133
Loss :  1.7062019109725952 1.887190341949463 11.142152786254883
Loss :  1.71272611618042 2.0595290660858154 12.010372161865234
  batch 40 loss: 1.71272611618042, 2.0595290660858154, 12.010372161865234
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 3, 4, 5], device='cuda:0')
Loss :  1.6862891912460327 2.011457920074463 11.74357795715332
Loss :  1.6771183013916016 1.84221613407135 10.888198852539062
Loss :  1.6705081462860107 2.0825092792510986 12.083054542541504
Loss :  1.6789162158966064 1.9592496156692505 11.475164413452148
Loss :  1.664696216583252 1.6626485586166382 9.97793960571289
Loss :  1.6849195957183838 1.9006911516189575 11.188375473022461
Loss :  1.7066500186920166 1.8571949005126953 10.992624282836914
Loss :  1.6747748851776123 2.2765142917633057 13.05734634399414
Loss :  1.7156544923782349 2.411540985107422 13.773359298706055
Loss :  1.6785035133361816 2.056035041809082 11.95867919921875
Loss :  1.700579285621643 2.2340548038482666 12.870853424072266
Loss :  1.6962032318115234 2.2806313037872314 13.099359512329102
Loss :  1.682312250137329 2.31307315826416 13.24767780303955
Loss :  1.7008646726608276 2.239489793777466 12.898313522338867
Loss :  1.6751439571380615 2.3302154541015625 13.326221466064453
Loss :  1.7156814336776733 2.1747148036956787 12.589255332946777
Loss :  1.6794602870941162 2.4795756340026855 14.077339172363281
Loss :  1.6688820123672485 2.2479844093322754 12.908803939819336
Loss :  1.6791670322418213 2.546643018722534 14.412382125854492
Loss :  1.7208064794540405 1.7748345136642456 10.594978332519531
  batch 60 loss: 1.7208064794540405, 1.7748345136642456, 10.594978332519531
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6690794229507446 1.93348228931427 11.336491584777832
Loss :  1.6883248090744019 1.7741307020187378 10.558978080749512
Loss :  1.6750872135162354 1.8423426151275635 10.886799812316895
Loss :  1.667399287223816 1.9456045627593994 11.395421981811523
Loss :  1.6543670892715454 1.6067997217178345 9.688365936279297
Loss :  1.6598231792449951 3.772050142288208 20.52007293701172
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6680784225463867 3.8077802658081055 20.706981658935547
Loss :  1.6664149761199951 3.6271212100982666 19.802021026611328
Loss :  1.6676291227340698 3.4345195293426514 18.840227127075195
Total LOSS train 12.403445331866925 valid 19.967325687408447
CE LOSS train 1.6845654047452487 valid 0.41690728068351746
Contrastive LOSS train 2.1437759912931003 valid 0.8586298823356628
EPOCH 292:
Loss :  1.6972700357437134 2.013759136199951 11.766066551208496
Loss :  1.7059555053710938 2.1643688678741455 12.527799606323242
Loss :  1.6857631206512451 1.7711470127105713 10.541498184204102
Loss :  1.6898601055145264 2.2569806575775146 12.974763870239258
Loss :  1.70138418674469 1.603800892829895 9.720389366149902
Loss :  1.6775522232055664 1.7974059581756592 10.664582252502441
Loss :  1.7011796236038208 2.3379101753234863 13.390729904174805
Loss :  1.6850603818893433 2.0789310932159424 12.079715728759766
Loss :  1.6787210702896118 2.590644121170044 14.631941795349121
Loss :  1.701171875 1.6364327669143677 9.883336067199707
Loss :  1.6726086139678955 1.9962821006774902 11.654019355773926
Loss :  1.672088861465454 1.8127104043960571 10.735640525817871
Loss :  1.670556664466858 2.047205686569214 11.906585693359375
Loss :  1.6741185188293457 1.9745895862579346 11.547065734863281
Loss :  1.7109512090682983 1.9595303535461426 11.508602142333984
Loss :  1.7075380086898804 2.0282585620880127 11.848830223083496
Loss :  1.6688371896743774 2.047866106033325 11.908167839050293
Loss :  1.686274766921997 1.841487169265747 10.893710136413574
Loss :  1.6664289236068726 1.9876128435134888 11.604493141174316
Loss :  1.7073147296905518 1.994004487991333 11.677336692810059
  batch 20 loss: 1.7073147296905518, 1.994004487991333, 11.677336692810059
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6831064224243164 1.8174073696136475 10.770143508911133
Loss :  1.6661020517349243 1.942853569984436 11.380370140075684
Loss :  1.6779319047927856 1.670443058013916 10.030146598815918
Loss :  1.6882444620132446 1.795444130897522 10.665465354919434
Loss :  1.7077014446258545 2.205408811569214 12.734745979309082
Loss :  1.6798752546310425 2.112959384918213 12.244671821594238
Loss :  1.6865369081497192 2.002128839492798 11.69718074798584
Loss :  1.682578206062317 1.8990497589111328 11.177826881408691
Loss :  1.6487619876861572 2.0763659477233887 12.03059196472168
Loss :  1.7049624919891357 2.786968946456909 15.63980770111084
Loss :  1.6494430303573608 2.312199115753174 13.210437774658203
Loss :  1.694373607635498 2.293346643447876 13.16110610961914
Loss :  1.6782935857772827 1.755712628364563 10.456856727600098
Loss :  1.6770457029342651 2.272571086883545 13.039901733398438
Loss :  1.6541821956634521 1.999194860458374 11.65015697479248
Loss :  1.6637941598892212 1.9380520582199097 11.35405445098877
Loss :  1.6628834009170532 1.8684020042419434 11.004894256591797
Loss :  1.7031725645065308 2.4249250888824463 13.827797889709473
Loss :  1.7065404653549194 1.7786515951156616 10.599798202514648
Loss :  1.7131164073944092 1.9991745948791504 11.708989143371582
  batch 40 loss: 1.7131164073944092, 1.9991745948791504, 11.708989143371582
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.686051845550537 2.627926826477051 14.825685501098633
Loss :  1.6771148443222046 1.9662959575653076 11.508594512939453
Loss :  1.6702457666397095 2.0089497566223145 11.714994430541992
Loss :  1.6791625022888184 1.9928364753723145 11.64334487915039
Loss :  1.6649115085601807 2.2090797424316406 12.710309982299805
Loss :  1.6851260662078857 1.7538102865219116 10.454177856445312
Loss :  1.7069528102874756 1.6291487216949463 9.852696418762207
Loss :  1.6750646829605103 1.8321740627288818 10.835935592651367
Loss :  1.71640944480896 2.4368090629577637 13.900455474853516
Loss :  1.679211974143982 1.517444372177124 9.266433715820312
Loss :  1.70111882686615 1.7141770124435425 10.272003173828125
Loss :  1.6970964670181274 1.7303657531738281 10.348925590515137
Loss :  1.6832349300384521 1.8504713773727417 10.935591697692871
Loss :  1.702196478843689 2.5820796489715576 14.612594604492188
Loss :  1.6755563020706177 2.4600744247436523 13.97592830657959
Loss :  1.7165026664733887 2.303614377975464 13.234575271606445
Loss :  1.6798322200775146 2.198042154312134 12.670042991638184
Loss :  1.6693286895751953 2.0398666858673096 11.868661880493164
Loss :  1.6794675588607788 2.186230182647705 12.610617637634277
Loss :  1.7212443351745605 1.5648014545440674 9.545251846313477
  batch 60 loss: 1.7212443351745605, 1.5648014545440674, 9.545251846313477
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6694101095199585 1.8763245344161987 11.051032066345215
Loss :  1.688661813735962 1.6561145782470703 9.969234466552734
Loss :  1.6754478216171265 1.7434002161026 10.392448425292969
Loss :  1.6676112413406372 2.2800395488739014 13.067809104919434
Loss :  1.6546627283096313 2.1713151931762695 12.511239051818848
Loss :  1.666741967201233 3.851884603500366 20.926164627075195
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.674763798713684 3.872215509414673 21.035842895507812
Loss :  1.6726998090744019 3.8031182289123535 20.688289642333984
Loss :  1.6758733987808228 3.5400755405426025 19.376251220703125
Total LOSS train 11.778904665433444 valid 20.50663709640503
CE LOSS train 1.6847831616034874 valid 0.4189683496952057
Contrastive LOSS train 2.018824305901161 valid 0.8850188851356506
EPOCH 293:
Loss :  1.6975786685943604 2.199697971343994 12.696067810058594
Loss :  1.7062129974365234 2.4304378032684326 13.858402252197266
Loss :  1.6859452724456787 2.2680985927581787 13.02643871307373
Loss :  1.6900323629379272 1.930076003074646 11.340412139892578
Loss :  1.7013989686965942 1.6189016103744507 9.795907020568848
Loss :  1.677741527557373 1.7967668771743774 10.661575317382812
Loss :  1.701322078704834 2.2605159282684326 13.003902435302734
Loss :  1.68538498878479 1.994343638420105 11.657102584838867
Loss :  1.6791523694992065 1.578966736793518 9.573986053466797
Loss :  1.7016760110855103 1.3805433511734009 8.604393005371094
Loss :  1.6732897758483887 1.7947217226028442 10.64689826965332
Loss :  1.6729297637939453 1.820993185043335 10.7778959274292
Loss :  1.6712590456008911 1.6311427354812622 9.826972961425781
Loss :  1.6749547719955444 1.8219166994094849 10.784538269042969
Loss :  1.71173894405365 2.291403293609619 13.168754577636719
Loss :  1.7082040309906006 2.6146645545959473 14.781526565551758
Loss :  1.6696770191192627 2.313717842102051 13.238265991210938
Loss :  1.6868435144424438 1.5946534872055054 9.660111427307129
Loss :  1.6672567129135132 1.911723256111145 11.225872993469238
Loss :  1.7076927423477173 1.6130727529525757 9.773056983947754
  batch 20 loss: 1.7076927423477173, 1.6130727529525757, 9.773056983947754
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.683379888534546 1.5072940587997437 9.219850540161133
Loss :  1.6664844751358032 2.09567928314209 12.144881248474121
Loss :  1.6781833171844482 1.8810725212097168 11.08354663848877
Loss :  1.6884310245513916 2.0807535648345947 12.092198371887207
Loss :  1.7079590559005737 2.393097400665283 13.673445701599121
Loss :  1.6800591945648193 2.0844767093658447 12.102442741394043
Loss :  1.6869643926620483 2.2216479778289795 12.795204162597656
Loss :  1.6831415891647339 2.452436685562134 13.945324897766113
Loss :  1.6494957208633423 2.5967776775360107 14.633384704589844
Loss :  1.7058030366897583 2.1728782653808594 12.570194244384766
Loss :  1.6505515575408936 1.8640062808990479 10.970582962036133
Loss :  1.6950749158859253 2.0886521339416504 12.138335227966309
Loss :  1.6790103912353516 2.278482675552368 13.071423530578613
Loss :  1.6777541637420654 2.3316240310668945 13.335874557495117
Loss :  1.6552578210830688 2.3720743656158447 13.515629768371582
Loss :  1.6647754907608032 1.9061869382858276 11.195710182189941
Loss :  1.664181113243103 1.9160330295562744 11.244345664978027
Loss :  1.7037677764892578 1.8779857158660889 11.093696594238281
Loss :  1.707267165184021 1.821326732635498 10.8139009475708
Loss :  1.7137012481689453 2.4061057567596436 13.744230270385742
  batch 40 loss: 1.7137012481689453, 2.4061057567596436, 13.744230270385742
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6872084140777588 2.1695942878723145 12.535179138183594
Loss :  1.6784160137176514 1.5538249015808105 9.447540283203125
Loss :  1.6717993021011353 2.1499507427215576 12.421553611755371
Loss :  1.6802775859832764 2.0245163440704346 11.80285930633545
Loss :  1.6660634279251099 2.1662092208862305 12.497109413146973
Loss :  1.6857432126998901 2.0808115005493164 12.089800834655762
Loss :  1.7070155143737793 2.4608237743377686 14.01113510131836
Loss :  1.6755070686340332 1.689203143119812 10.121522903442383
Loss :  1.716064214706421 1.6238263845443726 9.835196495056152
Loss :  1.6791017055511475 1.555322289466858 9.455713272094727
Loss :  1.7008575201034546 1.9122211933135986 11.261962890625
Loss :  1.6969735622406006 1.9302808046340942 11.34837818145752
Loss :  1.6831673383712769 2.5248188972473145 14.30726146697998
Loss :  1.7019420862197876 2.725909948348999 15.33149242401123
Loss :  1.6759979724884033 2.6370508670806885 14.861251831054688
Loss :  1.7165718078613281 2.585789203643799 14.645517349243164
Loss :  1.6804569959640503 2.836329936981201 15.862107276916504
Loss :  1.6700613498687744 2.173091411590576 12.535518646240234
Loss :  1.6800681352615356 2.377044439315796 13.565290451049805
Loss :  1.7213613986968994 2.1103882789611816 12.27330207824707
  batch 60 loss: 1.7213613986968994, 2.1103882789611816, 12.27330207824707
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6699445247650146 1.8671132326126099 11.005511283874512
Loss :  1.6889344453811646 1.8987689018249512 11.182779312133789
Loss :  1.6759703159332275 1.7058802843093872 10.205371856689453
Loss :  1.6680834293365479 2.6846957206726074 15.091562271118164
Loss :  1.6552891731262207 2.053414821624756 11.92236328125
Loss :  1.6655819416046143 4.08297061920166 22.080434799194336
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6734683513641357 4.048638820648193 21.916662216186523
Loss :  1.6715978384017944 3.8573973178863525 20.958585739135742
Loss :  1.6750108003616333 3.6893692016601562 20.121856689453125
Total LOSS train 12.047747157170223 valid 21.26938486099243
CE LOSS train 1.6852986372434176 valid 0.4187527000904083
Contrastive LOSS train 2.072489698116596 valid 0.9223423004150391
EPOCH 294:
Loss :  1.6981192827224731 1.8001272678375244 10.698755264282227
Loss :  1.7066887617111206 2.3859283924102783 13.636330604553223
Loss :  1.6869314908981323 2.2773232460021973 13.07354736328125
Loss :  1.6910125017166138 2.3879952430725098 13.630988121032715
Loss :  1.7023134231567383 1.598067045211792 9.692648887634277
Loss :  1.6788249015808105 1.7991044521331787 10.674346923828125
Loss :  1.702216625213623 1.8816051483154297 11.11024284362793
Loss :  1.6862940788269043 1.809518575668335 10.73388671875
Loss :  1.6799085140228271 2.373262643814087 13.546221733093262
Loss :  1.7021669149398804 1.6493723392486572 9.949028015136719
Loss :  1.6739736795425415 2.0923078060150146 12.135513305664062
Loss :  1.6735295057296753 2.3535754680633545 13.441407203674316
Loss :  1.6716315746307373 2.1394524574279785 12.36889362335205
Loss :  1.6754965782165527 1.9553378820419312 11.452186584472656
Loss :  1.7112109661102295 1.997338891029358 11.697905540466309
Loss :  1.7081630229949951 1.8552556037902832 10.984440803527832
Loss :  1.6697825193405151 2.4713528156280518 14.026546478271484
Loss :  1.6872148513793945 2.32915997505188 13.333014488220215
Loss :  1.6675288677215576 2.2801930904388428 13.06849479675293
Loss :  1.7080801725387573 2.252668857574463 12.971424102783203
  batch 20 loss: 1.7080801725387573, 2.252668857574463, 12.971424102783203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6841840744018555 2.194835662841797 12.65836238861084
Loss :  1.6674065589904785 2.0421037673950195 11.877925872802734
Loss :  1.679022192955017 1.9873255491256714 11.615649223327637
Loss :  1.689322829246521 1.949060320854187 11.434623718261719
Loss :  1.7086191177368164 2.347757577896118 13.447406768798828
Loss :  1.681276559829712 2.298377275466919 13.173162460327148
Loss :  1.6876293420791626 2.649988889694214 14.93757438659668
Loss :  1.6837527751922607 1.8520334959030151 10.943920135498047
Loss :  1.6503338813781738 2.2328169345855713 12.81441879272461
Loss :  1.7056089639663696 2.507308006286621 14.242149353027344
Loss :  1.6506708860397339 2.0709574222564697 12.005457878112793
Loss :  1.694912075996399 2.0057640075683594 11.723731994628906
Loss :  1.6786158084869385 2.6846723556518555 15.101977348327637
Loss :  1.6774265766143799 2.4412105083465576 13.883479118347168
Loss :  1.654805064201355 2.552060604095459 14.415108680725098
Loss :  1.6646817922592163 2.336846351623535 13.348913192749023
Loss :  1.6639769077301025 2.138370990753174 12.355831146240234
Loss :  1.7036765813827515 2.113044023513794 12.268896102905273
Loss :  1.7072103023529053 1.8975319862365723 11.194869995117188
Loss :  1.7137185335159302 2.4558768272399902 13.99310302734375
  batch 40 loss: 1.7137185335159302, 2.4558768272399902, 13.99310302734375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6870381832122803 2.5248000621795654 14.311038970947266
Loss :  1.6780515909194946 2.4620726108551025 13.988414764404297
Loss :  1.670938491821289 2.6465885639190674 14.903881072998047
Loss :  1.6797858476638794 1.6873416900634766 10.116494178771973
Loss :  1.6654607057571411 1.834553599357605 10.838228225708008
Loss :  1.685734510421753 2.033733367919922 11.854401588439941
Loss :  1.7074912786483765 2.5780587196350098 14.597784042358398
Loss :  1.6757010221481323 2.503659725189209 14.194000244140625
Loss :  1.7167859077453613 1.680658221244812 10.120077133178711
Loss :  1.6799757480621338 1.6218607425689697 9.789278984069824
Loss :  1.701590895652771 1.796331763267517 10.683249473571777
Loss :  1.697573184967041 2.2120118141174316 12.757631301879883
Loss :  1.6838901042938232 2.0172312259674072 11.77004623413086
Loss :  1.7025580406188965 2.287196636199951 13.138542175292969
Loss :  1.6761976480484009 2.0901083946228027 12.126740455627441
Loss :  1.7172054052352905 2.035189151763916 11.893150329589844
Loss :  1.680871605873108 1.8029807806015015 10.695775985717773
Loss :  1.670261025428772 2.404825448989868 13.694388389587402
Loss :  1.6804381608963013 2.452558755874634 13.943231582641602
Loss :  1.7218559980392456 1.6307576894760132 9.87564468383789
  batch 60 loss: 1.7218559980392456, 1.6307576894760132, 9.87564468383789
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6703840494155884 1.781572699546814 10.578248023986816
Loss :  1.6891789436340332 2.3177220821380615 13.277790069580078
Loss :  1.6763479709625244 2.062439203262329 11.988543510437012
Loss :  1.6684292554855347 2.892503261566162 16.130945205688477
Loss :  1.6556004285812378 2.295997142791748 13.135586738586426
Loss :  1.6663522720336914 3.6514604091644287 19.92365264892578
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6747111082077026 3.5557351112365723 19.453386306762695
Loss :  1.672132968902588 3.5868098735809326 19.606182098388672
Loss :  1.6746301651000977 3.356466054916382 18.456958770751953
Total LOSS train 12.46254612849309 valid 19.360044956207275
CE LOSS train 1.6856505393981933 valid 0.4186575412750244
Contrastive LOSS train 2.1553791247881375 valid 0.8391165137290955
EPOCH 295:
Loss :  1.6984280347824097 2.438957929611206 13.893218040466309
Loss :  1.7069697380065918 1.8325412273406982 10.86967658996582
Loss :  1.6870055198669434 2.0129451751708984 11.751731872558594
Loss :  1.6912142038345337 1.7855002880096436 10.6187162399292
Loss :  1.7025508880615234 1.5891454219818115 9.64827823638916
Loss :  1.6791448593139648 2.204265594482422 12.700472831726074
Loss :  1.7026855945587158 2.59967041015625 14.701037406921387
Loss :  1.6867960691452026 1.9781802892684937 11.57769775390625
Loss :  1.680547833442688 2.121392011642456 12.287508010864258
Loss :  1.7029997110366821 2.241630792617798 12.911153793334961
Loss :  1.6748645305633545 2.4923934936523438 14.136832237243652
Loss :  1.674426794052124 1.8771601915359497 11.06022834777832
Loss :  1.6725528240203857 1.7223342657089233 10.284224510192871
Loss :  1.67612624168396 2.340939521789551 13.380824089050293
Loss :  1.712080478668213 2.387286901473999 13.648515701293945
Loss :  1.7093219757080078 2.361381769180298 13.516230583190918
Loss :  1.670867919921875 1.7421753406524658 10.381744384765625
Loss :  1.6884311437606812 1.5156559944152832 9.266711235046387
Loss :  1.6683372259140015 2.3796091079711914 13.56638240814209
Loss :  1.7087817192077637 1.6691803932189941 10.054683685302734
  batch 20 loss: 1.7087817192077637, 1.6691803932189941, 10.054683685302734
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6845624446868896 2.2508232593536377 12.938678741455078
Loss :  1.6678248643875122 2.2449183464050293 12.892416954040527
Loss :  1.6794487237930298 1.6414599418640137 9.886749267578125
Loss :  1.689605951309204 1.8508989810943604 10.944100379943848
Loss :  1.7088892459869385 1.8526090383529663 10.97193431854248
Loss :  1.6815656423568726 2.1920533180236816 12.641831398010254
Loss :  1.688116192817688 2.013277769088745 11.754505157470703
Loss :  1.6840875148773193 1.5204664468765259 9.286419868469238
Loss :  1.6506072282791138 2.2037763595581055 12.669488906860352
Loss :  1.7061628103256226 1.8529988527297974 10.97115707397461
Loss :  1.6511791944503784 2.1189770698547363 12.246064186096191
Loss :  1.6957613229751587 2.284463405609131 13.11807918548584
Loss :  1.6797258853912354 2.6307690143585205 14.83357048034668
Loss :  1.6787104606628418 2.19985032081604 12.677961349487305
Loss :  1.656175136566162 2.7887048721313477 15.599699020385742
Loss :  1.6660860776901245 2.459343433380127 13.96280288696289
Loss :  1.6654002666473389 2.422389268875122 13.77734661102295
Loss :  1.7050848007202148 2.351087808609009 13.46052360534668
Loss :  1.7082961797714233 2.5241570472717285 14.329080581665039
Loss :  1.714744210243225 2.486797332763672 14.148731231689453
  batch 40 loss: 1.714744210243225, 2.486797332763672, 14.148731231689453
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.688267469406128 2.3952481746673584 13.664508819580078
Loss :  1.6796224117279053 1.8934403657913208 11.146824836730957
Loss :  1.6725980043411255 2.2344088554382324 12.844642639160156
Loss :  1.6815602779388428 2.3978915214538574 13.671018600463867
Loss :  1.667471170425415 2.2372453212738037 12.853697776794434
Loss :  1.6875349283218384 2.11706805229187 12.272875785827637
Loss :  1.709102749824524 2.036360740661621 11.89090633392334
Loss :  1.6774567365646362 2.122138023376465 12.28814697265625
Loss :  1.718392252922058 2.556058645248413 14.498685836791992
Loss :  1.681514859199524 1.8954737186431885 11.158883094787598
Loss :  1.7027852535247803 2.0674474239349365 12.040022850036621
Loss :  1.6991052627563477 1.9803235530853271 11.600723266601562
Loss :  1.6854232549667358 2.2946107387542725 13.158476829528809
Loss :  1.7040143013000488 2.555286169052124 14.480445861816406
Loss :  1.677171230316162 2.3638572692871094 13.496458053588867
Loss :  1.7176048755645752 2.5095903873443604 14.265556335449219
Loss :  1.6813313961029053 1.92965829372406 11.329623222351074
Loss :  1.6707842350006104 1.8169025182724 10.75529670715332
Loss :  1.6808072328567505 2.0827879905700684 12.094747543334961
Loss :  1.7219222784042358 1.7633299827575684 10.538572311401367
  batch 60 loss: 1.7219222784042358, 1.7633299827575684, 10.538572311401367
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6707565784454346 2.1233510971069336 12.287511825561523
Loss :  1.6894347667694092 1.968487024307251 11.531869888305664
Loss :  1.6768077611923218 2.3753111362457275 13.553362846374512
Loss :  1.6688181161880493 2.81748104095459 15.756223678588867
Loss :  1.6561118364334106 2.1747374534606934 12.529799461364746
Loss :  1.6675422191619873 4.218143939971924 22.75826072692871
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6751424074172974 4.235251426696777 22.85140037536621
Loss :  1.6731728315353394 3.9835691452026367 21.591018676757812
Loss :  1.6762018203735352 3.974321126937866 21.547805786132812
Total LOSS train 12.44732139293964 valid 22.187121391296387
CE LOSS train 1.6865010261535645 valid 0.4190504550933838
Contrastive LOSS train 2.1521640539169313 valid 0.9935802817344666
EPOCH 296:
Loss :  1.6989156007766724 2.3168487548828125 13.283159255981445
Loss :  1.7073549032211304 2.588144540786743 14.648077011108398
Loss :  1.6875925064086914 1.714345932006836 10.259322166442871
Loss :  1.6917206048965454 1.984492540359497 11.61418342590332
Loss :  1.7028963565826416 1.7938601970672607 10.672197341918945
Loss :  1.6794880628585815 2.058973789215088 11.974356651306152
Loss :  1.7028541564941406 1.8028913736343384 10.717310905456543
Loss :  1.6870050430297852 1.5928722620010376 9.651366233825684
Loss :  1.6807808876037598 2.136317491531372 12.362367630004883
Loss :  1.7029526233673096 1.640119194984436 9.903548240661621
Loss :  1.6751006841659546 1.9730048179626465 11.540124893188477
Loss :  1.6748536825180054 2.4146981239318848 13.748343467712402
Loss :  1.6727544069290161 1.9097546339035034 11.221527099609375
Loss :  1.676299810409546 1.9437631368637085 11.395115852355957
Loss :  1.7120546102523804 1.8683363199234009 11.053735733032227
Loss :  1.7088840007781982 2.3469746112823486 13.443757057189941
Loss :  1.6708632707595825 2.606236696243286 14.702046394348145
Loss :  1.688164234161377 2.4732184410095215 14.054256439208984
Loss :  1.6682980060577393 2.3658525943756104 13.497560501098633
Loss :  1.7082023620605469 2.0654802322387695 12.035603523254395
  batch 20 loss: 1.7082023620605469, 2.0654802322387695, 12.035603523254395
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6843281984329224 2.390671968460083 13.637687683105469
Loss :  1.6676143407821655 2.3054440021514893 13.194833755493164
Loss :  1.6791452169418335 1.9906306266784668 11.632298469543457
Loss :  1.6894381046295166 1.7852946519851685 10.615911483764648
Loss :  1.70859956741333 2.0990748405456543 12.203973770141602
Loss :  1.6816151142120361 2.0065507888793945 11.71436882019043
Loss :  1.6882561445236206 2.339390516281128 13.385209083557129
Loss :  1.6846239566802979 1.863451600074768 11.00188159942627
Loss :  1.6514708995819092 2.1267735958099365 12.28533935546875
Loss :  1.7069765329360962 2.1776223182678223 12.595088005065918
Loss :  1.6524206399917603 2.0423171520233154 11.864006996154785
Loss :  1.696304440498352 2.378359794616699 13.588103294372559
Loss :  1.6804178953170776 2.1128652095794678 12.244744300842285
Loss :  1.6793255805969238 2.0180203914642334 11.769428253173828
Loss :  1.656790852546692 2.23679780960083 12.840779304504395
Loss :  1.6663912534713745 1.8536508083343506 10.934645652770996
Loss :  1.6654052734375 2.495112419128418 14.14096736907959
Loss :  1.704651951789856 2.214977979660034 12.779541969299316
Loss :  1.7081005573272705 2.137476921081543 12.395484924316406
Loss :  1.7145789861679077 1.9430667161941528 11.429912567138672
  batch 40 loss: 1.7145789861679077, 1.9430667161941528, 11.429912567138672
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6881686449050903 1.6833330392837524 10.104833602905273
Loss :  1.679526925086975 1.6022322177886963 9.690688133239746
Loss :  1.6726131439208984 2.014146327972412 11.743345260620117
Loss :  1.681274175643921 1.99273681640625 11.64495849609375
Loss :  1.6672251224517822 2.2245376110076904 12.789913177490234
Loss :  1.6870747804641724 2.0196712017059326 11.785430908203125
Loss :  1.708330512046814 1.7379186153411865 10.397923469543457
Loss :  1.676863431930542 1.7784403562545776 10.56906509399414
Loss :  1.7173409461975098 1.738300085067749 10.408842086791992
Loss :  1.6805956363677979 1.6495107412338257 9.928149223327637
Loss :  1.7018104791641235 1.7897647619247437 10.650634765625
Loss :  1.6981948614120483 2.2037277221679688 12.716833114624023
Loss :  1.6846708059310913 2.4253804683685303 13.811573028564453
Loss :  1.7032519578933716 2.191495656967163 12.660730361938477
Loss :  1.67698335647583 2.452493906021118 13.939453125
Loss :  1.7174016237258911 2.4589459896087646 14.012131690979004
Loss :  1.6818002462387085 2.337878465652466 13.37119197845459
Loss :  1.67169189453125 1.896972894668579 11.156556129455566
Loss :  1.6817116737365723 2.152644157409668 12.44493293762207
Loss :  1.7226141691207886 1.6548690795898438 9.996959686279297
  batch 60 loss: 1.7226141691207886, 1.6548690795898438, 9.996959686279297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6722183227539062 1.9335616827011108 11.34002685546875
Loss :  1.6908998489379883 1.8219363689422607 10.800581932067871
Loss :  1.6780660152435303 1.7815356254577637 10.585744857788086
Loss :  1.6702461242675781 2.092280387878418 12.131648063659668
Loss :  1.6574079990386963 1.862180471420288 10.968310356140137
Loss :  1.6640677452087402 4.184680461883545 22.58747100830078
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.672324538230896 4.1679582595825195 22.512115478515625
Loss :  1.670398473739624 4.037166118621826 21.856229782104492
Loss :  1.6715279817581177 4.143880844116211 22.390932083129883
Total LOSS train 11.96434807410607 valid 22.336687088012695
CE LOSS train 1.6866380618168757 valid 0.4178819954395294
Contrastive LOSS train 2.0555420068594126 valid 1.0359702110290527
EPOCH 297:
Loss :  1.6993649005889893 2.1259000301361084 12.328865051269531
Loss :  1.7076612710952759 2.58747935295105 14.645058631896973
Loss :  1.6878485679626465 1.612528920173645 9.750494003295898
Loss :  1.691812515258789 1.78827702999115 10.633197784423828
Loss :  1.7028305530548096 1.9254133701324463 11.329896926879883
Loss :  1.6794739961624146 2.05824613571167 11.970705032348633
Loss :  1.7025614976882935 1.906124472618103 11.233183860778809
Loss :  1.6866484880447388 1.582993745803833 9.601616859436035
Loss :  1.6803374290466309 2.160109519958496 12.480884552001953
Loss :  1.702506184577942 2.3851139545440674 13.62807559967041
Loss :  1.6749987602233887 2.438899278640747 13.869495391845703
Loss :  1.6747301816940308 2.4607527256011963 13.978493690490723
Loss :  1.6729203462600708 2.0872600078582764 12.109220504760742
Loss :  1.6767113208770752 2.044131278991699 11.897367477416992
Loss :  1.7123159170150757 2.2647359371185303 13.035995483398438
Loss :  1.7092267274856567 1.827525019645691 10.846851348876953
Loss :  1.6715288162231445 1.9289132356643677 11.316095352172852
Loss :  1.6887764930725098 1.8228250741958618 10.802902221679688
Loss :  1.6692007780075073 1.662471890449524 9.981560707092285
Loss :  1.7093260288238525 2.0800492763519287 12.109572410583496
  batch 20 loss: 1.7093260288238525, 2.0800492763519287, 12.109572410583496
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6854058504104614 2.5829176902770996 14.599994659423828
Loss :  1.6687639951705933 2.5927178859710693 14.632353782653809
Loss :  1.6802308559417725 2.295111894607544 13.155790328979492
Loss :  1.6901543140411377 2.468881845474243 14.034563064575195
Loss :  1.7091375589370728 2.616945505142212 14.793865203857422
Loss :  1.6820671558380127 1.9171637296676636 11.267885208129883
Loss :  1.6885247230529785 2.308790445327759 13.232477188110352
Loss :  1.6844555139541626 2.1395344734191895 12.38212776184082
Loss :  1.651292085647583 2.2837483882904053 13.07003402709961
Loss :  1.7063032388687134 2.544192314147949 14.427265167236328
Loss :  1.651861548423767 2.071275472640991 12.008238792419434
Loss :  1.6959134340286255 1.9267526865005493 11.329676628112793
Loss :  1.6798917055130005 1.9985138177871704 11.672460556030273
Loss :  1.6787701845169067 2.200070381164551 12.679121971130371
Loss :  1.6562610864639282 2.558570146560669 14.449111938476562
Loss :  1.6661059856414795 2.5393919944763184 14.363066673278809
Loss :  1.6653286218643188 2.43583083152771 13.844483375549316
Loss :  1.7048723697662354 1.8522276878356934 10.966011047363281
Loss :  1.708374261856079 1.7288142442703247 10.352445602416992
Loss :  1.714722990989685 1.8265254497528076 10.847350120544434
  batch 40 loss: 1.714722990989685, 1.8265254497528076, 10.847350120544434
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6883867979049683 2.2059359550476074 12.718067169189453
Loss :  1.6796574592590332 1.6915608644485474 10.137462615966797
Loss :  1.6726114749908447 2.0063679218292236 11.704450607299805
Loss :  1.6814051866531372 1.82853102684021 10.824060440063477
Loss :  1.6672356128692627 1.6291190385818481 9.812830924987793
Loss :  1.68704354763031 2.4737892150878906 14.055989265441895
Loss :  1.7082738876342773 2.287208080291748 13.144314765930176
Loss :  1.676954984664917 2.5162391662597656 14.258151054382324
Loss :  1.7176531553268433 2.5554280281066895 14.494792938232422
Loss :  1.6810028553009033 2.076308488845825 12.062544822692871
Loss :  1.7022117376327515 2.0287296772003174 11.84585952758789
Loss :  1.698596715927124 1.9477275609970093 11.437234878540039
Loss :  1.6851540803909302 1.9802037477493286 11.586173057556152
Loss :  1.7040016651153564 2.557342052459717 14.49071216583252
Loss :  1.6775336265563965 1.921224594116211 11.28365707397461
Loss :  1.7182565927505493 1.743147611618042 10.433995246887207
Loss :  1.6824332475662231 2.2868032455444336 13.116449356079102
Loss :  1.6721422672271729 1.7798669338226318 10.571476936340332
Loss :  1.682068109512329 2.0017056465148926 11.690595626831055
Loss :  1.7226660251617432 1.8903858661651611 11.17459487915039
  batch 60 loss: 1.7226660251617432, 1.8903858661651611, 11.17459487915039
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6721991300582886 1.9971511363983154 11.657955169677734
Loss :  1.6905306577682495 2.4167144298553467 13.774103164672852
Loss :  1.6779963970184326 2.073793888092041 12.046965599060059
Loss :  1.6700860261917114 2.254035472869873 12.940263748168945
Loss :  1.657520055770874 1.7040855884552002 10.177947998046875
Loss :  1.6634148359298706 3.785370111465454 20.59026527404785
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6711777448654175 3.6817562580108643 20.079957962036133
Loss :  1.668676733970642 3.5655248165130615 19.496299743652344
Loss :  1.6729660034179688 3.4499926567077637 18.922929763793945
Total LOSS train 12.263053923386794 valid 19.77236318588257
CE LOSS train 1.6868129161687997 valid 0.4182415008544922
Contrastive LOSS train 2.1152481904396643 valid 0.8624981641769409
EPOCH 298:
Loss :  1.6996811628341675 1.98618745803833 11.63061809539795
Loss :  1.7080811262130737 1.9588836431503296 11.5024995803833
Loss :  1.6885257959365845 1.663399338722229 10.005522727966309
Loss :  1.6926549673080444 2.095992088317871 12.172615051269531
Loss :  1.7037668228149414 2.322038412094116 13.313959121704102
Loss :  1.6806902885437012 2.348196506500244 13.421672821044922
Loss :  1.7036877870559692 2.0892841815948486 12.150108337402344
Loss :  1.6879806518554688 2.223480224609375 12.805381774902344
Loss :  1.6815296411514282 1.6580778360366821 9.971919059753418
Loss :  1.7032005786895752 1.6929588317871094 10.167994499206543
Loss :  1.6755926609039307 2.137294054031372 12.362062454223633
Loss :  1.6752591133117676 2.7239980697631836 15.295249938964844
Loss :  1.6733930110931396 2.5242624282836914 14.294705390930176
Loss :  1.676958441734314 1.8232496976852417 10.793206214904785
Loss :  1.712436556816101 1.799437403678894 10.709623336791992
Loss :  1.709716558456421 2.0834434032440186 12.126934051513672
Loss :  1.671854853630066 1.9936912059783936 11.640311241149902
Loss :  1.6892180442810059 2.034376859664917 11.861103057861328
Loss :  1.6696757078170776 1.9587541818618774 11.463446617126465
Loss :  1.7095626592636108 2.1700828075408936 12.559976577758789
  batch 20 loss: 1.7095626592636108, 2.1700828075408936, 12.559976577758789
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6859955787658691 1.8730169534683228 11.051080703735352
Loss :  1.6695947647094727 1.9004913568496704 11.172051429748535
Loss :  1.6808347702026367 2.074157238006592 12.051621437072754
Loss :  1.6909254789352417 2.274878740310669 13.065319061279297
Loss :  1.7096939086914062 2.4967830181121826 14.193609237670898
Loss :  1.6827282905578613 2.1843762397766113 12.604608535766602
Loss :  1.6890199184417725 2.3782918453216553 13.580479621887207
Loss :  1.6849606037139893 1.8094455003738403 10.73218822479248
Loss :  1.6520545482635498 2.164881467819214 12.476462364196777
Loss :  1.70694100856781 2.7353055477142334 15.383468627929688
Loss :  1.652680516242981 2.4765636920928955 14.03549861907959
Loss :  1.6964712142944336 1.9885518550872803 11.639230728149414
Loss :  1.6805658340454102 2.075063943862915 12.055885314941406
Loss :  1.679540991783142 2.101334810256958 12.186214447021484
Loss :  1.657358169555664 2.298192024230957 13.14831829071045
Loss :  1.6671078205108643 2.0425047874450684 11.879631996154785
Loss :  1.6663081645965576 1.9535062313079834 11.433839797973633
Loss :  1.7055014371871948 2.018859386444092 11.799798965454102
Loss :  1.708901286125183 2.562345266342163 14.520627975463867
Loss :  1.7152197360992432 2.4127256870269775 13.778847694396973
  batch 40 loss: 1.7152197360992432, 2.4127256870269775, 13.778847694396973
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.688631296157837 2.6995530128479004 15.186395645141602
Loss :  1.679958701133728 2.447653293609619 13.918224334716797
Loss :  1.673004388809204 2.027928590774536 11.812646865844727
Loss :  1.6814086437225342 1.891178846359253 11.137303352355957
Loss :  1.667495846748352 1.6685022115707397 10.01000690460205
Loss :  1.6870529651641846 2.036944627761841 11.87177562713623
Loss :  1.708207368850708 1.9211807250976562 11.31411075592041
Loss :  1.6771906614303589 2.472386360168457 14.039122581481934
Loss :  1.7177971601486206 2.088007926940918 12.1578369140625
Loss :  1.6813831329345703 1.6125348806381226 9.744057655334473
Loss :  1.7029321193695068 1.7296539545059204 10.351202011108398
Loss :  1.699142575263977 1.8435155153274536 10.916720390319824
Loss :  1.6857333183288574 1.8942278623580933 11.156871795654297
Loss :  1.7043083906173706 2.3559718132019043 13.48416805267334
Loss :  1.678397536277771 2.798456907272339 15.670681953430176
Loss :  1.7184349298477173 1.8397806882858276 10.917338371276855
Loss :  1.6826916933059692 2.0125606060028076 11.745494842529297
Loss :  1.6723620891571045 2.1366775035858154 12.35575008392334
Loss :  1.6823606491088867 2.7076098918914795 15.220410346984863
Loss :  1.7230839729309082 2.2021708488464355 12.733938217163086
  batch 60 loss: 1.7230839729309082, 2.2021708488464355, 12.733938217163086
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.672782063484192 2.32232666015625 13.284415245056152
Loss :  1.6913684606552124 1.7181060314178467 10.281898498535156
Loss :  1.6787164211273193 1.959723711013794 11.477334976196289
Loss :  1.6709485054016113 2.179234266281128 12.567119598388672
Loss :  1.658339500427246 1.428009271621704 8.798385620117188
Loss :  1.6682478189468384 3.5207455158233643 19.271974563598633
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.676039457321167 3.4908299446105957 19.13018798828125
Loss :  1.673453450202942 3.2983920574188232 18.165414810180664
Loss :  1.680558443069458 3.246499538421631 17.913057327270508
Total LOSS train 12.233706210209773 valid 18.620158672332764
CE LOSS train 1.687378505560068 valid 0.4201396107673645
Contrastive LOSS train 2.1092655420303346 valid 0.8116248846054077
EPOCH 299:
Loss :  1.7003662586212158 1.675586223602295 10.07829761505127
Loss :  1.7085487842559814 2.574141263961792 14.579255104064941
Loss :  1.6888355016708374 1.7092292308807373 10.234981536865234
Loss :  1.6927556991577148 1.864423394203186 11.014872550964355
Loss :  1.7037426233291626 2.0154263973236084 11.780875205993652
Loss :  1.6807422637939453 1.9348556995391846 11.355020523071289
Loss :  1.703685998916626 2.6299147605895996 14.853260040283203
Loss :  1.6880207061767578 2.191650629043579 12.646273612976074
Loss :  1.6817584037780762 2.2839250564575195 13.101383209228516
Loss :  1.7037632465362549 2.3787009716033936 13.597268104553223
Loss :  1.6760079860687256 2.423335313796997 13.792684555053711
Loss :  1.6755400896072388 2.2438466548919678 12.894773483276367
Loss :  1.6738404035568237 1.7940558195114136 10.644119262695312
Loss :  1.6776107549667358 2.4999988079071045 14.177604675292969
Loss :  1.7132046222686768 1.8798925876617432 11.112667083740234
Loss :  1.7102054357528687 2.4296109676361084 13.858260154724121
Loss :  1.6726258993148804 2.190955877304077 12.627405166625977
Loss :  1.6896365880966187 2.0646278858184814 12.012775421142578
Loss :  1.6703075170516968 2.549062967300415 14.415621757507324
Loss :  1.7096854448318481 1.8422424793243408 10.920897483825684
  batch 20 loss: 1.7096854448318481, 1.8422424793243408, 10.920897483825684
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6862870454788208 1.667023777961731 10.021406173706055
Loss :  1.6696784496307373 2.1787219047546387 12.563288688659668
Loss :  1.680966854095459 1.7338318824768066 10.350126266479492
Loss :  1.6909762620925903 1.9189515113830566 11.285733222961426
Loss :  1.709806203842163 2.089813232421875 12.158872604370117
Loss :  1.6830072402954102 2.3327441215515137 13.346728324890137
Loss :  1.6894439458847046 2.232778310775757 12.8533353805542
Loss :  1.6855179071426392 2.0264222621917725 11.81762981414795
Loss :  1.652877926826477 2.48787784576416 14.092267036437988
Loss :  1.7071797847747803 2.5123708248138428 14.269034385681152
Loss :  1.6535942554473877 2.5437169075012207 14.372178077697754
Loss :  1.6969724893569946 2.131498098373413 12.354463577270508
Loss :  1.6812679767608643 2.1437253952026367 12.399894714355469
Loss :  1.6801791191101074 2.6832501888275146 15.096429824829102
Loss :  1.6579312086105347 2.6559722423553467 14.937792778015137
Loss :  1.6675291061401367 2.4053993225097656 13.694525718688965
Loss :  1.666698932647705 2.671558380126953 15.024490356445312
Loss :  1.7054786682128906 1.8222713470458984 10.816835403442383
Loss :  1.7090848684310913 1.9018610715866089 11.218389511108398
Loss :  1.7153799533843994 1.7090034484863281 10.260396957397461
  batch 40 loss: 1.7153799533843994, 1.7090034484863281, 10.260396957397461
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.689494252204895 2.2968945503234863 13.173966407775879
Loss :  1.6807588338851929 1.9272048473358154 11.31678295135498
Loss :  1.674042820930481 2.7908005714416504 15.628045082092285
Loss :  1.6825215816497803 2.383235216140747 13.598697662353516
Loss :  1.6687051057815552 1.6225531101226807 9.78147029876709
Loss :  1.6882128715515137 2.1113789081573486 12.245107650756836
Loss :  1.7091914415359497 2.215634822845459 12.787365913391113
Loss :  1.6785355806350708 2.1652650833129883 12.504860877990723
Loss :  1.7181650400161743 2.202831745147705 12.73232364654541
Loss :  1.6821879148483276 2.1271395683288574 12.317886352539062
Loss :  1.703330159187317 1.879009485244751 11.098377227783203
Loss :  1.699214220046997 2.2723228931427 13.06082820892334
Loss :  1.685901403427124 2.183110475540161 12.60145378112793
Loss :  1.7041558027267456 2.258527994155884 12.996795654296875
Loss :  1.6785495281219482 2.52189302444458 14.28801441192627
Loss :  1.71841299533844 2.062187910079956 12.029353141784668
Loss :  1.6829687356948853 2.113233804702759 12.249137878417969
Loss :  1.672847032546997 2.1407320499420166 12.376506805419922
Loss :  1.6828240156173706 2.4769301414489746 14.067475318908691
Loss :  1.7231929302215576 1.812995433807373 10.78817081451416
  batch 60 loss: 1.7231929302215576, 1.812995433807373, 10.78817081451416
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4, 5], device='cuda:0')
Loss :  1.6733224391937256 1.9445137977600098 11.395891189575195
Loss :  1.6919537782669067 1.8193026781082153 10.788467407226562
Loss :  1.6792858839035034 1.737613320350647 10.367352485656738
Loss :  1.6716002225875854 2.1133780479431152 12.23849105834961
Loss :  1.6590886116027832 1.5345031023025513 9.33160400390625
Loss :  1.6702046394348145 3.7972772121429443 20.65658950805664
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6776455640792847 3.59895658493042 19.672428131103516
Loss :  1.6755365133285522 3.499013662338257 19.170604705810547
Loss :  1.6806803941726685 3.352320671081543 18.442285537719727
Total LOSS train 12.436869870699368 valid 19.485476970672607
CE LOSS train 1.6878339327298677 valid 0.4201700985431671
Contrastive LOSS train 2.1498071945630586 valid 0.8380801677703857
EPOCH 300:
Loss :  1.7008429765701294 2.1468963623046875 12.435324668884277
Loss :  1.7091114521026611 2.454941749572754 13.983819961547852
Loss :  1.6895769834518433 1.8172441720962524 10.775797843933105
Loss :  1.6934775114059448 2.031785011291504 11.852402687072754
Loss :  1.7044167518615723 1.7957805395126343 10.683319091796875
Loss :  1.6818164587020874 1.861138939857483 10.987510681152344
Loss :  1.7046271562576294 1.7562732696533203 10.485993385314941
Loss :  1.689197063446045 1.7676856517791748 10.527626037597656
Loss :  1.6829383373260498 1.9884713888168335 11.625295639038086
Loss :  1.704886555671692 1.6924728155136108 10.167250633239746
Loss :  1.6771560907363892 2.6175591945648193 14.764952659606934
Loss :  1.6765058040618896 2.040863275527954 11.88082218170166
Loss :  1.6748019456863403 1.8564784526824951 10.957194328308105
Loss :  1.6780868768692017 2.1072380542755127 12.214277267456055
Loss :  1.7134734392166138 2.548642635345459 14.456686973571777
Loss :  1.7103906869888306 2.5877110958099365 14.648946762084961
Loss :  1.6727162599563599 2.5395290851593018 14.370361328125
Loss :  1.6898194551467896 2.0341427326202393 11.860532760620117
Loss :  1.6703026294708252 2.0362749099731445 11.851676940917969
Loss :  1.7099642753601074 1.5543270111083984 9.481599807739258
  batch 20 loss: 1.7099642753601074, 1.5543270111083984, 9.481599807739258
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6862902641296387 1.8282359838485718 10.827470779418945
Loss :  1.670097827911377 2.147617816925049 12.408185958862305
Loss :  1.6818238496780396 2.453831195831299 13.950979232788086
Loss :  1.6917550563812256 2.0933732986450195 12.158621788024902
Loss :  1.7104487419128418 2.146002769470215 12.440462112426758
Loss :  1.6838401556015015 1.6763137578964233 10.065408706665039
Loss :  1.6903584003448486 2.377682685852051 13.578771591186523
Loss :  1.6867282390594482 1.889194369316101 11.132699966430664
Loss :  1.6538140773773193 2.1987252235412598 12.647439956665039
Loss :  1.708340048789978 2.1116769313812256 12.266724586486816
Loss :  1.6545292139053345 1.9987989664077759 11.648524284362793
Loss :  1.698001503944397 1.7504533529281616 10.450268745422363
Loss :  1.682542324066162 2.359282970428467 13.478958129882812
Loss :  1.6815000772476196 1.8307689428329468 10.835345268249512
Loss :  1.6594679355621338 1.9786001443862915 11.552468299865723
Loss :  1.6688822507858276 1.7963825464248657 10.650794982910156
Loss :  1.6684927940368652 1.6902663707733154 10.11982536315918
Loss :  1.7069754600524902 1.8061624765396118 10.737787246704102
Loss :  1.710312843322754 2.2186272144317627 12.803448677062988
Loss :  1.7166614532470703 2.6090855598449707 14.762088775634766
  batch 40 loss: 1.7166614532470703, 2.6090855598449707, 14.762088775634766
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.6909106969833374 1.6297262907028198 9.8395414352417
Loss :  1.6823840141296387 2.1950345039367676 12.657556533813477
Loss :  1.6758958101272583 2.6383519172668457 14.867654800415039
Loss :  1.6843245029449463 2.053725242614746 11.952950477600098
Loss :  1.6702929735183716 2.135714292526245 12.348864555358887
Loss :  1.689500093460083 2.0838096141815186 12.108548164367676
Loss :  1.7104052305221558 1.9288127422332764 11.354469299316406
Loss :  1.6796597242355347 2.1713383197784424 12.536351203918457
Loss :  1.7191888093948364 2.059375047683716 12.016063690185547
Loss :  1.6835553646087646 1.492724061012268 9.147175788879395
Loss :  1.704728364944458 1.8010514974594116 10.709985733032227
Loss :  1.7008121013641357 1.8242905139923096 10.822264671325684
Loss :  1.687422275543213 1.619480848312378 9.784826278686523
Loss :  1.705858826637268 2.2053894996643066 12.732806205749512
Loss :  1.6800284385681152 1.751272201538086 10.436389923095703
Loss :  1.719597339630127 1.6136316061019897 9.787755966186523
Loss :  1.6842663288116455 2.31772780418396 13.272905349731445
Loss :  1.6739575862884521 2.5953612327575684 14.650764465332031
Loss :  1.683832049369812 2.702007293701172 15.193868637084961
Loss :  1.7240569591522217 1.9010759592056274 11.229436874389648
  batch 60 loss: 1.7240569591522217, 1.9010759592056274, 11.229436874389648
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 4], device='cuda:0')
Loss :  1.674235463142395 1.6508923768997192 9.92869758605957
Loss :  1.6926780939102173 1.9829503297805786 11.607430458068848
Loss :  1.67999267578125 1.8242642879486084 10.801314353942871
Loss :  1.672468662261963 2.683518409729004 15.09006118774414
Loss :  1.659878134727478 1.4948008060455322 9.133881568908691
Loss :  1.6730018854141235 3.8983869552612305 21.16493797302246
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6806285381317139 3.754481077194214 20.453033447265625
Loss :  1.6780693531036377 3.77264142036438 20.541276931762695
Loss :  1.6855101585388184 3.7483913898468018 20.427467346191406
Total LOSS train 11.88521894308237 valid 20.646678924560547
CE LOSS train 1.6888446422723624 valid 0.4213775396347046
Contrastive LOSS train 2.0392748557604277 valid 0.9370978474617004
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 1.547 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 1.547 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 1.547 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 1.547 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 1.547 MB uploaded (0.000 MB deduped)wandb: - 0.009 MB of 1.547 MB uploaded (0.000 MB deduped)wandb: \ 0.009 MB of 1.547 MB uploaded (0.000 MB deduped)wandb: | 0.189 MB of 1.547 MB uploaded (0.000 MB deduped)wandb: / 0.392 MB of 1.547 MB uploaded (0.000 MB deduped)wandb: - 0.571 MB of 1.547 MB uploaded (0.000 MB deduped)wandb: \ 0.767 MB of 1.547 MB uploaded (0.000 MB deduped)wandb: | 1.025 MB of 1.547 MB uploaded (0.000 MB deduped)wandb: / 1.235 MB of 1.547 MB uploaded (0.000 MB deduped)wandb: - 1.454 MB of 1.547 MB uploaded (0.000 MB deduped)wandb: \ 1.547 MB of 1.547 MB uploaded (0.000 MB deduped)wandb: | 1.547 MB of 1.547 MB uploaded (0.000 MB deduped)wandb: / 1.547 MB of 1.547 MB uploaded (0.000 MB deduped)wandb: - 1.547 MB of 1.547 MB uploaded (0.000 MB deduped)wandb: \ 1.547 MB of 1.547 MB uploaded (0.000 MB deduped)wandb: | 1.547 MB of 1.547 MB uploaded (0.000 MB deduped)wandb: / 1.547 MB of 1.547 MB uploaded (0.000 MB deduped)wandb: - 1.547 MB of 1.547 MB uploaded (0.000 MB deduped)wandb: \ 1.547 MB of 1.547 MB uploaded (0.000 MB deduped)wandb: | 1.547 MB of 1.547 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                    epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:                       lr ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   train contrastive loss ‚ñá‚ñà‚ñà‚ñá‚ñá‚ñÖ‚ñá‚ñà‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ
wandb: train cross-entropy loss ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:               train loss ‚ñá‚ñà‚ñà‚ñá‚ñá‚ñÑ‚ñá‚ñà‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ
wandb:     val contrastive loss ‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñá‚ñÜ‚ñÜ‚ñÅ‚ñá‚ñÑ‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÉ‚ñÖ
wandb:   val cross-entropy loss ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                 val loss ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            best_val_loss 16.49847
wandb:                    epoch 300
wandb:                       lr 0.00094
wandb:   train contrastive loss 2.03927
wandb: train cross-entropy loss 1.68884
wandb:               train loss 11.88522
wandb:     val contrastive loss 0.9371
wandb:   val cross-entropy loss 0.42138
wandb:                 val loss 20.64668
wandb: 
wandb: Synced radiant-ox-26: https://wandb.ai/harsh21122/part_segmentation/runs/2ipu888q
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230206_003657-2ipu888q/logs

wandb: Currently logged in as: harsh21122. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.3
wandb: Run data is saved locally in /ssd-scratch/harsh21122/ContextPromptPart/wandb/run-20230205_144935-29kcf1mc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bright-lamp-25
wandb: ‚≠êÔ∏è View project at https://wandb.ai/harsh21122/part_segmentation
wandb: üöÄ View run at https://wandb.ai/harsh21122/part_segmentation/runs/29kcf1mc
/home/harsh21122/.conda/envs/part/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
/home/harsh21122/.conda/envs/part/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Arguments are :  Namespace(batch_size=10, epochs=300, starting_epoch=1, base_lr=0.0003, clip_model='RN50', result_dir='./Results/', model_dir='../ContextPromptPart_model', dataset_dir='/home/harsh21122/tmp/cat_dataset', resume=False, model_name='../ContextPromptPart_model/last_model', calc_accuracy_training=False, multi_step_scheduler=True, lr_decay=0.1, milestones=[50, 100, 150, 200, 250, 300], wandb=True, temperature=0.19, layer_len=-1, ref_layer1='relu3_2', ref_layer2='relu5_4', ref_weight1=0.33, ref_weight2=1.0, lamda_contrastive=1.0, lamda_cross=1.0, weight_decay=0.01)
Using device :  cuda
Length of Train dataset : 646 and Test dataset : 35
Length of Train loader : 65 and Test loader : 4
self.partnames :  ['background', 'head', 'neck', 'torso', 'tail', 'legs']
prompts :  [' the background of the cat.', ' the head of the cat.', ' the neck of the cat.', ' the torso of the cat.', ' the tail of the cat.', ' the legs of the cat.']
model device :  cuda
self.prompts : torch.Size([6, 77])
self.prompts : torch.Size([6, 1024])
self.prompts rquires grad :  <built-in method requires_grad_ of Tensor object at 0x7f682ceb64f0>
layer: relu3_2,relu5_4
weighs: [0.33, 1.0]
Printing parameters and their gradient
gamma True
image_encoder.0.weight False
image_encoder.1.weight False
image_encoder.1.bias False
image_encoder.3.weight False
image_encoder.4.weight False
image_encoder.4.bias False
image_encoder.6.weight False
image_encoder.7.weight False
image_encoder.7.bias False
image_encoder.10.0.conv1.weight False
image_encoder.10.0.bn1.weight False
image_encoder.10.0.bn1.bias False
image_encoder.10.0.conv2.weight False
image_encoder.10.0.bn2.weight False
image_encoder.10.0.bn2.bias False
image_encoder.10.0.conv3.weight False
image_encoder.10.0.bn3.weight False
image_encoder.10.0.bn3.bias False
image_encoder.10.0.downsample.0.weight False
image_encoder.10.0.downsample.1.weight False
image_encoder.10.0.downsample.1.bias False
image_encoder.10.1.conv1.weight False
image_encoder.10.1.bn1.weight False
image_encoder.10.1.bn1.bias False
image_encoder.10.1.conv2.weight False
image_encoder.10.1.bn2.weight False
image_encoder.10.1.bn2.bias False
image_encoder.10.1.conv3.weight False
image_encoder.10.1.bn3.weight False
image_encoder.10.1.bn3.bias False
image_encoder.10.2.conv1.weight False
image_encoder.10.2.bn1.weight False
image_encoder.10.2.bn1.bias False
image_encoder.10.2.conv2.weight False
image_encoder.10.2.bn2.weight False
image_encoder.10.2.bn2.bias False
image_encoder.10.2.conv3.weight False
image_encoder.10.2.bn3.weight False
image_encoder.10.2.bn3.bias False
image_encoder.11.0.conv1.weight False
image_encoder.11.0.bn1.weight False
image_encoder.11.0.bn1.bias False
image_encoder.11.0.conv2.weight False
image_encoder.11.0.bn2.weight False
image_encoder.11.0.bn2.bias False
image_encoder.11.0.conv3.weight False
image_encoder.11.0.bn3.weight False
image_encoder.11.0.bn3.bias False
image_encoder.11.0.downsample.0.weight False
image_encoder.11.0.downsample.1.weight False
image_encoder.11.0.downsample.1.bias False
image_encoder.11.1.conv1.weight False
image_encoder.11.1.bn1.weight False
image_encoder.11.1.bn1.bias False
image_encoder.11.1.conv2.weight False
image_encoder.11.1.bn2.weight False
image_encoder.11.1.bn2.bias False
image_encoder.11.1.conv3.weight False
image_encoder.11.1.bn3.weight False
image_encoder.11.1.bn3.bias False
image_encoder.11.2.conv1.weight False
image_encoder.11.2.bn1.weight False
image_encoder.11.2.bn1.bias False
image_encoder.11.2.conv2.weight False
image_encoder.11.2.bn2.weight False
image_encoder.11.2.bn2.bias False
image_encoder.11.2.conv3.weight False
image_encoder.11.2.bn3.weight False
image_encoder.11.2.bn3.bias False
image_encoder.11.3.conv1.weight False
image_encoder.11.3.bn1.weight False
image_encoder.11.3.bn1.bias False
image_encoder.11.3.conv2.weight False
image_encoder.11.3.bn2.weight False
image_encoder.11.3.bn2.bias False
image_encoder.11.3.conv3.weight False
image_encoder.11.3.bn3.weight False
image_encoder.11.3.bn3.bias False
image_encoder.12.0.conv1.weight False
image_encoder.12.0.bn1.weight False
image_encoder.12.0.bn1.bias False
image_encoder.12.0.conv2.weight False
image_encoder.12.0.bn2.weight False
image_encoder.12.0.bn2.bias False
image_encoder.12.0.conv3.weight False
image_encoder.12.0.bn3.weight False
image_encoder.12.0.bn3.bias False
image_encoder.12.0.downsample.0.weight False
image_encoder.12.0.downsample.1.weight False
image_encoder.12.0.downsample.1.bias False
image_encoder.12.1.conv1.weight False
image_encoder.12.1.bn1.weight False
image_encoder.12.1.bn1.bias False
image_encoder.12.1.conv2.weight False
image_encoder.12.1.bn2.weight False
image_encoder.12.1.bn2.bias False
image_encoder.12.1.conv3.weight False
image_encoder.12.1.bn3.weight False
image_encoder.12.1.bn3.bias False
image_encoder.12.2.conv1.weight False
image_encoder.12.2.bn1.weight False
image_encoder.12.2.bn1.bias False
image_encoder.12.2.conv2.weight False
image_encoder.12.2.bn2.weight False
image_encoder.12.2.bn2.bias False
image_encoder.12.2.conv3.weight False
image_encoder.12.2.bn3.weight False
image_encoder.12.2.bn3.bias False
image_encoder.12.3.conv1.weight False
image_encoder.12.3.bn1.weight False
image_encoder.12.3.bn1.bias False
image_encoder.12.3.conv2.weight False
image_encoder.12.3.bn2.weight False
image_encoder.12.3.bn2.bias False
image_encoder.12.3.conv3.weight False
image_encoder.12.3.bn3.weight False
image_encoder.12.3.bn3.bias False
image_encoder.12.4.conv1.weight False
image_encoder.12.4.bn1.weight False
image_encoder.12.4.bn1.bias False
image_encoder.12.4.conv2.weight False
image_encoder.12.4.bn2.weight False
image_encoder.12.4.bn2.bias False
image_encoder.12.4.conv3.weight False
image_encoder.12.4.bn3.weight False
image_encoder.12.4.bn3.bias False
image_encoder.12.5.conv1.weight False
image_encoder.12.5.bn1.weight False
image_encoder.12.5.bn1.bias False
image_encoder.12.5.conv2.weight False
image_encoder.12.5.bn2.weight False
image_encoder.12.5.bn2.bias False
image_encoder.12.5.conv3.weight False
image_encoder.12.5.bn3.weight False
image_encoder.12.5.bn3.bias False
image_encoder.13.0.conv1.weight False
image_encoder.13.0.bn1.weight False
image_encoder.13.0.bn1.bias False
image_encoder.13.0.conv2.weight False
image_encoder.13.0.bn2.weight False
image_encoder.13.0.bn2.bias False
image_encoder.13.0.conv3.weight False
image_encoder.13.0.bn3.weight False
image_encoder.13.0.bn3.bias False
image_encoder.13.0.downsample.0.weight False
image_encoder.13.0.downsample.1.weight False
image_encoder.13.0.downsample.1.bias False
image_encoder.13.1.conv1.weight False
image_encoder.13.1.bn1.weight False
image_encoder.13.1.bn1.bias False
image_encoder.13.1.conv2.weight False
image_encoder.13.1.bn2.weight False
image_encoder.13.1.bn2.bias False
image_encoder.13.1.conv3.weight False
image_encoder.13.1.bn3.weight False
image_encoder.13.1.bn3.bias False
image_encoder.13.2.conv1.weight False
image_encoder.13.2.bn1.weight False
image_encoder.13.2.bn1.bias False
image_encoder.13.2.conv2.weight False
image_encoder.13.2.bn2.weight False
image_encoder.13.2.bn2.bias False
image_encoder.13.2.conv3.weight False
image_encoder.13.2.bn3.weight False
image_encoder.13.2.bn3.bias False
attnpool.positional_embedding True
attnpool.k_proj.weight True
attnpool.k_proj.bias True
attnpool.q_proj.weight True
attnpool.q_proj.bias True
attnpool.v_proj.weight True
attnpool.v_proj.bias True
attnpool.c_proj.weight True
attnpool.c_proj.bias True
align_context.memory_proj.0.weight True
align_context.memory_proj.0.bias True
align_context.memory_proj.1.weight True
align_context.memory_proj.1.bias True
align_context.memory_proj.2.weight True
align_context.memory_proj.2.bias True
align_context.text_proj.0.weight True
align_context.text_proj.0.bias True
align_context.text_proj.1.weight True
align_context.text_proj.1.bias True
align_context.decoder.0.self_attn.q_proj.weight True
align_context.decoder.0.self_attn.k_proj.weight True
align_context.decoder.0.self_attn.v_proj.weight True
align_context.decoder.0.self_attn.proj.weight True
align_context.decoder.0.self_attn.proj.bias True
align_context.decoder.0.cross_attn.q_proj.weight True
align_context.decoder.0.cross_attn.k_proj.weight True
align_context.decoder.0.cross_attn.v_proj.weight True
align_context.decoder.0.cross_attn.proj.weight True
align_context.decoder.0.cross_attn.proj.bias True
align_context.decoder.0.norm1.weight True
align_context.decoder.0.norm1.bias True
align_context.decoder.0.norm2.weight True
align_context.decoder.0.norm2.bias True
align_context.decoder.0.norm3.weight True
align_context.decoder.0.norm3.bias True
align_context.decoder.0.mlp.0.weight True
align_context.decoder.0.mlp.0.bias True
align_context.decoder.0.mlp.3.weight True
align_context.decoder.0.mlp.3.bias True
align_context.decoder.1.self_attn.q_proj.weight True
align_context.decoder.1.self_attn.k_proj.weight True
align_context.decoder.1.self_attn.v_proj.weight True
align_context.decoder.1.self_attn.proj.weight True
align_context.decoder.1.self_attn.proj.bias True
align_context.decoder.1.cross_attn.q_proj.weight True
align_context.decoder.1.cross_attn.k_proj.weight True
align_context.decoder.1.cross_attn.v_proj.weight True
align_context.decoder.1.cross_attn.proj.weight True
align_context.decoder.1.cross_attn.proj.bias True
align_context.decoder.1.norm1.weight True
align_context.decoder.1.norm1.bias True
align_context.decoder.1.norm2.weight True
align_context.decoder.1.norm2.bias True
align_context.decoder.1.norm3.weight True
align_context.decoder.1.norm3.bias True
align_context.decoder.1.mlp.0.weight True
align_context.decoder.1.mlp.0.bias True
align_context.decoder.1.mlp.3.weight True
align_context.decoder.1.mlp.3.bias True
align_context.decoder.2.self_attn.q_proj.weight True
align_context.decoder.2.self_attn.k_proj.weight True
align_context.decoder.2.self_attn.v_proj.weight True
align_context.decoder.2.self_attn.proj.weight True
align_context.decoder.2.self_attn.proj.bias True
align_context.decoder.2.cross_attn.q_proj.weight True
align_context.decoder.2.cross_attn.k_proj.weight True
align_context.decoder.2.cross_attn.v_proj.weight True
align_context.decoder.2.cross_attn.proj.weight True
align_context.decoder.2.cross_attn.proj.bias True
align_context.decoder.2.norm1.weight True
align_context.decoder.2.norm1.bias True
align_context.decoder.2.norm2.weight True
align_context.decoder.2.norm2.bias True
align_context.decoder.2.norm3.weight True
align_context.decoder.2.norm3.bias True
align_context.decoder.2.mlp.0.weight True
align_context.decoder.2.mlp.0.bias True
align_context.decoder.2.mlp.3.weight True
align_context.decoder.2.mlp.3.bias True
align_context.decoder.3.self_attn.q_proj.weight True
align_context.decoder.3.self_attn.k_proj.weight True
align_context.decoder.3.self_attn.v_proj.weight True
align_context.decoder.3.self_attn.proj.weight True
align_context.decoder.3.self_attn.proj.bias True
align_context.decoder.3.cross_attn.q_proj.weight True
align_context.decoder.3.cross_attn.k_proj.weight True
align_context.decoder.3.cross_attn.v_proj.weight True
align_context.decoder.3.cross_attn.proj.weight True
align_context.decoder.3.cross_attn.proj.bias True
align_context.decoder.3.norm1.weight True
align_context.decoder.3.norm1.bias True
align_context.decoder.3.norm2.weight True
align_context.decoder.3.norm2.bias True
align_context.decoder.3.norm3.weight True
align_context.decoder.3.norm3.bias True
align_context.decoder.3.mlp.0.weight True
align_context.decoder.3.mlp.0.bias True
align_context.decoder.3.mlp.3.weight True
align_context.decoder.3.mlp.3.bias True
align_context.decoder.4.self_attn.q_proj.weight True
align_context.decoder.4.self_attn.k_proj.weight True
align_context.decoder.4.self_attn.v_proj.weight True
align_context.decoder.4.self_attn.proj.weight True
align_context.decoder.4.self_attn.proj.bias True
align_context.decoder.4.cross_attn.q_proj.weight True
align_context.decoder.4.cross_attn.k_proj.weight True
align_context.decoder.4.cross_attn.v_proj.weight True
align_context.decoder.4.cross_attn.proj.weight True
align_context.decoder.4.cross_attn.proj.bias True
align_context.decoder.4.norm1.weight True
align_context.decoder.4.norm1.bias True
align_context.decoder.4.norm2.weight True
align_context.decoder.4.norm2.bias True
align_context.decoder.4.norm3.weight True
align_context.decoder.4.norm3.bias True
align_context.decoder.4.mlp.0.weight True
align_context.decoder.4.mlp.0.bias True
align_context.decoder.4.mlp.3.weight True
align_context.decoder.4.mlp.3.bias True
align_context.decoder.5.self_attn.q_proj.weight True
align_context.decoder.5.self_attn.k_proj.weight True
align_context.decoder.5.self_attn.v_proj.weight True
align_context.decoder.5.self_attn.proj.weight True
align_context.decoder.5.self_attn.proj.bias True
align_context.decoder.5.cross_attn.q_proj.weight True
align_context.decoder.5.cross_attn.k_proj.weight True
align_context.decoder.5.cross_attn.v_proj.weight True
align_context.decoder.5.cross_attn.proj.weight True
align_context.decoder.5.cross_attn.proj.bias True
align_context.decoder.5.norm1.weight True
align_context.decoder.5.norm1.bias True
align_context.decoder.5.norm2.weight True
align_context.decoder.5.norm2.bias True
align_context.decoder.5.norm3.weight True
align_context.decoder.5.norm3.bias True
align_context.decoder.5.mlp.0.weight True
align_context.decoder.5.mlp.0.bias True
align_context.decoder.5.mlp.3.weight True
align_context.decoder.5.mlp.3.bias True
align_context.out_proj.0.weight True
align_context.out_proj.0.bias True
align_context.out_proj.1.weight True
align_context.out_proj.1.bias True
decoder.conv_layers.0.weight True
decoder.conv_layers.0.bias True
decoder.conv_layers.2.weight True
decoder.conv_layers.2.bias True
decoder.conv_layers.3.weight True
decoder.conv_layers.3.bias True
decoder.conv_layers.5.weight True
decoder.conv_layers.5.bias True
Total epochs to be executed :  300
EPOCH 1:
Loss :  2.137052536010742 4.584985256195068 6.7220377922058105
Loss :  2.197599172592163 5.145777702331543 7.343377113342285
Loss :  2.240396499633789 4.6664509773254395 6.9068474769592285
Loss :  2.1388099193573 4.740206718444824 6.879016876220703
Loss :  2.171475887298584 4.52466344833374 6.696139335632324
Loss :  2.0820558071136475 4.47020959854126 6.552265167236328
Loss :  2.1597273349761963 4.445789813995361 6.605517387390137
Loss :  2.1173503398895264 4.673521518707275 6.790871620178223
Loss :  2.2079150676727295 4.636080265045166 6.843995094299316
Loss :  2.1491775512695312 4.580991744995117 6.730169296264648
Loss :  2.1217782497406006 4.40770149230957 6.52947998046875
Loss :  2.1224632263183594 4.761767387390137 6.884230613708496
Loss :  2.0472893714904785 4.693143367767334 6.7404327392578125
Loss :  2.1138367652893066 4.612214088439941 6.726050853729248
Loss :  2.144620895385742 4.537384510040283 6.682005405426025
Loss :  2.1178596019744873 4.366085529327393 6.483944892883301
Loss :  2.1085283756256104 4.499209403991699 6.6077375411987305
Loss :  2.0635247230529785 4.31865119934082 6.382175922393799
Loss :  2.1211493015289307 4.184304237365723 6.305453300476074
Loss :  2.091186046600342 4.553738117218018 6.644924163818359
  batch 20 loss: 2.091186046600342, 4.553738117218018, 6.644924163818359
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  2.1171231269836426 4.595879554748535 6.713002681732178
Loss :  2.1615424156188965 4.624207019805908 6.785749435424805
Loss :  2.1077771186828613 4.571500301361084 6.679277420043945
Loss :  2.111856460571289 4.576513767242432 6.688370227813721
Loss :  2.1888110637664795 4.81138277053833 7.0001935958862305
Loss :  2.1456713676452637 4.504592418670654 6.650263786315918
Loss :  2.1174798011779785 4.366950511932373 6.484430313110352
Loss :  2.0846638679504395 4.41340446472168 6.498068332672119
Loss :  2.080886125564575 4.573797225952148 6.6546831130981445
Loss :  2.0990991592407227 4.420248985290527 6.51934814453125
Loss :  2.0943758487701416 4.339987277984619 6.43436336517334
Loss :  2.0325045585632324 4.5238871574401855 6.556391716003418
Loss :  2.0564420223236084 4.782934665679932 6.839376449584961
Loss :  2.070474863052368 4.2957844734191895 6.366259574890137
Loss :  2.0882058143615723 4.630115985870361 6.718321800231934
Loss :  2.021350860595703 4.2493367195129395 6.270687580108643
Loss :  2.065563201904297 4.333496570587158 6.399059772491455
Loss :  2.000513792037964 4.748282432556152 6.748796463012695
Loss :  2.022930383682251 4.472534656524658 6.495465278625488
Loss :  2.041560173034668 4.475555419921875 6.517115592956543
  batch 40 loss: 2.041560173034668, 4.475555419921875, 6.517115592956543
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  2.052757501602173 4.631255149841309 6.684012413024902
Loss :  2.0623912811279297 4.331308364868164 6.393699645996094
Loss :  2.0750675201416016 4.572573661804199 6.647641181945801
Loss :  2.126955509185791 4.565251350402832 6.692206859588623
Loss :  2.0560173988342285 4.453798770904541 6.5098161697387695
Loss :  2.057692527770996 4.421262741088867 6.478955268859863
Loss :  2.0776262283325195 4.479191303253174 6.556817531585693
Loss :  2.070683002471924 4.5509233474731445 6.621606349945068
Loss :  2.095165491104126 4.438684463500977 6.533849716186523
Loss :  2.120290517807007 4.448270320892334 6.568560600280762
Loss :  2.093505859375 4.314391613006592 6.407897472381592
Loss :  2.0838143825531006 4.708322048187256 6.792136192321777
Loss :  2.1041982173919678 4.369776248931885 6.473974227905273
Loss :  2.0962677001953125 4.336714267730713 6.432981967926025
Loss :  2.1122965812683105 4.275643348693848 6.387939929962158
Loss :  2.0862255096435547 4.449538707733154 6.535764217376709
Loss :  2.0934348106384277 4.2252516746521 6.318686485290527
Loss :  2.0508408546447754 4.406834125518799 6.457674980163574
Loss :  2.0737948417663574 4.260664939880371 6.3344597816467285
Loss :  2.0198352336883545 4.122109413146973 6.141944885253906
  batch 60 loss: 2.0198352336883545, 4.122109413146973, 6.141944885253906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  2.0186831951141357 4.314546585083008 6.333230018615723
Loss :  1.9960216283798218 4.083528518676758 6.079550266265869
Loss :  2.1129350662231445 4.28359842300415 6.396533489227295
Loss :  1.997374176979065 4.5920000076293945 6.58937406539917
Loss :  2.0336086750030518 3.8428449630737305 5.876453399658203
Loss :  4.525877952575684 4.563784122467041 9.089662551879883
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  4.547282695770264 4.5097336769104 9.057016372680664
Loss :  4.6549177169799805 4.3671064376831055 9.022024154663086
Loss :  3.780118465423584 4.051577568054199 7.831696033477783
Total LOSS train 6.574179451282208 valid 8.750099778175354
CE LOSS train 2.0943094216860256 valid 0.945029616355896
Contrastive LOSS train 4.479870047936073 valid 1.0128943920135498
Saved best model. Old loss 1000000.0 and new best loss 8.750099778175354
EPOCH 2:
Loss :  2.0674312114715576 4.45729398727417 6.524724960327148
Loss :  2.0788300037384033 4.368454933166504 6.447284698486328
Loss :  2.076552629470825 4.373481273651123 6.450034141540527
Loss :  2.0965042114257812 4.175598621368408 6.2721028327941895
Loss :  2.048633575439453 4.2574076652526855 6.306041240692139
Loss :  2.097738265991211 4.287755012512207 6.385493278503418
Loss :  2.0968222618103027 4.182489395141602 6.279311656951904
Loss :  2.109551429748535 4.473191738128662 6.582743167877197
Loss :  2.1242830753326416 4.080533981323242 6.204816818237305
Loss :  2.0156047344207764 4.55682373046875 6.5724287033081055
Loss :  2.071223497390747 4.188753604888916 6.259977340698242
Loss :  2.140131950378418 4.577437400817871 6.717569351196289
Loss :  2.010749101638794 4.23489236831665 6.245641708374023
Loss :  2.0911192893981934 4.334129333496094 6.425248622894287
Loss :  2.1047284603118896 4.767737865447998 6.872466087341309
Loss :  2.0576155185699463 4.260141849517822 6.317757606506348
Loss :  2.0619397163391113 4.325375556945801 6.387315273284912
Loss :  2.0142054557800293 4.177682876586914 6.191888332366943
Loss :  2.0591437816619873 4.122985363006592 6.18212890625
Loss :  1.9949294328689575 4.090907573699951 6.085836887359619
  batch 20 loss: 1.9949294328689575, 4.090907573699951, 6.085836887359619
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  2.0360488891601562 4.180956840515137 6.217005729675293
Loss :  2.10323429107666 4.045353412628174 6.148587703704834
Loss :  2.0600531101226807 4.353960037231445 6.414012908935547
Loss :  2.1254312992095947 4.192138195037842 6.317569732666016
Loss :  2.1381731033325195 4.226057529449463 6.364230632781982
Loss :  2.099437713623047 3.879061698913574 5.978499412536621
Loss :  2.0960593223571777 4.130985736846924 6.227045059204102
Loss :  2.072941541671753 4.015786647796631 6.088727951049805
Loss :  2.078145980834961 3.8893744945526123 5.967520713806152
Loss :  2.0633554458618164 4.231423377990723 6.294778823852539
Loss :  2.1068341732025146 4.171195983886719 6.2780303955078125
Loss :  1.9884260892868042 4.013396739959717 6.0018229484558105
Loss :  2.048372507095337 4.031346797943115 6.079719543457031
Loss :  2.0539534091949463 3.8016138076782227 5.85556697845459
Loss :  2.077615737915039 4.1058197021484375 6.183435440063477
Loss :  2.0201051235198975 4.053330421447754 6.0734357833862305
Loss :  2.043778657913208 3.824024200439453 5.867802619934082
Loss :  1.9833760261535645 4.227607727050781 6.210983753204346
Loss :  2.0191991329193115 3.8698153495788574 5.88901424407959
Loss :  2.032564640045166 3.947070598602295 5.979635238647461
  batch 40 loss: 2.032564640045166, 3.947070598602295, 5.979635238647461
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  2.0345475673675537 4.263072490692139 6.297619819641113
Loss :  2.0188004970550537 4.053650379180908 6.072450637817383
Loss :  2.0069706439971924 3.878727912902832 5.885698318481445
Loss :  2.0559592247009277 4.323423862457275 6.379383087158203
Loss :  2.047447919845581 4.147613525390625 6.195061683654785
Loss :  1.9779072999954224 4.374208927154541 6.352116107940674
Loss :  2.0327157974243164 3.934030055999756 5.966745853424072
Loss :  2.050783157348633 4.290345191955566 6.341128349304199
Loss :  2.026963949203491 4.139098644256592 6.166062355041504
Loss :  2.0661568641662598 4.253279209136963 6.319436073303223
Loss :  2.0495593547821045 4.025937080383301 6.075496673583984
Loss :  2.0976786613464355 3.9513278007507324 6.049006462097168
Loss :  2.0156965255737305 4.086323261260986 6.102019786834717
Loss :  2.0581939220428467 3.9644172191619873 6.022611141204834
Loss :  2.062607765197754 4.033473491668701 6.096081256866455
Loss :  2.028705596923828 4.064968109130859 6.0936737060546875
Loss :  2.043663740158081 4.439345359802246 6.483009338378906
Loss :  2.045257329940796 4.1779046058654785 6.223161697387695
Loss :  2.08413028717041 4.085293769836426 6.169424057006836
Loss :  2.0652661323547363 4.224170684814453 6.2894368171691895
  batch 60 loss: 2.0652661323547363, 4.224170684814453, 6.2894368171691895
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  2.0139060020446777 4.073764324188232 6.08767032623291
Loss :  2.0213494300842285 4.1647844314575195 6.186133861541748
Loss :  2.0646700859069824 4.214097023010254 6.278767108917236
Loss :  1.9711235761642456 4.26532506942749 6.236448764801025
Loss :  2.013641119003296 3.789686441421509 5.803327560424805
Loss :  4.452365875244141 4.489972114562988 8.942337989807129
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  4.525207042694092 4.48478889465332 9.00999641418457
Loss :  4.44270658493042 4.331625461578369 8.774332046508789
Loss :  3.8529345989227295 4.351501941680908 8.204436302185059
Total LOSS train 6.220310893425575 valid 8.732775688171387
CE LOSS train 2.0556699422689584 valid 0.9632336497306824
Contrastive LOSS train 4.164640958492573 valid 1.087875485420227
Saved best model. Old loss 8.750099778175354 and new best loss 8.732775688171387
EPOCH 3:
Loss :  2.090851068496704 3.9980409145355225 6.088891983032227
Loss :  2.028437852859497 4.18166971206665 6.210107803344727
Loss :  1.9934470653533936 4.1801533699035645 6.173600196838379
Loss :  2.073132276535034 3.8241004943847656 5.897233009338379
Loss :  2.0346295833587646 4.235987663269043 6.270617485046387
Loss :  1.9997532367706299 4.041093349456787 6.040846824645996
Loss :  2.029157876968384 4.160140514373779 6.189298629760742
Loss :  2.0264573097229004 4.360752582550049 6.387209892272949
Loss :  2.0734119415283203 4.197144508361816 6.270556449890137
Loss :  1.9902355670928955 4.408274173736572 6.398509979248047
Loss :  2.0286872386932373 4.135383605957031 6.164071083068848
Loss :  2.1408743858337402 4.344030857086182 6.484905242919922
Loss :  1.9820220470428467 4.050638198852539 6.032660484313965
Loss :  2.0787160396575928 4.292039394378662 6.370755195617676
Loss :  2.067607879638672 4.172865390777588 6.24047327041626
Loss :  2.009935140609741 3.8284220695495605 5.838356971740723
Loss :  2.057501792907715 4.3428449630737305 6.400346755981445
Loss :  2.0209853649139404 4.109057426452637 6.130043029785156
Loss :  2.068387985229492 3.908712863922119 5.977100849151611
Loss :  1.9405360221862793 4.030054092407227 5.970590114593506
  batch 20 loss: 1.9405360221862793, 4.030054092407227, 5.970590114593506
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9466134309768677 4.159205436706543 6.105818748474121
Loss :  2.0648655891418457 4.104968547821045 6.169834136962891
Loss :  1.9880499839782715 4.109976768493652 6.098026752471924
Loss :  2.0245890617370605 4.138312816619873 6.162901878356934
Loss :  2.0956153869628906 4.3278021812438965 6.423417568206787
Loss :  2.0043423175811768 4.388308525085449 6.392650604248047
Loss :  2.04122257232666 4.126640796661377 6.167863368988037
Loss :  2.002553701400757 3.9888765811920166 5.991430282592773
Loss :  2.024207353591919 4.153502464294434 6.177709579467773
Loss :  2.0442659854888916 4.041934490203857 6.086200714111328
Loss :  2.086061716079712 4.187745571136475 6.273807525634766
Loss :  2.0167694091796875 3.8573975563049316 5.874166965484619
Loss :  2.045123815536499 3.9626095294952393 6.007733345031738
Loss :  2.032553195953369 4.0463128089904785 6.078866004943848
Loss :  2.060873031616211 4.127786159515381 6.188659191131592
Loss :  2.0368549823760986 3.9678328037261963 6.004687786102295
Loss :  2.0077037811279297 4.283154487609863 6.290858268737793
Loss :  1.9361510276794434 3.96994948387146 5.906100273132324
Loss :  2.0216033458709717 4.116812229156494 6.138415336608887
Loss :  1.9947445392608643 4.074779987335205 6.069524765014648
  batch 40 loss: 1.9947445392608643, 4.074779987335205, 6.069524765014648
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  2.0622382164001465 4.136003017425537 6.198241233825684
Loss :  2.0211029052734375 4.075644016265869 6.096746921539307
Loss :  2.0226149559020996 3.9879136085510254 6.010528564453125
Loss :  2.053734064102173 4.121199131011963 6.174933433532715
Loss :  2.0181939601898193 4.051293849945068 6.069487571716309
Loss :  1.9920135736465454 4.180941104888916 6.172954559326172
Loss :  2.009894847869873 4.076972007751465 6.086866855621338
Loss :  2.0379045009613037 3.996765375137329 6.034669876098633
Loss :  2.0386335849761963 3.8371517658233643 5.8757853507995605
Loss :  2.067767381668091 4.001934051513672 6.069701194763184
Loss :  2.0320258140563965 4.021684646606445 6.053710460662842
Loss :  2.090409278869629 3.971376895904541 6.06178617477417
Loss :  2.084656000137329 4.122774124145508 6.207429885864258
Loss :  2.116919994354248 3.8470232486724854 5.9639434814453125
Loss :  2.0164763927459717 4.0916666984558105 6.108142852783203
Loss :  1.9914027452468872 4.299583435058594 6.290986061096191
Loss :  2.0133862495422363 4.012673854827881 6.026060104370117
Loss :  2.048546552658081 3.7869160175323486 5.83546257019043
Loss :  2.0552330017089844 3.9997870922088623 6.055020332336426
Loss :  2.0358448028564453 4.145702838897705 6.18154764175415
  batch 60 loss: 2.0358448028564453, 4.145702838897705, 6.18154764175415
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  2.074293375015259 4.110818862915039 6.185111999511719
Loss :  1.9818271398544312 4.132353782653809 6.114181041717529
Loss :  2.0714046955108643 4.051230430603027 6.1226348876953125
Loss :  1.9630258083343506 4.212019443511963 6.175045013427734
Loss :  2.007314682006836 3.9738283157348633 5.981142997741699
Loss :  3.047650098800659 4.233317852020264 7.280967712402344
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  3.1002323627471924 3.9551196098327637 7.055352210998535
Loss :  3.0486371517181396 3.9606473445892334 7.009284496307373
Loss :  3.0032074451446533 3.732496738433838 6.73570442199707
Total LOSS train 6.127645683288574 valid 7.020327210426331
CE LOSS train 2.032559945033147 valid 0.7508018612861633
Contrastive LOSS train 4.0950857382554275 valid 0.9331241846084595
Saved best model. Old loss 8.732775688171387 and new best loss 7.020327210426331
EPOCH 4:
Loss :  2.063178777694702 4.0698561668396 6.133034706115723
Loss :  2.0054445266723633 4.300734996795654 6.306179523468018
Loss :  1.9916276931762695 4.1732072830200195 6.164834976196289
Loss :  2.076549768447876 4.306809902191162 6.383359909057617
Loss :  2.049698829650879 4.232827186584473 6.282526016235352
Loss :  2.0133159160614014 4.038204669952393 6.051520347595215
Loss :  2.044954299926758 4.043620586395264 6.0885748863220215
Loss :  2.0184178352355957 4.319611549377441 6.338029384613037
Loss :  2.040342092514038 4.072590351104736 6.112932205200195
Loss :  2.0136823654174805 3.9831466674804688 5.996829032897949
Loss :  1.9965906143188477 3.7155699729919434 5.712160587310791
Loss :  2.099947929382324 4.10328483581543 6.203232765197754
Loss :  2.0209238529205322 4.137500286102295 6.158424377441406
Loss :  2.059765338897705 3.9606146812438965 6.020380020141602
Loss :  2.068091630935669 4.377418518066406 6.445509910583496
Loss :  2.0202372074127197 3.9789013862609863 5.999138832092285
Loss :  2.0097033977508545 4.134671688079834 6.144374847412109
Loss :  2.017137289047241 4.037699222564697 6.054836273193359
Loss :  2.0141990184783936 4.038440227508545 6.052639007568359
Loss :  1.9613946676254272 3.975399971008301 5.936794757843018
  batch 20 loss: 1.9613946676254272, 3.975399971008301, 5.936794757843018
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9543465375900269 4.298408508300781 6.252755165100098
Loss :  2.0141305923461914 4.023029327392578 6.0371599197387695
Loss :  1.9875469207763672 4.328033924102783 6.31558084487915
Loss :  2.0207295417785645 3.945756196975708 5.966485977172852
Loss :  2.0561554431915283 4.0857672691345215 6.141922950744629
Loss :  2.0081024169921875 3.990659475326538 5.998762130737305
Loss :  2.025508165359497 3.8306875228881836 5.856195449829102
Loss :  1.9777730703353882 3.9498848915100098 5.9276580810546875
Loss :  2.0068578720092773 4.054017543792725 6.060875415802002
Loss :  2.0332181453704834 4.051934719085693 6.085152626037598
Loss :  2.0548274517059326 4.223675727844238 6.27850341796875
Loss :  1.9771062135696411 4.0620622634887695 6.039168357849121
Loss :  2.011636972427368 3.842087507247925 5.853724479675293
Loss :  1.9967771768569946 3.986912965774536 5.98369026184082
Loss :  2.0482394695281982 4.106687068939209 6.154926300048828
Loss :  1.96908700466156 4.056370258331299 6.025457382202148
Loss :  1.9257051944732666 3.8927721977233887 5.818477630615234
Loss :  1.8843461275100708 3.7016243934631348 5.585970401763916
Loss :  1.9418492317199707 4.04511833190918 5.98696756362915
Loss :  1.945531964302063 3.983367681503296 5.928899765014648
  batch 40 loss: 1.945531964302063, 3.983367681503296, 5.928899765014648
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9916843175888062 4.047082901000977 6.038767337799072
Loss :  2.0040056705474854 3.6753013134002686 5.679306983947754
Loss :  1.989074468612671 3.71999454498291 5.70906925201416
Loss :  2.0501747131347656 4.145945072174072 6.196119785308838
Loss :  2.0232441425323486 3.9246573448181152 5.947901725769043
Loss :  1.9453014135360718 4.148045063018799 6.09334659576416
Loss :  1.9947654008865356 4.086049556732178 6.080814838409424
Loss :  2.0239500999450684 4.043506622314453 6.0674567222595215
Loss :  1.9474296569824219 4.302955150604248 6.25038480758667
Loss :  2.000743865966797 3.9939417839050293 5.994685649871826
Loss :  1.982749342918396 3.905487060546875 5.8882365226745605
Loss :  2.0178303718566895 4.266449928283691 6.284280300140381
Loss :  1.9889490604400635 4.103198051452637 6.092146873474121
Loss :  2.0015854835510254 3.82177472114563 5.823360443115234
Loss :  2.0162668228149414 3.801274299621582 5.817541122436523
Loss :  1.983417272567749 3.7679696083068848 5.751386642456055
Loss :  1.9836894273757935 4.209266662597656 6.19295597076416
Loss :  1.983271598815918 3.8604087829589844 5.843680381774902
Loss :  2.0615286827087402 4.086223125457764 6.147751808166504
Loss :  2.028135299682617 3.7829713821411133 5.8111066818237305
  batch 60 loss: 2.028135299682617, 3.7829713821411133, 5.8111066818237305
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9789172410964966 4.082216739654541 6.061133861541748
Loss :  1.9528629779815674 4.329318046569824 6.2821807861328125
Loss :  2.042841911315918 3.9082014560699463 5.951043128967285
Loss :  1.9307951927185059 4.0787224769592285 6.009517669677734
Loss :  1.9779038429260254 3.627976417541504 5.605880260467529
Loss :  2.8498260974884033 4.365450382232666 7.215276718139648
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  2.7781617641448975 4.052793502807617 6.830955505371094
Loss :  2.7586636543273926 4.0537285804748535 6.812392234802246
Loss :  2.6837172508239746 4.147963047027588 6.8316802978515625
Total LOSS train 6.038518502162053 valid 6.922576189041138
CE LOSS train 2.0050122591165396 valid 0.6709293127059937
Contrastive LOSS train 4.0335062467134914 valid 1.036990761756897
Saved best model. Old loss 7.020327210426331 and new best loss 6.922576189041138
EPOCH 5:
Loss :  2.007681131362915 4.335175514221191 6.342856407165527
Loss :  2.0081429481506348 4.245064735412598 6.253207683563232
Loss :  1.9516981840133667 4.069997310638428 6.021695613861084
Loss :  1.9976065158843994 4.007595062255859 6.00520133972168
Loss :  1.9896633625030518 4.044310569763184 6.033973693847656
Loss :  1.9723093509674072 4.046743869781494 6.0190534591674805
Loss :  2.022183418273926 4.205482482910156 6.227665901184082
Loss :  1.9620641469955444 3.7721099853515625 5.7341742515563965
Loss :  2.0172345638275146 3.907252550125122 5.924487113952637
Loss :  1.9607212543487549 4.012042999267578 5.972764015197754
Loss :  1.9541481733322144 3.8311638832092285 5.785312175750732
Loss :  2.056501865386963 4.137138366699219 6.193640232086182
Loss :  1.9666757583618164 3.8586068153381348 5.825282573699951
Loss :  1.9927552938461304 4.044789791107178 6.037545204162598
Loss :  1.9980556964874268 3.650566577911377 5.648622512817383
Loss :  1.9687118530273438 4.003660678863525 5.972372531890869
Loss :  1.9668912887573242 4.10629415512085 6.073185443878174
Loss :  1.9621858596801758 4.124556064605713 6.086741924285889
Loss :  1.9617607593536377 3.6726672649383545 5.634428024291992
Loss :  1.9193673133850098 4.005570411682129 5.924937725067139
  batch 20 loss: 1.9193673133850098, 4.005570411682129, 5.924937725067139
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9211161136627197 3.9215738773345947 5.8426899909973145
Loss :  1.9769477844238281 4.022951126098633 5.999898910522461
Loss :  1.9040077924728394 4.03151273727417 5.935520648956299
Loss :  1.996894359588623 3.9732489585876465 5.9701433181762695
Loss :  2.040450096130371 4.135359764099121 6.175809860229492
Loss :  1.9616323709487915 4.112648010253906 6.074280261993408
Loss :  1.9993481636047363 3.938753604888916 5.938101768493652
Loss :  1.938294768333435 4.0568060874938965 5.995100975036621
Loss :  1.9977244138717651 3.7193171977996826 5.717041492462158
Loss :  2.002520799636841 3.702873706817627 5.705394744873047
Loss :  2.081444501876831 4.176224708557129 6.257669448852539
Loss :  1.9566866159439087 3.97532057762146 5.932007312774658
Loss :  1.9684678316116333 4.187650203704834 6.156117916107178
Loss :  1.9625900983810425 4.110040664672852 6.072630882263184
Loss :  2.0257534980773926 3.9384968280792236 5.964250564575195
Loss :  1.9910787343978882 3.8916332721710205 5.882711887359619
Loss :  1.9658677577972412 3.587783098220825 5.553650856018066
Loss :  1.938460350036621 4.01548433303833 5.953944683074951
Loss :  1.9790639877319336 3.855839729309082 5.834903717041016
Loss :  1.9457026720046997 4.084270000457764 6.029972553253174
  batch 40 loss: 1.9457026720046997, 4.084270000457764, 6.029972553253174
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9880187511444092 4.149891376495361 6.137909889221191
Loss :  1.9783767461776733 3.61421799659729 5.592594623565674
Loss :  1.9675748348236084 3.910374402999878 5.877949237823486
Loss :  2.055842161178589 4.078531265258789 6.134373664855957
Loss :  1.9956098794937134 3.8785598278045654 5.874169826507568
Loss :  1.9317160844802856 3.9155166149139404 5.847232818603516
Loss :  1.9448400735855103 3.970427989959717 5.9152679443359375
Loss :  1.9935734272003174 4.021118640899658 6.014692306518555
Loss :  1.9236135482788086 4.460338115692139 6.383951663970947
Loss :  1.9914084672927856 3.989715814590454 5.981124401092529
Loss :  1.9437971115112305 4.170175075531006 6.113972187042236
Loss :  2.0088608264923096 4.033753395080566 6.042613983154297
Loss :  1.9741477966308594 4.001631736755371 5.9757795333862305
Loss :  1.9984744787216187 3.983264684677124 5.981739044189453
Loss :  2.0077714920043945 3.821507453918457 5.829278945922852
Loss :  1.9414211511611938 3.966778039932251 5.908199310302734
Loss :  1.96604323387146 4.202649116516113 6.168692588806152
Loss :  1.9856592416763306 3.705613613128662 5.691272735595703
Loss :  2.0294010639190674 4.0044660568237305 6.033866882324219
Loss :  2.002065420150757 3.9106991291046143 5.912764549255371
  batch 60 loss: 2.002065420150757, 3.9106991291046143, 5.912764549255371
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9598444700241089 4.030655384063721 5.990499973297119
Loss :  1.946625828742981 4.022006511688232 5.968632221221924
Loss :  2.001237630844116 3.8862619400024414 5.887499809265137
Loss :  1.9051672220230103 3.8477680683135986 5.752935409545898
Loss :  1.9222345352172852 3.5200161933898926 5.442250728607178
Loss :  2.5492606163024902 4.103415012359619 6.652675628662109
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  2.5644772052764893 4.008131504058838 6.572608947753906
Loss :  2.6717026233673096 3.758251190185547 6.429953575134277
Loss :  2.5551042556762695 3.7963151931762695 6.351419448852539
Total LOSS train 5.956434675363394 valid 6.501664400100708
CE LOSS train 1.9777497988480788 valid 0.6387760639190674
Contrastive LOSS train 3.9786848618434028 valid 0.9490787982940674
Saved best model. Old loss 6.922576189041138 and new best loss 6.501664400100708
EPOCH 6:
Loss :  1.964115023612976 3.727728843688965 5.6918439865112305
Loss :  1.9745991230010986 4.1218743324279785 6.096473693847656
Loss :  1.9198750257492065 3.884273052215576 5.804148197174072
Loss :  1.9886596202850342 4.102766513824463 6.091425895690918
Loss :  1.9605460166931152 4.4377007484436035 6.398246765136719
Loss :  1.931037425994873 3.9929494857788086 5.923986911773682
Loss :  1.9624851942062378 3.783588409423828 5.7460737228393555
Loss :  1.9452109336853027 4.033395290374756 5.978606224060059
Loss :  1.9981986284255981 3.9749414920806885 5.973140239715576
Loss :  1.9426028728485107 3.761758327484131 5.7043609619140625
Loss :  1.9649951457977295 3.714963674545288 5.679958820343018
Loss :  2.050128698348999 4.1928887367248535 6.243017196655273
Loss :  1.9895888566970825 4.082995891571045 6.072584629058838
Loss :  1.9928507804870605 3.9155960083007812 5.908446788787842
Loss :  2.025416374206543 3.9428658485412598 5.968282222747803
Loss :  1.9474968910217285 3.8488667011260986 5.796363830566406
Loss :  1.9170818328857422 3.8781347274780273 5.7952165603637695
Loss :  1.9496493339538574 4.096253395080566 6.045902729034424
Loss :  1.947940468788147 4.107875823974609 6.055816173553467
Loss :  1.8901619911193848 4.093648910522461 5.983810901641846
  batch 20 loss: 1.8901619911193848, 4.093648910522461, 5.983810901641846
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8960812091827393 4.29875373840332 6.1948347091674805
Loss :  1.9645456075668335 4.152934551239014 6.117480278015137
Loss :  1.9162702560424805 3.971487522125244 5.887757778167725
Loss :  1.9566547870635986 4.267158031463623 6.223813056945801
Loss :  2.035531520843506 4.046947002410889 6.0824785232543945
Loss :  1.9503073692321777 4.2087202072143555 6.159027576446533
Loss :  2.0072388648986816 4.034846305847168 6.04208517074585
Loss :  1.9459770917892456 3.9680275917053223 5.914004802703857
Loss :  1.9704813957214355 3.8722081184387207 5.842689514160156
Loss :  1.9926012754440308 3.9832873344421387 5.975888729095459
Loss :  2.0490198135375977 4.064354419708252 6.11337423324585
Loss :  1.9767792224884033 4.074696063995361 6.051475524902344
Loss :  1.9774751663208008 4.094270706176758 6.071745872497559
Loss :  1.9741448163986206 3.981044054031372 5.955188751220703
Loss :  2.012444019317627 3.8159401416778564 5.8283843994140625
Loss :  1.975342035293579 3.7851741313934326 5.760516166687012
Loss :  1.9403761625289917 4.045608997344971 5.985985279083252
Loss :  1.8873364925384521 4.2811713218688965 6.1685075759887695
Loss :  1.9254847764968872 4.021777629852295 5.947262287139893
Loss :  1.9160865545272827 4.023863315582275 5.939949989318848
  batch 40 loss: 1.9160865545272827, 4.023863315582275, 5.939949989318848
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9471665620803833 3.9180080890655518 5.865174770355225
Loss :  1.9456673860549927 3.8803317546844482 5.8259992599487305
Loss :  1.9751372337341309 3.8534433841705322 5.828580856323242
Loss :  2.0206594467163086 4.070904731750488 6.091564178466797
Loss :  1.990830898284912 4.221420764923096 6.212251663208008
Loss :  1.9603101015090942 3.9560201168060303 5.916330337524414
Loss :  2.0034971237182617 3.9437355995178223 5.947232723236084
Loss :  2.0014662742614746 3.9063665866851807 5.907833099365234
Loss :  1.9797296524047852 4.092170715332031 6.071900367736816
Loss :  1.9980772733688354 3.882622241973877 5.880699634552002
Loss :  1.966896414756775 3.8821611404418945 5.849057674407959
Loss :  2.0005509853363037 4.0192108154296875 6.01976203918457
Loss :  1.9601590633392334 4.024241924285889 5.984400749206543
Loss :  2.007906436920166 3.7495346069335938 5.75744104385376
Loss :  1.9907199144363403 4.02465295791626 6.0153727531433105
Loss :  1.928174376487732 3.636357307434082 5.5645318031311035
Loss :  1.94692862033844 4.21776008605957 6.164688587188721
Loss :  2.0055668354034424 3.8422937393188477 5.847860336303711
Loss :  2.022737979888916 4.0068039894104 6.029541969299316
Loss :  2.0122883319854736 3.95509672164917 5.967385292053223
  batch 60 loss: 2.0122883319854736, 3.95509672164917, 5.967385292053223
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9820795059204102 3.807302951812744 5.789382457733154
Loss :  1.949255108833313 3.852691173553467 5.80194616317749
Loss :  2.003815174102783 3.621467113494873 5.625282287597656
Loss :  1.9155669212341309 3.7479934692382812 5.663560390472412
Loss :  1.954742193222046 3.5960235595703125 5.5507659912109375
Loss :  2.3446109294891357 4.065046787261963 6.4096574783325195
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  2.3486289978027344 3.99747371673584 6.346102714538574
Loss :  2.3323380947113037 3.6450624465942383 5.977400779724121
Loss :  2.2312347888946533 3.655917167663574 5.887151718139648
Total LOSS train 5.944595432281494 valid 6.155078172683716
CE LOSS train 1.9692730536827674 valid 0.5578086972236633
Contrastive LOSS train 3.9753223529228796 valid 0.9139792919158936
Saved best model. Old loss 6.501664400100708 and new best loss 6.155078172683716
EPOCH 7:
Loss :  2.003437042236328 3.6641385555267334 5.667575836181641
Loss :  1.9816521406173706 4.088973522186279 6.0706257820129395
Loss :  1.9174240827560425 3.742509603500366 5.659933567047119
Loss :  1.995916724205017 3.684532403945923 5.68044900894165
Loss :  1.9658775329589844 3.923527956008911 5.889405250549316
Loss :  1.9359713792800903 4.11767578125 6.053647041320801
Loss :  1.960280179977417 3.9284374713897705 5.8887176513671875
Loss :  1.9350578784942627 4.38489294052124 6.319951057434082
Loss :  1.9858919382095337 3.933696746826172 5.919588565826416
Loss :  1.9415382146835327 3.98498272895813 5.926520824432373
Loss :  1.9552981853485107 3.8285868167877197 5.7838850021362305
Loss :  2.0407814979553223 4.034544944763184 6.075326442718506
Loss :  1.973231554031372 4.3203935623168945 6.2936248779296875
Loss :  1.9797548055648804 4.159837245941162 6.139592170715332
Loss :  1.9625298976898193 3.821471929550171 5.78400182723999
Loss :  1.9684500694274902 3.8807930946350098 5.8492431640625
Loss :  1.9552274942398071 3.901541233062744 5.856768608093262
Loss :  1.9906718730926514 3.978438138961792 5.969110012054443
Loss :  1.9595342874526978 3.6761584281921387 5.635692596435547
Loss :  1.9161680936813354 4.088259220123291 6.004427433013916
  batch 20 loss: 1.9161680936813354, 4.088259220123291, 6.004427433013916
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9254584312438965 3.653515577316284 5.578973770141602
Loss :  1.9913051128387451 4.057145118713379 6.048450469970703
Loss :  1.9372824430465698 3.778643846511841 5.715926170349121
Loss :  1.9680521488189697 3.832637071609497 5.800689220428467
Loss :  2.0273938179016113 4.01742696762085 6.044820785522461
Loss :  1.9634438753128052 4.1648054122924805 6.128249168395996
Loss :  1.9963688850402832 4.035574913024902 6.0319437980651855
Loss :  1.9299389123916626 4.077800273895264 6.007739067077637
Loss :  1.9481106996536255 3.8459577560424805 5.794068336486816
Loss :  1.9503155946731567 3.7989959716796875 5.749311447143555
Loss :  2.0138022899627686 3.886500358581543 5.900302886962891
Loss :  1.9222691059112549 4.0101447105407715 5.9324140548706055
Loss :  1.9725736379623413 3.8056998252868652 5.778273582458496
Loss :  1.974636435508728 3.8811123371124268 5.855748653411865
Loss :  1.9979476928710938 4.106635570526123 6.104583263397217
Loss :  1.970211386680603 3.9576830863952637 5.927894592285156
Loss :  1.9202842712402344 4.039209365844727 5.959493637084961
Loss :  1.8832833766937256 3.914785623550415 5.798069000244141
Loss :  1.9353665113449097 4.296484470367432 6.231851100921631
Loss :  1.8972865343093872 3.797931432723999 5.695218086242676
  batch 40 loss: 1.8972865343093872, 3.797931432723999, 5.695218086242676
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9393786191940308 4.0424580574035645 5.981836795806885
Loss :  1.9323832988739014 3.924938917160034 5.8573222160339355
Loss :  1.9270124435424805 3.860684394836426 5.787696838378906
Loss :  1.986035704612732 3.8591153621673584 5.845150947570801
Loss :  1.9454439878463745 3.6020710468292236 5.547514915466309
Loss :  1.9112237691879272 3.8759655952453613 5.787189483642578
Loss :  1.9564428329467773 3.8081769943237305 5.764619827270508
Loss :  1.9590152502059937 3.8904502391815186 5.849465370178223
Loss :  1.9477238655090332 3.725166082382202 5.672889709472656
Loss :  1.9718990325927734 4.021054267883301 5.992953300476074
Loss :  1.9210375547409058 3.97851824760437 5.899555683135986
Loss :  1.9795708656311035 3.814471960067749 5.794042587280273
Loss :  1.9439491033554077 3.5930540561676025 5.537003040313721
Loss :  1.980389952659607 3.72912859916687 5.7095184326171875
Loss :  1.9547560214996338 3.9089713096618652 5.863727569580078
Loss :  1.9205968379974365 4.2298054695129395 6.150402069091797
Loss :  1.946158766746521 4.235015392303467 6.181174278259277
Loss :  1.971088171005249 3.8822624683380127 5.853350639343262
Loss :  1.9903664588928223 3.836230754852295 5.826597213745117
Loss :  1.9596798419952393 3.9546821117401123 5.914361953735352
  batch 60 loss: 1.9596798419952393, 3.9546821117401123, 5.914361953735352
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9592313766479492 3.997312068939209 5.956543445587158
Loss :  1.9168232679367065 3.995711088180542 5.912534236907959
Loss :  1.9960107803344727 4.0476508140563965 6.043661594390869
Loss :  1.8836054801940918 3.76059627532959 5.644201755523682
Loss :  1.939753532409668 3.6128382682800293 5.552591800689697
Loss :  2.261770725250244 4.048762321472168 6.310533046722412
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  2.3736112117767334 3.9654362201690674 6.339047431945801
Loss :  2.3366851806640625 4.053888320922852 6.390573501586914
Loss :  2.3148889541625977 3.8516781330108643 6.166566848754883
Total LOSS train 5.884277131007268 valid 6.301680207252502
CE LOSS train 1.9567631207979643 valid 0.5787222385406494
Contrastive LOSS train 3.9275140285491945 valid 0.9629195332527161
EPOCH 8:
Loss :  1.9638080596923828 3.68811297416687 5.651921272277832
Loss :  1.9352718591690063 3.896697521209717 5.831969261169434
Loss :  1.8811793327331543 4.395331382751465 6.276510715484619
Loss :  1.965174913406372 3.7978408336639404 5.7630157470703125
Loss :  1.9576376676559448 3.9026594161987305 5.860297203063965
Loss :  1.9140450954437256 3.7420992851257324 5.656144142150879
Loss :  1.953345775604248 3.6867270469665527 5.640072822570801
Loss :  1.9319828748703003 4.017843246459961 5.949826240539551
Loss :  1.9718241691589355 3.80312180519104 5.774946212768555
Loss :  1.9397072792053223 3.6235191822052 5.563226699829102
Loss :  1.9478590488433838 3.6914541721343994 5.639313220977783
Loss :  2.025148630142212 4.1076788902282715 6.1328277587890625
Loss :  1.9491745233535767 3.753899097442627 5.703073501586914
Loss :  1.954305648803711 3.912123441696167 5.866429328918457
Loss :  1.9667409658432007 3.816624402999878 5.783365249633789
Loss :  1.9359277486801147 3.7960245609283447 5.73195219039917
Loss :  1.9321762323379517 3.8002679347991943 5.7324442863464355
Loss :  1.9697507619857788 4.164085388183594 6.133836269378662
Loss :  1.9444530010223389 3.917293071746826 5.861745834350586
Loss :  1.8755501508712769 3.7619447708129883 5.637495040893555
  batch 20 loss: 1.8755501508712769, 3.7619447708129883, 5.637495040893555
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.916275978088379 3.717780828475952 5.63405704498291
Loss :  1.9586668014526367 3.936450719833374 5.89511775970459
Loss :  1.9199069738388062 3.7584710121154785 5.678378105163574
Loss :  1.9371908903121948 3.88031005859375 5.817501068115234
Loss :  1.9853721857070923 3.9275670051574707 5.912939071655273
Loss :  1.9578300714492798 3.5487327575683594 5.50656270980835
Loss :  2.000624656677246 3.906363010406494 5.90698766708374
Loss :  1.9230552911758423 3.785876512527466 5.708931922912598
Loss :  1.9467235803604126 3.621157169342041 5.567880630493164
Loss :  1.9524869918823242 3.7856171131134033 5.738103866577148
Loss :  2.011650562286377 3.8728830814361572 5.884533882141113
Loss :  1.9384973049163818 4.0642852783203125 6.002782821655273
Loss :  1.949355125427246 4.251072406768799 6.200427532196045
Loss :  1.9430183172225952 3.8392868041992188 5.7823052406311035
Loss :  1.981475591659546 4.557422161102295 6.538897514343262
Loss :  1.93366539478302 4.118268966674805 6.051934242248535
Loss :  1.921890377998352 4.151264667510986 6.073154926300049
Loss :  1.9006476402282715 3.878798484802246 5.779446125030518
Loss :  1.9219807386398315 4.272092819213867 6.194073677062988
Loss :  1.910256028175354 4.290545463562012 6.200801372528076
  batch 40 loss: 1.910256028175354, 4.290545463562012, 6.200801372528076
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9355405569076538 3.745224714279175 5.680765151977539
Loss :  1.9260149002075195 4.176865577697754 6.102880477905273
Loss :  1.907517671585083 3.8211095333099365 5.7286272048950195
Loss :  1.9602782726287842 3.935237407684326 5.895515441894531
Loss :  1.9355720281600952 3.8286774158477783 5.764249324798584
Loss :  1.8795827627182007 3.9564781188964844 5.836061000823975
Loss :  1.9226404428482056 3.751866340637207 5.674506664276123
Loss :  1.936142086982727 3.9051332473754883 5.841275215148926
Loss :  1.9127192497253418 3.871537685394287 5.784256935119629
Loss :  1.9583367109298706 3.7727725505828857 5.731109142303467
Loss :  1.9151848554611206 3.7619307041168213 5.677115440368652
Loss :  1.9763026237487793 3.6964313983917236 5.672734260559082
Loss :  1.9045385122299194 3.724313497543335 5.628851890563965
Loss :  1.974266529083252 3.9128544330596924 5.887121200561523
Loss :  1.915698766708374 3.9531924724578857 5.86889123916626
Loss :  1.9277056455612183 3.971928596496582 5.89963436126709
Loss :  1.9390363693237305 3.791980504989624 5.731017112731934
Loss :  1.9645665884017944 3.6007258892059326 5.5652923583984375
Loss :  1.965275764465332 3.845308542251587 5.81058406829834
Loss :  1.9802510738372803 3.635627508163452 5.615878582000732
  batch 60 loss: 1.9802510738372803, 3.635627508163452, 5.615878582000732
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.920983910560608 3.9184608459472656 5.839444637298584
Loss :  1.922399640083313 3.903172492980957 5.8255720138549805
Loss :  1.9607611894607544 3.783141851425171 5.743903160095215
Loss :  1.8990538120269775 3.6090452671051025 5.50809907913208
Loss :  1.9295110702514648 3.9704954624176025 5.900006294250488
Loss :  1.8968061208724976 4.408782005310059 6.305588245391846
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9341074228286743 4.317131519317627 6.251238822937012
Loss :  1.9614579677581787 4.482873916625977 6.444332122802734
Loss :  1.88542640209198 4.119096279144287 6.004522800445557
Total LOSS train 5.822748037484976 valid 6.251420497894287
CE LOSS train 1.9414694657692542 valid 0.471356600522995
Contrastive LOSS train 3.8812785662137546 valid 1.0297740697860718
EPOCH 9:
Loss :  1.9619624614715576 4.199460029602051 6.1614227294921875
Loss :  1.965977430343628 4.40936803817749 6.375345230102539
Loss :  1.9603745937347412 4.293230056762695 6.253604888916016
Loss :  1.961730718612671 4.156336307525635 6.118066787719727
Loss :  1.956958532333374 4.279356002807617 6.23631477355957
Loss :  1.9176961183547974 4.010979652404785 5.928675651550293
Loss :  1.933331847190857 4.15924596786499 6.092577934265137
Loss :  1.8917794227600098 4.327622413635254 6.219401836395264
Loss :  1.9449758529663086 4.455179691314697 6.400155544281006
Loss :  1.8963547945022583 4.295773029327393 6.192127704620361
Loss :  1.914198875427246 3.9878761768341064 5.902074813842773
Loss :  1.981666088104248 4.103212356567383 6.084878444671631
Loss :  1.902000904083252 3.776219129562378 5.678219795227051
Loss :  1.9381471872329712 3.9653117656707764 5.903459072113037
Loss :  1.9300886392593384 3.7926042079925537 5.722692966461182
Loss :  1.9192320108413696 3.757113456726074 5.676345348358154
Loss :  1.9029786586761475 4.054159164428711 5.9571380615234375
Loss :  1.937253713607788 3.88078236579895 5.818036079406738
Loss :  1.9065300226211548 3.989682912826538 5.896213054656982
Loss :  1.8345365524291992 3.7988240718841553 5.633360862731934
  batch 20 loss: 1.8345365524291992, 3.7988240718841553, 5.633360862731934
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.911028265953064 3.7511205673217773 5.662148952484131
Loss :  1.9382567405700684 3.7954447269439697 5.733701705932617
Loss :  1.8839938640594482 4.000642776489258 5.884636878967285
Loss :  1.9441078901290894 4.197879791259766 6.1419878005981445
Loss :  1.9681611061096191 4.203457355499268 6.171618461608887
Loss :  1.9414550065994263 3.8357083797454834 5.777163505554199
Loss :  1.9788038730621338 4.004239082336426 5.9830427169799805
Loss :  1.9038264751434326 3.929893732070923 5.8337202072143555
Loss :  1.9564392566680908 4.046319007873535 6.002758026123047
Loss :  1.9197874069213867 3.9054574966430664 5.825244903564453
Loss :  1.9850268363952637 3.8012197017669678 5.786246299743652
Loss :  1.917648196220398 3.7009425163269043 5.618590831756592
Loss :  1.9348266124725342 4.01865291595459 5.953479766845703
Loss :  1.9259170293807983 3.7672953605651855 5.693212509155273
Loss :  1.9633861780166626 3.7477941513061523 5.711180210113525
Loss :  1.9465793371200562 3.996727705001831 5.943306922912598
Loss :  1.9187177419662476 3.6320841312408447 5.550801753997803
Loss :  1.903579831123352 3.7782695293426514 5.681849479675293
Loss :  1.921453833580017 3.8600316047668457 5.781485557556152
Loss :  1.907921314239502 3.786107301712036 5.694028854370117
  batch 40 loss: 1.907921314239502, 3.786107301712036, 5.694028854370117
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9300168752670288 4.267843723297119 6.1978607177734375
Loss :  1.963985562324524 3.9305436611175537 5.894529342651367
Loss :  1.9528741836547852 3.8593509197235107 5.812225341796875
Loss :  1.9743831157684326 4.2659196853637695 6.240303039550781
Loss :  1.943017840385437 3.8376104831695557 5.780628204345703
Loss :  1.9228817224502563 4.0134124755859375 5.936294078826904
Loss :  1.9092249870300293 4.126211166381836 6.035436153411865
Loss :  1.9495762586593628 3.8638570308685303 5.8134331703186035
Loss :  1.9170269966125488 3.707899570465088 5.624926567077637
Loss :  1.9567068815231323 3.9190523624420166 5.875759124755859
Loss :  1.913895845413208 3.8244855403900146 5.738381385803223
Loss :  1.9582070112228394 3.980318784713745 5.938525676727295
Loss :  1.903691053390503 3.7702887058258057 5.673979759216309
Loss :  1.9442527294158936 4.093339920043945 6.037592887878418
Loss :  1.9238204956054688 3.7108747959136963 5.634695053100586
Loss :  1.9487303495407104 3.674757957458496 5.623488426208496
Loss :  1.9341744184494019 4.1844072341918945 6.118581771850586
Loss :  1.957900047302246 3.986542224884033 5.944442272186279
Loss :  1.9705146551132202 3.993147134780884 5.9636616706848145
Loss :  1.95536208152771 3.935410261154175 5.890772342681885
  batch 60 loss: 1.95536208152771, 3.935410261154175, 5.890772342681885
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.896854043006897 3.802184820175171 5.699038982391357
Loss :  1.9039127826690674 3.668257713317871 5.572170257568359
Loss :  1.9277364015579224 3.4375996589660645 5.365335941314697
Loss :  1.885934829711914 3.971271276473999 5.857206344604492
Loss :  1.9085309505462646 3.5849199295043945 5.493451118469238
Loss :  1.7378826141357422 4.4748406410217285 6.212723255157471
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8864388465881348 4.471721172332764 6.358160018920898
Loss :  1.8863708972930908 3.8013930320739746 5.6877641677856445
Loss :  1.8158345222473145 4.280400276184082 6.0962347984313965
Total LOSS train 5.883739023942214 valid 6.0887205600738525
CE LOSS train 1.9320292821297278 valid 0.4539586305618286
Contrastive LOSS train 3.951709717970628 valid 1.0701000690460205
Saved best model. Old loss 6.155078172683716 and new best loss 6.0887205600738525
EPOCH 10:
Loss :  1.9725685119628906 3.874431610107422 5.8470001220703125
Loss :  1.9420462846755981 3.8274662494659424 5.76951265335083
Loss :  1.8565157651901245 3.867488384246826 5.72400426864624
Loss :  1.9043081998825073 3.796867847442627 5.701176166534424
Loss :  1.9351836442947388 3.803607225418091 5.738790988922119
Loss :  1.8939169645309448 3.749260425567627 5.643177509307861
Loss :  1.9034533500671387 3.6347734928131104 5.538227081298828
Loss :  1.8744491338729858 3.791057586669922 5.665506839752197
Loss :  1.9205189943313599 3.5244972705841064 5.445016384124756
Loss :  1.8729215860366821 3.7401819229125977 5.61310338973999
Loss :  1.8741681575775146 3.6555838584899902 5.529751777648926
Loss :  1.9444910287857056 4.140039920806885 6.084530830383301
Loss :  1.8916853666305542 3.8734002113342285 5.765085697174072
Loss :  1.933279275894165 4.032393932342529 5.965673446655273
Loss :  1.900368571281433 3.8545889854431152 5.754957675933838
Loss :  1.8988547325134277 3.824284315109253 5.723138809204102
Loss :  1.8787842988967896 4.148565292358398 6.027349472045898
Loss :  1.9110220670700073 3.970700263977051 5.881722450256348
Loss :  1.8907493352890015 3.6835782527923584 5.57432746887207
Loss :  1.8717044591903687 4.196317672729492 6.06802225112915
  batch 20 loss: 1.8717044591903687, 4.196317672729492, 6.06802225112915
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9000972509384155 3.903040647506714 5.80313777923584
Loss :  1.9275462627410889 3.7676494121551514 5.69519567489624
Loss :  1.9113503694534302 3.9063100814819336 5.817660331726074
Loss :  1.907039761543274 3.892282009124756 5.79932165145874
Loss :  1.9153380393981934 4.013864517211914 5.929202556610107
Loss :  1.9180865287780762 3.9519202709198 5.870006561279297
Loss :  1.9372550249099731 3.9755756855010986 5.912830829620361
Loss :  1.8667043447494507 3.5506155490875244 5.4173197746276855
Loss :  1.9327818155288696 3.677940845489502 5.610722541809082
Loss :  1.9053130149841309 3.7558090686798096 5.6611223220825195
Loss :  1.965783715248108 4.064425945281982 6.030209541320801
Loss :  1.8876415491104126 3.811649799346924 5.699291229248047
Loss :  1.9197243452072144 3.8274505138397217 5.7471747398376465
Loss :  1.9152592420578003 4.042374134063721 5.9576334953308105
Loss :  1.9383041858673096 4.0054731369018555 5.943777084350586
Loss :  1.9205491542816162 3.790308952331543 5.710858345031738
Loss :  1.8647949695587158 4.265942096710205 6.1307373046875
Loss :  1.8735231161117554 4.042810440063477 5.9163336753845215
Loss :  1.8611875772476196 3.9696648120880127 5.830852508544922
Loss :  1.8577582836151123 3.8294105529785156 5.687169075012207
  batch 40 loss: 1.8577582836151123, 3.8294105529785156, 5.687169075012207
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9003028869628906 3.857839345932007 5.758142471313477
Loss :  1.9239643812179565 3.8789570331573486 5.802921295166016
Loss :  1.925431728363037 3.8876166343688965 5.813048362731934
Loss :  1.923885703086853 4.31570291519165 6.239588737487793
Loss :  1.916910171508789 3.8588407039642334 5.775751113891602
Loss :  1.8749607801437378 4.202787399291992 6.0777482986450195
Loss :  1.8674495220184326 3.6329915523529053 5.500441074371338
Loss :  1.908254861831665 3.7886128425598145 5.696867942810059
Loss :  1.8746663331985474 3.4429595470428467 5.317625999450684
Loss :  1.928633451461792 3.7241969108581543 5.652830123901367
Loss :  1.8748843669891357 3.946423292160034 5.82130765914917
Loss :  1.932754397392273 4.018867015838623 5.9516215324401855
Loss :  1.9068281650543213 3.727104663848877 5.633933067321777
Loss :  1.9396275281906128 3.8883259296417236 5.827953338623047
Loss :  1.904738187789917 3.8346874713897705 5.7394256591796875
Loss :  1.8891843557357788 3.973522901535034 5.862707138061523
Loss :  1.9199612140655518 4.092028617858887 6.011989593505859
Loss :  1.9393916130065918 3.7475759983062744 5.686967849731445
Loss :  1.9449914693832397 4.052029609680176 5.997021198272705
Loss :  1.9257233142852783 3.8496956825256348 5.775419235229492
  batch 60 loss: 1.9257233142852783, 3.8496956825256348, 5.775419235229492
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8688501119613647 3.7216970920562744 5.59054708480835
Loss :  1.8645957708358765 3.9066734313964844 5.77126932144165
Loss :  1.9071017503738403 3.713007688522339 5.620109558105469
Loss :  1.8700968027114868 3.869401216506958 5.739498138427734
Loss :  1.8946149349212646 3.234314203262329 5.128929138183594
Loss :  1.847680926322937 4.611569404602051 6.459250450134277
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8770151138305664 4.179520606994629 6.056535720825195
Loss :  1.8658850193023682 4.105986595153809 5.971871376037598
Loss :  1.8479952812194824 4.270571708679199 6.118566989898682
Total LOSS train 5.769604572883019 valid 6.151556134223938
CE LOSS train 1.904997416642996 valid 0.4619988203048706
Contrastive LOSS train 3.8646071213942306 valid 1.0676429271697998
EPOCH 11:
Loss :  1.895993709564209 3.9475910663604736 5.843585014343262
Loss :  1.9025390148162842 3.9631662368774414 5.865705490112305
Loss :  1.8362547159194946 3.7118992805480957 5.548153877258301
Loss :  1.9027657508850098 3.9960949420928955 5.898860931396484
Loss :  1.9202606678009033 3.737917184829712 5.658177852630615
Loss :  1.8536808490753174 3.9610018730163574 5.814682960510254
Loss :  1.884803295135498 4.0794548988342285 5.964258193969727
Loss :  1.8650740385055542 4.089672565460205 5.954746723175049
Loss :  1.9111534357070923 3.743171453475952 5.654325008392334
Loss :  1.8687173128128052 4.0877156257629395 5.956432819366455
Loss :  1.8648978471755981 3.8525354862213135 5.717433452606201
Loss :  1.9370810985565186 3.9313721656799316 5.868453025817871
Loss :  1.8865324258804321 3.8922035694122314 5.778736114501953
Loss :  1.8990451097488403 3.7769296169281006 5.6759748458862305
Loss :  1.890297770500183 3.8777287006378174 5.768026351928711
Loss :  1.883391261100769 3.8953466415405273 5.778738021850586
Loss :  1.8813551664352417 3.9112274646759033 5.7925825119018555
Loss :  1.9013949632644653 3.7343294620513916 5.6357245445251465
Loss :  1.8778880834579468 3.9909632205963135 5.868851184844971
Loss :  1.8312698602676392 4.191623210906982 6.022892951965332
  batch 20 loss: 1.8312698602676392, 4.191623210906982, 6.022892951965332
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8885552883148193 3.5685956478118896 5.457150936126709
Loss :  1.901305079460144 3.647759199142456 5.5490641593933105
Loss :  1.8641722202301025 3.7906103134155273 5.654782295227051
Loss :  1.886227011680603 4.026259899139404 5.912487030029297
Loss :  1.9160205125808716 3.846142292022705 5.762162685394287
Loss :  1.9247232675552368 4.0719218254089355 5.996644973754883
Loss :  1.9348236322402954 4.01281213760376 5.947635650634766
Loss :  1.8569293022155762 3.7593579292297363 5.6162872314453125
Loss :  1.9263554811477661 3.6995761394500732 5.625931739807129
Loss :  1.9092252254486084 3.791259288787842 5.700484275817871
Loss :  1.9596930742263794 3.745115041732788 5.704808235168457
Loss :  1.8865361213684082 3.8439667224884033 5.730503082275391
Loss :  1.8957632780075073 3.8091750144958496 5.7049384117126465
Loss :  1.9001036882400513 3.817988634109497 5.718092441558838
Loss :  1.9167765378952026 3.8050143718719482 5.721790790557861
Loss :  1.906816005706787 4.107526779174805 6.014342784881592
Loss :  1.8760570287704468 4.250894546508789 6.126951694488525
Loss :  1.8486247062683105 3.9699201583862305 5.818544864654541
Loss :  1.8623974323272705 4.04208517074585 5.904482841491699
Loss :  1.8565661907196045 3.9403460025787354 5.79691219329834
  batch 40 loss: 1.8565661907196045, 3.9403460025787354, 5.79691219329834
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8919460773468018 4.054976463317871 5.946922302246094
Loss :  1.88224196434021 4.199716091156006 6.081957817077637
Loss :  1.8758577108383179 3.995867967605591 5.871725559234619
Loss :  1.9135265350341797 3.6784379482269287 5.5919647216796875
Loss :  1.8884588479995728 3.7603485584259033 5.648807525634766
Loss :  1.8758314847946167 3.792876720428467 5.668708324432373
Loss :  1.882824420928955 3.8832895755767822 5.766114234924316
Loss :  1.9207754135131836 3.9174001216888428 5.8381757736206055
Loss :  1.8917630910873413 4.015408039093018 5.907171249389648
Loss :  1.8926922082901 4.127194881439209 6.0198869705200195
Loss :  1.8638811111450195 3.850778341293335 5.714659690856934
Loss :  1.9068591594696045 3.5719780921936035 5.478837013244629
Loss :  1.8735721111297607 3.714102029800415 5.587674140930176
Loss :  1.9004707336425781 3.639946222305298 5.540416717529297
Loss :  1.8787062168121338 3.7810020446777344 5.659708023071289
Loss :  1.8727161884307861 3.7637581825256348 5.636474609375
Loss :  1.891205906867981 3.948012590408325 5.839218616485596
Loss :  1.8934794664382935 3.8905420303344727 5.784021377563477
Loss :  1.911188006401062 3.775674343109131 5.686862468719482
Loss :  1.8960845470428467 3.6910061836242676 5.587090492248535
  batch 60 loss: 1.8960845470428467, 3.6910061836242676, 5.587090492248535
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8876687288284302 3.728673219680786 5.616342067718506
Loss :  1.879084587097168 3.5076754093170166 5.3867597579956055
Loss :  1.9148792028427124 3.733887195587158 5.64876651763916
Loss :  1.851484775543213 3.65409517288208 5.505579948425293
Loss :  1.8922463655471802 3.229814052581787 5.122060298919678
Loss :  1.85980224609375 4.224081039428711 6.083883285522461
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9351862668991089 4.272098064422607 6.207284450531006
Loss :  1.8772497177124023 3.9966044425964355 5.873854160308838
Loss :  1.8693050146102905 3.686840057373047 5.556145191192627
Total LOSS train 5.748711483295147 valid 5.930291771888733
CE LOSS train 1.889869420345013 valid 0.46732625365257263
Contrastive LOSS train 3.858842050112211 valid 0.9217100143432617
Saved best model. Old loss 6.0887205600738525 and new best loss 5.930291771888733
EPOCH 12:
Loss :  1.928295373916626 4.194369316101074 6.122664451599121
Loss :  1.90713369846344 3.947380542755127 5.854514122009277
Loss :  1.8560107946395874 4.281232833862305 6.137243747711182
Loss :  1.9136137962341309 4.069093227386475 5.9827070236206055
Loss :  1.9136801958084106 3.994114875793457 5.907794952392578
Loss :  1.870650291442871 3.8255510330200195 5.696201324462891
Loss :  1.9129071235656738 4.113376617431641 6.0262837409973145
Loss :  1.8693146705627441 3.8552238941192627 5.724538803100586
Loss :  1.900375247001648 3.7871556282043457 5.687530994415283
Loss :  1.8914254903793335 3.8267037868499756 5.7181291580200195
Loss :  1.8977265357971191 3.6628220081329346 5.560548782348633
Loss :  1.943766474723816 3.9216036796569824 5.865370273590088
Loss :  1.8917125463485718 3.93807053565979 5.829782962799072
Loss :  1.9089912176132202 3.9517908096313477 5.860782146453857
Loss :  1.8912272453308105 3.6561498641967773 5.547377109527588
Loss :  1.883945345878601 3.770944833755493 5.654890060424805
Loss :  1.8408541679382324 3.6613006591796875 5.50215482711792
Loss :  1.8961615562438965 3.6767611503601074 5.572922706604004
Loss :  1.8559757471084595 3.48893666267395 5.344912528991699
Loss :  1.7989075183868408 3.7473931312561035 5.546300888061523
  batch 20 loss: 1.7989075183868408, 3.7473931312561035, 5.546300888061523
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.889816164970398 3.6613881587982178 5.551204204559326
Loss :  1.889937400817871 3.952821969985962 5.842759132385254
Loss :  1.8856350183486938 3.8419337272644043 5.727568626403809
Loss :  1.879713773727417 4.145090103149414 6.02480411529541
Loss :  1.921834111213684 4.1138529777526855 6.03568696975708
Loss :  1.9104433059692383 3.943207025527954 5.853650093078613
Loss :  1.9427039623260498 3.7834677696228027 5.726171493530273
Loss :  1.8389531373977661 4.03652811050415 5.875481128692627
Loss :  1.9122295379638672 3.6585590839385986 5.570788383483887
Loss :  1.9043322801589966 3.7528419494628906 5.657174110412598
Loss :  1.9543827772140503 3.619492530822754 5.573875427246094
Loss :  1.8803482055664062 3.8746752738952637 5.75502347946167
Loss :  1.8948808908462524 3.8468267917633057 5.741707801818848
Loss :  1.9063776731491089 3.8222296237945557 5.728607177734375
Loss :  1.9034485816955566 3.905503273010254 5.8089518547058105
Loss :  1.9147274494171143 3.761126756668091 5.675854206085205
Loss :  1.883424162864685 3.888927698135376 5.7723517417907715
Loss :  1.8682851791381836 3.7140822410583496 5.582367420196533
Loss :  1.8704602718353271 3.9852991104125977 5.855759620666504
Loss :  1.8708288669586182 4.133737564086914 6.004566192626953
  batch 40 loss: 1.8708288669586182, 4.133737564086914, 6.004566192626953
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8863993883132935 4.016289234161377 5.902688503265381
Loss :  1.8987476825714111 3.6974711418151855 5.596219062805176
Loss :  1.9021947383880615 3.9157638549804688 5.817958831787109
Loss :  1.924897313117981 4.238650798797607 6.163547992706299
Loss :  1.9047777652740479 3.9170548915863037 5.821832656860352
Loss :  1.8692959547042847 4.072568893432617 5.941864967346191
Loss :  1.8727401494979858 3.9352188110351562 5.807959079742432
Loss :  1.8799165487289429 3.909282684326172 5.789199352264404
Loss :  1.869394063949585 3.6090688705444336 5.478463172912598
Loss :  1.9143441915512085 3.847994804382324 5.762339115142822
Loss :  1.8594635725021362 3.7740604877471924 5.633523941040039
Loss :  1.8991142511367798 3.5949056148529053 5.494019985198975
Loss :  1.8551071882247925 4.087939262390137 5.943046569824219
Loss :  1.9212062358856201 3.9800612926483154 5.9012675285339355
Loss :  1.874047875404358 3.8345444202423096 5.708592414855957
Loss :  1.869740605354309 3.596745491027832 5.466485977172852
Loss :  1.9068498611450195 4.177008628845215 6.083858489990234
Loss :  1.911004662513733 3.8125128746032715 5.723517417907715
Loss :  1.9103736877441406 3.935673952102661 5.846047401428223
Loss :  1.8983255624771118 3.8656017780303955 5.763927459716797
  batch 60 loss: 1.8983255624771118, 3.8656017780303955, 5.763927459716797
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8578076362609863 3.8563146591186523 5.714122295379639
Loss :  1.8616557121276855 3.720499277114868 5.582155227661133
Loss :  1.8968561887741089 3.656888246536255 5.553744316101074
Loss :  1.8510240316390991 3.9260077476501465 5.777031898498535
Loss :  1.88802170753479 3.5170676708221436 5.405089378356934
Loss :  1.8278130292892456 4.03440523147583 5.862218379974365
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8404172658920288 3.9755654335021973 5.815982818603516
Loss :  1.9162039756774902 3.5854756832122803 5.501679420471191
Loss :  1.8334290981292725 3.6296119689941406 5.463041305541992
Total LOSS train 5.756700104933518 valid 5.660730481147766
CE LOSS train 1.8904422209813045 valid 0.4583572745323181
Contrastive LOSS train 3.8662578802842362 valid 0.9074029922485352
Saved best model. Old loss 5.930291771888733 and new best loss 5.660730481147766
EPOCH 13:
Loss :  1.8947709798812866 3.648838996887207 5.543610095977783
Loss :  1.8760656118392944 4.09333610534668 5.969401836395264
Loss :  1.82985258102417 3.6242196559906006 5.454071998596191
Loss :  1.8656219244003296 4.021449089050293 5.887071132659912
Loss :  1.8941289186477661 3.6889843940734863 5.583113193511963
Loss :  1.850480079650879 3.6027097702026367 5.453189849853516
Loss :  1.902052879333496 4.315127372741699 6.217180252075195
Loss :  1.8573641777038574 4.322013854980469 6.179378032684326
Loss :  1.8911871910095215 3.7974541187286377 5.688641548156738
Loss :  1.864760160446167 4.013683319091797 5.878443717956543
Loss :  1.8796131610870361 4.117709636688232 5.997323036193848
Loss :  1.9401603937149048 4.198710918426514 6.138871192932129
Loss :  1.8560060262680054 3.728255033493042 5.584260940551758
Loss :  1.8857969045639038 3.7915868759155273 5.677383899688721
Loss :  1.835829734802246 3.811237335205078 5.647067070007324
Loss :  1.8869907855987549 3.938694715499878 5.825685501098633
Loss :  1.8321583271026611 4.222115516662598 6.05427360534668
Loss :  1.896301031112671 4.025766372680664 5.922067642211914
Loss :  1.8422410488128662 3.897213935852051 5.739455223083496
Loss :  1.7903542518615723 3.99179744720459 5.782151699066162
  batch 20 loss: 1.7903542518615723, 3.99179744720459, 5.782151699066162
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8883804082870483 4.22354793548584 6.111928462982178
Loss :  1.8728464841842651 3.9172134399414062 5.790060043334961
Loss :  1.8857930898666382 3.8057379722595215 5.691531181335449
Loss :  1.88894522190094 3.7595012187957764 5.648446559906006
Loss :  1.8877753019332886 3.948012351989746 5.835787773132324
Loss :  1.9067450761795044 4.045177936553955 5.95192289352417
Loss :  1.9253923892974854 3.860522508621216 5.785914897918701
Loss :  1.840972661972046 3.9097468852996826 5.7507195472717285
Loss :  1.9019324779510498 4.131511688232422 6.033444404602051
Loss :  1.874904990196228 3.7662196159362793 5.641124725341797
Loss :  1.9324228763580322 3.8766913414001465 5.809114456176758
Loss :  1.850533366203308 4.0526957511901855 5.903229236602783
Loss :  1.8721436262130737 3.9020333290100098 5.774177074432373
Loss :  1.876920461654663 3.8277359008789062 5.704656600952148
Loss :  1.8961657285690308 3.663876533508301 5.560042381286621
Loss :  1.9013018608093262 3.5977892875671387 5.499091148376465
Loss :  1.85272216796875 3.7451910972595215 5.5979132652282715
Loss :  1.8221529722213745 3.69872784614563 5.520880699157715
Loss :  1.8246794939041138 3.9268410205841064 5.75152063369751
Loss :  1.8347848653793335 3.850397825241089 5.685182571411133
  batch 40 loss: 1.8347848653793335, 3.850397825241089, 5.685182571411133
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8443855047225952 3.841104030609131 5.685489654541016
Loss :  1.8490349054336548 3.8059680461883545 5.655003070831299
Loss :  1.8551757335662842 4.0027079582214355 5.857883453369141
Loss :  1.8908966779708862 3.96494460105896 5.855841159820557
Loss :  1.8692156076431274 3.8267531394958496 5.6959686279296875
Loss :  1.8593825101852417 3.8983123302459717 5.757694721221924
Loss :  1.847152829170227 3.457976818084717 5.305129528045654
Loss :  1.8688230514526367 4.025886058807373 5.89470911026001
Loss :  1.8567322492599487 4.086534023284912 5.94326639175415
Loss :  1.8720152378082275 4.037533283233643 5.909548759460449
Loss :  1.8516572713851929 3.9976325035095215 5.849289894104004
Loss :  1.9113761186599731 4.329057216644287 6.240433216094971
Loss :  1.8731571435928345 3.641277313232422 5.514434337615967
Loss :  1.9007072448730469 3.8176825046539307 5.718389511108398
Loss :  1.8641380071640015 3.7003767490386963 5.564514636993408
Loss :  1.8511883020401 3.6794557571411133 5.530643939971924
Loss :  1.8705040216445923 3.876483917236328 5.746987819671631
Loss :  1.8831158876419067 3.7416505813598633 5.6247663497924805
Loss :  1.9130645990371704 3.9131927490234375 5.826257228851318
Loss :  1.8953931331634521 3.639141798019409 5.534534931182861
  batch 60 loss: 1.8953931331634521, 3.639141798019409, 5.534534931182861
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.859195351600647 3.728362798690796 5.587558269500732
Loss :  1.855794072151184 3.8197615146636963 5.67555570602417
Loss :  1.8651437759399414 3.769200086593628 5.634344100952148
Loss :  1.8325623273849487 3.7423441410064697 5.574906349182129
Loss :  1.867206335067749 3.155313730239868 5.022520065307617
Loss :  1.9915454387664795 4.266876697540283 6.258421897888184
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  2.0006585121154785 4.233335018157959 6.2339935302734375
Loss :  2.0844388008117676 4.483128547668457 6.567567348480225
Loss :  2.033761978149414 3.8416128158569336 5.875374794006348
Total LOSS train 5.745769243973952 valid 6.233839392662048
CE LOSS train 1.8710195321303147 valid 0.5084404945373535
Contrastive LOSS train 3.87474968616779 valid 0.9604032039642334
EPOCH 14:
Loss :  1.857356071472168 3.6688733100891113 5.526229381561279
Loss :  1.8520509004592896 3.959402561187744 5.811453342437744
Loss :  1.8024851083755493 3.579174280166626 5.381659507751465
Loss :  1.8552550077438354 3.706894636154175 5.562149524688721
Loss :  1.8644846677780151 3.6934895515441895 5.557974338531494
Loss :  1.8120416402816772 3.770198345184326 5.582240104675293
Loss :  1.849809169769287 3.748784303665161 5.598593711853027
Loss :  1.8409816026687622 3.992914915084839 5.833896636962891
Loss :  1.8807204961776733 3.6265721321105957 5.507292747497559
Loss :  1.853501796722412 3.6419661045074463 5.4954681396484375
Loss :  1.8438316583633423 3.843172550201416 5.687004089355469
Loss :  1.937822937965393 4.2833943367004395 6.221217155456543
Loss :  1.8762186765670776 4.208738803863525 6.084957599639893
Loss :  1.876008152961731 4.024885177612305 5.900893211364746
Loss :  1.8704369068145752 3.976318836212158 5.8467559814453125
Loss :  1.8757433891296387 3.711240291595459 5.586983680725098
Loss :  1.8267772197723389 3.404956579208374 5.231733798980713
Loss :  1.8940584659576416 4.007784843444824 5.901843070983887
Loss :  1.8606352806091309 3.8048207759857178 5.6654558181762695
Loss :  1.8097033500671387 3.9643514156341553 5.774054527282715
  batch 20 loss: 1.8097033500671387, 3.9643514156341553, 5.774054527282715
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8647191524505615 3.6659698486328125 5.530689239501953
Loss :  1.8709644079208374 3.765535831451416 5.636500358581543
Loss :  1.8489247560501099 3.577547788619995 5.4264726638793945
Loss :  1.8665661811828613 3.66382098197937 5.530386924743652
Loss :  1.889331340789795 3.9496281147003174 5.838959693908691
Loss :  1.8896766901016235 3.713243007659912 5.602919578552246
Loss :  1.918773889541626 3.7629177570343018 5.681691646575928
Loss :  1.825223445892334 3.76383113861084 5.589054584503174
Loss :  1.8807923793792725 3.7368109226226807 5.617603302001953
Loss :  1.8809025287628174 3.536141872406006 5.417044639587402
Loss :  1.918202519416809 3.752131938934326 5.670334339141846
Loss :  1.85807466506958 4.039234161376953 5.897308826446533
Loss :  1.8682658672332764 3.5719573497772217 5.440223217010498
Loss :  1.8693269491195679 3.715894937515259 5.585221767425537
Loss :  1.8953588008880615 3.837812900543213 5.733171463012695
Loss :  1.8909929990768433 3.621577262878418 5.512570381164551
Loss :  1.8634223937988281 3.9578778743743896 5.821300506591797
Loss :  1.845770001411438 3.8406929969787598 5.686462879180908
Loss :  1.8509373664855957 3.80985164642334 5.6607890129089355
Loss :  1.8408761024475098 3.6566474437713623 5.497523307800293
  batch 40 loss: 1.8408761024475098, 3.6566474437713623, 5.497523307800293
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8516587018966675 3.981534719467163 5.833193302154541
Loss :  1.8538216352462769 3.7374720573425293 5.591293811798096
Loss :  1.8560526371002197 3.696920156478882 5.552972793579102
Loss :  1.8865153789520264 3.8107409477233887 5.697256088256836
Loss :  1.8618441820144653 3.847186326980591 5.709030628204346
Loss :  1.8271116018295288 4.012616157531738 5.839727878570557
Loss :  1.885813593864441 4.138007640838623 6.0238213539123535
Loss :  1.885012149810791 4.315079689025879 6.20009183883667
Loss :  1.841044545173645 3.813469648361206 5.654514312744141
Loss :  1.8825602531433105 3.90329647064209 5.7858567237854
Loss :  1.824742078781128 3.9385902881622314 5.763332366943359
Loss :  1.8908321857452393 3.670474052429199 5.561305999755859
Loss :  1.8483349084854126 4.004598617553711 5.852933406829834
Loss :  1.8551884889602661 3.865856647491455 5.721045017242432
Loss :  1.8319909572601318 3.757413625717163 5.589404582977295
Loss :  1.8250267505645752 3.8052103519439697 5.630237102508545
Loss :  1.8539798259735107 4.076828479766846 5.930808067321777
Loss :  1.8549834489822388 3.698753595352173 5.553737163543701
Loss :  1.8915544748306274 3.7144341468811035 5.605988502502441
Loss :  1.8697459697723389 3.669304132461548 5.539050102233887
  batch 60 loss: 1.8697459697723389, 3.669304132461548, 5.539050102233887
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.841971516609192 3.7746822834014893 5.616653919219971
Loss :  1.8427834510803223 3.857652425765991 5.700435638427734
Loss :  1.8740549087524414 3.724945068359375 5.598999977111816
Loss :  1.8318731784820557 3.7763512134552 5.608224391937256
Loss :  1.8722729682922363 3.2584989070892334 5.130771636962891
Loss :  1.986793875694275 4.343271255493164 6.3300652503967285
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  2.0303902626037598 4.438826084136963 6.469216346740723
Loss :  1.9756624698638916 4.19720983505249 6.172872543334961
Loss :  2.00380802154541 3.3793420791625977 5.383150100708008
Total LOSS train 5.668073404752291 valid 6.088826060295105
CE LOSS train 1.8618122265889094 valid 0.5009520053863525
Contrastive LOSS train 3.806261187333327 valid 0.8448355197906494
EPOCH 15:
Loss :  1.8630715608596802 3.7971205711364746 5.660192012786865
Loss :  1.8691236972808838 3.8861732482910156 5.75529670715332
Loss :  1.801965355873108 3.8039369583129883 5.605902194976807
Loss :  1.851778268814087 3.7608423233032227 5.6126203536987305
Loss :  1.8622978925704956 4.073295593261719 5.935593605041504
Loss :  1.805916428565979 3.6935694217681885 5.499485969543457
Loss :  1.8552629947662354 3.80314040184021 5.658403396606445
Loss :  1.828052282333374 3.623680353164673 5.451732635498047
Loss :  1.8490875959396362 3.6851494312286377 5.534236907958984
Loss :  1.8161813020706177 3.7776589393615723 5.5938401222229
Loss :  1.8289806842803955 3.822988271713257 5.651968955993652
Loss :  1.8845486640930176 4.0690202713012695 5.953568935394287
Loss :  1.8154411315917969 3.895298480987549 5.710739612579346
Loss :  1.8290073871612549 3.9361720085144043 5.765179634094238
Loss :  1.8393436670303345 3.695692300796509 5.535036087036133
Loss :  1.8476481437683105 3.7542593479156494 5.601907730102539
Loss :  1.804109811782837 3.727909564971924 5.53201961517334
Loss :  1.8496159315109253 3.897958278656006 5.747574329376221
Loss :  1.824665904045105 3.7605626583099365 5.585228443145752
Loss :  1.781322717666626 3.6761183738708496 5.457441329956055
  batch 20 loss: 1.781322717666626, 3.6761183738708496, 5.457441329956055
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8368816375732422 3.7669999599456787 5.6038818359375
Loss :  1.8627774715423584 3.921966552734375 5.7847442626953125
Loss :  1.8270528316497803 3.6534929275512695 5.480545997619629
Loss :  1.8652633428573608 3.691664695739746 5.5569281578063965
Loss :  1.8794695138931274 3.9752197265625 5.854689121246338
Loss :  1.8622592687606812 3.8618359565734863 5.724095344543457
Loss :  1.8773380517959595 3.9689149856567383 5.846252918243408
Loss :  1.8113733530044556 3.5474162101745605 5.358789443969727
Loss :  1.8779551982879639 3.7610981464385986 5.6390533447265625
Loss :  1.8593294620513916 3.831158399581909 5.690487861633301
Loss :  1.9147385358810425 3.6792714595794678 5.594009876251221
Loss :  1.8370893001556396 4.208051681518555 6.045141220092773
Loss :  1.8669968843460083 3.7591042518615723 5.626101016998291
Loss :  1.8597685098648071 3.867056369781494 5.726824760437012
Loss :  1.8927537202835083 3.748666524887085 5.641420364379883
Loss :  1.89337956905365 4.026450157165527 5.919829845428467
Loss :  1.8420746326446533 3.7951481342315674 5.637222766876221
Loss :  1.8227250576019287 3.689518928527832 5.51224422454834
Loss :  1.8128145933151245 3.5701494216918945 5.382964134216309
Loss :  1.8175899982452393 3.983546018600464 5.801136016845703
  batch 40 loss: 1.8175899982452393, 3.983546018600464, 5.801136016845703
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8343286514282227 3.94981050491333 5.784139156341553
Loss :  1.852421760559082 3.7200849056243896 5.572506904602051
Loss :  1.8486225605010986 3.8774633407592773 5.726085662841797
Loss :  1.8697506189346313 3.86177921295166 5.731529712677002
Loss :  1.8373830318450928 3.7622392177581787 5.5996222496032715
Loss :  1.8317382335662842 3.6743950843811035 5.506133079528809
Loss :  1.8300846815109253 3.445014476776123 5.275099277496338
Loss :  1.8288602828979492 3.555222988128662 5.384083271026611
Loss :  1.8259251117706299 3.5884623527526855 5.4143877029418945
Loss :  1.851997971534729 3.781013250350952 5.633011341094971
Loss :  1.8198597431182861 4.108520030975342 5.928380012512207
Loss :  1.8623343706130981 3.8582510948181152 5.720585346221924
Loss :  1.8306812047958374 3.6793930530548096 5.510074138641357
Loss :  1.8667316436767578 3.6753509044647217 5.542082786560059
Loss :  1.8627424240112305 3.775935411453247 5.638677597045898
Loss :  1.8209632635116577 3.5517899990081787 5.372753143310547
Loss :  1.8674466609954834 3.929835081100464 5.797281742095947
Loss :  1.8749265670776367 3.7790942192077637 5.6540207862854
Loss :  1.8911069631576538 3.8567988872528076 5.747905731201172
Loss :  1.8727868795394897 3.5021779537200928 5.374964714050293
  batch 60 loss: 1.8727868795394897, 3.5021779537200928, 5.374964714050293
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8372704982757568 3.885690450668335 5.722960948944092
Loss :  1.8293753862380981 3.8887264728546143 5.718101978302002
Loss :  1.8418313264846802 3.8777623176574707 5.719593524932861
Loss :  1.8109002113342285 3.6307880878448486 5.441688537597656
Loss :  1.8407248258590698 3.209160804748535 5.0498857498168945
Loss :  2.1087164878845215 4.062470436096191 6.171186923980713
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  2.0773262977600098 4.091150760650635 6.1684770584106445
Loss :  2.0381524562835693 4.1374287605285645 6.175580978393555
Loss :  2.0486526489257812 3.892063856124878 5.940716743469238
Total LOSS train 5.628305941361647 valid 6.113990426063538
CE LOSS train 1.8456591881238498 valid 0.5121631622314453
Contrastive LOSS train 3.78264672939594 valid 0.9730159640312195
EPOCH 16:
Loss :  1.8375191688537598 3.660052537918091 5.49757194519043
Loss :  1.8337295055389404 3.8147478103637695 5.648477554321289
Loss :  1.7640239000320435 3.549034357070923 5.313058376312256
Loss :  1.8266242742538452 3.632063150405884 5.4586873054504395
Loss :  1.858585000038147 3.6212964057922363 5.479881286621094
Loss :  1.8154585361480713 3.61562442779541 5.431082725524902
Loss :  1.8396599292755127 3.7475123405456543 5.587172508239746
Loss :  1.8419363498687744 4.060233116149902 5.902169227600098
Loss :  1.8791528940200806 4.018218994140625 5.897371768951416
Loss :  1.8226757049560547 3.948843240737915 5.771518707275391
Loss :  1.8511536121368408 3.9894750118255615 5.840628623962402
Loss :  1.909544825553894 3.9648280143737793 5.874372959136963
Loss :  1.8391175270080566 4.056756019592285 5.895873546600342
Loss :  1.8530552387237549 3.913654088973999 5.766709327697754
Loss :  1.8259234428405762 3.878349542617798 5.704273223876953
Loss :  1.839841365814209 3.506957769393921 5.346798896789551
Loss :  1.8047199249267578 3.4100279808044434 5.214747905731201
Loss :  1.8671932220458984 3.8130993843078613 5.68029260635376
Loss :  1.816459059715271 3.8533363342285156 5.669795513153076
Loss :  1.7815160751342773 3.5817506313323975 5.363266944885254
  batch 20 loss: 1.7815160751342773, 3.5817506313323975, 5.363266944885254
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8466087579727173 3.815488576889038 5.662097454071045
Loss :  1.8517756462097168 3.679104804992676 5.530880451202393
Loss :  1.8452470302581787 3.760286808013916 5.605533599853516
Loss :  1.8644471168518066 3.913020372390747 5.777467727661133
Loss :  1.851836085319519 3.8216195106506348 5.673455715179443
Loss :  1.8538466691970825 3.6616265773773193 5.515473365783691
Loss :  1.8738478422164917 3.6568033695220947 5.530651092529297
Loss :  1.7978429794311523 3.665339469909668 5.46318244934082
Loss :  1.8591763973236084 3.599179744720459 5.458355903625488
Loss :  1.8381987810134888 3.549017906188965 5.387216567993164
Loss :  1.8985846042633057 3.561126708984375 5.459711074829102
Loss :  1.8193992376327515 3.744008779525757 5.563407897949219
Loss :  1.8235929012298584 3.4535017013549805 5.277094841003418
Loss :  1.8440711498260498 3.5760178565979004 5.420088768005371
Loss :  1.8532658815383911 3.8090641498565674 5.662330150604248
Loss :  1.835917592048645 3.886608362197876 5.7225260734558105
Loss :  1.8147450685501099 3.8691418170928955 5.683887004852295
Loss :  1.7906482219696045 3.937960147857666 5.728608131408691
Loss :  1.801111102104187 3.77445650100708 5.575567722320557
Loss :  1.8166354894638062 4.1272406578063965 5.943876266479492
  batch 40 loss: 1.8166354894638062, 4.1272406578063965, 5.943876266479492
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8216803073883057 4.251734256744385 6.0734148025512695
Loss :  1.8334290981292725 3.741929531097412 5.5753583908081055
Loss :  1.822501540184021 4.150424480438232 5.972926139831543
Loss :  1.8278357982635498 4.111885070800781 5.93972110748291
Loss :  1.8518908023834229 4.533318042755127 6.385209083557129
Loss :  1.8059887886047363 4.325883388519287 6.131872177124023
Loss :  1.8129812479019165 4.22088098526001 6.033862113952637
Loss :  1.8341165781021118 4.122434139251709 5.956550598144531
Loss :  1.8426158428192139 4.164706230163574 6.007322311401367
Loss :  1.8445929288864136 4.175303936004639 6.019896984100342
Loss :  1.8064298629760742 3.9242334365844727 5.730663299560547
Loss :  1.8583749532699585 3.971461534500122 5.829836368560791
Loss :  1.8092241287231445 3.8477962017059326 5.657020568847656
Loss :  1.8387064933776855 3.7570557594299316 5.595762252807617
Loss :  1.8167756795883179 3.928334951400757 5.745110511779785
Loss :  1.808440923690796 3.9105069637298584 5.718947887420654
Loss :  1.8503258228302002 4.095419883728027 5.945745468139648
Loss :  1.8551563024520874 3.7390401363372803 5.594196319580078
Loss :  1.8696008920669556 3.7104010581970215 5.5800018310546875
Loss :  1.8598273992538452 3.7499823570251465 5.609809875488281
  batch 60 loss: 1.8598273992538452, 3.7499823570251465, 5.609809875488281
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8292759656906128 4.099898338317871 5.929174423217773
Loss :  1.8224737644195557 4.01339054107666 5.835864067077637
Loss :  1.8509474992752075 3.7771687507629395 5.628116130828857
Loss :  1.813807487487793 3.5831263065338135 5.396933555603027
Loss :  1.8275175094604492 3.470980167388916 5.298497676849365
Loss :  1.9512317180633545 4.405592918395996 6.35682487487793
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.920601487159729 4.459839820861816 6.380441188812256
Loss :  1.944635272026062 3.965029001235962 5.909664154052734
Loss :  1.9059395790100098 4.365378379821777 6.271317958831787
Total LOSS train 5.6796458024245045 valid 6.229562044143677
CE LOSS train 1.8354339342850905 valid 0.47648489475250244
Contrastive LOSS train 3.8442118681394137 valid 1.0913445949554443
EPOCH 17:
Loss :  1.8398469686508179 3.6202054023742676 5.460052490234375
Loss :  1.825645923614502 3.7423689365386963 5.568015098571777
Loss :  1.7703574895858765 3.70658802986145 5.476945400238037
Loss :  1.8241748809814453 3.929896593093872 5.754071235656738
Loss :  1.8436052799224854 3.731670618057251 5.575275897979736
Loss :  1.7741631269454956 3.958096504211426 5.732259750366211
Loss :  1.801249623298645 3.739010810852051 5.540260314941406
Loss :  1.801609992980957 3.886721134185791 5.688331127166748
Loss :  1.8137450218200684 3.9327852725982666 5.746530532836914
Loss :  1.7618365287780762 3.6671698093414307 5.429006576538086
Loss :  1.7890679836273193 3.891345977783203 5.680414199829102
Loss :  1.8560479879379272 3.940138339996338 5.796186447143555
Loss :  1.798780083656311 3.936948537826538 5.735728740692139
Loss :  1.8186848163604736 4.121260166168213 5.939945220947266
Loss :  1.8177647590637207 3.8392364978790283 5.657001495361328
Loss :  1.8282310962677002 3.7460708618164062 5.574301719665527
Loss :  1.8039945363998413 3.7727885246276855 5.576783180236816
Loss :  1.829060673713684 3.9041380882263184 5.733198642730713
Loss :  1.8085273504257202 3.7517144680023193 5.56024169921875
Loss :  1.7731451988220215 3.843019962310791 5.6161651611328125
  batch 20 loss: 1.7731451988220215, 3.843019962310791, 5.6161651611328125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.812633991241455 3.8541741371154785 5.666808128356934
Loss :  1.821223497390747 3.939166307449341 5.760389804840088
Loss :  1.807360291481018 3.9329943656921387 5.740354537963867
Loss :  1.8140909671783447 3.9049646854400635 5.719055652618408
Loss :  1.8408541679382324 3.942863702774048 5.783718109130859
Loss :  1.822718858718872 3.7565150260925293 5.5792341232299805
Loss :  1.8441126346588135 3.7538301944732666 5.59794282913208
Loss :  1.7641645669937134 3.8369219303131104 5.601086616516113
Loss :  1.825081467628479 3.794283628463745 5.619365215301514
Loss :  1.793978214263916 3.8865532875061035 5.6805315017700195
Loss :  1.8567922115325928 3.794342517852783 5.651134490966797
Loss :  1.786144495010376 3.785306453704834 5.571451187133789
Loss :  1.7831615209579468 3.54069447517395 5.323855876922607
Loss :  1.8097306489944458 3.5431876182556152 5.3529181480407715
Loss :  1.8331019878387451 3.770843982696533 5.603945732116699
Loss :  1.8167883157730103 3.5329368114471436 5.349725246429443
Loss :  1.7860571146011353 3.4924046993255615 5.278461933135986
Loss :  1.7632406949996948 3.5581300258636475 5.321370601654053
Loss :  1.775665283203125 3.546924352645874 5.322589874267578
Loss :  1.7707420587539673 3.4961464405059814 5.266888618469238
  batch 40 loss: 1.7707420587539673, 3.4961464405059814, 5.266888618469238
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7898200750350952 3.3705077171325684 5.160327911376953
Loss :  1.7884734869003296 3.361499309539795 5.149972915649414
Loss :  1.7958909273147583 3.641571521759033 5.437462329864502
Loss :  1.8160955905914307 3.949582815170288 5.765678405761719
Loss :  1.8059940338134766 3.5720365047454834 5.378030776977539
Loss :  1.7773716449737549 3.8036108016967773 5.580982208251953
Loss :  1.7713072299957275 3.5146939754486084 5.286001205444336
Loss :  1.7927441596984863 3.6517655849456787 5.444509506225586
Loss :  1.774924874305725 3.4877583980560303 5.262683391571045
Loss :  1.8018865585327148 3.6463005542755127 5.448186874389648
Loss :  1.7682228088378906 3.849327802658081 5.617550849914551
Loss :  1.8201624155044556 3.6301567554473877 5.450319290161133
Loss :  1.7776381969451904 3.513756036758423 5.291394233703613
Loss :  1.7981529235839844 3.6918206214904785 5.489973545074463
Loss :  1.7858271598815918 3.697364330291748 5.48319149017334
Loss :  1.7550359964370728 3.4465653896331787 5.201601505279541
Loss :  1.7964783906936646 3.804046630859375 5.60052490234375
Loss :  1.81670081615448 3.710230588912964 5.526931285858154
Loss :  1.8328925371170044 3.7212862968444824 5.554178714752197
Loss :  1.8020665645599365 3.5496726036071777 5.351738929748535
  batch 60 loss: 1.8020665645599365, 3.5496726036071777, 5.351738929748535
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7803765535354614 3.712592124938965 5.492968559265137
Loss :  1.7677396535873413 3.57098126411438 5.338720798492432
Loss :  1.801042079925537 3.5166242122650146 5.317666053771973
Loss :  1.7698726654052734 3.422889232635498 5.1927618980407715
Loss :  1.7892495393753052 2.918853759765625 4.708103179931641
Loss :  1.930419921875 4.02868127822876 5.95910120010376
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.955822467803955 4.087049961090088 6.042872428894043
Loss :  1.9762884378433228 3.76597261428833 5.742260932922363
Loss :  1.91995370388031 4.063069820404053 5.983023643493652
Total LOSS train 5.510200060330904 valid 5.931814551353455
CE LOSS train 1.8012792183802677 valid 0.4799884259700775
Contrastive LOSS train 3.708920830946702 valid 1.0157674551010132
EPOCH 18:
Loss :  1.7844090461730957 3.6159400939941406 5.400349140167236
Loss :  1.8010681867599487 3.6229169368743896 5.423985004425049
Loss :  1.7392200231552124 3.612598180770874 5.351818084716797
Loss :  1.7701597213745117 3.4973788261413574 5.267538547515869
Loss :  1.8013333082199097 3.421247720718384 5.222580909729004
Loss :  1.7375565767288208 3.418036937713623 5.155593395233154
Loss :  1.7927725315093994 3.66207218170166 5.4548444747924805
Loss :  1.7731297016143799 3.4312989711761475 5.204428672790527
Loss :  1.7862902879714966 3.3682286739349365 5.154519081115723
Loss :  1.7520030736923218 3.9394452571868896 5.691448211669922
Loss :  1.768602967262268 3.7765860557556152 5.545188903808594
Loss :  1.819664478302002 3.9478580951690674 5.767522811889648
Loss :  1.770709753036499 3.776139974594116 5.546849727630615
Loss :  1.7760893106460571 3.586742877960205 5.362832069396973
Loss :  1.7884008884429932 3.49519419670105 5.283595085144043
Loss :  1.784628987312317 3.643975257873535 5.4286041259765625
Loss :  1.7707446813583374 3.7828025817871094 5.553547382354736
Loss :  1.7905343770980835 3.673966646194458 5.464500904083252
Loss :  1.7266699075698853 3.638603448867798 5.365273475646973
Loss :  1.7318097352981567 3.7100186347961426 5.44182825088501
  batch 20 loss: 1.7318097352981567, 3.7100186347961426, 5.44182825088501
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7845379114151 3.551525354385376 5.336063385009766
Loss :  1.7838630676269531 3.621734619140625 5.405597686767578
Loss :  1.7689261436462402 3.3634307384490967 5.132356643676758
Loss :  1.768054723739624 3.7917983531951904 5.5598530769348145
Loss :  1.7988345623016357 3.658083438873291 5.456917762756348
Loss :  1.8031419515609741 3.644028663635254 5.447170734405518
Loss :  1.836855173110962 3.5576233863830566 5.394478797912598
Loss :  1.754819631576538 3.512683391571045 5.267502784729004
Loss :  1.823357105255127 3.526137113571167 5.349493980407715
Loss :  1.7888259887695312 3.449587106704712 5.238412857055664
Loss :  1.84731125831604 3.5578911304473877 5.405202388763428
Loss :  1.7799466848373413 3.5926854610443115 5.372632026672363
Loss :  1.7820347547531128 3.4759974479675293 5.258032321929932
Loss :  1.8055601119995117 3.7184970378875732 5.524057388305664
Loss :  1.8150614500045776 3.7040529251098633 5.5191144943237305
Loss :  1.8059989213943481 3.523081064224243 5.329080104827881
Loss :  1.76906418800354 3.5653676986694336 5.3344316482543945
Loss :  1.7364243268966675 3.589961528778076 5.326385974884033
Loss :  1.7340724468231201 3.653696298599243 5.387768745422363
Loss :  1.7439802885055542 3.4808151721954346 5.224795341491699
  batch 40 loss: 1.7439802885055542, 3.4808151721954346, 5.224795341491699
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7619438171386719 3.5535011291503906 5.3154449462890625
Loss :  1.7613240480422974 3.529371976852417 5.290696144104004
Loss :  1.7749258279800415 3.608720064163208 5.383646011352539
Loss :  1.7683485746383667 3.7159457206726074 5.484294414520264
Loss :  1.773534893989563 3.319344997406006 5.092879772186279
Loss :  1.7641794681549072 3.674945116043091 5.439124584197998
Loss :  1.745102047920227 3.254626750946045 4.999728679656982
Loss :  1.7640243768692017 3.540142297744751 5.304166793823242
Loss :  1.7618964910507202 3.5408904552459717 5.302786827087402
Loss :  1.7755604982376099 3.6515657901763916 5.427126407623291
Loss :  1.748058795928955 3.6919336318969727 5.439992427825928
Loss :  1.795946478843689 3.589728832244873 5.385675430297852
Loss :  1.765615701675415 3.413621664047241 5.179237365722656
Loss :  1.769683837890625 3.3943064212799072 5.163990020751953
Loss :  1.749923586845398 3.695664405822754 5.445588111877441
Loss :  1.7185331583023071 3.632091760635376 5.350625038146973
Loss :  1.7807196378707886 3.6852152347564697 5.465934753417969
Loss :  1.7903077602386475 3.7133898735046387 5.503697395324707
Loss :  1.8112077713012695 3.7963876724243164 5.607595443725586
Loss :  1.7703628540039062 3.348209857940674 5.11857271194458
  batch 60 loss: 1.7703628540039062, 3.348209857940674, 5.11857271194458
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7573285102844238 3.589799404144287 5.347127914428711
Loss :  1.735300898551941 3.568713665008545 5.304014682769775
Loss :  1.774179458618164 3.4137816429138184 5.187961101531982
Loss :  1.7408385276794434 3.4520490169525146 5.192887306213379
Loss :  1.7555104494094849 3.0984854698181152 4.8539958000183105
Loss :  1.9500551223754883 3.9572062492370605 5.907261371612549
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  2.0159637928009033 4.092466354370117 6.108429908752441
Loss :  2.0023694038391113 3.8246474266052246 5.827016830444336
Loss :  1.9624947309494019 3.5269367694854736 5.489431381225586
Total LOSS train 5.3529690228975735 valid 5.833034873008728
CE LOSS train 1.7740127031619732 valid 0.49062368273735046
Contrastive LOSS train 3.5789563435774583 valid 0.8817341923713684
EPOCH 19:
Loss :  1.7539286613464355 3.4294190406799316 5.183347702026367
Loss :  1.7702159881591797 3.6019599437713623 5.372176170349121
Loss :  1.7156476974487305 3.5957601070404053 5.311408042907715
Loss :  1.741715669631958 3.4364538192749023 5.178169250488281
Loss :  1.7861042022705078 3.6181650161743164 5.404269218444824
Loss :  1.7171248197555542 3.603715181350708 5.320839881896973
Loss :  1.7787909507751465 3.72898530960083 5.507776260375977
Loss :  1.754256010055542 3.507871150970459 5.262126922607422
Loss :  1.7611974477767944 3.446890354156494 5.208087921142578
Loss :  1.7439295053482056 3.4538662433624268 5.197795867919922
Loss :  1.7629354000091553 3.5459866523742676 5.308921813964844
Loss :  1.809081792831421 3.721200466156006 5.530282020568848
Loss :  1.7580899000167847 3.530146360397339 5.288236141204834
Loss :  1.765055775642395 3.606379747390747 5.371435642242432
Loss :  1.765897274017334 3.6627390384674072 5.42863655090332
Loss :  1.7743194103240967 3.7125420570373535 5.486861228942871
Loss :  1.75954008102417 3.50522518157959 5.26476526260376
Loss :  1.7849830389022827 3.4272093772888184 5.212192535400391
Loss :  1.7228450775146484 3.261626958847046 4.984472274780273
Loss :  1.720198631286621 3.3702895641326904 5.090488433837891
  batch 20 loss: 1.720198631286621, 3.3702895641326904, 5.090488433837891
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7715604305267334 3.6707308292388916 5.442291259765625
Loss :  1.7712795734405518 3.47310733795166 5.244386672973633
Loss :  1.753030776977539 3.513427257537842 5.266458034515381
Loss :  1.7628121376037598 3.7231109142303467 5.485922813415527
Loss :  1.7895562648773193 3.8541665077209473 5.6437225341796875
Loss :  1.7800087928771973 3.649448871612549 5.429457664489746
Loss :  1.8053460121154785 3.6635468006134033 5.468893051147461
Loss :  1.7418965101242065 3.5190746784210205 5.2609710693359375
Loss :  1.7945492267608643 3.1908950805664062 4.985444068908691
Loss :  1.7671116590499878 3.361666440963745 5.128777980804443
Loss :  1.8227633237838745 3.516555070877075 5.33931827545166
Loss :  1.7541686296463013 3.3967349529266357 5.150903701782227
Loss :  1.7471296787261963 3.241652250289917 4.988781929016113
Loss :  1.772949457168579 3.5513453483581543 5.3242950439453125
Loss :  1.7962979078292847 3.6503188610076904 5.4466166496276855
Loss :  1.7810105085372925 3.59346866607666 5.374479293823242
Loss :  1.7534375190734863 3.429488182067871 5.182925701141357
Loss :  1.7143012285232544 3.3378233909606934 5.052124500274658
Loss :  1.7204890251159668 3.5545692443847656 5.275058269500732
Loss :  1.7351243495941162 3.3575961589813232 5.0927205085754395
  batch 40 loss: 1.7351243495941162, 3.3575961589813232, 5.0927205085754395
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7571818828582764 3.5681893825531006 5.325371265411377
Loss :  1.7426774501800537 3.500105381011963 5.2427825927734375
Loss :  1.7572067975997925 3.497933864593506 5.255140781402588
Loss :  1.7595841884613037 3.393895387649536 5.15347957611084
Loss :  1.755581259727478 3.3153066635131836 5.070888042449951
Loss :  1.749616265296936 3.5240793228149414 5.273695468902588
Loss :  1.729087471961975 3.4664721488952637 5.195559501647949
Loss :  1.7382746934890747 3.4021384716033936 5.140413284301758
Loss :  1.7348108291625977 3.4830307960510254 5.217841625213623
Loss :  1.753387689590454 3.6453616619110107 5.398749351501465
Loss :  1.7226579189300537 3.3619017601013184 5.084559440612793
Loss :  1.7654454708099365 3.344745397567749 5.1101908683776855
Loss :  1.751157283782959 3.323554754257202 5.074711799621582
Loss :  1.742732048034668 3.172210931777954 4.914942741394043
Loss :  1.7370424270629883 3.563234806060791 5.300277233123779
Loss :  1.7012075185775757 3.2773239612579346 4.978531360626221
Loss :  1.7661806344985962 3.5420544147491455 5.308235168457031
Loss :  1.7604970932006836 3.458165407180786 5.218662261962891
Loss :  1.7868863344192505 3.511065721511841 5.297952175140381
Loss :  1.7459412813186646 3.2784476280212402 5.024388790130615
  batch 60 loss: 1.7459412813186646, 3.2784476280212402, 5.024388790130615
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.732023000717163 3.3230996131896973 5.055122375488281
Loss :  1.7176432609558105 3.4968152046203613 5.214458465576172
Loss :  1.7453455924987793 3.4201724529266357 5.165517807006836
Loss :  1.7229262590408325 3.550269842147827 5.273196220397949
Loss :  1.7273831367492676 3.1040210723876953 4.831404209136963
Loss :  1.8924280405044556 4.035409450531006 5.927837371826172
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.9310522079467773 3.9627528190612793 5.893805027008057
Loss :  1.943359136581421 4.067465305328369 6.010824203491211
Loss :  1.92598295211792 3.8505966663360596 5.776579856872559
Total LOSS train 5.240337085723877 valid 5.9022616147994995
CE LOSS train 1.755125509775602 valid 0.48149573802948
Contrastive LOSS train 3.4852116071260895 valid 0.9626491665840149
EPOCH 20:
Loss :  1.7201110124588013 3.4044599533081055 5.124570846557617
Loss :  1.7440919876098633 3.545311450958252 5.289403438568115
Loss :  1.6870803833007812 3.4238290786743164 5.110909461975098
Loss :  1.7012802362442017 3.344425916671753 5.045706272125244
Loss :  1.7539924383163452 3.527951955795288 5.281944274902344
Loss :  1.685789942741394 3.5802738666534424 5.266063690185547
Loss :  1.752224326133728 3.519029140472412 5.27125358581543
Loss :  1.736331582069397 3.4008593559265137 5.137190818786621
Loss :  1.742787480354309 3.281256675720215 5.024044036865234
Loss :  1.7117412090301514 3.4436819553375244 5.155423164367676
Loss :  1.7466973066329956 3.574723720550537 5.321421146392822
Loss :  1.7981816530227661 3.7222869396209717 5.520468711853027
Loss :  1.751981258392334 3.4781570434570312 5.230138301849365
Loss :  1.7586475610733032 3.674241542816162 5.432888984680176
Loss :  1.7380017042160034 3.412428855895996 5.150430679321289
Loss :  1.7563613653182983 3.6380555629730225 5.394416809082031
Loss :  1.7607654333114624 3.5462419986724854 5.307007312774658
Loss :  1.7597969770431519 3.3341479301452637 5.093945026397705
Loss :  1.7150700092315674 3.1926662921905518 4.907736301422119
Loss :  1.7036995887756348 3.481912136077881 5.185611724853516
  batch 20 loss: 1.7036995887756348, 3.481912136077881, 5.185611724853516
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7425477504730225 3.454127550125122 5.1966753005981445
Loss :  1.7619839906692505 3.397644519805908 5.159628391265869
Loss :  1.7117220163345337 3.240312099456787 4.952033996582031
Loss :  1.7458642721176147 3.4878463745117188 5.233710765838623
Loss :  1.7627822160720825 3.6916093826293945 5.4543914794921875
Loss :  1.7442575693130493 3.493259906768799 5.237517356872559
Loss :  1.7926396131515503 3.619006395339966 5.411645889282227
Loss :  1.7214120626449585 3.291931390762329 5.013343334197998
Loss :  1.792219638824463 3.2633867263793945 5.055606365203857
Loss :  1.7319360971450806 3.302302598953247 5.034238815307617
Loss :  1.8036737442016602 3.5021750926971436 5.305849075317383
Loss :  1.7298321723937988 3.679469347000122 5.4093017578125
Loss :  1.728121280670166 3.4927327632904053 5.220853805541992
Loss :  1.745895266532898 3.441131591796875 5.1870269775390625
Loss :  1.7839479446411133 3.65830397605896 5.442252159118652
Loss :  1.7744197845458984 3.73184871673584 5.506268501281738
Loss :  1.7167917490005493 3.5839624404907227 5.300754070281982
Loss :  1.672298550605774 3.4992728233337402 5.171571254730225
Loss :  1.7062855958938599 3.679730176925659 5.386015892028809
Loss :  1.7075645923614502 3.48502254486084 5.192586898803711
  batch 40 loss: 1.7075645923614502, 3.48502254486084, 5.192586898803711
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7465866804122925 3.6154568195343018 5.362043380737305
Loss :  1.7407463788986206 3.4649763107299805 5.205722808837891
Loss :  1.750614047050476 3.401123285293579 5.151737213134766
Loss :  1.744102954864502 3.5489072799682617 5.293010234832764
Loss :  1.7457952499389648 3.253779411315918 4.999574661254883
Loss :  1.745653748512268 3.51786208152771 5.263515949249268
Loss :  1.7254984378814697 3.258476495742798 4.983974933624268
Loss :  1.7465994358062744 3.483574390411377 5.2301740646362305
Loss :  1.726099967956543 3.405176877975464 5.131277084350586
Loss :  1.7614506483078003 3.263036012649536 5.024486541748047
Loss :  1.7315174341201782 3.441622734069824 5.173140048980713
Loss :  1.7705904245376587 3.4056060314178467 5.176196575164795
Loss :  1.760201096534729 3.302311897277832 5.0625128746032715
Loss :  1.7465269565582275 3.4893991947174072 5.235926151275635
Loss :  1.7537016868591309 3.49802565574646 5.251727104187012
Loss :  1.6887712478637695 3.2107598781585693 4.899531364440918
Loss :  1.7689895629882812 3.4216742515563965 5.190663814544678
Loss :  1.775451421737671 3.331202745437622 5.106654167175293
Loss :  1.795750379562378 3.59614896774292 5.391899108886719
Loss :  1.7453250885009766 3.335669994354248 5.080995082855225
  batch 60 loss: 1.7453250885009766, 3.335669994354248, 5.080995082855225
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.750779151916504 3.253864049911499 5.004643440246582
Loss :  1.7279014587402344 3.411475658416748 5.139377117156982
Loss :  1.7536616325378418 3.236814260482788 4.990475654602051
Loss :  1.7376816272735596 3.409450054168701 5.14713191986084
Loss :  1.7527333498001099 2.7982873916625977 4.551020622253418
Loss :  1.8078947067260742 4.226545810699463 6.034440517425537
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.8521380424499512 4.178369522094727 6.030507564544678
Loss :  1.879494309425354 3.868980646133423 5.748475074768066
Loss :  1.805230975151062 3.7602181434631348 5.565449237823486
Total LOSS train 5.187219362992507 valid 5.844718098640442
CE LOSS train 1.742977837415842 valid 0.4513077437877655
Contrastive LOSS train 3.444241531078632 valid 0.9400545358657837
EPOCH 21:
Loss :  1.7254219055175781 3.390533208847046 5.115955352783203
Loss :  1.7600919008255005 3.413377285003662 5.173469066619873
Loss :  1.7074100971221924 3.1801836490631104 4.887593746185303
Loss :  1.7122504711151123 3.300724744796753 5.012975215911865
Loss :  1.7473444938659668 3.2484841346740723 4.995828628540039
Loss :  1.701087474822998 3.2139430046081543 4.915030479431152
Loss :  1.7492990493774414 3.2660086154937744 5.015307426452637
Loss :  1.7398728132247925 3.105679988861084 4.845552921295166
Loss :  1.746195912361145 2.9356749057769775 4.681870937347412
Loss :  1.7031352519989014 3.1169850826263428 4.820120334625244
Loss :  1.745396614074707 3.3291373252868652 5.074533939361572
Loss :  1.8017598390579224 3.3007171154022217 5.102477073669434
Loss :  1.7521671056747437 3.3004729747772217 5.052639961242676
Loss :  1.7463126182556152 3.3889706134796143 5.135283470153809
Loss :  1.7401902675628662 3.162203073501587 4.902393341064453
Loss :  1.7356427907943726 3.4863038063049316 5.221946716308594
Loss :  1.758123755455017 3.2577919960021973 5.015915870666504
Loss :  1.7481831312179565 3.074673891067505 4.822856903076172
Loss :  1.7037791013717651 2.92160701751709 4.6253862380981445
Loss :  1.6997133493423462 3.2422640323638916 4.941977500915527
  batch 20 loss: 1.6997133493423462, 3.2422640323638916, 4.941977500915527
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7394309043884277 3.2722928524017334 5.011723518371582
Loss :  1.7541958093643188 3.279663562774658 5.0338592529296875
Loss :  1.7164095640182495 3.3434410095214844 5.059850692749023
Loss :  1.735609769821167 3.5227925777435303 5.258402347564697
Loss :  1.7661329507827759 3.7165956497192383 5.482728481292725
Loss :  1.745027780532837 3.5267302989959717 5.271758079528809
Loss :  1.7940608263015747 3.4420783519744873 5.236139297485352
Loss :  1.7289848327636719 3.272914171218872 5.001898765563965
Loss :  1.7965680360794067 3.3661420345306396 5.162710189819336
Loss :  1.7408851385116577 3.505120038986206 5.246005058288574
Loss :  1.8181569576263428 3.721040964126587 5.53919792175293
Loss :  1.7394698858261108 3.823490619659424 5.562960624694824
Loss :  1.7461298704147339 3.6862645149230957 5.432394504547119
Loss :  1.7555859088897705 3.2842507362365723 5.039836883544922
Loss :  1.7882529497146606 3.7572901248931885 5.545543193817139
Loss :  1.7758232355117798 3.754456043243408 5.530279159545898
Loss :  1.7450746297836304 3.4927666187286377 5.2378411293029785
Loss :  1.6950056552886963 3.8956069946289062 5.590612411499023
Loss :  1.7384908199310303 3.705550193786621 5.4440412521362305
Loss :  1.7248036861419678 3.5436766147613525 5.26848030090332
  batch 40 loss: 1.7248036861419678, 3.5436766147613525, 5.26848030090332
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7473692893981934 3.551549196243286 5.298918724060059
Loss :  1.740584135055542 3.2578542232513428 4.998438358306885
Loss :  1.7501001358032227 3.6359808444976807 5.386080741882324
Loss :  1.7428070306777954 3.519150495529175 5.26195764541626
Loss :  1.7452566623687744 3.639374256134033 5.384631156921387
Loss :  1.7468736171722412 3.44641375541687 5.193287372589111
Loss :  1.720974087715149 3.2256991863250732 4.946673393249512
Loss :  1.7415916919708252 3.386817216873169 5.128408908843994
Loss :  1.7123488187789917 3.2905969619750977 5.002945899963379
Loss :  1.7529067993164062 3.345308780670166 5.098215579986572
Loss :  1.7289938926696777 3.6047301292419434 5.333724021911621
Loss :  1.7701162099838257 3.3356680870056152 5.1057844161987305
Loss :  1.765879511833191 3.424166440963745 5.1900458335876465
Loss :  1.7601033449172974 3.703439474105835 5.463542938232422
Loss :  1.765183448791504 3.636949062347412 5.402132511138916
Loss :  1.7114884853363037 3.7063801288604736 5.417868614196777
Loss :  1.7746553421020508 3.6885123252868652 5.463167667388916
Loss :  1.7635838985443115 3.4046311378479004 5.168214797973633
Loss :  1.7877066135406494 3.6465489864349365 5.434255599975586
Loss :  1.7611795663833618 3.4674394130706787 5.22861909866333
  batch 60 loss: 1.7611795663833618, 3.4674394130706787, 5.22861909866333
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7495949268341064 3.3004841804504395 5.050079345703125
Loss :  1.736220121383667 3.316554307937622 5.052774429321289
Loss :  1.749404788017273 3.200240135192871 4.949645042419434
Loss :  1.7303553819656372 3.2930474281311035 5.023402690887451
Loss :  1.7480639219284058 3.1290972232818604 4.877161026000977
Loss :  2.33660888671875 4.322549343109131 6.659158229827881
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  2.3630752563476562 4.052367687225342 6.415442943572998
Loss :  2.3955740928649902 4.133571147918701 6.529145240783691
Loss :  2.128171682357788 3.6583430767059326 5.786514759063721
Total LOSS train 5.156543907752404 valid 6.347565293312073
CE LOSS train 1.7457049058033869 valid 0.532042920589447
Contrastive LOSS train 3.4108389817751372 valid 0.9145857691764832
EPOCH 22:
Loss :  1.7272409200668335 3.400477170944214 5.127717971801758
Loss :  1.7479838132858276 3.3298802375793457 5.077864170074463
Loss :  1.718711018562317 3.168109893798828 4.8868207931518555
Loss :  1.7246793508529663 3.305687189102173 5.03036642074585
Loss :  1.7492191791534424 3.244645833969116 4.993865013122559
Loss :  1.702053189277649 3.581821918487549 5.283874988555908
Loss :  1.7437727451324463 3.617337942123413 5.361110687255859
Loss :  1.7303779125213623 3.354708671569824 5.085086822509766
Loss :  1.7374272346496582 3.028317928314209 4.765745162963867
Loss :  1.6735458374023438 3.087006092071533 4.760551929473877
Loss :  1.7254502773284912 3.336207628250122 5.061657905578613
Loss :  1.795106053352356 3.3459365367889404 5.141042709350586
Loss :  1.7260500192642212 3.1717233657836914 4.897773265838623
Loss :  1.7078577280044556 3.2312140464782715 4.9390716552734375
Loss :  1.722029685974121 3.0971152782440186 4.819145202636719
Loss :  1.708571195602417 3.3850512504577637 5.093622207641602
Loss :  1.7195968627929688 3.147235870361328 4.866832733154297
Loss :  1.7082487344741821 3.0592987537384033 4.767547607421875
Loss :  1.6713229417800903 2.9035556316375732 4.574878692626953
Loss :  1.6677619218826294 3.163339853286743 4.831101894378662
  batch 20 loss: 1.6677619218826294, 3.163339853286743, 4.831101894378662
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7099740505218506 3.0322282314300537 4.742202281951904
Loss :  1.7364442348480225 3.135406017303467 4.87185001373291
Loss :  1.6882710456848145 3.254756212234497 4.943027496337891
Loss :  1.7288241386413574 3.328721761703491 5.0575456619262695
Loss :  1.7427161931991577 3.7346408367156982 5.477356910705566
Loss :  1.7100328207015991 3.5234715938568115 5.233504295349121
Loss :  1.7438397407531738 3.6639280319213867 5.4077677726745605
Loss :  1.6890532970428467 3.5042192935943604 5.193272590637207
Loss :  1.7474720478057861 3.5255773067474365 5.273049354553223
Loss :  1.7064763307571411 3.7201433181762695 5.426619529724121
Loss :  1.7768744230270386 3.702404499053955 5.479279041290283
Loss :  1.7119845151901245 3.552292823791504 5.264277458190918
Loss :  1.6875849962234497 3.6108157634735107 5.29840087890625
Loss :  1.7220875024795532 3.5423741340637207 5.264461517333984
Loss :  1.7798439264297485 3.807149648666382 5.58699369430542
Loss :  1.7418960332870483 3.8818089962005615 5.62370491027832
Loss :  1.7195699214935303 3.5096962451934814 5.229266166687012
Loss :  1.64090895652771 3.372051954269409 5.012960910797119
Loss :  1.703627586364746 3.4348573684692383 5.138484954833984
Loss :  1.701672911643982 3.486015558242798 5.18768835067749
  batch 40 loss: 1.701672911643982, 3.486015558242798, 5.18768835067749
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7321958541870117 3.710426092147827 5.442622184753418
Loss :  1.6949800252914429 3.5979738235473633 5.292953968048096
Loss :  1.723543405532837 3.421473741531372 5.145017147064209
Loss :  1.718217134475708 3.6185142993927 5.336731433868408
Loss :  1.718252182006836 3.1364026069641113 4.854654788970947
Loss :  1.721900463104248 3.4004440307617188 5.122344493865967
Loss :  1.6850864887237549 3.246088743209839 4.931175231933594
Loss :  1.7086892127990723 3.0319607257843018 4.740650177001953
Loss :  1.6612694263458252 3.4263598918914795 5.087629318237305
Loss :  1.71860671043396 3.4339370727539062 5.152544021606445
Loss :  1.6981291770935059 3.0098049640655518 4.707934379577637
Loss :  1.7197048664093018 3.0902421474456787 4.8099470138549805
Loss :  1.7352300882339478 2.915513515472412 4.65074348449707
Loss :  1.6907689571380615 3.0395214557647705 4.730290412902832
Loss :  1.7321226596832275 3.4498066902160645 5.181929588317871
Loss :  1.6516631841659546 3.157750129699707 4.809413433074951
Loss :  1.7410945892333984 3.180361747741699 4.921456336975098
Loss :  1.7379162311553955 3.5560824871063232 5.293998718261719
Loss :  1.7675269842147827 3.403331995010376 5.170858860015869
Loss :  1.7035644054412842 3.1345489025115967 4.838113307952881
  batch 60 loss: 1.7035644054412842, 3.1345489025115967, 4.838113307952881
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7183891534805298 3.6394147872924805 5.357803821563721
Loss :  1.702099084854126 3.502609968185425 5.204709053039551
Loss :  1.7140742540359497 3.528748035430908 5.242822170257568
Loss :  1.717401146888733 3.519867181777954 5.237268447875977
Loss :  1.7144172191619873 3.0170211791992188 4.731438636779785
Loss :  3.118459701538086 4.415893077850342 7.534352779388428
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  3.160398006439209 4.352040767669678 7.512438774108887
Loss :  3.2650270462036133 4.297093868255615 7.5621209144592285
Loss :  2.6433160305023193 4.321249008178711 6.964565277099609
Total LOSS train 5.078037570073055 valid 7.393369436264038
CE LOSS train 1.717276987662682 valid 0.6608290076255798
Contrastive LOSS train 3.3607605677384598 valid 1.0803122520446777
EPOCH 23:
Loss :  1.677107572555542 3.14705228805542 4.824159622192383
Loss :  1.7076455354690552 3.5053608417510986 5.213006496429443
Loss :  1.6743987798690796 3.220625162124634 4.895023822784424
Loss :  1.6778045892715454 3.361449718475342 5.039254188537598
Loss :  1.7214688062667847 3.704584836959839 5.426053524017334
Loss :  1.6646397113800049 3.095562219619751 4.760201930999756
Loss :  1.6971373558044434 3.175286054611206 4.87242317199707
Loss :  1.706588864326477 3.13114595413208 4.837734699249268
Loss :  1.715592384338379 2.9453282356262207 4.6609206199646
Loss :  1.626076579093933 3.0668230056762695 4.692899703979492
Loss :  1.6972448825836182 3.162442207336426 4.859686851501465
Loss :  1.7892264127731323 3.4147727489471436 5.203999042510986
Loss :  1.7159510850906372 3.494612455368042 5.210563659667969
Loss :  1.693227767944336 3.5507872104644775 5.244014739990234
Loss :  1.7165108919143677 3.5788369178771973 5.295347690582275
Loss :  1.6684596538543701 3.737679958343506 5.406139373779297
Loss :  1.717166543006897 3.3924367427825928 5.109603404998779
Loss :  1.674957513809204 3.3571295738220215 5.032087326049805
Loss :  1.6619288921356201 3.4207053184509277 5.082633972167969
Loss :  1.6483135223388672 3.3561437129974365 5.004457473754883
  batch 20 loss: 1.6483135223388672, 3.3561437129974365, 5.004457473754883
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7104814052581787 3.4370923042297363 5.147573471069336
Loss :  1.738459587097168 3.1634445190429688 4.901904106140137
Loss :  1.6811739206314087 3.4708383083343506 5.152012348175049
Loss :  1.7161356210708618 3.60807466506958 5.324210166931152
Loss :  1.740729808807373 3.6064536571502686 5.3471832275390625
Loss :  1.7000101804733276 3.4693658351898193 5.169375896453857
Loss :  1.7277840375900269 3.292168617248535 5.019952774047852
Loss :  1.6631925106048584 3.561218023300171 5.224410533905029
Loss :  1.725211262702942 3.4508626461029053 5.176074028015137
Loss :  1.698712944984436 3.331848621368408 5.030561447143555
Loss :  1.7743802070617676 3.3858230113983154 5.160202980041504
Loss :  1.7002060413360596 3.508294105529785 5.208499908447266
Loss :  1.673898696899414 3.2484219074249268 4.922320365905762
Loss :  1.704555869102478 3.5573737621307373 5.261929512023926
Loss :  1.759016513824463 3.656327962875366 5.41534423828125
Loss :  1.7312201261520386 3.5580122470855713 5.28923225402832
Loss :  1.6952555179595947 3.0735392570495605 4.768795013427734
Loss :  1.6126047372817993 3.004500150680542 4.617105007171631
Loss :  1.6827086210250854 3.272939443588257 4.955647945404053
Loss :  1.6726810932159424 3.287705659866333 4.960386753082275
  batch 40 loss: 1.6726810932159424, 3.287705659866333, 4.960386753082275
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.711157202720642 3.0701217651367188 4.78127908706665
Loss :  1.6605000495910645 3.0272367000579834 4.687736511230469
Loss :  1.716783881187439 3.22578763961792 4.942571640014648
Loss :  1.674890160560608 3.4171268939971924 5.09201717376709
Loss :  1.7120869159698486 3.2282118797302246 4.940299034118652
Loss :  1.7113112211227417 3.304229736328125 5.015541076660156
Loss :  1.6512095928192139 2.9855434894561768 4.636753082275391
Loss :  1.6961477994918823 3.038869619369507 4.7350172996521
Loss :  1.6420433521270752 3.2409827709198 4.883026123046875
Loss :  1.6911571025848389 3.808255910873413 5.499413013458252
Loss :  1.6827130317687988 3.4923529624938965 5.175065994262695
Loss :  1.6977384090423584 3.4574992656707764 5.155237674713135
Loss :  1.7135621309280396 3.5945088863372803 5.308071136474609
Loss :  1.6450127363204956 3.408914089202881 5.053926944732666
Loss :  1.7418923377990723 3.4010205268859863 5.142912864685059
Loss :  1.6313881874084473 3.389519214630127 5.020907402038574
Loss :  1.721691608428955 3.6264455318450928 5.348136901855469
Loss :  1.7224417924880981 3.2285079956054688 4.950949668884277
Loss :  1.7579753398895264 3.539029598236084 5.297004699707031
Loss :  1.695543885231018 3.6056811809539795 5.301225185394287
  batch 60 loss: 1.695543885231018, 3.6056811809539795, 5.301225185394287
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.688855528831482 3.718547821044922 5.407403469085693
Loss :  1.693782091140747 3.417935371398926 5.111717224121094
Loss :  1.6878609657287598 3.8152849674224854 5.503146171569824
Loss :  1.7107257843017578 3.5853078365325928 5.29603385925293
Loss :  1.6962051391601562 3.293064594268799 4.989269733428955
Loss :  4.628263473510742 4.54347562789917 9.17173957824707
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  4.663729190826416 4.455679416656494 9.11940860748291
Loss :  4.722496032714844 4.3246259689331055 9.04712200164795
Loss :  3.9228217601776123 4.358319282531738 8.28114128112793
Total LOSS train 5.07688609636747 valid 8.904852867126465
CE LOSS train 1.697146804516132 valid 0.9807054400444031
Contrastive LOSS train 3.37973932486314 valid 1.0895798206329346
EPOCH 24:
Loss :  1.6554744243621826 3.4757883548736572 5.13126277923584
Loss :  1.6900720596313477 3.6405906677246094 5.330662727355957
Loss :  1.661704659461975 3.687840700149536 5.349545478820801
Loss :  1.6446558237075806 3.383819103240967 5.028474807739258
Loss :  1.7139195203781128 3.310232639312744 5.0241522789001465
Loss :  1.6463261842727661 3.4465653896331787 5.092891693115234
Loss :  1.7005177736282349 3.710822582244873 5.411340236663818
Loss :  1.7028785943984985 3.7429380416870117 5.445816516876221
Loss :  1.695732593536377 3.574660301208496 5.270392894744873
Loss :  1.6510276794433594 3.5728673934936523 5.223895072937012
Loss :  1.6966142654418945 3.355461597442627 5.0520758628845215
Loss :  1.782104730606079 3.568408250808716 5.350512981414795
Loss :  1.7235523462295532 3.581166982650757 5.3047194480896
Loss :  1.7047611474990845 3.6541478633880615 5.3589091300964355
Loss :  1.7036560773849487 3.1314759254455566 4.835132122039795
Loss :  1.685914158821106 3.6492958068847656 5.335209846496582
Loss :  1.7379350662231445 3.747408390045166 5.4853434562683105
Loss :  1.6873059272766113 3.544490337371826 5.2317962646484375
Loss :  1.6591075658798218 3.44350004196167 5.102607727050781
Loss :  1.6707663536071777 3.7144196033477783 5.385186195373535
  batch 20 loss: 1.6707663536071777, 3.7144196033477783, 5.385186195373535
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7082955837249756 3.60225248336792 5.310547828674316
Loss :  1.7360713481903076 3.5567026138305664 5.292774200439453
Loss :  1.6809725761413574 3.402503728866577 5.0834760665893555
Loss :  1.7074774503707886 3.604228973388672 5.31170654296875
Loss :  1.7333621978759766 3.5550742149353027 5.288436412811279
Loss :  1.7064104080200195 3.920323371887207 5.626733779907227
Loss :  1.756084680557251 3.5479955673217773 5.304080009460449
Loss :  1.6945444345474243 3.572139024734497 5.266683578491211
Loss :  1.7910826206207275 3.299736976623535 5.090819358825684
Loss :  1.7054418325424194 3.394453763961792 5.099895477294922
Loss :  1.7816264629364014 3.399219512939453 5.180846214294434
Loss :  1.7115203142166138 3.316678285598755 5.028198719024658
Loss :  1.7101893424987793 3.03100848197937 4.74119758605957
Loss :  1.738082766532898 3.238487958908081 4.9765706062316895
Loss :  1.7675210237503052 3.2892301082611084 5.056751251220703
Loss :  1.7723932266235352 3.17268443107605 4.945077896118164
Loss :  1.69846510887146 2.9720702171325684 4.670535087585449
Loss :  1.643772006034851 2.974242687225342 4.618014812469482
Loss :  1.686930775642395 3.283177614212036 4.970108509063721
Loss :  1.6777687072753906 3.30271053314209 4.9804792404174805
  batch 40 loss: 1.6777687072753906, 3.30271053314209, 4.9804792404174805
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7266275882720947 3.5100464820861816 5.2366743087768555
Loss :  1.7076328992843628 2.9935414791107178 4.701174259185791
Loss :  1.7211235761642456 3.138110876083374 4.85923433303833
Loss :  1.7211995124816895 2.917510747909546 4.638710021972656
Loss :  1.7105765342712402 2.8409650325775146 4.551541328430176
Loss :  1.7490379810333252 3.1454930305480957 4.89453125
Loss :  1.6871167421340942 2.9701621532440186 4.657279014587402
Loss :  1.7138009071350098 3.057400941848755 4.771202087402344
Loss :  1.6901277303695679 2.8166258335113525 4.506753444671631
Loss :  1.7405893802642822 3.2225022315979004 4.963091850280762
Loss :  1.7171058654785156 3.6179444789886475 5.335050582885742
Loss :  1.7260746955871582 3.4101080894470215 5.13618278503418
Loss :  1.7535860538482666 3.4161276817321777 5.169713973999023
Loss :  1.7373896837234497 3.522810935974121 5.260200500488281
Loss :  1.7536813020706177 3.5736215114593506 5.327302932739258
Loss :  1.6516815423965454 3.052096366882324 4.70377779006958
Loss :  1.7681503295898438 3.37214994430542 5.140300273895264
Loss :  1.7644919157028198 3.1971921920776367 4.961684226989746
Loss :  1.7844427824020386 3.635700225830078 5.420143127441406
Loss :  1.7018835544586182 3.391404867172241 5.093288421630859
  batch 60 loss: 1.7018835544586182, 3.391404867172241, 5.093288421630859
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7381497621536255 3.660085439682007 5.398235321044922
Loss :  1.7206625938415527 3.4426791667938232 5.163341522216797
Loss :  1.7184594869613647 3.379782199859619 5.098241806030273
Loss :  1.7485389709472656 3.4894444942474365 5.237983703613281
Loss :  1.7540652751922607 3.5079009532928467 5.261966228485107
Loss :  2.2257425785064697 4.09922981262207 6.324972152709961
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  2.21120285987854 4.307519435882568 6.5187225341796875
Loss :  2.244601249694824 3.9228322505950928 6.167433738708496
Loss :  2.015418529510498 4.05355978012085 6.068978309631348
Total LOSS train 5.10893024297861 valid 6.270026683807373
CE LOSS train 1.7142805613004244 valid 0.5038546323776245
Contrastive LOSS train 3.3946496596703164 valid 1.0133899450302124
EPOCH 25:
Loss :  1.702641487121582 3.2967779636383057 4.999419212341309
Loss :  1.7305800914764404 3.4457955360412598 5.176375389099121
Loss :  1.7094900608062744 3.1344680786132812 4.843957901000977
Loss :  1.698745608329773 3.7438933849334717 5.442638874053955
Loss :  1.7347382307052612 3.4393465518951416 5.174084663391113
Loss :  1.667909860610962 2.9171535968780518 4.585063457489014
Loss :  1.7312705516815186 3.186119556427002 4.917389869689941
Loss :  1.711722731590271 2.971590518951416 4.683313369750977
Loss :  1.7067698240280151 3.268571376800537 4.975341320037842
Loss :  1.666757583618164 2.991269826889038 4.658027648925781
Loss :  1.708720326423645 3.373832941055298 5.082553386688232
Loss :  1.7846958637237549 3.2153398990631104 5.000035762786865
Loss :  1.719754695892334 3.282115936279297 5.001870632171631
Loss :  1.6963274478912354 3.200793504714966 4.897120952606201
Loss :  1.68980872631073 3.0181756019592285 4.707984447479248
Loss :  1.6901922225952148 3.1752936840057373 4.865486145019531
Loss :  1.7209351062774658 2.9773526191711426 4.6982879638671875
Loss :  1.690956473350525 3.317204475402832 5.0081610679626465
Loss :  1.683618426322937 2.9267783164978027 4.610396862030029
Loss :  1.6731783151626587 3.173593759536743 4.846772193908691
  batch 20 loss: 1.6731783151626587, 3.173593759536743, 4.846772193908691
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7096929550170898 3.0399773120880127 4.749670028686523
Loss :  1.732954502105713 3.381674289703369 5.114628791809082
Loss :  1.6851978302001953 3.2237188816070557 4.908916473388672
Loss :  1.7156898975372314 3.385793447494507 5.101483345031738
Loss :  1.738787293434143 3.26963472366333 5.008421897888184
Loss :  1.6973860263824463 3.5830078125 5.280393600463867
Loss :  1.743508219718933 3.531470537185669 5.2749786376953125
Loss :  1.684891700744629 3.224919557571411 4.909811019897461
Loss :  1.7693605422973633 2.890636682510376 4.65999698638916
Loss :  1.7116345167160034 3.2435965538024902 4.955231189727783
Loss :  1.7857751846313477 3.4195563793182373 5.205331802368164
Loss :  1.709723949432373 3.5144240856170654 5.224147796630859
Loss :  1.6885221004486084 3.1404716968536377 4.828993797302246
Loss :  1.740869402885437 3.2442336082458496 4.985103130340576
Loss :  1.7589353322982788 3.6240344047546387 5.382969856262207
Loss :  1.762587070465088 3.412238836288452 5.174825668334961
Loss :  1.6953092813491821 3.4220151901245117 5.117324352264404
Loss :  1.6290810108184814 3.396679162979126 5.025760173797607
Loss :  1.691143274307251 3.3483147621154785 5.039458274841309
Loss :  1.6789644956588745 3.398986339569092 5.077950954437256
  batch 40 loss: 1.6789644956588745, 3.398986339569092, 5.077950954437256
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7183822393417358 3.9048867225646973 5.623269081115723
Loss :  1.7042670249938965 3.2847185134887695 4.988985538482666
Loss :  1.7195003032684326 3.3106415271759033 5.030141830444336
Loss :  1.7320077419281006 3.4608218669891357 5.192829608917236
Loss :  1.7072529792785645 3.6387317180633545 5.34598445892334
Loss :  1.7326165437698364 3.7071588039398193 5.439775466918945
Loss :  1.6831059455871582 3.1554031372070312 4.8385090827941895
Loss :  1.714324712753296 3.1396398544311523 4.853964805603027
Loss :  1.6524490118026733 3.034858465194702 4.687307357788086
Loss :  1.7501063346862793 3.1596224308013916 4.90972900390625
Loss :  1.7231214046478271 3.2497470378875732 4.9728684425354
Loss :  1.7183666229248047 3.214918375015259 4.933284759521484
Loss :  1.745527744293213 2.8065438270568848 4.552071571350098
Loss :  1.7489672899246216 2.878993034362793 4.627960205078125
Loss :  1.7662653923034668 3.1480872631073 4.9143524169921875
Loss :  1.6610429286956787 2.8659005165100098 4.526943206787109
Loss :  1.7670130729675293 3.224087953567505 4.991101264953613
Loss :  1.7591426372528076 3.1985275745391846 4.957670211791992
Loss :  1.774666666984558 3.402831792831421 5.1774983406066895
Loss :  1.7071073055267334 3.0405304431915283 4.747637748718262
  batch 60 loss: 1.7071073055267334, 3.0405304431915283, 4.747637748718262
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7372932434082031 3.1739161014556885 4.9112091064453125
Loss :  1.7305457592010498 3.308995485305786 5.039541244506836
Loss :  1.7162216901779175 3.1529347896575928 4.869156360626221
Loss :  1.7455109357833862 3.213433265686035 4.958944320678711
Loss :  1.7544869184494019 2.3890810012817383 4.14356803894043
Loss :  2.541057825088501 4.151843070983887 6.692900657653809
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  2.551353693008423 4.392544746398926 6.9438982009887695
Loss :  2.550649404525757 3.947944164276123 6.498593330383301
Loss :  2.2363972663879395 3.9149394035339355 6.151336669921875
Total LOSS train 4.960522805727445 valid 6.5716822147369385
CE LOSS train 1.71720185646644 valid 0.5590993165969849
Contrastive LOSS train 3.2433209676008956 valid 0.9787348508834839
EPOCH 26:
Loss :  1.7098209857940674 2.650738477706909 4.360559463500977
Loss :  1.7295531034469604 3.008430004119873 4.737983226776123
Loss :  1.728137731552124 2.9721264839172363 4.700263977050781
Loss :  1.7122642993927002 3.2468645572662354 4.9591288566589355
Loss :  1.729452133178711 3.0657999515533447 4.795251846313477
Loss :  1.6906993389129639 3.303978443145752 4.994677543640137
Loss :  1.710718035697937 3.7538442611694336 5.46456241607666
Loss :  1.7022253274917603 3.4544105529785156 5.156635761260986
Loss :  1.7116279602050781 3.573216199874878 5.284844398498535
Loss :  1.6631399393081665 3.4177708625793457 5.080910682678223
Loss :  1.717299461364746 3.647620677947998 5.364920139312744
Loss :  1.790298342704773 3.5396690368652344 5.329967498779297
Loss :  1.7395408153533936 3.57810640335083 5.3176469802856445
Loss :  1.7168300151824951 3.3763346672058105 5.093164443969727
Loss :  1.6907330751419067 3.226475715637207 4.917208671569824
Loss :  1.7116791009902954 3.5298385620117188 5.241517543792725
Loss :  1.7497048377990723 3.4049642086029053 5.154668807983398
Loss :  1.6980606317520142 3.676022529602051 5.374083042144775
Loss :  1.721545696258545 3.6856935024261475 5.407238960266113
Loss :  1.6854676008224487 3.1856963634490967 4.871163845062256
  batch 20 loss: 1.6854676008224487, 3.1856963634490967, 4.871163845062256
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.70805025100708 3.2812840938568115 4.9893341064453125
Loss :  1.741084337234497 3.523690700531006 5.264775276184082
Loss :  1.6917356252670288 3.357741117477417 5.049476623535156
Loss :  1.7188949584960938 3.56636643409729 5.285261154174805
Loss :  1.7436188459396362 3.6052377223968506 5.348856449127197
Loss :  1.700830101966858 3.3758950233459473 5.076725006103516
Loss :  1.7467702627182007 3.5852255821228027 5.331995964050293
Loss :  1.6863081455230713 3.2272560596466064 4.913564205169678
Loss :  1.7789863348007202 3.19622802734375 4.97521448135376
Loss :  1.7316700220108032 3.4964332580566406 5.228103160858154
Loss :  1.804829478263855 3.538916826248169 5.343746185302734
Loss :  1.7122883796691895 3.544113874435425 5.256402015686035
Loss :  1.7052644491195679 3.23410701751709 4.939371585845947
Loss :  1.7540276050567627 3.375220775604248 5.12924861907959
Loss :  1.7744760513305664 3.4454782009124756 5.219954490661621
Loss :  1.784472942352295 3.387827157974243 5.172300338745117
Loss :  1.7084321975708008 3.249058246612549 4.95749044418335
Loss :  1.6593362092971802 3.2195894718170166 4.878925800323486
Loss :  1.700638771057129 3.439980983734131 5.14061975479126
Loss :  1.6932177543640137 3.521454334259033 5.214672088623047
  batch 40 loss: 1.6932177543640137, 3.521454334259033, 5.214672088623047
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7278085947036743 3.2692337036132812 4.997042179107666
Loss :  1.726212501525879 3.188671588897705 4.914884090423584
Loss :  1.734017252922058 3.264058828353882 4.99807596206665
Loss :  1.7376900911331177 3.2401442527770996 4.977834224700928
Loss :  1.7153743505477905 3.023016929626465 4.738391399383545
Loss :  1.7336641550064087 3.1714484691619873 4.9051127433776855
Loss :  1.6837202310562134 3.1213440895080566 4.8050642013549805
Loss :  1.7138631343841553 3.367464303970337 5.081327438354492
Loss :  1.6479575634002686 3.0272531509399414 4.675210952758789
Loss :  1.7381927967071533 3.3716838359832764 5.10987663269043
Loss :  1.7157169580459595 2.815075635910034 4.530792713165283
Loss :  1.69863760471344 2.895596742630005 4.594234466552734
Loss :  1.7329163551330566 2.9611306190490723 4.694046974182129
Loss :  1.7290750741958618 2.9369916915893555 4.666066646575928
Loss :  1.7403610944747925 3.1819732189178467 4.92233419418335
Loss :  1.6424424648284912 3.1234774589538574 4.7659196853637695
Loss :  1.754294753074646 3.0807723999023438 4.835067272186279
Loss :  1.7446320056915283 3.2559869289398193 5.000618934631348
Loss :  1.7562675476074219 3.2284512519836426 4.9847187995910645
Loss :  1.6903388500213623 3.1182847023010254 4.808623313903809
  batch 60 loss: 1.6903388500213623, 3.1182847023010254, 4.808623313903809
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7175885438919067 3.1888134479522705 4.906402111053467
Loss :  1.7149481773376465 3.332637071609497 5.047585487365723
Loss :  1.695810317993164 2.7863667011260986 4.482176780700684
Loss :  1.7306936979293823 3.1093757152557373 4.84006929397583
Loss :  1.7300907373428345 2.6889071464538574 4.418997764587402
Loss :  3.1675233840942383 4.471318244934082 7.63884162902832
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  3.1608715057373047 4.316868782043457 7.477740287780762
Loss :  3.148869276046753 4.099053382873535 7.247922897338867
Loss :  2.7188377380371094 4.3219895362854 7.04082727432251
Total LOSS train 5.000352463355431 valid 7.351333022117615
CE LOSS train 1.7200930155240572 valid 0.6797094345092773
Contrastive LOSS train 3.2802594808431773 valid 1.08049738407135
EPOCH 27:
Loss :  1.6850889921188354 3.137583017349243 4.822671890258789
Loss :  1.711358666419983 3.47169828414917 5.183056831359863
Loss :  1.7028717994689941 3.264247417449951 4.967119216918945
Loss :  1.6959619522094727 3.3529553413391113 5.048917293548584
Loss :  1.7182903289794922 3.4361283779144287 5.1544189453125
Loss :  1.6843243837356567 3.5861289501190186 5.270453453063965
Loss :  1.7093592882156372 3.5273964405059814 5.236755847930908
Loss :  1.690399169921875 3.2624175548553467 4.952816963195801
Loss :  1.7018400430679321 3.7063941955566406 5.408234119415283
Loss :  1.648746132850647 3.164316415786743 4.81306266784668
Loss :  1.701332688331604 3.1241984367370605 4.825531005859375
Loss :  1.7899307012557983 3.3227641582489014 5.11269474029541
Loss :  1.7231369018554688 3.440387487411499 5.163524627685547
Loss :  1.702139973640442 3.208009958267212 4.910150051116943
Loss :  1.6883008480072021 3.256868839263916 4.945169448852539
Loss :  1.697963833808899 3.0535061359405518 4.75147008895874
Loss :  1.7302980422973633 2.848564386367798 4.578862190246582
Loss :  1.6871073246002197 3.2489254474639893 4.936032772064209
Loss :  1.7087713479995728 3.4910287857055664 5.19980001449585
Loss :  1.6664931774139404 3.8723340034484863 5.538826942443848
  batch 20 loss: 1.6664931774139404, 3.8723340034484863, 5.538826942443848
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7005752325057983 3.4633588790893555 5.163934230804443
Loss :  1.7342808246612549 3.372321844100952 5.106602668762207
Loss :  1.6811603307724 3.6814463138580322 5.362606525421143
Loss :  1.721578598022461 3.5032622814178467 5.224841117858887
Loss :  1.7415248155593872 3.1604506969451904 4.901975631713867
Loss :  1.691063404083252 3.0911459922790527 4.782209396362305
Loss :  1.7270010709762573 3.199227809906006 4.926229000091553
Loss :  1.6649277210235596 2.7735612392425537 4.438488960266113
Loss :  1.7570419311523438 2.8333778381347656 4.590419769287109
Loss :  1.707385540008545 3.315260410308838 5.022645950317383
Loss :  1.7939000129699707 3.2868566513061523 5.080756664276123
Loss :  1.7031325101852417 3.425165891647339 5.128298282623291
Loss :  1.6874401569366455 3.2276763916015625 4.915116310119629
Loss :  1.7429319620132446 3.0834450721740723 4.826376914978027
Loss :  1.7679075002670288 3.407521963119507 5.175429344177246
Loss :  1.761629581451416 2.820378541946411 4.582008361816406
Loss :  1.6946576833724976 2.8437817096710205 4.5384392738342285
Loss :  1.6383349895477295 2.6104342937469482 4.248769283294678
Loss :  1.692765474319458 2.7985501289367676 4.491315841674805
Loss :  1.684695839881897 2.7711384296417236 4.45583438873291
  batch 40 loss: 1.684695839881897, 2.7711384296417236, 4.45583438873291
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7081369161605835 2.8166584968566895 4.5247955322265625
Loss :  1.7028356790542603 2.5284318923950195 4.23126745223999
Loss :  1.7249623537063599 3.0720856189727783 4.797048091888428
Loss :  1.7341219186782837 2.910839796066284 4.644961833953857
Loss :  1.7144724130630493 2.838008165359497 4.552480697631836
Loss :  1.7147153615951538 2.9831674098968506 4.697882652282715
Loss :  1.671098232269287 2.648284435272217 4.319382667541504
Loss :  1.7084828615188599 3.011075735092163 4.7195587158203125
Loss :  1.6483412981033325 2.8817760944366455 4.530117511749268
Loss :  1.7372735738754272 2.93296217918396 4.670235633850098
Loss :  1.7107819318771362 2.9659712314605713 4.676753044128418
Loss :  1.7033822536468506 2.9144904613494873 4.617872714996338
Loss :  1.7251722812652588 3.124451160430908 4.849623680114746
Loss :  1.729256272315979 3.500594139099121 5.2298502922058105
Loss :  1.7382904291152954 3.701939582824707 5.440229892730713
Loss :  1.649396300315857 3.2105300426483154 4.859926223754883
Loss :  1.7486521005630493 3.625025987625122 5.373678207397461
Loss :  1.7421419620513916 3.4442262649536133 5.186367988586426
Loss :  1.7542437314987183 3.4209415912628174 5.175185203552246
Loss :  1.693701148033142 3.455929756164551 5.149631023406982
  batch 60 loss: 1.693701148033142, 3.455929756164551, 5.149631023406982
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7172365188598633 3.1523029804229736 4.869539260864258
Loss :  1.7130684852600098 3.1224052906036377 4.835474014282227
Loss :  1.6943328380584717 2.814850330352783 4.509182929992676
Loss :  1.7295838594436646 2.7672693729400635 4.496853351593018
Loss :  1.724678874015808 2.324627637863159 4.049306392669678
Loss :  3.454684019088745 4.263030529022217 7.717714309692383
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  3.507732391357422 4.290943622589111 7.798676013946533
Loss :  3.4634974002838135 4.207003593444824 7.670500755310059
Loss :  2.9432008266448975 4.232640743255615 7.175841331481934
Total LOSS train 4.87367803133451 valid 7.590683102607727
CE LOSS train 1.7104001595423772 valid 0.7358002066612244
Contrastive LOSS train 3.163277871792133 valid 1.0581601858139038
EPOCH 28:
Loss :  1.6844234466552734 2.688326597213745 4.372750282287598
Loss :  1.7020533084869385 2.986619472503662 4.68867301940918
Loss :  1.6885913610458374 2.564061164855957 4.252652645111084
Loss :  1.6907724142074585 2.7664530277252197 4.457225322723389
Loss :  1.7145428657531738 2.916363000869751 4.630906105041504
Loss :  1.6708296537399292 2.7640411853790283 4.434870719909668
Loss :  1.701786756515503 2.8531649112701416 4.5549516677856445
Loss :  1.6810739040374756 2.659435749053955 4.340509414672852
Loss :  1.6982706785202026 2.5641772747039795 4.262447834014893
Loss :  1.6508210897445679 2.6594207286834717 4.31024169921875
Loss :  1.6951849460601807 2.944135904312134 4.6393208503723145
Loss :  1.7772326469421387 3.1117007732391357 4.888933181762695
Loss :  1.7222641706466675 3.5139222145080566 5.236186504364014
Loss :  1.6990249156951904 2.9695053100585938 4.668530464172363
Loss :  1.6881901025772095 3.1326675415039062 4.820857524871826
Loss :  1.7010595798492432 3.126760244369507 4.82781982421875
Loss :  1.71604585647583 3.055711269378662 4.771757125854492
Loss :  1.691632628440857 3.2562215328216553 4.947854042053223
Loss :  1.700312614440918 2.981081962585449 4.681394577026367
Loss :  1.6637860536575317 3.338003635406494 5.001789569854736
  batch 20 loss: 1.6637860536575317, 3.338003635406494, 5.001789569854736
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6972057819366455 3.0470588207244873 4.744264602661133
Loss :  1.7187533378601074 3.0172739028930664 4.736027240753174
Loss :  1.6878502368927002 3.4321815967559814 5.120031833648682
Loss :  1.708125352859497 3.8784101009368896 5.586535453796387
Loss :  1.7278696298599243 3.600292205810547 5.328161716461182
Loss :  1.6966874599456787 3.3149735927581787 5.011661052703857
Loss :  1.7271037101745605 3.307992458343506 5.035096168518066
Loss :  1.6576088666915894 3.030651569366455 4.688260555267334
Loss :  1.7500745058059692 3.0723929405212402 4.82246732711792
Loss :  1.6998504400253296 3.2220542430877686 4.921904563903809
Loss :  1.784814476966858 3.282533884048462 5.067348480224609
Loss :  1.6980987787246704 3.2628767490386963 4.960975646972656
Loss :  1.6929304599761963 3.15759539604187 4.850525856018066
Loss :  1.734555959701538 3.2681658267974854 5.002721786499023
Loss :  1.7639106512069702 3.145423412322998 4.909334182739258
Loss :  1.7619423866271973 3.2791242599487305 5.041066646575928
Loss :  1.6997286081314087 3.304443836212158 5.004172325134277
Loss :  1.6479127407073975 3.2033238410949707 4.851236343383789
Loss :  1.6863007545471191 3.1762940883636475 4.8625946044921875
Loss :  1.6857283115386963 3.536527156829834 5.222255706787109
  batch 40 loss: 1.6857283115386963, 3.536527156829834, 5.222255706787109
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7056289911270142 3.1994872093200684 4.905116081237793
Loss :  1.7041093111038208 3.44510555267334 5.149214744567871
Loss :  1.722918152809143 3.0673251152038574 4.790243148803711
Loss :  1.7251334190368652 2.8806958198547363 4.605829238891602
Loss :  1.7109782695770264 3.104187488555908 4.8151655197143555
Loss :  1.715937614440918 2.9720606803894043 4.687998294830322
Loss :  1.6712058782577515 2.82450795173645 4.495713710784912
Loss :  1.7046819925308228 3.195187568664551 4.899869441986084
Loss :  1.6491131782531738 2.683784246444702 4.332897186279297
Loss :  1.7325838804244995 2.8954222202301025 4.6280059814453125
Loss :  1.701657772064209 2.8041598796844482 4.505817413330078
Loss :  1.7001404762268066 2.7356579303741455 4.435798645019531
Loss :  1.7167835235595703 2.696038246154785 4.4128217697143555
Loss :  1.7144399881362915 2.993913412094116 4.708353519439697
Loss :  1.7278856039047241 3.03289532661438 4.7607808113098145
Loss :  1.646438479423523 2.8710503578186035 4.517488956451416
Loss :  1.7326111793518066 2.9224095344543457 4.655020713806152
Loss :  1.7278354167938232 3.0522053241729736 4.780040740966797
Loss :  1.7465367317199707 3.271207571029663 5.017744064331055
Loss :  1.69062077999115 2.9920687675476074 4.682689666748047
  batch 60 loss: 1.69062077999115, 2.9920687675476074, 4.682689666748047
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7097091674804688 3.0050370693206787 4.714746475219727
Loss :  1.704963207244873 3.31335711479187 5.018320083618164
Loss :  1.6898854970932007 3.2832674980163574 4.973153114318848
Loss :  1.7194299697875977 3.0946764945983887 4.814106464385986
Loss :  1.7191029787063599 2.744466543197632 4.463569641113281
Loss :  3.6157662868499756 4.080169200897217 7.695935249328613
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  3.6521389484405518 4.200253009796143 7.852392196655273
Loss :  3.611349105834961 3.9628231525421143 7.574172019958496
Loss :  3.0576560497283936 3.8000400066375732 6.857696056365967
Total LOSS train 4.774258767641507 valid 7.495048880577087
CE LOSS train 1.7054658908110398 valid 0.7644140124320984
Contrastive LOSS train 3.0687928970043474 valid 0.9500100016593933
EPOCH 29:
Loss :  1.6874511241912842 3.323322296142578 5.010773658752441
Loss :  1.7041884660720825 3.2010419368743896 4.905230522155762
Loss :  1.7007341384887695 3.1018476486206055 4.802581787109375
Loss :  1.7112793922424316 3.1591949462890625 4.870474338531494
Loss :  1.7186309099197388 3.359844207763672 5.078474998474121
Loss :  1.6901100873947144 3.4916701316833496 5.1817803382873535
Loss :  1.696298360824585 3.3669846057891846 5.0632829666137695
Loss :  1.6899114847183228 3.5373754501342773 5.2272868156433105
Loss :  1.6991243362426758 3.029689311981201 4.728813648223877
Loss :  1.6487433910369873 2.9885854721069336 4.6373291015625
Loss :  1.6955987215042114 2.9637949466705322 4.659393787384033
Loss :  1.7783302068710327 3.2689757347106934 5.047306060791016
Loss :  1.720926284790039 3.281770944595337 5.002696990966797
Loss :  1.6970429420471191 2.993637800216675 4.690680503845215
Loss :  1.6832555532455444 2.82250714302063 4.505762577056885
Loss :  1.6953550577163696 2.8782811164855957 4.573636054992676
Loss :  1.7109671831130981 3.3783109188079834 5.089278221130371
Loss :  1.6903128623962402 3.123908281326294 4.814221382141113
Loss :  1.7010934352874756 2.740345001220703 4.441438674926758
Loss :  1.6677190065383911 3.23266339302063 4.9003825187683105
  batch 20 loss: 1.6677190065383911, 3.23266339302063, 4.9003825187683105
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.696954369544983 3.144603967666626 4.841558456420898
Loss :  1.717590570449829 3.2834548950195312 5.001045227050781
Loss :  1.6869688034057617 3.029654026031494 4.716622829437256
Loss :  1.712717890739441 3.2170135974884033 4.929731369018555
Loss :  1.7265911102294922 3.0709688663482666 4.79755973815918
Loss :  1.693647027015686 3.072079658508301 4.765726566314697
Loss :  1.7205239534378052 3.2710413932800293 4.991565227508545
Loss :  1.6575959920883179 2.664431571960449 4.322027683258057
Loss :  1.7353111505508423 2.722299814224243 4.457611083984375
Loss :  1.6963664293289185 2.7780706882476807 4.474437236785889
Loss :  1.7774958610534668 3.131366014480591 4.908862113952637
Loss :  1.6906111240386963 2.9180896282196045 4.608700752258301
Loss :  1.6811158657073975 2.6337389945983887 4.314854621887207
Loss :  1.7228270769119263 2.902501106262207 4.625328063964844
Loss :  1.751996397972107 3.0953075885772705 4.847303867340088
Loss :  1.7421854734420776 3.0201165676116943 4.762301921844482
Loss :  1.695959448814392 3.1082053184509277 4.804164886474609
Loss :  1.6350140571594238 3.3409008979797363 4.97591495513916
Loss :  1.6843315362930298 3.2606916427612305 4.945023059844971
Loss :  1.681784749031067 2.8719303607940674 4.553715229034424
  batch 40 loss: 1.681784749031067, 2.8719303607940674, 4.553715229034424
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6958317756652832 2.850930690765381 4.546762466430664
Loss :  1.6899609565734863 2.9206504821777344 4.610611438751221
Loss :  1.7157422304153442 2.8825886249542236 4.598330974578857
Loss :  1.7131900787353516 2.836331844329834 4.5495219230651855
Loss :  1.7056825160980225 2.6162056922912598 4.321887969970703
Loss :  1.705159068107605 3.2129018306732178 4.918060779571533
Loss :  1.6664376258850098 3.946009635925293 5.612447261810303
Loss :  1.6970181465148926 3.7277638912200928 5.424781799316406
Loss :  1.6455098390579224 3.480419635772705 5.125929355621338
Loss :  1.7239001989364624 3.5062661170959473 5.230166435241699
Loss :  1.69646155834198 3.5860674381256104 5.282528877258301
Loss :  1.696865200996399 3.396813154220581 5.0936784744262695
Loss :  1.7121576070785522 4.0076141357421875 5.719771862030029
Loss :  1.712242603302002 3.3363616466522217 5.0486040115356445
Loss :  1.7289139032363892 3.281458616256714 5.010372638702393
Loss :  1.6460195779800415 3.3951668739318848 5.041186332702637
Loss :  1.7321245670318604 3.784071683883667 5.516196250915527
Loss :  1.7197905778884888 3.3479037284851074 5.067694187164307
Loss :  1.7422983646392822 3.814173460006714 5.556471824645996
Loss :  1.6909410953521729 3.661350965499878 5.352292060852051
  batch 60 loss: 1.6909410953521729, 3.661350965499878, 5.352292060852051
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7026008367538452 3.4604756832122803 5.163076400756836
Loss :  1.6974586248397827 3.3545243740081787 5.051982879638672
Loss :  1.6856015920639038 3.8488364219665527 5.534438133239746
Loss :  1.7133727073669434 3.259869337081909 4.973241806030273
Loss :  1.6993809938430786 2.2697219848632812 3.9691028594970703
Loss :  2.879436492919922 4.4169697761535645 7.296406269073486
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  2.8849687576293945 4.333666801452637 7.218635559082031
Loss :  2.9320425987243652 4.290194511413574 7.2222371101379395
Loss :  2.482017755508423 4.248242378234863 6.730259895324707
Total LOSS train 4.895292597550612 valid 7.116884708404541
CE LOSS train 1.7019896012086135 valid 0.6205044388771057
Contrastive LOSS train 3.1933030128479003 valid 1.0620605945587158
EPOCH 30:
Loss :  1.6801761388778687 3.1851677894592285 4.865344047546387
Loss :  1.6926038265228271 3.366956949234009 5.059560775756836
Loss :  1.6759089231491089 3.155679225921631 4.831588268280029
Loss :  1.68277907371521 3.281095027923584 4.963873863220215
Loss :  1.7084423303604126 3.212038278579712 4.920480728149414
Loss :  1.6761614084243774 2.8530056476593018 4.529167175292969
Loss :  1.697253704071045 2.7968955039978027 4.494149208068848
Loss :  1.6805015802383423 2.833120584487915 4.513622283935547
Loss :  1.6943953037261963 2.8481080532073975 4.542503356933594
Loss :  1.643438696861267 2.9036664962768555 4.547105312347412
Loss :  1.6833207607269287 2.985687732696533 4.669008255004883
Loss :  1.7739678621292114 3.2688119411468506 5.042779922485352
Loss :  1.7040351629257202 3.26179838180542 4.96583366394043
Loss :  1.6908336877822876 3.4843389987945557 5.175172805786133
Loss :  1.675212025642395 3.2998647689819336 4.975076675415039
Loss :  1.6822726726531982 3.2936596870422363 4.9759321212768555
Loss :  1.6978590488433838 3.3232226371765137 5.021081924438477
Loss :  1.6794143915176392 3.3010783195495605 4.98049259185791
Loss :  1.6844847202301025 2.8065497875213623 4.491034507751465
Loss :  1.6609915494918823 3.0292046070098877 4.6901960372924805
  batch 20 loss: 1.6609915494918823, 3.0292046070098877, 4.6901960372924805
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6903564929962158 3.1176068782806396 4.8079633712768555
Loss :  1.7082151174545288 3.056441068649292 4.764656066894531
Loss :  1.6873778104782104 3.1802709102630615 4.867648601531982
Loss :  1.711730718612671 3.311969518661499 5.02370023727417
Loss :  1.720886468887329 3.3764970302581787 5.097383499145508
Loss :  1.6954675912857056 3.095214366912842 4.790681838989258
Loss :  1.719785451889038 3.3684797286987305 5.088265419006348
Loss :  1.6618750095367432 2.7193429470062256 4.381217956542969
Loss :  1.733075737953186 2.78507924079895 4.518155097961426
Loss :  1.6885994672775269 2.7317874431610107 4.420386791229248
Loss :  1.769403100013733 3.023935079574585 4.793338298797607
Loss :  1.6933345794677734 2.8846700191497803 4.578004837036133
Loss :  1.6848399639129639 2.6681509017944336 4.352991104125977
Loss :  1.7196662425994873 2.8490591049194336 4.5687255859375
Loss :  1.742510199546814 3.163837432861328 4.906347751617432
Loss :  1.7406779527664185 3.1674091815948486 4.908087253570557
Loss :  1.7003092765808105 3.500683069229126 5.200992584228516
Loss :  1.6555883884429932 3.078662633895874 4.734251022338867
Loss :  1.6912139654159546 3.0873565673828125 4.778570652008057
Loss :  1.6882752180099487 3.387362241744995 5.075637340545654
  batch 40 loss: 1.6882752180099487, 3.387362241744995, 5.075637340545654
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6967418193817139 3.454134941101074 5.150876998901367
Loss :  1.6957738399505615 3.339301586151123 5.0350751876831055
Loss :  1.7099597454071045 3.450072765350342 5.160032272338867
Loss :  1.7087403535842896 3.3206052780151367 5.029345512390137
Loss :  1.6999320983886719 3.2580995559692383 4.95803165435791
Loss :  1.7037497758865356 3.029895305633545 4.733644962310791
Loss :  1.678968071937561 2.900446653366089 4.5794148445129395
Loss :  1.6953679323196411 2.8284590244293213 4.523827075958252
Loss :  1.6659191846847534 2.8273801803588867 4.49329948425293
Loss :  1.719294786453247 3.1275088787078857 4.846803665161133
Loss :  1.6984961032867432 2.949723720550537 4.648220062255859
Loss :  1.7064100503921509 2.78560471534729 4.4920148849487305
Loss :  1.7194056510925293 2.5680582523345947 4.287464141845703
Loss :  1.7202686071395874 2.757331132888794 4.477599620819092
Loss :  1.7223517894744873 2.835163116455078 4.5575151443481445
Loss :  1.6631495952606201 2.6591148376464844 4.322264671325684
Loss :  1.733838438987732 2.8600502014160156 4.593888759613037
Loss :  1.7140047550201416 2.8963334560394287 4.61033821105957
Loss :  1.7432777881622314 3.28092360496521 5.024201393127441
Loss :  1.6973562240600586 2.8645286560058594 4.561884880065918
  batch 60 loss: 1.6973562240600586, 2.8645286560058594, 4.561884880065918
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.7051934003829956 3.0828702449798584 4.7880635261535645
Loss :  1.69581937789917 3.33703351020813 5.032853126525879
Loss :  1.688460111618042 2.921765089035034 4.610225200653076
Loss :  1.7137722969055176 3.2524821758270264 4.966254234313965
Loss :  1.704543948173523 3.4368975162506104 5.141441345214844
Loss :  4.088963508605957 4.439812183380127 8.528776168823242
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  4.076359272003174 4.503418445587158 8.579777717590332
Loss :  4.064277172088623 4.338229179382324 8.402505874633789
Loss :  3.3931281566619873 4.43155574798584 7.824684143066406
Total LOSS train 4.7774706106919504 valid 8.333935976028442
CE LOSS train 1.6995082671825703 valid 0.8482820391654968
Contrastive LOSS train 3.0779623104975773 valid 1.10788893699646
EPOCH 31:
Loss :  1.6778075695037842 3.7410151958465576 5.418822765350342
Loss :  1.6905924081802368 3.711876153945923 5.402468681335449
Loss :  1.670886516571045 3.0251810550689697 4.696067810058594
Loss :  1.6824922561645508 3.0562851428985596 4.738777160644531
Loss :  1.7076869010925293 3.4211010932922363 5.128787994384766
Loss :  1.671924114227295 3.243812084197998 4.915736198425293
Loss :  1.7009496688842773 3.230315923690796 4.931265830993652
Loss :  1.6796681880950928 2.9902493953704834 4.669917583465576
Loss :  1.690664291381836 3.315375804901123 5.006040096282959
Loss :  1.6435617208480835 3.378692388534546 5.02225399017334
Loss :  1.6841347217559814 3.6122565269470215 5.296391487121582
Loss :  1.764540433883667 3.8374667167663574 5.602006912231445
Loss :  1.706355333328247 3.418513536453247 5.124868869781494
Loss :  1.6866369247436523 3.430304765701294 5.116941452026367
Loss :  1.6790542602539062 3.1733250617980957 4.852379322052002
Loss :  1.68356192111969 3.8429133892059326 5.526475429534912
Loss :  1.7009708881378174 2.8638954162597656 4.564866065979004
Loss :  1.6733052730560303 2.9938864707946777 4.667191505432129
Loss :  1.690110445022583 3.029806613922119 4.719917297363281
Loss :  1.6564207077026367 3.0138306617736816 4.670251369476318
  batch 20 loss: 1.6564207077026367, 3.0138306617736816, 4.670251369476318
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.686951994895935 3.342547655105591 5.029499530792236
Loss :  1.706207513809204 3.216071367263794 4.922278881072998
Loss :  1.68221914768219 3.5689311027526855 5.251150131225586
Loss :  1.7041256427764893 3.814833641052246 5.518959045410156
Loss :  1.7122355699539185 3.6467764377593994 5.359012126922607
Loss :  1.6875518560409546 3.966801404953003 5.654353141784668
Loss :  1.717065691947937 3.7952029705047607 5.512268543243408
Loss :  1.6573476791381836 3.294588804244995 4.951936721801758
Loss :  1.7354021072387695 2.9589929580688477 4.694395065307617
Loss :  1.6838971376419067 3.217660665512085 4.901557922363281
Loss :  1.7669235467910767 3.1395485401153564 4.906472206115723
Loss :  1.6891613006591797 2.950092077255249 4.639253616333008
Loss :  1.6775201559066772 2.786898136138916 4.464418411254883
Loss :  1.716326355934143 2.86354923248291 4.579875469207764
Loss :  1.7402305603027344 3.3522768020629883 5.092507362365723
Loss :  1.738443374633789 3.323495388031006 5.061938762664795
Loss :  1.692976713180542 3.177408456802368 4.87038516998291
Loss :  1.6417101621627808 3.200456380844116 4.842166423797607
Loss :  1.6818875074386597 3.043062686920166 4.724950313568115
Loss :  1.6773213148117065 3.2284622192382812 4.905783653259277
  batch 40 loss: 1.6773213148117065, 3.2284622192382812, 4.905783653259277
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6912336349487305 3.6032204627990723 5.294454097747803
Loss :  1.6857725381851196 3.696505308151245 5.382277965545654
Loss :  1.69973623752594 3.608436346054077 5.308172702789307
Loss :  1.7045968770980835 3.02016282081604 4.724759578704834
Loss :  1.6922715902328491 2.8036935329437256 4.495965003967285
Loss :  1.6952656507492065 3.027480363845825 4.722745895385742
Loss :  1.66886305809021 3.1097605228424072 4.778623580932617
Loss :  1.6846508979797363 3.0138347148895264 4.698485374450684
Loss :  1.6522635221481323 3.10160756111145 4.753870964050293
Loss :  1.7108012437820435 3.130053758621216 4.840855121612549
Loss :  1.6856377124786377 3.2512826919555664 4.936920166015625
Loss :  1.693146824836731 3.2806599140167236 4.973806858062744
Loss :  1.7097092866897583 3.6178483963012695 5.327557563781738
Loss :  1.7083320617675781 3.060030460357666 4.768362522125244
Loss :  1.7148597240447998 3.3912367820739746 5.106096267700195
Loss :  1.6518014669418335 3.097060441970825 4.748861789703369
Loss :  1.725788950920105 3.440342664718628 5.166131496429443
Loss :  1.71446692943573 3.312659978866577 5.027126789093018
Loss :  1.7342253923416138 3.5454680919647217 5.279693603515625
Loss :  1.695665717124939 3.0060575008392334 4.701723098754883
  batch 60 loss: 1.695665717124939, 3.0060575008392334, 4.701723098754883
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7034053802490234 3.1737570762634277 4.877162456512451
Loss :  1.6975891590118408 2.829782009124756 4.527371406555176
Loss :  1.6937572956085205 2.6147208213806152 4.308478355407715
Loss :  1.706167221069336 2.6793854236602783 4.385552406311035
Loss :  1.69792902469635 2.24322509765625 3.9411540031433105
Loss :  2.26798152923584 4.253427028656006 6.521408557891846
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  2.2181365489959717 4.375415325164795 6.5935516357421875
Loss :  2.227496385574341 4.1209516525268555 6.348447799682617
Loss :  1.9585429430007935 4.053102016448975 6.0116448402404785
Total LOSS train 4.938935375213623 valid 6.368763208389282
CE LOSS train 1.6946887273054856 valid 0.48963573575019836
Contrastive LOSS train 3.24424666258005 valid 1.0132755041122437
EPOCH 32:
Loss :  1.6823502779006958 2.51603627204895 4.1983866691589355
Loss :  1.6947259902954102 2.8271830081939697 4.521908760070801
Loss :  1.6781401634216309 2.4970295429229736 4.175169944763184
Loss :  1.6876542568206787 2.821058988571167 4.508713245391846
Loss :  1.7066490650177002 3.0506372451782227 4.757286071777344
Loss :  1.683388590812683 3.4766526222229004 5.160041332244873
Loss :  1.7010340690612793 3.1516952514648438 4.852729320526123
Loss :  1.6866363286972046 3.4699480533599854 5.1565842628479
Loss :  1.6930264234542847 3.1200194358825684 4.813045978546143
Loss :  1.6463971138000488 3.25716495513916 4.903562068939209
Loss :  1.691762924194336 3.524787664413452 5.216550827026367
Loss :  1.7642995119094849 3.5712106227874756 5.33551025390625
Loss :  1.7095677852630615 3.3297927379608154 5.039360523223877
Loss :  1.6913666725158691 3.4086387157440186 5.100005149841309
Loss :  1.6761784553527832 3.295396089553833 4.971574783325195
Loss :  1.6833142042160034 2.8430793285369873 4.526393413543701
Loss :  1.6983996629714966 2.790452480316162 4.488852024078369
Loss :  1.6841846704483032 3.095059394836426 4.7792439460754395
Loss :  1.6904209852218628 3.16232967376709 4.852750778198242
Loss :  1.6605639457702637 3.460871458053589 5.121435165405273
  batch 20 loss: 1.6605639457702637, 3.460871458053589, 5.121435165405273
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6892611980438232 3.465532064437866 5.1547932624816895
Loss :  1.7099212408065796 3.3000426292419434 5.0099639892578125
Loss :  1.6837875843048096 3.098402976989746 4.782190322875977
Loss :  1.7133686542510986 2.9509241580963135 4.664292812347412
Loss :  1.7168354988098145 3.08298659324646 4.799821853637695
Loss :  1.6922563314437866 2.8920648097991943 4.584321022033691
Loss :  1.7194020748138428 2.883680820465088 4.603082656860352
Loss :  1.6646219491958618 2.572726011276245 4.2373480796813965
Loss :  1.735804796218872 2.666440010070801 4.402244567871094
Loss :  1.6854078769683838 3.142815589904785 4.82822322845459
Loss :  1.7651129961013794 3.2243809700012207 4.9894938468933105
Loss :  1.692524790763855 3.224067211151123 4.916592121124268
Loss :  1.6836473941802979 2.94040584564209 4.624053001403809
Loss :  1.7118867635726929 3.031771421432495 4.743658065795898
Loss :  1.7364845275878906 3.4982688426971436 5.234753608703613
Loss :  1.7323956489562988 3.220752239227295 4.953147888183594
Loss :  1.6984522342681885 2.8324320316314697 4.530884265899658
Loss :  1.6501024961471558 2.638434410095215 4.28853702545166
Loss :  1.691739559173584 2.947563648223877 4.639303207397461
Loss :  1.687458872795105 3.0917699337005615 4.779228687286377
  batch 40 loss: 1.687458872795105, 3.0917699337005615, 4.779228687286377
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6911166906356812 2.8767404556274414 4.567857265472412
Loss :  1.6875789165496826 2.996968984603882 4.6845479011535645
Loss :  1.7023639678955078 2.91241455078125 4.614778518676758
Loss :  1.7032047510147095 2.9006569385528564 4.6038618087768555
Loss :  1.6957004070281982 2.5630042552948 4.258704662322998
Loss :  1.6976393461227417 3.0518105030059814 4.749449729919434
Loss :  1.6749217510223389 3.126039743423462 4.800961494445801
Loss :  1.686099648475647 3.035402774810791 4.721502304077148
Loss :  1.667474389076233 3.1577069759368896 4.825181484222412
Loss :  1.7096061706542969 3.122243642807007 4.831850051879883
Loss :  1.6918503046035767 3.138054609298706 4.829905033111572
Loss :  1.7012348175048828 3.329900026321411 5.031134605407715
Loss :  1.7143932580947876 3.6718173027038574 5.3862104415893555
Loss :  1.6960891485214233 3.3078372478485107 5.0039262771606445
Loss :  1.7064265012741089 3.4686830043792725 5.175109386444092
Loss :  1.65061616897583 3.0357236862182617 4.686339855194092
Loss :  1.7257187366485596 3.3232436180114746 5.048962593078613
Loss :  1.7109445333480835 3.2778749465942383 4.988819599151611
Loss :  1.7354395389556885 3.607039213180542 5.3424787521362305
Loss :  1.6925358772277832 3.5837934017181396 5.276329040527344
  batch 60 loss: 1.6925358772277832, 3.5837934017181396, 5.276329040527344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7009503841400146 3.654176950454712 5.355127334594727
Loss :  1.6900866031646729 3.892261266708374 5.582347869873047
Loss :  1.6883130073547363 3.6181910037994385 5.306504249572754
Loss :  1.704662799835205 3.5555081367492676 5.260170936584473
Loss :  1.6978330612182617 3.215414047241211 4.913247108459473
Loss :  2.4458203315734863 3.8470184803009033 6.292839050292969
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  2.3782308101654053 4.212990760803223 6.591221809387207
Loss :  2.4187214374542236 3.8442749977111816 6.262996673583984
Loss :  2.0748186111450195 3.900089979171753 5.974908828735352
Total LOSS train 4.847543789790227 valid 6.280491590499878
CE LOSS train 1.6968205286906315 valid 0.5187046527862549
Contrastive LOSS train 3.150723277605497 valid 0.9750224947929382
EPOCH 33:
Loss :  1.675742745399475 3.3512980937957764 5.027040958404541
Loss :  1.6921377182006836 3.265770673751831 4.957908630371094
Loss :  1.6662179231643677 3.304121494293213 4.970339298248291
Loss :  1.6736348867416382 3.1896538734436035 4.863288879394531
Loss :  1.7037925720214844 3.2325828075408936 4.936375617980957
Loss :  1.6725506782531738 3.3816871643066406 5.0542378425598145
Loss :  1.7057714462280273 3.416236162185669 5.122007369995117
Loss :  1.682275414466858 3.3218915462493896 5.004167079925537
Loss :  1.6895639896392822 3.151787757873535 4.841351509094238
Loss :  1.6438031196594238 3.1479108333587646 4.791713714599609
Loss :  1.6881352663040161 3.1532082557678223 4.841343402862549
Loss :  1.7602441310882568 2.9342713356018066 4.694515228271484
Loss :  1.705549955368042 2.814000129699707 4.519550323486328
Loss :  1.6895712614059448 2.7380635738372803 4.4276347160339355
Loss :  1.6804841756820679 2.6676900386810303 4.348174095153809
Loss :  1.6855628490447998 2.7453627586364746 4.430925369262695
Loss :  1.6941821575164795 2.989029884338379 4.6832122802734375
Loss :  1.6858330965042114 2.809107780456543 4.494940757751465
Loss :  1.6924046277999878 2.7231428623199463 4.4155473709106445
Loss :  1.661855697631836 2.8571252822875977 4.518980979919434
  batch 20 loss: 1.661855697631836, 2.8571252822875977, 4.518980979919434
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6944807767868042 2.786993980407715 4.481474876403809
Loss :  1.7057794332504272 2.9702377319335938 4.6760172843933105
Loss :  1.687042236328125 2.9229629039764404 4.6100053787231445
Loss :  1.70858895778656 3.5383858680725098 5.246974945068359
Loss :  1.7113510370254517 3.358039140701294 5.069390296936035
Loss :  1.6920504570007324 3.3058292865753174 4.997879981994629
Loss :  1.7213644981384277 3.2366766929626465 4.958041191101074
Loss :  1.6622748374938965 3.0572218894958496 4.719496726989746
Loss :  1.733300805091858 3.142476797103882 4.875777721405029
Loss :  1.6838781833648682 3.429886817932129 5.113764762878418
Loss :  1.7657464742660522 3.5647943019866943 5.330540657043457
Loss :  1.6918002367019653 3.40069317817688 5.092493534088135
Loss :  1.680952787399292 3.1773600578308105 4.858312606811523
Loss :  1.7130826711654663 3.3140265941619873 5.027109146118164
Loss :  1.7383607625961304 3.640956163406372 5.379316806793213
Loss :  1.7330900430679321 3.317136287689209 5.050226211547852
Loss :  1.695447564125061 3.214622735977173 4.910070419311523
Loss :  1.645262598991394 3.1423680782318115 4.787630558013916
Loss :  1.687886118888855 3.1560730934143066 4.843959331512451
Loss :  1.683180809020996 3.268991231918335 4.95217227935791
  batch 40 loss: 1.683180809020996, 3.268991231918335, 4.95217227935791
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6865681409835815 3.245581865310669 4.932149887084961
Loss :  1.6861330270767212 2.9765119552612305 4.662644863128662
Loss :  1.7019351720809937 3.2375216484069824 4.939456939697266
Loss :  1.7067865133285522 3.0913782119750977 4.7981648445129395
Loss :  1.6968731880187988 2.667011260986328 4.363884449005127
Loss :  1.6978328227996826 3.0924150943756104 4.790247917175293
Loss :  1.6730509996414185 2.8710052967071533 4.544056415557861
Loss :  1.6877647638320923 3.1572906970977783 4.84505558013916
Loss :  1.6607608795166016 3.45416259765625 5.114923477172852
Loss :  1.708396315574646 3.3944995403289795 5.102895736694336
Loss :  1.6850523948669434 3.0587379932403564 4.743790626525879
Loss :  1.6978981494903564 3.3040974140167236 5.00199556350708
Loss :  1.7166746854782104 2.8369061946868896 4.5535807609558105
Loss :  1.6964945793151855 3.2046587467193604 4.901153564453125
Loss :  1.7039425373077393 3.0954699516296387 4.799412727355957
Loss :  1.6520657539367676 2.674546241760254 4.3266119956970215
Loss :  1.7267817258834839 2.915332555770874 4.642114162445068
Loss :  1.7086644172668457 3.1361165046691895 4.844780921936035
Loss :  1.7351374626159668 3.4059255123138428 5.1410627365112305
Loss :  1.6909846067428589 2.9038050174713135 4.594789505004883
  batch 60 loss: 1.6909846067428589, 2.9038050174713135, 4.594789505004883
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7055060863494873 3.04366135597229 4.749167442321777
Loss :  1.6897103786468506 2.980987548828125 4.670698165893555
Loss :  1.6935292482376099 2.756709575653076 4.4502387046813965
Loss :  1.7061262130737305 3.482893943786621 5.189020156860352
Loss :  1.697759747505188 2.9054670333862305 4.603226661682129
Loss :  2.2986159324645996 3.7634618282318115 6.062077522277832
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  2.2622463703155518 4.177484512329102 6.439730644226074
Loss :  2.2894513607025146 3.8436312675476074 6.133082389831543
Loss :  1.9816912412643433 3.729037284851074 5.710728645324707
Total LOSS train 4.818908214569092 valid 6.086404800415039
CE LOSS train 1.6953333047720103 valid 0.4954228103160858
Contrastive LOSS train 3.1235749061291034 valid 0.9322593212127686
EPOCH 34:
Loss :  1.6700152158737183 3.452829599380493 5.122844696044922
Loss :  1.703297734260559 3.455568552017212 5.1588664054870605
Loss :  1.674478530883789 3.0076346397399902 4.682113170623779
Loss :  1.673887848854065 2.9496259689331055 4.623513698577881
Loss :  1.703507661819458 2.910722494125366 4.614230155944824
Loss :  1.6751699447631836 3.071934223175049 4.747104167938232
Loss :  1.7018423080444336 3.283306360244751 4.9851484298706055
Loss :  1.683492660522461 3.085031747817993 4.768524169921875
Loss :  1.6873137950897217 2.7692630290985107 4.456576824188232
Loss :  1.6478074789047241 2.736541986465454 4.384349346160889
Loss :  1.691713571548462 2.731109380722046 4.422822952270508
Loss :  1.7577078342437744 2.9530673027038574 4.710775375366211
Loss :  1.7093762159347534 3.107088565826416 4.816464900970459
Loss :  1.6930021047592163 2.970045566558838 4.663047790527344
Loss :  1.6754988431930542 2.9474234580993652 4.622922420501709
Loss :  1.6883186101913452 3.0353715419769287 4.723690032958984
Loss :  1.6989494562149048 2.8898251056671143 4.588774681091309
Loss :  1.6871013641357422 2.9152297973632812 4.602331161499023
Loss :  1.6932674646377563 3.055042028427124 4.74830961227417
Loss :  1.6627968549728394 3.01175594329834 4.674552917480469
  batch 20 loss: 1.6627968549728394, 3.01175594329834, 4.674552917480469
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6914008855819702 3.062399387359619 4.753800392150879
Loss :  1.710021734237671 2.971799373626709 4.681820869445801
Loss :  1.6873607635498047 3.455496072769165 5.142856597900391
Loss :  1.7120590209960938 3.31809663772583 5.030155658721924
Loss :  1.7206599712371826 3.2375457286834717 4.958205699920654
Loss :  1.6966670751571655 3.600054979324341 5.296721935272217
Loss :  1.7240616083145142 3.3214147090911865 5.04547643661499
Loss :  1.6696239709854126 2.8887054920196533 4.5583295822143555
Loss :  1.730437159538269 2.7369368076324463 4.467373847961426
Loss :  1.685813546180725 3.165963888168335 4.85177755355835
Loss :  1.7625181674957275 3.4319980144500732 5.194516181945801
Loss :  1.6975586414337158 3.0564193725585938 4.7539777755737305
Loss :  1.6850064992904663 3.0097031593322754 4.694709777832031
Loss :  1.7068493366241455 3.2604620456695557 4.967311382293701
Loss :  1.7295552492141724 3.2593679428100586 4.988923072814941
Loss :  1.7267576456069946 3.335411787033081 5.062169551849365
Loss :  1.6970560550689697 3.316243886947632 5.013299942016602
Loss :  1.6497374773025513 2.950745105743408 4.60048246383667
Loss :  1.692025065422058 3.497422218322754 5.189447402954102
Loss :  1.6832897663116455 2.9005842208862305 4.583873748779297
  batch 40 loss: 1.6832897663116455, 2.9005842208862305, 4.583873748779297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6828910112380981 3.123959541320801 4.806850433349609
Loss :  1.6771653890609741 2.710073947906494 4.387239456176758
Loss :  1.693814992904663 3.0343658924102783 4.728180885314941
Loss :  1.6967496871948242 3.356193780899048 5.052943229675293
Loss :  1.6914843320846558 3.04052734375 4.732011795043945
Loss :  1.6848481893539429 3.7562029361724854 5.441051006317139
Loss :  1.6680983304977417 3.1922268867492676 4.860325336456299
Loss :  1.6823322772979736 3.2239990234375 4.9063310623168945
Loss :  1.6526615619659424 3.457500696182251 5.110162258148193
Loss :  1.6991305351257324 3.179108142852783 4.878238677978516
Loss :  1.6769152879714966 3.373210906982422 5.050126075744629
Loss :  1.684077262878418 3.5523881912231445 5.2364654541015625
Loss :  1.7056366205215454 3.3568503856658936 5.0624871253967285
Loss :  1.6838223934173584 3.241071939468384 4.924894332885742
Loss :  1.7099764347076416 3.4640114307403564 5.173987865447998
Loss :  1.6456029415130615 3.3496737480163574 4.99527645111084
Loss :  1.7090870141983032 3.630173921585083 5.339261054992676
Loss :  1.6973880529403687 3.5967893600463867 5.294177532196045
Loss :  1.7193191051483154 3.444103240966797 5.163422584533691
Loss :  1.6780210733413696 3.4937450885772705 5.17176628112793
  batch 60 loss: 1.6780210733413696, 3.4937450885772705, 5.17176628112793
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6886358261108398 3.0374794006347656 4.7261152267456055
Loss :  1.6873762607574463 3.2158803939819336 4.903256416320801
Loss :  1.686059832572937 3.3041298389434814 4.990189552307129
Loss :  1.6952226161956787 3.1865322589874268 4.8817548751831055
Loss :  1.6900233030319214 2.572403907775879 4.26242733001709
Loss :  4.054769039154053 4.451018333435059 8.505786895751953
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  4.031449317932129 4.329788684844971 8.361238479614258
Loss :  4.107095241546631 4.2728729248046875 8.379968643188477
Loss :  3.3945565223693848 4.105248928070068 7.499805450439453
Total LOSS train 4.862017462803768 valid 8.186699867248535
CE LOSS train 1.6926668533912073 valid 0.8486391305923462
Contrastive LOSS train 3.1693506204164947 valid 1.026312232017517
EPOCH 35:
Loss :  1.6700994968414307 3.0298571586608887 4.699956893920898
Loss :  1.6818954944610596 3.482311964035034 5.164207458496094
Loss :  1.6738214492797852 3.34738826751709 5.021209716796875
Loss :  1.6783353090286255 3.576592206954956 5.254927635192871
Loss :  1.6942625045776367 3.352813720703125 5.047076225280762
Loss :  1.6753462553024292 2.9963557720184326 4.671701908111572
Loss :  1.6823513507843018 3.268066883087158 4.950418472290039
Loss :  1.674741506576538 2.9768154621124268 4.651556968688965
Loss :  1.6798412799835205 2.48085355758667 4.1606950759887695
Loss :  1.6240208148956299 2.7175559997558594 4.34157657623291
Loss :  1.6819217205047607 2.9985427856445312 4.680464744567871
Loss :  1.755232810974121 3.4294722080230713 5.184704780578613
Loss :  1.7021440267562866 3.19726300239563 4.899406909942627
Loss :  1.6833151578903198 2.952981472015381 4.63629674911499
Loss :  1.6608481407165527 2.789154291152954 4.450002670288086
Loss :  1.6723190546035767 3.228332042694092 4.900650978088379
Loss :  1.6958606243133545 3.3719754219055176 5.067835807800293
Loss :  1.6729720830917358 3.336451292037964 5.00942325592041
Loss :  1.676176905632019 3.3547542095184326 5.030930995941162
Loss :  1.650077223777771 3.150351047515869 4.80042839050293
  batch 20 loss: 1.650077223777771, 3.150351047515869, 4.80042839050293
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.677823781967163 3.2848763465881348 4.962699890136719
Loss :  1.7008445262908936 3.2453348636627197 4.946179389953613
Loss :  1.6693607568740845 2.8129451274871826 4.482306003570557
Loss :  1.7005696296691895 2.996448278427124 4.697017669677734
Loss :  1.700908899307251 3.1449971199035645 4.8459062576293945
Loss :  1.6793547868728638 3.0987205505371094 4.778075218200684
Loss :  1.715160846710205 3.0323691368103027 4.747529983520508
Loss :  1.6570971012115479 2.7788238525390625 4.435920715332031
Loss :  1.7237980365753174 2.650164842605591 4.373962879180908
Loss :  1.6702849864959717 2.649082660675049 4.319367408752441
Loss :  1.7515575885772705 2.8897805213928223 4.641338348388672
Loss :  1.6855189800262451 2.7522895336151123 4.437808513641357
Loss :  1.6736863851547241 2.719714879989624 4.393401145935059
Loss :  1.6983197927474976 2.7461941242218018 4.44451379776001
Loss :  1.727494478225708 2.9776272773742676 4.705121994018555
Loss :  1.7201237678527832 2.764801502227783 4.484925270080566
Loss :  1.688965082168579 2.5157856941223145 4.204751014709473
Loss :  1.638412594795227 2.6075599193573 4.245972633361816
Loss :  1.683245301246643 3.0884828567504883 4.771728038787842
Loss :  1.6751400232315063 3.196840524673462 4.871980667114258
  batch 40 loss: 1.6751400232315063, 3.196840524673462, 4.871980667114258
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6829410791397095 3.492222785949707 5.175163745880127
Loss :  1.6792447566986084 3.1084322929382324 4.787676811218262
Loss :  1.696041464805603 3.1102030277252197 4.806244373321533
Loss :  1.7004514932632446 3.7017250061035156 5.402176380157471
Loss :  1.6909254789352417 3.241580009460449 4.9325056076049805
Loss :  1.6887257099151611 3.413203001022339 5.1019287109375
Loss :  1.6689989566802979 3.7836599349975586 5.452658653259277
Loss :  1.6862812042236328 3.8066985607147217 5.492980003356934
Loss :  1.652714490890503 3.5593960285186768 5.21211051940918
Loss :  1.710637092590332 3.3885204792022705 5.099157333374023
Loss :  1.6881186962127686 3.1979756355285645 4.886094093322754
Loss :  1.6867787837982178 3.5322678089141846 5.219046592712402
Loss :  1.7110015153884888 3.558893918991089 5.269895553588867
Loss :  1.7099685668945312 3.314218044281006 5.024186611175537
Loss :  1.7144100666046143 3.2683615684509277 4.982771873474121
Loss :  1.6624795198440552 3.3708372116088867 5.033316612243652
Loss :  1.7219066619873047 3.302959442138672 5.024866104125977
Loss :  1.7039787769317627 3.4048659801483154 5.108844757080078
Loss :  1.7277734279632568 3.59416127204895 5.321934700012207
Loss :  1.7009357213974 2.8549370765686035 4.555872917175293
  batch 60 loss: 1.7009357213974, 2.8549370765686035, 4.555872917175293
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7022547721862793 2.7705190181732178 4.472773551940918
Loss :  1.6978142261505127 3.023118734359741 4.720932960510254
Loss :  1.69481360912323 2.81150221824646 4.5063157081604
Loss :  1.6970205307006836 3.04795241355896 4.744973182678223
Loss :  1.688400387763977 2.314880132675171 4.0032806396484375
Loss :  2.139324903488159 3.9734947681427 6.112819671630859
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  2.0789737701416016 4.236414432525635 6.315388202667236
Loss :  2.1088154315948486 4.0804243087768555 6.189239501953125
Loss :  1.8661556243896484 3.9395217895507812 5.80567741394043
Total LOSS train 4.81156441615178 valid 6.105781197547913
CE LOSS train 1.6890441160935623 valid 0.4665389060974121
Contrastive LOSS train 3.1225203073941743 valid 0.9848804473876953
EPOCH 36:
Loss :  1.684565782546997 2.5836241245269775 4.268189907073975
Loss :  1.6931636333465576 2.8296401500701904 4.522803783416748
Loss :  1.675577163696289 2.6726036071777344 4.348180770874023
Loss :  1.682599663734436 2.7294809818267822 4.412080764770508
Loss :  1.70579195022583 2.937814235687256 4.643606185913086
Loss :  1.6851701736450195 2.8870365619659424 4.572206497192383
Loss :  1.696974277496338 3.168911933898926 4.865886211395264
Loss :  1.6802983283996582 3.154437780380249 4.834735870361328
Loss :  1.6875619888305664 3.1043853759765625 4.791947364807129
Loss :  1.6422559022903442 3.4500725269317627 5.0923285484313965
Loss :  1.6875579357147217 3.360750675201416 5.048308372497559
Loss :  1.7493525743484497 3.139183759689331 4.88853645324707
Loss :  1.696852684020996 3.1497719287872314 4.846624374389648
Loss :  1.68764328956604 3.018021583557129 4.70566463470459
Loss :  1.6705321073532104 3.256425142288208 4.926957130432129
Loss :  1.6786096096038818 2.976123094558716 4.654732704162598
Loss :  1.6854076385498047 2.789034366607666 4.474442005157471
Loss :  1.6830416917800903 2.970017671585083 4.653059482574463
Loss :  1.6876803636550903 2.5883138179779053 4.275994300842285
Loss :  1.6613746881484985 2.82118558883667 4.482560157775879
  batch 20 loss: 1.6613746881484985, 2.82118558883667, 4.482560157775879
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.694729208946228 3.240212917327881 4.934942245483398
Loss :  1.7043373584747314 3.44348406791687 5.147821426391602
Loss :  1.6876298189163208 3.20316743850708 4.890797138214111
Loss :  1.708132266998291 3.4181289672851562 5.126261234283447
Loss :  1.7153998613357544 3.6240036487579346 5.3394036293029785
Loss :  1.690047264099121 3.003068447113037 4.693115711212158
Loss :  1.7142672538757324 3.2943711280822754 5.008638381958008
Loss :  1.6586838960647583 2.926732063293457 4.585415840148926
Loss :  1.7191917896270752 2.8418021202087402 4.5609941482543945
Loss :  1.6733540296554565 3.0424928665161133 4.715847015380859
Loss :  1.7539740800857544 3.371825933456421 5.125800132751465
Loss :  1.6831241846084595 3.3047752380371094 4.987899303436279
Loss :  1.67197585105896 3.4283711910247803 5.10034704208374
Loss :  1.6931259632110596 3.2430851459503174 4.936211109161377
Loss :  1.7218815088272095 3.2218360900878906 4.9437174797058105
Loss :  1.7172869443893433 3.3581430912017822 5.075429916381836
Loss :  1.683443307876587 3.4460973739624023 5.12954044342041
Loss :  1.6375051736831665 3.609837770462036 5.247343063354492
Loss :  1.6696969270706177 3.1562724113464355 4.825969219207764
Loss :  1.6722325086593628 2.951462507247925 4.623694896697998
  batch 40 loss: 1.6722325086593628, 2.951462507247925, 4.623694896697998
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.674759030342102 2.5643343925476074 4.23909330368042
Loss :  1.6637146472930908 2.4238195419311523 4.087533950805664
Loss :  1.6907789707183838 2.7518954277038574 4.44267463684082
Loss :  1.6839160919189453 2.456157922744751 4.140073776245117
Loss :  1.6868261098861694 2.4136157035827637 4.100441932678223
Loss :  1.6877844333648682 3.338609218597412 5.026393890380859
Loss :  1.664736270904541 3.4031639099121094 5.06790018081665
Loss :  1.6770237684249878 2.9861907958984375 4.663214683532715
Loss :  1.6525338888168335 3.2753915786743164 4.9279255867004395
Loss :  1.6976125240325928 3.4897263050079346 5.187338829040527
Loss :  1.6701648235321045 3.545436143875122 5.215600967407227
Loss :  1.6845098733901978 3.220493793487549 4.905003547668457
Loss :  1.7028937339782715 3.3295984268188477 5.032492160797119
Loss :  1.6841446161270142 3.0486838817596436 4.732828617095947
Loss :  1.697692632675171 3.291372060775757 4.989064693450928
Loss :  1.6495331525802612 2.9432389736175537 4.592772006988525
Loss :  1.7145015001296997 3.5457513332366943 5.260252952575684
Loss :  1.7001579999923706 3.216200590133667 4.916358470916748
Loss :  1.7225810289382935 3.276432752609253 4.999013900756836
Loss :  1.6876367330551147 2.8376221656799316 4.525259017944336
  batch 60 loss: 1.6876367330551147, 2.8376221656799316, 4.525259017944336
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6981784105300903 3.1053483486175537 4.803526878356934
Loss :  1.6878832578659058 3.4455435276031494 5.133426666259766
Loss :  1.6909536123275757 2.8458282947540283 4.5367817878723145
Loss :  1.6984463930130005 3.4741451740264893 5.172591686248779
Loss :  1.6920288801193237 3.067063093185425 4.759091854095459
Loss :  2.229557991027832 3.961554765701294 6.191112518310547
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  2.183486223220825 4.238861560821533 6.4223480224609375
Loss :  2.220245122909546 4.00941276550293 6.229658126831055
Loss :  1.888821005821228 3.9761815071105957 5.865002632141113
Total LOSS train 4.796379859630878 valid 6.177030324935913
CE LOSS train 1.6885080465903648 valid 0.472205251455307
Contrastive LOSS train 3.1078718258784366 valid 0.9940453767776489
EPOCH 37:
Loss :  1.6711857318878174 2.951368808746338 4.622554779052734
Loss :  1.697434902191162 3.7363457679748535 5.433780670166016
Loss :  1.6741546392440796 3.545474052429199 5.219628810882568
Loss :  1.672714114189148 3.065023899078369 4.737738132476807
Loss :  1.6993350982666016 2.9748685359954834 4.674203872680664
Loss :  1.674835443496704 2.7693910598754883 4.444226264953613
Loss :  1.6956725120544434 2.73453426361084 4.430206775665283
Loss :  1.6754308938980103 2.574158191680908 4.249588966369629
Loss :  1.6804856061935425 2.659299373626709 4.339785099029541
Loss :  1.638786792755127 2.4336483478546143 4.07243537902832
Loss :  1.6831337213516235 2.819138526916504 4.502272129058838
Loss :  1.7517037391662598 3.0252439975738525 4.776947975158691
Loss :  1.6969283819198608 3.2046310901641846 4.901559352874756
Loss :  1.683508038520813 3.0765395164489746 4.760047435760498
Loss :  1.667606234550476 3.078749418258667 4.7463555335998535
Loss :  1.6788462400436401 3.235067844390869 4.913914203643799
Loss :  1.687610149383545 2.9809632301330566 4.668573379516602
Loss :  1.6792861223220825 2.906189203262329 4.585475444793701
Loss :  1.68012273311615 2.6794896125793457 4.359612464904785
Loss :  1.6588385105133057 3.5810179710388184 5.239856719970703
  batch 20 loss: 1.6588385105133057, 3.5810179710388184, 5.239856719970703
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6838977336883545 3.5701348781585693 5.254032611846924
Loss :  1.698514461517334 3.564779758453369 5.263294219970703
Loss :  1.6814734935760498 3.2326180934906006 4.91409158706665
Loss :  1.700709342956543 3.582230806350708 5.282939910888672
Loss :  1.7051409482955933 3.690417528152466 5.3955583572387695
Loss :  1.6885888576507568 3.3195314407348633 5.008120536804199
Loss :  1.718286156654358 3.0957188606262207 4.814004898071289
Loss :  1.6615980863571167 3.073310613632202 4.734908580780029
Loss :  1.726879358291626 3.0605287551879883 4.787407875061035
Loss :  1.6740643978118896 3.1971395015716553 4.871203899383545
Loss :  1.75474214553833 3.1238303184509277 4.878572463989258
Loss :  1.6854933500289917 3.067835807800293 4.753329277038574
Loss :  1.6699644327163696 3.1778676509857178 4.847832202911377
Loss :  1.7004826068878174 2.9762306213378906 4.676712989807129
Loss :  1.728938102722168 3.2044837474823 4.933422088623047
Loss :  1.7195104360580444 2.6862757205963135 4.405786037445068
Loss :  1.689605712890625 2.98042893409729 4.670034408569336
Loss :  1.6377694606781006 2.549919605255127 4.187688827514648
Loss :  1.6870650053024292 2.8563148975372314 4.543379783630371
Loss :  1.6799662113189697 3.515127658843994 5.195094108581543
  batch 40 loss: 1.6799662113189697, 3.515127658843994, 5.195094108581543
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.686684012413025 3.6853647232055664 5.372048854827881
Loss :  1.691389799118042 3.179785966873169 4.871175765991211
Loss :  1.7096128463745117 3.182729482650757 4.892342567443848
Loss :  1.7043585777282715 3.0796899795532227 4.784048557281494
Loss :  1.6961445808410645 3.6244423389434814 5.320587158203125
Loss :  1.6929773092269897 3.333540916442871 5.02651834487915
Loss :  1.6687088012695312 3.175997495651245 4.8447065353393555
Loss :  1.692915916442871 3.356421947479248 5.049337863922119
Loss :  1.6476256847381592 2.951873302459717 4.599498748779297
Loss :  1.7154403924942017 3.14090633392334 4.856346607208252
Loss :  1.6902517080307007 2.8818135261535645 4.572065353393555
Loss :  1.6898565292358398 2.823523759841919 4.51338005065918
Loss :  1.7071486711502075 3.1850743293762207 4.892222881317139
Loss :  1.702840805053711 2.8631715774536133 4.566012382507324
Loss :  1.7101365327835083 2.963676691055298 4.673813343048096
Loss :  1.649371862411499 3.139474630355835 4.788846492767334
Loss :  1.7156476974487305 3.403569459915161 5.1192169189453125
Loss :  1.697638988494873 3.222716808319092 4.920355796813965
Loss :  1.7241617441177368 3.2434871196746826 4.967648983001709
Loss :  1.6886253356933594 3.433992624282837 5.122617721557617
  batch 60 loss: 1.6886253356933594, 3.433992624282837, 5.122617721557617
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6934165954589844 3.203176975250244 4.8965935707092285
Loss :  1.6858230829238892 3.6517398357391357 5.3375630378723145
Loss :  1.687223196029663 3.153531312942505 4.840754508972168
Loss :  1.6964638233184814 2.922257661819458 4.6187214851379395
Loss :  1.6857810020446777 2.698275566101074 4.384056568145752
Loss :  2.224824905395508 3.9546854496002197 6.179510116577148
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  2.2150955200195312 4.300374507904053 6.515470027923584
Loss :  2.2385363578796387 3.9321324825286865 6.170668601989746
Loss :  1.9277817010879517 3.8446741104125977 5.77245569229126
Total LOSS train 4.8142562792851376 valid 6.159526109695435
CE LOSS train 1.690316236936129 valid 0.4819454252719879
Contrastive LOSS train 3.123940035013052 valid 0.9611685276031494
EPOCH 38:
Loss :  1.6670639514923096 3.8736417293548584 5.540705680847168
Loss :  1.6895804405212402 3.659421682357788 5.349001884460449
Loss :  1.6614010334014893 3.35819149017334 5.01959228515625
Loss :  1.6568111181259155 3.274566888809204 4.93137788772583
Loss :  1.6965444087982178 2.763732671737671 4.460277080535889
Loss :  1.6667810678482056 2.895015239715576 4.561796188354492
Loss :  1.692198395729065 2.977158308029175 4.669356822967529
Loss :  1.6731982231140137 2.939943552017212 4.613142013549805
Loss :  1.6771125793457031 3.161360025405884 4.838472366333008
Loss :  1.631736397743225 3.2173714637756348 4.84910774230957
Loss :  1.6803257465362549 3.408053398132324 5.08837890625
Loss :  1.7469170093536377 3.5181679725646973 5.265085220336914
Loss :  1.6930491924285889 3.5517125129699707 5.2447614669799805
Loss :  1.6801223754882812 3.610246181488037 5.290368556976318
Loss :  1.662401795387268 3.480891227722168 5.1432929039001465
Loss :  1.668993592262268 3.4081509113311768 5.077144622802734
Loss :  1.6831109523773193 3.172081470489502 4.855192184448242
Loss :  1.6666901111602783 3.71516489982605 5.381855010986328
Loss :  1.6731022596359253 3.451312780380249 5.124414920806885
Loss :  1.6513222455978394 3.014936685562134 4.666258811950684
  batch 20 loss: 1.6513222455978394, 3.014936685562134, 4.666258811950684
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6807079315185547 3.1393511295318604 4.820058822631836
Loss :  1.6942640542984009 3.2996363639831543 4.993900299072266
Loss :  1.6710819005966187 3.039024591445923 4.710106372833252
Loss :  1.702046513557434 2.7064623832702637 4.408508777618408
Loss :  1.7027255296707153 3.342437505722046 5.045163154602051
Loss :  1.6801478862762451 2.7670071125030518 4.447154998779297
Loss :  1.718453049659729 3.2299392223358154 4.948392391204834
Loss :  1.6634401082992554 2.5672755241394043 4.230715751647949
Loss :  1.7275230884552002 2.6053357124328613 4.332859039306641
Loss :  1.6721333265304565 3.101025342941284 4.773158550262451
Loss :  1.7501472234725952 2.741574287414551 4.4917216300964355
Loss :  1.6922928094863892 2.875415563583374 4.567708492279053
Loss :  1.6795247793197632 2.5925400257110596 4.272064685821533
Loss :  1.700632929801941 2.9500272274017334 4.650660037994385
Loss :  1.7287927865982056 3.0657012462615967 4.794494152069092
Loss :  1.722531795501709 3.13502836227417 4.857560157775879
Loss :  1.693288803100586 2.7980592250823975 4.4913482666015625
Loss :  1.6606093645095825 2.7515413761138916 4.412150859832764
Loss :  1.6877059936523438 2.4855287075042725 4.173234939575195
Loss :  1.6849761009216309 2.9583706855773926 4.643346786499023
  batch 40 loss: 1.6849761009216309, 2.9583706855773926, 4.643346786499023
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.686589241027832 3.091954469680786 4.778543472290039
Loss :  1.6849122047424316 2.523088216781616 4.208000183105469
Loss :  1.696948766708374 2.7941479682922363 4.491096496582031
Loss :  1.701931118965149 2.8687751293182373 4.570706367492676
Loss :  1.6907386779785156 2.745542287826538 4.436281204223633
Loss :  1.690938115119934 3.20833158493042 4.8992695808410645
Loss :  1.6740108728408813 2.7665343284606934 4.440545082092285
Loss :  1.6856904029846191 3.0886099338531494 4.774300575256348
Loss :  1.6618201732635498 3.4758806228637695 5.137701034545898
Loss :  1.7095348834991455 3.0500056743621826 4.759540557861328
Loss :  1.6866780519485474 2.9982686042785645 4.684946537017822
Loss :  1.6963272094726562 2.9798407554626465 4.676167964935303
Loss :  1.709384560585022 3.1060616970062256 4.815446376800537
Loss :  1.6937191486358643 3.007534980773926 4.701253890991211
Loss :  1.7068207263946533 3.0551552772521973 4.76197624206543
Loss :  1.6585017442703247 2.8384885787963867 4.496990203857422
Loss :  1.715964674949646 2.9128785133361816 4.628843307495117
Loss :  1.7020165920257568 3.3249166011810303 5.026933193206787
Loss :  1.7307085990905762 3.1320383548736572 4.8627471923828125
Loss :  1.7008644342422485 2.974503993988037 4.675368309020996
  batch 60 loss: 1.7008644342422485, 2.974503993988037, 4.675368309020996
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6957499980926514 3.2093310356140137 4.905080795288086
Loss :  1.6919811964035034 2.960529088973999 4.652510166168213
Loss :  1.6934728622436523 3.4692575931549072 5.1627302169799805
Loss :  1.6968454122543335 3.5931386947631836 5.289984226226807
Loss :  1.6854615211486816 3.0758438110351562 4.761305332183838
Loss :  2.2532241344451904 3.7766315937042236 6.029855728149414
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  2.2341389656066895 4.04349422454834 6.277633190155029
Loss :  2.2488889694213867 3.7033729553222656 5.952261924743652
Loss :  1.9220044612884521 3.720720052719116 5.642724514007568
Total LOSS train 4.778956295893742 valid 5.975618839263916
CE LOSS train 1.68890923169943 valid 0.48050111532211304
Contrastive LOSS train 3.090047084368192 valid 0.930180013179779
EPOCH 39:
Loss :  1.6780506372451782 3.1163482666015625 4.794398784637451
Loss :  1.6833475828170776 3.4974379539489746 5.180785655975342
Loss :  1.6638554334640503 3.0215821266174316 4.6854376792907715
Loss :  1.6696569919586182 2.925854444503784 4.595511436462402
Loss :  1.7003809213638306 3.2824018001556396 4.98278284072876
Loss :  1.6740959882736206 3.50166392326355 5.175759792327881
Loss :  1.6908544301986694 3.2237231731414795 4.914577484130859
Loss :  1.6703022718429565 3.1577961444854736 4.828098297119141
Loss :  1.6807317733764648 3.238990545272827 4.919722557067871
Loss :  1.633294939994812 2.8805949687957764 4.513889789581299
Loss :  1.6762349605560303 3.232659101486206 4.908894062042236
Loss :  1.751044750213623 3.3668854236602783 5.1179304122924805
Loss :  1.691947340965271 3.249558925628662 4.941506385803223
Loss :  1.678725242614746 3.2145750522613525 4.8933000564575195
Loss :  1.6725190877914429 3.6322543621063232 5.304773330688477
Loss :  1.6635991334915161 3.734995126724243 5.398594379425049
Loss :  1.6764825582504272 3.532527208328247 5.209009647369385
Loss :  1.664358139038086 3.5478177070617676 5.2121758460998535
Loss :  1.6745548248291016 3.1811232566833496 4.855678081512451
Loss :  1.6490519046783447 3.0424063205718994 4.691458225250244
  batch 20 loss: 1.6490519046783447, 3.0424063205718994, 4.691458225250244
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6853667497634888 2.660266637802124 4.345633506774902
Loss :  1.698339581489563 2.7354161739349365 4.433755874633789
Loss :  1.6760485172271729 2.5012974739074707 4.177346229553223
Loss :  1.7092260122299194 2.629709482192993 4.338935375213623
Loss :  1.706891655921936 2.978851318359375 4.6857428550720215
Loss :  1.6807674169540405 2.9412648677825928 4.622032165527344
Loss :  1.7105375528335571 2.911123752593994 4.621661186218262
Loss :  1.6604995727539062 2.961411476135254 4.62191104888916
Loss :  1.7160794734954834 3.0398268699645996 4.755906105041504
Loss :  1.6666709184646606 3.0843281745910645 4.7509989738464355
Loss :  1.7440701723098755 3.2323734760284424 4.976443767547607
Loss :  1.6850799322128296 2.8329567909240723 4.518036842346191
Loss :  1.670800805091858 2.6019699573516846 4.272770881652832
Loss :  1.6851662397384644 2.968794584274292 4.653960704803467
Loss :  1.720257043838501 3.342128038406372 5.062385082244873
Loss :  1.7080222368240356 3.1221864223480225 4.830208778381348
Loss :  1.6832174062728882 3.2368855476379395 4.920103073120117
Loss :  1.6421743631362915 3.1796610355377197 4.821835517883301
Loss :  1.6804099082946777 2.9062464237213135 4.58665657043457
Loss :  1.6737370491027832 3.01139235496521 4.685129165649414
  batch 40 loss: 1.6737370491027832, 3.01139235496521, 4.685129165649414
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6810340881347656 2.7403981685638428 4.4214324951171875
Loss :  1.6689196825027466 2.662390947341919 4.331310749053955
Loss :  1.693324327468872 3.275146245956421 4.968470573425293
Loss :  1.687813401222229 3.3412020206451416 5.02901554107666
Loss :  1.6858785152435303 2.950619697570801 4.63649845123291
Loss :  1.6799782514572144 2.953702449798584 4.633680820465088
Loss :  1.6599560976028442 3.064642906188965 4.7245988845825195
Loss :  1.6738674640655518 3.624293088912964 5.298160552978516
Loss :  1.6435667276382446 3.5353331565856934 5.178899765014648
Loss :  1.6936777830123901 3.5683350563049316 5.262012958526611
Loss :  1.6699820756912231 3.5748021602630615 5.244784355163574
Loss :  1.682228684425354 3.4395835399627686 5.121812343597412
Loss :  1.6969578266143799 3.6491265296936035 5.3460845947265625
Loss :  1.6813971996307373 3.8002569675445557 5.481654167175293
Loss :  1.7013400793075562 3.6117517948150635 5.31309175491333
Loss :  1.6436190605163574 3.417815685272217 5.061434745788574
Loss :  1.7091017961502075 3.5744616985321045 5.283563613891602
Loss :  1.6982132196426392 3.794090986251831 5.49230432510376
Loss :  1.7192670106887817 3.7227694988250732 5.4420366287231445
Loss :  1.7001211643218994 4.063854694366455 5.763976097106934
  batch 60 loss: 1.7001211643218994, 4.063854694366455, 5.763976097106934
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.681371808052063 4.284519672393799 5.965891361236572
Loss :  1.68397057056427 3.7680602073669434 5.452030658721924
Loss :  1.6849756240844727 3.7499327659606934 5.434908390045166
Loss :  1.6950770616531372 3.6595447063446045 5.354621887207031
Loss :  1.6808724403381348 3.2432141304016113 4.924086570739746
Loss :  4.294092655181885 4.213338851928711 8.507431030273438
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  4.194497108459473 4.3676862716674805 8.562183380126953
Loss :  4.363378047943115 4.047485828399658 8.410863876342773
Loss :  3.467318058013916 4.15703010559082 7.624348163604736
Total LOSS train 4.938032318995549 valid 8.276206612586975
CE LOSS train 1.683737899706914 valid 0.866829514503479
Contrastive LOSS train 3.2542943917787994 valid 1.039257526397705
EPOCH 40:
Loss :  1.673554539680481 3.8270108699798584 5.500565528869629
Loss :  1.6835232973098755 3.339792013168335 5.0233154296875
Loss :  1.6574207544326782 3.2246196269989014 4.882040500640869
Loss :  1.652394413948059 3.600294351577759 5.252688884735107
Loss :  1.6946749687194824 3.9747817516326904 5.669456481933594
Loss :  1.6596834659576416 3.5970633029937744 5.256746768951416
Loss :  1.693487286567688 3.8588337898254395 5.552320957183838
Loss :  1.6743723154067993 3.151432991027832 4.825805187225342
Loss :  1.67746102809906 2.6678881645202637 4.345349311828613
Loss :  1.6419655084609985 2.9280545711517334 4.5700201988220215
Loss :  1.6744402647018433 2.907426118850708 4.581866264343262
Loss :  1.743695855140686 3.1141276359558105 4.857823371887207
Loss :  1.6898361444473267 3.087045431137085 4.776881694793701
Loss :  1.6808149814605713 2.993903875350952 4.674718856811523
Loss :  1.6726529598236084 2.762946605682373 4.435599327087402
Loss :  1.6692614555358887 2.900235414505005 4.569497108459473
Loss :  1.6781790256500244 2.6079440116882324 4.286123275756836
Loss :  1.6775623559951782 2.7169859409332275 4.394548416137695
Loss :  1.6718473434448242 2.6106443405151367 4.282491683959961
Loss :  1.649857759475708 2.8837125301361084 4.533570289611816
  batch 20 loss: 1.649857759475708, 2.8837125301361084, 4.533570289611816
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6843161582946777 3.587475538253784 5.271791458129883
Loss :  1.6968297958374023 3.4512062072753906 5.148036003112793
Loss :  1.6776710748672485 3.0055201053619385 4.683191299438477
Loss :  1.696588397026062 3.037825107574463 4.7344136238098145
Loss :  1.6992610692977905 3.244774580001831 4.944035530090332
Loss :  1.6832810640335083 3.202563762664795 4.885844707489014
Loss :  1.7181031703948975 3.304246664047241 5.022349834442139
Loss :  1.662282943725586 3.4324827194213867 5.094765663146973
Loss :  1.719185471534729 2.9083642959594727 4.627549648284912
Loss :  1.6664828062057495 3.3728017807006836 5.039284706115723
Loss :  1.746796727180481 3.7917730808258057 5.538569927215576
Loss :  1.6909739971160889 3.2706706523895264 4.961644649505615
Loss :  1.6719039678573608 3.4487366676330566 5.120640754699707
Loss :  1.6937288045883179 3.4376065731048584 5.131335258483887
Loss :  1.717647671699524 3.564610719680786 5.2822585105896
Loss :  1.7176077365875244 3.858546733856201 5.576154708862305
Loss :  1.692140817642212 3.7368290424346924 5.428969860076904
Loss :  1.6516189575195312 3.6446330547332764 5.296252250671387
Loss :  1.680227518081665 3.493431568145752 5.173659324645996
Loss :  1.6803306341171265 3.2335433959960938 4.91387414932251
  batch 40 loss: 1.6803306341171265, 3.2335433959960938, 4.91387414932251
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6772634983062744 3.4055521488189697 5.082815647125244
Loss :  1.6686464548110962 3.049830198287964 4.71847677230835
Loss :  1.6835633516311646 2.867565870285034 4.551129341125488
Loss :  1.6856393814086914 2.9833438396453857 4.668983459472656
Loss :  1.6806962490081787 2.7148540019989014 4.39555025100708
Loss :  1.6847965717315674 2.9386825561523438 4.623478889465332
Loss :  1.671173095703125 2.845897674560547 4.517070770263672
Loss :  1.6707842350006104 3.0871729850769043 4.757957458496094
Loss :  1.6577309370040894 2.623720645904541 4.28145170211792
Loss :  1.691662311553955 2.783154010772705 4.47481632232666
Loss :  1.6673364639282227 2.8016040325164795 4.468940734863281
Loss :  1.6811023950576782 2.655815362930298 4.336917877197266
Loss :  1.7041800022125244 2.8521931171417236 4.556373119354248
Loss :  1.680996060371399 2.7768707275390625 4.457866668701172
Loss :  1.6905146837234497 3.271329641342163 4.961844444274902
Loss :  1.6515592336654663 2.999023914337158 4.650583267211914
Loss :  1.7161606550216675 3.3055384159088135 5.021698951721191
Loss :  1.7043836116790771 3.020134925842285 4.724518775939941
Loss :  1.7249910831451416 3.315558671951294 5.0405497550964355
Loss :  1.6863460540771484 2.975942373275757 4.662288665771484
  batch 60 loss: 1.6863460540771484, 2.975942373275757, 4.662288665771484
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7095204591751099 3.5038466453552246 5.213366985321045
Loss :  1.6879085302352905 3.471297025680542 5.159205436706543
Loss :  1.7003980875015259 3.615091562271118 5.315489768981934
Loss :  1.697908878326416 3.6987197399139404 5.396628379821777
Loss :  1.6872197389602661 2.99606990814209 4.683289527893066
Loss :  3.3720171451568604 4.246098518371582 7.618115425109863
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  3.340698719024658 4.293740749359131 7.634439468383789
Loss :  3.441866159439087 4.0821428298950195 7.524008750915527
Loss :  2.8085944652557373 3.893052577972412 6.70164680480957
Total LOSS train 4.874882221221924 valid 7.3695526123046875
CE LOSS train 1.6850176389400775 valid 0.7021486163139343
Contrastive LOSS train 3.189864547436054 valid 0.973263144493103
EPOCH 41:
Loss :  1.6613789796829224 3.0786027908325195 4.739981651306152
Loss :  1.6966179609298706 3.3738083839416504 5.0704264640808105
Loss :  1.6637905836105347 3.371006488800049 5.034797191619873
Loss :  1.659281611442566 3.522364377975464 5.18164587020874
Loss :  1.690248966217041 3.3786613941192627 5.068910598754883
Loss :  1.6669193506240845 3.47312331199646 5.140042781829834
Loss :  1.685854434967041 3.5305140018463135 5.216368675231934
Loss :  1.6701964139938354 2.892516613006592 4.562713146209717
Loss :  1.6766921281814575 2.7535057067871094 4.430197715759277
Loss :  1.6333270072937012 2.508133888244629 4.14146089553833
Loss :  1.6826560497283936 3.1065709590911865 4.78922700881958
Loss :  1.7467397451400757 2.889478921890259 4.636218547821045
Loss :  1.6982042789459229 3.280380964279175 4.978585243225098
Loss :  1.6894890069961548 2.813105583190918 4.502594470977783
Loss :  1.6655439138412476 3.1173300743103027 4.78287410736084
Loss :  1.6776330471038818 2.7681190967559814 4.445752143859863
Loss :  1.687852144241333 2.8734548091888428 4.561306953430176
Loss :  1.6778602600097656 3.0507352352142334 4.728595733642578
Loss :  1.6778528690338135 2.9638512134552 4.641704082489014
Loss :  1.6537851095199585 3.3110334873199463 4.964818477630615
  batch 20 loss: 1.6537851095199585, 3.3110334873199463, 4.964818477630615
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6824105978012085 3.2510812282562256 4.9334917068481445
Loss :  1.6927825212478638 3.2402658462524414 4.933048248291016
Loss :  1.6757732629776 3.2446181774139404 4.92039155960083
Loss :  1.699058175086975 2.8273942470550537 4.526452541351318
Loss :  1.7026058435440063 2.9453213214874268 4.647927284240723
Loss :  1.6838268041610718 2.8190786838531494 4.502905368804932
Loss :  1.7145694494247437 3.5548596382141113 5.2694292068481445
Loss :  1.6607705354690552 3.518137216567993 5.178907871246338
Loss :  1.7189996242523193 3.2088825702667236 4.927882194519043
Loss :  1.670670509338379 3.1250953674316406 4.7957658767700195
Loss :  1.749655842781067 3.0903146266937256 4.839970588684082
Loss :  1.684930443763733 2.7238705158233643 4.408801078796387
Loss :  1.6708627939224243 2.547295570373535 4.21815824508667
Loss :  1.691918969154358 2.925692081451416 4.617610931396484
Loss :  1.7171412706375122 3.1611218452453613 4.878262996673584
Loss :  1.7174570560455322 3.6141226291656494 5.331579685211182
Loss :  1.6873891353607178 3.4882071018218994 5.175596237182617
Loss :  1.644545078277588 3.3225150108337402 4.967060089111328
Loss :  1.6772667169570923 3.3490400314331055 5.026306629180908
Loss :  1.6741913557052612 3.1702640056610107 4.844455242156982
  batch 40 loss: 1.6741913557052612, 3.1702640056610107, 4.844455242156982
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6737008094787598 3.110023021697998 4.783723831176758
Loss :  1.666617512702942 3.083735704421997 4.7503533363342285
Loss :  1.6834794282913208 3.0705597400665283 4.754039287567139
Loss :  1.6793280839920044 3.092932939529419 4.772261142730713
Loss :  1.6785115003585815 2.9241690635681152 4.602680683135986
Loss :  1.6804476976394653 3.0911262035369873 4.771574020385742
Loss :  1.6624093055725098 2.8516671657562256 4.514076232910156
Loss :  1.6707757711410522 2.6082253456115723 4.279001235961914
Loss :  1.6469252109527588 2.971173048019409 4.618098258972168
Loss :  1.6922168731689453 3.2111599445343018 4.903376579284668
Loss :  1.6667840480804443 3.16283917427063 4.829623222351074
Loss :  1.6760503053665161 3.1864993572235107 4.862549781799316
Loss :  1.6994117498397827 3.223945140838623 4.923357009887695
Loss :  1.6743513345718384 2.984274387359619 4.658625602722168
Loss :  1.6904473304748535 3.3413279056549072 5.03177547454834
Loss :  1.647382378578186 2.6361348628997803 4.283517360687256
Loss :  1.7079483270645142 3.2415432929992676 4.949491500854492
Loss :  1.691908836364746 3.173163414001465 4.865072250366211
Loss :  1.7179675102233887 3.324922800064087 5.042890548706055
Loss :  1.6845248937606812 3.730609655380249 5.415134429931641
  batch 60 loss: 1.6845248937606812, 3.730609655380249, 5.415134429931641
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.7089793682098389 3.546564817428589 5.255544185638428
Loss :  1.6880249977111816 3.262155055999756 4.9501800537109375
Loss :  1.7024009227752686 3.540693998336792 5.2430949211120605
Loss :  1.6953202486038208 3.5052552223205566 5.200575351715088
Loss :  1.6811600923538208 2.7437520027160645 4.424911975860596
Loss :  3.763329029083252 4.489079475402832 8.252408981323242
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  3.6626875400543213 4.304502964019775 7.967190742492676
Loss :  3.8225741386413574 4.300493240356445 8.123067855834961
Loss :  3.0855653285980225 4.12204647064209 7.207612037658691
Total LOSS train 4.819196209540734 valid 7.887569904327393
CE LOSS train 1.6837819136106051 valid 0.7713913321495056
Contrastive LOSS train 3.135414281258216 valid 1.0305116176605225
EPOCH 42:
Loss :  1.660915493965149 2.965725898742676 4.626641273498535
Loss :  1.6957249641418457 3.073246717453003 4.7689714431762695
Loss :  1.6646708250045776 3.0460174083709717 4.71068811416626
Loss :  1.6616630554199219 3.4052393436431885 5.066902160644531
Loss :  1.693033218383789 3.5142242908477783 5.207257270812988
Loss :  1.6711572408676147 3.070866107940674 4.742023468017578
Loss :  1.6864337921142578 3.2385470867156982 4.924981117248535
Loss :  1.6712197065353394 2.511333703994751 4.182553291320801
Loss :  1.6751575469970703 2.2120182514190674 3.8871757984161377
Loss :  1.6309151649475098 2.4549949169158936 4.085909843444824
Loss :  1.6789393424987793 2.8077151775360107 4.486654281616211
Loss :  1.7446575164794922 3.3146660327911377 5.059323310852051
Loss :  1.6895651817321777 3.2474477291107178 4.937012672424316
Loss :  1.6835087537765503 3.2165589332580566 4.9000678062438965
Loss :  1.662286400794983 2.984103202819824 4.646389484405518
Loss :  1.6637678146362305 3.117178201675415 4.780945777893066
Loss :  1.676592469215393 3.0543668270111084 4.730959415435791
Loss :  1.6728456020355225 3.2744204998016357 4.947266101837158
Loss :  1.6700410842895508 3.1056649684906006 4.7757062911987305
Loss :  1.6486549377441406 3.1058878898620605 4.754542827606201
  batch 20 loss: 1.6486549377441406, 3.1058878898620605, 4.754542827606201
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6825284957885742 2.911757230758667 4.59428596496582
Loss :  1.6949690580368042 3.1105849742889404 4.805553913116455
Loss :  1.678787112236023 3.403127908706665 5.081914901733398
Loss :  1.708479642868042 2.793147563934326 4.501626968383789
Loss :  1.7049585580825806 2.8462328910827637 4.551191329956055
Loss :  1.684545636177063 2.811962604522705 4.4965081214904785
Loss :  1.71676766872406 3.2186408042907715 4.935408592224121
Loss :  1.667424201965332 2.740178108215332 4.407602310180664
Loss :  1.712133765220642 3.112431049346924 4.8245649337768555
Loss :  1.6710389852523804 3.135491371154785 4.806530475616455
Loss :  1.7484315633773804 3.4063193798065186 5.154750823974609
Loss :  1.688199758529663 3.2300894260406494 4.9182891845703125
Loss :  1.6758681535720825 2.821791410446167 4.497659683227539
Loss :  1.6860382556915283 3.15558123588562 4.841619491577148
Loss :  1.7128443717956543 3.3525404930114746 5.065384864807129
Loss :  1.7100409269332886 2.946702003479004 4.656743049621582
Loss :  1.690949559211731 2.8439221382141113 4.534871578216553
Loss :  1.6476706266403198 2.7076332569122314 4.355303764343262
Loss :  1.6798813343048096 2.675006151199341 4.35488748550415
Loss :  1.6764558553695679 2.5831401348114014 4.25959587097168
  batch 40 loss: 1.6764558553695679, 2.5831401348114014, 4.25959587097168
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.675228238105774 2.845186948776245 4.520415306091309
Loss :  1.6641432046890259 3.03202223777771 4.696165561676025
Loss :  1.683311939239502 2.976266622543335 4.659578323364258
Loss :  1.6760694980621338 3.1228396892547607 4.7989091873168945
Loss :  1.6795107126235962 3.1888844966888428 4.8683953285217285
Loss :  1.6839574575424194 3.024810314178467 4.708767890930176
Loss :  1.6677820682525635 2.9323854446411133 4.600167274475098
Loss :  1.6690516471862793 3.380302906036377 5.049354553222656
Loss :  1.6643255949020386 3.156198263168335 4.820523738861084
Loss :  1.6877977848052979 3.0980801582336426 4.7858781814575195
Loss :  1.6668384075164795 3.11287522315979 4.7797136306762695
Loss :  1.683923602104187 3.1522295475006104 4.836153030395508
Loss :  1.703507900238037 2.9190621376037598 4.622570037841797
Loss :  1.6758114099502563 3.2289845943450928 4.904796123504639
Loss :  1.6873352527618408 3.3114936351776123 4.998828887939453
Loss :  1.6527458429336548 3.2456908226013184 4.898436546325684
Loss :  1.706801176071167 3.272050380706787 4.978851318359375
Loss :  1.686587929725647 3.266115427017212 4.952703475952148
Loss :  1.715551495552063 3.419804573059082 5.1353559494018555
Loss :  1.691837191581726 3.314066171646118 5.005903244018555
  batch 60 loss: 1.691837191581726, 3.314066171646118, 5.005903244018555
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6951782703399658 3.386958122253418 5.082136154174805
Loss :  1.6885513067245483 3.0142152309417725 4.702766418457031
Loss :  1.6868832111358643 2.8193366527557373 4.506219863891602
Loss :  1.6886465549468994 2.804340124130249 4.492986679077148
Loss :  1.6759521961212158 2.604342222213745 4.280294418334961
Loss :  2.993154525756836 4.290334224700928 7.283488750457764
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3], device='cuda:0')
Loss :  2.8935208320617676 4.293421268463135 7.186942100524902
Loss :  3.0429039001464844 4.268651485443115 7.3115553855896
Loss :  2.5066473484039307 4.037003517150879 6.5436506271362305
Total LOSS train 4.731571325889001 valid 7.081409215927124
CE LOSS train 1.6830322082226092 valid 0.6266618371009827
Contrastive LOSS train 3.048539158014151 valid 1.0092508792877197
EPOCH 43:
Loss :  1.6641794443130493 3.3947722911834717 5.0589518547058105
Loss :  1.684361219406128 3.647371768951416 5.331732749938965
Loss :  1.6647635698318481 3.3633134365081787 5.028077125549316
Loss :  1.6680564880371094 3.448604106903076 5.1166605949401855
Loss :  1.6975294351577759 4.088151931762695 5.785681247711182
Loss :  1.67156982421875 3.537858009338379 5.209427833557129
Loss :  1.6877824068069458 3.275829315185547 4.963611602783203
Loss :  1.6697946786880493 2.7052879333496094 4.375082492828369
Loss :  1.6778178215026855 2.7327520847320557 4.41057014465332
Loss :  1.6314111948013306 2.6508212089538574 4.282232284545898
Loss :  1.684168815612793 3.1238925457000732 4.808061599731445
Loss :  1.7472583055496216 2.980574131011963 4.727832317352295
Loss :  1.6927988529205322 3.012737989425659 4.705536842346191
Loss :  1.68256413936615 3.203186511993408 4.885750770568848
Loss :  1.6707299947738647 2.8310511112213135 4.501780986785889
Loss :  1.6736193895339966 2.8696978092193604 4.5433173179626465
Loss :  1.6765893697738647 2.940173387527466 4.616762638092041
Loss :  1.680145263671875 2.8200271129608154 4.5001726150512695
Loss :  1.6848185062408447 2.4105992317199707 4.0954179763793945
Loss :  1.6553161144256592 2.5991246700286865 4.254440784454346
  batch 20 loss: 1.6553161144256592, 2.5991246700286865, 4.254440784454346
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6888725757598877 2.641941547393799 4.330814361572266
Loss :  1.6988056898117065 2.6535792350769043 4.3523850440979
Loss :  1.6834372282028198 2.711658477783203 4.3950958251953125
Loss :  1.7039680480957031 2.8149850368499756 4.518953323364258
Loss :  1.7108818292617798 2.904768943786621 4.615650653839111
Loss :  1.6881885528564453 2.7522711753845215 4.440459728240967
Loss :  1.7152736186981201 2.8392958641052246 4.554569244384766
Loss :  1.663420557975769 2.8026885986328125 4.466109275817871
Loss :  1.7167340517044067 3.006369113922119 4.723103046417236
Loss :  1.6681995391845703 3.7147955894470215 5.382995128631592
Loss :  1.74385666847229 3.657428503036499 5.401285171508789
Loss :  1.691108226776123 3.521404504776001 5.212512969970703
Loss :  1.6723475456237793 3.6707167625427246 5.343064308166504
Loss :  1.6889195442199707 3.4046337604522705 5.09355354309082
Loss :  1.7198551893234253 3.474193572998047 5.194048881530762
Loss :  1.7122602462768555 3.1554369926452637 4.867697238922119
Loss :  1.6889395713806152 3.0255846977233887 4.714524269104004
Loss :  1.6471325159072876 2.864060640335083 4.51119327545166
Loss :  1.6768360137939453 2.9991796016693115 4.676015853881836
Loss :  1.6700736284255981 2.991406202316284 4.661479949951172
  batch 40 loss: 1.6700736284255981, 2.991406202316284, 4.661479949951172
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6750808954238892 3.4207873344421387 5.095868110656738
Loss :  1.665311336517334 3.076521396636963 4.741832733154297
Loss :  1.6875804662704468 3.2295334339141846 4.917113780975342
Loss :  1.6758520603179932 2.7824296951293945 4.458281517028809
Loss :  1.6784030199050903 2.752732276916504 4.431135177612305
Loss :  1.6796048879623413 2.8933260440826416 4.572930812835693
Loss :  1.657585620880127 3.018869161605835 4.676454544067383
Loss :  1.670711636543274 2.9211814403533936 4.591893196105957
Loss :  1.6514661312103271 3.1121716499328613 4.763637542724609
Loss :  1.6913944482803345 2.9619643688201904 4.6533589363098145
Loss :  1.66538405418396 3.1556222438812256 4.8210062980651855
Loss :  1.6801857948303223 2.9594404697418213 4.639626502990723
Loss :  1.7004586458206177 2.8049988746643066 4.505457401275635
Loss :  1.6756340265274048 2.652039051055908 4.327672958374023
Loss :  1.6848523616790771 3.4097015857696533 5.0945539474487305
Loss :  1.652034878730774 3.4995839595794678 5.151618957519531
Loss :  1.7063837051391602 3.586219310760498 5.292603015899658
Loss :  1.6916264295578003 3.3076465129852295 4.99927282333374
Loss :  1.7161730527877808 3.452702283859253 5.168875217437744
Loss :  1.688002347946167 3.1440255641937256 4.832027912139893
  batch 60 loss: 1.688002347946167, 3.1440255641937256, 4.832027912139893
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6950085163116455 2.862273693084717 4.557282447814941
Loss :  1.6879059076309204 2.984099864959717 4.672005653381348
Loss :  1.6930569410324097 2.977677345275879 4.670734405517578
Loss :  1.6920642852783203 2.8218488693237305 4.513913154602051
Loss :  1.680592656135559 2.1764822006225586 3.857074737548828
Loss :  2.3845596313476562 4.310483932495117 6.695043563842773
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  2.3435122966766357 4.326699733734131 6.6702117919921875
Loss :  2.4202723503112793 4.253556728363037 6.673829078674316
Loss :  2.037184715270996 4.082963943481445 6.120148658752441
Total LOSS train 4.748659148583045 valid 6.53980827331543
CE LOSS train 1.6839498428198008 valid 0.509296178817749
Contrastive LOSS train 3.064709292925321 valid 1.0207409858703613
EPOCH 44:
Loss :  1.6702356338500977 2.684506416320801 4.354742050170898
Loss :  1.6866602897644043 3.157231092453003 4.843891143798828
Loss :  1.66647469997406 2.7166457176208496 4.383120536804199
Loss :  1.6705541610717773 2.7374322414398193 4.407986640930176
Loss :  1.6996713876724243 3.085268974304199 4.784940242767334
Loss :  1.6739921569824219 3.009272575378418 4.68326473236084
Loss :  1.6887576580047607 2.996413469314575 4.685171127319336
Loss :  1.6712197065353394 3.4488205909729004 5.120040416717529
Loss :  1.6768696308135986 3.0272529125213623 4.704122543334961
Loss :  1.6362601518630981 2.8784093856811523 4.514669418334961
Loss :  1.685294508934021 3.4351723194122314 5.120466709136963
Loss :  1.7474035024642944 3.563814401626587 5.311217784881592
Loss :  1.6924740076065063 3.426159381866455 5.118633270263672
Loss :  1.6872605085372925 3.607698917388916 5.294959545135498
Loss :  1.6712048053741455 3.037858486175537 4.709063529968262
Loss :  1.6751688718795776 3.199246406555176 4.874415397644043
Loss :  1.6853857040405273 3.034536838531494 4.7199225425720215
Loss :  1.6832659244537354 3.5657875537872314 5.249053478240967
Loss :  1.6917887926101685 3.0858404636383057 4.777629375457764
Loss :  1.6595125198364258 3.3568429946899414 5.016355514526367
  batch 20 loss: 1.6595125198364258, 3.3568429946899414, 5.016355514526367
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6891847848892212 3.443863868713379 5.1330485343933105
Loss :  1.69998300075531 3.1868295669555664 4.886812686920166
Loss :  1.6767144203186035 2.744011402130127 4.4207258224487305
Loss :  1.7068196535110474 3.0775704383850098 4.784389972686768
Loss :  1.7036406993865967 3.1053738594055176 4.809014320373535
Loss :  1.6832268238067627 3.2926573753356934 4.975884437561035
Loss :  1.7212554216384888 3.3942904472351074 5.115545749664307
Loss :  1.6670875549316406 3.3047878742218018 4.971875190734863
Loss :  1.722031593322754 3.1829705238342285 4.905002117156982
Loss :  1.6650739908218384 2.752030849456787 4.417104721069336
Loss :  1.739770531654358 3.2450921535491943 4.984862804412842
Loss :  1.7015594244003296 3.575486421585083 5.277045726776123
Loss :  1.6797759532928467 3.052684783935547 4.732460975646973
Loss :  1.6890939474105835 3.100219249725342 4.789313316345215
Loss :  1.716185212135315 3.1147589683532715 4.830944061279297
Loss :  1.7133474349975586 3.0678083896636963 4.781155586242676
Loss :  1.6925116777420044 2.947510242462158 4.640021800994873
Loss :  1.6677004098892212 2.9667539596557617 4.634454250335693
Loss :  1.68035888671875 2.9657788276672363 4.646137714385986
Loss :  1.6786242723464966 3.754838705062866 5.433463096618652
  batch 40 loss: 1.6786242723464966, 3.754838705062866, 5.433463096618652
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.682377576828003 3.565192699432373 5.247570037841797
Loss :  1.6722161769866943 2.876903772354126 4.54911994934082
Loss :  1.6834195852279663 2.9555561542510986 4.638975620269775
Loss :  1.6815049648284912 3.348921537399292 5.030426502227783
Loss :  1.6811543703079224 3.4355735778808594 5.116727828979492
Loss :  1.6902403831481934 3.0385148525238037 4.728754997253418
Loss :  1.6727409362792969 3.10608172416687 4.778822898864746
Loss :  1.6725326776504517 3.044665813446045 4.717198371887207
Loss :  1.6672749519348145 3.0709762573242188 4.738251209259033
Loss :  1.6904147863388062 2.687126636505127 4.377541542053223
Loss :  1.670462727546692 2.973304271697998 4.6437668800354
Loss :  1.686891794204712 2.6178483963012695 4.304739952087402
Loss :  1.705397605895996 2.6164817810058594 4.3218793869018555
Loss :  1.6872384548187256 2.644094705581665 4.331333160400391
Loss :  1.6919456720352173 2.7870049476623535 4.478950500488281
Loss :  1.6642946004867554 2.8581912517547607 4.522485733032227
Loss :  1.7121052742004395 2.888122320175171 4.600227355957031
Loss :  1.6957529783248901 2.9223830699920654 4.618135929107666
Loss :  1.7186102867126465 3.0695254802703857 4.788135528564453
Loss :  1.6983370780944824 2.876917600631714 4.575254440307617
  batch 60 loss: 1.6983370780944824, 2.876917600631714, 4.575254440307617
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6992743015289307 2.9789323806762695 4.678206443786621
Loss :  1.691531777381897 3.3443377017974854 5.035869598388672
Loss :  1.699942708015442 3.7319185733795166 5.431861400604248
Loss :  1.6928025484085083 3.7765724658966064 5.469375133514404
Loss :  1.6836310625076294 3.0285749435424805 4.71220588684082
Loss :  3.055953025817871 4.43653678894043 7.492489814758301
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  2.994213104248047 4.573028087615967 7.567241191864014
Loss :  3.077634334564209 4.467677593231201 7.54531192779541
Loss :  2.6125004291534424 4.209882736206055 6.822382926940918
Total LOSS train 4.804288387298584 valid 7.356856465339661
CE LOSS train 1.687776855322031 valid 0.6531251072883606
Contrastive LOSS train 3.1165115686563345 valid 1.0524706840515137
EPOCH 45:
Loss :  1.6718426942825317 3.4463772773742676 5.11821985244751
Loss :  1.6964728832244873 3.418084144592285 5.114557266235352
Loss :  1.676371693611145 3.242990016937256 4.919361591339111
Loss :  1.6727129220962524 3.3614068031311035 5.034119606018066
Loss :  1.7026305198669434 3.3942580223083496 5.096888542175293
Loss :  1.6794596910476685 3.1905410289764404 4.870000839233398
Loss :  1.6906347274780273 3.4452309608459473 5.135865688323975
Loss :  1.6758719682693481 3.210951566696167 4.886823654174805
Loss :  1.675437569618225 2.6177961826324463 4.293233871459961
Loss :  1.6351531744003296 2.991253137588501 4.626406192779541
Loss :  1.6863234043121338 2.8789761066436768 4.5652995109558105
Loss :  1.7366021871566772 3.037606716156006 4.774209022521973
Loss :  1.6928374767303467 2.916466474533081 4.609303951263428
Loss :  1.6858444213867188 3.181865692138672 4.867710113525391
Loss :  1.6613337993621826 3.007601737976074 4.668935775756836
Loss :  1.670607566833496 3.3674402236938477 5.038047790527344
Loss :  1.6764148473739624 2.9618797302246094 4.638294696807861
Loss :  1.6781822443008423 3.146484375 4.824666500091553
Loss :  1.6726735830307007 3.105538845062256 4.778212547302246
Loss :  1.6531789302825928 2.969738721847534 4.622917652130127
  batch 20 loss: 1.6531789302825928, 2.969738721847534, 4.622917652130127
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.686024785041809 3.241502523422241 4.92752742767334
Loss :  1.6950805187225342 3.0884509086608887 4.783531188964844
Loss :  1.6737234592437744 3.144232749938965 4.81795597076416
Loss :  1.706998586654663 3.1895110607147217 4.896509647369385
Loss :  1.7022755146026611 3.477799892425537 5.180075645446777
Loss :  1.6823139190673828 3.4962997436523438 5.178613662719727
Loss :  1.7191705703735352 3.3315658569335938 5.050736427307129
Loss :  1.6690237522125244 3.4257798194885254 5.094803810119629
Loss :  1.7130180597305298 2.785297393798828 4.498315334320068
Loss :  1.661183476448059 3.1991419792175293 4.860325336456299
Loss :  1.7387889623641968 3.5481679439544678 5.286956787109375
Loss :  1.6923706531524658 3.343860387802124 5.03623104095459
Loss :  1.6740800142288208 3.185758352279663 4.859838485717773
Loss :  1.682936668395996 2.9667890071868896 4.649725914001465
Loss :  1.716575026512146 3.2843194007873535 5.000894546508789
Loss :  1.7100582122802734 3.5902276039123535 5.300285816192627
Loss :  1.6947301626205444 3.041959285736084 4.736689567565918
Loss :  1.6606336832046509 3.0848591327667236 4.745492935180664
Loss :  1.6865196228027344 3.1492860317230225 4.835805892944336
Loss :  1.6818058490753174 2.8016655445098877 4.483471393585205
  batch 40 loss: 1.6818058490753174, 2.8016655445098877, 4.483471393585205
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6783527135849 3.5551645755767822 5.233517169952393
Loss :  1.6672115325927734 2.916074275970459 4.583285808563232
Loss :  1.6886292695999146 2.7861416339874268 4.474771022796631
Loss :  1.6781212091445923 3.0851216316223145 4.763242721557617
Loss :  1.6823468208312988 2.4247994422912598 4.107146263122559
Loss :  1.687765121459961 3.0156373977661133 4.703402519226074
Loss :  1.6705571413040161 2.5915989875793457 4.262156009674072
Loss :  1.6727880239486694 2.383310556411743 4.056098461151123
Loss :  1.6684726476669312 2.854572057723999 4.523044586181641
Loss :  1.6951874494552612 3.3538994789123535 5.049087047576904
Loss :  1.6739908456802368 3.401787042617798 5.075778007507324
Loss :  1.693573236465454 2.7501981258392334 4.4437713623046875
Loss :  1.7008967399597168 2.804002046585083 4.504899024963379
Loss :  1.6737326383590698 2.976144313812256 4.649877071380615
Loss :  1.684411883354187 3.170496702194214 4.854908466339111
Loss :  1.6583768129348755 2.8207879066467285 4.4791646003723145
Loss :  1.7026445865631104 2.854808807373047 4.557453155517578
Loss :  1.6908328533172607 3.201869010925293 4.892702102661133
Loss :  1.7189247608184814 3.133833646774292 4.852758407592773
Loss :  1.6934478282928467 3.0108654499053955 4.704313278198242
  batch 60 loss: 1.6934478282928467, 3.0108654499053955, 4.704313278198242
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6919512748718262 3.3889565467834473 5.080907821655273
Loss :  1.6855512857437134 3.253298044204712 4.938849449157715
Loss :  1.696093201637268 3.267016887664795 4.963109970092773
Loss :  1.6913278102874756 3.310990571975708 5.002318382263184
Loss :  1.6792603731155396 2.7793586254119873 4.458619117736816
Loss :  2.4668681621551514 4.465196132659912 6.932064056396484
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  2.435396194458008 4.584428787231445 7.019824981689453
Loss :  2.479100465774536 4.324382305145264 6.803483009338379
Loss :  2.11910343170166 4.407297134399414 6.526400566101074
Total LOSS train 4.7988006665156435 valid 6.820443153381348
CE LOSS train 1.6855745517290555 valid 0.529775857925415
Contrastive LOSS train 3.1132260946127084 valid 1.1018242835998535
EPOCH 46:
Loss :  1.6701667308807373 3.318030834197998 4.988197326660156
Loss :  1.6871087551116943 3.2573928833007812 4.944501876831055
Loss :  1.6690118312835693 3.333739995956421 5.00275182723999
Loss :  1.6673048734664917 3.362898588180542 5.030203342437744
Loss :  1.701323390007019 3.216021776199341 4.91734504699707
Loss :  1.6761261224746704 2.839381694793701 4.515507698059082
Loss :  1.690676212310791 2.9684906005859375 4.6591668128967285
Loss :  1.675284504890442 2.452303647994995 4.127588272094727
Loss :  1.675445795059204 2.518817901611328 4.194263458251953
Loss :  1.6395519971847534 2.526859998703003 4.166411876678467
Loss :  1.6888500452041626 2.731288194656372 4.420138359069824
Loss :  1.7387892007827759 2.9275357723236084 4.666325092315674
Loss :  1.6922348737716675 3.1366004943847656 4.828835487365723
Loss :  1.6854515075683594 3.2630693912506104 4.948520660400391
Loss :  1.6652157306671143 3.2615911960601807 4.926806926727295
Loss :  1.6764531135559082 3.1106536388397217 4.787106513977051
Loss :  1.6758438348770142 3.24539852142334 4.9212422370910645
Loss :  1.6859326362609863 3.4454259872436523 5.131358623504639
Loss :  1.6740798950195312 2.8701412677764893 4.544220924377441
Loss :  1.6525497436523438 3.368647336959839 5.021197319030762
  batch 20 loss: 1.6525497436523438, 3.368647336959839, 5.021197319030762
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6863974332809448 3.6561787128448486 5.342576026916504
Loss :  1.689862608909607 3.207956075668335 4.897818565368652
Loss :  1.675548791885376 3.346266269683838 5.021815299987793
Loss :  1.6980034112930298 3.7517600059509277 5.449763298034668
Loss :  1.699632167816162 3.5727250576019287 5.272356986999512
Loss :  1.6882656812667847 2.8896021842956543 4.5778679847717285
Loss :  1.722832202911377 2.9224460124969482 4.645277976989746
Loss :  1.6679975986480713 3.0529048442840576 4.720902442932129
Loss :  1.7208236455917358 3.0861642360687256 4.806987762451172
Loss :  1.6683082580566406 3.2385380268096924 4.906846046447754
Loss :  1.7417863607406616 3.163116455078125 4.904902935028076
Loss :  1.697575569152832 3.1723642349243164 4.869939804077148
Loss :  1.67539381980896 2.869295835494995 4.544689655303955
Loss :  1.692047119140625 2.602816343307495 4.294863700866699
Loss :  1.7195160388946533 3.488210916519165 5.207726955413818
Loss :  1.7143657207489014 3.0647549629211426 4.779120445251465
Loss :  1.6941649913787842 3.103381872177124 4.797546863555908
Loss :  1.6669343709945679 3.0663061141967773 4.733240604400635
Loss :  1.683426022529602 3.3394675254821777 5.02289342880249
Loss :  1.679193377494812 3.21777606010437 4.896969318389893
  batch 40 loss: 1.679193377494812, 3.21777606010437, 4.896969318389893
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6790006160736084 3.2359917163848877 4.914992332458496
Loss :  1.6702464818954468 2.832761526107788 4.503007888793945
Loss :  1.682952642440796 3.1523213386535645 4.835273742675781
Loss :  1.6810157299041748 3.225961923599243 4.906977653503418
Loss :  1.6790302991867065 2.794097423553467 4.473127841949463
Loss :  1.6828426122665405 3.431997060775757 5.114839553833008
Loss :  1.6673272848129272 3.5048630237579346 5.172190189361572
Loss :  1.670117735862732 3.5767805576324463 5.246898174285889
Loss :  1.6564863920211792 3.3761940002441406 5.032680511474609
Loss :  1.6906037330627441 2.51808762550354 4.208691596984863
Loss :  1.6640043258666992 2.761528968811035 4.425533294677734
Loss :  1.6816438436508179 2.598845958709717 4.280489921569824
Loss :  1.6983476877212524 2.75301194190979 4.451359748840332
Loss :  1.670737385749817 2.9265944957733154 4.597332000732422
Loss :  1.6835839748382568 2.6837069988250732 4.36729097366333
Loss :  1.6552850008010864 2.779125213623047 4.434410095214844
Loss :  1.704284429550171 3.264789342880249 4.96907377243042
Loss :  1.691102385520935 3.0197324752807617 4.710834980010986
Loss :  1.7147501707077026 3.291253089904785 5.006003379821777
Loss :  1.689484715461731 3.3204596042633057 5.009944438934326
  batch 60 loss: 1.689484715461731, 3.3204596042633057, 5.009944438934326
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6929388046264648 3.0902841091156006 4.7832231521606445
Loss :  1.681666374206543 3.156075954437256 4.837742328643799
Loss :  1.6995561122894287 3.1981935501098633 4.897749900817871
Loss :  1.6862939596176147 3.2072935104370117 4.893587589263916
Loss :  1.6783000230789185 2.6669578552246094 4.345257759094238
Loss :  3.1641223430633545 4.507501602172852 7.671624183654785
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  3.124868869781494 4.5210089683532715 7.645877838134766
Loss :  3.1886701583862305 4.305315971374512 7.493986129760742
Loss :  2.6432723999023438 4.214609146118164 6.857881546020508
Total LOSS train 4.782373978541448 valid 7.4173424243927
CE LOSS train 1.6849396723967331 valid 0.6608180999755859
Contrastive LOSS train 3.097434318982638 valid 1.053652286529541
EPOCH 47:
Loss :  1.668926477432251 3.2895500659942627 4.958476543426514
Loss :  1.696794867515564 3.6446988582611084 5.341493606567383
Loss :  1.6765642166137695 3.5406172275543213 5.217181205749512
Loss :  1.672034740447998 2.920109987258911 4.592144966125488
Loss :  1.6979782581329346 2.842715263366699 4.540693283081055
Loss :  1.6763697862625122 2.8332877159118652 4.509657382965088
Loss :  1.6847002506256104 2.833096504211426 4.517796516418457
Loss :  1.678959608078003 3.5079216957092285 5.186881065368652
Loss :  1.6722054481506348 3.2582106590270996 4.930416107177734
Loss :  1.627428650856018 2.827390432357788 4.454819202423096
Loss :  1.6834074258804321 3.329401731491089 5.0128092765808105
Loss :  1.743945598602295 3.286637544631958 5.030583381652832
Loss :  1.6881691217422485 3.335808753967285 5.023977756500244
Loss :  1.6800645589828491 3.6288671493530273 5.308931827545166
Loss :  1.66205894947052 3.7982683181762695 5.4603271484375
Loss :  1.6656094789505005 3.178816795349121 4.844426155090332
Loss :  1.6818138360977173 3.253814697265625 4.935628414154053
Loss :  1.6733499765396118 3.7397563457489014 5.413106441497803
Loss :  1.6834924221038818 3.208353281021118 4.891845703125
Loss :  1.677176833152771 3.7513632774353027 5.428540229797363
  batch 20 loss: 1.677176833152771, 3.7513632774353027, 5.428540229797363
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6825087070465088 3.4897756576538086 5.172284126281738
Loss :  1.69921875 3.547356605529785 5.246575355529785
Loss :  1.6755863428115845 3.112748622894287 4.788334846496582
Loss :  1.7080532312393188 3.018475294113159 4.726528644561768
Loss :  1.7035382986068726 3.361725091934204 5.065263271331787
Loss :  1.6849539279937744 2.878781318664551 4.563735008239746
Loss :  1.7152936458587646 3.280703067779541 4.995996475219727
Loss :  1.6614344120025635 2.6696829795837402 4.331117630004883
Loss :  1.7051782608032227 2.9823102951049805 4.687488555908203
Loss :  1.6619617938995361 2.849684476852417 4.511646270751953
Loss :  1.7380808591842651 2.9377660751342773 4.675847053527832
Loss :  1.6873884201049805 2.822054862976074 4.509443283081055
Loss :  1.670908808708191 2.62062668800354 4.291535377502441
Loss :  1.6816757917404175 2.97373628616333 4.655412197113037
Loss :  1.7108393907546997 3.0723376274108887 4.783176898956299
Loss :  1.7049881219863892 2.674954414367676 4.379942417144775
Loss :  1.6883149147033691 2.5968034267425537 4.285118103027344
Loss :  1.6603416204452515 2.453044891357422 4.113386631011963
Loss :  1.6768689155578613 2.856208086013794 4.533077239990234
Loss :  1.6781104803085327 2.5711896419525146 4.249300003051758
  batch 40 loss: 1.6781104803085327, 2.5711896419525146, 4.249300003051758
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6769962310791016 2.6594903469085693 4.33648681640625
Loss :  1.660807728767395 2.645618438720703 4.306426048278809
Loss :  1.6788413524627686 2.8198866844177246 4.498727798461914
Loss :  1.6717464923858643 3.1248621940612793 4.796608924865723
Loss :  1.6761815547943115 3.0770180225372314 4.753199577331543
Loss :  1.6835381984710693 3.274427652359009 4.957965850830078
Loss :  1.6674994230270386 2.953472852706909 4.620972156524658
Loss :  1.6654026508331299 2.8983328342437744 4.563735485076904
Loss :  1.6661416292190552 3.025547981262207 4.691689491271973
Loss :  1.6842764616012573 3.075489044189453 4.759765625
Loss :  1.6659715175628662 2.8977556228637695 4.563727378845215
Loss :  1.6863224506378174 2.961040496826172 4.64736270904541
Loss :  1.7053711414337158 2.504629373550415 4.210000514984131
Loss :  1.671553373336792 2.7469167709350586 4.41847038269043
Loss :  1.6784448623657227 2.9529786109924316 4.631423473358154
Loss :  1.6605005264282227 3.12937331199646 4.789874076843262
Loss :  1.7053940296173096 3.113964080810547 4.819357872009277
Loss :  1.6901509761810303 3.072617292404175 4.762768268585205
Loss :  1.7143950462341309 3.291377305984497 5.005772590637207
Loss :  1.6928967237472534 3.0299692153930664 4.722866058349609
  batch 60 loss: 1.6928967237472534, 3.0299692153930664, 4.722866058349609
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6902697086334229 3.65661358833313 5.346883296966553
Loss :  1.679607629776001 3.4706482887268066 5.150256156921387
Loss :  1.6928101778030396 3.3715317249298096 5.064342021942139
Loss :  1.680220365524292 2.910954475402832 4.591175079345703
Loss :  1.6696258783340454 2.551114797592163 4.220740795135498
Loss :  3.7193028926849365 4.493281364440918 8.212584495544434
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  3.6721036434173584 4.453577518463135 8.125680923461914
Loss :  3.7291629314422607 4.3307294845581055 8.059892654418945
Loss :  3.0897974967956543 4.293286323547363 7.383083820343018
Total LOSS train 4.759931446955754 valid 7.945310473442078
CE LOSS train 1.683096328148475 valid 0.7724493741989136
Contrastive LOSS train 3.076835118807279 valid 1.0733215808868408
EPOCH 48:
Loss :  1.6621251106262207 2.6292192935943604 4.29134464263916
Loss :  1.6772828102111816 3.0599477291107178 4.73723030090332
Loss :  1.6618119478225708 3.1649909019470215 4.826802730560303
Loss :  1.668419361114502 3.486241579055786 5.154661178588867
Loss :  1.697363018989563 3.4987168312072754 5.196079730987549
Loss :  1.6702741384506226 3.406111478805542 5.076385498046875
Loss :  1.6792781352996826 3.0593817234039307 4.738659858703613
Loss :  1.6659859418869019 2.9576005935668945 4.623586654663086
Loss :  1.6711350679397583 3.239227533340454 4.910362720489502
Loss :  1.6270010471343994 2.6902263164520264 4.317227363586426
Loss :  1.677694320678711 2.764003038406372 4.441697120666504
Loss :  1.7375026941299438 2.938875913619995 4.6763787269592285
Loss :  1.684889793395996 3.203728437423706 4.888618469238281
Loss :  1.6773449182510376 3.0814507007598877 4.758795738220215
Loss :  1.6621463298797607 2.9427289962768555 4.604875564575195
Loss :  1.6702908277511597 2.657201051712036 4.327491760253906
Loss :  1.6707160472869873 2.786672830581665 4.457388877868652
Loss :  1.678473949432373 2.7692601680755615 4.4477338790893555
Loss :  1.6734024286270142 2.8629953861236572 4.536397933959961
Loss :  1.6460750102996826 2.8931732177734375 4.539248466491699
  batch 20 loss: 1.6460750102996826, 2.8931732177734375, 4.539248466491699
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6794826984405518 2.882967710494995 4.562450408935547
Loss :  1.6858863830566406 2.8267338275909424 4.512619972229004
Loss :  1.667657494544983 2.6595730781555176 4.327230453491211
Loss :  1.6912668943405151 3.0522167682647705 4.743483543395996
Loss :  1.6941444873809814 3.1360890865325928 4.830233573913574
Loss :  1.6748918294906616 3.422966957092285 5.097858905792236
Loss :  1.7066179513931274 3.7569754123687744 5.463593482971191
Loss :  1.6491857767105103 3.0548787117004395 4.70406436920166
Loss :  1.7038663625717163 3.0292468070983887 4.7331132888793945
Loss :  1.6561874151229858 3.1422946453094482 4.7984819412231445
Loss :  1.7361581325531006 3.0823333263397217 4.818491458892822
Loss :  1.6780601739883423 3.088329792022705 4.766389846801758
Loss :  1.6616207361221313 2.7628469467163086 4.42446756362915
Loss :  1.6761703491210938 3.1593949794769287 4.835565567016602
Loss :  1.7092934846878052 3.3658409118652344 5.07513427734375
Loss :  1.700967788696289 3.3256707191467285 5.026638507843018
Loss :  1.677700400352478 3.084033250808716 4.761733531951904
Loss :  1.6363223791122437 3.160255193710327 4.796577453613281
Loss :  1.6647217273712158 3.6773810386657715 5.342103004455566
Loss :  1.6589845418930054 3.2521955966949463 4.911180019378662
  batch 40 loss: 1.6589845418930054, 3.2521955966949463, 4.911180019378662
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6664456129074097 3.300321340560913 4.966766834259033
Loss :  1.6514049768447876 3.290121555328369 4.941526412963867
Loss :  1.6792187690734863 3.0675814151763916 4.746800422668457
Loss :  1.6638307571411133 2.918301820755005 4.582132339477539
Loss :  1.671413540840149 3.466189384460449 5.137602806091309
Loss :  1.6737706661224365 3.5736498832702637 5.247420310974121
Loss :  1.6472657918930054 3.0557339191436768 4.702999591827393
Loss :  1.6637523174285889 2.947455644607544 4.611207962036133
Loss :  1.6368281841278076 2.9234249591827393 4.560253143310547
Loss :  1.6870536804199219 3.0141022205352783 4.701155662536621
Loss :  1.6574057340621948 2.920175790786743 4.577581405639648
Loss :  1.6731829643249512 3.1320230960845947 4.805206298828125
Loss :  1.6884183883666992 2.8389036655426025 4.527321815490723
Loss :  1.6689603328704834 2.7322418689727783 4.401202201843262
Loss :  1.682011365890503 3.1297199726104736 4.811731338500977
Loss :  1.6386746168136597 2.7854909896850586 4.424165725708008
Loss :  1.7006456851959229 3.023653984069824 4.724299430847168
Loss :  1.694549322128296 3.346932888031006 5.041481971740723
Loss :  1.7092106342315674 3.6788077354431152 5.388018608093262
Loss :  1.6774851083755493 3.4918129444122314 5.16929817199707
  batch 60 loss: 1.6774851083755493, 3.4918129444122314, 5.16929817199707
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6812494993209839 3.1993911266326904 4.880640506744385
Loss :  1.677915334701538 3.0022175312042236 4.680132865905762
Loss :  1.683579444885254 2.8212289810180664 4.50480842590332
Loss :  1.6756184101104736 2.9830241203308105 4.658642768859863
Loss :  1.6722445487976074 2.491696357727051 4.163940906524658
Loss :  2.6941044330596924 4.527771472930908 7.22187614440918
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  2.6722497940063477 4.57419490814209 7.2464447021484375
Loss :  2.689469814300537 4.326694965362549 7.016164779663086
Loss :  2.2551140785217285 4.300357341766357 6.555471420288086
Total LOSS train 4.754441789480356 valid 7.009989261627197
CE LOSS train 1.675239009123582 valid 0.5637785196304321
Contrastive LOSS train 3.0792027950286864 valid 1.0750893354415894
EPOCH 49:
Loss :  1.6648669242858887 2.906935214996338 4.571802139282227
Loss :  1.670854091644287 3.069469928741455 4.740324020385742
Loss :  1.669465184211731 2.999985694885254 4.669450759887695
Loss :  1.676835060119629 2.8866305351257324 4.563465595245361
Loss :  1.6938883066177368 2.822117567062378 4.516005992889404
Loss :  1.673015832901001 2.743675947189331 4.416691780090332
Loss :  1.6799852848052979 2.8352420330047607 4.515227317810059
Loss :  1.666812539100647 2.8032286167144775 4.470041275024414
Loss :  1.6727616786956787 2.5333328247070312 4.206094741821289
Loss :  1.6331802606582642 2.2866461277008057 3.9198265075683594
Loss :  1.6791013479232788 2.7078330516815186 4.386934280395508
Loss :  1.7365976572036743 2.806389331817627 4.542986869812012
Loss :  1.6824369430541992 2.6841366291046143 4.366573333740234
Loss :  1.6788851022720337 2.828887939453125 4.507772922515869
Loss :  1.6583969593048096 2.860541343688965 4.518938064575195
Loss :  1.6708767414093018 2.8593924045562744 4.530269145965576
Loss :  1.672884464263916 3.0607948303222656 4.733679294586182
Loss :  1.680822730064392 2.9715516567230225 4.652374267578125
Loss :  1.6721689701080322 3.1105127334594727 4.782681465148926
Loss :  1.6445038318634033 2.9298195838928223 4.574323654174805
  batch 20 loss: 1.6445038318634033, 2.9298195838928223, 4.574323654174805
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6766021251678467 3.5745153427124023 5.251117706298828
Loss :  1.687014102935791 3.2556817531585693 4.942695617675781
Loss :  1.6691081523895264 3.1131489276885986 4.782257080078125
Loss :  1.6937412023544312 3.1124684810638428 4.806209564208984
Loss :  1.6938658952713013 3.4111437797546387 5.10500955581665
Loss :  1.6801339387893677 3.4230940341949463 5.1032280921936035
Loss :  1.7166664600372314 3.0033605098724365 4.720026969909668
Loss :  1.6610835790634155 3.0987188816070557 4.759802341461182
Loss :  1.7145509719848633 3.0145814418792725 4.729132652282715
Loss :  1.660408854484558 3.0545578002929688 4.714966773986816
Loss :  1.738106608390808 3.0858356952667236 4.823942184448242
Loss :  1.689770221710205 3.327255964279175 5.017025947570801
Loss :  1.6684131622314453 2.9511287212371826 4.619542121887207
Loss :  1.6826648712158203 2.889068841934204 4.571733474731445
Loss :  1.7108092308044434 3.0592923164367676 4.770101547241211
Loss :  1.7025766372680664 3.0419921875 4.744568824768066
Loss :  1.6836397647857666 2.9957351684570312 4.679374694824219
Loss :  1.6491864919662476 2.7556815147399902 4.404868125915527
Loss :  1.6738229990005493 3.1651453971862793 4.838968276977539
Loss :  1.6712970733642578 2.8903183937072754 4.561615467071533
  batch 40 loss: 1.6712970733642578, 2.8903183937072754, 4.561615467071533
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  1.6722676753997803 3.5842015743255615 5.256469249725342
Loss :  1.6589986085891724 3.286754846572876 4.945753574371338
Loss :  1.680627465248108 3.247689962387085 4.928317546844482
Loss :  1.6719586849212646 3.214775800704956 4.886734485626221
Loss :  1.6754646301269531 2.7736663818359375 4.449131011962891
Loss :  1.67582368850708 2.4777092933654785 4.153532981872559
Loss :  1.6547636985778809 2.8021399974823 4.456903457641602
Loss :  1.6612917184829712 3.035954475402832 4.697246074676514
Loss :  1.6508231163024902 3.2640738487243652 4.9148969650268555
Loss :  1.6842447519302368 3.208688259124756 4.892932891845703
Loss :  1.6591819524765015 3.114884376525879 4.77406644821167
Loss :  1.6765862703323364 2.748980760574341 4.425567150115967
Loss :  1.6927039623260498 3.2912909984588623 4.983994960784912
Loss :  1.6694543361663818 2.8101348876953125 4.479589462280273
Loss :  1.6819370985031128 2.722452402114868 4.404389381408691
Loss :  1.6466752290725708 2.8309669494628906 4.477642059326172
Loss :  1.7007197141647339 3.080857753753662 4.7815775871276855
Loss :  1.6872774362564087 2.9969587326049805 4.6842360496521
Loss :  1.7089366912841797 3.2586255073547363 4.967562198638916
Loss :  1.683254599571228 3.356848955154419 5.040103435516357
  batch 60 loss: 1.683254599571228, 3.356848955154419, 5.040103435516357
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6872485876083374 3.3997697830200195 5.0870184898376465
Loss :  1.6775046586990356 3.0490882396698 4.726593017578125
Loss :  1.6924291849136353 2.872314929962158 4.564743995666504
Loss :  1.6816496849060059 3.0459089279174805 4.727558612823486
Loss :  1.674729824066162 2.5924301147460938 4.267159938812256
Loss :  3.4376351833343506 4.592959880828857 8.030594825744629
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  3.4038100242614746 4.469325065612793 7.873135089874268
Loss :  3.4592716693878174 4.353235721588135 7.812507629394531
Loss :  2.8339290618896484 4.258589267730713 7.092518329620361
Total LOSS train 4.678544176541842 valid 7.702188968658447
CE LOSS train 1.6785900849562425 valid 0.7084822654724121
Contrastive LOSS train 2.999954106257512 valid 1.0646473169326782
EPOCH 50:
Loss :  1.6634210348129272 2.8041462898254395 4.467567443847656
Loss :  1.682501196861267 2.9062297344207764 4.588730812072754
Loss :  1.662854552268982 3.1445424556732178 4.80739688873291
Loss :  1.6675457954406738 3.0097603797912598 4.677306175231934
Loss :  1.6923428773880005 3.127572774887085 4.819915771484375
Loss :  1.6713238954544067 2.997582197189331 4.668906211853027
Loss :  1.6764906644821167 2.636171579360962 4.312662124633789
Loss :  1.666407823562622 2.5839529037475586 4.250360488891602
Loss :  1.669983983039856 3.775576114654541 5.445559978485107
Loss :  1.6274935007095337 2.6904919147491455 4.317985534667969
Loss :  1.6799777746200562 3.1571741104125977 4.837152004241943
Loss :  1.7417441606521606 3.333706855773926 5.075450897216797
Loss :  1.6856828927993774 3.5493953227996826 5.23507833480835
Loss :  1.6759101152420044 3.268155336380005 4.944065570831299
Loss :  1.6679370403289795 3.442247152328491 5.110184192657471
Loss :  1.6679807901382446 3.8615102767944336 5.529490947723389
Loss :  1.6714897155761719 3.3971352577209473 5.068624973297119
Loss :  1.674565315246582 3.471177577972412 5.145742893218994
Loss :  1.680987000465393 3.69844388961792 5.379430770874023
Loss :  1.6522272825241089 3.5958993434906006 5.24812650680542
  batch 20 loss: 1.6522272825241089, 3.5958993434906006, 5.24812650680542
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6847102642059326 3.209352731704712 4.8940629959106445
Loss :  1.6922640800476074 3.5820398330688477 5.274303913116455
Loss :  1.6757270097732544 3.583468198776245 5.259195327758789
Loss :  1.707241415977478 2.9870944023132324 4.6943359375
Loss :  1.7116645574569702 3.9000353813171387 5.611700057983398
Loss :  1.683019757270813 3.2223451137542725 4.905364990234375
Loss :  1.7089163064956665 3.1638858318328857 4.872802257537842
Loss :  1.6550366878509521 2.805110454559326 4.460146903991699
Loss :  1.6939697265625 2.955935478210449 4.649905204772949
Loss :  1.6615229845046997 3.0277037620544434 4.6892266273498535
Loss :  1.736322045326233 2.9230797290802 4.659401893615723
Loss :  1.6827067136764526 2.9827795028686523 4.6654863357543945
Loss :  1.6658800840377808 2.9546027183532715 4.620482921600342
Loss :  1.6762783527374268 2.915353536605835 4.591631889343262
Loss :  1.7055574655532837 2.9179434776306152 4.623500823974609
Loss :  1.6983532905578613 2.837611198425293 4.535964488983154
Loss :  1.6804423332214355 2.5482115745544434 4.228653907775879
Loss :  1.6452648639678955 2.3260574340820312 3.9713222980499268
Loss :  1.6691558361053467 2.4739925861358643 4.143148422241211
Loss :  1.6669378280639648 2.388904094696045 4.05584192276001
  batch 40 loss: 1.6669378280639648, 2.388904094696045, 4.05584192276001
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6708431243896484 2.8711557388305664 4.541998863220215
Loss :  1.6578954458236694 2.618830680847168 4.276726245880127
Loss :  1.675214409828186 3.0264580249786377 4.701672554016113
Loss :  1.6727802753448486 3.0350072383880615 4.70778751373291
Loss :  1.6708669662475586 2.8475568294525146 4.518424034118652
Loss :  1.6736459732055664 3.2598531246185303 4.933499336242676
Loss :  1.6586893796920776 3.1895246505737305 4.848214149475098
Loss :  1.661065697669983 3.1393582820892334 4.800424098968506
Loss :  1.6444265842437744 3.208855390548706 4.8532819747924805
Loss :  1.6816755533218384 3.1907594203948975 4.872435092926025
Loss :  1.6532560586929321 3.0078914165496826 4.661147594451904
Loss :  1.6736705303192139 3.032367467880249 4.706037998199463
Loss :  1.6932437419891357 3.172520160675049 4.8657636642456055
Loss :  1.6668610572814941 2.972257137298584 4.639118194580078
Loss :  1.6765812635421753 3.4692025184631348 5.1457839012146
Loss :  1.6464073657989502 3.239074468612671 4.885481834411621
Loss :  1.698091983795166 3.157334804534912 4.855426788330078
Loss :  1.682849645614624 3.304090976715088 4.986940383911133
Loss :  1.7080198526382446 3.3559353351593018 5.063955307006836
Loss :  1.6823291778564453 2.967419147491455 4.6497483253479
  batch 60 loss: 1.6823291778564453, 2.967419147491455, 4.6497483253479
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6789132356643677 2.525716543197632 4.204629898071289
Loss :  1.6732364892959595 2.8109021186828613 4.484138488769531
Loss :  1.683521032333374 2.7396466732025146 4.423167705535889
Loss :  1.6745086908340454 2.7591941356658936 4.4337029457092285
Loss :  1.664839267730713 2.221611261367798 3.8864505290985107
Loss :  4.149029731750488 4.4959635734558105 8.64499282836914
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  4.089921951293945 4.479010105133057 8.568931579589844
Loss :  4.169163703918457 4.376554012298584 8.545717239379883
Loss :  3.400571823120117 4.325331211090088 7.725903034210205
Total LOSS train 4.742802693293645 valid 8.371386170387268
CE LOSS train 1.6769734125870925 valid 0.8501429557800293
Contrastive LOSS train 3.0658292623666616 valid 1.081332802772522
EPOCH 51:
Loss :  1.6585510969161987 2.3748385906219482 4.033389568328857
Loss :  1.6778161525726318 2.7581026554107666 4.435918807983398
Loss :  1.6545004844665527 2.478952169418335 4.133452415466309
Loss :  1.6571420431137085 2.5373175144195557 4.194459438323975
Loss :  1.6898491382598877 2.6122214794158936 4.302070617675781
Loss :  1.6637649536132812 2.5821871757507324 4.245952129364014
Loss :  1.6771479845046997 2.7102243900299072 4.3873724937438965
Loss :  1.6651434898376465 2.3754942417144775 4.040637969970703
Loss :  1.6636998653411865 2.6638786792755127 4.327578544616699
Loss :  1.6254262924194336 2.5255422592163086 4.150968551635742
Loss :  1.6780411005020142 2.898730754852295 4.5767717361450195
Loss :  1.7305188179016113 3.136807918548584 4.867326736450195
Loss :  1.6820409297943115 3.090439558029175 4.772480487823486
Loss :  1.6764791011810303 3.0114634037017822 4.6879425048828125
Loss :  1.6530121564865112 2.9552183151245117 4.6082305908203125
Loss :  1.6649197340011597 3.242218494415283 4.907138347625732
Loss :  1.66998291015625 2.631530284881592 4.301513195037842
Loss :  1.671034574508667 2.837350368499756 4.508384704589844
Loss :  1.6666474342346191 2.5773110389709473 4.243958473205566
Loss :  1.6385246515274048 2.7740185260772705 4.412543296813965
  batch 20 loss: 1.6385246515274048, 2.7740185260772705, 4.412543296813965
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6743667125701904 2.723314046859741 4.397680759429932
Loss :  1.6822035312652588 2.8212578296661377 4.5034613609313965
Loss :  1.6606212854385376 2.283710241317749 3.944331645965576
Loss :  1.6915509700775146 2.5649521350860596 4.256503105163574
Loss :  1.692104458808899 2.803349494934082 4.495453834533691
Loss :  1.6691488027572632 2.5609443187713623 4.230093002319336
Loss :  1.7074344158172607 2.6366817951202393 4.3441162109375
Loss :  1.6508128643035889 2.4488418102264404 4.099654674530029
Loss :  1.7038413286209106 2.5007450580596924 4.204586505889893
Loss :  1.6539392471313477 2.5984177589416504 4.252357006072998
Loss :  1.7314330339431763 3.0552690029144287 4.7867021560668945
Loss :  1.677977442741394 2.72796893119812 4.405946254730225
Loss :  1.6622920036315918 2.813119411468506 4.475411415100098
Loss :  1.6737116575241089 3.0019023418426514 4.675613880157471
Loss :  1.7063112258911133 3.0699093341827393 4.776220321655273
Loss :  1.6978954076766968 3.087712049484253 4.78560733795166
Loss :  1.6780208349227905 3.0237162113189697 4.701736927032471
Loss :  1.6460868120193481 3.222059726715088 4.8681464195251465
Loss :  1.6699281930923462 2.866380214691162 4.536308288574219
Loss :  1.665359377861023 2.83035945892334 4.495718955993652
  batch 40 loss: 1.665359377861023, 2.83035945892334, 4.495718955993652
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6694639921188354 2.8295724391937256 4.4990363121032715
Loss :  1.6563128232955933 2.517287015914917 4.173599720001221
Loss :  1.6753226518630981 3.0053513050079346 4.680674076080322
Loss :  1.6724486351013184 3.496258497238159 5.168706893920898
Loss :  1.6722450256347656 2.4928221702575684 4.165067195892334
Loss :  1.6732901334762573 2.6369409561157227 4.3102312088012695
Loss :  1.656044602394104 2.4165985584259033 4.072643280029297
Loss :  1.663959264755249 2.446991205215454 4.110950469970703
Loss :  1.6442221403121948 2.5773696899414062 4.221591949462891
Loss :  1.6896600723266602 2.59616756439209 4.28582763671875
Loss :  1.6609846353530884 2.6803250312805176 4.341309547424316
Loss :  1.6778894662857056 2.5238037109375 4.201693058013916
Loss :  1.690848469734192 2.5117805004119873 4.202629089355469
Loss :  1.676350474357605 2.668767213821411 4.345117568969727
Loss :  1.6829921007156372 2.7390670776367188 4.422059059143066
Loss :  1.648018479347229 2.493744134902954 4.141762733459473
Loss :  1.700631022453308 2.651475191116333 4.352106094360352
Loss :  1.688510775566101 2.604193925857544 4.2927045822143555
Loss :  1.709653377532959 2.985592842102051 4.69524621963501
Loss :  1.6861865520477295 2.4895689487457275 4.175755500793457
  batch 60 loss: 1.6861865520477295, 2.4895689487457275, 4.175755500793457
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6813236474990845 2.555457830429077 4.236781597137451
Loss :  1.6793206930160522 2.4664742946624756 4.145794868469238
Loss :  1.6871613264083862 2.2963640689849854 3.983525276184082
Loss :  1.6772798299789429 2.5108461380004883 4.188126087188721
Loss :  1.6710704565048218 2.1428909301757812 3.8139615058898926
Loss :  2.8736462593078613 4.581438064575195 7.455084323883057
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  2.8479013442993164 4.5167622566223145 7.364663600921631
Loss :  2.883469820022583 4.428995132446289 7.312464714050293
Loss :  2.389822244644165 4.142173767089844 6.53199577331543
Total LOSS train 4.3784714185274565 valid 7.1660521030426025
CE LOSS train 1.6746226640848012 valid 0.5974555611610413
Contrastive LOSS train 2.7038487727825458 valid 1.035543441772461
EPOCH 52:
Loss :  1.6638389825820923 2.638983964920044 4.302823066711426
Loss :  1.6768265962600708 2.780575752258301 4.457402229309082
Loss :  1.662310004234314 2.698437213897705 4.360747337341309
Loss :  1.6686326265335083 2.6903059482574463 4.358938694000244
Loss :  1.6932944059371948 2.9215214252471924 4.614815711975098
Loss :  1.671940803527832 2.725947380065918 4.39788818359375
Loss :  1.677209496498108 2.5930700302124023 4.270279407501221
Loss :  1.666806936264038 2.9539783000946045 4.620785236358643
Loss :  1.6692031621932983 2.893843173980713 4.563046455383301
Loss :  1.6270674467086792 3.329241991043091 4.9563093185424805
Loss :  1.6798036098480225 3.826479196548462 5.506282806396484
Loss :  1.736938714981079 3.357381582260132 5.094320297241211
Loss :  1.6826735734939575 3.0404114723205566 4.723084926605225
Loss :  1.6770761013031006 2.836365222930908 4.51344108581543
Loss :  1.659501552581787 2.793114185333252 4.452615737915039
Loss :  1.6641463041305542 2.8817973136901855 4.545943737030029
Loss :  1.6696281433105469 2.823171377182007 4.492799758911133
Loss :  1.6719145774841309 2.6341986656188965 4.306113243103027
Loss :  1.674286961555481 2.601766347885132 4.276053428649902
Loss :  1.6414977312088013 2.7465829849243164 4.388080596923828
  batch 20 loss: 1.6414977312088013, 2.7465829849243164, 4.388080596923828
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6773656606674194 2.8062965869903564 4.483662128448486
Loss :  1.6865837574005127 2.708484172821045 4.395068168640137
Loss :  1.6661016941070557 2.9247283935546875 4.590829849243164
Loss :  1.6990025043487549 2.6124749183654785 4.3114776611328125
Loss :  1.699360966682434 2.8662703037261963 4.56563138961792
Loss :  1.6732981204986572 2.5336124897003174 4.206910610198975
Loss :  1.708119511604309 2.782907724380493 4.491027355194092
Loss :  1.6530320644378662 2.4694886207580566 4.122520446777344
Loss :  1.7003289461135864 2.491159200668335 4.191488265991211
Loss :  1.6593416929244995 2.781177282333374 4.440518856048584
Loss :  1.7359834909439087 3.0795340538024902 4.815517425537109
Loss :  1.6811984777450562 2.9950332641601562 4.676231861114502
Loss :  1.6644206047058105 2.7923381328582764 4.456758499145508
Loss :  1.6770232915878296 2.813493013381958 4.490516185760498
Loss :  1.7092987298965454 3.0012452602386475 4.710544109344482
Loss :  1.6992582082748413 2.869809150695801 4.569067478179932
Loss :  1.682167649269104 2.867889881134033 4.550057411193848
Loss :  1.6464905738830566 2.677849292755127 4.324339866638184
Loss :  1.675290822982788 2.772641181945801 4.447932243347168
Loss :  1.6695481538772583 2.644904613494873 4.314452648162842
  batch 40 loss: 1.6695481538772583, 2.644904613494873, 4.314452648162842
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6723921298980713 2.8200552463531494 4.492447376251221
Loss :  1.6613082885742188 3.0612776279449463 4.722585678100586
Loss :  1.6809486150741577 2.969465494155884 4.650413990020752
Loss :  1.6766530275344849 2.75054931640625 4.427202224731445
Loss :  1.6765220165252686 2.523141622543335 4.1996636390686035
Loss :  1.6771825551986694 2.8695051670074463 4.546687602996826
Loss :  1.6598135232925415 2.9607093334198 4.620522975921631
Loss :  1.6689558029174805 2.8646674156188965 4.533623218536377
Loss :  1.6482470035552979 2.9331259727478027 4.58137321472168
Loss :  1.6954323053359985 2.9776289463043213 4.673061370849609
Loss :  1.6680574417114258 3.158841848373413 4.826899528503418
Loss :  1.6832059621810913 3.0701165199279785 4.753322601318359
Loss :  1.6939443349838257 3.285121440887451 4.979065895080566
Loss :  1.684361219406128 2.939077854156494 4.623438835144043
Loss :  1.6868029832839966 2.9260482788085938 4.612851142883301
Loss :  1.652273178100586 2.948261022567749 4.600534439086914
Loss :  1.7041912078857422 2.708588123321533 4.412779331207275
Loss :  1.691697597503662 2.762200355529785 4.453897953033447
Loss :  1.7120862007141113 3.080954074859619 4.7930402755737305
Loss :  1.689780354499817 2.7105791568756104 4.400359630584717
  batch 60 loss: 1.689780354499817, 2.7105791568756104, 4.400359630584717
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6832740306854248 3.298417568206787 4.981691360473633
Loss :  1.6818381547927856 3.1741299629211426 4.855967998504639
Loss :  1.6886656284332275 2.5162734985351562 4.204938888549805
Loss :  1.6779509782791138 2.7462315559387207 4.424182415008545
Loss :  1.6720354557037354 2.3432905673980713 4.015326023101807
Loss :  2.515009880065918 4.514455318450928 7.029465198516846
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  2.498673439025879 4.430083274841309 6.9287567138671875
Loss :  2.520233392715454 4.381028175354004 6.901261329650879
Loss :  2.1365790367126465 4.3872151374816895 6.523794174194336
Total LOSS train 4.534495405050424 valid 6.845819354057312
CE LOSS train 1.678237425363981 valid 0.5341447591781616
Contrastive LOSS train 2.8562579851884107 valid 1.0968037843704224
EPOCH 53:
Loss :  1.666232943534851 2.4994802474975586 4.165713310241699
Loss :  1.677952527999878 2.7235400676727295 4.401492595672607
Loss :  1.662746548652649 2.4823708534240723 4.145117282867432
Loss :  1.668045163154602 2.5997328758239746 4.267777919769287
Loss :  1.6938234567642212 2.5368075370788574 4.230630874633789
Loss :  1.67197585105896 2.7421798706054688 4.414155960083008
Loss :  1.6794873476028442 2.9866371154785156 4.66612434387207
Loss :  1.667493462562561 2.6286911964416504 4.296184539794922
Loss :  1.6685482263565063 2.562361240386963 4.23090934753418
Loss :  1.6293672323226929 2.647156000137329 4.276523113250732
Loss :  1.6800020933151245 3.0270586013793945 4.707060813903809
Loss :  1.7348772287368774 3.3096604347229004 5.044537544250488
Loss :  1.6829049587249756 2.9259140491485596 4.608819007873535
Loss :  1.678959846496582 2.890571117401123 4.569530963897705
Loss :  1.6571911573410034 3.1735024452209473 4.83069372177124
Loss :  1.6660616397857666 3.407203197479248 5.073265075683594
Loss :  1.6701587438583374 3.0270047187805176 4.6971635818481445
Loss :  1.6739176511764526 3.361372947692871 5.035290718078613
Loss :  1.6706780195236206 2.9662318229675293 4.6369099617004395
Loss :  1.6419062614440918 3.253594160079956 4.895500183105469
  batch 20 loss: 1.6419062614440918, 3.253594160079956, 4.895500183105469
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.677033543586731 3.2152669429779053 4.892300605773926
Loss :  1.6844435930252075 3.239616632461548 4.924060344696045
Loss :  1.6663753986358643 3.007915735244751 4.674291133880615
Loss :  1.6950753927230835 3.1960983276367188 4.891173839569092
Loss :  1.6936254501342773 3.375731945037842 5.069357395172119
Loss :  1.6741389036178589 3.1789920330047607 4.85313081741333
Loss :  1.7094441652297974 3.2409238815307617 4.9503679275512695
Loss :  1.6537885665893555 2.571744441986084 4.2255330085754395
Loss :  1.7049200534820557 2.7379872798919678 4.442907333374023
Loss :  1.6551834344863892 2.759056568145752 4.414239883422852
Loss :  1.7326111793518066 2.8286843299865723 4.561295509338379
Loss :  1.6801985502243042 2.6825783252716064 4.362776756286621
Loss :  1.663624882698059 2.447568416595459 4.1111931800842285
Loss :  1.6750556230545044 3.0258734226226807 4.700929164886475
Loss :  1.7035478353500366 3.0863254070281982 4.789873123168945
Loss :  1.6993966102600098 2.9555954933166504 4.65499210357666
Loss :  1.6799284219741821 2.9405126571655273 4.62044095993042
Loss :  1.6468029022216797 2.9800543785095215 4.626857280731201
Loss :  1.6711992025375366 3.129117012023926 4.800316333770752
Loss :  1.6675901412963867 3.1003575325012207 4.767947673797607
  batch 40 loss: 1.6675901412963867, 3.1003575325012207, 4.767947673797607
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6695019006729126 2.8944733142852783 4.5639753341674805
Loss :  1.6561479568481445 3.4483461380004883 5.104494094848633
Loss :  1.6756333112716675 3.0241456031799316 4.699779033660889
Loss :  1.6702698469161987 2.7369544506073 4.407224178314209
Loss :  1.672624111175537 2.40032958984375 4.072953701019287
Loss :  1.674556016921997 2.7137134075164795 4.388269424438477
Loss :  1.657004714012146 2.5165319442749023 4.173536777496338
Loss :  1.663560390472412 2.613968849182129 4.277529239654541
Loss :  1.647760272026062 2.692042112350464 4.339802265167236
Loss :  1.6885586977005005 2.5050721168518066 4.193630695343018
Loss :  1.661683440208435 2.5942270755767822 4.255910396575928
Loss :  1.6790533065795898 2.7858707904815674 4.464923858642578
Loss :  1.6916841268539429 2.989562511444092 4.681246757507324
Loss :  1.6737135648727417 2.9277987480163574 4.601512432098389
Loss :  1.6818606853485107 3.1718862056732178 4.8537468910217285
Loss :  1.6473989486694336 2.8332951068878174 4.480693817138672
Loss :  1.7001502513885498 3.20945143699646 4.90960168838501
Loss :  1.6875603199005127 3.3281354904174805 5.015695571899414
Loss :  1.709828495979309 3.2538912296295166 4.963719844818115
Loss :  1.685118317604065 2.663886308670044 4.349004745483398
  batch 60 loss: 1.685118317604065, 2.663886308670044, 4.349004745483398
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6810909509658813 3.025667667388916 4.706758499145508
Loss :  1.677390456199646 3.3163578510284424 4.993748188018799
Loss :  1.6856718063354492 3.231130361557007 4.916802406311035
Loss :  1.676175832748413 2.8788726329803467 4.55504846572876
Loss :  1.6678507328033447 2.6565027236938477 4.324353218078613
Loss :  2.9302637577056885 4.483459949493408 7.413723945617676
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  2.9032955169677734 4.54468297958374 7.447978496551514
Loss :  2.9357516765594482 4.4007086753845215 7.336460113525391
Loss :  2.435743570327759 4.239551544189453 6.675294876098633
Total LOSS train 4.597251488612248 valid 7.218364357948303
CE LOSS train 1.6762794256210327 valid 0.6089358925819397
Contrastive LOSS train 2.9209720758291393 valid 1.0598878860473633
EPOCH 54:
Loss :  1.6605393886566162 2.8505756855010986 4.511115074157715
Loss :  1.6765002012252808 3.1631529331207275 4.839653015136719
Loss :  1.6568567752838135 2.6981425285339355 4.354999542236328
Loss :  1.660233736038208 2.745697498321533 4.40593147277832
Loss :  1.6904340982437134 2.6769087314605713 4.367342948913574
Loss :  1.6667718887329102 2.7332398891448975 4.400012016296387
Loss :  1.6769615411758423 2.726036548614502 4.402997970581055
Loss :  1.665112853050232 2.5117592811584473 4.176872253417969
Loss :  1.665772557258606 2.769970417022705 4.4357428550720215
Loss :  1.62604558467865 2.550772190093994 4.176817893981934
Loss :  1.6772656440734863 2.8347320556640625 4.511997699737549
Loss :  1.7337639331817627 2.686997175216675 4.4207611083984375
Loss :  1.6811262369155884 2.6724019050598145 4.353528022766113
Loss :  1.6767308712005615 2.635974407196045 4.312705039978027
Loss :  1.6567972898483276 2.4306864738464355 4.087483882904053
Loss :  1.6647855043411255 2.542900562286377 4.207685947418213
Loss :  1.669432282447815 2.966399669647217 4.635831832885742
Loss :  1.672267198562622 2.740816831588745 4.413084030151367
Loss :  1.6718649864196777 2.5609099864959717 4.23277473449707
Loss :  1.641710877418518 2.7685155868530273 4.410226345062256
  batch 20 loss: 1.641710877418518, 2.7685155868530273, 4.410226345062256
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6764987707138062 2.6957881450653076 4.372286796569824
Loss :  1.6840307712554932 2.990873098373413 4.674903869628906
Loss :  1.6644256114959717 2.799792528152466 4.4642181396484375
Loss :  1.6954355239868164 2.746708631515503 4.442144393920898
Loss :  1.6950733661651611 3.369861602783203 5.064934730529785
Loss :  1.6720244884490967 3.1238927841186523 4.795917510986328
Loss :  1.708033561706543 3.2557647228240967 4.963798522949219
Loss :  1.6530743837356567 3.089672327041626 4.742746829986572
Loss :  1.7028108835220337 2.934983015060425 4.637794017791748
Loss :  1.656152367591858 3.134589195251465 4.790741443634033
Loss :  1.7329001426696777 3.0441906452178955 4.777091026306152
Loss :  1.6807148456573486 2.9794721603393555 4.660186767578125
Loss :  1.6633622646331787 2.83170223236084 4.495064735412598
Loss :  1.6753178834915161 3.141925096511841 4.8172430992126465
Loss :  1.7064729928970337 3.451566457748413 5.158039569854736
Loss :  1.6986956596374512 3.1575162410736084 4.8562116622924805
Loss :  1.6803942918777466 2.9820544719696045 4.662448883056641
Loss :  1.647857427597046 2.757235050201416 4.405092239379883
Loss :  1.6729110479354858 2.6381404399871826 4.311051368713379
Loss :  1.6678862571716309 2.6333677768707275 4.3012542724609375
  batch 40 loss: 1.6678862571716309, 2.6333677768707275, 4.3012542724609375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6709448099136353 2.7386319637298584 4.409576892852783
Loss :  1.6581603288650513 2.6458919048309326 4.304052352905273
Loss :  1.6776740550994873 2.8854076862335205 4.563081741333008
Loss :  1.6731432676315308 2.863471269607544 4.536614418029785
Loss :  1.6741178035736084 2.6595253944396973 4.333642959594727
Loss :  1.6749722957611084 2.499281406402588 4.174253463745117
Loss :  1.6576876640319824 2.32914137840271 3.9868290424346924
Loss :  1.6659557819366455 2.481771945953369 4.147727966308594
Loss :  1.64714515209198 2.650801420211792 4.297946453094482
Loss :  1.69187593460083 2.6080121994018555 4.2998881340026855
Loss :  1.6647599935531616 2.5667340755462646 4.231493949890137
Loss :  1.6808977127075195 2.4483840465545654 4.129281997680664
Loss :  1.6924363374710083 2.393831491470337 4.086267948150635
Loss :  1.6785497665405273 2.648995876312256 4.327545642852783
Loss :  1.683790922164917 2.6459991931915283 4.329790115356445
Loss :  1.6503602266311646 2.5347588062286377 4.185119152069092
Loss :  1.7011851072311401 2.642172336578369 4.343357563018799
Loss :  1.688950538635254 3.0585076808929443 4.747458457946777
Loss :  1.7105028629302979 2.9874253273010254 4.697928428649902
Loss :  1.6877381801605225 2.625439167022705 4.313177108764648
  batch 60 loss: 1.6877381801605225, 2.625439167022705, 4.313177108764648
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6818863153457642 2.847651243209839 4.529537677764893
Loss :  1.6796914339065552 3.1508638858795166 4.830555438995361
Loss :  1.6872719526290894 2.860358715057373 4.547630786895752
Loss :  1.676996111869812 3.455876588821411 5.132872581481934
Loss :  1.669578194618225 2.708125114440918 4.3777031898498535
Loss :  2.5391736030578613 4.436426639556885 6.975600242614746
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  2.5231754779815674 4.501616954803467 7.024792671203613
Loss :  2.5425667762756348 4.378282070159912 6.920848846435547
Loss :  2.1461057662963867 4.424676418304443 6.57078218460083
Total LOSS train 4.475601031230046 valid 6.873005986213684
CE LOSS train 1.6761741344745342 valid 0.5365264415740967
Contrastive LOSS train 2.799426878415621 valid 1.1061691045761108
EPOCH 55:
Loss :  1.6631982326507568 3.060863733291626 4.724061965942383
Loss :  1.6771433353424072 3.455380916595459 5.132524490356445
Loss :  1.659475326538086 3.18636155128479 4.845836639404297
Loss :  1.6628344058990479 2.861821174621582 4.524655342102051
Loss :  1.6924052238464355 2.597672939300537 4.290078163146973
Loss :  1.6691585779190063 2.7248404026031494 4.393999099731445
Loss :  1.6775543689727783 2.6696577072143555 4.347211837768555
Loss :  1.6661311388015747 2.4640276432037354 4.1301589012146
Loss :  1.6675190925598145 2.306617021560669 3.9741361141204834
Loss :  1.6276477575302124 2.8535256385803223 4.481173515319824
Loss :  1.6785364151000977 3.5527472496032715 5.231283664703369
Loss :  1.7336851358413696 3.483703374862671 5.21738862991333
Loss :  1.6817848682403564 3.444805383682251 5.126590251922607
Loss :  1.6772090196609497 3.502619743347168 5.179828643798828
Loss :  1.6569637060165405 3.181284189224243 4.838247776031494
Loss :  1.6635057926177979 3.4464900493621826 5.1099958419799805
Loss :  1.6693907976150513 3.0955893993377686 4.764980316162109
Loss :  1.6722412109375 3.0100257396698 4.682267189025879
Loss :  1.6695940494537354 2.5184059143066406 4.187999725341797
Loss :  1.6413731575012207 2.6801722049713135 4.321545600891113
  batch 20 loss: 1.6413731575012207, 2.6801722049713135, 4.321545600891113
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6761229038238525 2.7069284915924072 4.38305139541626
Loss :  1.6844711303710938 2.8044822216033936 4.488953590393066
Loss :  1.6655548810958862 2.587184190750122 4.252738952636719
Loss :  1.6949867010116577 2.8190178871154785 4.514004707336426
Loss :  1.6940152645111084 3.1870453357696533 4.881060600280762
Loss :  1.6726188659667969 2.9285120964050293 4.601130962371826
Loss :  1.7084760665893555 3.00785756111145 4.716333389282227
Loss :  1.6535390615463257 2.8245880603790283 4.4781270027160645
Loss :  1.70236074924469 3.0263237953186035 4.728684425354004
Loss :  1.654863715171814 3.189527988433838 4.844391822814941
Loss :  1.7315070629119873 3.288818120956421 5.020325183868408
Loss :  1.6809395551681519 3.0071499347686768 4.688089370727539
Loss :  1.662966012954712 3.0516510009765625 4.714616775512695
Loss :  1.6739355325698853 2.9415953159332275 4.615530967712402
Loss :  1.704943060874939 3.429147243499756 5.134090423583984
Loss :  1.6977571249008179 3.4264461994171143 5.124203205108643
Loss :  1.679847240447998 3.432098865509033 5.111946105957031
Loss :  1.6478068828582764 2.95312762260437 4.6009345054626465
Loss :  1.67183518409729 2.9294304847717285 4.601265907287598
Loss :  1.6674171686172485 2.9665441513061523 4.633961200714111
  batch 40 loss: 1.6674171686172485, 2.9665441513061523, 4.633961200714111
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6700525283813477 2.4946110248565674 4.164663314819336
Loss :  1.6558867692947388 2.350605010986328 4.006491661071777
Loss :  1.6765623092651367 2.5186028480529785 4.195165157318115
Loss :  1.67026948928833 2.404085636138916 4.074355125427246
Loss :  1.6735727787017822 2.2498409748077393 3.9234137535095215
Loss :  1.6752959489822388 2.529895544052124 4.205191612243652
Loss :  1.6571139097213745 2.398676633834839 4.055790424346924
Loss :  1.6645475625991821 2.60854434967041 4.273091793060303
Loss :  1.6488630771636963 2.5841102600097656 4.232973098754883
Loss :  1.6891758441925049 2.649005889892578 4.338181495666504
Loss :  1.6625739336013794 2.734635353088379 4.397209167480469
Loss :  1.6802701950073242 2.7473344802856445 4.427604675292969
Loss :  1.6926052570343018 2.8354697227478027 4.528075218200684
Loss :  1.675789713859558 2.952794313430786 4.628583908081055
Loss :  1.6824421882629395 2.8527863025665283 4.535228729248047
Loss :  1.6497654914855957 2.7550346851348877 4.4048004150390625
Loss :  1.7006349563598633 2.7483530044555664 4.44898796081543
Loss :  1.6892290115356445 2.7977828979492188 4.487011909484863
Loss :  1.7104204893112183 3.0395915508270264 4.750011920928955
Loss :  1.6878011226654053 2.606483221054077 4.294284343719482
  batch 60 loss: 1.6878011226654053, 2.606483221054077, 4.294284343719482
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.681738018989563 3.0771806240081787 4.758918762207031
Loss :  1.6798313856124878 3.09759783744812 4.777429103851318
Loss :  1.6876479387283325 2.670133113861084 4.357780933380127
Loss :  1.6765297651290894 2.949364185333252 4.625894069671631
Loss :  1.6694279909133911 2.436842679977417 4.106270790100098
Loss :  2.5036654472351074 4.483814716339111 6.987480163574219
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  2.4872424602508545 4.486688613891602 6.973931312561035
Loss :  2.505922555923462 4.268899917602539 6.774822235107422
Loss :  2.11972713470459 4.370393753051758 6.490120887756348
Total LOSS train 4.56361251610976 valid 6.806588649749756
CE LOSS train 1.6760209762133085 valid 0.5299317836761475
Contrastive LOSS train 2.887591549066397 valid 1.0925984382629395
EPOCH 56:
Loss :  1.66395103931427 2.8017072677612305 4.465658187866211
Loss :  1.676464557647705 3.0128211975097656 4.689285755157471
Loss :  1.6597901582717896 2.8746514320373535 4.5344414710998535
Loss :  1.6644375324249268 2.6524243354797363 4.316862106323242
Loss :  1.6929391622543335 2.817237377166748 4.510176658630371
Loss :  1.6700892448425293 2.6991147994995117 4.369204044342041
Loss :  1.6782346963882446 2.8958983421325684 4.574132919311523
Loss :  1.6663708686828613 2.5360357761383057 4.202406883239746
Loss :  1.66777503490448 2.542454481124878 4.210229396820068
Loss :  1.6274195909500122 2.519895076751709 4.147314548492432
Loss :  1.6784192323684692 2.6400959491729736 4.318515300750732
Loss :  1.7345527410507202 2.8682937622070312 4.602846622467041
Loss :  1.6817232370376587 2.854750633239746 4.536473751068115
Loss :  1.6766777038574219 2.6824355125427246 4.3591132164001465
Loss :  1.659074306488037 2.903468132019043 4.56254243850708
Loss :  1.6641933917999268 2.828718423843384 4.4929118156433105
Loss :  1.669348120689392 3.1028711795806885 4.772219181060791
Loss :  1.6727807521820068 2.813441753387451 4.486222267150879
Loss :  1.6715879440307617 2.859558343887329 4.531146049499512
Loss :  1.6427375078201294 2.9719460010528564 4.614683628082275
  batch 20 loss: 1.6427375078201294, 2.9719460010528564, 4.614683628082275
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6777291297912598 2.7093050479888916 4.3870344161987305
Loss :  1.6854455471038818 2.92714262008667 4.612587928771973
Loss :  1.6671141386032104 2.714613676071167 4.381727695465088
Loss :  1.6963815689086914 2.7542266845703125 4.450608253479004
Loss :  1.6968692541122437 3.0154290199279785 4.712298393249512
Loss :  1.6743777990341187 2.731142997741699 4.405520915985107
Loss :  1.7093309164047241 2.9534590244293213 4.662789821624756
Loss :  1.6542948484420776 2.8235042095184326 4.477798938751221
Loss :  1.7028594017028809 3.1638762950897217 4.866735458374023
Loss :  1.657588243484497 3.177539110183716 4.835127353668213
Loss :  1.7342743873596191 3.258798837661743 4.993073463439941
Loss :  1.6834919452667236 3.063176155090332 4.746667861938477
Loss :  1.664793610572815 3.2702245712280273 4.935018062591553
Loss :  1.6777451038360596 2.825531005859375 4.5032758712768555
Loss :  1.7087081670761108 3.021759510040283 4.730467796325684
Loss :  1.7001161575317383 3.0470261573791504 4.747142314910889
Loss :  1.683152198791504 2.849243402481079 4.532395362854004
Loss :  1.6505626440048218 2.6694467067718506 4.320009231567383
Loss :  1.6738680601119995 2.799830198287964 4.473698139190674
Loss :  1.6701041460037231 3.0155677795410156 4.685671806335449
  batch 40 loss: 1.6701041460037231, 3.0155677795410156, 4.685671806335449
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6723980903625488 2.988469362258911 4.660867691040039
Loss :  1.65830397605896 2.9562828540802 4.61458683013916
Loss :  1.6790108680725098 3.0369842052459717 4.715994834899902
Loss :  1.6715247631072998 3.1732633113861084 4.844788074493408
Loss :  1.6752806901931763 2.798309564590454 4.47359037399292
Loss :  1.6769777536392212 3.0227532386779785 4.69973087310791
Loss :  1.6580805778503418 2.8676865100860596 4.5257673263549805
Loss :  1.665105938911438 2.9691998958587646 4.634305953979492
Loss :  1.6496614217758179 3.013324499130249 4.662985801696777
Loss :  1.6899938583374023 3.173967123031616 4.863961219787598
Loss :  1.664506435394287 3.1075711250305176 4.772077560424805
Loss :  1.6810119152069092 3.267643928527832 4.94865608215332
Loss :  1.692524790763855 3.044205904006958 4.736730575561523
Loss :  1.6751904487609863 2.6697123050689697 4.344902992248535
Loss :  1.6839253902435303 2.5455985069274902 4.229523658752441
Loss :  1.6488784551620483 2.404027223587036 4.052905559539795
Loss :  1.7006053924560547 2.760283946990967 4.4608893394470215
Loss :  1.6892833709716797 2.74806809425354 4.437351226806641
Loss :  1.7114192247390747 2.911921739578247 4.623341083526611
Loss :  1.6879810094833374 2.670742988586426 4.358724117279053
  batch 60 loss: 1.6879810094833374, 2.670742988586426, 4.358724117279053
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6810252666473389 2.6951053142547607 4.3761305809021
Loss :  1.6786952018737793 2.676999092102051 4.35569429397583
Loss :  1.6859996318817139 2.359282970428467 4.045282363891602
Loss :  1.6762633323669434 2.775815010070801 4.452078342437744
Loss :  1.6686668395996094 2.7562201023101807 4.424886703491211
Loss :  2.613579034805298 4.537608623504639 7.151187896728516
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  2.595639705657959 4.308907985687256 6.904547691345215
Loss :  2.6147632598876953 4.342275619506836 6.957038879394531
Loss :  2.204418659210205 4.368393421173096 6.572812080383301
Total LOSS train 4.539565981351412 valid 6.896396636962891
CE LOSS train 1.6770721343847421 valid 0.5511046648025513
Contrastive LOSS train 2.862493870808528 valid 1.092098355293274
EPOCH 57:
Loss :  1.6616361141204834 2.9996345043182373 4.661270618438721
Loss :  1.6766732931137085 2.973951578140259 4.650624752044678
Loss :  1.6575829982757568 3.190329074859619 4.847911834716797
Loss :  1.6597176790237427 3.146660089492798 4.80637788772583
Loss :  1.6928088665008545 3.024481773376465 4.717290878295898
Loss :  1.667250633239746 2.7793633937835693 4.4466142654418945
Loss :  1.678754210472107 3.2631373405456543 4.941891670227051
Loss :  1.6657248735427856 3.23238468170166 4.898109436035156
Loss :  1.666518211364746 3.2022242546081543 4.8687424659729
Loss :  1.6282212734222412 2.829897403717041 4.458118438720703
Loss :  1.6786116361618042 3.3771181106567383 5.055729866027832
Loss :  1.731649398803711 3.3833956718444824 5.115045070648193
Loss :  1.6816329956054688 3.2521376609802246 4.933770656585693
Loss :  1.6774100065231323 3.3488991260528564 5.026309013366699
Loss :  1.6559820175170898 2.985652446746826 4.641634464263916
Loss :  1.6634737253189087 2.962813138961792 4.62628698348999
Loss :  1.668419361114502 2.8380837440490723 4.506503105163574
Loss :  1.6728943586349487 2.864539861679077 4.537434101104736
Loss :  1.6677062511444092 3.1235265731811523 4.791233062744141
Loss :  1.6406790018081665 3.19167423248291 4.832353115081787
  batch 20 loss: 1.6406790018081665, 3.19167423248291, 4.832353115081787
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6757224798202515 3.190258264541626 4.865980625152588
Loss :  1.684002161026001 3.5366711616516113 5.220673561096191
Loss :  1.665648102760315 3.0224673748016357 4.68811559677124
Loss :  1.693832278251648 3.3730056285858154 5.066837787628174
Loss :  1.6923531293869019 3.468158483505249 5.160511493682861
Loss :  1.6729180812835693 3.455087184906006 5.128005027770996
Loss :  1.708687424659729 3.6657371520996094 5.374424457550049
Loss :  1.653134822845459 3.5016164779663086 5.154751300811768
Loss :  1.7020436525344849 2.7949397563934326 4.496983528137207
Loss :  1.652640461921692 2.583743095397949 4.236383438110352
Loss :  1.7301244735717773 2.6568686962127686 4.386993408203125
Loss :  1.6811612844467163 2.63307523727417 4.314236640930176
Loss :  1.6625221967697144 2.707469940185547 4.369992256164551
Loss :  1.6734225749969482 2.542592763900757 4.216015338897705
Loss :  1.704030990600586 2.899996042251587 4.604026794433594
Loss :  1.6971461772918701 2.770441770553589 4.467587947845459
Loss :  1.6803702116012573 2.655559778213501 4.335929870605469
Loss :  1.649032473564148 2.721987009048462 4.37101936340332
Loss :  1.6714235544204712 2.88509202003479 4.556515693664551
Loss :  1.6682344675064087 2.8248274326324463 4.4930620193481445
  batch 40 loss: 1.6682344675064087, 2.8248274326324463, 4.4930620193481445
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.670630931854248 2.9220645427703857 4.592695236206055
Loss :  1.6555063724517822 2.7564802169799805 4.411986351013184
Loss :  1.6770020723342896 2.927109718322754 4.604111671447754
Loss :  1.6697561740875244 2.82377028465271 4.493526458740234
Loss :  1.6742150783538818 2.660088062286377 4.33430290222168
Loss :  1.6759977340698242 2.925750970840454 4.601748466491699
Loss :  1.6576197147369385 3.0237503051757812 4.681369781494141
Loss :  1.6646322011947632 2.9376723766326904 4.602304458618164
Loss :  1.6486761569976807 2.9408204555511475 4.589496612548828
Loss :  1.6884920597076416 3.0184366703033447 4.706928730010986
Loss :  1.6618808507919312 3.0917611122131348 4.7536420822143555
Loss :  1.6799395084381104 2.8943824768066406 4.574321746826172
Loss :  1.692128300666809 2.949734926223755 4.6418633460998535
Loss :  1.673933982849121 2.9038617610931396 4.57779598236084
Loss :  1.6817994117736816 3.0690088272094727 4.750808238983154
Loss :  1.6485430002212524 2.798535108566284 4.447078227996826
Loss :  1.699294090270996 3.02810001373291 4.727394104003906
Loss :  1.6879836320877075 3.0621039867401123 4.750087738037109
Loss :  1.7094676494598389 3.2133142948150635 4.922781944274902
Loss :  1.6859123706817627 2.8866140842437744 4.572526454925537
  batch 60 loss: 1.6859123706817627, 2.8866140842437744, 4.572526454925537
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.680247187614441 2.9325239658355713 4.612771034240723
Loss :  1.6774704456329346 2.9018707275390625 4.579340934753418
Loss :  1.68561589717865 2.689213752746582 4.3748297691345215
Loss :  1.6750683784484863 2.611720323562622 4.2867889404296875
Loss :  1.6677675247192383 2.0796077251434326 3.747375249862671
Loss :  2.834893226623535 4.433167934417725 7.26806116104126
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  2.8111541271209717 4.536169052124023 7.347323417663574
Loss :  2.8373963832855225 4.356221675872803 7.193617820739746
Loss :  2.375077486038208 4.387588024139404 6.762665748596191
Total LOSS train 4.658602681526771 valid 7.142917037010193
CE LOSS train 1.6754058251014123 valid 0.593769371509552
Contrastive LOSS train 2.983196871097271 valid 1.096897006034851
EPOCH 58:
Loss :  1.6608351469039917 2.312997341156006 3.973832607269287
Loss :  1.6755521297454834 2.7875757217407227 4.463128089904785
Loss :  1.6569490432739258 2.3192927837371826 3.9762418270111084
Loss :  1.6601165533065796 2.5068888664245605 4.16700553894043
Loss :  1.6917611360549927 2.638901948928833 4.330663204193115
Loss :  1.6669385433197021 2.559230089187622 4.226168632507324
Loss :  1.6766197681427002 2.5977940559387207 4.2744140625
Loss :  1.665148138999939 2.4133098125457764 4.078457832336426
Loss :  1.666113257408142 2.664182424545288 4.330295562744141
Loss :  1.6269723176956177 2.51667857170105 4.143651008605957
Loss :  1.6788753271102905 2.850247621536255 4.529122829437256
Loss :  1.7314518690109253 2.9664151668548584 4.697866916656494
Loss :  1.6819485425949097 2.8577585220336914 4.539707183837891
Loss :  1.6767487525939941 3.0506436824798584 4.727392196655273
Loss :  1.6554452180862427 3.0520970821380615 4.707542419433594
Loss :  1.6632311344146729 3.2217586040496826 4.8849897384643555
Loss :  1.6691064834594727 3.1176300048828125 4.786736488342285
Loss :  1.6716147661209106 3.083214521408081 4.754829406738281
Loss :  1.6680954694747925 3.025963068008423 4.694058418273926
Loss :  1.6408791542053223 3.054731607437134 4.695611000061035
  batch 20 loss: 1.6408791542053223, 3.054731607437134, 4.695611000061035
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.675899863243103 3.133334159851074 4.809234142303467
Loss :  1.6841130256652832 3.1755728721618652 4.859685897827148
Loss :  1.6641348600387573 2.799474000930786 4.463608741760254
Loss :  1.693429946899414 2.7580065727233887 4.451436519622803
Loss :  1.6920208930969238 3.0124778747558594 4.704498767852783
Loss :  1.6713604927062988 3.1310832500457764 4.802443504333496
Loss :  1.7080488204956055 3.7291362285614014 5.437185287475586
Loss :  1.6535812616348267 3.3806610107421875 5.034242153167725
Loss :  1.7028602361679077 3.1642260551452637 4.867086410522461
Loss :  1.6525930166244507 3.3383283615112305 4.990921497344971
Loss :  1.7298400402069092 3.361941337585449 5.0917816162109375
Loss :  1.6806130409240723 3.669525146484375 5.350138187408447
Loss :  1.6624462604522705 2.8939483165740967 4.556394577026367
Loss :  1.672437310218811 3.1317269802093506 4.804164409637451
Loss :  1.704465389251709 3.0426337718963623 4.747098922729492
Loss :  1.6965500116348267 2.8397867679595947 4.536336898803711
Loss :  1.6791322231292725 2.502734899520874 4.1818671226501465
Loss :  1.6473186016082764 2.386209726333618 4.0335283279418945
Loss :  1.6706830263137817 2.4798460006713867 4.150528907775879
Loss :  1.666414499282837 2.3901748657226562 4.056589126586914
  batch 40 loss: 1.666414499282837, 2.3901748657226562, 4.056589126586914
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.669919729232788 2.4971303939819336 4.167050361633301
Loss :  1.6545096635818481 2.3119044303894043 3.966413974761963
Loss :  1.6759922504425049 2.643568992614746 4.319561004638672
Loss :  1.6698507070541382 2.76542592048645 4.435276508331299
Loss :  1.6734477281570435 2.413968086242676 4.08741569519043
Loss :  1.6743608713150024 2.5267093181610107 4.201070308685303
Loss :  1.6572039127349854 2.596184730529785 4.253388404846191
Loss :  1.6646474599838257 2.568798542022705 4.23344612121582
Loss :  1.6463515758514404 2.8022077083587646 4.448559284210205
Loss :  1.687788486480713 2.593043088912964 4.280831336975098
Loss :  1.6604551076889038 2.91056752204895 4.5710225105285645
Loss :  1.678486943244934 2.7997589111328125 4.478245735168457
Loss :  1.6914186477661133 2.8664207458496094 4.557839393615723
Loss :  1.6726654767990112 2.8079466819763184 4.480612277984619
Loss :  1.681040644645691 2.7994892597198486 4.48052978515625
Loss :  1.648641586303711 2.7447946071624756 4.393436431884766
Loss :  1.698209285736084 2.774522066116333 4.472731590270996
Loss :  1.6866463422775269 3.375187397003174 5.06183385848999
Loss :  1.707751989364624 3.4093406200408936 5.117092609405518
Loss :  1.6848808526992798 3.1143763065338135 4.799257278442383
  batch 60 loss: 1.6848808526992798, 3.1143763065338135, 4.799257278442383
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6801413297653198 3.1584298610687256 4.838571071624756
Loss :  1.676993489265442 3.091881036758423 4.768874645233154
Loss :  1.6859862804412842 3.001085042953491 4.687071323394775
Loss :  1.6736342906951904 3.056027889251709 4.72966194152832
Loss :  1.666214108467102 2.750741720199585 4.416955947875977
Loss :  2.923795700073242 4.509167671203613 7.4329633712768555
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  2.8958230018615723 4.474884986877441 7.370707988739014
Loss :  2.92427659034729 4.2990641593933105 7.22334098815918
Loss :  2.454294204711914 4.454422950744629 6.908717155456543
Total LOSS train 4.540880544369037 valid 7.233932375907898
CE LOSS train 1.6747628358694224 valid 0.6135735511779785
Contrastive LOSS train 2.8661177011636587 valid 1.1136057376861572
EPOCH 59:
Loss :  1.6599195003509521 2.8854870796203613 4.545406341552734
Loss :  1.6760796308517456 3.153165102005005 4.829244613647461
Loss :  1.655977487564087 2.7797088623046875 4.435686111450195
Loss :  1.6592472791671753 2.523249387741089 4.182496547698975
Loss :  1.6891977787017822 2.844982862472534 4.534180641174316
Loss :  1.6664531230926514 2.8195669651031494 4.486020088195801
Loss :  1.6739494800567627 2.8012423515319824 4.475192070007324
Loss :  1.6634981632232666 2.537919521331787 4.201417922973633
Loss :  1.6650688648223877 2.5510215759277344 4.216090202331543
Loss :  1.62216055393219 2.47601580619812 4.0981764793396
Loss :  1.6755940914154053 2.73364520072937 4.409239292144775
Loss :  1.7329145669937134 2.9294862747192383 4.662400722503662
Loss :  1.6798778772354126 2.9935781955718994 4.673456192016602
Loss :  1.6754173040390015 2.7577064037323 4.433123588562012
Loss :  1.6554287672042847 2.6584222316741943 4.3138508796691895
Loss :  1.660077691078186 2.8338732719421387 4.493950843811035
Loss :  1.6690127849578857 2.7119367122650146 4.3809494972229
Loss :  1.6679713726043701 2.590945243835449 4.258916854858398
Loss :  1.66887629032135 2.4994020462036133 4.168278217315674
Loss :  1.6399543285369873 3.1545236110687256 4.794477939605713
  batch 20 loss: 1.6399543285369873, 3.1545236110687256, 4.794477939605713
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6749138832092285 2.8510093688964844 4.525923252105713
Loss :  1.6847357749938965 2.9877545833587646 4.672490119934082
Loss :  1.6633650064468384 3.1365201473236084 4.799885272979736
Loss :  1.694075107574463 2.8790693283081055 4.573144435882568
Loss :  1.6932876110076904 2.926215887069702 4.619503498077393
Loss :  1.670032262802124 3.1151950359344482 4.785227298736572
Loss :  1.7061818838119507 3.106372356414795 4.812554359436035
Loss :  1.6518217325210571 3.3038344383239746 4.955656051635742
Loss :  1.7016700506210327 3.0548970699310303 4.756567001342773
Loss :  1.65389883518219 3.046926736831665 4.7008256912231445
Loss :  1.7314757108688354 3.0755066871643066 4.806982517242432
Loss :  1.679229736328125 3.034243583679199 4.713473320007324
Loss :  1.661849021911621 3.0229432582855225 4.684792518615723
Loss :  1.6726206541061401 2.9264755249023438 4.599096298217773
Loss :  1.70542311668396 3.4258415699005127 5.131264686584473
Loss :  1.6970840692520142 3.324202060699463 5.0212860107421875
Loss :  1.6794233322143555 3.0912394523620605 4.770662784576416
Loss :  1.6449676752090454 2.9335334300994873 4.578501224517822
Loss :  1.6715388298034668 3.021721839904785 4.693260669708252
Loss :  1.6658517122268677 2.825864553451538 4.491716384887695
  batch 40 loss: 1.6658517122268677, 2.825864553451538, 4.491716384887695
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6699211597442627 2.6759769916534424 4.345898151397705
Loss :  1.656587839126587 2.527759313583374 4.184347152709961
Loss :  1.6773738861083984 2.705510377883911 4.3828840255737305
Loss :  1.6727603673934937 2.6809725761413574 4.353733062744141
Loss :  1.674773931503296 2.437227964401245 4.112001895904541
Loss :  1.6749564409255981 2.544159412384033 4.219115734100342
Loss :  1.6586765050888062 2.410572052001953 4.069248676300049
Loss :  1.6669871807098389 2.4633758068084717 4.1303629875183105
Loss :  1.6461856365203857 2.5724809169769287 4.2186665534973145
Loss :  1.6916152238845825 2.7660281658172607 4.457643508911133
Loss :  1.6640974283218384 2.930305004119873 4.594402313232422
Loss :  1.6801245212554932 2.928466320037842 4.608591079711914
Loss :  1.6926162242889404 2.75219988822937 4.4448161125183105
Loss :  1.6786384582519531 3.2694783210754395 4.948116779327393
Loss :  1.6841230392456055 3.142909288406372 4.827032089233398
Loss :  1.6505175828933716 3.036423921585083 4.686941623687744
Loss :  1.700441598892212 3.0512242317199707 4.751666069030762
Loss :  1.689727783203125 2.9632644653320312 4.652992248535156
Loss :  1.709177851676941 3.307807683944702 5.0169854164123535
Loss :  1.6862763166427612 2.81430983543396 4.500586032867432
  batch 60 loss: 1.6862763166427612, 2.81430983543396, 4.500586032867432
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6811625957489014 3.2373175621032715 4.918479919433594
Loss :  1.6800049543380737 2.880420207977295 4.560425281524658
Loss :  1.687253475189209 2.6315841674804688 4.318837642669678
Loss :  1.6760499477386475 2.6752724647521973 4.351322174072266
Loss :  1.6696805953979492 2.3383514881134033 4.008031845092773
Loss :  2.8426098823547363 4.547433853149414 7.39004373550415
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  2.817423105239868 4.514750003814697 7.3321733474731445
Loss :  2.846385955810547 4.372564315795898 7.218950271606445
Loss :  2.3921191692352295 4.364986896514893 6.757105827331543
Total LOSS train 4.5376691818237305 valid 7.174568295478821
CE LOSS train 1.6750746690309966 valid 0.5980297923088074
Contrastive LOSS train 2.8625945237966683 valid 1.0912467241287231
EPOCH 60:
Loss :  1.6629528999328613 2.5829577445983887 4.24591064453125
Loss :  1.6762889623641968 2.8352160453796387 4.511505126953125
Loss :  1.6602346897125244 2.6252665519714355 4.285501480102539
Loss :  1.6655203104019165 2.8043365478515625 4.4698567390441895
Loss :  1.691465973854065 3.1473824977874756 4.83884859085083
Loss :  1.6707342863082886 2.887085199356079 4.557819366455078
Loss :  1.6749917268753052 3.207075595855713 4.8820672035217285
Loss :  1.6653300523757935 2.7974700927734375 4.462800025939941
Loss :  1.6674728393554688 3.1131112575531006 4.780584335327148
Loss :  1.6243866682052612 2.7682340145111084 4.39262056350708
Loss :  1.6777763366699219 3.0571208000183105 4.734897136688232
Loss :  1.7359302043914795 3.2105605602264404 4.94649076461792
Loss :  1.6801120042800903 3.222933769226074 4.903045654296875
Loss :  1.675747275352478 2.968545436859131 4.644292831420898
Loss :  1.6581203937530518 3.0190930366516113 4.677213668823242
Loss :  1.6623090505599976 3.258106231689453 4.92041540145874
Loss :  1.6690542697906494 3.328798532485962 4.997852802276611
Loss :  1.6700618267059326 3.171858072280884 4.841919898986816
Loss :  1.6726534366607666 2.8909783363342285 4.563632011413574
Loss :  1.6405936479568481 3.056318998336792 4.69691276550293
  batch 20 loss: 1.6405936479568481, 3.056318998336792, 4.69691276550293
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6764782667160034 3.0828967094421387 4.759375095367432
Loss :  1.6856682300567627 3.1402058601379395 4.825874328613281
Loss :  1.6642446517944336 2.776738166809082 4.440982818603516
Loss :  1.697374701499939 2.876861333847046 4.574235916137695
Loss :  1.6965826749801636 3.1718742847442627 4.868456840515137
Loss :  1.6714807748794556 2.743711233139038 4.415192127227783
Loss :  1.7071795463562012 3.1298251152038574 4.837004661560059
Loss :  1.6529736518859863 3.168248176574707 4.821221828460693
Loss :  1.7007733583450317 3.3884246349334717 5.089198112487793
Loss :  1.6562798023223877 2.937851667404175 4.5941314697265625
Loss :  1.733656883239746 3.192949056625366 4.926606178283691
Loss :  1.6804648637771606 3.613650321960449 5.29411506652832
Loss :  1.663296103477478 3.5863099098205566 5.249606132507324
Loss :  1.6738812923431396 3.328186511993408 5.002067565917969
Loss :  1.7080042362213135 3.343557119369507 5.05156135559082
Loss :  1.6977496147155762 3.609337091445923 5.307086944580078
Loss :  1.6808454990386963 3.1988422870635986 4.879687786102295
Loss :  1.6459301710128784 3.5481526851654053 5.194082736968994
Loss :  1.6741687059402466 3.3801510334014893 5.054319858551025
Loss :  1.66774582862854 3.4025025367736816 5.070248603820801
  batch 40 loss: 1.66774582862854, 3.4025025367736816, 5.070248603820801
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6719614267349243 3.651648759841919 5.323610305786133
Loss :  1.6590992212295532 3.2440361976623535 4.903135299682617
Loss :  1.6793767213821411 3.4473090171813965 5.126685619354248
Loss :  1.6732988357543945 3.422499895095825 5.095798492431641
Loss :  1.6753736734390259 3.4505133628845215 5.125886917114258
Loss :  1.6759730577468872 3.0933167934417725 4.769289970397949
Loss :  1.6584053039550781 3.243971586227417 4.902377128601074
Loss :  1.6669515371322632 3.170046806335449 4.836998462677002
Loss :  1.6461048126220703 3.392648935317993 5.038753509521484
Loss :  1.69015371799469 3.429896354675293 5.120049953460693
Loss :  1.662894368171692 2.9439330101013184 4.606827259063721
Loss :  1.6797605752944946 2.8575501441955566 4.537310600280762
Loss :  1.6926140785217285 2.6385622024536133 4.331176280975342
Loss :  1.6758731603622437 2.7222797870635986 4.398152828216553
Loss :  1.6840957403182983 2.8813047409057617 4.56540060043335
Loss :  1.6498897075653076 2.5022993087768555 4.152189254760742
Loss :  1.7003953456878662 2.579376459121704 4.27977180480957
Loss :  1.6884344816207886 2.6113808155059814 4.2998151779174805
Loss :  1.7098181247711182 2.876199960708618 4.586018085479736
Loss :  1.686671257019043 2.764392614364624 4.451064109802246
  batch 60 loss: 1.686671257019043, 2.764392614364624, 4.451064109802246
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6814405918121338 2.778414726257324 4.459855079650879
Loss :  1.6800200939178467 3.019805908203125 4.699826240539551
Loss :  1.6875122785568237 3.1119635105133057 4.79947566986084
Loss :  1.676650881767273 2.974558115005493 4.651208877563477
Loss :  1.6709680557250977 2.653994560241699 4.324962615966797
Loss :  2.6847052574157715 4.559351921081543 7.2440571784973145
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  2.667684555053711 4.616478443145752 7.284162998199463
Loss :  2.6900196075439453 4.317652702331543 7.007672309875488
Loss :  2.2775936126708984 4.31660795211792 6.594201564788818
Total LOSS train 4.753767424363357 valid 7.032523512840271
CE LOSS train 1.6763115809513973 valid 0.5693984031677246
Contrastive LOSS train 3.0774558250720685 valid 1.07915198802948
EPOCH 61:
Loss :  1.6635632514953613 3.112619638442993 4.776183128356934
Loss :  1.678257942199707 3.1988232135772705 4.877080917358398
Loss :  1.6623750925064087 3.2367918491363525 4.899167060852051
Loss :  1.6655213832855225 3.0301992893218994 4.695720672607422
Loss :  1.692726492881775 2.9646799564361572 4.657406330108643
Loss :  1.6712431907653809 2.9128053188323975 4.584048271179199
Loss :  1.6759145259857178 3.0025887489318848 4.678503036499023
Loss :  1.6660585403442383 3.0483787059783936 4.714437484741211
Loss :  1.6672567129135132 2.7336266040802 4.400883197784424
Loss :  1.6266242265701294 3.021488666534424 4.648112773895264
Loss :  1.680047869682312 3.141355037689209 4.8214030265808105
Loss :  1.7334001064300537 2.9199435710906982 4.653343677520752
Loss :  1.6817327737808228 3.4179799556732178 5.09971284866333
Loss :  1.6777522563934326 3.2029483318328857 4.880700588226318
Loss :  1.655430793762207 2.91318416595459 4.568614959716797
Loss :  1.6642472743988037 2.767774820327759 4.4320220947265625
Loss :  1.670383095741272 2.68689227104187 4.357275485992432
Loss :  1.6718811988830566 2.839096784591675 4.510977745056152
Loss :  1.671500325202942 2.481426477432251 4.152926921844482
Loss :  1.640511393547058 2.8684918880462646 4.509003162384033
  batch 20 loss: 1.640511393547058, 2.8684918880462646, 4.509003162384033
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6754343509674072 3.0272340774536133 4.702668190002441
Loss :  1.6847221851348877 3.277678966522217 4.962401390075684
Loss :  1.6623879671096802 3.0949900150299072 4.757378101348877
Loss :  1.6943343877792358 2.8654229640960693 4.559757232666016
Loss :  1.6923738718032837 3.450381278991699 5.142755031585693
Loss :  1.6704297065734863 3.419628620147705 5.090058326721191
Loss :  1.7084842920303345 3.138824224472046 4.84730863571167
Loss :  1.6540484428405762 3.2291388511657715 4.883187294006348
Loss :  1.7047432661056519 3.25569486618042 4.960438251495361
Loss :  1.6538771390914917 3.3887999057769775 5.04267692565918
Loss :  1.7309566736221313 3.5210020542144775 5.251958847045898
Loss :  1.6801817417144775 3.812904119491577 5.493085861206055
Loss :  1.6642223596572876 2.8609817028045654 4.525204181671143
Loss :  1.6729532480239868 3.3510656356811523 5.02401876449585
Loss :  1.7049384117126465 3.141129493713379 4.846067905426025
Loss :  1.6983964443206787 2.962139368057251 4.66053581237793
Loss :  1.6791210174560547 2.7438294887542725 4.422950744628906
Loss :  1.6488525867462158 2.7707436084747314 4.419596195220947
Loss :  1.6713697910308838 2.639789581298828 4.311159133911133
Loss :  1.6665982007980347 2.6307826042175293 4.2973809242248535
  batch 40 loss: 1.6665982007980347, 2.6307826042175293, 4.2973809242248535
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6696734428405762 2.843249559402466 4.512923240661621
Loss :  1.6572579145431519 2.7985928058624268 4.455850601196289
Loss :  1.6760650873184204 2.6283018589019775 4.3043670654296875
Loss :  1.670918583869934 2.5760419368743896 4.246960639953613
Loss :  1.6737937927246094 2.30407977104187 3.9778735637664795
Loss :  1.6753734350204468 2.5969297885894775 4.272303104400635
Loss :  1.6581307649612427 2.432415246963501 4.090546131134033
Loss :  1.6654452085494995 2.420344591140747 4.085789680480957
Loss :  1.6482594013214111 2.496619462966919 4.14487886428833
Loss :  1.688551902770996 2.5307624340057373 4.2193145751953125
Loss :  1.6609770059585571 2.5372402667999268 4.198217391967773
Loss :  1.6794815063476562 2.691054105758667 4.370535850524902
Loss :  1.692563772201538 2.8628363609313965 4.5553998947143555
Loss :  1.6733509302139282 2.9156291484832764 4.588980197906494
Loss :  1.6819196939468384 3.412243127822876 5.094162940979004
Loss :  1.6485145092010498 3.2150824069976807 4.8635969161987305
Loss :  1.6994997262954712 3.2094979286193848 4.908997535705566
Loss :  1.6883740425109863 3.120306968688965 4.808681011199951
Loss :  1.7089180946350098 3.304934501647949 5.013852596282959
Loss :  1.6840150356292725 3.049095630645752 4.733110427856445
  batch 60 loss: 1.6840150356292725, 3.049095630645752, 4.733110427856445
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6821955442428589 3.390869379043579 5.073064804077148
Loss :  1.6779253482818604 3.7957959175109863 5.473721504211426
Loss :  1.6875066757202148 3.2717225551605225 4.959229469299316
Loss :  1.6761384010314941 3.5125527381896973 5.188691139221191
Loss :  1.6688132286071777 3.0090770721435547 4.677890300750732
Loss :  2.827233076095581 4.474288463592529 7.301521301269531
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  2.799574851989746 4.499786853790283 7.299361705780029
Loss :  2.829913854598999 4.354347229003906 7.184261322021484
Loss :  2.3742613792419434 4.333956241607666 6.708217620849609
Total LOSS train 4.675954624322745 valid 7.123340487480164
CE LOSS train 1.675823347385113 valid 0.5935653448104858
Contrastive LOSS train 3.000131265933697 valid 1.0834890604019165
EPOCH 62:
Loss :  1.6609269380569458 2.964561700820923 4.625488758087158
Loss :  1.6788921356201172 3.2099435329437256 4.888835906982422
Loss :  1.658176064491272 2.604748010635376 4.2629241943359375
Loss :  1.6606884002685547 2.8309426307678223 4.491631031036377
Loss :  1.690280795097351 2.8104376792907715 4.500718593597412
Loss :  1.6680999994277954 2.468877077102661 4.136977195739746
Loss :  1.6751497983932495 2.5867910385131836 4.261940956115723
Loss :  1.665266990661621 2.2245824337005615 3.8898494243621826
Loss :  1.6656627655029297 2.4006571769714355 4.066319942474365
Loss :  1.6256823539733887 2.446678638458252 4.072360992431641
Loss :  1.678941249847412 2.699374198913574 4.378315448760986
Loss :  1.7321785688400269 3.287898302078247 5.020076751708984
Loss :  1.6816909313201904 3.3772358894348145 5.058926582336426
Loss :  1.6781748533248901 3.71431303024292 5.3924880027771
Loss :  1.653348445892334 3.391677141189575 5.045025825500488
Loss :  1.6650062799453735 3.5195963382720947 5.184602737426758
Loss :  1.67146897315979 3.429880380630493 5.101349353790283
Loss :  1.6712247133255005 2.8864312171936035 4.5576558113098145
Loss :  1.6704437732696533 3.36799955368042 5.038443565368652
Loss :  1.6400712728500366 3.22361421585083 4.863685607910156
  batch 20 loss: 1.6400712728500366, 3.22361421585083, 4.863685607910156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.675100326538086 3.5201072692871094 5.195207595825195
Loss :  1.6831341981887817 3.2965590953826904 4.979693412780762
Loss :  1.6608469486236572 3.2277636528015137 4.88861083984375
Loss :  1.6927217245101929 3.061779737472534 4.7545013427734375
Loss :  1.6910005807876587 3.225863218307495 4.916863918304443
Loss :  1.6699596643447876 2.834540605545044 4.504500389099121
Loss :  1.7081307172775269 2.635742425918579 4.343873023986816
Loss :  1.653441309928894 2.4628896713256836 4.116331100463867
Loss :  1.7054063081741333 2.5612313747406006 4.266637802124023
Loss :  1.6545491218566895 2.5730326175689697 4.227581977844238
Loss :  1.7318592071533203 2.909761667251587 4.641620635986328
Loss :  1.6795506477355957 2.6282808780670166 4.307831764221191
Loss :  1.6645556688308716 2.445943593978882 4.110499382019043
Loss :  1.6744208335876465 2.7037858963012695 4.378206729888916
Loss :  1.7058902978897095 2.860931873321533 4.566822052001953
Loss :  1.6993615627288818 2.5922391414642334 4.291600704193115
Loss :  1.6801313161849976 2.6169755458831787 4.297106742858887
Loss :  1.6485283374786377 2.537855863571167 4.186384201049805
Loss :  1.6736146211624146 2.61745285987854 4.291067600250244
Loss :  1.6677080392837524 2.406745672225952 4.074453830718994
  batch 40 loss: 1.6677080392837524, 2.406745672225952, 4.074453830718994
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6712218523025513 2.602168321609497 4.273390293121338
Loss :  1.660746693611145 2.2753243446350098 3.9360709190368652
Loss :  1.67833411693573 2.669133424758911 4.347467422485352
Loss :  1.6746752262115479 2.506664752960205 4.181340217590332
Loss :  1.6753802299499512 2.5038139820098877 4.179194450378418
Loss :  1.6762373447418213 2.722838878631592 4.399076461791992
Loss :  1.6596521139144897 2.49845290184021 4.15810489654541
Loss :  1.6678776741027832 2.6770479679107666 4.344925880432129
Loss :  1.6481894254684448 2.735751152038574 4.383940696716309
Loss :  1.6920982599258423 2.538557529449463 4.230655670166016
Loss :  1.664474606513977 2.778028726577759 4.442503452301025
Loss :  1.6807727813720703 2.405296564102173 4.086069107055664
Loss :  1.6934820413589478 2.3522233963012695 4.045705318450928
Loss :  1.6794559955596924 2.5814974308013916 4.260953426361084
Loss :  1.6848764419555664 2.7684926986694336 4.453369140625
Loss :  1.6517066955566406 2.923863649368286 4.575570106506348
Loss :  1.701413631439209 3.025120496749878 4.726533889770508
Loss :  1.6893539428710938 3.103731393814087 4.793085098266602
Loss :  1.7099554538726807 3.4954981803894043 5.205453872680664
Loss :  1.688134789466858 3.005845308303833 4.6939802169799805
  batch 60 loss: 1.688134789466858, 3.005845308303833, 4.6939802169799805
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6822459697723389 3.051175355911255 4.733421325683594
Loss :  1.6809802055358887 2.8794479370117188 4.560428142547607
Loss :  1.6877946853637695 2.7237205505371094 4.411515235900879
Loss :  1.676418662071228 3.0211172103881836 4.697535991668701
Loss :  1.6703978776931763 2.5104618072509766 4.180859565734863
Loss :  2.6621482372283936 4.5189032554626465 7.181051254272461
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  2.6393492221832275 4.51732063293457 7.156669616699219
Loss :  2.6642367839813232 4.343916416168213 7.008152961730957
Loss :  2.259819984436035 4.337062358856201 6.596882343292236
Total LOSS train 4.499664038878221 valid 6.985689043998718
CE LOSS train 1.6762640531246478 valid 0.5649549961090088
Contrastive LOSS train 2.8233999509077807 valid 1.0842655897140503
EPOCH 63:
Loss :  1.664307713508606 3.082502841949463 4.746810436248779
Loss :  1.6774356365203857 3.092628240585327 4.770063877105713
Loss :  1.6610974073410034 2.854079008102417 4.515176296234131
Loss :  1.6663575172424316 3.2078635692596436 4.874220848083496
Loss :  1.6918933391571045 3.2833352088928223 4.975228309631348
Loss :  1.6722670793533325 3.18538761138916 4.857654571533203
Loss :  1.6761730909347534 3.023437976837158 4.699611186981201
Loss :  1.6659682989120483 2.9789633750915527 4.644931793212891
Loss :  1.6684807538986206 3.094071388244629 4.762552261352539
Loss :  1.62599515914917 2.805824041366577 4.431818962097168
Loss :  1.6788381338119507 3.0357019901275635 4.714540004730225
Loss :  1.7361525297164917 3.088162899017334 4.824315547943115
Loss :  1.6816056966781616 3.093337297439575 4.774942874908447
Loss :  1.6773253679275513 3.0032966136932373 4.680622100830078
Loss :  1.6586092710494995 2.938606023788452 4.597215175628662
Loss :  1.665035367012024 3.1220571994781494 4.787092685699463
Loss :  1.6709641218185425 2.6649246215820312 4.335888862609863
Loss :  1.6722705364227295 2.426356315612793 4.098627090454102
Loss :  1.6747007369995117 2.538149356842041 4.212850093841553
Loss :  1.6417829990386963 2.607734441757202 4.249517440795898
  batch 20 loss: 1.6417829990386963, 2.607734441757202, 4.249517440795898
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6766663789749146 2.572895050048828 4.249561309814453
Loss :  1.68533456325531 2.9532968997955322 4.638631343841553
Loss :  1.6637041568756104 3.284148693084717 4.947853088378906
Loss :  1.696637749671936 3.1022093296051025 4.798847198486328
Loss :  1.6957018375396729 3.247654676437378 4.943356513977051
Loss :  1.6712843179702759 3.0137240886688232 4.685008525848389
Loss :  1.7089204788208008 3.542344093322754 5.251264572143555
Loss :  1.6543691158294678 3.099165201187134 4.753534317016602
Loss :  1.7041184902191162 2.9986963272094727 4.702815055847168
Loss :  1.6558150053024292 2.988420248031616 4.644235134124756
Loss :  1.7329453229904175 3.545552968978882 5.27849817276001
Loss :  1.6811069250106812 3.4097700119018555 5.090877056121826
Loss :  1.6647650003433228 3.313918113708496 4.978682994842529
Loss :  1.6745747327804565 3.533067226409912 5.207642078399658
Loss :  1.7054533958435059 3.399193286895752 5.104646682739258
Loss :  1.6989611387252808 3.3192641735076904 5.018225193023682
Loss :  1.6803028583526611 3.2516024112701416 4.931905269622803
Loss :  1.648113489151001 3.2152669429779053 4.863380432128906
Loss :  1.6707762479782104 3.4665756225585938 5.137351989746094
Loss :  1.6677271127700806 3.348088502883911 5.015815734863281
  batch 40 loss: 1.6677271127700806, 3.348088502883911, 5.015815734863281
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6690973043441772 3.332442283630371 5.001539707183838
Loss :  1.6554042100906372 3.255955457687378 4.911359786987305
Loss :  1.6758379936218262 3.25022554397583 4.926063537597656
Loss :  1.6676093339920044 3.3320071697235107 4.999616622924805
Loss :  1.6727312803268433 2.999190092086792 4.671921253204346
Loss :  1.6759339570999146 3.5467278957366943 5.222661972045898
Loss :  1.656730055809021 3.059938907623291 4.716669082641602
Loss :  1.6636245250701904 3.4311165809631348 5.094740867614746
Loss :  1.6495386362075806 2.7842838764190674 4.4338226318359375
Loss :  1.6870753765106201 3.3134915828704834 5.0005669593811035
Loss :  1.6606088876724243 3.010694980621338 4.671303749084473
Loss :  1.6789684295654297 2.9633049964904785 4.642273426055908
Loss :  1.693366527557373 2.965967893600464 4.659334182739258
Loss :  1.673430323600769 3.2461154460906982 4.919545650482178
Loss :  1.682432770729065 3.0188100337982178 4.701242923736572
Loss :  1.6476351022720337 3.394106149673462 5.041741371154785
Loss :  1.7005583047866821 2.9885075092315674 4.689065933227539
Loss :  1.6889064311981201 3.1071321964263916 4.796038627624512
Loss :  1.7103267908096313 3.3878185749053955 5.098145484924316
Loss :  1.6852775812149048 3.0681967735290527 4.753474235534668
  batch 60 loss: 1.6852775812149048, 3.0681967735290527, 4.753474235534668
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6827003955841064 3.1481635570526123 4.830863952636719
Loss :  1.6788992881774902 2.7434229850769043 4.4223222732543945
Loss :  1.6875369548797607 2.762943744659424 4.4504804611206055
Loss :  1.6774548292160034 2.8178598880767822 4.495314598083496
Loss :  1.670966386795044 2.2592947483062744 3.9302611351013184
Loss :  2.804776668548584 4.522040843963623 7.326817512512207
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  2.7802979946136475 4.3943305015563965 7.174628257751465
Loss :  2.809671401977539 4.333888530731201 7.14355993270874
Loss :  2.3715665340423584 4.2069478034973145 6.578514099121094
Total LOSS train 4.767325900151179 valid 7.0558799505233765
CE LOSS train 1.6762644731081449 valid 0.5928916335105896
Contrastive LOSS train 3.0910614270430345 valid 1.0517369508743286
EPOCH 64:
Loss :  1.6617729663848877 2.8177592754364014 4.479532241821289
Loss :  1.6794215440750122 2.648094654083252 4.327516078948975
Loss :  1.6606160402297974 2.958045244216919 4.618661403656006
Loss :  1.6627589464187622 3.05668044090271 4.719439506530762
Loss :  1.692227840423584 2.899601936340332 4.591829776763916
Loss :  1.669937014579773 2.8984994888305664 4.568436622619629
Loss :  1.6768745183944702 2.7972681522369385 4.474142551422119
Loss :  1.6662808656692505 3.159329652786255 4.825610637664795
Loss :  1.6671860218048096 3.0947463512420654 4.761932373046875
Loss :  1.62787926197052 3.200110673904419 4.8279900550842285
Loss :  1.6803085803985596 3.0768866539001465 4.757195472717285
Loss :  1.7334890365600586 3.1067068576812744 4.840195655822754
Loss :  1.6822869777679443 2.979689836502075 4.6619768142700195
Loss :  1.6778590679168701 2.739946126937866 4.417805194854736
Loss :  1.655583143234253 2.897883653640747 4.553466796875
Loss :  1.6650278568267822 2.936627149581909 4.601655006408691
Loss :  1.670432686805725 2.7720770835876465 4.442509651184082
Loss :  1.6714059114456177 2.643641471862793 4.315047264099121
Loss :  1.6715087890625 2.6901473999023438 4.361656188964844
Loss :  1.6397860050201416 3.065406560897827 4.705192565917969
  batch 20 loss: 1.6397860050201416, 3.065406560897827, 4.705192565917969
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6750521659851074 2.8051915168762207 4.480243682861328
Loss :  1.6832835674285889 2.9506566524505615 4.63394021987915
Loss :  1.6626676321029663 2.8104329109191895 4.473100662231445
Loss :  1.694161295890808 3.1208269596099854 4.814988136291504
Loss :  1.6933989524841309 3.059831380844116 4.753230094909668
Loss :  1.6704423427581787 3.3595099449157715 5.029952049255371
Loss :  1.7071810960769653 2.8210625648498535 4.528243541717529
Loss :  1.6512784957885742 2.8532955646514893 4.504573822021484
Loss :  1.7021113634109497 2.977374792098999 4.679486274719238
Loss :  1.6551098823547363 3.036900043487549 4.692009925842285
Loss :  1.7316583395004272 3.2262144088745117 4.9578728675842285
Loss :  1.677882432937622 3.2660107612609863 4.9438934326171875
Loss :  1.6632813215255737 3.06258487701416 4.725866317749023
Loss :  1.6727030277252197 2.934110403060913 4.606813430786133
Loss :  1.7046583890914917 3.2057433128356934 4.910401821136475
Loss :  1.6974973678588867 3.291029453277588 4.988526821136475
Loss :  1.6780511140823364 3.4842584133148193 5.162309646606445
Loss :  1.647300124168396 3.061434268951416 4.708734512329102
Loss :  1.6709123849868774 3.0208582878112793 4.691770553588867
Loss :  1.6654478311538696 2.6551272869110107 4.32057523727417
  batch 40 loss: 1.6654478311538696, 2.6551272869110107, 4.32057523727417
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6688852310180664 3.113024950027466 4.781909942626953
Loss :  1.6573504209518433 2.7824738025665283 4.439824104309082
Loss :  1.6757444143295288 3.1261980533599854 4.801942348480225
Loss :  1.6703368425369263 3.077974796295166 4.748311519622803
Loss :  1.6729719638824463 3.0826659202575684 4.755638122558594
Loss :  1.674385905265808 3.6587398052215576 5.333125591278076
Loss :  1.65546452999115 3.4057624340057373 5.061226844787598
Loss :  1.6651753187179565 3.6803929805755615 5.3455681800842285
Loss :  1.6463149785995483 3.491511106491089 5.137825965881348
Loss :  1.6883548498153687 3.682849884033203 5.371204853057861
Loss :  1.6591368913650513 3.1579906940460205 4.817127704620361
Loss :  1.6771245002746582 3.4545035362243652 5.131628036499023
Loss :  1.6911354064941406 2.6437199115753174 4.334855079650879
Loss :  1.6742794513702393 2.849550485610962 4.523829936981201
Loss :  1.6808043718338013 2.7797200679779053 4.460524559020996
Loss :  1.6461809873580933 3.480616569519043 5.126797676086426
Loss :  1.6990056037902832 3.2951700687408447 4.994175910949707
Loss :  1.6875170469284058 3.1991140842437744 4.886631011962891
Loss :  1.707384705543518 3.577410936355591 5.284795761108398
Loss :  1.683109164237976 2.6425998210906982 4.325708866119385
  batch 60 loss: 1.683109164237976, 2.6425998210906982, 4.325708866119385
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6804229021072388 3.2395668029785156 4.919989585876465
Loss :  1.6778589487075806 3.3541178703308105 5.031976699829102
Loss :  1.6862670183181763 2.8189711570739746 4.505238056182861
Loss :  1.6754730939865112 2.794095039367676 4.469568252563477
Loss :  1.6694259643554688 2.585070848464966 4.2544965744018555
Loss :  2.943918228149414 4.507556438446045 7.451474666595459
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  2.9048309326171875 4.558152198791504 7.462983131408691
Loss :  2.951105833053589 4.25820255279541 7.209308624267578
Loss :  2.476891279220581 4.271117210388184 6.748008728027344
Total LOSS train 4.727726862980769 valid 7.217943787574768
CE LOSS train 1.6751820417550893 valid 0.6192228198051453
Contrastive LOSS train 3.0525448322296143 valid 1.067779302597046
EPOCH 65:
Loss :  1.6609023809432983 2.8262133598327637 4.487115859985352
Loss :  1.677313208580017 3.15838623046875 4.835699558258057
Loss :  1.6595097780227661 2.6916635036468506 4.351173400878906
Loss :  1.6633987426757812 3.031303882598877 4.694702625274658
Loss :  1.6895695924758911 3.1064453125 4.796014785766602
Loss :  1.669075846672058 2.9596524238586426 4.62872838973999
Loss :  1.6744316816329956 3.102949619293213 4.777381420135498
Loss :  1.6645768880844116 2.4673707485198975 4.1319475173950195
Loss :  1.666139841079712 3.0616400241851807 4.727779865264893
Loss :  1.6255789995193481 2.718963861465454 4.344542980194092
Loss :  1.6782386302947998 2.8959083557128906 4.5741472244262695
Loss :  1.7336442470550537 3.2013890743255615 4.935033321380615
Loss :  1.6813337802886963 3.054088830947876 4.735422611236572
Loss :  1.6766266822814941 2.9135711193084717 4.590197563171387
Loss :  1.6555594205856323 2.8788769245147705 4.534436225891113
Loss :  1.6642179489135742 2.7884368896484375 4.452654838562012
Loss :  1.6711119413375854 2.9440886974334717 4.615200519561768
Loss :  1.6698023080825806 3.2585206031799316 4.928322792053223
Loss :  1.6736688613891602 3.060840606689453 4.734509468078613
Loss :  1.6404215097427368 3.1450583934783936 4.78548002243042
  batch 20 loss: 1.6404215097427368, 3.1450583934783936, 4.78548002243042
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.674742341041565 3.0903358459472656 4.765078067779541
Loss :  1.684695839881897 3.4943716526031494 5.179067611694336
Loss :  1.6614497900009155 2.904989004135132 4.566438674926758
Loss :  1.695023775100708 3.0317471027374268 4.726770877838135
Loss :  1.695784091949463 3.2721452713012695 4.967929363250732
Loss :  1.6683143377304077 3.009955644607544 4.678269863128662
Loss :  1.7066437005996704 3.0785796642303467 4.785223484039307
Loss :  1.6516488790512085 3.1180260181427 4.769674777984619
Loss :  1.7018228769302368 2.8627867698669434 4.564609527587891
Loss :  1.6575502157211304 3.32627272605896 4.983822822570801
Loss :  1.7344014644622803 3.434979200363159 5.1693806648254395
Loss :  1.678967833518982 2.94606876373291 4.625036716461182
Loss :  1.6637881994247437 2.6080682277679443 4.271856307983398
Loss :  1.6747769117355347 2.717491865158081 4.392268657684326
Loss :  1.7085297107696533 2.9234964847564697 4.632026195526123
Loss :  1.6982505321502686 2.7494938373565674 4.447744369506836
Loss :  1.6797188520431519 2.752678155899048 4.43239688873291
Loss :  1.6459413766860962 3.068701982498169 4.714643478393555
Loss :  1.6729931831359863 2.887129545211792 4.560122489929199
Loss :  1.666121244430542 3.18174147605896 4.847862720489502
  batch 40 loss: 1.666121244430542, 3.18174147605896, 4.847862720489502
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6705498695373535 2.9229273796081543 4.593477249145508
Loss :  1.6602349281311035 2.8195109367370605 4.479745864868164
Loss :  1.6786589622497559 2.8328351974487305 4.511494159698486
Loss :  1.6739757061004639 2.964402914047241 4.638378620147705
Loss :  1.6740628480911255 2.597618818283081 4.271681785583496
Loss :  1.6740226745605469 3.0882344245910645 4.762257099151611
Loss :  1.6556636095046997 2.7597129344940186 4.415376663208008
Loss :  1.665915608406067 2.887162923812866 4.553078651428223
Loss :  1.6447422504425049 2.816502809524536 4.461245059967041
Loss :  1.6910297870635986 2.8348679542541504 4.525897979736328
Loss :  1.6621848344802856 2.537200927734375 4.199385643005371
Loss :  1.6786640882492065 2.6191580295562744 4.297821998596191
Loss :  1.690782904624939 2.6951358318328857 4.385918617248535
Loss :  1.6760632991790771 3.3255999088287354 5.0016632080078125
Loss :  1.6832294464111328 3.1302490234375 4.813478469848633
Loss :  1.6468030214309692 3.2680823802948 4.914885520935059
Loss :  1.6992040872573853 3.029557466506958 4.728761672973633
Loss :  1.6888076066970825 3.034748077392578 4.723555564880371
Loss :  1.7089685201644897 3.1591861248016357 4.868154525756836
Loss :  1.6832919120788574 2.7233355045318604 4.406627655029297
  batch 60 loss: 1.6832919120788574, 2.7233355045318604, 4.406627655029297
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6800233125686646 3.4306912422180176 5.110714435577393
Loss :  1.6768150329589844 3.202890634536743 4.879705429077148
Loss :  1.6843527555465698 3.105912446975708 4.790265083312988
Loss :  1.6747157573699951 3.051036834716797 4.725752830505371
Loss :  1.667358160018921 2.8804643154144287 4.54782247543335
Loss :  2.8580307960510254 4.529623985290527 7.387654781341553
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  2.833785057067871 4.370221138000488 7.204006195068359
Loss :  2.8631083965301514 4.281692028045654 7.144800186157227
Loss :  2.40908145904541 4.3334221839904785 6.742503643035889
Total LOSS train 4.651505550971398 valid 7.119741201400757
CE LOSS train 1.6754832066022434 valid 0.6022703647613525
Contrastive LOSS train 2.976022349871122 valid 1.0833555459976196
EPOCH 66:
Loss :  1.6588186025619507 3.0468993186950684 4.705718040466309
Loss :  1.6751759052276611 3.392162561416626 5.067338466644287
Loss :  1.655526041984558 3.1497035026550293 4.805229663848877
Loss :  1.6587599515914917 3.2160439491271973 4.8748040199279785
Loss :  1.6889524459838867 2.8360471725463867 4.524999618530273
Loss :  1.6652590036392212 3.143704891204834 4.808963775634766
Loss :  1.674041748046875 3.4453125 5.119354248046875
Loss :  1.6628884077072144 3.1133227348327637 4.776211261749268
Loss :  1.663831353187561 2.9665515422821045 4.630383014678955
Loss :  1.6248112916946411 2.9815049171447754 4.606316089630127
Loss :  1.6760315895080566 2.9871230125427246 4.663154602050781
Loss :  1.7296664714813232 3.1755669116973877 4.905233383178711
Loss :  1.6785434484481812 3.035844087600708 4.7143874168396
Loss :  1.6762263774871826 3.0586063861846924 4.734832763671875
Loss :  1.6520107984542847 2.8854880332946777 4.537498950958252
Loss :  1.6638259887695312 2.915663957595825 4.579489707946777
Loss :  1.66820228099823 3.0563578605651855 4.724560260772705
Loss :  1.6697466373443604 2.9659183025360107 4.635664939880371
Loss :  1.6686488389968872 2.8030922412872314 4.471741199493408
Loss :  1.6381419897079468 2.865617513656616 4.503759384155273
  batch 20 loss: 1.6381419897079468, 2.865617513656616, 4.503759384155273
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6740567684173584 3.450340509414673 5.124397277832031
Loss :  1.6815294027328491 3.302720546722412 4.984250068664551
Loss :  1.6602567434310913 3.0410702228546143 4.701326847076416
Loss :  1.6905426979064941 3.1062562465667725 4.7967987060546875
Loss :  1.6911448240280151 2.8308115005493164 4.521956443786621
Loss :  1.6689666509628296 2.8328335285186768 4.501800060272217
Loss :  1.7066787481307983 2.749337673187256 4.456016540527344
Loss :  1.6509026288986206 2.669546604156494 4.320449352264404
Loss :  1.7026524543762207 2.521125316619873 4.223777770996094
Loss :  1.6530927419662476 2.7570276260375977 4.410120487213135
Loss :  1.7309634685516357 2.949073314666748 4.680036544799805
Loss :  1.6779248714447021 2.877145290374756 4.555069923400879
Loss :  1.6623543500900269 2.6133534908294678 4.275707721710205
Loss :  1.6735824346542358 2.7998592853546143 4.4734416007995605
Loss :  1.7045071125030518 3.1675572395324707 4.872064590454102
Loss :  1.697663426399231 2.6193339824676514 4.316997528076172
Loss :  1.6782662868499756 2.716688632965088 4.394954681396484
Loss :  1.6445484161376953 2.61997127532959 4.264519691467285
Loss :  1.6703413724899292 2.679842948913574 4.350184440612793
Loss :  1.666189432144165 2.475111961364746 4.141301155090332
  batch 40 loss: 1.666189432144165, 2.475111961364746, 4.141301155090332
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.668678641319275 3.016475200653076 4.685153961181641
Loss :  1.65756356716156 2.63433837890625 4.2919020652771
Loss :  1.6779369115829468 2.6786701679229736 4.356606960296631
Loss :  1.6711913347244263 2.621865749359131 4.293056964874268
Loss :  1.6741535663604736 2.506035566329956 4.18018913269043
Loss :  1.6741511821746826 2.71993088722229 4.394082069396973
Loss :  1.6553795337677002 2.532092809677124 4.187472343444824
Loss :  1.6665561199188232 2.64878511428833 4.315340995788574
Loss :  1.644256353378296 2.8513247966766357 4.495581150054932
Loss :  1.6922622919082642 2.6907131671905518 4.3829755783081055
Loss :  1.664605975151062 2.7757339477539062 4.440340042114258
Loss :  1.679919719696045 2.5173423290252686 4.197261810302734
Loss :  1.690811038017273 2.676476240158081 4.3672871589660645
Loss :  1.6796064376831055 2.814882755279541 4.4944891929626465
Loss :  1.686449646949768 2.886826992034912 4.573276519775391
Loss :  1.6481248140335083 2.912764072418213 4.560888767242432
Loss :  1.7004892826080322 2.981612205505371 4.682101249694824
Loss :  1.6908373832702637 3.1618311405181885 4.852668762207031
Loss :  1.7100571393966675 3.4192278385162354 5.129284858703613
Loss :  1.6865184307098389 2.958488941192627 4.645007133483887
  batch 60 loss: 1.6865184307098389, 2.958488941192627, 4.645007133483887
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.679672122001648 3.015611410140991 4.69528341293335
Loss :  1.6806302070617676 3.085280418395996 4.765910625457764
Loss :  1.6852613687515259 2.8874051570892334 4.572666645050049
Loss :  1.6753147840499878 3.285243511199951 4.9605584144592285
Loss :  1.6708080768585205 2.829155445098877 4.499963760375977
Loss :  2.719167947769165 4.468875408172607 7.188043594360352
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  2.6951327323913574 4.475214004516602 7.170346736907959
Loss :  2.721109628677368 4.280684471130371 7.00179386138916
Loss :  2.318570852279663 4.430260181427002 6.748830795288086
Total LOSS train 4.581140951009897 valid 7.027253746986389
CE LOSS train 1.6745616912841796 valid 0.5796427130699158
Contrastive LOSS train 2.9065792743976298 valid 1.1075650453567505
EPOCH 67:
Loss :  1.6638834476470947 2.8057312965393066 4.4696149826049805
Loss :  1.6739038228988647 3.2724809646606445 4.946384906768799
Loss :  1.6612087488174438 3.1550469398498535 4.816255569458008
Loss :  1.6676843166351318 3.2613308429718018 4.929015159606934
Loss :  1.6914690732955933 3.2779362201690674 4.969405174255371
Loss :  1.6711971759796143 3.077930450439453 4.749127388000488
Loss :  1.6743308305740356 2.9184043407440186 4.592735290527344
Loss :  1.6640828847885132 2.7989654541015625 4.463048458099365
Loss :  1.6675617694854736 3.0134899616241455 4.681051731109619
Loss :  1.6254361867904663 2.7416763305664062 4.367112636566162
Loss :  1.6782883405685425 2.8342437744140625 4.5125322341918945
Loss :  1.7344367504119873 2.9635202884674072 4.6979570388793945
Loss :  1.680684208869934 2.954449415206909 4.635133743286133
Loss :  1.6758743524551392 2.9497804641723633 4.625654697418213
Loss :  1.6579296588897705 2.4078867435455322 4.065816402435303
Loss :  1.6652473211288452 2.938227891921997 4.603475093841553
Loss :  1.6693083047866821 2.9364354610443115 4.605743885040283
Loss :  1.6715703010559082 2.6851706504821777 4.356740951538086
Loss :  1.6740463972091675 2.7277157306671143 4.401762008666992
Loss :  1.6399275064468384 2.712961435317993 4.352889060974121
  batch 20 loss: 1.6399275064468384, 2.712961435317993, 4.352889060974121
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6762139797210693 2.906306505203247 4.582520484924316
Loss :  1.6839089393615723 2.7523727416992188 4.436281681060791
Loss :  1.6636391878128052 2.7792398929595947 4.4428791999816895
Loss :  1.6950644254684448 3.1369540691375732 4.8320183753967285
Loss :  1.6960227489471436 3.244081974029541 4.9401044845581055
Loss :  1.6706945896148682 3.065932273864746 4.736626625061035
Loss :  1.707172155380249 3.0748586654663086 4.782031059265137
Loss :  1.6509160995483398 3.178922414779663 4.829838752746582
Loss :  1.700589656829834 2.6317074298858643 4.332297325134277
Loss :  1.6562515497207642 2.985948324203491 4.642199993133545
Loss :  1.7333040237426758 3.246488094329834 4.97979211807251
Loss :  1.6784650087356567 3.128349542617798 4.806814670562744
Loss :  1.663883090019226 3.359572172164917 5.0234551429748535
Loss :  1.6737035512924194 3.0663297176361084 4.740033149719238
Loss :  1.7061294317245483 3.3420965671539307 5.0482258796691895
Loss :  1.6982097625732422 3.306849718093872 5.005059242248535
Loss :  1.6790179014205933 3.3836138248443604 5.062631607055664
Loss :  1.6467477083206177 3.4320950508117676 5.078842639923096
Loss :  1.6715153455734253 3.3579487800598145 5.029464244842529
Loss :  1.66616952419281 3.3645644187927246 5.030734062194824
  batch 40 loss: 1.66616952419281, 3.3645644187927246, 5.030734062194824
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6691056489944458 3.6637649536132812 5.3328704833984375
Loss :  1.6587891578674316 2.910060405731201 4.568849563598633
Loss :  1.6769763231277466 3.611285448074341 5.288261890411377
Loss :  1.6712082624435425 3.8032147884368896 5.474422931671143
Loss :  1.6745870113372803 3.406460762023926 5.081048011779785
Loss :  1.67457914352417 3.655924081802368 5.330503463745117
Loss :  1.6552784442901611 3.945939064025879 5.601217269897461
Loss :  1.6658223867416382 3.7402913570404053 5.406113624572754
Loss :  1.6468318700790405 3.023070812225342 4.669902801513672
Loss :  1.6895791292190552 3.0853419303894043 4.77492094039917
Loss :  1.660090684890747 3.1988472938537598 4.858938217163086
Loss :  1.6779654026031494 2.7552993297576904 4.43326473236084
Loss :  1.6911076307296753 2.780451774597168 4.471559524536133
Loss :  1.6755465269088745 2.857874631881714 4.533421039581299
Loss :  1.6824100017547607 2.8425066471099854 4.524916648864746
Loss :  1.6466915607452393 2.750889301300049 4.397581100463867
Loss :  1.6996288299560547 2.889335870742798 4.588964462280273
Loss :  1.6880583763122559 2.895707607269287 4.583765983581543
Loss :  1.7089638710021973 3.2535789012908936 4.962542533874512
Loss :  1.68474543094635 2.9728684425354004 4.657613754272461
  batch 60 loss: 1.68474543094635, 2.9728684425354004, 4.657613754272461
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.680639624595642 3.0115952491760254 4.692234992980957
Loss :  1.6789922714233398 3.1369850635528564 4.815977096557617
Loss :  1.6856603622436523 2.8532609939575195 4.538921356201172
Loss :  1.6763681173324585 2.8848161697387695 4.561184406280518
Loss :  1.6706675291061401 2.5879628658294678 4.258630275726318
Loss :  2.6701836585998535 4.485395431518555 7.155579090118408
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  2.645918846130371 4.514127731323242 7.160046577453613
Loss :  2.6794703006744385 4.239234447479248 6.918704986572266
Loss :  2.2667317390441895 4.472485542297363 6.739217281341553
Total LOSS train 4.747860527038574 valid 6.99338698387146
CE LOSS train 1.6756305181063138 valid 0.5666829347610474
Contrastive LOSS train 3.0722300089322605 valid 1.1181213855743408
EPOCH 68:
Loss :  1.6628929376602173 2.605482816696167 4.268375873565674
Loss :  1.675514817237854 2.9138708114624023 4.589385509490967
Loss :  1.6595547199249268 2.765641927719116 4.425196647644043
Loss :  1.6641546487808228 2.8260838985443115 4.490238666534424
Loss :  1.6908098459243774 2.8124756813049316 4.5032854080200195
Loss :  1.6686232089996338 2.744100332260132 4.412723541259766
Loss :  1.67508065700531 2.6734840869903564 4.348564624786377
Loss :  1.6636604070663452 2.607903480529785 4.27156400680542
Loss :  1.667227864265442 2.484941244125366 4.152169227600098
Loss :  1.627108097076416 2.660823106765747 4.287931442260742
Loss :  1.677087426185608 2.724545955657959 4.401633262634277
Loss :  1.7327868938446045 3.1477408409118652 4.880527496337891
Loss :  1.6807721853256226 3.136975049972534 4.817747116088867
Loss :  1.675955891609192 2.9595320224761963 4.635488033294678
Loss :  1.658309817314148 2.8372507095336914 4.495560646057129
Loss :  1.6649893522262573 2.957740068435669 4.622729301452637
Loss :  1.6694982051849365 2.968858242034912 4.6383562088012695
Loss :  1.6705893278121948 2.8695497512817383 4.540139198303223
Loss :  1.6737653017044067 3.0107150077819824 4.6844801902771
Loss :  1.6403708457946777 2.925905704498291 4.566276550292969
  batch 20 loss: 1.6403708457946777, 2.925905704498291, 4.566276550292969
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6762696504592896 2.8160924911499023 4.492362022399902
Loss :  1.684658169746399 2.9664130210876465 4.651071071624756
Loss :  1.6632499694824219 2.78151273727417 4.444762706756592
Loss :  1.6954535245895386 3.2516603469848633 4.947113990783691
Loss :  1.6968492269515991 3.3941569328308105 5.091006278991699
Loss :  1.6699515581130981 3.2329933643341064 4.902945041656494
Loss :  1.7070199251174927 3.3330681324005127 5.040088176727295
Loss :  1.6517800092697144 3.2327980995178223 4.884578227996826
Loss :  1.7006441354751587 3.453784704208374 5.154428958892822
Loss :  1.6568697690963745 3.109524965286255 4.76639461517334
Loss :  1.7346731424331665 3.4433462619781494 5.1780195236206055
Loss :  1.679040551185608 3.330134153366089 5.009174823760986
Loss :  1.6640245914459229 2.8896901607513428 4.553714752197266
Loss :  1.6744083166122437 3.0488126277923584 4.7232208251953125
Loss :  1.7074640989303589 3.2415575981140137 4.949021816253662
Loss :  1.6972345113754272 2.9489071369171143 4.646141529083252
Loss :  1.679404616355896 2.809157371520996 4.488562107086182
Loss :  1.6417796611785889 2.70536470413208 4.34714412689209
Loss :  1.672863245010376 2.94027042388916 4.613133430480957
Loss :  1.6665472984313965 2.650557279586792 4.317104339599609
  batch 40 loss: 1.6665472984313965, 2.650557279586792, 4.317104339599609
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6682764291763306 2.701671838760376 4.369948387145996
Loss :  1.6580790281295776 2.3990046977996826 4.057083606719971
Loss :  1.6786761283874512 2.8091137409210205 4.487790107727051
Loss :  1.672594428062439 2.745840549468994 4.418435096740723
Loss :  1.6741639375686646 2.321174383163452 3.9953384399414062
Loss :  1.6720962524414062 3.230297327041626 4.902393341064453
Loss :  1.6548514366149902 2.5213544368743896 4.176205635070801
Loss :  1.666954517364502 2.9072608947753906 4.574215412139893
Loss :  1.6429693698883057 3.0051162242889404 4.648085594177246
Loss :  1.6935144662857056 3.055394411087036 4.748908996582031
Loss :  1.6652312278747559 3.2077438831329346 4.8729753494262695
Loss :  1.6804403066635132 2.916499614715576 4.596940040588379
Loss :  1.6906875371932983 2.9023101329803467 4.5929975509643555
Loss :  1.679563045501709 3.0684242248535156 4.747987270355225
Loss :  1.6861485242843628 2.905407428741455 4.591556072235107
Loss :  1.6486326456069946 2.734920024871826 4.383552551269531
Loss :  1.6995271444320679 2.872692823410034 4.5722198486328125
Loss :  1.6899080276489258 2.783398389816284 4.473306655883789
Loss :  1.709492564201355 3.176772356033325 4.886264801025391
Loss :  1.685386061668396 2.6051008701324463 4.290486812591553
  batch 60 loss: 1.685386061668396, 2.6051008701324463, 4.290486812591553
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6793599128723145 2.961207151412964 4.640566825866699
Loss :  1.6793686151504517 3.569702386856079 5.24907112121582
Loss :  1.6852272748947144 3.3436758518218994 5.028903007507324
Loss :  1.6744439601898193 3.254239797592163 4.928683757781982
Loss :  1.6695886850357056 2.4665448665618896 4.136133670806885
Loss :  2.6174705028533936 4.493363857269287 7.110834121704102
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  2.5891997814178467 4.463301181793213 7.0525007247924805
Loss :  2.631584882736206 4.317664623260498 6.949249267578125
Loss :  2.237837076187134 4.308319091796875 6.54615592956543
Total LOSS train 4.609267865694486 valid 6.914685010910034
CE LOSS train 1.6757556915283203 valid 0.5594592690467834
Contrastive LOSS train 2.933512177834144 valid 1.0770797729492188
EPOCH 69:
Loss :  1.6630444526672363 3.2410552501678467 4.904099464416504
Loss :  1.6742721796035767 3.3570048809051514 5.031277179718018
Loss :  1.6595145463943481 3.4665729999542236 5.126087665557861
Loss :  1.6645060777664185 3.225954294204712 4.89046049118042
Loss :  1.6909916400909424 3.5853240489959717 5.276315689086914
Loss :  1.668609857559204 3.035501003265381 4.704111099243164
Loss :  1.6735275983810425 3.4010164737701416 5.0745439529418945
Loss :  1.6630853414535522 3.050067186355591 4.7131524085998535
Loss :  1.6655513048171997 2.817380666732788 4.482932090759277
Loss :  1.624552607536316 2.6124942302703857 4.237046718597412
Loss :  1.6772871017456055 3.0926153659820557 4.769902229309082
Loss :  1.7326421737670898 3.3399107456207275 5.072552680969238
Loss :  1.6792817115783691 3.5570764541625977 5.236358165740967
Loss :  1.6756397485733032 3.247222900390625 4.922862529754639
Loss :  1.6543487310409546 3.0197460651397705 4.6740946769714355
Loss :  1.6634804010391235 2.950026273727417 4.61350679397583
Loss :  1.6686594486236572 3.3160080909729004 4.984667778015137
Loss :  1.6700689792633057 3.1935360431671143 4.86360502243042
Loss :  1.67169189453125 2.9097182750701904 4.5814104080200195
Loss :  1.6377719640731812 3.0294501781463623 4.667222023010254
  batch 20 loss: 1.6377719640731812, 3.0294501781463623, 4.667222023010254
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6739648580551147 2.9859871864318848 4.659952163696289
Loss :  1.6829314231872559 2.8538622856140137 4.5367937088012695
Loss :  1.6618270874023438 2.689115285873413 4.350942611694336
Loss :  1.6933399438858032 2.6889090538024902 4.382248878479004
Loss :  1.692586064338684 2.957355260848999 4.649941444396973
Loss :  1.669010877609253 2.823612928390503 4.492623805999756
Loss :  1.706624150276184 2.8173258304595947 4.523950099945068
Loss :  1.6511932611465454 2.742227792739868 4.393421173095703
Loss :  1.7018481492996216 2.8295111656188965 4.5313591957092285
Loss :  1.654152274131775 3.012246608734131 4.666399002075195
Loss :  1.7314202785491943 3.0202722549438477 4.751692771911621
Loss :  1.6778790950775146 3.218899965286255 4.8967790603637695
Loss :  1.6633937358856201 2.9961788654327393 4.659572601318359
Loss :  1.6728824377059937 2.906890630722046 4.57977294921875
Loss :  1.7050141096115112 3.3369710445404053 5.041985034942627
Loss :  1.6977699995040894 3.443828821182251 5.141598701477051
Loss :  1.6778008937835693 3.272772789001465 4.950573921203613
Loss :  1.6462022066116333 3.5947444438934326 5.2409467697143555
Loss :  1.6701109409332275 3.6838629245758057 5.353973865509033
Loss :  1.6647812128067017 3.564774990081787 5.229556083679199
  batch 40 loss: 1.6647812128067017, 3.564774990081787, 5.229556083679199
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6674773693084717 3.380619764328003 5.048097133636475
Loss :  1.6566343307495117 3.28554630279541 4.942180633544922
Loss :  1.6748486757278442 3.2155494689941406 4.890398025512695
Loss :  1.6696945428848267 3.623227834701538 5.292922496795654
Loss :  1.6721196174621582 3.075026750564575 4.7471466064453125
Loss :  1.6710751056671143 3.1368658542633057 4.80794095993042
Loss :  1.6539499759674072 3.129347801208496 4.783297538757324
Loss :  1.663565754890442 3.1012699604034424 4.764835834503174
Loss :  1.6432385444641113 3.0582756996154785 4.70151424407959
Loss :  1.687654733657837 3.305047035217285 4.992701530456543
Loss :  1.6588636636734009 3.1786530017852783 4.837516784667969
Loss :  1.6762871742248535 3.3432512283325195 5.019538402557373
Loss :  1.6899914741516113 3.0135507583618164 4.703542232513428
Loss :  1.6725540161132812 2.859151601791382 4.531705856323242
Loss :  1.6819419860839844 3.0942890644073486 4.776230812072754
Loss :  1.645434856414795 2.914458751678467 4.559893608093262
Loss :  1.697513222694397 2.9099936485290527 4.60750675201416
Loss :  1.6867283582687378 3.1281590461730957 4.814887523651123
Loss :  1.7079906463623047 3.2911229133605957 4.9991135597229
Loss :  1.6835527420043945 2.9683616161346436 4.651914596557617
  batch 60 loss: 1.6835527420043945, 2.9683616161346436, 4.651914596557617
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6790412664413452 3.3599908351898193 5.039031982421875
Loss :  1.677819848060608 3.129338026046753 4.80715799331665
Loss :  1.6839933395385742 3.224510669708252 4.908504009246826
Loss :  1.6741982698440552 3.188539743423462 4.862738132476807
Loss :  1.6687179803848267 2.7636842727661133 4.43240213394165
Loss :  2.931840419769287 4.523125648498535 7.454966068267822
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  2.9055559635162354 4.4524455070495605 7.358001708984375
Loss :  2.9394850730895996 4.330121040344238 7.269606113433838
Loss :  2.4600703716278076 4.376238822937012 6.836309432983398
Total LOSS train 4.8058924968426044 valid 7.229720830917358
CE LOSS train 1.6741253577745878 valid 0.6150175929069519
Contrastive LOSS train 3.131767126230093 valid 1.094059705734253
EPOCH 70:
Loss :  1.6614229679107666 3.03688383102417 4.698307037353516
Loss :  1.6739509105682373 3.082735776901245 4.756686687469482
Loss :  1.657162070274353 2.414612054824829 4.071774005889893
Loss :  1.6621203422546387 2.744035243988037 4.406155586242676
Loss :  1.6894783973693848 2.523897886276245 4.213376045227051
Loss :  1.6668301820755005 2.554929494857788 4.221759796142578
Loss :  1.6739412546157837 2.503925085067749 4.177866458892822
Loss :  1.6624287366867065 2.320939302444458 3.983367919921875
Loss :  1.66571044921875 2.38034987449646 4.046060562133789
Loss :  1.6251654624938965 2.5511162281036377 4.176281929016113
Loss :  1.6758733987808228 2.6409077644348145 4.316781044006348
Loss :  1.7320138216018677 3.0019853115081787 4.733999252319336
Loss :  1.6790889501571655 3.1625702381134033 4.841659069061279
Loss :  1.6747794151306152 2.7850608825683594 4.459840297698975
Loss :  1.6557062864303589 3.130730628967285 4.786437034606934
Loss :  1.6631911993026733 2.6761655807495117 4.339356899261475
Loss :  1.6678705215454102 3.243344783782959 4.911215305328369
Loss :  1.669119954109192 3.102372884750366 4.771492958068848
Loss :  1.6709893941879272 3.1334176063537598 4.804407119750977
Loss :  1.6371675729751587 3.538886070251465 5.176053524017334
  batch 20 loss: 1.6371675729751587, 3.538886070251465, 5.176053524017334
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.673653244972229 3.701385498046875 5.3750386238098145
Loss :  1.6824325323104858 3.9035818576812744 5.586014270782471
Loss :  1.6617920398712158 3.3544955253601074 5.016287803649902
Loss :  1.692482352256775 3.423261880874634 5.115744113922119
Loss :  1.6934665441513062 3.3561224937438965 5.049589157104492
Loss :  1.6681873798370361 3.0735790729522705 4.741766452789307
Loss :  1.7054353952407837 3.195178985595703 4.900614261627197
Loss :  1.6490404605865479 2.871260166168213 4.52030086517334
Loss :  1.6990286111831665 3.1183066368103027 4.81733512878418
Loss :  1.6548075675964355 3.1380021572113037 4.79280948638916
Loss :  1.7325800657272339 3.3442187309265137 5.076798915863037
Loss :  1.6764153242111206 2.9105114936828613 4.5869269371032715
Loss :  1.6622854471206665 3.148381471633911 4.810667037963867
Loss :  1.6725339889526367 3.1667983531951904 4.839332580566406
Loss :  1.7058178186416626 3.2065269947052 4.912344932556152
Loss :  1.6959024667739868 3.1023831367492676 4.798285484313965
Loss :  1.677492618560791 3.174907922744751 4.852400779724121
Loss :  1.6410796642303467 3.1390726566314697 4.780152320861816
Loss :  1.6719216108322144 3.3681957721710205 5.040117263793945
Loss :  1.665209412574768 3.1184730529785156 4.783682346343994
  batch 40 loss: 1.665209412574768, 3.1184730529785156, 4.783682346343994
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.667093276977539 3.3649516105651855 5.032044887542725
Loss :  1.6575126647949219 3.037261486053467 4.694774150848389
Loss :  1.677110195159912 3.514943838119507 5.19205379486084
Loss :  1.673256278038025 3.4673893451690674 5.140645503997803
Loss :  1.672930359840393 3.495187282562256 5.168117523193359
Loss :  1.6695712804794312 3.9932146072387695 5.66278600692749
Loss :  1.653032898902893 3.488420009613037 5.141452789306641
Loss :  1.6655510663986206 3.5062546730041504 5.1718058586120605
Loss :  1.6410634517669678 3.2302279472351074 4.871291160583496
Loss :  1.6916522979736328 2.8822221755981445 4.573874473571777
Loss :  1.6633940935134888 3.2936208248138428 4.957015037536621
Loss :  1.678345799446106 2.8407015800476074 4.519047260284424
Loss :  1.6907461881637573 2.819784641265869 4.510530948638916
Loss :  1.6781588792800903 3.0914695262908936 4.769628524780273
Loss :  1.6850149631500244 3.10217022895813 4.787185192108154
Loss :  1.6486905813217163 2.7081174850463867 4.356808185577393
Loss :  1.6989880800247192 3.1007232666015625 4.799711227416992
Loss :  1.6896593570709229 3.177307605743408 4.86696720123291
Loss :  1.7084786891937256 3.364173650741577 5.072652339935303
Loss :  1.684740662574768 3.47141170501709 5.156152248382568
  batch 60 loss: 1.684740662574768, 3.47141170501709, 5.156152248382568
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6796070337295532 2.929056406021118 4.608663558959961
Loss :  1.6789699792861938 3.0590980052948 4.738068103790283
Loss :  1.6854630708694458 2.9762284755706787 4.661691665649414
Loss :  1.6722553968429565 3.4087934494018555 5.081048965454102
Loss :  1.667001724243164 2.395601272583008 4.062602996826172
Loss :  2.7599050998687744 4.503789901733398 7.263694763183594
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  2.725285530090332 4.457702159881592 7.182987689971924
Loss :  2.7503437995910645 4.378199577331543 7.128543376922607
Loss :  2.3425912857055664 4.34515905380249 6.687750339508057
Total LOSS train 4.767472413870005 valid 7.065744042396545
CE LOSS train 1.6742132938825167 valid 0.5856478214263916
Contrastive LOSS train 3.093259099813608 valid 1.0862897634506226
EPOCH 71:
Loss :  1.661284327507019 2.9811525344848633 4.642436981201172
Loss :  1.6731761693954468 3.658024787902832 5.331201076507568
Loss :  1.6575416326522827 2.8510794639587402 4.5086212158203125
Loss :  1.6626076698303223 3.5215678215026855 5.184175491333008
Loss :  1.6891307830810547 3.6770339012145996 5.366164684295654
Loss :  1.6683789491653442 3.0481956005096436 4.716574668884277
Loss :  1.6725202798843384 3.016024589538574 4.688544750213623
Loss :  1.6621917486190796 3.0275356769561768 4.689727306365967
Loss :  1.6648329496383667 2.4716227054595947 4.136455535888672
Loss :  1.622639775276184 2.6016714572906494 4.224311351776123
Loss :  1.6763211488723755 2.8706979751586914 4.547019004821777
Loss :  1.731438159942627 3.0519769191741943 4.783414840698242
Loss :  1.6775076389312744 2.8266351222991943 4.504142761230469
Loss :  1.6740014553070068 2.8659780025482178 4.539979457855225
Loss :  1.654955506324768 2.777458429336548 4.4324140548706055
Loss :  1.6617321968078613 2.7391555309295654 4.400887489318848
Loss :  1.6664243936538696 2.8946657180786133 4.561089992523193
Loss :  1.6697865724563599 2.38643217086792 4.05621862411499
Loss :  1.6695504188537598 2.440955638885498 4.110506057739258
Loss :  1.6373673677444458 2.7392985820770264 4.376666069030762
  batch 20 loss: 1.6373673677444458, 2.7392985820770264, 4.376666069030762
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6736047267913818 2.846688985824585 4.520293712615967
Loss :  1.6826575994491577 3.1034960746765137 4.786153793334961
Loss :  1.6613715887069702 2.6648762226104736 4.326247692108154
Loss :  1.6928261518478394 2.669416904449463 4.362243175506592
Loss :  1.6920721530914307 3.2705273628234863 4.962599754333496
Loss :  1.6675915718078613 2.8388779163360596 4.5064697265625
Loss :  1.706374168395996 3.227050304412842 4.933424472808838
Loss :  1.6507316827774048 2.872230052947998 4.522961616516113
Loss :  1.6997311115264893 3.070244550704956 4.769975662231445
Loss :  1.6524772644042969 3.153034210205078 4.805511474609375
Loss :  1.7301253080368042 2.954495906829834 4.684621334075928
Loss :  1.676985502243042 2.74979567527771 4.426781177520752
Loss :  1.662172555923462 3.0411770343780518 4.703349590301514
Loss :  1.67105233669281 3.098684072494507 4.769736289978027
Loss :  1.7035291194915771 3.028484582901001 4.732013702392578
Loss :  1.6955184936523438 3.471466064453125 5.166984558105469
Loss :  1.6771022081375122 2.859269857406616 4.536372184753418
Loss :  1.643339991569519 3.148087739944458 4.7914276123046875
Loss :  1.6688026189804077 3.1031394004821777 4.771942138671875
Loss :  1.6647579669952393 3.3473997116088867 5.012157440185547
  batch 40 loss: 1.6647579669952393, 3.3473997116088867, 5.012157440185547
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6664791107177734 3.2060775756835938 4.872556686401367
Loss :  1.654102087020874 3.3006906509399414 4.9547929763793945
Loss :  1.6746248006820679 3.371650218963623 5.0462751388549805
Loss :  1.6680890321731567 3.610391139984131 5.278480052947998
Loss :  1.6715495586395264 3.6740875244140625 5.345637321472168
Loss :  1.6710106134414673 3.364816427230835 5.035827159881592
Loss :  1.6541775465011597 3.6881332397460938 5.342310905456543
Loss :  1.662306308746338 3.6167068481445312 5.279013156890869
Loss :  1.6437121629714966 3.498035192489624 5.14174747467041
Loss :  1.6868922710418701 3.4351441860198975 5.122036457061768
Loss :  1.6589018106460571 3.1087915897369385 4.767693519592285
Loss :  1.6764886379241943 3.156022071838379 4.832510948181152
Loss :  1.6902769804000854 2.842644214630127 4.532921314239502
Loss :  1.6723904609680176 3.171478271484375 4.843868732452393
Loss :  1.682050108909607 3.343701124191284 5.025751113891602
Loss :  1.6464961767196655 3.1005454063415527 4.747041702270508
Loss :  1.6977952718734741 3.1314594745635986 4.829254627227783
Loss :  1.6870490312576294 3.275330066680908 4.962378978729248
Loss :  1.7087656259536743 3.563448190689087 5.272213935852051
Loss :  1.6842525005340576 3.3632726669311523 5.047525405883789
  batch 60 loss: 1.6842525005340576, 3.3632726669311523, 5.047525405883789
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.679157018661499 3.523679494857788 5.202836513519287
Loss :  1.6767067909240723 3.290801763534546 4.967508316040039
Loss :  1.6841983795166016 2.9335129261016846 4.617711067199707
Loss :  1.6721807718276978 3.2693917751312256 4.941572666168213
Loss :  1.6668320894241333 2.7546334266662598 4.4214653968811035
Loss :  2.8877761363983154 4.506319046020508 7.394095420837402
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  2.8544278144836426 4.4644646644592285 7.318892478942871
Loss :  2.888387680053711 4.264008045196533 7.152395725250244
Loss :  2.424889326095581 4.310736656188965 6.735626220703125
Total LOSS train 4.774196155254657 valid 7.150252461433411
CE LOSS train 1.6732722832606388 valid 0.6062223315238953
Contrastive LOSS train 3.100923857322106 valid 1.0776841640472412
EPOCH 72:
Loss :  1.6608326435089111 3.1520678997039795 4.812900543212891
Loss :  1.6739635467529297 3.2780086994171143 4.951972007751465
Loss :  1.6578614711761475 2.995232105255127 4.653093338012695
Loss :  1.6630696058273315 3.133765697479248 4.796835422515869
Loss :  1.6904535293579102 3.1292824745178223 4.819736003875732
Loss :  1.6691548824310303 3.1299850940704346 4.799139976501465
Loss :  1.673437476158142 2.7831084728240967 4.456545829772949
Loss :  1.6629514694213867 2.630497455596924 4.2934489250183105
Loss :  1.6660943031311035 2.63254976272583 4.298644065856934
Loss :  1.6235883235931396 2.4323019981384277 4.055890083312988
Loss :  1.67734956741333 2.3842127323150635 4.061562538146973
Loss :  1.7322062253952026 2.9332735538482666 4.66547966003418
Loss :  1.67919921875 2.873961925506592 4.553161144256592
Loss :  1.6734542846679688 2.8030707836151123 4.47652530670166
Loss :  1.6577880382537842 2.5463452339172363 4.204133033752441
Loss :  1.6632661819458008 2.6355645656585693 4.298830986022949
Loss :  1.6673744916915894 2.559695243835449 4.227069854736328
Loss :  1.671172022819519 2.9252519607543945 4.596424102783203
Loss :  1.6726086139678955 2.430365800857544 4.1029744148254395
Loss :  1.6385250091552734 2.6043972969055176 4.242922306060791
  batch 20 loss: 1.6385250091552734, 2.6043972969055176, 4.242922306060791
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6749730110168457 2.7551939487457275 4.430167198181152
Loss :  1.6844704151153564 2.573758602142334 4.2582292556762695
Loss :  1.663039207458496 2.3878912925720215 4.050930500030518
Loss :  1.695847511291504 2.444181203842163 4.140028953552246
Loss :  1.6960163116455078 3.0133090019226074 4.709325313568115
Loss :  1.6691522598266602 2.4098432064056396 4.078995704650879
Loss :  1.7066442966461182 2.6682209968566895 4.374865531921387
Loss :  1.6509268283843994 2.3396317958831787 3.990558624267578
Loss :  1.6969283819198608 2.7492289543151855 4.446157455444336
Loss :  1.6543984413146973 2.7013778686523438 4.355776309967041
Loss :  1.7313909530639648 3.030534267425537 4.761925220489502
Loss :  1.6779863834381104 3.256817102432251 4.934803485870361
Loss :  1.6633492708206177 3.0211689472198486 4.684518337249756
Loss :  1.6716002225875854 3.146759033203125 4.818359375
Loss :  1.705306887626648 3.5345091819763184 5.239816188812256
Loss :  1.6950366497039795 3.530601978302002 5.225638389587402
Loss :  1.677824854850769 3.4299981594085693 5.107822895050049
Loss :  1.6430169343948364 3.3762505054473877 5.019267559051514
Loss :  1.6697163581848145 3.3214218616485596 4.991138458251953
Loss :  1.6653684377670288 3.329155683517456 4.994524002075195
  batch 40 loss: 1.6653684377670288, 3.329155683517456, 4.994524002075195
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6665799617767334 3.5754828453063965 5.242062568664551
Loss :  1.6547549962997437 3.130340576171875 4.785095691680908
Loss :  1.6757876873016357 3.014845371246338 4.6906328201293945
Loss :  1.668207049369812 2.948665142059326 4.616872310638428
Loss :  1.6720961332321167 3.0623247623443604 4.7344207763671875
Loss :  1.6697893142700195 3.2826578617095947 4.952446937561035
Loss :  1.653283715248108 3.4217758178710938 5.075059413909912
Loss :  1.6622925996780396 3.2317328453063965 4.8940253257751465
Loss :  1.6442604064941406 2.9494214057922363 4.593681812286377
Loss :  1.6879568099975586 3.234846353530884 4.922802925109863
Loss :  1.660341739654541 2.9586637020111084 4.61900520324707
Loss :  1.678248643875122 3.0064985752105713 4.684747219085693
Loss :  1.6898936033248901 3.6020326614379883 5.291926383972168
Loss :  1.6730517148971558 3.404393434524536 5.077445030212402
Loss :  1.6814994812011719 3.407527446746826 5.089026927947998
Loss :  1.6463077068328857 3.254873752593994 4.901181221008301
Loss :  1.696785569190979 2.8714439868927 4.568229675292969
Loss :  1.6874502897262573 3.2075235843658447 4.8949737548828125
Loss :  1.7082163095474243 3.62143874168396 5.329655170440674
Loss :  1.683883786201477 2.7742702960968018 4.458154201507568
  batch 60 loss: 1.683883786201477, 2.7742702960968018, 4.458154201507568
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6791560649871826 2.8679420948028564 4.547098159790039
Loss :  1.676121711730957 2.957775831222534 4.63389778137207
Loss :  1.6839882135391235 2.561187267303467 4.245175361633301
Loss :  1.6729024648666382 2.9865574836730957 4.659460067749023
Loss :  1.6662079095840454 2.4908509254455566 4.1570587158203125
Loss :  2.640756845474243 4.55700159072876 7.197758674621582
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  2.6178719997406006 4.509253025054932 7.127124786376953
Loss :  2.645393133163452 4.309754848480225 6.955147743225098
Loss :  2.234590530395508 4.412102222442627 6.646692752838135
Total LOSS train 4.640681134737455 valid 6.981680989265442
CE LOSS train 1.673944744696984 valid 0.558647632598877
Contrastive LOSS train 2.9667363863724927 valid 1.1030255556106567
EPOCH 73:
Loss :  1.6603399515151978 2.877152681350708 4.537492752075195
Loss :  1.6744678020477295 3.2192089557647705 4.8936767578125
Loss :  1.6555215120315552 2.7796714305877686 4.435193061828613
Loss :  1.6586084365844727 2.639742612838745 4.298351287841797
Loss :  1.6894302368164062 2.5489001274108887 4.238330364227295
Loss :  1.6657884120941162 2.679264783859253 4.345053195953369
Loss :  1.6741783618927002 2.8298146724700928 4.503993034362793
Loss :  1.662692904472351 2.1779844760894775 3.840677261352539
Loss :  1.6640323400497437 2.2499887943267822 3.9140210151672363
Loss :  1.624029278755188 2.4100193977355957 4.034048557281494
Loss :  1.67613685131073 2.8315680027008057 4.507704734802246
Loss :  1.7289061546325684 3.3547418117523193 5.083647727966309
Loss :  1.6783314943313599 2.7944960594177246 4.472827434539795
Loss :  1.67429780960083 2.8320956230163574 4.5063934326171875
Loss :  1.654114842414856 3.0746450424194336 4.728759765625
Loss :  1.6619060039520264 2.9056742191314697 4.567580223083496
Loss :  1.666398048400879 3.0411226749420166 4.707520484924316
Loss :  1.6699151992797852 2.825298309326172 4.495213508605957
Loss :  1.6687430143356323 3.4661920070648193 5.134934902191162
Loss :  1.636430263519287 3.527052164077759 5.163482666015625
  batch 20 loss: 1.636430263519287, 3.527052164077759, 5.163482666015625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6731743812561035 3.4276580810546875 5.100832462310791
Loss :  1.6823838949203491 3.4341180324554443 5.116501808166504
Loss :  1.660154104232788 3.5516812801361084 5.2118353843688965
Loss :  1.6928181648254395 3.2265522480010986 4.919370651245117
Loss :  1.691630244255066 3.532395839691162 5.224026203155518
Loss :  1.66672682762146 3.1868672370910645 4.853593826293945
Loss :  1.7060495615005493 3.488304853439331 5.19435453414917
Loss :  1.649986743927002 3.092026710510254 4.742013454437256
Loss :  1.6987217664718628 3.265763759613037 4.9644856452941895
Loss :  1.6519652605056763 3.497878313064575 5.149843692779541
Loss :  1.730448842048645 3.2642972469329834 4.994746208190918
Loss :  1.6763099431991577 3.1498947143554688 4.826204776763916
Loss :  1.6618282794952393 3.13142728805542 4.793255805969238
Loss :  1.6697888374328613 3.2516424655914307 4.921431541442871
Loss :  1.7041685581207275 3.236935615539551 4.941103935241699
Loss :  1.6944361925125122 2.8645427227020264 4.558979034423828
Loss :  1.6764476299285889 3.067169666290283 4.743617057800293
Loss :  1.6425451040267944 2.723010301589966 4.365555286407471
Loss :  1.6706961393356323 2.8793065547943115 4.550002574920654
Loss :  1.6645334959030151 2.6410746574401855 4.30560827255249
  batch 40 loss: 1.6645334959030151, 2.6410746574401855, 4.30560827255249
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6666511297225952 3.123894453048706 4.790545463562012
Loss :  1.6549614667892456 2.9036474227905273 4.5586090087890625
Loss :  1.6747939586639404 2.6812584400177 4.356052398681641
Loss :  1.6699724197387695 2.650914192199707 4.320886611938477
Loss :  1.6722524166107178 2.5049855709075928 4.1772379875183105
Loss :  1.6701481342315674 2.840010643005371 4.510158538818359
Loss :  1.6545913219451904 2.673326015472412 4.327917098999023
Loss :  1.6639127731323242 2.5076303482055664 4.171543121337891
Loss :  1.6435350179672241 2.7472763061523438 4.390811443328857
Loss :  1.6887619495391846 2.784715175628662 4.473477363586426
Loss :  1.6603550910949707 2.9094269275665283 4.569782257080078
Loss :  1.6768161058425903 2.984834909439087 4.661651134490967
Loss :  1.6903496980667114 2.7462503910064697 4.436600208282471
Loss :  1.6739088296890259 2.656019687652588 4.329928398132324
Loss :  1.6828149557113647 2.5406363010406494 4.223451137542725
Loss :  1.6473294496536255 2.4083991050720215 4.055728435516357
Loss :  1.697346806526184 2.609032154083252 4.3063788414001465
Loss :  1.6875724792480469 2.562657594680786 4.250229835510254
Loss :  1.7071846723556519 2.8040237426757812 4.511208534240723
Loss :  1.6838841438293457 2.5518736839294434 4.235757827758789
  batch 60 loss: 1.6838841438293457, 2.5518736839294434, 4.235757827758789
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6788527965545654 2.862752676010132 4.541605472564697
Loss :  1.6783195734024048 3.222468137741089 4.900787830352783
Loss :  1.6848657131195068 3.3129334449768066 4.997798919677734
Loss :  1.6720632314682007 3.046114444732666 4.718177795410156
Loss :  1.666642665863037 2.6786677837371826 4.345310211181641
Loss :  2.944692373275757 4.5090250968933105 7.453717231750488
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  2.9145143032073975 4.442916393280029 7.357430458068848
Loss :  2.941188335418701 4.328908920288086 7.270097255706787
Loss :  2.472787380218506 4.3929009437561035 6.865688323974609
Total LOSS train 4.600736926152156 valid 7.236733317375183
CE LOSS train 1.6731841490818904 valid 0.6181968450546265
Contrastive LOSS train 2.9275527844062217 valid 1.0982252359390259
EPOCH 74:
Loss :  1.6608613729476929 3.163825035095215 4.824686527252197
Loss :  1.6739346981048584 3.241614818572998 4.915549278259277
Loss :  1.6581584215164185 3.01652193069458 4.674680233001709
Loss :  1.6630433797836304 3.028276205062866 4.691319465637207
Loss :  1.6885862350463867 3.3429110050201416 5.031497001647949
Loss :  1.6686214208602905 3.3039896488189697 4.972610950469971
Loss :  1.671244502067566 3.5474417209625244 5.218686103820801
Loss :  1.6616979837417603 3.5783255100250244 5.240023612976074
Loss :  1.6642729043960571 3.3550949096679688 5.019367694854736
Loss :  1.6196060180664062 3.153430223464966 4.773036003112793
Loss :  1.6753789186477661 3.6177258491516113 5.293104648590088
Loss :  1.7329657077789307 3.3750619888305664 5.108027458190918
Loss :  1.6769983768463135 3.4844374656677246 5.161436080932617
Loss :  1.6729539632797241 3.2952115535736084 4.968165397644043
Loss :  1.6546109914779663 3.117388963699341 4.771999835968018
Loss :  1.6592075824737549 3.217625379562378 4.876832962036133
Loss :  1.6657187938690186 3.1093904972076416 4.77510929107666
Loss :  1.6676784753799438 2.7882797718048096 4.455958366394043
Loss :  1.670396089553833 3.19417667388916 4.864572525024414
Loss :  1.6360716819763184 2.737816333770752 4.37388801574707
  batch 20 loss: 1.6360716819763184, 2.737816333770752, 4.37388801574707
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6729238033294678 2.772230386734009 4.445154190063477
Loss :  1.6833925247192383 3.137740135192871 4.821132659912109
Loss :  1.6601393222808838 2.6292564868927 4.289395809173584
Loss :  1.6941888332366943 2.994961738586426 4.689150810241699
Loss :  1.6940897703170776 3.3077893257141113 5.0018792152404785
Loss :  1.6666144132614136 2.952437400817871 4.619051933288574
Loss :  1.705190896987915 2.941669464111328 4.646860122680664
Loss :  1.6501927375793457 2.7754945755004883 4.425687313079834
Loss :  1.6974716186523438 2.74548602104187 4.442957878112793
Loss :  1.654859185218811 2.69574236869812 4.350601673126221
Loss :  1.7323287725448608 2.8008086681365967 4.533137321472168
Loss :  1.6770719289779663 2.622626543045044 4.299698352813721
Loss :  1.6623841524124146 2.4283483028411865 4.090732574462891
Loss :  1.6719831228256226 2.4713222980499268 4.14330530166626
Loss :  1.7063566446304321 2.8120696544647217 4.518426418304443
Loss :  1.6948360204696655 2.4861762523651123 4.181012153625488
Loss :  1.6778554916381836 2.5264317989349365 4.204287528991699
Loss :  1.6415135860443115 2.328352212905884 3.9698657989501953
Loss :  1.6717596054077148 2.34004545211792 4.011805057525635
Loss :  1.664719820022583 2.393495798110962 4.058215618133545
  batch 40 loss: 1.664719820022583, 2.393495798110962, 4.058215618133545
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6683059930801392 2.7836947441101074 4.452000617980957
Loss :  1.6578714847564697 2.546919584274292 4.204791069030762
Loss :  1.6772338151931763 3.103365898132324 4.780599594116211
Loss :  1.6726089715957642 3.003037452697754 4.6756463050842285
Loss :  1.6742078065872192 2.6427502632141113 4.316957950592041
Loss :  1.6711369752883911 2.9641611576080322 4.635298252105713
Loss :  1.6552281379699707 3.0242650508880615 4.679492950439453
Loss :  1.6661062240600586 2.753148078918457 4.419254302978516
Loss :  1.6419005393981934 2.796410322189331 4.438310623168945
Loss :  1.6908973455429077 2.6292169094085693 4.3201141357421875
Loss :  1.660943865776062 3.090230703353882 4.751174449920654
Loss :  1.6775972843170166 2.7562673091888428 4.433864593505859
Loss :  1.689954161643982 2.7627012729644775 4.45265531539917
Loss :  1.675707221031189 3.020306348800659 4.696013450622559
Loss :  1.6828018426895142 3.096715211868286 4.77951717376709
Loss :  1.6478416919708252 2.7096633911132812 4.357504844665527
Loss :  1.6971347332000732 3.2572038173675537 4.954338550567627
Loss :  1.6862386465072632 2.918887138366699 4.605125904083252
Loss :  1.7063566446304321 3.2823331356048584 4.98868989944458
Loss :  1.684126853942871 2.9293453693389893 4.613471984863281
  batch 60 loss: 1.684126853942871, 2.9293453693389893, 4.613471984863281
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6782710552215576 3.090968132019043 4.76923942565918
Loss :  1.6764154434204102 3.1017425060272217 4.778158187866211
Loss :  1.6836997270584106 3.108978271484375 4.792677879333496
Loss :  1.6706242561340332 3.3637571334838867 5.03438138961792
Loss :  1.6650869846343994 2.3813135623931885 4.046400547027588
Loss :  2.8760576248168945 4.469796180725098 7.345853805541992
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  2.836514472961426 4.46140718460083 7.297921657562256
Loss :  2.877265691757202 4.2772393226623535 7.154504776000977
Loss :  2.4119155406951904 4.315225124359131 6.727140426635742
Total LOSS train 4.6265936704782336 valid 7.131355166435242
CE LOSS train 1.6735411919080294 valid 0.6029788851737976
Contrastive LOSS train 2.953052509748019 valid 1.0788062810897827
EPOCH 75:
Loss :  1.6606793403625488 2.9647154808044434 4.625394821166992
Loss :  1.6726605892181396 3.5355618000030518 5.208222389221191
Loss :  1.6567150354385376 2.7800867557525635 4.436801910400391
Loss :  1.6617921590805054 3.0022833347320557 4.6640753746032715
Loss :  1.6882879734039307 3.253828763961792 4.942116737365723
Loss :  1.667501449584961 3.022461175918579 4.689962387084961
Loss :  1.671850323677063 2.821568727493286 4.493419170379639
Loss :  1.66171395778656 2.857903242111206 4.519617080688477
Loss :  1.6638537645339966 2.67771577835083 4.341569423675537
Loss :  1.6222864389419556 2.8113341331481934 4.433620452880859
Loss :  1.6760085821151733 3.1871285438537598 4.863137245178223
Loss :  1.7315746545791626 3.0543246269226074 4.7858991622924805
Loss :  1.6777855157852173 2.9278013706207275 4.605587005615234
Loss :  1.6735897064208984 3.0871167182922363 4.760706424713135
Loss :  1.6540534496307373 2.7674098014831543 4.4214630126953125
Loss :  1.6610369682312012 2.7393951416015625 4.400432109832764
Loss :  1.6664321422576904 3.2227847576141357 4.889216899871826
Loss :  1.6683796644210815 2.976560354232788 4.64493989944458
Loss :  1.6692595481872559 2.7904052734375 4.459664821624756
Loss :  1.6362435817718506 2.873072385787964 4.5093159675598145
  batch 20 loss: 1.6362435817718506, 2.873072385787964, 4.5093159675598145
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6725209951400757 3.334670066833496 5.007191181182861
Loss :  1.6817399263381958 3.1494343280792236 4.831174373626709
Loss :  1.6606701612472534 3.0723278522491455 4.732997894287109
Loss :  1.6920138597488403 3.2819032669067383 4.973917007446289
Loss :  1.6916590929031372 3.2035887241363525 4.895247936248779
Loss :  1.6669775247573853 3.437626361846924 5.1046037673950195
Loss :  1.704881191253662 3.353637933731079 5.05851936340332
Loss :  1.6495907306671143 3.3847548961639404 5.034345626831055
Loss :  1.698258399963379 3.4036149978637695 5.101873397827148
Loss :  1.6523306369781494 3.1676361560821533 4.819966793060303
Loss :  1.7307809591293335 3.634784698486328 5.365565776824951
Loss :  1.6753672361373901 3.543958902359009 5.219326019287109
Loss :  1.6617355346679688 3.1951987743377686 4.856934547424316
Loss :  1.6706620454788208 3.35317325592041 5.023835182189941
Loss :  1.70331871509552 3.3049161434173584 5.008234977722168
Loss :  1.6952226161956787 3.2964699268341064 4.991692543029785
Loss :  1.6768736839294434 3.1952247619628906 4.872098445892334
Loss :  1.6413543224334717 3.0924150943756104 4.733769416809082
Loss :  1.669698715209961 3.1609320640563965 4.830630779266357
Loss :  1.6639583110809326 3.5269577503204346 5.190916061401367
  batch 40 loss: 1.6639583110809326, 3.5269577503204346, 5.190916061401367
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.666438341140747 3.1216187477111816 4.788057327270508
Loss :  1.6557629108428955 2.9199345111846924 4.575697422027588
Loss :  1.67526376247406 2.855632781982422 4.5308966636657715
Loss :  1.6699535846710205 2.9056217670440674 4.575575351715088
Loss :  1.67207670211792 2.2888121604919434 3.9608888626098633
Loss :  1.6686854362487793 2.673421621322632 4.342106819152832
Loss :  1.6538292169570923 2.5090744495391846 4.162903785705566
Loss :  1.6632202863693237 2.958038330078125 4.621258735656738
Loss :  1.6431175470352173 2.607731342315674 4.250848770141602
Loss :  1.6889986991882324 3.098381519317627 4.787380218505859
Loss :  1.6607693433761597 3.051807165145874 4.712576389312744
Loss :  1.6770613193511963 3.006314992904663 4.683376312255859
Loss :  1.69046151638031 2.623676300048828 4.314137935638428
Loss :  1.672989845275879 2.926039934158325 4.599029541015625
Loss :  1.6830426454544067 2.9394378662109375 4.622480392456055
Loss :  1.6468888521194458 3.1844189167022705 4.831307888031006
Loss :  1.697632908821106 2.8877456188201904 4.585378646850586
Loss :  1.6860800981521606 3.471360445022583 5.157440662384033
Loss :  1.7078698873519897 3.5849485397338867 5.292818546295166
Loss :  1.6848037242889404 3.004530668258667 4.689334392547607
  batch 60 loss: 1.6848037242889404, 3.004530668258667, 4.689334392547607
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6793509721755981 3.5167388916015625 5.196089744567871
Loss :  1.6775240898132324 3.3274571895599365 5.00498104095459
Loss :  1.6836081743240356 3.5858829021453857 5.269491195678711
Loss :  1.6715433597564697 3.577314615249634 5.2488579750061035
Loss :  1.6652835607528687 3.095076084136963 4.760359764099121
Loss :  2.999171257019043 4.477078914642334 7.476250171661377
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  2.965129852294922 4.464420795440674 7.429550647735596
Loss :  2.9982144832611084 4.30757474899292 7.305788993835449
Loss :  2.518527030944824 4.308404445648193 6.826931476593018
Total LOSS train 4.767865811861478 valid 7.25963032245636
CE LOSS train 1.6729780967418963 valid 0.629631757736206
Contrastive LOSS train 3.094887715119582 valid 1.0771011114120483
EPOCH 76:
Loss :  1.6598323583602905 3.1275851726531982 4.787417411804199
Loss :  1.6730074882507324 3.450458288192749 5.123465538024902
Loss :  1.6557928323745728 3.515829563140869 5.171622276306152
Loss :  1.6608633995056152 3.1337709426879883 4.7946343421936035
Loss :  1.6881252527236938 3.3442182540893555 5.03234338760376
Loss :  1.6673001050949097 3.716670274734497 5.383970260620117
Loss :  1.6721270084381104 3.329932928085327 5.0020599365234375
Loss :  1.6611928939819336 3.478315830230713 5.1395087242126465
Loss :  1.6647292375564575 3.6258575916290283 5.290586948394775
Loss :  1.6210397481918335 3.3801424503326416 5.0011820793151855
Loss :  1.6750868558883667 3.103428602218628 4.778515338897705
Loss :  1.7318706512451172 3.5471043586730957 5.278975009918213
Loss :  1.6768531799316406 3.4291598796844482 5.106013298034668
Loss :  1.673454761505127 2.9779419898986816 4.651396751403809
Loss :  1.6556038856506348 2.9424855709075928 4.598089218139648
Loss :  1.6604297161102295 2.7642312049865723 4.424660682678223
Loss :  1.6657793521881104 2.920581340789795 4.586360931396484
Loss :  1.668676495552063 2.618224859237671 4.286901473999023
Loss :  1.6697783470153809 2.9896578788757324 4.659436225891113
Loss :  1.6365550756454468 2.536297082901001 4.172852039337158
  batch 20 loss: 1.6365550756454468, 2.536297082901001, 4.172852039337158
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6734843254089355 2.677700996398926 4.351185321807861
Loss :  1.6833999156951904 2.667675733566284 4.351075649261475
Loss :  1.6618237495422363 2.301159620285034 3.9629833698272705
Loss :  1.694603443145752 2.609154224395752 4.303757667541504
Loss :  1.6947996616363525 2.826463222503662 4.521263122558594
Loss :  1.6673614978790283 2.838573694229126 4.505935192108154
Loss :  1.7052066326141357 2.7634170055389404 4.468623638153076
Loss :  1.6503371000289917 2.7416839599609375 4.392021179199219
Loss :  1.6964770555496216 2.7480711936950684 4.4445481300354
Loss :  1.6547777652740479 2.580773115158081 4.235550880432129
Loss :  1.7318167686462402 2.8979547023773193 4.6297712326049805
Loss :  1.67698073387146 2.8123960494995117 4.489377021789551
Loss :  1.6627140045166016 2.7920117378234863 4.454725742340088
Loss :  1.6713316440582275 2.834012031555176 4.505343437194824
Loss :  1.705970287322998 2.954054355621338 4.660024642944336
Loss :  1.694683313369751 2.6748316287994385 4.3695149421691895
Loss :  1.677809715270996 2.681251049041748 4.359060764312744
Loss :  1.641218900680542 2.664471387863159 4.305690288543701
Loss :  1.6716252565383911 2.8411474227905273 4.512772560119629
Loss :  1.6647565364837646 2.6386735439300537 4.303430080413818
  batch 40 loss: 1.6647565364837646, 2.6386735439300537, 4.303430080413818
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6670634746551514 2.961317300796509 4.62838077545166
Loss :  1.6561833620071411 3.115936279296875 4.772119522094727
Loss :  1.6765016317367554 2.9278900623321533 4.604391574859619
Loss :  1.6711463928222656 3.008150577545166 4.679296970367432
Loss :  1.6730717420578003 2.9193685054779053 4.592440128326416
Loss :  1.6696547269821167 2.954171657562256 4.623826503753662
Loss :  1.654845952987671 3.1904492378234863 4.845294952392578
Loss :  1.6649140119552612 3.2054858207702637 4.8703999519348145
Loss :  1.6422085762023926 3.0892138481140137 4.731422424316406
Loss :  1.6896240711212158 3.014723062515259 4.704347133636475
Loss :  1.6603583097457886 3.4041740894317627 5.064532279968262
Loss :  1.6774778366088867 3.227156400680542 4.904634475708008
Loss :  1.6910263299942017 3.1853506565093994 4.876377105712891
Loss :  1.6741880178451538 3.1641244888305664 4.83831262588501
Loss :  1.6825283765792847 2.8831942081451416 4.565722465515137
Loss :  1.64875066280365 2.871443510055542 4.520194053649902
Loss :  1.6968562602996826 2.9817473888397217 4.678603649139404
Loss :  1.6865507364273071 3.045203924179077 4.731754779815674
Loss :  1.7066991329193115 3.244887351989746 4.951586723327637
Loss :  1.6840934753417969 2.727543830871582 4.411637306213379
  batch 60 loss: 1.6840934753417969, 2.727543830871582, 4.411637306213379
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6797552108764648 3.2462661266326904 4.926021575927734
Loss :  1.6775290966033936 3.3297173976898193 5.007246494293213
Loss :  1.6850042343139648 3.3095450401306152 4.99454927444458
Loss :  1.6705529689788818 2.9663846492767334 4.636937618255615
Loss :  1.6657670736312866 2.6575279235839844 4.3232951164245605
Loss :  2.940969228744507 4.501229286193848 7.442198753356934
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  2.9146904945373535 4.462634563446045 7.377325057983398
Loss :  2.9455513954162598 4.355887413024902 7.301438808441162
Loss :  2.492974042892456 4.317343711853027 6.8103179931640625
Total LOSS train 4.67507649568411 valid 7.232820153236389
CE LOSS train 1.6734096710498516 valid 0.623243510723114
Contrastive LOSS train 3.0016668319702147 valid 1.0793359279632568
EPOCH 77:
Loss :  1.6598044633865356 3.2771801948547363 4.936984539031982
Loss :  1.674923062324524 3.5444014072418213 5.219324588775635
Loss :  1.6581876277923584 3.266864061355591 4.925051689147949
Loss :  1.6626020669937134 3.3984618186950684 5.061063766479492
Loss :  1.6888231039047241 3.727649688720703 5.416472911834717
Loss :  1.6697794198989868 2.8450677394866943 4.514847278594971
Loss :  1.6714361906051636 3.0087687969207764 4.68020486831665
Loss :  1.6626384258270264 2.6602439880371094 4.322882652282715
Loss :  1.6647244691848755 2.535719871520996 4.200444221496582
Loss :  1.6203043460845947 2.373176097869873 3.9934804439544678
Loss :  1.6765384674072266 2.701125383377075 4.377663612365723
Loss :  1.732666015625 2.8464903831481934 4.579156398773193
Loss :  1.6779866218566895 2.7996058464050293 4.477592468261719
Loss :  1.6743789911270142 2.703200340270996 4.377579212188721
Loss :  1.6545275449752808 2.561978816986084 4.216506481170654
Loss :  1.6599388122558594 2.6297781467437744 4.289716720581055
Loss :  1.6679445505142212 2.591671943664551 4.259616374969482
Loss :  1.6684948205947876 2.9918601512908936 4.660355091094971
Loss :  1.6702404022216797 2.2357373237609863 3.905977725982666
Loss :  1.6367082595825195 2.558824062347412 4.195532321929932
  batch 20 loss: 1.6367082595825195, 2.558824062347412, 4.195532321929932
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6725213527679443 2.5521228313446045 4.224644184112549
Loss :  1.6830626726150513 2.690243721008301 4.3733062744140625
Loss :  1.6600617170333862 2.4706294536590576 4.130691051483154
Loss :  1.6933693885803223 2.6808879375457764 4.3742570877075195
Loss :  1.6908073425292969 2.773242950439453 4.46405029296875
Loss :  1.665925145149231 2.7906174659729004 4.456542491912842
Loss :  1.7050806283950806 2.968472719192505 4.673553466796875
Loss :  1.6507782936096191 2.583888292312622 4.23466682434082
Loss :  1.6983399391174316 2.8962690830230713 4.594609260559082
Loss :  1.6519120931625366 3.048903226852417 4.700815200805664
Loss :  1.7293728590011597 3.5337491035461426 5.263122081756592
Loss :  1.675662875175476 3.216184139251709 4.891847133636475
Loss :  1.662477731704712 2.973968029022217 4.636445999145508
Loss :  1.6690666675567627 3.273282766342163 4.942349433898926
Loss :  1.7027138471603394 3.3618369102478027 5.064550876617432
Loss :  1.694034218788147 3.369994878768921 5.064029216766357
Loss :  1.6759653091430664 3.212928295135498 4.8888936042785645
Loss :  1.6414976119995117 3.1186447143554688 4.7601423263549805
Loss :  1.6690362691879272 3.1742308139801025 4.84326696395874
Loss :  1.6631718873977661 3.3971617221832275 5.060333728790283
  batch 40 loss: 1.6631718873977661, 3.3971617221832275, 5.060333728790283
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6658546924591064 3.22648024559021 4.892334938049316
Loss :  1.653890609741211 3.3038179874420166 4.957708358764648
Loss :  1.6741405725479126 3.203235626220703 4.877376079559326
Loss :  1.6686711311340332 3.0672693252563477 4.735940456390381
Loss :  1.6709169149398804 3.1817615032196045 4.852678298950195
Loss :  1.6676565408706665 3.1906585693359375 4.8583149909973145
Loss :  1.6536041498184204 3.028923749923706 4.682528018951416
Loss :  1.663163423538208 3.12226939201355 4.785432815551758
Loss :  1.641008973121643 2.8747682571411133 4.515777111053467
Loss :  1.6886677742004395 2.5987026691436768 4.287370681762695
Loss :  1.6601074934005737 2.9428837299346924 4.602991104125977
Loss :  1.677128791809082 2.834033966064453 4.511162757873535
Loss :  1.6900054216384888 2.6482863426208496 4.338291645050049
Loss :  1.6748253107070923 2.6641104221343994 4.338935852050781
Loss :  1.6835628747940063 2.63753342628479 4.321096420288086
Loss :  1.648390531539917 2.522693395614624 4.171083927154541
Loss :  1.6972354650497437 2.8090403079986572 4.506275653839111
Loss :  1.688104271888733 2.9728124141693115 4.660916805267334
Loss :  1.7075071334838867 3.0142834186553955 4.721790313720703
Loss :  1.6851787567138672 2.813790798187256 4.498969554901123
  batch 60 loss: 1.6851787567138672, 2.813790798187256, 4.498969554901123
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6786032915115356 2.7780139446258545 4.45661735534668
Loss :  1.6791778802871704 2.675832509994507 4.355010509490967
Loss :  1.684170126914978 2.867579936981201 4.551750183105469
Loss :  1.672078013420105 2.753403902053833 4.425481796264648
Loss :  1.6675846576690674 2.574531078338623 4.2421159744262695
Loss :  2.94832181930542 4.446868896484375 7.395190715789795
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  2.919994831085205 4.434478759765625 7.35447359085083
Loss :  2.943747043609619 4.285366535186768 7.229113578796387
Loss :  2.487279176712036 4.213656425476074 6.700935363769531
Total LOSS train 4.5907772687765265 valid 7.169928312301636
CE LOSS train 1.6730575433144204 valid 0.621819794178009
Contrastive LOSS train 2.9177197236281174 valid 1.0534141063690186
EPOCH 78:
Loss :  1.6625511646270752 2.6521008014678955 4.314651966094971
Loss :  1.6725261211395264 3.101123809814453 4.773650169372559
Loss :  1.6594135761260986 2.8696281909942627 4.529041767120361
Loss :  1.665663480758667 2.753084897994995 4.418748378753662
Loss :  1.6899542808532715 3.1922857761383057 4.882240295410156
Loss :  1.6709622144699097 2.796734571456909 4.467696666717529
Loss :  1.6715552806854248 2.9651553630828857 4.6367106437683105
Loss :  1.6620097160339355 2.9071600437164307 4.569169998168945
Loss :  1.6665325164794922 2.7261455059051514 4.392678260803223
Loss :  1.6216830015182495 2.5976624488830566 4.219345569610596
Loss :  1.6764798164367676 3.2203080654144287 4.896787643432617
Loss :  1.7338213920593262 3.367122173309326 5.100943565368652
Loss :  1.6774694919586182 2.9245762825012207 4.602046012878418
Loss :  1.6729035377502441 2.9690022468566895 4.641905784606934
Loss :  1.657410740852356 3.1171445846557617 4.774555206298828
Loss :  1.660627841949463 3.4806272983551025 5.1412553787231445
Loss :  1.6664769649505615 3.430812358856201 5.097289085388184
Loss :  1.669087529182434 3.4538841247558594 5.122971534729004
Loss :  1.6711796522140503 3.1673014163970947 4.8384809494018555
Loss :  1.6351650953292847 3.207608222961426 4.8427734375
  batch 20 loss: 1.6351650953292847, 3.207608222961426, 4.8427734375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6735880374908447 3.131147623062134 4.8047356605529785
Loss :  1.6832839250564575 3.3349616527557373 5.018245697021484
Loss :  1.6612720489501953 2.9201037883758545 4.581376075744629
Loss :  1.6947959661483765 3.7061827182769775 5.4009785652160645
Loss :  1.6938600540161133 3.469381093978882 5.163241386413574
Loss :  1.6672817468643188 3.4350950717926025 5.102376937866211
Loss :  1.7050280570983887 3.4756176471710205 5.180645942687988
Loss :  1.6492929458618164 3.259239673614502 4.908532619476318
Loss :  1.6951749324798584 3.134882926940918 4.8300580978393555
Loss :  1.6527738571166992 3.2293612957000732 4.882135391235352
Loss :  1.7306653261184692 3.213099479675293 4.943764686584473
Loss :  1.675399899482727 3.34822416305542 5.023623943328857
Loss :  1.663047194480896 2.8635075092315674 4.526554584503174
Loss :  1.6688458919525146 2.962071418762207 4.630917549133301
Loss :  1.7033394575119019 3.0661540031433105 4.769493579864502
Loss :  1.6943402290344238 3.425455331802368 5.119795799255371
Loss :  1.6761990785598755 2.949951171875 4.626150131225586
Loss :  1.6425892114639282 3.0062992572784424 4.64888858795166
Loss :  1.6694051027297974 3.133298397064209 4.802703380584717
Loss :  1.6637375354766846 3.085587978363037 4.749325752258301
  batch 40 loss: 1.6637375354766846, 3.085587978363037, 4.749325752258301
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6664499044418335 2.948709011077881 4.615159034729004
Loss :  1.6549673080444336 2.414036273956299 4.069003582000732
Loss :  1.67401123046875 2.8836379051208496 4.5576491355896
Loss :  1.6688674688339233 2.5833146572113037 4.2521820068359375
Loss :  1.6716808080673218 2.2786338329315186 3.950314521789551
Loss :  1.6692692041397095 2.9369161128997803 4.606185436248779
Loss :  1.6544334888458252 3.1342294216156006 4.788662910461426
Loss :  1.6631300449371338 2.5487027168273926 4.2118330001831055
Loss :  1.6438884735107422 2.8001646995544434 4.4440531730651855
Loss :  1.687076449394226 2.593073844909668 4.280150413513184
Loss :  1.6581357717514038 3.1937148571014404 4.851850509643555
Loss :  1.6763241291046143 3.150240182876587 4.826564311981201
Loss :  1.6907179355621338 3.1038498878479004 4.794568061828613
Loss :  1.671167016029358 3.206528425216675 4.877695560455322
Loss :  1.6808160543441772 3.043390989303589 4.724206924438477
Loss :  1.646743893623352 2.9242587089538574 4.57100248336792
Loss :  1.6955841779708862 3.4525656700134277 5.1481499671936035
Loss :  1.6862554550170898 3.134469509124756 4.820724964141846
Loss :  1.706400990486145 3.589733600616455 5.2961344718933105
Loss :  1.6824839115142822 3.3902032375335693 5.072687149047852
  batch 60 loss: 1.6824839115142822, 3.3902032375335693, 5.072687149047852
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6796917915344238 3.652909517288208 5.332601547241211
Loss :  1.6767088174819946 3.484638214111328 5.161346912384033
Loss :  1.6843528747558594 3.7412703037261963 5.425622940063477
Loss :  1.6711210012435913 3.6012978553771973 5.272418975830078
Loss :  1.665989875793457 3.2541821002960205 4.920171737670898
Loss :  3.263543128967285 4.521243095397949 7.784786224365234
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  3.217047929763794 4.449511528015137 7.666559219360352
Loss :  3.2598154544830322 4.38328742980957 7.643102645874023
Loss :  2.7406442165374756 4.4082441329956055 7.14888858795166
Total LOSS train 4.782268098684457 valid 7.560834169387817
CE LOSS train 1.6731332613871648 valid 0.6851610541343689
Contrastive LOSS train 3.1091347987835225 valid 1.1020610332489014
EPOCH 79:
Loss :  1.6597228050231934 3.224375009536743 4.884098052978516
Loss :  1.6747676134109497 3.611980676651001 5.28674840927124
Loss :  1.6578463315963745 3.560687780380249 5.218533992767334
Loss :  1.6624000072479248 3.344855785369873 5.007255554199219
Loss :  1.6890665292739868 3.6644279956817627 5.353494644165039
Loss :  1.669040560722351 3.195739507675171 4.864779949188232
Loss :  1.6716575622558594 3.3074920177459717 4.97914981842041
Loss :  1.6619564294815063 3.0133962631225586 4.675352573394775
Loss :  1.6649620532989502 3.2088358402252197 4.87379789352417
Loss :  1.6196476221084595 3.00080943107605 4.620457172393799
Loss :  1.6759483814239502 3.1619374752044678 4.837885856628418
Loss :  1.73250412940979 3.433755397796631 5.166259765625
Loss :  1.677170753479004 3.3685975074768066 5.0457682609558105
Loss :  1.6723389625549316 3.325700044631958 4.998039245605469
Loss :  1.6558291912078857 3.2023963928222656 4.8582258224487305
Loss :  1.6593230962753296 3.4506194591522217 5.109942436218262
Loss :  1.6655678749084473 3.2840864658355713 4.949654579162598
Loss :  1.6678351163864136 3.2600674629211426 4.927902698516846
Loss :  1.6715837717056274 3.168074369430542 4.839658260345459
Loss :  1.635803461074829 3.375807523727417 5.011610984802246
  batch 20 loss: 1.635803461074829, 3.375807523727417, 5.011610984802246
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6721712350845337 3.335477590560913 5.007648944854736
Loss :  1.683846116065979 3.503164529800415 5.187010765075684
Loss :  1.6600401401519775 2.713362693786621 4.3734025955200195
Loss :  1.6960514783859253 3.367115020751953 5.063166618347168
Loss :  1.695125937461853 3.157724618911743 4.852850437164307
Loss :  1.666324496269226 2.731764554977417 4.3980889320373535
Loss :  1.7054535150527954 2.965700387954712 4.671154022216797
Loss :  1.6498054265975952 2.972954273223877 4.622759819030762
Loss :  1.6949265003204346 3.018092632293701 4.713019371032715
Loss :  1.6547266244888306 2.7294249534606934 4.384151458740234
Loss :  1.731544017791748 2.947157859802246 4.678701877593994
Loss :  1.676743507385254 3.0542471408843994 4.730990409851074
Loss :  1.662870168685913 2.4723799228668213 4.135250091552734
Loss :  1.6707791090011597 3.0135486125946045 4.684327602386475
Loss :  1.7061376571655273 2.8943328857421875 4.600470542907715
Loss :  1.693265438079834 2.6494040489196777 4.342669486999512
Loss :  1.6782751083374023 2.634296417236328 4.3125715255737305
Loss :  1.6407972574234009 2.645552635192871 4.286349773406982
Loss :  1.671676516532898 2.8893280029296875 4.561004638671875
Loss :  1.6651748418807983 3.097153663635254 4.762328624725342
  batch 40 loss: 1.6651748418807983, 3.097153663635254, 4.762328624725342
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.667511224746704 2.827867031097412 4.495378494262695
Loss :  1.656286358833313 2.237464666366577 3.8937511444091797
Loss :  1.677433967590332 2.716949462890625 4.394383430480957
Loss :  1.6712955236434937 2.4969193935394287 4.168214797973633
Loss :  1.672756314277649 2.2209951877593994 3.893751621246338
Loss :  1.6688162088394165 3.056899309158325 4.725715637207031
Loss :  1.6536284685134888 2.853971004486084 4.507599353790283
Loss :  1.6653656959533691 2.7692954540252686 4.434660911560059
Loss :  1.6413570642471313 2.7975475788116455 4.438904762268066
Loss :  1.6909070014953613 3.215656280517578 4.9065632820129395
Loss :  1.661733627319336 2.695913553237915 4.357646942138672
Loss :  1.6785743236541748 2.6806561946868896 4.3592305183410645
Loss :  1.6903181076049805 3.066494941711426 4.756813049316406
Loss :  1.6749790906906128 3.095482349395752 4.770461559295654
Loss :  1.6828665733337402 2.8700766563415527 4.552943229675293
Loss :  1.6486990451812744 2.7296535968780518 4.378352642059326
Loss :  1.6965069770812988 3.076453447341919 4.772960662841797
Loss :  1.687331199645996 3.273545742034912 4.960876941680908
Loss :  1.7060142755508423 3.341904878616333 5.047919273376465
Loss :  1.6850321292877197 3.169715166091919 4.854747295379639
  batch 60 loss: 1.6850321292877197, 3.169715166091919, 4.854747295379639
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6788359880447388 3.0782406330108643 4.757076740264893
Loss :  1.6771297454833984 3.0850822925567627 4.762211799621582
Loss :  1.6851327419281006 2.9592859745025635 4.644418716430664
Loss :  1.6699944734573364 2.997454881668091 4.667449474334717
Loss :  1.66505765914917 2.434939384460449 4.099997043609619
Loss :  3.104127883911133 4.440027713775635 7.544155597686768
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  3.064119338989258 4.42860746383667 7.492726802825928
Loss :  3.096245527267456 4.270698070526123 7.366943359375
Loss :  2.596572160720825 4.272379398345947 6.868951797485352
Total LOSS train 4.699670197413518 valid 7.318194389343262
CE LOSS train 1.6734503250855666 valid 0.6491430401802063
Contrastive LOSS train 3.0262198448181152 valid 1.0680948495864868
EPOCH 80:
Loss :  1.660609483718872 3.241281747817993 4.901891231536865
Loss :  1.6734963655471802 3.227426052093506 4.9009222984313965
Loss :  1.6571615934371948 2.7815282344818115 4.438689708709717
Loss :  1.6622264385223389 3.181372880935669 4.843599319458008
Loss :  1.6885404586791992 3.1007165908813477 4.789257049560547
Loss :  1.6685758829116821 2.7758567333221436 4.444432735443115
Loss :  1.6719361543655396 2.724581003189087 4.396517276763916
Loss :  1.6616921424865723 3.0407607555389404 4.702452659606934
Loss :  1.6641759872436523 2.632981300354004 4.297157287597656
Loss :  1.620511770248413 2.4878199100494385 4.108331680297852
Loss :  1.676345705986023 3.1444003582000732 4.820745944976807
Loss :  1.7302675247192383 2.712810516357422 4.44307804107666
Loss :  1.6777088642120361 2.833042860031128 4.510751724243164
Loss :  1.6730563640594482 3.045708656311035 4.7187652587890625
Loss :  1.6544424295425415 2.745295286178589 4.39973783493042
Loss :  1.659665822982788 2.854555606842041 4.51422119140625
Loss :  1.666090726852417 2.855013132095337 4.521103858947754
Loss :  1.6683377027511597 2.8128654956817627 4.481203079223633
Loss :  1.6699550151824951 2.9057083129882812 4.5756635665893555
Loss :  1.6356149911880493 2.7008659839630127 4.336481094360352
  batch 20 loss: 1.6356149911880493, 2.7008659839630127, 4.336481094360352
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6723607778549194 3.2908732891082764 4.963233947753906
Loss :  1.6825467348098755 3.1060800552368164 4.788626670837402
Loss :  1.6594692468643188 2.7945072650909424 4.453976631164551
Loss :  1.693364143371582 2.956211566925049 4.649575710296631
Loss :  1.6900111436843872 3.051241636276245 4.741252899169922
Loss :  1.6646807193756104 3.2457950115203857 4.910475730895996
Loss :  1.7052288055419922 3.1215336322784424 4.8267621994018555
Loss :  1.6505084037780762 3.262146234512329 4.912654876708984
Loss :  1.6968482732772827 3.176284074783325 4.873132228851318
Loss :  1.6505205631256104 3.1252782344818115 4.775798797607422
Loss :  1.7281752824783325 3.1316235065460205 4.859798908233643
Loss :  1.6758065223693848 3.1984236240386963 4.87423038482666
Loss :  1.661491870880127 2.6128687858581543 4.274360656738281
Loss :  1.667734980583191 3.0428030490875244 4.710537910461426
Loss :  1.7023781538009644 3.2270638942718506 4.929441928863525
Loss :  1.6926395893096924 3.051285982131958 4.74392557144165
Loss :  1.6758499145507812 3.325981616973877 5.001831531524658
Loss :  1.6426345109939575 3.256417989730835 4.899052619934082
Loss :  1.669100284576416 3.6076338291168213 5.276734352111816
Loss :  1.6630759239196777 3.051189422607422 4.7142653465271
  batch 40 loss: 1.6630759239196777, 3.051189422607422, 4.7142653465271
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.665802001953125 3.3307957649230957 4.996597766876221
Loss :  1.6527016162872314 3.3786494731903076 5.031351089477539
Loss :  1.6727374792099 3.1988985538482666 4.871635913848877
Loss :  1.6674091815948486 2.9924051761627197 4.659814357757568
Loss :  1.6710314750671387 2.9428610801696777 4.613892555236816
Loss :  1.6673779487609863 2.933215856552124 4.600593566894531
Loss :  1.6537095308303833 2.992980718612671 4.646690368652344
Loss :  1.662395715713501 2.87103009223938 4.533425807952881
Loss :  1.6410565376281738 3.101477861404419 4.742534637451172
Loss :  1.6871538162231445 2.9623444080352783 4.649497985839844
Loss :  1.6573923826217651 3.1869168281555176 4.844309329986572
Loss :  1.6752432584762573 3.372180223464966 5.047423362731934
Loss :  1.6899183988571167 2.916733741760254 4.60665225982666
Loss :  1.672937035560608 3.1996798515319824 4.872616767883301
Loss :  1.680853247642517 3.185170888900757 4.866024017333984
Loss :  1.6474685668945312 3.163079023361206 4.810547828674316
Loss :  1.6955169439315796 3.0181398391723633 4.713656902313232
Loss :  1.6868690252304077 2.9731924533843994 4.660061359405518
Loss :  1.7054023742675781 3.2091875076293945 4.914589881896973
Loss :  1.6835904121398926 3.1198649406433105 4.803455352783203
  batch 60 loss: 1.6835904121398926, 3.1198649406433105, 4.803455352783203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6787385940551758 3.3203864097595215 4.999125003814697
Loss :  1.6780970096588135 3.0697712898254395 4.747868537902832
Loss :  1.6848138570785522 2.692826747894287 4.377640724182129
Loss :  1.6705435514450073 3.0980565547943115 4.768599987030029
Loss :  1.666324496269226 2.7540674209594727 4.420392036437988
Loss :  3.6348791122436523 4.496869087219238 8.13174819946289
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  3.5811116695404053 4.428806781768799 8.009918212890625
Loss :  3.629547595977783 4.219173431396484 7.848721027374268
Loss :  3.0541718006134033 4.280104160308838 7.33427619934082
Total LOSS train 4.709594909961407 valid 7.831165909767151
CE LOSS train 1.672306488110469 valid 0.7635429501533508
Contrastive LOSS train 3.0372884126809927 valid 1.0700260400772095
EPOCH 81:
Loss :  1.6606208086013794 3.2016303539276123 4.862251281738281
Loss :  1.6736502647399902 3.1409249305725098 4.8145751953125
Loss :  1.6604853868484497 2.9101922512054443 4.570677757263184
Loss :  1.6666600704193115 3.1190614700317383 4.785721778869629
Loss :  1.6885517835617065 3.2627928256988525 4.9513444900512695
Loss :  1.671662449836731 2.8991198539733887 4.57078218460083
Loss :  1.6701833009719849 2.73429012298584 4.404473304748535
Loss :  1.661921501159668 2.706366777420044 4.368288040161133
Loss :  1.665087103843689 2.6685233116149902 4.333610534667969
Loss :  1.6187036037445068 2.9397459030151367 4.558449745178223
Loss :  1.6768946647644043 3.178816795349121 4.855711460113525
Loss :  1.7343257665634155 3.459942579269409 5.194268226623535
Loss :  1.6779794692993164 3.255519151687622 4.933498382568359
Loss :  1.6712672710418701 3.465346574783325 5.136613845825195
Loss :  1.6553187370300293 3.277181386947632 4.932499885559082
Loss :  1.6604511737823486 3.383685350418091 5.0441365242004395
Loss :  1.666422724723816 3.0756938457489014 4.742116451263428
Loss :  1.6677241325378418 3.061326265335083 4.729050636291504
Loss :  1.6740148067474365 3.05611515045166 4.730130195617676
Loss :  1.6346769332885742 2.93522310256958 4.569900035858154
  batch 20 loss: 1.6346769332885742, 2.93522310256958, 4.569900035858154
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6716774702072144 3.2240378856658936 4.895715236663818
Loss :  1.6832866668701172 3.168811559677124 4.85209846496582
Loss :  1.6575415134429932 3.1935715675354004 4.851113319396973
Loss :  1.6961195468902588 2.6940906047821045 4.390210151672363
Loss :  1.6947379112243652 3.0327980518341064 4.727536201477051
Loss :  1.6632808446884155 3.1663904190063477 4.829671382904053
Loss :  1.7042661905288696 2.9037442207336426 4.608010292053223
Loss :  1.6485414505004883 2.996516466140747 4.645057678222656
Loss :  1.6927155256271362 3.069934606552124 4.762650012969971
Loss :  1.6538937091827393 3.3652398586273193 5.019133567810059
Loss :  1.7294635772705078 3.2435643672943115 4.973028182983398
Loss :  1.67581307888031 2.929023265838623 4.604836463928223
Loss :  1.6615631580352783 2.724356174468994 4.385919570922852
Loss :  1.668351173400879 2.658954381942749 4.327305793762207
Loss :  1.7049089670181274 2.9880762100219727 4.6929850578308105
Loss :  1.691151738166809 2.9543097019195557 4.645461559295654
Loss :  1.6766571998596191 2.6657025814056396 4.34235954284668
Loss :  1.6411410570144653 2.418187141418457 4.059328079223633
Loss :  1.670717477798462 2.517052173614502 4.187769889831543
Loss :  1.6636343002319336 2.345899820327759 4.009533882141113
  batch 40 loss: 1.6636343002319336, 2.345899820327759, 4.009533882141113
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6675647497177124 2.755352735519409 4.422917366027832
Loss :  1.6554964780807495 2.382903814315796 4.038400173187256
Loss :  1.6757378578186035 2.5462162494659424 4.221954345703125
Loss :  1.6707499027252197 2.760883331298828 4.431632995605469
Loss :  1.6727209091186523 2.3100368976593018 3.982757806777954
Loss :  1.6682473421096802 3.5630319118499756 5.231279373168945
Loss :  1.6532790660858154 2.901259422302246 4.554538726806641
Loss :  1.663122296333313 2.6567888259887695 4.319911003112793
Loss :  1.6416257619857788 2.7870898246765137 4.428715705871582
Loss :  1.687776803970337 2.661766290664673 4.34954309463501
Loss :  1.658470630645752 3.0261709690093994 4.6846418380737305
Loss :  1.6762244701385498 2.870696544647217 4.5469207763671875
Loss :  1.6895029544830322 2.9733126163482666 4.662815570831299
Loss :  1.6710915565490723 3.0492613315582275 4.720353126525879
Loss :  1.680943250656128 3.273949384689331 4.954892635345459
Loss :  1.6471062898635864 2.837289571762085 4.484395980834961
Loss :  1.6936891078948975 3.093285083770752 4.78697395324707
Loss :  1.6842669248580933 2.9659969806671143 4.650263786315918
Loss :  1.7044135332107544 3.375119209289551 5.079532623291016
Loss :  1.683127760887146 2.8461129665374756 4.529240608215332
  batch 60 loss: 1.683127760887146, 2.8461129665374756, 4.529240608215332
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6777303218841553 3.0668222904205322 4.7445526123046875
Loss :  1.675398588180542 3.0956833362579346 4.771081924438477
Loss :  1.684003472328186 3.202528953552246 4.886532306671143
Loss :  1.6690027713775635 3.159871816635132 4.828874588012695
Loss :  1.6638364791870117 2.4418349266052246 4.105671405792236
Loss :  3.191599130630493 4.48042631149292 7.672025680541992
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 5], device='cuda:0')
Loss :  3.141477108001709 4.397897720336914 7.539374828338623
Loss :  3.1864888668060303 4.334254741668701 7.520743370056152
Loss :  2.654297351837158 4.288458347320557 6.942755699157715
Total LOSS train 4.635634132531973 valid 7.418724894523621
CE LOSS train 1.6726337506220892 valid 0.6635743379592896
Contrastive LOSS train 2.9630003745739275 valid 1.0721145868301392
EPOCH 82:
Loss :  1.6579068899154663 3.534818410873413 5.19272518157959
Loss :  1.6727112531661987 3.6788179874420166 5.351529121398926
Loss :  1.654091715812683 3.0261571407318115 4.680248737335205
Loss :  1.6594034433364868 3.021167516708374 4.68057107925415
Loss :  1.6870027780532837 3.3287386894226074 5.015741348266602
Loss :  1.667284369468689 3.4525110721588135 5.119795322418213
Loss :  1.6708483695983887 3.481602907180786 5.152451515197754
Loss :  1.661323070526123 3.883758544921875 5.545081615447998
Loss :  1.6624394655227661 3.974513053894043 5.6369524002075195
Loss :  1.6204572916030884 3.293332099914551 4.91378927230835
Loss :  1.6766780614852905 3.140151262283325 4.816829204559326
Loss :  1.72995126247406 3.4746901988983154 5.204641342163086
Loss :  1.6781880855560303 3.362628221511841 5.040816307067871
Loss :  1.6726528406143188 3.306737184524536 4.9793901443481445
Loss :  1.6539191007614136 3.1309814453125 4.784900665283203
Loss :  1.6596338748931885 3.5127146244049072 5.172348499298096
Loss :  1.6656900644302368 3.528609275817871 5.194299221038818
Loss :  1.6686952114105225 3.104674816131592 4.773369789123535
Loss :  1.6683835983276367 2.63927960395813 4.3076629638671875
Loss :  1.6357412338256836 3.038421630859375 4.674162864685059
  batch 20 loss: 1.6357412338256836, 3.038421630859375, 4.674162864685059
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.672333002090454 2.8658523559570312 4.538185119628906
Loss :  1.682241678237915 3.0323448181152344 4.71458625793457
Loss :  1.6609641313552856 2.8427939414978027 4.503757953643799
Loss :  1.692980408668518 3.264267683029175 4.957248210906982
Loss :  1.6912298202514648 3.276717185974121 4.967947006225586
Loss :  1.6667252779006958 2.776503324508667 4.443228721618652
Loss :  1.7054978609085083 3.013699769973755 4.719197750091553
Loss :  1.6501026153564453 2.870518445968628 4.520621299743652
Loss :  1.6960344314575195 2.8854448795318604 4.581479072570801
Loss :  1.6525583267211914 3.1056411266326904 4.758199691772461
Loss :  1.7302758693695068 3.0854344367980957 4.815710067749023
Loss :  1.6760075092315674 3.403700590133667 5.079708099365234
Loss :  1.6622588634490967 2.991685152053833 4.65394401550293
Loss :  1.6700053215026855 2.815166711807251 4.485172271728516
Loss :  1.7043426036834717 3.1447348594665527 4.849077224731445
Loss :  1.6932214498519897 3.372856616973877 5.066078186035156
Loss :  1.6770610809326172 2.795427083969116 4.4724884033203125
Loss :  1.6395933628082275 2.5837900638580322 4.22338342666626
Loss :  1.6692893505096436 3.061908006668091 4.731197357177734
Loss :  1.6637018918991089 2.904852867126465 4.568554878234863
  batch 40 loss: 1.6637018918991089, 2.904852867126465, 4.568554878234863
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6657179594039917 3.181443691253662 4.847161769866943
Loss :  1.6542348861694336 2.567331552505493 4.221566200256348
Loss :  1.6763792037963867 2.7940657138824463 4.470444679260254
Loss :  1.6699557304382324 2.8057820796966553 4.475737571716309
Loss :  1.6724375486373901 2.3161351680755615 3.988572597503662
Loss :  1.6671898365020752 2.6706063747406006 4.337796211242676
Loss :  1.6529715061187744 3.0835788249969482 4.736550331115723
Loss :  1.663953185081482 2.700610637664795 4.364563941955566
Loss :  1.63938570022583 3.024366855621338 4.663752555847168
Loss :  1.6909652948379517 3.305225372314453 4.996190547943115
Loss :  1.6616508960723877 3.5347342491149902 5.196385383605957
Loss :  1.6767632961273193 3.0222561359405518 4.699019432067871
Loss :  1.6886961460113525 3.389305591583252 5.078001976013184
Loss :  1.6763545274734497 2.8751182556152344 4.5514726638793945
Loss :  1.6840105056762695 3.073530912399292 4.757541656494141
Loss :  1.6487277746200562 2.6325523853302 4.281280040740967
Loss :  1.6955434083938599 2.8026556968688965 4.498198986053467
Loss :  1.6883785724639893 2.7935221195220947 4.481900691986084
Loss :  1.705985188484192 3.269576072692871 4.975561141967773
Loss :  1.6845557689666748 3.1051816940307617 4.789737701416016
  batch 60 loss: 1.6845557689666748, 3.1051816940307617, 4.789737701416016
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6766821146011353 2.9147911071777344 4.59147310256958
Loss :  1.6777435541152954 3.0995001792907715 4.777243614196777
Loss :  1.6828272342681885 2.7360029220581055 4.418829917907715
Loss :  1.6682007312774658 3.0055432319641113 4.673744201660156
Loss :  1.664605736732483 2.7578463554382324 4.422451972961426
Loss :  3.239126682281494 4.523797512054443 7.7629241943359375
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  3.1980159282684326 4.401845455169678 7.599861145019531
Loss :  3.2320656776428223 4.3087029457092285 7.540768623352051
Loss :  2.7181122303009033 4.240013122558594 6.958125114440918
Total LOSS train 4.756650007688082 valid 7.465419769287109
CE LOSS train 1.6725129714378943 valid 0.6795280575752258
Contrastive LOSS train 3.0841370582580567 valid 1.0600032806396484
EPOCH 83:
Loss :  1.662376046180725 2.5472846031188965 4.209660530090332
Loss :  1.6696964502334595 2.6868910789489746 4.3565874099731445
Loss :  1.6579395532608032 2.6089932918548584 4.266932964324951
Loss :  1.6659289598464966 2.620159387588501 4.286088466644287
Loss :  1.6884645223617554 2.981064796447754 4.669529438018799
Loss :  1.6700979471206665 2.8568952083587646 4.526993274688721
Loss :  1.6690603494644165 3.2066850662231445 4.8757452964782715
Loss :  1.6597269773483276 3.049034595489502 4.708761692047119
Loss :  1.663174033164978 3.222050428390503 4.885224342346191
Loss :  1.6181660890579224 2.996558427810669 4.614724636077881
Loss :  1.6737717390060425 3.7908363342285156 5.464608192443848
Loss :  1.7318013906478882 2.9582741260528564 4.690075397491455
Loss :  1.6755298376083374 3.123624563217163 4.799154281616211
Loss :  1.6696722507476807 2.75492262840271 4.424594879150391
Loss :  1.6559125185012817 2.5827877521514893 4.2387003898620605
Loss :  1.6576101779937744 2.961853265762329 4.6194634437561035
Loss :  1.662473201751709 2.5764517784118652 4.238924980163574
Loss :  1.6673336029052734 2.736839771270752 4.404173374176025
Loss :  1.6699416637420654 2.549008369445801 4.218950271606445
Loss :  1.63306725025177 2.739757537841797 4.372824668884277
  batch 20 loss: 1.63306725025177, 2.739757537841797, 4.372824668884277
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6714842319488525 2.5553297996520996 4.226814270019531
Loss :  1.6808701753616333 2.9078965187072754 4.588766574859619
Loss :  1.6590362787246704 2.5393805503845215 4.198416709899902
Loss :  1.6937495470046997 2.5805089473724365 4.274258613586426
Loss :  1.6924216747283936 3.081507921218872 4.773929595947266
Loss :  1.664673089981079 2.859778881072998 4.524452209472656
Loss :  1.7031701803207397 2.8251473903656006 4.528317451477051
Loss :  1.6485234498977661 2.847989797592163 4.496513366699219
Loss :  1.6931954622268677 2.569026470184326 4.262221813201904
Loss :  1.6512815952301025 2.8027608394622803 4.454042434692383
Loss :  1.7271134853363037 3.2628085613250732 4.989922046661377
Loss :  1.6745822429656982 3.472533702850342 5.147115707397461
Loss :  1.661123514175415 3.0001769065856934 4.6613006591796875
Loss :  1.6670396327972412 2.9754209518432617 4.642460823059082
Loss :  1.7003071308135986 3.2637250423431396 4.964032173156738
Loss :  1.6916638612747192 3.336888313293457 5.028552055358887
Loss :  1.675183892250061 3.0930750370025635 4.768259048461914
Loss :  1.6403778791427612 3.132007598876953 4.772385597229004
Loss :  1.6672440767288208 3.1578638553619385 4.825108051300049
Loss :  1.661664366722107 2.8424770832061768 4.504141330718994
  batch 40 loss: 1.661664366722107, 2.8424770832061768, 4.504141330718994
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6647369861602783 3.355199098587036 5.0199360847473145
Loss :  1.652705192565918 3.088745355606079 4.741450309753418
Loss :  1.6719251871109009 3.1387133598327637 4.810638427734375
Loss :  1.6677221059799194 2.798771619796753 4.466493606567383
Loss :  1.671138048171997 3.0699055194854736 4.741043567657471
Loss :  1.6663514375686646 3.1729743480682373 4.839325904846191
Loss :  1.653246283531189 3.146552324295044 4.799798488616943
Loss :  1.6614642143249512 2.97312331199646 4.634587287902832
Loss :  1.6413898468017578 3.5310723781585693 5.172462463378906
Loss :  1.6847783327102661 3.2943148612976074 4.979093074798584
Loss :  1.6543508768081665 3.114917516708374 4.76926851272583
Loss :  1.673366665840149 3.249164342880249 4.9225311279296875
Loss :  1.6890512704849243 3.145500421524048 4.834551811218262
Loss :  1.669766902923584 2.898120164871216 4.567887306213379
Loss :  1.6773344278335571 3.260788917541504 4.9381232261657715
Loss :  1.6463701725006104 3.060119152069092 4.706489562988281
Loss :  1.6934466361999512 2.8518333435058594 4.5452799797058105
Loss :  1.6832953691482544 3.094025135040283 4.777320384979248
Loss :  1.7030144929885864 3.508493185043335 5.211507797241211
Loss :  1.681361198425293 2.7752177715301514 4.456579208374023
  batch 60 loss: 1.681361198425293, 2.7752177715301514, 4.456579208374023
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6778336763381958 2.7696681022644043 4.4475016593933105
Loss :  1.6746206283569336 2.6826508045196533 4.357271194458008
Loss :  1.6837716102600098 2.792301893234253 4.476073265075684
Loss :  1.6687215566635132 3.060397148132324 4.729118824005127
Loss :  1.662251591682434 2.057649612426758 3.7199010848999023
Loss :  3.5024709701538086 4.493610858917236 7.996081829071045
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  3.458360433578491 4.44221830368042 7.900578498840332
Loss :  3.50766921043396 4.300261497497559 7.807930946350098
Loss :  2.9134914875030518 4.290693759918213 7.204185485839844
Total LOSS train 4.633337886516864 valid 7.72719419002533
CE LOSS train 1.6710840775416447 valid 0.7283728718757629
Contrastive LOSS train 2.9622537979712855 valid 1.0726734399795532
EPOCH 84:
Loss :  1.6564825773239136 3.144479274749756 4.800961971282959
Loss :  1.6735625267028809 3.3800508975982666 5.053613662719727
Loss :  1.653826355934143 3.019838571548462 4.6736650466918945
Loss :  1.6569390296936035 3.3136651515960693 4.970603942871094
Loss :  1.6851258277893066 3.2514901161193848 4.936615943908691
Loss :  1.664081335067749 2.7444283962249756 4.408509731292725
Loss :  1.6710954904556274 2.9761123657226562 4.647207736968994
Loss :  1.6602067947387695 3.553394079208374 5.213601112365723
Loss :  1.6621347665786743 3.147207021713257 4.809341907501221
Loss :  1.6200391054153442 3.124824285507202 4.744863510131836
Loss :  1.6736034154891968 3.256971836090088 4.930575370788574
Loss :  1.7279670238494873 3.6778106689453125 5.405777931213379
Loss :  1.6762334108352661 3.130798578262329 4.807032108306885
Loss :  1.671431541442871 2.783101797103882 4.454533576965332
Loss :  1.6529669761657715 2.8496451377868652 4.502612113952637
Loss :  1.65834379196167 3.1593005657196045 4.817644119262695
Loss :  1.6635541915893555 2.789288282394409 4.452842712402344
Loss :  1.666240930557251 2.478315830230713 4.144556999206543
Loss :  1.6664350032806396 2.7410528659820557 4.407487869262695
Loss :  1.63382089138031 2.954991340637207 4.588812351226807
  batch 20 loss: 1.63382089138031, 2.954991340637207, 4.588812351226807
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6707944869995117 2.7509958744049072 4.42179012298584
Loss :  1.6794730424880981 2.8496625423431396 4.529135704040527
Loss :  1.6596719026565552 2.4399986267089844 4.09967041015625
Loss :  1.6913031339645386 2.708110809326172 4.3994140625
Loss :  1.6929036378860474 2.9619314670562744 4.654835224151611
Loss :  1.665050983428955 2.593581438064575 4.258632659912109
Loss :  1.7031148672103882 2.8078503608703613 4.510965347290039
Loss :  1.6465458869934082 2.735062837600708 4.381608963012695
Loss :  1.6922476291656494 2.449362277984619 4.141610145568848
Loss :  1.6534428596496582 2.9055659770965576 4.559008598327637
Loss :  1.7306369543075562 3.3281288146972656 5.058765888214111
Loss :  1.6746655702590942 3.3813416957855225 5.056007385253906
Loss :  1.6608079671859741 2.920572519302368 4.581380367279053
Loss :  1.6701642274856567 2.859222173690796 4.529386520385742
Loss :  1.7054554224014282 3.392191171646118 5.097646713256836
Loss :  1.6924610137939453 3.7499611377716064 5.442421913146973
Loss :  1.6762038469314575 3.4526491165161133 5.128852844238281
Loss :  1.6393377780914307 3.240590810775757 4.8799285888671875
Loss :  1.6696505546569824 2.747715473175049 4.417366027832031
Loss :  1.663948893547058 2.954216957092285 4.618165969848633
  batch 40 loss: 1.663948893547058, 2.954216957092285, 4.618165969848633
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.666731357574463 2.7991738319396973 4.46590518951416
Loss :  1.6553840637207031 2.877174139022827 4.532558441162109
Loss :  1.6776740550994873 3.2750725746154785 4.952746391296387
Loss :  1.671012282371521 3.236449956893921 4.907462120056152
Loss :  1.6733875274658203 2.8022055625915527 4.475593090057373
Loss :  1.666534423828125 2.9907710552215576 4.657305717468262
Loss :  1.6512384414672852 2.633324384689331 4.284563064575195
Loss :  1.6646524667739868 2.7687747478485107 4.433427333831787
Loss :  1.6380035877227783 2.918168067932129 4.556171417236328
Loss :  1.6923645734786987 2.746548652648926 4.438913345336914
Loss :  1.6632347106933594 2.9384963512420654 4.601731300354004
Loss :  1.6778779029846191 3.356198310852051 5.03407621383667
Loss :  1.6874221563339233 3.0409305095672607 4.7283525466918945
Loss :  1.6790047883987427 3.1549081802368164 4.8339128494262695
Loss :  1.6843494176864624 3.4012560844421387 5.085605621337891
Loss :  1.6458414793014526 2.7960448265075684 4.4418864250183105
Loss :  1.6951864957809448 2.8226020336151123 4.517788410186768
Loss :  1.6898325681686401 3.0483362674713135 4.738168716430664
Loss :  1.7055103778839111 3.2372467517852783 4.9427571296691895
Loss :  1.6828101873397827 2.7930521965026855 4.475862503051758
  batch 60 loss: 1.6828101873397827, 2.7930521965026855, 4.475862503051758
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.675436019897461 3.1387598514556885 4.81419563293457
Loss :  1.6766798496246338 3.239100217819214 4.915780067443848
Loss :  1.6823854446411133 3.27944016456604 4.961825370788574
Loss :  1.667448878288269 2.900080680847168 4.567529678344727
Loss :  1.6644127368927002 2.995234251022339 4.659646987915039
Loss :  3.120335102081299 4.414642333984375 7.534977436065674
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  3.0696229934692383 4.469679832458496 7.539302825927734
Loss :  3.1141133308410645 4.347720146179199 7.461833477020264
Loss :  2.6222548484802246 4.331340312957764 6.953595161437988
Total LOSS train 4.685557211362399 valid 7.372427225112915
CE LOSS train 1.6717905759811402 valid 0.6555637121200562
Contrastive LOSS train 3.013766596867488 valid 1.082835078239441
EPOCH 85:
Loss :  1.6610738039016724 2.9169692993164062 4.578042984008789
Loss :  1.6689848899841309 3.4420347213745117 5.111019611358643
Loss :  1.6564401388168335 2.589252471923828 4.245692729949951
Loss :  1.6633387804031372 3.068141222000122 4.731480121612549
Loss :  1.6883841753005981 3.389984607696533 5.078368663787842
Loss :  1.668151617050171 2.7909440994262695 4.4590959548950195
Loss :  1.6698521375656128 3.5078277587890625 5.177680015563965
Loss :  1.659548044204712 2.6677355766296387 4.32728385925293
Loss :  1.6622382402420044 3.037031888961792 4.699270248413086
Loss :  1.6200554370880127 2.631840467453003 4.251895904541016
Loss :  1.675606369972229 3.222712755203247 4.898319244384766
Loss :  1.7298393249511719 3.2471976280212402 4.977036952972412
Loss :  1.6767175197601318 3.1424460411071777 4.8191633224487305
Loss :  1.6711844205856323 3.065016984939575 4.736201286315918
Loss :  1.654023289680481 2.893510103225708 4.5475335121154785
Loss :  1.6600430011749268 2.905881643295288 4.565924644470215
Loss :  1.6645618677139282 2.9996418952941895 4.664203643798828
Loss :  1.6685187816619873 2.930753469467163 4.59927225112915
Loss :  1.6687943935394287 2.7414803504943848 4.410274505615234
Loss :  1.6347019672393799 3.2939224243164062 4.928624153137207
  batch 20 loss: 1.6347019672393799, 3.2939224243164062, 4.928624153137207
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6715251207351685 3.2812376022338867 4.952762603759766
Loss :  1.6818957328796387 3.3332645893096924 5.01516056060791
Loss :  1.6594417095184326 3.0432350635528564 4.702676773071289
Loss :  1.6929450035095215 3.376533269882202 5.0694780349731445
Loss :  1.6919689178466797 3.1381003856658936 4.830069541931152
Loss :  1.664955973625183 3.185992479324341 4.850948333740234
Loss :  1.7050938606262207 2.881779432296753 4.5868730545043945
Loss :  1.64895761013031 3.204712152481079 4.8536696434021
Loss :  1.6958346366882324 3.0938539505004883 4.789688587188721
Loss :  1.651063084602356 3.1960856914520264 4.847148895263672
Loss :  1.7295149564743042 3.405410051345825 5.13492488861084
Loss :  1.6755471229553223 3.022313356399536 4.6978607177734375
Loss :  1.6616721153259277 2.916853427886963 4.578525543212891
Loss :  1.668565034866333 3.1134297847747803 4.781994819641113
Loss :  1.702721118927002 3.320430040359497 5.023151397705078
Loss :  1.6934670209884644 2.8739356994628906 4.5674028396606445
Loss :  1.676140308380127 3.0285468101501465 4.704687118530273
Loss :  1.6403558254241943 2.989382266998291 4.629737854003906
Loss :  1.668277382850647 3.0232019424438477 4.691479206085205
Loss :  1.662786602973938 3.421949863433838 5.084736347198486
  batch 40 loss: 1.662786602973938, 3.421949863433838, 5.084736347198486
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6651536226272583 3.4724314212799072 5.137585163116455
Loss :  1.654420256614685 2.8659658432006836 4.520386219024658
Loss :  1.6745092868804932 3.388263463973999 5.062772750854492
Loss :  1.6688114404678345 3.2210886478424072 4.889900207519531
Loss :  1.6723597049713135 2.9333202838897705 4.605679988861084
Loss :  1.6673780679702759 2.7979736328125 4.465351581573486
Loss :  1.6528120040893555 3.196495532989502 4.849307537078857
Loss :  1.6632386445999146 2.8926169872283936 4.555855751037598
Loss :  1.640527606010437 3.291991710662842 4.932519435882568
Loss :  1.688236951828003 3.241647243499756 4.92988395690918
Loss :  1.6583763360977173 3.081780433654785 4.740156650543213
Loss :  1.6761881113052368 3.010298728942871 4.686486721038818
Loss :  1.6885236501693726 2.67607045173645 4.364593982696533
Loss :  1.6742899417877197 2.94193959236145 4.61622953414917
Loss :  1.6810321807861328 3.0853283405303955 4.766360282897949
Loss :  1.646231770515442 3.04231333732605 4.688545227050781
Loss :  1.6942907571792603 3.057281970977783 4.751572608947754
Loss :  1.6871899366378784 3.1341519355773926 4.8213419914245605
Loss :  1.7048239707946777 3.2704274654388428 4.975251197814941
Loss :  1.6822364330291748 2.849270820617676 4.53150749206543
  batch 60 loss: 1.6822364330291748, 2.849270820617676, 4.53150749206543
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6764072179794312 2.868945360183716 4.545352458953857
Loss :  1.6750917434692383 2.9379942417144775 4.613085746765137
Loss :  1.6833893060684204 3.1813721656799316 4.8647613525390625
Loss :  1.6674686670303345 3.106311321258545 4.77377986907959
Loss :  1.6629568338394165 2.255366325378418 3.918323040008545
Loss :  3.289149522781372 4.462489128112793 7.751638412475586
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  3.244755744934082 4.367544174194336 7.612299919128418
Loss :  3.288466215133667 4.302426815032959 7.590892791748047
Loss :  2.74487566947937 4.399317264556885 7.144192695617676
Total LOSS train 4.735476171053373 valid 7.524755954742432
CE LOSS train 1.6718574120448186 valid 0.6862189173698425
Contrastive LOSS train 3.063618777348445 valid 1.0998293161392212
EPOCH 86:
Loss :  1.6588902473449707 3.003570079803467 4.6624603271484375
Loss :  1.6702067852020264 3.017404079437256 4.687610626220703
Loss :  1.6553397178649902 2.8325414657592773 4.487881183624268
Loss :  1.6610490083694458 2.7674660682678223 4.4285149574279785
Loss :  1.687516212463379 3.1373021602630615 4.8248186111450195
Loss :  1.667150855064392 2.668142080307007 4.335292816162109
Loss :  1.6696034669876099 2.953338384628296 4.622941970825195
Loss :  1.659529209136963 2.2442548274993896 3.9037840366363525
Loss :  1.662170648574829 2.342886209487915 4.005056858062744
Loss :  1.6192389726638794 2.30353045463562 3.922769546508789
Loss :  1.6747967004776 2.834564447402954 4.509361267089844
Loss :  1.729853630065918 3.0345277786254883 4.764381408691406
Loss :  1.6760953664779663 2.873655080795288 4.549750328063965
Loss :  1.670518398284912 3.0786798000335693 4.749197959899902
Loss :  1.6538875102996826 2.575901746749878 4.2297892570495605
Loss :  1.658437967300415 2.9887266159057617 4.647164344787598
Loss :  1.663743019104004 3.188547372817993 4.852290153503418
Loss :  1.6670200824737549 2.815089225769043 4.482109069824219
Loss :  1.6680806875228882 2.7237837314605713 4.39186429977417
Loss :  1.63361656665802 2.8216826915740967 4.455299377441406
  batch 20 loss: 1.63361656665802, 2.8216826915740967, 4.455299377441406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6705436706542969 3.214115619659424 4.884659290313721
Loss :  1.679609775543213 3.655895471572876 5.335505485534668
Loss :  1.657267689704895 2.9229979515075684 4.580265522003174
Loss :  1.69196355342865 3.4761290550231934 5.168092727661133
Loss :  1.6912707090377808 3.5740630626678467 5.265333652496338
Loss :  1.6628406047821045 3.293496608734131 4.956336975097656
Loss :  1.7036900520324707 3.3669564723968506 5.070646286010742
Loss :  1.6468884944915771 3.419602870941162 5.06649112701416
Loss :  1.694152593612671 3.3526339530944824 5.046786308288574
Loss :  1.6506702899932861 3.6895699501037598 5.340240478515625
Loss :  1.728031873703003 3.6271004676818848 5.355132102966309
Loss :  1.674060344696045 3.2792134284973145 4.953273773193359
Loss :  1.6610546112060547 3.6933441162109375 5.354398727416992
Loss :  1.6660616397857666 3.4890291690826416 5.155090808868408
Loss :  1.7020918130874634 3.7465107440948486 5.448602676391602
Loss :  1.6920863389968872 3.48405122756958 5.176137447357178
Loss :  1.6742339134216309 3.5545496940612793 5.22878360748291
Loss :  1.6399199962615967 3.4869236946105957 5.126843452453613
Loss :  1.6674715280532837 3.507887601852417 5.17535924911499
Loss :  1.6611309051513672 3.000869035720825 4.661999702453613
  batch 40 loss: 1.6611309051513672, 3.000869035720825, 4.661999702453613
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6645461320877075 3.5110185146331787 5.175564765930176
Loss :  1.6535338163375854 2.8544790744781494 4.508012771606445
Loss :  1.6731290817260742 2.734772205352783 4.407901287078857
Loss :  1.6684058904647827 3.1103363037109375 4.77874231338501
Loss :  1.6710569858551025 2.59921932220459 4.270276069641113
Loss :  1.665848731994629 2.7915561199188232 4.457405090332031
Loss :  1.6526950597763062 2.796441078186035 4.449136257171631
Loss :  1.6625179052352905 2.6613502502441406 4.323868274688721
Loss :  1.6396461725234985 2.9007933139801025 4.540439605712891
Loss :  1.6872526407241821 2.631542205810547 4.3187947273254395
Loss :  1.6578253507614136 2.7630107402801514 4.420835971832275
Loss :  1.6749660968780518 3.032170295715332 4.707136154174805
Loss :  1.6885989904403687 2.628237724304199 4.316836833953857
Loss :  1.673019289970398 2.7493174076080322 4.422336578369141
Loss :  1.6817700862884521 2.6473162174224854 4.3290863037109375
Loss :  1.6462316513061523 2.532909393310547 4.179141044616699
Loss :  1.6939677000045776 2.9162347316741943 4.610202312469482
Loss :  1.6864171028137207 2.710937976837158 4.397355079650879
Loss :  1.7049790620803833 3.19647216796875 4.901451110839844
Loss :  1.682560682296753 2.3686256408691406 4.051186561584473
  batch 60 loss: 1.682560682296753, 2.3686256408691406, 4.051186561584473
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6765203475952148 2.6020023822784424 4.278522491455078
Loss :  1.6763136386871338 2.83095121383667 4.507265090942383
Loss :  1.681692361831665 2.8129584789276123 4.494650840759277
Loss :  1.667800784111023 2.4364356994628906 4.104236602783203
Loss :  1.6640323400497437 2.480804681777954 4.144836902618408
Loss :  3.833137273788452 4.416134357452393 8.249271392822266
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  3.7779269218444824 4.450225353240967 8.22815227508545
Loss :  3.843012571334839 4.283253192901611 8.126265525817871
Loss :  3.208498001098633 4.247195243835449 7.455693244934082
Total LOSS train 4.660885212971614 valid 8.014845609664917
CE LOSS train 1.6710325131049524 valid 0.8021245002746582
Contrastive LOSS train 2.989852732878465 valid 1.0617988109588623
EPOCH 87:
Loss :  1.6602970361709595 2.4989829063415527 4.159279823303223
Loss :  1.6690571308135986 2.991394519805908 4.660451889038086
Loss :  1.6565145254135132 2.5020039081573486 4.158518314361572
Loss :  1.6636408567428589 2.6686978340148926 4.332338809967041
Loss :  1.6877024173736572 2.929993152618408 4.6176958084106445
Loss :  1.6684298515319824 2.6124331951141357 4.280862808227539
Loss :  1.6684093475341797 2.812366247177124 4.480775833129883
Loss :  1.6585712432861328 3.0418076515197754 4.700378894805908
Loss :  1.664175271987915 2.5284016132354736 4.192576885223389
Loss :  1.618630290031433 2.393463611602783 4.012094020843506
Loss :  1.674559473991394 3.225395679473877 4.8999552726745605
Loss :  1.7319729328155518 3.835353374481201 5.567326545715332
Loss :  1.6761164665222168 3.3978540897369385 5.073970794677734
Loss :  1.669015884399414 3.318549871444702 4.987565994262695
Loss :  1.656421184539795 3.026153087615967 4.682574272155762
Loss :  1.6587551832199097 3.827758550643921 5.486513614654541
Loss :  1.663883924484253 3.4867348670959473 5.150618553161621
Loss :  1.6668940782546997 3.3296959400177 4.9965901374816895
Loss :  1.6718543767929077 3.071192979812622 4.74304723739624
Loss :  1.6328195333480835 3.1463348865509033 4.779154300689697
  batch 20 loss: 1.6328195333480835, 3.1463348865509033, 4.779154300689697
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6708049774169922 3.234173059463501 4.904977798461914
Loss :  1.6829993724822998 3.7043943405151367 5.387393951416016
Loss :  1.657741904258728 3.3994429111480713 5.05718469619751
Loss :  1.6955440044403076 3.3357739448547363 5.031317710876465
Loss :  1.695143222808838 3.201519012451172 4.89666223526001
Loss :  1.663733959197998 3.1344480514526367 4.798182010650635
Loss :  1.703345775604248 3.4974253177642822 5.200771331787109
Loss :  1.646894097328186 2.809892416000366 4.456786632537842
Loss :  1.689895749092102 2.8811144828796387 4.571010112762451
Loss :  1.652306079864502 3.3898086547851562 5.042114734649658
Loss :  1.7293769121170044 3.5664267539978027 5.295803546905518
Loss :  1.6740248203277588 3.297619342803955 4.971644401550293
Loss :  1.6612179279327393 3.4610595703125 5.12227725982666
Loss :  1.666267991065979 3.514028310775757 5.180296421051025
Loss :  1.7039695978164673 3.501495122909546 5.205464839935303
Loss :  1.6906341314315796 3.524352788925171 5.214986801147461
Loss :  1.6756172180175781 3.1651482582092285 4.840765476226807
Loss :  1.637144684791565 3.4295573234558105 5.066701889038086
Loss :  1.6699249744415283 3.235121726989746 4.905046463012695
Loss :  1.6631779670715332 2.8479106426239014 4.5110883712768555
  batch 40 loss: 1.6631779670715332, 2.8479106426239014, 4.5110883712768555
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6660985946655273 3.3040337562561035 4.970132350921631
Loss :  1.654378056526184 2.5286359786987305 4.183013916015625
Loss :  1.6760812997817993 2.820091962814331 4.49617338180542
Loss :  1.6688575744628906 2.7233152389526367 4.392172813415527
Loss :  1.6719379425048828 2.4423725605010986 4.114310264587402
Loss :  1.6653543710708618 2.8437509536743164 4.509105205535889
Loss :  1.6519492864608765 2.5593831539154053 4.211332321166992
Loss :  1.6638760566711426 2.464049816131592 4.127925872802734
Loss :  1.6388697624206543 3.0491788387298584 4.688048362731934
Loss :  1.6892906427383423 3.301234483718872 4.990525245666504
Loss :  1.6597434282302856 2.8863120079040527 4.546055316925049
Loss :  1.6763713359832764 2.6951353549957275 4.371506690979004
Loss :  1.6888659000396729 2.623967409133911 4.312833309173584
Loss :  1.6726871728897095 3.029114007949829 4.701801300048828
Loss :  1.6814974546432495 2.9104270935058594 4.591924667358398
Loss :  1.6463655233383179 2.8285019397735596 4.474867343902588
Loss :  1.6936880350112915 2.7533187866210938 4.447006702423096
Loss :  1.6869525909423828 2.7176904678344727 4.4046430587768555
Loss :  1.705142617225647 3.258615255355835 4.9637579917907715
Loss :  1.6824108362197876 3.060502052307129 4.742912769317627
  batch 60 loss: 1.6824108362197876, 3.060502052307129, 4.742912769317627
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.677292823791504 2.8391177654266357 4.516410827636719
Loss :  1.676867127418518 2.701003074645996 4.377870082855225
Loss :  1.683526873588562 3.2102978229522705 4.893824577331543
Loss :  1.6672614812850952 3.0114428997039795 4.678704261779785
Loss :  1.6632192134857178 2.1941757202148438 3.8573949337005615
Loss :  3.6209421157836914 4.443488597869873 8.064430236816406
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  3.5530216693878174 4.438847064971924 7.99186897277832
Loss :  3.5865731239318848 4.357395648956299 7.943968772888184
Loss :  3.002148389816284 4.244493007659912 7.246641159057617
Total LOSS train 4.710569539436928 valid 7.811727285385132
CE LOSS train 1.6716318827409011 valid 0.750537097454071
Contrastive LOSS train 3.0389376676999604 valid 1.061123251914978
EPOCH 88:
Loss :  1.6615111827850342 3.097958564758301 4.759469985961914
Loss :  1.6709948778152466 3.028437376022339 4.699432373046875
Loss :  1.6565425395965576 3.00966477394104 4.666207313537598
Loss :  1.6615831851959229 2.7255349159240723 4.387118339538574
Loss :  1.686341404914856 2.899545431137085 4.5858869552612305
Loss :  1.668111801147461 2.650916814804077 4.319028854370117
Loss :  1.6678603887557983 2.84867000579834 4.516530513763428
Loss :  1.658247470855713 2.6675150394439697 4.325762748718262
Loss :  1.6624521017074585 3.12101674079895 4.783468723297119
Loss :  1.6174590587615967 2.7602479457855225 4.377707004547119
Loss :  1.672971248626709 2.8120710849761963 4.485042572021484
Loss :  1.7304316759109497 3.205983877182007 4.936415672302246
Loss :  1.6749329566955566 3.0050761699676514 4.680008888244629
Loss :  1.6695518493652344 2.878516435623169 4.548068046569824
Loss :  1.6541496515274048 2.5901284217834473 4.2442779541015625
Loss :  1.6572827100753784 3.1294682025909424 4.786750793457031
Loss :  1.6641589403152466 2.66367769241333 4.327836513519287
Loss :  1.666244387626648 2.910433053970337 4.576677322387695
Loss :  1.6704139709472656 2.489870309829712 4.160284042358398
Loss :  1.632280707359314 3.1660892963409424 4.798369884490967
  batch 20 loss: 1.632280707359314, 3.1660892963409424, 4.798369884490967
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6689859628677368 2.8568108081817627 4.525796890258789
Loss :  1.680822730064392 2.9422430992126465 4.623065948486328
Loss :  1.6566658020019531 3.085043430328369 4.741709232330322
Loss :  1.6929339170455933 3.5741124153137207 5.2670464515686035
Loss :  1.6910091638565063 3.591655731201172 5.282664775848389
Loss :  1.6614315509796143 3.5350594520568848 5.196491241455078
Loss :  1.7023956775665283 3.3646838665008545 5.067079544067383
Loss :  1.6465805768966675 3.0286331176757812 4.675213813781738
Loss :  1.692155361175537 2.8452844619750977 4.537439823150635
Loss :  1.6500884294509888 3.3774237632751465 5.027512073516846
Loss :  1.7273637056350708 3.115729570388794 4.843093395233154
Loss :  1.6735317707061768 3.676945209503174 5.35047721862793
Loss :  1.6600879430770874 3.0871148109436035 4.7472028732299805
Loss :  1.6643527746200562 3.1638388633728027 4.828191757202148
Loss :  1.7009639739990234 3.2477991580963135 4.948762893676758
Loss :  1.6902934312820435 3.4275095462799072 5.11780309677124
Loss :  1.6733512878417969 2.909613847732544 4.582964897155762
Loss :  1.6385023593902588 3.0961568355560303 4.734659194946289
Loss :  1.6661800146102905 3.140862464904785 4.807042598724365
Loss :  1.6600162982940674 2.9131360054016113 4.573152542114258
  batch 40 loss: 1.6600162982940674, 2.9131360054016113, 4.573152542114258
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.66385817527771 2.8677520751953125 4.531610488891602
Loss :  1.65097177028656 3.0699448585510254 4.720916748046875
Loss :  1.6719768047332764 2.984081983566284 4.6560587882995605
Loss :  1.6649595499038696 3.397299289703369 5.062258720397949
Loss :  1.6683123111724854 2.775782346725464 4.444094657897949
Loss :  1.6640057563781738 3.2408995628356934 4.904905319213867
Loss :  1.6504181623458862 2.77740216255188 4.427820205688477
Loss :  1.6588751077651978 2.7682712078094482 4.4271464347839355
Loss :  1.639608383178711 2.3689184188842773 4.008526802062988
Loss :  1.684256672859192 2.5733988285064697 4.257655620574951
Loss :  1.6545764207839966 2.9918911457061768 4.646467685699463
Loss :  1.6734864711761475 2.746983528137207 4.420470237731934
Loss :  1.6872330904006958 2.5894503593444824 4.276683330535889
Loss :  1.6681848764419556 2.872299909591675 4.54048490524292
Loss :  1.6767467260360718 2.7661478519439697 4.442894458770752
Loss :  1.6443450450897217 2.755943536758423 4.4002885818481445
Loss :  1.6926645040512085 2.971038341522217 4.663702964782715
Loss :  1.6840509176254272 3.0352694988250732 4.719320297241211
Loss :  1.7025569677352905 3.1437642574310303 4.846321105957031
Loss :  1.6800742149353027 3.0397284030914307 4.7198028564453125
  batch 60 loss: 1.6800742149353027, 3.0397284030914307, 4.7198028564453125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6772618293762207 3.225560188293457 4.902822017669678
Loss :  1.6745173931121826 3.120222568511963 4.794739723205566
Loss :  1.6844741106033325 3.7102835178375244 5.3947577476501465
Loss :  1.6674753427505493 3.769341468811035 5.436816692352295
Loss :  1.662674069404602 3.243497133255005 4.9061713218688965
Loss :  3.6658103466033936 4.472040176391602 8.137850761413574
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  3.603794574737549 4.464857578277588 8.068652153015137
Loss :  3.6352059841156006 4.301163673400879 7.936369895935059
Loss :  3.0376813411712646 4.409000873565674 7.446681976318359
Total LOSS train 4.69222234579233 valid 7.897388696670532
CE LOSS train 1.6699815309964694 valid 0.7594203352928162
Contrastive LOSS train 3.022240785452036 valid 1.1022502183914185
EPOCH 89:
Loss :  1.6555371284484863 3.2869622707366943 4.942499160766602
Loss :  1.67292058467865 3.598345994949341 5.271266460418701
Loss :  1.6527234315872192 3.306980848312378 4.959704399108887
Loss :  1.655624270439148 3.5977590084075928 5.253383159637451
Loss :  1.6846929788589478 3.4258761405944824 5.110569000244141
Loss :  1.6652146577835083 3.102856397628784 4.768071174621582
Loss :  1.6693379878997803 3.738072156906128 5.407410144805908
Loss :  1.658940076828003 3.5591518878936768 5.21809196472168
Loss :  1.6610939502716064 3.412231683731079 5.0733256340026855
Loss :  1.617606520652771 3.4753963947296143 5.093002796173096
Loss :  1.6743804216384888 3.4003384113311768 5.074718952178955
Loss :  1.730027437210083 3.4273529052734375 5.157380104064941
Loss :  1.6761082410812378 3.4067280292510986 5.082836151123047
Loss :  1.6707987785339355 3.543607234954834 5.2144060134887695
Loss :  1.6520495414733887 3.0926716327667236 4.744721412658691
Loss :  1.6585922241210938 3.002546787261963 4.661139011383057
Loss :  1.6651203632354736 2.968224287033081 4.633344650268555
Loss :  1.6672539710998535 2.675863742828369 4.343117713928223
Loss :  1.6678290367126465 2.502307653427124 4.170136451721191
Loss :  1.6340227127075195 2.7252399921417236 4.359262466430664
  batch 20 loss: 1.6340227127075195, 2.7252399921417236, 4.359262466430664
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6694966554641724 2.5472285747528076 4.2167253494262695
Loss :  1.6804709434509277 2.8080601692199707 4.488531112670898
Loss :  1.65791916847229 2.452501058578491 4.110420227050781
Loss :  1.6921738386154175 2.7868192195892334 4.478992938995361
Loss :  1.6916264295578003 3.085803747177124 4.777430057525635
Loss :  1.6624290943145752 2.8029062747955322 4.465335369110107
Loss :  1.7041906118392944 2.7784037590026855 4.4825944900512695
Loss :  1.6479456424713135 2.4694931507110596 4.117438793182373
Loss :  1.6936558485031128 2.662421464920044 4.356077194213867
Loss :  1.652152180671692 2.8674674034118652 4.519619464874268
Loss :  1.7296113967895508 3.0361487865448 4.76576042175293
Loss :  1.674648404121399 2.7778990268707275 4.452547550201416
Loss :  1.660680890083313 3.0931360721588135 4.753817081451416
Loss :  1.6689358949661255 2.9576528072357178 4.626588821411133
Loss :  1.7039940357208252 3.1359243392944336 4.83991813659668
Loss :  1.6925793886184692 2.9003801345825195 4.592959403991699
Loss :  1.6755033731460571 3.106459856033325 4.781963348388672
Loss :  1.6390008926391602 2.9687001705169678 4.607701301574707
Loss :  1.6673662662506104 3.151207685470581 4.818573951721191
Loss :  1.6633093357086182 2.8993980884552 4.562707424163818
  batch 40 loss: 1.6633093357086182, 2.8993980884552, 4.562707424163818
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6651078462600708 3.029754161834717 4.694861888885498
Loss :  1.6539386510849 2.5194144248962402 4.17335319519043
Loss :  1.6753129959106445 2.6363115310668945 4.311624526977539
Loss :  1.6688629388809204 2.6172773838043213 4.286140441894531
Loss :  1.6715587377548218 2.4006032943725586 4.07216215133667
Loss :  1.6660057306289673 2.922360897064209 4.588366508483887
Loss :  1.651696801185608 2.379852533340454 4.031549453735352
Loss :  1.6631436347961426 3.046696901321411 4.709840774536133
Loss :  1.6385927200317383 3.5246989727020264 5.163291931152344
Loss :  1.6898934841156006 3.07572603225708 4.765619277954102
Loss :  1.6605371236801147 2.9442858695983887 4.604823112487793
Loss :  1.675735354423523 2.778088092803955 4.453823566436768
Loss :  1.6878011226654053 3.258190631866455 4.945991516113281
Loss :  1.675874948501587 3.3723251819610596 5.0482001304626465
Loss :  1.6821749210357666 3.433821201324463 5.115996360778809
Loss :  1.6460157632827759 2.77996826171875 4.425983905792236
Loss :  1.6942108869552612 3.2943875789642334 4.988598346710205
Loss :  1.6885385513305664 3.3143908977508545 5.0029296875
Loss :  1.7043507099151611 3.4670145511627197 5.171365261077881
Loss :  1.6812790632247925 2.9503073692321777 4.63158655166626
  batch 60 loss: 1.6812790632247925, 2.9503073692321777, 4.63158655166626
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6761798858642578 3.1980843544006348 4.874264240264893
Loss :  1.676102876663208 3.4431581497192383 5.119260787963867
Loss :  1.6820347309112549 3.1931676864624023 4.875202178955078
Loss :  1.6663037538528442 2.983753204345703 4.650056838989258
Loss :  1.6632680892944336 2.4901609420776367 4.15342903137207
Loss :  3.7861766815185547 4.459588050842285 8.24576473236084
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  3.7291324138641357 4.3655219078063965 8.094654083251953
Loss :  3.7875890731811523 4.2799458503723145 8.067535400390625
Loss :  3.1870176792144775 4.329809188842773 7.516826629638672
Total LOSS train 4.710960168104905 valid 7.9811952114105225
CE LOSS train 1.6711090142910296 valid 0.7967544198036194
Contrastive LOSS train 3.039851159315843 valid 1.0824522972106934
EPOCH 90:
Loss :  1.6590626239776611 2.805330753326416 4.464393615722656
Loss :  1.6687644720077515 3.107322931289673 4.776087284088135
Loss :  1.6568822860717773 2.5873992443084717 4.244281768798828
Loss :  1.6627492904663086 3.2284672260284424 4.891216278076172
Loss :  1.6866039037704468 3.340106725692749 5.026710510253906
Loss :  1.668558955192566 2.9902548789978027 4.658813953399658
Loss :  1.6677290201187134 3.590975522994995 5.258704662322998
Loss :  1.6579276323318481 2.9800689220428467 4.637996673583984
Loss :  1.6616935729980469 2.996889352798462 4.65858268737793
Loss :  1.6169055700302124 2.6249775886535645 4.241883277893066
Loss :  1.675073266029358 3.065476179122925 4.740549564361572
Loss :  1.7315388917922974 3.5226337909698486 5.2541728019714355
Loss :  1.6756199598312378 3.262465715408325 4.938085556030273
Loss :  1.6691120862960815 3.5167229175567627 5.185834884643555
Loss :  1.6532713174819946 2.9891936779022217 4.642465114593506
Loss :  1.6571710109710693 3.0794055461883545 4.736576557159424
Loss :  1.6638234853744507 3.0666329860687256 4.730456352233887
Loss :  1.666185975074768 2.929334878921509 4.595520973205566
Loss :  1.6695539951324463 2.843552350997925 4.513106346130371
Loss :  1.6321494579315186 2.76590895652771 4.3980584144592285
  batch 20 loss: 1.6321494579315186, 2.76590895652771, 4.3980584144592285
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6686056852340698 2.975555896759033 4.644161701202393
Loss :  1.6811244487762451 3.2980704307556152 4.979194641113281
Loss :  1.6558078527450562 3.0622775554656982 4.718085289001465
Loss :  1.6939866542816162 3.0824105739593506 4.776397228240967
Loss :  1.6903302669525146 3.142580509185791 4.832910537719727
Loss :  1.6595394611358643 3.502779245376587 5.162318706512451
Loss :  1.702853798866272 3.1142287254333496 4.817082405090332
Loss :  1.6471288204193115 2.5618889331817627 4.209017753601074
Loss :  1.693008542060852 2.609114646911621 4.302123069763184
Loss :  1.6507689952850342 2.9492034912109375 4.599972724914551
Loss :  1.7275364398956299 2.8526062965393066 4.580142974853516
Loss :  1.674113154411316 3.0708022117614746 4.74491548538208
Loss :  1.6608327627182007 2.5983693599700928 4.259202003479004
Loss :  1.6657415628433228 2.6442108154296875 4.309952259063721
Loss :  1.7017139196395874 2.9987833499908447 4.700497150421143
Loss :  1.6920686960220337 2.9751534461975098 4.667222023010254
Loss :  1.6741199493408203 2.5858774185180664 4.259997367858887
Loss :  1.6400121450424194 2.581683874130249 4.221695899963379
Loss :  1.6671411991119385 2.901008367538452 4.568149566650391
Loss :  1.6608911752700806 2.76396107673645 4.42485237121582
  batch 40 loss: 1.6608911752700806, 2.76396107673645, 4.42485237121582
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6652296781539917 2.9456725120544434 4.610902309417725
Loss :  1.6535489559173584 2.3891959190368652 4.0427446365356445
Loss :  1.672442078590393 2.7105329036712646 4.382975101470947
Loss :  1.668108344078064 2.682384729385376 4.35049295425415
Loss :  1.6703981161117554 2.391777276992798 4.062175273895264
Loss :  1.666115403175354 2.6481776237487793 4.314292907714844
Loss :  1.6531323194503784 2.845734119415283 4.498866558074951
Loss :  1.660639762878418 2.6949617862701416 4.3556013107299805
Loss :  1.6400182247161865 2.7309317588806152 4.370949745178223
Loss :  1.684835433959961 2.790006399154663 4.474842071533203
Loss :  1.6544463634490967 3.144245147705078 4.798691749572754
Loss :  1.6724456548690796 2.8812942504882812 4.55374002456665
Loss :  1.689034104347229 3.065004587173462 4.7540388107299805
Loss :  1.669012427330017 3.232820749282837 4.9018330574035645
Loss :  1.6773079633712769 3.590231418609619 5.2675395011901855
Loss :  1.6456115245819092 2.9854543209075928 4.631065845489502
Loss :  1.692482352256775 3.3183372020721436 5.010819435119629
Loss :  1.6828856468200684 2.9373652935028076 4.620250701904297
Loss :  1.7019354104995728 3.3691062927246094 5.071041584014893
Loss :  1.6807990074157715 3.040539026260376 4.721338272094727
  batch 60 loss: 1.6807990074157715, 3.040539026260376, 4.721338272094727
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6779515743255615 3.219033718109131 4.896985054016113
Loss :  1.6743353605270386 2.8974618911743164 4.5717973709106445
Loss :  1.6847865581512451 2.643428325653076 4.328214645385742
Loss :  1.6661522388458252 2.750352382659912 4.416504859924316
Loss :  1.6614608764648438 2.403876543045044 4.065337181091309
Loss :  3.88565993309021 4.520080089569092 8.405739784240723
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  3.823366641998291 4.352182388305664 8.175548553466797
Loss :  3.8805415630340576 4.35864782333374 8.239189147949219
Loss :  3.243847131729126 4.322324752807617 7.566171646118164
Total LOSS train 4.622221990732046 valid 8.096662282943726
CE LOSS train 1.6703510412803062 valid 0.8109617829322815
Contrastive LOSS train 2.9518709622896635 valid 1.0805811882019043
EPOCH 91:
Loss :  1.6572439670562744 3.011357069015503 4.668601036071777
Loss :  1.6730862855911255 3.218018054962158 4.891104221343994
Loss :  1.6557940244674683 3.0752882957458496 4.731082439422607
Loss :  1.6591347455978394 2.936173915863037 4.595308780670166
Loss :  1.6852186918258667 3.0479352474212646 4.733153820037842
Loss :  1.6668287515640259 2.662914752960205 4.329743385314941
Loss :  1.6690927743911743 2.7630512714385986 4.4321441650390625
Loss :  1.6595474481582642 2.0680298805236816 3.7275772094726562
Loss :  1.6610946655273438 2.535860538482666 4.19695520401001
Loss :  1.6176822185516357 2.470433235168457 4.088115692138672
Loss :  1.6740156412124634 2.7190017700195312 4.393017292022705
Loss :  1.7290873527526855 2.899543523788452 4.628630638122559
Loss :  1.6753368377685547 2.7280399799346924 4.403376579284668
Loss :  1.6711647510528564 2.8709592819213867 4.542123794555664
Loss :  1.6514780521392822 2.6342756748199463 4.2857537269592285
Loss :  1.6560825109481812 2.7584521770477295 4.414534568786621
Loss :  1.6643640995025635 2.7358741760253906 4.400238037109375
Loss :  1.6653833389282227 2.5124690532684326 4.177852630615234
Loss :  1.6665054559707642 2.4956557750701904 4.162161350250244
Loss :  1.631996750831604 2.7122340202331543 4.344230651855469
  batch 20 loss: 1.631996750831604, 2.7122340202331543, 4.344230651855469
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.668081283569336 3.2388217449188232 4.906903266906738
Loss :  1.6794897317886353 2.822230100631714 4.501719951629639
Loss :  1.6554219722747803 3.0089240074157715 4.664345741271973
Loss :  1.6906862258911133 3.0934460163116455 4.78413200378418
Loss :  1.6875940561294556 3.629340410232544 5.316934585571289
Loss :  1.65940260887146 2.988736391067505 4.648138999938965
Loss :  1.701968789100647 3.5249664783477783 5.226935386657715
Loss :  1.6472147703170776 3.227442979812622 4.87465763092041
Loss :  1.6940174102783203 3.3411409854888916 5.035158157348633
Loss :  1.6478525400161743 3.3603708744049072 5.008223533630371
Loss :  1.7261004447937012 3.6383471488952637 5.364447593688965
Loss :  1.6721829175949097 3.6231422424316406 5.29532527923584
Loss :  1.6592111587524414 3.1370186805725098 4.796229839324951
Loss :  1.664333462715149 3.5004959106445312 5.164829254150391
Loss :  1.698498010635376 3.622809410095215 5.321307182312012
Loss :  1.6915459632873535 3.64449143409729 5.336037635803223
Loss :  1.6722158193588257 3.361168384552002 5.033384323120117
Loss :  1.636971354484558 3.071232318878174 4.7082037925720215
Loss :  1.665016770362854 3.1290857791900635 4.794102668762207
Loss :  1.6594955921173096 3.014855146408081 4.674350738525391
  batch 40 loss: 1.6594955921173096, 3.014855146408081, 4.674350738525391
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6638704538345337 3.6961538791656494 5.360024452209473
Loss :  1.6518445014953613 3.0930211544036865 4.744865417480469
Loss :  1.6710318326950073 3.4672162532806396 5.138247966766357
Loss :  1.6680619716644287 3.22493314743042 4.8929948806762695
Loss :  1.6692450046539307 2.6886065006256104 4.357851505279541
Loss :  1.6648294925689697 3.1122350692749023 4.777064323425293
Loss :  1.652805209159851 3.2478086948394775 4.900613784790039
Loss :  1.6611443758010864 2.893662929534912 4.554807186126709
Loss :  1.6374515295028687 2.919739246368408 4.557190895080566
Loss :  1.6858272552490234 2.582918643951416 4.2687458992004395
Loss :  1.654672622680664 2.72922682762146 4.383899688720703
Loss :  1.6712522506713867 2.7824084758758545 4.45366096496582
Loss :  1.688773274421692 2.5272819995880127 4.216055393218994
Loss :  1.6727374792099 2.7100820541381836 4.382819652557373
Loss :  1.6793229579925537 2.836083173751831 4.515406131744385
Loss :  1.644571304321289 2.949897527694702 4.59446907043457
Loss :  1.693253517150879 2.649919033050537 4.343172550201416
Loss :  1.6863043308258057 2.874203681945801 4.560507774353027
Loss :  1.7028560638427734 2.9609076976776123 4.663763999938965
Loss :  1.6802092790603638 2.7482082843780518 4.428417682647705
  batch 60 loss: 1.6802092790603638, 2.7482082843780518, 4.428417682647705
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6769802570343018 2.8323886394500732 4.509368896484375
Loss :  1.6758091449737549 2.6165168285369873 4.292325973510742
Loss :  1.682693362236023 2.7511250972747803 4.433818340301514
Loss :  1.6662741899490356 2.943446397781372 4.609720706939697
Loss :  1.6637784242630005 2.2318928241729736 3.8956713676452637
Loss :  4.3304595947265625 4.426751613616943 8.757211685180664
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  4.261179447174072 4.415582180023193 8.676761627197266
Loss :  4.334473609924316 4.23734712600708 8.571821212768555
Loss :  3.639890670776367 4.220298767089844 7.860189437866211
Total LOSS train 4.637485496814435 valid 8.466495990753174
CE LOSS train 1.6697390666374794 valid 0.9099726676940918
Contrastive LOSS train 2.9677464338449333 valid 1.055074691772461
EPOCH 92:
Loss :  1.6567078828811646 2.868138313293457 4.524846076965332
Loss :  1.6702171564102173 3.263990640640259 4.934207916259766
Loss :  1.6571847200393677 3.348484754562378 5.005669593811035
Loss :  1.6621754169464111 3.084545135498047 4.746720314025879
Loss :  1.686991572380066 3.1598129272460938 4.846804618835449
Loss :  1.6680619716644287 2.8086938858032227 4.4767560958862305
Loss :  1.6681568622589111 2.7417588233947754 4.409915924072266
Loss :  1.6579533815383911 2.4981417655944824 4.156095027923584
Loss :  1.6621204614639282 2.643136739730835 4.305257320404053
Loss :  1.6170374155044556 2.5521559715270996 4.169193267822266
Loss :  1.6743625402450562 3.1416943073272705 4.816056728363037
Loss :  1.73250150680542 3.3535690307617188 5.086070537567139
Loss :  1.6755660772323608 3.471266269683838 5.146832466125488
Loss :  1.6681723594665527 3.0868306159973145 4.755002975463867
Loss :  1.6536903381347656 3.2567875385284424 4.910477638244629
Loss :  1.65744948387146 2.778671979904175 4.436121463775635
Loss :  1.6634330749511719 2.885972023010254 4.549405097961426
Loss :  1.666610836982727 2.777965784072876 4.444576740264893
Loss :  1.6711488962173462 2.5960958003997803 4.267244815826416
Loss :  1.6324702501296997 3.19193696975708 4.82440710067749
  batch 20 loss: 1.6324702501296997, 3.19193696975708, 4.82440710067749
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6691195964813232 3.191300630569458 4.860420227050781
Loss :  1.681200623512268 2.8664650917053223 4.547665596008301
Loss :  1.6579384803771973 2.8041398525238037 4.462078094482422
Loss :  1.6941516399383545 2.7736589908599854 4.46781063079834
Loss :  1.69322669506073 3.367985486984253 5.061212062835693
Loss :  1.6619515419006348 2.883416175842285 4.54536771774292
Loss :  1.7023875713348389 3.2542335987091064 4.956621170043945
Loss :  1.6461272239685059 3.106015920639038 4.752142906188965
Loss :  1.6901843547821045 2.7195024490356445 4.409687042236328
Loss :  1.6517610549926758 3.1237800121307373 4.775541305541992
Loss :  1.7287700176239014 3.207104444503784 4.9358744621276855
Loss :  1.6737065315246582 3.0311288833618164 4.704835414886475
Loss :  1.6605526208877563 2.7502548694610596 4.4108076095581055
Loss :  1.6666064262390137 2.6009938716888428 4.267600059509277
Loss :  1.7016496658325195 2.853384256362915 4.5550336837768555
Loss :  1.6913813352584839 2.5646955966949463 4.256076812744141
Loss :  1.6743005514144897 2.6046645641326904 4.278964996337891
Loss :  1.6379225254058838 2.4979188442230225 4.135841369628906
Loss :  1.6670246124267578 2.3885498046875 4.055574417114258
Loss :  1.6620233058929443 2.285836935043335 3.9478602409362793
  batch 40 loss: 1.6620233058929443, 2.285836935043335, 3.9478602409362793
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6644872426986694 2.9293015003204346 4.5937886238098145
Loss :  1.6531840562820435 2.832000255584717 4.485184192657471
Loss :  1.6739643812179565 3.0349841117858887 4.708948612213135
Loss :  1.6678287982940674 2.6996848583221436 4.367513656616211
Loss :  1.6706656217575073 2.39052152633667 4.061187267303467
Loss :  1.664839267730713 2.6988518238067627 4.363691329956055
Loss :  1.6512641906738281 2.6042251586914062 4.255489349365234
Loss :  1.6612865924835205 2.6043295860290527 4.265616416931152
Loss :  1.639227271080017 3.1596362590789795 4.798863410949707
Loss :  1.6864001750946045 2.5112507343292236 4.197650909423828
Loss :  1.6563091278076172 2.608515739440918 4.264824867248535
Loss :  1.673709750175476 2.9750030040740967 4.648712635040283
Loss :  1.6881935596466064 2.7001149654388428 4.388308525085449
Loss :  1.6703007221221924 2.909882068634033 4.580183029174805
Loss :  1.6784011125564575 2.9508509635925293 4.629251956939697
Loss :  1.6449549198150635 2.8492136001586914 4.494168281555176
Loss :  1.691572666168213 2.79626727104187 4.487839698791504
Loss :  1.682952880859375 2.971440315246582 4.654393196105957
Loss :  1.7021772861480713 3.1080613136291504 4.810238838195801
Loss :  1.6807197332382202 2.786904811859131 4.467624664306641
  batch 60 loss: 1.6807197332382202, 2.786904811859131, 4.467624664306641
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.67776358127594 3.0567688941955566 4.734532356262207
Loss :  1.6746697425842285 3.100451707839966 4.775121688842773
Loss :  1.682782769203186 2.9917471408843994 4.674530029296875
Loss :  1.6658729314804077 2.9595468044281006 4.625419616699219
Loss :  1.661143183708191 3.115962028503418 4.777105331420898
Loss :  3.940673589706421 4.4174909591674805 8.35816478729248
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  3.8722243309020996 4.444558620452881 8.31678295135498
Loss :  3.9467012882232666 4.233240604400635 8.17994213104248
Loss :  3.288877248764038 4.2555341720581055 7.544411659240723
Total LOSS train 4.558597938831036 valid 8.099825382232666
CE LOSS train 1.6704410791397095 valid 0.8222193121910095
Contrastive LOSS train 2.8881568615253155 valid 1.0638835430145264
EPOCH 93:
Loss :  1.6564478874206543 2.908693552017212 4.565141677856445
Loss :  1.6711441278457642 3.6804606914520264 5.35160493850708
Loss :  1.6551623344421387 3.1129510402679443 4.768113136291504
Loss :  1.6595911979675293 3.1957476139068604 4.855339050292969
Loss :  1.6857850551605225 3.640434980392456 5.3262200355529785
Loss :  1.667330265045166 3.21760892868042 4.884939193725586
Loss :  1.6676632165908813 3.2647855281829834 4.932448863983154
Loss :  1.6582907438278198 2.6741151809692383 4.332406044006348
Loss :  1.6614649295806885 2.9921231269836426 4.65358829498291
Loss :  1.615830421447754 2.5543484687805176 4.1701788902282715
Loss :  1.6741061210632324 2.943250894546509 4.61735725402832
Loss :  1.7313072681427002 3.596627712249756 5.327935218811035
Loss :  1.6753686666488647 3.1426608562469482 4.818029403686523
Loss :  1.6697840690612793 3.550595998764038 5.220379829406738
Loss :  1.6516245603561401 2.8782544136047363 4.529879093170166
Loss :  1.6568206548690796 3.010085344314575 4.666905879974365
Loss :  1.6644610166549683 2.88881778717041 4.553278923034668
Loss :  1.6664808988571167 2.922929286956787 4.589410305023193
Loss :  1.668436050415039 2.3480072021484375 4.016443252563477
Loss :  1.632886290550232 2.7311036586761475 4.36398983001709
  batch 20 loss: 1.632886290550232, 2.7311036586761475, 4.36398983001709
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6684274673461914 3.2078781127929688 4.87630558013916
Loss :  1.6806402206420898 2.893017292022705 4.573657512664795
Loss :  1.6577458381652832 2.683790683746338 4.341536521911621
Loss :  1.6917470693588257 3.419631242752075 5.111378192901611
Loss :  1.6903055906295776 3.098719358444214 4.789024829864502
Loss :  1.6615941524505615 3.1863009929656982 4.84789514541626
Loss :  1.7021888494491577 3.060100555419922 4.762289524078369
Loss :  1.6472853422164917 3.0207390785217285 4.66802453994751
Loss :  1.692175269126892 2.9562199115753174 4.64839506149292
Loss :  1.6497108936309814 3.0913023948669434 4.741013526916504
Loss :  1.7279987335205078 2.9083712100982666 4.636369705200195
Loss :  1.6732043027877808 3.338075876235962 5.011280059814453
Loss :  1.6602944135665894 2.8931949138641357 4.5534892082214355
Loss :  1.6664378643035889 2.854172945022583 4.520610809326172
Loss :  1.7017232179641724 3.0629563331604004 4.764679431915283
Loss :  1.6914448738098145 2.8057782649993896 4.497222900390625
Loss :  1.67391836643219 3.0605392456054688 4.734457492828369
Loss :  1.6362665891647339 2.6790668964385986 4.315333366394043
Loss :  1.666239857673645 3.0452842712402344 4.71152400970459
Loss :  1.6610455513000488 3.361490488052368 5.022536277770996
  batch 40 loss: 1.6610455513000488, 3.361490488052368, 5.022536277770996
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.664604663848877 3.612788677215576 5.277393341064453
Loss :  1.6515331268310547 3.1502838134765625 4.801816940307617
Loss :  1.6738098859786987 3.188180446624756 4.861990451812744
Loss :  1.667185664176941 3.063859462738037 4.731045246124268
Loss :  1.6696382761001587 3.2609894275665283 4.930627822875977
Loss :  1.662718415260315 3.692946434020996 5.3556647300720215
Loss :  1.6495115756988525 3.4636781215667725 5.113189697265625
Loss :  1.6607298851013184 3.499256134033203 5.1599860191345215
Loss :  1.6363109350204468 3.847956657409668 5.484267711639404
Loss :  1.6865332126617432 3.2263405323028564 4.9128737449646
Loss :  1.6565645933151245 3.4515774250030518 5.108141899108887
Loss :  1.672836184501648 3.285672903060913 4.9585089683532715
Loss :  1.6877294778823853 3.358304023742676 5.0460333824157715
Loss :  1.6725975275039673 3.0755972862243652 4.748194694519043
Loss :  1.679956078529358 3.489306926727295 5.169262886047363
Loss :  1.6451117992401123 3.29551100730896 4.940622806549072
Loss :  1.6926215887069702 3.178934335708618 4.871555805206299
Loss :  1.6864475011825562 3.2531933784484863 4.939640998840332
Loss :  1.7024743556976318 3.3822333812713623 5.084707736968994
Loss :  1.680179238319397 3.713062047958374 5.3932414054870605
  batch 60 loss: 1.680179238319397, 3.713062047958374, 5.3932414054870605
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6769870519638062 3.2881863117218018 4.965173244476318
Loss :  1.6773313283920288 3.020627498626709 4.697958946228027
Loss :  1.6832876205444336 3.300441026687622 4.983728408813477
Loss :  1.6651275157928467 3.2380239963531494 4.903151512145996
Loss :  1.6621986627578735 2.573625087738037 4.235823631286621
Loss :  4.321563720703125 4.396972179412842 8.718536376953125
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  4.251767635345459 4.435394287109375 8.687162399291992
Loss :  4.32948112487793 4.1601243019104 8.489604949951172
Loss :  3.613748550415039 4.204835891723633 7.818584442138672
Total LOSS train 4.820695612980769 valid 8.42847204208374
CE LOSS train 1.6700677908383883 valid 0.9034371376037598
Contrastive LOSS train 3.1506278258103593 valid 1.0512089729309082
EPOCH 94:
Loss :  1.6598130464553833 3.3070068359375 4.966819763183594
Loss :  1.6690657138824463 3.3901610374450684 5.059226989746094
Loss :  1.657393455505371 3.052022695541382 4.709416389465332
Loss :  1.6636180877685547 3.1648411750793457 4.8284592628479
Loss :  1.6871166229248047 3.460345983505249 5.147462844848633
Loss :  1.6703500747680664 3.1125595569610596 4.782909393310547
Loss :  1.6665793657302856 3.0287857055664062 4.695364952087402
Loss :  1.6578630208969116 2.875253200531006 4.533116340637207
Loss :  1.6629691123962402 2.5353951454162598 4.1983642578125
Loss :  1.6155169010162354 2.849482536315918 4.464999198913574
Loss :  1.6747299432754517 3.1556711196899414 4.8304009437561035
Loss :  1.733272671699524 3.24640154838562 4.979674339294434
Loss :  1.6755199432373047 3.1530160903930664 4.828536033630371
Loss :  1.6675254106521606 3.611412525177002 5.278937816619873
Loss :  1.6538156270980835 2.7716355323791504 4.425451278686523
Loss :  1.657147765159607 3.166774272918701 4.823922157287598
Loss :  1.6631762981414795 3.5398380756378174 5.203014373779297
Loss :  1.665970802307129 3.7266464233398438 5.392617225646973
Loss :  1.6701769828796387 3.2591805458068848 4.929357528686523
Loss :  1.6316155195236206 3.3857662677764893 5.01738166809082
  batch 20 loss: 1.6316155195236206, 3.3857662677764893, 5.01738166809082
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6679331064224243 3.54063081741333 5.208563804626465
Loss :  1.6810262203216553 2.6679370403289795 4.348963260650635
Loss :  1.655351161956787 2.742284059524536 4.397635459899902
Loss :  1.6950926780700684 2.8068041801452637 4.501896858215332
Loss :  1.6921807527542114 3.177215337753296 4.869396209716797
Loss :  1.6598711013793945 2.516998052597046 4.1768693923950195
Loss :  1.7022827863693237 2.7115843296051025 4.413866996765137
Loss :  1.6460050344467163 2.8408315181732178 4.4868364334106445
Loss :  1.690487027168274 2.6667068004608154 4.357193946838379
Loss :  1.6495829820632935 2.9375033378601074 4.587086200714111
Loss :  1.7268387079238892 2.9210047721862793 4.647843360900879
Loss :  1.6740541458129883 2.9042441844940186 4.578298568725586
Loss :  1.6605525016784668 2.562251329421997 4.222804069519043
Loss :  1.664565086364746 2.7187299728393555 4.383295059204102
Loss :  1.7014572620391846 2.9132490158081055 4.614706039428711
Loss :  1.6913219690322876 2.663942575454712 4.355264663696289
Loss :  1.6737979650497437 2.6359200477600098 4.309718132019043
Loss :  1.6392470598220825 2.918227434158325 4.557474613189697
Loss :  1.666316032409668 2.7548091411590576 4.421125411987305
Loss :  1.6608375310897827 2.6593096256256104 4.3201470375061035
  batch 40 loss: 1.6608375310897827, 2.6593096256256104, 4.3201470375061035
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6640948057174683 2.954909324645996 4.619004249572754
Loss :  1.6520273685455322 2.626600503921509 4.278627872467041
Loss :  1.6714283227920532 2.8182713985443115 4.489699840545654
Loss :  1.6675243377685547 2.651468515396118 4.318992614746094
Loss :  1.6690186262130737 2.428271770477295 4.097290515899658
Loss :  1.664658546447754 3.014395236968994 4.679053783416748
Loss :  1.6522407531738281 2.7274327278137207 4.379673480987549
Loss :  1.6584999561309814 2.7547223567962646 4.413222312927246
Loss :  1.6396965980529785 3.1188414096832275 4.758538246154785
Loss :  1.6820480823516846 2.5405304431915283 4.222578525543213
Loss :  1.652824878692627 2.7145538330078125 4.3673787117004395
Loss :  1.6709338426589966 2.7531750202178955 4.424108982086182
Loss :  1.6898744106292725 2.73294997215271 4.422824382781982
Loss :  1.6667156219482422 2.8916947841644287 4.55841064453125
Loss :  1.6764416694641113 3.0866739749908447 4.763115882873535
Loss :  1.6450865268707275 2.7633001804351807 4.408386707305908
Loss :  1.691496729850769 2.9063806533813477 4.597877502441406
Loss :  1.683632493019104 3.132890462875366 4.81652307510376
Loss :  1.7015990018844604 3.499756336212158 5.201355457305908
Loss :  1.6790567636489868 3.3770883083343506 5.056145191192627
  batch 60 loss: 1.6790567636489868, 3.3770883083343506, 5.056145191192627
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6788674592971802 3.644465684890747 5.323333263397217
Loss :  1.6745355129241943 2.919691324234009 4.594226837158203
Loss :  1.6851955652236938 3.3865084648132324 5.071703910827637
Loss :  1.6663892269134521 3.487938404083252 5.154327392578125
Loss :  1.660996675491333 2.8443703651428223 4.505367279052734
Loss :  4.034284591674805 4.421905994415283 8.45619010925293
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  3.938553810119629 4.386294364929199 8.324848175048828
Loss :  4.047154426574707 4.2923455238342285 8.339500427246094
Loss :  3.361205816268921 4.34306526184082 7.70427131652832
Total LOSS train 4.651941306774432 valid 8.206202507019043
CE LOSS train 1.6699526346646822 valid 0.8403014540672302
Contrastive LOSS train 2.9819886354299694 valid 1.085766315460205
EPOCH 95:
Loss :  1.6562697887420654 3.164588689804077 4.820858478546143
Loss :  1.6739554405212402 2.9780893325805664 4.652044773101807
Loss :  1.6559064388275146 2.7940468788146973 4.449953079223633
Loss :  1.6566904783248901 3.3341352939605713 4.990825653076172
Loss :  1.6854132413864136 3.2582993507385254 4.9437127113342285
Loss :  1.66708242893219 2.802037000656128 4.469119548797607
Loss :  1.669272541999817 3.0794882774353027 4.74876070022583
Loss :  1.6589462757110596 2.8209965229034424 4.479942798614502
Loss :  1.6601788997650146 2.8559257984161377 4.516104698181152
Loss :  1.6177983283996582 3.034363031387329 4.652161598205566
Loss :  1.675525426864624 3.1571435928344727 4.832669258117676
Loss :  1.7281677722930908 3.661442279815674 5.389610290527344
Loss :  1.6772159337997437 2.9508655071258545 4.628081321716309
Loss :  1.671635389328003 3.261324882507324 4.932960510253906
Loss :  1.6483529806137085 3.4584872722625732 5.106840133666992
Loss :  1.6579370498657227 3.5522639751434326 5.210201263427734
Loss :  1.6651979684829712 3.407193899154663 5.072391986846924
Loss :  1.667031168937683 3.3891632556915283 5.056194305419922
Loss :  1.6666293144226074 2.6099953651428223 4.27662467956543
Loss :  1.632232427597046 2.5382802486419678 4.170512676239014
  batch 20 loss: 1.632232427597046, 2.5382802486419678, 4.170512676239014
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6680024862289429 3.271631956100464 4.939634323120117
Loss :  1.6793828010559082 2.9981226921081543 4.6775054931640625
Loss :  1.656450629234314 2.716726541519165 4.3731770515441895
Loss :  1.6909735202789307 2.8030145168304443 4.493988037109375
Loss :  1.6881791353225708 3.5227646827697754 5.210943698883057
Loss :  1.661454677581787 2.5057005882263184 4.1671552658081055
Loss :  1.70350980758667 2.7957236766815186 4.499233245849609
Loss :  1.6489193439483643 2.85133695602417 4.500256538391113
Loss :  1.694522500038147 2.937058448791504 4.631580829620361
Loss :  1.648144006729126 3.015500068664551 4.663643836975098
Loss :  1.7264653444290161 2.982074737548828 4.708539962768555
Loss :  1.6742968559265137 2.9335198402404785 4.607816696166992
Loss :  1.6605597734451294 2.805147409439087 4.465707302093506
Loss :  1.665189266204834 3.1374728679656982 4.802661895751953
Loss :  1.7000305652618408 3.0579872131347656 4.758017539978027
Loss :  1.692615032196045 2.8741369247436523 4.566751956939697
Loss :  1.673621654510498 2.857658624649048 4.531280517578125
Loss :  1.64031982421875 2.928760528564453 4.569080352783203
Loss :  1.6654891967773438 3.2522523403167725 4.917741775512695
Loss :  1.6607128381729126 2.9530177116394043 4.613730430603027
  batch 40 loss: 1.6607128381729126, 2.9530177116394043, 4.613730430603027
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.664955973625183 2.864335775375366 4.52929162979126
Loss :  1.6524996757507324 3.0756871700286865 4.72818660736084
Loss :  1.6719036102294922 2.8561177253723145 4.528021335601807
Loss :  1.6675972938537598 3.1263558864593506 4.793952941894531
Loss :  1.6696540117263794 2.9460129737854004 4.61566686630249
Loss :  1.6660420894622803 3.1212377548217773 4.787280082702637
Loss :  1.6526458263397217 3.1780877113342285 4.830733299255371
Loss :  1.6611473560333252 2.889583110809326 4.5507307052612305
Loss :  1.638390064239502 3.059296131134033 4.697686195373535
Loss :  1.685226559638977 2.908806562423706 4.594033241271973
Loss :  1.6538474559783936 3.4000961780548096 5.053943634033203
Loss :  1.6711478233337402 2.992217779159546 4.663365364074707
Loss :  1.6896885633468628 3.0876517295837402 4.777340412139893
Loss :  1.6694533824920654 3.357415199279785 5.02686882019043
Loss :  1.6776182651519775 3.655611515045166 5.333230018615723
Loss :  1.6440162658691406 3.314671516418457 4.958687782287598
Loss :  1.692471981048584 3.3053555488586426 4.997827529907227
Loss :  1.6853331327438354 3.2634479999542236 4.9487810134887695
Loss :  1.7021681070327759 3.377053737640381 5.079221725463867
Loss :  1.678715467453003 3.281015157699585 4.959730625152588
  batch 60 loss: 1.678715467453003, 3.281015157699585, 4.959730625152588
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6783733367919922 3.423861265182495 5.102234840393066
Loss :  1.6749510765075684 3.315620183944702 4.990571022033691
Loss :  1.6839022636413574 3.100839376449585 4.784741401672363
Loss :  1.6659109592437744 3.2555832862854004 4.921494483947754
Loss :  1.6625514030456543 3.2898082733154297 4.952359676361084
Loss :  4.809580326080322 4.378901958465576 9.188482284545898
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  4.747656345367432 4.3638739585876465 9.111530303955078
Loss :  4.831840991973877 4.3796067237854 9.211447715759277
Loss :  4.056103706359863 4.4300336837768555 8.486137390136719
Total LOSS train 4.758523053389329 valid 8.999399423599243
CE LOSS train 1.6700075461314274 valid 1.0140259265899658
Contrastive LOSS train 3.088515512759869 valid 1.1075084209442139
EPOCH 96:
Loss :  1.6575379371643066 3.214712619781494 4.872250556945801
Loss :  1.673030972480774 3.311279296875 4.984310150146484
Loss :  1.658072829246521 2.9620354175567627 4.620108127593994
Loss :  1.6616767644882202 3.082231283187866 4.743907928466797
Loss :  1.6859657764434814 3.0186879634857178 4.704653739929199
Loss :  1.669927716255188 3.194035768508911 4.863963603973389
Loss :  1.6670582294464111 3.6885063648223877 5.355564594268799
Loss :  1.6583672761917114 2.7653517723083496 4.4237189292907715
Loss :  1.6620619297027588 2.7818961143493652 4.443958282470703
Loss :  1.6147629022598267 2.6863691806793213 4.3011322021484375
Loss :  1.6762974262237549 3.068575859069824 4.744873046875
Loss :  1.733028769493103 3.3954505920410156 5.128479480743408
Loss :  1.6766530275344849 3.3869268894195557 5.06358003616333
Loss :  1.6692646741867065 3.351077079772949 5.020341873168945
Loss :  1.650776743888855 2.9433834552764893 4.594160079956055
Loss :  1.6574286222457886 3.1886086463928223 4.8460373878479
Loss :  1.6658135652542114 3.119779348373413 4.785593032836914
Loss :  1.6664959192276 2.9800360202789307 4.64653205871582
Loss :  1.6716029644012451 2.8698105812072754 4.541413307189941
Loss :  1.6318176984786987 2.9867451190948486 4.618562698364258
  batch 20 loss: 1.6318176984786987, 2.9867451190948486, 4.618562698364258
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.666222095489502 3.2971673011779785 4.9633893966674805
Loss :  1.681382417678833 3.0541584491729736 4.735540866851807
Loss :  1.653124451637268 2.835437297821045 4.488561630249023
Loss :  1.6937565803527832 2.8428404331207275 4.53659725189209
Loss :  1.6903959512710571 3.356083631515503 5.04647970199585
Loss :  1.6581923961639404 3.3473668098449707 5.005558967590332
Loss :  1.7021379470825195 3.147930145263672 4.850068092346191
Loss :  1.6459964513778687 2.958930253982544 4.604926586151123
Loss :  1.6907615661621094 3.203510046005249 4.8942718505859375
Loss :  1.649713397026062 3.2782905101776123 4.928003787994385
Loss :  1.7269635200500488 2.992969036102295 4.719932556152344
Loss :  1.6735984086990356 2.9495902061462402 4.623188495635986
Loss :  1.6600067615509033 2.4505627155303955 4.110569477081299
Loss :  1.6638565063476562 2.8889927864074707 4.552849292755127
Loss :  1.6996492147445679 2.78662371635437 4.486272811889648
Loss :  1.6903001070022583 2.797125816345215 4.487425804138184
Loss :  1.6727427244186401 2.5240297317504883 4.196772575378418
Loss :  1.6375901699066162 2.64062762260437 4.278217792510986
Loss :  1.665759563446045 3.147383451461792 4.813142776489258
Loss :  1.6603167057037354 2.9761903285980225 4.636507034301758
  batch 40 loss: 1.6603167057037354, 2.9761903285980225, 4.636507034301758
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6637438535690308 2.8131613731384277 4.476905345916748
Loss :  1.6511588096618652 2.2578845024108887 3.909043312072754
Loss :  1.6714965105056763 2.621206521987915 4.292703151702881
Loss :  1.6666656732559204 2.760916233062744 4.427581787109375
Loss :  1.6684635877609253 2.2694907188415527 3.9379544258117676
Loss :  1.6640009880065918 2.6605443954467773 4.324545383453369
Loss :  1.6514947414398193 2.544149160385132 4.195643901824951
Loss :  1.6597150564193726 2.3883795738220215 4.048094749450684
Loss :  1.639030933380127 2.527961254119873 4.1669921875
Loss :  1.683581829071045 2.5890274047851562 4.272609233856201
Loss :  1.6533268690109253 2.778395414352417 4.431722164154053
Loss :  1.6712336540222168 2.7246475219726562 4.395881175994873
Loss :  1.6880826950073242 2.776710271835327 4.4647932052612305
Loss :  1.6672202348709106 3.04457426071167 4.711794376373291
Loss :  1.676497220993042 2.7772107124328613 4.453707695007324
Loss :  1.6440073251724243 2.693652868270874 4.337660312652588
Loss :  1.690463662147522 2.752643346786499 4.4431071281433105
Loss :  1.6825989484786987 2.858870029449463 4.541469097137451
Loss :  1.70041823387146 2.894522190093994 4.594940185546875
Loss :  1.6788256168365479 2.727391242980957 4.406216621398926
  batch 60 loss: 1.6788256168365479, 2.727391242980957, 4.406216621398926
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6775411367416382 3.3335320949554443 5.011073112487793
Loss :  1.6733734607696533 3.1513988971710205 4.824772357940674
Loss :  1.6825870275497437 3.5324251651763916 5.215012073516846
Loss :  1.6650161743164062 3.5734505653381348 5.238466739654541
Loss :  1.6602685451507568 3.2053380012512207 4.865606307983398
Loss :  4.101425647735596 4.452870845794678 8.554296493530273
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  4.059812068939209 4.463811874389648 8.523624420166016
Loss :  4.095081806182861 4.1955132484436035 8.290595054626465
Loss :  3.3931076526641846 4.083099365234375 7.4762067794799805
Total LOSS train 4.619226367657001 valid 8.211180686950684
CE LOSS train 1.6695526379805345 valid 0.8482769131660461
Contrastive LOSS train 2.949673744348379 valid 1.0207748413085938
EPOCH 97:
Loss :  1.6550965309143066 3.2164103984832764 4.871506690979004
Loss :  1.6710113286972046 3.1819162368774414 4.8529276847839355
Loss :  1.6543035507202148 3.2267651557922363 4.881068706512451
Loss :  1.6563507318496704 3.683878183364868 5.340229034423828
Loss :  1.6846365928649902 2.9856009483337402 4.6702375411987305
Loss :  1.6661661863327026 3.236842155456543 4.903008460998535
Loss :  1.6675175428390503 3.001624345779419 4.66914176940918
Loss :  1.6578452587127686 3.6989195346832275 5.356764793395996
Loss :  1.6606101989746094 3.3937714099884033 5.054381370544434
Loss :  1.6168466806411743 2.920292615890503 4.537139415740967
Loss :  1.6736910343170166 2.863588809967041 4.537280082702637
Loss :  1.7290371656417847 3.1324493885040283 4.861486434936523
Loss :  1.6752283573150635 3.0090949535369873 4.684323310852051
Loss :  1.6704293489456177 2.9408066272735596 4.611236095428467
Loss :  1.649715781211853 2.8682212829589844 4.517937183380127
Loss :  1.655775785446167 3.2156970500946045 4.8714728355407715
Loss :  1.66384756565094 2.7628681659698486 4.426715850830078
Loss :  1.666050672531128 3.1307566165924072 4.796807289123535
Loss :  1.6652724742889404 2.7935831546783447 4.458855628967285
Loss :  1.631532907485962 3.015439748764038 4.64697265625
  batch 20 loss: 1.631532907485962, 3.015439748764038, 4.64697265625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6662614345550537 3.187406063079834 4.853667259216309
Loss :  1.6790863275527954 2.6964354515075684 4.375521659851074
Loss :  1.6554428339004517 3.1284103393554688 4.783853054046631
Loss :  1.6899645328521729 3.0252273082733154 4.715191841125488
Loss :  1.687425971031189 3.118300676345825 4.805726528167725
Loss :  1.6583912372589111 2.936871290206909 4.59526252746582
Loss :  1.7009700536727905 3.1012065410614014 4.802176475524902
Loss :  1.647234559059143 3.3261337280273438 4.973368167877197
Loss :  1.6924107074737549 2.9759037494659424 4.668314456939697
Loss :  1.64804208278656 2.7435503005981445 4.391592502593994
Loss :  1.7263426780700684 3.3456149101257324 5.071957588195801
Loss :  1.6730774641036987 2.8060216903686523 4.479099273681641
Loss :  1.6599299907684326 3.4978151321411133 5.157745361328125
Loss :  1.6642200946807861 3.29272198677063 4.956942081451416
Loss :  1.6993943452835083 3.3122644424438477 5.011658668518066
Loss :  1.6912410259246826 3.0320451259613037 4.723286151885986
Loss :  1.671888828277588 3.3203790187835693 4.992267608642578
Loss :  1.638197660446167 3.1077258586883545 4.7459235191345215
Loss :  1.6648130416870117 2.987680673599243 4.652493476867676
Loss :  1.6599880456924438 3.263293981552124 4.923282146453857
  batch 40 loss: 1.6599880456924438, 3.263293981552124, 4.923282146453857
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.662940502166748 3.4301631450653076 5.093103408813477
Loss :  1.649227261543274 2.891904354095459 4.541131496429443
Loss :  1.6700892448425293 3.108391761779785 4.7784810066223145
Loss :  1.6640697717666626 3.092547655105591 4.756617546081543
Loss :  1.6665992736816406 2.9820470809936523 4.648646354675293
Loss :  1.6632492542266846 3.561767101287842 5.2250165939331055
Loss :  1.6505341529846191 3.363255739212036 5.013790130615234
Loss :  1.6581159830093384 2.963747262954712 4.62186336517334
Loss :  1.6375030279159546 3.8683464527130127 5.505849361419678
Loss :  1.6825950145721436 3.132848024368286 4.81544303894043
Loss :  1.6531822681427002 3.0420048236846924 4.695187091827393
Loss :  1.670007586479187 3.1360526084899902 4.806060314178467
Loss :  1.6880254745483398 3.342434883117676 5.030460357666016
Loss :  1.6690012216567993 3.2526352405548096 4.921636581420898
Loss :  1.6757985353469849 3.6826281547546387 5.358426570892334
Loss :  1.6439290046691895 2.903043031692505 4.546972274780273
Loss :  1.691630482673645 3.331113338470459 5.0227437019348145
Loss :  1.6833451986312866 3.4086074829101562 5.091952800750732
Loss :  1.7015981674194336 3.52984619140625 5.231444358825684
Loss :  1.6791294813156128 3.2244303226470947 4.903559684753418
  batch 60 loss: 1.6791294813156128, 3.2244303226470947, 4.903559684753418
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6783539056777954 3.5565733909606934 5.234927177429199
Loss :  1.6741670370101929 3.405731678009033 5.079898834228516
Loss :  1.6827119588851929 3.448662519454956 5.131374359130859
Loss :  1.6651281118392944 3.22843861579895 4.893566608428955
Loss :  1.662049412727356 2.783123731613159 4.445173263549805
Loss :  4.5042619705200195 4.39857816696167 8.902839660644531
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  4.412595748901367 4.405951023101807 8.818546295166016
Loss :  4.509843349456787 4.287817001342773 8.797660827636719
Loss :  3.7557525634765625 4.203037261962891 7.958789825439453
Total LOSS train 4.840340761037973 valid 8.61945915222168
CE LOSS train 1.6687426145260151 valid 0.9389381408691406
Contrastive LOSS train 3.1715981483459474 valid 1.0507593154907227
EPOCH 98:
Loss :  1.6557555198669434 3.0669949054718018 4.722750663757324
Loss :  1.6723544597625732 3.134249448776245 4.806603908538818
Loss :  1.6568187475204468 2.9057390689849854 4.562557697296143
Loss :  1.6595101356506348 3.0659964084625244 4.725506782531738
Loss :  1.6853981018066406 3.164344549179077 4.849742889404297
Loss :  1.668164610862732 2.8716139793395996 4.539778709411621
Loss :  1.6673556566238403 3.0384342670440674 4.705790042877197
Loss :  1.6576576232910156 2.9388160705566406 4.596473693847656
Loss :  1.6610524654388428 2.6355817317962646 4.296634197235107
Loss :  1.6146212816238403 2.7767326831817627 4.391354084014893
Loss :  1.6747390031814575 2.88037371635437 4.555112838745117
Loss :  1.732098937034607 3.2149391174316406 4.947038173675537
Loss :  1.6751000881195068 3.1497178077697754 4.824817657470703
Loss :  1.6682064533233643 3.08915114402771 4.757357597351074
Loss :  1.6505388021469116 2.8538978099823 4.504436492919922
Loss :  1.6556845903396606 3.1750383377075195 4.830722808837891
Loss :  1.6638562679290771 3.2357993125915527 4.899655342102051
Loss :  1.6650365591049194 3.0477938652038574 4.712830543518066
Loss :  1.6689400672912598 3.0236740112304688 4.6926140785217285
Loss :  1.6308298110961914 2.728299617767334 4.359129428863525
  batch 20 loss: 1.6308298110961914, 2.728299617767334, 4.359129428863525
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6654791831970215 3.4735820293426514 5.139060974121094
Loss :  1.6796939373016357 2.856079578399658 4.535773277282715
Loss :  1.6541324853897095 3.392326831817627 5.046459197998047
Loss :  1.6915640830993652 3.1687982082366943 4.8603620529174805
Loss :  1.6882661581039429 3.254488945007324 4.942755222320557
Loss :  1.6577601432800293 2.6738319396972656 4.331592082977295
Loss :  1.7009040117263794 2.577256202697754 4.278160095214844
Loss :  1.6460156440734863 2.3265061378479004 3.9725217819213867
Loss :  1.6901706457138062 2.2778701782226562 3.968040943145752
Loss :  1.6474840641021729 2.4711103439331055 4.118594169616699
Loss :  1.7253204584121704 2.9829838275909424 4.708304405212402
Loss :  1.6727427244186401 2.663468360900879 4.336211204528809
Loss :  1.6590702533721924 2.4625816345214844 4.121651649475098
Loss :  1.6626571416854858 2.5193824768066406 4.182039737701416
Loss :  1.6995984315872192 2.7556064128875732 4.455204963684082
Loss :  1.6895170211791992 2.708613395690918 4.398130416870117
Loss :  1.6717052459716797 2.5430288314819336 4.214734077453613
Loss :  1.6376383304595947 2.6997406482696533 4.337378978729248
Loss :  1.664611577987671 2.564061403274536 4.228672981262207
Loss :  1.6589490175247192 2.5655229091644287 4.2244720458984375
  batch 40 loss: 1.6589490175247192, 2.5655229091644287, 4.2244720458984375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6633168458938599 2.7845919132232666 4.447908878326416
Loss :  1.6507596969604492 2.4177961349487305 4.06855583190918
Loss :  1.670216679573059 2.573857069015503 4.244073867797852
Loss :  1.6660027503967285 2.8626701831817627 4.52867317199707
Loss :  1.6681610345840454 2.5373923778533936 4.2055535316467285
Loss :  1.6625226736068726 2.7053146362304688 4.367837429046631
Loss :  1.6503515243530273 2.426560640335083 4.076911926269531
Loss :  1.6596466302871704 2.6927578449249268 4.352404594421387
Loss :  1.6366244554519653 2.794412136077881 4.431036472320557
Loss :  1.6834572553634644 2.6031086444854736 4.286565780639648
Loss :  1.6517270803451538 2.8736977577209473 4.525424957275391
Loss :  1.67023766040802 2.9504523277282715 4.620689868927002
Loss :  1.6867250204086304 3.056471347808838 4.743196487426758
Loss :  1.6687980890274048 2.9359183311462402 4.6047163009643555
Loss :  1.6750516891479492 2.6836140155792236 4.358665466308594
Loss :  1.6433929204940796 2.620347023010254 4.263740062713623
Loss :  1.6894680261611938 2.875983476638794 4.565451622009277
Loss :  1.6831117868423462 3.082519769668579 4.765631675720215
Loss :  1.6990162134170532 3.3638970851898193 5.062913417816162
Loss :  1.676120400428772 3.0526044368743896 4.728724956512451
  batch 60 loss: 1.676120400428772, 3.0526044368743896, 4.728724956512451
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6761693954467773 2.9104549884796143 4.5866241455078125
Loss :  1.6723542213439941 3.014564275741577 4.686918258666992
Loss :  1.6824679374694824 3.2466583251953125 4.929126262664795
Loss :  1.6637840270996094 3.442452907562256 5.106236934661865
Loss :  1.6601911783218384 2.8067290782928467 4.466920375823975
Loss :  4.5391082763671875 4.445840358734131 8.984949111938477
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  4.4602460861206055 4.408427715301514 8.868673324584961
Loss :  4.523869514465332 4.321885585784912 8.845754623413086
Loss :  3.8106918334960938 4.277819633483887 8.08851146697998
Total LOSS train 4.533931233332708 valid 8.696972131729126
CE LOSS train 1.6685642297451313 valid 0.9526729583740234
Contrastive LOSS train 2.8653669980856087 valid 1.0694549083709717
EPOCH 99:
Loss :  1.6542928218841553 3.601865291595459 5.256157875061035
Loss :  1.6711095571517944 3.642336130142212 5.313445568084717
Loss :  1.6552447080612183 3.3461225032806396 5.001367092132568
Loss :  1.656740427017212 3.0963728427886963 4.753113269805908
Loss :  1.6836901903152466 3.2190308570861816 4.902720928192139
Loss :  1.6656274795532227 2.889122724533081 4.554750442504883
Loss :  1.6664224863052368 3.2714574337005615 4.937880039215088
Loss :  1.6566083431243896 3.012430429458618 4.669038772583008
Loss :  1.6594114303588867 3.3027963638305664 4.962207794189453
Loss :  1.6161017417907715 2.6473758220672607 4.263477325439453
Loss :  1.6753915548324585 3.1873698234558105 4.862761497497559
Loss :  1.7289721965789795 3.3010053634643555 5.029977798461914
Loss :  1.6763532161712646 2.986344814300537 4.662697792053223
Loss :  1.6702170372009277 3.3011014461517334 4.971318244934082
Loss :  1.6477973461151123 2.6082122325897217 4.256009578704834
Loss :  1.656855583190918 3.178696393966675 4.835552215576172
Loss :  1.6650407314300537 3.2827610969543457 4.94780158996582
Loss :  1.6656473875045776 2.6056339740753174 4.2712812423706055
Loss :  1.6671305894851685 2.5190749168395996 4.1862053871154785
Loss :  1.6312353610992432 2.6286768913269043 4.259912490844727
  batch 20 loss: 1.6312353610992432, 2.6286768913269043, 4.259912490844727
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6657779216766357 2.9887824058532715 4.654560089111328
Loss :  1.6795270442962646 2.714064359664917 4.393591403961182
Loss :  1.6533896923065186 3.116309881210327 4.769699573516846
Loss :  1.6899572610855103 3.2311863899230957 4.921143531799316
Loss :  1.687483549118042 3.0840401649475098 4.771523475646973
Loss :  1.6567078828811646 3.034477472305298 4.691185474395752
Loss :  1.7020323276519775 3.1895947456359863 4.891627311706543
Loss :  1.6461321115493774 2.743195056915283 4.389327049255371
Loss :  1.6936469078063965 3.1552021503448486 4.848849296569824
Loss :  1.6478248834609985 3.376052141189575 5.023877143859863
Loss :  1.7270995378494263 3.303973436355591 5.031073093414307
Loss :  1.6723718643188477 3.547882556915283 5.220254421234131
Loss :  1.6592893600463867 2.8932228088378906 4.552512168884277
Loss :  1.663402795791626 3.1955888271331787 4.858991622924805
Loss :  1.6998164653778076 3.1811397075653076 4.880956172943115
Loss :  1.6914290189743042 3.5844457149505615 5.275874614715576
Loss :  1.6708112955093384 3.2380833625793457 4.9088945388793945
Loss :  1.6365197896957397 3.2427332401275635 4.879252910614014
Loss :  1.6646924018859863 3.37404203414917 5.038734436035156
Loss :  1.6572798490524292 3.105471611022949 4.762751579284668
  batch 40 loss: 1.6572798490524292, 3.105471611022949, 4.762751579284668
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6635266542434692 2.956087350845337 4.619614124298096
Loss :  1.652906894683838 3.594298839569092 5.24720573425293
Loss :  1.6707773208618164 3.2728490829467773 4.943626403808594
Loss :  1.6683331727981567 2.6515390872955322 4.3198723793029785
Loss :  1.6694881916046143 2.4513769149780273 4.1208648681640625
Loss :  1.6629657745361328 2.9493815898895264 4.612347602844238
Loss :  1.6507313251495361 2.851973533630371 4.502704620361328
Loss :  1.661277174949646 2.8861887454986572 4.547465801239014
Loss :  1.6341465711593628 3.0537147521972656 4.687861442565918
Loss :  1.6852890253067017 2.7685201168060303 4.4538092613220215
Loss :  1.652360200881958 2.8977420330047607 4.550102233886719
Loss :  1.669858694076538 2.832615613937378 4.502474308013916
Loss :  1.6865912675857544 2.4962172508239746 4.1828083992004395
Loss :  1.671542763710022 2.564272165298462 4.235815048217773
Loss :  1.6765880584716797 2.812490940093994 4.489078998565674
Loss :  1.6425235271453857 2.479846715927124 4.12237024307251
Loss :  1.6897510290145874 2.9104409217834473 4.600192070007324
Loss :  1.6847947835922241 2.689216375350952 4.374011039733887
Loss :  1.6995468139648438 3.230072259902954 4.929618835449219
Loss :  1.676437258720398 2.6791746616363525 4.355611801147461
  batch 60 loss: 1.676437258720398, 2.6791746616363525, 4.355611801147461
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6751296520233154 3.3153936862945557 4.990523338317871
Loss :  1.6733266115188599 3.2348825931549072 4.908209323883057
Loss :  1.6816548109054565 2.993124485015869 4.674779415130615
Loss :  1.663145899772644 3.1094717979431152 4.772617816925049
Loss :  1.6600559949874878 2.6174585819244385 4.277514457702637
Loss :  5.042785167694092 4.405081272125244 9.447866439819336
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  4.9545369148254395 4.328564643859863 9.283102035522461
Loss :  5.042893409729004 4.217390537261963 9.260284423828125
Loss :  4.254661560058594 4.2240986824035645 8.478759765625
Total LOSS train 4.70282240647536 valid 9.11750316619873
CE LOSS train 1.6685820249410777 valid 1.0636653900146484
Contrastive LOSS train 3.0342403925382175 valid 1.0560246706008911
EPOCH 100:
Loss :  1.6555052995681763 3.1203083992004395 4.775813579559326
Loss :  1.6687090396881104 3.0806095600128174 4.749318599700928
Loss :  1.6549922227859497 2.46193790435791 4.11693000793457
Loss :  1.6590206623077393 2.8782927989959717 4.537313461303711
Loss :  1.6839734315872192 2.989905595779419 4.673879146575928
Loss :  1.6666630506515503 2.6713624000549316 4.3380255699157715
Loss :  1.663282871246338 2.8620524406433105 4.525335311889648
Loss :  1.6541974544525146 2.5013270378112793 4.155524253845215
Loss :  1.6588183641433716 2.3375356197357178 3.996354103088379
Loss :  1.6123418807983398 2.5173866748809814 4.129728317260742
Loss :  1.6727511882781982 2.8342175483703613 4.5069684982299805
Loss :  1.730266809463501 3.135528087615967 4.865795135498047
Loss :  1.6740909814834595 3.226760149002075 4.900851249694824
Loss :  1.6666944026947021 3.3928887844085693 5.0595831871032715
Loss :  1.648227334022522 2.7993032932281494 4.447530746459961
Loss :  1.6542898416519165 2.985457181930542 4.639747142791748
Loss :  1.6624667644500732 3.0030369758605957 4.66550350189209
Loss :  1.663309097290039 2.8383376598358154 4.501646995544434
Loss :  1.6676162481307983 2.515174150466919 4.182790279388428
Loss :  1.6282744407653809 2.7510335445404053 4.379307746887207
  batch 20 loss: 1.6282744407653809, 2.7510335445404053, 4.379307746887207
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.664719581604004 2.8942434787750244 4.558962821960449
Loss :  1.678351879119873 2.6071012020111084 4.285452842712402
Loss :  1.6514406204223633 3.3214950561523438 4.972935676574707
Loss :  1.6904000043869019 3.394526243209839 5.084926128387451
Loss :  1.6875226497650146 3.555617332458496 5.24314022064209
Loss :  1.6549378633499146 3.332425832748413 4.987363815307617
Loss :  1.7002124786376953 3.182180643081665 4.882392883300781
Loss :  1.6438236236572266 3.060781240463257 4.7046051025390625
Loss :  1.6902449131011963 2.8352739810943604 4.525518894195557
Loss :  1.6456843614578247 3.528336524963379 5.174020767211914
Loss :  1.7246447801589966 3.327531337738037 5.052175998687744
Loss :  1.6721502542495728 3.2910895347595215 4.963239669799805
Loss :  1.658398985862732 3.061903238296509 4.720302104949951
Loss :  1.6608606576919556 3.3841540813446045 5.04501485824585
Loss :  1.6992979049682617 3.183303117752075 4.882600784301758
Loss :  1.690148949623108 2.584062099456787 4.2742109298706055
Loss :  1.6694769859313965 2.815624952316284 4.485101699829102
Loss :  1.6375949382781982 2.8083765506744385 4.445971488952637
Loss :  1.663663387298584 2.6213419437408447 4.285005569458008
Loss :  1.656828761100769 2.495220899581909 4.152049541473389
  batch 40 loss: 1.656828761100769, 2.495220899581909, 4.152049541473389
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6626559495925903 2.8226728439331055 4.485328674316406
Loss :  1.6507909297943115 2.4148566722869873 4.065647602081299
Loss :  1.669771432876587 2.7987658977508545 4.468537330627441
Loss :  1.666072964668274 3.0502302646636963 4.71630334854126
Loss :  1.6691443920135498 2.620213031768799 4.2893571853637695
Loss :  1.661907434463501 2.9175877571105957 4.579495429992676
Loss :  1.6494812965393066 2.958014965057373 4.60749626159668
Loss :  1.6601927280426025 2.837235927581787 4.497428894042969
Loss :  1.6345068216323853 2.709181547164917 4.343688488006592
Loss :  1.6831388473510742 2.722564458847046 4.405703544616699
Loss :  1.6504522562026978 2.9593310356140137 4.609783172607422
Loss :  1.6688963174819946 3.0380361080169678 4.706932544708252
Loss :  1.6869831085205078 3.240311861038208 4.927294731140137
Loss :  1.669316053390503 2.8166496753692627 4.485965728759766
Loss :  1.6751763820648193 3.224604845046997 4.899781227111816
Loss :  1.6404297351837158 3.2001211643218994 4.840550899505615
Loss :  1.6891080141067505 2.9960222244262695 4.6851301193237305
Loss :  1.683964729309082 2.8672988414764404 4.551263809204102
Loss :  1.6989398002624512 3.55474591255188 5.25368595123291
Loss :  1.6749519109725952 2.73777174949646 4.412723541259766
  batch 60 loss: 1.6749519109725952, 2.73777174949646, 4.412723541259766
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.676909327507019 3.3540828227996826 5.030992031097412
Loss :  1.6731922626495361 3.545895576477051 5.219087600708008
Loss :  1.6823638677597046 3.3231351375579834 5.005498886108398
Loss :  1.6642193794250488 3.5316367149353027 5.195856094360352
Loss :  1.6607859134674072 3.4880189895629883 5.148804664611816
Loss :  5.111179828643799 4.3931779861450195 9.504358291625977
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.0219316482543945 4.285717487335205 9.307649612426758
Loss :  5.091651439666748 4.309187889099121 9.400838851928711
Loss :  4.304913520812988 4.330071926116943 8.634984970092773
Total LOSS train 4.650850406059852 valid 9.211957931518555
CE LOSS train 1.6675268741754385 valid 1.076228380203247
Contrastive LOSS train 2.9833235557262716 valid 1.0825179815292358
EPOCH 101:
Loss :  1.6549793481826782 3.7512872219085693 5.406266689300537
Loss :  1.672174334526062 3.5044376850128174 5.17661190032959
Loss :  1.6559834480285645 3.1086137294769287 4.764596939086914
Loss :  1.6573911905288696 3.5949182510375977 5.252309322357178
Loss :  1.683944821357727 3.383242607116699 5.067187309265137
Loss :  1.6666454076766968 3.0530238151550293 4.719669342041016
Loss :  1.6646697521209717 3.3351831436157227 4.999853134155273
Loss :  1.655643343925476 3.1131975650787354 4.768840789794922
Loss :  1.6594074964523315 3.54569149017334 5.205099105834961
Loss :  1.6139882802963257 3.2067160606384277 4.820704460144043
Loss :  1.6749939918518066 3.247213840484619 4.922207832336426
Loss :  1.7318470478057861 3.360328435897827 5.092175483703613
Loss :  1.6772563457489014 3.49279522895813 5.170051574707031
Loss :  1.66958487033844 3.508470296859741 5.178055286407471
Loss :  1.646058201789856 3.5593810081481934 5.20543909072876
Loss :  1.6563091278076172 3.426978349685669 5.083287239074707
Loss :  1.6663644313812256 3.6308493614196777 5.297213554382324
Loss :  1.6653029918670654 3.3447184562683105 5.010021209716797
Loss :  1.6698453426361084 3.198122978210449 4.867968559265137
Loss :  1.6293708086013794 3.2019717693328857 4.831342697143555
  batch 20 loss: 1.6293708086013794, 3.2019717693328857, 4.831342697143555
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6638152599334717 3.5638184547424316 5.227633476257324
Loss :  1.6797146797180176 3.489849805831909 5.169564247131348
Loss :  1.6502794027328491 3.611802816390991 5.262082099914551
Loss :  1.6902943849563599 3.480422019958496 5.170716285705566
Loss :  1.6869810819625854 3.4424784183502197 5.129459381103516
Loss :  1.65461266040802 3.3330793380737305 4.987691879272461
Loss :  1.7019975185394287 3.4935109615325928 5.1955084800720215
Loss :  1.645323634147644 3.226886034011841 4.872209548950195
Loss :  1.69450044631958 3.0507137775421143 4.745214462280273
Loss :  1.6452915668487549 3.707531690597534 5.352823257446289
Loss :  1.7260897159576416 3.2023191452026367 4.928408622741699
Loss :  1.6732145547866821 3.2908730506896973 4.96408748626709
Loss :  1.6594350337982178 3.081939220428467 4.7413740158081055
Loss :  1.6622600555419922 3.4084439277648926 5.070703983306885
Loss :  1.6997665166854858 3.240816593170166 4.940583229064941
Loss :  1.692689299583435 3.20493221282959 4.8976216316223145
Loss :  1.6706359386444092 2.98163104057312 4.652266979217529
Loss :  1.6388436555862427 3.0218186378479004 4.6606621742248535
Loss :  1.6643705368041992 3.3172242641448975 4.981595039367676
Loss :  1.6569873094558716 3.539523124694824 5.196510314941406
  batch 40 loss: 1.6569873094558716, 3.539523124694824, 5.196510314941406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6634231805801392 3.4580819606781006 5.121505260467529
Loss :  1.6528291702270508 3.278857469558716 4.9316864013671875
Loss :  1.669398546218872 3.130460500717163 4.799859046936035
Loss :  1.6681190729141235 2.7054443359375 4.373563289642334
Loss :  1.669737458229065 2.9272921085357666 4.597029685974121
Loss :  1.6634442806243896 3.205920457839966 4.8693647384643555
Loss :  1.6521552801132202 3.0828638076782227 4.735019207000732
Loss :  1.6609143018722534 3.2386415004730225 4.899555683135986
Loss :  1.6350427865982056 4.066038608551025 5.701081275939941
Loss :  1.6840969324111938 2.8935508728027344 4.577647686004639
Loss :  1.6497153043746948 3.302158832550049 4.951874256134033
Loss :  1.6686227321624756 3.059467315673828 4.728090286254883
Loss :  1.687500238418579 3.6414642333984375 5.3289642333984375
Loss :  1.670735478401184 3.0894672870635986 4.760202884674072
Loss :  1.6745907068252563 3.160034656524658 4.834625244140625
Loss :  1.641263484954834 2.9298205375671387 4.571084022521973
Loss :  1.6900347471237183 3.1310081481933594 4.821043014526367
Loss :  1.6839408874511719 3.167593240737915 4.851533889770508
Loss :  1.6994150876998901 3.472280740737915 5.171695709228516
Loss :  1.6757140159606934 2.954969644546509 4.630683898925781
  batch 60 loss: 1.6757140159606934, 2.954969644546509, 4.630683898925781
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.676960825920105 3.415310859680176 5.09227180480957
Loss :  1.6735137701034546 2.9324114322662354 4.6059250831604
Loss :  1.6824308633804321 3.296154737472534 4.978585720062256
Loss :  1.6637475490570068 2.8561956882476807 4.5199432373046875
Loss :  1.660421371459961 2.4108142852783203 4.071235656738281
Loss :  5.125507831573486 4.449555397033691 9.575063705444336
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.033651351928711 4.431382656097412 9.465034484863281
Loss :  5.110393047332764 4.241652965545654 9.352046012878418
Loss :  4.312312602996826 4.3249711990356445 8.637283325195312
Total LOSS train 4.946302912785456 valid 9.257356882095337
CE LOSS train 1.6684097216679499 valid 1.0780781507492065
Contrastive LOSS train 3.2778932167933537 valid 1.0812427997589111
EPOCH 102:
Loss :  1.6549893617630005 2.777574062347412 4.432563304901123
Loss :  1.6712433099746704 3.3405795097351074 5.011822700500488
Loss :  1.6557737588882446 2.639162540435791 4.294936180114746
Loss :  1.6580568552017212 3.0101535320281982 4.668210506439209
Loss :  1.6840323209762573 3.190526008605957 4.874558448791504
Loss :  1.6668086051940918 2.749401569366455 4.416210174560547
Loss :  1.6643047332763672 3.3957409858703613 5.0600457191467285
Loss :  1.655476689338684 2.790058135986328 4.445534706115723
Loss :  1.65944504737854 2.321324348449707 3.980769395828247
Loss :  1.6135735511779785 3.0812878608703613 4.69486141204834
Loss :  1.6742442846298218 2.9918875694274902 4.666131973266602
Loss :  1.7319413423538208 3.166489362716675 4.898430824279785
Loss :  1.676344871520996 3.245673179626465 4.922018051147461
Loss :  1.668761968612671 3.1631691455841064 4.831931114196777
Loss :  1.6473345756530762 2.8818514347076416 4.529186248779297
Loss :  1.6556042432785034 3.304260015487671 4.959864139556885
Loss :  1.6654247045516968 2.795016288757324 4.4604411125183105
Loss :  1.6644856929779053 2.7002742290496826 4.364759922027588
Loss :  1.669694185256958 2.4462087154388428 4.115902900695801
Loss :  1.6294562816619873 3.078253984451294 4.707710266113281
  batch 20 loss: 1.6294562816619873, 3.078253984451294, 4.707710266113281
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6641075611114502 2.6900205612182617 4.354127883911133
Loss :  1.6795603036880493 2.9414939880371094 4.621054172515869
Loss :  1.6508797407150269 2.673907995223999 4.324787616729736
Loss :  1.6909735202789307 2.539214849472046 4.230188369750977
Loss :  1.6877681016921997 3.0314395427703857 4.719207763671875
Loss :  1.6548019647598267 2.819993257522583 4.474795341491699
Loss :  1.701245665550232 3.0024306774139404 4.703676223754883
Loss :  1.6449249982833862 2.580021381378174 4.22494649887085
Loss :  1.6927937269210815 2.7439048290252686 4.4366984367370605
Loss :  1.6460586786270142 3.360457420349121 5.006515979766846
Loss :  1.72605299949646 2.8285582065582275 4.5546112060546875
Loss :  1.6726621389389038 3.228576898574829 4.901238918304443
Loss :  1.6587706804275513 2.6009418964385986 4.2597126960754395
Loss :  1.662002444267273 2.7701690196990967 4.43217134475708
Loss :  1.699615716934204 2.9682211875915527 4.667837142944336
Loss :  1.6915745735168457 2.701911211013794 4.393486022949219
Loss :  1.6705454587936401 2.6674742698669434 4.338019847869873
Loss :  1.6374380588531494 2.8663110733032227 4.503748893737793
Loss :  1.6642593145370483 2.5945494174957275 4.258808612823486
Loss :  1.6568565368652344 2.889888286590576 4.5467448234558105
  batch 40 loss: 1.6568565368652344, 2.889888286590576, 4.5467448234558105
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6632212400436401 2.912848949432373 4.576070308685303
Loss :  1.6519676446914673 2.6870181560516357 4.338985919952393
Loss :  1.6695635318756104 2.8970415592193604 4.566605091094971
Loss :  1.6677634716033936 2.623518943786621 4.291282653808594
Loss :  1.669262170791626 2.5013375282287598 4.170599937438965
Loss :  1.6624691486358643 2.772238254547119 4.4347076416015625
Loss :  1.651058554649353 2.855252981185913 4.506311416625977
Loss :  1.6607933044433594 2.7778916358947754 4.438684940338135
Loss :  1.6340969800949097 3.031574010848999 4.665670871734619
Loss :  1.684370994567871 3.2999179363250732 4.984289169311523
Loss :  1.6507880687713623 2.8255507946014404 4.476338863372803
Loss :  1.6688657999038696 2.824465751647949 4.493331432342529
Loss :  1.686535358428955 2.588704824447632 4.275239944458008
Loss :  1.6708604097366333 2.925713300704956 4.596573829650879
Loss :  1.6752580404281616 2.7104544639587402 4.385712623596191
Loss :  1.6412994861602783 2.4046051502227783 4.045904636383057
Loss :  1.6894757747650146 3.329867362976074 5.019343376159668
Loss :  1.6841627359390259 2.7878127098083496 4.471975326538086
Loss :  1.6992018222808838 2.9941792488098145 4.693381309509277
Loss :  1.6759439706802368 2.751976251602173 4.427920341491699
  batch 60 loss: 1.6759439706802368, 2.751976251602173, 4.427920341491699
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6757776737213135 3.213423013687134 4.889200687408447
Loss :  1.6734042167663574 3.1222615242004395 4.795665740966797
Loss :  1.681671142578125 2.7217745780944824 4.403445720672607
Loss :  1.6632272005081177 2.435920000076294 4.099147319793701
Loss :  1.6600146293640137 2.1016125679016113 3.761627197265625
Loss :  5.120208740234375 4.382116794586182 9.502325057983398
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.030416488647461 4.447118282318115 9.477535247802734
Loss :  5.109278678894043 4.291383743286133 9.400662422180176
Loss :  4.31027364730835 4.228798866271973 8.539072036743164
Total LOSS train 4.524558203036968 valid 9.229898691177368
CE LOSS train 1.6681683375285221 valid 1.0775684118270874
Contrastive LOSS train 2.8563898453345664 valid 1.0571997165679932
EPOCH 103:
Loss :  1.6551684141159058 2.9606969356536865 4.615865230560303
Loss :  1.6696650981903076 3.1220123767852783 4.791677474975586
Loss :  1.6555428504943848 2.4610469341278076 4.116589546203613
Loss :  1.658915400505066 3.203591823577881 4.862507343292236
Loss :  1.683984637260437 3.386119842529297 5.070104598999023
Loss :  1.6668614149093628 2.509272575378418 4.17613410949707
Loss :  1.6633609533309937 3.1317591667175293 4.7951202392578125
Loss :  1.6547311544418335 2.627117156982422 4.281848430633545
Loss :  1.6590135097503662 2.476262331008911 4.135275840759277
Loss :  1.612857460975647 2.594733476638794 4.2075910568237305
Loss :  1.673471212387085 2.6932923793792725 4.366763591766357
Loss :  1.731839656829834 3.4293642044067383 5.161203861236572
Loss :  1.6757713556289673 3.6538727283477783 5.329644203186035
Loss :  1.6676549911499023 3.374413251876831 5.0420684814453125
Loss :  1.6473066806793213 2.7085039615631104 4.355810642242432
Loss :  1.655076026916504 3.1917715072631836 4.8468475341796875
Loss :  1.6647225618362427 2.8395802974700928 4.504302978515625
Loss :  1.6639986038208008 3.020076274871826 4.684074878692627
Loss :  1.6702131032943726 2.5153205394744873 4.18553352355957
Loss :  1.628709316253662 3.1877763271331787 4.816485404968262
  batch 20 loss: 1.628709316253662, 3.1877763271331787, 4.816485404968262
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6636039018630981 2.7681078910827637 4.431711673736572
Loss :  1.6793192625045776 3.271588087081909 4.950907230377197
Loss :  1.6500205993652344 3.1931865215301514 4.843207359313965
Loss :  1.6913539171218872 2.8903071880340576 4.581661224365234
Loss :  1.6882883310317993 3.521688938140869 5.209977149963379
Loss :  1.6537977457046509 3.206488847732544 4.860286712646484
Loss :  1.7003802061080933 3.660203456878662 5.360583782196045
Loss :  1.6439920663833618 3.045912981033325 4.689905166625977
Loss :  1.6913706064224243 3.1871132850646973 4.878483772277832
Loss :  1.6459285020828247 3.387694835662842 5.033623218536377
Loss :  1.725725769996643 3.2584455013275146 4.984171390533447
Loss :  1.6726245880126953 3.2335429191589355 4.906167507171631
Loss :  1.6582558155059814 2.982595682144165 4.6408514976501465
Loss :  1.6610406637191772 3.572667121887207 5.233707904815674
Loss :  1.7000443935394287 3.4769015312194824 5.176945686340332
Loss :  1.6907336711883545 3.4499220848083496 5.140655517578125
Loss :  1.670087456703186 3.1505045890808105 4.820591926574707
Loss :  1.6372836828231812 3.0875518321990967 4.724835395812988
Loss :  1.6641098260879517 3.1195595264434814 4.783669471740723
Loss :  1.656346321105957 3.0980546474456787 4.754401206970215
  batch 40 loss: 1.656346321105957, 3.0980546474456787, 4.754401206970215
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.663109302520752 3.2354493141174316 4.898558616638184
Loss :  1.6517244577407837 3.2454771995544434 4.8972015380859375
Loss :  1.6694021224975586 3.2716217041015625 4.941023826599121
Loss :  1.6677520275115967 3.1196324825286865 4.787384510040283
Loss :  1.669482707977295 3.1688265800476074 4.838309288024902
Loss :  1.6616877317428589 2.812469959259033 4.474157810211182
Loss :  1.650611400604248 3.0116724967956543 4.662283897399902
Loss :  1.6611238718032837 3.3164021968841553 4.9775261878967285
Loss :  1.6328798532485962 3.300750494003296 4.933630466461182
Loss :  1.6848088502883911 3.2991490364074707 4.983957767486572
Loss :  1.65010404586792 3.564035177230835 5.214138984680176
Loss :  1.668508768081665 3.085071086883545 4.753580093383789
Loss :  1.68594491481781 3.063990592956543 4.749935626983643
Loss :  1.6714106798171997 3.2762844562530518 4.947695255279541
Loss :  1.6748363971710205 3.425266981124878 5.100103378295898
Loss :  1.6413742303848267 3.177896738052368 4.819271087646484
Loss :  1.68876314163208 3.1820759773254395 4.8708391189575195
Loss :  1.684125542640686 3.260051727294922 4.944177150726318
Loss :  1.6985722780227661 3.261007070541382 4.9595794677734375
Loss :  1.675653100013733 3.179746150970459 4.855399131774902
  batch 60 loss: 1.675653100013733, 3.179746150970459, 4.855399131774902
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6752206087112427 3.680493116378784 5.355713844299316
Loss :  1.673429250717163 3.445385456085205 5.118814468383789
Loss :  1.6818221807479858 3.11545467376709 4.797276973724365
Loss :  1.6623610258102417 3.225529909133911 4.887890815734863
Loss :  1.6594123840332031 2.80197811126709 4.461390495300293
Loss :  5.110558032989502 4.4322333335876465 9.542791366577148
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.012904644012451 4.457842826843262 9.470746994018555
Loss :  5.1054205894470215 4.308478832244873 9.413899421691895
Loss :  4.294430732727051 4.270907878875732 8.565338134765625
Total LOSS train 4.808948208735539 valid 9.248193979263306
CE LOSS train 1.6678045016068679 valid 1.0736076831817627
Contrastive LOSS train 3.141143696124737 valid 1.067726969718933
EPOCH 104:
Loss :  1.6557588577270508 2.9842071533203125 4.639966011047363
Loss :  1.669234037399292 3.4967477321624756 5.165981769561768
Loss :  1.6558157205581665 2.79203200340271 4.447847843170166
Loss :  1.6596379280090332 3.0887651443481445 4.748403072357178
Loss :  1.683992624282837 3.284865617752075 4.968858242034912
Loss :  1.667418360710144 3.183098554611206 4.8505167961120605
Loss :  1.6626254320144653 3.3095803260803223 4.972205638885498
Loss :  1.6541166305541992 2.8526954650878906 4.50681209564209
Loss :  1.6588715314865112 2.702575445175171 4.361446857452393
Loss :  1.6117225885391235 3.229409694671631 4.841132164001465
Loss :  1.6731613874435425 3.354254722595215 5.027416229248047
Loss :  1.7325068712234497 3.6948225498199463 5.4273295402526855
Loss :  1.6751289367675781 3.411249876022339 5.086379051208496
Loss :  1.667069673538208 3.794055938720703 5.461125373840332
Loss :  1.6472041606903076 3.227041482925415 4.874245643615723
Loss :  1.654707670211792 3.5140841007232666 5.168791770935059
Loss :  1.6640095710754395 3.4614899158477783 5.125499725341797
Loss :  1.6636251211166382 3.152881383895874 4.816506385803223
Loss :  1.6699926853179932 2.757398843765259 4.427391529083252
Loss :  1.6279618740081787 3.3290226459503174 4.956984519958496
  batch 20 loss: 1.6279618740081787, 3.3290226459503174, 4.956984519958496
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.663320541381836 3.20880126953125 4.872121810913086
Loss :  1.6792391538619995 3.339233160018921 5.018472194671631
Loss :  1.6495361328125 3.450435161590576 5.099971294403076
Loss :  1.6918292045593262 3.0294668674468994 4.721296310424805
Loss :  1.6884719133377075 3.663503885269165 5.351975917816162
Loss :  1.6535998582839966 3.083111047744751 4.736711025238037
Loss :  1.699932336807251 3.401181221008301 5.101113319396973
Loss :  1.6433172225952148 2.82466197013855 4.467979431152344
Loss :  1.6900968551635742 3.179304599761963 4.869401454925537
Loss :  1.6453921794891357 3.773955821990967 5.419347763061523
Loss :  1.7254976034164429 3.31827449798584 5.043772220611572
Loss :  1.6723546981811523 3.2410173416137695 4.913372039794922
Loss :  1.657766342163086 3.0500755310058594 4.707841873168945
Loss :  1.66036057472229 3.394747257232666 5.055108070373535
Loss :  1.6997147798538208 3.3411269187927246 5.040841579437256
Loss :  1.6899999380111694 3.2663094997406006 4.9563093185424805
Loss :  1.6698161363601685 3.226227045059204 4.896043300628662
Loss :  1.6363087892532349 3.3774077892303467 5.013716697692871
Loss :  1.6639198064804077 3.3243672847747803 4.988286972045898
Loss :  1.6560040712356567 3.0894830226898193 4.745487213134766
  batch 40 loss: 1.6560040712356567, 3.0894830226898193, 4.745487213134766
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.66264009475708 3.488654613494873 5.151294708251953
Loss :  1.6510201692581177 3.2496042251586914 4.9006242752075195
Loss :  1.6692707538604736 3.3769781589508057 5.046248912811279
Loss :  1.667110800743103 3.3085968494415283 4.975707530975342
Loss :  1.669329285621643 2.4884722232818604 4.157801628112793
Loss :  1.6609143018722534 3.6299386024475098 5.290853023529053
Loss :  1.6497448682785034 3.02048397064209 4.670228958129883
Loss :  1.6608963012695312 3.594701051712036 5.255597114562988
Loss :  1.6323320865631104 3.4690637588500977 5.101395606994629
Loss :  1.6847330331802368 3.2746665477752686 4.959399700164795
Loss :  1.6501094102859497 3.406399965286255 5.056509494781494
Loss :  1.6684380769729614 3.3981339931488037 5.066572189331055
Loss :  1.6857187747955322 2.7745819091796875 4.460300445556641
Loss :  1.6710091829299927 3.0819122791290283 4.7529215812683105
Loss :  1.674964427947998 3.4123055934906006 5.0872697830200195
Loss :  1.641298770904541 2.912870168685913 4.554168701171875
Loss :  1.688612937927246 3.1946394443511963 4.883252143859863
Loss :  1.683683156967163 3.3355727195739746 5.019255638122559
Loss :  1.6986517906188965 3.369845151901245 5.0684967041015625
Loss :  1.6759767532348633 2.9144461154937744 4.590422630310059
  batch 60 loss: 1.6759767532348633, 2.9144461154937744, 4.590422630310059
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.674739956855774 3.1428842544555664 4.817624092102051
Loss :  1.6730531454086304 3.3892717361450195 5.0623250007629395
Loss :  1.6809078454971313 3.1848037242889404 4.865711688995361
Loss :  1.6620198488235474 2.8606650829315186 4.5226850509643555
Loss :  1.658751368522644 2.942126750946045 4.6008782386779785
Loss :  5.080953598022461 4.4290876388549805 9.510041236877441
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  4.991569519042969 4.3929314613342285 9.384500503540039
Loss :  5.073813438415527 4.224457263946533 9.298271179199219
Loss :  4.275570392608643 4.294755935668945 8.57032585144043
Total LOSS train 4.904793152442346 valid 9.190784692764282
CE LOSS train 1.6674917991344744 valid 1.0688925981521606
Contrastive LOSS train 3.237301364311805 valid 1.0736889839172363
EPOCH 105:
Loss :  1.6553821563720703 2.9478602409362793 4.60324239730835
Loss :  1.6684515476226807 3.283874273300171 4.952325820922852
Loss :  1.6549228429794312 2.555452823638916 4.210375785827637
Loss :  1.659091830253601 2.8153457641601562 4.474437713623047
Loss :  1.683756709098816 3.49576735496521 5.179523944854736
Loss :  1.667051911354065 2.5208489894866943 4.187901020050049
Loss :  1.662387490272522 2.8057963848114014 4.468183994293213
Loss :  1.653873324394226 2.736565351486206 4.390438556671143
Loss :  1.6587849855422974 2.5922186374664307 4.251003742218018
Loss :  1.611633539199829 2.8644516468048096 4.476085186004639
Loss :  1.67258620262146 2.9089462757110596 4.5815324783325195
Loss :  1.7317492961883545 3.3674185276031494 5.099167823791504
Loss :  1.6748868227005005 3.2421679496765137 4.917054653167725
Loss :  1.6669594049453735 3.096713066101074 4.763672351837158
Loss :  1.6474825143814087 2.6565678119659424 4.304050445556641
Loss :  1.6542444229125977 3.3034071922302246 4.957651615142822
Loss :  1.663619041442871 2.901423215866089 4.565042495727539
Loss :  1.6634070873260498 2.7621397972106934 4.425546646118164
Loss :  1.6695384979248047 2.3513805866241455 4.020918846130371
Loss :  1.6281867027282715 2.7841713428497314 4.412358283996582
  batch 20 loss: 1.6281867027282715, 2.7841713428497314, 4.412358283996582
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6635925769805908 2.5480880737304688 4.2116804122924805
Loss :  1.6791754961013794 2.6306798458099365 4.3098554611206055
Loss :  1.6501694917678833 2.688373565673828 4.338542938232422
Loss :  1.6914905309677124 2.517772912979126 4.209263324737549
Loss :  1.6883729696273804 3.157803535461426 4.846176624298096
Loss :  1.6540107727050781 2.7167813777923584 4.370792388916016
Loss :  1.6998735666275024 2.908576250076294 4.608449935913086
Loss :  1.6436612606048584 2.333735942840576 3.9773972034454346
Loss :  1.6901766061782837 2.627371072769165 4.317547798156738
Loss :  1.6454118490219116 2.9682376384735107 4.613649368286133
Loss :  1.7254444360733032 2.780745029449463 4.506189346313477
Loss :  1.6721481084823608 2.665623664855957 4.337771892547607
Loss :  1.6579402685165405 2.6239216327667236 4.281861782073975
Loss :  1.6607388257980347 2.613006830215454 4.273745536804199
Loss :  1.6995035409927368 2.7790653705596924 4.478569030761719
Loss :  1.6901161670684814 2.7665960788726807 4.456712245941162
Loss :  1.6700093746185303 2.7678487300872803 4.4378581047058105
Loss :  1.636399745941162 2.7214815616607666 4.357881546020508
Loss :  1.6638938188552856 2.651844024658203 4.315737724304199
Loss :  1.6563078165054321 2.601109504699707 4.25741720199585
  batch 40 loss: 1.6563078165054321, 2.601109504699707, 4.25741720199585
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6626590490341187 2.886387825012207 4.549046993255615
Loss :  1.651026964187622 2.8868935108184814 4.5379204750061035
Loss :  1.6696391105651855 2.8195810317993164 4.489220142364502
Loss :  1.6667789220809937 2.7218196392059326 4.388598442077637
Loss :  1.6690897941589355 2.3226630687713623 3.991752862930298
Loss :  1.6612733602523804 2.794158458709717 4.455431938171387
Loss :  1.649930477142334 2.6111063957214355 4.2610368728637695
Loss :  1.6605838537216187 2.7172763347625732 4.377860069274902
Loss :  1.6331276893615723 2.8243818283081055 4.457509517669678
Loss :  1.6843795776367188 2.7980775833129883 4.482457160949707
Loss :  1.6505364179611206 2.9880683422088623 4.638604640960693
Loss :  1.668936014175415 2.6283559799194336 4.2972917556762695
Loss :  1.685644268989563 2.6569647789001465 4.34260892868042
Loss :  1.6703177690505981 2.7688963413238525 4.43921422958374
Loss :  1.6750943660736084 2.96014404296875 4.6352386474609375
Loss :  1.641237735748291 2.58567214012146 4.226909637451172
Loss :  1.6886059045791626 2.7837226390838623 4.4723286628723145
Loss :  1.6836061477661133 2.7493550777435303 4.432961463928223
Loss :  1.6987806558609009 2.9217355251312256 4.620516300201416
Loss :  1.6758923530578613 2.537105083465576 4.2129974365234375
  batch 60 loss: 1.6758923530578613, 2.537105083465576, 4.2129974365234375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6748309135437012 2.663112163543701 4.337943077087402
Loss :  1.6729340553283691 2.5872573852539062 4.260191440582275
Loss :  1.680898904800415 2.474604845046997 4.155503749847412
Loss :  1.662609338760376 2.443358898162842 4.105968475341797
Loss :  1.659123182296753 2.2867300510406494 3.9458532333374023
Loss :  5.021402835845947 4.473808288574219 9.495210647583008
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  4.933376312255859 4.516403675079346 9.449779510498047
Loss :  5.0111589431762695 4.339584827423096 9.350744247436523
Loss :  4.219230651855469 4.366380214691162 8.585611343383789
Total LOSS train 4.428655074192927 valid 9.220336437225342
CE LOSS train 1.6674456981512216 valid 1.0548076629638672
Contrastive LOSS train 2.7612093668717606 valid 1.0915950536727905
EPOCH 106:
Loss :  1.6550897359848022 2.3939621448516846 4.049051761627197
Loss :  1.6686147451400757 3.009840965270996 4.678455829620361
Loss :  1.654703140258789 2.409759044647217 4.064462184906006
Loss :  1.6586107015609741 2.5249736309051514 4.183584213256836
Loss :  1.6834006309509277 2.799813747406006 4.483214378356934
Loss :  1.666324257850647 2.466357469558716 4.132681846618652
Loss :  1.6627306938171387 2.755479097366333 4.418210029602051
Loss :  1.654127836227417 2.2446587085723877 3.8987865447998047
Loss :  1.6587234735488892 2.3969132900238037 4.055636882781982
Loss :  1.6123043298721313 2.5086371898651123 4.120941638946533
Loss :  1.6725258827209473 2.583181619644165 4.255707740783691
Loss :  1.7309892177581787 3.1526525020599365 4.883641719818115
Loss :  1.674664855003357 2.874704122543335 4.549368858337402
Loss :  1.66707181930542 3.2521636486053467 4.9192352294921875
Loss :  1.64763605594635 2.758319616317749 4.405955791473389
Loss :  1.654140830039978 2.9217774868011475 4.575918197631836
Loss :  1.663766622543335 2.479625701904297 4.143392562866211
Loss :  1.6631157398223877 2.6660449504852295 4.329160690307617
Loss :  1.6689715385437012 2.622471332550049 4.29144287109375
Loss :  1.628193736076355 2.793701410293579 4.4218950271606445
  batch 20 loss: 1.628193736076355, 2.793701410293579, 4.4218950271606445
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6637036800384521 2.5682625770568848 4.231966018676758
Loss :  1.6787585020065308 2.568804979324341 4.247563362121582
Loss :  1.6503963470458984 2.685535192489624 4.335931777954102
Loss :  1.690651774406433 2.464578151702881 4.1552300453186035
Loss :  1.6876169443130493 3.359102964401245 5.046720027923584
Loss :  1.6539041996002197 2.540449380874634 4.1943535804748535
Loss :  1.699891209602356 2.7321369647979736 4.432028293609619
Loss :  1.6437593698501587 2.7104921340942383 4.354251384735107
Loss :  1.6906671524047852 2.515646457672119 4.206313610076904
Loss :  1.645400047302246 3.2200498580932617 4.865449905395508
Loss :  1.7249723672866821 2.7078163623809814 4.432788848876953
Loss :  1.671967625617981 3.0624961853027344 4.734463691711426
Loss :  1.6577876806259155 2.4283716678619385 4.0861592292785645
Loss :  1.660609245300293 2.664293050765991 4.324902534484863
Loss :  1.698974370956421 2.8342247009277344 4.533199310302734
Loss :  1.6900392770767212 3.155106782913208 4.845146179199219
Loss :  1.6697744131088257 2.414390802383423 4.084165096282959
Loss :  1.6366660594940186 2.534757137298584 4.171422958374023
Loss :  1.66331148147583 2.596374750137329 4.259686470031738
Loss :  1.6560908555984497 2.805570125579834 4.461660861968994
  batch 40 loss: 1.6560908555984497, 2.805570125579834, 4.461660861968994
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6624815464019775 2.9832093715667725 4.64569091796875
Loss :  1.6505651473999023 3.1710751056671143 4.8216400146484375
Loss :  1.6691166162490845 2.904426097869873 4.573542594909668
Loss :  1.6660974025726318 2.6972222328186035 4.363319396972656
Loss :  1.6684588193893433 2.5804061889648438 4.248865127563477
Loss :  1.661318302154541 3.09126877784729 4.75258731842041
Loss :  1.6496148109436035 2.891719341278076 4.54133415222168
Loss :  1.6596646308898926 2.9538497924804688 4.613514423370361
Loss :  1.6334236860275269 3.285346746444702 4.9187703132629395
Loss :  1.6836013793945312 2.8536949157714844 4.537296295166016
Loss :  1.6497445106506348 3.1972110271453857 4.846955299377441
Loss :  1.6684436798095703 3.0320255756378174 4.700469017028809
Loss :  1.6852647066116333 2.722055196762085 4.407320022583008
Loss :  1.6691646575927734 2.8350541591644287 4.504219055175781
Loss :  1.674383521080017 3.0257465839385986 4.700129985809326
Loss :  1.6406859159469604 2.7471156120300293 4.387801647186279
Loss :  1.6883248090744019 3.1225786209106445 4.810903549194336
Loss :  1.6831066608428955 2.9079103469848633 4.59101676940918
Loss :  1.698380947113037 3.029581308364868 4.727962493896484
Loss :  1.6752663850784302 2.6761491298675537 4.351415634155273
  batch 60 loss: 1.6752663850784302, 2.6761491298675537, 4.351415634155273
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6745352745056152 3.450993299484253 5.125528335571289
Loss :  1.672057867050171 3.47360897064209 5.14566707611084
Loss :  1.6807031631469727 2.8636841773986816 4.544387340545654
Loss :  1.6622014045715332 2.5806620121002197 4.242863655090332
Loss :  1.6586049795150757 2.7453513145446777 4.403956413269043
Loss :  5.032838344573975 4.459908962249756 9.49274730682373
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  4.937027931213379 4.427164077758789 9.364192008972168
Loss :  5.0206379890441895 4.31097936630249 9.33161735534668
Loss :  4.22727632522583 4.23933219909668 8.466608047485352
Total LOSS train 4.467250831310565 valid 9.163791179656982
CE LOSS train 1.667167065693782 valid 1.0568190813064575
Contrastive LOSS train 2.800083750944871 valid 1.05983304977417
EPOCH 107:
Loss :  1.6542062759399414 2.690932035446167 4.3451385498046875
Loss :  1.6683603525161743 3.0526540279388428 4.721014499664307
Loss :  1.6541357040405273 2.6335625648498535 4.287698268890381
Loss :  1.657865285873413 2.855026960372925 4.512892246246338
Loss :  1.6829224824905396 3.1973257064819336 4.880248069763184
Loss :  1.6655945777893066 2.5896334648132324 4.255228042602539
Loss :  1.662706971168518 3.0802865028381348 4.742993354797363
Loss :  1.6539381742477417 2.429395914077759 4.083333969116211
Loss :  1.6580294370651245 2.7061071395874023 4.364136695861816
Loss :  1.6120530366897583 3.3167576789855957 4.9288105964660645
Loss :  1.6723806858062744 2.8960213661193848 4.568402290344238
Loss :  1.7304660081863403 3.3588967323303223 5.089362621307373
Loss :  1.6743258237838745 2.9075746536254883 4.581900596618652
Loss :  1.6669906377792358 3.483074903488159 5.1500654220581055
Loss :  1.6468007564544678 2.6484835147857666 4.295284271240234
Loss :  1.653849482536316 3.550548791885376 5.204398155212402
Loss :  1.6633671522140503 2.9295425415039062 4.592909812927246
Loss :  1.662895679473877 3.0615599155426025 4.724455833435059
Loss :  1.668537974357605 2.587721109390259 4.256258964538574
Loss :  1.6275768280029297 3.0762827396392822 4.703859329223633
  batch 20 loss: 1.6275768280029297, 3.0762827396392822, 4.703859329223633
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6631723642349243 3.171290874481201 4.834463119506836
Loss :  1.6783263683319092 2.7270398139953613 4.405365943908691
Loss :  1.649550437927246 2.8970484733581543 4.5465989112854
Loss :  1.6902897357940674 2.881906747817993 4.5721964836120605
Loss :  1.6870955228805542 3.3794476985931396 5.066543102264404
Loss :  1.6532526016235352 2.655308723449707 4.308561325073242
Loss :  1.6996874809265137 3.6164116859436035 5.316099166870117
Loss :  1.6433521509170532 2.9163730144500732 4.559725284576416
Loss :  1.6905447244644165 2.8049769401550293 4.495521545410156
Loss :  1.6449538469314575 3.2291653156280518 4.874119281768799
Loss :  1.7245451211929321 3.0461695194244385 4.77071475982666
Loss :  1.671709418296814 3.1330225467681885 4.804731845855713
Loss :  1.6574994325637817 2.704535484313965 4.362034797668457
Loss :  1.6599299907684326 3.052391529083252 4.7123212814331055
Loss :  1.6987932920455933 3.162954807281494 4.861748218536377
Loss :  1.6896870136260986 2.833897829055786 4.523584842681885
Loss :  1.669294834136963 2.7998509407043457 4.469145774841309
Loss :  1.6368818283081055 3.0028202533721924 4.639701843261719
Loss :  1.6632291078567505 2.7836084365844727 4.446837425231934
Loss :  1.655703067779541 3.210297107696533 4.866000175476074
  batch 40 loss: 1.655703067779541, 3.210297107696533, 4.866000175476074
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6625266075134277 3.292550802230835 4.955077171325684
Loss :  1.6502920389175415 3.2988035678863525 4.949095726013184
Loss :  1.668420672416687 3.2006449699401855 4.869065761566162
Loss :  1.6660794019699097 2.74027943611145 4.40635871887207
Loss :  1.668313980102539 2.990642547607422 4.658956527709961
Loss :  1.6610944271087646 3.3892643451690674 5.050358772277832
Loss :  1.649336814880371 3.2838869094848633 4.933223724365234
Loss :  1.6593685150146484 3.4337098598480225 5.09307861328125
Loss :  1.6331994533538818 3.373375415802002 5.006574630737305
Loss :  1.6832407712936401 2.8437228202819824 4.526963710784912
Loss :  1.6490838527679443 3.4721832275390625 5.121267318725586
Loss :  1.6679848432540894 3.0796189308166504 4.747603893280029
Loss :  1.6850768327713013 2.7951724529266357 4.480249404907227
Loss :  1.668959617614746 3.057913064956665 4.726872444152832
Loss :  1.6736496686935425 3.2532296180725098 4.926879405975342
Loss :  1.6402080059051514 2.789700984954834 4.429908752441406
Loss :  1.6879723072052002 2.9775278568267822 4.665500164031982
Loss :  1.6829006671905518 2.891879081726074 4.574779510498047
Loss :  1.6980589628219604 2.980466604232788 4.678525447845459
Loss :  1.6748640537261963 2.9318268299102783 4.606690883636475
  batch 60 loss: 1.6748640537261963, 2.9318268299102783, 4.606690883636475
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6742889881134033 3.223147392272949 4.897436141967773
Loss :  1.6718188524246216 3.258277177810669 4.93009614944458
Loss :  1.6804522275924683 2.93635630607605 4.6168084144592285
Loss :  1.6619770526885986 2.686859369277954 4.348836421966553
Loss :  1.6582789421081543 2.2777798175811768 3.936058759689331
Loss :  5.038788318634033 4.434035778045654 9.472824096679688
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  4.943831920623779 4.404155254364014 9.347987174987793
Loss :  5.045662879943848 4.3110575675964355 9.356719970703125
Loss :  4.231043338775635 4.195733547210693 8.426776885986328
Total LOSS train 4.674779587525588 valid 9.151077032089233
CE LOSS train 1.6667992188380314 valid 1.0577608346939087
Contrastive LOSS train 3.007980390695425 valid 1.0489333868026733
EPOCH 108:
Loss :  1.6537963151931763 2.9034345149993896 4.5572309494018555
Loss :  1.6681725978851318 3.532198667526245 5.200371265411377
Loss :  1.6540157794952393 2.2948238849639893 3.9488396644592285
Loss :  1.6574128866195679 2.9543187618255615 4.61173152923584
Loss :  1.6826752424240112 2.893393039703369 4.57606840133667
Loss :  1.6653121709823608 2.494703769683838 4.160016059875488
Loss :  1.6626355648040771 2.664613962173462 4.327249526977539
Loss :  1.6538275480270386 2.5040132999420166 4.157840728759766
Loss :  1.6579452753067017 2.523489475250244 4.181434631347656
Loss :  1.6121164560317993 2.9570963382720947 4.569212913513184
Loss :  1.6721590757369995 2.6856331825256348 4.357792377471924
Loss :  1.7298821210861206 3.649026393890381 5.378908634185791
Loss :  1.674258828163147 3.4075002670288086 5.081758975982666
Loss :  1.6668840646743774 3.3948700428009033 5.06175422668457
Loss :  1.6466315984725952 2.674663543701172 4.321295261383057
Loss :  1.6539316177368164 2.984790802001953 4.6387224197387695
Loss :  1.6630619764328003 2.8052773475646973 4.468339443206787
Loss :  1.662905216217041 2.650158643722534 4.313063621520996
Loss :  1.6681456565856934 2.713798999786377 4.38194465637207
Loss :  1.6275490522384644 3.0737645626068115 4.701313495635986
  batch 20 loss: 1.6275490522384644, 3.0737645626068115, 4.701313495635986
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.663149118423462 2.932213544845581 4.595362663269043
Loss :  1.6781290769577026 2.795823097229004 4.473952293395996
Loss :  1.649820327758789 3.1556732654571533 4.805493354797363
Loss :  1.6898924112319946 2.816974401473999 4.506866931915283
Loss :  1.6869251728057861 3.381831645965576 5.068757057189941
Loss :  1.6535115242004395 2.7800276279449463 4.433539390563965
Loss :  1.6995784044265747 3.1075351238250732 4.8071136474609375
Loss :  1.6434305906295776 2.992636203765869 4.636066913604736
Loss :  1.6904182434082031 2.5650174617767334 4.255435943603516
Loss :  1.6448990106582642 3.3139278888702393 4.958827018737793
Loss :  1.7244421243667603 3.225330114364624 4.949772357940674
Loss :  1.67147958278656 3.446120262145996 5.117599964141846
Loss :  1.6574100255966187 2.8731510639190674 4.5305609703063965
Loss :  1.660134196281433 2.9680213928222656 4.628155708312988
Loss :  1.6985608339309692 3.0169312953948975 4.715492248535156
Loss :  1.6896405220031738 3.2508902549743652 4.940530776977539
Loss :  1.6692801713943481 2.7621188163757324 4.431398868560791
Loss :  1.6366366147994995 2.8086040019989014 4.445240497589111
Loss :  1.6629719734191895 2.529846668243408 4.192818641662598
Loss :  1.6558061838150024 3.1813855171203613 4.837191581726074
  batch 40 loss: 1.6558061838150024, 3.1813855171203613, 4.837191581726074
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6624664068222046 3.218295097351074 4.880761623382568
Loss :  1.650092601776123 3.199246644973755 4.849339485168457
Loss :  1.6685363054275513 2.9450294971466064 4.613565921783447
Loss :  1.6659834384918213 2.905726432800293 4.571709632873535
Loss :  1.668311595916748 2.9544756412506104 4.6227874755859375
Loss :  1.6611212491989136 3.574605703353882 5.235726833343506
Loss :  1.6491763591766357 3.2313687801361084 4.880545139312744
Loss :  1.6594014167785645 2.8660502433776855 4.52545166015625
Loss :  1.6332128047943115 3.2939486503601074 4.92716121673584
Loss :  1.683211326599121 2.958960771560669 4.642171859741211
Loss :  1.6494334936141968 3.4851691722869873 5.1346025466918945
Loss :  1.668013572692871 3.1254799365997314 4.793493270874023
Loss :  1.685139536857605 2.8460798263549805 4.531219482421875
Loss :  1.6691261529922485 2.76436185836792 4.433487892150879
Loss :  1.6736823320388794 3.253267526626587 4.926949977874756
Loss :  1.6403099298477173 2.7742207050323486 4.4145307540893555
Loss :  1.688097596168518 3.4015743732452393 5.089672088623047
Loss :  1.6830028295516968 3.1468052864074707 4.829808235168457
Loss :  1.6982543468475342 3.152132511138916 4.850386619567871
Loss :  1.6748747825622559 3.164963483810425 4.839838027954102
  batch 60 loss: 1.6748747825622559, 3.164963483810425, 4.839838027954102
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6743528842926025 3.4172873497009277 5.091640472412109
Loss :  1.6717718839645386 3.110536813735962 4.782308578491211
Loss :  1.6803771257400513 3.3149430751800537 4.9953203201293945
Loss :  1.6620335578918457 2.962735176086426 4.6247687339782715
Loss :  1.6583425998687744 2.444753408432007 4.103096008300781
Loss :  5.04289436340332 4.377821922302246 9.420716285705566
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  4.952115058898926 4.388804912567139 9.340919494628906
Loss :  5.043697834014893 4.203477382659912 9.247175216674805
Loss :  4.236774444580078 4.243020534515381 8.479795455932617
Total LOSS train 4.669467837993915 valid 9.122151613235474
CE LOSS train 1.6667347889680129 valid 1.0591936111450195
Contrastive LOSS train 3.0027330325200006 valid 1.0607551336288452
EPOCH 109:
Loss :  1.6539205312728882 2.6649765968322754 4.318897247314453
Loss :  1.66832435131073 3.1258223056793213 4.794146537780762
Loss :  1.654015064239502 2.548759937286377 4.202775001525879
Loss :  1.657371163368225 3.187608242034912 4.844979286193848
Loss :  1.6827102899551392 3.3335516452789307 5.016262054443359
Loss :  1.6651893854141235 2.9779279232025146 4.643117427825928
Loss :  1.662620186805725 3.1084468364715576 4.771067142486572
Loss :  1.6537818908691406 2.760709524154663 4.414491653442383
Loss :  1.6577541828155518 2.723064661026001 4.380818843841553
Loss :  1.6119887828826904 3.5036306381225586 5.115619659423828
Loss :  1.6721769571304321 3.1147475242614746 4.786924362182617
Loss :  1.730013370513916 3.655599355697632 5.385612487792969
Loss :  1.674250602722168 3.2610535621643066 4.935304164886475
Loss :  1.666808843612671 3.659564256668091 5.326373100280762
Loss :  1.6465092897415161 2.9354047775268555 4.581913948059082
Loss :  1.6538984775543213 3.365779161453247 5.019677639007568
Loss :  1.6630454063415527 3.113381862640381 4.776427268981934
Loss :  1.663004994392395 3.3513455390930176 5.014350414276123
Loss :  1.66826593875885 2.852725028991699 4.52099084854126
Loss :  1.6272342205047607 3.2069945335388184 4.834228515625
  batch 20 loss: 1.6272342205047607, 3.2069945335388184, 4.834228515625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6629506349563599 3.210747718811035 4.8736982345581055
Loss :  1.6780517101287842 3.1860151290893555 4.864067077636719
Loss :  1.6495143175125122 3.1352081298828125 4.784722328186035
Loss :  1.6899793148040771 2.8297033309936523 4.519682884216309
Loss :  1.6869010925292969 3.390805244445801 5.077706336975098
Loss :  1.6533191204071045 3.125868797302246 4.77918815612793
Loss :  1.6998047828674316 3.204179525375366 4.903984069824219
Loss :  1.6432396173477173 3.501446008682251 5.144685745239258
Loss :  1.6904619932174683 2.9735238552093506 4.663985729217529
Loss :  1.6447676420211792 3.321578025817871 4.96634578704834
Loss :  1.7246534824371338 3.225492238998413 4.950145721435547
Loss :  1.671530842781067 3.2089884281158447 4.880519390106201
Loss :  1.657439112663269 3.066709041595459 4.724148273468018
Loss :  1.6600894927978516 3.3978822231292725 5.057971954345703
Loss :  1.6985753774642944 3.1821136474609375 4.8806891441345215
Loss :  1.6895909309387207 3.467601776123047 5.157192707061768
Loss :  1.6692606210708618 3.1332926750183105 4.802553176879883
Loss :  1.6366711854934692 2.9814600944519043 4.618131160736084
Loss :  1.6630427837371826 3.083725690841675 4.746768474578857
Loss :  1.6558804512023926 2.712122917175293 4.3680033683776855
  batch 40 loss: 1.6558804512023926, 2.712122917175293, 4.3680033683776855
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6624902486801147 3.295361280441284 4.957851409912109
Loss :  1.6500160694122314 3.1466546058654785 4.796670913696289
Loss :  1.6683851480484009 3.0859768390655518 4.754362106323242
Loss :  1.6658610105514526 2.6983234882354736 4.364184379577637
Loss :  1.668337345123291 2.76845383644104 4.43679141998291
Loss :  1.6612194776535034 3.155005693435669 4.816225051879883
Loss :  1.6491683721542358 3.0448081493377686 4.693976402282715
Loss :  1.6593854427337646 2.9527344703674316 4.612119674682617
Loss :  1.6332776546478271 3.3289718627929688 4.962249755859375
Loss :  1.6832168102264404 3.2945175170898438 4.977734565734863
Loss :  1.6494196653366089 3.3054471015930176 4.954866886138916
Loss :  1.668265700340271 3.13224458694458 4.800510406494141
Loss :  1.6851320266723633 2.7577455043792725 4.442877769470215
Loss :  1.6691608428955078 2.954442262649536 4.623602867126465
Loss :  1.6736502647399902 3.483372211456299 5.157022476196289
Loss :  1.6401145458221436 2.8468923568725586 4.487007141113281
Loss :  1.6883269548416138 3.160599708557129 4.848926544189453
Loss :  1.683064579963684 3.0513999462127686 4.734464645385742
Loss :  1.6984347105026245 3.1782729625701904 4.876707553863525
Loss :  1.6750558614730835 3.0131795406341553 4.688235282897949
  batch 60 loss: 1.6750558614730835, 3.0131795406341553, 4.688235282897949
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6745498180389404 2.992351770401001 4.666901588439941
Loss :  1.6720073223114014 2.844611167907715 4.516618728637695
Loss :  1.6805087327957153 3.2630889415740967 4.943597793579102
Loss :  1.6622360944747925 2.680295467376709 4.342531681060791
Loss :  1.658560037612915 2.594085931777954 4.252645969390869
Loss :  5.097161293029785 4.432343482971191 9.529504776000977
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.010723114013672 4.411713123321533 9.422435760498047
Loss :  5.092216491699219 4.289731979370117 9.381948471069336
Loss :  4.275691509246826 4.183725833892822 8.459417343139648
Total LOSS train 4.771643851353572 valid 9.198326587677002
CE LOSS train 1.6667454334405751 valid 1.0689228773117065
Contrastive LOSS train 3.1048983940711388 valid 1.0459314584732056
EPOCH 110:
Loss :  1.6539928913116455 2.7691519260406494 4.423144817352295
Loss :  1.668424367904663 3.1161246299743652 4.784548759460449
Loss :  1.6538357734680176 2.382901430130005 4.036737442016602
Loss :  1.6573243141174316 2.975972890853882 4.633296966552734
Loss :  1.683075189590454 3.4191086292266846 5.102183818817139
Loss :  1.665266990661621 2.671072483062744 4.336339473724365
Loss :  1.6628429889678955 3.1335997581481934 4.796442985534668
Loss :  1.653841257095337 2.4290359020233154 4.082877159118652
Loss :  1.657915711402893 2.5066592693328857 4.164575099945068
Loss :  1.612319827079773 2.8105661869049072 4.422885894775391
Loss :  1.6723343133926392 2.796549081802368 4.468883514404297
Loss :  1.7301206588745117 3.5655629634857178 5.295683860778809
Loss :  1.6744879484176636 3.04058837890625 4.715076446533203
Loss :  1.6669944524765015 3.233788251876831 4.900782585144043
Loss :  1.646736741065979 2.7953317165374756 4.442068576812744
Loss :  1.6540329456329346 3.1109511852264404 4.764984130859375
Loss :  1.663350224494934 3.0492329597473145 4.712583065032959
Loss :  1.6630603075027466 2.8470582962036133 4.51011848449707
Loss :  1.668394684791565 2.8522021770477295 4.520596981048584
Loss :  1.6276172399520874 2.828547477722168 4.456164836883545
  batch 20 loss: 1.6276172399520874, 2.828547477722168, 4.456164836883545
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6631057262420654 2.985295295715332 4.648401260375977
Loss :  1.6782230138778687 2.6853342056274414 4.3635573387146
Loss :  1.6499279737472534 3.072610378265381 4.722538471221924
Loss :  1.6899741888046265 2.629868507385254 4.31984281539917
Loss :  1.6871140003204346 3.40678071975708 5.093894958496094
Loss :  1.653460144996643 2.6072661876678467 4.260726451873779
Loss :  1.6998999118804932 3.396843671798706 5.096743583679199
Loss :  1.6436524391174316 2.7937872409820557 4.437439918518066
Loss :  1.690830945968628 2.8337252140045166 4.5245561599731445
Loss :  1.6448992490768433 3.4100053310394287 5.054904460906982
Loss :  1.7247660160064697 2.8235092163085938 4.548274993896484
Loss :  1.6716886758804321 3.4519357681274414 5.123624324798584
Loss :  1.6575140953063965 2.6870462894439697 4.344560623168945
Loss :  1.6604443788528442 2.778093099594116 4.43853759765625
Loss :  1.6989983320236206 2.9191203117370605 4.618118762969971
Loss :  1.690024495124817 3.1082613468170166 4.798285961151123
Loss :  1.6695349216461182 2.704158306121826 4.373693466186523
Loss :  1.6368335485458374 3.0477750301361084 4.684608459472656
Loss :  1.6629066467285156 2.661921262741089 4.324828147888184
Loss :  1.6558688879013062 3.280409097671509 4.936277866363525
  batch 40 loss: 1.6558688879013062, 3.280409097671509, 4.936277866363525
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6625969409942627 3.1393492221832275 4.80194616317749
Loss :  1.6503936052322388 3.1537110805511475 4.804104804992676
Loss :  1.6687637567520142 3.116971492767334 4.785735130310059
Loss :  1.6659564971923828 2.718655824661255 4.384612083435059
Loss :  1.6684951782226562 2.5355546474456787 4.204050064086914
Loss :  1.661623477935791 3.519728422164917 5.181351661682129
Loss :  1.6494028568267822 3.3803422451019287 5.029745101928711
Loss :  1.6594902276992798 3.1702702045440674 4.829760551452637
Loss :  1.633300542831421 3.297084331512451 4.930384635925293
Loss :  1.6836788654327393 3.1916375160217285 4.875316619873047
Loss :  1.6493980884552002 3.023627519607544 4.673025608062744
Loss :  1.6680009365081787 3.5810799598693848 5.249080657958984
Loss :  1.685201644897461 2.8877904415130615 4.572992324829102
Loss :  1.669115662574768 3.196760654449463 4.865876197814941
Loss :  1.6737431287765503 3.4789934158325195 5.152736663818359
Loss :  1.6400338411331177 3.1506354808807373 4.7906694412231445
Loss :  1.6883083581924438 2.8824455738067627 4.570754051208496
Loss :  1.6832334995269775 2.9889090061187744 4.672142505645752
Loss :  1.6984868049621582 3.359808921813965 5.058295726776123
Loss :  1.674984335899353 2.686490774154663 4.361474990844727
  batch 60 loss: 1.674984335899353, 2.686490774154663, 4.361474990844727
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6745672225952148 3.562413215637207 5.236980438232422
Loss :  1.6721282005310059 3.591862201690674 5.26399040222168
Loss :  1.6806176900863647 3.162281036376953 4.842898845672607
Loss :  1.6621983051300049 2.8225576877593994 4.484755992889404
Loss :  1.6587716341018677 2.86649751663208 4.525269031524658
Loss :  5.242697715759277 4.471177101135254 9.713874816894531
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.157649517059326 4.393734455108643 9.551383972167969
Loss :  5.248222827911377 4.23283052444458 9.481053352355957
Loss :  4.401012420654297 4.103879928588867 8.504892349243164
Total LOSS train 4.68355908027062 valid 9.312801122665405
CE LOSS train 1.666894272657541 valid 1.1002531051635742
Contrastive LOSS train 3.016664776435265 valid 1.0259699821472168
EPOCH 111:
Loss :  1.6540533304214478 3.0969276428222656 4.750980854034424
Loss :  1.6684880256652832 3.3549277782440186 5.023415565490723
Loss :  1.6542224884033203 2.73834228515625 4.39256477355957
Loss :  1.6577441692352295 2.9730300903320312 4.63077449798584
Loss :  1.6828774213790894 3.4053709506988525 5.088248252868652
Loss :  1.665741205215454 2.804918050765991 4.470659255981445
Loss :  1.6626708507537842 3.369418144226074 5.0320892333984375
Loss :  1.6538382768630981 2.918891668319702 4.57273006439209
Loss :  1.6579447984695435 2.9382641315460205 4.5962090492248535
Loss :  1.611804723739624 3.0457749366760254 4.65757942199707
Loss :  1.672457218170166 3.0981130599975586 4.770570278167725
Loss :  1.7306712865829468 3.633892059326172 5.364563465118408
Loss :  1.674978494644165 3.5579543113708496 5.232933044433594
Loss :  1.6669236421585083 3.4404122829437256 5.107336044311523
Loss :  1.6463921070098877 3.0096144676208496 4.656006813049316
Loss :  1.6543049812316895 3.075676202774048 4.729981422424316
Loss :  1.6635572910308838 3.2471814155578613 4.910738945007324
Loss :  1.6631195545196533 3.19970440864563 4.862823963165283
Loss :  1.669148325920105 2.8483011722564697 4.517449378967285
Loss :  1.627545714378357 3.5375959873199463 5.165141582489014
  batch 20 loss: 1.627545714378357, 3.5375959873199463, 5.165141582489014
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6627037525177002 3.2487776279449463 4.9114813804626465
Loss :  1.6784522533416748 3.124878168106079 4.803330421447754
Loss :  1.6493483781814575 2.9875905513763428 4.63693904876709
Loss :  1.6903634071350098 3.5697555541992188 5.2601189613342285
Loss :  1.6874979734420776 3.589221239089966 5.276719093322754
Loss :  1.653118371963501 2.859426975250244 4.512545585632324
Loss :  1.6997344493865967 3.432938575744629 5.132673263549805
Loss :  1.6433935165405273 3.2274210453033447 4.870814323425293
Loss :  1.690489649772644 2.9816606044769287 4.672150135040283
Loss :  1.644998550415039 3.296314239501953 4.941312789916992
Loss :  1.7249735593795776 3.144228219985962 4.86920166015625
Loss :  1.6717572212219238 3.4959261417388916 5.1676836013793945
Loss :  1.6573790311813354 2.8159539699554443 4.47333288192749
Loss :  1.660138487815857 3.2763640880584717 4.936502456665039
Loss :  1.6994553804397583 3.3895249366760254 5.088980197906494
Loss :  1.6898564100265503 3.4359617233276367 5.125818252563477
Loss :  1.6693897247314453 2.9875411987304688 4.656930923461914
Loss :  1.6365997791290283 3.321155309677124 4.957755088806152
Loss :  1.663334846496582 3.0426032543182373 4.705938339233398
Loss :  1.6557198762893677 3.210716485977173 4.86643648147583
  batch 40 loss: 1.6557198762893677, 3.210716485977173, 4.86643648147583
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.662690281867981 3.404170036315918 5.066860198974609
Loss :  1.6507372856140137 3.0846052169799805 4.735342502593994
Loss :  1.6687088012695312 3.209386110305786 4.878094673156738
Loss :  1.6666053533554077 2.9446065425872803 4.611211776733398
Loss :  1.6686995029449463 2.9612905979156494 4.629990100860596
Loss :  1.661134958267212 3.327594757080078 4.988729476928711
Loss :  1.6494808197021484 3.139326333999634 4.788806915283203
Loss :  1.6601073741912842 3.2946465015411377 4.954753875732422
Loss :  1.6325385570526123 3.520233392715454 5.152771949768066
Loss :  1.6841078996658325 3.294621706008911 4.978729724884033
Loss :  1.6494656801223755 3.271937131881714 4.921402931213379
Loss :  1.6680854558944702 3.5319721698760986 5.200057506561279
Loss :  1.6851483583450317 2.665126085281372 4.350274562835693
Loss :  1.6701239347457886 3.119563102722168 4.789687156677246
Loss :  1.6739609241485596 3.521819829940796 5.1957807540893555
Loss :  1.6404030323028564 2.9296793937683105 4.570082664489746
Loss :  1.6882749795913696 3.4773850440979004 5.1656599044799805
Loss :  1.6833879947662354 3.040276288986206 4.723664283752441
Loss :  1.698337197303772 3.3228297233581543 5.021166801452637
Loss :  1.6751375198364258 2.9542930126190186 4.629430770874023
  batch 60 loss: 1.6751375198364258, 2.9542930126190186, 4.629430770874023
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.674501657485962 3.2441771030426025 4.9186787605285645
Loss :  1.6723389625549316 3.4172873497009277 5.089626312255859
Loss :  1.6805205345153809 3.3060224056243896 4.986542701721191
Loss :  1.6616923809051514 2.968712091445923 4.630404472351074
Loss :  1.6584429740905762 2.2471208572387695 3.9055638313293457
Loss :  5.263781547546387 4.440627574920654 9.704408645629883
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.1791090965271 4.394413471221924 9.573522567749023
Loss :  5.26527738571167 4.2858662605285645 9.551143646240234
Loss :  4.424213409423828 4.368704795837402 8.79291820526123
Total LOSS train 4.850504236954909 valid 9.405498266220093
CE LOSS train 1.666951091472919 valid 1.106053352355957
Contrastive LOSS train 3.1835531344780557 valid 1.0921761989593506
EPOCH 112:
Loss :  1.6544371843338013 2.6183624267578125 4.272799491882324
Loss :  1.668243646621704 3.24469256401062 4.912936210632324
Loss :  1.6542984247207642 2.2757627964019775 3.9300613403320312
Loss :  1.6582872867584229 3.0783255100250244 4.736612796783447
Loss :  1.6830629110336304 3.6416385173797607 5.324701309204102
Loss :  1.6661372184753418 2.7707865238189697 4.436923980712891
Loss :  1.6624755859375 2.802813768386841 4.465289115905762
Loss :  1.6537225246429443 2.546221971511841 4.199944496154785
Loss :  1.6580541133880615 2.241472005844116 3.8995261192321777
Loss :  1.6115995645523071 2.764666795730591 4.3762664794921875
Loss :  1.6720763444900513 2.6790504455566406 4.351126670837402
Loss :  1.7307014465332031 3.184262275695801 4.914963722229004
Loss :  1.6743121147155762 3.140637159347534 4.814949035644531
Loss :  1.6667596101760864 3.0555875301361084 4.722347259521484
Loss :  1.6468640565872192 2.873948574066162 4.520812511444092
Loss :  1.6538628339767456 3.190581798553467 4.844444751739502
Loss :  1.6631858348846436 2.9613029956817627 4.624488830566406
Loss :  1.6626993417739868 2.9547007083892822 4.617400169372559
Loss :  1.6688830852508545 2.323242664337158 3.9921257495880127
Loss :  1.627504587173462 2.8310317993164062 4.458536148071289
  batch 20 loss: 1.627504587173462, 2.8310317993164062, 4.458536148071289
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6628855466842651 2.873955011367798 4.536840438842773
Loss :  1.67844820022583 2.745903491973877 4.424351692199707
Loss :  1.6496913433074951 2.72904896736145 4.378740310668945
Loss :  1.6905291080474854 2.629108190536499 4.319637298583984
Loss :  1.6875512599945068 3.425123691558838 5.112674713134766
Loss :  1.65312922000885 2.8937957286834717 4.546925067901611
Loss :  1.6998378038406372 2.940768241882324 4.640605926513672
Loss :  1.6433861255645752 2.6553590297698975 4.298745155334473
Loss :  1.6902724504470825 2.664454698562622 4.354727268218994
Loss :  1.644930362701416 3.193936586380005 4.8388671875
Loss :  1.7250263690948486 2.8114397525787354 4.536466121673584
Loss :  1.6716737747192383 3.177385091781616 4.849059104919434
Loss :  1.6573904752731323 2.526862859725952 4.184253215789795
Loss :  1.6603788137435913 2.6986489295959473 4.359027862548828
Loss :  1.6994688510894775 2.8644726276397705 4.563941478729248
Loss :  1.68987238407135 2.940732002258301 4.630604267120361
Loss :  1.669484257698059 2.7252843379974365 4.394768714904785
Loss :  1.636429786682129 2.689694881439209 4.326124668121338
Loss :  1.6631269454956055 2.509226083755493 4.1723527908325195
Loss :  1.6558144092559814 2.859656572341919 4.5154709815979
  batch 40 loss: 1.6558144092559814, 2.859656572341919, 4.5154709815979
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.662587285041809 2.9342093467712402 4.59679651260376
Loss :  1.65052330493927 3.0673224925994873 4.717845916748047
Loss :  1.668927788734436 2.897230386734009 4.566158294677734
Loss :  1.6662909984588623 2.5856597423553467 4.251950740814209
Loss :  1.668576955795288 2.2333781719207764 3.9019551277160645
Loss :  1.6612663269042969 2.7807440757751465 4.442010402679443
Loss :  1.6492466926574707 3.0530683994293213 4.702315330505371
Loss :  1.6599133014678955 2.8498802185058594 4.509793281555176
Loss :  1.6327534914016724 2.8082480430603027 4.4410014152526855
Loss :  1.6839284896850586 2.8481063842773438 4.532034873962402
Loss :  1.6498712301254272 3.0197603702545166 4.669631481170654
Loss :  1.6680762767791748 2.806562662124634 4.474638938903809
Loss :  1.6850107908248901 2.5966885089874268 4.281699180603027
Loss :  1.669542908668518 3.1178903579711914 4.78743314743042
Loss :  1.6741116046905518 2.8803069591522217 4.554418563842773
Loss :  1.640077829360962 2.840379238128662 4.480457305908203
Loss :  1.688134789466858 2.992873430252075 4.681008338928223
Loss :  1.6832091808319092 2.753735065460205 4.436944007873535
Loss :  1.698270320892334 3.2498810291290283 4.948151588439941
Loss :  1.6749967336654663 2.652008533477783 4.327005386352539
  batch 60 loss: 1.6749967336654663, 2.652008533477783, 4.327005386352539
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6742134094238281 3.355111598968506 5.029325008392334
Loss :  1.672168254852295 3.142554759979248 4.814723014831543
Loss :  1.6801178455352783 2.9803457260131836 4.660463333129883
Loss :  1.6618092060089111 2.4758963584899902 4.1377058029174805
Loss :  1.6584181785583496 2.6943769454956055 4.352795124053955
Loss :  5.3202409744262695 4.410538673400879 9.730779647827148
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.228487491607666 4.378255844116211 9.606742858886719
Loss :  5.319888591766357 4.3157877922058105 9.635676383972168
Loss :  4.481268882751465 4.245532512664795 8.726800918579102
Total LOSS train 4.518441578058096 valid 9.424999952316284
CE LOSS train 1.6669005907498873 valid 1.1203172206878662
Contrastive LOSS train 2.851540990976187 valid 1.0613831281661987
EPOCH 113:
Loss :  1.653903841972351 2.6298484802246094 4.28375244140625
Loss :  1.6679017543792725 3.3070592880249023 4.974961280822754
Loss :  1.6541410684585571 2.3943803310394287 4.048521518707275
Loss :  1.6579622030258179 2.8307409286499023 4.48870325088501
Loss :  1.6826590299606323 3.0709068775177 4.753565788269043
Loss :  1.6656925678253174 2.6161563396453857 4.281848907470703
Loss :  1.6622505187988281 2.7226786613464355 4.384929180145264
Loss :  1.6535377502441406 2.5815722942352295 4.235110282897949
Loss :  1.6578303575515747 2.466792345046997 4.124622821807861
Loss :  1.6116132736206055 3.1471328735351562 4.758746147155762
Loss :  1.671756625175476 2.819920539855957 4.491677284240723
Loss :  1.7303553819656372 3.068131446838379 4.798486709594727
Loss :  1.6744736433029175 3.2597851753234863 4.934258937835693
Loss :  1.6665359735488892 3.3002769947052 4.966813087463379
Loss :  1.646835207939148 2.891871929168701 4.538707256317139
Loss :  1.6535334587097168 3.1346473693847656 4.788180828094482
Loss :  1.663179636001587 2.644618034362793 4.307797431945801
Loss :  1.6625128984451294 2.9217112064361572 4.584224224090576
Loss :  1.6689077615737915 2.5537986755371094 4.222706317901611
Loss :  1.6274162530899048 3.129216432571411 4.7566328048706055
  batch 20 loss: 1.6274162530899048, 3.129216432571411, 4.7566328048706055
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.66265869140625 2.8403706550598145 4.5030293464660645
Loss :  1.6782697439193726 2.807737112045288 4.486006736755371
Loss :  1.6494050025939941 2.866025686264038 4.515430450439453
Loss :  1.6900501251220703 2.83396315574646 4.524013519287109
Loss :  1.6871187686920166 3.6245856285095215 5.311704635620117
Loss :  1.6528314352035522 3.2221996784210205 4.875030994415283
Loss :  1.6994285583496094 3.2995989322662354 4.999027252197266
Loss :  1.6432795524597168 3.179410219192505 4.822690010070801
Loss :  1.6901826858520508 2.676936388015747 4.367118835449219
Loss :  1.6445717811584473 3.245155096054077 4.889726638793945
Loss :  1.7247306108474731 3.027322292327881 4.7520527839660645
Loss :  1.6712135076522827 3.532970428466797 5.204184055328369
Loss :  1.6571353673934937 2.632380962371826 4.289516448974609
Loss :  1.6599278450012207 3.1660308837890625 4.825958728790283
Loss :  1.698906421661377 2.849785566329956 4.548691749572754
Loss :  1.6896004676818848 3.0193188190460205 4.708919525146484
Loss :  1.6691522598266602 2.7928318977355957 4.461984157562256
Loss :  1.6360092163085938 2.945714235305786 4.581723213195801
Loss :  1.6628272533416748 2.7972824573516846 4.460109710693359
Loss :  1.6553609371185303 3.0286407470703125 4.684001922607422
  batch 40 loss: 1.6553609371185303, 3.0286407470703125, 4.684001922607422
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6622077226638794 3.3129498958587646 4.975157737731934
Loss :  1.6501649618148804 3.1069061756134033 4.757071018218994
Loss :  1.668532133102417 3.1945154666900635 4.8630475997924805
Loss :  1.6661401987075806 2.8698699474334717 4.536010265350342
Loss :  1.6682696342468262 2.6592657566070557 4.327535629272461
Loss :  1.6608479022979736 3.037344455718994 4.698192596435547
Loss :  1.648988127708435 3.4788920879364014 5.127880096435547
Loss :  1.659569263458252 3.3961997032165527 5.055768966674805
Loss :  1.6321818828582764 3.3099350929260254 4.942116737365723
Loss :  1.6836965084075928 3.076493740081787 4.760190010070801
Loss :  1.6493134498596191 3.180241823196411 4.829555511474609
Loss :  1.6675848960876465 3.0905072689056396 4.758091926574707
Loss :  1.684739112854004 2.7202565670013428 4.404995918273926
Loss :  1.6694438457489014 3.342334747314453 5.011778831481934
Loss :  1.6737056970596313 3.3159689903259277 4.9896745681762695
Loss :  1.6398100852966309 2.833972454071045 4.473782539367676
Loss :  1.6878677606582642 3.4376027584075928 5.1254706382751465
Loss :  1.6830682754516602 2.9656460285186768 4.648714065551758
Loss :  1.6979719400405884 3.246393918991089 4.944365978240967
Loss :  1.6745529174804688 2.9802405834198 4.654793739318848
  batch 60 loss: 1.6745529174804688, 2.9802405834198, 4.654793739318848
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6741355657577515 3.400099277496338 5.074234962463379
Loss :  1.671872615814209 3.3958702087402344 5.067742824554443
Loss :  1.680318832397461 3.2435672283172607 4.923886299133301
Loss :  1.661503553390503 3.0905420780181885 4.752045631408691
Loss :  1.6582335233688354 2.5950944423675537 4.2533278465271
Loss :  5.3739013671875 4.449207782745361 9.823108673095703
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.283133029937744 4.393910884857178 9.677043914794922
Loss :  5.372758865356445 4.2656426429748535 9.63840103149414
Loss :  4.523197174072266 4.2581095695495605 8.781307220458984
Total LOSS train 4.684470756237324 valid 9.479965209960938
CE LOSS train 1.6666212283647976 valid 1.1307992935180664
Contrastive LOSS train 3.017849504030668 valid 1.0645273923873901
EPOCH 114:
Loss :  1.6540369987487793 2.722769021987915 4.376806259155273
Loss :  1.6679372787475586 3.2242329120635986 4.892169952392578
Loss :  1.654146432876587 2.690768003463745 4.344914436340332
Loss :  1.6578779220581055 3.1468617916107178 4.804739952087402
Loss :  1.6825064420700073 3.3367271423339844 5.019233703613281
Loss :  1.6656664609909058 2.736548662185669 4.402215003967285
Loss :  1.66189706325531 2.859529733657837 4.521426677703857
Loss :  1.6531659364700317 3.1200692653656006 4.773235321044922
Loss :  1.657482385635376 2.823537588119507 4.481019973754883
Loss :  1.611080527305603 3.211158037185669 4.822238445281982
Loss :  1.6719727516174316 3.149697780609131 4.8216705322265625
Loss :  1.7304056882858276 3.1679952144622803 4.898400783538818
Loss :  1.6744260787963867 3.56811261177063 5.2425384521484375
Loss :  1.6663110256195068 3.5515682697296143 5.217879295349121
Loss :  1.6461727619171143 3.535980463027954 5.182153224945068
Loss :  1.6535794734954834 3.2363150119781494 4.889894485473633
Loss :  1.6628509759902954 3.0194590091705322 4.682310104370117
Loss :  1.6623213291168213 3.5830981731414795 5.245419502258301
Loss :  1.668850064277649 2.900930881500244 4.5697808265686035
Loss :  1.6267054080963135 3.2668635845184326 4.893568992614746
  batch 20 loss: 1.6267054080963135, 3.2668635845184326, 4.893568992614746
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6622142791748047 3.0183982849121094 4.680612564086914
Loss :  1.6781309843063354 3.0027098655700684 4.680840969085693
Loss :  1.6486914157867432 3.232814073562622 4.881505489349365
Loss :  1.6899265050888062 3.5254807472229004 5.215407371520996
Loss :  1.6870248317718506 3.6345255374908447 5.321550369262695
Loss :  1.6524837017059326 3.242117166519165 4.894600868225098
Loss :  1.6992541551589966 3.1149075031280518 4.814161777496338
Loss :  1.6426241397857666 3.009369373321533 4.651993751525879
Loss :  1.6898036003112793 3.0641396045684814 4.75394344329834
Loss :  1.6443672180175781 3.4799110889434814 5.1242780685424805
Loss :  1.7246136665344238 3.20515513420105 4.9297685623168945
Loss :  1.6711934804916382 3.2500154972076416 4.92120885848999
Loss :  1.6568156480789185 2.721360921859741 4.378176689147949
Loss :  1.659620761871338 2.836982011795044 4.496603012084961
Loss :  1.6989065408706665 3.4053378105163574 5.104244232177734
Loss :  1.6892118453979492 3.4174318313598633 5.1066436767578125
Loss :  1.6689599752426147 2.9562673568725586 4.625227451324463
Loss :  1.635972023010254 3.03303599357605 4.669008255004883
Loss :  1.6628303527832031 2.867591142654419 4.530421257019043
Loss :  1.6551750898361206 3.013813018798828 4.668988227844238
  batch 40 loss: 1.6551750898361206, 3.013813018798828, 4.668988227844238
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6621437072753906 3.286473035812378 4.948616981506348
Loss :  1.650015950202942 3.26491641998291 4.9149322509765625
Loss :  1.668358325958252 3.2978639602661133 4.966222286224365
Loss :  1.6661088466644287 2.9619784355163574 4.628087043762207
Loss :  1.6685268878936768 2.4869744777679443 4.155501365661621
Loss :  1.6606720685958862 3.534999132156372 5.195671081542969
Loss :  1.6488475799560547 2.8135125637054443 4.462360382080078
Loss :  1.6599600315093994 3.2426435947418213 4.902603626251221
Loss :  1.6319001913070679 3.148057460784912 4.7799577713012695
Loss :  1.683854579925537 3.342769145965576 5.026623725891113
Loss :  1.6491209268569946 3.562140464782715 5.21126127243042
Loss :  1.6676485538482666 3.502544403076172 5.170192718505859
Loss :  1.6846176385879517 2.8252243995666504 4.5098419189453125
Loss :  1.6697840690612793 3.1211371421813965 4.790921211242676
Loss :  1.673674464225769 3.4165380001068115 5.090212345123291
Loss :  1.6398696899414062 3.003044605255127 4.642914295196533
Loss :  1.6877110004425049 3.2761642932891846 4.9638752937316895
Loss :  1.6831178665161133 3.1075360774993896 4.790654182434082
Loss :  1.6977866888046265 3.1907663345336914 4.888553142547607
Loss :  1.6746186017990112 3.016108274459839 4.6907267570495605
  batch 60 loss: 1.6746186017990112, 3.016108274459839, 4.6907267570495605
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6740936040878296 3.369621753692627 5.043715476989746
Loss :  1.6720250844955444 3.4078049659729004 5.079830169677734
Loss :  1.6805999279022217 3.2273330688476562 4.907933235168457
Loss :  1.6611465215682983 3.2224061489105225 4.883552551269531
Loss :  1.6581385135650635 2.3496413230895996 4.007780075073242
Loss :  5.293560028076172 4.431138515472412 9.724699020385742
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.197854518890381 4.438493251800537 9.636347770690918
Loss :  5.285649299621582 4.242305278778076 9.5279541015625
Loss :  4.45086145401001 4.3291778564453125 8.780038833618164
Total LOSS train 4.818205261230469 valid 9.417259931564331
CE LOSS train 1.6664854544859666 valid 1.1127153635025024
Contrastive LOSS train 3.1517197939065786 valid 1.0822944641113281
EPOCH 115:
Loss :  1.6542057991027832 2.6773979663848877 4.33160400390625
Loss :  1.667906403541565 3.097059726715088 4.764966011047363
Loss :  1.654334545135498 2.2893946170806885 3.9437291622161865
Loss :  1.658387541770935 3.1332671642303467 4.791654586791992
Loss :  1.68281888961792 3.6226611137390137 5.305480003356934
Loss :  1.6660690307617188 2.8078742027282715 4.47394323348999
Loss :  1.6618709564208984 3.0404510498046875 4.702322006225586
Loss :  1.6532609462738037 2.6530516147613525 4.306312561035156
Loss :  1.657568335533142 2.5417661666870117 4.199334621429443
Loss :  1.6110388040542603 3.314277172088623 4.925315856933594
Loss :  1.671639084815979 2.94623064994812 4.617869853973389
Loss :  1.7309647798538208 3.6551737785339355 5.386138439178467
Loss :  1.6741280555725098 3.1101534366607666 4.7842817306518555
Loss :  1.6661630868911743 3.1533315181732178 4.819494724273682
Loss :  1.6465811729431152 2.9883275032043457 4.634908676147461
Loss :  1.653412103652954 3.112750768661499 4.766162872314453
Loss :  1.6629756689071655 2.852959394454956 4.515934944152832
Loss :  1.6621382236480713 2.790062665939331 4.452200889587402
Loss :  1.6688618659973145 2.3894965648651123 4.058358192443848
Loss :  1.627029299736023 2.862842559814453 4.489871978759766
  batch 20 loss: 1.627029299736023, 2.862842559814453, 4.489871978759766
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.662410855293274 2.870136260986328 4.5325469970703125
Loss :  1.678260326385498 2.699265241622925 4.377525329589844
Loss :  1.6492000818252563 2.866286277770996 4.515486240386963
Loss :  1.6903749704360962 2.6202025413513184 4.310577392578125
Loss :  1.6874364614486694 3.4114503860473633 5.098886966705322
Loss :  1.6528074741363525 2.6076316833496094 4.260438919067383
Loss :  1.6994208097457886 2.9850685596466064 4.6844892501831055
Loss :  1.642854928970337 2.7861123085021973 4.428967475891113
Loss :  1.689664363861084 2.672170877456665 4.361835479736328
Loss :  1.64462411403656 3.0010077953338623 4.645631790161133
Loss :  1.7247520685195923 2.866417407989502 4.591169357299805
Loss :  1.6710546016693115 3.307755708694458 4.9788103103637695
Loss :  1.656769037246704 2.4031827449798584 4.0599517822265625
Loss :  1.660049319267273 2.7633275985717773 4.42337703704834
Loss :  1.699033498764038 2.989473581314087 4.688507080078125
Loss :  1.6894952058792114 2.7818846702575684 4.47137975692749
Loss :  1.6692146062850952 2.836815118789673 4.5060296058654785
Loss :  1.6357293128967285 2.699925184249878 4.335654258728027
Loss :  1.6627098321914673 2.693800926208496 4.356510639190674
Loss :  1.6554324626922607 3.2178142070770264 4.873246669769287
  batch 40 loss: 1.6554324626922607, 3.2178142070770264, 4.873246669769287
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6621233224868774 2.800502300262451 4.462625503540039
Loss :  1.6500166654586792 2.731703758239746 4.381720542907715
Loss :  1.6686713695526123 2.7635338306427 4.4322052001953125
Loss :  1.6659177541732788 2.5855441093444824 4.251461982727051
Loss :  1.668212652206421 2.4228529930114746 4.091065406799316
Loss :  1.6608697175979614 3.1795382499694824 4.840407848358154
Loss :  1.6486092805862427 3.111323356628418 4.759932518005371
Loss :  1.6595042943954468 3.1112265586853027 4.770730972290039
Loss :  1.6323480606079102 2.8123421669006348 4.444690227508545
Loss :  1.6838628053665161 2.8286478519439697 4.512510776519775
Loss :  1.6495965719223022 2.805260181427002 4.454856872558594
Loss :  1.6677017211914062 2.6781933307647705 4.345894813537598
Loss :  1.6846519708633423 2.783292293548584 4.467944145202637
Loss :  1.669202446937561 3.0755207538604736 4.744723320007324
Loss :  1.6739588975906372 3.101069450378418 4.775028228759766
Loss :  1.6394128799438477 2.73445200920105 4.373865127563477
Loss :  1.6879165172576904 2.7760732173919678 4.463989734649658
Loss :  1.6831233501434326 2.6400437355041504 4.323166847229004
Loss :  1.6981865167617798 2.9916040897369385 4.689790725708008
Loss :  1.6746026277542114 2.5092897415161133 4.183892250061035
  batch 60 loss: 1.6746026277542114, 2.5092897415161133, 4.183892250061035
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6739760637283325 3.0786521434783936 4.752628326416016
Loss :  1.6717606782913208 3.081786632537842 4.753547191619873
Loss :  1.6798709630966187 2.745692729949951 4.425563812255859
Loss :  1.661637544631958 2.644580602645874 4.306218147277832
Loss :  1.6582776308059692 2.515122175216675 4.173399925231934
Loss :  5.278770446777344 4.457976341247559 9.736746788024902
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.185049533843994 4.382705211639404 9.567754745483398
Loss :  5.278285980224609 4.216120719909668 9.494406700134277
Loss :  4.4457221031188965 4.33885383605957 8.784576416015625
Total LOSS train 4.537734878980197 valid 9.39587116241455
CE LOSS train 1.6665640189097477 valid 1.1114305257797241
Contrastive LOSS train 2.8711708765763504 valid 1.0847134590148926
EPOCH 116:
Loss :  1.6538642644882202 2.64475417137146 4.298618316650391
Loss :  1.6676676273345947 3.079827070236206 4.747494697570801
Loss :  1.6539394855499268 2.25380802154541 3.907747507095337
Loss :  1.6577543020248413 2.59464955329895 4.252403736114502
Loss :  1.6823861598968506 3.132321834564209 4.8147077560424805
Loss :  1.6654471158981323 2.586686372756958 4.252133369445801
Loss :  1.6620943546295166 2.6655325889587402 4.327627182006836
Loss :  1.6533008813858032 2.749925374984741 4.403226375579834
Loss :  1.6575578451156616 2.4901123046875 4.147670269012451
Loss :  1.6115397214889526 2.8114254474639893 4.422965049743652
Loss :  1.6714973449707031 2.6319680213928223 4.303465366363525
Loss :  1.7302868366241455 3.1551320552825928 4.885418891906738
Loss :  1.6743113994598389 2.887620210647583 4.561931610107422
Loss :  1.6662161350250244 2.9442927837371826 4.610508918762207
Loss :  1.6466693878173828 2.6105449199676514 4.257214546203613
Loss :  1.6534039974212646 2.8660600185394287 4.519464015960693
Loss :  1.66289222240448 2.801400899887085 4.464293003082275
Loss :  1.6623141765594482 2.68601393699646 4.348328113555908
Loss :  1.6687122583389282 2.3830504417419434 4.051762580871582
Loss :  1.6270430088043213 3.200031042098999 4.82707405090332
  batch 20 loss: 1.6270430088043213, 3.200031042098999, 4.82707405090332
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6624003648757935 2.928065776824951 4.590466022491455
Loss :  1.6780530214309692 2.45151686668396 4.129570007324219
Loss :  1.6491485834121704 2.782198429107666 4.431346893310547
Loss :  1.6899605989456177 2.6744930744171143 4.3644537925720215
Loss :  1.6871312856674194 3.4054055213928223 5.092536926269531
Loss :  1.652486801147461 2.562056541442871 4.214543342590332
Loss :  1.699191927909851 3.093665599822998 4.792857646942139
Loss :  1.6428438425064087 2.97344970703125 4.616293430328369
Loss :  1.6896886825561523 2.764561891555786 4.454250335693359
Loss :  1.644425630569458 3.246936559677124 4.891362190246582
Loss :  1.724455714225769 3.060572624206543 4.785028457641602
Loss :  1.6710214614868164 3.373692035675049 5.044713497161865
Loss :  1.6567225456237793 2.9816012382507324 4.638323783874512
Loss :  1.6595580577850342 2.8585658073425293 4.518123626708984
Loss :  1.6988370418548584 2.795318365097046 4.494155406951904
Loss :  1.6893097162246704 2.9372739791870117 4.626583576202393
Loss :  1.668991208076477 2.6404426097869873 4.309433937072754
Loss :  1.635641098022461 2.941861391067505 4.577502250671387
Loss :  1.6625252962112427 3.0519094467163086 4.714434623718262
Loss :  1.6550328731536865 3.0739939212799072 4.729026794433594
  batch 40 loss: 1.6550328731536865, 3.0739939212799072, 4.729026794433594
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6620309352874756 3.0489792823791504 4.711009979248047
Loss :  1.6498159170150757 2.8649494647979736 4.51476526260376
Loss :  1.6682682037353516 2.823354482650757 4.4916229248046875
Loss :  1.6659501791000366 2.7563531398773193 4.422303199768066
Loss :  1.6680657863616943 2.4549551010131836 4.123021125793457
Loss :  1.660556674003601 3.172417640686035 4.832974433898926
Loss :  1.6483303308486938 3.1141955852508545 4.762526035308838
Loss :  1.659403920173645 3.151892900466919 4.8112969398498535
Loss :  1.631932258605957 3.366673707962036 4.998605728149414
Loss :  1.6835122108459473 3.3068389892578125 4.99035120010376
Loss :  1.6490870714187622 3.1031904220581055 4.752277374267578
Loss :  1.6672049760818481 3.029158592224121 4.69636344909668
Loss :  1.6844347715377808 2.8823118209838867 4.566746711730957
Loss :  1.6692698001861572 2.9739274978637695 4.643197059631348
Loss :  1.6735905408859253 3.192706346511841 4.866296768188477
Loss :  1.6393725872039795 2.6827075481414795 4.322080135345459
Loss :  1.6875752210617065 3.4722657203674316 5.159841060638428
Loss :  1.6830143928527832 3.306133985519409 4.989148139953613
Loss :  1.6976948976516724 3.0168519020080566 4.7145466804504395
Loss :  1.6743484735488892 3.1385300159454346 4.812878608703613
  batch 60 loss: 1.6743484735488892, 3.1385300159454346, 4.812878608703613
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6736294031143188 3.3669514656066895 5.040580749511719
Loss :  1.671594500541687 3.3032760620117188 4.974870681762695
Loss :  1.6800119876861572 3.1963603496551514 4.876372337341309
Loss :  1.66109037399292 2.9283950328826904 4.589485168457031
Loss :  1.6578916311264038 2.3791017532348633 4.036993503570557
Loss :  5.362414360046387 4.430992603302002 9.793407440185547
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.261942386627197 4.406651973724365 9.668594360351562
Loss :  5.356317043304443 4.231691360473633 9.588008880615234
Loss :  4.521717548370361 4.171546459197998 8.69326400756836
Total LOSS train 4.586449494728675 valid 9.435818672180176
CE LOSS train 1.6663692511045016 valid 1.1304293870925903
Contrastive LOSS train 2.9200802656320426 valid 1.0428866147994995
EPOCH 117:
Loss :  1.6537861824035645 2.964308500289917 4.618094444274902
Loss :  1.667396903038025 3.2518954277038574 4.919292449951172
Loss :  1.6540722846984863 2.6186416149139404 4.272713661193848
Loss :  1.65830659866333 3.2335219383239746 4.891828536987305
Loss :  1.6825225353240967 3.057377815246582 4.739900588989258
Loss :  1.6657863855361938 3.0350162982940674 4.700802803039551
Loss :  1.6615252494812012 3.2875075340270996 4.949032783508301
Loss :  1.6528822183609009 2.9356744289398193 4.58855676651001
Loss :  1.657291293144226 2.9346108436584473 4.591902256011963
Loss :  1.6108216047286987 3.2076330184936523 4.818454742431641
Loss :  1.6715275049209595 2.814668893814087 4.486196517944336
Loss :  1.7307400703430176 3.6189520359039307 5.349692344665527
Loss :  1.6741353273391724 3.3555777072906494 5.029713153839111
Loss :  1.6659281253814697 3.646726369857788 5.312654495239258
Loss :  1.6462275981903076 3.106074810028076 4.752302169799805
Loss :  1.6533138751983643 2.9561421871185303 4.6094560623168945
Loss :  1.662723183631897 3.0395925045013428 4.702315807342529
Loss :  1.6621038913726807 3.1896474361419678 4.851751327514648
Loss :  1.6688838005065918 2.572216033935547 4.241099834442139
Loss :  1.6265448331832886 3.1223652362823486 4.748909950256348
  batch 20 loss: 1.6265448331832886, 3.1223652362823486, 4.748909950256348
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6619809865951538 3.169665575027466 4.83164644241333
Loss :  1.6780778169631958 2.670788288116455 4.348865985870361
Loss :  1.648662805557251 3.018153190612793 4.666815757751465
Loss :  1.6901094913482666 3.0631632804870605 4.753273010253906
Loss :  1.68739914894104 4.024875164031982 5.712274551391602
Loss :  1.6521252393722534 3.168449878692627 4.82057523727417
Loss :  1.699007511138916 3.3912100791931152 5.090217590332031
Loss :  1.642516851425171 2.8884994983673096 4.5310163497924805
Loss :  1.6893341541290283 2.7898776531219482 4.479211807250977
Loss :  1.644400954246521 3.5483922958374023 5.192793369293213
Loss :  1.7245326042175293 3.5504984855651855 5.275031089782715
Loss :  1.671169400215149 3.4276206493377686 5.098790168762207
Loss :  1.656575083732605 2.7610056400299072 4.417580604553223
Loss :  1.659419059753418 3.4237332344055176 5.0831522941589355
Loss :  1.6992112398147583 3.3523571491241455 5.051568508148193
Loss :  1.6893161535263062 3.5344033241271973 5.223719596862793
Loss :  1.668795108795166 2.9568779468536377 4.625673294067383
Loss :  1.6358414888381958 2.961148738861084 4.59699010848999
Loss :  1.662613868713379 2.7406349182128906 4.4032487869262695
Loss :  1.6549062728881836 3.4158880710601807 5.070794105529785
  batch 40 loss: 1.6549062728881836, 3.4158880710601807, 5.070794105529785
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6620904207229614 3.470085382461548 5.132175922393799
Loss :  1.6500358581542969 2.8950788974761963 4.545114517211914
Loss :  1.668325424194336 3.2588653564453125 4.927190780639648
Loss :  1.66606867313385 3.215343713760376 4.881412506103516
Loss :  1.6684727668762207 2.6341965198516846 4.302669525146484
Loss :  1.6605114936828613 3.1893954277038574 4.849906921386719
Loss :  1.648321270942688 3.1517837047576904 4.800105094909668
Loss :  1.6597734689712524 2.9824869632720947 4.642260551452637
Loss :  1.6316864490509033 3.2460105419158936 4.877696990966797
Loss :  1.6839622259140015 3.1871652603149414 4.871127605438232
Loss :  1.6490373611450195 3.3407938480377197 4.98983097076416
Loss :  1.6673082113265991 3.090427875518799 4.7577362060546875
Loss :  1.6843181848526 2.7907299995422363 4.475048065185547
Loss :  1.6698181629180908 2.947500467300415 4.617318630218506
Loss :  1.6735398769378662 3.0897927284240723 4.763332366943359
Loss :  1.6394096612930298 2.854644775390625 4.494054317474365
Loss :  1.6874361038208008 3.11914324760437 4.80657958984375
Loss :  1.6830681562423706 2.872051954269409 4.55511999130249
Loss :  1.697706937789917 3.1834442615509033 4.88115119934082
Loss :  1.6744993925094604 2.610309600830078 4.284809112548828
  batch 60 loss: 1.6744993925094604, 2.610309600830078, 4.284809112548828
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6735416650772095 3.5908100605010986 5.264351844787598
Loss :  1.67178475856781 3.1295905113220215 4.801375389099121
Loss :  1.6798797845840454 3.4717776775360107 5.151657581329346
Loss :  1.6609768867492676 3.2020676136016846 4.863044738769531
Loss :  1.6577752828598022 2.58544921875 4.243224620819092
Loss :  5.305705547332764 4.391963481903076 9.69766902923584
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.211700439453125 4.410516262054443 9.622217178344727
Loss :  5.3041276931762695 4.311282157897949 9.615409851074219
Loss :  4.468796730041504 4.19666862487793 8.665465354919434
Total LOSS train 4.788095452235296 valid 9.400190353393555
CE LOSS train 1.666336356676542 valid 1.117199182510376
Contrastive LOSS train 3.121759066214928 valid 1.0491671562194824
EPOCH 118:
Loss :  1.653734564781189 2.964125156402588 4.617859840393066
Loss :  1.6674275398254395 3.480898380279541 5.1483259201049805
Loss :  1.6538172960281372 2.4119749069213867 4.065792083740234
Loss :  1.658007264137268 3.112208843231201 4.77021598815918
Loss :  1.6825684309005737 3.26670241355896 4.949270725250244
Loss :  1.6656583547592163 2.5749220848083496 4.2405805587768555
Loss :  1.66166090965271 2.8908603191375732 4.552521228790283
Loss :  1.6528047323226929 2.806873083114624 4.459677696228027
Loss :  1.6572929620742798 2.714672803878784 4.3719658851623535
Loss :  1.6107854843139648 3.099623918533325 4.710409164428711
Loss :  1.6714608669281006 3.1267874240875244 4.798248291015625
Loss :  1.7304563522338867 3.4976868629455566 5.228143215179443
Loss :  1.6741278171539307 3.297045946121216 4.9711737632751465
Loss :  1.665916919708252 3.7979862689971924 5.463903427124023
Loss :  1.6463159322738647 3.1348392963409424 4.781155109405518
Loss :  1.6535125970840454 3.082475423812866 4.735988140106201
Loss :  1.6625627279281616 3.0747807025909424 4.7373433113098145
Loss :  1.6623554229736328 3.403934955596924 5.066290378570557
Loss :  1.6688305139541626 2.614295244216919 4.283125877380371
Loss :  1.6266028881072998 3.201254367828369 4.82785701751709
  batch 20 loss: 1.6266028881072998, 3.201254367828369, 4.82785701751709
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.66199791431427 3.0958447456359863 4.757842540740967
Loss :  1.6780445575714111 3.0121958255767822 4.690240383148193
Loss :  1.648685336112976 3.1557295322418213 4.804414749145508
Loss :  1.690190076828003 3.0799460411071777 4.770135879516602
Loss :  1.6873767375946045 3.375033140182495 5.0624098777771
Loss :  1.6523263454437256 2.8038089275360107 4.456135272979736
Loss :  1.6987998485565186 3.4406628608703613 5.139462471008301
Loss :  1.6426068544387817 2.9455831050872803 4.588190078735352
Loss :  1.6891095638275146 2.734166383743286 4.423275947570801
Loss :  1.6442056894302368 3.377180337905884 5.02138614654541
Loss :  1.7243791818618774 3.2726187705993652 4.996997833251953
Loss :  1.671036958694458 3.3135905265808105 4.984627723693848
Loss :  1.6565924882888794 3.307354211807251 4.96394681930542
Loss :  1.6593683958053589 2.693580389022827 4.3529486656188965
Loss :  1.6988030672073364 3.1133482456207275 4.8121514320373535
Loss :  1.689115285873413 2.7958567142486572 4.48497200012207
Loss :  1.6687061786651611 3.0941460132598877 4.762852191925049
Loss :  1.6355448961257935 3.0925607681274414 4.728105545043945
Loss :  1.6626516580581665 2.7205705642700195 4.3832221031188965
Loss :  1.6551321744918823 2.889984607696533 4.545116901397705
  batch 40 loss: 1.6551321744918823, 2.889984607696533, 4.545116901397705
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.661950945854187 2.719041585922241 4.380992412567139
Loss :  1.6497174501419067 2.893486499786377 4.543203830718994
Loss :  1.6681926250457764 2.912421464920044 4.58061408996582
Loss :  1.6657496690750122 2.5888986587524414 4.254648208618164
Loss :  1.6682425737380981 2.480074882507324 4.148317337036133
Loss :  1.6602767705917358 3.261240243911743 4.9215168952941895
Loss :  1.6482152938842773 3.3985531330108643 5.0467681884765625
Loss :  1.6594854593276978 3.4738640785217285 5.133349418640137
Loss :  1.6316546201705933 3.4890105724334717 5.120665073394775
Loss :  1.68380606174469 3.1697278022766113 4.853533744812012
Loss :  1.649113416671753 3.316357374191284 4.965470790863037
Loss :  1.6673789024353027 3.1770198345184326 4.844398498535156
Loss :  1.6843398809432983 2.8876264095306396 4.571966171264648
Loss :  1.6696373224258423 3.149914503097534 4.819551944732666
Loss :  1.6735427379608154 3.4453465938568115 5.118889331817627
Loss :  1.6396502256393433 2.6809680461883545 4.320618152618408
Loss :  1.6873458623886108 3.133979558944702 4.821325302124023
Loss :  1.6830329895019531 3.5383670330047607 5.221400260925293
Loss :  1.6978949308395386 3.332630157470703 5.030525207519531
Loss :  1.6745976209640503 3.0153305530548096 4.68992805480957
  batch 60 loss: 1.6745976209640503, 3.0153305530548096, 4.68992805480957
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6736477613449097 2.8342161178588867 4.507863998413086
Loss :  1.6718577146530151 3.280686855316162 4.952544689178467
Loss :  1.6799615621566772 3.1945912837982178 4.8745527267456055
Loss :  1.6609888076782227 2.9539361000061035 4.614924907684326
Loss :  1.657668113708496 2.645961284637451 4.303629398345947
Loss :  5.273482322692871 4.434934139251709 9.708415985107422
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.175929546356201 4.446882724761963 9.622812271118164
Loss :  5.275193214416504 4.289741516113281 9.564934730529785
Loss :  4.430273532867432 4.311020374298096 8.741293907165527
Total LOSS train 4.7407612433800335 valid 9.409364223480225
CE LOSS train 1.6662849554648766 valid 1.107568383216858
Contrastive LOSS train 3.0744763190929705 valid 1.077755093574524
EPOCH 119:
Loss :  1.6539406776428223 3.078183650970459 4.732124328613281
Loss :  1.6673990488052368 3.3233962059020996 4.990795135498047
Loss :  1.6539033651351929 2.3535609245300293 4.007464408874512
Loss :  1.6581015586853027 3.032498598098755 4.690600395202637
Loss :  1.68254816532135 3.3480422496795654 5.030590534210205
Loss :  1.665855050086975 3.0342965126037598 4.700151443481445
Loss :  1.6616215705871582 3.1880099773406982 4.849631309509277
Loss :  1.6529325246810913 2.674970865249634 4.3279032707214355
Loss :  1.6575124263763428 2.6781365871429443 4.335649013519287
Loss :  1.6110568046569824 3.2191765308380127 4.830233573913574
Loss :  1.6715418100357056 2.867520332336426 4.539062023162842
Loss :  1.7302888631820679 3.5966176986694336 5.326906681060791
Loss :  1.6739157438278198 3.476705312728882 5.150620937347412
Loss :  1.6659027338027954 3.538318634033203 5.204221248626709
Loss :  1.6464720964431763 2.6454713344573975 4.291943550109863
Loss :  1.653410792350769 3.2085204124450684 4.861931324005127
Loss :  1.6624698638916016 2.7306289672851562 4.393098831176758
Loss :  1.6623116731643677 3.376889705657959 5.039201259613037
Loss :  1.668601155281067 2.4889421463012695 4.157543182373047
Loss :  1.6267640590667725 3.070885181427002 4.697649002075195
  batch 20 loss: 1.6267640590667725, 3.070885181427002, 4.697649002075195
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.662173867225647 2.760176420211792 4.4223504066467285
Loss :  1.678170084953308 2.82521390914917 4.503384113311768
Loss :  1.6491299867630005 3.011139392852783 4.660269260406494
Loss :  1.690315842628479 2.9779319763183594 4.668247699737549
Loss :  1.6872097253799438 3.454362392425537 5.141571998596191
Loss :  1.6526297330856323 3.0721137523651123 4.724743366241455
Loss :  1.6989072561264038 3.614633798599243 5.313540935516357
Loss :  1.6426799297332764 2.8933587074279785 4.536038398742676
Loss :  1.6889821290969849 2.61211895942688 4.301101207733154
Loss :  1.6444895267486572 3.183419942855835 4.827909469604492
Loss :  1.7244701385498047 3.3414840698242188 5.065954208374023
Loss :  1.6709110736846924 3.5655672550201416 5.236478328704834
Loss :  1.6566171646118164 3.0387041568756104 4.695321083068848
Loss :  1.6595475673675537 3.3477320671081543 5.007279396057129
Loss :  1.6989494562149048 3.4144582748413086 5.113407611846924
Loss :  1.6892378330230713 2.937553882598877 4.626791954040527
Loss :  1.6689867973327637 2.7073068618774414 4.376293659210205
Loss :  1.635136365890503 2.808079242706299 4.443215370178223
Loss :  1.6626025438308716 2.586010217666626 4.248612880706787
Loss :  1.6553512811660767 2.926671266555786 4.582022666931152
  batch 40 loss: 1.6553512811660767, 2.926671266555786, 4.582022666931152
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6617507934570312 2.986424684524536 4.648175239562988
Loss :  1.6497128009796143 2.9630095958709717 4.612722396850586
Loss :  1.6685584783554077 3.166161060333252 4.834719657897949
Loss :  1.6657205820083618 2.554389238357544 4.220109939575195
Loss :  1.6680355072021484 2.556931972503662 4.2249674797058105
Loss :  1.6605371236801147 3.2980117797851562 4.9585490226745605
Loss :  1.648180603981018 3.2771193981170654 4.925300121307373
Loss :  1.6592738628387451 3.352586030960083 5.011859893798828
Loss :  1.6321648359298706 3.5266952514648438 5.158860206604004
Loss :  1.6840169429779053 3.1174705028533936 4.801487445831299
Loss :  1.649531364440918 2.843615770339966 4.493146896362305
Loss :  1.6675554513931274 3.002791404724121 4.670346736907959
Loss :  1.684513807296753 2.717843532562256 4.40235710144043
Loss :  1.6689698696136475 2.7769179344177246 4.445887565612793
Loss :  1.6739134788513184 3.1667709350585938 4.840684413909912
Loss :  1.6395008563995361 2.6694464683532715 4.308947563171387
Loss :  1.687704086303711 3.0970304012298584 4.784734725952148
Loss :  1.6829652786254883 2.9443633556365967 4.627328872680664
Loss :  1.6981148719787598 2.985405683517456 4.683520317077637
Loss :  1.674738883972168 2.6883795261383057 4.3631181716918945
  batch 60 loss: 1.674738883972168, 2.6883795261383057, 4.3631181716918945
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6738359928131104 3.3916778564453125 5.065513610839844
Loss :  1.6716285943984985 2.97269344329834 4.644321918487549
Loss :  1.6798958778381348 2.9473917484283447 4.627287864685059
Loss :  1.6612677574157715 2.6226415634155273 4.283909320831299
Loss :  1.6579872369766235 2.074512243270874 3.732499599456787
Loss :  5.296873092651367 4.338749408721924 9.635622024536133
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.198073863983154 4.334192752838135 9.532266616821289
Loss :  5.292623996734619 4.220482349395752 9.513106346130371
Loss :  4.456973552703857 4.344183921813965 8.801156997680664
Total LOSS train 4.677264793102558 valid 9.370537996292114
CE LOSS train 1.6663557419410118 valid 1.1142433881759644
Contrastive LOSS train 3.010909073169415 valid 1.0860459804534912
EPOCH 120:
Loss :  1.6539795398712158 2.438943386077881 4.092923164367676
Loss :  1.6673003435134888 2.9475626945495605 4.61486291885376
Loss :  1.6537977457046509 2.2832326889038086 3.93703031539917
Loss :  1.65810227394104 2.603832483291626 4.261934757232666
Loss :  1.6823877096176147 2.815129518508911 4.497517108917236
Loss :  1.6658459901809692 2.5244929790496826 4.190339088439941
Loss :  1.6618597507476807 2.82871150970459 4.490571022033691
Loss :  1.6530680656433105 2.547333240509033 4.200401306152344
Loss :  1.657731294631958 2.433493137359619 4.091224670410156
Loss :  1.611281156539917 2.6663968563079834 4.2776780128479
Loss :  1.6712992191314697 2.4575541019439697 4.1288533210754395
Loss :  1.7305229902267456 3.3429205417633057 5.073443412780762
Loss :  1.6739898920059204 2.949462413787842 4.623452186584473
Loss :  1.6660033464431763 3.137600898742676 4.8036041259765625
Loss :  1.6467963457107544 2.695112466812134 4.341908931732178
Loss :  1.6532224416732788 3.0965070724487305 4.749729633331299
Loss :  1.6627702713012695 2.9875943660736084 4.650364875793457
Loss :  1.6620261669158936 2.7454915046691895 4.407517433166504
Loss :  1.6686650514602661 2.6000683307647705 4.268733501434326
Loss :  1.627050518989563 2.8847672939300537 4.511817932128906
  batch 20 loss: 1.627050518989563, 2.8847672939300537, 4.511817932128906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6623802185058594 3.0022048950195312 4.664585113525391
Loss :  1.6781343221664429 2.6399800777435303 4.318114280700684
Loss :  1.6490802764892578 2.831425666809082 4.48050594329834
Loss :  1.6902499198913574 2.5970458984375 4.287295818328857
Loss :  1.6874948740005493 3.587308168411255 5.274803161621094
Loss :  1.6524995565414429 2.9732582569122314 4.625757694244385
Loss :  1.699135184288025 3.677173376083374 5.376308441162109
Loss :  1.6427675485610962 2.7421629428863525 4.384930610656738
Loss :  1.6894737482070923 2.8790743350982666 4.568548202514648
Loss :  1.6443265676498413 3.5564026832580566 5.2007293701171875
Loss :  1.7245879173278809 2.826537609100342 4.551125526428223
Loss :  1.671158790588379 3.1257071495056152 4.796865940093994
Loss :  1.6568374633789062 3.0421900749206543 4.6990275382995605
Loss :  1.6597087383270264 2.8374202251434326 4.497128963470459
Loss :  1.6992301940917969 2.9017748832702637 4.6010050773620605
Loss :  1.6894550323486328 3.0882909297943115 4.777746200561523
Loss :  1.6690056324005127 2.7376608848571777 4.4066667556762695
Loss :  1.6358360052108765 2.838456153869629 4.474292278289795
Loss :  1.6626694202423096 2.734970808029175 4.397640228271484
Loss :  1.6551262140274048 2.874377727508545 4.52950382232666
  batch 40 loss: 1.6551262140274048, 2.874377727508545, 4.52950382232666
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6618937253952026 3.1578078269958496 4.819701671600342
Loss :  1.650119662284851 2.7718615531921387 4.421981334686279
Loss :  1.668476939201355 3.3033905029296875 4.971867561340332
Loss :  1.6660304069519043 2.6465089321136475 4.312539100646973
Loss :  1.6682814359664917 2.558326482772827 4.226607799530029
Loss :  1.6604186296463013 3.156411647796631 4.816830158233643
Loss :  1.6486388444900513 3.267381191253662 4.916019916534424
Loss :  1.6595267057418823 2.9412615299224854 4.600788116455078
Loss :  1.6322108507156372 2.8249449729919434 4.457155704498291
Loss :  1.6839629411697388 3.01853084564209 4.702493667602539
Loss :  1.649146556854248 3.2441904544830322 4.893337249755859
Loss :  1.6674503087997437 3.3824925422668457 5.049942970275879
Loss :  1.6843546628952026 2.9880430698394775 4.672397613525391
Loss :  1.6692842245101929 3.112452983856201 4.781737327575684
Loss :  1.6735072135925293 3.553112030029297 5.226619243621826
Loss :  1.6396476030349731 3.2261030673980713 4.865750789642334
Loss :  1.6874991655349731 3.118865489959717 4.8063645362854
Loss :  1.6829168796539307 3.180710792541504 4.8636274337768555
Loss :  1.697883129119873 3.3201448917388916 5.018028259277344
Loss :  1.6745588779449463 3.061004400253296 4.735563278198242
  batch 60 loss: 1.6745588779449463, 3.061004400253296, 4.735563278198242
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.673738956451416 3.25608229637146 4.929821014404297
Loss :  1.6715492010116577 3.2648680210113525 4.936417102813721
Loss :  1.6802151203155518 3.150778293609619 4.83099365234375
Loss :  1.6609523296356201 2.8905036449432373 4.551455974578857
Loss :  1.6577686071395874 2.4522035121917725 4.10997200012207
Loss :  5.248971462249756 4.408327579498291 9.657299041748047
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.154241561889648 4.3127546310424805 9.466996192932129
Loss :  5.247476577758789 4.2754011154174805 9.52287769317627
Loss :  4.410811901092529 4.194072723388672 8.60488510131836
Total LOSS train 4.6099154178912825 valid 9.313014507293701
CE LOSS train 1.6664137033315805 valid 1.1027029752731323
Contrastive LOSS train 2.9435017108917236 valid 1.048518180847168
EPOCH 121:
Loss :  1.6538511514663696 2.70184326171875 4.35569429397583
Loss :  1.667341709136963 3.5041351318359375 5.1714768409729
Loss :  1.6537827253341675 2.50390625 4.157689094543457
Loss :  1.657995581626892 3.114124059677124 4.772119522094727
Loss :  1.6824140548706055 3.373757839202881 5.056171894073486
Loss :  1.6655977964401245 3.36222767829895 5.027825355529785
Loss :  1.6619404554367065 3.137275218963623 4.799215793609619
Loss :  1.652966856956482 2.7659974098205566 4.418964385986328
Loss :  1.6574392318725586 2.988999843597412 4.646439075469971
Loss :  1.6110970973968506 2.82155704498291 4.43265438079834
Loss :  1.6712912321090698 2.9076457023620605 4.57893705368042
Loss :  1.7304366827011108 3.707371950149536 5.437808513641357
Loss :  1.6739469766616821 3.5526974201202393 5.226644515991211
Loss :  1.666040062904358 3.5110220909118652 5.177062034606934
Loss :  1.6465457677841187 3.084617853164673 4.731163501739502
Loss :  1.6532018184661865 3.101289987564087 4.754491806030273
Loss :  1.6624752283096313 3.58013916015625 5.242614269256592
Loss :  1.6620275974273682 3.31680965423584 4.978837013244629
Loss :  1.6685165166854858 3.1417901515960693 4.810306549072266
Loss :  1.62667977809906 3.221250057220459 4.847929954528809
  batch 20 loss: 1.62667977809906, 3.221250057220459, 4.847929954528809
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.662061333656311 3.457770824432373 5.1198320388793945
Loss :  1.678013563156128 2.93406081199646 4.612074375152588
Loss :  1.6488502025604248 3.3667025566101074 5.015552520751953
Loss :  1.6900248527526855 3.69510555267334 5.385130405426025
Loss :  1.6873009204864502 3.719299077987671 5.406599998474121
Loss :  1.6522551774978638 2.912691593170166 4.56494665145874
Loss :  1.699203610420227 3.348971366882324 5.048174858093262
Loss :  1.6424717903137207 3.215397357940674 4.8578691482543945
Loss :  1.6892201900482178 2.8821301460266113 4.57135009765625
Loss :  1.6444194316864014 3.543168067932129 5.187587738037109
Loss :  1.72456693649292 3.5290820598602295 5.25364875793457
Loss :  1.6712920665740967 3.159318208694458 4.830610275268555
Loss :  1.656530499458313 2.933626413345337 4.5901570320129395
Loss :  1.659475326538086 3.422390937805176 5.081866264343262
Loss :  1.6991442441940308 3.4970085620880127 5.196152687072754
Loss :  1.6893056631088257 3.0704197883605957 4.759725570678711
Loss :  1.6688576936721802 3.2735090255737305 4.942366600036621
Loss :  1.6355104446411133 3.246528148651123 4.882038593292236
Loss :  1.6628270149230957 3.0523734092712402 4.715200424194336
Loss :  1.655086874961853 3.283342123031616 4.93842887878418
  batch 40 loss: 1.655086874961853, 3.283342123031616, 4.93842887878418
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.661909580230713 3.476302146911621 5.138211727142334
Loss :  1.649963140487671 2.9821817874908447 4.632144927978516
Loss :  1.6682666540145874 3.0197176933288574 4.687984466552734
Loss :  1.6660280227661133 2.935349941253662 4.601377964019775
Loss :  1.6682548522949219 2.7599339485168457 4.428188800811768
Loss :  1.6599620580673218 3.590334415435791 5.250296592712402
Loss :  1.6486971378326416 3.1399495601654053 4.788646697998047
Loss :  1.6595724821090698 3.3756721019744873 5.035244464874268
Loss :  1.6315299272537231 3.1861987113952637 4.817728519439697
Loss :  1.6839144229888916 3.274775266647339 4.9586896896362305
Loss :  1.6491326093673706 3.2280619144439697 4.877194404602051
Loss :  1.6673732995986938 3.314643144607544 4.982016563415527
Loss :  1.6845147609710693 3.342003107070923 5.026517868041992
Loss :  1.6698663234710693 3.565910816192627 5.235776901245117
Loss :  1.6735435724258423 3.680105447769165 5.353649139404297
Loss :  1.639869213104248 3.2471652030944824 4.8870344161987305
Loss :  1.6874969005584717 3.492222547531128 5.1797194480896
Loss :  1.6830908060073853 3.4333932399749756 5.11648416519165
Loss :  1.6978169679641724 3.2218170166015625 4.919633865356445
Loss :  1.6746728420257568 3.434000253677368 5.108673095703125
  batch 60 loss: 1.6746728420257568, 3.434000253677368, 5.108673095703125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6738333702087402 3.073181390762329 4.747014999389648
Loss :  1.672054409980774 3.049992084503174 4.722046375274658
Loss :  1.6801167726516724 3.268625497817993 4.948742389678955
Loss :  1.6608327627182007 2.913072109222412 4.573904991149902
Loss :  1.6576179265975952 2.7541046142578125 4.411722660064697
Loss :  5.261161804199219 4.4440202713012695 9.705182075500488
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.167663097381592 4.381638050079346 9.549301147460938
Loss :  5.265904903411865 4.272313594818115 9.53821849822998
Loss :  4.427313327789307 4.184985637664795 8.612298965454102
Total LOSS train 4.892492367671086 valid 9.351250171661377
CE LOSS train 1.666337491915776 valid 1.1068283319473267
Contrastive LOSS train 3.2261548885932334 valid 1.0462464094161987
EPOCH 122:
Loss :  1.6543329954147339 2.5887463092803955 4.24307918548584
Loss :  1.6674600839614868 3.0351970195770264 4.702657222747803
Loss :  1.6541147232055664 2.471808910369873 4.1259236335754395
Loss :  1.6584683656692505 3.5201642513275146 5.178632736206055
Loss :  1.682470440864563 3.385140895843506 5.067611217498779
Loss :  1.666412353515625 2.914997100830078 4.581409454345703
Loss :  1.6615668535232544 3.1886444091796875 4.850211143493652
Loss :  1.6529077291488647 2.6948938369750977 4.347801685333252
Loss :  1.6576919555664062 2.542386054992676 4.200078010559082
Loss :  1.610648274421692 3.3403213024139404 4.950969696044922
Loss :  1.6713286638259888 3.2337257862091064 4.905054569244385
Loss :  1.730978012084961 3.575409173965454 5.306386947631836
Loss :  1.6739553213119507 3.7094621658325195 5.38341760635376
Loss :  1.665864109992981 3.48403263092041 5.149896621704102
Loss :  1.6466292142868042 2.6283085346221924 4.274937629699707
Loss :  1.6533690690994263 3.4718213081359863 5.125190258026123
Loss :  1.6625189781188965 2.9751429557800293 4.637661933898926
Loss :  1.6622042655944824 3.1558172702789307 4.818021774291992
Loss :  1.6691211462020874 2.6211798191070557 4.2903008460998535
Loss :  1.6265066862106323 3.0670087337493896 4.693515300750732
  batch 20 loss: 1.6265066862106323, 3.0670087337493896, 4.693515300750732
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6617772579193115 3.021535634994507 4.683312892913818
Loss :  1.6780246496200562 2.9146623611450195 4.592687129974365
Loss :  1.648544430732727 3.0253305435180664 4.673874855041504
Loss :  1.69051992893219 3.329700469970703 5.0202202796936035
Loss :  1.687768578529358 3.292196035385132 4.979964733123779
Loss :  1.6521003246307373 2.7430593967437744 4.395159721374512
Loss :  1.6990699768066406 3.347647190093994 5.046717166900635
Loss :  1.6424750089645386 2.947524309158325 4.589999198913574
Loss :  1.6889429092407227 2.872713327407837 4.5616559982299805
Loss :  1.6443002223968506 3.3552823066711426 4.999582290649414
Loss :  1.7244936227798462 3.1757395267486572 4.900233268737793
Loss :  1.6713241338729858 3.0636417865753174 4.734965801239014
Loss :  1.6565319299697876 3.048825263977051 4.705357074737549
Loss :  1.6593782901763916 2.8030483722686768 4.462426662445068
Loss :  1.6991950273513794 2.8002288341522217 4.499423980712891
Loss :  1.6892691850662231 2.857987642288208 4.547256946563721
Loss :  1.669054627418518 2.8789048194885254 4.547959327697754
Loss :  1.6354275941848755 2.716362953186035 4.351790428161621
Loss :  1.6626089811325073 2.708045244216919 4.370654106140137
Loss :  1.655055284500122 2.6885175704956055 4.343572616577148
  batch 40 loss: 1.655055284500122, 2.6885175704956055, 4.343572616577148
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6617684364318848 2.8298401832580566 4.491608619689941
Loss :  1.6500998735427856 2.753962516784668 4.404062271118164
Loss :  1.6684850454330444 2.8917267322540283 4.560211658477783
Loss :  1.6661573648452759 2.6056277751922607 4.271785259246826
Loss :  1.6683475971221924 2.2594783306121826 3.927825927734375
Loss :  1.6602132320404053 2.7505316734313965 4.410744667053223
Loss :  1.6482768058776855 3.0768847465515137 4.725161552429199
Loss :  1.659541368484497 2.8597843647003174 4.5193257331848145
Loss :  1.632051706314087 2.9678823947906494 4.599934101104736
Loss :  1.6843204498291016 3.203688144683838 4.8880085945129395
Loss :  1.6494767665863037 2.898939371109009 4.5484161376953125
Loss :  1.6675970554351807 2.802990198135376 4.470587253570557
Loss :  1.6844117641448975 2.6105895042419434 4.295001029968262
Loss :  1.6698130369186401 2.8572421073913574 4.527055263519287
Loss :  1.673951268196106 2.8551790714263916 4.529130458831787
Loss :  1.6399060487747192 2.7375714778900146 4.377477645874023
Loss :  1.6876630783081055 2.8913917541503906 4.579054832458496
Loss :  1.683158278465271 2.768139123916626 4.451297283172607
Loss :  1.6979066133499146 3.1346633434295654 4.8325700759887695
Loss :  1.6749457120895386 2.4834070205688477 4.158352851867676
  batch 60 loss: 1.6749457120895386, 2.4834070205688477, 4.158352851867676
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.673662543296814 2.708778142929077 4.382440567016602
Loss :  1.6717103719711304 2.696016550064087 4.367726802825928
Loss :  1.6799980401992798 2.645179271697998 4.325177192687988
Loss :  1.6610643863677979 2.6244630813598633 4.285527229309082
Loss :  1.6577931642532349 2.2038931846618652 3.8616862297058105
Loss :  5.209772109985352 4.4263715744018555 9.636143684387207
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.108661651611328 4.354040622711182 9.462701797485352
Loss :  5.211062908172607 4.280473232269287 9.491536140441895
Loss :  4.367070198059082 4.143097400665283 8.510168075561523
Total LOSS train 4.594303710644062 valid 9.275137424468994
CE LOSS train 1.666411249454205 valid 1.0917675495147705
Contrastive LOSS train 2.92789249420166 valid 1.0357743501663208
EPOCH 123:
Loss :  1.654039978981018 2.519362688064575 4.173402786254883
Loss :  1.6671249866485596 2.867648124694824 4.534772872924805
Loss :  1.6538443565368652 2.4176137447357178 4.071457862854004
Loss :  1.6583791971206665 2.5466792583465576 4.205058574676514
Loss :  1.6824828386306763 2.754492998123169 4.436975955963135
Loss :  1.6658157110214233 2.5740368366241455 4.239852428436279
Loss :  1.6620242595672607 2.8294355869293213 4.491459846496582
Loss :  1.6529167890548706 2.5045924186706543 4.1575093269348145
Loss :  1.657531976699829 2.6092658042907715 4.26679801940918
Loss :  1.6112327575683594 2.774940013885498 4.386172771453857
Loss :  1.6710517406463623 2.767578601837158 4.438630104064941
Loss :  1.7301987409591675 3.2309138774871826 4.9611124992370605
Loss :  1.673816204071045 3.0356321334838867 4.709448337554932
Loss :  1.6658178567886353 2.8216090202331543 4.4874267578125
Loss :  1.6470304727554321 2.53781795501709 4.184848308563232
Loss :  1.653141736984253 2.828172206878662 4.481313705444336
Loss :  1.6623235940933228 2.6610753536224365 4.323399066925049
Loss :  1.6622017621994019 2.5839555263519287 4.246157169342041
Loss :  1.6686183214187622 2.568636417388916 4.237254619598389
Loss :  1.6265302896499634 2.8683395385742188 4.494869709014893
  batch 20 loss: 1.6265302896499634, 2.8683395385742188, 4.494869709014893
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6622989177703857 2.810346841812134 4.4726457595825195
Loss :  1.6778671741485596 2.7009177207946777 4.378785133361816
Loss :  1.6492053270339966 2.5561516284942627 4.205357074737549
Loss :  1.6899865865707397 2.6741626262664795 4.36414909362793
Loss :  1.6873890161514282 3.326878309249878 5.014267444610596
Loss :  1.6524728536605835 2.7201526165008545 4.372625350952148
Loss :  1.6987165212631226 3.281764507293701 4.980481147766113
Loss :  1.6427721977233887 2.64520263671875 4.287974834442139
Loss :  1.6890603303909302 2.619161367416382 4.308221817016602
Loss :  1.6442683935165405 3.163029432296753 4.807297706604004
Loss :  1.7244212627410889 2.6864383220672607 4.41085958480835
Loss :  1.6707286834716797 2.8044254779815674 4.475153923034668
Loss :  1.656530499458313 2.598696231842041 4.2552266120910645
Loss :  1.6595176458358765 2.9250411987304688 4.584558963775635
Loss :  1.69870126247406 2.8523430824279785 4.551044464111328
Loss :  1.6893402338027954 2.7221291065216064 4.411469459533691
Loss :  1.668912410736084 2.5472280979156494 4.2161407470703125
Loss :  1.6351464986801147 2.6519734859466553 4.2871198654174805
Loss :  1.6625081300735474 2.5208795070648193 4.183387756347656
Loss :  1.6551135778427124 3.0537145137786865 4.708827972412109
  batch 40 loss: 1.6551135778427124, 3.0537145137786865, 4.708827972412109
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6617484092712402 2.8744609355926514 4.5362091064453125
Loss :  1.650047779083252 2.8985495567321777 4.54859733581543
Loss :  1.6683168411254883 2.9143459796905518 4.582662582397461
Loss :  1.6660476922988892 2.320380449295044 3.9864282608032227
Loss :  1.6680651903152466 2.2217226028442383 3.8897876739501953
Loss :  1.6603189706802368 3.136106252670288 4.7964253425598145
Loss :  1.648237943649292 3.047102451324463 4.695340156555176
Loss :  1.6594332456588745 2.9937565326690674 4.653189659118652
Loss :  1.6320942640304565 3.020735740661621 4.652830123901367
Loss :  1.6839325428009033 3.2262797355651855 4.910212516784668
Loss :  1.6492940187454224 3.0552451610565186 4.7045392990112305
Loss :  1.667313575744629 2.927027463912964 4.594341278076172
Loss :  1.6843267679214478 2.670863628387451 4.355190277099609
Loss :  1.6693445444107056 2.866594076156616 4.535938739776611
Loss :  1.6734907627105713 2.876671075820923 4.550161838531494
Loss :  1.639404535293579 2.914254903793335 4.553659439086914
Loss :  1.6873241662979126 3.047656536102295 4.734980583190918
Loss :  1.683002233505249 2.9945337772369385 4.6775360107421875
Loss :  1.6975330114364624 3.0683302879333496 4.765863418579102
Loss :  1.674288272857666 2.593249559402466 4.267538070678711
  batch 60 loss: 1.674288272857666, 2.593249559402466, 4.267538070678711
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6736831665039062 3.3033268451690674 4.9770097732543945
Loss :  1.6713757514953613 3.187608480453491 4.858983993530273
Loss :  1.6801337003707886 2.9361226558685303 4.616256237030029
Loss :  1.6607369184494019 2.8313956260681152 4.492132663726807
Loss :  1.657485008239746 2.412642478942871 4.070127487182617
Loss :  5.243062496185303 4.36283016204834 9.605892181396484
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.1314778327941895 4.405209541320801 9.536687850952148
Loss :  5.233201026916504 4.319981098175049 9.553182601928711
Loss :  4.379276752471924 4.212174892425537 8.591451644897461
Total LOSS train 4.473991650801438 valid 9.321803569793701
CE LOSS train 1.6662778524252084 valid 1.094819188117981
Contrastive LOSS train 2.8077138093801643 valid 1.0530437231063843
EPOCH 124:
Loss :  1.6535199880599976 2.7522504329681396 4.405770301818848
Loss :  1.6673829555511475 3.13765811920166 4.805041313171387
Loss :  1.6537244319915771 2.283684730529785 3.9374091625213623
Loss :  1.6579025983810425 3.0460574626922607 4.703959941864014
Loss :  1.681936502456665 3.2376656532287598 4.919602394104004
Loss :  1.6656862497329712 2.660027265548706 4.325713634490967
Loss :  1.661838173866272 2.8159921169281006 4.477830410003662
Loss :  1.6527376174926758 2.504532814025879 4.157270431518555
Loss :  1.6568984985351562 2.878410816192627 4.535309314727783
Loss :  1.6109257936477661 3.0045652389526367 4.615490913391113
Loss :  1.6709811687469482 2.8762474060058594 4.547228813171387
Loss :  1.7301689386367798 3.375293493270874 5.105462551116943
Loss :  1.6738009452819824 3.4945099353790283 5.16831111907959
Loss :  1.665897011756897 3.178403615951538 4.844300746917725
Loss :  1.6463960409164429 3.1637089252471924 4.810104846954346
Loss :  1.6528809070587158 3.3986001014709473 5.051481246948242
Loss :  1.6623529195785522 2.839686632156372 4.502039432525635
Loss :  1.6619147062301636 2.9601058959960938 4.622020721435547
Loss :  1.6685566902160645 2.5568366050720215 4.225393295288086
Loss :  1.6262105703353882 3.177104949951172 4.80331563949585
  batch 20 loss: 1.6262105703353882, 3.177104949951172, 4.80331563949585
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6618691682815552 2.9684205055236816 4.630289554595947
Loss :  1.6777186393737793 2.873842477798462 4.55156135559082
Loss :  1.648677945137024 3.1556572914123535 4.804335117340088
Loss :  1.6897447109222412 2.744666337966919 4.43441104888916
Loss :  1.6870638132095337 3.6770105361938477 5.364074230194092
Loss :  1.6521251201629639 3.2304584980010986 4.8825836181640625
Loss :  1.6987800598144531 3.316770315170288 5.01555061340332
Loss :  1.6425665616989136 3.035409450531006 4.677976131439209
Loss :  1.689172625541687 3.0201449394226074 4.709317684173584
Loss :  1.643955111503601 3.5212812423706055 5.165236473083496
Loss :  1.724230170249939 3.232314109802246 4.956544399261475
Loss :  1.670512318611145 2.9300014972686768 4.600513935089111
Loss :  1.6564337015151978 3.0205790996551514 4.677012920379639
Loss :  1.6590410470962524 2.964099645614624 4.623140811920166
Loss :  1.6984366178512573 2.9552619457244873 4.653698444366455
Loss :  1.6889673471450806 2.977389335632324 4.666356563568115
Loss :  1.668664574623108 3.1076972484588623 4.77636194229126
Loss :  1.6354371309280396 2.964858055114746 4.600295066833496
Loss :  1.6623464822769165 2.7358574867248535 4.3982038497924805
Loss :  1.6547772884368896 3.1058287620544434 4.760605812072754
  batch 40 loss: 1.6547772884368896, 3.1058287620544434, 4.760605812072754
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6616021394729614 3.571148157119751 5.232750415802002
Loss :  1.649699091911316 3.2316386699676514 4.881337642669678
Loss :  1.6679728031158447 3.0248372554779053 4.69281005859375
Loss :  1.6654157638549805 2.5864834785461426 4.251899242401123
Loss :  1.6678006649017334 2.6154942512512207 4.283294677734375
Loss :  1.6600631475448608 3.107374906539917 4.767437934875488
Loss :  1.6479216814041138 3.074254035949707 4.722175598144531
Loss :  1.658882737159729 3.2968413829803467 4.955724239349365
Loss :  1.6321097612380981 3.014064073562622 4.64617395401001
Loss :  1.6835384368896484 2.8999292850494385 4.583467483520508
Loss :  1.6485393047332764 2.9308674335479736 4.57940673828125
Loss :  1.6669840812683105 3.232241153717041 4.899225234985352
Loss :  1.6839770078659058 3.115992307662964 4.79996919631958
Loss :  1.6692451238632202 3.2253668308258057 4.894611835479736
Loss :  1.673046350479126 3.0617544651031494 4.734800815582275
Loss :  1.639318585395813 2.5499215126037598 4.189239978790283
Loss :  1.6871000528335571 3.2029194831848145 4.890019416809082
Loss :  1.6826903820037842 3.237417697906494 4.920107841491699
Loss :  1.6971447467803955 3.206157922744751 4.9033026695251465
Loss :  1.6743062734603882 2.9003031253814697 4.574609279632568
  batch 60 loss: 1.6743062734603882, 2.9003031253814697, 4.574609279632568
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6733542680740356 3.403254270553589 5.076608657836914
Loss :  1.6711231470108032 3.2972421646118164 4.96836519241333
Loss :  1.679755687713623 2.9599673748016357 4.63972282409668
Loss :  1.6604269742965698 2.385059356689453 4.0454864501953125
Loss :  1.6569757461547852 2.7569494247436523 4.4139251708984375
Loss :  5.26218843460083 4.361825466156006 9.624013900756836
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.151909351348877 4.403059959411621 9.554969787597656
Loss :  5.258118629455566 4.241665840148926 9.499784469604492
Loss :  4.415538311004639 4.2327704429626465 8.648308753967285
Total LOSS train 4.69319382080665 valid 9.331769227981567
CE LOSS train 1.6660188784966101 valid 1.1038845777511597
Contrastive LOSS train 3.027174938642062 valid 1.0581926107406616
EPOCH 125:
Loss :  1.6537095308303833 2.85623836517334 4.509947776794434
Loss :  1.6670944690704346 3.3224093914031982 4.989503860473633
Loss :  1.6535825729370117 2.6506495475769043 4.304232120513916
Loss :  1.6577461957931519 2.9929893016815186 4.650735378265381
Loss :  1.681954264640808 2.8881146907806396 4.570068836212158
Loss :  1.6655669212341309 3.134934186935425 4.800500869750977
Loss :  1.6616045236587524 3.251479148864746 4.913083553314209
Loss :  1.6526005268096924 2.8583524227142334 4.510952949523926
Loss :  1.657023549079895 2.8014719486236572 4.458495616912842
Loss :  1.6106497049331665 3.0074658393859863 4.618115425109863
Loss :  1.6708338260650635 3.0587265491485596 4.729560375213623
Loss :  1.7302722930908203 3.3503003120422363 5.080572605133057
Loss :  1.6735801696777344 3.4135451316833496 5.087125301361084
Loss :  1.6655919551849365 3.1742024421691895 4.839794158935547
Loss :  1.6459988355636597 2.643903970718384 4.289902687072754
Loss :  1.652979850769043 3.4391543865203857 5.092134475708008
Loss :  1.6622239351272583 3.0536575317382812 4.71588134765625
Loss :  1.6619787216186523 2.6240336894989014 4.286012649536133
Loss :  1.6683294773101807 2.566216230392456 4.234545707702637
Loss :  1.6261250972747803 2.9359943866729736 4.562119483947754
  batch 20 loss: 1.6261250972747803, 2.9359943866729736, 4.562119483947754
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.661665678024292 3.09889817237854 4.760563850402832
Loss :  1.6776357889175415 2.98854398727417 4.666179656982422
Loss :  1.6484649181365967 3.0480856895446777 4.696550369262695
Loss :  1.6897588968276978 2.9721381664276123 4.6618971824646
Loss :  1.686858057975769 3.6446421146392822 5.331500053405762
Loss :  1.6518986225128174 3.090615749359131 4.742514610290527
Loss :  1.6987645626068115 3.351039409637451 5.049803733825684
Loss :  1.6422337293624878 3.201411247253418 4.843645095825195
Loss :  1.6888753175735474 2.80934476852417 4.498219966888428
Loss :  1.6439869403839111 3.490755558013916 5.134742736816406
Loss :  1.7239863872528076 3.3547332286834717 5.078719615936279
Loss :  1.6708097457885742 3.1729447841644287 4.843754768371582
Loss :  1.6563146114349365 2.676964282989502 4.333278656005859
Loss :  1.6588908433914185 3.3037607669830322 4.96265172958374
Loss :  1.6983720064163208 3.2714011669158936 4.969773292541504
Loss :  1.68892240524292 3.2489826679229736 4.937905311584473
Loss :  1.6684526205062866 2.9630773067474365 4.631529808044434
Loss :  1.6356911659240723 2.762077808380127 4.397768974304199
Loss :  1.6623280048370361 2.954828977584839 4.617156982421875
Loss :  1.6547298431396484 2.8997819423675537 4.554512023925781
  batch 40 loss: 1.6547298431396484, 2.8997819423675537, 4.554512023925781
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.661716341972351 3.0209970474243164 4.682713508605957
Loss :  1.6495702266693115 3.355208396911621 5.004778861999512
Loss :  1.6677709817886353 2.9308924674987793 4.598663330078125
Loss :  1.6656938791275024 2.733769416809082 4.399463176727295
Loss :  1.668113350868225 2.66753888130188 4.3356523513793945
Loss :  1.6598669290542603 3.121497869491577 4.781364917755127
Loss :  1.6480623483657837 2.8961293697357178 4.544191837310791
Loss :  1.6591185331344604 3.0984580516815186 4.7575764656066895
Loss :  1.6316502094268799 3.487839937210083 5.119490146636963
Loss :  1.6834661960601807 3.1997597217559814 4.883225917816162
Loss :  1.6484935283660889 3.332157850265503 4.980651378631592
Loss :  1.6670258045196533 3.4267475605010986 5.093773365020752
Loss :  1.683963656425476 2.955453634262085 4.6394171714782715
Loss :  1.6692492961883545 3.1691601276397705 4.838409423828125
Loss :  1.67268967628479 3.656041383743286 5.328731060028076
Loss :  1.639339804649353 2.682694673538208 4.3220343589782715
Loss :  1.686876893043518 3.3899614810943604 5.076838493347168
Loss :  1.6826225519180298 2.772923231124878 4.455545902252197
Loss :  1.697204351425171 3.1344335079193115 4.831637859344482
Loss :  1.6742695569992065 2.609250068664551 4.283519744873047
  batch 60 loss: 1.6742695569992065, 2.609250068664551, 4.283519744873047
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6733263731002808 3.619509220123291 5.292835712432861
Loss :  1.6712127923965454 3.398512601852417 5.069725513458252
Loss :  1.6796571016311646 3.0078580379486084 4.6875152587890625
Loss :  1.6601500511169434 2.777237892150879 4.437387943267822
Loss :  1.6568015813827515 2.705230951309204 4.362032413482666
Loss :  5.215802192687988 4.444695472717285 9.660497665405273
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.10908842086792 4.444894313812256 9.553982734680176
Loss :  5.212601184844971 4.259818077087402 9.472419738769531
Loss :  4.370317459106445 4.398422718048096 8.768739700317383
Total LOSS train 4.734817380171556 valid 9.36390995979309
CE LOSS train 1.6659384397359995 valid 1.0925793647766113
Contrastive LOSS train 3.0688789330996 valid 1.099605679512024
EPOCH 126:
Loss :  1.6537514925003052 2.545941114425659 4.199692726135254
Loss :  1.6671652793884277 3.246861219406128 4.914026260375977
Loss :  1.6533808708190918 2.2874972820281982 3.94087815284729
Loss :  1.6576007604599 2.791722536087036 4.4493231773376465
Loss :  1.6821253299713135 3.4073057174682617 5.089430809020996
Loss :  1.6654610633850098 2.7509915828704834 4.416452407836914
Loss :  1.6616625785827637 2.771143674850464 4.432806015014648
Loss :  1.65262770652771 2.730534553527832 4.383162498474121
Loss :  1.6569308042526245 2.6509861946105957 4.30791711807251
Loss :  1.6107312440872192 3.0575156211853027 4.668246746063232
Loss :  1.6707522869110107 2.6026291847229004 4.273381233215332
Loss :  1.7299025058746338 3.1758296489715576 4.905732154846191
Loss :  1.6734576225280762 3.387097120285034 5.060554504394531
Loss :  1.6655235290527344 3.2477619647979736 4.913285255432129
Loss :  1.6461504697799683 2.7795472145080566 4.4256978034973145
Loss :  1.6529828310012817 2.9554479122161865 4.608430862426758
Loss :  1.662115216255188 3.110349178314209 4.772464275360107
Loss :  1.6619247198104858 2.8979380130767822 4.5598626136779785
Loss :  1.668371558189392 2.5239601135253906 4.192331790924072
Loss :  1.6261639595031738 2.7259185314178467 4.352082252502441
  batch 20 loss: 1.6261639595031738, 2.7259185314178467, 4.352082252502441
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6616605520248413 3.080871343612671 4.742531776428223
Loss :  1.6775085926055908 2.636246681213379 4.313755035400391
Loss :  1.648816704750061 2.8808250427246094 4.529641628265381
Loss :  1.6895928382873535 2.7395386695861816 4.429131507873535
Loss :  1.6868793964385986 3.660170078277588 5.347049713134766
Loss :  1.652050256729126 2.805860996246338 4.457911491394043
Loss :  1.6986924409866333 3.1343166828155518 4.833009243011475
Loss :  1.6423605680465698 2.6618239879608154 4.304184436798096
Loss :  1.6889160871505737 2.756554365158081 4.445470333099365
Loss :  1.6438010931015015 2.999600648880005 4.643401622772217
Loss :  1.7238779067993164 2.9833106994628906 4.707188606262207
Loss :  1.6707345247268677 2.8436155319213867 4.514349937438965
Loss :  1.6561663150787354 2.706409454345703 4.362575531005859
Loss :  1.6589165925979614 2.755999803543091 4.414916515350342
Loss :  1.6983718872070312 2.8800337314605713 4.578405380249023
Loss :  1.6888779401779175 2.98746395111084 4.676342010498047
Loss :  1.6686893701553345 2.740039110183716 4.40872859954834
Loss :  1.6354953050613403 2.944549798965454 4.580045223236084
Loss :  1.6621050834655762 2.8257880210876465 4.487893104553223
Loss :  1.654806137084961 2.873145341873169 4.527951240539551
  batch 40 loss: 1.654806137084961, 2.873145341873169, 4.527951240539551
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6614930629730225 3.0331742763519287 4.694667339324951
Loss :  1.6494543552398682 2.6577649116516113 4.307219505310059
Loss :  1.6678826808929443 2.696301221847534 4.3641839027404785
Loss :  1.6652660369873047 2.9845550060272217 4.6498212814331055
Loss :  1.6676678657531738 2.3719189167022705 4.039587020874023
Loss :  1.6599973440170288 2.9523768424987793 4.612374305725098
Loss :  1.6478468179702759 3.0271637439727783 4.675010681152344
Loss :  1.658677577972412 2.866541624069214 4.525218963623047
Loss :  1.6323438882827759 3.0119919776916504 4.644335746765137
Loss :  1.6831773519515991 2.9039337635040283 4.587110996246338
Loss :  1.6485599279403687 2.8950302600860596 4.543590068817139
Loss :  1.6671195030212402 2.5455543994903564 4.212674140930176
Loss :  1.683973789215088 2.7967729568481445 4.480746746063232
Loss :  1.6688618659973145 2.7258810997009277 4.394742965698242
Loss :  1.6726317405700684 3.322195291519165 4.9948272705078125
Loss :  1.6392508745193481 2.505199432373047 4.1444501876831055
Loss :  1.6872951984405518 3.3175089359283447 5.0048041343688965
Loss :  1.682416319847107 3.0975327491760254 4.779949188232422
Loss :  1.6973735094070435 3.0592424869537354 4.756616115570068
Loss :  1.6743643283843994 2.55055832862854 4.2249226570129395
  batch 60 loss: 1.6743643283843994, 2.55055832862854, 4.2249226570129395
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.673573613166809 3.0872251987457275 4.760798931121826
Loss :  1.6708883047103882 2.9829118251800537 4.653800010681152
Loss :  1.6794711351394653 2.843050718307495 4.52252197265625
Loss :  1.6604429483413696 2.512482166290283 4.172924995422363
Loss :  1.6570870876312256 2.496866464614868 4.153953552246094
Loss :  5.176987648010254 4.41098690032959 9.587974548339844
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.069185256958008 4.3899126052856445 9.459097862243652
Loss :  5.167849063873291 4.350170612335205 9.518019676208496
Loss :  4.334137916564941 4.242114067077637 8.576251983642578
Total LOSS train 4.53955529653109 valid 9.285336017608643
CE LOSS train 1.6659110546112061 valid 1.0835344791412354
Contrastive LOSS train 2.873644260259775 valid 1.0605285167694092
EPOCH 127:
Loss :  1.6536624431610107 2.3021388053894043 3.955801248550415
Loss :  1.6671030521392822 2.9974982738494873 4.6646013259887695
Loss :  1.653324842453003 2.438166379928589 4.091491222381592
Loss :  1.6575415134429932 3.138075113296509 4.795616626739502
Loss :  1.681917667388916 3.2722203731536865 4.954137802124023
Loss :  1.665310025215149 2.5488905906677246 4.214200496673584
Loss :  1.661848545074463 2.8313143253326416 4.493163108825684
Loss :  1.6528531312942505 2.492562770843506 4.145415782928467
Loss :  1.6570546627044678 2.350222110748291 4.00727653503418
Loss :  1.6111787557601929 2.569270372390747 4.18044900894165
Loss :  1.6707603931427002 2.839796304702759 4.510556697845459
Loss :  1.729878306388855 3.3617680072784424 5.091646194458008
Loss :  1.673474669456482 3.257335901260376 4.930810451507568
Loss :  1.6658354997634888 3.3704769611358643 5.036312580108643
Loss :  1.6460750102996826 3.0186126232147217 4.664687633514404
Loss :  1.652834415435791 3.0957515239715576 4.7485857009887695
Loss :  1.6621263027191162 2.7749710083007812 4.437097549438477
Loss :  1.6619466543197632 2.4766652584075928 4.138611793518066
Loss :  1.6679608821868896 2.407557249069214 4.0755181312561035
Loss :  1.6261953115463257 3.2753233909606934 4.901518821716309
  batch 20 loss: 1.6261953115463257, 3.2753233909606934, 4.901518821716309
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.661928415298462 2.9016637802124023 4.563591957092285
Loss :  1.6773734092712402 2.591686487197876 4.269060134887695
Loss :  1.6489214897155762 2.819563150405884 4.468484878540039
Loss :  1.6893349885940552 2.8352174758911133 4.524552345275879
Loss :  1.6867173910140991 3.473433017730713 5.160150527954102
Loss :  1.6522386074066162 2.5809850692749023 4.233223915100098
Loss :  1.6988600492477417 3.518332004547119 5.21719217300415
Loss :  1.642417073249817 3.0396530628204346 4.682070255279541
Loss :  1.6891106367111206 2.8501498699188232 4.539260387420654
Loss :  1.644107460975647 3.0225484371185303 4.666656017303467
Loss :  1.7241798639297485 3.536299228668213 5.260478973388672
Loss :  1.670395851135254 2.958453416824341 4.628849029541016
Loss :  1.6562714576721191 3.421943426132202 5.078214645385742
Loss :  1.6590176820755005 3.0581448078155518 4.717162609100342
Loss :  1.6980581283569336 3.117027521133423 4.815085411071777
Loss :  1.688995361328125 2.9682247638702393 4.657219886779785
Loss :  1.6686006784439087 2.8967883586883545 4.565389156341553
Loss :  1.635334849357605 3.3480312824249268 4.983366012573242
Loss :  1.661966323852539 2.5854413509368896 4.247407913208008
Loss :  1.654971718788147 2.989553213119507 4.644525051116943
  batch 40 loss: 1.654971718788147, 2.989553213119507, 4.644525051116943
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6615484952926636 3.1528608798980713 4.814409255981445
Loss :  1.6493871212005615 3.1228995323181152 4.772286415100098
Loss :  1.6676945686340332 2.8865807056427 4.5542755126953125
Loss :  1.6650068759918213 2.7973239421844482 4.4623308181762695
Loss :  1.6674937009811401 3.3272666931152344 4.994760513305664
Loss :  1.6602314710617065 3.342928647994995 5.003159999847412
Loss :  1.6477364301681519 3.1802408695220947 4.827977180480957
Loss :  1.6585514545440674 3.020779848098755 4.679331302642822
Loss :  1.6324272155761719 3.146339178085327 4.778766632080078
Loss :  1.6830942630767822 3.274932384490967 4.958026885986328
Loss :  1.6483200788497925 3.1402618885040283 4.788581848144531
Loss :  1.6669832468032837 3.065354347229004 4.732337474822998
Loss :  1.6838663816452026 3.4779112339019775 5.161777496337891
Loss :  1.6687490940093994 3.300239324569702 4.968988418579102
Loss :  1.6721665859222412 3.7616868019104004 5.4338531494140625
Loss :  1.638529896736145 3.028224229812622 4.666754245758057
Loss :  1.6871415376663208 3.4046337604522705 5.091775417327881
Loss :  1.6826667785644531 3.2107627391815186 4.893429756164551
Loss :  1.6974132061004639 3.3460581302642822 5.043471336364746
Loss :  1.6739360094070435 3.080883741378784 4.754819869995117
  batch 60 loss: 1.6739360094070435, 3.080883741378784, 4.754819869995117
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6738206148147583 3.2156622409820557 4.8894829750061035
Loss :  1.6710355281829834 3.6250786781311035 5.296113967895508
Loss :  1.6798365116119385 3.4790596961975098 5.158896446228027
Loss :  1.6604093313217163 3.365928888320923 5.02633810043335
Loss :  1.6572636365890503 2.416409969329834 4.073673725128174
Loss :  5.248852729797363 4.384945869445801 9.633798599243164
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.154445648193359 4.329176902770996 9.483622550964355
Loss :  5.247260093688965 4.372581958770752 9.619842529296875
Loss :  4.391643524169922 4.276966571807861 8.668609619140625
Total LOSS train 4.70438551902771 valid 9.351468324661255
CE LOSS train 1.6658922085395227 valid 1.0979108810424805
Contrastive LOSS train 3.0384933141561654 valid 1.0692416429519653
EPOCH 128:
Loss :  1.6536076068878174 2.9191479682922363 4.572755813598633
Loss :  1.6675267219543457 3.319519281387329 4.987046241760254
Loss :  1.653419852256775 2.668797731399536 4.3222174644470215
Loss :  1.6571844816207886 3.245844602584839 4.903028964996338
Loss :  1.6819003820419312 3.228121042251587 4.9100213050842285
Loss :  1.6653542518615723 3.1160888671875 4.781443119049072
Loss :  1.6617074012756348 3.3822360038757324 5.043943405151367
Loss :  1.6525547504425049 3.020026445388794 4.672581195831299
Loss :  1.6565865278244019 3.081005573272705 4.7375922203063965
Loss :  1.6106317043304443 3.2110254764556885 4.821657180786133
Loss :  1.6708521842956543 3.0374417304992676 4.708293914794922
Loss :  1.7303180694580078 3.3090808391571045 5.039399147033691
Loss :  1.673958420753479 3.2113988399505615 4.88535737991333
Loss :  1.6658111810684204 3.25628924369812 4.92210054397583
Loss :  1.6456317901611328 3.4105522632598877 5.056183815002441
Loss :  1.6530343294143677 3.4394192695617676 5.092453479766846
Loss :  1.662545084953308 3.2153618335723877 4.877906799316406
Loss :  1.6620653867721558 2.9761343002319336 4.638199806213379
Loss :  1.6686238050460815 3.0011496543884277 4.669773578643799
Loss :  1.6260631084442139 3.440610408782959 5.066673278808594
  batch 20 loss: 1.6260631084442139, 3.440610408782959, 5.066673278808594
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.661283016204834 3.132906198501587 4.794189453125
Loss :  1.6774625778198242 2.9938154220581055 4.67127799987793
Loss :  1.6479744911193848 3.5014450550079346 5.149419784545898
Loss :  1.6895757913589478 3.2399566173553467 4.929532527923584
Loss :  1.686537504196167 3.621544361114502 5.30808162689209
Loss :  1.6514675617218018 3.2442572116851807 4.895724773406982
Loss :  1.6990845203399658 3.303105592727661 5.002190113067627
Loss :  1.642164945602417 3.139251708984375 4.781416893005371
Loss :  1.6892337799072266 3.025869131088257 4.7151031494140625
Loss :  1.6437928676605225 3.2944159507751465 4.93820858001709
Loss :  1.7242292165756226 3.3419580459594727 5.066187381744385
Loss :  1.671096682548523 3.3727571964263916 5.043853759765625
Loss :  1.6562339067459106 2.9189066886901855 4.575140476226807
Loss :  1.658821940422058 3.533081293106079 5.191903114318848
Loss :  1.6983208656311035 3.3812222480773926 5.079543113708496
Loss :  1.6892917156219482 3.669621706008911 5.358913421630859
Loss :  1.6684986352920532 3.154848575592041 4.823347091674805
Loss :  1.636113166809082 3.2667503356933594 4.902863502502441
Loss :  1.662184238433838 3.1066160202026367 4.768800258636475
Loss :  1.654543399810791 3.5055220127105713 5.160065650939941
  batch 40 loss: 1.654543399810791, 3.5055220127105713, 5.160065650939941
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.661901593208313 3.0907318592071533 4.752633571624756
Loss :  1.6499658823013306 3.4612507820129395 5.1112165451049805
Loss :  1.667760968208313 3.0079686641693115 4.675729751586914
Loss :  1.6659363508224487 3.059119701385498 4.725056171417236
Loss :  1.6681749820709229 2.7828445434570312 4.451019287109375
Loss :  1.6600817441940308 3.156816005706787 4.816897869110107
Loss :  1.6483958959579468 3.312049627304077 4.960445404052734
Loss :  1.6592168807983398 2.9112954139709473 4.570512294769287
Loss :  1.6317440271377563 3.395705223083496 5.027449131011963
Loss :  1.6834477186203003 3.2660529613494873 4.949500560760498
Loss :  1.6480555534362793 3.5304338932037354 5.178489685058594
Loss :  1.6669931411743164 3.2165162563323975 4.883509635925293
Loss :  1.684080719947815 3.0790693759918213 4.763150215148926
Loss :  1.6695224046707153 3.1295764446258545 4.799098968505859
Loss :  1.6723390817642212 3.611480474472046 5.283819675445557
Loss :  1.6389013528823853 3.3973050117492676 5.036206245422363
Loss :  1.6870718002319336 3.3751442432403564 5.062215805053711
Loss :  1.6827986240386963 3.071815252304077 4.754613876342773
Loss :  1.6974419355392456 3.273606538772583 4.971048355102539
Loss :  1.6743286848068237 2.8445193767547607 4.518847942352295
  batch 60 loss: 1.6743286848068237, 2.8445193767547607, 4.518847942352295
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6738340854644775 3.320810556411743 4.994644641876221
Loss :  1.671378254890442 3.266618251800537 4.9379963874816895
Loss :  1.6801389455795288 3.1949360370635986 4.875074863433838
Loss :  1.6603503227233887 2.9328274726867676 4.593177795410156
Loss :  1.6571065187454224 2.467198371887207 4.12430477142334
Loss :  5.291452407836914 4.403056621551514 9.694509506225586
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.196874141693115 4.4095354080200195 9.606409072875977
Loss :  5.281335353851318 4.284269332885742 9.565605163574219
Loss :  4.423497200012207 4.228466510772705 8.65196418762207
Total LOSS train 4.872016165806697 valid 9.379621982574463
CE LOSS train 1.665973158983084 valid 1.1058743000030518
Contrastive LOSS train 3.206043001321646 valid 1.0571166276931763
EPOCH 129:
Loss :  1.6537283658981323 2.6977014541625977 4.3514299392700195
Loss :  1.6676125526428223 3.429091215133667 5.09670352935791
Loss :  1.6534051895141602 2.572333574295044 4.225738525390625
Loss :  1.657256841659546 2.932220697402954 4.5894775390625
Loss :  1.6821047067642212 3.1986541748046875 4.880758762359619
Loss :  1.6653783321380615 2.804096221923828 4.469474792480469
Loss :  1.6618702411651611 3.06099009513855 4.722860336303711
Loss :  1.6526918411254883 2.6793596744537354 4.3320512771606445
Loss :  1.6569116115570068 2.4621121883392334 4.11902379989624
Loss :  1.610701560974121 2.7661867141723633 4.376888275146484
Loss :  1.670939326286316 2.5127928256988525 4.183732032775879
Loss :  1.7302051782608032 3.295825481414795 5.026030540466309
Loss :  1.6739559173583984 3.390212297439575 5.0641679763793945
Loss :  1.66591477394104 3.0654749870300293 4.731389999389648
Loss :  1.6458977460861206 2.9063832759857178 4.552280902862549
Loss :  1.6531485319137573 3.135131359100342 4.788280010223389
Loss :  1.662453532218933 2.649691343307495 4.312144756317139
Loss :  1.662047028541565 2.8936831951141357 4.55573034286499
Loss :  1.6684095859527588 2.574852466583252 4.24326229095459
Loss :  1.6262346506118774 3.21456241607666 4.840796947479248
  batch 20 loss: 1.6262346506118774, 3.21456241607666, 4.840796947479248
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6615095138549805 2.950700044631958 4.612209320068359
Loss :  1.6775891780853271 2.5859010219573975 4.263490200042725
Loss :  1.6486178636550903 2.742755651473999 4.391373634338379
Loss :  1.6895718574523926 2.702481269836426 4.392053127288818
Loss :  1.686571717262268 3.3249430656433105 5.011514663696289
Loss :  1.6519131660461426 2.707313060760498 4.359226226806641
Loss :  1.6990101337432861 3.0550010204315186 4.754011154174805
Loss :  1.6424765586853027 2.7281243801116943 4.370600700378418
Loss :  1.6891868114471436 2.557088613510132 4.246275424957275
Loss :  1.6439001560211182 2.900454044342041 4.544354438781738
Loss :  1.7242742776870728 2.8951990604400635 4.619473457336426
Loss :  1.6705803871154785 2.8271844387054443 4.497764587402344
Loss :  1.6563714742660522 2.6867458820343018 4.3431172370910645
Loss :  1.6592658758163452 2.6274824142456055 4.28674840927124
Loss :  1.6979767084121704 2.936615467071533 4.634592056274414
Loss :  1.6895031929016113 2.622790575027466 4.312294006347656
Loss :  1.668514370918274 2.5839147567749023 4.252429008483887
Loss :  1.6355177164077759 2.7914116382598877 4.426929473876953
Loss :  1.6619832515716553 2.334845781326294 3.996829032897949
Loss :  1.6549865007400513 2.7672953605651855 4.422281742095947
  batch 40 loss: 1.6549865007400513, 2.7672953605651855, 4.422281742095947
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.661600947380066 2.685662031173706 4.347262859344482
Loss :  1.6496385335922241 2.635619878768921 4.2852582931518555
Loss :  1.6679953336715698 2.6278676986694336 4.295863151550293
Loss :  1.6654841899871826 2.544184446334839 4.2096686363220215
Loss :  1.6678434610366821 2.2108616828918457 3.8787050247192383
Loss :  1.6601941585540771 2.9090495109558105 4.569243431091309
Loss :  1.647999882698059 3.26908540725708 4.91708517074585
Loss :  1.658765435218811 2.9858462810516357 4.644611835479736
Loss :  1.6320782899856567 3.1896116733551025 4.821690082550049
Loss :  1.6833391189575195 2.679586172103882 4.3629255294799805
Loss :  1.6486921310424805 2.840080499649048 4.488772392272949
Loss :  1.6669906377792358 2.7520456314086914 4.419036388397217
Loss :  1.6840118169784546 3.065293550491333 4.749305248260498
Loss :  1.6687954664230347 2.9676766395568848 4.636472225189209
Loss :  1.672576904296875 3.313239812850952 4.985816955566406
Loss :  1.6384992599487305 2.770066976547241 4.408566474914551
Loss :  1.6870827674865723 2.86954402923584 4.556626796722412
Loss :  1.6827161312103271 3.105847120285034 4.788563251495361
Loss :  1.6975600719451904 2.88997220993042 4.587532043457031
Loss :  1.6739580631256104 2.976130247116089 4.650088310241699
  batch 60 loss: 1.6739580631256104, 2.976130247116089, 4.650088310241699
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6737325191497803 3.1510074138641357 4.824739933013916
Loss :  1.6708588600158691 3.057225465774536 4.728084564208984
Loss :  1.679222583770752 2.878980875015259 4.55820369720459
Loss :  1.660523772239685 2.7999279499053955 4.460451602935791
Loss :  1.6570378541946411 2.3439629077911377 4.001000881195068
Loss :  5.30132532119751 4.42262601852417 9.72395133972168
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.201818466186523 4.294329643249512 9.496148109436035
Loss :  5.288642406463623 4.295029640197754 9.583671569824219
Loss :  4.428994178771973 4.126341819763184 8.555335998535156
Total LOSS train 4.513467157804049 valid 9.339776754379272
CE LOSS train 1.6659597910367525 valid 1.1072485446929932
Contrastive LOSS train 2.8475073741032526 valid 1.031585454940796
EPOCH 130:
Loss :  1.6531362533569336 2.412630319595337 4.065766334533691
Loss :  1.667323112487793 3.0830509662628174 4.750373840332031
Loss :  1.653247356414795 2.491239070892334 4.144486427307129
Loss :  1.65696120262146 2.7747929096221924 4.431754112243652
Loss :  1.6816613674163818 2.8456544876098633 4.527316093444824
Loss :  1.665161371231079 2.5907626152038574 4.255924224853516
Loss :  1.6619991064071655 2.950896739959717 4.612895965576172
Loss :  1.652658462524414 2.4166815280914307 4.069339752197266
Loss :  1.656855821609497 2.507481575012207 4.164337158203125
Loss :  1.6108542680740356 2.6315596103668213 4.2424139976501465
Loss :  1.6705125570297241 2.79360032081604 4.464112758636475
Loss :  1.7297228574752808 3.163196325302124 4.892919063568115
Loss :  1.6738678216934204 3.133462429046631 4.807330131530762
Loss :  1.665990948677063 3.0842528343200684 4.750243663787842
Loss :  1.6456347703933716 2.7619574069976807 4.407592296600342
Loss :  1.652912974357605 3.2995407581329346 4.95245361328125
Loss :  1.6623071432113647 2.9272241592407227 4.589531421661377
Loss :  1.6618279218673706 2.9175901412963867 4.579418182373047
Loss :  1.6680232286453247 2.891227960586548 4.559251308441162
Loss :  1.6260758638381958 3.134046792984009 4.760122776031494
  batch 20 loss: 1.6260758638381958, 3.134046792984009, 4.760122776031494
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6614978313446045 2.8546392917633057 4.51613712310791
Loss :  1.6774570941925049 2.6486449241638184 4.326102256774902
Loss :  1.6484620571136475 3.2024354934692383 4.850897789001465
Loss :  1.6890499591827393 3.1832449436187744 4.872294902801514
Loss :  1.6863199472427368 3.6144864559173584 5.300806522369385
Loss :  1.6516250371932983 2.7088825702667236 4.360507488250732
Loss :  1.6990575790405273 3.203495979309082 4.902553558349609
Loss :  1.642377495765686 2.616523504257202 4.258901119232178
Loss :  1.6895067691802979 2.8665125370025635 4.556019306182861
Loss :  1.6437089443206787 3.254147529602051 4.897856712341309
Loss :  1.7242095470428467 3.0166893005371094 4.740899085998535
Loss :  1.6708468198776245 3.067335367202759 4.738182067871094
Loss :  1.6562713384628296 2.90486216545105 4.56113338470459
Loss :  1.6589833498001099 3.0174238681793213 4.676407337188721
Loss :  1.697917103767395 3.0626912117004395 4.760608196258545
Loss :  1.6896803379058838 3.215769052505493 4.905449390411377
Loss :  1.668367862701416 2.5770750045776367 4.245442867279053
Loss :  1.6357139348983765 2.712857484817505 4.348571300506592
Loss :  1.6617968082427979 2.82503342628479 4.486830234527588
Loss :  1.6546554565429688 2.9409854412078857 4.595641136169434
  batch 40 loss: 1.6546554565429688, 2.9409854412078857, 4.595641136169434
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.661533236503601 2.9057180881500244 4.567251205444336
Loss :  1.6498278379440308 2.7332205772399902 4.3830485343933105
Loss :  1.6675201654434204 2.8437392711639404 4.51125955581665
Loss :  1.6656956672668457 2.578569173812866 4.244264602661133
Loss :  1.6677143573760986 2.383305311203003 4.051019668579102
Loss :  1.6602213382720947 3.0479981899261475 4.708219528198242
Loss :  1.648109793663025 3.0519440174102783 4.700053691864014
Loss :  1.6586580276489258 3.0053067207336426 4.663964748382568
Loss :  1.6321054697036743 3.2585878372192383 4.890693187713623
Loss :  1.6829510927200317 3.223660945892334 4.906611919403076
Loss :  1.6484349966049194 3.201754093170166 4.850189208984375
Loss :  1.666656732559204 3.2834279537200928 4.950084686279297
Loss :  1.6840764284133911 2.7987468242645264 4.482823371887207
Loss :  1.6688264608383179 2.9154372215270996 4.584263801574707
Loss :  1.6721537113189697 3.1317715644836426 4.803925514221191
Loss :  1.6384230852127075 2.8767638206481934 4.515186786651611
Loss :  1.687009334564209 3.4519455432891846 5.138955116271973
Loss :  1.682800531387329 2.986428737640381 4.669229507446289
Loss :  1.697291612625122 3.1524949073791504 4.849786758422852
Loss :  1.6736304759979248 3.3362021446228027 5.009832382202148
  batch 60 loss: 1.6736304759979248, 3.3362021446228027, 5.009832382202148
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6737862825393677 3.304142951965332 4.97792911529541
Loss :  1.6709918975830078 2.9752376079559326 4.6462297439575195
Loss :  1.6798795461654663 3.0706114768981934 4.750491142272949
Loss :  1.6603924036026 2.890526056289673 4.5509185791015625
Loss :  1.657195806503296 2.2543933391571045 3.9115891456604004
Loss :  5.341855525970459 4.424088478088379 9.76594352722168
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.242088794708252 4.329567909240723 9.571657180786133
Loss :  5.318516254425049 4.180531978607178 9.499048233032227
Loss :  4.4613471031188965 4.384149551391602 8.845497131347656
Total LOSS train 4.603794560065636 valid 9.420536518096924
CE LOSS train 1.6658480919324434 valid 1.1153367757797241
Contrastive LOSS train 2.937946444291335 valid 1.0960373878479004
EPOCH 131:
Loss :  1.6528692245483398 2.7761900424957275 4.429059028625488
Loss :  1.6677746772766113 2.9347190856933594 4.602493762969971
Loss :  1.6534864902496338 2.5249905586242676 4.1784772872924805
Loss :  1.657354474067688 3.3345251083374023 4.991879463195801
Loss :  1.6819868087768555 3.067845106124878 4.7498321533203125
Loss :  1.6652194261550903 2.6962578296661377 4.361477375030518
Loss :  1.661975622177124 3.3887429237365723 5.050718307495117
Loss :  1.6526294946670532 2.5976004600524902 4.250229835510254
Loss :  1.6567462682724 2.6780877113342285 4.334834098815918
Loss :  1.6107243299484253 2.8393876552581787 4.4501118659973145
Loss :  1.6709659099578857 2.8158013820648193 4.486767292022705
Loss :  1.7300750017166138 3.031029462814331 4.761104583740234
Loss :  1.6739858388900757 3.2425131797790527 4.916499137878418
Loss :  1.6659889221191406 3.313408374786377 4.979397296905518
Loss :  1.645550012588501 2.9108548164367676 4.556405067443848
Loss :  1.6533327102661133 3.145679235458374 4.799012184143066
Loss :  1.662596344947815 2.705615758895874 4.3682122230529785
Loss :  1.662028193473816 3.071742296218872 4.733770370483398
Loss :  1.6683698892593384 2.8758251667022705 4.544195175170898
Loss :  1.6260638236999512 3.0029075145721436 4.628971099853516
  batch 20 loss: 1.6260638236999512, 3.0029075145721436, 4.628971099853516
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.661576747894287 2.7425355911254883 4.404112339019775
Loss :  1.6775299310684204 3.0090270042419434 4.686556816101074
Loss :  1.648434042930603 3.4058494567871094 5.054283618927002
Loss :  1.6893198490142822 2.9400742053985596 4.629394054412842
Loss :  1.6864070892333984 3.511650800704956 5.198058128356934
Loss :  1.6515964269638062 2.778552293777466 4.430148601531982
Loss :  1.6988977193832397 3.392540216445923 5.091437816619873
Loss :  1.6425857543945312 3.4478628635406494 5.090448379516602
Loss :  1.6895630359649658 3.157130718231201 4.846693992614746
Loss :  1.644053339958191 3.2997851371765137 4.943838596343994
Loss :  1.7242774963378906 3.2137863636016846 4.938063621520996
Loss :  1.6705788373947144 3.1346611976623535 4.805240154266357
Loss :  1.656306266784668 3.065939426422119 4.722245693206787
Loss :  1.6590650081634521 3.0760436058044434 4.735108375549316
Loss :  1.6980440616607666 3.110792875289917 4.808836936950684
Loss :  1.6896874904632568 3.3301074504852295 5.019794940948486
Loss :  1.668317198753357 2.8406481742858887 4.508965492248535
Loss :  1.6355348825454712 2.9084010124206543 4.543935775756836
Loss :  1.6618857383728027 2.784843683242798 4.44672966003418
Loss :  1.6544864177703857 2.891679525375366 4.546165943145752
  batch 40 loss: 1.6544864177703857, 2.891679525375366, 4.546165943145752
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6616060733795166 3.0898172855377197 4.751423358917236
Loss :  1.6501195430755615 3.2756593227386475 4.925778865814209
Loss :  1.667746663093567 2.761337995529175 4.429084777832031
Loss :  1.6658740043640137 3.1106526851654053 4.77652645111084
Loss :  1.6682251691818237 2.853217124938965 4.521442413330078
Loss :  1.6602625846862793 3.2212209701538086 4.881483554840088
Loss :  1.6480025053024292 3.487696409225464 5.1356987953186035
Loss :  1.6591017246246338 3.4234619140625 5.082563400268555
Loss :  1.6319031715393066 3.3583121299743652 4.990215301513672
Loss :  1.6835380792617798 3.2262003421783447 4.909738540649414
Loss :  1.6484146118164062 3.334177255630493 4.98259162902832
Loss :  1.6666799783706665 3.3968021869659424 5.063482284545898
Loss :  1.6840648651123047 3.444141149520874 5.128206253051758
Loss :  1.6691367626190186 3.111037492752075 4.780174255371094
Loss :  1.6724066734313965 3.632848024368286 5.305254936218262
Loss :  1.6388498544692993 2.7180838584899902 4.35693359375
Loss :  1.6869657039642334 3.2784740924835205 4.965439796447754
Loss :  1.6828972101211548 3.267158031463623 4.950055122375488
Loss :  1.6972497701644897 3.4048688411712646 5.102118492126465
Loss :  1.6736918687820435 3.0959384441375732 4.769630432128906
  batch 60 loss: 1.6736918687820435, 3.0959384441375732, 4.769630432128906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.673783540725708 3.4589426517486572 5.132726192474365
Loss :  1.6711156368255615 3.327956199645996 4.999072074890137
Loss :  1.679839849472046 3.4065816402435303 5.086421489715576
Loss :  1.660187840461731 2.7729923725128174 4.433180332183838
Loss :  1.6569709777832031 2.724447011947632 4.381418228149414
Loss :  5.384696006774902 4.344987869262695 9.729683876037598
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.282792568206787 4.4085893630981445 9.691381454467773
Loss :  5.371253967285156 4.240963935852051 9.612217903137207
Loss :  4.4974799156188965 4.3967485427856445 8.894227981567383
Total LOSS train 4.760987186431885 valid 9.48187780380249
CE LOSS train 1.6659462378575252 valid 1.1243699789047241
Contrastive LOSS train 3.095040933902447 valid 1.0991871356964111
EPOCH 132:
Loss :  1.6530277729034424 2.7972400188446045 4.450267791748047
Loss :  1.6674201488494873 3.1354408264160156 4.802861213684082
Loss :  1.65364408493042 2.93289852142334 4.58654260635376
Loss :  1.657682180404663 3.0697669982910156 4.727449417114258
Loss :  1.681990385055542 3.126819610595703 4.808810234069824
Loss :  1.6654423475265503 3.190922498703003 4.856364727020264
Loss :  1.6615917682647705 3.6149754524230957 5.276567459106445
Loss :  1.6524207592010498 2.78814697265625 4.440567970275879
Loss :  1.6568185091018677 2.7978017330169678 4.454620361328125
Loss :  1.6104179620742798 3.1050496101379395 4.71546745300293
Loss :  1.670883297920227 3.187070369720459 4.8579535484313965
Loss :  1.7301552295684814 3.2828657627105713 5.013020992279053
Loss :  1.6740343570709229 3.4877893924713135 5.161823749542236
Loss :  1.6655863523483276 3.3098652362823486 4.975451469421387
Loss :  1.6453945636749268 2.7213521003723145 4.36674690246582
Loss :  1.6532115936279297 3.2698287963867188 4.923040390014648
Loss :  1.662366271018982 3.236651659011841 4.899017810821533
Loss :  1.6621320247650146 2.7300541400909424 4.392186164855957
Loss :  1.6685737371444702 3.145042657852173 4.8136162757873535
Loss :  1.6259430646896362 3.185873508453369 4.811816692352295
  batch 20 loss: 1.6259430646896362, 3.185873508453369, 4.811816692352295
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.661329746246338 3.1894729137420654 4.850802421569824
Loss :  1.6774671077728271 2.909119129180908 4.586585998535156
Loss :  1.648180603981018 3.240413188934326 4.888593673706055
Loss :  1.6895829439163208 2.8904123306274414 4.579995155334473
Loss :  1.6867297887802124 3.69929575920105 5.386025428771973
Loss :  1.6514110565185547 2.729173421859741 4.380584716796875
Loss :  1.6987286806106567 3.5874266624450684 5.2861552238464355
Loss :  1.6421858072280884 2.977597713470459 4.619783401489258
Loss :  1.6889435052871704 2.6161272525787354 4.305070877075195
Loss :  1.643884301185608 3.1825249195098877 4.826409339904785
Loss :  1.7242597341537476 3.218222141265869 4.942481994628906
Loss :  1.6705830097198486 3.4463837146759033 5.116966724395752
Loss :  1.6560996770858765 2.681358814239502 4.337458610534668
Loss :  1.6587762832641602 2.9048783779144287 4.563654899597168
Loss :  1.6981239318847656 3.0383965969085693 4.736520767211914
Loss :  1.689375877380371 3.1068007946014404 4.796176910400391
Loss :  1.6682465076446533 2.9604618549346924 4.628708362579346
Loss :  1.6353760957717896 3.0615947246551514 4.6969709396362305
Loss :  1.6620440483093262 2.756554126739502 4.418598175048828
Loss :  1.6544468402862549 3.0823168754577637 4.736763954162598
  batch 40 loss: 1.6544468402862549, 3.0823168754577637, 4.736763954162598
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6615747213363647 3.027275800704956 4.688850402832031
Loss :  1.6498936414718628 2.7259418964385986 4.375835418701172
Loss :  1.6676368713378906 3.042701244354248 4.710338115692139
Loss :  1.666031002998352 3.048887014389038 4.71491813659668
Loss :  1.6683231592178345 2.8173744678497314 4.4856977462768555
Loss :  1.6598032712936401 3.0223257541656494 4.68212890625
Loss :  1.6476540565490723 3.3199517726898193 4.9676055908203125
Loss :  1.6594634056091309 3.2026543617248535 4.862117767333984
Loss :  1.6314314603805542 3.225830078125 4.857261657714844
Loss :  1.6837736368179321 3.206892967224121 4.890666484832764
Loss :  1.6483688354492188 3.4371445178985596 5.085513114929199
Loss :  1.6665595769882202 3.2061543464660645 4.872714042663574
Loss :  1.683903455734253 3.069875478744507 4.75377893447876
Loss :  1.669537901878357 3.0424418449401855 4.711979866027832
Loss :  1.6725306510925293 3.4623937606811523 5.134924411773682
Loss :  1.6387228965759277 2.635777235031128 4.274499893188477
Loss :  1.686725378036499 3.194145441055298 4.880870819091797
Loss :  1.682660698890686 3.042128324508667 4.724789142608643
Loss :  1.6971142292022705 3.3400561809539795 5.03717041015625
Loss :  1.6740630865097046 2.525904417037964 4.199967384338379
  batch 60 loss: 1.6740630865097046, 2.525904417037964, 4.199967384338379
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.673316478729248 3.2009408473968506 4.8742570877075195
Loss :  1.6710689067840576 3.0621633529663086 4.733232498168945
Loss :  1.6796132326126099 3.0979294776916504 4.777542591094971
Loss :  1.6599607467651367 2.6903672218322754 4.350327968597412
Loss :  1.6569286584854126 2.3821842670440674 4.0391130447387695
Loss :  5.374039173126221 4.400599479675293 9.774639129638672
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.277961730957031 4.349230766296387 9.627192497253418
Loss :  5.3727874755859375 4.2409796714782715 9.613767623901367
Loss :  4.493441581726074 4.309144496917725 8.80258560180664
Total LOSS train 4.733916957561786 valid 9.454546213150024
CE LOSS train 1.6658641833525438 valid 1.1233603954315186
Contrastive LOSS train 3.068052757703341 valid 1.0772861242294312
EPOCH 133:
Loss :  1.6531506776809692 2.6963038444519043 4.349454402923584
Loss :  1.6670783758163452 3.1836860179901123 4.850764274597168
Loss :  1.6535223722457886 2.535198450088501 4.188720703125
Loss :  1.6577730178833008 2.7560243606567383 4.413797378540039
Loss :  1.681906819343567 3.513901710510254 5.195808410644531
Loss :  1.665332317352295 2.6143453121185303 4.279677391052246
Loss :  1.661551594734192 3.5424437522888184 5.203995227813721
Loss :  1.652389645576477 2.41632080078125 4.0687103271484375
Loss :  1.6567631959915161 2.7478890419006348 4.404652118682861
Loss :  1.6102170944213867 2.798919677734375 4.409136772155762
Loss :  1.670493483543396 2.7873518466949463 4.457845211029053
Loss :  1.7302309274673462 3.1418304443359375 4.872061252593994
Loss :  1.6738109588623047 2.9928812980651855 4.66669225692749
Loss :  1.6655755043029785 3.3952713012695312 5.06084680557251
Loss :  1.645416498184204 2.842052698135376 4.48746919631958
Loss :  1.652864933013916 3.0303280353546143 4.683193206787109
Loss :  1.6620784997940063 2.623443365097046 4.285521984100342
Loss :  1.6616064310073853 2.855760097503662 4.517366409301758
Loss :  1.6681370735168457 2.769531726837158 4.437668800354004
Loss :  1.6258265972137451 3.0836994647979736 4.709526062011719
  batch 20 loss: 1.6258265972137451, 3.0836994647979736, 4.709526062011719
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6614323854446411 2.658998966217041 4.320431232452393
Loss :  1.6773689985275269 2.5809543132781982 4.2583231925964355
Loss :  1.6484427452087402 2.77876615524292 4.42720890045166
Loss :  1.6894574165344238 2.713428020477295 4.402885437011719
Loss :  1.6867042779922485 3.3378078937530518 5.02451229095459
Loss :  1.6515841484069824 2.788555145263672 4.440139293670654
Loss :  1.698804497718811 3.3755366802215576 5.074341297149658
Loss :  1.6420727968215942 2.5909688472747803 4.233041763305664
Loss :  1.6889986991882324 3.200975179672241 4.8899736404418945
Loss :  1.6436574459075928 2.971851110458374 4.615508556365967
Loss :  1.7243009805679321 3.2913992404937744 5.015700340270996
Loss :  1.6703602075576782 2.8687875270843506 4.539147853851318
Loss :  1.6561391353607178 2.939321994781494 4.595460891723633
Loss :  1.6590452194213867 3.0262291431427 4.685274124145508
Loss :  1.698093056678772 2.8341193199157715 4.532212257385254
Loss :  1.6896979808807373 2.9562060832977295 4.645904064178467
Loss :  1.6684283018112183 2.912024974822998 4.580453395843506
Loss :  1.635248064994812 2.973802089691162 4.609050273895264
Loss :  1.6620110273361206 2.4405198097229004 4.1025309562683105
Loss :  1.6545711755752563 3.2818193435668945 4.936390399932861
  batch 40 loss: 1.6545711755752563, 3.2818193435668945, 4.936390399932861
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6616054773330688 3.1031506061553955 4.764756202697754
Loss :  1.6498651504516602 2.7865967750549316 4.436461925506592
Loss :  1.6676174402236938 2.930079221725464 4.597696781158447
Loss :  1.6655687093734741 2.544306755065918 4.209875583648682
Loss :  1.6679413318634033 2.6292848587036133 4.2972259521484375
Loss :  1.660031795501709 3.2140533924102783 4.874085426330566
Loss :  1.647436499595642 3.2878708839416504 4.935307502746582
Loss :  1.6591076850891113 3.1552412509918213 4.814349174499512
Loss :  1.6316759586334229 2.8642518520355225 4.495927810668945
Loss :  1.683554768562317 3.015343427658081 4.6988983154296875
Loss :  1.648532748222351 3.0006303787231445 4.649163246154785
Loss :  1.6664866209030151 3.0735864639282227 4.740073204040527
Loss :  1.6837854385375977 2.8294637203216553 4.513249397277832
Loss :  1.6692248582839966 3.0454392433166504 4.714663982391357
Loss :  1.6722712516784668 3.3412086963653564 5.013480186462402
Loss :  1.6383901834487915 2.782141923904419 4.4205322265625
Loss :  1.6867986917495728 3.2832751274108887 4.970073699951172
Loss :  1.6827480792999268 3.088136911392212 4.770884990692139
Loss :  1.6972402334213257 3.293050527572632 4.990290641784668
Loss :  1.6740235090255737 2.8276584148406982 4.501681804656982
  batch 60 loss: 1.6740235090255737, 2.8276584148406982, 4.501681804656982
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6735124588012695 2.90256667137146 4.576079368591309
Loss :  1.6704922914505005 3.4206438064575195 5.0911359786987305
Loss :  1.6792874336242676 2.8974757194519043 4.576763153076172
Loss :  1.6601039171218872 2.666836738586426 4.326940536499023
Loss :  1.6567955017089844 2.3546876907348633 4.011483192443848
Loss :  5.34635066986084 4.49012565612793 9.83647632598877
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.231168270111084 4.36667537689209 9.597843170166016
Loss :  5.3437347412109375 4.253631591796875 9.597366333007812
Loss :  4.474050521850586 4.003649711608887 8.477700233459473
Total LOSS train 4.6071150559645435 valid 9.377346515655518
CE LOSS train 1.6657883479044988 valid 1.1185126304626465
Contrastive LOSS train 2.9413267098940334 valid 1.0009124279022217
EPOCH 134:
Loss :  1.6530134677886963 2.735809564590454 4.38882303237915
Loss :  1.6669119596481323 3.215327262878418 4.88223934173584
Loss :  1.6532899141311646 2.2489640712738037 3.902254104614258
Loss :  1.6573647260665894 3.0214078426361084 4.678772449493408
Loss :  1.6816320419311523 2.9100561141967773 4.59168815612793
Loss :  1.665160894393921 2.604637384414673 4.269798278808594
Loss :  1.6615887880325317 3.1907639503479004 4.852352619171143
Loss :  1.6521637439727783 2.589456081390381 4.241620063781738
Loss :  1.656579613685608 2.9488911628723145 4.605470657348633
Loss :  1.6103174686431885 2.591233015060425 4.201550483703613
Loss :  1.6705005168914795 2.735825538635254 4.4063262939453125
Loss :  1.7300299406051636 3.0865373611450195 4.816567420959473
Loss :  1.6740363836288452 3.4409737586975098 5.1150102615356445
Loss :  1.6654331684112549 3.0776166915893555 4.743049621582031
Loss :  1.645311951637268 2.8138625621795654 4.459174633026123
Loss :  1.6527718305587769 3.670544147491455 5.3233160972595215
Loss :  1.6623085737228394 2.6082022190093994 4.270510673522949
Loss :  1.6619170904159546 3.035862445831299 4.697779655456543
Loss :  1.6686517000198364 2.4440486431121826 4.112700462341309
Loss :  1.6258444786071777 3.193425416946411 4.819270133972168
  batch 20 loss: 1.6258444786071777, 3.193425416946411, 4.819270133972168
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6611099243164062 3.0485830307006836 4.70969295501709
Loss :  1.6773744821548462 2.5725338459014893 4.249908447265625
Loss :  1.6480540037155151 3.05875563621521 4.7068095207214355
Loss :  1.6893723011016846 2.851640462875366 4.541012763977051
Loss :  1.6866848468780518 3.6159026622772217 5.302587509155273
Loss :  1.6513746976852417 2.540076494216919 4.191451072692871
Loss :  1.6987810134887695 3.0359385013580322 4.734719276428223
Loss :  1.641910195350647 2.695956230163574 4.337866306304932
Loss :  1.6890908479690552 2.8741888999938965 4.563279628753662
Loss :  1.643473744392395 3.0011279582977295 4.644601821899414
Loss :  1.724212884902954 3.41284441947937 5.137057304382324
Loss :  1.6703585386276245 3.220914363861084 4.891273021697998
Loss :  1.6558849811553955 3.058354616165161 4.714239597320557
Loss :  1.6588478088378906 2.5050573348999023 4.163905143737793
Loss :  1.698255181312561 2.8758232593536377 4.574078559875488
Loss :  1.6895767450332642 3.141406536102295 4.8309831619262695
Loss :  1.6682839393615723 2.6808230876922607 4.349106788635254
Loss :  1.635399341583252 2.955941677093506 4.591341018676758
Loss :  1.661747932434082 2.6735305786132812 4.335278511047363
Loss :  1.6540381908416748 3.5021424293518066 5.156180381774902
  batch 40 loss: 1.6540381908416748, 3.5021424293518066, 5.156180381774902
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6613386869430542 3.096839666366577 4.758178234100342
Loss :  1.6499336957931519 3.1841020584106445 4.834035873413086
Loss :  1.6676795482635498 2.992150068283081 4.659829616546631
Loss :  1.6655491590499878 2.825228214263916 4.490777492523193
Loss :  1.6680837869644165 2.9384353160858154 4.6065192222595215
Loss :  1.6598351001739502 3.1383535861968994 4.79818868637085
Loss :  1.6474965810775757 3.1454150676727295 4.792911529541016
Loss :  1.6591448783874512 3.085387706756592 4.744532585144043
Loss :  1.6313536167144775 3.5350263118743896 5.166379928588867
Loss :  1.6836518049240112 3.0580995082855225 4.741751194000244
Loss :  1.6481140851974487 3.1086983680725098 4.756812572479248
Loss :  1.6662930250167847 3.3262245655059814 4.992517471313477
Loss :  1.6837522983551025 3.1427900791168213 4.826542377471924
Loss :  1.66953706741333 3.4460067749023438 5.115543842315674
Loss :  1.6723042726516724 3.3825361728668213 5.054840564727783
Loss :  1.6382830142974854 2.7490570545196533 4.387340068817139
Loss :  1.6865240335464478 3.230689764022827 4.9172139167785645
Loss :  1.6830304861068726 3.075057029724121 4.758087635040283
Loss :  1.696936011314392 3.5539605617523193 5.250896453857422
Loss :  1.6733675003051758 3.176072120666504 4.84943962097168
  batch 60 loss: 1.6733675003051758, 3.176072120666504, 4.84943962097168
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6732416152954102 3.398883581161499 5.072125434875488
Loss :  1.670852541923523 2.973468065261841 4.644320487976074
Loss :  1.6795542240142822 3.149683952331543 4.829237937927246
Loss :  1.6598812341690063 2.894723415374756 4.554604530334473
Loss :  1.6567578315734863 2.906290292739868 4.563048362731934
Loss :  5.475967884063721 4.458673477172852 9.934640884399414
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.365102291107178 4.328154563903809 9.693256378173828
Loss :  5.472482681274414 4.218348026275635 9.69083023071289
Loss :  4.596700668334961 4.314384937286377 8.91108512878418
Total LOSS train 4.681066505725568 valid 9.557453155517578
CE LOSS train 1.6657100915908813 valid 1.1491751670837402
Contrastive LOSS train 3.0153564086327185 valid 1.0785962343215942
EPOCH 135:
Loss :  1.6530544757843018 2.474285840988159 4.127340316772461
Loss :  1.666813850402832 3.1651713848114014 4.8319854736328125
Loss :  1.6535342931747437 2.594890832901001 4.248425006866455
Loss :  1.6576288938522339 3.2324578762054443 4.890086650848389
Loss :  1.681767225265503 3.2025251388549805 4.8842926025390625
Loss :  1.665230393409729 2.807652711868286 4.472883224487305
Loss :  1.6612610816955566 3.523984432220459 5.185245513916016
Loss :  1.6518913507461548 2.8153297901153564 4.467221260070801
Loss :  1.656678318977356 2.847181558609009 4.503859996795654
Loss :  1.609938621520996 2.6858842372894287 4.295823097229004
Loss :  1.6703059673309326 2.6930205821990967 4.363326549530029
Loss :  1.7304977178573608 3.371880054473877 5.102377891540527
Loss :  1.6738452911376953 3.2250900268554688 4.898935317993164
Loss :  1.6653975248336792 3.225170135498047 4.890567779541016
Loss :  1.6452701091766357 2.6643853187561035 4.30965518951416
Loss :  1.6528286933898926 3.5367579460144043 5.189586639404297
Loss :  1.6621849536895752 2.627146005630493 4.289330959320068
Loss :  1.6615866422653198 3.0832927227020264 4.744879245758057
Loss :  1.668470859527588 2.800478219985962 4.468949317932129
Loss :  1.625654935836792 3.1722471714019775 4.7979021072387695
  batch 20 loss: 1.625654935836792, 3.1722471714019775, 4.7979021072387695
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6609208583831787 2.860276937484741 4.52119779586792
Loss :  1.6772239208221436 2.9647469520568848 4.641970634460449
Loss :  1.6477380990982056 2.7451272010803223 4.392865180969238
Loss :  1.6892045736312866 2.995100498199463 4.684305191040039
Loss :  1.6867163181304932 3.543785333633423 5.230501651763916
Loss :  1.651027798652649 3.206943988800049 4.857971668243408
Loss :  1.6984840631484985 3.486513376235962 5.18499755859375
Loss :  1.641674518585205 2.7754926681518555 4.4171671867370605
Loss :  1.6888847351074219 2.7418901920318604 4.430774688720703
Loss :  1.6432522535324097 3.5348806381225586 5.178133010864258
Loss :  1.7239238023757935 3.734034538269043 5.457958221435547
Loss :  1.6704009771347046 3.3576834201812744 5.0280842781066895
Loss :  1.6558197736740112 3.271350145339966 4.9271697998046875
Loss :  1.658491611480713 2.65617036819458 4.314661979675293
Loss :  1.6983104944229126 2.980050563812256 4.678360939025879
Loss :  1.6892664432525635 2.931217908859253 4.620484352111816
Loss :  1.6680290699005127 3.057373285293579 4.725402355194092
Loss :  1.635321021080017 2.973869800567627 4.609190940856934
Loss :  1.6616032123565674 2.788581371307373 4.4501848220825195
Loss :  1.6537569761276245 3.3729774951934814 5.026734352111816
  batch 40 loss: 1.6537569761276245, 3.3729774951934814, 5.026734352111816
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6612968444824219 3.357806444168091 5.019103050231934
Loss :  1.6497230529785156 3.544710636138916 5.194433689117432
Loss :  1.6674554347991943 3.1664388179779053 4.8338942527771
Loss :  1.6658798456192017 2.942373752593994 4.608253479003906
Loss :  1.6684119701385498 3.040283203125 4.708695411682129
Loss :  1.6596062183380127 3.2248103618621826 4.884416580200195
Loss :  1.6472591161727905 3.222368001937866 4.869626998901367
Loss :  1.6591867208480835 3.721043348312378 5.380229949951172
Loss :  1.6308358907699585 3.649376153945923 5.280211925506592
Loss :  1.6837985515594482 3.5519042015075684 5.2357025146484375
Loss :  1.647701382637024 3.4298207759857178 5.077522277832031
Loss :  1.6660515069961548 3.584172010421753 5.250223636627197
Loss :  1.6835731267929077 3.491614580154419 5.175187587738037
Loss :  1.669366478919983 3.565760612487793 5.235126972198486
Loss :  1.6720311641693115 3.2547223567962646 4.926753520965576
Loss :  1.6381210088729858 3.0326058864593506 4.670726776123047
Loss :  1.6864006519317627 3.384287118911743 5.070687770843506
Loss :  1.6828300952911377 3.214890480041504 4.8977203369140625
Loss :  1.696671485900879 3.352600336074829 5.049271583557129
Loss :  1.6735931634902954 2.9521443843841553 4.62573766708374
  batch 60 loss: 1.6735931634902954, 2.9521443843841553, 4.62573766708374
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.673049807548523 3.266118288040161 4.9391679763793945
Loss :  1.6708120107650757 3.191688299179077 4.862500190734863
Loss :  1.6793173551559448 3.291184663772583 4.970501899719238
Loss :  1.6595090627670288 2.829939603805542 4.489448547363281
Loss :  1.6562621593475342 2.746695041656494 4.402956962585449
Loss :  5.440905570983887 4.416121482849121 9.857027053833008
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.335628032684326 4.334275722503662 9.669903755187988
Loss :  5.424572467803955 4.219620227813721 9.644192695617676
Loss :  4.55245304107666 4.265397548675537 8.817850112915039
Total LOSS train 4.799982958573562 valid 9.497243404388428
CE LOSS train 1.6655790127240695 valid 1.138113260269165
Contrastive LOSS train 3.1344039696913497 valid 1.0663493871688843
EPOCH 136:
Loss :  1.6529182195663452 2.7595503330230713 4.412468433380127
Loss :  1.6668039560317993 3.16634464263916 4.83314847946167
Loss :  1.653308629989624 2.5349013805389404 4.1882100105285645
Loss :  1.6574604511260986 3.256344795227051 4.91380500793457
Loss :  1.6815848350524902 3.269503355026245 4.951087951660156
Loss :  1.6653143167495728 2.4946722984313965 4.15998649597168
Loss :  1.6611016988754272 3.33030366897583 4.991405487060547
Loss :  1.6518105268478394 2.4585483074188232 4.110358715057373
Loss :  1.6564522981643677 2.7655484676361084 4.422000885009766
Loss :  1.6099804639816284 3.2175586223602295 4.827538967132568
Loss :  1.6700936555862427 3.2006101608276367 4.87070369720459
Loss :  1.7296476364135742 3.415888547897339 5.145536422729492
Loss :  1.6736960411071777 3.2712597846984863 4.944955825805664
Loss :  1.66542649269104 3.259507894515991 4.924934387207031
Loss :  1.644894003868103 2.9015471935272217 4.546441078186035
Loss :  1.6525450944900513 3.4142019748687744 5.066747188568115
Loss :  1.661857008934021 2.801043748855591 4.462900638580322
Loss :  1.661454200744629 3.296032190322876 4.957486152648926
Loss :  1.6682058572769165 3.0217649936676025 4.689970970153809
Loss :  1.6252262592315674 3.011117696762085 4.636343955993652
  batch 20 loss: 1.6252262592315674, 3.011117696762085, 4.636343955993652
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6609424352645874 3.0323336124420166 4.6932759284973145
Loss :  1.6771570444107056 2.9067366123199463 4.583893775939941
Loss :  1.6478511095046997 2.66321063041687 4.311061859130859
Loss :  1.6892414093017578 2.9910311698913574 4.680272579193115
Loss :  1.686533808708191 3.5291757583618164 5.215709686279297
Loss :  1.65105140209198 2.9580867290496826 4.609138011932373
Loss :  1.6987384557724 3.373798370361328 5.072536945343018
Loss :  1.6416702270507812 2.9778921604156494 4.619562149047852
Loss :  1.6887853145599365 2.602909803390503 4.2916951179504395
Loss :  1.6431854963302612 3.4665679931640625 5.109753608703613
Loss :  1.7240182161331177 3.3119211196899414 5.0359392166137695
Loss :  1.6704378128051758 3.359107255935669 5.029544830322266
Loss :  1.6558549404144287 2.799065589904785 4.454920768737793
Loss :  1.6586171388626099 2.864790916442871 4.523407936096191
Loss :  1.6982357501983643 3.1441164016723633 4.842351913452148
Loss :  1.689362645149231 3.251720428466797 4.941082954406738
Loss :  1.6681666374206543 2.889702081680298 4.557868957519531
Loss :  1.6354010105133057 3.072676658630371 4.708077430725098
Loss :  1.6615763902664185 2.9302709102630615 4.5918474197387695
Loss :  1.653944730758667 3.0942647457122803 4.748209476470947
  batch 40 loss: 1.653944730758667, 3.0942647457122803, 4.748209476470947
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6612972021102905 3.1574394702911377 4.818736553192139
Loss :  1.64991295337677 2.9516825675964355 4.601595401763916
Loss :  1.667580485343933 3.1044833660125732 4.772063732147217
Loss :  1.6655373573303223 2.761049509048462 4.426587104797363
Loss :  1.668286919593811 2.6844606399536133 4.352747440338135
Loss :  1.6597379446029663 3.1188113689422607 4.7785491943359375
Loss :  1.6476812362670898 3.223583459854126 4.871264457702637
Loss :  1.6592930555343628 2.8251214027404785 4.484414577484131
Loss :  1.631344199180603 3.267383098602295 4.8987274169921875
Loss :  1.6837708950042725 3.1349053382873535 4.818675994873047
Loss :  1.6480509042739868 3.1675047874450684 4.815555572509766
Loss :  1.6665436029434204 3.5115134716033936 5.1780571937561035
Loss :  1.683726191520691 3.3903982639312744 5.074124336242676
Loss :  1.6695585250854492 3.05483078956604 4.72438907623291
Loss :  1.6721642017364502 3.515843629837036 5.188007831573486
Loss :  1.6382269859313965 3.101985454559326 4.740212440490723
Loss :  1.6864086389541626 3.3461995124816895 5.0326080322265625
Loss :  1.682800531387329 2.6745848655700684 4.357385635375977
Loss :  1.6968055963516235 3.201310634613037 4.898116111755371
Loss :  1.6735597848892212 3.083691358566284 4.757251262664795
  batch 60 loss: 1.6735597848892212, 3.083691358566284, 4.757251262664795
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6733530759811401 3.315793752670288 4.989146709442139
Loss :  1.671043038368225 3.097080707550049 4.768123626708984
Loss :  1.6794312000274658 2.9910991191864014 4.670530319213867
Loss :  1.6597439050674438 2.87392520904541 4.5336689949035645
Loss :  1.6563626527786255 2.999044895172119 4.655407428741455
Loss :  5.381871700286865 4.378230571746826 9.760102272033691
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.279567241668701 4.354832649230957 9.6343994140625
Loss :  5.3705058097839355 4.16126012802124 9.531765937805176
Loss :  4.499817848205566 4.173322677612305 8.673140525817871
Total LOSS train 4.736648119412935 valid 9.39985203742981
CE LOSS train 1.665581149321336 valid 1.1249544620513916
Contrastive LOSS train 3.0710670104393594 valid 1.0433306694030762
EPOCH 137:
Loss :  1.6531028747558594 2.5907390117645264 4.243842124938965
Loss :  1.6669540405273438 3.187918186187744 4.854872226715088
Loss :  1.653465747833252 2.4686152935028076 4.1220808029174805
Loss :  1.6575183868408203 2.6448493003845215 4.302367687225342
Loss :  1.681961178779602 3.3446383476257324 5.026599407196045
Loss :  1.66555655002594 2.3435957431793213 4.009152412414551
Loss :  1.6613643169403076 3.303114414215088 4.964478492736816
Loss :  1.6518328189849854 2.3039028644561768 3.955735683441162
Loss :  1.6566284894943237 2.333681344985962 3.990309715270996
Loss :  1.609843134880066 2.7121968269348145 4.32204008102417
Loss :  1.6701390743255615 2.652294635772705 4.3224334716796875
Loss :  1.730287790298462 3.325745105743408 5.056033134460449
Loss :  1.6738121509552002 2.970564126968384 4.644376277923584
Loss :  1.6654210090637207 3.1412408351898193 4.806661605834961
Loss :  1.6454663276672363 2.641484022140503 4.28695011138916
Loss :  1.6524152755737305 3.0953941345214844 4.747809410095215
Loss :  1.661808967590332 2.5362374782562256 4.198046684265137
Loss :  1.6614067554473877 3.0297281742095947 4.691134929656982
Loss :  1.6680967807769775 2.7428557872772217 4.410952568054199
Loss :  1.6254537105560303 2.9203808307647705 4.545834541320801
  batch 20 loss: 1.6254537105560303, 2.9203808307647705, 4.545834541320801
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6610469818115234 2.7530148029327393 4.414061546325684
Loss :  1.6771601438522339 2.8749048709869385 4.552064895629883
Loss :  1.6481231451034546 3.156467914581299 4.804591178894043
Loss :  1.68898344039917 3.063579559326172 4.752562999725342
Loss :  1.686396837234497 3.422394275665283 5.108791351318359
Loss :  1.6512986421585083 2.865540027618408 4.516838550567627
Loss :  1.6983258724212646 3.345728874206543 5.044054985046387
Loss :  1.6417710781097412 2.72489595413208 4.366666793823242
Loss :  1.688529372215271 3.0308709144592285 4.719400405883789
Loss :  1.643234372138977 3.194329023361206 4.837563514709473
Loss :  1.7240095138549805 2.9466638565063477 4.670673370361328
Loss :  1.669811487197876 2.9645540714263916 4.634365558624268
Loss :  1.6557140350341797 2.924055814743042 4.579770088195801
Loss :  1.6586610078811646 2.846832036972046 4.5054931640625
Loss :  1.697795033454895 2.9189252853393555 4.616720199584961
Loss :  1.6893354654312134 3.0541059970855713 4.743441581726074
Loss :  1.6680774688720703 2.71714186668396 4.385219573974609
Loss :  1.6346213817596436 2.9395956993103027 4.574216842651367
Loss :  1.6612932682037354 2.498138666152954 4.1594319343566895
Loss :  1.6539306640625 2.878286600112915 4.532217025756836
  batch 40 loss: 1.6539306640625, 2.878286600112915, 4.532217025756836
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6608479022979736 2.8348629474639893 4.495710849761963
Loss :  1.6493197679519653 2.9883036613464355 4.637623310089111
Loss :  1.667410135269165 2.925095796585083 4.592505931854248
Loss :  1.6649730205535889 2.4442412853240967 4.1092143058776855
Loss :  1.6679041385650635 2.389665126800537 4.05756950378418
Loss :  1.6592891216278076 3.0444321632385254 4.703721046447754
Loss :  1.646820306777954 3.215035915374756 4.861856460571289
Loss :  1.6587128639221191 3.039242744445801 4.69795560836792
Loss :  1.6310573816299438 2.9722275733947754 4.60328483581543
Loss :  1.6833851337432861 3.008744239807129 4.692129135131836
Loss :  1.6478822231292725 2.567152500152588 4.215034484863281
Loss :  1.6661334037780762 3.078333854675293 4.744467258453369
Loss :  1.683488130569458 3.09702205657959 4.780509948730469
Loss :  1.668864369392395 2.9075961112976074 4.576460361480713
Loss :  1.6719063520431519 3.2840514183044434 4.955957889556885
Loss :  1.6377215385437012 2.6117143630981445 4.249435901641846
Loss :  1.6862655878067017 2.7867279052734375 4.47299337387085
Loss :  1.6825792789459229 2.7665395736694336 4.449118614196777
Loss :  1.6966924667358398 3.031378746032715 4.728071212768555
Loss :  1.6733616590499878 2.525303363800049 4.198665142059326
  batch 60 loss: 1.6733616590499878, 2.525303363800049, 4.198665142059326
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6730241775512695 3.170348644256592 4.843372821807861
Loss :  1.670729160308838 2.9545836448669434 4.625312805175781
Loss :  1.6789276599884033 3.0345680713653564 4.71349573135376
Loss :  1.6597437858581543 2.473436117172241 4.133179664611816
Loss :  1.6563740968704224 2.446413993835449 4.102787971496582
Loss :  5.403755187988281 4.427913188934326 9.831668853759766
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.287596702575684 4.429287433624268 9.71688461303711
Loss :  5.387988090515137 4.2770466804504395 9.665035247802734
Loss :  4.525407791137695 4.180126667022705 8.705533981323242
Total LOSS train 4.542466016916128 valid 9.479780673980713
CE LOSS train 1.6654472039296078 valid 1.1313519477844238
Contrastive LOSS train 2.8770188368283787 valid 1.0450316667556763
EPOCH 138:
Loss :  1.652802586555481 2.7609362602233887 4.41373872756958
Loss :  1.6668051481246948 2.9109814167022705 4.577786445617676
Loss :  1.6531397104263306 2.3748342990875244 4.0279741287231445
Loss :  1.6571329832077026 3.0590462684631348 4.716179370880127
Loss :  1.6813957691192627 3.0892446041107178 4.7706403732299805
Loss :  1.6651607751846313 2.8900821208953857 4.555243015289307
Loss :  1.6611201763153076 2.977200984954834 4.6383209228515625
Loss :  1.651782512664795 2.553910970687866 4.205693244934082
Loss :  1.656296968460083 2.529325246810913 4.185622215270996
Loss :  1.6100608110427856 2.9557738304138184 4.5658345222473145
Loss :  1.6699604988098145 3.114051103591919 4.7840118408203125
Loss :  1.7297759056091309 3.079207420349121 4.808983325958252
Loss :  1.6738585233688354 3.0517208576202393 4.725579261779785
Loss :  1.6652514934539795 3.2291312217712402 4.894382476806641
Loss :  1.6450937986373901 2.716813802719116 4.361907482147217
Loss :  1.6525121927261353 3.1912031173706055 4.843715190887451
Loss :  1.6619658470153809 2.765986919403076 4.427952766418457
Loss :  1.6611692905426025 2.663311004638672 4.324480056762695
Loss :  1.6684128046035767 2.5791232585906982 4.2475361824035645
Loss :  1.6254596710205078 3.1940953731536865 4.819555282592773
  batch 20 loss: 1.6254596710205078, 3.1940953731536865, 4.819555282592773
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.660851001739502 3.1095635890960693 4.770414352416992
Loss :  1.6770029067993164 2.8084747791290283 4.485477447509766
Loss :  1.6478071212768555 3.157912492752075 4.805719375610352
Loss :  1.6889989376068115 3.1093435287475586 4.798342704772949
Loss :  1.6863608360290527 3.4962170124053955 5.182578086853027
Loss :  1.6508886814117432 2.5145914554595947 4.165480136871338
Loss :  1.698392629623413 3.097378969192505 4.795771598815918
Loss :  1.6415354013442993 2.859692096710205 4.501227378845215
Loss :  1.6886106729507446 2.627892017364502 4.316502571105957
Loss :  1.6431100368499756 3.193279981613159 4.836390018463135
Loss :  1.7239843606948853 3.27534818649292 4.999332427978516
Loss :  1.670254111289978 3.1798696517944336 4.850123882293701
Loss :  1.6556870937347412 2.705303907394409 4.36099100112915
Loss :  1.6583473682403564 2.9700369834899902 4.628384590148926
Loss :  1.697845458984375 3.08003830909729 4.777883529663086
Loss :  1.6893318891525269 2.9288995265960693 4.618231296539307
Loss :  1.667910099029541 2.7467942237854004 4.414704322814941
Loss :  1.6350687742233276 3.2261903285980225 4.8612589836120605
Loss :  1.6612203121185303 2.7349798679351807 4.396200180053711
Loss :  1.653709053993225 2.8896212577819824 4.543330192565918
  batch 40 loss: 1.653709053993225, 2.8896212577819824, 4.543330192565918
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6610063314437866 3.2116520404815674 4.8726582527160645
Loss :  1.649340271949768 2.748932123184204 4.398272514343262
Loss :  1.6672694683074951 3.161086320877075 4.82835578918457
Loss :  1.6650536060333252 2.3882951736450195 4.053348541259766
Loss :  1.667762279510498 2.440112352371216 4.107874870300293
Loss :  1.6592674255371094 3.084489107131958 4.743756294250488
Loss :  1.6467795372009277 3.1448206901550293 4.791600227355957
Loss :  1.658460021018982 3.342364549636841 5.000824451446533
Loss :  1.6311191320419312 3.2427518367767334 4.873870849609375
Loss :  1.682755708694458 2.958056688308716 4.640812397003174
Loss :  1.6475285291671753 2.9330992698669434 4.580627918243408
Loss :  1.6660118103027344 3.019453525543213 4.685465335845947
Loss :  1.6834651231765747 3.033935308456421 4.717400550842285
Loss :  1.6688898801803589 2.946383476257324 4.615273475646973
Loss :  1.6717181205749512 3.4174139499664307 5.089132308959961
Loss :  1.6375240087509155 2.9453301429748535 4.582854270935059
Loss :  1.6861287355422974 3.38802433013916 5.074152946472168
Loss :  1.6825932264328003 3.253180742263794 4.935773849487305
Loss :  1.6965464353561401 3.439063787460327 5.135610103607178
Loss :  1.6731020212173462 2.9054291248321533 4.578531265258789
  batch 60 loss: 1.6731020212173462, 2.9054291248321533, 4.578531265258789
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6729952096939087 3.444200277328491 5.1171956062316895
Loss :  1.6705082654953003 3.0071139335632324 4.677622318267822
Loss :  1.6792229413986206 2.95450496673584 4.63372802734375
Loss :  1.6594535112380981 3.0541110038757324 4.713564395904541
Loss :  1.6564143896102905 2.6223866939544678 4.278800964355469
Loss :  5.37003231048584 4.333216190338135 9.703248977661133
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.258233070373535 4.326506614685059 9.584739685058594
Loss :  5.352444171905518 4.307527542114258 9.659971237182617
Loss :  4.5081658363342285 4.217070579528809 8.725236892700195
Total LOSS train 4.642009052863488 valid 9.418299198150635
CE LOSS train 1.6653383108285758 valid 1.1270414590835571
Contrastive LOSS train 2.976670764042781 valid 1.0542676448822021
EPOCH 139:
Loss :  1.6526676416397095 2.933671474456787 4.586338996887207
Loss :  1.666638731956482 3.278550863265991 4.945189476013184
Loss :  1.6528249979019165 2.906161069869995 4.558986186981201
Loss :  1.6563093662261963 3.141050100326538 4.797359466552734
Loss :  1.6813405752182007 3.0583393573760986 4.73967981338501
Loss :  1.6645492315292358 3.180649995803833 4.845199108123779
Loss :  1.6613562107086182 3.3265321254730225 4.987888336181641
Loss :  1.6515778303146362 2.706993341445923 4.3585710525512695
Loss :  1.6555458307266235 2.8474252223968506 4.502971172332764
Loss :  1.6097080707550049 2.768113136291504 4.37782096862793
Loss :  1.6699105501174927 3.2444562911987305 4.914366722106934
Loss :  1.7300904989242554 3.5535454750061035 5.283636093139648
Loss :  1.6736862659454346 3.5693559646606445 5.2430419921875
Loss :  1.66523015499115 3.6395938396453857 5.304823875427246
Loss :  1.6445415019989014 3.2888543605804443 4.933395862579346
Loss :  1.6526621580123901 3.364473819732666 5.017136096954346
Loss :  1.6619874238967896 2.831096649169922 4.493083953857422
Loss :  1.6613622903823853 3.449556589126587 5.110918998718262
Loss :  1.6682316064834595 2.940138101577759 4.608369827270508
Loss :  1.6254409551620483 3.1247408390045166 4.750181674957275
  batch 20 loss: 1.6254409551620483, 3.1247408390045166, 4.750181674957275
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.660659670829773 3.267533540725708 4.928193092346191
Loss :  1.6770707368850708 3.0232431888580322 4.700314044952393
Loss :  1.6476340293884277 3.09049391746521 4.738127708435059
Loss :  1.688948392868042 3.0722291469573975 4.7611775398254395
Loss :  1.686319351196289 3.43906307220459 5.125382423400879
Loss :  1.6507270336151123 2.687551975250244 4.338278770446777
Loss :  1.6984596252441406 3.4983198642730713 5.196779251098633
Loss :  1.6416655778884888 3.1596484184265137 4.801313877105713
Loss :  1.6887434720993042 2.7024996280670166 4.391242980957031
Loss :  1.6431440114974976 3.3702876567840576 5.013431549072266
Loss :  1.7238963842391968 3.360877752304077 5.084774017333984
Loss :  1.6705282926559448 3.41628360748291 5.0868120193481445
Loss :  1.65597403049469 3.073317766189575 4.729291915893555
Loss :  1.6583447456359863 3.395212411880493 5.053557395935059
Loss :  1.6979886293411255 3.321337938308716 5.019326686859131
Loss :  1.6896607875823975 3.3433072566986084 5.032968044281006
Loss :  1.6679706573486328 3.097896099090576 4.765866756439209
Loss :  1.635446310043335 2.9519171714782715 4.587363243103027
Loss :  1.661383032798767 2.7531464099884033 4.414529323577881
Loss :  1.6537384986877441 3.1932034492492676 4.846941947937012
  batch 40 loss: 1.6537384986877441, 3.1932034492492676, 4.846941947937012
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6611559391021729 3.338160991668701 4.999317169189453
Loss :  1.649519920349121 3.4030168056488037 5.052536964416504
Loss :  1.6672950983047485 3.3147902488708496 4.982085227966309
Loss :  1.6653212308883667 2.9912571907043457 4.656578540802002
Loss :  1.6683000326156616 3.0957369804382324 4.764037132263184
Loss :  1.659248948097229 3.4081614017486572 5.067410469055176
Loss :  1.6474014520645142 3.142015218734741 4.789416790008545
Loss :  1.6589429378509521 3.440345525741577 5.099288463592529
Loss :  1.6310155391693115 3.15988826751709 4.7909040451049805
Loss :  1.6833198070526123 3.2407212257385254 4.924040794372559
Loss :  1.6473817825317383 3.4200022220611572 5.067383766174316
Loss :  1.6661490201950073 3.314373016357422 4.980522155761719
Loss :  1.6834492683410645 3.4972314834594727 5.180680751800537
Loss :  1.6692501306533813 3.4363694190979004 5.105619430541992
Loss :  1.671722412109375 3.3668549060821533 5.038577079772949
Loss :  1.6378614902496338 2.9602556228637695 4.598116874694824
Loss :  1.6861953735351562 3.1093711853027344 4.795566558837891
Loss :  1.6827107667922974 3.3351492881774902 5.017859935760498
Loss :  1.6965545415878296 3.356659412384033 5.053214073181152
Loss :  1.6731594800949097 3.3373684883117676 5.010528087615967
  batch 60 loss: 1.6731594800949097, 3.3373684883117676, 5.010528087615967
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6733158826828003 3.464871644973755 5.138187408447266
Loss :  1.6708699464797974 3.556063652038574 5.226933479309082
Loss :  1.6794599294662476 3.211829662322998 4.891289710998535
Loss :  1.65933096408844 3.00289249420166 4.6622233390808105
Loss :  1.656075119972229 3.1298117637634277 4.785886764526367
Loss :  5.438151836395264 4.424862861633301 9.863014221191406
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.338702201843262 4.457978248596191 9.796680450439453
Loss :  5.429016590118408 4.306563377380371 9.735580444335938
Loss :  4.535244941711426 4.118257522583008 8.653502464294434
Total LOSS train 4.871582111945519 valid 9.512194395065308
CE LOSS train 1.6653688027308537 valid 1.1338112354278564
Contrastive LOSS train 3.2062133385584906 valid 1.029564380645752
EPOCH 140:
Loss :  1.6528688669204712 3.094025135040283 4.746893882751465
Loss :  1.6671154499053955 3.343320369720459 5.010436058044434
Loss :  1.653404712677002 2.9630842208862305 4.616488933563232
Loss :  1.6574928760528564 3.2845988273620605 4.942091941833496
Loss :  1.6817208528518677 3.6447436809539795 5.326464653015137
Loss :  1.6657276153564453 2.6987295150756836 4.364457130432129
Loss :  1.6608266830444336 3.275951385498047 4.9367780685424805
Loss :  1.6516698598861694 2.7516045570373535 4.4032745361328125
Loss :  1.6561285257339478 3.0044503211975098 4.660578727722168
Loss :  1.6094343662261963 3.19931697845459 4.808751106262207
Loss :  1.670214295387268 2.988856077194214 4.6590704917907715
Loss :  1.730689525604248 3.0521509647369385 4.782840728759766
Loss :  1.6739850044250488 3.213707208633423 4.887692451477051
Loss :  1.6651474237442017 3.323585033416748 4.98873233795166
Loss :  1.6445438861846924 3.042100429534912 4.686644554138184
Loss :  1.652616024017334 3.0625698566436768 4.71518611907959
Loss :  1.6619691848754883 2.89656925201416 4.558538436889648
Loss :  1.6616201400756836 2.9569623470306396 4.618582725524902
Loss :  1.668320655822754 2.6445422172546387 4.312862873077393
Loss :  1.6251561641693115 3.4003233909606934 5.025479316711426
  batch 20 loss: 1.6251561641693115, 3.4003233909606934, 5.025479316711426
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6603662967681885 3.411637544631958 5.0720038414001465
Loss :  1.6770026683807373 2.9401919841766357 4.617194652557373
Loss :  1.6476799249649048 3.2764251232147217 4.924105167388916
Loss :  1.6895302534103394 3.1177594661712646 4.8072896003723145
Loss :  1.6870263814926147 3.2389321327209473 4.925958633422852
Loss :  1.6507207155227661 2.8168766498565674 4.467597484588623
Loss :  1.698678731918335 3.1307179927825928 4.829396724700928
Loss :  1.6414576768875122 3.063169002532959 4.704626560211182
Loss :  1.6884349584579468 2.707026243209839 4.395461082458496
Loss :  1.642844319343567 2.944058418273926 4.586902618408203
Loss :  1.7240705490112305 3.426471710205078 5.150542259216309
Loss :  1.6705799102783203 3.3922903537750244 5.062870025634766
Loss :  1.6556930541992188 2.813096523284912 4.468789577484131
Loss :  1.6583373546600342 2.7552454471588135 4.413582801818848
Loss :  1.69801664352417 2.799360513687134 4.497377395629883
Loss :  1.6892086267471313 2.914008378982544 4.603217124938965
Loss :  1.668005108833313 2.530473232269287 4.1984782218933105
Loss :  1.63533616065979 2.724623680114746 4.359959602355957
Loss :  1.6615700721740723 2.6810758113861084 4.342645645141602
Loss :  1.654023289680481 2.6518099308013916 4.305833339691162
  batch 40 loss: 1.654023289680481, 2.6518099308013916, 4.305833339691162
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.661194920539856 3.1625211238861084 4.823716163635254
Loss :  1.649620532989502 2.695775270462036 4.345396041870117
Loss :  1.6672840118408203 2.716402530670166 4.383686542510986
Loss :  1.6654787063598633 2.5887837409973145 4.254262447357178
Loss :  1.6681139469146729 2.363180637359619 4.031294822692871
Loss :  1.6590533256530762 3.0178627967834473 4.676916122436523
Loss :  1.6470197439193726 3.1150660514831543 4.762085914611816
Loss :  1.658787488937378 2.886552095413208 4.545339584350586
Loss :  1.6312288045883179 3.2839598655700684 4.915188789367676
Loss :  1.6831480264663696 2.7716856002807617 4.454833507537842
Loss :  1.647995948791504 2.666557788848877 4.314553737640381
Loss :  1.6660716533660889 3.2021613121032715 4.868232727050781
Loss :  1.683502197265625 2.8942172527313232 4.577719688415527
Loss :  1.6689484119415283 2.8939998149871826 4.562948226928711
Loss :  1.6719533205032349 3.207946300506592 4.879899501800537
Loss :  1.6372368335723877 2.7703850269317627 4.40762186050415
Loss :  1.6860817670822144 2.8942954540252686 4.580377101898193
Loss :  1.6826386451721191 3.0850350856781006 4.767673492431641
Loss :  1.6969032287597656 3.005206823348999 4.702110290527344
Loss :  1.6732712984085083 2.625077486038208 4.298348903656006
  batch 60 loss: 1.6732712984085083, 2.625077486038208, 4.298348903656006
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6729834079742432 2.9745213985443115 4.647504806518555
Loss :  1.6710331439971924 2.7538974285125732 4.424930572509766
Loss :  1.6787632703781128 2.6372780799865723 4.316041469573975
Loss :  1.6597713232040405 2.7611937522888184 4.420965194702148
Loss :  1.6564545631408691 2.340341329574585 3.996795892715454
Loss :  5.400702476501465 4.411504745483398 9.812207221984863
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.29599142074585 4.330511093139648 9.626502990722656
Loss :  5.390398025512695 4.291481018066406 9.681879043579102
Loss :  4.524828910827637 4.235250949859619 8.760080337524414
Total LOSS train 4.626832628250122 valid 9.470167398452759
CE LOSS train 1.6654426666406486 valid 1.1312072277069092
Contrastive LOSS train 2.9613899377676156 valid 1.0588127374649048
EPOCH 141:
Loss :  1.6527451276779175 2.4235644340515137 4.076309680938721
Loss :  1.6668100357055664 2.8999898433685303 4.566800117492676
Loss :  1.652945637702942 2.269639253616333 3.9225850105285645
Loss :  1.6569987535476685 2.8578226566314697 4.514821529388428
Loss :  1.681483507156372 2.9606192111968994 4.6421027183532715
Loss :  1.664990782737732 2.8437581062316895 4.508749008178711
Loss :  1.6612021923065186 2.765930414199829 4.427132606506348
Loss :  1.651732325553894 2.5697758197784424 4.221508026123047
Loss :  1.6559350490570068 2.442751407623291 4.098686218261719
Loss :  1.6100436449050903 2.758568286895752 4.368611812591553
Loss :  1.6699689626693726 2.886470079421997 4.55643892288208
Loss :  1.7296783924102783 2.791443347930908 4.521121978759766
Loss :  1.6739702224731445 2.990825653076172 4.664795875549316
Loss :  1.6652246713638306 2.8213298320770264 4.4865546226501465
Loss :  1.6446434259414673 2.84938645362854 4.494029998779297
Loss :  1.6522623300552368 2.9285824298858643 4.580844879150391
Loss :  1.661855936050415 2.8746449947357178 4.536500930786133
Loss :  1.6612651348114014 3.011237144470215 4.672502517700195
Loss :  1.6679236888885498 2.7830018997192383 4.450925827026367
Loss :  1.6252391338348389 2.922304391860962 4.547543525695801
  batch 20 loss: 1.6252391338348389, 2.922304391860962, 4.547543525695801
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6607093811035156 2.9479782581329346 4.608687400817871
Loss :  1.6768966913223267 2.6531295776367188 4.330026149749756
Loss :  1.6478244066238403 2.9456627368927 4.59348726272583
Loss :  1.6888078451156616 2.736314058303833 4.425121784210205
Loss :  1.685901165008545 2.9983601570129395 4.684261322021484
Loss :  1.6508771181106567 2.7724063396453857 4.423283576965332
Loss :  1.6984193325042725 3.2624239921569824 4.960843086242676
Loss :  1.641385555267334 2.494215488433838 4.135601043701172
Loss :  1.6887011528015137 2.815202474594116 4.503903388977051
Loss :  1.642883539199829 3.260925769805908 4.903809547424316
Loss :  1.7237738370895386 3.186440944671631 4.910214900970459
Loss :  1.6700185537338257 2.85245943069458 4.522478103637695
Loss :  1.6555333137512207 2.9144489765167236 4.569982528686523
Loss :  1.6583837270736694 2.811462640762329 4.469846248626709
Loss :  1.6978747844696045 2.9754090309143066 4.673283576965332
Loss :  1.6893588304519653 3.022190570831299 4.711549282073975
Loss :  1.6679182052612305 2.8628649711608887 4.530783176422119
Loss :  1.6349642276763916 2.6634440422058105 4.298408508300781
Loss :  1.6611394882202148 2.5807783603668213 4.241917610168457
Loss :  1.6539465188980103 2.765449285507202 4.419395923614502
  batch 40 loss: 1.6539465188980103, 2.765449285507202, 4.419395923614502
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6610100269317627 2.6751928329467773 4.336202621459961
Loss :  1.649524211883545 2.724074125289917 4.373598098754883
Loss :  1.6671496629714966 2.591747760772705 4.258897304534912
Loss :  1.6648104190826416 2.526669502258301 4.191479682922363
Loss :  1.6676698923110962 2.7217698097229004 4.389439582824707
Loss :  1.6592382192611694 3.399975538253784 5.059213638305664
Loss :  1.6465997695922852 3.2107596397399902 4.857359409332275
Loss :  1.658325433731079 2.786668300628662 4.44499397277832
Loss :  1.6313356161117554 3.067692279815674 4.699028015136719
Loss :  1.6826716661453247 2.7911217212677 4.4737935066223145
Loss :  1.6475472450256348 2.7914817333221436 4.439028739929199
Loss :  1.6657458543777466 2.707873582839966 4.373619556427002
Loss :  1.6834592819213867 2.8657429218292236 4.549201965332031
Loss :  1.6686885356903076 2.8857216835021973 4.554409980773926
Loss :  1.671533226966858 3.319709062576294 4.991242408752441
Loss :  1.6374555826187134 2.7970471382141113 4.434502601623535
Loss :  1.6861600875854492 2.7088053226470947 4.394965171813965
Loss :  1.68247389793396 3.0392119884490967 4.721685886383057
Loss :  1.6965972185134888 3.178156852722168 4.874753952026367
Loss :  1.6728010177612305 2.9041521549224854 4.576952934265137
  batch 60 loss: 1.6728010177612305, 2.9041521549224854, 4.576952934265137
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6732019186019897 3.146789789199829 4.819991588592529
Loss :  1.6701472997665405 3.035625696182251 4.705772876739502
Loss :  1.67877197265625 2.4031243324279785 4.0818963050842285
Loss :  1.659584641456604 2.683542013168335 4.3431267738342285
Loss :  1.6560430526733398 2.530900001525879 4.186943054199219
Loss :  5.4381561279296875 4.427154541015625 9.865310668945312
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.320887565612793 4.3475847244262695 9.668472290039062
Loss :  5.43104887008667 4.246807098388672 9.6778564453125
Loss :  4.550908088684082 4.132004261016846 8.682912826538086
Total LOSS train 4.506269997816819 valid 9.47363805770874
CE LOSS train 1.6652428058477549 valid 1.1377270221710205
Contrastive LOSS train 2.841027208474966 valid 1.0330010652542114
EPOCH 142:
Loss :  1.6521461009979248 2.6198461055755615 4.271992206573486
Loss :  1.666865348815918 2.973581075668335 4.640446662902832
Loss :  1.6528490781784058 2.610666513442993 4.263515472412109
Loss :  1.656463623046875 3.3166584968566895 4.9731221199035645
Loss :  1.680848479270935 3.229719638824463 4.9105682373046875
Loss :  1.664719820022583 2.6614112854003906 4.3261308670043945
Loss :  1.6613523960113525 3.0243821144104004 4.685734748840332
Loss :  1.6516380310058594 2.6005849838256836 4.252223014831543
Loss :  1.6558454036712646 2.376272678375244 4.03211784362793
Loss :  1.6102409362792969 2.6263034343719482 4.236544609069824
Loss :  1.6698389053344727 3.1276113986968994 4.797450065612793
Loss :  1.7297089099884033 2.7994384765625 4.529147148132324
Loss :  1.6738263368606567 3.1943118572235107 4.868138313293457
Loss :  1.66533362865448 3.0362050533294678 4.701538562774658
Loss :  1.6446667909622192 2.7356698513031006 4.380336761474609
Loss :  1.6523140668869019 2.917180299758911 4.569494247436523
Loss :  1.6620314121246338 2.8023414611816406 4.464372634887695
Loss :  1.6608587503433228 2.7870914936065674 4.44795036315918
Loss :  1.6676223278045654 2.732757329940796 4.400379657745361
Loss :  1.625157117843628 2.8618288040161133 4.48698616027832
  batch 20 loss: 1.625157117843628, 2.8618288040161133, 4.48698616027832
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6606781482696533 3.2648379802703857 4.925516128540039
Loss :  1.6765966415405273 3.0351104736328125 4.71170711517334
Loss :  1.647827386856079 2.807993173599243 4.455820560455322
Loss :  1.6884033679962158 3.0981314182281494 4.786534786224365
Loss :  1.6854960918426514 3.271122694015503 4.956618785858154
Loss :  1.6505080461502075 2.839061737060547 4.489569664001465
Loss :  1.6983569860458374 3.2174460887908936 4.915802955627441
Loss :  1.6415343284606934 2.6987860202789307 4.340320587158203
Loss :  1.6891965866088867 2.5035433769226074 4.192739963531494
Loss :  1.6426082849502563 3.044295072555542 4.686903476715088
Loss :  1.7238740921020508 3.183274030685425 4.907148361206055
Loss :  1.669797658920288 3.218310594558716 4.888108253479004
Loss :  1.65571928024292 2.855834722518921 4.511553764343262
Loss :  1.6581650972366333 3.0579795837402344 4.716144561767578
Loss :  1.6977952718734741 3.0354857444763184 4.733281135559082
Loss :  1.689705491065979 3.17240047454834 4.862105846405029
Loss :  1.6680469512939453 2.738177537918091 4.406224250793457
Loss :  1.6351816654205322 2.8890600204467773 4.5242414474487305
Loss :  1.6612192392349243 2.8946261405944824 4.555845260620117
Loss :  1.6537301540374756 3.224738359451294 4.8784685134887695
  batch 40 loss: 1.6537301540374756, 3.224738359451294, 4.8784685134887695
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.660947561264038 2.97810697555542 4.639054298400879
Loss :  1.649841070175171 2.8551528453826904 4.504993915557861
Loss :  1.667266845703125 2.7935140132904053 4.460781097412109
Loss :  1.6653326749801636 2.651472568511963 4.316805362701416
Loss :  1.6678441762924194 3.1967339515686035 4.8645782470703125
Loss :  1.6594820022583008 3.34942626953125 5.008908271789551
Loss :  1.6469429731369019 3.388033866882324 5.034976959228516
Loss :  1.6585581302642822 3.1621649265289307 4.820723056793213
Loss :  1.6312521696090698 3.4480650424957275 5.079317092895508
Loss :  1.682908535003662 3.2157366275787354 4.898645401000977
Loss :  1.6475069522857666 2.84360408782959 4.491110801696777
Loss :  1.665877342224121 3.206629514694214 4.872507095336914
Loss :  1.6835310459136963 3.508141279220581 5.191672325134277
Loss :  1.6689378023147583 3.050143003463745 4.719080924987793
Loss :  1.6711853742599487 3.4147870540618896 5.085972309112549
Loss :  1.6371477842330933 2.9190893173217773 4.55623722076416
Loss :  1.6860600709915161 3.2765400409698486 4.962600231170654
Loss :  1.6827008724212646 2.9873735904693604 4.670074462890625
Loss :  1.6964843273162842 3.1827547550201416 4.879239082336426
Loss :  1.6727350950241089 3.317194938659668 4.989930152893066
  batch 60 loss: 1.6727350950241089, 3.317194938659668, 4.989930152893066
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.673243761062622 3.188256025314331 4.861499786376953
Loss :  1.670465350151062 2.9566361904144287 4.627101421356201
Loss :  1.6789275407791138 2.7784371376037598 4.457364559173584
Loss :  1.6595518589019775 2.8707149028778076 4.530266761779785
Loss :  1.6562072038650513 2.8100240230560303 4.466231346130371
Loss :  5.478827476501465 4.404817581176758 9.883645057678223
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.360891819000244 4.356893539428711 9.717784881591797
Loss :  5.483068466186523 4.24992561340332 9.732994079589844
Loss :  4.579558372497559 4.225970268249512 8.80552864074707
Total LOSS train 4.656500266148494 valid 9.534988164901733
CE LOSS train 1.6652262577643762 valid 1.1448895931243896
Contrastive LOSS train 2.9912740083841176 valid 1.056492567062378
EPOCH 143:
Loss :  1.6522963047027588 2.9578945636749268 4.6101908683776855
Loss :  1.6668428182601929 3.2515618801116943 4.918404579162598
Loss :  1.6532111167907715 2.550316095352173 4.203527450561523
Loss :  1.6568751335144043 2.8491923809051514 4.506067276000977
Loss :  1.6810578107833862 3.028172254562378 4.709229946136475
Loss :  1.6649315357208252 2.979459285736084 4.644391059875488
Loss :  1.6614738702774048 3.1450016498565674 4.806475639343262
Loss :  1.651550531387329 2.84814453125 4.49969482421875
Loss :  1.6559104919433594 2.8709795475006104 4.526889801025391
Loss :  1.6102216243743896 3.0205090045928955 4.630730628967285
Loss :  1.6697636842727661 3.355726957321167 5.025490760803223
Loss :  1.7299031019210815 3.8364999294281006 5.566402912139893
Loss :  1.6741360425949097 3.308861255645752 4.982997417449951
Loss :  1.6649999618530273 3.314785957336426 4.979785919189453
Loss :  1.6439108848571777 3.253005266189575 4.896916389465332
Loss :  1.6525756120681763 3.3340160846710205 4.986591815948486
Loss :  1.6620135307312012 3.0749621391296387 4.73697566986084
Loss :  1.6611294746398926 3.0780839920043945 4.739213466644287
Loss :  1.6681157350540161 3.0425946712493896 4.710710525512695
Loss :  1.6250391006469727 3.196518898010254 4.821557998657227
  batch 20 loss: 1.6250391006469727, 3.196518898010254, 4.821557998657227
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602050065994263 3.673804521560669 5.334009647369385
Loss :  1.6767590045928955 3.081942558288574 4.758701324462891
Loss :  1.6471514701843262 3.297210216522217 4.944361686706543
Loss :  1.688875675201416 3.0282695293426514 4.717144966125488
Loss :  1.6857949495315552 3.3813390731811523 5.067133903503418
Loss :  1.6502596139907837 3.1756575107574463 4.8259172439575195
Loss :  1.6982263326644897 3.4022016525268555 5.100428104400635
Loss :  1.6411035060882568 3.1174192428588867 4.758522987365723
Loss :  1.6886873245239258 2.7314960956573486 4.420183181762695
Loss :  1.6424087285995483 3.4490408897399902 5.091449737548828
Loss :  1.7238110303878784 3.4581594467163086 5.181970596313477
Loss :  1.6699837446212769 3.51442813873291 5.184412002563477
Loss :  1.655593991279602 2.8695900440216064 4.525184154510498
Loss :  1.6577026844024658 3.198819160461426 4.8565216064453125
Loss :  1.6978716850280762 3.287328004837036 4.985199928283691
Loss :  1.689342737197876 3.561666965484619 5.251009941101074
Loss :  1.6674702167510986 3.0376133918762207 4.705083847045898
Loss :  1.635130524635315 3.244197368621826 4.879327774047852
Loss :  1.6613068580627441 3.2386701107025146 4.89997673034668
Loss :  1.6534485816955566 3.351963758468628 5.0054121017456055
  batch 40 loss: 1.6534485816955566, 3.351963758468628, 5.0054121017456055
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6608431339263916 3.3232836723327637 4.984127044677734
Loss :  1.6494592428207397 3.0724639892578125 4.721923351287842
Loss :  1.666709303855896 3.1694133281707764 4.836122512817383
Loss :  1.6653729677200317 2.514873743057251 4.180246829986572
Loss :  1.6679528951644897 2.669569969177246 4.337522983551025
Loss :  1.6589709520339966 3.3235831260681152 4.982553958892822
Loss :  1.6469790935516357 3.280036211013794 4.92701530456543
Loss :  1.6588003635406494 2.986431837081909 4.645232200622559
Loss :  1.630497932434082 3.1702663898468018 4.800764083862305
Loss :  1.6830039024353027 3.107041120529175 4.790044784545898
Loss :  1.6471418142318726 3.1916823387145996 4.838824272155762
Loss :  1.6657899618148804 3.179682493209839 4.84547233581543
Loss :  1.6833546161651611 3.134474039077759 4.81782865524292
Loss :  1.668836236000061 3.3541486263275146 5.022984981536865
Loss :  1.6712429523468018 3.4928090572357178 5.1640520095825195
Loss :  1.6371959447860718 2.68058443069458 4.317780494689941
Loss :  1.68584144115448 3.300830364227295 4.9866719245910645
Loss :  1.6824164390563965 3.0155932903289795 4.698009490966797
Loss :  1.6962838172912598 3.242107391357422 4.938391208648682
Loss :  1.6727226972579956 2.6238906383514404 4.2966132164001465
  batch 60 loss: 1.6727226972579956, 2.6238906383514404, 4.2966132164001465
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6730523109436035 3.4946229457855225 5.167675018310547
Loss :  1.67023503780365 2.7414259910583496 4.411661148071289
Loss :  1.6790392398834229 3.2573325634002686 4.936371803283691
Loss :  1.6590551137924194 2.9351508617401123 4.594205856323242
Loss :  1.6558141708374023 2.6194515228271484 4.275265693664551
Loss :  5.463374137878418 4.418667793273926 9.882041931152344
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.362729549407959 4.4040961265563965 9.766825675964355
Loss :  5.458938121795654 4.189450263977051 9.648387908935547
Loss :  4.544400691986084 4.147763252258301 8.692163467407227
Total LOSS train 4.807870116600624 valid 9.497354745864868
CE LOSS train 1.6651339017427884 valid 1.136100172996521
Contrastive LOSS train 3.142736214857835 valid 1.0369408130645752
EPOCH 144:
Loss :  1.6520607471466064 3.005906820297241 4.657967567443848
Loss :  1.66665518283844 3.0464022159576416 4.713057518005371
Loss :  1.6527005434036255 2.467608690261841 4.120309352874756
Loss :  1.6561890840530396 2.795714855194092 4.451903820037842
Loss :  1.6807310581207275 3.521557569503784 5.202288627624512
Loss :  1.6643548011779785 3.1280035972595215 4.7923583984375
Loss :  1.6614018678665161 3.2039825916290283 4.865384578704834
Loss :  1.651356816291809 2.8479621410369873 4.499319076538086
Loss :  1.655591368675232 2.80233097076416 4.457922458648682
Loss :  1.6098655462265015 2.882077693939209 4.491943359375
Loss :  1.6696968078613281 3.0595383644104004 4.7292351722717285
Loss :  1.7298014163970947 3.296708583831787 5.026510238647461
Loss :  1.6745661497116089 3.150474786758423 4.825040817260742
Loss :  1.6647428274154663 3.147702217102051 4.812445163726807
Loss :  1.6441556215286255 2.817988157272339 4.462143898010254
Loss :  1.6522670984268188 3.0419821739196777 4.694249153137207
Loss :  1.6617828607559204 2.789412498474121 4.451195240020752
Loss :  1.6611179113388062 2.388122081756592 4.0492401123046875
Loss :  1.6679781675338745 2.8841376304626465 4.5521159172058105
Loss :  1.6249489784240723 3.298133373260498 4.92308235168457
  batch 20 loss: 1.6249489784240723, 3.298133373260498, 4.92308235168457
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6600645780563354 3.002175807952881 4.662240505218506
Loss :  1.676524043083191 2.949592351913452 4.6261162757873535
Loss :  1.6473344564437866 3.0149953365325928 4.66232967376709
Loss :  1.6887965202331543 3.0150978565216064 4.70389461517334
Loss :  1.6860538721084595 3.546290874481201 5.232344627380371
Loss :  1.6502130031585693 2.6933434009552 4.3435564041137695
Loss :  1.6980206966400146 3.1207401752471924 4.818760871887207
Loss :  1.6410281658172607 2.846782922744751 4.487811088562012
Loss :  1.688515543937683 2.391472578048706 4.0799880027771
Loss :  1.6423649787902832 3.0680580139160156 4.710422992706299
Loss :  1.7236427068710327 2.8916544914245605 4.615297317504883
Loss :  1.6699389219284058 3.334469795227051 5.004408836364746
Loss :  1.6553670167922974 2.6003260612487793 4.255692958831787
Loss :  1.6575406789779663 2.53035569190979 4.187896251678467
Loss :  1.6976330280303955 2.912379503250122 4.610012531280518
Loss :  1.6892192363739014 3.034480571746826 4.723699569702148
Loss :  1.6674855947494507 2.6579015254974365 4.325387001037598
Loss :  1.6350959539413452 2.569544792175293 4.204640865325928
Loss :  1.6609355211257935 2.4931886196136475 4.1541242599487305
Loss :  1.6533918380737305 2.9455935955047607 4.59898567199707
  batch 40 loss: 1.6533918380737305, 2.9455935955047607, 4.59898567199707
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6605485677719116 2.8849737644195557 4.545522212982178
Loss :  1.6492810249328613 3.0369532108306885 4.686234474182129
Loss :  1.666703224182129 3.0336146354675293 4.700317859649658
Loss :  1.6652367115020752 2.523287534713745 4.18852424621582
Loss :  1.667616367340088 2.708376169204712 4.375992774963379
Loss :  1.659034252166748 2.836679220199585 4.495713233947754
Loss :  1.6466255187988281 3.7790420055389404 5.425667762756348
Loss :  1.658615231513977 2.8685200214385986 4.527135372161865
Loss :  1.6306891441345215 3.0268731117248535 4.657562255859375
Loss :  1.6826549768447876 3.1043574810028076 4.787012577056885
Loss :  1.6473503112792969 2.482801675796509 4.130151748657227
Loss :  1.6654037237167358 3.0326666831970215 4.698070526123047
Loss :  1.6834908723831177 2.735267162322998 4.418757915496826
Loss :  1.6685526371002197 2.8799309730529785 4.548483848571777
Loss :  1.6712368726730347 3.4554238319396973 5.1266608238220215
Loss :  1.6364136934280396 2.8181283473968506 4.45454216003418
Loss :  1.685546636581421 3.233264684677124 4.918811321258545
Loss :  1.6826499700546265 2.6811821460723877 4.363831996917725
Loss :  1.6964551210403442 3.005584955215454 4.702040195465088
Loss :  1.6723688840866089 2.655804395675659 4.3281731605529785
  batch 60 loss: 1.6723688840866089, 2.655804395675659, 4.3281731605529785
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6729754209518433 3.0484654903411865 4.72144079208374
Loss :  1.670481562614441 2.880615472793579 4.5510969161987305
Loss :  1.6788244247436523 2.8041751384735107 4.482999801635742
Loss :  1.659199833869934 2.9257264137268066 4.584926128387451
Loss :  1.6557390689849854 2.423879861831665 4.07961893081665
Loss :  5.564825534820557 4.48090124130249 10.045726776123047
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.467392921447754 4.379395008087158 9.84678840637207
Loss :  5.552556037902832 4.227950096130371 9.780506134033203
Loss :  4.65616512298584 4.096615791320801 8.75278091430664
Total LOSS train 4.588994033520038 valid 9.60645055770874
CE LOSS train 1.6649973117388213 valid 1.16404128074646
Contrastive LOSS train 2.923996697939359 valid 1.0241539478302002
EPOCH 145:
Loss :  1.6521618366241455 2.477795362472534 4.12995719909668
Loss :  1.6665927171707153 3.1044747829437256 4.7710676193237305
Loss :  1.6529070138931274 2.3498663902282715 4.002773284912109
Loss :  1.6567862033843994 2.6959421634674072 4.352728366851807
Loss :  1.6806501150131226 2.7570815086364746 4.437731742858887
Loss :  1.6645654439926147 2.5917482376098633 4.256313800811768
Loss :  1.660867691040039 2.676257848739624 4.337125778198242
Loss :  1.6512935161590576 2.4745001792907715 4.12579345703125
Loss :  1.6556107997894287 2.6358346939086914 4.291445732116699
Loss :  1.6096770763397217 2.6644127368927 4.274089813232422
Loss :  1.669594645500183 2.9877469539642334 4.657341480255127
Loss :  1.7298132181167603 3.0343785285949707 4.764191627502441
Loss :  1.675060749053955 2.8149466514587402 4.490007400512695
Loss :  1.6647164821624756 3.277526617050171 4.9422430992126465
Loss :  1.6442495584487915 2.755357503890991 4.399607181549072
Loss :  1.6518895626068115 3.384505033493042 5.0363945960998535
Loss :  1.661913275718689 2.7301080226898193 4.392021179199219
Loss :  1.6608060598373413 2.850562334060669 4.511368274688721
Loss :  1.6682708263397217 2.6952052116394043 4.363475799560547
Loss :  1.6248359680175781 3.09266996383667 4.717505931854248
  batch 20 loss: 1.6248359680175781, 3.09266996383667, 4.717505931854248
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6601128578186035 2.9934725761413574 4.653585433959961
Loss :  1.6762299537658691 2.8055028915405273 4.4817328453063965
Loss :  1.6472970247268677 3.411146640777588 5.058443546295166
Loss :  1.6884104013442993 2.8950400352478027 4.5834503173828125
Loss :  1.6858947277069092 3.1437346935272217 4.829629421234131
Loss :  1.649968147277832 2.7083427906036377 4.358310699462891
Loss :  1.6979457139968872 3.2105698585510254 4.908515453338623
Loss :  1.640919804573059 2.847358465194702 4.488278388977051
Loss :  1.6887317895889282 2.580045461654663 4.268777370452881
Loss :  1.6425082683563232 2.9346249103546143 4.5771331787109375
Loss :  1.7238273620605469 2.9087700843811035 4.63259744644165
Loss :  1.6695356369018555 3.179797649383545 4.8493332862854
Loss :  1.655472755432129 2.6354775428771973 4.290950298309326
Loss :  1.65775728225708 2.7790932655334473 4.436850547790527
Loss :  1.6975196599960327 2.961092710494995 4.658612251281738
Loss :  1.6891051530838013 2.7908968925476074 4.480001926422119
Loss :  1.6674445867538452 2.9366061687469482 4.604050636291504
Loss :  1.6349306106567383 2.612144708633423 4.247075080871582
Loss :  1.660863995552063 2.557670831680298 4.21853494644165
Loss :  1.6532539129257202 3.0716025829315186 4.724856376647949
  batch 40 loss: 1.6532539129257202, 3.0716025829315186, 4.724856376647949
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6607571840286255 2.8817808628082275 4.542538166046143
Loss :  1.6495156288146973 2.727775812149048 4.377291679382324
Loss :  1.6667479276657104 2.7980058193206787 4.4647536277771
Loss :  1.6649008989334106 2.989745855331421 4.654646873474121
Loss :  1.6676028966903687 2.6904001235961914 4.35800313949585
Loss :  1.6591110229492188 3.201259136199951 4.86037015914917
Loss :  1.646528720855713 3.032588005065918 4.679116725921631
Loss :  1.6583489179611206 3.0879318714141846 4.746280670166016
Loss :  1.630831003189087 2.911228656768799 4.542059898376465
Loss :  1.6823309659957886 3.169769287109375 4.852100372314453
Loss :  1.6471353769302368 2.9145667552948 4.561702251434326
Loss :  1.6652939319610596 3.2268829345703125 4.892176628112793
Loss :  1.6832168102264404 3.252403974533081 4.9356207847595215
Loss :  1.6685616970062256 2.9878287315368652 4.656390190124512
Loss :  1.6712027788162231 3.323564052581787 4.994766712188721
Loss :  1.6359453201293945 3.1735944747924805 4.809539794921875
Loss :  1.6855133771896362 3.4535789489746094 5.139092445373535
Loss :  1.6827201843261719 3.2124850749969482 4.895205497741699
Loss :  1.6961356401443481 3.312183380126953 5.008318901062012
Loss :  1.6719495058059692 3.5877251625061035 5.259674549102783
  batch 60 loss: 1.6719495058059692, 3.5877251625061035, 5.259674549102783
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.673054814338684 3.6593666076660156 5.33242130279541
Loss :  1.6699360609054565 2.6662800312042236 4.336215972900391
Loss :  1.678909420967102 3.173771381378174 4.852680683135986
Loss :  1.6590930223464966 3.4624874591827393 5.121580600738525
Loss :  1.6557639837265015 2.994213342666626 4.649977207183838
Loss :  5.560745716094971 4.299698829650879 9.860445022583008
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.442038059234619 4.299134731292725 9.741172790527344
Loss :  5.558159351348877 4.208578109741211 9.76673698425293
Loss :  4.640314102172852 4.359663963317871 8.999978065490723
Total LOSS train 4.617329656160795 valid 9.592083215713501
CE LOSS train 1.664939992244427 valid 1.160078525543213
Contrastive LOSS train 2.9523896804222693 valid 1.0899159908294678
EPOCH 146:
Loss :  1.6518934965133667 2.607592821121216 4.259486198425293
Loss :  1.6667823791503906 2.931182384490967 4.597964763641357
Loss :  1.6529736518859863 2.668976068496704 4.3219499588012695
Loss :  1.656299114227295 2.9339842796325684 4.590283393859863
Loss :  1.6807126998901367 3.068204641342163 4.748917579650879
Loss :  1.664291501045227 2.9205355644226074 4.584826946258545
Loss :  1.6615506410598755 3.17901611328125 4.840566635131836
Loss :  1.6511465311050415 2.884068727493286 4.535215377807617
Loss :  1.6555198431015015 2.593996524810791 4.249516487121582
Loss :  1.6096464395523071 2.822448968887329 4.432095527648926
Loss :  1.6695524454116821 3.005397081375122 4.674949645996094
Loss :  1.7300589084625244 2.7881581783294678 4.518217086791992
Loss :  1.6744030714035034 3.2631406784057617 4.937543869018555
Loss :  1.6650753021240234 3.2931220531463623 4.958197593688965
Loss :  1.6438485383987427 3.012298107147217 4.65614652633667
Loss :  1.6522748470306396 2.8253285884857178 4.477603435516357
Loss :  1.661881685256958 2.8300485610961914 4.49193000793457
Loss :  1.6605193614959717 2.9134230613708496 4.573942184448242
Loss :  1.6679860353469849 2.6916756629943848 4.35966157913208
Loss :  1.6248981952667236 2.956193208694458 4.581091403961182
  batch 20 loss: 1.6248981952667236, 2.956193208694458, 4.581091403961182
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.659867286682129 3.224106550216675 4.883974075317383
Loss :  1.6760917901992798 2.998547315597534 4.6746392250061035
Loss :  1.6468623876571655 3.1432275772094727 4.790090084075928
Loss :  1.6882728338241577 3.433608055114746 5.121881008148193
Loss :  1.6855604648590088 3.604268789291382 5.289829254150391
Loss :  1.649699330329895 3.0606720447540283 4.710371494293213
Loss :  1.6980148553848267 3.406348466873169 5.104363441467285
Loss :  1.6407322883605957 2.8678841590881348 4.5086164474487305
Loss :  1.6888375282287598 2.860870361328125 4.549707889556885
Loss :  1.6421869993209839 3.6190943717956543 5.261281490325928
Loss :  1.7236510515213013 3.268162727355957 4.991813659667969
Loss :  1.6697603464126587 3.1251330375671387 4.794893264770508
Loss :  1.6553707122802734 2.7656335830688477 4.421004295349121
Loss :  1.6578370332717896 2.72275447845459 4.38059139251709
Loss :  1.6976491212844849 3.2482316493988037 4.945880889892578
Loss :  1.6889293193817139 3.2450687885284424 4.933998107910156
Loss :  1.6673784255981445 3.037442922592163 4.704821586608887
Loss :  1.6349132061004639 2.9890823364257812 4.623995780944824
Loss :  1.6608604192733765 2.6186673641204834 4.27952766418457
Loss :  1.653393268585205 3.0322258472442627 4.685619354248047
  batch 40 loss: 1.653393268585205, 3.0322258472442627, 4.685619354248047
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6609444618225098 3.3405539989471436 5.001498222351074
Loss :  1.6494675874710083 3.173251152038574 4.822718620300293
Loss :  1.666723370552063 2.986283779144287 4.6530070304870605
Loss :  1.6652250289916992 2.9594826698303223 4.6247076988220215
Loss :  1.6675655841827393 3.197885751724243 4.865451335906982
Loss :  1.6590975522994995 3.4390954971313477 5.098193168640137
Loss :  1.6463103294372559 3.4970266819000244 5.143337249755859
Loss :  1.6583433151245117 3.5019690990448 5.160312652587891
Loss :  1.630454659461975 3.179718255996704 4.810173034667969
Loss :  1.6825385093688965 3.0865111351013184 4.769049644470215
Loss :  1.646822452545166 3.261186361312866 4.908008575439453
Loss :  1.6652284860610962 3.2290170192718506 4.894245624542236
Loss :  1.6833587884902954 3.6386399269104004 5.321998596191406
Loss :  1.6686184406280518 3.1392457485198975 4.807864189147949
Loss :  1.6712331771850586 3.575852870941162 5.247086048126221
Loss :  1.636088490486145 3.3514344692230225 4.987523078918457
Loss :  1.6855964660644531 3.175380229949951 4.860976696014404
Loss :  1.68253493309021 3.2067513465881348 4.889286041259766
Loss :  1.6959798336029053 3.5943892002105713 5.290369033813477
Loss :  1.6719748973846436 3.064263343811035 4.736238479614258
  batch 60 loss: 1.6719748973846436, 3.064263343811035, 4.736238479614258
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6729736328125 3.7197885513305664 5.392762184143066
Loss :  1.670225739479065 3.1374144554138184 4.807640075683594
Loss :  1.678640604019165 3.0410757064819336 4.7197160720825195
Loss :  1.6586004495620728 3.135721445083618 4.7943220138549805
Loss :  1.6554713249206543 3.0363247394561768 4.69179630279541
Loss :  5.675088882446289 4.481424808502197 10.156513214111328
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.566686153411865 4.395200252532959 9.961886405944824
Loss :  5.660812854766846 4.172534942626953 9.83334732055664
Loss :  4.751122951507568 4.156205654144287 8.907328605651855
Total LOSS train 4.77454289656419 valid 9.714768886566162
CE LOSS train 1.6648800226358267 valid 1.187780737876892
Contrastive LOSS train 3.1096628482525164 valid 1.0390514135360718
EPOCH 147:
Loss :  1.6521756649017334 2.571016550064087 4.22319221496582
Loss :  1.6666028499603271 3.375502347946167 5.042105197906494
Loss :  1.6527621746063232 3.161654472351074 4.814416885375977
Loss :  1.656663179397583 3.092750072479248 4.74941349029541
Loss :  1.6810343265533447 3.5238535404205322 5.204887866973877
Loss :  1.664852499961853 2.677145481109619 4.341998100280762
Loss :  1.6609141826629639 3.103672981262207 4.76458740234375
Loss :  1.650998592376709 2.5413448810577393 4.192343711853027
Loss :  1.6555068492889404 2.553926944732666 4.209433555603027
Loss :  1.6087816953659058 2.6325926780700684 4.241374492645264
Loss :  1.669669270515442 3.0732760429382324 4.742945194244385
Loss :  1.7305220365524292 3.1576449871063232 4.888166904449463
Loss :  1.6745883226394653 3.644941568374634 5.319530010223389
Loss :  1.664689302444458 3.5480661392211914 5.21275520324707
Loss :  1.643794059753418 3.016561985015869 4.660356044769287
Loss :  1.6523371934890747 3.2857422828674316 4.938079357147217
Loss :  1.661810040473938 3.211167812347412 4.8729777336120605
Loss :  1.6604864597320557 3.2295594215393066 4.890046119689941
Loss :  1.6678142547607422 2.528203010559082 4.196017265319824
Loss :  1.6244823932647705 3.1349329948425293 4.759415626525879
  batch 20 loss: 1.6244823932647705, 3.1349329948425293, 4.759415626525879
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.659921646118164 3.050488233566284 4.710410118103027
Loss :  1.6760838031768799 2.7319347858428955 4.408018589019775
Loss :  1.6467487812042236 3.416537284851074 5.063285827636719
Loss :  1.6889121532440186 3.3649115562438965 5.053823471069336
Loss :  1.6856563091278076 3.4935028553009033 5.179159164428711
Loss :  1.6496778726577759 2.8998310565948486 4.549509048461914
Loss :  1.6980313062667847 3.32598876953125 5.024020195007324
Loss :  1.640444278717041 3.0404982566833496 4.680942535400391
Loss :  1.6882140636444092 3.3412370681762695 5.029451370239258
Loss :  1.6423922777175903 3.1300337314605713 4.772426128387451
Loss :  1.7235820293426514 3.1821348667144775 4.905716896057129
Loss :  1.6696522235870361 3.581493377685547 5.251145362854004
Loss :  1.6549901962280273 2.6626079082489014 4.317598342895508
Loss :  1.6574345827102661 3.2054624557495117 4.862896919250488
Loss :  1.6976611614227295 3.61281156539917 5.31047248840332
Loss :  1.6888642311096191 2.954659938812256 4.643524169921875
Loss :  1.667326807975769 2.7614121437072754 4.428739070892334
Loss :  1.6344889402389526 3.000183582305908 4.63467264175415
Loss :  1.6605767011642456 2.8532562255859375 4.513833045959473
Loss :  1.653312087059021 3.1870572566986084 4.84036922454834
  batch 40 loss: 1.653312087059021, 3.1870572566986084, 4.84036922454834
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6606686115264893 3.2714991569519043 4.932168006896973
Loss :  1.648941993713379 2.9726388454437256 4.621581077575684
Loss :  1.6666700839996338 3.2179324626922607 4.8846025466918945
Loss :  1.6651972532272339 2.761835813522339 4.427032947540283
Loss :  1.6677353382110596 2.7327356338500977 4.400470733642578
Loss :  1.6588770151138306 3.2055344581604004 4.864411354064941
Loss :  1.6463712453842163 3.181243896484375 4.827615261077881
Loss :  1.6581553220748901 3.2355122566223145 4.893667697906494
Loss :  1.6299537420272827 3.5457639694213867 5.175717830657959
Loss :  1.6824544668197632 3.312222480773926 4.9946770668029785
Loss :  1.6467257738113403 3.1237075328826904 4.77043342590332
Loss :  1.6650155782699585 3.160346031188965 4.825361728668213
Loss :  1.683030128479004 3.30395770072937 4.986988067626953
Loss :  1.6683123111724854 3.035191774368286 4.7035040855407715
Loss :  1.6712874174118042 3.0567357540130615 4.728023052215576
Loss :  1.6360559463500977 3.0661699771881104 4.702225685119629
Loss :  1.685472846031189 3.5471463203430176 5.232619285583496
Loss :  1.6823451519012451 3.300257921218872 4.982603073120117
Loss :  1.6959470510482788 3.2421810626983643 4.9381279945373535
Loss :  1.6719392538070679 2.9242026805877686 4.596141815185547
  batch 60 loss: 1.6719392538070679, 2.9242026805877686, 4.596141815185547
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6728073358535767 3.055180072784424 4.727987289428711
Loss :  1.6699395179748535 3.2621042728424072 4.93204402923584
Loss :  1.678252100944519 2.923220157623291 4.6014723777771
Loss :  1.6586096286773682 2.894551992416382 4.55316162109375
Loss :  1.6554754972457886 2.7266507148742676 4.382126331329346
Loss :  5.606955051422119 4.400744915008545 10.007699966430664
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.488638401031494 4.357011318206787 9.845649719238281
Loss :  5.6034674644470215 4.257460117340088 9.86092758178711
Loss :  4.7026495933532715 4.2060747146606445 8.908723831176758
Total LOSS train 4.7712126365074745 valid 9.655750274658203
CE LOSS train 1.664779960192167 valid 1.1756623983383179
Contrastive LOSS train 3.1064326469714825 valid 1.0515186786651611
EPOCH 148:
Loss :  1.6516913175582886 2.87101674079895 4.522707939147949
Loss :  1.666283130645752 3.1607472896575928 4.827030181884766
Loss :  1.6526187658309937 2.604907989501953 4.257526874542236
Loss :  1.6565344333648682 2.996899366378784 4.653433799743652
Loss :  1.6802926063537598 3.3382673263549805 5.01855993270874
Loss :  1.6645961999893188 2.879809617996216 4.544405937194824
Loss :  1.6606690883636475 3.0176291465759277 4.678297996520996
Loss :  1.6510331630706787 2.7509427070617676 4.401975631713867
Loss :  1.655410647392273 2.5710856914520264 4.22649621963501
Loss :  1.60890531539917 2.4602341651916504 4.06913948059082
Loss :  1.6695003509521484 3.1261110305786133 4.795611381530762
Loss :  1.730023741722107 2.9609479904174805 4.690971851348877
Loss :  1.6745667457580566 3.189920663833618 4.864487648010254
Loss :  1.6646528244018555 3.0674591064453125 4.732111930847168
Loss :  1.6439024209976196 2.837613105773926 4.481515407562256
Loss :  1.6523045301437378 2.7737221717834473 4.426026821136475
Loss :  1.6618969440460205 2.7116284370422363 4.373525619506836
Loss :  1.6603586673736572 2.9388608932495117 4.59921932220459
Loss :  1.6680392026901245 2.781090021133423 4.449129104614258
Loss :  1.624760627746582 2.4770843982696533 4.101844787597656
  batch 20 loss: 1.624760627746582, 2.4770843982696533, 4.101844787597656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6599695682525635 3.0018088817596436 4.661778450012207
Loss :  1.6759109497070312 2.6331772804260254 4.309088230133057
Loss :  1.6470255851745605 3.186675786972046 4.833701133728027
Loss :  1.688609004020691 2.7434356212615967 4.432044506072998
Loss :  1.6857340335845947 3.5236141681671143 5.209348201751709
Loss :  1.6497282981872559 2.4905478954315186 4.140275955200195
Loss :  1.69795823097229 3.2608985900878906 4.958856582641602
Loss :  1.6405705213546753 2.6038730144500732 4.244443416595459
Loss :  1.6885260343551636 2.713348627090454 4.401874542236328
Loss :  1.6425161361694336 2.8057992458343506 4.448315620422363
Loss :  1.723591685295105 2.8094394207000732 4.533030986785889
Loss :  1.66975998878479 3.107839345932007 4.777599334716797
Loss :  1.6551694869995117 2.664853096008301 4.3200225830078125
Loss :  1.6577383279800415 2.985496759414673 4.643235206604004
Loss :  1.6973026990890503 3.1107542514801025 4.808056831359863
Loss :  1.6891236305236816 2.955429792404175 4.644553184509277
Loss :  1.6672526597976685 2.890660524368286 4.557913303375244
Loss :  1.6345258951187134 2.853044271469116 4.487570285797119
Loss :  1.6603819131851196 2.956212043762207 4.616593837738037
Loss :  1.6529062986373901 2.933506965637207 4.586413383483887
  batch 40 loss: 1.6529062986373901, 2.933506965637207, 4.586413383483887
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.660703420639038 3.0900909900665283 4.750794410705566
Loss :  1.6491395235061646 2.958456516265869 4.607595920562744
Loss :  1.6665886640548706 2.8944952487945557 4.561083793640137
Loss :  1.6647429466247559 3.0361835956573486 4.700926780700684
Loss :  1.6676435470581055 2.6261305809020996 4.293774127960205
Loss :  1.6588817834854126 3.1784913539886475 4.83737325668335
Loss :  1.646394968032837 2.8466274738311768 4.493022441864014
Loss :  1.6580082178115845 2.904325246810913 4.562333583831787
Loss :  1.6302632093429565 3.0035226345062256 4.633785724639893
Loss :  1.6820849180221558 2.7496354579925537 4.43172025680542
Loss :  1.6468976736068726 2.6910440921783447 4.337941646575928
Loss :  1.665384292602539 2.7802774906158447 4.445661544799805
Loss :  1.6832728385925293 2.8811137676239014 4.564386367797852
Loss :  1.6686502695083618 2.9599993228912354 4.628649711608887
Loss :  1.6714407205581665 2.9778876304626465 4.649328231811523
Loss :  1.636372685432434 2.666473388671875 4.3028459548950195
Loss :  1.6856917142868042 3.1252331733703613 4.810925006866455
Loss :  1.6823548078536987 2.8092620372772217 4.491616725921631
Loss :  1.6962608098983765 2.8620858192443848 4.558346748352051
Loss :  1.6721614599227905 2.638301372528076 4.310462951660156
  batch 60 loss: 1.6721614599227905, 2.638301372528076, 4.310462951660156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6729474067687988 3.022494316101074 4.695441722869873
Loss :  1.6700114011764526 2.481468677520752 4.151480197906494
Loss :  1.6782917976379395 2.9044878482818604 4.582779884338379
Loss :  1.6589910984039307 2.8126626014709473 4.471653938293457
Loss :  1.6555410623550415 2.4805006980895996 4.136041641235352
Loss :  5.632769584655762 4.408942699432373 10.041711807250977
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.527139186859131 4.4479475021362305 9.975086212158203
Loss :  5.628596305847168 4.304966449737549 9.933563232421875
Loss :  4.730279445648193 4.186004161834717 8.91628360748291
Total LOSS train 4.5436724002544695 valid 9.716661214828491
CE LOSS train 1.6647855832026555 valid 1.1825698614120483
Contrastive LOSS train 2.878886842727661 valid 1.0465010404586792
EPOCH 149:
Loss :  1.6517040729522705 2.50791072845459 4.159614562988281
Loss :  1.666591763496399 3.110642433166504 4.777234077453613
Loss :  1.6527934074401855 2.7596404552459717 4.412433624267578
Loss :  1.65671706199646 2.8038933277130127 4.460610389709473
Loss :  1.6806068420410156 3.0788772106170654 4.75948429107666
Loss :  1.6644583940505981 2.6517651081085205 4.316223621368408
Loss :  1.6609023809432983 2.8032872676849365 4.464189529418945
Loss :  1.65109121799469 2.2328948974609375 3.883985996246338
Loss :  1.6551322937011719 2.533036231994629 4.188168525695801
Loss :  1.6093093156814575 2.3500328063964844 3.9593420028686523
Loss :  1.6693402528762817 3.2026028633117676 4.87194299697876
Loss :  1.7293801307678223 2.8470966815948486 4.57647705078125
Loss :  1.6747593879699707 3.241642713546753 4.9164018630981445
Loss :  1.6645773649215698 3.278458595275879 4.943036079406738
Loss :  1.6438617706298828 2.709306478500366 4.353168487548828
Loss :  1.6520004272460938 3.089085102081299 4.741085529327393
Loss :  1.661679983139038 2.93375563621521 4.595435619354248
Loss :  1.6603401899337769 2.5323281288146973 4.192668437957764
Loss :  1.6680023670196533 2.4787728786468506 4.146775245666504
Loss :  1.6247954368591309 2.6927614212036133 4.317556858062744
  batch 20 loss: 1.6247954368591309, 2.6927614212036133, 4.317556858062744
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6600641012191772 3.1170217990875244 4.777085781097412
Loss :  1.6757748126983643 2.81660532951355 4.492380142211914
Loss :  1.6472549438476562 2.76838755607605 4.415642738342285
Loss :  1.6880842447280884 3.3174021244049072 5.005486488342285
Loss :  1.6853033304214478 3.4126811027526855 5.097984313964844
Loss :  1.6496751308441162 2.8804428577423096 4.530117988586426
Loss :  1.6978881359100342 3.290703773498535 4.988592147827148
Loss :  1.6406986713409424 2.7010574340820312 4.3417558670043945
Loss :  1.688568353652954 3.216400146484375 4.90496826171875
Loss :  1.6423410177230835 3.301297664642334 4.943638801574707
Loss :  1.7234911918640137 3.3577187061309814 5.081210136413574
Loss :  1.669323205947876 2.912975549697876 4.582298755645752
Loss :  1.655259609222412 2.557582139968872 4.212841987609863
Loss :  1.657545804977417 2.84798264503479 4.505528450012207
Loss :  1.69732666015625 3.3828771114349365 5.080204010009766
Loss :  1.6890541315078735 2.605496644973755 4.294550895690918
Loss :  1.6672332286834717 2.6033732891082764 4.270606517791748
Loss :  1.6341360807418823 2.984410524368286 4.618546485900879
Loss :  1.6600532531738281 2.801236629486084 4.461289882659912
Loss :  1.6526024341583252 2.9308066368103027 4.583409309387207
  batch 40 loss: 1.6526024341583252, 2.9308066368103027, 4.583409309387207
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6606231927871704 3.024712562561035 4.685335636138916
Loss :  1.6493122577667236 2.433953046798706 4.08326530456543
Loss :  1.6663354635238647 2.560451030731201 4.2267866134643555
Loss :  1.665233850479126 2.825035572052002 4.490269660949707
Loss :  1.667231559753418 2.608996868133545 4.276228427886963
Loss :  1.6587903499603271 3.1186845302581787 4.777474880218506
Loss :  1.645840048789978 2.832669496536255 4.478509426116943
Loss :  1.6580151319503784 3.0871872901916504 4.745202541351318
Loss :  1.6303189992904663 3.1100549697875977 4.7403740882873535
Loss :  1.6820106506347656 2.993328094482422 4.6753387451171875
Loss :  1.6466108560562134 2.903088092803955 4.549698829650879
Loss :  1.664909839630127 3.007996082305908 4.672905921936035
Loss :  1.683073878288269 2.9335310459136963 4.616604804992676
Loss :  1.668324589729309 2.9644007682800293 4.632725238800049
Loss :  1.6713067293167114 3.357637643814087 5.028944492340088
Loss :  1.6360129117965698 3.1661388874053955 4.802151679992676
Loss :  1.685713768005371 3.107715368270874 4.793429374694824
Loss :  1.682370662689209 3.0122385025024414 4.69460916519165
Loss :  1.6961214542388916 3.246784210205078 4.942905426025391
Loss :  1.6717779636383057 2.803973436355591 4.4757513999938965
  batch 60 loss: 1.6717779636383057, 2.803973436355591, 4.4757513999938965
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.672904133796692 3.4769160747528076 5.149820327758789
Loss :  1.6695488691329956 2.56683349609375 4.236382484436035
Loss :  1.678248643875122 3.299548625946045 4.977797508239746
Loss :  1.65879487991333 2.9640309810638428 4.622825622558594
Loss :  1.6554170846939087 2.6222996711730957 4.277716636657715
Loss :  5.5987958908081055 4.409925937652588 10.008722305297852
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.492970943450928 4.377081871032715 9.870052337646484
Loss :  5.58848762512207 4.271284103393555 9.859771728515625
Loss :  4.678847312927246 4.232863903045654 8.911710739135742
Total LOSS train 4.582723507514367 valid 9.662564277648926
CE LOSS train 1.6646856949879574 valid 1.1697118282318115
Contrastive LOSS train 2.918037799688486 valid 1.0582159757614136
EPOCH 150:
Loss :  1.6515512466430664 2.5678091049194336 4.2193603515625
Loss :  1.6663689613342285 3.2894508838653564 4.955820083618164
Loss :  1.652675986289978 3.0649302005767822 4.717606067657471
Loss :  1.6563938856124878 2.7046687602996826 4.361062526702881
Loss :  1.6802031993865967 2.7884862422943115 4.468689441680908
Loss :  1.664007544517517 2.8333346843719482 4.497342109680176
Loss :  1.6611614227294922 3.041861057281494 4.703022480010986
Loss :  1.6510932445526123 2.5110011100769043 4.1620941162109375
Loss :  1.655145287513733 2.196820020675659 3.8519654273986816
Loss :  1.6094950437545776 2.4330649375915527 4.04256010055542
Loss :  1.6693984270095825 3.261101245880127 4.93049955368042
Loss :  1.7297776937484741 2.887425661087036 4.617203235626221
Loss :  1.6753292083740234 3.477623701095581 5.152953147888184
Loss :  1.6644920110702515 3.4193966388702393 5.083888530731201
Loss :  1.6435589790344238 2.8392252922058105 4.482784271240234
Loss :  1.6522531509399414 3.164461374282837 4.816714286804199
Loss :  1.661786437034607 2.9142038822174072 4.575990200042725
Loss :  1.6608136892318726 2.8732821941375732 4.534095764160156
Loss :  1.6678110361099243 2.883608341217041 4.551419258117676
Loss :  1.6246193647384644 3.1053833961486816 4.7300028800964355
  batch 20 loss: 1.6246193647384644, 3.1053833961486816, 4.7300028800964355
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6597836017608643 3.3583314418792725 5.018115043640137
Loss :  1.675676941871643 2.913975477218628 4.5896525382995605
Loss :  1.6466113328933716 3.260887384414673 4.907498836517334
Loss :  1.68788480758667 3.2168002128601074 4.904685020446777
Loss :  1.6852413415908813 3.8110764026641846 5.4963178634643555
Loss :  1.6491477489471436 3.1390416622161865 4.78818941116333
Loss :  1.6979286670684814 3.423051118850708 5.1209797859191895
Loss :  1.6404364109039307 3.1924984455108643 4.832934856414795
Loss :  1.6885074377059937 3.0967299938201904 4.7852373123168945
Loss :  1.6421329975128174 3.438812732696533 5.08094596862793
Loss :  1.7232251167297363 3.588038682937622 5.3112640380859375
Loss :  1.6697518825531006 3.4760355949401855 5.145787239074707
Loss :  1.6549314260482788 3.064258575439453 4.7191901206970215
Loss :  1.6570477485656738 2.982858896255493 4.639906883239746
Loss :  1.6975597143173218 3.3634772300720215 5.061037063598633
Loss :  1.6888998746871948 3.191168785095215 4.880068778991699
Loss :  1.6669669151306152 3.0832982063293457 4.750265121459961
Loss :  1.6345492601394653 2.8852851390838623 4.519834518432617
Loss :  1.6602813005447388 2.702528476715088 4.362809658050537
Loss :  1.6527650356292725 3.2729039192199707 4.925668716430664
  batch 40 loss: 1.6527650356292725, 3.2729039192199707, 4.925668716430664
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6607362031936646 3.1497960090637207 4.810532093048096
Loss :  1.6490980386734009 2.9644575119018555 4.613555431365967
Loss :  1.6661311388015747 3.1850738525390625 4.851204872131348
Loss :  1.6651570796966553 2.7314672470092773 4.396624565124512
Loss :  1.667278528213501 3.5916085243225098 5.25888729095459
Loss :  1.6586687564849854 2.883430242538452 4.5420989990234375
Loss :  1.6460825204849243 3.190216541290283 4.836298942565918
Loss :  1.6581093072891235 3.130171298980713 4.788280487060547
Loss :  1.63022780418396 3.1382930278778076 4.768520832061768
Loss :  1.6820905208587646 2.8143904209136963 4.496480941772461
Loss :  1.6461504697799683 3.303096055984497 4.949246406555176
Loss :  1.6648298501968384 3.327953815460205 4.992783546447754
Loss :  1.6829692125320435 3.3584158420562744 5.041385173797607
Loss :  1.668528437614441 3.1410725116729736 4.809600830078125
Loss :  1.6708312034606934 3.3966479301452637 5.067479133605957
Loss :  1.635981798171997 3.284505605697632 4.920487403869629
Loss :  1.685225248336792 3.2004809379577637 4.885705947875977
Loss :  1.6824079751968384 3.1440911293029785 4.826498985290527
Loss :  1.6956455707550049 3.2321524620056152 4.927798271179199
Loss :  1.6714508533477783 3.4011452198028564 5.072596073150635
  batch 60 loss: 1.6714508533477783, 3.4011452198028564, 5.072596073150635
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6727279424667358 3.2551469802856445 4.92787504196167
Loss :  1.6698850393295288 2.7948572635650635 4.464742183685303
Loss :  1.6786144971847534 3.1311275959014893 4.809741973876953
Loss :  1.6584198474884033 3.157365322113037 4.8157854080200195
Loss :  1.6556063890457153 2.741581916809082 4.397188186645508
Loss :  5.657718658447266 4.402380466461182 10.060098648071289
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.5489277839660645 4.4436163902282715 9.992544174194336
Loss :  5.639823913574219 4.278343677520752 9.918167114257812
Loss :  4.749806880950928 4.348298072814941 9.098104476928711
Total LOSS train 4.762567101992094 valid 9.767228603363037
CE LOSS train 1.6646172248400175 valid 1.187451720237732
Contrastive LOSS train 3.0979498826540435 valid 1.0870745182037354
EPOCH 151:
Loss :  1.6519452333450317 2.7312262058258057 4.383171558380127
Loss :  1.6664725542068481 3.4732091426849365 5.139681816101074
Loss :  1.652914047241211 3.216813087463379 4.86972713470459
Loss :  1.6565546989440918 3.1946191787719727 4.8511738777160645
Loss :  1.6802949905395508 3.3268918991088867 5.0071868896484375
Loss :  1.6642879247665405 2.9938454627990723 4.658133506774902
Loss :  1.660603642463684 3.2235894203186035 4.884192943572998
Loss :  1.650563359260559 2.5178415775299072 4.168405055999756
Loss :  1.6550358533859253 2.9363625049591064 4.591398239135742
Loss :  1.6089223623275757 2.9843122959136963 4.593234539031982
Loss :  1.6694279909133911 3.2723376750946045 4.941765785217285
Loss :  1.7298728227615356 3.297506093978882 5.027379035949707
Loss :  1.6756861209869385 3.5617406368255615 5.2374267578125
Loss :  1.664417028427124 3.8137009143829346 5.478117942810059
Loss :  1.6431119441986084 3.127957820892334 4.771069526672363
Loss :  1.6523430347442627 3.338831901550293 4.991174697875977
Loss :  1.6617119312286377 3.0498194694519043 4.711531639099121
Loss :  1.6608459949493408 3.0760834217071533 4.736929416656494
Loss :  1.6681382656097412 3.0956032276153564 4.763741493225098
Loss :  1.624161958694458 3.2728970050811768 4.897058963775635
  batch 20 loss: 1.624161958694458, 3.2728970050811768, 4.897058963775635
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6593518257141113 2.8514583110809326 4.510809898376465
Loss :  1.6757375001907349 2.8959527015686035 4.571690082550049
Loss :  1.646308422088623 3.5837130546569824 5.2300214767456055
Loss :  1.6883282661437988 3.325850248336792 5.014178276062012
Loss :  1.685652256011963 3.601386785507202 5.287038803100586
Loss :  1.6489965915679932 3.013455867767334 4.662452697753906
Loss :  1.6977043151855469 3.212956428527832 4.910660743713379
Loss :  1.6402794122695923 2.914435386657715 4.554714679718018
Loss :  1.6882015466690063 2.9909965991973877 4.679198265075684
Loss :  1.6421303749084473 3.1951818466186523 4.8373122215271
Loss :  1.7232820987701416 3.3227040767669678 5.045986175537109
Loss :  1.6698673963546753 3.705873966217041 5.375741481781006
Loss :  1.6548243761062622 2.8844785690307617 4.539302825927734
Loss :  1.657139539718628 2.783245086669922 4.440384864807129
Loss :  1.6975789070129395 3.1484031677246094 4.845982074737549
Loss :  1.6888381242752075 3.4671266078948975 5.1559648513793945
Loss :  1.666840672492981 3.077824592590332 4.744665145874023
Loss :  1.6346814632415771 3.2875938415527344 4.922275543212891
Loss :  1.6601526737213135 3.3321726322174072 4.992325305938721
Loss :  1.6526129245758057 2.991393804550171 4.644006729125977
  batch 40 loss: 1.6526129245758057, 2.991393804550171, 4.644006729125977
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6606335639953613 3.2854554653167725 4.946088790893555
Loss :  1.6488505601882935 2.7854578495025635 4.4343085289001465
Loss :  1.6660467386245728 3.2374215126037598 4.903468132019043
Loss :  1.664848804473877 2.9281187057495117 4.592967510223389
Loss :  1.6673370599746704 3.0406687259674072 4.708005905151367
Loss :  1.6585668325424194 3.5188562870025635 5.177423000335693
Loss :  1.64581298828125 3.319214105606079 4.96502685546875
Loss :  1.6578015089035034 3.6557908058166504 5.313592433929443
Loss :  1.6302599906921387 3.4551360607147217 5.085395812988281
Loss :  1.682113766670227 3.4036872386932373 5.085801124572754
Loss :  1.6460864543914795 3.111140012741089 4.757226467132568
Loss :  1.6649060249328613 2.8340935707092285 4.49899959564209
Loss :  1.682802677154541 3.4099502563476562 5.092752933502197
Loss :  1.6685651540756226 3.5919368267059326 5.260501861572266
Loss :  1.6708003282546997 3.4315688610076904 5.10236930847168
Loss :  1.6358228921890259 3.2196173667907715 4.855440139770508
Loss :  1.6851351261138916 3.2062363624572754 4.891371726989746
Loss :  1.6823631525039673 2.985668659210205 4.668031692504883
Loss :  1.6956677436828613 3.0805249214172363 4.776192665100098
Loss :  1.6715974807739258 3.2504122257232666 4.922009468078613
  batch 60 loss: 1.6715974807739258, 3.2504122257232666, 4.922009468078613
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726869344711304 3.54852557182312 5.221212387084961
Loss :  1.6699928045272827 2.7176475524902344 4.387640476226807
Loss :  1.6786866188049316 2.787919759750366 4.466606140136719
Loss :  1.658466100692749 3.0353970527648926 4.6938629150390625
Loss :  1.6555187702178955 2.888167381286621 4.5436859130859375
Loss :  5.662712574005127 4.407058238983154 10.069770812988281
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.552555561065674 4.368407726287842 9.920963287353516
Loss :  5.646642684936523 4.278382301330566 9.92502498626709
Loss :  4.75372838973999 4.303469657897949 9.057197570800781
Total LOSS train 4.846418380737305 valid 9.743239164352417
CE LOSS train 1.6645721618945781 valid 1.1884320974349976
Contrastive LOSS train 3.181846240850595 valid 1.0758674144744873
EPOCH 152:
Loss :  1.6519993543624878 2.625322103500366 4.2773213386535645
Loss :  1.6665667295455933 3.117968797683716 4.7845354080200195
Loss :  1.6528328657150269 2.9203882217407227 4.573221206665039
Loss :  1.6566003561019897 2.8686702251434326 4.525270462036133
Loss :  1.6804620027542114 3.270611524581909 4.95107364654541
Loss :  1.6643714904785156 2.8296456336975098 4.494017124176025
Loss :  1.6606557369232178 3.2991836071014404 4.959839344024658
Loss :  1.6506202220916748 2.8651816844940186 4.515801906585693
Loss :  1.6551039218902588 2.8592138290405273 4.514317512512207
Loss :  1.6088476181030273 2.9218225479125977 4.530670166015625
Loss :  1.6693669557571411 3.1415510177612305 4.810917854309082
Loss :  1.730149269104004 3.1049537658691406 4.8351030349731445
Loss :  1.6754785776138306 3.3150360584259033 4.990514755249023
Loss :  1.6644874811172485 3.665419816970825 5.329907417297363
Loss :  1.6432878971099854 2.8745362758636475 4.517824172973633
Loss :  1.6523233652114868 3.144092082977295 4.796415328979492
Loss :  1.661726713180542 2.9229214191436768 4.584648132324219
Loss :  1.6607688665390015 2.988477945327759 4.649246692657471
Loss :  1.6680097579956055 2.7492661476135254 4.417275905609131
Loss :  1.6243441104888916 3.1207096576690674 4.745053768157959
  batch 20 loss: 1.6243441104888916, 3.1207096576690674, 4.745053768157959
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6593990325927734 3.3247268199920654 4.984126091003418
Loss :  1.675750494003296 2.670128345489502 4.345878601074219
Loss :  1.646377682685852 3.1643693447113037 4.810747146606445
Loss :  1.688394546508789 3.2089226245880127 4.897316932678223
Loss :  1.6856926679611206 3.4494028091430664 5.135095596313477
Loss :  1.6490696668624878 3.006685733795166 4.655755519866943
Loss :  1.697736144065857 3.079683780670166 4.7774200439453125
Loss :  1.6403083801269531 3.0570380687713623 4.6973466873168945
Loss :  1.6881532669067383 2.898271083831787 4.586424350738525
Loss :  1.6422308683395386 2.8672635555267334 4.509494304656982
Loss :  1.7232825756072998 3.176840305328369 4.90012264251709
Loss :  1.6698602437973022 3.1605565547943115 4.830416679382324
Loss :  1.6548079252243042 2.811781644821167 4.466589450836182
Loss :  1.6571717262268066 2.593512535095215 4.2506842613220215
Loss :  1.6975373029708862 3.2904670238494873 4.988004207611084
Loss :  1.6888231039047241 3.2061989307403564 4.895021915435791
Loss :  1.6668596267700195 3.247872829437256 4.914732456207275
Loss :  1.63456392288208 2.8072152137756348 4.441779136657715
Loss :  1.6602387428283691 2.99393367767334 4.654172420501709
Loss :  1.6527154445648193 3.031266689300537 4.683981895446777
  batch 40 loss: 1.6527154445648193, 3.031266689300537, 4.683981895446777
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.660649299621582 3.0807931423187256 4.741442680358887
Loss :  1.6488803625106812 2.843747854232788 4.49262809753418
Loss :  1.6660350561141968 3.09029221534729 4.756327152252197
Loss :  1.6648808717727661 2.842068910598755 4.5069499015808105
Loss :  1.6672910451889038 2.819349765777588 4.486640930175781
Loss :  1.6586097478866577 2.9211161136627197 4.579725742340088
Loss :  1.645832896232605 3.133326768875122 4.7791595458984375
Loss :  1.657800555229187 3.341330051422119 4.999130725860596
Loss :  1.6303164958953857 3.2951433658599854 4.925459861755371
Loss :  1.6820955276489258 3.150439977645874 4.832535743713379
Loss :  1.6461236476898193 3.0960397720336914 4.74216365814209
Loss :  1.6649264097213745 2.9097490310668945 4.574675559997559
Loss :  1.682826280593872 3.6209352016448975 5.3037614822387695
Loss :  1.6685348749160767 3.4173824787139893 5.0859174728393555
Loss :  1.6708468198776245 3.5247902870178223 5.195637226104736
Loss :  1.6358559131622314 2.9368879795074463 4.572743892669678
Loss :  1.6851637363433838 3.33188796043396 5.017051696777344
Loss :  1.6823253631591797 3.3576464653015137 5.039971828460693
Loss :  1.6956837177276611 3.2715580463409424 4.9672417640686035
Loss :  1.6716612577438354 2.9543793201446533 4.626040458679199
  batch 60 loss: 1.6716612577438354, 2.9543793201446533, 4.626040458679199
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6727221012115479 3.505694627761841 5.178416728973389
Loss :  1.669962763786316 2.5380544662475586 4.208017349243164
Loss :  1.6786062717437744 3.1429734230041504 4.821579933166504
Loss :  1.658475399017334 3.1012120246887207 4.759687423706055
Loss :  1.655556082725525 2.850459575653076 4.506015777587891
Loss :  5.650583267211914 4.427319526672363 10.077902793884277
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.539895534515381 4.397730827331543 9.937625885009766
Loss :  5.635973930358887 4.17154598236084 9.807519912719727
Loss :  4.741617202758789 4.109530448913574 8.851147651672363
Total LOSS train 4.73733858695397 valid 9.668549060821533
CE LOSS train 1.664594448529757 valid 1.1854043006896973
Contrastive LOSS train 3.0727441347562348 valid 1.0273826122283936
EPOCH 153:
Loss :  1.6519370079040527 2.492976665496826 4.144913673400879
Loss :  1.666565179824829 3.1873505115509033 4.853915691375732
Loss :  1.6527752876281738 2.9309611320495605 4.583736419677734
Loss :  1.656542181968689 3.0809743404388428 4.737516403198242
Loss :  1.6804156303405762 3.2710928916931152 4.951508522033691
Loss :  1.6643315553665161 2.673044443130493 4.337376117706299
Loss :  1.6607125997543335 3.239783525466919 4.900496006011963
Loss :  1.6506567001342773 2.4630377292633057 4.113694190979004
Loss :  1.6551464796066284 2.5962767601013184 4.251423358917236
Loss :  1.6089295148849487 2.4291887283325195 4.038118362426758
Loss :  1.6693363189697266 2.915546417236328 4.584882736206055
Loss :  1.7300339937210083 3.281554698944092 5.0115885734558105
Loss :  1.6753312349319458 3.5074379444122314 5.182769298553467
Loss :  1.6645112037658691 3.272639274597168 4.937150478363037
Loss :  1.6433088779449463 2.8262217044830322 4.4695305824279785
Loss :  1.652321219444275 3.0343470573425293 4.686668395996094
Loss :  1.6616811752319336 2.73791241645813 4.399593353271484
Loss :  1.660639762878418 2.7693891525268555 4.430028915405273
Loss :  1.6679705381393433 2.6512160301208496 4.319186687469482
Loss :  1.6243386268615723 3.214948892593384 4.839287757873535
  batch 20 loss: 1.6243386268615723, 3.214948892593384, 4.839287757873535
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6594369411468506 3.2477221488952637 4.907158851623535
Loss :  1.6757339239120483 2.655277729034424 4.331011772155762
Loss :  1.6464581489562988 3.022881507873535 4.669339656829834
Loss :  1.6883841753005981 2.9253623485565186 4.613746643066406
Loss :  1.6856575012207031 3.152998208999634 4.838655471801758
Loss :  1.6490713357925415 2.8841919898986816 4.533263206481934
Loss :  1.6977595090866089 3.172689914703369 4.870449542999268
Loss :  1.6402766704559326 2.841552495956421 4.4818291664123535
Loss :  1.688148021697998 2.7875022888183594 4.475650310516357
Loss :  1.6422697305679321 2.7955665588378906 4.437836170196533
Loss :  1.7232364416122437 3.408649444580078 5.131886005401611
Loss :  1.6697447299957275 3.0290679931640625 4.698812484741211
Loss :  1.6547795534133911 2.8891310691833496 4.543910503387451
Loss :  1.6571322679519653 2.912414073944092 4.569546222686768
Loss :  1.6975029706954956 3.235969066619873 4.933472156524658
Loss :  1.688801646232605 3.2363946437835693 4.925196170806885
Loss :  1.666861653327942 3.068408727645874 4.7352705001831055
Loss :  1.6344510316848755 2.906524419784546 4.540975570678711
Loss :  1.6601535081863403 2.9517529010772705 4.6119065284729
Loss :  1.6526546478271484 3.05884051322937 4.711495399475098
  batch 40 loss: 1.6526546478271484, 3.05884051322937, 4.711495399475098
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6606107950210571 2.840376377105713 4.5009870529174805
Loss :  1.648861289024353 2.67566180229187 4.324522972106934
Loss :  1.6660044193267822 2.92749285697937 4.593497276306152
Loss :  1.664824366569519 3.091933250427246 4.756757736206055
Loss :  1.6672776937484741 2.5484089851379395 4.215686798095703
Loss :  1.6585747003555298 2.940882921218872 4.599457740783691
Loss :  1.6456681489944458 3.241840124130249 4.887508392333984
Loss :  1.6577624082565308 3.3381359577178955 4.995898246765137
Loss :  1.6302540302276611 3.566462993621826 5.196717262268066
Loss :  1.6820446252822876 3.4384710788726807 5.120515823364258
Loss :  1.6461520195007324 3.1259772777557373 4.772129058837891
Loss :  1.6648895740509033 2.9328818321228027 4.597771644592285
Loss :  1.6828185319900513 3.2182443141937256 4.901062965393066
Loss :  1.6684234142303467 3.5864715576171875 5.254895210266113
Loss :  1.6707898378372192 3.7994284629821777 5.470218181610107
Loss :  1.6358031034469604 2.9780266284942627 4.613829612731934
Loss :  1.6851425170898438 3.331015110015869 5.016157627105713
Loss :  1.6823217868804932 3.023461103439331 4.705782890319824
Loss :  1.6956679821014404 3.214564085006714 4.910232067108154
Loss :  1.6716054677963257 2.9744818210601807 4.646087169647217
  batch 60 loss: 1.6716054677963257, 2.9744818210601807, 4.646087169647217
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.672730803489685 3.4750924110412598 5.147823333740234
Loss :  1.6699098348617554 2.761434316635132 4.431344032287598
Loss :  1.6785167455673218 2.9597537517547607 4.638270378112793
Loss :  1.6584229469299316 2.879561185836792 4.5379838943481445
Loss :  1.6555061340332031 2.7046077251434326 4.360114097595215
Loss :  5.6459479331970215 4.431514263153076 10.077462196350098
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.538121223449707 4.297297477722168 9.835418701171875
Loss :  5.633214473724365 4.293696880340576 9.926911354064941
Loss :  4.735489845275879 4.221127986907959 8.95661735534668
Total LOSS train 4.685077681908241 valid 9.699102401733398
CE LOSS train 1.6645628103843102 valid 1.1838724613189697
Contrastive LOSS train 3.0205148660219634 valid 1.0552819967269897
EPOCH 154:
Loss :  1.6519155502319336 2.705671548843384 4.357586860656738
Loss :  1.66649329662323 3.0777359008789062 4.744229316711426
Loss :  1.652751088142395 2.8855667114257812 4.538317680358887
Loss :  1.656494379043579 2.8834240436553955 4.539918422698975
Loss :  1.680364727973938 3.1508371829986572 4.831202030181885
Loss :  1.6642875671386719 2.9878907203674316 4.6521782875061035
Loss :  1.6607261896133423 3.7284200191497803 5.389146327972412
Loss :  1.650609016418457 2.681847095489502 4.332456111907959
Loss :  1.6551430225372314 2.4994747638702393 4.154617786407471
Loss :  1.6089140176773071 2.6286795139312744 4.237593650817871
Loss :  1.669301152229309 3.065690279006958 4.734991550445557
Loss :  1.7300019264221191 3.228671073913574 4.958673000335693
Loss :  1.6754285097122192 3.6618266105651855 5.337255001068115
Loss :  1.664457082748413 3.191910743713379 4.856368064880371
Loss :  1.6433056592941284 2.8718626499176025 4.515168190002441
Loss :  1.6522715091705322 3.2931089401245117 4.945380210876465
Loss :  1.6616398096084595 2.65108585357666 4.31272554397583
Loss :  1.660639762878418 2.892000913619995 4.552640914916992
Loss :  1.667954921722412 2.571436882019043 4.239391803741455
Loss :  1.6242865324020386 3.021578073501587 4.645864486694336
  batch 20 loss: 1.6242865324020386, 3.021578073501587, 4.645864486694336
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6594336032867432 3.140836238861084 4.800270080566406
Loss :  1.675710916519165 2.791145086288452 4.466856002807617
Loss :  1.6463961601257324 3.3505241870880127 4.996920585632324
Loss :  1.6883442401885986 2.815317392349243 4.503661632537842
Loss :  1.6856714487075806 3.4635121822357178 5.149183750152588
Loss :  1.6490362882614136 2.9198102951049805 4.568846702575684
Loss :  1.697704553604126 3.254793405532837 4.952497959136963
Loss :  1.6402474641799927 3.142167568206787 4.78241491317749
Loss :  1.688118577003479 2.6110732555389404 4.299191951751709
Loss :  1.6422085762023926 2.9096505641937256 4.551858901977539
Loss :  1.7232722043991089 3.213067054748535 4.936339378356934
Loss :  1.6697279214859009 3.0177013874053955 4.687429428100586
Loss :  1.6548157930374146 2.9703712463378906 4.625186920166016
Loss :  1.6571416854858398 2.6527018547058105 4.30984354019165
Loss :  1.6974952220916748 3.2522456645965576 4.949740886688232
Loss :  1.6887625455856323 3.2253239154815674 4.91408634185791
Loss :  1.6668457984924316 3.009643793106079 4.67648983001709
Loss :  1.634428858757019 2.6525371074676514 4.286965847015381
Loss :  1.6601613759994507 2.8668439388275146 4.527005195617676
Loss :  1.6526480913162231 2.989011526107788 4.641659736633301
  batch 40 loss: 1.6526480913162231, 2.989011526107788, 4.641659736633301
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6605767011642456 3.2564423084259033 4.917018890380859
Loss :  1.6487934589385986 2.604339838027954 4.253133296966553
Loss :  1.6659716367721558 3.088282585144043 4.754254341125488
Loss :  1.6647834777832031 2.767268419265747 4.432051658630371
Loss :  1.6671762466430664 2.5597503185272217 4.226926803588867
Loss :  1.6585551500320435 2.9647767543792725 4.6233320236206055
Loss :  1.6456712484359741 3.4367730617523193 5.082444190979004
Loss :  1.6577341556549072 3.3847744464874268 5.042508602142334
Loss :  1.6302636861801147 3.3770625591278076 5.007326126098633
Loss :  1.68194580078125 3.010187864303589 4.692133903503418
Loss :  1.6461372375488281 2.624894142150879 4.271031379699707
Loss :  1.664908528327942 2.970531702041626 4.635440349578857
Loss :  1.6828335523605347 3.0036802291870117 4.686513900756836
Loss :  1.6684091091156006 3.2915101051330566 4.959918975830078
Loss :  1.670854091644287 3.5131800174713135 5.18403434753418
Loss :  1.6358582973480225 2.9269092082977295 4.562767505645752
Loss :  1.6851305961608887 3.313293695449829 4.998424530029297
Loss :  1.6822898387908936 2.965165853500366 4.64745569229126
Loss :  1.6957141160964966 3.122288942337036 4.818003177642822
Loss :  1.6716607809066772 2.952263832092285 4.623924732208252
  batch 60 loss: 1.6716607809066772, 2.952263832092285, 4.623924732208252
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6727015972137451 3.469515562057495 5.14221715927124
Loss :  1.66985023021698 2.625458002090454 4.2953081130981445
Loss :  1.6783732175827026 2.6500837802886963 4.328456878662109
Loss :  1.6584079265594482 3.0041275024414062 4.662535667419434
Loss :  1.65544593334198 2.361614942550659 4.01706075668335
Loss :  5.633598804473877 4.398379802703857 10.031978607177734
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.524969100952148 4.4097418785095215 9.934711456298828
Loss :  5.622381210327148 4.3310441970825195 9.953425407409668
Loss :  4.721236228942871 4.309841632843018 9.031078338623047
Total LOSS train 4.667175043546236 valid 9.73779845237732
CE LOSS train 1.6645415947987483 valid 1.1803090572357178
Contrastive LOSS train 3.0026334285736085 valid 1.0774604082107544
EPOCH 155:
Loss :  1.651810884475708 2.4751462936401367 4.126956939697266
Loss :  1.6664419174194336 3.4002485275268555 5.066690444946289
Loss :  1.6526838541030884 2.7691433429718018 4.42182731628418
Loss :  1.6564221382141113 2.8586831092834473 4.515105247497559
Loss :  1.680293321609497 3.3355283737182617 5.01582145690918
Loss :  1.6642203330993652 2.731504201889038 4.395724296569824
Loss :  1.6607372760772705 3.1860885620117188 4.84682559967041
Loss :  1.6506152153015137 2.6113228797912598 4.261938095092773
Loss :  1.6551463603973389 2.5851495265960693 4.240295886993408
Loss :  1.6089897155761719 2.4978206157684326 4.106810569763184
Loss :  1.6692286729812622 2.949629306793213 4.6188578605651855
Loss :  1.729819416999817 2.9907591342926025 4.720578670501709
Loss :  1.6753617525100708 3.2332377433776855 4.908599376678467
Loss :  1.664497971534729 3.29486346244812 4.959361553192139
Loss :  1.6433268785476685 2.9128410816192627 4.556168079376221
Loss :  1.6522371768951416 2.9644248485565186 4.61666202545166
Loss :  1.6615620851516724 2.5984489917755127 4.260011196136475
Loss :  1.6604938507080078 3.0310583114624023 4.69155216217041
Loss :  1.6678049564361572 2.321723699569702 3.9895286560058594
Loss :  1.6243232488632202 2.8757195472717285 4.500042915344238
  batch 20 loss: 1.6243232488632202, 2.8757195472717285, 4.500042915344238
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.659467101097107 3.300156831741333 4.95962381362915
Loss :  1.6757131814956665 2.559828996658325 4.235542297363281
Loss :  1.6464589834213257 2.9171688556671143 4.56362771987915
Loss :  1.6882582902908325 2.8339600563049316 4.522218227386475
Loss :  1.6855837106704712 3.4774177074432373 5.163001537322998
Loss :  1.6490564346313477 2.661663293838501 4.3107194900512695
Loss :  1.6976662874221802 3.1587748527526855 4.856441020965576
Loss :  1.6402411460876465 2.9543237686157227 4.594564914703369
Loss :  1.688118815422058 2.7737298011779785 4.461848735809326
Loss :  1.6422011852264404 2.8655476570129395 4.507748603820801
Loss :  1.7232601642608643 3.3728525638580322 5.0961127281188965
Loss :  1.669588565826416 3.0879924297332764 4.757580757141113
Loss :  1.6547434329986572 2.8322818279266357 4.487025260925293
Loss :  1.657121181488037 2.9028170108795166 4.559938430786133
Loss :  1.6974365711212158 2.955721139907837 4.653157711029053
Loss :  1.6886836290359497 2.8940043449401855 4.582687854766846
Loss :  1.666857361793518 2.963524103164673 4.6303815841674805
Loss :  1.6343339681625366 2.8201425075531006 4.454476356506348
Loss :  1.6601091623306274 2.6207950115203857 4.280904293060303
Loss :  1.652616262435913 2.975264549255371 4.627881050109863
  batch 40 loss: 1.652616262435913, 2.975264549255371, 4.627881050109863
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6605592966079712 2.8281280994415283 4.488687515258789
Loss :  1.6487563848495483 2.5017611980438232 4.150517463684082
Loss :  1.66598641872406 3.0330286026000977 4.699015140533447
Loss :  1.6646983623504639 2.864994764328003 4.529693126678467
Loss :  1.6671202182769775 2.709785223007202 4.37690544128418
Loss :  1.6585685014724731 2.9945456981658936 4.653114318847656
Loss :  1.645554780960083 3.1489622592926025 4.7945170402526855
Loss :  1.6576874256134033 3.105252504348755 4.762939929962158
Loss :  1.630280613899231 3.1783981323242188 4.80867862701416
Loss :  1.6819125413894653 3.0512547492980957 4.7331671714782715
Loss :  1.6461169719696045 2.857404947280884 4.503521919250488
Loss :  1.6649127006530762 2.801999092102051 4.466911792755127
Loss :  1.6827723979949951 3.217857837677002 4.900629997253418
Loss :  1.6683111190795898 2.8980801105499268 4.5663909912109375
Loss :  1.670872688293457 3.352109670639038 5.022982597351074
Loss :  1.6358811855316162 2.797569751739502 4.433450698852539
Loss :  1.6851345300674438 3.0966622829437256 4.781796932220459
Loss :  1.6822354793548584 2.742795705795288 4.4250311851501465
Loss :  1.6956872940063477 3.1725733280181885 4.868260383605957
Loss :  1.6716750860214233 2.87484073638916 4.546515941619873
  batch 60 loss: 1.6716750860214233, 2.87484073638916, 4.546515941619873
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726739406585693 3.2885918617248535 4.961265563964844
Loss :  1.6698133945465088 2.7705729007720947 4.4403862953186035
Loss :  1.6782737970352173 2.8448104858398438 4.5230841636657715
Loss :  1.6584457159042358 2.7329976558685303 4.391443252563477
Loss :  1.655411958694458 2.573058843612671 4.228470802307129
Loss :  5.6378350257873535 4.51682710647583 10.154662132263184
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.528842926025391 4.364343643188477 9.893186569213867
Loss :  5.625863552093506 4.225842475891113 9.851705551147461
Loss :  4.723310470581055 4.318264484405518 9.041574478149414
Total LOSS train 4.587418754284198 valid 9.735282182693481
CE LOSS train 1.6645057733242328 valid 1.1808276176452637
Contrastive LOSS train 2.9229130048018237 valid 1.0795661211013794
EPOCH 156:
Loss :  1.651778221130371 2.611549139022827 4.263327598571777
Loss :  1.6664094924926758 3.0771400928497314 4.743549346923828
Loss :  1.652616024017334 3.295240879058838 4.947856903076172
Loss :  1.6563705205917358 2.8582546710968018 4.514625072479248
Loss :  1.6802525520324707 3.2910819053649902 4.971334457397461
Loss :  1.6641392707824707 2.956894874572754 4.621034145355225
Loss :  1.6607576608657837 3.1307382583618164 4.7914958000183105
Loss :  1.6506036520004272 2.620156764984131 4.270760536193848
Loss :  1.655212640762329 2.338318347930908 3.9935309886932373
Loss :  1.6089756488800049 2.5405266284942627 4.149502277374268
Loss :  1.6691969633102417 2.9112532138824463 4.580450057983398
Loss :  1.7298600673675537 2.9442121982574463 4.674072265625
Loss :  1.6753578186035156 3.1558470726013184 4.831204891204834
Loss :  1.6644917726516724 3.224504232406616 4.888996124267578
Loss :  1.6434382200241089 2.6441895961761475 4.287627696990967
Loss :  1.6522256135940552 2.8604414463043213 4.512667179107666
Loss :  1.661546230316162 2.5487864017486572 4.210332870483398
Loss :  1.660353422164917 2.988502264022827 4.648855686187744
Loss :  1.6678136587142944 2.8550572395324707 4.522871017456055
Loss :  1.6243871450424194 2.9070229530334473 4.531410217285156
  batch 20 loss: 1.6243871450424194, 2.9070229530334473, 4.531410217285156
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6595100164413452 3.1622283458709717 4.821738243103027
Loss :  1.6757005453109741 2.7325639724731445 4.408264636993408
Loss :  1.6465216875076294 3.034186601638794 4.680708408355713
Loss :  1.6882426738739014 2.710667848587036 4.3989105224609375
Loss :  1.6855909824371338 3.5421230792999268 5.2277140617370605
Loss :  1.64911687374115 2.6696314811706543 4.318748474121094
Loss :  1.6976014375686646 3.210054874420166 4.907656192779541
Loss :  1.6402592658996582 2.3850021362304688 4.025261402130127
Loss :  1.6880868673324585 2.688014507293701 4.376101493835449
Loss :  1.6422377824783325 2.8295629024505615 4.471800804138184
Loss :  1.7232778072357178 3.089247226715088 4.812524795532227
Loss :  1.6695172786712646 2.9674363136291504 4.636953353881836
Loss :  1.6547436714172363 2.7826356887817383 4.437379360198975
Loss :  1.6571470499038696 2.6501524448394775 4.307299613952637
Loss :  1.6973876953125 3.0050930976867676 4.702480792999268
Loss :  1.6886992454528809 2.9099724292755127 4.598671913146973
Loss :  1.6668442487716675 2.875361204147339 4.542205333709717
Loss :  1.6342990398406982 2.895246982574463 4.529545783996582
Loss :  1.660123586654663 2.9443857669830322 4.604509353637695
Loss :  1.6525819301605225 2.7868075370788574 4.439389228820801
  batch 40 loss: 1.6525819301605225, 2.7868075370788574, 4.439389228820801
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.66055428981781 2.9348134994506836 4.595367908477783
Loss :  1.6487823724746704 2.453709840774536 4.102492332458496
Loss :  1.6660332679748535 2.867481231689453 4.533514499664307
Loss :  1.6646790504455566 2.721407890319824 4.386086940765381
Loss :  1.6671533584594727 2.344029188156128 4.01118278503418
Loss :  1.658555269241333 2.8096096515655518 4.468164920806885
Loss :  1.6455681324005127 3.1490752696990967 4.794643402099609
Loss :  1.6577248573303223 3.188129186630249 4.845853805541992
Loss :  1.6302767992019653 3.2231035232543945 4.85338020324707
Loss :  1.681922435760498 3.0235321521759033 4.7054548263549805
Loss :  1.6461970806121826 2.6664721965789795 4.312669277191162
Loss :  1.6649082899093628 2.645617723464966 4.310525894165039
Loss :  1.6827778816223145 3.2870078086853027 4.969785690307617
Loss :  1.668309211730957 2.9047746658325195 4.573083877563477
Loss :  1.6708688735961914 3.5985801219940186 5.269449234008789
Loss :  1.6357992887496948 2.727543592453003 4.363342761993408
Loss :  1.6851155757904053 3.4231371879577637 5.10825252532959
Loss :  1.682294249534607 2.994476556777954 4.6767706871032715
Loss :  1.6957138776779175 3.1336569786071777 4.829370975494385
Loss :  1.6716536283493042 2.8363921642303467 4.508045673370361
  batch 60 loss: 1.6716536283493042, 2.8363921642303467, 4.508045673370361
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726497411727905 3.529080390930176 5.201730251312256
Loss :  1.669766902923584 2.3607304096221924 4.0304975509643555
Loss :  1.6782114505767822 2.8788692951202393 4.5570807456970215
Loss :  1.6584466695785522 2.6518423557281494 4.310288906097412
Loss :  1.65542733669281 2.495689630508423 4.151116847991943
Loss :  5.621793270111084 4.3637590408325195 9.985551834106445
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.5143513679504395 4.3665618896484375 9.880912780761719
Loss :  5.609630584716797 4.259169101715088 9.868799209594727
Loss :  4.705048084259033 4.308093547821045 9.013141632080078
Total LOSS train 4.564177252696111 valid 9.687101364135742
CE LOSS train 1.664502556507404 valid 1.1762620210647583
Contrastive LOSS train 2.8996746943547174 valid 1.0770233869552612
EPOCH 157:
Loss :  1.6516950130462646 2.7039148807525635 4.355609893798828
Loss :  1.6663575172424316 3.0688884258270264 4.735245704650879
Loss :  1.6526153087615967 2.4108407497406006 4.063456058502197
Loss :  1.6563210487365723 2.924445390701294 4.580766677856445
Loss :  1.6803042888641357 3.4053163528442383 5.085620880126953
Loss :  1.664079189300537 3.012723684310913 4.676802635192871
Loss :  1.66079580783844 3.1331229209899902 4.793918609619141
Loss :  1.6505769491195679 2.595512866973877 4.246089935302734
Loss :  1.655164122581482 2.2776806354522705 3.932844638824463
Loss :  1.6090179681777954 2.4972784519195557 4.106296539306641
Loss :  1.6691734790802002 2.9388339519500732 4.608007431030273
Loss :  1.72976553440094 2.8415024280548096 4.571268081665039
Loss :  1.6755460500717163 3.1043155193328857 4.7798614501953125
Loss :  1.6644644737243652 3.1728858947753906 4.837350368499756
Loss :  1.6434204578399658 2.518406629562378 4.161827087402344
Loss :  1.6521852016448975 2.9671289920806885 4.619314193725586
Loss :  1.6615461111068726 2.7956762313842773 4.4572224617004395
Loss :  1.6603267192840576 3.1675379276275635 4.827864646911621
Loss :  1.6677805185317993 2.637362003326416 4.305142402648926
Loss :  1.6243557929992676 2.8247199058532715 4.449075698852539
  batch 20 loss: 1.6243557929992676, 2.8247199058532715, 4.449075698852539
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6595796346664429 3.229771852493286 4.8893513679504395
Loss :  1.6756726503372192 2.611605167388916 4.287277698516846
Loss :  1.6465932130813599 3.047374963760376 4.693968296051025
Loss :  1.6881812810897827 2.9049603939056396 4.593141555786133
Loss :  1.6855807304382324 3.4555323123931885 5.14111328125
Loss :  1.6491483449935913 2.549508571624756 4.198657035827637
Loss :  1.697628140449524 3.079831838607788 4.777460098266602
Loss :  1.640275239944458 2.546570062637329 4.186845302581787
Loss :  1.6881259679794312 2.608595371246338 4.296721458435059
Loss :  1.6422545909881592 2.560622215270996 4.202877044677734
Loss :  1.7232683897018433 3.0537943840026855 4.777062892913818
Loss :  1.6695128679275513 2.8587002754211426 4.528213024139404
Loss :  1.6547166109085083 2.7306759357452393 4.385392665863037
Loss :  1.6571617126464844 2.7351126670837402 4.392274379730225
Loss :  1.6974064111709595 3.0665535926818848 4.763959884643555
Loss :  1.6887493133544922 2.672999620437622 4.361748695373535
Loss :  1.6668754816055298 2.9581940174102783 4.625069618225098
Loss :  1.6342768669128418 2.9749960899353027 4.6092729568481445
Loss :  1.6600768566131592 2.7666168212890625 4.426693916320801
Loss :  1.652584195137024 3.070889711380005 4.723474025726318
  batch 40 loss: 1.652584195137024, 3.070889711380005, 4.723474025726318
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6605474948883057 2.819944143295288 4.480491638183594
Loss :  1.6488182544708252 2.6365437507629395 4.285362243652344
Loss :  1.666053295135498 2.8133816719055176 4.479434967041016
Loss :  1.6645963191986084 2.6471035480499268 4.311699867248535
Loss :  1.6671167612075806 2.273193120956421 3.940310001373291
Loss :  1.6586142778396606 2.987084150314331 4.645698547363281
Loss :  1.6455434560775757 3.2267441749572754 4.872287750244141
Loss :  1.6576566696166992 3.3753533363342285 5.033010005950928
Loss :  1.630319595336914 3.52559232711792 5.155911922454834
Loss :  1.6819119453430176 3.199714183807373 4.881626129150391
Loss :  1.6462010145187378 2.692814350128174 4.339015483856201
Loss :  1.6649103164672852 2.957949161529541 4.622859477996826
Loss :  1.6828105449676514 3.2009494304656982 4.88375997543335
Loss :  1.668212652206421 3.1911959648132324 4.859408378601074
Loss :  1.670828938484192 3.4057531356811523 5.076581954956055
Loss :  1.635748028755188 2.7856686115264893 4.421416759490967
Loss :  1.6851286888122559 3.0607614517211914 4.745890140533447
Loss :  1.6822391748428345 2.76936936378479 4.451608657836914
Loss :  1.6957571506500244 2.9915661811828613 4.687323570251465
Loss :  1.6716368198394775 2.80238676071167 4.474023818969727
  batch 60 loss: 1.6716368198394775, 2.80238676071167, 4.474023818969727
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.672681212425232 3.876368522644043 5.5490498542785645
Loss :  1.6697001457214355 2.4520955085754395 4.121795654296875
Loss :  1.678134560585022 2.972832679748535 4.650967121124268
Loss :  1.6584322452545166 2.859299659729004 4.517731666564941
Loss :  1.6554397344589233 2.6500635147094727 4.3055033683776855
Loss :  5.6048736572265625 4.374417781829834 9.979291915893555
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.496774673461914 4.3634114265441895 9.860185623168945
Loss :  5.592386722564697 4.3305983543396 9.922985076904297
Loss :  4.681033611297607 4.262336254119873 8.94336986541748
Total LOSS train 4.566875839233399 valid 9.67645812034607
CE LOSS train 1.664495374606206 valid 1.1702584028244019
Contrastive LOSS train 2.9023804371173565 valid 1.0655840635299683
EPOCH 158:
Loss :  1.6515395641326904 2.5226125717163086 4.174152374267578
Loss :  1.6663042306900024 3.258044958114624 4.924349308013916
Loss :  1.6525391340255737 2.478182792663574 4.1307220458984375
Loss :  1.6561648845672607 2.831627368927002 4.487792015075684
Loss :  1.6802759170532227 3.203575849533081 4.883852005004883
Loss :  1.6639630794525146 2.90150785446167 4.5654706954956055
Loss :  1.6608635187149048 2.688217878341675 4.349081516265869
Loss :  1.6505746841430664 2.58951735496521 4.2400922775268555
Loss :  1.6551438570022583 2.207408905029297 3.8625526428222656
Loss :  1.609112024307251 2.581139087677002 4.190251350402832
Loss :  1.6691503524780273 2.865039587020874 4.5341901779174805
Loss :  1.7296311855316162 2.894667863845825 4.624299049377441
Loss :  1.6755528450012207 3.1067776679992676 4.782330513000488
Loss :  1.6644700765609741 3.2519350051879883 4.916405200958252
Loss :  1.6434130668640137 2.601534605026245 4.24494743347168
Loss :  1.6521464586257935 2.810598611831665 4.462745189666748
Loss :  1.661499261856079 2.532351493835449 4.193850517272949
Loss :  1.6602638959884644 2.987929582595825 4.648193359375
Loss :  1.6676924228668213 2.5484778881073 4.216170310974121
Loss :  1.6243586540222168 2.8371875286102295 4.461545944213867
  batch 20 loss: 1.6243586540222168, 2.8371875286102295, 4.461545944213867
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6596183776855469 3.1450998783111572 4.804718017578125
Loss :  1.6756629943847656 2.514220714569092 4.189883708953857
Loss :  1.6466808319091797 3.103541135787964 4.750222206115723
Loss :  1.688105583190918 2.9929091930389404 4.6810150146484375
Loss :  1.685557246208191 3.3139290809631348 4.999486446380615
Loss :  1.6491260528564453 2.786438226699829 4.435564041137695
Loss :  1.6976383924484253 3.1713180541992188 4.868956565856934
Loss :  1.6402711868286133 2.5354180335998535 4.175689220428467
Loss :  1.68816077709198 2.655991315841675 4.344151973724365
Loss :  1.6422553062438965 3.0552682876586914 4.697523593902588
Loss :  1.7232294082641602 3.0818989276885986 4.80512809753418
Loss :  1.669456958770752 2.988987684249878 4.658444404602051
Loss :  1.6547245979309082 2.5440444946289062 4.1987690925598145
Loss :  1.657175898551941 2.6885085105895996 4.34568452835083
Loss :  1.697381615638733 3.1698038578033447 4.867185592651367
Loss :  1.6887092590332031 2.8626699447631836 4.551379203796387
Loss :  1.6668732166290283 3.0892205238342285 4.756093978881836
Loss :  1.6342658996582031 2.683987855911255 4.318253517150879
Loss :  1.660006046295166 2.917139768600464 4.577145576477051
Loss :  1.6525737047195435 3.0535383224487305 4.706111907958984
  batch 40 loss: 1.6525737047195435, 3.0535383224487305, 4.706111907958984
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6605219841003418 2.6685636043548584 4.329085350036621
Loss :  1.6487808227539062 2.583590030670166 4.232370853424072
Loss :  1.666020154953003 2.699192523956299 4.365212440490723
Loss :  1.6645017862319946 2.811854600906372 4.476356506347656
Loss :  1.6670414209365845 2.3835079669952393 4.050549507141113
Loss :  1.6586309671401978 3.0797221660614014 4.738353252410889
Loss :  1.6455074548721313 3.615447759628296 5.260955333709717
Loss :  1.6576279401779175 3.226832389831543 4.88446044921875
Loss :  1.630365014076233 3.459480047225952 5.089845180511475
Loss :  1.681834101676941 3.1268112659454346 4.808645248413086
Loss :  1.646187663078308 2.8302927017211914 4.476480484008789
Loss :  1.6649014949798584 3.0506951808929443 4.715596675872803
Loss :  1.6828124523162842 3.0290915966033936 4.711904048919678
Loss :  1.66813063621521 2.884430408477783 4.552560806274414
Loss :  1.6708264350891113 3.2789039611816406 4.949730396270752
Loss :  1.6357749700546265 2.7915868759155273 4.427361965179443
Loss :  1.6851282119750977 2.8374712467193604 4.522599220275879
Loss :  1.6822251081466675 2.7232871055603027 4.40551233291626
Loss :  1.6957310438156128 3.0597341060638428 4.755465030670166
Loss :  1.6715869903564453 2.795994997024536 4.467581748962402
  batch 60 loss: 1.6715869903564453, 2.795994997024536, 4.467581748962402
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.672683835029602 3.5275022983551025 5.200186252593994
Loss :  1.669630765914917 2.578204393386841 4.247835159301758
Loss :  1.6780445575714111 3.075188159942627 4.753232955932617
Loss :  1.6584126949310303 2.7225379943847656 4.380950927734375
Loss :  1.6553795337677002 2.7342052459716797 4.389584541320801
Loss :  5.5949387550354 4.404244422912598 9.999183654785156
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.487948894500732 4.432397365570068 9.9203462600708
Loss :  5.584117889404297 4.23379373550415 9.817911148071289
Loss :  4.672107219696045 4.13713264465332 8.809240341186523
Total LOSS train 4.551027958209698 valid 9.636670351028442
CE LOSS train 1.6644680078213032 valid 1.1680268049240112
Contrastive LOSS train 2.886559952222384 valid 1.03428316116333
EPOCH 159:
Loss :  1.6514848470687866 2.462822198867798 4.114306926727295
Loss :  1.666267991065979 3.1415421962738037 4.807810306549072
Loss :  1.6525160074234009 2.8583481311798096 4.5108642578125
Loss :  1.6561050415039062 2.568894624710083 4.22499942779541
Loss :  1.6802400350570679 3.370007038116455 5.0502471923828125
Loss :  1.6639147996902466 3.0461630821228027 4.71007776260376
Loss :  1.660857915878296 2.9178221225738525 4.578680038452148
Loss :  1.6505625247955322 2.598407030105591 4.248969554901123
Loss :  1.6551504135131836 2.1700313091278076 3.825181722640991
Loss :  1.609117865562439 2.3443877696990967 3.953505516052246
Loss :  1.6691066026687622 2.9785215854644775 4.647628307342529
Loss :  1.7295751571655273 2.8597004413604736 4.589275360107422
Loss :  1.6755961179733276 3.266787528991699 4.942383766174316
Loss :  1.6644715070724487 3.465296983718872 5.129768371582031
Loss :  1.6434102058410645 2.5942301750183105 4.237640380859375
Loss :  1.6521254777908325 2.9989142417907715 4.6510396003723145
Loss :  1.6614842414855957 2.7450785636901855 4.406562805175781
Loss :  1.6601194143295288 2.881943702697754 4.542063236236572
Loss :  1.667618751525879 2.6277694702148438 4.295388221740723
Loss :  1.6244014501571655 2.800464391708374 4.42486572265625
  batch 20 loss: 1.6244014501571655, 2.800464391708374, 4.42486572265625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6596077680587769 2.977236032485962 4.636843681335449
Loss :  1.6756477355957031 2.596980571746826 4.272628307342529
Loss :  1.6466699838638306 2.9564285278320312 4.603098392486572
Loss :  1.6880428791046143 2.9612350463867188 4.649277687072754
Loss :  1.6854417324066162 3.2654809951782227 4.950922966003418
Loss :  1.6491312980651855 2.7178432941436768 4.366974830627441
Loss :  1.6976186037063599 3.294398784637451 4.9920172691345215
Loss :  1.6402909755706787 2.661245584487915 4.301536560058594
Loss :  1.688175916671753 2.661438226699829 4.349614143371582
Loss :  1.642172932624817 2.7482807636260986 4.390453815460205
Loss :  1.7232002019882202 3.0817816257476807 4.804981708526611
Loss :  1.6693588495254517 2.9350571632385254 4.6044158935546875
Loss :  1.6547441482543945 2.9430060386657715 4.597750186920166
Loss :  1.6571241617202759 2.7261621952056885 4.383286476135254
Loss :  1.69731867313385 2.886200189590454 4.583518981933594
Loss :  1.6887478828430176 3.0138940811157227 4.70264196395874
Loss :  1.6668833494186401 2.629808187484741 4.296691417694092
Loss :  1.6342244148254395 2.7404887676239014 4.374712944030762
Loss :  1.6600394248962402 2.782565116882324 4.4426045417785645
Loss :  1.6525392532348633 2.9525320529937744 4.605071067810059
  batch 40 loss: 1.6525392532348633, 2.9525320529937744, 4.605071067810059
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6605288982391357 2.739150285720825 4.399679183959961
Loss :  1.6487913131713867 2.499931573867798 4.1487226486206055
Loss :  1.6659871339797974 2.787609100341797 4.453596115112305
Loss :  1.6645146608352661 2.936960458755493 4.601475238800049
Loss :  1.6670150756835938 2.713897466659546 4.380912780761719
Loss :  1.65852952003479 2.7490031719207764 4.407532691955566
Loss :  1.6454828977584839 3.114408016204834 4.759891033172607
Loss :  1.6575978994369507 3.2439424991607666 4.901540279388428
Loss :  1.6303436756134033 3.292175531387329 4.922519207000732
Loss :  1.6817395687103271 3.225458860397339 4.907198429107666
Loss :  1.6461870670318604 2.8678085803985596 4.51399564743042
Loss :  1.664864182472229 2.805560350418091 4.470424652099609
Loss :  1.682840347290039 3.0094869136810303 4.692327499389648
Loss :  1.6681042909622192 3.190108060836792 4.858212471008301
Loss :  1.6708093881607056 2.9716861248016357 4.642495632171631
Loss :  1.6357529163360596 2.9223830699920654 4.558135986328125
Loss :  1.6851038932800293 3.2735793590545654 4.958683013916016
Loss :  1.682268500328064 2.82027268409729 4.5025410652160645
Loss :  1.6957415342330933 3.2538552284240723 4.949596881866455
Loss :  1.6715610027313232 2.976215362548828 4.6477766036987305
  batch 60 loss: 1.6715610027313232, 2.976215362548828, 4.6477766036987305
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6727118492126465 3.7830259799957275 5.455738067626953
Loss :  1.6696425676345825 2.4943253993988037 4.163968086242676
Loss :  1.6779683828353882 2.9028730392456055 4.580841541290283
Loss :  1.6583940982818604 2.813891649246216 4.472285747528076
Loss :  1.6553490161895752 2.5990262031555176 4.254375457763672
Loss :  5.583226203918457 4.4079766273498535 9.991203308105469
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.478165149688721 4.29731559753418 9.775480270385742
Loss :  5.573623180389404 4.3269476890563965 9.9005708694458
Loss :  4.666140079498291 4.152642250061035 8.818782806396484
Total LOSS train 4.560073342690101 valid 9.621509313583374
CE LOSS train 1.6644451728233924 valid 1.1665350198745728
Contrastive LOSS train 2.8956281661987306 valid 1.0381605625152588
EPOCH 160:
Loss :  1.651516318321228 2.76956844329834 4.421084880828857
Loss :  1.666292667388916 3.252183437347412 4.918476104736328
Loss :  1.6525551080703735 2.532813310623169 4.185368537902832
Loss :  1.6560742855072021 2.797488212585449 4.4535627365112305
Loss :  1.6802253723144531 3.0695948600769043 4.749820232391357
Loss :  1.6638680696487427 2.882281541824341 4.546149730682373
Loss :  1.6609439849853516 2.884856939315796 4.545801162719727
Loss :  1.650559902191162 2.5863680839538574 4.2369279861450195
Loss :  1.6551671028137207 2.182673692703247 3.8378407955169678
Loss :  1.609165072441101 2.6602654457092285 4.269430637359619
Loss :  1.6691442728042603 3.0242879390716553 4.693432331085205
Loss :  1.729589819908142 3.0656797885894775 4.79526948928833
Loss :  1.6756889820098877 3.1795859336853027 4.8552751541137695
Loss :  1.6645047664642334 3.328019380569458 4.992524147033691
Loss :  1.6433906555175781 2.4445767402648926 4.087967395782471
Loss :  1.6521542072296143 2.9836745262145996 4.635828971862793
Loss :  1.6615208387374878 2.6490330696105957 4.310554027557373
Loss :  1.6602399349212646 3.1776604652404785 4.837900161743164
Loss :  1.6676241159439087 2.7022602558135986 4.369884490966797
Loss :  1.6244359016418457 3.02309513092041 4.647531032562256
  batch 20 loss: 1.6244359016418457, 3.02309513092041, 4.647531032562256
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6596516370773315 2.8292393684387207 4.488891124725342
Loss :  1.6756640672683716 2.6517117023468018 4.327375888824463
Loss :  1.6466413736343384 3.384195566177368 5.030837059020996
Loss :  1.6880725622177124 2.7381701469421387 4.426242828369141
Loss :  1.6854835748672485 3.143643379211426 4.829126834869385
Loss :  1.6491397619247437 2.8280463218688965 4.47718620300293
Loss :  1.697634220123291 3.1548638343811035 4.8524980545043945
Loss :  1.640294075012207 2.5941760540008545 4.234470367431641
Loss :  1.6882195472717285 2.744884729385376 4.433104515075684
Loss :  1.6422488689422607 2.8959033489227295 4.53815221786499
Loss :  1.7232587337493896 3.1078779697418213 4.831136703491211
Loss :  1.669420599937439 3.1031346321105957 4.772555351257324
Loss :  1.6548125743865967 2.632368803024292 4.287181377410889
Loss :  1.6571987867355347 2.859011173248291 4.516210079193115
Loss :  1.697386384010315 2.9740211963653564 4.671407699584961
Loss :  1.6887933015823364 3.063981771469116 4.752775192260742
Loss :  1.6668630838394165 2.9889895915985107 4.655852794647217
Loss :  1.634353518486023 2.933619737625122 4.5679731369018555
Loss :  1.66000497341156 2.780076742172241 4.440081596374512
Loss :  1.6525096893310547 3.0642812252044678 4.716791152954102
  batch 40 loss: 1.6525096893310547, 3.0642812252044678, 4.716791152954102
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6606028079986572 3.0755085945129395 4.736111640930176
Loss :  1.6488902568817139 2.595912218093872 4.244802474975586
Loss :  1.6660034656524658 2.678664207458496 4.344667434692383
Loss :  1.664599895477295 2.710709810256958 4.375309944152832
Loss :  1.6671319007873535 2.20758318901062 3.8747150897979736
Loss :  1.6585999727249146 2.7440130710601807 4.402613162994385
Loss :  1.6455121040344238 3.2444162368774414 4.889928340911865
Loss :  1.657734990119934 3.240619421005249 4.898354530334473
Loss :  1.6303584575653076 3.5948758125305176 5.225234031677246
Loss :  1.6817892789840698 3.340707302093506 5.022496700286865
Loss :  1.646201252937317 2.755016326904297 4.401217460632324
Loss :  1.6649258136749268 2.9937448501586914 4.658670425415039
Loss :  1.6829208135604858 3.145740032196045 4.82866096496582
Loss :  1.6681267023086548 3.1328482627868652 4.8009748458862305
Loss :  1.6708197593688965 3.4197499752044678 5.090569496154785
Loss :  1.6357401609420776 2.874485731124878 4.510225772857666
Loss :  1.6851118803024292 3.027270555496216 4.7123823165893555
Loss :  1.6823137998580933 3.1661322116851807 4.848445892333984
Loss :  1.6956932544708252 3.0172119140625 4.712904930114746
Loss :  1.671477198600769 2.994856357574463 4.6663336753845215
  batch 60 loss: 1.671477198600769, 2.994856357574463, 4.6663336753845215
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6728038787841797 3.4023239612579346 5.075127601623535
Loss :  1.6696895360946655 2.6134791374206543 4.283168792724609
Loss :  1.6780116558074951 2.7573585510253906 4.435370445251465
Loss :  1.6583877801895142 3.149054527282715 4.8074421882629395
Loss :  1.6553694009780884 3.2953310012817383 4.950700283050537
Loss :  5.575389862060547 4.380323886871338 9.955713272094727
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.468072414398193 4.3829851150512695 9.851057052612305
Loss :  5.56592321395874 4.205994129180908 9.771917343139648
Loss :  4.659038066864014 4.221663951873779 8.880702018737793
Total LOSS train 4.600567825023944 valid 9.614847421646118
CE LOSS train 1.664478903550368 valid 1.1647595167160034
Contrastive LOSS train 2.9360888884617733 valid 1.0554159879684448
EPOCH 161:
Loss :  1.6514736413955688 2.731788158416748 4.383261680603027
Loss :  1.666405200958252 3.185607433319092 4.852012634277344
Loss :  1.652572751045227 2.558899164199829 4.211472034454346
Loss :  1.656093716621399 2.8812947273254395 4.537388324737549
Loss :  1.6802786588668823 2.9479353427886963 4.628213882446289
Loss :  1.6638518571853638 2.828052520751953 4.491904258728027
Loss :  1.6609656810760498 2.9260854721069336 4.5870513916015625
Loss :  1.6506009101867676 2.6347501277923584 4.285350799560547
Loss :  1.6551594734191895 2.103154182434082 3.7583136558532715
Loss :  1.6091562509536743 2.628802537918091 4.237958908081055
Loss :  1.669140100479126 2.8939802646636963 4.563120365142822
Loss :  1.7296836376190186 2.9186809062957764 4.648364543914795
Loss :  1.675646185874939 2.897451162338257 4.573097229003906
Loss :  1.6645824909210205 3.2316322326660156 4.896214485168457
Loss :  1.6433093547821045 2.5881216526031494 4.231431007385254
Loss :  1.652197003364563 2.844852924346924 4.497049808502197
Loss :  1.6615476608276367 2.654831886291504 4.316379547119141
Loss :  1.6601831912994385 2.9335641860961914 4.593747138977051
Loss :  1.6676100492477417 2.524684190750122 4.192294120788574
Loss :  1.6244274377822876 2.911841869354248 4.536269187927246
  batch 20 loss: 1.6244274377822876, 2.911841869354248, 4.536269187927246
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6596137285232544 3.1393556594848633 4.798969268798828
Loss :  1.675681710243225 2.648547887802124 4.324229717254639
Loss :  1.6466060876846313 3.135794162750244 4.782400131225586
Loss :  1.688102126121521 3.118185520172119 4.80628776550293
Loss :  1.6854506731033325 3.561720848083496 5.247171401977539
Loss :  1.6490734815597534 2.70393967628479 4.353013038635254
Loss :  1.6976617574691772 3.104763984680176 4.802425861358643
Loss :  1.640275001525879 2.6989779472351074 4.339252948760986
Loss :  1.6882107257843018 2.616443395614624 4.304654121398926
Loss :  1.6422477960586548 2.9393229484558105 4.581570625305176
Loss :  1.7232509851455688 3.285137176513672 5.008388042449951
Loss :  1.6693655252456665 3.0300517082214355 4.6994171142578125
Loss :  1.6548044681549072 2.8323817253112793 4.487186431884766
Loss :  1.6571519374847412 2.772627830505371 4.429780006408691
Loss :  1.697368860244751 3.187512159347534 4.884881019592285
Loss :  1.6887154579162598 3.0069427490234375 4.695658206939697
Loss :  1.6668440103530884 2.956991672515869 4.623835563659668
Loss :  1.6343151330947876 2.922607898712158 4.556922912597656
Loss :  1.6599804162979126 2.9745595455169678 4.63454008102417
Loss :  1.6524734497070312 3.013324022293091 4.665797233581543
  batch 40 loss: 1.6524734497070312, 3.013324022293091, 4.665797233581543
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6606215238571167 2.8267838954925537 4.487405300140381
Loss :  1.648884892463684 2.5367965698242188 4.185681343078613
Loss :  1.665946364402771 2.8087360858917236 4.474682331085205
Loss :  1.664647102355957 2.6697936058044434 4.3344407081604
Loss :  1.6670873165130615 2.667567014694214 4.334654331207275
Loss :  1.658547043800354 2.6881535053253174 4.346700668334961
Loss :  1.645442008972168 3.1365609169006348 4.782002925872803
Loss :  1.6577459573745728 3.3358519077301025 4.993597984313965
Loss :  1.6302748918533325 3.447535514831543 5.077810287475586
Loss :  1.6817989349365234 3.117075204849243 4.7988739013671875
Loss :  1.6461896896362305 2.8442699909210205 4.490459442138672
Loss :  1.6649374961853027 2.6738333702087402 4.338770866394043
Loss :  1.6828874349594116 3.141453981399536 4.824341297149658
Loss :  1.668106198310852 3.1194045543670654 4.787510871887207
Loss :  1.6708074808120728 3.6249337196350098 5.295741081237793
Loss :  1.6358187198638916 2.6671648025512695 4.302983283996582
Loss :  1.6851357221603394 2.8505332469940186 4.535668849945068
Loss :  1.6822746992111206 2.980424404144287 4.662699222564697
Loss :  1.6956822872161865 2.893277645111084 4.588959693908691
Loss :  1.6715024709701538 3.2573864459991455 4.92888879776001
  batch 60 loss: 1.6715024709701538, 3.2573864459991455, 4.92888879776001
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6727616786956787 3.3026881217956543 4.975449562072754
Loss :  1.6696361303329468 2.4781641960144043 4.147800445556641
Loss :  1.6779240369796753 2.7892091274261475 4.467133045196533
Loss :  1.6583985090255737 2.9013631343841553 4.5597615242004395
Loss :  1.6553680896759033 2.710495710372925 4.365863800048828
Loss :  5.583290100097656 4.382976055145264 9.966266632080078
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.477653503417969 4.334413051605225 9.812067031860352
Loss :  5.574312210083008 4.208798408508301 9.783110618591309
Loss :  4.662749767303467 4.027731895446777 8.690481185913086
Total LOSS train 4.57131012403048 valid 9.562981367111206
CE LOSS train 1.664469311787532 valid 1.1656874418258667
Contrastive LOSS train 2.906840863594642 valid 1.0069329738616943
EPOCH 162:
Loss :  1.6514350175857544 2.7473795413970947 4.398814678192139
Loss :  1.666333794593811 3.29608416557312 4.962418079376221
Loss :  1.6525452136993408 2.6267993450164795 4.27934455871582
Loss :  1.6561298370361328 3.0336709022521973 4.68980073928833
Loss :  1.6801824569702148 3.0617246627807617 4.741907119750977
Loss :  1.6638989448547363 2.874638557434082 4.538537502288818
Loss :  1.6608725786209106 3.0312626361846924 4.692135334014893
Loss :  1.6505810022354126 2.43780779838562 4.088388919830322
Loss :  1.655147671699524 2.209247589111328 3.8643951416015625
Loss :  1.6091413497924805 2.7644426822662354 4.373583793640137
Loss :  1.6691068410873413 3.0769741535186768 4.7460808753967285
Loss :  1.7296069860458374 2.7043442726135254 4.433951377868652
Loss :  1.6756579875946045 3.2819173336029053 4.95757532119751
Loss :  1.6645243167877197 3.230109691619873 4.894634246826172
Loss :  1.6432517766952515 2.5491955280303955 4.192447185516357
Loss :  1.6521425247192383 2.732116460800171 4.384259223937988
Loss :  1.661531925201416 2.540825128555298 4.202357292175293
Loss :  1.6601492166519165 3.209218740463257 4.869368076324463
Loss :  1.6676565408706665 2.444295644760132 4.111952304840088
Loss :  1.624407172203064 3.145427703857422 4.769834995269775
  batch 20 loss: 1.624407172203064, 3.145427703857422, 4.769834995269775
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.659546971321106 3.0150399208068848 4.674586772918701
Loss :  1.6756514310836792 2.3801422119140625 4.055793762207031
Loss :  1.646541953086853 2.9490694999694824 4.595611572265625
Loss :  1.6880861520767212 2.630889415740967 4.318975448608398
Loss :  1.6854239702224731 3.4223203659057617 5.107744216918945
Loss :  1.6489802598953247 2.8878886699676514 4.536869049072266
Loss :  1.6976525783538818 3.224534749984741 4.922187328338623
Loss :  1.640271782875061 2.864464044570923 4.504735946655273
Loss :  1.688212275505066 2.7282822132110596 4.416494369506836
Loss :  1.6421291828155518 2.701392650604248 4.343522071838379
Loss :  1.7232418060302734 3.136171817779541 4.8594136238098145
Loss :  1.6693689823150635 3.4550163745880127 5.124385356903076
Loss :  1.6548017263412476 2.8844809532165527 4.53928279876709
Loss :  1.6571022272109985 2.658230781555176 4.315332889556885
Loss :  1.6973369121551514 3.3013105392456055 4.998647689819336
Loss :  1.6887580156326294 3.0679285526275635 4.756686687469482
Loss :  1.6668195724487305 3.1223556995391846 4.789175033569336
Loss :  1.6343722343444824 2.925231456756592 4.559603691101074
Loss :  1.6599608659744263 2.7477991580963135 4.407760143280029
Loss :  1.652444839477539 3.2861626148223877 4.938607215881348
  batch 40 loss: 1.652444839477539, 3.2861626148223877, 4.938607215881348
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.660605788230896 2.9519646167755127 4.612570285797119
Loss :  1.6489031314849854 2.7860074043273926 4.434910774230957
Loss :  1.6659730672836304 2.722062349319458 4.388035297393799
Loss :  1.6645888090133667 2.9076292514801025 4.57221794128418
Loss :  1.667075276374817 2.843775749206543 4.51085090637207
Loss :  1.6584683656692505 2.995455503463745 4.653923988342285
Loss :  1.6454659700393677 3.071488857269287 4.716954708099365
Loss :  1.657727599143982 3.2243120670318604 4.882039546966553
Loss :  1.6302821636199951 3.481455087661743 5.111737251281738
Loss :  1.6818426847457886 3.3513405323028564 5.0331830978393555
Loss :  1.6461737155914307 2.7118141651153564 4.357987880706787
Loss :  1.6649022102355957 2.689819097518921 4.3547210693359375
Loss :  1.6828898191452026 3.296227216720581 4.979116916656494
Loss :  1.6680890321731567 2.9891750812530518 4.657264232635498
Loss :  1.6707842350006104 3.517359733581543 5.188143730163574
Loss :  1.6357370615005493 3.1501567363739014 4.78589391708374
Loss :  1.6850783824920654 3.257629156112671 4.942707538604736
Loss :  1.6822975873947144 3.0107686519622803 4.693066120147705
Loss :  1.6956530809402466 3.1474812030792236 4.84313440322876
Loss :  1.6714411973953247 3.1675057411193848 4.83894681930542
  batch 60 loss: 1.6714411973953247, 3.1675057411193848, 4.83894681930542
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6727194786071777 3.236525774002075 4.909245491027832
Loss :  1.669611930847168 2.731137752532959 4.400749683380127
Loss :  1.67784583568573 2.9156453609466553 4.593491077423096
Loss :  1.6583117246627808 3.0160164833068848 4.674328327178955
Loss :  1.6553282737731934 2.8838863372802734 4.539214611053467
Loss :  5.57658052444458 4.376251697540283 9.952832221984863
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.471700191497803 4.33616304397583 9.807863235473633
Loss :  5.569655418395996 4.3018879890441895 9.871543884277344
Loss :  4.659167766571045 4.2648420333862305 8.924009323120117
Total LOSS train 4.625102123847374 valid 9.63906216621399
CE LOSS train 1.6644431279255794 valid 1.1647919416427612
Contrastive LOSS train 2.96065898675185 valid 1.0662105083465576
EPOCH 163:
Loss :  1.6513537168502808 2.5972769260406494 4.248630523681641
Loss :  1.6662794351577759 3.3801429271698 5.046422481536865
Loss :  1.6525236368179321 2.695919990539551 4.348443508148193
Loss :  1.656077265739441 2.5961833000183105 4.252260684967041
Loss :  1.680163025856018 2.885488748550415 4.565651893615723
Loss :  1.6638659238815308 3.0341615676879883 4.698027610778809
Loss :  1.6609059572219849 3.4327871799468994 5.093693256378174
Loss :  1.6505436897277832 2.5511186122894287 4.201662063598633
Loss :  1.655121922492981 2.1874947547912598 3.842616558074951
Loss :  1.6091177463531494 2.5339951515197754 4.143113136291504
Loss :  1.6691099405288696 2.8897178173065186 4.558827877044678
Loss :  1.7295854091644287 2.919528007507324 4.649113655090332
Loss :  1.6756576299667358 3.127737522125244 4.8033952713012695
Loss :  1.6645617485046387 3.1105306148529053 4.775092124938965
Loss :  1.643187403678894 2.696019172668457 4.339206695556641
Loss :  1.6521779298782349 3.2521398067474365 4.904317855834961
Loss :  1.6615047454833984 2.690419912338257 4.351924896240234
Loss :  1.660116195678711 3.0408098697662354 4.700925827026367
Loss :  1.6676275730133057 2.7168285846710205 4.384456157684326
Loss :  1.6243926286697388 2.9933676719665527 4.617760181427002
  batch 20 loss: 1.6243926286697388, 2.9933676719665527, 4.617760181427002
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6595598459243774 3.2176496982574463 4.877209663391113
Loss :  1.6756092309951782 2.764012575149536 4.439621925354004
Loss :  1.6465628147125244 3.1190903186798096 4.765653133392334
Loss :  1.6880121231079102 2.931438446044922 4.619450569152832
Loss :  1.6853896379470825 3.4851176738739014 5.170507431030273
Loss :  1.6490421295166016 2.6344428062438965 4.283484935760498
Loss :  1.6976171731948853 3.13185453414917 4.829471588134766
Loss :  1.6402701139450073 2.9412922859191895 4.581562519073486
Loss :  1.688199758529663 3.158883810043335 4.847083568572998
Loss :  1.6421481370925903 2.8253273963928223 4.467475414276123
Loss :  1.723218321800232 3.007420778274536 4.7306389808654785
Loss :  1.6692930459976196 3.3767950534820557 5.046088218688965
Loss :  1.6547945737838745 2.659846067428589 4.314640522003174
Loss :  1.6571078300476074 2.6395959854125977 4.296703815460205
Loss :  1.6973496675491333 3.34201717376709 5.039366722106934
Loss :  1.688716173171997 2.9454455375671387 4.634161949157715
Loss :  1.666808843612671 2.9473607540130615 4.614169597625732
Loss :  1.6343168020248413 2.8927266597747803 4.527043342590332
Loss :  1.6599708795547485 2.8769237995147705 4.536894798278809
Loss :  1.652490258216858 3.1378366947174072 4.790327072143555
  batch 40 loss: 1.652490258216858, 3.1378366947174072, 4.790327072143555
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6606131792068481 3.071765422821045 4.7323784828186035
Loss :  1.6488837003707886 2.713824987411499 4.362708568572998
Loss :  1.665958285331726 2.700737714767456 4.366695880889893
Loss :  1.6645500659942627 2.831972599029541 4.496522903442383
Loss :  1.6670844554901123 2.6609740257263184 4.328058242797852
Loss :  1.658514380455017 2.8732314109802246 4.531745910644531
Loss :  1.6454529762268066 3.2603189945220947 4.9057722091674805
Loss :  1.6577577590942383 3.1258609294891357 4.783618927001953
Loss :  1.6303120851516724 3.5316858291625977 5.1619977951049805
Loss :  1.6818358898162842 3.2308456897735596 4.912681579589844
Loss :  1.646256685256958 2.8396334648132324 4.4858903884887695
Loss :  1.6649010181427002 2.963312864303589 4.628213882446289
Loss :  1.6828980445861816 3.0501949787139893 4.73309326171875
Loss :  1.668093204498291 3.185041904449463 4.853135108947754
Loss :  1.6708036661148071 3.622530698776245 5.293334484100342
Loss :  1.6357444524765015 2.8252062797546387 4.46095085144043
Loss :  1.6850959062576294 2.91318416595459 4.59827995300293
Loss :  1.6823009252548218 3.2644309997558594 4.946732044219971
Loss :  1.6956943273544312 3.3170294761657715 5.012723922729492
Loss :  1.6714229583740234 3.1364896297454834 4.807912826538086
  batch 60 loss: 1.6714229583740234, 3.1364896297454834, 4.807912826538086
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.672711968421936 3.46956467628479 5.142276763916016
Loss :  1.6695985794067383 2.6648850440979004 4.334483623504639
Loss :  1.677876591682434 3.2652230262756348 4.943099498748779
Loss :  1.6582911014556885 2.881089687347412 4.53938102722168
Loss :  1.655318021774292 3.2918405532836914 4.9471588134765625
Loss :  5.565712928771973 4.38585901260376 9.95157241821289
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.458409309387207 4.377657890319824 9.836067199707031
Loss :  5.5570173263549805 4.2476019859313965 9.804618835449219
Loss :  4.646390914916992 4.074766159057617 8.72115707397461
Total LOSS train 4.6499376150277945 valid 9.578353881835938
CE LOSS train 1.6644357094397912 valid 1.161597728729248
Contrastive LOSS train 2.9855018652402436 valid 1.0186915397644043
EPOCH 164:
Loss :  1.6513240337371826 2.92291259765625 4.574236869812012
Loss :  1.6663317680358887 3.305687427520752 4.972019195556641
Loss :  1.652531623840332 2.4969141483306885 4.149445533752441
Loss :  1.6560168266296387 3.1033401489257812 4.75935697555542
Loss :  1.6801927089691162 3.067859649658203 4.748052597045898
Loss :  1.6637719869613647 2.8919105529785156 4.55568265914917
Loss :  1.660939335823059 3.2866508960723877 4.947590351104736
Loss :  1.6505358219146729 2.606722831726074 4.257258415222168
Loss :  1.6550743579864502 2.221666097640991 3.8767404556274414
Loss :  1.609093189239502 2.470951795578003 4.080044746398926
Loss :  1.6691032648086548 3.17250919342041 4.841612339019775
Loss :  1.7296000719070435 3.2484242916107178 4.978024482727051
Loss :  1.675764799118042 3.02280330657959 4.698568344116211
Loss :  1.6645153760910034 3.3104913234710693 4.975006580352783
Loss :  1.643196702003479 2.5701327323913574 4.213329315185547
Loss :  1.6521471738815308 3.171583414077759 4.82373046875
Loss :  1.661496877670288 2.7937352657318115 4.4552321434021
Loss :  1.660126805305481 2.97957444190979 4.6397013664245605
Loss :  1.6676486730575562 3.051774501800537 4.719423294067383
Loss :  1.624335765838623 2.9183576107025146 4.542693138122559
  batch 20 loss: 1.624335765838623, 2.9183576107025146, 4.542693138122559
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6595734357833862 3.2948005199432373 4.954373836517334
Loss :  1.6755753755569458 2.680006504058838 4.355581760406494
Loss :  1.646523118019104 3.4081273078918457 5.05465030670166
Loss :  1.6880077123641968 2.9663467407226562 4.654354572296143
Loss :  1.685344934463501 3.6953330039978027 5.380678176879883
Loss :  1.6490052938461304 2.8535261154174805 4.5025315284729
Loss :  1.6975994110107422 3.1435306072235107 4.841130256652832
Loss :  1.6402562856674194 2.8637571334838867 4.504013538360596
Loss :  1.6881674528121948 2.7149620056152344 4.403129577636719
Loss :  1.6420872211456299 2.8264379501342773 4.468524932861328
Loss :  1.7232002019882202 3.060086488723755 4.7832865715026855
Loss :  1.6692817211151123 2.955723285675049 4.625004768371582
Loss :  1.6547677516937256 2.983635663986206 4.638403415679932
Loss :  1.6570590734481812 2.648391008377075 4.305449962615967
Loss :  1.6972832679748535 2.999229907989502 4.6965131759643555
Loss :  1.6887348890304565 2.6267924308776855 4.315527439117432
Loss :  1.6667622327804565 3.03513240814209 4.701894760131836
Loss :  1.634301781654358 3.1830129623413086 4.817314624786377
Loss :  1.6599270105361938 2.716613292694092 4.376540184020996
Loss :  1.6524145603179932 3.171093702316284 4.823508262634277
  batch 40 loss: 1.6524145603179932, 3.171093702316284, 4.823508262634277
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.660609245300293 2.868670701980591 4.529279708862305
Loss :  1.6488673686981201 2.417527675628662 4.066394805908203
Loss :  1.6659448146820068 2.7276461124420166 4.393590927124023
Loss :  1.664472222328186 2.753185272216797 4.417657375335693
Loss :  1.667087197303772 2.682561159133911 4.349648475646973
Loss :  1.6584700345993042 2.829425096511841 4.4878950119018555
Loss :  1.645411729812622 2.9659173488616943 4.611329078674316
Loss :  1.657715082168579 2.9554543495178223 4.6131696701049805
Loss :  1.6302518844604492 3.780876636505127 5.411128520965576
Loss :  1.681803584098816 3.2767608165740967 4.958564281463623
Loss :  1.6462386846542358 2.8245584964752197 4.470797061920166
Loss :  1.6648911237716675 3.3286523818969727 4.99354362487793
Loss :  1.6828840970993042 2.942984104156494 4.625868320465088
Loss :  1.6680549383163452 3.332897186279297 5.000952243804932
Loss :  1.6707223653793335 3.492246150970459 5.162968635559082
Loss :  1.6357113122940063 2.7366631031036377 4.372374534606934
Loss :  1.68505859375 3.0030081272125244 4.688066482543945
Loss :  1.6822808980941772 3.075599193572998 4.757880210876465
Loss :  1.6956729888916016 3.130859136581421 4.826532363891602
Loss :  1.6714187860488892 2.8861947059631348 4.557613372802734
  batch 60 loss: 1.6714187860488892, 2.8861947059631348, 4.557613372802734
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726456880569458 3.438688039779663 5.111333847045898
Loss :  1.6696215867996216 2.563481092453003 4.233102798461914
Loss :  1.67781662940979 2.9387316703796387 4.616548538208008
Loss :  1.6582216024398804 3.0797767639160156 4.7379984855651855
Loss :  1.655231237411499 3.1038808822631836 4.759112358093262
Loss :  5.56818151473999 4.424330234527588 9.992511749267578
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.463525772094727 4.345682621002197 9.809207916259766
Loss :  5.558743000030518 4.255433559417725 9.814176559448242
Loss :  4.645933151245117 4.2533135414123535 8.899246215820312
Total LOSS train 4.642515563964844 valid 9.628785610198975
CE LOSS train 1.6644111321522639 valid 1.1614832878112793
Contrastive LOSS train 2.9781044226426343 valid 1.0633283853530884
EPOCH 165:
Loss :  1.6513358354568481 2.8264009952545166 4.477736949920654
Loss :  1.666267991065979 3.292546033859253 4.9588141441345215
Loss :  1.6524996757507324 2.969531774520874 4.622031211853027
Loss :  1.6560250520706177 2.8936524391174316 4.54967737197876
Loss :  1.6801624298095703 3.153414011001587 4.833576202392578
Loss :  1.663838505744934 2.7692949771881104 4.433133602142334
Loss :  1.6608999967575073 3.2543890476226807 4.915288925170898
Loss :  1.6504881381988525 2.670555830001831 4.321043968200684
Loss :  1.6550304889678955 2.3119919300079346 3.96702241897583
Loss :  1.6090387105941772 2.600602626800537 4.209641456604004
Loss :  1.6690642833709717 3.2168166637420654 4.885880947113037
Loss :  1.7296202182769775 3.163525342941284 4.893145561218262
Loss :  1.6758238077163696 3.219726324081421 4.89555025100708
Loss :  1.6644678115844727 3.2047219276428223 4.869189739227295
Loss :  1.6431291103363037 2.858635425567627 4.501764297485352
Loss :  1.652122139930725 2.9825246334075928 4.634646892547607
Loss :  1.6614876985549927 2.781337261199951 4.442824840545654
Loss :  1.6601752042770386 3.0463249683380127 4.706500053405762
Loss :  1.667693853378296 2.748722791671753 4.416416645050049
Loss :  1.6243900060653687 3.110605478286743 4.734995365142822
  batch 20 loss: 1.6243900060653687, 3.110605478286743, 4.734995365142822
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6595364809036255 3.2980728149414062 4.957609176635742
Loss :  1.675589919090271 2.658686876296997 4.3342766761779785
Loss :  1.6465158462524414 3.2941830158233643 4.940698623657227
Loss :  1.6880191564559937 3.1214358806610107 4.809454917907715
Loss :  1.6854021549224854 3.550920009613037 5.236322402954102
Loss :  1.6489551067352295 2.9024627208709717 4.551417827606201
Loss :  1.6976152658462524 3.5205442905426025 5.2181596755981445
Loss :  1.6402511596679688 3.146169424057007 4.786420822143555
Loss :  1.6881651878356934 2.5997414588928223 4.287906646728516
Loss :  1.6421211957931519 2.9347879886627197 4.576909065246582
Loss :  1.7231868505477905 3.1461684703826904 4.869355201721191
Loss :  1.6693626642227173 2.966104745864868 4.635467529296875
Loss :  1.6547565460205078 3.08089280128479 4.735649108886719
Loss :  1.6570528745651245 2.6563451290130615 4.3133978843688965
Loss :  1.6973519325256348 3.393531084060669 5.090883255004883
Loss :  1.6887595653533936 2.866511583328247 4.555271148681641
Loss :  1.6667757034301758 3.0414559841156006 4.7082319259643555
Loss :  1.634325623512268 2.7407853603363037 4.375111103057861
Loss :  1.659954309463501 2.6989221572875977 4.3588762283325195
Loss :  1.6524728536605835 3.0453603267669678 4.697833061218262
  batch 40 loss: 1.6524728536605835, 3.0453603267669678, 4.697833061218262
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6606078147888184 2.8392248153686523 4.499832630157471
Loss :  1.6488323211669922 3.263620138168335 4.912452697753906
Loss :  1.6659212112426758 3.056025266647339 4.721946716308594
Loss :  1.6644471883773804 2.608393430709839 4.27284049987793
Loss :  1.6670421361923218 2.791626214981079 4.458668231964111
Loss :  1.658455491065979 3.2776830196380615 4.93613862991333
Loss :  1.6454060077667236 3.3414769172668457 4.986883163452148
Loss :  1.6576625108718872 3.664581298828125 5.322243690490723
Loss :  1.6302716732025146 3.442338228225708 5.072609901428223
Loss :  1.6818238496780396 3.241232395172119 4.923056125640869
Loss :  1.6462010145187378 2.6683623790740967 4.314563274383545
Loss :  1.6648859977722168 2.92095947265625 4.585845470428467
Loss :  1.6828734874725342 3.1624958515167236 4.845369338989258
Loss :  1.668049931526184 3.236341714859009 4.904391765594482
Loss :  1.6707121133804321 3.484983205795288 5.15569543838501
Loss :  1.6356384754180908 3.140169620513916 4.775808334350586
Loss :  1.6850016117095947 3.0763137340545654 4.76131534576416
Loss :  1.6822962760925293 3.143646240234375 4.825942516326904
Loss :  1.6956639289855957 3.101524591445923 4.797188758850098
Loss :  1.67132568359375 3.342149496078491 5.01347541809082
  batch 60 loss: 1.67132568359375, 3.342149496078491, 5.01347541809082
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726881265640259 3.704364538192749 5.3770527839660645
Loss :  1.669671893119812 2.6593518257141113 4.329023838043213
Loss :  1.6779059171676636 3.015026807785034 4.692932605743408
Loss :  1.6582660675048828 3.067246437072754 4.725512504577637
Loss :  1.6552975177764893 3.1976068019866943 4.852904319763184
Loss :  5.562148094177246 4.394270896911621 9.956418991088867
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.453454494476318 4.364219665527344 9.81767463684082
Loss :  5.553468704223633 4.223453998565674 9.776922225952148
Loss :  4.63701868057251 4.2990827560424805 8.936101913452148
Total LOSS train 4.713412725008451 valid 9.621779441833496
CE LOSS train 1.6644104554102972 valid 1.1592546701431274
Contrastive LOSS train 3.0490022622621975 valid 1.0747706890106201
EPOCH 166:
Loss :  1.6513304710388184 2.4895474910736084 4.140877723693848
Loss :  1.6663098335266113 3.3698461055755615 5.036155700683594
Loss :  1.652490258216858 2.5919528007507324 4.244442939758301
Loss :  1.6559810638427734 2.671056032180786 4.3270368576049805
Loss :  1.6801985502243042 3.15035343170166 4.830552101135254
Loss :  1.6637675762176514 2.8700952529907227 4.533863067626953
Loss :  1.660911202430725 3.2746567726135254 4.935567855834961
Loss :  1.6504716873168945 2.5375771522521973 4.188048839569092
Loss :  1.6550517082214355 2.3011717796325684 3.956223487854004
Loss :  1.6091054677963257 2.5133237838745117 4.122429370880127
Loss :  1.6690778732299805 3.195854663848877 4.864932537078857
Loss :  1.7296181917190552 3.0463571548461914 4.775975227355957
Loss :  1.67600417137146 3.015441656112671 4.691445827484131
Loss :  1.6645642518997192 2.9801430702209473 4.644707202911377
Loss :  1.6430541276931763 2.7234420776367188 4.3664960861206055
Loss :  1.6522241830825806 3.0825343132019043 4.734758377075195
Loss :  1.6614910364151 2.865238666534424 4.526729583740234
Loss :  1.6602017879486084 3.049119710922241 4.70932149887085
Loss :  1.6676969528198242 2.678853750228882 4.346550941467285
Loss :  1.6243401765823364 2.9265928268432617 4.550932884216309
  batch 20 loss: 1.6243401765823364, 2.9265928268432617, 4.550932884216309
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6594964265823364 3.2763724327087402 4.935868740081787
Loss :  1.6755692958831787 2.609135627746582 4.28470516204834
Loss :  1.6465258598327637 2.9854471683502197 4.6319732666015625
Loss :  1.6879104375839233 3.1484062671661377 4.8363165855407715
Loss :  1.6854053735733032 3.5854177474975586 5.270823001861572
Loss :  1.6489500999450684 2.872516632080078 4.5214667320251465
Loss :  1.6976062059402466 3.299654245376587 4.997260570526123
Loss :  1.6402326822280884 2.811596155166626 4.451828956604004
Loss :  1.6881828308105469 2.9077095985412598 4.595892429351807
Loss :  1.642048716545105 2.8417019844055176 4.483750820159912
Loss :  1.723219633102417 3.3137800693511963 5.036999702453613
Loss :  1.6693731546401978 3.4186947345733643 5.088068008422852
Loss :  1.6547884941101074 2.963451862335205 4.6182403564453125
Loss :  1.6570708751678467 2.87451171875 4.531582832336426
Loss :  1.6974457502365112 3.2231032848358154 4.920548915863037
Loss :  1.6888017654418945 3.1490888595581055 4.837890625
Loss :  1.6667537689208984 3.122709035873413 4.789463043212891
Loss :  1.6343637704849243 2.818488121032715 4.45285177230835
Loss :  1.6599671840667725 2.6582870483398438 4.318254470825195
Loss :  1.652457594871521 3.1478285789489746 4.800286293029785
  batch 40 loss: 1.652457594871521, 3.1478285789489746, 4.800286293029785
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6605976819992065 3.276423215866089 4.937020778656006
Loss :  1.6488350629806519 2.7135322093963623 4.362367153167725
Loss :  1.6658962965011597 3.2410659790039062 4.9069623947143555
Loss :  1.6643985509872437 2.6750073432922363 4.3394060134887695
Loss :  1.667040228843689 2.6705031394958496 4.337543487548828
Loss :  1.658467173576355 2.958857297897339 4.617324352264404
Loss :  1.6454546451568604 3.1675376892089844 4.812992095947266
Loss :  1.6576647758483887 3.002464532852173 4.660129547119141
Loss :  1.6302529573440552 3.299109697341919 4.929362773895264
Loss :  1.6817984580993652 3.04358172416687 4.725379943847656
Loss :  1.646198034286499 2.7700281143188477 4.416226387023926
Loss :  1.664881706237793 3.1518194675445557 4.8167009353637695
Loss :  1.6828889846801758 3.2954306602478027 4.9783196449279785
Loss :  1.6680412292480469 3.263512134552002 4.931553363800049
Loss :  1.6707466840744019 3.522221803665161 5.192968368530273
Loss :  1.6357001066207886 3.0119028091430664 4.6476030349731445
Loss :  1.6850115060806274 3.032200574874878 4.717212200164795
Loss :  1.6822668313980103 3.280392646789551 4.9626593589782715
Loss :  1.6957184076309204 3.234764814376831 4.930483341217041
Loss :  1.671359896659851 2.9431002140045166 4.614459991455078
  batch 60 loss: 1.671359896659851, 2.9431002140045166, 4.614459991455078
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726772785186768 3.282956838607788 4.955634117126465
Loss :  1.6696690320968628 2.575913190841675 4.245582103729248
Loss :  1.677809715270996 3.0273079872131348 4.705117702484131
Loss :  1.6582682132720947 3.021665334701538 4.679933547973633
Loss :  1.6552255153656006 3.2080676555633545 4.863293170928955
Loss :  5.561873435974121 4.345642566680908 9.907516479492188
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.455570697784424 4.3720269203186035 9.827597618103027
Loss :  5.554897785186768 4.247005462646484 9.801902770996094
Loss :  4.638364315032959 4.2096052169799805 8.847969055175781
Total LOSS train 4.66488240315364 valid 9.596246480941772
CE LOSS train 1.6644142994513877 valid 1.1595910787582397
Contrastive LOSS train 3.000468103702252 valid 1.0524013042449951
EPOCH 167:
Loss :  1.6513009071350098 2.6949946880340576 4.346295356750488
Loss :  1.666292667388916 3.2794480323791504 4.945740699768066
Loss :  1.6524628400802612 3.307020902633667 4.959483623504639
Loss :  1.6559028625488281 3.1284191608428955 4.7843217849731445
Loss :  1.6801515817642212 3.27340030670166 4.953551769256592
Loss :  1.6637221574783325 2.728132486343384 4.391854763031006
Loss :  1.6609243154525757 3.3860108852386475 5.046935081481934
Loss :  1.65044105052948 2.372447967529297 4.022889137268066
Loss :  1.6550301313400269 2.2561469078063965 3.911177158355713
Loss :  1.6091104745864868 2.7276928424835205 4.336803436279297
Loss :  1.6690725088119507 3.153327465057373 4.822400093078613
Loss :  1.7295252084732056 2.9069125652313232 4.636437892913818
Loss :  1.6759673357009888 3.2736289501190186 4.949596405029297
Loss :  1.6645121574401855 3.180798292160034 4.845310211181641
Loss :  1.6430656909942627 2.9165008068084717 4.559566497802734
Loss :  1.65215265750885 3.070648193359375 4.7228007316589355
Loss :  1.6614805459976196 2.7524971961975098 4.41397762298584
Loss :  1.6601474285125732 2.8222391605377197 4.482386589050293
Loss :  1.6676478385925293 3.0522191524505615 4.719866752624512
Loss :  1.6243617534637451 2.9260287284851074 4.550390243530273
  batch 20 loss: 1.6243617534637451, 2.9260287284851074, 4.550390243530273
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.659533143043518 3.116285562515259 4.775818824768066
Loss :  1.675561785697937 2.7064154148101807 4.381977081298828
Loss :  1.6465016603469849 3.3677420616149902 5.0142436027526855
Loss :  1.6879374980926514 3.101306438446045 4.789243698120117
Loss :  1.6853547096252441 3.3466598987579346 5.032014846801758
Loss :  1.6489301919937134 3.016211986541748 4.665142059326172
Loss :  1.697635531425476 3.167189359664917 4.8648247718811035
Loss :  1.640223741531372 2.8892745971679688 4.529498100280762
Loss :  1.688173532485962 2.8386473655700684 4.526821136474609
Loss :  1.642037272453308 2.6854186058044434 4.327455997467041
Loss :  1.7232162952423096 3.271639347076416 4.994855880737305
Loss :  1.669372797012329 3.2374141216278076 4.906786918640137
Loss :  1.6547455787658691 3.00285005569458 4.657595634460449
Loss :  1.6570342779159546 2.782766580581665 4.43980073928833
Loss :  1.6973588466644287 3.593945026397705 5.291303634643555
Loss :  1.6887965202331543 3.006608009338379 4.695404529571533
Loss :  1.6667557954788208 3.1799516677856445 4.846707344055176
Loss :  1.6343499422073364 2.6774444580078125 4.311794281005859
Loss :  1.6599252223968506 2.753758430480957 4.413683891296387
Loss :  1.6524441242218018 3.3029630184173584 4.95540714263916
  batch 40 loss: 1.6524441242218018, 3.3029630184173584, 4.95540714263916
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6605356931686401 3.0328855514526367 4.693421363830566
Loss :  1.6487681865692139 2.5235211849212646 4.1722893714904785
Loss :  1.6658737659454346 3.128504514694214 4.794378280639648
Loss :  1.6642385721206665 3.0423147678375244 4.7065534591674805
Loss :  1.666982889175415 3.105654001235962 4.772636890411377
Loss :  1.6584734916687012 3.2296807765960693 4.888154029846191
Loss :  1.6453999280929565 3.164191961288452 4.809591770172119
Loss :  1.6576159000396729 3.5819332599639893 5.239549160003662
Loss :  1.6302458047866821 3.603081703186035 5.233327388763428
Loss :  1.6817519664764404 3.398371696472168 5.0801239013671875
Loss :  1.6461782455444336 2.529337167739868 4.175515174865723
Loss :  1.6648414134979248 3.028186559677124 4.693027973175049
Loss :  1.6828956604003906 3.141602039337158 4.824497699737549
Loss :  1.6679662466049194 3.3715622425079346 5.0395283699035645
Loss :  1.6707205772399902 3.529191493988037 5.199912071228027
Loss :  1.6356415748596191 2.9632976055145264 4.598938941955566
Loss :  1.6850509643554688 3.074967384338379 4.760018348693848
Loss :  1.6822726726531982 3.057957172393799 4.740229606628418
Loss :  1.6957194805145264 3.2587637901306152 4.9544830322265625
Loss :  1.6713119745254517 3.249508857727051 4.920820713043213
  batch 60 loss: 1.6713119745254517, 3.249508857727051, 4.920820713043213
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726772785186768 3.3796944618225098 5.052371978759766
Loss :  1.6696020364761353 2.709622859954834 4.37922477722168
Loss :  1.6778337955474854 2.8805618286132812 4.5583953857421875
Loss :  1.6582609415054321 3.1849770545959473 4.84323787689209
Loss :  1.6552677154541016 3.056206703186035 4.711474418640137
Loss :  5.553801536560059 4.356202125549316 9.910003662109375
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.448782920837402 4.411626815795898 9.8604097366333
Loss :  5.543528079986572 4.291314601898193 9.834842681884766
Loss :  4.6317901611328125 4.274407386779785 8.906197547912598
Total LOSS train 4.717905660775991 valid 9.62786340713501
CE LOSS train 1.6643890362519485 valid 1.1579475402832031
Contrastive LOSS train 3.0535166667057916 valid 1.0686018466949463
EPOCH 168:
Loss :  1.6512141227722168 2.4348371028900146 4.086050987243652
Loss :  1.6663013696670532 3.3821001052856445 5.048401355743408
Loss :  1.6524145603179932 2.99861478805542 4.651029586791992
Loss :  1.6558036804199219 2.6151344776153564 4.270937919616699
Loss :  1.680163860321045 2.949917793273926 4.630081653594971
Loss :  1.6636868715286255 2.8600234985351562 4.523710250854492
Loss :  1.660965919494629 3.363347291946411 5.024312973022461
Loss :  1.6504013538360596 2.456888437271118 4.107289791107178
Loss :  1.6549845933914185 2.5247442722320557 4.179728984832764
Loss :  1.6090800762176514 2.432931423187256 4.042011260986328
Loss :  1.669062614440918 2.8773748874664307 4.5464372634887695
Loss :  1.7294678688049316 3.1690189838409424 4.898487091064453
Loss :  1.6761592626571655 3.146587610244751 4.822746753692627
Loss :  1.6644823551177979 3.310192823410034 4.974675178527832
Loss :  1.6430516242980957 3.2149131298065186 4.857964515686035
Loss :  1.6521382331848145 3.4104678630828857 5.062605857849121
Loss :  1.661434292793274 3.184666395187378 4.846100807189941
Loss :  1.6601089239120483 3.278809070587158 4.938918113708496
Loss :  1.667655348777771 2.696387767791748 4.364043235778809
Loss :  1.624302625656128 3.2719240188598633 4.89622688293457
  batch 20 loss: 1.624302625656128, 3.2719240188598633, 4.89622688293457
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6595109701156616 3.2089855670928955 4.868496417999268
Loss :  1.6754839420318604 2.7249386310577393 4.4004225730896
Loss :  1.6465176343917847 3.098768711090088 4.745286464691162
Loss :  1.687854290008545 2.97745943069458 4.665313720703125
Loss :  1.685322642326355 3.6282856464385986 5.313608169555664
Loss :  1.6489397287368774 2.9348998069763184 4.583839416503906
Loss :  1.6976547241210938 3.131964683532715 4.829619407653809
Loss :  1.6402088403701782 3.0117499828338623 4.65195894241333
Loss :  1.6882095336914062 3.023108720779419 4.711318016052246
Loss :  1.6419492959976196 3.0835485458374023 4.725497722625732
Loss :  1.7232192754745483 3.241757869720459 4.964977264404297
Loss :  1.6693494319915771 3.335108995437622 5.004458427429199
Loss :  1.654773235321045 3.295513153076172 4.950286388397217
Loss :  1.6570091247558594 2.7722904682159424 4.429299354553223
Loss :  1.6973189115524292 3.2706401348114014 4.967958927154541
Loss :  1.688830018043518 2.773390531539917 4.462220668792725
Loss :  1.6667298078536987 3.3559956550598145 5.022725582122803
Loss :  1.6344083547592163 2.9570460319519043 4.59145450592041
Loss :  1.65994131565094 2.8113274574279785 4.471268653869629
Loss :  1.6524626016616821 3.388056993484497 5.040519714355469
  batch 40 loss: 1.6524626016616821, 3.388056993484497, 5.040519714355469
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6605416536331177 2.7977051734924316 4.45824670791626
Loss :  1.6487988233566284 2.571589708328247 4.220388412475586
Loss :  1.6658471822738647 3.02528977394104 4.691136837005615
Loss :  1.6642718315124512 2.8124351501464844 4.4767069816589355
Loss :  1.6670081615447998 2.8906049728393555 4.557613372802734
Loss :  1.6584744453430176 2.893963575363159 4.552437782287598
Loss :  1.6454185247421265 3.078843116760254 4.72426176071167
Loss :  1.6576504707336426 3.379297971725464 5.036948204040527
Loss :  1.630293369293213 3.177884340286255 4.808177947998047
Loss :  1.6818329095840454 2.9506545066833496 4.6324872970581055
Loss :  1.6461246013641357 2.9210751056671143 4.56719970703125
Loss :  1.664886713027954 3.0177488327026367 4.682635307312012
Loss :  1.682861089706421 3.2400405406951904 4.922901630401611
Loss :  1.667960286140442 3.5138001441955566 5.181760311126709
Loss :  1.6707160472869873 3.4743359088897705 5.145051956176758
Loss :  1.6356201171875 2.8531954288482666 4.4888153076171875
Loss :  1.685036540031433 2.744579792022705 4.429616451263428
Loss :  1.682270884513855 2.8816559314727783 4.563926696777344
Loss :  1.6956790685653687 3.098262310028076 4.793941497802734
Loss :  1.6712769269943237 3.4488112926483154 5.12008810043335
  batch 60 loss: 1.6712769269943237, 3.4488112926483154, 5.12008810043335
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.672719120979309 3.3277482986450195 5.000467300415039
Loss :  1.6696343421936035 2.690765619277954 4.360400199890137
Loss :  1.677847981452942 3.270629405975342 4.948477268218994
Loss :  1.6581776142120361 3.1756176948547363 4.833795547485352
Loss :  1.6552551984786987 3.5656118392944336 5.220867156982422
Loss :  5.551393032073975 4.381577968597412 9.932971000671387
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.443192958831787 4.33463191986084 9.777824401855469
Loss :  5.540675640106201 4.205885887145996 9.746561050415039
Loss :  4.629672527313232 4.1682257652282715 8.797898292541504
Total LOSS train 4.716779085306021 valid 9.56381368637085
CE LOSS train 1.6643811867787288 valid 1.157418131828308
Contrastive LOSS train 3.0523979260371283 valid 1.0420564413070679
EPOCH 169:
Loss :  1.6511567831039429 2.692568302154541 4.343725204467773
Loss :  1.6663341522216797 3.4409356117248535 5.107269763946533
Loss :  1.6524752378463745 2.6794350147247314 4.331910133361816
Loss :  1.6558552980422974 2.9029746055603027 4.5588297843933105
Loss :  1.6802388429641724 3.283496379852295 4.963735103607178
Loss :  1.6637073755264282 2.786651134490967 4.4503583908081055
Loss :  1.6609210968017578 3.412367820739746 5.073288917541504
Loss :  1.6504123210906982 2.5528461933135986 4.203258514404297
Loss :  1.6549373865127563 2.3981094360351562 4.053046703338623
Loss :  1.6091022491455078 3.1445107460021973 4.753612995147705
Loss :  1.6690739393234253 3.2882885932922363 4.957362651824951
Loss :  1.7294964790344238 3.184608221054077 4.914104461669922
Loss :  1.676112174987793 2.928880214691162 4.604992389678955
Loss :  1.664530634880066 3.6628952026367188 5.327425956726074
Loss :  1.6430014371871948 3.045837640762329 4.688838958740234
Loss :  1.6522243022918701 3.0811281204223633 4.7333526611328125
Loss :  1.6614105701446533 2.8441269397735596 4.505537509918213
Loss :  1.6600594520568848 3.0598113536834717 4.719870567321777
Loss :  1.667545199394226 2.577906847000122 4.245451927185059
Loss :  1.624306082725525 3.022951364517212 4.647257328033447
  batch 20 loss: 1.624306082725525, 3.022951364517212, 4.647257328033447
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6595276594161987 3.3155181407928467 4.975045680999756
Loss :  1.6754573583602905 2.59371280670166 4.26917028427124
Loss :  1.6465436220169067 3.4359190464019775 5.082462787628174
Loss :  1.6878422498703003 2.7952895164489746 4.4831318855285645
Loss :  1.6852778196334839 3.6658549308776855 5.351132869720459
Loss :  1.6489511728286743 2.927520275115967 4.576471328735352
Loss :  1.6976594924926758 3.0137627124786377 4.711421966552734
Loss :  1.6401580572128296 3.261439085006714 4.901597023010254
Loss :  1.688183307647705 2.7969777584075928 4.485160827636719
Loss :  1.6419378519058228 3.161749839782715 4.803687572479248
Loss :  1.7231587171554565 3.2803666591644287 5.003525257110596
Loss :  1.6693334579467773 3.4391961097717285 5.108529567718506
Loss :  1.6547272205352783 2.9856836795806885 4.640410900115967
Loss :  1.6569467782974243 2.80342698097229 4.460373878479004
Loss :  1.6973168849945068 3.31449818611145 5.011815071105957
Loss :  1.6887644529342651 2.7539637088775635 4.442728042602539
Loss :  1.6667333841323853 3.2320244312286377 4.8987579345703125
Loss :  1.6343460083007812 2.9174506664276123 4.551796913146973
Loss :  1.6599297523498535 3.200228452682495 4.8601579666137695
Loss :  1.6524790525436401 2.9360580444335938 4.588537216186523
  batch 40 loss: 1.6524790525436401, 2.9360580444335938, 4.588537216186523
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6604950428009033 3.326629638671875 4.987124443054199
Loss :  1.6487587690353394 2.793456554412842 4.442215442657471
Loss :  1.6657919883728027 3.1303842067718506 4.796175956726074
Loss :  1.6642557382583618 3.001737356185913 4.6659932136535645
Loss :  1.6669687032699585 2.879237651824951 4.546206474304199
Loss :  1.658434271812439 3.2216808795928955 4.880115032196045
Loss :  1.6453667879104614 2.9175963401794434 4.562963008880615
Loss :  1.6576015949249268 3.2237086296081543 4.88131046295166
Loss :  1.6302849054336548 3.4194111824035645 5.04969596862793
Loss :  1.681709885597229 3.020695447921753 4.7024054527282715
Loss :  1.6461520195007324 2.8568811416625977 4.50303316116333
Loss :  1.6648694276809692 2.8842570781707764 4.549126625061035
Loss :  1.6828197240829468 3.184943437576294 4.867763042449951
Loss :  1.6680079698562622 3.1752233505249023 4.843231201171875
Loss :  1.6707143783569336 3.6311872005462646 5.301901817321777
Loss :  1.6356446743011475 3.0704846382141113 4.70612907409668
Loss :  1.6850541830062866 3.066149950027466 4.751204013824463
Loss :  1.682234764099121 3.20778226852417 4.890017032623291
Loss :  1.6956555843353271 3.032749891281128 4.728405475616455
Loss :  1.671351671218872 3.227438449859619 4.89879035949707
  batch 60 loss: 1.671351671218872, 3.227438449859619, 4.89879035949707
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6727079153060913 3.635378122329712 5.308085918426514
Loss :  1.669621467590332 2.5585293769836426 4.228150844573975
Loss :  1.6778372526168823 3.003560781478882 4.681397914886475
Loss :  1.6581977605819702 2.953097105026245 4.611294746398926
Loss :  1.6552438735961914 3.1995840072631836 4.854827880859375
Loss :  5.5593461990356445 4.412459373474121 9.971805572509766
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.454796314239502 4.392794609069824 9.847591400146484
Loss :  5.550384044647217 4.28023624420166 9.830619812011719
Loss :  4.6373066902160645 4.165172100067139 8.802478790283203
Total LOSS train 4.732718607095571 valid 9.613123893737793
CE LOSS train 1.6643685487600473 valid 1.1593266725540161
Contrastive LOSS train 3.0683500840113713 valid 1.0412930250167847
EPOCH 170:
Loss :  1.6512824296951294 2.5743441581726074 4.225626468658447
Loss :  1.6663286685943604 3.347637176513672 5.013965606689453
Loss :  1.6524693965911865 2.860299587249756 4.512768745422363
Loss :  1.6558457612991333 2.906179904937744 4.562025547027588
Loss :  1.6801029443740845 3.185002565383911 4.865105628967285
Loss :  1.6636682748794556 2.988391637802124 4.652060031890869
Loss :  1.6608787775039673 3.4692108631134033 5.13008975982666
Loss :  1.6504180431365967 2.360722780227661 4.011140823364258
Loss :  1.6549495458602905 2.3144381046295166 3.9693875312805176
Loss :  1.6090421676635742 2.490299701690674 4.099341869354248
Loss :  1.6690707206726074 3.2497458457946777 4.918816566467285
Loss :  1.7295448780059814 2.8774867057800293 4.60703182220459
Loss :  1.6761369705200195 3.0969624519348145 4.773099422454834
Loss :  1.66448175907135 3.4593636989593506 5.12384557723999
Loss :  1.6430612802505493 3.016108751296997 4.659170150756836
Loss :  1.6521506309509277 3.019522190093994 4.671672821044922
Loss :  1.6614112854003906 2.959193468093872 4.620604515075684
Loss :  1.6601678133010864 3.097287893295288 4.757455825805664
Loss :  1.6676284074783325 2.6826367378234863 4.350265026092529
Loss :  1.6242787837982178 3.2258262634277344 4.850105285644531
  batch 20 loss: 1.6242787837982178, 3.2258262634277344, 4.850105285644531
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.659497857093811 3.314591646194458 4.974089622497559
Loss :  1.6754374504089355 2.632173776626587 4.307611465454102
Loss :  1.6464452743530273 3.066814422607422 4.713259696960449
Loss :  1.6879228353500366 3.0504446029663086 4.738367557525635
Loss :  1.6853071451187134 3.254945993423462 4.940253257751465
Loss :  1.6489397287368774 2.9193012714385986 4.568241119384766
Loss :  1.6975969076156616 3.4479849338531494 5.1455817222595215
Loss :  1.6401715278625488 2.930738687515259 4.570910453796387
Loss :  1.6881418228149414 2.7702434062957764 4.458385467529297
Loss :  1.6419429779052734 3.067790985107422 4.709733963012695
Loss :  1.7231420278549194 3.151007652282715 4.874149799346924
Loss :  1.6693428754806519 3.667875051498413 5.337217807769775
Loss :  1.6547868251800537 2.984994411468506 4.6397809982299805
Loss :  1.6569896936416626 2.8458685874938965 4.5028581619262695
Loss :  1.697319507598877 3.109757423400879 4.807076930999756
Loss :  1.688838243484497 3.3280344009399414 5.016872406005859
Loss :  1.6667137145996094 3.2485289573669434 4.915242671966553
Loss :  1.6343899965286255 2.813366651535034 4.447756767272949
Loss :  1.6599360704421997 2.784712553024292 4.444648742675781
Loss :  1.6524778604507446 2.963793992996216 4.61627197265625
  batch 40 loss: 1.6524778604507446, 2.963793992996216, 4.61627197265625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6604843139648438 3.4565670490264893 5.117051124572754
Loss :  1.6487878561019897 2.404780387878418 4.053568363189697
Loss :  1.6657625436782837 3.2311854362487793 4.896947860717773
Loss :  1.6643438339233398 2.72044038772583 4.38478422164917
Loss :  1.666963815689087 2.9656262397766113 4.632590293884277
Loss :  1.6583565473556519 2.9141926765441895 4.572549343109131
Loss :  1.6454029083251953 2.7459452152252197 4.391347885131836
Loss :  1.6575781106948853 3.53760027885437 5.195178508758545
Loss :  1.6302106380462646 3.537475109100342 5.167685508728027
Loss :  1.681701421737671 3.2465052604675293 4.928206443786621
Loss :  1.6461256742477417 2.7650723457336426 4.411198139190674
Loss :  1.664825677871704 3.1792988777160645 4.844124794006348
Loss :  1.6828234195709229 3.0570504665374756 4.739873886108398
Loss :  1.6680301427841187 3.232618808746338 4.900649070739746
Loss :  1.6706935167312622 3.490316152572632 5.161009788513184
Loss :  1.635657787322998 2.8785998821258545 4.514257431030273
Loss :  1.6849842071533203 2.9690892696380615 4.654073715209961
Loss :  1.6822459697723389 3.0046510696411133 4.686897277832031
Loss :  1.6957203149795532 3.124791383743286 4.820511817932129
Loss :  1.67134428024292 2.655397891998291 4.326742172241211
  batch 60 loss: 1.67134428024292, 2.655397891998291, 4.326742172241211
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726783514022827 3.1275312900543213 4.8002095222473145
Loss :  1.669603943824768 2.5159339904785156 4.185537815093994
Loss :  1.6777962446212769 2.8123245239257812 4.490120887756348
Loss :  1.6581604480743408 3.056332588195801 4.7144927978515625
Loss :  1.6551698446273804 3.1128780841827393 4.76804780960083
Loss :  5.5467753410339355 4.421079158782959 9.967854499816895
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.441806316375732 4.380987167358398 9.822793960571289
Loss :  5.53835391998291 4.350438594818115 9.888792037963867
Loss :  4.6238813400268555 4.292111396789551 8.915992736816406
Total LOSS train 4.683993016756498 valid 9.648858308792114
CE LOSS train 1.6643647799125085 valid 1.1559703350067139
Contrastive LOSS train 3.019628224006066 valid 1.0730278491973877
EPOCH 171:
Loss :  1.651169776916504 2.5942788124084473 4.245448589324951
Loss :  1.666320562362671 3.229177713394165 4.895498275756836
Loss :  1.652430772781372 2.8909659385681152 4.543396949768066
Loss :  1.6557743549346924 2.9755024909973145 4.631277084350586
Loss :  1.6801621913909912 3.0995099544525146 4.779672145843506
Loss :  1.6636112928390503 2.986639976501465 4.650251388549805
Loss :  1.6609137058258057 3.1227118968963623 4.783625602722168
Loss :  1.6503826379776 2.7230076789855957 4.373390197753906
Loss :  1.6548938751220703 2.517258882522583 4.172152519226074
Loss :  1.6091195344924927 2.6464478969573975 4.25556755065918
Loss :  1.669030785560608 3.1206369400024414 4.78966760635376
Loss :  1.729499340057373 3.2004339694976807 4.929933547973633
Loss :  1.676221489906311 3.161212205886841 4.837433815002441
Loss :  1.6645177602767944 3.029999017715454 4.694516658782959
Loss :  1.6430951356887817 2.706286668777466 4.349381923675537
Loss :  1.652127742767334 2.926290988922119 4.578418731689453
Loss :  1.6613508462905884 2.832892417907715 4.494243144989014
Loss :  1.6600373983383179 2.9632999897003174 4.623337268829346
Loss :  1.6674460172653198 2.575953483581543 4.243399620056152
Loss :  1.6242899894714355 3.0486841201782227 4.672974109649658
  batch 20 loss: 1.6242899894714355, 3.0486841201782227, 4.672974109649658
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6595580577850342 3.3037304878234863 4.963288307189941
Loss :  1.6754201650619507 2.592653512954712 4.268073558807373
Loss :  1.6464771032333374 3.2114996910095215 4.857976913452148
Loss :  1.6878156661987305 3.5531504154205322 5.240965843200684
Loss :  1.6852892637252808 3.46329665184021 5.148585796356201
Loss :  1.648913025856018 2.9884531497955322 4.63736629486084
Loss :  1.6975994110107422 3.13785719871521 4.835456848144531
Loss :  1.6401293277740479 2.963773488998413 4.603902816772461
Loss :  1.688148021697998 2.8476150035858154 4.535762786865234
Loss :  1.6419174671173096 3.082576036453247 4.724493503570557
Loss :  1.7231791019439697 3.462632894515991 5.185811996459961
Loss :  1.66930091381073 3.1990208625793457 4.868321895599365
Loss :  1.654682993888855 3.1017794609069824 4.756462574005127
Loss :  1.6569817066192627 2.977921724319458 4.634903430938721
Loss :  1.6973085403442383 3.5897552967071533 5.2870635986328125
Loss :  1.6887786388397217 2.7753264904022217 4.464105129241943
Loss :  1.6667495965957642 3.5530385971069336 5.219788074493408
Loss :  1.6343053579330444 3.0345311164855957 4.66883659362793
Loss :  1.659862995147705 2.9740986824035645 4.6339616775512695
Loss :  1.6524558067321777 2.8903658390045166 4.542821884155273
  batch 40 loss: 1.6524558067321777, 2.8903658390045166, 4.542821884155273
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6604636907577515 3.1012794971466064 4.761743068695068
Loss :  1.6487513780593872 2.509310722351074 4.158061981201172
Loss :  1.6657685041427612 3.002800226211548 4.6685686111450195
Loss :  1.664254069328308 2.856745481491089 4.520999431610107
Loss :  1.6669293642044067 2.798126220703125 4.465055465698242
Loss :  1.6583352088928223 2.9480502605438232 4.606385231018066
Loss :  1.6453193426132202 3.0492310523986816 4.694550514221191
Loss :  1.6575474739074707 3.281013250350952 4.938560485839844
Loss :  1.6302560567855835 3.505628824234009 5.135884761810303
Loss :  1.6816409826278687 3.5452682971954346 5.226909160614014
Loss :  1.6461429595947266 2.7084908485412598 4.354633808135986
Loss :  1.664801001548767 3.186461925506592 4.851263046264648
Loss :  1.6827882528305054 3.3074700832366943 4.99025821685791
Loss :  1.6679586172103882 3.182354211807251 4.85031270980835
Loss :  1.6707698106765747 3.3681528568267822 5.0389227867126465
Loss :  1.6357016563415527 2.710787057876587 4.346488952636719
Loss :  1.6850242614746094 3.0013906955718994 4.68641471862793
Loss :  1.6822419166564941 3.0626416206359863 4.7448835372924805
Loss :  1.6956740617752075 3.2768797874450684 4.972553730010986
Loss :  1.6713441610336304 2.790562868118286 4.461906909942627
  batch 60 loss: 1.6713441610336304, 2.790562868118286, 4.461906909942627
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.672683596611023 3.2663116455078125 4.938995361328125
Loss :  1.6695799827575684 2.9105136394500732 4.5800933837890625
Loss :  1.6777403354644775 2.9253249168395996 4.603065490722656
Loss :  1.6581823825836182 3.0453927516937256 4.703575134277344
Loss :  1.6551545858383179 3.031982183456421 4.687136650085449
Loss :  5.541450023651123 4.413746356964111 9.955196380615234
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.435046672821045 4.334723949432373 9.769770622253418
Loss :  5.534987926483154 4.236942291259766 9.771930694580078
Loss :  4.619689464569092 4.272049903869629 8.891738891601562
Total LOSS train 4.701673221588135 valid 9.597159147262573
CE LOSS train 1.664343415773832 valid 1.154922366142273
Contrastive LOSS train 3.0373298241541935 valid 1.0680124759674072
EPOCH 172:
Loss :  1.6511203050613403 2.8123419284820557 4.4634623527526855
Loss :  1.666266918182373 3.346562623977661 5.012829780578613
Loss :  1.652366042137146 2.651634931564331 4.3040008544921875
Loss :  1.6557626724243164 3.0886662006378174 4.744428634643555
Loss :  1.680160403251648 3.0868382453918457 4.766998767852783
Loss :  1.6635940074920654 3.1409966945648193 4.804590702056885
Loss :  1.6609265804290771 3.505754232406616 5.166680812835693
Loss :  1.6503427028656006 2.634192943572998 4.2845354080200195
Loss :  1.6548733711242676 2.456494092941284 4.111367225646973
Loss :  1.6090943813323975 2.7314982414245605 4.340592384338379
Loss :  1.6689858436584473 3.4844818115234375 5.153467655181885
Loss :  1.7294427156448364 2.9292821884155273 4.658724784851074
Loss :  1.6762498617172241 3.180896282196045 4.857146263122559
Loss :  1.6645402908325195 3.3417000770568848 5.006240367889404
Loss :  1.6429634094238281 2.93575119972229 4.578714370727539
Loss :  1.652185082435608 3.021515130996704 4.673700332641602
Loss :  1.6613609790802002 2.8722949028015137 4.533656120300293
Loss :  1.659978985786438 2.987323522567749 4.647302627563477
Loss :  1.6674758195877075 2.8002710342407227 4.467746734619141
Loss :  1.6242876052856445 3.1861159801483154 4.810403823852539
  batch 20 loss: 1.6242876052856445, 3.1861159801483154, 4.810403823852539
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6595174074172974 3.475693941116333 5.13521146774292
Loss :  1.6754499673843384 2.957850217819214 4.633300304412842
Loss :  1.6464701890945435 2.96455717086792 4.611027240753174
Loss :  1.687796711921692 3.2969393730163574 4.98473596572876
Loss :  1.685206413269043 3.6479270458221436 5.333133697509766
Loss :  1.6488115787506104 3.018803834915161 4.6676154136657715
Loss :  1.6976298093795776 3.355003833770752 5.052633762359619
Loss :  1.6401060819625854 3.3624818325042725 5.002587795257568
Loss :  1.688173532485962 2.6200146675109863 4.308188438415527
Loss :  1.6419037580490112 2.747952938079834 4.389856815338135
Loss :  1.7231508493423462 3.29984712600708 5.022997856140137
Loss :  1.6692854166030884 3.566913366317749 5.236198902130127
Loss :  1.6546880006790161 3.327369213104248 4.982057094573975
Loss :  1.6569318771362305 2.868729591369629 4.525661468505859
Loss :  1.697278380393982 3.5356879234313965 5.232966423034668
Loss :  1.6888424158096313 3.3014791011810303 4.990321636199951
Loss :  1.666752815246582 2.9552454948425293 4.621998310089111
Loss :  1.6343696117401123 2.774566173553467 4.408935546875
Loss :  1.6599009037017822 3.0470566749572754 4.706957817077637
Loss :  1.65243399143219 3.2046449184417725 4.857079029083252
  batch 40 loss: 1.65243399143219, 3.2046449184417725, 4.857079029083252
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6604727506637573 3.267078399658203 4.92755126953125
Loss :  1.648763656616211 2.8737032413482666 4.522466659545898
Loss :  1.665761113166809 3.1558146476745605 4.82157564163208
Loss :  1.6642051935195923 2.687999725341797 4.3522047996521
Loss :  1.6669116020202637 2.678151845932007 4.345063209533691
Loss :  1.6583445072174072 2.951014518737793 4.609358787536621
Loss :  1.6453896760940552 3.038938045501709 4.684327602386475
Loss :  1.6575430631637573 3.449533700942993 5.107076644897461
Loss :  1.6302776336669922 3.355074405670166 4.985352039337158
Loss :  1.6816728115081787 3.410428524017334 5.092101097106934
Loss :  1.6461241245269775 2.6528079509735107 4.298932075500488
Loss :  1.664768099784851 2.8652734756469727 4.530041694641113
Loss :  1.6828224658966064 2.9313292503356934 4.614151954650879
Loss :  1.6680082082748413 3.132930278778076 4.800938606262207
Loss :  1.6707183122634888 3.5505473613739014 5.22126579284668
Loss :  1.635644555091858 2.7687342166900635 4.404378890991211
Loss :  1.6850007772445679 2.8046839237213135 4.489684581756592
Loss :  1.6822737455368042 3.181326389312744 4.863600254058838
Loss :  1.695698618888855 3.369694232940674 5.065392971038818
Loss :  1.6713112592697144 2.9883944988250732 4.659705638885498
  batch 60 loss: 1.6713112592697144, 2.9883944988250732, 4.659705638885498
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726585626602173 3.369433641433716 5.042092323303223
Loss :  1.6696209907531738 2.636446237564087 4.30606746673584
Loss :  1.677773118019104 2.9007670879364014 4.578540325164795
Loss :  1.658111333847046 3.0410959720611572 4.699207305908203
Loss :  1.6551644802093506 3.0438270568847656 4.698991775512695
Loss :  5.555552959442139 4.371832370758057 9.927385330200195
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.450768947601318 4.352812767028809 9.803581237792969
Loss :  5.547526836395264 4.244909286499023 9.792436599731445
Loss :  4.636590003967285 4.244760036468506 8.881349563598633
Total LOSS train 4.735540375342736 valid 9.60118818283081
CE LOSS train 1.6643341284531814 valid 1.1591475009918213
Contrastive LOSS train 3.07120623588562 valid 1.0611900091171265
EPOCH 173:
Loss :  1.6511883735656738 2.7291648387908936 4.380352973937988
Loss :  1.666304111480713 3.461352586746216 5.127656936645508
Loss :  1.652409553527832 2.8861892223358154 4.538599014282227
Loss :  1.6557883024215698 2.8918540477752686 4.547642230987549
Loss :  1.6801238059997559 3.0240585803985596 4.7041826248168945
Loss :  1.6636512279510498 2.6909191608428955 4.354570388793945
Loss :  1.6608861684799194 3.2394485473632812 4.90033483505249
Loss :  1.6503342390060425 2.606379270553589 4.256713390350342
Loss :  1.6548867225646973 2.5110292434692383 4.1659159660339355
Loss :  1.6090590953826904 2.62092924118042 4.229988098144531
Loss :  1.6690775156021118 3.1686782836914062 4.8377556800842285
Loss :  1.7294800281524658 3.152470588684082 4.881950378417969
Loss :  1.6763137578964233 3.2090601921081543 4.885374069213867
Loss :  1.6644854545593262 3.564932346343994 5.22941780090332
Loss :  1.6430240869522095 2.9343199729919434 4.577343940734863
Loss :  1.6522122621536255 3.0491840839385986 4.701396465301514
Loss :  1.6613844633102417 2.8966190814971924 4.5580034255981445
Loss :  1.6601110696792603 2.8164689540863037 4.4765801429748535
Loss :  1.6676145792007446 2.7609622478485107 4.428576946258545
Loss :  1.624284029006958 3.361314058303833 4.985598087310791
  batch 20 loss: 1.624284029006958, 3.361314058303833, 4.985598087310791
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6595103740692139 3.035829782485962 4.695340156555176
Loss :  1.6753981113433838 2.6600987911224365 4.33549690246582
Loss :  1.6464821100234985 3.096968412399292 4.74345064163208
Loss :  1.6878215074539185 3.18868350982666 4.876504898071289
Loss :  1.6852920055389404 3.533536195755005 5.218828201293945
Loss :  1.6488523483276367 3.2045540809631348 4.8534064292907715
Loss :  1.6976242065429688 3.1545462608337402 4.852170467376709
Loss :  1.640124797821045 3.0109641551971436 4.651088714599609
Loss :  1.6881577968597412 2.8060302734375 4.49418830871582
Loss :  1.6419011354446411 2.794419050216675 4.4363203048706055
Loss :  1.7231484651565552 3.0275001525878906 4.750648498535156
Loss :  1.6693533658981323 3.2223801612854004 4.891733646392822
Loss :  1.6547412872314453 3.0645601749420166 4.719301223754883
Loss :  1.6569832563400269 2.849987030029297 4.506970405578613
Loss :  1.697298526763916 3.4828221797943115 5.180120468139648
Loss :  1.6889076232910156 2.794776439666748 4.483684062957764
Loss :  1.6667309999465942 2.9732472896575928 4.639978408813477
Loss :  1.634366750717163 2.8256845474243164 4.460051536560059
Loss :  1.6599385738372803 2.8959908485412598 4.555929183959961
Loss :  1.652425765991211 3.287431001663208 4.93985652923584
  batch 40 loss: 1.652425765991211, 3.287431001663208, 4.93985652923584
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6604567766189575 3.16644549369812 4.826902389526367
Loss :  1.6487987041473389 2.643301248550415 4.292099952697754
Loss :  1.665778636932373 3.044295072555542 4.710073471069336
Loss :  1.6642181873321533 2.769505023956299 4.433723449707031
Loss :  1.6669710874557495 3.02441668510437 4.69138765335083
Loss :  1.6583352088928223 2.8809738159179688 4.539309024810791
Loss :  1.6454625129699707 3.5573604106903076 5.202822685241699
Loss :  1.6576002836227417 3.065521717071533 4.7231221199035645
Loss :  1.6302050352096558 3.4229483604431152 5.0531535148620605
Loss :  1.681706428527832 3.5198066234588623 5.201513290405273
Loss :  1.646117925643921 2.805264472961426 4.451382637023926
Loss :  1.664728045463562 3.031846523284912 4.696574687957764
Loss :  1.682807445526123 3.4325833320617676 5.115390777587891
Loss :  1.6680279970169067 3.191941738128662 4.859969615936279
Loss :  1.6707135438919067 3.4795095920562744 5.150223255157471
Loss :  1.635633945465088 2.7666666507720947 4.402300834655762
Loss :  1.684937596321106 3.266547441482544 4.9514851570129395
Loss :  1.6823232173919678 3.2134902477264404 4.895813465118408
Loss :  1.6956493854522705 3.4000630378723145 5.095712661743164
Loss :  1.671295404434204 2.7358694076538086 4.407164573669434
  batch 60 loss: 1.671295404434204, 2.7358694076538086, 4.407164573669434
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726582050323486 3.211974620819092 4.8846330642700195
Loss :  1.669636607170105 2.7792327404022217 4.448869228363037
Loss :  1.6778124570846558 3.074538469314575 4.752350807189941
Loss :  1.65813410282135 2.929370880126953 4.587504863739014
Loss :  1.6552222967147827 2.9052796363830566 4.560502052307129
Loss :  5.556976795196533 4.417231559753418 9.97420883178711
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.45018196105957 4.299562454223633 9.749744415283203
Loss :  5.551174163818359 4.237698078155518 9.788871765136719
Loss :  4.639500617980957 4.206820964813232 8.846321105957031
Total LOSS train 4.707492424891545 valid 9.589786529541016
CE LOSS train 1.6643524445020235 valid 1.1598751544952393
Contrastive LOSS train 3.0431399712195764 valid 1.051705241203308
EPOCH 174:
Loss :  1.6511595249176025 2.5501744747161865 4.201333999633789
Loss :  1.6664024591445923 3.3373539447784424 5.003756523132324
Loss :  1.6524569988250732 3.0572237968444824 4.709680557250977
Loss :  1.6558265686035156 3.0617361068725586 4.717562675476074
Loss :  1.680181860923767 2.9578394889831543 4.638021469116211
Loss :  1.6636669635772705 2.885220527648926 4.548887252807617
Loss :  1.6609047651290894 3.3166143894195557 4.9775190353393555
Loss :  1.650394082069397 2.7123687267303467 4.362762928009033
Loss :  1.65492582321167 2.3401832580566406 3.9951090812683105
Loss :  1.6090394258499146 2.5435142517089844 4.152553558349609
Loss :  1.6690738201141357 2.971682071685791 4.640755653381348
Loss :  1.729610800743103 3.5090901851654053 5.238700866699219
Loss :  1.6762787103652954 3.0749831199645996 4.7512617111206055
Loss :  1.664528489112854 3.2328290939331055 4.89735746383667
Loss :  1.6430031061172485 2.7298707962036133 4.372873783111572
Loss :  1.6522897481918335 2.844679117202759 4.496968746185303
Loss :  1.6613891124725342 2.8202247619628906 4.481614112854004
Loss :  1.660022497177124 2.7829911708831787 4.443013668060303
Loss :  1.6676536798477173 2.607640504837036 4.275294303894043
Loss :  1.6242154836654663 3.3869810104370117 5.011196613311768
  batch 20 loss: 1.6242154836654663, 3.3869810104370117, 5.011196613311768
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.659430980682373 3.4333155155181885 5.092746734619141
Loss :  1.675370454788208 2.8157503604888916 4.4911208152771
Loss :  1.6464409828186035 3.2748358249664307 4.921277046203613
Loss :  1.6878365278244019 3.2688708305358887 4.95670747756958
Loss :  1.685339331626892 3.5770909786224365 5.262430191040039
Loss :  1.648787498474121 2.8269128799438477 4.475700378417969
Loss :  1.697603464126587 3.2738873958587646 4.971490859985352
Loss :  1.6400396823883057 3.03338623046875 4.673425674438477
Loss :  1.688152551651001 2.6563634872436523 4.344515800476074
Loss :  1.6418873071670532 2.861581563949585 4.503468990325928
Loss :  1.7231241464614868 3.102912425994873 4.82603645324707
Loss :  1.6693428754806519 3.27510929107666 4.944452285766602
Loss :  1.6547688245773315 3.1102426052093506 4.765011310577393
Loss :  1.6569724082946777 3.015883684158325 4.672856330871582
Loss :  1.6974031925201416 3.487738609313965 5.185141563415527
Loss :  1.6889101266860962 2.9286038875579834 4.617514133453369
Loss :  1.6667072772979736 3.3617937564849854 5.028501033782959
Loss :  1.6344019174575806 2.740440845489502 4.374842643737793
Loss :  1.6599172353744507 2.8051679134368896 4.465085029602051
Loss :  1.6523858308792114 3.2156014442443848 4.867987155914307
  batch 40 loss: 1.6523858308792114, 3.2156014442443848, 4.867987155914307
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6605026721954346 3.0735397338867188 4.734042167663574
Loss :  1.6488265991210938 2.728520393371582 4.377346992492676
Loss :  1.6657856702804565 3.1981098651885986 4.863895416259766
Loss :  1.6643140316009521 2.828090190887451 4.492403984069824
Loss :  1.6670587062835693 2.8238584995269775 4.490917205810547
Loss :  1.6583267450332642 3.091036081314087 4.749362945556641
Loss :  1.6454994678497314 3.048896074295044 4.694395542144775
Loss :  1.6576772928237915 3.5032947063446045 5.1609721183776855
Loss :  1.6302101612091064 3.586766242980957 5.216976165771484
Loss :  1.6817620992660522 3.1817500591278076 4.86351203918457
Loss :  1.6461704969406128 2.6148719787597656 4.261042594909668
Loss :  1.6647758483886719 3.0108730792999268 4.6756486892700195
Loss :  1.6828272342681885 3.163252353668213 4.8460798263549805
Loss :  1.6681443452835083 3.230013847351074 4.898158073425293
Loss :  1.6707334518432617 3.462319850921631 5.133053302764893
Loss :  1.6356709003448486 2.82889461517334 4.464565277099609
Loss :  1.6849561929702759 2.9895529747009277 4.674509048461914
Loss :  1.6823028326034546 3.0442686080932617 4.726571559906006
Loss :  1.6957107782363892 3.126434087753296 4.822144985198975
Loss :  1.6713062524795532 2.457317590713501 4.128623962402344
  batch 60 loss: 1.6713062524795532, 2.457317590713501, 4.128623962402344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726144552230835 3.3413751125335693 5.013989448547363
Loss :  1.669650912284851 2.502023458480835 4.1716742515563965
Loss :  1.6777970790863037 2.804804801940918 4.482602119445801
Loss :  1.6581274271011353 2.8383729457855225 4.496500492095947
Loss :  1.6551707983016968 3.179800033569336 4.834970951080322
Loss :  5.557742118835449 4.409287929534912 9.967029571533203
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.450967311859131 4.434229850769043 9.885196685791016
Loss :  5.551187038421631 4.3052215576171875 9.856409072875977
Loss :  4.642979621887207 4.21770715713501 8.860687255859375
Total LOSS train 4.686623001098633 valid 9.642330646514893
CE LOSS train 1.6643656767331636 valid 1.1607449054718018
Contrastive LOSS train 3.0222573463733378 valid 1.0544267892837524
EPOCH 175:
Loss :  1.6512192487716675 2.720038414001465 4.371257781982422
Loss :  1.6663764715194702 3.230726480484009 4.8971028327941895
Loss :  1.6524585485458374 2.9526984691619873 4.605156898498535
Loss :  1.6558668613433838 3.066592216491699 4.722458839416504
Loss :  1.6801706552505493 3.068850040435791 4.749020576477051
Loss :  1.6636408567428589 2.947817087173462 4.611457824707031
Loss :  1.6609083414077759 3.138662576675415 4.7995710372924805
Loss :  1.6503552198410034 2.6290886402130127 4.279443740844727
Loss :  1.6549464464187622 2.6768298149108887 4.331776142120361
Loss :  1.609006404876709 2.6100900173187256 4.2190961837768555
Loss :  1.6690454483032227 2.933847188949585 4.602892875671387
Loss :  1.7296189069747925 2.9837117195129395 4.7133307456970215
Loss :  1.6762945652008057 3.1365551948547363 4.812849998474121
Loss :  1.6644824743270874 3.459838390350342 5.124320983886719
Loss :  1.6430355310440063 3.1115033626556396 4.7545390129089355
Loss :  1.6522128582000732 3.1664233207702637 4.818635940551758
Loss :  1.6613508462905884 2.7896358966827393 4.450986862182617
Loss :  1.660019040107727 2.684674024581909 4.344693183898926
Loss :  1.6675829887390137 2.5592546463012695 4.226837635040283
Loss :  1.624220609664917 2.868058204650879 4.492279052734375
  batch 20 loss: 1.624220609664917, 2.868058204650879, 4.492279052734375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6594754457473755 3.173718214035034 4.833193778991699
Loss :  1.6753166913986206 2.4268319606781006 4.102148532867432
Loss :  1.6464288234710693 3.600358247756958 5.246787071228027
Loss :  1.6878118515014648 3.236438274383545 4.92425012588501
Loss :  1.6852487325668335 3.5564956665039062 5.241744518280029
Loss :  1.6488018035888672 2.924834966659546 4.573637008666992
Loss :  1.6975558996200562 3.0167267322540283 4.714282512664795
Loss :  1.6399917602539062 3.1872875690460205 4.827279090881348
Loss :  1.6880992650985718 2.701077938079834 4.389177322387695
Loss :  1.6418380737304688 2.9494199752807617 4.5912580490112305
Loss :  1.7231082916259766 3.0957186222076416 4.818826675415039
Loss :  1.6692818403244019 3.67706298828125 5.346344947814941
Loss :  1.6547136306762695 3.125941514968872 4.7806549072265625
Loss :  1.6569305658340454 2.7408339977264404 4.397764682769775
Loss :  1.6973260641098022 3.5803678035736084 5.277693748474121
Loss :  1.6888700723648071 2.9238667488098145 4.612736701965332
Loss :  1.6667299270629883 3.255671739578247 4.922401428222656
Loss :  1.6343090534210205 2.9909603595733643 4.625269412994385
Loss :  1.6598864793777466 2.9589028358459473 4.618789196014404
Loss :  1.6523330211639404 3.1748039722442627 4.827136993408203
  batch 40 loss: 1.6523330211639404, 3.1748039722442627, 4.827136993408203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.660460352897644 3.0804758071899414 4.740936279296875
Loss :  1.6487871408462524 2.6098544597625732 4.258641719818115
Loss :  1.6657116413116455 3.110644578933716 4.776356220245361
Loss :  1.6642041206359863 3.0697929859161377 4.733997344970703
Loss :  1.666948676109314 2.6295626163482666 4.296511173248291
Loss :  1.6582708358764648 2.808992624282837 4.467263221740723
Loss :  1.6453649997711182 3.1453702449798584 4.790735244750977
Loss :  1.657572865486145 3.220759391784668 4.878332138061523
Loss :  1.630285382270813 3.63670015335083 5.2669854164123535
Loss :  1.6816751956939697 3.4670567512512207 5.1487321853637695
Loss :  1.6461173295974731 2.9429686069488525 4.589086055755615
Loss :  1.6646945476531982 2.8097524642944336 4.474447250366211
Loss :  1.682805061340332 3.3367435932159424 5.019548416137695
Loss :  1.667924165725708 3.1241180896759033 4.792042255401611
Loss :  1.6707587242126465 3.211268663406372 4.882027626037598
Loss :  1.6356223821640015 2.663001298904419 4.298623561859131
Loss :  1.6849669218063354 3.374845266342163 5.059812068939209
Loss :  1.6822659969329834 3.1592369079589844 4.841503143310547
Loss :  1.6957391500473022 3.259596347808838 4.95533561706543
Loss :  1.6713027954101562 2.9615423679351807 4.632844924926758
  batch 60 loss: 1.6713027954101562, 2.9615423679351807, 4.632844924926758
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726423501968384 3.169234275817871 4.84187650680542
Loss :  1.6695700883865356 2.640882730484009 4.310452938079834
Loss :  1.67770254611969 2.9516329765319824 4.629335403442383
Loss :  1.6580979824066162 2.8844094276428223 4.542507171630859
Loss :  1.6552070379257202 3.0657095909118652 4.720916748046875
Loss :  5.543930530548096 4.446572780609131 9.990503311157227
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.436668872833252 4.452755928039551 9.889425277709961
Loss :  5.539890766143799 4.3305182456970215 9.87040901184082
Loss :  4.6284379959106445 4.24405574798584 8.872493743896484
Total LOSS train 4.700737469012921 valid 9.655707836151123
CE LOSS train 1.664331813958975 valid 1.1571094989776611
Contrastive LOSS train 3.036405662389902 valid 1.06101393699646
EPOCH 176:
Loss :  1.651121735572815 2.523019313812256 4.174140930175781
Loss :  1.666337490081787 3.250105857849121 4.916443347930908
Loss :  1.6523809432983398 2.682234048843384 4.3346147537231445
Loss :  1.6558165550231934 3.1774113178253174 4.83322811126709
Loss :  1.6801550388336182 2.938967227935791 4.619122505187988
Loss :  1.6635295152664185 2.9992599487304688 4.662789344787598
Loss :  1.6610081195831299 3.268059015274048 4.929067134857178
Loss :  1.6503288745880127 2.5264952182769775 4.17682409286499
Loss :  1.6549819707870483 2.4660661220550537 4.1210479736328125
Loss :  1.6090724468231201 2.705875873565674 4.314948081970215
Loss :  1.6690679788589478 3.144301652908325 4.8133697509765625
Loss :  1.7295547723770142 2.851842164993286 4.58139705657959
Loss :  1.6763463020324707 2.9663634300231934 4.642709732055664
Loss :  1.6645961999893188 3.3082375526428223 4.972833633422852
Loss :  1.642964243888855 2.8374016284942627 4.480365753173828
Loss :  1.6522632837295532 2.9416043758392334 4.593867778778076
Loss :  1.661379098892212 2.808830738067627 4.470210075378418
Loss :  1.6600394248962402 2.6292426586151123 4.289281845092773
Loss :  1.6676888465881348 2.859866142272949 4.527554988861084
Loss :  1.6242095232009888 3.0650532245635986 4.689262866973877
  batch 20 loss: 1.6242095232009888, 3.0650532245635986, 4.689262866973877
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6594544649124146 3.1064226627349854 4.7658772468566895
Loss :  1.6754150390625 2.6129541397094727 4.288369178771973
Loss :  1.6464886665344238 3.5352237224578857 5.1817121505737305
Loss :  1.6878101825714111 2.9516801834106445 4.639490127563477
Loss :  1.6853197813034058 3.546337842941284 5.2316575050354
Loss :  1.6487996578216553 2.6933021545410156 4.34210205078125
Loss :  1.697589635848999 3.264070749282837 4.961660385131836
Loss :  1.6400139331817627 3.260514259338379 4.9005279541015625
Loss :  1.688137412071228 3.016239643096924 4.704377174377441
Loss :  1.6418302059173584 3.068838596343994 4.710668563842773
Loss :  1.7230796813964844 3.293221950531006 5.01630163192749
Loss :  1.6693201065063477 3.4375908374786377 5.106910705566406
Loss :  1.6547470092773438 2.92340087890625 4.578147888183594
Loss :  1.656981110572815 2.9423277378082275 4.599308967590332
Loss :  1.6973854303359985 3.340733528137207 5.038118839263916
Loss :  1.6888911724090576 2.7146902084350586 4.403581619262695
Loss :  1.666722059249878 3.1034929752349854 4.770215034484863
Loss :  1.6343719959259033 2.8088598251342773 4.443231582641602
Loss :  1.659926414489746 2.844762086868286 4.504688262939453
Loss :  1.6524021625518799 3.4211525917053223 5.073554992675781
  batch 40 loss: 1.6524021625518799, 3.4211525917053223, 5.073554992675781
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6604756116867065 3.061974287033081 4.722449779510498
Loss :  1.6488291025161743 2.6449713706970215 4.293800354003906
Loss :  1.6657387018203735 3.076124668121338 4.741863250732422
Loss :  1.664229393005371 3.049508810043335 4.713738441467285
Loss :  1.6669878959655762 3.1983020305633545 4.865289688110352
Loss :  1.658274531364441 2.871367931365967 4.529642581939697
Loss :  1.6454228162765503 3.146815061569214 4.792237758636475
Loss :  1.6576330661773682 3.117882013320923 4.775515079498291
Loss :  1.6303107738494873 3.501242160797119 5.131552696228027
Loss :  1.6816365718841553 3.2626428604125977 4.944279670715332
Loss :  1.6460684537887573 2.5529091358184814 4.198977470397949
Loss :  1.6647021770477295 2.941049337387085 4.6057515144348145
Loss :  1.6828219890594482 3.4278759956359863 5.1106977462768555
Loss :  1.6679927110671997 3.4238626956939697 5.091855525970459
Loss :  1.6707096099853516 3.6009738445281982 5.271683692932129
Loss :  1.6356422901153564 2.648308515548706 4.2839508056640625
Loss :  1.6849255561828613 3.1653730869293213 4.850298881530762
Loss :  1.6823031902313232 3.196312665939331 4.878615856170654
Loss :  1.6956508159637451 3.261087656021118 4.956738471984863
Loss :  1.6712511777877808 3.0338311195373535 4.705082416534424
  batch 60 loss: 1.6712511777877808, 3.0338311195373535, 4.705082416534424
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.672620415687561 3.7487683296203613 5.421388626098633
Loss :  1.669594645500183 2.687764883041382 4.357359409332275
Loss :  1.6777180433273315 2.9034056663513184 4.5811238288879395
Loss :  1.6580712795257568 2.958448648452759 4.616519927978516
Loss :  1.6551743745803833 2.762295722961426 4.4174699783325195
Loss :  5.544393062591553 4.31736946105957 9.861763000488281
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.437434673309326 4.392584323883057 9.830018997192383
Loss :  5.540738582611084 4.310132026672363 9.850870132446289
Loss :  4.627943515777588 4.420947074890137 9.048891067504883
Total LOSS train 4.696330231886644 valid 9.647885799407959
CE LOSS train 1.664343287394597 valid 1.156985878944397
Contrastive LOSS train 3.031986962831937 valid 1.1052367687225342
EPOCH 177:
Loss :  1.6511073112487793 2.765091896057129 4.416199207305908
Loss :  1.6663823127746582 3.248784303665161 4.915166854858398
Loss :  1.652418851852417 2.8938746452331543 4.546293258666992
Loss :  1.6558263301849365 3.057119131088257 4.712945461273193
Loss :  1.6801717281341553 3.021766424179077 4.701938152313232
Loss :  1.6635563373565674 2.733779191970825 4.397335529327393
Loss :  1.6609975099563599 3.423908233642578 5.084905624389648
Loss :  1.6503043174743652 2.5455312728881836 4.195835590362549
Loss :  1.654987096786499 2.204359531402588 3.859346628189087
Loss :  1.6090117692947388 2.593639850616455 4.202651500701904
Loss :  1.66904616355896 3.299776315689087 4.968822479248047
Loss :  1.7295832633972168 2.879267930984497 4.608851432800293
Loss :  1.676302433013916 3.1451222896575928 4.82142448425293
Loss :  1.664575219154358 3.3040878772735596 4.968663215637207
Loss :  1.643006682395935 3.024470329284668 4.667477130889893
Loss :  1.6522672176361084 2.760091543197632 4.41235876083374
Loss :  1.6613556146621704 3.025296211242676 4.686651706695557
Loss :  1.6600276231765747 2.8718810081481934 4.5319085121154785
Loss :  1.6676084995269775 2.8780083656311035 4.54561710357666
Loss :  1.6242060661315918 3.2826902866363525 4.906896591186523
  batch 20 loss: 1.6242060661315918, 3.2826902866363525, 4.906896591186523
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6594992876052856 3.168937921524048 4.828437328338623
Loss :  1.6753907203674316 2.9599416255950928 4.635332107543945
Loss :  1.6464492082595825 3.234539270401001 4.880988597869873
Loss :  1.687839150428772 2.846122980117798 4.533962249755859
Loss :  1.685261607170105 3.4800381660461426 5.165299892425537
Loss :  1.6488192081451416 3.129117250442505 4.7779364585876465
Loss :  1.6975651979446411 3.07179856300354 4.769363880157471
Loss :  1.6400082111358643 3.147495985031128 4.787504196166992
Loss :  1.6881070137023926 2.6686582565307617 4.356765270233154
Loss :  1.6419070959091187 3.0837390422821045 4.725646018981934
Loss :  1.7230442762374878 3.1861507892608643 4.9091949462890625
Loss :  1.6692726612091064 3.1539809703826904 4.823253631591797
Loss :  1.6546809673309326 2.9230101108551025 4.577691078186035
Loss :  1.656996250152588 2.6405510902404785 4.297547340393066
Loss :  1.6972934007644653 3.225356101989746 4.922649383544922
Loss :  1.6888771057128906 2.835014581680298 4.523891448974609
Loss :  1.666715383529663 3.162855386734009 4.829570770263672
Loss :  1.6343523263931274 2.7606167793273926 4.3949689865112305
Loss :  1.6598637104034424 2.725346565246582 4.385210037231445
Loss :  1.652401089668274 3.244490385055542 4.8968915939331055
  batch 40 loss: 1.652401089668274, 3.244490385055542, 4.8968915939331055
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6604125499725342 3.0701253414154053 4.7305378913879395
Loss :  1.648775577545166 3.0374722480773926 4.686247825622559
Loss :  1.6657180786132812 3.2034873962402344 4.869205474853516
Loss :  1.6642125844955444 2.99695086479187 4.661163330078125
Loss :  1.6669645309448242 2.694349527359009 4.361313819885254
Loss :  1.6582947969436646 3.256164312362671 4.914459228515625
Loss :  1.6453542709350586 3.500800609588623 5.146154880523682
Loss :  1.6576225757598877 3.243638277053833 4.901260852813721
Loss :  1.6303104162216187 3.3275763988494873 4.957886695861816
Loss :  1.6817378997802734 3.294647455215454 4.976385116577148
Loss :  1.6460996866226196 2.5589489936828613 4.205048561096191
Loss :  1.6646463871002197 3.302231550216675 4.9668779373168945
Loss :  1.682826280593872 3.818812131881714 5.501638412475586
Loss :  1.6679316759109497 3.303431987762451 4.971363544464111
Loss :  1.6707675457000732 3.5285685062408447 5.199336051940918
Loss :  1.63552987575531 2.7774107456207275 4.412940502166748
Loss :  1.6849132776260376 2.9833436012268066 4.668256759643555
Loss :  1.682299017906189 3.1559832096099854 4.838282108306885
Loss :  1.6956290006637573 3.4517087936401367 5.147337913513184
Loss :  1.6712284088134766 3.270040988922119 4.941269397735596
  batch 60 loss: 1.6712284088134766, 3.270040988922119, 4.941269397735596
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726106405258179 3.5469422340393066 5.219552993774414
Loss :  1.6695607900619507 2.6153814792633057 4.284942150115967
Loss :  1.677754521369934 2.8086767196655273 4.486431121826172
Loss :  1.658050537109375 2.988384246826172 4.646434783935547
Loss :  1.6552045345306396 3.0147857666015625 4.669990539550781
Loss :  5.539251804351807 4.391311168670654 9.930562973022461
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.428213596343994 4.295470714569092 9.723684310913086
Loss :  5.534669399261475 4.281807899475098 9.816476821899414
Loss :  4.6224365234375 4.3235249519348145 8.945960998535156
Total LOSS train 4.71596480516287 valid 9.60417127609253
CE LOSS train 1.6643314104813796 valid 1.155609130859375
Contrastive LOSS train 3.051633413021381 valid 1.0808812379837036
EPOCH 178:
Loss :  1.6509915590286255 2.89858341217041 4.549574851989746
Loss :  1.666331171989441 3.6155526638031006 5.281883716583252
Loss :  1.652401328086853 2.700734853744507 4.35313606262207
Loss :  1.6557998657226562 2.8507089614868164 4.506508827209473
Loss :  1.6802128553390503 2.865391492843628 4.545604228973389
Loss :  1.6635468006134033 3.087947130203247 4.75149393081665
Loss :  1.6609939336776733 3.4040684700012207 5.065062522888184
Loss :  1.6502999067306519 2.4967994689941406 4.147099494934082
Loss :  1.6548866033554077 2.2595181465148926 3.91440486907959
Loss :  1.6090061664581299 2.7387125492095947 4.347718715667725
Loss :  1.6689785718917847 3.0821802616119385 4.751158714294434
Loss :  1.72954523563385 3.1164045333862305 4.845949649810791
Loss :  1.6762120723724365 3.130833625793457 4.807045936584473
Loss :  1.6645281314849854 3.17029070854187 4.8348188400268555
Loss :  1.6429953575134277 2.6587674617767334 4.301762580871582
Loss :  1.6522186994552612 3.326206684112549 4.9784255027771
Loss :  1.6613353490829468 2.7582814693450928 4.41961669921875
Loss :  1.6599642038345337 3.07503080368042 4.734994888305664
Loss :  1.6676098108291626 2.6342036724090576 4.30181360244751
Loss :  1.6242239475250244 2.933377504348755 4.557601451873779
  batch 20 loss: 1.6242239475250244, 2.933377504348755, 4.557601451873779
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6594438552856445 3.4193148612976074 5.078758716583252
Loss :  1.675362229347229 2.9201340675354004 4.59549617767334
Loss :  1.6464438438415527 3.0783886909484863 4.724832534790039
Loss :  1.6877937316894531 2.993252754211426 4.681046485900879
Loss :  1.6852442026138306 3.4030280113220215 5.0882720947265625
Loss :  1.6487483978271484 3.1056783199310303 4.754426956176758
Loss :  1.6975011825561523 3.266676902770996 4.964178085327148
Loss :  1.6400182247161865 3.3096048831939697 4.949623107910156
Loss :  1.6881030797958374 2.6808693408966064 4.368972301483154
Loss :  1.641764760017395 2.9124462604522705 4.554211139678955
Loss :  1.723024606704712 2.9046883583068848 4.627713203430176
Loss :  1.669231653213501 3.19950795173645 4.868739604949951
Loss :  1.6547484397888184 2.7666232585906982 4.4213714599609375
Loss :  1.656948447227478 2.944878339767456 4.6018266677856445
Loss :  1.6972721815109253 3.5795373916625977 5.2768096923828125
Loss :  1.6889125108718872 3.0948567390441895 4.783769130706787
Loss :  1.6666860580444336 3.007495164871216 4.67418098449707
Loss :  1.6343367099761963 2.9335732460021973 4.567910194396973
Loss :  1.659862995147705 2.766763210296631 4.426626205444336
Loss :  1.652382493019104 3.0084238052368164 4.660806179046631
  batch 40 loss: 1.652382493019104, 3.0084238052368164, 4.660806179046631
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.660384178161621 3.20418643951416 4.864570617675781
Loss :  1.6487752199172974 2.6880993843078613 4.336874485015869
Loss :  1.6657055616378784 3.090235471725464 4.755940914154053
Loss :  1.6641786098480225 2.6576147079467773 4.321793556213379
Loss :  1.6668939590454102 2.988325595855713 4.655219554901123
Loss :  1.6582520008087158 2.824270725250244 4.482522964477539
Loss :  1.6453542709350586 3.0607032775878906 4.706057548522949
Loss :  1.6575403213500977 3.0991950035095215 4.756735324859619
Loss :  1.6302542686462402 3.548006534576416 5.178260803222656
Loss :  1.6816047430038452 3.6203882694244385 5.301992893218994
Loss :  1.6460998058319092 2.731416940689087 4.377516746520996
Loss :  1.664650797843933 3.0304718017578125 4.695122718811035
Loss :  1.6827815771102905 3.06862473487854 4.751406192779541
Loss :  1.6678526401519775 2.9440481662750244 4.611900806427002
Loss :  1.6707991361618042 3.2413153648376465 4.91211462020874
Loss :  1.6356470584869385 3.123563289642334 4.759210586547852
Loss :  1.684898853302002 3.2499277591705322 4.934826850891113
Loss :  1.6822502613067627 3.1829099655151367 4.86515998840332
Loss :  1.6957058906555176 3.0702788829803467 4.765984535217285
Loss :  1.6712520122528076 3.0381455421447754 4.709397315979004
  batch 60 loss: 1.6712520122528076, 3.0381455421447754, 4.709397315979004
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.672630786895752 3.3962817192077637 5.068912506103516
Loss :  1.66948664188385 2.662506580352783 4.331993103027344
Loss :  1.677711009979248 2.8950271606445312 4.572738170623779
Loss :  1.6581066846847534 2.9058375358581543 4.563944339752197
Loss :  1.655178189277649 3.2829787731170654 4.938157081604004
Loss :  5.541118144989014 4.40323543548584 9.944353103637695
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.433523654937744 4.372244358062744 9.805768013000488
Loss :  5.538753509521484 4.325807571411133 9.864561080932617
Loss :  4.62101936340332 4.158248424530029 8.779268264770508
Total LOSS train 4.690516926692083 valid 9.598487615585327
CE LOSS train 1.6643062408153828 valid 1.15525484085083
Contrastive LOSS train 3.026210693212656 valid 1.0395621061325073
EPOCH 179:
Loss :  1.6509941816329956 2.755776882171631 4.406771183013916
Loss :  1.6663107872009277 3.360327959060669 5.026638984680176
Loss :  1.6523224115371704 2.7242090702056885 4.376531600952148
Loss :  1.65567946434021 3.0462350845336914 4.7019147872924805
Loss :  1.680148720741272 3.1982128620147705 4.878361701965332
Loss :  1.6634021997451782 2.8277804851531982 4.491182804107666
Loss :  1.661088466644287 3.2491190433502197 4.910207748413086
Loss :  1.6502606868743896 2.7011988162994385 4.351459503173828
Loss :  1.6548385620117188 2.3355727195739746 3.9904112815856934
Loss :  1.6090980768203735 2.9396984577178955 4.548796653747559
Loss :  1.66901433467865 3.1065351963043213 4.775549411773682
Loss :  1.7293572425842285 3.2406015396118164 4.969958782196045
Loss :  1.6763572692871094 3.1152310371398926 4.791588306427002
Loss :  1.6645283699035645 3.467411518096924 5.131939888000488
Loss :  1.6429569721221924 3.01519775390625 4.658154487609863
Loss :  1.6522024869918823 3.0258359909057617 4.678038597106934
Loss :  1.6612720489501953 2.882164239883423 4.543436050415039
Loss :  1.6599421501159668 2.9159839153289795 4.575925827026367
Loss :  1.6675186157226562 2.8610193729400635 4.528537750244141
Loss :  1.6241273880004883 3.194098949432373 4.818226337432861
  batch 20 loss: 1.6241273880004883, 3.194098949432373, 4.818226337432861
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6594915390014648 3.2564520835876465 4.915943622589111
Loss :  1.6752675771713257 2.480808973312378 4.156076431274414
Loss :  1.6464788913726807 3.158212184906006 4.804691314697266
Loss :  1.6876542568206787 2.974689245223999 4.662343502044678
Loss :  1.6851879358291626 3.2695798873901367 4.95476770401001
Loss :  1.6487795114517212 2.9676060676574707 4.616385459899902
Loss :  1.6975629329681396 3.0087497234344482 4.706312656402588
Loss :  1.6400065422058105 3.597621440887451 5.237627983093262
Loss :  1.68812894821167 2.823436975479126 4.511566162109375
Loss :  1.641770362854004 3.191757917404175 4.833528518676758
Loss :  1.7230565547943115 3.0032312870025635 4.726287841796875
Loss :  1.669221043586731 3.1111950874328613 4.780416011810303
Loss :  1.6546639204025269 2.8049027919769287 4.459566593170166
Loss :  1.6569163799285889 2.7411599159240723 4.398076057434082
Loss :  1.6971752643585205 3.6201274394989014 5.317302703857422
Loss :  1.6888935565948486 2.8917107582092285 4.580604553222656
Loss :  1.6667134761810303 3.1224019527435303 4.7891154289245605
Loss :  1.6342947483062744 2.848881721496582 4.483176231384277
Loss :  1.6598395109176636 2.8597195148468018 4.519558906555176
Loss :  1.6524040699005127 3.185616970062256 4.838021278381348
  batch 40 loss: 1.6524040699005127, 3.185616970062256, 4.838021278381348
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.660351276397705 2.8405494689941406 4.500900745391846
Loss :  1.6487317085266113 2.568028688430786 4.216760635375977
Loss :  1.6656570434570312 2.933539628982544 4.599196434020996
Loss :  1.664055347442627 3.0426909923553467 4.7067461013793945
Loss :  1.6668365001678467 3.0960488319396973 4.762885093688965
Loss :  1.6582369804382324 2.9891984462738037 4.647435188293457
Loss :  1.6453404426574707 3.2498152256011963 4.895155906677246
Loss :  1.6574547290802002 3.1334078311920166 4.790862560272217
Loss :  1.6302686929702759 3.7174997329711914 5.347768306732178
Loss :  1.6816205978393555 3.281355142593384 4.96297550201416
Loss :  1.646091103553772 2.7080490589141846 4.354140281677246
Loss :  1.6646262407302856 2.9381144046783447 4.60274076461792
Loss :  1.6827418804168701 3.464005708694458 5.146747589111328
Loss :  1.6677876710891724 3.2806756496429443 4.948463439941406
Loss :  1.6706949472427368 3.4530131816864014 5.123708248138428
Loss :  1.6354888677597046 2.751983165740967 4.387472152709961
Loss :  1.6848821640014648 3.2546756267547607 4.939558029174805
Loss :  1.6822806596755981 3.1144919395446777 4.796772480010986
Loss :  1.6957210302352905 3.3082470893859863 5.003968238830566
Loss :  1.6712455749511719 3.2564611434936523 4.927706718444824
  batch 60 loss: 1.6712455749511719, 3.2564611434936523, 4.927706718444824
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6725927591323853 3.5346312522888184 5.207223892211914
Loss :  1.669421672821045 2.685729503631592 4.355151176452637
Loss :  1.6776516437530518 2.6900999546051025 4.367751598358154
Loss :  1.6580126285552979 2.7989652156829834 4.456977844238281
Loss :  1.6551541090011597 2.4508302211761475 4.105984210968018
Loss :  5.5365681648254395 4.389140605926514 9.925708770751953
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.430049419403076 4.3074774742126465 9.737526893615723
Loss :  5.532792568206787 4.319581031799316 9.852373123168945
Loss :  4.615391731262207 4.332758903503418 8.948150634765625
Total LOSS train 4.70153928903433 valid 9.615939855575562
CE LOSS train 1.6642749804716843 valid 1.1538479328155518
Contrastive LOSS train 3.0372643067286567 valid 1.0831897258758545
EPOCH 180:
Loss :  1.6509149074554443 2.7452378273010254 4.396152496337891
Loss :  1.6662529706954956 3.1456284523010254 4.8118815422058105
Loss :  1.6523330211639404 2.6330323219299316 4.285365104675293
Loss :  1.6557002067565918 2.9015257358551025 4.557226181030273
Loss :  1.6800645589828491 3.0012998580932617 4.6813645362854
Loss :  1.6634666919708252 2.9623115062713623 4.6257781982421875
Loss :  1.6610143184661865 3.451676845550537 5.1126909255981445
Loss :  1.650281548500061 2.5584590435028076 4.208740711212158
Loss :  1.6548264026641846 2.5103724002838135 4.165198802947998
Loss :  1.6090863943099976 2.885505437850952 4.49459171295166
Loss :  1.6689841747283936 3.1753549575805664 4.844339370727539
Loss :  1.7293908596038818 2.8488354682922363 4.578226089477539
Loss :  1.67628014087677 3.2326300144195557 4.908910274505615
Loss :  1.6644991636276245 3.268101930618286 4.932600975036621
Loss :  1.6430367231369019 2.784766674041748 4.4278035163879395
Loss :  1.6521308422088623 3.0304629802703857 4.682593822479248
Loss :  1.661250114440918 2.638946533203125 4.300196647644043
Loss :  1.6598864793777466 2.9759395122528076 4.635826110839844
Loss :  1.6676069498062134 3.157688856124878 4.825295925140381
Loss :  1.6241480112075806 3.1878013610839844 4.811949253082275
  batch 20 loss: 1.6241480112075806, 3.1878013610839844, 4.811949253082275
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.659449577331543 3.066673755645752 4.726123332977295
Loss :  1.6752647161483765 2.7395107746124268 4.414775371551514
Loss :  1.6464494466781616 2.9930922985076904 4.6395416259765625
Loss :  1.6876722574234009 2.6182637214660645 4.305935859680176
Loss :  1.685167908668518 3.4021711349487305 5.087338924407959
Loss :  1.648771047592163 2.866680383682251 4.515451431274414
Loss :  1.6975394487380981 3.2350234985351562 4.932562828063965
Loss :  1.6400253772735596 2.736133098602295 4.376158714294434
Loss :  1.6881115436553955 2.6103665828704834 4.298478126525879
Loss :  1.6417206525802612 2.8971147537231445 4.538835525512695
Loss :  1.7230514287948608 3.1827569007873535 4.905808448791504
Loss :  1.6691895723342896 3.0018885135650635 4.671078205108643
Loss :  1.6546170711517334 3.182142734527588 4.836759567260742
Loss :  1.656912922859192 2.6085774898529053 4.265490531921387
Loss :  1.6970841884613037 3.217820167541504 4.914904594421387
Loss :  1.6888014078140259 2.759854793548584 4.44865608215332
Loss :  1.6666185855865479 2.844616174697876 4.511234760284424
Loss :  1.6342626810073853 2.923302412033081 4.557565212249756
Loss :  1.6598405838012695 2.6812057495117188 4.341046333312988
Loss :  1.6523841619491577 2.9567742347717285 4.609158515930176
  batch 40 loss: 1.6523841619491577, 2.9567742347717285, 4.609158515930176
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6603634357452393 3.0183422565460205 4.67870569229126
Loss :  1.6486626863479614 2.837095022201538 4.485757827758789
Loss :  1.665690541267395 3.1771914958953857 4.84288215637207
Loss :  1.6639784574508667 2.824910879135132 4.488889217376709
Loss :  1.6668918132781982 2.67926287651062 4.346154689788818
Loss :  1.6582661867141724 3.039123058319092 4.697389125823975
Loss :  1.6454135179519653 3.1032984256744385 4.748712062835693
Loss :  1.6574475765228271 3.1311635971069336 4.78861141204834
Loss :  1.6302964687347412 3.484313488006592 5.114609718322754
Loss :  1.681663155555725 3.1072006225585938 4.788863658905029
Loss :  1.6460710763931274 2.6262171268463135 4.2722883224487305
Loss :  1.6646888256072998 2.770542621612549 4.4352312088012695
Loss :  1.6827256679534912 3.295771360397339 4.97849702835083
Loss :  1.6678473949432373 2.9899251461029053 4.657772541046143
Loss :  1.670667052268982 3.4507322311401367 5.121399402618408
Loss :  1.6354511976242065 2.6233816146850586 4.258832931518555
Loss :  1.6848230361938477 2.9564661979675293 4.641289234161377
Loss :  1.6822282075881958 3.164677619934082 4.846905708312988
Loss :  1.6956008672714233 3.1738803386688232 4.869481086730957
Loss :  1.671221375465393 2.789280414581299 4.460501670837402
  batch 60 loss: 1.671221375465393, 2.789280414581299, 4.460501670837402
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6725815534591675 3.2343597412109375 4.9069414138793945
Loss :  1.6694616079330444 2.6580114364624023 4.327473163604736
Loss :  1.6776165962219238 2.922229051589966 4.599845886230469
Loss :  1.658048152923584 3.1202237606048584 4.778271675109863
Loss :  1.6551674604415894 3.5492606163024902 5.204428195953369
Loss :  5.54793643951416 4.363264560699463 9.911201477050781
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.442709922790527 4.4517741203308105 9.89448356628418
Loss :  5.542086124420166 4.276153564453125 9.818239212036133
Loss :  4.626020431518555 4.131157398223877 8.757177352905273
Total LOSS train 4.638821095686692 valid 9.595275402069092
CE LOSS train 1.6642609687951895 valid 1.1565051078796387
Contrastive LOSS train 2.9745601213895356 valid 1.0327893495559692
EPOCH 181:
Loss :  1.6509454250335693 2.449554681777954 4.100500106811523
Loss :  1.6663086414337158 2.939422369003296 4.605731010437012
Loss :  1.652266502380371 2.985360622406006 4.637627124786377
Loss :  1.6556740999221802 2.7830824851989746 4.438756465911865
Loss :  1.680090308189392 2.909595012664795 4.589685440063477
Loss :  1.6634374856948853 2.9149699211120605 4.578407287597656
Loss :  1.6610345840454102 3.3233535289764404 4.98438835144043
Loss :  1.6502246856689453 2.3930130004882812 4.043237686157227
Loss :  1.654879093170166 2.2019851207733154 3.8568642139434814
Loss :  1.609010934829712 2.4561996459960938 4.065210342407227
Loss :  1.6689791679382324 2.8698666095733643 4.538846015930176
Loss :  1.7294384241104126 2.8012073040008545 4.530645847320557
Loss :  1.676179051399231 3.2181472778320312 4.894326210021973
Loss :  1.6644986867904663 3.1804471015930176 4.844945907592773
Loss :  1.64297616481781 2.7739927768707275 4.416968822479248
Loss :  1.6521327495574951 3.002185106277466 4.654317855834961
Loss :  1.6612259149551392 2.856870174407959 4.518095970153809
Loss :  1.659969687461853 3.03371000289917 4.6936798095703125
Loss :  1.6674941778182983 2.7459864616394043 4.413480758666992
Loss :  1.6241778135299683 2.9174296855926514 4.54160737991333
  batch 20 loss: 1.6241778135299683, 2.9174296855926514, 4.54160737991333
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6595017910003662 3.024540901184082 4.684042930603027
Loss :  1.6752516031265259 2.727227210998535 4.4024786949157715
Loss :  1.646482229232788 2.898792028427124 4.545274257659912
Loss :  1.687609314918518 2.74515438079834 4.432763576507568
Loss :  1.6851204633712769 3.4029834270477295 5.088103771209717
Loss :  1.6487796306610107 2.7985992431640625 4.447379112243652
Loss :  1.6975212097167969 3.4019110202789307 5.099431991577148
Loss :  1.6400275230407715 3.157285213470459 4.7973127365112305
Loss :  1.688110113143921 2.7627017498016357 4.450811862945557
Loss :  1.6417224407196045 2.813917636871338 4.455639839172363
Loss :  1.7229708433151245 3.065067768096924 4.788038730621338
Loss :  1.6690791845321655 3.0855963230133057 4.754675388336182
Loss :  1.6546056270599365 2.8276708126068115 4.482276439666748
Loss :  1.6569042205810547 2.693176507949829 4.350080490112305
Loss :  1.6970150470733643 3.2526280879974365 4.949643135070801
Loss :  1.6888012886047363 2.7925117015838623 4.4813127517700195
Loss :  1.6666415929794312 3.078580379486084 4.745222091674805
Loss :  1.6342172622680664 2.8464748859405518 4.480691909790039
Loss :  1.6597918272018433 2.670452833175659 4.330244541168213
Loss :  1.652398943901062 2.7113583087921143 4.363757133483887
  batch 40 loss: 1.652398943901062, 2.7113583087921143, 4.363757133483887
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6603429317474365 3.2844252586364746 4.944767951965332
Loss :  1.6486337184906006 2.6317245960235596 4.28035831451416
Loss :  1.6656633615493774 2.990078926086426 4.655742168426514
Loss :  1.6640348434448242 3.2442543506622314 4.908288955688477
Loss :  1.6668431758880615 2.9003376960754395 4.567180633544922
Loss :  1.6582326889038086 3.2301418781280518 4.888374328613281
Loss :  1.645250678062439 3.554084539413452 5.199335098266602
Loss :  1.6574153900146484 3.2261767387390137 4.883592128753662
Loss :  1.630272388458252 3.3990352153778076 5.0293073654174805
Loss :  1.6815221309661865 3.523594856262207 5.205117225646973
Loss :  1.6460645198822021 2.930590867996216 4.576655387878418
Loss :  1.6645894050598145 2.8964853286743164 4.561074733734131
Loss :  1.6827548742294312 3.4679248332977295 5.150679588317871
Loss :  1.6676857471466064 3.247065782546997 4.9147515296936035
Loss :  1.670690655708313 3.4190685749053955 5.089759349822998
Loss :  1.635436773300171 2.6697206497192383 4.305157661437988
Loss :  1.6848809719085693 2.977569103240967 4.662449836730957
Loss :  1.6822118759155273 3.248755693435669 4.930967330932617
Loss :  1.69564950466156 2.964324712753296 4.659974098205566
Loss :  1.671172857284546 2.843193531036377 4.514366149902344
  batch 60 loss: 1.671172857284546, 2.843193531036377, 4.514366149902344
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6725733280181885 3.6427223682403564 5.315295696258545
Loss :  1.6694388389587402 2.587202310562134 4.256641387939453
Loss :  1.6775342226028442 2.813764810562134 4.491299152374268
Loss :  1.6580524444580078 2.7292163372039795 4.387269020080566
Loss :  1.655158519744873 2.5139284133911133 4.169086933135986
Loss :  5.535555362701416 4.375925540924072 9.911480903625488
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.42824649810791 4.381044387817383 9.809290885925293
Loss :  5.532972812652588 4.277719974517822 9.81069278717041
Loss :  4.614037990570068 4.301447868347168 8.915485382080078
Total LOSS train 4.624984554144052 valid 9.611737489700317
CE LOSS train 1.66424002464001 valid 1.153509497642517
Contrastive LOSS train 2.960744564349835 valid 1.075361967086792
EPOCH 182:
Loss :  1.6508631706237793 2.8252508640289307 4.476114273071289
Loss :  1.6662580966949463 3.0658013820648193 4.732059478759766
Loss :  1.652262806892395 2.6660208702087402 4.318283557891846
Loss :  1.6556358337402344 2.9532716274261475 4.608907699584961
Loss :  1.6800724267959595 3.0881850719451904 4.7682576179504395
Loss :  1.6633589267730713 2.8166921138763428 4.480051040649414
Loss :  1.6610417366027832 3.229407548904419 4.890449523925781
Loss :  1.6502095460891724 2.3661394119262695 4.016348838806152
Loss :  1.6548118591308594 2.204867124557495 3.8596789836883545
Loss :  1.6090787649154663 2.485424518585205 4.094503402709961
Loss :  1.6689679622650146 2.924755334854126 4.593723297119141
Loss :  1.7293668985366821 3.01434588432312 4.743712902069092
Loss :  1.676094651222229 3.247941493988037 4.924036026000977
Loss :  1.6644803285598755 3.3227486610412598 4.987228870391846
Loss :  1.6429530382156372 2.8932230472564697 4.5361762046813965
Loss :  1.6521393060684204 3.105862617492676 4.758001804351807
Loss :  1.661248803138733 2.9477927684783936 4.609041690826416
Loss :  1.6598913669586182 3.065638542175293 4.725529670715332
Loss :  1.6675426959991455 2.6635820865631104 4.331124782562256
Loss :  1.6241350173950195 3.3688454627990723 4.992980480194092
  batch 20 loss: 1.6241350173950195, 3.3688454627990723, 4.992980480194092
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6594661474227905 3.214433431625366 4.873899459838867
Loss :  1.6752756834030151 2.4490087032318115 4.124284267425537
Loss :  1.6464242935180664 3.233937978744507 4.880362510681152
Loss :  1.687721848487854 2.6951990127563477 4.382920742034912
Loss :  1.6851814985275269 3.5414302349090576 5.226611614227295
Loss :  1.648729920387268 3.1222269535064697 4.770956993103027
Loss :  1.6975215673446655 3.0799219608306885 4.7774434089660645
Loss :  1.6400251388549805 3.1134583950042725 4.753483772277832
Loss :  1.6880991458892822 2.7387471199035645 4.426846504211426
Loss :  1.641724705696106 3.2364180088043213 4.878142833709717
Loss :  1.722979187965393 3.0036823749542236 4.726661682128906
Loss :  1.6691749095916748 2.9657633304595947 4.6349382400512695
Loss :  1.6546138525009155 3.1887433528900146 4.843357086181641
Loss :  1.6568635702133179 2.8403141498565674 4.497177600860596
Loss :  1.6971538066864014 3.3635098934173584 5.06066370010376
Loss :  1.6888129711151123 2.681304454803467 4.3701171875
Loss :  1.6666338443756104 2.9827492237091064 4.649383068084717
Loss :  1.6342087984085083 2.88505220413208 4.519260883331299
Loss :  1.6597992181777954 2.956129312515259 4.615928649902344
Loss :  1.6523152589797974 3.034576416015625 4.686891555786133
  batch 40 loss: 1.6523152589797974, 3.034576416015625, 4.686891555786133
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.660386562347412 3.362457036972046 5.022843360900879
Loss :  1.6486799716949463 2.7967522144317627 4.445432186126709
Loss :  1.6656651496887207 3.3945157527923584 5.0601806640625
Loss :  1.6639901399612427 2.968515634536743 4.632505893707275
Loss :  1.6668530702590942 2.5454657077789307 4.2123188972473145
Loss :  1.6582505702972412 2.8819780349731445 4.540228843688965
Loss :  1.645215630531311 3.159777879714966 4.804993629455566
Loss :  1.6574687957763672 3.1148061752319336 4.772274971008301
Loss :  1.630317211151123 3.6917991638183594 5.322116374969482
Loss :  1.6815440654754639 3.5908801555633545 5.272424221038818
Loss :  1.646034836769104 2.689415693283081 4.335450649261475
Loss :  1.6645731925964355 2.889706611633301 4.554279804229736
Loss :  1.682734727859497 3.26176118850708 4.944496154785156
Loss :  1.6676644086837769 3.1728854179382324 4.840549945831299
Loss :  1.6706868410110474 3.363593101501465 5.034279823303223
Loss :  1.6354453563690186 2.473367929458618 4.108813285827637
Loss :  1.6848434209823608 2.983009099960327 4.667852401733398
Loss :  1.6822426319122314 3.3592960834503174 5.041538715362549
Loss :  1.6956863403320312 3.2908666133880615 4.986553192138672
Loss :  1.671205997467041 2.8014872074127197 4.47269344329834
  batch 60 loss: 1.671205997467041, 2.8014872074127197, 4.47269344329834
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6725831031799316 3.276431083679199 4.949014186859131
Loss :  1.6694542169570923 2.5680880546569824 4.237542152404785
Loss :  1.6775423288345337 3.179532766342163 4.857075214385986
Loss :  1.6580568552017212 2.9670321941375732 4.625089168548584
Loss :  1.6551780700683594 2.7958619594573975 4.451040267944336
Loss :  5.528072357177734 4.440045356750488 9.968117713928223
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.418566703796387 4.3719162940979 9.790483474731445
Loss :  5.5254411697387695 4.286999702453613 9.812440872192383
Loss :  4.602420330047607 4.276228427886963 8.87864875793457
Total LOSS train 4.666725066991953 valid 9.612422704696655
CE LOSS train 1.6642375707626342 valid 1.1506050825119019
Contrastive LOSS train 3.0024874723874606 valid 1.0690571069717407
EPOCH 183:
Loss :  1.6507784128189087 2.617798328399658 4.268576622009277
Loss :  1.666269302368164 3.338566780090332 5.004836082458496
Loss :  1.6522724628448486 2.551506996154785 4.203779220581055
Loss :  1.6556318998336792 2.62796950340271 4.2836012840271
Loss :  1.6801284551620483 2.9598910808563232 4.640019416809082
Loss :  1.6633291244506836 3.077110767364502 4.7404398918151855
Loss :  1.6610468626022339 3.1271615028381348 4.788208484649658
Loss :  1.6502023935317993 2.7729759216308594 4.423178195953369
Loss :  1.6548073291778564 2.1678242683410645 3.822631597518921
Loss :  1.6090819835662842 2.510488986968994 4.119570732116699
Loss :  1.6689298152923584 3.1360929012298584 4.805022716522217
Loss :  1.7293280363082886 3.0402581691741943 4.769586086273193
Loss :  1.6762498617172241 3.1070170402526855 4.783267021179199
Loss :  1.6644654273986816 3.3662846088409424 5.030750274658203
Loss :  1.6429100036621094 3.0601372718811035 4.703047275543213
Loss :  1.6521427631378174 3.1906018257141113 4.842744827270508
Loss :  1.661248803138733 2.9798662662506104 4.641115188598633
Loss :  1.6598479747772217 3.2916204929351807 4.951468467712402
Loss :  1.6676040887832642 2.530940532684326 4.198544502258301
Loss :  1.6241490840911865 2.996260166168213 4.62040901184082
  batch 20 loss: 1.6241490840911865, 2.996260166168213, 4.62040901184082
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6594148874282837 3.15376353263855 4.813178539276123
Loss :  1.675256371498108 2.469625949859619 4.1448822021484375
Loss :  1.646402359008789 3.505444049835205 5.151846408843994
Loss :  1.687653660774231 2.8403334617614746 4.527987003326416
Loss :  1.6850992441177368 3.310178518295288 4.9952778816223145
Loss :  1.6487162113189697 3.1431801319122314 4.791896343231201
Loss :  1.6974507570266724 3.2217249870300293 4.919175624847412
Loss :  1.640014410018921 3.3337175846099854 4.973731994628906
Loss :  1.6881057024002075 2.742978572845459 4.431084156036377
Loss :  1.6416819095611572 3.0865275859832764 4.728209495544434
Loss :  1.7229067087173462 3.1767799854278564 4.899686813354492
Loss :  1.66917884349823 3.2562873363494873 4.925466060638428
Loss :  1.6546127796173096 2.919571876525879 4.574184417724609
Loss :  1.6569286584854126 2.7740278244018555 4.4309563636779785
Loss :  1.6971596479415894 3.298304796218872 4.995464324951172
Loss :  1.688812017440796 3.1043624877929688 4.793174743652344
Loss :  1.6666066646575928 3.2658469676971436 4.932453632354736
Loss :  1.6342257261276245 3.0400660037994385 4.674291610717773
Loss :  1.6597859859466553 2.9459047317504883 4.605690956115723
Loss :  1.652314305305481 3.2839627265930176 4.936276912689209
  batch 40 loss: 1.652314305305481, 3.2839627265930176, 4.936276912689209
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6603641510009766 3.1329638957977295 4.793328285217285
Loss :  1.6485999822616577 2.7845299243927 4.433129787445068
Loss :  1.6656053066253662 3.172795057296753 4.838400363922119
Loss :  1.6639472246170044 2.793848991394043 4.457796096801758
Loss :  1.6668180227279663 2.6523735523223877 4.3191914558410645
Loss :  1.6582469940185547 3.632869005203247 5.291115760803223
Loss :  1.645248293876648 3.19840145111084 4.843649864196777
Loss :  1.6574442386627197 3.429072618484497 5.086516857147217
Loss :  1.6302810907363892 3.5183000564575195 5.148581027984619
Loss :  1.6815537214279175 3.3630871772766113 5.044641017913818
Loss :  1.6460438966751099 3.024879217147827 4.670923233032227
Loss :  1.6645336151123047 2.9104650020599365 4.57499885559082
Loss :  1.6827136278152466 3.2482783794403076 4.930992126464844
Loss :  1.667716145515442 3.2296409606933594 4.897356986999512
Loss :  1.6706714630126953 3.468212842941284 5.138884544372559
Loss :  1.6353839635849 2.7769687175750732 4.412352561950684
Loss :  1.6848334074020386 3.0648086071014404 4.7496418952941895
Loss :  1.68220853805542 3.2853078842163086 4.9675164222717285
Loss :  1.6956363916397095 2.991029977798462 4.686666488647461
Loss :  1.671140432357788 2.8862433433532715 4.5573835372924805
  batch 60 loss: 1.671140432357788, 2.8862433433532715, 4.5573835372924805
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726014614105225 3.6920523643493652 5.364653587341309
Loss :  1.6694037914276123 2.9044415950775146 4.573845386505127
Loss :  1.6776028871536255 2.7868313789367676 4.4644341468811035
Loss :  1.6580586433410645 3.146505117416382 4.804563522338867
Loss :  1.6551787853240967 3.3126001358032227 4.967779159545898
Loss :  5.539272785186768 4.398351192474365 9.937623977661133
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.4281487464904785 4.401498317718506 9.829647064208984
Loss :  5.537718772888184 4.312320232391357 9.850038528442383
Loss :  4.612398147583008 4.293463230133057 8.905860900878906
Total LOSS train 4.721600851645836 valid 9.630792617797852
CE LOSS train 1.664224415559035 valid 1.153099536895752
Contrastive LOSS train 3.0573764580946703 valid 1.0733658075332642
EPOCH 184:
Loss :  1.6508028507232666 2.701676607131958 4.352479457855225
Loss :  1.6663137674331665 3.6465542316436768 5.312868118286133
Loss :  1.6522200107574463 2.680905342102051 4.333125114440918
Loss :  1.6555451154708862 2.886763572692871 4.542308807373047
Loss :  1.680132269859314 3.22768235206604 4.9078145027160645
Loss :  1.6632754802703857 2.7679941654205322 4.431269645690918
Loss :  1.6610208749771118 3.4757027626037598 5.136723518371582
Loss :  1.650191068649292 2.634079694747925 4.284270763397217
Loss :  1.654761791229248 2.460493803024292 4.115255355834961
Loss :  1.6090844869613647 2.9413902759552 4.550474643707275
Loss :  1.6689223051071167 3.219160556793213 4.888082981109619
Loss :  1.7293282747268677 3.2784180641174316 5.00774621963501
Loss :  1.676238775253296 3.00118350982666 4.677422523498535
Loss :  1.664443016052246 3.480588436126709 5.145031452178955
Loss :  1.6429102420806885 2.8958868980407715 4.538797378540039
Loss :  1.652135968208313 3.2445361614227295 4.896672248840332
Loss :  1.6612106561660767 2.887019634246826 4.548230171203613
Loss :  1.6598567962646484 3.101449728012085 4.7613067626953125
Loss :  1.6675266027450562 2.5752928256988525 4.242819309234619
Loss :  1.6241087913513184 3.2872726917266846 4.911381721496582
  batch 20 loss: 1.6241087913513184, 3.2872726917266846, 4.911381721496582
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6594206094741821 3.1555826663970947 4.815003395080566
Loss :  1.6752272844314575 2.8789737224578857 4.554201126098633
Loss :  1.6464362144470215 2.9819531440734863 4.628389358520508
Loss :  1.6876378059387207 2.6061551570892334 4.293792724609375
Loss :  1.6851065158843994 3.496582508087158 5.181689262390137
Loss :  1.6487270593643188 3.0201756954193115 4.66890287399292
Loss :  1.6974762678146362 3.1156396865844727 4.813116073608398
Loss :  1.6400054693222046 3.1488394737243652 4.788845062255859
Loss :  1.6881102323532104 2.829741954803467 4.517852306365967
Loss :  1.6416840553283691 2.811910390853882 4.453594207763672
Loss :  1.7229028940200806 3.577406167984009 5.300309181213379
Loss :  1.669148564338684 3.3170325756073 4.986181259155273
Loss :  1.6546529531478882 3.170600414276123 4.825253486633301
Loss :  1.6569174528121948 2.7826879024505615 4.439605236053467
Loss :  1.6970597505569458 3.1032395362854004 4.800299167633057
Loss :  1.6888309717178345 2.8558952808380127 4.544726371765137
Loss :  1.666601538658142 3.0438942909240723 4.710495948791504
Loss :  1.6342179775238037 2.742920160293579 4.377138137817383
Loss :  1.659830093383789 2.686462879180908 4.346292972564697
Loss :  1.6523765325546265 3.0031704902648926 4.655547142028809
  batch 40 loss: 1.6523765325546265, 3.0031704902648926, 4.655547142028809
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6603724956512451 3.5845820903778076 5.244954586029053
Loss :  1.6486042737960815 2.8830199241638184 4.5316243171691895
Loss :  1.6656062602996826 3.076019525527954 4.741625785827637
Loss :  1.6639347076416016 2.921602964401245 4.585537910461426
Loss :  1.666795253753662 2.5573768615722656 4.224172115325928
Loss :  1.6582175493240356 3.009881019592285 4.668098449707031
Loss :  1.6452312469482422 2.944492816925049 4.589724063873291
Loss :  1.6574238538742065 3.1689884662628174 4.826412200927734
Loss :  1.6302580833435059 3.4306113719940186 5.060869216918945
Loss :  1.6814794540405273 3.3226311206817627 5.004110336303711
Loss :  1.646018147468567 2.807689905166626 4.453708171844482
Loss :  1.6645392179489136 2.9072422981262207 4.571781635284424
Loss :  1.6827222108840942 3.0671632289886475 4.749885559082031
Loss :  1.6676145792007446 3.2298202514648438 4.897434711456299
Loss :  1.670667290687561 3.4478654861450195 5.118532657623291
Loss :  1.6352893114089966 2.9452507495880127 4.580540180206299
Loss :  1.6848225593566895 3.313086748123169 4.9979095458984375
Loss :  1.6822649240493774 3.265911817550659 4.948176860809326
Loss :  1.6957006454467773 3.023529052734375 4.719229698181152
Loss :  1.6711399555206299 2.7802505493164062 4.451390266418457
  batch 60 loss: 1.6711399555206299, 2.7802505493164062, 4.451390266418457
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6725800037384033 3.3655290603637695 5.038108825683594
Loss :  1.6694525480270386 2.734847068786621 4.404299736022949
Loss :  1.6774989366531372 3.2253258228302 4.902824878692627
Loss :  1.6580404043197632 2.8648428916931152 4.522883415222168
Loss :  1.655134916305542 3.083925485610962 4.739060401916504
Loss :  5.527217388153076 4.369567394256592 9.896784782409668
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.414168357849121 4.371325492858887 9.785493850708008
Loss :  5.526848793029785 4.290638446807861 9.817487716674805
Loss :  4.603501319885254 4.210939884185791 8.814441680908203
Total LOSS train 4.705510946420523 valid 9.578552007675171
CE LOSS train 1.6642124341084408 valid 1.1508753299713135
Contrastive LOSS train 3.0412984921382025 valid 1.0527349710464478
EPOCH 185:
Loss :  1.6507617235183716 2.9332666397094727 4.584028244018555
Loss :  1.6662254333496094 3.36024808883667 5.026473522186279
Loss :  1.6521849632263184 2.5173559188842773 4.169540882110596
Loss :  1.6554807424545288 3.0026135444641113 4.65809440612793
Loss :  1.6801269054412842 2.8483493328094482 4.528476238250732
Loss :  1.6632061004638672 2.817990303039551 4.481196403503418
Loss :  1.6610020399093628 3.5721535682678223 5.233155727386475
Loss :  1.6501281261444092 2.7164618968963623 4.3665900230407715
Loss :  1.6547300815582275 2.128783702850342 3.7835137844085693
Loss :  1.609032154083252 2.610621690750122 4.219654083251953
Loss :  1.6688282489776611 3.0068562030792236 4.675684452056885
Loss :  1.7293462753295898 3.221583127975464 4.950929641723633
Loss :  1.6764487028121948 3.1747524738311768 4.851201057434082
Loss :  1.6645526885986328 3.4054715633392334 5.070024490356445
Loss :  1.6428415775299072 2.620630979537964 4.263472557067871
Loss :  1.6521337032318115 2.633200168609619 4.285333633422852
Loss :  1.6611790657043457 2.7576634883880615 4.418842315673828
Loss :  1.6597927808761597 3.158627986907959 4.818420886993408
Loss :  1.6674338579177856 2.96623158454895 4.633665561676025
Loss :  1.6241297721862793 3.0326781272888184 4.656807899475098
  batch 20 loss: 1.6241297721862793, 3.0326781272888184, 4.656807899475098
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6594412326812744 3.383836269378662 5.043277740478516
Loss :  1.675251841545105 2.7957794666290283 4.471031188964844
Loss :  1.6464303731918335 3.311816930770874 4.958247184753418
Loss :  1.687556266784668 3.1275320053100586 4.815088272094727
Loss :  1.6850872039794922 3.3070626258850098 4.992149829864502
Loss :  1.6486948728561401 2.983896493911743 4.632591247558594
Loss :  1.6974655389785767 3.0255391597747803 4.7230048179626465
Loss :  1.6399662494659424 3.0473501682281494 4.687316417694092
Loss :  1.6881368160247803 2.8220086097717285 4.51014518737793
Loss :  1.641679048538208 2.9108128547668457 4.552492141723633
Loss :  1.7228968143463135 3.616333484649658 5.339230537414551
Loss :  1.6691702604293823 3.556194543838501 5.225364685058594
Loss :  1.6545963287353516 3.0100314617156982 4.664628028869629
Loss :  1.656874418258667 2.7992918491363525 4.4561662673950195
Loss :  1.6972395181655884 3.560838222503662 5.258077621459961
Loss :  1.688838005065918 2.831049680709839 4.519887924194336
Loss :  1.6666529178619385 3.080096960067749 4.7467498779296875
Loss :  1.6342533826828003 2.827176094055176 4.461429595947266
Loss :  1.6597994565963745 3.035691499710083 4.695490837097168
Loss :  1.6522992849349976 3.1852071285247803 4.837506294250488
  batch 40 loss: 1.6522992849349976, 3.1852071285247803, 4.837506294250488
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6603883504867554 3.335242986679077 4.995631217956543
Loss :  1.6486148834228516 2.8955507278442383 4.54416561126709
Loss :  1.6655857563018799 3.138014554977417 4.803600311279297
Loss :  1.6639386415481567 2.8719542026519775 4.535892963409424
Loss :  1.6667914390563965 2.962423801422119 4.629215240478516
Loss :  1.6582164764404297 3.0645930767059326 4.722809791564941
Loss :  1.6452369689941406 3.337944984436035 4.983181953430176
Loss :  1.657411813735962 3.4056236743927 5.063035488128662
Loss :  1.6302785873413086 3.664635181427002 5.2949137687683105
Loss :  1.6815038919448853 3.048536777496338 4.730040550231934
Loss :  1.6459646224975586 2.8357598781585693 4.481724739074707
Loss :  1.6645225286483765 2.937514305114746 4.602036952972412
Loss :  1.6826881170272827 3.1861891746520996 4.868877410888672
Loss :  1.6676944494247437 3.2413957118988037 4.909090042114258
Loss :  1.6706746816635132 3.365088939666748 5.035763740539551
Loss :  1.635378360748291 3.1888937950134277 4.824272155761719
Loss :  1.6847847700119019 3.2455976009368896 4.930382251739502
Loss :  1.6822803020477295 3.3527283668518066 5.035008430480957
Loss :  1.6956840753555298 2.9112133979797363 4.606897354125977
Loss :  1.6710760593414307 3.2100377082824707 4.8811140060424805
  batch 60 loss: 1.6710760593414307, 3.2100377082824707, 4.8811140060424805
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726058721542358 3.2184336185455322 4.8910393714904785
Loss :  1.669353723526001 2.814728021621704 4.484081745147705
Loss :  1.6775563955307007 3.052460193634033 4.730016708374023
Loss :  1.6580721139907837 2.8973376750946045 4.555409908294678
Loss :  1.6551806926727295 2.7356655597686768 4.390846252441406
Loss :  5.526161193847656 4.346889019012451 9.873050689697266
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.413492202758789 4.436944484710693 9.85043716430664
Loss :  5.526421546936035 4.24691915512085 9.773340225219727
Loss :  4.601716041564941 4.170465469360352 8.772181510925293
Total LOSS train 4.719908145757822 valid 9.567252397537231
CE LOSS train 1.6642058207438541 valid 1.1504290103912354
Contrastive LOSS train 3.055702304840088 valid 1.042616367340088
EPOCH 186:
Loss :  1.6507381200790405 2.6196365356445312 4.270374774932861
Loss :  1.6662697792053223 3.4917731285095215 5.158042907714844
Loss :  1.6522595882415771 2.949465036392212 4.601724624633789
Loss :  1.6554964780807495 3.469844341278076 5.125340938568115
Loss :  1.68013596534729 3.04162335395813 4.72175931930542
Loss :  1.6631749868392944 2.5792126655578613 4.242387771606445
Loss :  1.661030650138855 3.2771074771881104 4.938138008117676
Loss :  1.6501516103744507 2.6819674968719482 4.332118988037109
Loss :  1.6546720266342163 2.492692708969116 4.147364616394043
Loss :  1.6090914011001587 2.535411834716797 4.144503116607666
Loss :  1.6689070463180542 3.022550344467163 4.691457271575928
Loss :  1.7293394804000854 2.86718487739563 4.596524238586426
Loss :  1.6763801574707031 2.9263317584991455 4.6027116775512695
Loss :  1.6645065546035767 3.356254816055298 5.020761489868164
Loss :  1.6428155899047852 2.8916168212890625 4.534432411193848
Loss :  1.6520850658416748 2.932321786880493 4.584406852722168
Loss :  1.6612327098846436 2.807544231414795 4.468776702880859
Loss :  1.6598410606384277 3.3009355068206787 4.960776329040527
Loss :  1.6674633026123047 2.7988085746765137 4.466271877288818
Loss :  1.6241134405136108 3.0128519535064697 4.636965274810791
  batch 20 loss: 1.6241134405136108, 3.0128519535064697, 4.636965274810791
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6594796180725098 3.0338425636291504 4.69332218170166
Loss :  1.6752371788024902 2.7528226375579834 4.4280595779418945
Loss :  1.6464548110961914 2.986504316329956 4.632959365844727
Loss :  1.68764328956604 2.7465033531188965 4.434146881103516
Loss :  1.6850926876068115 3.533284902572632 5.218377590179443
Loss :  1.6487536430358887 2.9595885276794434 4.608342170715332
Loss :  1.6975195407867432 3.4615771770477295 5.159096717834473
Loss :  1.6400322914123535 2.6542537212371826 4.294285774230957
Loss :  1.6881000995635986 2.673197031021118 4.361297130584717
Loss :  1.6417227983474731 3.1780970096588135 4.819819927215576
Loss :  1.7229031324386597 3.0032896995544434 4.726192951202393
Loss :  1.66914963722229 3.0141186714172363 4.6832685470581055
Loss :  1.6545723676681519 3.138819694519043 4.793392181396484
Loss :  1.6569496393203735 2.99927020072937 4.656219959259033
Loss :  1.6970393657684326 3.250243902206421 4.9472832679748535
Loss :  1.6888128519058228 2.787689447402954 4.476502418518066
Loss :  1.6665960550308228 2.865415096282959 4.532011032104492
Loss :  1.6342021226882935 2.816598415374756 4.45080041885376
Loss :  1.6597661972045898 3.037627935409546 4.697394371032715
Loss :  1.6523102521896362 2.8448522090911865 4.497162342071533
  batch 40 loss: 1.6523102521896362, 2.8448522090911865, 4.497162342071533
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6603021621704102 3.173801898956299 4.834104061126709
Loss :  1.6485209465026855 2.6818833351135254 4.330404281616211
Loss :  1.665549635887146 3.280886650085449 4.946436405181885
Loss :  1.6638675928115845 2.9479105472564697 4.611778259277344
Loss :  1.6666501760482788 3.156860589981079 4.823510646820068
Loss :  1.6581951379776 3.1629443168640137 4.821139335632324
Loss :  1.6451961994171143 3.304346799850464 4.949542999267578
Loss :  1.657295823097229 3.3522565364837646 5.009552478790283
Loss :  1.6302469968795776 3.554753541946411 5.185000419616699
Loss :  1.6813678741455078 2.7696406841278076 4.4510087966918945
Loss :  1.646043300628662 2.535374641418457 4.181417942047119
Loss :  1.664416790008545 2.8332982063293457 4.497714996337891
Loss :  1.682673692703247 3.4636480808258057 5.146321773529053
Loss :  1.6675710678100586 2.956857919692993 4.624428749084473
Loss :  1.6706424951553345 3.5432629585266113 5.213905334472656
Loss :  1.63533353805542 2.892036199569702 4.527369499206543
Loss :  1.6848034858703613 3.3646063804626465 5.049409866333008
Loss :  1.6821770668029785 3.1817843914031982 4.863961219787598
Loss :  1.6956713199615479 3.1742331981658936 4.869904518127441
Loss :  1.6710765361785889 2.7872276306152344 4.458304405212402
  batch 60 loss: 1.6710765361785889, 2.7872276306152344, 4.458304405212402
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6725678443908691 3.4825823307037354 5.155150413513184
Loss :  1.669326901435852 2.6885592937469482 4.35788631439209
Loss :  1.6774742603302002 2.911219358444214 4.588693618774414
Loss :  1.6580886840820312 2.953007698059082 4.611096382141113
Loss :  1.655151128768921 2.8965604305267334 4.551711559295654
Loss :  5.532327175140381 4.342838764190674 9.875165939331055
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.422576427459717 4.298075199127197 9.720651626586914
Loss :  5.532761573791504 4.299779891967773 9.832541465759277
Loss :  4.608845233917236 4.251742362976074 8.860588073730469
Total LOSS train 4.677146588839017 valid 9.572236776351929
CE LOSS train 1.6641885115550115 valid 1.152211308479309
Contrastive LOSS train 3.012958082785973 valid 1.0629355907440186
EPOCH 187:
Loss :  1.6507762670516968 2.436598777770996 4.087375164031982
Loss :  1.6661946773529053 3.4398207664489746 5.106015205383301
Loss :  1.6521811485290527 3.025089979171753 4.677270889282227
Loss :  1.6554709672927856 2.839268684387207 4.494739532470703
Loss :  1.680101752281189 3.067706823348999 4.747808456420898
Loss :  1.6631273031234741 2.83219838142395 4.495325565338135
Loss :  1.6610301733016968 3.318911552429199 4.9799418449401855
Loss :  1.6501309871673584 2.6998376846313477 4.349968910217285
Loss :  1.6547389030456543 2.366018056869507 4.020756721496582
Loss :  1.609083890914917 2.6886610984802246 4.2977447509765625
Loss :  1.6688635349273682 3.4041478633880615 5.07301139831543
Loss :  1.7292872667312622 2.9900622367858887 4.719349384307861
Loss :  1.676228642463684 3.127002716064453 4.803231239318848
Loss :  1.6645004749298096 3.330385446548462 4.9948859214782715
Loss :  1.642777681350708 3.123950481414795 4.766728401184082
Loss :  1.6521193981170654 2.9657557010650635 4.617875099182129
Loss :  1.6611953973770142 3.0227456092834473 4.683940887451172
Loss :  1.6598862409591675 2.664811611175537 4.324697971343994
Loss :  1.667415738105774 2.7663681507110596 4.433784008026123
Loss :  1.624097466468811 3.4458909034729004 5.069988250732422
  batch 20 loss: 1.624097466468811, 3.4458909034729004, 5.069988250732422
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6593925952911377 3.0966272354125977 4.756019592285156
Loss :  1.6752241849899292 2.59663987159729 4.27186393737793
Loss :  1.646433711051941 2.916731834411621 4.563165664672852
Loss :  1.6875598430633545 2.923328161239624 4.6108880043029785
Loss :  1.6850452423095703 3.190847635269165 4.875892639160156
Loss :  1.6486622095108032 2.959726572036743 4.608388900756836
Loss :  1.6974703073501587 3.088810443878174 4.786280632019043
Loss :  1.6399668455123901 2.7796311378479004 4.41959810256958
Loss :  1.6881242990493774 2.9450924396514893 4.633216857910156
Loss :  1.6416256427764893 2.742561101913452 4.384186744689941
Loss :  1.7228552103042603 3.223548650741577 4.946403980255127
Loss :  1.669218897819519 3.1357343196868896 4.804953098297119
Loss :  1.6545771360397339 3.0260090827941895 4.680586338043213
Loss :  1.656894564628601 2.843482732772827 4.500377178192139
Loss :  1.6970574855804443 3.438394069671631 5.135451316833496
Loss :  1.6888415813446045 3.014228582382202 4.703070163726807
Loss :  1.6666346788406372 3.0694193840026855 4.736053943634033
Loss :  1.6342638731002808 2.8761098384857178 4.510373592376709
Loss :  1.659814715385437 2.822185754776001 4.482000350952148
Loss :  1.6523497104644775 2.7402470111846924 4.39259672164917
  batch 40 loss: 1.6523497104644775, 2.7402470111846924, 4.39259672164917
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.660364031791687 3.41083025932312 5.071194171905518
Loss :  1.64858877658844 2.703129768371582 4.351718425750732
Loss :  1.6655218601226807 2.744033098220825 4.409554958343506
Loss :  1.6640053987503052 2.9377527236938477 4.601758003234863
Loss :  1.6667187213897705 2.9248220920562744 4.591540813446045
Loss :  1.6581835746765137 3.033381462097168 4.691565036773682
Loss :  1.6452897787094116 3.0460081100463867 4.691298007965088
Loss :  1.657389760017395 3.358931064605713 5.016320705413818
Loss :  1.6302894353866577 3.5064430236816406 5.136732578277588
Loss :  1.681485652923584 3.1395790576934814 4.8210649490356445
Loss :  1.6459287405014038 2.9354491233825684 4.581377983093262
Loss :  1.6644799709320068 2.9289135932922363 4.593393325805664
Loss :  1.6827222108840942 3.4789302349090576 5.161652565002441
Loss :  1.6676281690597534 3.4282913208007812 5.095919609069824
Loss :  1.6707286834716797 3.525230646133423 5.195959091186523
Loss :  1.6351993083953857 3.039778709411621 4.674978256225586
Loss :  1.6847914457321167 3.36810564994812 5.052896976470947
Loss :  1.6822737455368042 3.4002318382263184 5.082505702972412
Loss :  1.6957024335861206 3.2548444271087646 4.950546741485596
Loss :  1.671065330505371 3.014709711074829 4.685774803161621
  batch 60 loss: 1.671065330505371, 3.014709711074829, 4.685774803161621
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726022958755493 3.402581214904785 5.075183391571045
Loss :  1.6694663763046265 2.995041847229004 4.66450834274292
Loss :  1.6775834560394287 2.8977673053741455 4.575350761413574
Loss :  1.6581034660339355 2.826573133468628 4.484676361083984
Loss :  1.6551975011825562 3.133425235748291 4.788622856140137
Loss :  5.536199569702148 4.340095043182373 9.87629508972168
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.426776885986328 4.31782341003418 9.744600296020508
Loss :  5.5384297370910645 4.206176280975342 9.744606018066406
Loss :  4.60777473449707 4.195851802825928 8.803627014160156
Total LOSS train 4.701413873525766 valid 9.542282104492188
CE LOSS train 1.6641927810815664 valid 1.1519436836242676
Contrastive LOSS train 3.037221123622014 valid 1.048962950706482
EPOCH 188:
Loss :  1.6507502794265747 2.731585741043091 4.382336139678955
Loss :  1.666240930557251 3.3034005165100098 4.96964168548584
Loss :  1.652269721031189 2.782616376876831 4.4348859786987305
Loss :  1.6555240154266357 3.282569408416748 4.938093185424805
Loss :  1.6801403760910034 2.8850257396698 4.565165996551514
Loss :  1.66318941116333 3.090766191482544 4.753955841064453
Loss :  1.6610310077667236 3.185892343521118 4.846923351287842
Loss :  1.6501622200012207 2.731234550476074 4.381396770477295
Loss :  1.6547456979751587 2.5653631687164307 4.220108985900879
Loss :  1.6090847253799438 2.864048719406128 4.473133563995361
Loss :  1.6688889265060425 3.247462749481201 4.916351795196533
Loss :  1.7293213605880737 2.944187879562378 4.673509120941162
Loss :  1.6763463020324707 3.270272970199585 4.946619033813477
Loss :  1.664454698562622 3.1952898502349854 4.859744548797607
Loss :  1.6427483558654785 2.800631046295166 4.4433794021606445
Loss :  1.652194857597351 2.998112201690674 4.6503071784973145
Loss :  1.6612586975097656 2.8960695266723633 4.557328224182129
Loss :  1.6598107814788818 3.122178077697754 4.781989097595215
Loss :  1.6675820350646973 2.795698642730713 4.46328067779541
Loss :  1.6241036653518677 3.2750532627105713 4.8991570472717285
  batch 20 loss: 1.6241036653518677, 3.2750532627105713, 4.8991570472717285
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.659327745437622 3.096729040145874 4.756056785583496
Loss :  1.6752022504806519 2.953617811203003 4.628819942474365
Loss :  1.6463990211486816 3.0763027667999268 4.7227020263671875
Loss :  1.6875789165496826 3.164116859436035 4.851696014404297
Loss :  1.6850230693817139 3.5046610832214355 5.18968391418457
Loss :  1.648669719696045 3.081854820251465 4.73052453994751
Loss :  1.6974831819534302 3.4067978858947754 5.104280948638916
Loss :  1.639984369277954 2.9994893074035645 4.639473915100098
Loss :  1.688156008720398 2.6914398670196533 4.379595756530762
Loss :  1.6416494846343994 3.1501049995422363 4.791754722595215
Loss :  1.7228633165359497 3.056021213531494 4.778884410858154
Loss :  1.6692142486572266 3.1857292652130127 4.85494327545166
Loss :  1.6546324491500854 3.010728597640991 4.665360927581787
Loss :  1.6569626331329346 2.7574589252471924 4.414421558380127
Loss :  1.6970763206481934 3.4440064430236816 5.141082763671875
Loss :  1.6889244318008423 2.8128721714019775 4.501796722412109
Loss :  1.6666231155395508 3.24050235748291 4.907125473022461
Loss :  1.6343828439712524 2.8807473182678223 4.515130043029785
Loss :  1.6597959995269775 2.9788742065429688 4.638669967651367
Loss :  1.6523261070251465 3.194234609603882 4.846560478210449
  batch 40 loss: 1.6523261070251465, 3.194234609603882, 4.846560478210449
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.660379409790039 3.283552885055542 4.94393253326416
Loss :  1.64866042137146 2.750889301300049 4.39954948425293
Loss :  1.6656259298324585 2.856682062149048 4.522307872772217
Loss :  1.6640158891677856 2.7701406478881836 4.43415641784668
Loss :  1.6668072938919067 2.5706095695495605 4.237416744232178
Loss :  1.658259630203247 2.863402843475342 4.521662712097168
Loss :  1.6453763246536255 3.009185791015625 4.654561996459961
Loss :  1.6574352979660034 3.289372682571411 4.946807861328125
Loss :  1.6303386688232422 3.4799106121063232 5.1102495193481445
Loss :  1.6815439462661743 3.187880754470825 4.869424819946289
Loss :  1.645926833152771 2.7402913570404053 4.386218070983887
Loss :  1.6644881963729858 2.977301836013794 4.64178991317749
Loss :  1.6826856136322021 3.1860995292663574 4.8687849044799805
Loss :  1.667714238166809 3.0586283206939697 4.726342678070068
Loss :  1.670734167098999 3.5372891426086426 5.2080230712890625
Loss :  1.6352618932724 3.1469242572784424 4.782186031341553
Loss :  1.6847950220108032 3.1673736572265625 4.852168560028076
Loss :  1.682242512702942 3.370912790298462 5.053155422210693
Loss :  1.6956958770751953 3.0025858879089355 4.698281764984131
Loss :  1.6711015701293945 3.2439262866973877 4.915027618408203
  batch 60 loss: 1.6711015701293945, 3.2439262866973877, 4.915027618408203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.67264723777771 3.564301013946533 5.236948013305664
Loss :  1.6694895029067993 2.8479580879211426 4.517447471618652
Loss :  1.6775784492492676 3.1566860675811768 4.834264755249023
Loss :  1.6581645011901855 2.728827714920044 4.386992454528809
Loss :  1.6551340818405151 2.951939821243286 4.607073783874512
Loss :  5.548277378082275 4.413523197174072 9.961800575256348
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.436944484710693 4.363337516784668 9.800281524658203
Loss :  5.5474748611450195 4.148367881774902 9.695842742919922
Loss :  4.622382640838623 4.301024436950684 8.923406600952148
Total LOSS train 4.716471481323242 valid 9.595332860946655
CE LOSS train 1.664218858572153 valid 1.1555956602096558
Contrastive LOSS train 3.0522526374230017 valid 1.075256109237671
EPOCH 189:
Loss :  1.6508289575576782 2.4283840656280518 4.0792131423950195
Loss :  1.6663124561309814 3.263990879058838 4.930303573608398
Loss :  1.652266502380371 2.475883722305298 4.12814998626709
Loss :  1.6556159257888794 2.9956183433532715 4.651234149932861
Loss :  1.6800943613052368 3.2041268348693848 4.884221076965332
Loss :  1.663209080696106 2.763909101486206 4.427118301391602
Loss :  1.6610506772994995 2.957188129425049 4.618238925933838
Loss :  1.6501739025115967 2.6045913696289062 4.254765510559082
Loss :  1.654780626296997 2.42055344581604 4.075334072113037
Loss :  1.6090589761734009 2.7135000228881836 4.322558879852295
Loss :  1.6688854694366455 2.9377012252807617 4.606586456298828
Loss :  1.7294127941131592 2.9657068252563477 4.695119857788086
Loss :  1.6764495372772217 2.993009090423584 4.669458389282227
Loss :  1.664419174194336 3.307084083557129 4.971503257751465
Loss :  1.642782211303711 3.1624090671539307 4.8051910400390625
Loss :  1.6521730422973633 2.928205728530884 4.580378532409668
Loss :  1.6612626314163208 2.51362943649292 4.174891948699951
Loss :  1.6599066257476807 2.914363384246826 4.574270248413086
Loss :  1.6675747632980347 2.6313047409057617 4.298879623413086
Loss :  1.6241711378097534 3.2708280086517334 4.894999027252197
  batch 20 loss: 1.6241711378097534, 3.2708280086517334, 4.894999027252197
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6593714952468872 3.0768356323242188 4.736207008361816
Loss :  1.6752197742462158 2.549497127532959 4.224717140197754
Loss :  1.646403193473816 3.1833086013793945 4.8297119140625
Loss :  1.687624454498291 3.068713426589966 4.756338119506836
Loss :  1.6849615573883057 3.4321699142456055 5.117131233215332
Loss :  1.648634910583496 3.2438244819641113 4.892459392547607
Loss :  1.6975089311599731 3.070249080657959 4.767757892608643
Loss :  1.6400002241134644 2.8627829551696777 4.502783298492432
Loss :  1.6881698369979858 2.6347901821136475 4.322959899902344
Loss :  1.6415679454803467 3.2025113105773926 4.84407901763916
Loss :  1.7228517532348633 3.186911106109619 4.909762859344482
Loss :  1.669264316558838 3.2298548221588135 4.8991193771362305
Loss :  1.6545848846435547 3.3487472534179688 5.003332138061523
Loss :  1.6568772792816162 2.8674275875091553 4.5243048667907715
Loss :  1.6970455646514893 3.222525119781494 4.9195709228515625
Loss :  1.6889503002166748 2.880953550338745 4.56990385055542
Loss :  1.6666289567947388 3.161773681640625 4.828402519226074
Loss :  1.6343584060668945 3.0491340160369873 4.683492660522461
Loss :  1.6597685813903809 2.7929675579071045 4.452735900878906
Loss :  1.6522750854492188 3.126999855041504 4.779274940490723
  batch 40 loss: 1.6522750854492188, 3.126999855041504, 4.779274940490723
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6603872776031494 3.6715962886810303 5.33198356628418
Loss :  1.6486735343933105 2.615919589996338 4.264593124389648
Loss :  1.6655522584915161 3.0745184421539307 4.740070819854736
Loss :  1.6640561819076538 2.781712532043457 4.4457688331604
Loss :  1.666861653327942 2.8159937858581543 4.482855319976807
Loss :  1.6582038402557373 3.0403597354888916 4.698563575744629
Loss :  1.6454054117202759 3.199012517929077 4.844418048858643
Loss :  1.6575078964233398 3.249988079071045 4.907495975494385
Loss :  1.6302268505096436 3.285568952560425 4.915795803070068
Loss :  1.6814641952514648 3.1013200283050537 4.782784461975098
Loss :  1.645929217338562 2.738201856613159 4.384130954742432
Loss :  1.664428472518921 2.893974542617798 4.558403015136719
Loss :  1.6827192306518555 3.6347928047180176 5.317512035369873
Loss :  1.6678053140640259 2.949031114578247 4.6168365478515625
Loss :  1.6706956624984741 3.5243635177612305 5.195059299468994
Loss :  1.6352860927581787 2.633366823196411 4.26865291595459
Loss :  1.684781551361084 3.409085512161255 5.093867301940918
Loss :  1.6822599172592163 3.336484670639038 5.018744468688965
Loss :  1.6956415176391602 3.0514345169067383 4.747076034545898
Loss :  1.671068549156189 2.871184825897217 4.542253494262695
  batch 60 loss: 1.671068549156189, 2.871184825897217, 4.542253494262695
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726300716400146 3.530026912689209 5.2026567459106445
Loss :  1.669525384902954 2.7621405124664307 4.431665897369385
Loss :  1.677596926689148 2.9750823974609375 4.652679443359375
Loss :  1.6580674648284912 3.104480743408203 4.762548446655273
Loss :  1.6551485061645508 2.7226693630218506 4.3778181076049805
Loss :  5.5441575050354 4.45289421081543 9.997051239013672
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.433228015899658 4.348031520843506 9.781259536743164
Loss :  5.543044567108154 4.209089756011963 9.752134323120117
Loss :  4.624774932861328 4.188572406768799 8.813346862792969
Total LOSS train 4.673703002929687 valid 9.58594799041748
CE LOSS train 1.6642218351364135 valid 1.156193733215332
Contrastive LOSS train 3.0094811512873725 valid 1.0471431016921997
EPOCH 190:
Loss :  1.6508134603500366 2.9782307147979736 4.629044055938721
Loss :  1.666303277015686 3.2772440910339355 4.943547248840332
Loss :  1.6523170471191406 2.898406505584717 4.550723552703857
Loss :  1.6556254625320435 3.213141679763794 4.868767261505127
Loss :  1.6800719499588013 3.39204478263855 5.072116851806641
Loss :  1.6632139682769775 2.5871667861938477 4.250380516052246
Loss :  1.6609654426574707 3.233964204788208 4.894929885864258
Loss :  1.6501556634902954 2.663027286529541 4.313182830810547
Loss :  1.6547999382019043 2.6410861015319824 4.295886039733887
Loss :  1.609037160873413 2.5785980224609375 4.18763542175293
Loss :  1.6689358949661255 3.5867180824279785 5.2556538581848145
Loss :  1.729339838027954 3.0619149208068848 4.791254997253418
Loss :  1.6765515804290771 3.3696227073669434 5.046174049377441
Loss :  1.6644278764724731 3.5289206504821777 5.193348407745361
Loss :  1.642719030380249 3.0102102756500244 4.652929306030273
Loss :  1.6522414684295654 3.2795069217681885 4.931748390197754
Loss :  1.6612496376037598 2.983220100402832 4.644469738006592
Loss :  1.659922480583191 2.9381253719329834 4.598047733306885
Loss :  1.6676477193832397 2.608518123626709 4.276165962219238
Loss :  1.6240742206573486 3.163400411605835 4.787474632263184
  batch 20 loss: 1.6240742206573486, 3.163400411605835, 4.787474632263184
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6593130826950073 3.125751256942749 4.785064220428467
Loss :  1.675213098526001 2.484232187271118 4.159445285797119
Loss :  1.6464130878448486 2.4860053062438965 4.132418632507324
Loss :  1.6876890659332275 3.1646957397460938 4.852384567260742
Loss :  1.6850048303604126 3.533535957336426 5.218540668487549
Loss :  1.6485952138900757 3.1317546367645264 4.7803497314453125
Loss :  1.6975170373916626 3.0736896991729736 4.771206855773926
Loss :  1.6399636268615723 2.9681503772735596 4.608114242553711
Loss :  1.6881067752838135 2.963991641998291 4.652098655700684
Loss :  1.6415939331054688 2.9951884746551514 4.636782646179199
Loss :  1.7228041887283325 3.1145095825195312 4.837313652038574
Loss :  1.6692626476287842 3.2759599685668945 4.945222854614258
Loss :  1.6545684337615967 2.8103573322296143 4.464925765991211
Loss :  1.6568849086761475 2.603672742843628 4.260557651519775
Loss :  1.697013020515442 3.215080499649048 4.912093639373779
Loss :  1.6888914108276367 2.905864953994751 4.594756126403809
Loss :  1.6665925979614258 2.9689292907714844 4.63552188873291
Loss :  1.6343860626220703 2.533301830291748 4.167687892913818
Loss :  1.6597330570220947 3.143381357192993 4.803114414215088
Loss :  1.6522793769836426 2.915950298309326 4.568229675292969
  batch 40 loss: 1.6522793769836426, 2.915950298309326, 4.568229675292969
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.66033136844635 3.443125009536743 5.103456497192383
Loss :  1.648612380027771 2.7469823360443115 4.395594596862793
Loss :  1.6656265258789062 3.148359537124634 4.813985824584961
Loss :  1.6640779972076416 2.804419994354248 4.468498229980469
Loss :  1.6668691635131836 2.7546584606170654 4.421527862548828
Loss :  1.6582188606262207 2.9031283855438232 4.561347007751465
Loss :  1.6453332901000977 3.239786386489868 4.885119438171387
Loss :  1.6575597524642944 3.0517334938049316 4.709293365478516
Loss :  1.6301782131195068 3.212628126144409 4.842806339263916
Loss :  1.6814895868301392 3.195817470550537 4.877306938171387
Loss :  1.645928978919983 2.5784823894500732 4.224411487579346
Loss :  1.6644407510757446 2.979818820953369 4.644259452819824
Loss :  1.6827397346496582 3.2340147495269775 4.916754722595215
Loss :  1.667805552482605 2.9548609256744385 4.622666358947754
Loss :  1.6707124710083008 3.3452889919281006 5.0160017013549805
Loss :  1.6352616548538208 2.917444944381714 4.552706718444824
Loss :  1.6847915649414062 3.355787515640259 5.040578842163086
Loss :  1.6822417974472046 3.1616134643554688 4.843855381011963
Loss :  1.6956267356872559 3.100043535232544 4.795670509338379
Loss :  1.6710680723190308 2.675251007080078 4.346319198608398
  batch 60 loss: 1.6710680723190308, 2.675251007080078, 4.346319198608398
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726094484329224 3.1420609951019287 4.814670562744141
Loss :  1.6695208549499512 2.6802361011505127 4.349757194519043
Loss :  1.677590012550354 3.1861979961395264 4.86378812789917
Loss :  1.658064365386963 2.7435824871063232 4.401646614074707
Loss :  1.6551109552383423 3.1039488315582275 4.759059906005859
Loss :  5.539120674133301 4.385765075683594 9.924885749816895
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.427764415740967 4.3975911140441895 9.825355529785156
Loss :  5.535125255584717 4.263449192047119 9.798574447631836
Loss :  4.61826229095459 4.342966079711914 8.961228370666504
Total LOSS train 4.680621425922101 valid 9.627511024475098
CE LOSS train 1.664216133264395 valid 1.1545655727386475
Contrastive LOSS train 3.016405274317815 valid 1.0857415199279785
EPOCH 191:
Loss :  1.6508400440216064 2.795603036880493 4.4464430809021
Loss :  1.6663187742233276 3.1674532890319824 4.8337721824646
Loss :  1.6522637605667114 2.7868831157684326 4.439146995544434
Loss :  1.6556611061096191 2.7344162464141846 4.390077590942383
Loss :  1.6801625490188599 2.906036615371704 4.5861992835998535
Loss :  1.6632453203201294 2.805316209793091 4.46856164932251
Loss :  1.6610134840011597 3.2492635250091553 4.910276889801025
Loss :  1.650132179260254 2.3575990200042725 4.0077314376831055
Loss :  1.6547496318817139 2.384185314178467 4.038934707641602
Loss :  1.6090508699417114 2.572901725769043 4.181952476501465
Loss :  1.6689324378967285 3.4727892875671387 5.141721725463867
Loss :  1.7294087409973145 2.8592958450317383 4.588704586029053
Loss :  1.676393747329712 3.12957763671875 4.805971145629883
Loss :  1.6644437313079834 3.210894823074341 4.875338554382324
Loss :  1.642727255821228 2.9046683311462402 4.547395706176758
Loss :  1.6522550582885742 3.111877679824829 4.764132499694824
Loss :  1.6612685918807983 2.839019775390625 4.500288486480713
Loss :  1.6599574089050293 2.7970123291015625 4.456969738006592
Loss :  1.6676486730575562 3.1396420001983643 4.807290554046631
Loss :  1.6240735054016113 3.043334484100342 4.667407989501953
  batch 20 loss: 1.6240735054016113, 3.043334484100342, 4.667407989501953
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592786312103271 3.2319133281707764 4.8911919593811035
Loss :  1.6752039194107056 2.920001745223999 4.595205783843994
Loss :  1.64643394947052 2.9178993701934814 4.564333438873291
Loss :  1.6877503395080566 3.0801925659179688 4.767942905426025
Loss :  1.6850237846374512 3.4083235263824463 5.093347549438477
Loss :  1.6485778093338013 3.243912696838379 4.892490386962891
Loss :  1.6975668668746948 3.181140422821045 4.878707408905029
Loss :  1.639958143234253 3.2186882495880127 4.858646392822266
Loss :  1.6881238222122192 2.7469613552093506 4.435085296630859
Loss :  1.6416274309158325 2.835841178894043 4.477468490600586
Loss :  1.7228302955627441 3.376218557357788 5.099048614501953
Loss :  1.6692456007003784 3.2188782691955566 4.888123989105225
Loss :  1.654584288597107 2.8239333629608154 4.478517532348633
Loss :  1.6569064855575562 3.19970965385437 4.856616020202637
Loss :  1.6970133781433105 3.209038019180298 4.9060516357421875
Loss :  1.6889153718948364 2.963587999343872 4.652503490447998
Loss :  1.6666239500045776 2.9969584941864014 4.6635823249816895
Loss :  1.6343748569488525 2.555832862854004 4.190207481384277
Loss :  1.659737467765808 3.017165184020996 4.676902770996094
Loss :  1.652335286140442 3.179793119430542 4.832128524780273
  batch 40 loss: 1.652335286140442, 3.179793119430542, 4.832128524780273
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6603620052337646 3.4897842407226562 5.150146484375
Loss :  1.6486482620239258 2.688868761062622 4.337516784667969
Loss :  1.6656553745269775 3.246323347091675 4.911978721618652
Loss :  1.664137363433838 3.0907320976257324 4.75486946105957
Loss :  1.6668877601623535 2.798724412918091 4.465612411499023
Loss :  1.6582155227661133 3.2621307373046875 4.920346260070801
Loss :  1.6453508138656616 3.395441770553589 5.040792465209961
Loss :  1.6575305461883545 3.0798263549804688 4.737357139587402
Loss :  1.630207896232605 3.029020071029663 4.6592278480529785
Loss :  1.6814911365509033 3.4372143745422363 5.118705749511719
Loss :  1.6459236145019531 2.756955623626709 4.402879238128662
Loss :  1.6644456386566162 2.93093204498291 4.5953779220581055
Loss :  1.6827611923217773 3.393145799636841 5.075906753540039
Loss :  1.667733907699585 2.9641330242156982 4.631866931915283
Loss :  1.6707038879394531 3.4697301387786865 5.140434265136719
Loss :  1.635166883468628 3.1575169563293457 4.7926836013793945
Loss :  1.6847484111785889 3.404284715652466 5.089033126831055
Loss :  1.6822457313537598 3.346297025680542 5.028542518615723
Loss :  1.6956738233566284 3.133124351501465 4.828798294067383
Loss :  1.67097806930542 3.156452178955078 4.827430248260498
  batch 60 loss: 1.67097806930542, 3.156452178955078, 4.827430248260498
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726288795471191 3.174849033355713 4.847477912902832
Loss :  1.6695421934127808 2.7326345443725586 4.402176856994629
Loss :  1.67764413356781 3.3752083778381348 5.052852630615234
Loss :  1.6580828428268433 2.8383004665374756 4.496383190155029
Loss :  1.6551100015640259 3.3181817531585693 4.973291873931885
Loss :  5.532243728637695 4.353349208831787 9.88559341430664
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.420660495758057 4.372337818145752 9.792998313903809
Loss :  5.528651237487793 4.189578533172607 9.718229293823242
Loss :  4.613059043884277 4.154256343841553 8.767314910888672
Total LOSS train 4.714432430267334 valid 9.54103398323059
CE LOSS train 1.6642236067698553 valid 1.1532647609710693
Contrastive LOSS train 3.050208806991577 valid 1.0385640859603882
EPOCH 192:
Loss :  1.6507937908172607 2.5928568840026855 4.243650436401367
Loss :  1.6663378477096558 3.3958563804626465 5.062194347381592
Loss :  1.6522504091262817 2.6735284328460693 4.325778961181641
Loss :  1.6556627750396729 2.8077845573425293 4.463447570800781
Loss :  1.680130124092102 3.2181859016418457 4.898315906524658
Loss :  1.6632169485092163 2.819999933242798 4.483216762542725
Loss :  1.6609851121902466 3.0803580284118652 4.741343021392822
Loss :  1.6501264572143555 2.6741936206817627 4.324319839477539
Loss :  1.654748558998108 2.5175790786743164 4.172327518463135
Loss :  1.6090316772460938 2.715118646621704 4.324150085449219
Loss :  1.6688356399536133 3.094552755355835 4.763388633728027
Loss :  1.7294203042984009 3.1713180541992188 4.90073823928833
Loss :  1.676313042640686 3.318112373352051 4.994425296783447
Loss :  1.6645005941390991 3.2112479209899902 4.875748634338379
Loss :  1.6427830457687378 2.9052460193634033 4.548028945922852
Loss :  1.652186632156372 2.867231845855713 4.519418716430664
Loss :  1.661186695098877 2.713688373565674 4.374875068664551
Loss :  1.6597096920013428 3.0014777183532715 4.661187171936035
Loss :  1.6674952507019043 2.5187313556671143 4.186226844787598
Loss :  1.62407386302948 2.6625235080718994 4.28659725189209
  batch 20 loss: 1.62407386302948, 2.6625235080718994, 4.28659725189209
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.659367322921753 3.1817986965179443 4.841166019439697
Loss :  1.6751787662506104 2.710042715072632 4.385221481323242
Loss :  1.6464711427688599 3.1522216796875 4.79869270324707
Loss :  1.6876273155212402 2.9486536979675293 4.6362810134887695
Loss :  1.6849784851074219 3.3107004165649414 4.995678901672363
Loss :  1.6486362218856812 3.0189309120178223 4.667567253112793
Loss :  1.6975311040878296 3.3446097373962402 5.042140960693359
Loss :  1.639874815940857 2.84274959564209 4.482624530792236
Loss :  1.6880906820297241 2.667799472808838 4.355890274047852
Loss :  1.641585350036621 2.821715831756592 4.463301181793213
Loss :  1.722841739654541 3.2101919651031494 4.9330339431762695
Loss :  1.6692125797271729 3.3669042587280273 5.036116600036621
Loss :  1.6544564962387085 3.041792154312134 4.696248531341553
Loss :  1.6569087505340576 2.819802761077881 4.476711273193359
Loss :  1.6970134973526 3.5447616577148438 5.241775035858154
Loss :  1.6888500452041626 2.993770122528076 4.682620048522949
Loss :  1.6666007041931152 3.3008110523223877 4.967411994934082
Loss :  1.6343388557434082 2.7561886310577393 4.390527725219727
Loss :  1.6596919298171997 3.067190408706665 4.726882457733154
Loss :  1.6523008346557617 3.200634717941284 4.852935791015625
  batch 40 loss: 1.6523008346557617, 3.200634717941284, 4.852935791015625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6603105068206787 3.284508228302002 4.944818496704102
Loss :  1.6485971212387085 2.8037681579589844 4.452365398406982
Loss :  1.665571689605713 3.4461629390716553 5.111734390258789
Loss :  1.6639512777328491 2.820178270339966 4.484129428863525
Loss :  1.6667982339859009 3.240436315536499 4.9072346687316895
Loss :  1.6581815481185913 3.029625654220581 4.687807083129883
Loss :  1.6452850103378296 3.304831027984619 4.950116157531738
Loss :  1.6573740243911743 3.425875663757324 5.083249568939209
Loss :  1.6301792860031128 3.38454270362854 5.014721870422363
Loss :  1.681377649307251 3.1199262142181396 4.801303863525391
Loss :  1.6458886861801147 2.8133962154388428 4.459284782409668
Loss :  1.6643978357315063 3.005549907684326 4.669947624206543
Loss :  1.6826701164245605 3.210627794265747 4.893298149108887
Loss :  1.667641043663025 2.861708164215088 4.529349327087402
Loss :  1.670664668083191 3.4120147228240967 5.082679271697998
Loss :  1.635251522064209 2.836984395980835 4.472235679626465
Loss :  1.684721827507019 3.266432762145996 4.951154708862305
Loss :  1.6821550130844116 3.304579496383667 4.986734390258789
Loss :  1.6956357955932617 3.0315637588500977 4.727199554443359
Loss :  1.6710011959075928 3.0414950847625732 4.712496280670166
  batch 60 loss: 1.6710011959075928, 3.0414950847625732, 4.712496280670166
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726181507110596 3.4670982360839844 5.139716148376465
Loss :  1.6694612503051758 2.8303792476654053 4.49984073638916
Loss :  1.6775552034378052 3.2161710262298584 4.893726348876953
Loss :  1.658125877380371 2.6460118293762207 4.304137706756592
Loss :  1.6551094055175781 2.725081443786621 4.380190849304199
Loss :  5.524567604064941 4.407861709594727 9.932429313659668
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.414536476135254 4.3799614906311035 9.794498443603516
Loss :  5.523833751678467 4.2584404945373535 9.78227424621582
Loss :  4.605906009674072 4.372434616088867 8.978340148925781
Total LOSS train 4.691718145517203 valid 9.621885538101196
CE LOSS train 1.6641826006082387 valid 1.151476502418518
Contrastive LOSS train 3.0275355559128982 valid 1.0931086540222168
EPOCH 193:
Loss :  1.6508170366287231 2.79919171333313 4.450008869171143
Loss :  1.6663521528244019 3.2914247512817383 4.95777702331543
Loss :  1.6521861553192139 2.581465244293213 4.233651161193848
Loss :  1.655583143234253 3.0147337913513184 4.670316696166992
Loss :  1.6801384687423706 2.8125085830688477 4.492647171020508
Loss :  1.6631430387496948 2.841839075088501 4.504981994628906
Loss :  1.6610649824142456 3.2251791954040527 4.886244297027588
Loss :  1.6501359939575195 2.607278347015381 4.2574143409729
Loss :  1.6547497510910034 2.3226981163024902 3.977447986602783
Loss :  1.609086513519287 2.516669273376465 4.125755786895752
Loss :  1.6688460111618042 3.2093327045440674 4.878178596496582
Loss :  1.7294011116027832 3.09808087348938 4.827482223510742
Loss :  1.6762100458145142 3.3334944248199463 5.00970458984375
Loss :  1.664490818977356 3.1238253116607666 4.788316249847412
Loss :  1.6428275108337402 2.8799889087677 4.5228166580200195
Loss :  1.6521532535552979 2.8637497425079346 4.515902996063232
Loss :  1.661173939704895 2.617405891418457 4.2785797119140625
Loss :  1.659787893295288 2.86244797706604 4.522235870361328
Loss :  1.6674692630767822 2.6765966415405273 4.3440656661987305
Loss :  1.624158501625061 3.1565346717834473 4.780693054199219
  batch 20 loss: 1.624158501625061, 3.1565346717834473, 4.780693054199219
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6593655347824097 3.072307825088501 4.731673240661621
Loss :  1.6751922369003296 2.985684394836426 4.660876750946045
Loss :  1.6464534997940063 3.036633253097534 4.68308687210083
Loss :  1.6875966787338257 2.8062353134155273 4.493832111358643
Loss :  1.6849145889282227 3.4176278114318848 5.102542400360107
Loss :  1.6486568450927734 2.9509520530700684 4.599608898162842
Loss :  1.6974620819091797 3.164177179336548 4.861639022827148
Loss :  1.639920711517334 3.138967275619507 4.778887748718262
Loss :  1.6880710124969482 2.86852765083313 4.556598663330078
Loss :  1.6416150331497192 2.8735666275024414 4.515181541442871
Loss :  1.7227892875671387 3.15498685836792 4.877776145935059
Loss :  1.6691070795059204 3.180499792098999 4.849606990814209
Loss :  1.654533863067627 2.855729103088379 4.510262966156006
Loss :  1.656929612159729 2.530355215072632 4.18728494644165
Loss :  1.696935772895813 3.4233081340789795 5.120244026184082
Loss :  1.6888126134872437 2.8814280033111572 4.570240497589111
Loss :  1.6665809154510498 2.9343948364257812 4.60097599029541
Loss :  1.6343086957931519 2.7245264053344727 4.358835220336914
Loss :  1.6597105264663696 2.8071229457855225 4.466833591461182
Loss :  1.6523391008377075 2.855116128921509 4.507455348968506
  batch 40 loss: 1.6523391008377075, 2.855116128921509, 4.507455348968506
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602840423583984 3.284165620803833 4.944449424743652
Loss :  1.6485494375228882 2.85568904876709 4.504238605499268
Loss :  1.665560245513916 3.0283966064453125 4.6939568519592285
Loss :  1.664011001586914 2.747424602508545 4.411435604095459
Loss :  1.66678786277771 2.5330090522766113 4.199796676635742
Loss :  1.6581659317016602 3.1777985095977783 4.835964202880859
Loss :  1.6452724933624268 3.3389999866485596 4.984272480010986
Loss :  1.657397985458374 3.0391669273376465 4.696564674377441
Loss :  1.6301711797714233 3.3216612339019775 4.951832294464111
Loss :  1.681396722793579 3.364163875579834 5.045560836791992
Loss :  1.6459181308746338 2.649042844772339 4.294960975646973
Loss :  1.6643263101577759 2.982623815536499 4.6469502449035645
Loss :  1.6826508045196533 3.162646770477295 4.845297813415527
Loss :  1.6676515340805054 3.0497519969940186 4.717403411865234
Loss :  1.6706607341766357 3.581366777420044 5.25202751159668
Loss :  1.6351947784423828 2.930795907974243 4.565990447998047
Loss :  1.6847108602523804 3.2444381713867188 4.929149150848389
Loss :  1.6821825504302979 3.2904343605041504 4.972617149353027
Loss :  1.695638656616211 3.2314751148223877 4.9271135330200195
Loss :  1.6710536479949951 2.99634051322937 4.667394161224365
  batch 60 loss: 1.6710536479949951, 2.99634051322937, 4.667394161224365
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6725122928619385 3.7816555500030518 5.45416784286499
Loss :  1.669371247291565 2.5710957050323486 4.240467071533203
Loss :  1.677581787109375 3.072049140930176 4.749630928039551
Loss :  1.6580379009246826 2.7488021850585938 4.4068403244018555
Loss :  1.655043601989746 3.0672826766967773 4.722326278686523
Loss :  5.5169854164123535 4.442951679229736 9.95993709564209
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.404709815979004 4.298973560333252 9.703683853149414
Loss :  5.514793872833252 4.241187572479248 9.7559814453125
Loss :  4.60135555267334 4.253180027008057 8.854536056518555
Total LOSS train 4.657201114067664 valid 9.56853461265564
CE LOSS train 1.6641723541113047 valid 1.150338888168335
Contrastive LOSS train 2.9930287544543925 valid 1.0632950067520142
EPOCH 194:
Loss :  1.6507288217544556 2.693295955657959 4.344024658203125
Loss :  1.6663142442703247 3.325199604034424 4.991513729095459
Loss :  1.652201771736145 3.011387825012207 4.6635894775390625
Loss :  1.6555421352386475 3.0177111625671387 4.673253059387207
Loss :  1.6801037788391113 2.9812676906585693 4.661371231079102
Loss :  1.6631406545639038 2.7418441772460938 4.404984951019287
Loss :  1.6610177755355835 3.571147918701172 5.232165813446045
Loss :  1.650123953819275 2.8684158325195312 4.518539905548096
Loss :  1.654655933380127 2.395540475845337 4.050196647644043
Loss :  1.6090996265411377 2.6757376194000244 4.284837245941162
Loss :  1.6688414812088013 3.3955118656158447 5.0643534660339355
Loss :  1.7292941808700562 3.0251121520996094 4.754406452178955
Loss :  1.6761420965194702 3.2789766788482666 4.955118656158447
Loss :  1.6643873453140259 3.522566318511963 5.186953544616699
Loss :  1.642698884010315 3.0588014125823975 4.701500415802002
Loss :  1.6522324085235596 3.014575481414795 4.666808128356934
Loss :  1.6611815690994263 2.7115988731384277 4.3727803230285645
Loss :  1.6597672700881958 2.977891445159912 4.637658596038818
Loss :  1.6675114631652832 2.7109811305999756 4.37849235534668
Loss :  1.6240856647491455 3.0601935386657715 4.684279441833496
  batch 20 loss: 1.6240856647491455, 3.0601935386657715, 4.684279441833496
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6593501567840576 3.4410793781280518 5.100429534912109
Loss :  1.6751736402511597 2.638197660446167 4.313371181488037
Loss :  1.6463924646377563 2.685096025466919 4.331488609313965
Loss :  1.6876397132873535 2.816689968109131 4.504329681396484
Loss :  1.6849514245986938 3.2613556385040283 4.946307182312012
Loss :  1.648615837097168 3.0092873573303223 4.65790319442749
Loss :  1.6975247859954834 3.3307015895843506 5.028226375579834
Loss :  1.6399325132369995 3.0930235385894775 4.7329559326171875
Loss :  1.6880789995193481 2.564962148666382 4.2530412673950195
Loss :  1.6415588855743408 2.9647583961486816 4.606317520141602
Loss :  1.7228119373321533 3.219212770462036 4.9420247077941895
Loss :  1.6692465543746948 3.382446527481079 5.051692962646484
Loss :  1.6544829607009888 2.7717931270599365 4.426276206970215
Loss :  1.6568869352340698 2.7828688621520996 4.439755916595459
Loss :  1.696982502937317 3.3993451595306396 5.096327781677246
Loss :  1.6889005899429321 2.9147655963897705 4.603666305541992
Loss :  1.6665825843811035 3.3792500495910645 5.045832633972168
Loss :  1.6344188451766968 2.8670506477355957 4.501469612121582
Loss :  1.6596853733062744 2.8091187477111816 4.468804359436035
Loss :  1.6523114442825317 3.379805326461792 5.032116889953613
  batch 40 loss: 1.6523114442825317, 3.379805326461792, 5.032116889953613
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6603269577026367 3.559736728668213 5.22006368637085
Loss :  1.6485813856124878 2.90702486038208 4.555606365203857
Loss :  1.665517807006836 3.002716302871704 4.668233871459961
Loss :  1.6638994216918945 2.8568663597106934 4.520765781402588
Loss :  1.6668206453323364 3.181697368621826 4.848517894744873
Loss :  1.6582157611846924 3.1121251583099365 4.770340919494629
Loss :  1.645267367362976 3.314328670501709 4.959596157073975
Loss :  1.657422661781311 3.3539421558380127 5.011364936828613
Loss :  1.6301984786987305 3.572099208831787 5.202297687530518
Loss :  1.6814879179000854 3.4260027408599854 5.107490539550781
Loss :  1.645881175994873 2.8678183555603027 4.513699531555176
Loss :  1.6643503904342651 3.2649006843566895 4.929251194000244
Loss :  1.6826893091201782 3.5826780796051025 5.26536750793457
Loss :  1.667611837387085 3.0180704593658447 4.68568229675293
Loss :  1.6707462072372437 3.4684083461761475 5.139154434204102
Loss :  1.6352109909057617 2.76523756980896 4.400448799133301
Loss :  1.684715747833252 3.2607386112213135 4.9454545974731445
Loss :  1.6822084188461304 3.3913748264312744 5.073583126068115
Loss :  1.695690631866455 3.1787266731262207 4.874417304992676
Loss :  1.6710964441299438 2.8101484775543213 4.481245040893555
  batch 60 loss: 1.6710964441299438, 2.8101484775543213, 4.481245040893555
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726665496826172 3.182800769805908 4.855467319488525
Loss :  1.6694318056106567 2.9639744758605957 4.633406162261963
Loss :  1.677573561668396 3.2118828296661377 4.889456272125244
Loss :  1.6581194400787354 2.6489293575286865 4.307048797607422
Loss :  1.6550488471984863 3.0895204544067383 4.744569301605225
Loss :  5.491697788238525 4.408518314361572 9.900216102600098
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.379524230957031 4.332653999328613 9.712178230285645
Loss :  5.493727207183838 4.217556953430176 9.711284637451172
Loss :  4.57763147354126 4.11557674407959 8.693208694458008
Total LOSS train 4.7371030073899485 valid 9.50422191619873
CE LOSS train 1.664175061079172 valid 1.144407868385315
Contrastive LOSS train 3.0729279261368974 valid 1.0288941860198975
EPOCH 195:
Loss :  1.6506558656692505 2.564871311187744 4.215527057647705
Loss :  1.66635262966156 3.0519628524780273 4.718315601348877
Loss :  1.6521828174591064 2.459716796875 4.111899375915527
Loss :  1.655513048171997 2.8149895668029785 4.470502853393555
Loss :  1.6802066564559937 3.1850504875183105 4.865257263183594
Loss :  1.6630864143371582 2.784691333770752 4.44777774810791
Loss :  1.661112666130066 3.264068841934204 4.9251813888549805
Loss :  1.6500928401947021 2.635282278060913 4.285375118255615
Loss :  1.6546788215637207 2.522382974624634 4.177062034606934
Loss :  1.609090805053711 2.8103954792022705 4.419486045837402
Loss :  1.6688036918640137 2.978585958480835 4.6473894119262695
Loss :  1.7293514013290405 3.013697385787964 4.743048667907715
Loss :  1.676388144493103 3.387354850769043 5.0637431144714355
Loss :  1.6644823551177979 3.202427387237549 4.866909980773926
Loss :  1.6426825523376465 2.9203224182128906 4.563004970550537
Loss :  1.652195692062378 2.9164113998413086 4.568607330322266
Loss :  1.6611465215682983 2.9671614170074463 4.628307819366455
Loss :  1.6597079038619995 3.0938308238983154 4.753538608551025
Loss :  1.6674633026123047 2.805070400238037 4.472533702850342
Loss :  1.6240413188934326 3.1603026390075684 4.784343719482422
  batch 20 loss: 1.6240413188934326, 3.1603026390075684, 4.784343719482422
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6593610048294067 3.503352165222168 5.162713050842285
Loss :  1.6751658916473389 2.8738038539886475 4.548969745635986
Loss :  1.6464661359786987 3.048102617263794 4.694568634033203
Loss :  1.6876115798950195 2.9550511837005615 4.64266300201416
Loss :  1.6848992109298706 3.397317409515381 5.082216739654541
Loss :  1.6485953330993652 2.9752888679504395 4.623884201049805
Loss :  1.697503924369812 3.120718479156494 4.818222522735596
Loss :  1.6398868560791016 2.984938859939575 4.624825477600098
Loss :  1.688066005706787 2.9511733055114746 4.639239311218262
Loss :  1.6415948867797852 2.6433541774749756 4.28494930267334
Loss :  1.7228329181671143 3.3205578327178955 5.04339075088501
Loss :  1.6691538095474243 3.083238363265991 4.752392292022705
Loss :  1.6544394493103027 2.9718635082244873 4.626302719116211
Loss :  1.6568958759307861 2.807421922683716 4.464317798614502
Loss :  1.6969830989837646 3.306549310684204 5.003532409667969
Loss :  1.6887619495391846 2.9118916988372803 4.600653648376465
Loss :  1.6665990352630615 2.9569270610809326 4.623526096343994
Loss :  1.6343486309051514 3.068316698074341 4.702665328979492
Loss :  1.659729242324829 2.777862310409546 4.437591552734375
Loss :  1.6523334980010986 3.069209337234497 4.721542835235596
  batch 40 loss: 1.6523334980010986, 3.069209337234497, 4.721542835235596
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602717638015747 3.2695136070251465 4.929785251617432
Loss :  1.6486016511917114 2.6258366107940674 4.274438381195068
Loss :  1.6655349731445312 3.1770710945129395 4.842606067657471
Loss :  1.6640046834945679 2.9010424613952637 4.565047264099121
Loss :  1.6668312549591064 2.5177392959594727 4.1845703125
Loss :  1.6581361293792725 3.1460676193237305 4.804203987121582
Loss :  1.6453073024749756 3.2882080078125 4.933515548706055
Loss :  1.6574161052703857 3.3058745861053467 4.963290691375732
Loss :  1.6301147937774658 3.4146971702575684 5.044812202453613
Loss :  1.681391954421997 3.2532966136932373 4.934688568115234
Loss :  1.6459332704544067 2.603788137435913 4.249721527099609
Loss :  1.664352297782898 2.7589144706726074 4.423266887664795
Loss :  1.682684063911438 3.324932813644409 5.007616996765137
Loss :  1.6676963567733765 2.756000518798828 4.423696994781494
Loss :  1.670693039894104 3.5241496562957764 5.19484281539917
Loss :  1.635241985321045 2.9439408779144287 4.5791826248168945
Loss :  1.6847578287124634 2.9705772399902344 4.655334949493408
Loss :  1.682163119316101 3.1154417991638184 4.797605037689209
Loss :  1.695654034614563 3.207256555557251 4.9029107093811035
Loss :  1.6710625886917114 2.935163736343384 4.606226444244385
  batch 60 loss: 1.6710625886917114, 2.935163736343384, 4.606226444244385
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.672560453414917 3.5858771800994873 5.258437633514404
Loss :  1.669412612915039 2.639533281326294 4.308945655822754
Loss :  1.6775583028793335 3.0640528202056885 4.741611003875732
Loss :  1.6580663919448853 2.6584174633026123 4.316483974456787
Loss :  1.6550264358520508 2.908424139022827 4.563450813293457
Loss :  5.519439220428467 4.355461120605469 9.874900817871094
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.409425735473633 4.37276029586792 9.782186508178711
Loss :  5.516637325286865 4.244199752807617 9.76083755493164
Loss :  4.604791164398193 4.057374477386475 8.662165641784668
Total LOSS train 4.666650331937349 valid 9.520022630691528
CE LOSS train 1.6641682643156785 valid 1.1511977910995483
Contrastive LOSS train 3.0024820511157695 valid 1.0143436193466187
EPOCH 196:
Loss :  1.6506891250610352 2.820960521697998 4.471649646759033
Loss :  1.66629958152771 3.3618006706237793 5.02810001373291
Loss :  1.652222752571106 2.5119338035583496 4.164156436920166
Loss :  1.6555434465408325 2.951874017715454 4.607417583465576
Loss :  1.680076003074646 3.124873399734497 4.8049492835998535
Loss :  1.6631356477737427 3.1352808475494385 4.798416614532471
Loss :  1.6610747575759888 3.512007713317871 5.17308235168457
Loss :  1.6500788927078247 2.3719065189361572 4.0219855308532715
Loss :  1.654704213142395 2.37203311920166 4.026737213134766
Loss :  1.609002709388733 2.437077283859253 4.046080112457275
Loss :  1.6687871217727661 3.1997365951538086 4.868523597717285
Loss :  1.7294816970825195 3.0109078884124756 4.740389823913574
Loss :  1.676138997077942 3.351243734359741 5.027382850646973
Loss :  1.6644034385681152 3.346116542816162 5.010519981384277
Loss :  1.6427565813064575 2.8926498889923096 4.535406589508057
Loss :  1.6521642208099365 2.8737223148345947 4.525886535644531
Loss :  1.6611506938934326 2.619783878326416 4.2809343338012695
Loss :  1.6597042083740234 2.7363979816436768 4.396101951599121
Loss :  1.6675406694412231 2.7686197757720947 4.436160564422607
Loss :  1.6240655183792114 3.1817739009857178 4.805839538574219
  batch 20 loss: 1.6240655183792114, 3.1817739009857178, 4.805839538574219
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6593297719955444 3.438746452331543 5.098076343536377
Loss :  1.6751497983932495 2.760139226913452 4.435288906097412
Loss :  1.6464433670043945 3.0330042839050293 4.679447650909424
Loss :  1.6875602006912231 2.8015964031219482 4.489156723022461
Loss :  1.684852957725525 3.618553638458252 5.303406715393066
Loss :  1.6485856771469116 3.146190643310547 4.794776439666748
Loss :  1.697426199913025 3.164590358734131 4.862016677856445
Loss :  1.63996160030365 3.1066102981567383 4.746572017669678
Loss :  1.6880850791931152 2.9812252521514893 4.669310569763184
Loss :  1.6414986848831177 2.911407232284546 4.552906036376953
Loss :  1.7228032350540161 3.1211819648742676 4.843985080718994
Loss :  1.6691032648086548 3.232123851776123 4.901226997375488
Loss :  1.6545171737670898 2.9537339210510254 4.608251094818115
Loss :  1.6568964719772339 2.6174707412719727 4.274367332458496
Loss :  1.6969645023345947 3.2847952842712402 4.981760025024414
Loss :  1.688887357711792 2.7291107177734375 4.417998313903809
Loss :  1.666539192199707 3.072936534881592 4.739475727081299
Loss :  1.6344504356384277 3.0293023586273193 4.663752555847168
Loss :  1.6596895456314087 2.9804534912109375 4.640142917633057
Loss :  1.6522526741027832 3.1671829223632812 4.8194355964660645
  batch 40 loss: 1.6522526741027832, 3.1671829223632812, 4.8194355964660645
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.660298228263855 3.3525381088256836 5.012836456298828
Loss :  1.648582100868225 3.1169025897979736 4.765484809875488
Loss :  1.6655374765396118 3.2467188835144043 4.912256240844727
Loss :  1.6639868021011353 3.0521843433380127 4.7161712646484375
Loss :  1.666823148727417 2.500837802886963 4.167660713195801
Loss :  1.6582311391830444 3.3060858249664307 4.9643168449401855
Loss :  1.645304560661316 3.111891031265259 4.757195472717285
Loss :  1.657505989074707 3.161297082901001 4.818802833557129
Loss :  1.6301777362823486 3.391758441925049 5.021936416625977
Loss :  1.6814519166946411 3.353947639465332 5.035399436950684
Loss :  1.6458964347839355 2.63285756111145 4.278754234313965
Loss :  1.6642839908599854 2.945651054382324 4.6099348068237305
Loss :  1.682663083076477 3.7747597694396973 5.457422733306885
Loss :  1.6676959991455078 3.261653184890747 4.929348945617676
Loss :  1.6706901788711548 3.6863632202148438 5.357053279876709
Loss :  1.6351536512374878 3.1081907749176025 4.743344306945801
Loss :  1.6846705675125122 3.0999414920806885 4.78461217880249
Loss :  1.682227611541748 3.1751210689544678 4.857348442077637
Loss :  1.6956079006195068 2.955545663833618 4.651153564453125
Loss :  1.6710336208343506 3.0232632160186768 4.694296836853027
  batch 60 loss: 1.6710336208343506, 3.0232632160186768, 4.694296836853027
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6725856065750122 3.3906238079071045 5.063209533691406
Loss :  1.669397234916687 3.1658501625061035 4.83524751663208
Loss :  1.6775916814804077 2.9900288581848145 4.667620658874512
Loss :  1.658051609992981 3.132685661315918 4.790737152099609
Loss :  1.6550801992416382 3.2142601013183594 4.869340419769287
Loss :  5.513020992279053 4.40886116027832 9.921882629394531
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.3991169929504395 4.4295430183410645 9.828660011291504
Loss :  5.511109352111816 4.23516321182251 9.746273040771484
Loss :  4.593852519989014 4.260335445404053 8.854187965393066
Total LOSS train 4.723885528857892 valid 9.587750911712646
CE LOSS train 1.6641622451635507 valid 1.1484631299972534
Contrastive LOSS train 3.0597232818603515 valid 1.0650838613510132
EPOCH 197:
Loss :  1.6506153345108032 2.638984441757202 4.289599895477295
Loss :  1.6663063764572144 3.166832208633423 4.833138465881348
Loss :  1.652172565460205 2.6801483631134033 4.3323211669921875
Loss :  1.6555330753326416 2.8965964317321777 4.552129745483398
Loss :  1.6801866292953491 3.045881509780884 4.726068019866943
Loss :  1.6630851030349731 2.724497079849243 4.387582302093506
Loss :  1.6611554622650146 3.634070634841919 5.295226097106934
Loss :  1.6500548124313354 2.7659003734588623 4.415955066680908
Loss :  1.6546927690505981 2.3496248722076416 4.004317760467529
Loss :  1.6090596914291382 2.611036539077759 4.220096111297607
Loss :  1.668749213218689 3.024782657623291 4.6935319900512695
Loss :  1.7293148040771484 2.9970505237579346 4.726365089416504
Loss :  1.6759138107299805 3.2400400638580322 4.915953636169434
Loss :  1.6644090414047241 3.440978765487671 5.1053876876831055
Loss :  1.642656683921814 3.079383134841919 4.722039699554443
Loss :  1.6521753072738647 3.0183815956115723 4.670557022094727
Loss :  1.6611405611038208 2.9266152381896973 4.5877556800842285
Loss :  1.6596945524215698 3.0596120357513428 4.719306468963623
Loss :  1.6675182580947876 2.5312936305999756 4.198812007904053
Loss :  1.6240684986114502 3.205968141555786 4.830036640167236
  batch 20 loss: 1.6240684986114502, 3.205968141555786, 4.830036640167236
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.659366250038147 3.1356701850891113 4.795036315917969
Loss :  1.6751134395599365 2.683952569961548 4.359066009521484
Loss :  1.646468162536621 3.170323371887207 4.816791534423828
Loss :  1.6874914169311523 3.0678703784942627 4.755361557006836
Loss :  1.6849056482315063 3.4729349613189697 5.157840728759766
Loss :  1.6485713720321655 3.1349375247955322 4.783508777618408
Loss :  1.697385549545288 3.1455318927764893 4.842917442321777
Loss :  1.6399613618850708 3.005157470703125 4.645118713378906
Loss :  1.6880757808685303 3.0542633533477783 4.742339134216309
Loss :  1.641491413116455 3.105461359024048 4.746953010559082
Loss :  1.722834587097168 3.14212703704834 4.864961624145508
Loss :  1.6690731048583984 3.095816135406494 4.764889240264893
Loss :  1.6545052528381348 3.286860704421997 4.941366195678711
Loss :  1.6568903923034668 3.138648509979248 4.795538902282715
Loss :  1.6970036029815674 3.457119941711426 5.154123306274414
Loss :  1.688874363899231 2.4926111698150635 4.181485652923584
Loss :  1.6665723323822021 3.281195878982544 4.947768211364746
Loss :  1.634350061416626 3.3070566654205322 4.941406726837158
Loss :  1.6597121953964233 3.2367970943450928 4.896509170532227
Loss :  1.652229905128479 3.1585888862609863 4.810818672180176
  batch 40 loss: 1.652229905128479, 3.1585888862609863, 4.810818672180176
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6603014469146729 3.231921911239624 4.892223358154297
Loss :  1.6485893726348877 2.630580425262451 4.279170036315918
Loss :  1.6654876470565796 3.2829482555389404 4.9484357833862305
Loss :  1.6638263463974 2.778024435043335 4.441850662231445
Loss :  1.6668211221694946 3.3488266468048096 5.015647888183594
Loss :  1.6582036018371582 2.8615753650665283 4.519779205322266
Loss :  1.64523446559906 3.5567915439605713 5.202025890350342
Loss :  1.6574195623397827 2.927762031555176 4.585181713104248
Loss :  1.630073070526123 3.588521718978882 5.218594551086426
Loss :  1.6813182830810547 3.1555609703063965 4.836879253387451
Loss :  1.6458747386932373 2.7214205265045166 4.367295265197754
Loss :  1.6642606258392334 2.9596476554870605 4.623908042907715
Loss :  1.6826781034469604 3.607389211654663 5.290067195892334
Loss :  1.6676396131515503 3.0553839206695557 4.723023414611816
Loss :  1.6707286834716797 3.4056899547576904 5.076418876647949
Loss :  1.6351348161697388 2.775073289871216 4.410208225250244
Loss :  1.68472421169281 3.256016969680786 4.940741062164307
Loss :  1.6821939945220947 3.493420362472534 5.175614356994629
Loss :  1.6956799030303955 3.171731472015381 4.8674116134643555
Loss :  1.671033263206482 2.9361732006073 4.607206344604492
  batch 60 loss: 1.671033263206482, 2.9361732006073, 4.607206344604492
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726213693618774 3.3772683143615723 5.04988956451416
Loss :  1.6694570779800415 2.981801748275757 4.651258945465088
Loss :  1.6775158643722534 2.8794641494750977 4.556980133056641
Loss :  1.6580142974853516 3.242011308670044 4.900025367736816
Loss :  1.655109167098999 3.1812427043914795 4.8363518714904785
Loss :  5.504631519317627 4.378687858581543 9.883319854736328
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.391164779663086 4.344862937927246 9.736027717590332
Loss :  5.500973224639893 4.291971683502197 9.79294490814209
Loss :  4.587596893310547 4.344418048858643 8.932014465332031
Total LOSS train 4.741325693864089 valid 9.586076736450195
CE LOSS train 1.6641433752500094 valid 1.1468992233276367
Contrastive LOSS train 3.077182329618014 valid 1.0861045122146606
EPOCH 198:
Loss :  1.6506106853485107 2.4453184604644775 4.095929145812988
Loss :  1.6663010120391846 3.2708921432495117 4.937192916870117
Loss :  1.6521624326705933 2.864658832550049 4.516821384429932
Loss :  1.655476450920105 2.9543073177337646 4.60978364944458
Loss :  1.6800445318222046 3.0463855266571045 4.7264299392700195
Loss :  1.663022518157959 2.8905234336853027 4.553545951843262
Loss :  1.6610883474349976 3.4131174087524414 5.0742058753967285
Loss :  1.6500383615493774 2.6404614448547363 4.290499687194824
Loss :  1.654748797416687 2.525836229324341 4.180584907531738
Loss :  1.608972430229187 2.5988402366638184 4.207812786102295
Loss :  1.668726921081543 3.2604165077209473 4.92914342880249
Loss :  1.729418396949768 3.067345142364502 4.7967634201049805
Loss :  1.6761341094970703 3.186962604522705 4.863096714019775
Loss :  1.6644036769866943 3.1411705017089844 4.805574417114258
Loss :  1.6427034139633179 2.967900276184082 4.6106038093566895
Loss :  1.6521331071853638 3.2243454456329346 4.876478672027588
Loss :  1.6611568927764893 2.9625418186187744 4.623698711395264
Loss :  1.6596403121948242 2.873680830001831 4.533321380615234
Loss :  1.6674761772155762 2.613952159881592 4.281428337097168
Loss :  1.6241202354431152 2.928453207015991 4.552573204040527
  batch 20 loss: 1.6241202354431152, 2.928453207015991, 4.552573204040527
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6593717336654663 3.2332770824432373 4.892648696899414
Loss :  1.6751164197921753 2.4164555072784424 4.091571807861328
Loss :  1.6464203596115112 3.4420015811920166 5.088421821594238
Loss :  1.687552809715271 3.356297492980957 5.043850421905518
Loss :  1.6849414110183716 3.299623966217041 4.984565258026123
Loss :  1.648582935333252 3.0160837173461914 4.664666652679443
Loss :  1.697356104850769 3.434563398361206 5.1319193840026855
Loss :  1.6399232149124146 3.004206418991089 4.644129753112793
Loss :  1.6880429983139038 3.0716145038604736 4.759657382965088
Loss :  1.6415516138076782 2.8193089962005615 4.460860729217529
Loss :  1.7228398323059082 3.3468687534332275 5.069708824157715
Loss :  1.669050931930542 2.9131052494049072 4.582156181335449
Loss :  1.654456615447998 2.8386054039001465 4.4930620193481445
Loss :  1.656887173652649 3.0218164920806885 4.678703784942627
Loss :  1.6970129013061523 3.417527437210083 5.114540100097656
Loss :  1.6887964010238647 2.8362584114074707 4.525054931640625
Loss :  1.6665832996368408 3.2875521183013916 4.954135417938232
Loss :  1.6343077421188354 2.8406240940093994 4.474931716918945
Loss :  1.6597203016281128 2.909344434738159 4.569064617156982
Loss :  1.6521881818771362 3.2227516174316406 4.874939918518066
  batch 40 loss: 1.6521881818771362, 3.2227516174316406, 4.874939918518066
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6603057384490967 3.1393017768859863 4.799607276916504
Loss :  1.6486390829086304 2.5503458976745605 4.1989850997924805
Loss :  1.6654952764511108 3.0375187397003174 4.703013896942139
Loss :  1.663880467414856 3.186800718307495 4.850681304931641
Loss :  1.6668102741241455 2.6740543842315674 4.340864658355713
Loss :  1.6581430435180664 2.9113612174987793 4.569504261016846
Loss :  1.6451780796051025 3.561384677886963 5.2065629959106445
Loss :  1.6574283838272095 3.2601253986358643 4.917553901672363
Loss :  1.6300777196884155 3.3206546306610107 4.950732231140137
Loss :  1.6813533306121826 2.8168110847473145 4.498164176940918
Loss :  1.645845651626587 2.7141802310943604 4.360025882720947
Loss :  1.664198637008667 2.6959991455078125 4.360198020935059
Loss :  1.6826567649841309 3.583268404006958 5.265925407409668
Loss :  1.6676439046859741 3.0564193725585938 4.724063396453857
Loss :  1.6706804037094116 3.3321919441223145 5.002872467041016
Loss :  1.6351845264434814 2.7080142498016357 4.343198776245117
Loss :  1.6847188472747803 3.1360373497009277 4.820755958557129
Loss :  1.6821722984313965 3.1940879821777344 4.876260280609131
Loss :  1.6956843137741089 3.034550666809082 4.7302350997924805
Loss :  1.67106032371521 3.07110333442688 4.74216365814209
  batch 60 loss: 1.67106032371521, 3.07110333442688, 4.74216365814209
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.672587275505066 3.3586926460266113 5.031280040740967
Loss :  1.6694245338439941 2.62666916847229 4.296093940734863
Loss :  1.6774508953094482 2.982377052307129 4.659828186035156
Loss :  1.658062219619751 2.6489946842193604 4.307056903839111
Loss :  1.655081868171692 2.883681297302246 4.538763046264648
Loss :  5.524837970733643 4.356631755828857 9.8814697265625
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.413143634796143 4.353794574737549 9.766938209533691
Loss :  5.524438858032227 4.284277439117432 9.8087158203125
Loss :  4.604313850402832 4.312535762786865 8.916849136352539
Total LOSS train 4.680899517352764 valid 9.593493223190308
CE LOSS train 1.664136086977445 valid 1.151078462600708
Contrastive LOSS train 3.016763419371385 valid 1.0781339406967163
EPOCH 199:
Loss :  1.650618553161621 2.6936593055725098 4.344277858734131
Loss :  1.666311264038086 3.168776750564575 4.835087776184082
Loss :  1.6521942615509033 2.877535343170166 4.529729843139648
Loss :  1.6555335521697998 2.7275140285491943 4.383047580718994
Loss :  1.6800200939178467 3.0026276111602783 4.682647705078125
Loss :  1.6631014347076416 2.9690024852752686 4.63210391998291
Loss :  1.661104440689087 3.263099193572998 4.924203872680664
Loss :  1.6500749588012695 2.498453378677368 4.148528099060059
Loss :  1.6547210216522217 2.3711702823638916 4.025891304016113
Loss :  1.6090196371078491 2.4995102882385254 4.108530044555664
Loss :  1.6687426567077637 3.104492664337158 4.773235321044922
Loss :  1.729328989982605 2.7563884258270264 4.485717296600342
Loss :  1.6761090755462646 3.3973474502563477 5.073456764221191
Loss :  1.6643822193145752 3.2844321727752686 4.948814392089844
Loss :  1.6427034139633179 3.0143516063690186 4.657054901123047
Loss :  1.6521153450012207 3.200317859649658 4.852433204650879
Loss :  1.6611696481704712 2.7730650901794434 4.434234619140625
Loss :  1.6597155332565308 2.8199355602264404 4.479650974273682
Loss :  1.6675524711608887 2.585651397705078 4.253203868865967
Loss :  1.6241356134414673 2.841438055038452 4.465573787689209
  batch 20 loss: 1.6241356134414673, 2.841438055038452, 4.465573787689209
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.659349799156189 3.346832752227783 5.006182670593262
Loss :  1.6751190423965454 2.7191288471221924 4.394248008728027
Loss :  1.6463866233825684 2.741701602935791 4.388088226318359
Loss :  1.6875940561294556 2.75323748588562 4.440831661224365
Loss :  1.6849087476730347 3.6826889514923096 5.367597579956055
Loss :  1.6485850811004639 2.735651969909668 4.384237289428711
Loss :  1.6973637342453003 3.3094534873962402 5.00681734085083
Loss :  1.6399643421173096 2.9400861263275146 4.580050468444824
Loss :  1.6880587339401245 3.0646309852600098 4.752689838409424
Loss :  1.6415256261825562 2.8573877811431885 4.498913288116455
Loss :  1.7228291034698486 3.4655988216400146 5.188427925109863
Loss :  1.6690369844436646 3.4391865730285645 5.1082234382629395
Loss :  1.6544640064239502 2.9295663833618164 4.5840301513671875
Loss :  1.6569056510925293 2.879782199859619 4.536687850952148
Loss :  1.6969640254974365 3.6961281299591064 5.393092155456543
Loss :  1.6888618469238281 2.7109344005584717 4.399796485900879
Loss :  1.6666086912155151 3.146289587020874 4.8128981590271
Loss :  1.6343317031860352 2.7256581783294678 4.359990119934082
Loss :  1.6597403287887573 2.62007737159729 4.279817581176758
Loss :  1.6522059440612793 2.863278865814209 4.515484809875488
  batch 40 loss: 1.6522059440612793, 2.863278865814209, 4.515484809875488
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.660310983657837 3.585170269012451 5.245481491088867
Loss :  1.6485893726348877 2.7987289428710938 4.447318077087402
Loss :  1.6655713319778442 2.8761343955993652 4.54170560836792
Loss :  1.6639455556869507 2.8498730659484863 4.513818740844727
Loss :  1.666820764541626 2.8363077640533447 4.503128528594971
Loss :  1.658164620399475 3.074805974960327 4.732970714569092
Loss :  1.645208477973938 3.1443240642547607 4.789532661437988
Loss :  1.6574947834014893 2.9251768589019775 4.582671642303467
Loss :  1.6300691366195679 3.3866729736328125 5.01674222946167
Loss :  1.6813483238220215 3.0973241329193115 4.778672218322754
Loss :  1.6459029912948608 2.6932148933410645 4.339118003845215
Loss :  1.6642181873321533 2.68854022026062 4.352758407592773
Loss :  1.6827175617218018 3.441227674484253 5.123945236206055
Loss :  1.6676759719848633 2.9564669132232666 4.624142646789551
Loss :  1.6706868410110474 3.6937320232391357 5.364418983459473
Loss :  1.6351308822631836 2.952568769454956 4.587699890136719
Loss :  1.6846832036972046 3.0601158142089844 4.7447991371154785
Loss :  1.6821789741516113 3.18172287940979 4.8639020919799805
Loss :  1.6956508159637451 3.101857900619507 4.797508716583252
Loss :  1.6710140705108643 2.669356107711792 4.340370178222656
  batch 60 loss: 1.6710140705108643, 2.669356107711792, 4.340370178222656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6725900173187256 3.2151389122009277 4.887728691101074
Loss :  1.6694691181182861 2.7531158924102783 4.4225850105285645
Loss :  1.6775063276290894 3.072885036468506 4.750391483306885
Loss :  1.6580543518066406 2.839118003845215 4.4971723556518555
Loss :  1.6550182104110718 3.0917298793792725 4.746747970581055
Loss :  5.51912784576416 4.370344638824463 9.889472961425781
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.408143997192383 4.375435829162598 9.78357982635498
Loss :  5.514984130859375 4.242813587188721 9.757797241210938
Loss :  4.598325252532959 4.085743427276611 8.68406867980957
Total LOSS train 4.655859367664044 valid 9.528729677200317
CE LOSS train 1.664145832795363 valid 1.1495813131332397
Contrastive LOSS train 2.991713520196768 valid 1.0214358568191528
EPOCH 200:
Loss :  1.650636076927185 2.5591788291931152 4.20981502532959
Loss :  1.6662617921829224 3.374213457107544 5.040475368499756
Loss :  1.6521519422531128 2.988966703414917 4.64111852645874
Loss :  1.655407190322876 2.886134147644043 4.54154109954834
Loss :  1.680013656616211 3.0527641773223877 4.7327775955200195
Loss :  1.6630340814590454 2.936939239501953 4.599973201751709
Loss :  1.6611038446426392 3.0112555027008057 4.672359466552734
Loss :  1.6499830484390259 2.5626108646392822 4.212594032287598
Loss :  1.6546770334243774 2.3714146614074707 4.026091575622559
Loss :  1.6090112924575806 2.663562297821045 4.272573471069336
Loss :  1.6686962842941284 3.2462258338928223 4.91492223739624
Loss :  1.7292816638946533 2.6924118995666504 4.421693801879883
Loss :  1.676162600517273 3.194887399673462 4.871049880981445
Loss :  1.6644351482391357 3.1438841819763184 4.808319091796875
Loss :  1.6427007913589478 3.123526096343994 4.766226768493652
Loss :  1.6520228385925293 3.301999807357788 4.954022407531738
Loss :  1.661103367805481 2.970897912979126 4.6320013999938965
Loss :  1.6595920324325562 2.866450071334839 4.5260419845581055
Loss :  1.6674336194992065 2.5894100666046143 4.256843566894531
Loss :  1.6240897178649902 2.950568914413452 4.574658393859863
  batch 20 loss: 1.6240897178649902, 2.950568914413452, 4.574658393859863
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.659421682357788 2.94242262840271 4.601844310760498
Loss :  1.6750924587249756 2.6434199810028076 4.318512439727783
Loss :  1.6464656591415405 2.8316752910614014 4.478140830993652
Loss :  1.6874767541885376 2.638606309890747 4.326083183288574
Loss :  1.6848862171173096 3.4369444847106934 5.121830940246582
Loss :  1.6485944986343384 2.9923253059387207 4.6409196853637695
Loss :  1.6973776817321777 2.9882071018218994 4.685585021972656
Loss :  1.639886736869812 2.6372649669647217 4.277151584625244
Loss :  1.6880762577056885 2.8542685508728027 4.54234504699707
Loss :  1.6415144205093384 3.0814931392669678 4.723007678985596
Loss :  1.722800612449646 3.396897077560425 5.119697570800781
Loss :  1.6690802574157715 3.2787158489227295 4.947795867919922
Loss :  1.6544448137283325 2.987354278564453 4.641798973083496
Loss :  1.656842827796936 2.8549039363861084 4.511746883392334
Loss :  1.6969674825668335 3.2050111293792725 4.901978492736816
Loss :  1.6888407468795776 2.8512320518493652 4.540072917938232
Loss :  1.6666043996810913 3.088965654373169 4.755569934844971
Loss :  1.6342567205429077 2.6937813758850098 4.328038215637207
Loss :  1.6597087383270264 2.644627094268799 4.304335594177246
Loss :  1.6522178649902344 2.8203201293945312 4.472537994384766
  batch 40 loss: 1.6522178649902344, 2.8203201293945312, 4.472537994384766
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6603277921676636 3.175330638885498 4.835658550262451
Loss :  1.648616909980774 2.6429853439331055 4.29160213470459
Loss :  1.6655380725860596 3.0798797607421875 4.745417594909668
Loss :  1.6638718843460083 3.2135720252990723 4.877443790435791
Loss :  1.6668355464935303 2.7702653408050537 4.437100887298584
Loss :  1.6581681966781616 2.7869768142700195 4.445145130157471
Loss :  1.645119547843933 3.0334482192993164 4.678567886352539
Loss :  1.657335638999939 3.089604377746582 4.7469401359558105
Loss :  1.6301428079605103 3.3142988681793213 4.944441795349121
Loss :  1.6812522411346436 3.131133556365967 4.812385559082031
Loss :  1.6458706855773926 2.8206450939178467 4.46651554107666
Loss :  1.6641685962677002 3.067765712738037 4.731934547424316
Loss :  1.6827455759048462 3.7105000019073486 5.393245697021484
Loss :  1.6675015687942505 2.8996617794036865 4.567163467407227
Loss :  1.6706604957580566 3.3987796306610107 5.069439888000488
Loss :  1.6350162029266357 2.5481791496276855 4.183195114135742
Loss :  1.6847275495529175 3.308070182800293 4.9927978515625
Loss :  1.6821513175964355 3.205446720123291 4.887598037719727
Loss :  1.6956771612167358 3.1862540245056152 4.881931304931641
Loss :  1.6709415912628174 3.105356454849243 4.7762980461120605
  batch 60 loss: 1.6709415912628174, 3.105356454849243, 4.7762980461120605
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726163625717163 3.1327874660491943 4.805403709411621
Loss :  1.6693793535232544 2.447134494781494 4.116513729095459
Loss :  1.6773966550827026 3.0451879501342773 4.7225847244262695
Loss :  1.6580283641815186 2.745680332183838 4.403708457946777
Loss :  1.6550151109695435 2.8941869735717773 4.549201965332031
Loss :  5.515241622924805 4.315289497375488 9.830531120300293
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.4049177169799805 4.306358814239502 9.71127700805664
Loss :  5.508083343505859 4.325573444366455 9.833656311035156
Loss :  4.590518474578857 4.173305034637451 8.763823509216309
Total LOSS train 4.635020424769475 valid 9.5348219871521
CE LOSS train 1.6641147705224844 valid 1.1476296186447144
Contrastive LOSS train 2.9709056817568267 valid 1.0433262586593628
EPOCH 201:
Loss :  1.6505368947982788 2.5963847637176514 4.246921539306641
Loss :  1.6662746667861938 3.1202445030212402 4.7865190505981445
Loss :  1.6520873308181763 2.791590452194214 4.44367790222168
Loss :  1.6553534269332886 2.8964381217956543 4.551791667938232
Loss :  1.6799547672271729 3.145512580871582 4.825467109680176
Loss :  1.6629867553710938 2.918789863586426 4.5817766189575195
Loss :  1.6610671281814575 3.3462982177734375 5.0073652267456055
Loss :  1.650019884109497 2.3945860862731934 4.0446062088012695
Loss :  1.6546939611434937 2.21162748336792 3.866321563720703
Loss :  1.6090426445007324 2.683781147003174 4.292823791503906
Loss :  1.6687430143356323 3.3151345252990723 4.983877658843994
Loss :  1.7292280197143555 2.860541820526123 4.5897698402404785
Loss :  1.6760914325714111 3.190988063812256 4.867079734802246
Loss :  1.6643348932266235 3.5375986099243164 5.20193338394165
Loss :  1.6427088975906372 3.0648627281188965 4.707571506500244
Loss :  1.6520494222640991 2.9694323539733887 4.621481895446777
Loss :  1.6611182689666748 2.7960212230682373 4.457139492034912
Loss :  1.6596143245697021 2.617837429046631 4.277451515197754
Loss :  1.6674450635910034 2.682600736618042 4.350045680999756
Loss :  1.624091625213623 3.1969552040100098 4.821046829223633
  batch 20 loss: 1.624091625213623, 3.1969552040100098, 4.821046829223633
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6593929529190063 3.0642504692077637 4.7236433029174805
Loss :  1.67510986328125 2.532762050628662 4.207871913909912
Loss :  1.6464500427246094 2.8441896438598633 4.490639686584473
Loss :  1.6875286102294922 2.6803174018859863 4.3678460121154785
Loss :  1.6848503351211548 3.4264578819274902 5.1113080978393555
Loss :  1.648598074913025 3.0996854305267334 4.748283386230469
Loss :  1.6973443031311035 3.1920456886291504 4.889389991760254
Loss :  1.639918565750122 2.7431890964508057 4.383107662200928
Loss :  1.6880452632904053 2.881342887878418 4.569388389587402
Loss :  1.641487717628479 2.9108498096466064 4.552337646484375
Loss :  1.7227668762207031 3.009119987487793 4.731886863708496
Loss :  1.6690478324890137 3.133981943130493 4.803030014038086
Loss :  1.6543840169906616 2.988157033920288 4.64254093170166
Loss :  1.6568429470062256 2.6235339641571045 4.28037691116333
Loss :  1.6967368125915527 2.9240264892578125 4.620763301849365
Loss :  1.688786268234253 2.8124337196350098 4.501219749450684
Loss :  1.666547417640686 2.8047473430633545 4.47129487991333
Loss :  1.6342297792434692 2.746157169342041 4.380386829376221
Loss :  1.6597355604171753 2.795461416244507 4.455196857452393
Loss :  1.6522413492202759 3.214494466781616 4.866735935211182
  batch 40 loss: 1.6522413492202759, 3.214494466781616, 4.866735935211182
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6603187322616577 3.464400053024292 5.12471866607666
Loss :  1.6485034227371216 3.0261850357055664 4.674688339233398
Loss :  1.6655312776565552 2.836595058441162 4.502126216888428
Loss :  1.663851022720337 2.87093186378479 4.534782886505127
Loss :  1.666780948638916 2.3879153728485107 4.054696083068848
Loss :  1.6581778526306152 3.327218770980835 4.985396385192871
Loss :  1.6452147960662842 3.1199264526367188 4.765141487121582
Loss :  1.6573994159698486 3.0278923511505127 4.685291767120361
Loss :  1.6301469802856445 3.3552558422088623 4.985403060913086
Loss :  1.6812233924865723 2.841479539871216 4.522703170776367
Loss :  1.6459054946899414 2.770470380783081 4.416376113891602
Loss :  1.6642404794692993 2.8550376892089844 4.519278049468994
Loss :  1.6827564239501953 2.9273571968078613 4.610113620758057
Loss :  1.667559266090393 2.8713772296905518 4.538936614990234
Loss :  1.6706993579864502 3.4573423862457275 5.128041744232178
Loss :  1.6350797414779663 2.6354634761810303 4.270543098449707
Loss :  1.684766411781311 3.184889078140259 4.869655609130859
Loss :  1.6821396350860596 3.131538152694702 4.813677787780762
Loss :  1.6956729888916016 2.9687983989715576 4.664471626281738
Loss :  1.6709576845169067 2.9275290966033936 4.59848690032959
  batch 60 loss: 1.6709576845169067, 2.9275290966033936, 4.59848690032959
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726157665252686 3.339050769805908 5.011666297912598
Loss :  1.6693867444992065 2.6113078594207764 4.280694484710693
Loss :  1.6774052381515503 3.1163597106933594 4.793765068054199
Loss :  1.6580227613449097 2.787264108657837 4.445286750793457
Loss :  1.6550170183181763 3.009490728378296 4.664507865905762
Loss :  5.512720584869385 4.351308822631836 9.864028930664062
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.402889251708984 4.380629539489746 9.78351879119873
Loss :  5.50669527053833 4.281404972076416 9.788100242614746
Loss :  4.589302062988281 4.267297267913818 8.856599807739258
Total LOSS train 4.6120364042428825 valid 9.5730619430542
CE LOSS train 1.6641055363875168 valid 1.1473255157470703
Contrastive LOSS train 2.9479308678553653 valid 1.0668243169784546
EPOCH 202:
Loss :  1.6505109071731567 2.6997883319854736 4.35029935836792
Loss :  1.6662620306015015 3.0636565685272217 4.729918479919434
Loss :  1.6520988941192627 2.8641932010650635 4.516292095184326
Loss :  1.6553455591201782 2.9994871616363525 4.65483283996582
Loss :  1.6799286603927612 3.2496743202209473 4.929603099822998
Loss :  1.662986159324646 2.9240362644195557 4.587022304534912
Loss :  1.661027193069458 3.5256035327911377 5.186630725860596
Loss :  1.6500217914581299 2.446932077407837 4.096953868865967
Loss :  1.6546798944473267 2.486022472381592 4.140702247619629
Loss :  1.6090595722198486 2.7971789836883545 4.406238555908203
Loss :  1.6687355041503906 3.2296342849731445 4.898369789123535
Loss :  1.729193091392517 2.8378872871398926 4.567080497741699
Loss :  1.6761376857757568 3.1576826572418213 4.833820343017578
Loss :  1.6643364429473877 3.225900411605835 4.890236854553223
Loss :  1.6426748037338257 3.0395047664642334 4.6821794509887695
Loss :  1.6520689725875854 3.0319087505340576 4.6839776039123535
Loss :  1.661118984222412 2.9061803817749023 4.5672993659973145
Loss :  1.659589171409607 3.043181896209717 4.702771186828613
Loss :  1.6674801111221313 2.76922869682312 4.436708927154541
Loss :  1.624075174331665 3.2961385250091553 4.92021369934082
  batch 20 loss: 1.624075174331665, 3.2961385250091553, 4.92021369934082
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6593629121780396 3.276427984237671 4.935791015625
Loss :  1.6750998497009277 2.7958738803863525 4.470973968505859
Loss :  1.646451711654663 3.003375768661499 4.649827480316162
Loss :  1.687497854232788 2.821887254714966 4.509385108947754
Loss :  1.6848361492156982 3.4151437282562256 5.099979877471924
Loss :  1.6485726833343506 3.027799367904663 4.676372051239014
Loss :  1.6973576545715332 3.4059598445892334 5.1033172607421875
Loss :  1.6399136781692505 2.812368631362915 4.452282428741455
Loss :  1.6880600452423096 2.6924784183502197 4.380538463592529
Loss :  1.6414738893508911 2.724287271499634 4.3657612800598145
Loss :  1.7227531671524048 3.174572229385376 4.89732551574707
Loss :  1.669055700302124 2.903327465057373 4.572382926940918
Loss :  1.6543946266174316 3.0259203910827637 4.680315017700195
Loss :  1.6568418741226196 2.8659274578094482 4.522769451141357
Loss :  1.6967555284500122 3.2510578632354736 4.947813510894775
Loss :  1.6888054609298706 2.8747549057006836 4.563560485839844
Loss :  1.6665529012680054 2.596846103668213 4.263399124145508
Loss :  1.6342464685440063 2.6194117069244385 4.253658294677734
Loss :  1.659717321395874 2.7969274520874023 4.4566450119018555
Loss :  1.652226209640503 3.0291922092437744 4.681418418884277
  batch 40 loss: 1.652226209640503, 3.0291922092437744, 4.681418418884277
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6603223085403442 3.2890312671661377 4.9493536949157715
Loss :  1.648518681526184 2.862241506576538 4.510760307312012
Loss :  1.6655166149139404 2.7320406436920166 4.397557258605957
Loss :  1.6638386249542236 3.118265390396118 4.782104015350342
Loss :  1.666792869567871 2.5394859313964844 4.2062788009643555
Loss :  1.6581801176071167 3.1828548908233643 4.841034889221191
Loss :  1.6451977491378784 3.223881483078003 4.869079113006592
Loss :  1.6573967933654785 2.923522710800171 4.58091926574707
Loss :  1.6301387548446655 3.370781421661377 5.000920295715332
Loss :  1.6812231540679932 3.075838565826416 4.757061958312988
Loss :  1.6458872556686401 2.612787961959839 4.2586750984191895
Loss :  1.6642167568206787 2.806705951690674 4.470922470092773
Loss :  1.6827445030212402 3.071537494659424 4.754281997680664
Loss :  1.6675349473953247 2.831109046936035 4.49864387512207
Loss :  1.6706736087799072 3.271973133087158 4.9426469802856445
Loss :  1.6350469589233398 2.7774946689605713 4.412541389465332
Loss :  1.6847434043884277 3.129624605178833 4.81436824798584
Loss :  1.6821515560150146 3.202003002166748 4.884154319763184
Loss :  1.6956571340560913 3.1118040084838867 4.807461261749268
Loss :  1.670941710472107 3.2338225841522217 4.904764175415039
  batch 60 loss: 1.670941710472107, 3.2338225841522217, 4.904764175415039
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726248264312744 3.4936864376068115 5.166311264038086
Loss :  1.6693754196166992 2.69136118888855 4.360736846923828
Loss :  1.6774145364761353 3.1654365062713623 4.842851161956787
Loss :  1.6580204963684082 2.985201358795166 4.643221855163574
Loss :  1.6550267934799194 2.787472724914551 4.44249963760376
Loss :  5.51017951965332 4.450592994689941 9.960772514343262
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.400413990020752 4.32912015914917 9.729534149169922
Loss :  5.504819393157959 4.245869159698486 9.750688552856445
Loss :  4.586657524108887 4.3289289474487305 8.915586471557617
Total LOSS train 4.651751048748309 valid 9.589145421981812
CE LOSS train 1.6640998748632578 valid 1.1466643810272217
Contrastive LOSS train 2.9876511537111723 valid 1.0822322368621826
EPOCH 203:
Loss :  1.6504930257797241 2.7208592891693115 4.371352195739746
Loss :  1.6662657260894775 3.3447887897491455 5.011054515838623
Loss :  1.652103066444397 2.689452648162842 4.341555595397949
Loss :  1.6553233861923218 2.9750571250915527 4.630380630493164
Loss :  1.6799300909042358 3.434329032897949 5.114259243011475
Loss :  1.6629751920700073 2.5499796867370605 4.212954998016357
Loss :  1.6610218286514282 3.4617269039154053 5.122748851776123
Loss :  1.650020956993103 2.5002806186676025 4.150301456451416
Loss :  1.654670238494873 2.3465969562530518 4.001267433166504
Loss :  1.6090686321258545 2.8117218017578125 4.420790672302246
Loss :  1.6687363386154175 3.15826678276062 4.827003002166748
Loss :  1.7291837930679321 3.0049808025360107 4.734164714813232
Loss :  1.6761853694915771 2.981562852859497 4.657748222351074
Loss :  1.6643377542495728 3.3730881214141846 5.037425994873047
Loss :  1.6426639556884766 2.885672092437744 4.528336048126221
Loss :  1.6520863771438599 3.0400497913360596 4.692136287689209
Loss :  1.661125659942627 2.8872523307800293 4.548377990722656
Loss :  1.6595783233642578 2.8309946060180664 4.490572929382324
Loss :  1.6674941778182983 2.7030789852142334 4.370573043823242
Loss :  1.6240679025650024 2.9781854152679443 4.602253437042236
  batch 20 loss: 1.6240679025650024, 2.9781854152679443, 4.602253437042236
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6593523025512695 3.109177589416504 4.768529891967773
Loss :  1.6750905513763428 2.6639442443847656 4.3390350341796875
Loss :  1.6464534997940063 2.7512288093566895 4.397682189941406
Loss :  1.6874860525131226 2.5738775730133057 4.261363506317139
Loss :  1.6848357915878296 3.248668670654297 4.933504581451416
Loss :  1.6485662460327148 3.058372735977173 4.706938743591309
Loss :  1.6973655223846436 3.0580618381500244 4.755427360534668
Loss :  1.6399166584014893 2.5737550258636475 4.213671684265137
Loss :  1.6880643367767334 2.9180920124053955 4.606156349182129
Loss :  1.6414735317230225 2.98881459236145 4.630288124084473
Loss :  1.722741961479187 2.9798829555511475 4.702624797821045
Loss :  1.6690552234649658 2.787932872772217 4.456988334655762
Loss :  1.654402494430542 2.945676565170288 4.60007905960083
Loss :  1.6568424701690674 2.859171152114868 4.5160136222839355
Loss :  1.6967668533325195 3.6626272201538086 5.359394073486328
Loss :  1.6888209581375122 2.837592840194702 4.526413917541504
Loss :  1.666551113128662 2.8907766342163086 4.557327747344971
Loss :  1.6342580318450928 2.676877021789551 4.311135292053223
Loss :  1.6597071886062622 3.002755641937256 4.6624627113342285
Loss :  1.6522186994552612 3.08986759185791 4.742086410522461
  batch 40 loss: 1.6522186994552612, 3.08986759185791, 4.742086410522461
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6603256464004517 3.269524574279785 4.929850101470947
Loss :  1.6485328674316406 2.6775944232940674 4.326127052307129
Loss :  1.665513277053833 2.7152509689331055 4.380764007568359
Loss :  1.6638431549072266 2.9545748233795166 4.618417739868164
Loss :  1.6667990684509277 2.3982818126678467 4.065080642700195
Loss :  1.6581779718399048 3.042971134185791 4.701148986816406
Loss :  1.6452001333236694 2.920775890350342 4.565976142883301
Loss :  1.6573964357376099 2.828031301498413 4.4854278564453125
Loss :  1.6301319599151611 3.5083324909210205 5.138464450836182
Loss :  1.6812355518341064 3.06697154045105 4.748207092285156
Loss :  1.6458779573440552 2.584101915359497 4.229979991912842
Loss :  1.6642167568206787 2.8488450050354004 4.5130615234375
Loss :  1.6827436685562134 3.0786314010620117 4.7613749504089355
Loss :  1.6675407886505127 2.9962737560272217 4.663814544677734
Loss :  1.6706757545471191 3.26027774810791 4.930953502655029
Loss :  1.6350431442260742 2.579448938369751 4.214491844177246
Loss :  1.6847326755523682 2.9649786949157715 4.649711608886719
Loss :  1.682163953781128 3.1661369800567627 4.848300933837891
Loss :  1.6956533193588257 3.0193798542022705 4.715033054351807
Loss :  1.6709375381469727 3.1681268215179443 4.839064598083496
  batch 60 loss: 1.6709375381469727, 3.1681268215179443, 4.839064598083496
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726208925247192 3.5419740676879883 5.214594841003418
Loss :  1.6693686246871948 2.689077615737915 4.35844612121582
Loss :  1.6774187088012695 2.9983859062194824 4.675804615020752
Loss :  1.6580277681350708 3.1742093563079834 4.832237243652344
Loss :  1.6550383567810059 2.6944053173065186 4.349443435668945
Loss :  5.508919715881348 4.4011335372924805 9.910053253173828
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.398881435394287 4.37946891784668 9.778350830078125
Loss :  5.50370454788208 4.30681037902832 9.810514450073242
Loss :  4.586418628692627 4.16581392288208 8.752232551574707
Total LOSS train 4.6102793473463795 valid 9.562787771224976
CE LOSS train 1.6641002655029298 valid 1.1466046571731567
Contrastive LOSS train 2.9461790855114276 valid 1.04145348072052
EPOCH 204:
Loss :  1.6504884958267212 2.5245325565338135 4.175021171569824
Loss :  1.6662687063217163 3.2109215259552 4.877190113067627
Loss :  1.652105689048767 3.0857646465301514 4.737870216369629
Loss :  1.6553237438201904 2.73931622505188 4.39463996887207
Loss :  1.6799354553222656 3.1265182495117188 4.806453704833984
Loss :  1.6629716157913208 2.663198947906494 4.326170444488525
Loss :  1.6610239744186401 3.50732684135437 5.168350696563721
Loss :  1.6500188112258911 2.7384278774261475 4.388446807861328
Loss :  1.6546642780303955 2.2211811542510986 3.875845432281494
Loss :  1.6090705394744873 2.837582588195801 4.446653366088867
Loss :  1.668734073638916 3.1108059883117676 4.779540061950684
Loss :  1.729182243347168 2.9586684703826904 4.6878509521484375
Loss :  1.6761894226074219 2.966487407684326 4.642676830291748
Loss :  1.6643322706222534 3.307654619216919 4.971986770629883
Loss :  1.64265775680542 2.848975658416748 4.491633415222168
Loss :  1.6520897150039673 3.041124105453491 4.693213939666748
Loss :  1.6611300706863403 2.7953174114227295 4.456447601318359
Loss :  1.659570574760437 2.8667502403259277 4.526320934295654
Loss :  1.6674988269805908 2.712158203125 4.379656791687012
Loss :  1.6240671873092651 3.100644588470459 4.724711894989014
  batch 20 loss: 1.6240671873092651, 3.100644588470459, 4.724711894989014
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6593513488769531 2.9877612590789795 4.647112846374512
Loss :  1.6750860214233398 2.571978807449341 4.247064590454102
Loss :  1.646449327468872 3.18680739402771 4.833256721496582
Loss :  1.6874724626541138 2.6005239486694336 4.287996292114258
Loss :  1.6848379373550415 3.3659026622772217 5.050740718841553
Loss :  1.648560881614685 3.150660753250122 4.799221515655518
Loss :  1.6973720788955688 3.2128965854644775 4.910268783569336
Loss :  1.6399158239364624 2.6177122592926025 4.257627964019775
Loss :  1.6880669593811035 2.666348695755005 4.3544158935546875
Loss :  1.6414674520492554 2.94163179397583 4.583099365234375
Loss :  1.7227442264556885 2.8960747718811035 4.618819236755371
Loss :  1.669055461883545 2.946180582046509 4.615236282348633
Loss :  1.654400110244751 3.204298973083496 4.858698844909668
Loss :  1.6568418741226196 2.5063436031341553 4.1631855964660645
Loss :  1.6967753171920776 3.3246994018554688 5.021474838256836
Loss :  1.6888176202774048 3.0323026180267334 4.721120357513428
Loss :  1.6665517091751099 2.91788387298584 4.58443546295166
Loss :  1.6342604160308838 2.5552754402160645 4.189536094665527
Loss :  1.6597039699554443 2.827826499938965 4.487530708312988
Loss :  1.6522130966186523 2.9557929039001465 4.608006000518799
  batch 40 loss: 1.6522130966186523, 2.9557929039001465, 4.608006000518799
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6603254079818726 3.1078927516937256 4.768218040466309
Loss :  1.6485294103622437 3.024588108062744 4.673117637634277
Loss :  1.665501594543457 2.711876153945923 4.377377510070801
Loss :  1.6638232469558716 2.9023616313934326 4.566184997558594
Loss :  1.6667892932891846 2.532985210418701 4.199774742126465
Loss :  1.6581761837005615 2.9069294929504395 4.565105438232422
Loss :  1.645188331604004 3.1725687980651855 4.8177571296691895
Loss :  1.6573864221572876 3.01438045501709 4.671766757965088
Loss :  1.6301312446594238 3.2714617252349854 4.901593208312988
Loss :  1.681232213973999 2.7835981845855713 4.46483039855957
Loss :  1.6458715200424194 2.8103039264678955 4.456175327301025
Loss :  1.6642045974731445 2.9171042442321777 4.581308841705322
Loss :  1.6827309131622314 3.464810848236084 5.1475419998168945
Loss :  1.667526364326477 2.8280603885650635 4.49558687210083
Loss :  1.6706711053848267 3.258641481399536 4.929312705993652
Loss :  1.6350338459014893 2.7162094116210938 4.351243019104004
Loss :  1.6847193241119385 3.260352611541748 4.945072174072266
Loss :  1.6821662187576294 2.9556329250335693 4.637799263000488
Loss :  1.6956489086151123 3.0615923404693604 4.757241249084473
Loss :  1.6709409952163696 3.069627285003662 4.740568161010742
  batch 60 loss: 1.6709409952163696, 3.069627285003662, 4.740568161010742
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726176738739014 3.5671300888061523 5.239748001098633
Loss :  1.6693696975708008 2.684776544570923 4.3541460037231445
Loss :  1.6774091720581055 3.2089526653289795 4.886362075805664
Loss :  1.6580244302749634 3.0532374382019043 4.711261749267578
Loss :  1.6550225019454956 2.8991713523864746 4.55419397354126
Loss :  5.508245944976807 4.400235176086426 9.90848159790039
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.398280620574951 4.411774158477783 9.810054779052734
Loss :  5.503488540649414 4.231202125549316 9.73469066619873
Loss :  4.585391044616699 4.1575140953063965 8.742904663085938
Total LOSS train 4.618197177006648 valid 9.549032926559448
CE LOSS train 1.6640970486861009 valid 1.1463477611541748
Contrastive LOSS train 2.954100095308744 valid 1.0393785238265991
EPOCH 205:
Loss :  1.6504881381988525 2.5244500637054443 4.174938201904297
Loss :  1.6662503480911255 3.1590805053710938 4.82533073425293
Loss :  1.6521086692810059 2.7483162879943848 4.400424957275391
Loss :  1.6553266048431396 2.933354139328003 4.588680744171143
Loss :  1.6799405813217163 3.458831548690796 5.138772010803223
Loss :  1.662973165512085 2.794839382171631 4.457812309265137
Loss :  1.6610273122787476 3.138607978820801 4.799635410308838
Loss :  1.650012731552124 2.583153247833252 4.233165740966797
Loss :  1.654662013053894 2.2988152503967285 3.953477382659912
Loss :  1.6090679168701172 2.737893581390381 4.346961498260498
Loss :  1.6687229871749878 3.415158271789551 5.083881378173828
Loss :  1.7291898727416992 2.9524950981140137 4.681684970855713
Loss :  1.6762018203735352 3.017225742340088 4.693427562713623
Loss :  1.6643344163894653 3.1001553535461426 4.764489650726318
Loss :  1.642657995223999 2.9056203365325928 4.548278331756592
Loss :  1.6520847082138062 3.1244726181030273 4.776557445526123
Loss :  1.6611350774765015 2.6740479469299316 4.335183143615723
Loss :  1.6595772504806519 2.612907886505127 4.272485256195068
Loss :  1.6674970388412476 2.74446702003479 4.411963939666748
Loss :  1.6240752935409546 3.0843026638031006 4.708377838134766
  batch 20 loss: 1.6240752935409546, 3.0843026638031006, 4.708377838134766
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6593483686447144 2.974177837371826 4.63352632522583
Loss :  1.675085186958313 2.619692087173462 4.2947773933410645
Loss :  1.6464357376098633 3.188730478286743 4.835165977478027
Loss :  1.6874735355377197 2.610948085784912 4.298421859741211
Loss :  1.6848502159118652 3.231605291366577 4.916455268859863
Loss :  1.6485530138015747 3.264352560043335 4.912905693054199
Loss :  1.6973766088485718 3.2189548015594482 4.9163312911987305
Loss :  1.6399208307266235 2.465559720993042 4.105480670928955
Loss :  1.6880663633346558 2.82029128074646 4.508357524871826
Loss :  1.6414653062820435 2.9194235801696777 4.560888767242432
Loss :  1.722740650177002 2.96811842918396 4.690858840942383
Loss :  1.6690782308578491 3.09859037399292 4.767668724060059
Loss :  1.6544065475463867 2.9264979362487793 4.580904483795166
Loss :  1.6568464040756226 2.68796968460083 4.344816207885742
Loss :  1.6967837810516357 3.335097074508667 5.031880855560303
Loss :  1.6888242959976196 2.8031680583953857 4.491992473602295
Loss :  1.666548490524292 2.790970802307129 4.45751953125
Loss :  1.634268879890442 2.5525405406951904 4.186809539794922
Loss :  1.65971040725708 3.027535915374756 4.687246322631836
Loss :  1.6522222757339478 2.904796600341797 4.557018756866455
  batch 40 loss: 1.6522222757339478, 2.904796600341797, 4.557018756866455
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6603306531906128 2.9266140460968018 4.586944580078125
Loss :  1.6485302448272705 2.579223394393921 4.227753639221191
Loss :  1.6654998064041138 2.7979440689086914 4.463443756103516
Loss :  1.6638305187225342 2.9843485355377197 4.648179054260254
Loss :  1.666786551475525 2.649456739425659 4.3162431716918945
Loss :  1.6581710577011108 2.9504973888397217 4.608668327331543
Loss :  1.645199179649353 3.2957327365875244 4.940931797027588
Loss :  1.6573874950408936 3.016484260559082 4.673871994018555
Loss :  1.6301318407058716 2.988657236099243 4.618789196014404
Loss :  1.6812371015548706 3.2293734550476074 4.910610675811768
Loss :  1.645865797996521 2.698075294494629 4.3439412117004395
Loss :  1.6642037630081177 3.0338833332061768 4.698087215423584
Loss :  1.682731032371521 3.3514106273651123 5.034141540527344
Loss :  1.6675244569778442 2.750969886779785 4.41849422454834
Loss :  1.6706724166870117 3.4839107990264893 5.154582977294922
Loss :  1.6350209712982178 2.619234800338745 4.254255771636963
Loss :  1.68471097946167 3.3325369358062744 5.017248153686523
Loss :  1.6821775436401367 3.2422444820404053 4.924422264099121
Loss :  1.6956465244293213 3.1465981006622314 4.842244625091553
Loss :  1.6709320545196533 2.837451457977295 4.508383750915527
  batch 60 loss: 1.6709320545196533, 2.837451457977295, 4.508383750915527
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726289987564087 3.4684433937072754 5.1410722732543945
Loss :  1.6693776845932007 2.811459541320801 4.480837345123291
Loss :  1.6774226427078247 3.1021955013275146 4.779618263244629
Loss :  1.6580275297164917 3.1247339248657227 4.782761573791504
Loss :  1.6550369262695312 3.330075979232788 4.985113143920898
Loss :  5.504293441772461 4.4276814460754395 9.931974411010742
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.394430637359619 4.328511714935303 9.722942352294922
Loss :  5.499629497528076 4.243438243865967 9.743067741394043
Loss :  4.582803726196289 4.16511869430542 8.747922897338867
Total LOSS train 4.620541469867413 valid 9.536476850509644
CE LOSS train 1.6640988129835863 valid 1.1457009315490723
Contrastive LOSS train 2.9564426458798923 valid 1.041279673576355
EPOCH 206:
Loss :  1.650477409362793 2.816178321838379 4.466655731201172
Loss :  1.666247844696045 3.2583730220794678 4.924620628356934
Loss :  1.652117371559143 3.2103450298309326 4.862462520599365
Loss :  1.6553233861923218 3.152043342590332 4.807366847991943
Loss :  1.6799461841583252 3.2122116088867188 4.892157554626465
Loss :  1.6629688739776611 2.855178117752075 4.518146991729736
Loss :  1.6610177755355835 3.5471792221069336 5.208197116851807
Loss :  1.6500097513198853 2.587817907333374 4.237827777862549
Loss :  1.654645323753357 2.3070175647735596 3.961662769317627
Loss :  1.609076976776123 3.053839921951294 4.662917137145996
Loss :  1.6687344312667847 3.4695308208465576 5.138265132904053
Loss :  1.7291814088821411 3.0418343544006348 4.771015644073486
Loss :  1.6762651205062866 2.978362798690796 4.654627799987793
Loss :  1.6643375158309937 3.4183640480041504 5.082701683044434
Loss :  1.6426342725753784 2.9256043434143066 4.568238735198975
Loss :  1.6521036624908447 3.0462121963500977 4.698315620422363
Loss :  1.6611485481262207 2.8079111576080322 4.469059944152832
Loss :  1.6595810651779175 2.5962817668914795 4.255862712860107
Loss :  1.6675218343734741 2.889390707015991 4.556912422180176
Loss :  1.6240713596343994 3.2684695720672607 4.89254093170166
  batch 20 loss: 1.6240713596343994, 3.2684695720672607, 4.89254093170166
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6593396663665771 3.193213939666748 4.852553367614746
Loss :  1.6750863790512085 2.7543745040893555 4.4294610023498535
Loss :  1.6464450359344482 2.796485185623169 4.442930221557617
Loss :  1.687453269958496 3.1655125617980957 4.852965831756592
Loss :  1.6848498582839966 3.4971578121185303 5.182007789611816
Loss :  1.648541808128357 3.1183061599731445 4.766848087310791
Loss :  1.6973906755447388 3.3104140758514404 5.007804870605469
Loss :  1.6399197578430176 2.9599671363830566 4.599886894226074
Loss :  1.6880781650543213 2.743807792663574 4.431885719299316
Loss :  1.6414579153060913 3.0335304737091064 4.674988269805908
Loss :  1.722731590270996 3.2180817127227783 4.940813064575195
Loss :  1.669089436531067 3.03326153755188 4.702351093292236
Loss :  1.6544086933135986 3.1421968936920166 4.796605587005615
Loss :  1.6568427085876465 2.669928789138794 4.3267717361450195
Loss :  1.6968085765838623 3.4531090259552 5.1499176025390625
Loss :  1.6888411045074463 2.6625940799713135 4.35143518447876
Loss :  1.666550874710083 2.667125940322876 4.333676815032959
Loss :  1.6342883110046387 2.5909299850463867 4.225218296051025
Loss :  1.659711480140686 2.89168381690979 4.551395416259766
Loss :  1.652217984199524 2.979231595993042 4.6314496994018555
  batch 40 loss: 1.652217984199524, 2.979231595993042, 4.6314496994018555
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6603342294692993 3.2171013355255127 4.877435684204102
Loss :  1.6485445499420166 2.7175145149230957 4.366059303283691
Loss :  1.6655011177062988 2.904841899871826 4.570343017578125
Loss :  1.6638236045837402 3.00923752784729 4.673061370849609
Loss :  1.6667916774749756 2.6975157260894775 4.364307403564453
Loss :  1.6581798791885376 3.0007805824279785 4.658960342407227
Loss :  1.645196795463562 3.1377882957458496 4.782985210418701
Loss :  1.657388687133789 3.215285062789917 4.872673988342285
Loss :  1.6301342248916626 3.4329135417938232 5.063047885894775
Loss :  1.6812502145767212 2.912553071975708 4.593803405761719
Loss :  1.645857334136963 2.731680154800415 4.377537727355957
Loss :  1.6641991138458252 2.5664639472961426 4.230663299560547
Loss :  1.6827330589294434 3.458984375 5.141717433929443
Loss :  1.6675256490707397 3.0073697566986084 4.674895286560059
Loss :  1.6706656217575073 3.152984857559204 4.823650360107422
Loss :  1.6350082159042358 2.802603244781494 4.4376115798950195
Loss :  1.684700846672058 3.2616934776306152 4.946394443511963
Loss :  1.6821871995925903 3.197401762008667 4.879589080810547
Loss :  1.695648193359375 3.125138759613037 4.820786952972412
Loss :  1.6709195375442505 2.8356740474700928 4.506593704223633
  batch 60 loss: 1.6709195375442505, 2.8356740474700928, 4.506593704223633
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726330518722534 3.4197185039520264 5.09235143661499
Loss :  1.6693886518478394 2.669268846511841 4.338657379150391
Loss :  1.6774338483810425 2.973475694656372 4.650909423828125
Loss :  1.6580278873443604 3.031858205795288 4.689886093139648
Loss :  1.6550475358963013 2.946620464324951 4.601667881011963
Loss :  5.50377082824707 4.416808128356934 9.920578956604004
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.3940558433532715 4.3627848625183105 9.756840705871582
Loss :  5.499754428863525 4.318609714508057 9.818364143371582
Loss :  4.582561492919922 4.285384654998779 8.86794662475586
Total LOSS train 4.675632491478553 valid 9.590932607650757
CE LOSS train 1.664101294370798 valid 1.1456403732299805
Contrastive LOSS train 3.0115311769338753 valid 1.0713461637496948
EPOCH 207:
Loss :  1.6504765748977661 2.682278871536255 4.3327555656433105
Loss :  1.6662565469741821 3.213083028793335 4.879339694976807
Loss :  1.6521263122558594 2.8765997886657715 4.528726100921631
Loss :  1.6553150415420532 2.8363029956817627 4.4916181564331055
Loss :  1.6799530982971191 3.122492551803589 4.802445411682129
Loss :  1.662968635559082 2.7661802768707275 4.4291486740112305
Loss :  1.6610180139541626 3.2434566020965576 4.90447473526001
Loss :  1.65000581741333 2.4378297328948975 4.087835311889648
Loss :  1.6546406745910645 2.284898281097412 3.9395389556884766
Loss :  1.6090775728225708 2.771838903427124 4.380916595458984
Loss :  1.6687352657318115 3.38881254196167 5.057547569274902
Loss :  1.7291852235794067 3.0091421604156494 4.738327503204346
Loss :  1.6762882471084595 3.046261787414551 4.722549915313721
Loss :  1.6643364429473877 3.286501884460449 4.950838088989258
Loss :  1.6426324844360352 3.1975197792053223 4.840152263641357
Loss :  1.6521104574203491 3.108128547668457 4.760239124298096
Loss :  1.6611557006835938 2.8914809226989746 4.552636623382568
Loss :  1.6595780849456787 2.8470890522003174 4.506667137145996
Loss :  1.667528510093689 2.938164472579956 4.6056928634643555
Loss :  1.6240746974945068 3.284552574157715 4.908627510070801
  batch 20 loss: 1.6240746974945068, 3.284552574157715, 4.908627510070801
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6593366861343384 3.498211145401001 5.157547950744629
Loss :  1.6750746965408325 2.8613927364349365 4.536467552185059
Loss :  1.6464345455169678 3.2010343074798584 4.847468852996826
Loss :  1.6874462366104126 3.0392327308654785 4.726678848266602
Loss :  1.6848565340042114 3.4373412132263184 5.12219762802124
Loss :  1.648535132408142 3.0574121475219727 4.705947399139404
Loss :  1.6973943710327148 3.093364715576172 4.790759086608887
Loss :  1.6399199962615967 2.7247276306152344 4.36464786529541
Loss :  1.6880815029144287 2.6617419719696045 4.349823474884033
Loss :  1.6414600610733032 2.9723360538482666 4.613796234130859
Loss :  1.7227301597595215 3.1072020530700684 4.82993221282959
Loss :  1.6690905094146729 3.0259625911712646 4.6950531005859375
Loss :  1.6544221639633179 3.0735599994659424 4.727982044219971
Loss :  1.6568503379821777 2.7841317653656006 4.440981864929199
Loss :  1.6968145370483398 3.3173844814300537 5.014199256896973
Loss :  1.688851237297058 2.8578786849975586 4.546730041503906
Loss :  1.6665509939193726 2.9452662467956543 4.611817359924316
Loss :  1.6342997550964355 2.4367587566375732 4.07105827331543
Loss :  1.6597157716751099 2.9109559059143066 4.570671558380127
Loss :  1.6522166728973389 3.1709468364715576 4.8231635093688965
  batch 40 loss: 1.6522166728973389, 3.1709468364715576, 4.8231635093688965
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6603318452835083 3.237255334854126 4.897587299346924
Loss :  1.648547649383545 2.8858091831207275 4.534357070922852
Loss :  1.6654951572418213 2.9506540298461914 4.616148948669434
Loss :  1.6638388633728027 3.360590696334839 5.0244293212890625
Loss :  1.666789174079895 2.466533660888672 4.133322715759277
Loss :  1.6581743955612183 2.768228769302368 4.426403045654297
Loss :  1.6452016830444336 2.925647497177124 4.570849418640137
Loss :  1.657394528388977 3.092639446258545 4.750033855438232
Loss :  1.6301249265670776 3.5729820728302 5.203106880187988
Loss :  1.6812421083450317 2.772538185119629 4.453780174255371
Loss :  1.6458545923233032 2.523686408996582 4.169540882110596
Loss :  1.664192795753479 2.762960195541382 4.42715311050415
Loss :  1.6827338933944702 3.2225639820098877 4.905297756195068
Loss :  1.6675351858139038 3.107919216156006 4.775454521179199
Loss :  1.6706655025482178 3.3843064308166504 5.054971694946289
Loss :  1.635023832321167 2.9252476692199707 4.560271263122559
Loss :  1.6846963167190552 3.058605432510376 4.743301868438721
Loss :  1.6821874380111694 3.0918681621551514 4.774055480957031
Loss :  1.695654034614563 3.022928476333618 4.718582630157471
Loss :  1.6709240674972534 2.706829309463501 4.377753257751465
  batch 60 loss: 1.6709240674972534, 2.706829309463501, 4.377753257751465
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726347208023071 3.367511510848999 5.040146350860596
Loss :  1.669388771057129 2.8442647457122803 4.513653755187988
Loss :  1.6774377822875977 3.077857494354248 4.755295276641846
Loss :  1.6580387353897095 3.06675124168396 4.724790096282959
Loss :  1.6550498008728027 3.08266544342041 4.737715244293213
Loss :  5.505118370056152 4.3328094482421875 9.83792781829834
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.395259380340576 4.390838623046875 9.78609848022461
Loss :  5.501575469970703 4.310784339904785 9.812359809875488
Loss :  4.582696437835693 4.2200493812561035 8.802745819091797
Total LOSS train 4.659307751288781 valid 9.559782981872559
CE LOSS train 1.6641031246918898 valid 1.1456741094589233
Contrastive LOSS train 2.9952046357668363 valid 1.0550123453140259
EPOCH 208:
Loss :  1.65049409866333 2.7391631603240967 4.389657020568848
Loss :  1.6662653684616089 2.880155086517334 4.546420574188232
Loss :  1.6521302461624146 3.0872485637664795 4.739378929138184
Loss :  1.6553184986114502 3.083890438079834 4.739209175109863
Loss :  1.6799544095993042 3.596851348876953 5.276805877685547
Loss :  1.662968635559082 2.668091058731079 4.331059455871582
Loss :  1.661020040512085 3.3725087642669678 5.033528804779053
Loss :  1.65000319480896 2.6002037525177 4.25020694732666
Loss :  1.6546398401260376 2.349496603012085 4.004136562347412
Loss :  1.6090731620788574 2.9244325160980225 4.533505439758301
Loss :  1.6687324047088623 3.188572645187378 4.85730504989624
Loss :  1.7292139530181885 2.888152837753296 4.617366790771484
Loss :  1.6763029098510742 3.033238649368286 4.709541320800781
Loss :  1.6643379926681519 3.2382490634918213 4.902586936950684
Loss :  1.6426374912261963 2.8715200424194336 4.514157295227051
Loss :  1.652118444442749 3.0408473014831543 4.692965507507324
Loss :  1.6611629724502563 2.754127025604248 4.415289878845215
Loss :  1.6595746278762817 2.963230609893799 4.622805118560791
Loss :  1.6675269603729248 2.712258815765381 4.379785537719727
Loss :  1.6240757703781128 3.1003270149230957 4.724402904510498
  batch 20 loss: 1.6240757703781128, 3.1003270149230957, 4.724402904510498
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.659333348274231 3.212938070297241 4.872271537780762
Loss :  1.6750737428665161 2.624373197555542 4.299447059631348
Loss :  1.6464258432388306 3.2696030139923096 4.91602897644043
Loss :  1.6874573230743408 2.9660470485687256 4.653504371643066
Loss :  1.6848617792129517 3.4615767002105713 5.1464385986328125
Loss :  1.648532748222351 3.036870241165161 4.685402870178223
Loss :  1.697399377822876 3.305260181427002 5.002659797668457
Loss :  1.6399226188659668 2.6431329250335693 4.283055305480957
Loss :  1.6880801916122437 2.6907379627227783 4.378818035125732
Loss :  1.6414557695388794 2.840988874435425 4.482444763183594
Loss :  1.722732424736023 3.067065477371216 4.789797782897949
Loss :  1.6690900325775146 3.1631052494049072 4.832195281982422
Loss :  1.6544280052185059 3.274536609649658 4.928964614868164
Loss :  1.6568489074707031 2.683518648147583 4.340367317199707
Loss :  1.6968069076538086 3.3512415885925293 5.048048496246338
Loss :  1.6888474225997925 2.990676164627075 4.679523468017578
Loss :  1.6665503978729248 2.709373950958252 4.375924110412598
Loss :  1.6342976093292236 2.6331846714019775 4.267482280731201
Loss :  1.6597132682800293 3.0199482440948486 4.679661750793457
Loss :  1.652216911315918 3.081425428390503 4.733642578125
  batch 40 loss: 1.652216911315918, 3.081425428390503, 4.733642578125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.660327672958374 3.3454599380493164 5.0057878494262695
Loss :  1.648543119430542 2.7991366386413574 4.44767951965332
Loss :  1.6654863357543945 2.895217180252075 4.560703277587891
Loss :  1.6638420820236206 2.9104113578796387 4.574253559112549
Loss :  1.6667717695236206 2.496772289276123 4.163544178009033
Loss :  1.6581634283065796 3.123344659805298 4.781507968902588
Loss :  1.6451923847198486 3.1400370597839355 4.785229682922363
Loss :  1.6573803424835205 2.8766746520996094 4.534054756164551
Loss :  1.6301302909851074 3.522775173187256 5.152905464172363
Loss :  1.6812368631362915 3.1893022060394287 4.87053918838501
Loss :  1.6458534002304077 2.6717050075531006 4.317558288574219
Loss :  1.6641851663589478 2.927647352218628 4.591832637786865
Loss :  1.6827272176742554 3.210477590560913 4.893204689025879
Loss :  1.6675148010253906 3.0059540271759033 4.673468589782715
Loss :  1.6706582307815552 3.2357146739959717 4.906373023986816
Loss :  1.6350159645080566 2.6212422847747803 4.256258010864258
Loss :  1.6846946477890015 3.2904388904571533 4.975133419036865
Loss :  1.6821844577789307 3.185286521911621 4.867470741271973
Loss :  1.6956533193588257 3.125762701034546 4.821415901184082
Loss :  1.6709214448928833 2.77030086517334 4.441222190856934
  batch 60 loss: 1.6709214448928833, 2.77030086517334, 4.441222190856934
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.672635555267334 3.596719264984131 5.269354820251465
Loss :  1.6693819761276245 2.6375324726104736 4.306914329528809
Loss :  1.6774299144744873 3.0021677017211914 4.679597854614258
Loss :  1.6580369472503662 2.7351558208465576 4.393192768096924
Loss :  1.6550401449203491 3.06488037109375 4.719920635223389
Loss :  5.504748344421387 4.396595478057861 9.901344299316406
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.394785404205322 4.352759838104248 9.74754524230957
Loss :  5.5018486976623535 4.288184642791748 9.790033340454102
Loss :  4.581806659698486 4.27923059463501 8.861037254333496
Total LOSS train 4.656383345677303 valid 9.574990034103394
CE LOSS train 1.6641020481403057 valid 1.1454516649246216
Contrastive LOSS train 2.9922813268808217 valid 1.0698076486587524
EPOCH 209:
Loss :  1.650488018989563 2.850963830947876 4.5014519691467285
Loss :  1.6662545204162598 3.352109670639038 5.018363952636719
Loss :  1.6521254777908325 2.8072104454040527 4.459335803985596
Loss :  1.655318021774292 3.016437530517578 4.671755790710449
Loss :  1.6799486875534058 3.1379079818725586 4.817856788635254
Loss :  1.6629681587219238 2.7857885360717773 4.448756694793701
Loss :  1.6610252857208252 3.2471253871917725 4.908150672912598
Loss :  1.6500009298324585 2.4804232120513916 4.1304240226745605
Loss :  1.6546391248703003 2.3440744876861572 3.998713493347168
Loss :  1.609070062637329 2.8588454723358154 4.4679155349731445
Loss :  1.6687216758728027 3.2591004371643066 4.927822113037109
Loss :  1.7292183637619019 3.159818172454834 4.889036655426025
Loss :  1.6762909889221191 3.39302659034729 5.069317817687988
Loss :  1.6643444299697876 3.649686574935913 5.31403112411499
Loss :  1.642630696296692 2.89029860496521 4.532929420471191
Loss :  1.6521224975585938 2.968219518661499 4.620342254638672
Loss :  1.6611638069152832 2.650942802429199 4.312106609344482
Loss :  1.6595619916915894 2.8158340454101562 4.475396156311035
Loss :  1.6675128936767578 2.699200391769409 4.366713523864746
Loss :  1.62408447265625 2.8299033641815186 4.453988075256348
  batch 20 loss: 1.62408447265625, 2.8299033641815186, 4.453988075256348
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6593338251113892 3.160759925842285 4.820093631744385
Loss :  1.675074815750122 2.4098503589630127 4.084925174713135
Loss :  1.6464245319366455 2.8695387840270996 4.515963554382324
Loss :  1.687450885772705 2.9599103927612305 4.6473612785339355
Loss :  1.6848649978637695 3.8205337524414062 5.505398750305176
Loss :  1.6485244035720825 3.036374092102051 4.684898376464844
Loss :  1.6974010467529297 3.485645294189453 5.183046340942383
Loss :  1.6399154663085938 2.72432017326355 4.364235877990723
Loss :  1.6880792379379272 2.960031270980835 4.648110389709473
Loss :  1.6414557695388794 2.938427686691284 4.579883575439453
Loss :  1.7227225303649902 3.0398216247558594 4.76254415512085
Loss :  1.669088363647461 3.0558340549468994 4.724922180175781
Loss :  1.6544235944747925 3.0552570819854736 4.709680557250977
Loss :  1.6568397283554077 2.7675294876098633 4.4243693351745605
Loss :  1.6968152523040771 3.2506744861602783 4.9474897384643555
Loss :  1.6888489723205566 2.753398895263672 4.4422478675842285
Loss :  1.666549563407898 3.008435010910034 4.674984455108643
Loss :  1.6342989206314087 2.6134190559387207 4.24771785736084
Loss :  1.6597118377685547 2.9102301597595215 4.569941997528076
Loss :  1.6522109508514404 3.2196128368377686 4.871823787689209
  batch 40 loss: 1.6522109508514404, 3.2196128368377686, 4.871823787689209
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6603220701217651 3.2617576122283936 4.922079563140869
Loss :  1.6485501527786255 2.722822904586792 4.371373176574707
Loss :  1.6654810905456543 2.637603521347046 4.303084373474121
Loss :  1.6638473272323608 3.091559886932373 4.755407333374023
Loss :  1.6667720079421997 2.74391770362854 4.410689830780029
Loss :  1.6581627130508423 2.9305808544158936 4.588743686676025
Loss :  1.6451822519302368 3.2233476638793945 4.868529796600342
Loss :  1.6573816537857056 3.0607922077178955 4.718173980712891
Loss :  1.630133032798767 3.6528546810150146 5.282987594604492
Loss :  1.6812437772750854 2.950897455215454 4.63214111328125
Loss :  1.6458430290222168 2.8373706340789795 4.483213424682617
Loss :  1.664172887802124 2.9898521900177 4.654025077819824
Loss :  1.682724952697754 3.6631975173950195 5.345922470092773
Loss :  1.6675137281417847 2.9207637310028076 4.588277339935303
Loss :  1.6706551313400269 3.0920722484588623 4.7627272605896
Loss :  1.635010004043579 2.676175832748413 4.311185836791992
Loss :  1.6846872568130493 3.2027781009674072 4.887465476989746
Loss :  1.682186245918274 3.372537136077881 5.054723262786865
Loss :  1.6956455707550049 3.1608970165252686 4.856542587280273
Loss :  1.670926809310913 2.9814345836639404 4.6523613929748535
  batch 60 loss: 1.670926809310913, 2.9814345836639404, 4.6523613929748535
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726319789886475 3.5282607078552246 5.200892448425293
Loss :  1.6693909168243408 2.9365880489349365 4.605978965759277
Loss :  1.6774321794509888 3.2188639640808105 4.89629602432251
Loss :  1.6580396890640259 3.10709285736084 4.765132427215576
Loss :  1.6550410985946655 3.1976592540740967 4.852700233459473
Loss :  5.504806995391846 4.398964881896973 9.903772354125977
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.394493579864502 4.374918460845947 9.76941204071045
Loss :  5.502215385437012 4.262932777404785 9.765148162841797
Loss :  4.581458568572998 4.136404037475586 8.717863082885742
Total LOSS train 4.685610800523024 valid 9.539048910140991
CE LOSS train 1.664100036254296 valid 1.1453646421432495
Contrastive LOSS train 3.0215107661027174 valid 1.0341010093688965
EPOCH 210:
Loss :  1.6504884958267212 2.8345181941986084 4.485006809234619
Loss :  1.6662591695785522 3.49454402923584 5.160803318023682
Loss :  1.65213143825531 3.0288140773773193 4.68094539642334
Loss :  1.6553215980529785 3.0641977787017822 4.71951961517334
Loss :  1.6799542903900146 3.385612964630127 5.0655670166015625
Loss :  1.6629700660705566 2.903045892715454 4.56601619720459
Loss :  1.6610230207443237 3.478445529937744 5.139468669891357
Loss :  1.6500004529953003 2.461613178253174 4.111613750457764
Loss :  1.6546381711959839 2.4096672534942627 4.064305305480957
Loss :  1.6090660095214844 2.7612621784210205 4.370327949523926
Loss :  1.668723225593567 3.1453511714935303 4.814074516296387
Loss :  1.7292144298553467 2.9812400341033936 4.71045446395874
Loss :  1.676314115524292 2.852242946624756 4.528556823730469
Loss :  1.6643457412719727 3.471482276916504 5.135828018188477
Loss :  1.6426202058792114 2.949601173400879 4.592221260070801
Loss :  1.6521213054656982 3.074418306350708 4.726539611816406
Loss :  1.6611604690551758 2.9969828128814697 4.658143043518066
Loss :  1.659574270248413 2.92522931098938 4.584803581237793
Loss :  1.667521357536316 2.7762484550476074 4.443769931793213
Loss :  1.624078392982483 3.23231840133667 4.856396675109863
  batch 20 loss: 1.624078392982483, 3.23231840133667, 4.856396675109863
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6593267917633057 3.302762269973755 4.9620890617370605
Loss :  1.6750736236572266 2.460674285888672 4.135747909545898
Loss :  1.6464165449142456 3.043884515762329 4.690300941467285
Loss :  1.6874481439590454 2.846632480621338 4.534080505371094
Loss :  1.6848686933517456 3.5456924438476562 5.230561256408691
Loss :  1.648521065711975 3.192844867706299 4.841365814208984
Loss :  1.6973896026611328 2.986546277999878 4.68393611907959
Loss :  1.6399123668670654 2.726022243499756 4.365934371948242
Loss :  1.6880754232406616 2.8030688762664795 4.491144180297852
Loss :  1.641457200050354 2.842639923095703 4.484097003936768
Loss :  1.7227152585983276 3.223663568496704 4.946378707885742
Loss :  1.66908860206604 3.0681185722351074 4.737207412719727
Loss :  1.6544268131256104 2.9368791580200195 4.591305732727051
Loss :  1.6568408012390137 2.654038667678833 4.310879707336426
Loss :  1.6968145370483398 3.6240594387054443 5.320874214172363
Loss :  1.6888457536697388 2.717254400253296 4.406100273132324
Loss :  1.6665443181991577 2.9697766304016113 4.636321067810059
Loss :  1.6342947483062744 2.697946071624756 4.332241058349609
Loss :  1.659708857536316 3.043003797531128 4.702712535858154
Loss :  1.6522080898284912 3.1448357105255127 4.797043800354004
  batch 40 loss: 1.6522080898284912, 3.1448357105255127, 4.797043800354004
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6603169441223145 2.9509360790252686 4.611252784729004
Loss :  1.6485397815704346 2.681018829345703 4.329558372497559
Loss :  1.6654855012893677 3.0162675380706787 4.681753158569336
Loss :  1.6638457775115967 2.991698741912842 4.655544281005859
Loss :  1.6667612791061401 2.6457951068878174 4.312556266784668
Loss :  1.6581592559814453 2.885737657546997 4.543896675109863
Loss :  1.6451796293258667 3.1528820991516113 4.798061847686768
Loss :  1.6573799848556519 3.01816987991333 4.6755499839782715
Loss :  1.630126714706421 3.402899742126465 5.033026695251465
Loss :  1.6812502145767212 3.014986276626587 4.696236610412598
Loss :  1.6458489894866943 2.653812885284424 4.299661636352539
Loss :  1.6641807556152344 3.030778169631958 4.694958686828613
Loss :  1.6827192306518555 3.4039359092712402 5.086655139923096
Loss :  1.6675140857696533 2.9275689125061035 4.595083236694336
Loss :  1.6706761121749878 3.1399192810058594 4.810595512390137
Loss :  1.635022521018982 2.7782320976257324 4.413254737854004
Loss :  1.6846922636032104 2.992546319961548 4.677238464355469
Loss :  1.6821905374526978 3.3657195568084717 5.047910213470459
Loss :  1.6956452131271362 3.0888214111328125 4.784466743469238
Loss :  1.6709249019622803 2.9841392040252686 4.655064105987549
  batch 60 loss: 1.6709249019622803, 2.9841392040252686, 4.655064105987549
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726323366165161 3.619553804397583 5.292186260223389
Loss :  1.6693888902664185 2.7168304920196533 4.386219501495361
Loss :  1.677438735961914 3.273465633392334 4.950904369354248
Loss :  1.658042550086975 3.0756542682647705 4.733696937561035
Loss :  1.6550488471984863 2.8721606731414795 4.527209281921387
Loss :  5.50580358505249 4.387350559234619 9.89315414428711
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.395595550537109 4.341784954071045 9.737380981445312
Loss :  5.503394603729248 4.209259986877441 9.712654113769531
Loss :  4.581787109375 4.123551845550537 8.705339431762695
Total LOSS train 4.675495771261362 valid 9.512132167816162
CE LOSS train 1.664100223321181 valid 1.14544677734375
Contrastive LOSS train 3.01139554977417 valid 1.0308879613876343
EPOCH 211:
Loss :  1.650490641593933 2.868375301361084 4.518866062164307
Loss :  1.6662529706954956 3.3845341205596924 5.050786972045898
Loss :  1.6521332263946533 2.9674222469329834 4.619555473327637
Loss :  1.6553250551223755 2.758913993835449 4.414238929748535
Loss :  1.6799448728561401 3.3472256660461426 5.027170658111572
Loss :  1.6629698276519775 2.7821104526519775 4.445080280303955
Loss :  1.6610205173492432 3.449448347091675 5.110468864440918
Loss :  1.6499994993209839 2.381033182144165 4.031032562255859
Loss :  1.654636025428772 2.2921178340911865 3.946753978729248
Loss :  1.6090699434280396 3.1045892238616943 4.713659286499023
Loss :  1.668723702430725 3.2781190872192383 4.946842670440674
Loss :  1.7292048931121826 2.906019926071167 4.63522481918335
Loss :  1.6762933731079102 3.04080867767334 4.71710205078125
Loss :  1.664351463317871 3.271599292755127 4.935950756072998
Loss :  1.6426126956939697 3.1760663986206055 4.818678855895996
Loss :  1.6521347761154175 3.013913869857788 4.666048526763916
Loss :  1.6611614227294922 2.8961334228515625 4.557294845581055
Loss :  1.6595807075500488 2.953030586242676 4.612611293792725
Loss :  1.6675409078598022 2.767242193222046 4.434782981872559
Loss :  1.6240670680999756 3.2881171703338623 4.912184238433838
  batch 20 loss: 1.6240670680999756, 3.2881171703338623, 4.912184238433838
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6593149900436401 3.081601619720459 4.740916728973389
Loss :  1.6750768423080444 2.6889548301696777 4.364031791687012
Loss :  1.646417498588562 2.748483419418335 4.394900798797607
Loss :  1.6874477863311768 2.9210031032562256 4.608450889587402
Loss :  1.6848703622817993 3.6605124473571777 5.3453826904296875
Loss :  1.6485130786895752 2.8849101066589355 4.53342342376709
Loss :  1.6973903179168701 2.961758852005005 4.659149169921875
Loss :  1.6399165391921997 2.959613561630249 4.599530220031738
Loss :  1.6880813837051392 2.6469204425811768 4.3350019454956055
Loss :  1.6414533853530884 3.065101385116577 4.706554889678955
Loss :  1.7227083444595337 2.988147497177124 4.710855960845947
Loss :  1.6690841913223267 3.183504581451416 4.852588653564453
Loss :  1.6544368267059326 3.0315072536468506 4.685944080352783
Loss :  1.6568427085876465 2.704257011413574 4.361099720001221
Loss :  1.6968183517456055 3.6279184818267822 5.324736595153809
Loss :  1.6888465881347656 2.6021065711975098 4.290953159332275
Loss :  1.666540503501892 3.0377917289733887 4.70433235168457
Loss :  1.6342978477478027 2.773806095123291 4.408103942871094
Loss :  1.659709095954895 3.0241715908050537 4.683880805969238
Loss :  1.6522128582000732 3.3631250858306885 5.015337944030762
  batch 40 loss: 1.6522128582000732, 3.3631250858306885, 5.015337944030762
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6603193283081055 3.2409942150115967 4.901313781738281
Loss :  1.6485406160354614 2.845586061477661 4.494126796722412
Loss :  1.6654859781265259 3.027958393096924 4.69344425201416
Loss :  1.6638506650924683 3.010666847229004 4.674517631530762
Loss :  1.6667540073394775 2.5075998306274414 4.17435359954834
Loss :  1.6581614017486572 2.7739005088806152 4.432062149047852
Loss :  1.6451807022094727 2.9920096397399902 4.637190341949463
Loss :  1.6573807001113892 2.9989001750946045 4.656280994415283
Loss :  1.6301321983337402 3.042869806289673 4.673002243041992
Loss :  1.6812540292739868 3.054575204849243 4.7358293533325195
Loss :  1.6458516120910645 2.6716437339782715 4.317495346069336
Loss :  1.6641886234283447 2.8104803562164307 4.474668979644775
Loss :  1.682719349861145 3.398507595062256 5.081226825714111
Loss :  1.6675097942352295 2.820077657699585 4.4875874519348145
Loss :  1.6706794500350952 3.143885612487793 4.814565181732178
Loss :  1.635021686553955 2.828721284866333 4.463743209838867
Loss :  1.6846967935562134 3.108612537384033 4.793309211730957
Loss :  1.682189702987671 3.1482744216918945 4.8304643630981445
Loss :  1.6956512928009033 3.0235493183135986 4.719200611114502
Loss :  1.670935034751892 2.952324390411377 4.623259544372559
  batch 60 loss: 1.670935034751892, 2.952324390411377, 4.623259544372559
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726295948028564 3.471130132675171 5.143759727478027
Loss :  1.6693929433822632 2.7935633659362793 4.462956428527832
Loss :  1.6774370670318604 3.39103102684021 5.06846809387207
Loss :  1.6580430269241333 2.7872836589813232 4.445326805114746
Loss :  1.655043125152588 3.179734230041504 4.834777355194092
Loss :  5.5048346519470215 4.359619140625 9.86445426940918
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.394299030303955 4.367613315582275 9.76191234588623
Loss :  5.50247859954834 4.1602349281311035 9.662714004516602
Loss :  4.580759525299072 4.195380687713623 8.776140213012695
Total LOSS train 4.662652925344614 valid 9.516305208206177
CE LOSS train 1.664100643304678 valid 1.145189881324768
Contrastive LOSS train 2.9985522563640887 valid 1.0488451719284058
EPOCH 212:
Loss :  1.6504924297332764 2.9171178340911865 4.567610263824463
Loss :  1.6662527322769165 3.1260504722595215 4.792303085327148
Loss :  1.652127742767334 2.8406479358673096 4.492775917053223
Loss :  1.6553207635879517 2.794684886932373 4.450005531311035
Loss :  1.6799514293670654 3.3784561157226562 5.058407783508301
Loss :  1.6629718542099 2.632476329803467 4.295448303222656
Loss :  1.6610348224639893 3.2814218997955322 4.9424567222595215
Loss :  1.6499981880187988 2.5567076206207275 4.2067060470581055
Loss :  1.6546379327774048 2.5402190685272217 4.194857120513916
Loss :  1.6090682744979858 2.9138827323913574 4.522951126098633
Loss :  1.6687206029891968 3.276925802230835 4.945646286010742
Loss :  1.7292134761810303 3.058758020401001 4.787971496582031
Loss :  1.6762813329696655 3.0413005352020264 4.717581748962402
Loss :  1.6643636226654053 3.287053346633911 4.951416969299316
Loss :  1.6426202058792114 2.8936688899993896 4.536289215087891
Loss :  1.6521317958831787 3.163060188293457 4.815192222595215
Loss :  1.6611593961715698 2.9988296031951904 4.659988880157471
Loss :  1.6595709323883057 2.6339094638824463 4.293480396270752
Loss :  1.6675225496292114 2.977149486541748 4.64467191696167
Loss :  1.624075174331665 3.1823012828826904 4.8063764572143555
  batch 20 loss: 1.624075174331665, 3.1823012828826904, 4.8063764572143555
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.659324288368225 3.160041332244873 4.819365501403809
Loss :  1.6750726699829102 2.829962730407715 4.505035400390625
Loss :  1.6464178562164307 2.9223573207855225 4.568775177001953
Loss :  1.687450885772705 3.115037202835083 4.802488327026367
Loss :  1.6848771572113037 3.230652332305908 4.915529251098633
Loss :  1.6485151052474976 3.212407112121582 4.860922336578369
Loss :  1.6973949670791626 3.3333914279937744 5.030786514282227
Loss :  1.6399116516113281 3.085141181945801 4.725052833557129
Loss :  1.6880767345428467 2.6205661296844482 4.308642864227295
Loss :  1.6414514780044556 2.807863712310791 4.449315071105957
Loss :  1.7227026224136353 3.118225574493408 4.840928077697754
Loss :  1.6690824031829834 2.846064567565918 4.5151472091674805
Loss :  1.6544318199157715 2.9220197200775146 4.576451301574707
Loss :  1.6568357944488525 3.0445666313171387 4.70140266418457
Loss :  1.6968083381652832 3.3655054569244385 5.062314033508301
Loss :  1.6888530254364014 2.7790112495422363 4.467864036560059
Loss :  1.666547179222107 3.016331672668457 4.6828789710998535
Loss :  1.63428795337677 2.5343024730682373 4.168590545654297
Loss :  1.6597094535827637 2.9375221729278564 4.597231864929199
Loss :  1.652215838432312 3.070115804672241 4.722331523895264
  batch 40 loss: 1.652215838432312, 3.070115804672241, 4.722331523895264
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6603161096572876 3.1713483333587646 4.831664562225342
Loss :  1.6485483646392822 2.9087600708007812 4.557308197021484
Loss :  1.6654821634292603 3.0011532306671143 4.666635513305664
Loss :  1.663852572441101 2.9786717891693115 4.642524242401123
Loss :  1.666744589805603 2.781376838684082 4.448121547698975
Loss :  1.6581557989120483 2.868682861328125 4.526838779449463
Loss :  1.645174264907837 3.0765433311462402 4.721717834472656
Loss :  1.6573641300201416 3.138213872909546 4.7955780029296875
Loss :  1.630128026008606 3.392791986465454 5.02292013168335
Loss :  1.681237816810608 2.9652130603790283 4.646450996398926
Loss :  1.6458401679992676 2.7333149909973145 4.379155158996582
Loss :  1.6641745567321777 2.7315688133239746 4.395743370056152
Loss :  1.6827222108840942 3.3148512840270996 4.997573375701904
Loss :  1.6674890518188477 3.1526479721069336 4.820137023925781
Loss :  1.670664668083191 3.5952675342559814 5.265932083129883
Loss :  1.6350157260894775 2.6915695667266846 4.326585292816162
Loss :  1.6846879720687866 3.250523328781128 4.935211181640625
Loss :  1.6821812391281128 3.3391518592834473 5.02133321762085
Loss :  1.6956517696380615 2.9623255729675293 4.657977104187012
Loss :  1.6709322929382324 2.9752039909362793 4.646136283874512
  batch 60 loss: 1.6709322929382324, 2.9752039909362793, 4.646136283874512
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726315021514893 3.299945831298828 4.972577095031738
Loss :  1.6693838834762573 2.817596912384033 4.48698091506958
Loss :  1.6774317026138306 3.2960963249206543 4.973527908325195
Loss :  1.6580435037612915 2.815880060195923 4.473923683166504
Loss :  1.655051827430725 2.824956178665161 4.480008125305176
Loss :  5.501664638519287 4.416602611541748 9.918267250061035
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.390987396240234 4.408365726470947 9.799352645874023
Loss :  5.499832630157471 4.238622665405273 9.738454818725586
Loss :  4.578005790710449 4.320272445678711 8.89827823638916
Total LOSS train 4.672242348010723 valid 9.588588237762451
CE LOSS train 1.664098282960745 valid 1.1445014476776123
Contrastive LOSS train 3.0081440448760985 valid 1.0800681114196777
EPOCH 213:
Loss :  1.6504801511764526 2.5581462383270264 4.2086262702941895
Loss :  1.6662575006484985 3.1905288696289062 4.856786251068115
Loss :  1.6521191596984863 2.93536376953125 4.587482929229736
Loss :  1.655287265777588 2.80566143989563 4.460948944091797
Loss :  1.679945468902588 3.115208148956299 4.795153617858887
Loss :  1.662951946258545 2.914428234100342 4.577380180358887
Loss :  1.6610397100448608 3.6050937175750732 5.2661333084106445
Loss :  1.649998426437378 2.4947075843811035 4.144705772399902
Loss :  1.654634952545166 2.707275390625 4.361910343170166
Loss :  1.6090741157531738 3.2677712440490723 4.876845359802246
Loss :  1.668718695640564 3.2702715396881104 4.938990116119385
Loss :  1.7291913032531738 2.749373197555542 4.478564262390137
Loss :  1.6763089895248413 3.0527873039245605 4.729096412658691
Loss :  1.6643667221069336 3.2029829025268555 4.867349624633789
Loss :  1.64261794090271 3.012249231338501 4.654867172241211
Loss :  1.652124285697937 3.176304817199707 4.828429222106934
Loss :  1.6611520051956177 2.6617913246154785 4.322943210601807
Loss :  1.659558892250061 3.0919392108917236 4.751498222351074
Loss :  1.6675145626068115 3.117068290710449 4.78458309173584
Loss :  1.6240662336349487 2.990192413330078 4.614258766174316
  batch 20 loss: 1.6240662336349487, 2.990192413330078, 4.614258766174316
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6593327522277832 2.938570261001587 4.597903251647949
Loss :  1.6750643253326416 2.457892894744873 4.132957458496094
Loss :  1.6464242935180664 3.1149468421936035 4.76137113571167
Loss :  1.6874399185180664 3.2609028816223145 4.948342800140381
Loss :  1.6848753690719604 3.408273458480835 5.093148708343506
Loss :  1.6485226154327393 3.1122169494628906 4.760739326477051
Loss :  1.6973909139633179 2.9810409545898438 4.678431987762451
Loss :  1.6399049758911133 2.867461919784546 4.507367134094238
Loss :  1.6880760192871094 2.7470905780792236 4.435166358947754
Loss :  1.6414588689804077 2.908646583557129 4.550105571746826
Loss :  1.722700595855713 2.979840040206909 4.702540397644043
Loss :  1.669076681137085 3.2532777786254883 4.922354698181152
Loss :  1.654428482055664 3.2442288398742676 4.898657321929932
Loss :  1.6568406820297241 2.7662150859832764 4.423055648803711
Loss :  1.6968119144439697 3.3774726390838623 5.074284553527832
Loss :  1.6888536214828491 2.6475508213043213 4.336404323577881
Loss :  1.6665410995483398 3.029482126235962 4.696022987365723
Loss :  1.634283423423767 2.6333324909210205 4.267615795135498
Loss :  1.6597094535827637 3.041630506515503 4.7013397216796875
Loss :  1.6522088050842285 3.1068687438964844 4.759077548980713
  batch 40 loss: 1.6522088050842285, 3.1068687438964844, 4.759077548980713
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6603091955184937 3.401057004928589 5.061366081237793
Loss :  1.6485400199890137 2.9383747577667236 4.586915016174316
Loss :  1.6654884815216064 2.852487087249756 4.517975807189941
Loss :  1.6638437509536743 3.053270101547241 4.717113971710205
Loss :  1.6667453050613403 2.876960039138794 4.543705463409424
Loss :  1.6581599712371826 3.061691999435425 4.719851970672607
Loss :  1.645174264907837 3.1594340801239014 4.804608345031738
Loss :  1.6573623418807983 3.118555784225464 4.775918006896973
Loss :  1.6301263570785522 3.524888515472412 5.155014991760254
Loss :  1.6812551021575928 2.776498556137085 4.457753658294678
Loss :  1.6458508968353271 2.7080750465393066 4.353925704956055
Loss :  1.6641746759414673 2.938551425933838 4.602725982666016
Loss :  1.6827211380004883 3.1830379962921143 4.865758895874023
Loss :  1.667489767074585 3.048008441925049 4.715497970581055
Loss :  1.6706637144088745 3.4102911949157715 5.0809550285339355
Loss :  1.6350057125091553 2.66550612449646 4.300511837005615
Loss :  1.6846879720687866 3.2395706176757812 4.924258708953857
Loss :  1.6821835041046143 3.313502788543701 4.9956865310668945
Loss :  1.6956535577774048 3.1663143634796143 4.861968040466309
Loss :  1.67092764377594 2.8637518882751465 4.534679412841797
  batch 60 loss: 1.67092764377594, 2.8637518882751465, 4.534679412841797
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726248264312744 3.5464837551116943 5.219108581542969
Loss :  1.6693944931030273 2.7142512798309326 4.383646011352539
Loss :  1.677445650100708 3.3027074337005615 4.9801530838012695
Loss :  1.6580474376678467 2.7045655250549316 4.362612724304199
Loss :  1.6550508737564087 3.2707347869873047 4.925785541534424
Loss :  5.504413604736328 4.4136552810668945 9.918068885803223
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.393803596496582 4.375987529754639 9.769790649414062
Loss :  5.501953125 4.244487285614014 9.746440887451172
Loss :  4.581408977508545 4.163033485412598 8.744441986083984
Total LOSS train 4.6892451873192424 valid 9.54468560218811
CE LOSS train 1.6640966121967022 valid 1.1453522443771362
Contrastive LOSS train 3.0251485824584963 valid 1.0407583713531494
EPOCH 214:
Loss :  1.6504942178726196 2.7325656414031982 4.383059978485107
Loss :  1.6662739515304565 3.2151293754577637 4.88140344619751
Loss :  1.6521246433258057 2.961414098739624 4.61353874206543
Loss :  1.6552956104278564 3.3762810230255127 5.031576633453369
Loss :  1.6799535751342773 3.137769937515259 4.817723274230957
Loss :  1.6629544496536255 2.6842148303985596 4.347169399261475
Loss :  1.661043405532837 3.1658904552459717 4.826933860778809
Loss :  1.6499974727630615 2.405738592147827 4.055736064910889
Loss :  1.6546372175216675 2.4873147010803223 4.141952037811279
Loss :  1.6090636253356934 3.0974977016448975 4.706561088562012
Loss :  1.6687262058258057 3.0953028202056885 4.764029026031494
Loss :  1.7292104959487915 3.1085333824157715 4.837743759155273
Loss :  1.6763076782226562 2.999506950378418 4.675814628601074
Loss :  1.664357304573059 3.6363108158111572 5.300668239593506
Loss :  1.6426292657852173 3.0600945949554443 4.702723979949951
Loss :  1.6521285772323608 3.1466634273529053 4.798791885375977
Loss :  1.66116201877594 2.578369617462158 4.239531517028809
Loss :  1.6595641374588013 2.479238271713257 4.138802528381348
Loss :  1.667525053024292 2.7572948932647705 4.4248199462890625
Loss :  1.62406325340271 3.131080150604248 4.755143165588379
  batch 20 loss: 1.62406325340271, 3.131080150604248, 4.755143165588379
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6593328714370728 3.1752517223358154 4.834584712982178
Loss :  1.6750580072402954 2.3848931789398193 4.059951305389404
Loss :  1.6464102268218994 2.9609358310699463 4.607346057891846
Loss :  1.687454342842102 2.804027795791626 4.491482257843018
Loss :  1.6848807334899902 3.4993362426757812 5.1842169761657715
Loss :  1.6485259532928467 3.2314207553863525 4.879946708679199
Loss :  1.6973837614059448 3.051300048828125 4.748683929443359
Loss :  1.6399075984954834 2.6847312450408936 4.324638843536377
Loss :  1.6880699396133423 2.7291042804718018 4.417174339294434
Loss :  1.641459345817566 2.9988999366760254 4.640359401702881
Loss :  1.7227014303207397 3.0472404956817627 4.769941806793213
Loss :  1.6690750122070312 3.206063747406006 4.875138759613037
Loss :  1.6544374227523804 3.0374159812927246 4.6918535232543945
Loss :  1.6568461656570435 2.9239649772644043 4.580811023712158
Loss :  1.6968023777008057 3.601815700531006 5.298618316650391
Loss :  1.688852071762085 2.755382537841797 4.444234848022461
Loss :  1.666536808013916 2.934476613998413 4.60101318359375
Loss :  1.6342828273773193 2.5132408142089844 4.147523880004883
Loss :  1.6597141027450562 2.86653733253479 4.526251316070557
Loss :  1.6522043943405151 3.451878070831299 5.1040825843811035
  batch 40 loss: 1.6522043943405151, 3.451878070831299, 5.1040825843811035
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6603059768676758 3.3336987495422363 4.994004726409912
Loss :  1.6485391855239868 2.921048402786255 4.569587707519531
Loss :  1.6654905080795288 2.9643707275390625 4.629861354827881
Loss :  1.6638531684875488 2.9610300064086914 4.62488317489624
Loss :  1.6667460203170776 2.7579965591430664 4.424742698669434
Loss :  1.6581501960754395 2.938220262527466 4.596370697021484
Loss :  1.645179271697998 3.209911346435547 4.855090618133545
Loss :  1.6573671102523804 2.9931116104125977 4.650478839874268
Loss :  1.630110740661621 3.541761875152588 5.171872615814209
Loss :  1.6812608242034912 3.291473388671875 4.972734451293945
Loss :  1.6458640098571777 2.685314178466797 4.331178188323975
Loss :  1.6641768217086792 3.1675901412963867 4.8317670822143555
Loss :  1.6827192306518555 3.2011749744415283 4.883893966674805
Loss :  1.6675058603286743 2.8468756675720215 4.514381408691406
Loss :  1.6706743240356445 3.469592809677124 5.140267372131348
Loss :  1.635013222694397 2.749687671661377 4.384700775146484
Loss :  1.6846867799758911 3.3410840034484863 5.025770664215088
Loss :  1.6821914911270142 3.2697501182556152 4.95194149017334
Loss :  1.6956534385681152 2.8765769004821777 4.572230339050293
Loss :  1.6709246635437012 2.900277614593506 4.571202278137207
  batch 60 loss: 1.6709246635437012, 2.900277614593506, 4.571202278137207
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726198196411133 3.6120262145996094 5.284646034240723
Loss :  1.6693984270095825 2.6770055294036865 4.346404075622559
Loss :  1.6774482727050781 3.0601913928985596 4.737639427185059
Loss :  1.658048391342163 2.8938379287719727 4.551886558532715
Loss :  1.6550445556640625 3.212730884552002 4.8677754402160645
Loss :  5.504213809967041 4.394676208496094 9.898889541625977
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.393672466278076 4.458812236785889 9.852484703063965
Loss :  5.50195837020874 4.3458075523376465 9.847765922546387
Loss :  4.581892967224121 4.3069257736206055 8.888818740844727
Total LOSS train 4.6793367532583385 valid 9.621989727020264
CE LOSS train 1.6640987671338594 valid 1.1454732418060303
Contrastive LOSS train 3.015237962282621 valid 1.0767314434051514
EPOCH 215:
Loss :  1.650496482849121 2.55602765083313 4.206523895263672
Loss :  1.6662707328796387 3.2331812381744385 4.899452209472656
Loss :  1.6521357297897339 2.994450330734253 4.646585941314697
Loss :  1.6553089618682861 2.916670799255371 4.571979522705078
Loss :  1.6799522638320923 3.4512412548065186 5.1311936378479
Loss :  1.6629605293273926 2.6507861614227295 4.313746452331543
Loss :  1.6610324382781982 3.30682635307312 4.967858791351318
Loss :  1.6499968767166138 2.9711761474609375 4.621172904968262
Loss :  1.654636263847351 2.6269164085388184 4.281552791595459
Loss :  1.6090575456619263 3.049635887145996 4.658693313598633
Loss :  1.6687201261520386 3.3655571937561035 5.034277439117432
Loss :  1.7292098999023438 3.0729002952575684 4.802110195159912
Loss :  1.6763137578964233 3.009650945663452 4.685964584350586
Loss :  1.6643561124801636 3.5072381496429443 5.171594142913818
Loss :  1.6426244974136353 3.1743698120117188 4.8169941902160645
Loss :  1.6521309614181519 3.0710396766662598 4.723170757293701
Loss :  1.661165475845337 2.752659559249878 4.413825035095215
Loss :  1.6595618724822998 2.9884092807769775 4.647971153259277
Loss :  1.6675357818603516 3.0891036987304688 4.75663948059082
Loss :  1.6240638494491577 3.0752036571502686 4.699267387390137
  batch 20 loss: 1.6240638494491577, 3.0752036571502686, 4.699267387390137
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6593248844146729 3.323086738586426 4.9824113845825195
Loss :  1.6750527620315552 2.6715757846832275 4.346628665924072
Loss :  1.6463931798934937 2.8743252754211426 4.520718574523926
Loss :  1.6874475479125977 2.98578143119812 4.673229217529297
Loss :  1.6848660707473755 3.6676080226898193 5.352474212646484
Loss :  1.6485108137130737 3.2215561866760254 4.870067119598389
Loss :  1.697391152381897 2.912140369415283 4.609531402587891
Loss :  1.6399043798446655 2.8812220096588135 4.5211262702941895
Loss :  1.6880770921707153 2.7606496810913086 4.448726654052734
Loss :  1.6414573192596436 2.77950382232666 4.420961380004883
Loss :  1.7227017879486084 3.0269510746002197 4.749652862548828
Loss :  1.669087290763855 3.070359468460083 4.739446640014648
Loss :  1.6544376611709595 2.998547315597534 4.652985095977783
Loss :  1.6568446159362793 2.8675124645233154 4.524356842041016
Loss :  1.6968060731887817 3.4267449378967285 5.123550891876221
Loss :  1.6888562440872192 2.669013023376465 4.3578691482543945
Loss :  1.6665409803390503 2.8594939708709717 4.526034832000732
Loss :  1.6343013048171997 2.5338361263275146 4.168137550354004
Loss :  1.6597117185592651 2.853848695755005 4.5135602951049805
Loss :  1.6522048711776733 3.0547595024108887 4.706964492797852
  batch 40 loss: 1.6522048711776733, 3.0547595024108887, 4.706964492797852
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.660316824913025 2.9444782733917236 4.604794979095459
Loss :  1.6485424041748047 2.605938196182251 4.254480361938477
Loss :  1.6654914617538452 3.3184099197387695 4.983901500701904
Loss :  1.6638437509536743 3.387838840484619 5.051682472229004
Loss :  1.6667535305023193 2.5944266319274902 4.2611799240112305
Loss :  1.6581592559814453 3.1394643783569336 4.797623634338379
Loss :  1.6451807022094727 3.262000322341919 4.9071807861328125
Loss :  1.6573725938796997 3.0563089847564697 4.713681697845459
Loss :  1.6301156282424927 3.069474697113037 4.69959020614624
Loss :  1.6812771558761597 3.045464515686035 4.726741790771484
Loss :  1.6458543539047241 2.6366891860961914 4.282543659210205
Loss :  1.6641758680343628 2.9071736335754395 4.571349620819092
Loss :  1.6827088594436646 3.3008296489715576 4.983538627624512
Loss :  1.6674948930740356 3.0088040828704834 4.676299095153809
Loss :  1.670669436454773 3.3474624156951904 5.018131732940674
Loss :  1.635001540184021 2.7412917613983154 4.376293182373047
Loss :  1.6846779584884644 3.254166603088379 4.938844680786133
Loss :  1.6821969747543335 3.551196575164795 5.233393669128418
Loss :  1.695639967918396 2.954824924468994 4.65046501159668
Loss :  1.670920729637146 3.0595290660858154 4.730449676513672
  batch 60 loss: 1.670920729637146, 3.0595290660858154, 4.730449676513672
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726266145706177 3.5855624675750732 5.2581892013549805
Loss :  1.669396996498108 2.760093927383423 4.42949104309082
Loss :  1.677453637123108 3.3510987758636475 5.028552532196045
Loss :  1.6580411195755005 3.0458524227142334 4.703893661499023
Loss :  1.655060052871704 3.0133628845214844 4.668422698974609
Loss :  5.5026726722717285 4.362680912017822 9.86535358428955
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.39190149307251 4.384158611297607 9.776060104370117
Loss :  5.500474452972412 4.243093013763428 9.74356746673584
Loss :  4.578769683837891 4.118208885192871 8.696978569030762
Total LOSS train 4.69861112007728 valid 9.520489931106567
CE LOSS train 1.6640987726358267 valid 1.1446924209594727
Contrastive LOSS train 3.034512362113366 valid 1.0295522212982178
EPOCH 216:
Loss :  1.650486946105957 2.7704527378082275 4.4209394454956055
Loss :  1.6662706136703491 3.1879515647888184 4.854222297668457
Loss :  1.652130126953125 2.7391035556793213 4.391233444213867
Loss :  1.6553058624267578 2.841381072998047 4.496686935424805
Loss :  1.6799589395523071 3.1494789123535156 4.829437732696533
Loss :  1.6629606485366821 2.696216583251953 4.359177112579346
Loss :  1.6610419750213623 3.4907686710357666 5.151810646057129
Loss :  1.6499961614608765 2.8351190090179443 4.485115051269531
Loss :  1.6546270847320557 2.7245991230010986 4.379226207733154
Loss :  1.6090655326843262 2.943397283554077 4.552462577819824
Loss :  1.6687182188034058 3.209712028503418 4.878430366516113
Loss :  1.7292035818099976 2.9078261852264404 4.637029647827148
Loss :  1.6762927770614624 3.22145676612854 4.897749423980713
Loss :  1.6643720865249634 3.457141876220703 5.121513843536377
Loss :  1.6426153182983398 3.052255868911743 4.694870948791504
Loss :  1.652135968208313 3.0589468479156494 4.711082935333252
Loss :  1.661165714263916 2.8518455028533936 4.5130109786987305
Loss :  1.6595624685287476 2.680744171142578 4.340306758880615
Loss :  1.6675232648849487 2.7044646739959717 4.371987819671631
Loss :  1.6240620613098145 3.260908603668213 4.884970664978027
  batch 20 loss: 1.6240620613098145, 3.260908603668213, 4.884970664978027
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.659331202507019 3.0488293170928955 4.708160400390625
Loss :  1.675052285194397 2.8172948360443115 4.492347240447998
Loss :  1.6463944911956787 3.3548452854156494 5.001239776611328
Loss :  1.687450885772705 2.8900692462921143 4.577520370483398
Loss :  1.6848714351654053 3.566803455352783 5.251674652099609
Loss :  1.648513674736023 3.1814608573913574 4.82997465133667
Loss :  1.6973953247070312 2.864816427230835 4.562211990356445
Loss :  1.6399074792861938 2.7735037803649902 4.4134111404418945
Loss :  1.6880775690078735 2.617713689804077 4.30579137802124
Loss :  1.6414607763290405 2.9094531536102295 4.5509138107299805
Loss :  1.7227058410644531 3.0159213542938232 4.7386274337768555
Loss :  1.6690818071365356 3.033240795135498 4.702322483062744
Loss :  1.6544415950775146 2.9268486499786377 4.581290245056152
Loss :  1.6568490266799927 2.783078193664551 4.439927101135254
Loss :  1.6968168020248413 3.132266044616699 4.82908296585083
Loss :  1.6888471841812134 2.741969108581543 4.430816173553467
Loss :  1.6665396690368652 2.9393718242645264 4.6059112548828125
Loss :  1.634297490119934 2.6852145195007324 4.319511890411377
Loss :  1.6597070693969727 2.8351807594299316 4.494887828826904
Loss :  1.652202844619751 3.157365322113037 4.809568405151367
  batch 40 loss: 1.652202844619751, 3.157365322113037, 4.809568405151367
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6603103876113892 3.380746364593506 5.0410566329956055
Loss :  1.6485483646392822 2.475529432296753 4.124077796936035
Loss :  1.665493369102478 3.018547534942627 4.6840410232543945
Loss :  1.6638470888137817 3.00498366355896 4.668830871582031
Loss :  1.6667524576187134 2.8596177101135254 4.526370048522949
Loss :  1.658158540725708 2.9417335987091064 4.5998921394348145
Loss :  1.6451740264892578 3.2483086585998535 4.893482685089111
Loss :  1.6573820114135742 2.939937114715576 4.59731912612915
Loss :  1.6301122903823853 3.573561191558838 5.203673362731934
Loss :  1.6812777519226074 3.019141435623169 4.7004194259643555
Loss :  1.6458631753921509 2.500674247741699 4.1465373039245605
Loss :  1.6641820669174194 3.0335700511932373 4.697751998901367
Loss :  1.68271005153656 3.426597833633423 5.109307765960693
Loss :  1.6674937009811401 2.9246926307678223 4.592186450958252
Loss :  1.6706750392913818 3.348090648651123 5.018765449523926
Loss :  1.635000467300415 2.538620710372925 4.17362117767334
Loss :  1.684677004814148 2.997126817703247 4.6818037033081055
Loss :  1.682205080986023 3.360286235809326 5.042491436004639
Loss :  1.6956382989883423 3.163365364074707 4.85900354385376
Loss :  1.6709184646606445 3.029261589050293 4.7001800537109375
  batch 60 loss: 1.6709184646606445, 3.029261589050293, 4.7001800537109375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726354360580444 3.5411272048950195 5.2137627601623535
Loss :  1.669403076171875 2.8805091381073 4.549912452697754
Loss :  1.677457332611084 3.08594012260437 4.763397216796875
Loss :  1.6580435037612915 2.8645665645599365 4.522610187530518
Loss :  1.6550670862197876 3.13858699798584 4.793653964996338
Loss :  5.501811504364014 4.411205768585205 9.913017272949219
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.391629695892334 4.352316856384277 9.743946075439453
Loss :  5.499887943267822 4.2375993728637695 9.73748779296875
Loss :  4.578330039978027 4.184524059295654 8.762853622436523
Total LOSS train 4.669547763237587 valid 9.539326190948486
CE LOSS train 1.6640994750536404 valid 1.1445825099945068
Contrastive LOSS train 3.005448315693782 valid 1.0461310148239136
EPOCH 217:
Loss :  1.6504859924316406 2.7446491718292236 4.395134925842285
Loss :  1.666268229484558 3.405353307723999 5.071621417999268
Loss :  1.6521382331848145 2.788588047027588 4.440726280212402
Loss :  1.6553010940551758 2.969515562057495 4.62481689453125
Loss :  1.6799471378326416 3.5618667602539062 5.241813659667969
Loss :  1.6629579067230225 2.919649362564087 4.582607269287109
Loss :  1.661036491394043 3.3941454887390137 5.055181980133057
Loss :  1.649993658065796 2.6953065395355225 4.345300197601318
Loss :  1.6546294689178467 2.158421277999878 3.8130507469177246
Loss :  1.6090635061264038 3.0142312049865723 4.623294830322266
Loss :  1.6687166690826416 3.3538899421691895 5.02260684967041
Loss :  1.729199767112732 2.967623710632324 4.696823596954346
Loss :  1.6763136386871338 3.3173458576202393 4.993659496307373
Loss :  1.6643704175949097 3.2775180339813232 4.941888332366943
Loss :  1.642607569694519 3.02366304397583 4.666270732879639
Loss :  1.6521375179290771 2.9753003120422363 4.627437591552734
Loss :  1.6611649990081787 2.942333221435547 4.603498458862305
Loss :  1.6595604419708252 2.792984962463379 4.452545166015625
Loss :  1.6675418615341187 2.78064227104187 4.448184013366699
Loss :  1.6240540742874146 3.1586480140686035 4.7827019691467285
  batch 20 loss: 1.6240540742874146, 3.1586480140686035, 4.7827019691467285
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6593221426010132 3.046832323074341 4.7061543464660645
Loss :  1.6750494241714478 2.6332509517669678 4.308300495147705
Loss :  1.6463985443115234 2.7979049682617188 4.444303512573242
Loss :  1.6874521970748901 3.0986199378967285 4.786072254180908
Loss :  1.6848785877227783 3.537858724594116 5.2227373123168945
Loss :  1.6485120058059692 3.2089293003082275 4.857441425323486
Loss :  1.6973938941955566 3.103374719619751 4.800768852233887
Loss :  1.6399037837982178 2.880628824234009 4.520532608032227
Loss :  1.6880781650543213 2.6018497943878174 4.289927959442139
Loss :  1.6414657831192017 2.7454144954681396 4.386880397796631
Loss :  1.722702980041504 3.0390405654907227 4.761743545532227
Loss :  1.6690776348114014 2.98888897895813 4.657966613769531
Loss :  1.6544408798217773 3.1048858165740967 4.759326934814453
Loss :  1.6568440198898315 2.943821907043457 4.600666046142578
Loss :  1.696815013885498 3.5303359031677246 5.227150917053223
Loss :  1.6888543367385864 2.5567309856414795 4.2455854415893555
Loss :  1.6665372848510742 3.092290163040161 4.758827209472656
Loss :  1.6342999935150146 2.658717632293701 4.293017387390137
Loss :  1.659704566001892 2.9751031398773193 4.634807586669922
Loss :  1.652204155921936 3.33613657951355 4.988340854644775
  batch 40 loss: 1.652204155921936, 3.33613657951355, 4.988340854644775
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6603062152862549 3.276564121246338 4.936870574951172
Loss :  1.6485509872436523 2.700171947479248 4.3487229347229
Loss :  1.665494441986084 3.0612359046936035 4.7267303466796875
Loss :  1.6638391017913818 2.9605915546417236 4.6244306564331055
Loss :  1.6667463779449463 2.753833293914795 4.42057991027832
Loss :  1.6581534147262573 3.0420291423797607 4.7001824378967285
Loss :  1.6451784372329712 3.2700321674346924 4.915210723876953
Loss :  1.6573705673217773 3.048007011413574 4.705377578735352
Loss :  1.6301159858703613 3.206472873687744 4.8365888595581055
Loss :  1.6812810897827148 2.778207540512085 4.459488868713379
Loss :  1.645853877067566 2.700421094894409 4.3462748527526855
Loss :  1.6641844511032104 3.1344408988952637 4.798625469207764
Loss :  1.6827070713043213 3.207134485244751 4.889841556549072
Loss :  1.6674944162368774 2.951955556869507 4.619450092315674
Loss :  1.6706726551055908 3.5887606143951416 5.259433269500732
Loss :  1.6350042819976807 2.672839879989624 4.307844161987305
Loss :  1.6846774816513062 3.22599458694458 4.910672187805176
Loss :  1.6822011470794678 3.274933099746704 4.957134246826172
Loss :  1.6956413984298706 3.057551860809326 4.753193378448486
Loss :  1.670925259590149 2.7421770095825195 4.413102149963379
  batch 60 loss: 1.670925259590149, 2.7421770095825195, 4.413102149963379
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726298332214355 3.5979535579681396 5.270583152770996
Loss :  1.6694049835205078 2.7462246417999268 4.4156293869018555
Loss :  1.6774516105651855 3.514058828353882 5.191510200500488
Loss :  1.6580498218536377 3.028693199157715 4.686742782592773
Loss :  1.655060887336731 2.9501261711120605 4.605186939239502
Loss :  5.501275539398193 4.404372692108154 9.905648231506348
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.390840530395508 4.344977378845215 9.735817909240723
Loss :  5.499359607696533 4.242987155914307 9.74234676361084
Loss :  4.5777907371521 4.304957866668701 8.8827486038208
Total LOSS train 4.688909581991342 valid 9.566640377044678
CE LOSS train 1.6640987671338594 valid 1.144447684288025
Contrastive LOSS train 3.024810813023494 valid 1.0762394666671753
EPOCH 218:
Loss :  1.6504865884780884 2.5573298931121826 4.2078166007995605
Loss :  1.6662673950195312 3.2442851066589355 4.910552501678467
Loss :  1.652134656906128 2.9422619342803955 4.594396591186523
Loss :  1.6553043127059937 2.7732560634613037 4.428560256958008
Loss :  1.679956078529358 3.2729735374450684 4.952929496765137
Loss :  1.6629579067230225 2.9157230854034424 4.578680992126465
Loss :  1.6610453128814697 3.269477128982544 4.930522441864014
Loss :  1.6499913930892944 2.7702929973602295 4.420284271240234
Loss :  1.6546249389648438 2.5407698154449463 4.195394515991211
Loss :  1.6090606451034546 2.9888880252838135 4.5979485511779785
Loss :  1.6687138080596924 3.116015672683716 4.784729480743408
Loss :  1.7292143106460571 2.7341325283050537 4.4633469581604
Loss :  1.676314353942871 3.0292773246765137 4.705591678619385
Loss :  1.6643747091293335 3.3628053665161133 5.027180194854736
Loss :  1.6426090002059937 3.05275297164917 4.695362091064453
Loss :  1.6521410942077637 3.35410213470459 5.0062432289123535
Loss :  1.6611673831939697 2.7930448055267334 4.454212188720703
Loss :  1.6595618724822998 3.04758358001709 4.707145690917969
Loss :  1.6675275564193726 2.821855068206787 4.489382743835449
Loss :  1.6240562200546265 3.388014078140259 5.012070178985596
  batch 20 loss: 1.6240562200546265, 3.388014078140259, 5.012070178985596
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.659323811531067 3.2976865768432617 4.957010269165039
Loss :  1.6750483512878418 2.751784563064575 4.426833152770996
Loss :  1.6463936567306519 3.3787105083465576 5.02510404586792
Loss :  1.687448263168335 2.926090717315674 4.61353874206543
Loss :  1.6848869323730469 3.450208902359009 5.135095596313477
Loss :  1.6485086679458618 3.172179698944092 4.820688247680664
Loss :  1.697390079498291 2.931828260421753 4.629218101501465
Loss :  1.639904499053955 2.873129367828369 4.513033866882324
Loss :  1.6880786418914795 2.871603488922119 4.5596818923950195
Loss :  1.6414570808410645 3.17303729057312 4.8144941329956055
Loss :  1.722704529762268 3.01358699798584 4.736291408538818
Loss :  1.669081211090088 2.9560658931732178 4.625146865844727
Loss :  1.6544498205184937 3.1395645141601562 4.7940144538879395
Loss :  1.65684175491333 2.7771804332733154 4.434021949768066
Loss :  1.696823239326477 3.3507351875305176 5.047558307647705
Loss :  1.6888551712036133 2.6906425952911377 4.379497528076172
Loss :  1.6665390729904175 3.054400682449341 4.720939636230469
Loss :  1.6342986822128296 2.77786922454834 4.412168025970459
Loss :  1.6597098112106323 2.621581792831421 4.281291484832764
Loss :  1.6522035598754883 3.3509650230407715 5.00316858291626
  batch 40 loss: 1.6522035598754883, 3.3509650230407715, 5.00316858291626
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.660306692123413 3.6566624641418457 5.31696891784668
Loss :  1.648557424545288 2.6554312705993652 4.303988456726074
Loss :  1.665495753288269 3.0228381156921387 4.688333988189697
Loss :  1.6638469696044922 2.839784860610962 4.503631591796875
Loss :  1.6667479276657104 2.5375137329101562 4.204261779785156
Loss :  1.658147931098938 2.9859812259674072 4.644129276275635
Loss :  1.6451802253723145 3.278463840484619 4.923644065856934
Loss :  1.657376766204834 3.1000442504882812 4.757421016693115
Loss :  1.6301181316375732 3.3926427364349365 5.02276086807251
Loss :  1.6812831163406372 3.253749370574951 4.935032367706299
Loss :  1.6458584070205688 2.6879987716674805 4.33385705947876
Loss :  1.6641825437545776 2.816791534423828 4.480974197387695
Loss :  1.6827055215835571 3.185725212097168 4.8684306144714355
Loss :  1.6675047874450684 2.9190597534179688 4.586564540863037
Loss :  1.6706726551055908 3.325145959854126 4.995818614959717
Loss :  1.635013222694397 2.5407707691192627 4.175784111022949
Loss :  1.6846777200698853 3.2291791439056396 4.9138569831848145
Loss :  1.6822023391723633 3.2511727809906006 4.933375358581543
Loss :  1.6956446170806885 3.43287992477417 5.1285247802734375
Loss :  1.6709316968917847 2.7082550525665283 4.379186630249023
  batch 60 loss: 1.6709316968917847, 2.7082550525665283, 4.379186630249023
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726305484771729 3.415829658508301 5.0884599685668945
Loss :  1.6694108247756958 2.893040418624878 4.562451362609863
Loss :  1.677458643913269 2.969825267791748 4.647284030914307
Loss :  1.6580555438995361 2.8498871326446533 4.5079426765441895
Loss :  1.65506911277771 2.7571468353271484 4.4122161865234375
Loss :  5.502029895782471 4.385336399078369 9.88736629486084
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.391693592071533 4.366076469421387 9.757770538330078
Loss :  5.50044059753418 4.237586498260498 9.738027572631836
Loss :  4.5787506103515625 4.386916637420654 8.965667724609375
Total LOSS train 4.683169944469745 valid 9.587208032608032
CE LOSS train 1.6641005461032574 valid 1.1446876525878906
Contrastive LOSS train 3.0190694295443024 valid 1.0967291593551636
EPOCH 219:
Loss :  1.6504954099655151 2.6406302452087402 4.291125774383545
Loss :  1.6662789583206177 3.4084525108337402 5.074731349945068
Loss :  1.652132511138916 3.231905221939087 4.884037971496582
Loss :  1.6553040742874146 3.0040745735168457 4.659378528594971
Loss :  1.6799501180648804 3.41168475151062 5.091634750366211
Loss :  1.6629581451416016 2.6776175498962402 4.340575695037842
Loss :  1.6610428094863892 3.388493299484253 5.049536228179932
Loss :  1.6499961614608765 2.745180368423462 4.395176410675049
Loss :  1.6546351909637451 2.2293365001678467 3.883971691131592
Loss :  1.6090620756149292 3.1237711906433105 4.732833385467529
Loss :  1.668716549873352 3.128415584564209 4.7971320152282715
Loss :  1.7292174100875854 2.8602306842803955 4.589447975158691
Loss :  1.6762999296188354 3.18701434135437 4.863314151763916
Loss :  1.6643834114074707 3.123816967010498 4.788200378417969
Loss :  1.6426162719726562 3.258167266845703 4.900783538818359
Loss :  1.6521440744400024 3.0323145389556885 4.6844587326049805
Loss :  1.6611673831939697 2.6465201377868652 4.307687759399414
Loss :  1.6595596075057983 2.7452456951141357 4.4048051834106445
Loss :  1.6675260066986084 2.855029582977295 4.522555351257324
Loss :  1.6240615844726562 2.9925990104675293 4.6166605949401855
  batch 20 loss: 1.6240615844726562, 2.9925990104675293, 4.6166605949401855
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6593236923217773 3.143479585647583 4.802803039550781
Loss :  1.6750434637069702 2.7501535415649414 4.425197124481201
Loss :  1.646399974822998 3.1352179050445557 4.781618118286133
Loss :  1.6874496936798096 2.819160223007202 4.506609916687012
Loss :  1.6848814487457275 3.5872573852539062 5.272138595581055
Loss :  1.648511528968811 3.5015709400177 5.150082588195801
Loss :  1.6973861455917358 3.213625431060791 4.911011695861816
Loss :  1.639905571937561 2.8074281215667725 4.447333812713623
Loss :  1.6880760192871094 2.9931135177612305 4.68118953704834
Loss :  1.6414623260498047 2.8268651962280273 4.468327522277832
Loss :  1.7226976156234741 3.331076145172119 5.053773880004883
Loss :  1.669071912765503 3.106872320175171 4.775944232940674
Loss :  1.65444815158844 3.230139970779419 4.884588241577148
Loss :  1.6568454504013062 2.843930244445801 4.5007758140563965
Loss :  1.696810245513916 3.3618319034576416 5.058642387390137
Loss :  1.688858985900879 2.7710225582122803 4.459881782531738
Loss :  1.6665366888046265 2.831678628921509 4.498215198516846
Loss :  1.6342867612838745 2.403658151626587 4.037944793701172
Loss :  1.659701943397522 2.7910714149475098 4.450773239135742
Loss :  1.6522024869918823 3.0623435974121094 4.714546203613281
  batch 40 loss: 1.6522024869918823, 3.0623435974121094, 4.714546203613281
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602976322174072 3.3861758708953857 5.046473503112793
Loss :  1.6485481262207031 2.80107045173645 4.449618339538574
Loss :  1.6654959917068481 3.189305067062378 4.854801177978516
Loss :  1.6638425588607788 2.866018056869507 4.529860496520996
Loss :  1.6667348146438599 2.6519384384155273 4.318673133850098
Loss :  1.6581445932388306 2.865389585494995 4.523534297943115
Loss :  1.645175576210022 3.1758246421813965 4.821000099182129
Loss :  1.657365083694458 2.9570529460906982 4.614418029785156
Loss :  1.630115270614624 3.579559564590454 5.209674835205078
Loss :  1.6812763214111328 3.053914785385132 4.735191345214844
Loss :  1.6458606719970703 2.572039842605591 4.217900276184082
Loss :  1.664171576499939 2.9041872024536133 4.568358898162842
Loss :  1.6827059984207153 3.225099802017212 4.907805919647217
Loss :  1.6674753427505493 2.6333603858947754 4.300835609436035
Loss :  1.6706657409667969 3.331122875213623 5.00178861618042
Loss :  1.6349989175796509 2.7198684215545654 4.354867458343506
Loss :  1.68467378616333 3.150505304336548 4.835179328918457
Loss :  1.6821948289871216 3.163039207458496 4.845233917236328
Loss :  1.6956382989883423 3.2272982597351074 4.92293643951416
Loss :  1.670921802520752 2.859907627105713 4.530829429626465
  batch 60 loss: 1.670921802520752, 2.859907627105713, 4.530829429626465
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726303100585938 3.6690428256988525 5.341672897338867
Loss :  1.66940176486969 2.766357898712158 4.435759544372559
Loss :  1.6774390935897827 3.11299729347229 4.790436267852783
Loss :  1.6580556631088257 2.8545663356781006 4.512621879577637
Loss :  1.655051350593567 3.2619173526763916 4.916968822479248
Loss :  5.499057292938232 4.392180442810059 9.891237258911133
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.388991832733154 4.470951557159424 9.859943389892578
Loss :  5.497366905212402 4.282303333282471 9.779670715332031
Loss :  4.5773515701293945 4.2443671226501465 8.821718215942383
Total LOSS train 4.682244396209716 valid 9.588142395019531
CE LOSS train 1.6640973678001991 valid 1.1443378925323486
Contrastive LOSS train 3.0181470284095178 valid 1.0610917806625366
EPOCH 220:
Loss :  1.6504744291305542 2.792402744293213 4.442877292633057
Loss :  1.6662720441818237 3.3613972663879395 5.027669429779053
Loss :  1.6521297693252563 2.954768419265747 4.606898307800293
Loss :  1.6552873849868774 2.9377248287200928 4.59301233291626
Loss :  1.6799399852752686 3.4393012523651123 5.119241237640381
Loss :  1.6629469394683838 2.7201855182647705 4.383132457733154
Loss :  1.6610411405563354 3.2405974864959717 4.901638507843018
Loss :  1.649996280670166 2.399996757507324 4.04999303817749
Loss :  1.6546272039413452 2.5593583583831787 4.213985443115234
Loss :  1.6090641021728516 2.9553897380828857 4.564454078674316
Loss :  1.6687135696411133 3.2435150146484375 4.912228584289551
Loss :  1.7292124032974243 2.937263250350952 4.666475772857666
Loss :  1.6763352155685425 3.2423620223999023 4.918697357177734
Loss :  1.6643770933151245 3.3950369358062744 5.059413909912109
Loss :  1.6426087617874146 2.7596371173858643 4.402245998382568
Loss :  1.652147889137268 2.966993570327759 4.619141578674316
Loss :  1.661170482635498 2.757949113845825 4.419119834899902
Loss :  1.659554362297058 2.920180559158325 4.579734802246094
Loss :  1.6675444841384888 2.7775213718414307 4.445065975189209
Loss :  1.6240545511245728 3.444927453994751 5.068982124328613
  batch 20 loss: 1.6240545511245728, 3.444927453994751, 5.068982124328613
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6593120098114014 3.0835776329040527 4.742889404296875
Loss :  1.6750471591949463 2.5163636207580566 4.191411018371582
Loss :  1.6464003324508667 3.2512528896331787 4.897653102874756
Loss :  1.6874487400054932 2.8479504585266113 4.535399436950684
Loss :  1.6848832368850708 3.439601421356201 5.124484539031982
Loss :  1.6485017538070679 3.417335271835327 5.0658369064331055
Loss :  1.6973932981491089 3.1080501079559326 4.805443286895752
Loss :  1.639905571937561 2.7243778705596924 4.364283561706543
Loss :  1.6880786418914795 2.9365882873535156 4.624667167663574
Loss :  1.6414668560028076 2.7129569053649902 4.354423522949219
Loss :  1.7226999998092651 3.2309176921844482 4.953617572784424
Loss :  1.6690864562988281 3.170619010925293 4.839705467224121
Loss :  1.6544456481933594 3.044092893600464 4.698538780212402
Loss :  1.6568447351455688 2.950862407684326 4.6077070236206055
Loss :  1.6968190670013428 3.5667524337768555 5.263571739196777
Loss :  1.6888632774353027 2.5768558979034424 4.265719413757324
Loss :  1.6665364503860474 2.989462375640869 4.655998706817627
Loss :  1.6342960596084595 2.877751588821411 4.51204776763916
Loss :  1.65970778465271 2.9415130615234375 4.601221084594727
Loss :  1.6522012948989868 3.3850388526916504 5.037240028381348
  batch 40 loss: 1.6522012948989868, 3.3850388526916504, 5.037240028381348
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6603071689605713 3.147951602935791 4.808259010314941
Loss :  1.648555874824524 2.372138023376465 4.020693778991699
Loss :  1.6654975414276123 3.0218708515167236 4.687368392944336
Loss :  1.6638425588607788 2.9776928424835205 4.64153528213501
Loss :  1.6667444705963135 2.6383931636810303 4.305137634277344
Loss :  1.6581512689590454 3.05568265914917 4.713833808898926
Loss :  1.6451855897903442 3.2577157020568848 4.9029011726379395
Loss :  1.6573759317398071 3.220137357711792 4.877513408660889
Loss :  1.6301143169403076 3.0467448234558105 4.676858901977539
Loss :  1.6812857389450073 3.0396249294281006 4.720910549163818
Loss :  1.645856499671936 2.5228664875030518 4.168723106384277
Loss :  1.664175033569336 3.169734477996826 4.833909511566162
Loss :  1.6826980113983154 3.2879176139831543 4.970615386962891
Loss :  1.6674916744232178 2.9733998775482178 4.6408915519714355
Loss :  1.6706666946411133 3.4321649074554443 5.102831840515137
Loss :  1.6349987983703613 2.663539171218872 4.2985382080078125
Loss :  1.6846719980239868 3.3683934211730957 5.053065299987793
Loss :  1.6822093725204468 3.124056816101074 4.8062663078308105
Loss :  1.695634365081787 3.0893983840942383 4.785032749176025
Loss :  1.6709201335906982 2.824146270751953 4.4950666427612305
  batch 60 loss: 1.6709201335906982, 2.824146270751953, 4.4950666427612305
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726280450820923 3.489868402481079 5.162496566772461
Loss :  1.6694037914276123 2.802215099334717 4.47161865234375
Loss :  1.6774506568908691 3.326293468475342 5.003744125366211
Loss :  1.6580567359924316 2.7795534133911133 4.437610149383545
Loss :  1.655057430267334 3.052945137023926 4.70800256729126
Loss :  5.4992570877075195 4.375333309173584 9.874589920043945
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.388635635375977 4.330906391143799 9.719541549682617
Loss :  5.496982097625732 4.284486770629883 9.781469345092773
Loss :  4.578935623168945 4.387011528015137 8.965947151184082
Total LOSS train 4.683527572338398 valid 9.585386991500854
CE LOSS train 1.6640987102801983 valid 1.1447339057922363
Contrastive LOSS train 3.0194288363823523 valid 1.0967528820037842
EPOCH 221:
Loss :  1.6504732370376587 2.566101551055908 4.216574668884277
Loss :  1.6662673950195312 3.2045912742614746 4.870858669281006
Loss :  1.6521382331848145 2.881882667541504 4.534020900726318
Loss :  1.6553095579147339 3.1811363697052 4.8364458084106445
Loss :  1.6799567937850952 3.3830435276031494 5.063000202178955
Loss :  1.6629558801651 2.7323176860809326 4.395273685455322
Loss :  1.661043643951416 3.2679762840270996 4.929019927978516
Loss :  1.649990200996399 2.703157901763916 4.353147983551025
Loss :  1.6546226739883423 2.676203966140747 4.330826759338379
Loss :  1.6090641021728516 2.835841178894043 4.4449052810668945
Loss :  1.6687097549438477 3.0530178546905518 4.72172737121582
Loss :  1.729217767715454 3.0109915733337402 4.740209579467773
Loss :  1.6763274669647217 3.0818841457366943 4.758211612701416
Loss :  1.6643785238265991 3.560579776763916 5.224958419799805
Loss :  1.6426070928573608 2.997946262359619 4.6405534744262695
Loss :  1.6521457433700562 3.445246696472168 5.097392559051514
Loss :  1.6611696481704712 2.9510838985443115 4.612253665924072
Loss :  1.659549593925476 2.666179656982422 4.3257293701171875
Loss :  1.6675320863723755 2.729854106903076 4.397386074066162
Loss :  1.6240556240081787 3.021705150604248 4.645760536193848
  batch 20 loss: 1.6240556240081787, 3.021705150604248, 4.645760536193848
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6593211889266968 2.9347777366638184 4.594099044799805
Loss :  1.6750495433807373 2.587266206741333 4.26231575012207
Loss :  1.6463979482650757 3.507211923599243 5.153609752655029
Loss :  1.6874475479125977 3.2510986328125 4.938546180725098
Loss :  1.6848807334899902 3.4724209308624268 5.157301902770996
Loss :  1.648498773574829 3.1464321613311768 4.794930934906006
Loss :  1.6973975896835327 2.9486162662506104 4.6460137367248535
Loss :  1.6399002075195312 3.0226047039031982 4.662505149841309
Loss :  1.68807852268219 2.625601291656494 4.3136796951293945
Loss :  1.6414715051651 2.844255208969116 4.485726833343506
Loss :  1.722700834274292 3.0930936336517334 4.815794467926025
Loss :  1.669082760810852 3.033928632736206 4.703011512756348
Loss :  1.65443754196167 3.179539203643799 4.833976745605469
Loss :  1.656841516494751 2.99613881111145 4.652980327606201
Loss :  1.6968148946762085 3.470442295074463 5.167257308959961
Loss :  1.6888540983200073 2.6668014526367188 4.355655670166016
Loss :  1.6665371656417847 3.158708095550537 4.825245380401611
Loss :  1.6342999935150146 3.046365261077881 4.680665016174316
Loss :  1.659698486328125 2.8897483348846436 4.549447059631348
Loss :  1.6521949768066406 3.4938228130340576 5.146018028259277
  batch 40 loss: 1.6521949768066406, 3.4938228130340576, 5.146018028259277
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6603010892868042 3.597745180130005 5.2580461502075195
Loss :  1.6485568284988403 2.572685956954956 4.221242904663086
Loss :  1.6654974222183228 2.993487596511841 4.658985137939453
Loss :  1.6638410091400146 2.790419578552246 4.45426082611084
Loss :  1.666739821434021 2.6141130924224854 4.280852794647217
Loss :  1.6581525802612305 3.149505853652954 4.8076581954956055
Loss :  1.6451752185821533 3.1602723598480225 4.805447578430176
Loss :  1.657371163368225 2.963494062423706 4.620865345001221
Loss :  1.630122423171997 3.4214205741882324 5.051543235778809
Loss :  1.6812812089920044 3.2027390003204346 4.8840203285217285
Loss :  1.6458501815795898 2.540515422821045 4.186365604400635
Loss :  1.6641765832901 3.0051324367523193 4.669309139251709
Loss :  1.6826921701431274 3.0928702354431152 4.775562286376953
Loss :  1.6674823760986328 3.113551378250122 4.781033515930176
Loss :  1.6706650257110596 3.3816113471984863 5.052276611328125
Loss :  1.6350020170211792 2.758833408355713 4.393835544586182
Loss :  1.6846685409545898 3.21183443069458 4.89650297164917
Loss :  1.6821997165679932 3.242691993713379 4.924891471862793
Loss :  1.6956379413604736 3.002058744430542 4.697696685791016
Loss :  1.670927882194519 2.751858949661255 4.422786712646484
  batch 60 loss: 1.670927882194519, 2.751858949661255, 4.422786712646484
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726303100585938 3.783356189727783 5.455986499786377
Loss :  1.6694051027297974 2.684725046157837 4.354130268096924
Loss :  1.677449345588684 3.119952917098999 4.797402381896973
Loss :  1.6580569744110107 2.7894110679626465 4.447467803955078
Loss :  1.6550567150115967 3.249627113342285 4.904684066772461
Loss :  5.499046325683594 4.460345268249512 9.959391593933105
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.388286113739014 4.35628604888916 9.744571685791016
Loss :  5.4965739250183105 4.246195316314697 9.742769241333008
Loss :  4.577591896057129 4.388604640960693 8.966196060180664
Total LOSS train 4.702767555530254 valid 9.603232145309448
CE LOSS train 1.6640978538073026 valid 1.1443979740142822
Contrastive LOSS train 3.038669677881094 valid 1.0971511602401733
EPOCH 222:
Loss :  1.650465726852417 2.5365495681762695 4.187015533447266
Loss :  1.6662769317626953 3.2120676040649414 4.878344535827637
Loss :  1.6521323919296265 3.0009589195251465 4.6530914306640625
Loss :  1.6553106307983398 2.7878994941711426 4.443210124969482
Loss :  1.679958462715149 3.2807397842407227 4.960698127746582
Loss :  1.6629585027694702 2.6991167068481445 4.362075328826904
Loss :  1.6610416173934937 3.363957166671753 5.024998664855957
Loss :  1.649990439414978 2.684699535369873 4.334690093994141
Loss :  1.6546192169189453 2.9236550331115723 4.578274250030518
Loss :  1.609058141708374 3.1776785850524902 4.786736488342285
Loss :  1.6687084436416626 3.2253761291503906 4.894084453582764
Loss :  1.7292203903198242 2.8780267238616943 4.607247352600098
Loss :  1.6762961149215698 3.2132046222686768 4.889500617980957
Loss :  1.6643813848495483 3.3238539695739746 4.9882354736328125
Loss :  1.6426092386245728 3.3086116313934326 4.951220989227295
Loss :  1.6521522998809814 3.1369197368621826 4.789072036743164
Loss :  1.6611745357513428 3.038440227508545 4.699614524841309
Loss :  1.6595463752746582 2.7496159076690674 4.409162521362305
Loss :  1.6675336360931396 2.884840250015259 4.552373886108398
Loss :  1.624055027961731 3.1407041549682617 4.764759063720703
  batch 20 loss: 1.624055027961731, 3.1407041549682617, 4.764759063720703
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6593215465545654 3.2865750789642334 4.945896625518799
Loss :  1.675046682357788 2.5984294414520264 4.2734761238098145
Loss :  1.6463998556137085 3.2581212520599365 4.9045209884643555
Loss :  1.6874504089355469 2.9621686935424805 4.649619102478027
Loss :  1.684885025024414 3.489598035812378 5.174483299255371
Loss :  1.6484975814819336 3.2939422130584717 4.942440032958984
Loss :  1.6973952054977417 2.913789987564087 4.611185073852539
Loss :  1.6399033069610596 2.6626908779144287 4.302594184875488
Loss :  1.6880781650543213 2.649578094482422 4.337656021118164
Loss :  1.6414709091186523 3.1001715660095215 4.741642475128174
Loss :  1.7226985692977905 3.180640935897827 4.903339385986328
Loss :  1.6690900325775146 3.3859283924102783 5.055018424987793
Loss :  1.6544400453567505 3.006229877471924 4.660669803619385
Loss :  1.6568424701690674 2.754014253616333 4.4108567237854
Loss :  1.696823239326477 3.465489625930786 5.162312984466553
Loss :  1.6888614892959595 2.7948832511901855 4.4837446212768555
Loss :  1.6665364503860474 3.144671678543091 4.811208248138428
Loss :  1.634307622909546 2.7925899028778076 4.4268975257873535
Loss :  1.6597007513046265 3.067110538482666 4.726811408996582
Loss :  1.6521961688995361 2.9704513549804688 4.622647285461426
  batch 40 loss: 1.6521961688995361, 2.9704513549804688, 4.622647285461426
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6603044271469116 3.258561372756958 4.91886568069458
Loss :  1.6485567092895508 2.9181556701660156 4.566712379455566
Loss :  1.665505051612854 3.2147130966186523 4.880218029022217
Loss :  1.6638301610946655 3.0411057472229004 4.7049360275268555
Loss :  1.666748046875 2.8523218631744385 4.519069671630859
Loss :  1.6581530570983887 2.9899723529815674 4.648125648498535
Loss :  1.645173192024231 3.4986677169799805 5.143840789794922
Loss :  1.6573723554611206 3.0263876914978027 4.683760166168213
Loss :  1.6301240921020508 3.477013111114502 5.107137203216553
Loss :  1.681289553642273 2.9660065174102783 4.647295951843262
Loss :  1.6458512544631958 2.6068925857543945 4.252743721008301
Loss :  1.6641788482666016 3.4371261596679688 5.10130500793457
Loss :  1.6826903820037842 3.256575107574463 4.939265251159668
Loss :  1.667481541633606 2.9541471004486084 4.621628761291504
Loss :  1.6706647872924805 3.5419833660125732 5.212648391723633
Loss :  1.6349966526031494 2.816287040710449 4.4512834548950195
Loss :  1.684666633605957 3.2419493198394775 4.9266157150268555
Loss :  1.682208776473999 3.3631932735443115 5.0454020500183105
Loss :  1.6956321001052856 2.938779592514038 4.634411811828613
Loss :  1.6709233522415161 3.0031216144561768 4.674045085906982
  batch 60 loss: 1.6709233522415161, 3.0031216144561768, 4.674045085906982
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726306676864624 3.235454797744751 4.908085346221924
Loss :  1.6694058179855347 2.81606388092041 4.485469818115234
Loss :  1.6774448156356812 3.06162166595459 4.7390666007995605
Loss :  1.658052921295166 2.744041919708252 4.402094841003418
Loss :  1.655068278312683 3.3451671600341797 5.000235557556152
Loss :  5.498752593994141 4.434682369232178 9.933435440063477
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.388264179229736 4.39884614944458 9.787110328674316
Loss :  5.496532917022705 4.275416851043701 9.771949768066406
Loss :  4.578238487243652 4.1298065185546875 8.70804500579834
Total LOSS train 4.724856750781719 valid 9.550135135650635
CE LOSS train 1.664098284794734 valid 1.144559621810913
Contrastive LOSS train 3.0607584696549637 valid 1.0324516296386719
EPOCH 223:
Loss :  1.650461196899414 2.6770169734954834 4.327478408813477
Loss :  1.666276216506958 3.327636480331421 4.993912696838379
Loss :  1.6521334648132324 2.959224224090576 4.611357688903809
Loss :  1.6553109884262085 2.9816031455993652 4.636914253234863
Loss :  1.6799558401107788 3.3400442600250244 5.019999980926514
Loss :  1.6629588603973389 2.6555466651916504 4.31850528717041
Loss :  1.6610286235809326 3.579789161682129 5.240818023681641
Loss :  1.6499855518341064 2.4058878421783447 4.055873394012451
Loss :  1.6546217203140259 2.3469605445861816 4.001582145690918
Loss :  1.609054684638977 2.9358410835266113 4.544895648956299
Loss :  1.668707013130188 3.2171759605407715 4.88588285446167
Loss :  1.7292183637619019 2.929994821548462 4.659213066101074
Loss :  1.6763081550598145 3.1848485469818115 4.861156463623047
Loss :  1.6643863916397095 3.2671499252319336 4.9315361976623535
Loss :  1.642608642578125 2.9530014991760254 4.59561014175415
Loss :  1.6521559953689575 3.0403220653533936 4.692478179931641
Loss :  1.661177396774292 2.7927887439727783 4.45396614074707
Loss :  1.6595453023910522 2.6014556884765625 4.261001110076904
Loss :  1.6675407886505127 2.761963367462158 4.42950439453125
Loss :  1.6240571737289429 3.0583863258361816 4.682443618774414
  batch 20 loss: 1.6240571737289429, 3.0583863258361816, 4.682443618774414
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6593108177185059 3.3829596042633057 5.042270660400391
Loss :  1.675048589706421 2.7650349140167236 4.4400835037231445
Loss :  1.64639151096344 3.248703956604004 4.895095348358154
Loss :  1.6874462366104126 2.8751752376556396 4.562621593475342
Loss :  1.684880018234253 3.371277093887329 5.056157112121582
Loss :  1.648490309715271 3.2108545303344727 4.859344959259033
Loss :  1.6973921060562134 2.7279813289642334 4.425373554229736
Loss :  1.6399006843566895 2.998765230178833 4.638666152954102
Loss :  1.688080072402954 2.8122854232788086 4.500365257263184
Loss :  1.6414661407470703 2.914848804473877 4.556314945220947
Loss :  1.7226910591125488 3.0263397693634033 4.749031066894531
Loss :  1.66909658908844 2.9046783447265625 4.573774814605713
Loss :  1.6544378995895386 2.97448468208313 4.628922462463379
Loss :  1.6568375825881958 2.7915737628936768 4.448411464691162
Loss :  1.6968202590942383 3.4759652614593506 5.172785758972168
Loss :  1.6888630390167236 2.758631944656372 4.447494983673096
Loss :  1.666534662246704 2.839728832244873 4.506263732910156
Loss :  1.6343084573745728 2.7513842582702637 4.385692596435547
Loss :  1.6596969366073608 2.7228729724884033 4.382569789886475
Loss :  1.6522018909454346 3.1723291873931885 4.824531078338623
  batch 40 loss: 1.6522018909454346, 3.1723291873931885, 4.824531078338623
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602990627288818 3.2525973320007324 4.912896156311035
Loss :  1.6485453844070435 2.7964513301849365 4.4449968338012695
Loss :  1.6655067205429077 3.1592657566070557 4.824772357940674
Loss :  1.6638380289077759 2.7930867671966553 4.456924915313721
Loss :  1.6667416095733643 2.642467737197876 4.30920934677124
Loss :  1.6581512689590454 2.986379861831665 4.64453125
Loss :  1.64518141746521 3.275892972946167 4.921074390411377
Loss :  1.6573764085769653 3.140255928039551 4.797632217407227
Loss :  1.6301215887069702 3.14229416847229 4.772415637969971
Loss :  1.6812915802001953 3.3569796085357666 5.038270950317383
Loss :  1.6458501815795898 2.9421257972717285 4.587975978851318
Loss :  1.6641795635223389 3.0575335025787354 4.721713066101074
Loss :  1.6826874017715454 3.402048349380493 5.084735870361328
Loss :  1.667475700378418 3.3458988666534424 5.013374328613281
Loss :  1.6706690788269043 3.3351805210113525 5.005849838256836
Loss :  1.6349830627441406 2.581848382949829 4.216831207275391
Loss :  1.6846613883972168 3.325305223464966 5.009966850280762
Loss :  1.682214617729187 3.2850804328918457 4.967295169830322
Loss :  1.6956219673156738 2.9537627696990967 4.649384498596191
Loss :  1.6709034442901611 2.710362195968628 4.381265640258789
  batch 60 loss: 1.6709034442901611, 2.710362195968628, 4.381265640258789
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726360321044922 3.2099266052246094 4.882562637329102
Loss :  1.6694022417068481 2.915806770324707 4.585208892822266
Loss :  1.6774566173553467 3.138084650039673 4.8155412673950195
Loss :  1.6580475568771362 2.958524465560913 4.61657190322876
Loss :  1.655060052871704 3.4175734519958496 5.072633743286133
Loss :  5.499096393585205 4.379091262817383 9.87818717956543
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.3886799812316895 4.282433032989502 9.671113014221191
Loss :  5.496667861938477 4.2106428146362305 9.707310676574707
Loss :  4.579354286193848 4.181247711181641 8.760601997375488
Total LOSS train 4.677439007392296 valid 9.504303216934204
CE LOSS train 1.6640967570818388 valid 1.144838571548462
Contrastive LOSS train 3.0133422448084906 valid 1.0453119277954102
EPOCH 224:
Loss :  1.6504586935043335 2.6979169845581055 4.3483757972717285
Loss :  1.6662743091583252 3.1397650241851807 4.806039333343506
Loss :  1.6521406173706055 2.84594464302063 4.498085021972656
Loss :  1.6553130149841309 2.945401430130005 4.600714683532715
Loss :  1.679951548576355 3.4006717205047607 5.080623149871826
Loss :  1.6629507541656494 3.03450345993042 4.697454452514648
Loss :  1.6610344648361206 3.2303574085235596 4.891391754150391
Loss :  1.6499823331832886 2.792041063308716 4.442023277282715
Loss :  1.654618263244629 2.3697445392608643 4.024362564086914
Loss :  1.609055757522583 2.7407963275909424 4.349852085113525
Loss :  1.6687078475952148 3.2881479263305664 4.956855773925781
Loss :  1.729217290878296 3.0757062435150146 4.8049235343933105
Loss :  1.6762892007827759 3.298405885696411 4.974695205688477
Loss :  1.6643866300582886 3.5060739517211914 5.1704607009887695
Loss :  1.6426039934158325 2.835193634033203 4.477797508239746
Loss :  1.6521530151367188 3.2822391986846924 4.934391975402832
Loss :  1.6611732244491577 3.047593832015991 4.708766937255859
Loss :  1.6595381498336792 2.712885856628418 4.372424125671387
Loss :  1.6675384044647217 3.125619411468506 4.793157577514648
Loss :  1.6240485906600952 3.232365369796753 4.856413841247559
  batch 20 loss: 1.6240485906600952, 3.232365369796753, 4.856413841247559
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.659315586090088 3.2290632724761963 4.888379096984863
Loss :  1.6750456094741821 2.9055585861206055 4.580604076385498
Loss :  1.646396279335022 3.2217838764190674 4.868180274963379
Loss :  1.6874455213546753 2.820611000061035 4.508056640625
Loss :  1.6848841905593872 3.7250304222106934 5.409914493560791
Loss :  1.6484920978546143 3.399665355682373 5.048157691955566
Loss :  1.6973923444747925 3.087303638458252 4.784696102142334
Loss :  1.6399003267288208 2.955671787261963 4.595571994781494
Loss :  1.6880747079849243 2.6065828800201416 4.2946577072143555
Loss :  1.6414713859558105 3.034726858139038 4.6761980056762695
Loss :  1.7226978540420532 3.2572133541107178 4.9799113273620605
Loss :  1.6690930128097534 3.2526204586029053 4.921713352203369
Loss :  1.6544311046600342 3.250807523727417 4.905238628387451
Loss :  1.6568403244018555 2.8161346912384033 4.47297477722168
Loss :  1.6968189477920532 3.4289438724517822 5.125762939453125
Loss :  1.6888619661331177 2.673203706741333 4.36206579208374
Loss :  1.6665314435958862 3.2685844898223877 4.935115814208984
Loss :  1.634303331375122 2.9786508083343506 4.612954139709473
Loss :  1.6596966981887817 3.1787796020507812 4.838476181030273
Loss :  1.652197241783142 3.2743043899536133 4.926501750946045
  batch 40 loss: 1.652197241783142, 3.2743043899536133, 4.926501750946045
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6603057384490967 3.4912352561950684 5.151540756225586
Loss :  1.6485525369644165 2.7969119548797607 4.445464611053467
Loss :  1.6654900312423706 3.0778462886810303 4.743336200714111
Loss :  1.6638453006744385 2.9855740070343018 4.64941930770874
Loss :  1.6667358875274658 2.761603593826294 4.42833948135376
Loss :  1.6581507921218872 2.752545118331909 4.410696029663086
Loss :  1.645176649093628 3.1108829975128174 4.756059646606445
Loss :  1.6573702096939087 3.1707632541656494 4.828133583068848
Loss :  1.6301299333572388 3.5227606296539307 5.152890682220459
Loss :  1.6812894344329834 3.00480580329895 4.686095237731934
Loss :  1.6458498239517212 2.581339120864868 4.227189064025879
Loss :  1.664182424545288 3.100597858428955 4.764780044555664
Loss :  1.6826941967010498 3.2469756603240967 4.9296698570251465
Loss :  1.6674836874008179 3.0821712017059326 4.749654769897461
Loss :  1.670677661895752 3.5713913440704346 5.242069244384766
Loss :  1.634995698928833 2.7747719287872314 4.4097676277160645
Loss :  1.6846603155136108 3.2869622707366943 4.971622467041016
Loss :  1.6822105646133423 3.321932315826416 5.004142761230469
Loss :  1.6956325769424438 3.2193751335144043 4.915007591247559
Loss :  1.6709198951721191 2.9476356506347656 4.618555545806885
  batch 60 loss: 1.6709198951721191, 2.9476356506347656, 4.618555545806885
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726332902908325 3.294616460800171 4.967249870300293
Loss :  1.6693921089172363 2.774631977081299 4.444024085998535
Loss :  1.6774474382400513 2.935307264328003 4.612754821777344
Loss :  1.6580458879470825 2.7139463424682617 4.371992111206055
Loss :  1.6550601720809937 3.1218767166137695 4.776937007904053
Loss :  5.4966607093811035 4.392606258392334 9.889266967773438
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.386112213134766 4.410834312438965 9.79694652557373
Loss :  5.4944024085998535 4.212594985961914 9.70699691772461
Loss :  4.576829433441162 4.295646667480469 8.872476577758789
Total LOSS train 4.73509742296659 valid 9.566421747207642
CE LOSS train 1.6640962820786696 valid 1.1442073583602905
Contrastive LOSS train 3.071001148223877 valid 1.0739116668701172
EPOCH 225:
Loss :  1.6504548788070679 2.4122726917266846 4.062727451324463
Loss :  1.6662640571594238 3.4442763328552246 5.110540390014648
Loss :  1.652133822441101 3.0367093086242676 4.688843250274658
Loss :  1.6553038358688354 2.8565030097961426 4.511806964874268
Loss :  1.6799495220184326 3.3497533798217773 5.029703140258789
Loss :  1.6629455089569092 2.7373955249786377 4.400341033935547
Loss :  1.6610366106033325 3.346074342727661 5.007111072540283
Loss :  1.6499757766723633 2.9700279235839844 4.620003700256348
Loss :  1.654610276222229 2.492004632949829 4.146615028381348
Loss :  1.60905921459198 3.010612964630127 4.6196722984313965
Loss :  1.668697714805603 3.317162036895752 4.9858598709106445
Loss :  1.7292088270187378 3.062640905380249 4.791849613189697
Loss :  1.6762744188308716 3.1298208236694336 4.806095123291016
Loss :  1.6643879413604736 3.2428138256073 4.907201766967773
Loss :  1.6425893306732178 2.908083200454712 4.55067253112793
Loss :  1.652148723602295 3.142874240875244 4.795022964477539
Loss :  1.6611684560775757 3.1754038333892822 4.836572170257568
Loss :  1.6595386266708374 3.1393322944641113 4.798871040344238
Loss :  1.6675347089767456 3.0001819133758545 4.6677165031433105
Loss :  1.6240407228469849 3.2670178413391113 4.891058444976807
  batch 20 loss: 1.6240407228469849, 3.2670178413391113, 4.891058444976807
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6593101024627686 3.1290123462677 4.788322448730469
Loss :  1.6750410795211792 2.847801685333252 4.522842884063721
Loss :  1.646394968032837 3.0935864448547363 4.739981651306152
Loss :  1.687426209449768 3.1117169857025146 4.799143314361572
Loss :  1.684871792793274 3.697186231613159 5.382058143615723
Loss :  1.6484750509262085 3.4243879318237305 5.0728631019592285
Loss :  1.6973857879638672 3.1423628330230713 4.839748382568359
Loss :  1.6398996114730835 3.098410129547119 4.738309860229492
Loss :  1.6880813837051392 2.8576622009277344 4.545743465423584
Loss :  1.6414464712142944 2.92802095413208 4.569467544555664
Loss :  1.7226903438568115 3.140728712081909 4.863419055938721
Loss :  1.6690874099731445 3.0774624347686768 4.746549606323242
Loss :  1.6544328927993774 3.0717196464538574 4.726152420043945
Loss :  1.6568310260772705 2.841921091079712 4.498752117156982
Loss :  1.6968152523040771 3.3953049182891846 5.092120170593262
Loss :  1.6888682842254639 2.8207573890686035 4.509625434875488
Loss :  1.666534185409546 3.1222457885742188 4.788780212402344
Loss :  1.6343051195144653 2.815030336380005 4.44933557510376
Loss :  1.659687876701355 3.047410726547241 4.707098484039307
Loss :  1.6521930694580078 3.3092944622039795 4.961487770080566
  batch 40 loss: 1.6521930694580078, 3.3092944622039795, 4.961487770080566
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602953672409058 3.2906062602996826 4.950901508331299
Loss :  1.6485368013381958 2.9516282081604004 4.600164890289307
Loss :  1.6654894351959229 2.9953813552856445 4.660870552062988
Loss :  1.6638330221176147 2.971956968307495 4.63578987121582
Loss :  1.6667194366455078 3.0699892044067383 4.736708641052246
Loss :  1.6581475734710693 3.3089680671691895 4.96711540222168
Loss :  1.6451702117919922 3.396648645401001 5.041818618774414
Loss :  1.6573617458343506 3.120074510574341 4.777436256408691
Loss :  1.6301286220550537 2.9025180339813232 4.532646656036377
Loss :  1.6812736988067627 3.2271616458892822 4.908435344696045
Loss :  1.6458494663238525 2.634413480758667 4.2802629470825195
Loss :  1.6641753911972046 3.029836893081665 4.69401216506958
Loss :  1.6826963424682617 3.2266478538513184 4.90934419631958
Loss :  1.6674683094024658 2.7893409729003906 4.456809043884277
Loss :  1.6706739664077759 3.112271547317505 4.78294563293457
Loss :  1.634995937347412 2.7039222717285156 4.338918209075928
Loss :  1.684664249420166 3.284442901611328 4.969107151031494
Loss :  1.6822009086608887 3.3392553329467773 5.021456241607666
Loss :  1.6956276893615723 3.237931489944458 4.933559417724609
Loss :  1.6709182262420654 2.8333518505096436 4.504270076751709
  batch 60 loss: 1.6709182262420654, 2.8333518505096436, 4.504270076751709
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.672630786895752 3.5295395851135254 5.202170372009277
Loss :  1.669395089149475 2.68145751953125 4.3508524894714355
Loss :  1.677443265914917 3.1929590702056885 4.8704023361206055
Loss :  1.6580415964126587 2.969129800796509 4.627171516418457
Loss :  1.6550549268722534 3.1089603900909424 4.764015197753906
Loss :  5.497669219970703 4.377931594848633 9.875600814819336
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.387266635894775 4.387425422668457 9.77469253540039
Loss :  5.495264053344727 4.2410173416137695 9.736281394958496
Loss :  4.577698707580566 4.216391563415527 8.794090270996094
Total LOSS train 4.739804165179913 valid 9.545166254043579
CE LOSS train 1.66409066090217 valid 1.1444246768951416
Contrastive LOSS train 3.07571350977971 valid 1.0540978908538818
EPOCH 226:
Loss :  1.6504547595977783 2.7539165019989014 4.40437126159668
Loss :  1.6662667989730835 3.0986366271972656 4.764903545379639
Loss :  1.6521308422088623 3.3448593616485596 4.996990203857422
Loss :  1.6553000211715698 2.943495750427246 4.5987958908081055
Loss :  1.6799343824386597 3.5149013996124268 5.194835662841797
Loss :  1.662935495376587 2.838412046432495 4.501347541809082
Loss :  1.6610313653945923 3.376218557357788 5.03725004196167
Loss :  1.6499781608581543 2.5676074028015137 4.217585563659668
Loss :  1.6546168327331543 2.3859803676605225 4.040596961975098
Loss :  1.6090517044067383 2.9504623413085938 4.559514045715332
Loss :  1.6686993837356567 3.3374013900756836 5.006100654602051
Loss :  1.7292168140411377 2.901319980621338 4.630537033081055
Loss :  1.6762837171554565 3.2102949619293213 4.886578559875488
Loss :  1.6643848419189453 3.2400310039520264 4.904416084289551
Loss :  1.642601490020752 3.1521990299224854 4.794800758361816
Loss :  1.6521514654159546 3.3961029052734375 5.048254489898682
Loss :  1.6611695289611816 2.85971999168396 4.5208892822265625
Loss :  1.6595388650894165 2.822537422180176 4.482076168060303
Loss :  1.667537808418274 2.901482582092285 4.5690202713012695
Loss :  1.6240410804748535 3.061850070953369 4.685891151428223
  batch 20 loss: 1.6240410804748535, 3.061850070953369, 4.685891151428223
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6593090295791626 3.1246063709259033 4.7839155197143555
Loss :  1.6750385761260986 2.770120859146118 4.445159435272217
Loss :  1.6463966369628906 2.986893653869629 4.6332902908325195
Loss :  1.6874322891235352 3.462643623352051 5.150075912475586
Loss :  1.6848819255828857 3.1937756538391113 4.878657341003418
Loss :  1.6484781503677368 3.16923189163208 4.817709922790527
Loss :  1.6973860263824463 2.9838831424713135 4.68126916885376
Loss :  1.6398957967758179 3.1901488304138184 4.830044746398926
Loss :  1.6880742311477661 2.4816246032714844 4.169698715209961
Loss :  1.6414594650268555 2.82131028175354 4.462769508361816
Loss :  1.7226953506469727 3.072157382965088 4.7948527336120605
Loss :  1.669089436531067 3.229301929473877 4.898391246795654
Loss :  1.6544289588928223 2.938952684402466 4.593381881713867
Loss :  1.6568351984024048 3.009331226348877 4.666166305541992
Loss :  1.6968168020248413 3.5016088485717773 5.198425769805908
Loss :  1.6888619661331177 2.687227487564087 4.376089572906494
Loss :  1.6665291786193848 3.208716869354248 4.875246047973633
Loss :  1.6343005895614624 2.863560438156128 4.497860908508301
Loss :  1.6596920490264893 2.9694671630859375 4.629158973693848
Loss :  1.6521960496902466 3.1596288681030273 4.811824798583984
  batch 40 loss: 1.6521960496902466, 3.1596288681030273, 4.811824798583984
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6603012084960938 3.250002145767212 4.910303115844727
Loss :  1.6485427618026733 2.7434380054473877 4.3919806480407715
Loss :  1.6654919385910034 3.058619737625122 4.724111557006836
Loss :  1.6638342142105103 3.096233367919922 4.760067462921143
Loss :  1.666731595993042 2.718601942062378 4.38533353805542
Loss :  1.6581453084945679 3.208338975906372 4.86648416519165
Loss :  1.6451795101165771 3.3985354900360107 5.043715000152588
Loss :  1.6573692560195923 2.9650321006774902 4.622401237487793
Loss :  1.6301302909851074 3.510131359100342 5.140261650085449
Loss :  1.6812937259674072 2.893145799636841 4.574439525604248
Loss :  1.6458544731140137 2.5535943508148193 4.199448585510254
Loss :  1.6641716957092285 3.206601142883301 4.870772838592529
Loss :  1.6826972961425781 3.50868558883667 5.191382884979248
Loss :  1.6674680709838867 2.7883849143981934 4.45585298538208
Loss :  1.6706656217575073 3.171922445297241 4.842587947845459
Loss :  1.6349793672561646 2.641468048095703 4.276447296142578
Loss :  1.6846601963043213 3.3067262172698975 4.991386413574219
Loss :  1.6822043657302856 3.080702304840088 4.762906551361084
Loss :  1.6956226825714111 3.0622975826263428 4.757920265197754
Loss :  1.6709120273590088 2.7541935443878174 4.425105571746826
  batch 60 loss: 1.6709120273590088, 2.7541935443878174, 4.425105571746826
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726306676864624 3.5121865272521973 5.184817314147949
Loss :  1.6693997383117676 2.784517765045166 4.453917503356934
Loss :  1.6774506568908691 3.441977024078369 5.119427680969238
Loss :  1.658044695854187 2.8409788608551025 4.4990234375
Loss :  1.6550662517547607 3.4036505222320557 5.058716773986816
Loss :  5.497901916503906 4.385976791381836 9.883878707885742
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.3872270584106445 4.333621025085449 9.720848083496094
Loss :  5.4955830574035645 4.196664333343506 9.69224739074707
Loss :  4.578768730163574 4.185471057891846 8.764240264892578
Total LOSS train 4.716116244976337 valid 9.515303611755371
CE LOSS train 1.6640918566630436 valid 1.1446921825408936
Contrastive LOSS train 3.0520244194911075 valid 1.0463677644729614
EPOCH 227:
Loss :  1.650460124015808 2.735363006591797 4.3858232498168945
Loss :  1.6662700176239014 3.3351211547851562 5.001391410827637
Loss :  1.6521365642547607 2.9942626953125 4.64639949798584
Loss :  1.6553035974502563 2.801831007003784 4.45713472366333
Loss :  1.679937481880188 3.273725986480713 4.953663349151611
Loss :  1.662940502166748 2.65970778465271 4.322648048400879
Loss :  1.6610299348831177 3.3268988132476807 4.987928867340088
Loss :  1.6499732732772827 2.591397762298584 4.241371154785156
Loss :  1.6546164751052856 2.6283693313598633 4.282985687255859
Loss :  1.6090465784072876 2.8877971172332764 4.4968438148498535
Loss :  1.6686985492706299 3.152153491973877 4.820852279663086
Loss :  1.7292191982269287 3.0974502563476562 4.826669692993164
Loss :  1.6763077974319458 3.2466893196105957 4.922996997833252
Loss :  1.6643880605697632 3.306218385696411 4.970606327056885
Loss :  1.6425920724868774 3.2742340564727783 4.916826248168945
Loss :  1.6521520614624023 3.2166593074798584 4.86881160736084
Loss :  1.661167860031128 2.9044058322906494 4.565573692321777
Loss :  1.6595408916473389 2.8078835010528564 4.467424392700195
Loss :  1.6675431728363037 3.0219943523406982 4.689537525177002
Loss :  1.6240426301956177 3.251420497894287 4.875463008880615
  batch 20 loss: 1.6240426301956177, 3.251420497894287, 4.875463008880615
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6593011617660522 3.257493019104004 4.916794300079346
Loss :  1.675042748451233 2.760221242904663 4.4352641105651855
Loss :  1.6463946104049683 3.2345314025878906 4.880926132202148
Loss :  1.6874315738677979 3.01228666305542 4.699718475341797
Loss :  1.6848734617233276 3.550583600997925 5.235456943511963
Loss :  1.6484758853912354 3.122521162033081 4.770997047424316
Loss :  1.6973755359649658 3.123718738555908 4.821094512939453
Loss :  1.6398996114730835 3.2879185676574707 4.927818298339844
Loss :  1.6880759000778198 3.0137875080108643 4.7018632888793945
Loss :  1.6414490938186646 3.061171531677246 4.702620506286621
Loss :  1.7226835489273071 3.1120426654815674 4.834726333618164
Loss :  1.6690837144851685 3.2457826137542725 4.9148664474487305
Loss :  1.6544365882873535 3.271226644515991 4.925662994384766
Loss :  1.6568338871002197 2.6919662952423096 4.348800182342529
Loss :  1.696812629699707 3.3376474380493164 5.034460067749023
Loss :  1.6888582706451416 2.6783230304718018 4.367181301116943
Loss :  1.6665233373641968 3.0511255264282227 4.717648983001709
Loss :  1.634306788444519 2.956983804702759 4.591290473937988
Loss :  1.659700632095337 2.8676769733428955 4.527377605438232
Loss :  1.6521929502487183 3.420833110809326 5.073026180267334
  batch 40 loss: 1.6521929502487183, 3.420833110809326, 5.073026180267334
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.660295009613037 3.3835082054138184 5.0438032150268555
Loss :  1.6485371589660645 3.1640970706939697 4.812634468078613
Loss :  1.6654891967773438 3.356361150741577 5.0218505859375
Loss :  1.663831114768982 3.0263593196868896 4.690190315246582
Loss :  1.6667202711105347 2.570817470550537 4.237537860870361
Loss :  1.6581391096115112 3.2311336994171143 4.889272689819336
Loss :  1.6451749801635742 3.2205164432525635 4.865691184997559
Loss :  1.6573734283447266 2.796832799911499 4.454206466674805
Loss :  1.6301333904266357 3.344346523284912 4.974479675292969
Loss :  1.6812849044799805 3.0085272789001465 4.689812183380127
Loss :  1.6458520889282227 2.757859468460083 4.403711318969727
Loss :  1.6641706228256226 2.862734079360962 4.526904582977295
Loss :  1.6826975345611572 3.549522876739502 5.232220649719238
Loss :  1.6674734354019165 2.9818217754364014 4.649295330047607
Loss :  1.6706606149673462 3.6260390281677246 5.296699523925781
Loss :  1.6349936723709106 2.778095245361328 4.413088798522949
Loss :  1.6846587657928467 3.3015999794006348 4.986258506774902
Loss :  1.6822036504745483 3.389935255050659 5.072138786315918
Loss :  1.6956217288970947 3.4043545722961426 5.099976539611816
Loss :  1.670922875404358 2.6453802585601807 4.316303253173828
  batch 60 loss: 1.670922875404358, 2.6453802585601807, 4.316303253173828
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726255416870117 3.367455244064331 5.040081024169922
Loss :  1.669387698173523 2.8909597396850586 4.560347557067871
Loss :  1.6774426698684692 2.904649019241333 4.582091808319092
Loss :  1.658048391342163 2.838156223297119 4.496204376220703
Loss :  1.6550506353378296 2.979374885559082 4.634425640106201
Loss :  5.497556686401367 4.3957600593566895 9.893316268920898
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.387039661407471 4.478598594665527 9.865638732910156
Loss :  5.495552062988281 4.219254016876221 9.714805603027344
Loss :  4.5785231590271 4.305418968200684 8.883941650390625
Total LOSS train 4.740273416959322 valid 9.589425563812256
CE LOSS train 1.6640908809808583 valid 1.144630789756775
Contrastive LOSS train 3.07618250480065 valid 1.076354742050171
EPOCH 228:
Loss :  1.6504652500152588 2.7833263874053955 4.433791637420654
Loss :  1.666263222694397 3.2039923667907715 4.870255470275879
Loss :  1.6521310806274414 3.0139551162719727 4.666086196899414
Loss :  1.6552940607070923 2.905439615249634 4.560733795166016
Loss :  1.6799291372299194 3.1480419635772705 4.8279709815979
Loss :  1.662935733795166 2.760531425476074 4.42346715927124
Loss :  1.6610321998596191 3.638170003890991 5.299201965332031
Loss :  1.6499661207199097 2.548328161239624 4.198294162750244
Loss :  1.6546080112457275 2.359565258026123 4.01417350769043
Loss :  1.6090492010116577 2.5973916053771973 4.2064409255981445
Loss :  1.6686952114105225 3.6030802726745605 5.271775245666504
Loss :  1.7292062044143677 2.679842233657837 4.409048557281494
Loss :  1.6763007640838623 3.256120443344116 4.9324212074279785
Loss :  1.6643905639648438 3.5373692512512207 5.2017598152160645
Loss :  1.6425858736038208 3.28688907623291 4.929474830627441
Loss :  1.6521505117416382 3.1080963611602783 4.760246753692627
Loss :  1.6611617803573608 2.8069815635681152 4.468143463134766
Loss :  1.6595628261566162 3.110748052597046 4.770310878753662
Loss :  1.667542815208435 2.823748826980591 4.491291522979736
Loss :  1.6240317821502686 3.2456719875335693 4.869703769683838
  batch 20 loss: 1.6240317821502686, 3.2456719875335693, 4.869703769683838
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.659299612045288 3.242955207824707 4.902255058288574
Loss :  1.6750463247299194 2.793900966644287 4.468947410583496
Loss :  1.6463842391967773 3.186492919921875 4.832877159118652
Loss :  1.6874223947525024 2.9527697563171387 4.640192031860352
Loss :  1.6848641633987427 3.3126933574676514 4.997557640075684
Loss :  1.6484726667404175 3.2656681537628174 4.914140701293945
Loss :  1.6973744630813599 3.165536403656006 4.862910747528076
Loss :  1.639902949333191 2.933035373687744 4.572938442230225
Loss :  1.6880767345428467 2.986832857131958 4.674909591674805
Loss :  1.641442894935608 2.937721014022827 4.579164028167725
Loss :  1.7226860523223877 3.2947256565093994 5.017411708831787
Loss :  1.6690843105316162 3.0809426307678223 4.750026702880859
Loss :  1.654431700706482 2.9129960536956787 4.567427635192871
Loss :  1.656832218170166 3.0553948879241943 4.712226867675781
Loss :  1.6968132257461548 3.608736276626587 5.305549621582031
Loss :  1.688857913017273 2.7674975395202637 4.456355571746826
Loss :  1.6665273904800415 3.156967878341675 4.823495388031006
Loss :  1.6343151330947876 2.7857179641723633 4.420032978057861
Loss :  1.659702181816101 3.0873167514801025 4.747018814086914
Loss :  1.6521931886672974 3.155052423477173 4.80724573135376
  batch 40 loss: 1.6521931886672974, 3.155052423477173, 4.80724573135376
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602954864501953 3.242441415786743 4.902736663818359
Loss :  1.6485419273376465 2.757976770401001 4.406518936157227
Loss :  1.6654880046844482 3.261212110519409 4.926700115203857
Loss :  1.6638317108154297 2.99180269241333 4.65563440322876
Loss :  1.666717767715454 3.145430564880371 4.812148094177246
Loss :  1.658137321472168 3.2855122089385986 4.9436492919921875
Loss :  1.6451694965362549 3.2120490074157715 4.8572187423706055
Loss :  1.65736722946167 2.793696641921997 4.451064109802246
Loss :  1.6301441192626953 3.1990482807159424 4.829192161560059
Loss :  1.6812808513641357 3.561617851257324 5.242898941040039
Loss :  1.6458544731140137 2.494208574295044 4.140063285827637
Loss :  1.6641680002212524 2.933762550354004 4.597930431365967
Loss :  1.6826971769332886 3.2414627075195312 4.924160003662109
Loss :  1.6674706935882568 2.9156203269958496 4.583090782165527
Loss :  1.6706680059432983 3.3873581886291504 5.058026313781738
Loss :  1.6349865198135376 2.6219913959503174 4.2569780349731445
Loss :  1.6846568584442139 3.2116522789001465 4.896308898925781
Loss :  1.682203769683838 3.27134370803833 4.953547477722168
Loss :  1.6956239938735962 3.186729669570923 4.882353782653809
Loss :  1.6709212064743042 2.5337483882904053 4.20466947555542
  batch 60 loss: 1.6709212064743042, 2.5337483882904053, 4.20466947555542
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726223230361938 3.0816352367401123 4.754257678985596
Loss :  1.6693925857543945 2.782635450363159 4.452028274536133
Loss :  1.677440881729126 3.3669633865356445 5.044404029846191
Loss :  1.6580418348312378 2.700284719467163 4.358326435089111
Loss :  1.6550716161727905 3.1142899990081787 4.76936149597168
Loss :  5.4971184730529785 4.382795333862305 9.879913330078125
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.38643217086792 4.218521595001221 9.60495376586914
Loss :  5.495028972625732 4.245502948760986 9.740531921386719
Loss :  4.577014446258545 4.1262688636779785 8.703283309936523
Total LOSS train 4.716316054417537 valid 9.482170581817627
CE LOSS train 1.6640896302003128 valid 1.1442536115646362
Contrastive LOSS train 3.0522264333871694 valid 1.0315672159194946
EPOCH 229:
Loss :  1.6504555940628052 2.770885467529297 4.4213409423828125
Loss :  1.6662631034851074 3.3271963596343994 4.993459701538086
Loss :  1.6521329879760742 3.0008108615875244 4.6529436111450195
Loss :  1.6552879810333252 2.84566593170166 4.500953674316406
Loss :  1.6799342632293701 3.232466459274292 4.912400722503662
Loss :  1.6629350185394287 2.8452727794647217 4.50820779800415
Loss :  1.6610286235809326 3.4889657497406006 5.149994373321533
Loss :  1.6499605178833008 2.6924424171447754 4.342402935028076
Loss :  1.6546038389205933 2.5336995124816895 4.188303470611572
Loss :  1.6090426445007324 2.8117165565490723 4.420759201049805
Loss :  1.668700098991394 3.3726255893707275 5.041325569152832
Loss :  1.7292132377624512 3.019296169281006 4.748509407043457
Loss :  1.6763007640838623 3.3059029579162598 4.982203483581543
Loss :  1.6643873453140259 3.613694667816162 5.278081893920898
Loss :  1.6425867080688477 2.8867735862731934 4.529360294342041
Loss :  1.652160406112671 3.4455811977386475 5.097741603851318
Loss :  1.6611685752868652 2.7146496772766113 4.375818252563477
Loss :  1.6595640182495117 2.625302314758301 4.2848663330078125
Loss :  1.6675440073013306 2.9093384742736816 4.576882362365723
Loss :  1.624039649963379 3.086029529571533 4.710069179534912
  batch 20 loss: 1.624039649963379, 3.086029529571533, 4.710069179534912
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.659296989440918 2.8492696285247803 4.508566856384277
Loss :  1.6750444173812866 2.7585060596466064 4.4335503578186035
Loss :  1.6463826894760132 3.5170419216156006 5.163424491882324
Loss :  1.6874278783798218 2.653994083404541 4.341422080993652
Loss :  1.6848710775375366 3.6720263957977295 5.356897354125977
Loss :  1.6484670639038086 3.492995023727417 5.141462326049805
Loss :  1.6973735094070435 3.2404227256774902 4.937796115875244
Loss :  1.6399017572402954 2.977325916290283 4.617227554321289
Loss :  1.6880767345428467 2.7157368659973145 4.403813362121582
Loss :  1.6414415836334229 2.968827486038208 4.610269069671631
Loss :  1.7226778268814087 3.2560524940490723 4.978730201721191
Loss :  1.6690967082977295 3.0437722206115723 4.712868690490723
Loss :  1.6544321775436401 3.2909979820251465 4.945430278778076
Loss :  1.65683114528656 2.860163688659668 4.516994953155518
Loss :  1.6968165636062622 3.3869338035583496 5.083750247955322
Loss :  1.6888643503189087 2.7409579753875732 4.4298224449157715
Loss :  1.6665279865264893 3.037728786468506 4.704257011413574
Loss :  1.6343199014663696 3.0459649562835693 4.6802849769592285
Loss :  1.6597023010253906 2.9427096843719482 4.602412223815918
Loss :  1.65218985080719 3.4545414447784424 5.106731414794922
  batch 40 loss: 1.65218985080719, 3.4545414447784424, 5.106731414794922
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602932214736938 3.6410434246063232 5.301336765289307
Loss :  1.6485450267791748 2.7461421489715576 4.394687175750732
Loss :  1.6654934883117676 3.0820231437683105 4.747516632080078
Loss :  1.6638299226760864 2.945638418197632 4.609468460083008
Loss :  1.6667231321334839 2.9143688678741455 4.58109188079834
Loss :  1.6581307649612427 2.845242738723755 4.503373622894287
Loss :  1.6451752185821533 3.126594305038452 4.7717695236206055
Loss :  1.6573657989501953 2.736945629119873 4.394311428070068
Loss :  1.6301336288452148 3.5037999153137207 5.1339335441589355
Loss :  1.6812807321548462 3.195814847946167 4.877095699310303
Loss :  1.645849347114563 2.5743796825408936 4.220229148864746
Loss :  1.6641690731048584 2.788241386413574 4.452410697937012
Loss :  1.6826919317245483 3.4795095920562744 5.162201404571533
Loss :  1.6674646139144897 3.1128060817718506 4.780270576477051
Loss :  1.6706678867340088 3.4767959117889404 5.147463798522949
Loss :  1.6349879503250122 2.791485548019409 4.426473617553711
Loss :  1.6846518516540527 3.3695762157440186 5.054227828979492
Loss :  1.6822043657302856 3.2335798740386963 4.9157843589782715
Loss :  1.6956201791763306 3.234846591949463 4.930466651916504
Loss :  1.6709237098693848 2.6569671630859375 4.327890872955322
  batch 60 loss: 1.6709237098693848, 2.6569671630859375, 4.327890872955322
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726185083389282 3.1603996753692627 4.8330183029174805
Loss :  1.6693931818008423 2.557669162750244 4.227062225341797
Loss :  1.6774351596832275 3.0338478088378906 4.711282730102539
Loss :  1.6580382585525513 2.643270969390869 4.301309108734131
Loss :  1.655053734779358 3.040179967880249 4.6952338218688965
Loss :  5.497162818908691 4.443509578704834 9.940671920776367
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.38662052154541 4.402275562286377 9.788896560668945
Loss :  5.495462417602539 4.215363025665283 9.710824966430664
Loss :  4.576842784881592 4.16935396194458 8.746196746826172
Total LOSS train 4.715249949235183 valid 9.546647548675537
CE LOSS train 1.6640891166833731 valid 1.144210696220398
Contrastive LOSS train 3.0511608380537765 valid 1.042338490486145
EPOCH 230:
Loss :  1.6504623889923096 2.606910228729248 4.257372856140137
Loss :  1.6662663221359253 3.361629009246826 5.027895450592041
Loss :  1.6521307229995728 3.00305438041687 4.655185222625732
Loss :  1.655288577079773 2.8444457054138184 4.499734401702881
Loss :  1.679932951927185 3.2508838176727295 4.930816650390625
Loss :  1.6629337072372437 2.6844446659088135 4.347378253936768
Loss :  1.6610311269760132 3.228593111038208 4.889624118804932
Loss :  1.6499598026275635 2.7694854736328125 4.419445037841797
Loss :  1.6546144485473633 2.4747281074523926 4.129342555999756
Loss :  1.6090364456176758 2.6847407817840576 4.2937774658203125
Loss :  1.6686934232711792 3.514989137649536 5.183682441711426
Loss :  1.729229211807251 2.956059694290161 4.685288906097412
Loss :  1.676282525062561 3.492133140563965 5.168415546417236
Loss :  1.6643961668014526 3.602757215499878 5.267153263092041
Loss :  1.6426000595092773 3.173441171646118 4.816040992736816
Loss :  1.6521562337875366 3.3525898456573486 5.004745960235596
Loss :  1.66116464138031 2.6835615634918213 4.344726085662842
Loss :  1.6595573425292969 2.691105842590332 4.350663185119629
Loss :  1.6675384044647217 2.9079232215881348 4.575461387634277
Loss :  1.6240465641021729 3.003344774246216 4.627391338348389
  batch 20 loss: 1.6240465641021729, 3.003344774246216, 4.627391338348389
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592985391616821 3.2823896408081055 4.941688060760498
Loss :  1.6750457286834717 3.0503244400024414 4.725370407104492
Loss :  1.646375298500061 3.3362045288085938 4.982579708099365
Loss :  1.6874326467514038 2.626777172088623 4.314209938049316
Loss :  1.6848632097244263 3.53572416305542 5.220587253570557
Loss :  1.6484705209732056 3.1728675365448 4.821338176727295
Loss :  1.6973700523376465 3.374335765838623 5.0717058181762695
Loss :  1.639901876449585 2.9669570922851562 4.60685920715332
Loss :  1.6880724430084229 2.9155099391937256 4.603582382202148
Loss :  1.6414433717727661 2.898512840270996 4.539956092834473
Loss :  1.7226805686950684 3.122676134109497 4.8453569412231445
Loss :  1.6690866947174072 3.164940118789673 4.83402681350708
Loss :  1.65442955493927 2.9002974033355713 4.554727077484131
Loss :  1.656840205192566 2.788928985595703 4.445769309997559
Loss :  1.69680655002594 3.494699001312256 5.191505432128906
Loss :  1.6888558864593506 2.6836936473846436 4.372549533843994
Loss :  1.666523814201355 2.919726610183716 4.586250305175781
Loss :  1.634310245513916 2.8131096363067627 4.447420120239258
Loss :  1.6597025394439697 3.0260674953460693 4.685770034790039
Loss :  1.652193546295166 3.268181085586548 4.920374870300293
  batch 40 loss: 1.652193546295166, 3.268181085586548, 4.920374870300293
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602932214736938 3.3031513690948486 4.963444709777832
Loss :  1.6485309600830078 2.7858047485351562 4.434335708618164
Loss :  1.6654870510101318 3.39294171333313 5.058428764343262
Loss :  1.6638303995132446 2.9310271739959717 4.594857692718506
Loss :  1.6667147874832153 2.8427445888519287 4.509459495544434
Loss :  1.6581292152404785 3.0752177238464355 4.733346939086914
Loss :  1.6451629400253296 3.1550981998443604 4.8002610206604
Loss :  1.6573541164398193 2.8243749141693115 4.481729030609131
Loss :  1.6301300525665283 3.1137654781341553 4.743895530700684
Loss :  1.6812716722488403 3.0601069927215576 4.7413787841796875
Loss :  1.645855188369751 2.578462839126587 4.224318027496338
Loss :  1.6641669273376465 2.721332311630249 4.385499000549316
Loss :  1.6826941967010498 3.429335594177246 5.112030029296875
Loss :  1.6674551963806152 2.9268300533294678 4.594285011291504
Loss :  1.6706771850585938 3.352484703063965 5.023161888122559
Loss :  1.6349937915802002 2.6317787170410156 4.266772270202637
Loss :  1.6846590042114258 3.1764025688171387 4.8610615730285645
Loss :  1.6821988821029663 3.07427978515625 4.756478786468506
Loss :  1.695630431175232 3.1276955604553223 4.823326110839844
Loss :  1.670914888381958 2.86784291267395 4.538757801055908
  batch 60 loss: 1.670914888381958, 2.86784291267395, 4.538757801055908
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726194620132446 3.2718136310577393 4.944433212280273
Loss :  1.6693865060806274 2.7155532836914062 4.384939670562744
Loss :  1.6774230003356934 2.966745615005493 4.644168853759766
Loss :  1.6580398082733154 2.700434684753418 4.3584747314453125
Loss :  1.6550440788269043 3.229494333267212 4.884538650512695
Loss :  5.494598865509033 4.462035655975342 9.956634521484375
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.383901119232178 4.388918876647949 9.772819519042969
Loss :  5.4929609298706055 4.240475177764893 9.733436584472656
Loss :  4.573126792907715 4.22437858581543 8.797505378723145
Total LOSS train 4.693063875345083 valid 9.565099000930786
CE LOSS train 1.6640874972710242 valid 1.1432816982269287
Contrastive LOSS train 3.0289763634021467 valid 1.0560946464538574
EPOCH 231:
Loss :  1.650448203086853 2.699157238006592 4.349605560302734
Loss :  1.666263222694397 3.149543523788452 4.815806865692139
Loss :  1.6521247625350952 3.0611984729766846 4.71332311630249
Loss :  1.6552786827087402 2.6753621101379395 4.33064079284668
Loss :  1.6799297332763672 3.430504560470581 5.110434532165527
Loss :  1.6629160642623901 2.791652202606201 4.454568386077881
Loss :  1.661037802696228 3.537964344024658 5.199002265930176
Loss :  1.6499577760696411 2.382920503616333 4.032878398895264
Loss :  1.6546064615249634 2.2140896320343018 3.8686962127685547
Loss :  1.6090493202209473 3.2943429946899414 4.903392314910889
Loss :  1.6686878204345703 3.3240888118743896 4.992776870727539
Loss :  1.729202389717102 3.0180866718292236 4.747289180755615
Loss :  1.676292896270752 3.054877758026123 4.731170654296875
Loss :  1.6643975973129272 3.293987512588501 4.958384990692139
Loss :  1.6425895690917969 3.023635149002075 4.666224479675293
Loss :  1.652151346206665 3.245575428009033 4.897727012634277
Loss :  1.6611558198928833 2.636200189590454 4.297356128692627
Loss :  1.6595454216003418 2.6883881092071533 4.347933769226074
Loss :  1.6675306558609009 2.7962772846221924 4.463808059692383
Loss :  1.6240369081497192 2.925713539123535 4.549750328063965
  batch 20 loss: 1.6240369081497192, 2.925713539123535, 4.549750328063965
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6593022346496582 2.874811887741089 4.534113883972168
Loss :  1.6750411987304688 2.733452558517456 4.408493995666504
Loss :  1.6463879346847534 2.9566843509674072 4.603072166442871
Loss :  1.6874165534973145 3.0261311531066895 4.713547706604004
Loss :  1.6848639249801636 3.3236091136932373 5.008472919464111
Loss :  1.6484713554382324 3.229750156402588 4.87822151184082
Loss :  1.6973711252212524 2.9633195400238037 4.660690784454346
Loss :  1.6398955583572388 2.6097030639648438 4.249598503112793
Loss :  1.6880730390548706 2.5872762203216553 4.275349140167236
Loss :  1.641435980796814 2.8152010440826416 4.456636905670166
Loss :  1.7226861715316772 2.9517502784729004 4.674436569213867
Loss :  1.6690775156021118 3.1927266120910645 4.861804008483887
Loss :  1.6544259786605835 2.7958908081054688 4.450316905975342
Loss :  1.6568371057510376 2.7435083389282227 4.400345325469971
Loss :  1.6967992782592773 3.46860671043396 5.165406227111816
Loss :  1.6888601779937744 2.8279693126678467 4.516829490661621
Loss :  1.6665197610855103 3.1589455604553223 4.825465202331543
Loss :  1.6343008279800415 2.8405051231384277 4.47480583190918
Loss :  1.659698247909546 2.805980920791626 4.465679168701172
Loss :  1.6521875858306885 3.197096347808838 4.8492841720581055
  batch 40 loss: 1.6521875858306885, 3.197096347808838, 4.8492841720581055
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602896451950073 3.2289726734161377 4.8892621994018555
Loss :  1.648529291152954 2.813093900680542 4.461623191833496
Loss :  1.6654928922653198 3.193711996078491 4.8592047691345215
Loss :  1.6638216972351074 2.9938976764678955 4.657719612121582
Loss :  1.6667094230651855 2.9647512435913086 4.631460666656494
Loss :  1.658129334449768 3.0773472785949707 4.735476493835449
Loss :  1.6451475620269775 2.9752073287963867 4.620354652404785
Loss :  1.6573402881622314 2.733973503112793 4.391313552856445
Loss :  1.6301298141479492 3.25744366645813 4.8875732421875
Loss :  1.6812664270401 3.1521058082580566 4.833372116088867
Loss :  1.6458570957183838 2.5819976329803467 4.2278547286987305
Loss :  1.6641608476638794 2.8736655712127686 4.5378265380859375
Loss :  1.6826969385147095 3.3986175060272217 5.081314563751221
Loss :  1.6674540042877197 3.0509724617004395 4.718426704406738
Loss :  1.6706740856170654 3.4650187492370605 5.135692596435547
Loss :  1.6349884271621704 2.7098214626312256 4.3448100090026855
Loss :  1.6846578121185303 3.005683660507202 4.690341472625732
Loss :  1.6821969747543335 3.2735791206359863 4.955776214599609
Loss :  1.6956310272216797 3.0239157676696777 4.719546794891357
Loss :  1.6709123849868774 2.7513813972473145 4.422293663024902
  batch 60 loss: 1.6709123849868774, 2.7513813972473145, 4.422293663024902
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726093292236328 3.3168528079986572 4.989461898803711
Loss :  1.669382095336914 2.790459156036377 4.459841251373291
Loss :  1.6774121522903442 3.156860828399658 4.834272861480713
Loss :  1.6580379009246826 2.8739778995513916 4.532015800476074
Loss :  1.6550517082214355 3.316727638244629 4.9717793464660645
Loss :  5.493803024291992 4.4045867919921875 9.89838981628418
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.382932186126709 4.365588188171387 9.748519897460938
Loss :  5.492272853851318 4.22487211227417 9.717144966125488
Loss :  4.57240104675293 4.171983242034912 8.744384765625
Total LOSS train 4.653722388927753 valid 9.527109861373901
CE LOSS train 1.664083587206327 valid 1.1431002616882324
Contrastive LOSS train 2.989638798053448 valid 1.042995810508728
EPOCH 232:
Loss :  1.6504405736923218 2.9307992458343506 4.581239700317383
Loss :  1.6662613153457642 3.145036220550537 4.811297416687012
Loss :  1.652118444442749 3.0021564960479736 4.654274940490723
Loss :  1.6552753448486328 2.727163791656494 4.382439136505127
Loss :  1.6799280643463135 3.3904855251312256 5.070413589477539
Loss :  1.6629095077514648 2.6732444763183594 4.336153984069824
Loss :  1.6610429286956787 3.497065305709839 5.158108234405518
Loss :  1.6499600410461426 2.5812981128692627 4.231258392333984
Loss :  1.6546066999435425 2.6386258602142334 4.293232440948486
Loss :  1.6090505123138428 2.7754030227661133 4.384453773498535
Loss :  1.6686874628067017 3.3588459491729736 5.027533531188965
Loss :  1.729206919670105 2.94667911529541 4.675886154174805
Loss :  1.6762815713882446 3.6236512660980225 5.299932956695557
Loss :  1.6643962860107422 3.534604072570801 5.199000358581543
Loss :  1.6425954103469849 3.0801475048065186 4.722743034362793
Loss :  1.652156114578247 3.4244165420532227 5.076572418212891
Loss :  1.6611523628234863 2.6759588718414307 4.337111473083496
Loss :  1.6595407724380493 2.922187328338623 4.581727981567383
Loss :  1.6675236225128174 2.7243642807006836 4.391887664794922
Loss :  1.624032974243164 3.0919229984283447 4.71595573425293
  batch 20 loss: 1.624032974243164, 3.0919229984283447, 4.71595573425293
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.659302830696106 3.1047871112823486 4.764090061187744
Loss :  1.6750353574752808 2.714038133621216 4.389073371887207
Loss :  1.646389126777649 2.932525873184204 4.578915119171143
Loss :  1.6874094009399414 3.0259809494018555 4.713390350341797
Loss :  1.6848623752593994 3.4992029666900635 5.184065341949463
Loss :  1.6484720706939697 3.3740437030792236 5.022515773773193
Loss :  1.6973646879196167 2.843881368637085 4.541245937347412
Loss :  1.6398954391479492 3.140784502029419 4.780679702758789
Loss :  1.6880710124969482 2.6782710552215576 4.366342067718506
Loss :  1.641442894935608 2.9962778091430664 4.637720584869385
Loss :  1.7226850986480713 3.0805702209472656 4.803255081176758
Loss :  1.6690748929977417 3.2898128032684326 4.958887577056885
Loss :  1.6544209718704224 3.0564990043640137 4.7109198570251465
Loss :  1.6568421125411987 2.8119099140167236 4.468751907348633
Loss :  1.696794033050537 3.4958274364471436 5.192621231079102
Loss :  1.688858151435852 2.6259207725524902 4.314778804779053
Loss :  1.6665235757827759 3.178771495819092 4.845294952392578
Loss :  1.6342912912368774 3.1442248821258545 4.7785162925720215
Loss :  1.6596986055374146 3.0876753330230713 4.747374057769775
Loss :  1.6521950960159302 3.102055788040161 4.754251003265381
  batch 40 loss: 1.6521950960159302, 3.102055788040161, 4.754251003265381
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602869033813477 3.1742281913757324 4.83451509475708
Loss :  1.6485298871994019 2.8295722007751465 4.478102207183838
Loss :  1.665494680404663 2.7612686157226562 4.426763534545898
Loss :  1.663822889328003 3.0166027545928955 4.680425643920898
Loss :  1.6667035818099976 2.947700023651123 4.61440372467041
Loss :  1.658132553100586 2.9316842555999756 4.589817047119141
Loss :  1.6451481580734253 2.97763991355896 4.622787952423096
Loss :  1.6573355197906494 2.8424391746520996 4.499774932861328
Loss :  1.6301348209381104 3.363389492034912 4.993524551391602
Loss :  1.681270956993103 2.977543592453003 4.658814430236816
Loss :  1.6458616256713867 2.6171650886535645 4.263026714324951
Loss :  1.6641631126403809 2.671285390853882 4.335448265075684
Loss :  1.6826910972595215 3.267096757888794 4.9497880935668945
Loss :  1.667445421218872 3.1060590744018555 4.773504257202148
Loss :  1.6706727743148804 3.4531898498535156 5.1238627433776855
Loss :  1.6349842548370361 2.6763579845428467 4.311342239379883
Loss :  1.68466055393219 3.3398141860961914 5.024474620819092
Loss :  1.6821978092193604 3.2051925659179688 4.88739013671875
Loss :  1.695634365081787 3.0406494140625 4.736283779144287
Loss :  1.6709150075912476 2.8800196647644043 4.550934791564941
  batch 60 loss: 1.6709150075912476, 2.8800196647644043, 4.550934791564941
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726154088974 3.441810369491577 5.1144256591796875
Loss :  1.669385313987732 2.7426958084106445 4.412081241607666
Loss :  1.677417278289795 3.155026912689209 4.832444190979004
Loss :  1.6580374240875244 2.7355356216430664 4.393572807312012
Loss :  1.6550495624542236 2.837649345397949 4.492698669433594
Loss :  5.49360466003418 4.3775482177734375 9.871152877807617
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.382685661315918 4.3830180168151855 9.765703201293945
Loss :  5.491605758666992 4.3039021492004395 9.795507431030273
Loss :  4.5717244148254395 4.285255432128906 8.856979370117188
Total LOSS train 4.693601835691012 valid 9.572335720062256
CE LOSS train 1.664082875618568 valid 1.1429311037063599
Contrastive LOSS train 3.0295189747443567 valid 1.0713138580322266
EPOCH 233:
Loss :  1.6504511833190918 2.6605124473571777 4.3109636306762695
Loss :  1.6662629842758179 3.044001817703247 4.710264682769775
Loss :  1.6521189212799072 2.7830543518066406 4.435173034667969
Loss :  1.655273199081421 2.6871824264526367 4.342455863952637
Loss :  1.679923415184021 3.141545057296753 4.821468353271484
Loss :  1.6629091501235962 2.9106321334838867 4.573541164398193
Loss :  1.6610438823699951 3.3522934913635254 5.013337135314941
Loss :  1.6499584913253784 2.6208577156066895 4.270816326141357
Loss :  1.6546151638031006 2.2612099647521973 3.915825128555298
Loss :  1.6090550422668457 2.6110548973083496 4.220109939575195
Loss :  1.6686885356903076 3.425616979598999 5.094305515289307
Loss :  1.729207992553711 2.93293833732605 4.66214656829834
Loss :  1.676275610923767 3.2466142177581787 4.922889709472656
Loss :  1.66439950466156 3.413158893585205 5.077558517456055
Loss :  1.6426035165786743 3.3420908451080322 4.984694480895996
Loss :  1.6521475315093994 2.809887409210205 4.462035179138184
Loss :  1.6611547470092773 2.7347495555877686 4.395904541015625
Loss :  1.6595503091812134 2.854428768157959 4.513978958129883
Loss :  1.6675375699996948 2.6846063137054443 4.35214376449585
Loss :  1.624036431312561 2.868969202041626 4.493005752563477
  batch 20 loss: 1.624036431312561, 2.868969202041626, 4.493005752563477
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6593018770217896 3.099163770675659 4.758465766906738
Loss :  1.6750415563583374 2.855013370513916 4.530055046081543
Loss :  1.646393060684204 2.867583990097046 4.51397705078125
Loss :  1.6874067783355713 2.9394044876098633 4.6268110275268555
Loss :  1.684861660003662 3.477311849594116 5.162173271179199
Loss :  1.6484757661819458 3.2262797355651855 4.874755382537842
Loss :  1.6973682641983032 3.0230133533477783 4.720381736755371
Loss :  1.6399083137512207 2.8373844623565674 4.477293014526367
Loss :  1.6880770921707153 2.924049139022827 4.612126350402832
Loss :  1.641430139541626 2.835719347000122 4.477149486541748
Loss :  1.7226905822753906 3.0604395866394043 4.783130168914795
Loss :  1.669079303741455 3.4730515480041504 5.1421308517456055
Loss :  1.6544241905212402 2.969619035720825 4.6240434646606445
Loss :  1.6568490266799927 2.678257942199707 4.33510684967041
Loss :  1.6967990398406982 3.489206075668335 5.186005115509033
Loss :  1.6888676881790161 2.641841173171997 4.330708980560303
Loss :  1.666523814201355 3.200601816177368 4.867125511169434
Loss :  1.6343097686767578 2.975262403488159 4.609572410583496
Loss :  1.659696102142334 3.0338070392608643 4.693503379821777
Loss :  1.6521915197372437 3.239386558532715 4.891578197479248
  batch 40 loss: 1.6521915197372437, 3.239386558532715, 4.891578197479248
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602909564971924 3.112055778503418 4.772346496582031
Loss :  1.6485358476638794 2.8022286891937256 4.4507646560668945
Loss :  1.6655004024505615 2.9926490783691406 4.658149719238281
Loss :  1.663813591003418 2.8117268085479736 4.4755401611328125
Loss :  1.6667094230651855 3.1713309288024902 4.838040351867676
Loss :  1.6581389904022217 3.1570615768432617 4.8152008056640625
Loss :  1.6451431512832642 3.5230720043182373 5.168215274810791
Loss :  1.6573413610458374 2.7957258224487305 4.453067302703857
Loss :  1.6301425695419312 3.3139190673828125 4.944061756134033
Loss :  1.681278944015503 3.1708664894104004 4.852145195007324
Loss :  1.6458637714385986 2.57112717628479 4.216990947723389
Loss :  1.664161205291748 2.952698230743408 4.616859436035156
Loss :  1.6826868057250977 3.3151299953460693 4.997817039489746
Loss :  1.6674529314041138 2.858158826828003 4.525611877441406
Loss :  1.6706719398498535 3.4935343265533447 5.164206504821777
Loss :  1.6349798440933228 2.6587209701538086 4.293700695037842
Loss :  1.6846555471420288 3.2364108562469482 4.9210662841796875
Loss :  1.68220055103302 3.4230244159698486 5.105225086212158
Loss :  1.6956325769424438 3.0917155742645264 4.78734827041626
Loss :  1.670914888381958 2.697413921356201 4.368329048156738
  batch 60 loss: 1.670914888381958, 2.697413921356201, 4.368329048156738
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726160049438477 3.5115010738372803 5.184117317199707
Loss :  1.6693841218948364 2.880786657333374 4.5501708984375
Loss :  1.6774187088012695 3.029453992843628 4.706872940063477
Loss :  1.658041000366211 2.865784168243408 4.523825168609619
Loss :  1.6550519466400146 2.8669233322143555 4.521975517272949
Loss :  5.49385929107666 4.333498001098633 9.827357292175293
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.382843971252441 4.389503002166748 9.772346496582031
Loss :  5.4921746253967285 4.214960098266602 9.707134246826172
Loss :  4.572293758392334 4.267129898071289 8.839424133300781
Total LOSS train 4.672313231688279 valid 9.53656554222107
CE LOSS train 1.6640851662709162 valid 1.1430734395980835
Contrastive LOSS train 3.0082280195676363 valid 1.0667824745178223
EPOCH 234:
Loss :  1.6504520177841187 2.6732144355773926 4.323666572570801
Loss :  1.6662696599960327 3.1391286849975586 4.805398464202881
Loss :  1.6521239280700684 2.8143904209136963 4.466514587402344
Loss :  1.6552752256393433 2.9889776706695557 4.644252777099609
Loss :  1.6799269914627075 3.5751876831054688 5.255114555358887
Loss :  1.6629058122634888 2.8021528720855713 4.46505880355835
Loss :  1.6610487699508667 3.2601685523986816 4.921217441558838
Loss :  1.6499593257904053 2.4873478412628174 4.137307167053223
Loss :  1.6546118259429932 2.3458914756774902 4.0005035400390625
Loss :  1.6090553998947144 2.6449732780456543 4.254028797149658
Loss :  1.6686931848526 3.3152318000793457 4.983924865722656
Loss :  1.7292206287384033 2.948854684829712 4.678075313568115
Loss :  1.6762728691101074 3.238691568374634 4.91496467590332
Loss :  1.6644035577774048 3.3909544944763184 5.055357933044434
Loss :  1.6425999402999878 2.9787118434906006 4.621311664581299
Loss :  1.6521565914154053 3.1809983253479004 4.833154678344727
Loss :  1.6611567735671997 2.775012731552124 4.436169624328613
Loss :  1.659554123878479 3.1472744941711426 4.806828498840332
Loss :  1.6675344705581665 2.856938362121582 4.524472713470459
Loss :  1.6240392923355103 2.950684070587158 4.574723243713379
  batch 20 loss: 1.6240392923355103, 2.950684070587158, 4.574723243713379
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.659303069114685 3.1563973426818848 4.815700531005859
Loss :  1.6750409603118896 2.5546648502349854 4.229705810546875
Loss :  1.6463946104049683 2.958219051361084 4.604613780975342
Loss :  1.6874080896377563 3.0021636486053467 4.689571857452393
Loss :  1.6848665475845337 3.597386360168457 5.282252788543701
Loss :  1.648474097251892 3.1370646953582764 4.785538673400879
Loss :  1.697372317314148 3.067652940750122 4.7650251388549805
Loss :  1.6399048566818237 2.9347984790802 4.574703216552734
Loss :  1.6880748271942139 2.714268684387207 4.40234375
Loss :  1.641438603401184 3.003598928451538 4.645037651062012
Loss :  1.7226916551589966 3.2337992191314697 4.956490993499756
Loss :  1.6690822839736938 3.350484609603882 5.019567012786865
Loss :  1.6544270515441895 3.205941915512085 4.860368728637695
Loss :  1.6568490266799927 2.9258644580841064 4.582713603973389
Loss :  1.6968097686767578 3.4248361587524414 5.121645927429199
Loss :  1.6888660192489624 2.7088277339935303 4.397693634033203
Loss :  1.6665222644805908 2.9369966983795166 4.603518962860107
Loss :  1.6343125104904175 2.928048849105835 4.562361240386963
Loss :  1.659690022468567 3.0810887813568115 4.740778923034668
Loss :  1.6521881818771362 3.430788516998291 5.082976818084717
  batch 40 loss: 1.6521881818771362, 3.430788516998291, 5.082976818084717
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.660294532775879 3.479177951812744 5.139472484588623
Loss :  1.648539662361145 2.7239978313446045 4.372537612915039
Loss :  1.6654939651489258 3.06257963180542 4.728073596954346
Loss :  1.6638175249099731 3.164524555206299 4.828341960906982
Loss :  1.6667176485061646 2.5428695678710938 4.209587097167969
Loss :  1.6581424474716187 3.041335105895996 4.699477672576904
Loss :  1.6451445817947388 3.3770220279693604 5.022166728973389
Loss :  1.6573511362075806 3.259124279022217 4.916475296020508
Loss :  1.6301429271697998 3.145040988922119 4.77518367767334
Loss :  1.6812853813171387 3.178326368331909 4.859611511230469
Loss :  1.6458566188812256 2.557993173599243 4.203849792480469
Loss :  1.6641539335250854 3.2102205753326416 4.8743743896484375
Loss :  1.6826825141906738 3.3099982738494873 4.992680549621582
Loss :  1.6674526929855347 3.0232694149017334 4.6907219886779785
Loss :  1.6706790924072266 3.2765884399414062 4.947267532348633
Loss :  1.634979009628296 2.6203560829162598 4.255334854125977
Loss :  1.684655785560608 3.104543447494507 4.789199352264404
Loss :  1.6822088956832886 3.363435983657837 5.045644760131836
Loss :  1.695630431175232 3.2911460399627686 4.986776351928711
Loss :  1.6709083318710327 2.7516887187957764 4.4225969314575195
  batch 60 loss: 1.6709083318710327, 2.7516887187957764, 4.4225969314575195
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726188659667969 3.281385660171509 4.954004287719727
Loss :  1.6693915128707886 2.641730785369873 4.311122417449951
Loss :  1.677433967590332 3.4674339294433594 5.144867897033691
Loss :  1.658033013343811 3.0013022422790527 4.659335136413574
Loss :  1.6550613641738892 3.4897868633270264 5.144848346710205
Loss :  5.493217945098877 4.4675211906433105 9.960739135742188
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.381598472595215 4.38334321975708 9.764942169189453
Loss :  5.491447925567627 4.295389175415039 9.786836624145508
Loss :  4.570815563201904 4.153573989868164 8.724390029907227
Total LOSS train 4.713818572117732 valid 9.559226989746094
CE LOSS train 1.664086969082172 valid 1.142703890800476
Contrastive LOSS train 3.0497316177074727 valid 1.038393497467041
EPOCH 235:
Loss :  1.6504545211791992 2.7370169162750244 4.3874711990356445
Loss :  1.6662744283676147 3.229297161102295 4.895571708679199
Loss :  1.6521250009536743 3.3019731044769287 4.954098224639893
Loss :  1.6552879810333252 2.77516770362854 4.430455684661865
Loss :  1.679944396018982 3.198390007019043 4.8783345222473145
Loss :  1.6629133224487305 2.7609212398529053 4.423834800720215
Loss :  1.661050796508789 3.2176241874694824 4.8786749839782715
Loss :  1.6499576568603516 2.5810656547546387 4.23102331161499
Loss :  1.6546088457107544 2.6701765060424805 4.324785232543945
Loss :  1.609057068824768 2.4777867794036865 4.086843967437744
Loss :  1.6686917543411255 3.526226043701172 5.194917678833008
Loss :  1.7292207479476929 3.0248332023620605 4.754054069519043
Loss :  1.6762645244598389 3.314110040664673 4.990374565124512
Loss :  1.6644036769866943 3.330709934234619 4.995113372802734
Loss :  1.6425901651382446 3.0643320083618164 4.7069220542907715
Loss :  1.6521586179733276 3.2735681533813477 4.925726890563965
Loss :  1.661160945892334 2.962247133255005 4.623408317565918
Loss :  1.6595466136932373 2.87263822555542 4.532184600830078
Loss :  1.6675355434417725 2.740389823913574 4.407925605773926
Loss :  1.6240360736846924 3.049039602279663 4.6730756759643555
  batch 20 loss: 1.6240360736846924, 3.049039602279663, 4.6730756759643555
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6593017578125 2.9433276653289795 4.602629661560059
Loss :  1.6750354766845703 2.46162486076355 4.136660575866699
Loss :  1.6463871002197266 2.9499473571777344 4.596334457397461
Loss :  1.687403917312622 3.1348509788513184 4.8222551345825195
Loss :  1.6848657131195068 3.6253585815429688 5.310224533081055
Loss :  1.6484653949737549 3.181940793991089 4.830406188964844
Loss :  1.6973718404769897 3.1015841960906982 4.798955917358398
Loss :  1.639899730682373 2.9433164596557617 4.583216190338135
Loss :  1.6880736351013184 2.6845335960388184 4.372607231140137
Loss :  1.6414324045181274 2.8471407890319824 4.48857307434082
Loss :  1.722687840461731 3.407040596008301 5.129728317260742
Loss :  1.6690858602523804 3.1425602436065674 4.811645984649658
Loss :  1.6544215679168701 3.12665057182312 4.78107213973999
Loss :  1.6568348407745361 2.927645683288574 4.584480285644531
Loss :  1.696810007095337 3.3451647758483887 5.041975021362305
Loss :  1.6888654232025146 2.597013235092163 4.285878658294678
Loss :  1.6665228605270386 3.1833739280700684 4.8498969078063965
Loss :  1.6343069076538086 2.829102039337158 4.463408946990967
Loss :  1.6596876382827759 2.778486490249634 4.438174247741699
Loss :  1.6521854400634766 3.082782030105591 4.734967231750488
  batch 40 loss: 1.6521854400634766, 3.082782030105591, 4.734967231750488
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602905988693237 3.547657012939453 5.207947731018066
Loss :  1.6485400199890137 2.598919630050659 4.247459411621094
Loss :  1.665494680404663 3.0406579971313477 4.70615291595459
Loss :  1.6638150215148926 3.05531644821167 4.7191314697265625
Loss :  1.6667085886001587 3.223564624786377 4.890273094177246
Loss :  1.6581348180770874 2.9384193420410156 4.596554279327393
Loss :  1.6451367139816284 3.2125747203826904 4.857711315155029
Loss :  1.6573392152786255 2.9262101650238037 4.583549499511719
Loss :  1.6301404237747192 3.1993720531463623 4.829512596130371
Loss :  1.681268572807312 3.3296823501586914 5.010951042175293
Loss :  1.6458522081375122 2.7618215084075928 4.4076738357543945
Loss :  1.6641457080841064 2.88356614112854 4.5477118492126465
Loss :  1.682680606842041 3.410684108734131 5.093364715576172
Loss :  1.667445182800293 3.017751932144165 4.685196876525879
Loss :  1.670669436454773 3.8763961791992188 5.547065734863281
Loss :  1.6349645853042603 2.8697516918182373 4.504716396331787
Loss :  1.6846468448638916 3.4881298542022705 5.172776699066162
Loss :  1.6822057962417603 3.2670531272888184 4.949258804321289
Loss :  1.695630669593811 2.9925765991210938 4.688207149505615
Loss :  1.6709022521972656 2.739865779876709 4.410768032073975
  batch 60 loss: 1.6709022521972656, 2.739865779876709, 4.410768032073975
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726164817810059 3.556708335876465 5.229324817657471
Loss :  1.6693809032440186 2.8289506435394287 4.498331546783447
Loss :  1.6774262189865112 3.326878547668457 5.004304885864258
Loss :  1.6580300331115723 3.0845766067504883 4.7426066398620605
Loss :  1.655058741569519 3.255958080291748 4.911016941070557
Loss :  5.490951061248779 4.449305534362793 9.940256118774414
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.379599094390869 4.402573108673096 9.782172203063965
Loss :  5.489659786224365 4.2698774337768555 9.759536743164062
Loss :  4.568960189819336 4.313958168029785 8.882918357849121
Total LOSS train 4.7230685454148515 valid 9.59122085571289
CE LOSS train 1.6640838824785673 valid 1.142240047454834
Contrastive LOSS train 3.0589846427624043 valid 1.0784895420074463
EPOCH 236:
Loss :  1.6504402160644531 2.7804243564605713 4.430864334106445
Loss :  1.6662719249725342 3.008631944656372 4.674903869628906
Loss :  1.652121663093567 2.980431079864502 4.632552623748779
Loss :  1.6552681922912598 2.9222116470336914 4.577479839324951
Loss :  1.6799311637878418 3.2028069496154785 4.88273811340332
Loss :  1.6628965139389038 2.957676887512207 4.6205735206604
Loss :  1.6610435247421265 3.285881519317627 4.946925163269043
Loss :  1.6499567031860352 2.4856715202331543 4.1356282234191895
Loss :  1.6545993089675903 2.370009183883667 4.024608612060547
Loss :  1.6090552806854248 2.7038748264312744 4.312930107116699
Loss :  1.6686869859695435 3.1205601692199707 4.789247035980225
Loss :  1.7292131185531616 2.8403804302215576 4.56959342956543
Loss :  1.6762784719467163 3.0556252002716064 4.731903553009033
Loss :  1.6644090414047241 3.5265953540802 5.191004276275635
Loss :  1.6425994634628296 3.0772452354431152 4.719844818115234
Loss :  1.652152419090271 3.3786723613739014 5.030824661254883
Loss :  1.6611518859863281 2.8365674018859863 4.4977192878723145
Loss :  1.659535527229309 2.918616771697998 4.578152179718018
Loss :  1.6675242185592651 2.944342613220215 4.6118669509887695
Loss :  1.6240357160568237 3.093233346939087 4.717268943786621
  batch 20 loss: 1.6240357160568237, 3.093233346939087, 4.717268943786621
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6593077182769775 2.969872236251831 4.629179954528809
Loss :  1.6750338077545166 2.852269411087036 4.527303218841553
Loss :  1.6463927030563354 3.506701707839966 5.153094291687012
Loss :  1.6874064207077026 3.050886392593384 4.738292694091797
Loss :  1.6848684549331665 3.6253535747528076 5.310222148895264
Loss :  1.6484706401824951 3.0528151988983154 4.7012858390808105
Loss :  1.6973657608032227 2.9333062171936035 4.630671977996826
Loss :  1.6398974657058716 2.46928334236145 4.109180927276611
Loss :  1.6880650520324707 2.8142917156219482 4.50235652923584
Loss :  1.6414350271224976 3.058854579925537 4.700289726257324
Loss :  1.7226829528808594 3.4011573791503906 5.12384033203125
Loss :  1.6690781116485596 3.0176479816436768 4.686726093292236
Loss :  1.6544216871261597 2.6509482860565186 4.305369853973389
Loss :  1.6568444967269897 2.836240530014038 4.493084907531738
Loss :  1.6968111991882324 3.2943148612976074 4.99112606048584
Loss :  1.6888610124588013 3.061209201812744 4.750070095062256
Loss :  1.6665197610855103 3.320242404937744 4.986762046813965
Loss :  1.634298324584961 2.904147148132324 4.538445472717285
Loss :  1.6596863269805908 3.1448590755462646 4.8045454025268555
Loss :  1.652182936668396 3.309705972671509 4.961888790130615
  batch 40 loss: 1.652182936668396, 3.309705972671509, 4.961888790130615
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602892875671387 3.2552757263183594 4.915565013885498
Loss :  1.6485368013381958 2.968532085418701 4.617068767547607
Loss :  1.6654961109161377 3.2407257556915283 4.906221866607666
Loss :  1.6638176441192627 2.9699742794036865 4.633791923522949
Loss :  1.6667101383209229 2.6136879920959473 4.280398368835449
Loss :  1.658131718635559 2.8678274154663086 4.525959014892578
Loss :  1.6451345682144165 3.410562753677368 5.055697441101074
Loss :  1.6573375463485718 2.765141010284424 4.422478675842285
Loss :  1.6301378011703491 3.4925947189331055 5.122732639312744
Loss :  1.681275486946106 3.4887804985046387 5.170055866241455
Loss :  1.6458542346954346 2.668459177017212 4.3143134117126465
Loss :  1.6641483306884766 3.1071672439575195 4.771315574645996
Loss :  1.6826797723770142 3.3966054916381836 5.079285144805908
Loss :  1.6674542427062988 3.1061553955078125 4.773609638214111
Loss :  1.6706743240356445 3.3765788078308105 5.047253131866455
Loss :  1.6349756717681885 2.6056275367736816 4.240603446960449
Loss :  1.6846462488174438 3.269010066986084 4.953656196594238
Loss :  1.682208776473999 3.4291322231292725 5.1113409996032715
Loss :  1.6956276893615723 3.0809059143066406 4.776533603668213
Loss :  1.6709115505218506 2.5630621910095215 4.233973503112793
  batch 60 loss: 1.6709115505218506, 2.5630621910095215, 4.233973503112793
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726083755493164 3.554546594619751 5.227154731750488
Loss :  1.6693769693374634 2.527427911758423 4.196805000305176
Loss :  1.6774214506149292 2.956604242324829 4.634025573730469
Loss :  1.658033013343811 2.8783814907073975 4.536414623260498
Loss :  1.6550484895706177 2.8693361282348633 4.524384498596191
Loss :  5.492644309997559 4.429075717926025 9.921720504760742
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.381280899047852 4.365782737731934 9.747063636779785
Loss :  5.491347312927246 4.225257396697998 9.716604232788086
Loss :  4.570540428161621 4.261100769042969 8.83164119720459
Total LOSS train 4.6983231471135065 valid 9.5542573928833
CE LOSS train 1.6640821145131037 valid 1.1426351070404053
Contrastive LOSS train 3.034241056442261 valid 1.0652751922607422
EPOCH 237:
Loss :  1.650443196296692 2.5659849643707275 4.216428279876709
Loss :  1.6662731170654297 3.0090155601501465 4.675288677215576
Loss :  1.6521234512329102 3.0528061389923096 4.704929351806641
Loss :  1.6552764177322388 3.0511133670806885 4.706389904022217
Loss :  1.679932951927185 3.240269899368286 4.920202732086182
Loss :  1.662906289100647 2.709263801574707 4.3721699714660645
Loss :  1.6610475778579712 3.2585794925689697 4.9196271896362305
Loss :  1.6499521732330322 2.8095390796661377 4.45949125289917
Loss :  1.6545981168746948 2.3341407775878906 3.988739013671875
Loss :  1.6090465784072876 2.6463992595672607 4.255445957183838
Loss :  1.6686851978302002 3.2644543647766113 4.933139801025391
Loss :  1.7292197942733765 3.0671544075012207 4.796374320983887
Loss :  1.6762696504592896 3.3034794330596924 4.9797492027282715
Loss :  1.664400339126587 3.529696464538574 5.194096565246582
Loss :  1.6426033973693848 3.1163899898529053 4.758993148803711
Loss :  1.6521527767181396 3.1705400943756104 4.82269287109375
Loss :  1.661156177520752 2.7246649265289307 4.385821342468262
Loss :  1.6595392227172852 2.84618878364563 4.505727767944336
Loss :  1.6675310134887695 2.905421495437622 4.5729522705078125
Loss :  1.6240355968475342 2.9132652282714844 4.537301063537598
  batch 20 loss: 1.6240355968475342, 2.9132652282714844, 4.537301063537598
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.659305453300476 3.0579049587249756 4.717210292816162
Loss :  1.675032615661621 2.8302886486053467 4.505321502685547
Loss :  1.6463817358016968 3.4289259910583496 5.075307846069336
Loss :  1.6874092817306519 2.9001481533050537 4.587557315826416
Loss :  1.6848621368408203 3.517678737640381 5.202540874481201
Loss :  1.6484631299972534 3.392019510269165 5.040482521057129
Loss :  1.6973598003387451 2.9357998371124268 4.633159637451172
Loss :  1.639901876449585 2.7560875415802 4.395989418029785
Loss :  1.6880675554275513 2.7231101989746094 4.411177635192871
Loss :  1.6414332389831543 2.860276460647583 4.501709938049316
Loss :  1.7226773500442505 3.242743730545044 4.965421199798584
Loss :  1.6690863370895386 3.36075496673584 5.029841423034668
Loss :  1.6544233560562134 3.0535402297973633 4.707963466644287
Loss :  1.6568470001220703 2.588855504989624 4.245702743530273
Loss :  1.696799397468567 3.3364593982696533 5.03325891494751
Loss :  1.688860535621643 2.808953285217285 4.497813701629639
Loss :  1.666516900062561 2.984424114227295 4.650940895080566
Loss :  1.6343023777008057 2.454251289367676 4.088553428649902
Loss :  1.6596884727478027 3.0587565898895264 4.71844482421875
Loss :  1.6521883010864258 3.030999183654785 4.683187484741211
  batch 40 loss: 1.6521883010864258, 3.030999183654785, 4.683187484741211
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602863073349 3.103604793548584 4.763891220092773
Loss :  1.6485309600830078 2.7793128490448 4.427844047546387
Loss :  1.6654950380325317 3.1761889457702637 4.841683864593506
Loss :  1.6638166904449463 3.210916757583618 4.8747334480285645
Loss :  1.6667072772979736 3.14682674407959 4.813533782958984
Loss :  1.658130168914795 2.90998911857605 4.568119049072266
Loss :  1.6451423168182373 3.4661788940429688 5.111321449279785
Loss :  1.657333493232727 2.7648675441741943 4.422201156616211
Loss :  1.630131721496582 3.2950549125671387 4.925186634063721
Loss :  1.681267261505127 3.067984104156494 4.749251365661621
Loss :  1.6458518505096436 2.7385213375091553 4.384373188018799
Loss :  1.664151906967163 2.978882312774658 4.643033981323242
Loss :  1.6826775074005127 3.4089906215667725 5.091668128967285
Loss :  1.6674480438232422 2.841287851333618 4.508735656738281
Loss :  1.6706781387329102 3.4137558937072754 5.0844340324401855
Loss :  1.6349682807922363 2.7704017162323 4.405369758605957
Loss :  1.6846528053283691 3.1949610710144043 4.879613876342773
Loss :  1.6822059154510498 3.31781268119812 5.00001859664917
Loss :  1.6956188678741455 3.0746493339538574 4.770268440246582
Loss :  1.6709094047546387 2.7884154319763184 4.459324836730957
  batch 60 loss: 1.6709094047546387, 2.7884154319763184, 4.459324836730957
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726092100143433 3.5093019008636475 5.181910991668701
Loss :  1.6693838834762573 2.6378531455993652 4.307237148284912
Loss :  1.6774263381958008 3.0994670391082764 4.776893615722656
Loss :  1.6580281257629395 2.898078441619873 4.5561065673828125
Loss :  1.6550626754760742 2.9775936603546143 4.632656097412109
Loss :  5.492348670959473 4.417945384979248 9.910293579101562
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.381030559539795 4.346284866333008 9.727315902709961
Loss :  5.4907073974609375 4.208942890167236 9.699649810791016
Loss :  4.569396495819092 4.247054100036621 8.816450119018555
Total LOSS train 4.685793179732102 valid 9.538427352905273
CE LOSS train 1.6640817550512461 valid 1.142349123954773
Contrastive LOSS train 3.021711430182824 valid 1.0617635250091553
EPOCH 238:
Loss :  1.6504414081573486 2.625279188156128 4.275720596313477
Loss :  1.6662675142288208 3.3473503589630127 5.013617992401123
Loss :  1.6521265506744385 3.2588229179382324 4.91094970703125
Loss :  1.6552852392196655 3.042943000793457 4.698228359222412
Loss :  1.6799323558807373 3.072882652282715 4.752815246582031
Loss :  1.6629087924957275 2.607532501220703 4.270441055297852
Loss :  1.6610406637191772 3.3380892276763916 4.999129772186279
Loss :  1.649953007698059 2.4885993003845215 4.138552188873291
Loss :  1.6545953750610352 2.2594704627990723 3.9140658378601074
Loss :  1.6090474128723145 2.698042869567871 4.3070902824401855
Loss :  1.6686841249465942 3.226623296737671 4.895307540893555
Loss :  1.7292124032974243 2.9504776000976562 4.679689884185791
Loss :  1.6762441396713257 3.281075954437256 4.957320213317871
Loss :  1.664400339126587 3.3168694972991943 4.981269836425781
Loss :  1.642593502998352 2.9680681228637695 4.610661506652832
Loss :  1.6521614789962769 3.022346258163452 4.6745076179504395
Loss :  1.6611562967300415 2.704810380935669 4.365966796875
Loss :  1.6595460176467896 2.727553129196167 4.387099266052246
Loss :  1.6675430536270142 2.913407325744629 4.5809502601623535
Loss :  1.6240252256393433 3.1555020809173584 4.779527187347412
  batch 20 loss: 1.6240252256393433, 3.1555020809173584, 4.779527187347412
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592934131622314 2.848100185394287 4.507393836975098
Loss :  1.6750309467315674 2.8248331546783447 4.499864101409912
Loss :  1.646381139755249 3.3590927124023438 5.005474090576172
Loss :  1.6874110698699951 3.1895785331726074 4.876989364624023
Loss :  1.6848645210266113 3.2572638988494873 4.9421281814575195
Loss :  1.6484556198120117 3.2188808917999268 4.867336273193359
Loss :  1.6973636150360107 2.7834420204162598 4.480805397033691
Loss :  1.6399005651474 2.9883768558502197 4.62827730178833
Loss :  1.688066005706787 2.5591893196105957 4.247255325317383
Loss :  1.6414331197738647 3.0223429203033447 4.66377592086792
Loss :  1.722673773765564 3.2353339195251465 4.9580078125
Loss :  1.6690983772277832 3.377027750015259 5.046126365661621
Loss :  1.6544232368469238 3.1233603954315186 4.777783393859863
Loss :  1.6568431854248047 2.8338491916656494 4.490692138671875
Loss :  1.696812391281128 3.767508029937744 5.464320182800293
Loss :  1.6888625621795654 2.839268684387207 4.528131484985352
Loss :  1.6665139198303223 2.8674910068511963 4.534005165100098
Loss :  1.6343214511871338 2.9614081382751465 4.595729827880859
Loss :  1.6596876382827759 2.8312828540802 4.490970611572266
Loss :  1.6521822214126587 3.115509510040283 4.767691612243652
  batch 40 loss: 1.6521822214126587, 3.115509510040283, 4.767691612243652
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602928638458252 3.2180674076080322 4.878360271453857
Loss :  1.6485393047332764 2.662327766418457 4.3108673095703125
Loss :  1.6654976606369019 3.068786144256592 4.734283924102783
Loss :  1.6638063192367554 2.884188175201416 4.547994613647461
Loss :  1.6667126417160034 2.7804152965545654 4.447127819061279
Loss :  1.6581299304962158 3.3775675296783447 5.0356974601745605
Loss :  1.645147681236267 3.2988240718841553 4.943971633911133
Loss :  1.6573419570922852 2.7747912406921387 4.432133197784424
Loss :  1.6301344633102417 3.1666831970214844 4.796817779541016
Loss :  1.6812859773635864 2.9378445148468018 4.619130611419678
Loss :  1.6458499431610107 2.8101072311401367 4.455957412719727
Loss :  1.664158582687378 2.9278104305267334 4.591969013214111
Loss :  1.6826775074005127 3.577129364013672 5.2598066329956055
Loss :  1.6674569845199585 2.900646209716797 4.568103313446045
Loss :  1.6706781387329102 3.351717233657837 5.022395133972168
Loss :  1.6349689960479736 2.657810926437378 4.292779922485352
Loss :  1.6846461296081543 3.355438709259033 5.0400848388671875
Loss :  1.6822127103805542 3.317843198776245 5.00005578994751
Loss :  1.6956181526184082 2.9833645820617676 4.678982734680176
Loss :  1.6709177494049072 2.9441540241241455 4.615071773529053
  batch 60 loss: 1.6709177494049072, 2.9441540241241455, 4.615071773529053
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.672611117362976 3.4670333862304688 5.139644622802734
Loss :  1.6693845987319946 2.8973805904388428 4.566765308380127
Loss :  1.6774301528930664 3.402684450149536 5.080114364624023
Loss :  1.6580299139022827 2.705850839614868 4.363880634307861
Loss :  1.655062198638916 2.939406394958496 4.594468593597412
Loss :  5.491701126098633 4.419026851654053 9.910728454589844
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.380861759185791 4.27009916305542 9.650960922241211
Loss :  5.490581512451172 4.272091865539551 9.762673377990723
Loss :  4.569140434265137 4.263330936431885 8.83247184753418
Total LOSS train 4.6858789884127106 valid 9.53920865058899
CE LOSS train 1.664082697721628 valid 1.1422851085662842
Contrastive LOSS train 3.0217962925250714 valid 1.0658327341079712
EPOCH 239:
Loss :  1.6504403352737427 2.7969932556152344 4.4474334716796875
Loss :  1.666273593902588 3.2116458415985107 4.8779191970825195
Loss :  1.652127742767334 3.1101114749908447 4.762239456176758
Loss :  1.6552869081497192 2.8721132278442383 4.527400016784668
Loss :  1.6799290180206299 3.582465171813965 5.262393951416016
Loss :  1.6629085540771484 2.57666015625 4.239568710327148
Loss :  1.661037564277649 3.3708152770996094 5.031852722167969
Loss :  1.6499513387680054 2.5202527046203613 4.170204162597656
Loss :  1.6545965671539307 2.5144412517547607 4.169037818908691
Loss :  1.6090492010116577 2.896904230117798 4.505953311920166
Loss :  1.6686828136444092 3.3471760749816895 5.0158586502075195
Loss :  1.7292168140411377 3.105806589126587 4.835023403167725
Loss :  1.6762334108352661 3.0542244911193848 4.730457782745361
Loss :  1.6644068956375122 3.181652307510376 4.846059322357178
Loss :  1.642594814300537 3.007584810256958 4.650179862976074
Loss :  1.6521623134613037 3.050039052963257 4.7022013664245605
Loss :  1.6611562967300415 2.893151044845581 4.554307460784912
Loss :  1.6595450639724731 3.0562546253204346 4.715799808502197
Loss :  1.6675467491149902 2.9467201232910156 4.614266872406006
Loss :  1.6240262985229492 2.911219358444214 4.535245895385742
  batch 20 loss: 1.6240262985229492, 2.911219358444214, 4.535245895385742
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.659290075302124 3.028805732727051 4.688096046447754
Loss :  1.6750304698944092 2.5971415042877197 4.272171974182129
Loss :  1.6463813781738281 3.446129322052002 5.09251070022583
Loss :  1.6874102354049683 2.9200217723846436 4.607431888580322
Loss :  1.6848642826080322 3.266465663909912 4.951330184936523
Loss :  1.6484557390213013 3.1755330562591553 4.823988914489746
Loss :  1.697366714477539 2.913037061691284 4.610404014587402
Loss :  1.6398985385894775 3.181983709335327 4.821882247924805
Loss :  1.6880689859390259 2.8506600856781006 4.538729190826416
Loss :  1.6414260864257812 2.77280330657959 4.414229393005371
Loss :  1.7226743698120117 3.3396987915039062 5.062373161315918
Loss :  1.6690906286239624 3.4700369834899902 5.139127731323242
Loss :  1.6544219255447388 2.925262451171875 4.579684257507324
Loss :  1.656842589378357 3.0033531188964844 4.660195827484131
Loss :  1.6968024969100952 3.2172868251800537 4.914089202880859
Loss :  1.6888583898544312 2.6608738899230957 4.349732398986816
Loss :  1.666513442993164 2.9679718017578125 4.634485244750977
Loss :  1.6343164443969727 2.699991464614868 4.334307670593262
Loss :  1.6596858501434326 2.816215991973877 4.4759016036987305
Loss :  1.6521883010864258 3.351027011871338 5.003215312957764
  batch 40 loss: 1.6521883010864258, 3.351027011871338, 5.003215312957764
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602858304977417 3.1689529418945312 4.8292388916015625
Loss :  1.648526668548584 3.094759702682495 4.7432861328125
Loss :  1.6654937267303467 3.176621913909912 4.84211540222168
Loss :  1.663804054260254 2.9870946407318115 4.6508989334106445
Loss :  1.6667046546936035 2.779728412628174 4.446433067321777
Loss :  1.658126950263977 3.305861711502075 4.963988780975342
Loss :  1.6451536417007446 3.095747947692871 4.740901470184326
Loss :  1.6573266983032227 2.9931578636169434 4.650484561920166
Loss :  1.6301350593566895 3.4379565715789795 5.06809139251709
Loss :  1.681277871131897 3.2482683658599854 4.929546356201172
Loss :  1.6458579301834106 2.6812856197357178 4.327143669128418
Loss :  1.6641567945480347 2.962498426437378 4.626655101776123
Loss :  1.682674765586853 3.4077024459838867 5.090377330780029
Loss :  1.6674540042877197 2.938349485397339 4.605803489685059
Loss :  1.670682430267334 3.281517267227173 4.952199935913086
Loss :  1.6349762678146362 2.780143976211548 4.4151201248168945
Loss :  1.6846492290496826 3.3262128829956055 5.010862350463867
Loss :  1.6822057962417603 3.2541046142578125 4.936310291290283
Loss :  1.695622444152832 3.3274455070495605 5.023067951202393
Loss :  1.670919418334961 2.6300206184387207 4.300940036773682
  batch 60 loss: 1.670919418334961, 2.6300206184387207, 4.300940036773682
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.672601580619812 3.6660003662109375 5.338602066040039
Loss :  1.6693698167800903 2.5677506923675537 4.237120628356934
Loss :  1.677420735359192 3.4276249408721924 5.105045795440674
Loss :  1.6580321788787842 2.878390073776245 4.536422252655029
Loss :  1.6550637483596802 2.8018507957458496 4.45691442489624
Loss :  5.492772102355957 4.404023170471191 9.896795272827148
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.382116794586182 4.3579816818237305 9.74009895324707
Loss :  5.491375923156738 4.326960563659668 9.818336486816406
Loss :  4.569873809814453 4.148116588592529 8.71799087524414
Total LOSS train 4.707613240755522 valid 9.543305397033691
CE LOSS train 1.6640812543722299 valid 1.1424684524536133
Contrastive LOSS train 3.043531975379357 valid 1.0370291471481323
EPOCH 240:
Loss :  1.6504355669021606 2.923229932785034 4.573665618896484
Loss :  1.6662652492523193 3.2471420764923096 4.913407325744629
Loss :  1.6521167755126953 3.1725993156433105 4.824716091156006
Loss :  1.6552742719650269 3.111070156097412 4.7663445472717285
Loss :  1.679919719696045 3.2366836071014404 4.916603088378906
Loss :  1.6629000902175903 2.8011906147003174 4.464090824127197
Loss :  1.6610389947891235 3.4122681617736816 5.073307037353516
Loss :  1.6499474048614502 2.7392690181732178 4.389216423034668
Loss :  1.6545970439910889 2.4228720664978027 4.0774688720703125
Loss :  1.6090519428253174 2.7003135681152344 4.309365272521973
Loss :  1.6686785221099854 3.089216947555542 4.757895469665527
Loss :  1.7291934490203857 2.886204957962036 4.615398406982422
Loss :  1.6761934757232666 2.947692394256592 4.6238861083984375
Loss :  1.6643998622894287 3.288315534591675 4.9527153968811035
Loss :  1.6426042318344116 3.175926446914673 4.818530559539795
Loss :  1.6521533727645874 3.1116788387298584 4.763832092285156
Loss :  1.6611430644989014 3.011497735977173 4.672640800476074
Loss :  1.6595367193222046 2.802762508392334 4.462299346923828
Loss :  1.6675349473953247 3.1380059719085693 4.805541038513184
Loss :  1.6240195035934448 3.164043426513672 4.788063049316406
  batch 20 loss: 1.6240195035934448, 3.164043426513672, 4.788063049316406
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592938899993896 3.0185179710388184 4.677811622619629
Loss :  1.6750223636627197 2.560999870300293 4.236021995544434
Loss :  1.6463884115219116 3.4497339725494385 5.0961222648620605
Loss :  1.6874094009399414 2.7617995738983154 4.449209213256836
Loss :  1.684862732887268 3.370724678039551 5.055587291717529
Loss :  1.6484686136245728 3.2031843662261963 4.851653099060059
Loss :  1.6973531246185303 3.0779428482055664 4.775296211242676
Loss :  1.6398981809616089 2.9984419345855713 4.638339996337891
Loss :  1.6880600452423096 2.784993886947632 4.473053932189941
Loss :  1.6414265632629395 2.8862388134002686 4.527665138244629
Loss :  1.7226824760437012 3.270045518875122 4.992728233337402
Loss :  1.6690737009048462 3.3123512268066406 4.981424808502197
Loss :  1.6544214487075806 2.8835036754608154 4.5379252433776855
Loss :  1.6568446159362793 2.8795368671417236 4.536381721496582
Loss :  1.6967780590057373 3.535691738128662 5.23246955871582
Loss :  1.6888507604599 2.73077130317688 4.41962194442749
Loss :  1.6665102243423462 2.977288246154785 4.643798351287842
Loss :  1.6343055963516235 2.96736478805542 4.601670265197754
Loss :  1.6596837043762207 2.7595508098602295 4.419234275817871
Loss :  1.652182698249817 3.151271343231201 4.8034539222717285
  batch 40 loss: 1.652182698249817, 3.151271343231201, 4.8034539222717285
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602855920791626 3.3682775497436523 5.028563022613525
Loss :  1.6485296487808228 2.6754860877990723 4.3240156173706055
Loss :  1.665487289428711 2.973430871963501 4.638917922973633
Loss :  1.6638057231903076 2.9796946048736572 4.643500328063965
Loss :  1.6667042970657349 2.7704572677612305 4.437161445617676
Loss :  1.6581223011016846 3.373525381088257 5.031647682189941
Loss :  1.6451404094696045 3.3154501914978027 4.960590362548828
Loss :  1.6573301553726196 2.9633333683013916 4.620663642883301
Loss :  1.6301316022872925 3.6359918117523193 5.266123294830322
Loss :  1.681272029876709 3.28559947013855 4.96687126159668
Loss :  1.6458638906478882 2.8704099655151367 4.5162739753723145
Loss :  1.664151906967163 2.8472683429718018 4.511420249938965
Loss :  1.6826775074005127 3.6067864894866943 5.289463996887207
Loss :  1.6674432754516602 2.8736634254455566 4.541106700897217
Loss :  1.670684576034546 3.4179465770721436 5.0886311531066895
Loss :  1.6349786520004272 2.622422218322754 4.257400989532471
Loss :  1.6846458911895752 3.009239912033081 4.693885803222656
Loss :  1.6822004318237305 3.195904016494751 4.878104209899902
Loss :  1.6956311464309692 3.165785551071167 4.861416816711426
Loss :  1.6709251403808594 2.835742712020874 4.5066680908203125
  batch 60 loss: 1.6709251403808594, 2.835742712020874, 4.5066680908203125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726067066192627 3.2900278568267822 4.962634563446045
Loss :  1.6693710088729858 2.6455800533294678 4.314950942993164
Loss :  1.6774228811264038 3.326697826385498 5.004120826721191
Loss :  1.6580365896224976 2.970292806625366 4.628329277038574
Loss :  1.655065894126892 3.2268729209899902 4.881938934326172
Loss :  5.4917073249816895 4.429579257965088 9.921286582946777
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.3811750411987305 4.395840167999268 9.777015686035156
Loss :  5.49068546295166 4.238607883453369 9.729293823242188
Loss :  4.56812858581543 4.205100059509277 8.773228645324707
Total LOSS train 4.713428578009972 valid 9.550206184387207
CE LOSS train 1.6640774671848004 valid 1.1420321464538574
Contrastive LOSS train 3.049351138335008 valid 1.0512750148773193
EPOCH 241:
Loss :  1.650442123413086 2.677093982696533 4.327536106109619
Loss :  1.666268229484558 3.361124277114868 5.027392387390137
Loss :  1.6521046161651611 3.0352842807769775 4.687388896942139
Loss :  1.6552715301513672 3.110684871673584 4.765956401824951
Loss :  1.6799310445785522 3.3473641872406006 5.027295112609863
Loss :  1.6628981828689575 2.5787556171417236 4.241653919219971
Loss :  1.6610475778579712 3.3987679481506348 5.059815406799316
Loss :  1.6499472856521606 2.4987399578094482 4.148687362670898
Loss :  1.6545988321304321 2.1425423622131348 3.7971410751342773
Loss :  1.6090443134307861 2.6282074451446533 4.2372517585754395
Loss :  1.6686714887619019 3.3022615909576416 4.970932960510254
Loss :  1.72921884059906 2.9508814811706543 4.680100440979004
Loss :  1.6761950254440308 3.0699048042297363 4.746099948883057
Loss :  1.6644045114517212 3.029649257659912 4.694053649902344
Loss :  1.6426116228103638 3.221552848815918 4.864164352416992
Loss :  1.6521517038345337 3.1694228649139404 4.821574687957764
Loss :  1.6611469984054565 2.7117295265197754 4.3728766441345215
Loss :  1.6595319509506226 2.8967809677124023 4.5563130378723145
Loss :  1.6675258874893188 2.8059298992156982 4.473455905914307
Loss :  1.624023199081421 2.8912317752838135 4.515254974365234
  batch 20 loss: 1.624023199081421, 2.8912317752838135, 4.515254974365234
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592965126037598 3.1237478256225586 4.783044338226318
Loss :  1.6750223636627197 2.940697193145752 4.615719795227051
Loss :  1.6463823318481445 3.181506395339966 4.827888488769531
Loss :  1.687410593032837 2.9672787189483643 4.654689311981201
Loss :  1.684862494468689 3.407320976257324 5.092183589935303
Loss :  1.6484577655792236 3.260542631149292 4.909000396728516
Loss :  1.6973546743392944 3.023610830307007 4.720965385437012
Loss :  1.6398956775665283 3.0543737411499023 4.694269180297852
Loss :  1.6880580186843872 2.907970905303955 4.596028804779053
Loss :  1.641419529914856 2.6853384971618652 4.326757907867432
Loss :  1.7226780652999878 3.4264702796936035 5.149148464202881
Loss :  1.6690752506256104 3.1068944931030273 4.775969505310059
Loss :  1.6544145345687866 3.1853458881378174 4.8397603034973145
Loss :  1.6568381786346436 2.7501227855682373 4.406960964202881
Loss :  1.696785569190979 3.4449431896209717 5.14172887802124
Loss :  1.688847303390503 2.733106851577759 4.421954154968262
Loss :  1.6665136814117432 2.910108804702759 4.576622486114502
Loss :  1.6343151330947876 2.762695789337158 4.397010803222656
Loss :  1.65967857837677 2.9829471111297607 4.64262580871582
Loss :  1.6521801948547363 3.007829189300537 4.660009384155273
  batch 40 loss: 1.6521801948547363, 3.007829189300537, 4.660009384155273
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602838039398193 3.356374502182007 5.016658306121826
Loss :  1.648531436920166 2.508343458175659 4.156874656677246
Loss :  1.6654903888702393 3.0423247814178467 4.707815170288086
Loss :  1.663801908493042 3.051485776901245 4.715287685394287
Loss :  1.6667027473449707 2.834122896194458 4.500825881958008
Loss :  1.6581171751022339 3.3478784561157227 5.005995750427246
Loss :  1.6451345682144165 3.630523204803467 5.275657653808594
Loss :  1.6573295593261719 3.053267240524292 4.710597038269043
Loss :  1.6301360130310059 3.2782866954803467 4.908422470092773
Loss :  1.6812794208526611 3.0530731678009033 4.7343525886535645
Loss :  1.645853042602539 2.952310085296631 4.59816312789917
Loss :  1.6641521453857422 2.884460687637329 4.548612594604492
Loss :  1.6826705932617188 3.1525845527648926 4.835255146026611
Loss :  1.6674472093582153 2.9052445888519287 4.572691917419434
Loss :  1.6706846952438354 3.458977460861206 5.129662036895752
Loss :  1.634973406791687 2.6271743774414062 4.262147903442383
Loss :  1.6846468448638916 3.25990629196167 4.944553375244141
Loss :  1.6822023391723633 3.125633478164673 4.807835578918457
Loss :  1.6956288814544678 3.3155977725982666 5.011226654052734
Loss :  1.6709221601486206 2.7421414852142334 4.4130635261535645
  batch 60 loss: 1.6709221601486206, 2.7421414852142334, 4.4130635261535645
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726117134094238 3.1924960613250732 4.865107536315918
Loss :  1.6693787574768066 2.875281572341919 4.544660568237305
Loss :  1.6774295568466187 3.0836563110351562 4.7610859870910645
Loss :  1.6580305099487305 2.7730185985565186 4.431049346923828
Loss :  1.65506911277771 3.3671560287475586 5.022225379943848
Loss :  5.491191864013672 4.352797508239746 9.843989372253418
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.380416393280029 4.421450614929199 9.80186653137207
Loss :  5.490262985229492 4.305294990539551 9.795557975769043
Loss :  4.567983627319336 4.23647928237915 8.804462432861328
Total LOSS train 4.688109397888184 valid 9.561469078063965
CE LOSS train 1.6640773754853468 valid 1.141995906829834
Contrastive LOSS train 3.024032024236826 valid 1.0591198205947876
EPOCH 242:
Loss :  1.650438904762268 3.0188684463500977 4.669307231903076
Loss :  1.6662689447402954 3.2052268981933594 4.871495723724365
Loss :  1.6521110534667969 3.006413221359253 4.658524513244629
Loss :  1.6552746295928955 2.8984999656677246 4.553774833679199
Loss :  1.6799209117889404 3.394134998321533 5.0740556716918945
Loss :  1.6628965139389038 2.721165418624878 4.384061813354492
Loss :  1.6610462665557861 3.510882616043091 5.171928882598877
Loss :  1.6499487161636353 2.6534810066223145 4.30342960357666
Loss :  1.654596209526062 2.6278328895568848 4.282429218292236
Loss :  1.6090482473373413 2.6375479698181152 4.246596336364746
Loss :  1.6686737537384033 3.4371583461761475 5.105832099914551
Loss :  1.7292168140411377 2.9498422145843506 4.679059028625488
Loss :  1.6762021780014038 3.1432766914367676 4.819478988647461
Loss :  1.66441011428833 3.393040895462036 5.057451248168945
Loss :  1.642599105834961 3.0942792892456055 4.736878395080566
Loss :  1.6521629095077515 3.1952641010284424 4.847426891326904
Loss :  1.6611511707305908 2.906919240951538 4.568070411682129
Loss :  1.6595419645309448 2.622565269470215 4.282107353210449
Loss :  1.6675406694412231 2.8593297004699707 4.526870250701904
Loss :  1.6240237951278687 3.1096622943878174 4.7336859703063965
  batch 20 loss: 1.6240237951278687, 3.1096622943878174, 4.7336859703063965
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592884063720703 3.155115842819214 4.814404487609863
Loss :  1.6750189065933228 2.704824209213257 4.379843235015869
Loss :  1.6463897228240967 2.985934019088745 4.632323741912842
Loss :  1.687406063079834 2.8408401012420654 4.52824592590332
Loss :  1.6848702430725098 3.3212382793426514 5.006108283996582
Loss :  1.648456335067749 2.9828269481658936 4.631283283233643
Loss :  1.6973518133163452 3.0571258068084717 4.754477500915527
Loss :  1.6399000883102417 3.1876158714294434 4.827516078948975
Loss :  1.6880600452423096 2.5832746028900146 4.271334648132324
Loss :  1.6414241790771484 3.014995574951172 4.65641975402832
Loss :  1.7226712703704834 3.213884115219116 4.9365553855896
Loss :  1.6690737009048462 3.332240581512451 5.001314163208008
Loss :  1.6544207334518433 3.0244741439819336 4.678894996643066
Loss :  1.6568442583084106 2.9179043769836426 4.574748516082764
Loss :  1.696791172027588 3.678683042526245 5.375473976135254
Loss :  1.688854694366455 2.853593587875366 4.542448043823242
Loss :  1.6665111780166626 3.3380866050720215 5.0045976638793945
Loss :  1.634312629699707 2.903716802597046 4.538029670715332
Loss :  1.6596862077713013 3.0164475440979004 4.676133632659912
Loss :  1.6521834135055542 3.1504600048065186 4.802643299102783
  batch 40 loss: 1.6521834135055542, 3.1504600048065186, 4.802643299102783
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.660287857055664 3.428950786590576 5.08923864364624
Loss :  1.6485329866409302 2.756455421447754 4.4049882888793945
Loss :  1.6654876470565796 3.0403635501861572 4.705851078033447
Loss :  1.6638115644454956 3.1343603134155273 4.7981719970703125
Loss :  1.6667017936706543 2.913471221923828 4.580173015594482
Loss :  1.6581186056137085 2.9359426498413086 4.594061374664307
Loss :  1.6451433897018433 3.215834856033325 4.860978126525879
Loss :  1.6573351621627808 2.957180976867676 4.614516258239746
Loss :  1.6301331520080566 3.3514697551727295 4.981602668762207
Loss :  1.6812810897827148 3.2407584190368652 4.92203950881958
Loss :  1.645854115486145 2.7648119926452637 4.410665988922119
Loss :  1.664150595664978 2.626035451889038 4.290185928344727
Loss :  1.6826738119125366 3.324437141418457 5.007111072540283
Loss :  1.6674528121948242 3.0110011100769043 4.6784539222717285
Loss :  1.670684814453125 3.595184564590454 5.265869140625
Loss :  1.6349684000015259 2.7499873638153076 4.384955883026123
Loss :  1.6846425533294678 3.2334930896759033 4.918135643005371
Loss :  1.6822081804275513 3.4297239780426025 5.111932277679443
Loss :  1.695619821548462 3.453601598739624 5.149221420288086
Loss :  1.6709158420562744 2.902169704437256 4.573085784912109
  batch 60 loss: 1.6709158420562744, 2.902169704437256, 4.573085784912109
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726055145263672 3.5301125049591064 5.2027177810668945
Loss :  1.6693730354309082 2.620598793029785 4.289971828460693
Loss :  1.6774303913116455 3.212010383605957 4.889440536499023
Loss :  1.658016562461853 2.782214641571045 4.4402313232421875
Loss :  1.655055046081543 2.6070494651794434 4.262104511260986
Loss :  5.490297317504883 4.38776159286499 9.878059387207031
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.3796305656433105 4.330573558807373 9.710204124450684
Loss :  5.489017963409424 4.240451335906982 9.729469299316406
Loss :  4.568153381347656 4.154913902282715 8.723067283630371
Total LOSS train 4.7173994577848 valid 9.510200023651123
CE LOSS train 1.6640780412233793 valid 1.142038345336914
Contrastive LOSS train 3.053321434901311 valid 1.0387284755706787
EPOCH 243:
Loss :  1.6504393815994263 2.710601806640625 4.361041069030762
Loss :  1.6662664413452148 3.2197976112365723 4.886064052581787
Loss :  1.6521196365356445 2.9658310413360596 4.617950439453125
Loss :  1.6552767753601074 2.941953420639038 4.597229957580566
Loss :  1.679921269416809 3.4619991779327393 5.141920566558838
Loss :  1.6628999710083008 2.7790141105651855 4.441914081573486
Loss :  1.6610376834869385 3.258863925933838 4.9199018478393555
Loss :  1.6499412059783936 2.880260944366455 4.5302019119262695
Loss :  1.6545886993408203 2.43858003616333 4.09316873550415
Loss :  1.6090381145477295 2.6050102710723877 4.214048385620117
Loss :  1.668672800064087 3.35762095451355 5.026293754577637
Loss :  1.729214072227478 2.87469220161438 4.603906154632568
Loss :  1.676229476928711 3.2075798511505127 4.8838090896606445
Loss :  1.6644060611724854 3.5995066165924072 5.263912677764893
Loss :  1.6425902843475342 3.01628041267395 4.658870697021484
Loss :  1.6521577835083008 2.9776899814605713 4.629847526550293
Loss :  1.6611549854278564 2.7676379680633545 4.428792953491211
Loss :  1.6595431566238403 3.039405345916748 4.698948383331299
Loss :  1.6675453186035156 2.784810781478882 4.452356338500977
Loss :  1.624015212059021 3.2435429096221924 4.867558002471924
  batch 20 loss: 1.624015212059021, 3.2435429096221924, 4.867558002471924
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.659284234046936 3.1248414516448975 4.784125804901123
Loss :  1.6750192642211914 2.752861976623535 4.427881240844727
Loss :  1.6463825702667236 3.2165133953094482 4.862895965576172
Loss :  1.6873979568481445 3.0714240074157715 4.758821964263916
Loss :  1.6848634481430054 3.422452688217163 5.107316017150879
Loss :  1.6484419107437134 3.3560690879821777 5.004510879516602
Loss :  1.69735586643219 3.1482300758361816 4.845585823059082
Loss :  1.6399009227752686 2.992262601852417 4.6321635246276855
Loss :  1.6880650520324707 2.8593413829803467 4.547406196594238
Loss :  1.6413991451263428 2.818385124206543 4.459784507751465
Loss :  1.7226711511611938 3.360431671142578 5.083102703094482
Loss :  1.6690746545791626 3.170107364654541 4.839181900024414
Loss :  1.6544201374053955 3.015536069869995 4.669956207275391
Loss :  1.6568297147750854 2.7658839225769043 4.422713756561279
Loss :  1.6967917680740356 3.381927967071533 5.078719615936279
Loss :  1.6888669729232788 2.732416868209839 4.421283721923828
Loss :  1.6665118932724 2.823505401611328 4.490017414093018
Loss :  1.6343210935592651 2.9361135959625244 4.5704345703125
Loss :  1.6596839427947998 2.7178187370300293 4.37750244140625
Loss :  1.652181625366211 3.280775547027588 4.932957172393799
  batch 40 loss: 1.652181625366211, 3.280775547027588, 4.932957172393799
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602917909622192 3.004462480545044 4.664754390716553
Loss :  1.648537516593933 2.657219409942627 4.30575704574585
Loss :  1.6654839515686035 3.0171024799346924 4.682586669921875
Loss :  1.6638095378875732 2.9629883766174316 4.626797676086426
Loss :  1.666703701019287 2.9949984550476074 4.6617021560668945
Loss :  1.6581119298934937 2.984243154525757 4.642354965209961
Loss :  1.6451501846313477 3.44973087310791 5.094881057739258
Loss :  1.6573331356048584 2.8944108486175537 4.551743984222412
Loss :  1.6301294565200806 3.394563913345337 5.024693489074707
Loss :  1.681283712387085 3.433654546737671 5.114938259124756
Loss :  1.6458450555801392 2.8884778022766113 4.534322738647461
Loss :  1.6641432046890259 3.0262293815612793 4.690372467041016
Loss :  1.682677984237671 3.5734963417053223 5.256174087524414
Loss :  1.667445421218872 2.9834115505218506 4.650856971740723
Loss :  1.6706764698028564 3.5464982986450195 5.217174530029297
Loss :  1.6349540948867798 2.617861032485962 4.252815246582031
Loss :  1.6846297979354858 3.549891710281372 5.234521389007568
Loss :  1.6822165250778198 3.313617467880249 4.995833873748779
Loss :  1.6956197023391724 3.1599652767181396 4.855585098266602
Loss :  1.670906662940979 2.936352491378784 4.607259273529053
  batch 60 loss: 1.670906662940979, 2.936352491378784, 4.607259273529053
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.672609806060791 3.424396276473999 5.097005844116211
Loss :  1.669381022453308 2.6575980186462402 4.326979160308838
Loss :  1.6774325370788574 3.2516860961914062 4.929118633270264
Loss :  1.6580173969268799 2.8713924884796143 4.529409885406494
Loss :  1.6550606489181519 3.2888402938842773 4.943901062011719
Loss :  5.489895343780518 4.338962078094482 9.828857421875
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.379603385925293 4.355343818664551 9.734947204589844
Loss :  5.488900184631348 4.321460247039795 9.810359954833984
Loss :  4.568015098571777 4.088324546813965 8.656339645385742
Total LOSS train 4.725009815509503 valid 9.507626056671143
CE LOSS train 1.6640765061745277 valid 1.1420037746429443
Contrastive LOSS train 3.0609333441807673 valid 1.0220811367034912
EPOCH 244:
Loss :  1.6504393815994263 2.7043981552124023 4.354837417602539
Loss :  1.6662670373916626 3.2653961181640625 4.9316630363464355
Loss :  1.6521202325820923 2.7812414169311523 4.433361530303955
Loss :  1.6552767753601074 3.0804688930511475 4.735745429992676
Loss :  1.679919958114624 3.26497220993042 4.944891929626465
Loss :  1.6628988981246948 3.1309354305267334 4.793834209442139
Loss :  1.6610392332077026 3.196113348007202 4.857152462005615
Loss :  1.6499390602111816 2.8860065937042236 4.535945892333984
Loss :  1.654586911201477 2.460571050643921 4.1151580810546875
Loss :  1.6090282201766968 2.557159185409546 4.166187286376953
Loss :  1.6686657667160034 3.3640296459198 5.032695293426514
Loss :  1.72922945022583 2.941448211669922 4.670677661895752
Loss :  1.6762402057647705 3.101064920425415 4.7773051261901855
Loss :  1.664412021636963 3.1150529384613037 4.7794647216796875
Loss :  1.6425886154174805 3.0717077255249023 4.714296340942383
Loss :  1.6521542072296143 3.4169504642486572 5.0691046714782715
Loss :  1.661157250404358 2.9585988521575928 4.61975622177124
Loss :  1.6595314741134644 2.7246780395507812 4.384209632873535
Loss :  1.6675442457199097 3.034003257751465 4.701547622680664
Loss :  1.6240166425704956 2.813838481903076 4.437855243682861
  batch 20 loss: 1.6240166425704956, 2.813838481903076, 4.437855243682861
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592824459075928 3.085479974746704 4.744762420654297
Loss :  1.675015926361084 2.6860132217407227 4.361029148101807
Loss :  1.6463773250579834 3.301024913787842 4.947402000427246
Loss :  1.6873977184295654 3.1835215091705322 4.870919227600098
Loss :  1.6848597526550293 3.4551901817321777 5.140049934387207
Loss :  1.6484373807907104 3.1711621284484863 4.819599628448486
Loss :  1.697363018989563 3.0516884326934814 4.749051570892334
Loss :  1.6398963928222656 3.1023058891296387 4.742202281951904
Loss :  1.6880640983581543 2.668743371963501 4.356807708740234
Loss :  1.641404151916504 3.2415449619293213 4.882948875427246
Loss :  1.7226711511611938 3.253239631652832 4.975910663604736
Loss :  1.6690701246261597 3.0040297508239746 4.673099994659424
Loss :  1.6544158458709717 3.1801342964172363 4.834549903869629
Loss :  1.6568330526351929 2.670095682144165 4.326928615570068
Loss :  1.696783423423767 3.2928788661956787 4.989662170410156
Loss :  1.6888569593429565 2.7312872409820557 4.420144081115723
Loss :  1.6665033102035522 3.2942895889282227 4.9607930183410645
Loss :  1.6343194246292114 2.8359527587890625 4.470272064208984
Loss :  1.6596786975860596 2.8444035053253174 4.504082202911377
Loss :  1.6521764993667603 3.2352054119110107 4.8873820304870605
  batch 40 loss: 1.6521764993667603, 3.2352054119110107, 4.8873820304870605
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.660286784172058 3.3761115074157715 5.036398410797119
Loss :  1.6485347747802734 2.8403544425964355 4.488889217376709
Loss :  1.665475845336914 3.155170440673828 4.820646286010742
Loss :  1.6637967824935913 3.1077985763549805 4.771595478057861
Loss :  1.6666953563690186 2.80833101272583 4.4750261306762695
Loss :  1.658109426498413 3.0003812313079834 4.6584906578063965
Loss :  1.645145058631897 3.3020718097686768 4.947216987609863
Loss :  1.6573327779769897 2.950521230697632 4.607853889465332
Loss :  1.6301285028457642 3.2571258544921875 4.887254238128662
Loss :  1.6812772750854492 3.273247003555298 4.954524040222168
Loss :  1.6458436250686646 2.7826638221740723 4.428507328033447
Loss :  1.6641430854797363 3.0440690517425537 4.708211898803711
Loss :  1.6826677322387695 3.388700008392334 5.0713677406311035
Loss :  1.667433261871338 2.964656114578247 4.632089614868164
Loss :  1.6706780195236206 3.201570749282837 4.872248649597168
Loss :  1.6349631547927856 2.7675671577453613 4.402530193328857
Loss :  1.6846377849578857 3.4507288932800293 5.135366439819336
Loss :  1.6822068691253662 2.987022876739502 4.669229507446289
Loss :  1.6956160068511963 3.064990997314453 4.76060676574707
Loss :  1.6709089279174805 2.8347136974334717 4.505622863769531
  batch 60 loss: 1.6709089279174805, 2.8347136974334717, 4.505622863769531
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726100444793701 3.569427728652954 5.242037773132324
Loss :  1.669381022453308 2.766914129257202 4.436295032501221
Loss :  1.6774210929870605 2.926771402359009 4.604192733764648
Loss :  1.6580150127410889 2.8687071800231934 4.526721954345703
Loss :  1.6550493240356445 2.535243511199951 4.190292835235596
Loss :  5.488901615142822 4.436682224273682 9.925583839416504
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.378633499145508 4.416731357574463 9.795364379882812
Loss :  5.48818826675415 4.34073543548584 9.828924179077148
Loss :  4.565766334533691 4.281842231750488 8.84760856628418
Total LOSS train 4.700715446472168 valid 9.599370241165161
CE LOSS train 1.6640741513325619 valid 1.1414415836334229
Contrastive LOSS train 3.036641333653377 valid 1.070460557937622
EPOCH 245:
Loss :  1.6504265069961548 2.5213723182678223 4.1717987060546875
Loss :  1.6662564277648926 3.408646583557129 5.0749030113220215
Loss :  1.652109980583191 3.1354146003723145 4.787524700164795
Loss :  1.6552703380584717 3.0927541255950928 4.7480244636535645
Loss :  1.6799168586730957 3.2999329566955566 4.979849815368652
Loss :  1.6628906726837158 2.8677022457122803 4.530592918395996
Loss :  1.6610462665557861 3.8458683490753174 5.5069146156311035
Loss :  1.64993417263031 2.6547420024871826 4.304676055908203
Loss :  1.6545928716659546 2.3394181728363037 3.9940109252929688
Loss :  1.6090352535247803 2.8904473781585693 4.49948263168335
Loss :  1.6686694622039795 3.378347158432007 5.047016620635986
Loss :  1.7292166948318481 3.1015806198120117 4.83079719543457
Loss :  1.676217794418335 3.4336068630218506 5.1098246574401855
Loss :  1.6644113063812256 3.369603157043457 5.034014701843262
Loss :  1.6425838470458984 3.0363032817840576 4.678887367248535
Loss :  1.6521544456481934 3.1664280891418457 4.818582534790039
Loss :  1.661152720451355 2.7876737117767334 4.448826313018799
Loss :  1.659544587135315 2.7032244205474854 4.36276912689209
Loss :  1.667548656463623 2.769505739212036 4.437054634094238
Loss :  1.6240127086639404 2.65986967086792 4.283882141113281
  batch 20 loss: 1.6240127086639404, 2.65986967086792, 4.283882141113281
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592798233032227 3.164132595062256 4.8234124183654785
Loss :  1.6750237941741943 2.779627799987793 4.454651832580566
Loss :  1.646376371383667 3.3447141647338867 4.991090774536133
Loss :  1.6874024868011475 3.079583168029785 4.766985893249512
Loss :  1.6848640441894531 3.643453359603882 5.328317642211914
Loss :  1.6484376192092896 3.0307939052581787 4.679231643676758
Loss :  1.697358250617981 3.2460665702819824 4.943424701690674
Loss :  1.6398996114730835 3.2610890865325928 4.900988578796387
Loss :  1.6880629062652588 2.648928642272949 4.336991310119629
Loss :  1.6414083242416382 2.891216278076172 4.5326247215271
Loss :  1.722671389579773 3.1635451316833496 4.886216640472412
Loss :  1.6690778732299805 3.155327081680298 4.824404716491699
Loss :  1.654421329498291 2.9207003116607666 4.575121879577637
Loss :  1.6568326950073242 2.883335590362549 4.540168285369873
Loss :  1.6967849731445312 3.2572450637817383 4.9540300369262695
Loss :  1.6888635158538818 2.715205430984497 4.404068946838379
Loss :  1.6664985418319702 2.9472970962524414 4.613795757293701
Loss :  1.6343214511871338 2.7220890522003174 4.356410503387451
Loss :  1.659680962562561 2.9542078971862793 4.613888740539551
Loss :  1.6521741151809692 3.1505930423736572 4.802767276763916
  batch 40 loss: 1.6521741151809692, 3.1505930423736572, 4.802767276763916
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602821350097656 3.375352144241333 5.0356340408325195
Loss :  1.6485399007797241 2.727715253829956 4.376255035400391
Loss :  1.6654852628707886 3.2375566959381104 4.903041839599609
Loss :  1.6638036966323853 2.9423513412475586 4.606154918670654
Loss :  1.666701316833496 3.005755662918091 4.672456741333008
Loss :  1.6581108570098877 3.3008904457092285 4.959001541137695
Loss :  1.6451447010040283 3.364081382751465 5.009225845336914
Loss :  1.6573377847671509 2.9959449768066406 4.653282642364502
Loss :  1.6301242113113403 3.4830482006073 5.11317253112793
Loss :  1.6812727451324463 3.297118663787842 4.978391647338867
Loss :  1.6458463668823242 2.6564223766326904 4.302268981933594
Loss :  1.6641466617584229 3.0130205154418945 4.677166938781738
Loss :  1.6826797723770142 3.4522323608398438 5.134912014007568
Loss :  1.6674368381500244 2.712320566177368 4.379757404327393
Loss :  1.6706788539886475 3.1275856494903564 4.798264503479004
Loss :  1.6349573135375977 2.8555245399475098 4.490481853485107
Loss :  1.6846383810043335 3.4092037677764893 5.093842029571533
Loss :  1.682214617729187 3.0866341590881348 4.768848896026611
Loss :  1.6956156492233276 3.2719805240631104 4.967596054077148
Loss :  1.670910358428955 3.064467191696167 4.735377311706543
  batch 60 loss: 1.670910358428955, 3.064467191696167, 4.735377311706543
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726123094558716 3.4534287452697754 5.126040935516357
Loss :  1.669389009475708 2.772552013397217 4.441941261291504
Loss :  1.6774269342422485 3.164820671081543 4.842247486114502
Loss :  1.6580138206481934 2.8424112796783447 4.500425338745117
Loss :  1.6550570726394653 3.63570499420166 5.290761947631836
Loss :  5.489466190338135 4.386024475097656 9.875490188598633
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.379313945770264 4.378697395324707 9.758010864257812
Loss :  5.488574981689453 4.2566633224487305 9.745238304138184
Loss :  4.5667033195495605 4.265509605407715 8.832212448120117
Total LOSS train 4.735916541172908 valid 9.552737951278687
CE LOSS train 1.6640747418770423 valid 1.1416758298873901
Contrastive LOSS train 3.0718417974618766 valid 1.0663774013519287
EPOCH 246:
Loss :  1.6504381895065308 2.8956446647644043 4.546082973480225
Loss :  1.6662588119506836 3.135619878768921 4.801878929138184
Loss :  1.652115821838379 2.749176502227783 4.401292324066162
Loss :  1.6552798748016357 2.874335527420044 4.52961540222168
Loss :  1.6799200773239136 3.0353002548217773 4.7152204513549805
Loss :  1.662899136543274 2.773932695388794 4.436831951141357
Loss :  1.661049723625183 3.2245073318481445 4.885557174682617
Loss :  1.649933934211731 2.7060322761535645 4.355966091156006
Loss :  1.654590368270874 2.387270212173462 4.041860580444336
Loss :  1.6090341806411743 2.6347272396087646 4.2437615394592285
Loss :  1.6686701774597168 3.4102323055267334 5.078902244567871
Loss :  1.7292263507843018 2.8966495990753174 4.625875949859619
Loss :  1.6762081384658813 3.2512550354003906 4.927463054656982
Loss :  1.6644084453582764 3.029608964920044 4.69401741027832
Loss :  1.6425907611846924 3.0737199783325195 4.716310501098633
Loss :  1.652157187461853 3.1867973804473877 4.838954448699951
Loss :  1.6611533164978027 2.8688366413116455 4.529990196228027
Loss :  1.6595381498336792 2.885432243347168 4.544970512390137
Loss :  1.667553424835205 2.7200543880462646 4.387607574462891
Loss :  1.624017357826233 2.8265597820281982 4.450577259063721
  batch 20 loss: 1.624017357826233, 2.8265597820281982, 4.450577259063721
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592775583267212 2.9677584171295166 4.627036094665527
Loss :  1.6750191450119019 2.429051637649536 4.104070663452148
Loss :  1.6463799476623535 3.233130693435669 4.879510879516602
Loss :  1.6874008178710938 2.9137792587280273 4.601180076599121
Loss :  1.684870719909668 3.4326391220092773 5.117509841918945
Loss :  1.6484352350234985 3.2350199222564697 4.883455276489258
Loss :  1.6973564624786377 2.8445327281951904 4.541889190673828
Loss :  1.6398959159851074 2.9312031269073486 4.571099281311035
Loss :  1.6880615949630737 2.752295732498169 4.440357208251953
Loss :  1.6414062976837158 3.019857168197632 4.661263465881348
Loss :  1.7226706743240356 3.1811776161193848 4.903848171234131
Loss :  1.6690691709518433 3.1994972229003906 4.868566513061523
Loss :  1.654410481452942 3.2100446224212646 4.864455223083496
Loss :  1.656830906867981 2.9468252658843994 4.60365629196167
Loss :  1.6967757940292358 3.486759662628174 5.183535575866699
Loss :  1.6888610124588013 2.681382179260254 4.370243072509766
Loss :  1.6665030717849731 3.205235004425049 4.871737957000732
Loss :  1.6343165636062622 3.1548736095428467 4.789190292358398
Loss :  1.6596699953079224 2.770064353942871 4.429734230041504
Loss :  1.652170181274414 3.3218071460723877 4.973977088928223
  batch 40 loss: 1.652170181274414, 3.3218071460723877, 4.973977088928223
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602766513824463 3.3248276710510254 4.985104560852051
Loss :  1.6485413312911987 2.6918041706085205 4.34034538269043
Loss :  1.6654748916625977 3.144233465194702 4.809708595275879
Loss :  1.6637922525405884 3.0875391960144043 4.751331329345703
Loss :  1.6666978597640991 2.820103406906128 4.4868011474609375
Loss :  1.6581119298934937 3.022958278656006 4.681070327758789
Loss :  1.645145058631897 3.1285858154296875 4.773730754852295
Loss :  1.6573349237442017 2.844391107559204 4.501726150512695
Loss :  1.6301318407058716 3.2572929859161377 4.887424945831299
Loss :  1.6812777519226074 3.1922426223754883 4.873520374298096
Loss :  1.6458357572555542 2.6988656520843506 4.344701290130615
Loss :  1.6641417741775513 2.99755597114563 4.661697864532471
Loss :  1.6826692819595337 3.4938607215881348 5.176529884338379
Loss :  1.667428970336914 3.277569532394409 4.944998741149902
Loss :  1.6706804037094116 3.2625441551208496 4.933224678039551
Loss :  1.6349564790725708 2.6739695072174072 4.308926105499268
Loss :  1.6846365928649902 3.395493507385254 5.080130100250244
Loss :  1.6822123527526855 3.4212646484375 5.1034770011901855
Loss :  1.6956195831298828 3.314089059829712 5.009708404541016
Loss :  1.6709033250808716 2.780853748321533 4.451756954193115
  batch 60 loss: 1.6709033250808716, 2.780853748321533, 4.451756954193115
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726163625717163 3.557032823562622 5.229649066925049
Loss :  1.6693788766860962 2.7861390113830566 4.455517768859863
Loss :  1.6774262189865112 3.3154494762420654 4.992875576019287
Loss :  1.6580092906951904 3.0336310863494873 4.691640377044678
Loss :  1.655060887336731 3.1378870010375977 4.792947769165039
Loss :  5.485744476318359 4.405883312225342 9.89162826538086
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.375373840332031 4.405686855316162 9.781061172485352
Loss :  5.484918594360352 4.25945520401001 9.744373321533203
Loss :  4.5644073486328125 4.257213115692139 8.82162094116211
Total LOSS train 4.697039970984826 valid 9.55967092514038
CE LOSS train 1.6640736249776986 valid 1.1411018371582031
Contrastive LOSS train 3.0329663386711707 valid 1.0643032789230347
EPOCH 247:
Loss :  1.6504162549972534 2.551175117492676 4.201591491699219
Loss :  1.6662582159042358 3.1487889289855957 4.815047264099121
Loss :  1.6521135568618774 3.213529109954834 4.865642547607422
Loss :  1.6552681922912598 2.9849581718444824 4.640226364135742
Loss :  1.6799226999282837 3.444188356399536 5.124111175537109
Loss :  1.662885069847107 2.582634925842285 4.245520114898682
Loss :  1.661055564880371 3.5272958278656006 5.188351631164551
Loss :  1.6499367952346802 2.7094967365264893 4.359433650970459
Loss :  1.6545779705047607 2.525493621826172 4.180071830749512
Loss :  1.6090492010116577 3.0494565963745117 4.658505916595459
Loss :  1.6686781644821167 3.459773302078247 5.128451347351074
Loss :  1.7292011976242065 2.9325571060180664 4.6617584228515625
Loss :  1.6762311458587646 3.2293689250946045 4.905600070953369
Loss :  1.664406418800354 3.673248291015625 5.3376545906066895
Loss :  1.642573595046997 3.0429749488830566 4.685548782348633
Loss :  1.6521698236465454 2.799269914627075 4.45143985748291
Loss :  1.6611579656600952 2.908184051513672 4.569342136383057
Loss :  1.6595298051834106 3.194356679916382 4.853886604309082
Loss :  1.6675610542297363 2.871914863586426 4.539475917816162
Loss :  1.6240034103393555 3.2329704761505127 4.856973648071289
  batch 20 loss: 1.6240034103393555, 3.2329704761505127, 4.856973648071289
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.659273624420166 3.148338794708252 4.807612419128418
Loss :  1.675012230873108 2.7488014698028564 4.423813819885254
Loss :  1.6463860273361206 3.397714853286743 5.044100761413574
Loss :  1.6873903274536133 2.917799472808838 4.605189800262451
Loss :  1.6848692893981934 3.225799560546875 4.910668849945068
Loss :  1.6484341621398926 3.107680559158325 4.756114959716797
Loss :  1.6973626613616943 2.903914213180542 4.601276874542236
Loss :  1.6398887634277344 3.0863726139068604 4.726261138916016
Loss :  1.688066005706787 2.7813069820404053 4.469372749328613
Loss :  1.6414132118225098 3.230196952819824 4.871610164642334
Loss :  1.7226639986038208 3.2530012130737305 4.975665092468262
Loss :  1.6690748929977417 3.1995460987091064 4.868620872497559
Loss :  1.6544198989868164 2.9925878047943115 4.647007942199707
Loss :  1.6568338871002197 2.8254120349884033 4.482245922088623
Loss :  1.6967872381210327 3.298264503479004 4.995051860809326
Loss :  1.688869833946228 2.6608123779296875 4.349682331085205
Loss :  1.6665029525756836 3.1094911098480225 4.775994300842285
Loss :  1.6343226432800293 2.8292012214660645 4.463523864746094
Loss :  1.6596732139587402 2.930039405822754 4.589712619781494
Loss :  1.6521762609481812 3.1418042182922363 4.793980598449707
  batch 40 loss: 1.6521762609481812, 3.1418042182922363, 4.793980598449707
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602773666381836 3.516810178756714 5.177087783813477
Loss :  1.648544192314148 2.69732928276062 4.3458733558654785
Loss :  1.6654809713363647 3.1941211223602295 4.859601974487305
Loss :  1.6637879610061646 2.9277379512786865 4.591526031494141
Loss :  1.6666992902755737 2.38268780708313 4.049386978149414
Loss :  1.6581166982650757 2.9348642826080322 4.592980861663818
Loss :  1.6451528072357178 3.4146530628204346 5.059805870056152
Loss :  1.6573351621627808 2.873497486114502 4.530832767486572
Loss :  1.630131483078003 3.725200891494751 5.355332374572754
Loss :  1.681279182434082 3.3457250595092773 5.027004241943359
Loss :  1.6458439826965332 2.7676827907562256 4.41352653503418
Loss :  1.6641409397125244 2.938307762145996 4.602448463439941
Loss :  1.6826756000518799 3.300755500793457 4.983430862426758
Loss :  1.6674253940582275 3.11083984375 4.778264999389648
Loss :  1.6706721782684326 3.5358738899230957 5.206545829772949
Loss :  1.6349526643753052 2.530661106109619 4.165613651275635
Loss :  1.6846369504928589 3.2439868450164795 4.928623676300049
Loss :  1.68220853805542 3.2569758892059326 4.939184188842773
Loss :  1.6956212520599365 3.346433401107788 5.042054653167725
Loss :  1.670906662940979 2.5733799934387207 4.24428653717041
  batch 60 loss: 1.670906662940979, 2.5733799934387207, 4.24428653717041
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726081371307373 3.350194215774536 5.022802352905273
Loss :  1.6693743467330933 2.7957513332366943 4.465125560760498
Loss :  1.6774204969406128 3.2340128421783447 4.911433219909668
Loss :  1.6580138206481934 2.8409390449523926 4.498952865600586
Loss :  1.6550580263137817 2.9431567192077637 4.598214626312256
Loss :  5.485515594482422 4.398015975952148 9.88353157043457
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.375428199768066 4.382279396057129 9.757707595825195
Loss :  5.484317779541016 4.232632637023926 9.716950416564941
Loss :  4.564235210418701 4.019176006317139 8.58341121673584
Total LOSS train 4.720247393388014 valid 9.485400199890137
CE LOSS train 1.6640735589540923 valid 1.1410588026046753
Contrastive LOSS train 3.0561738417698785 valid 1.0047940015792847
EPOCH 248:
Loss :  1.6504135131835938 2.6700074672698975 4.32042121887207
Loss :  1.6662652492523193 3.1970324516296387 4.863297462463379
Loss :  1.6521103382110596 3.310163736343384 4.962274074554443
Loss :  1.6552549600601196 3.2247235774993896 4.879978656768799
Loss :  1.6799119710922241 3.360609292984009 5.040521144866943
Loss :  1.6628801822662354 2.9830944538116455 4.645974636077881
Loss :  1.6610469818115234 2.948408842086792 4.6094560623168945
Loss :  1.6499358415603638 2.5663137435913086 4.216249465942383
Loss :  1.6545813083648682 2.556870937347412 4.211452484130859
Loss :  1.6090384721755981 2.6833608150482178 4.2923994064331055
Loss :  1.6686656475067139 3.2923033237457275 4.960968971252441
Loss :  1.729203701019287 2.8822805881500244 4.611484527587891
Loss :  1.6762386560440063 3.1007392406463623 4.776978015899658
Loss :  1.6644065380096436 3.5451998710632324 5.209606170654297
Loss :  1.6425873041152954 3.121934413909912 4.764521598815918
Loss :  1.6521505117416382 3.2228758335113525 4.875026226043701
Loss :  1.6611461639404297 2.6962826251983643 4.357428550720215
Loss :  1.6595207452774048 2.7610769271850586 4.420597553253174
Loss :  1.6675524711608887 2.9046735763549805 4.572226047515869
Loss :  1.624005913734436 2.9824273586273193 4.606433391571045
  batch 20 loss: 1.624005913734436, 2.9824273586273193, 4.606433391571045
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592777967453003 3.3032710552215576 4.962548732757568
Loss :  1.6750011444091797 2.5255682468414307 4.200569152832031
Loss :  1.6463854312896729 3.005767822265625 4.652153015136719
Loss :  1.6873847246170044 2.8397276401519775 4.5271124839782715
Loss :  1.6848539113998413 3.3167381286621094 5.00159215927124
Loss :  1.6484336853027344 3.414315700531006 5.06274938583374
Loss :  1.6973536014556885 2.8541438579559326 4.551497459411621
Loss :  1.6398866176605225 3.1592118740081787 4.799098491668701
Loss :  1.6880639791488647 2.752763509750366 4.440827369689941
Loss :  1.6413989067077637 3.0587399005889893 4.700139045715332
Loss :  1.7226550579071045 3.308476448059082 5.031131744384766
Loss :  1.6690495014190674 2.9934158325195312 4.6624650955200195
Loss :  1.6544113159179688 3.270146608352661 4.924557685852051
Loss :  1.6568288803100586 2.9551732540130615 4.612002372741699
Loss :  1.6967488527297974 3.485827684402466 5.182576656341553
Loss :  1.6888562440872192 2.7301595211029053 4.419015884399414
Loss :  1.6665019989013672 2.8445868492126465 4.511088848114014
Loss :  1.6343040466308594 2.85781192779541 4.4921159744262695
Loss :  1.659671664237976 2.6461105346679688 4.305782318115234
Loss :  1.652174949645996 3.3600716590881348 5.012246608734131
  batch 40 loss: 1.652174949645996, 3.3600716590881348, 5.012246608734131
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602699756622314 3.19338059425354 4.8536505699157715
Loss :  1.6485263109207153 3.122000217437744 4.77052640914917
Loss :  1.6654694080352783 2.8607568740844727 4.526226043701172
Loss :  1.6637879610061646 2.9746899604797363 4.638477802276611
Loss :  1.6666841506958008 2.9713640213012695 4.63804817199707
Loss :  1.658107876777649 3.0134828090667725 4.671590805053711
Loss :  1.6451504230499268 3.0479040145874023 4.69305419921875
Loss :  1.6573158502578735 2.639813184738159 4.297129154205322
Loss :  1.6301273107528687 3.103226900100708 4.733354091644287
Loss :  1.6812586784362793 2.8992903232574463 4.580549240112305
Loss :  1.6458426713943481 2.5061233043670654 4.151966094970703
Loss :  1.6641346216201782 2.886337995529175 4.550472736358643
Loss :  1.6826684474945068 3.154414415359497 4.837082862854004
Loss :  1.6674124002456665 3.04142165184021 4.708834171295166
Loss :  1.6706655025482178 3.2807953357696533 4.951460838317871
Loss :  1.6349515914916992 2.7794930934906006 4.414444923400879
Loss :  1.6846325397491455 3.491330146789551 5.175962448120117
Loss :  1.682201862335205 3.375640630722046 5.057842254638672
Loss :  1.6956126689910889 3.416043519973755 5.111656188964844
Loss :  1.6709002256393433 3.067551851272583 4.738451957702637
  batch 60 loss: 1.6709002256393433, 3.067551851272583, 4.738451957702637
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726046800613403 3.5106537342071533 5.183258533477783
Loss :  1.6693693399429321 2.6074492931365967 4.276818752288818
Loss :  1.677410364151001 3.274169683456421 4.951580047607422
Loss :  1.6580079793930054 3.150181770324707 4.808189868927002
Loss :  1.6550462245941162 2.7502975463867188 4.405344009399414
Loss :  5.4869704246521 4.306956768035889 9.793927192687988
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.377014636993408 4.379150867462158 9.756165504455566
Loss :  5.485982894897461 4.287844181060791 9.773826599121094
Loss :  4.56564998626709 4.154669761657715 8.720319747924805
Total LOSS train 4.691915974250207 valid 9.511059761047363
CE LOSS train 1.664066428404588 valid 1.1414124965667725
Contrastive LOSS train 3.0278495385096624 valid 1.0386674404144287
EPOCH 249:
Loss :  1.6504079103469849 2.692370653152466 4.34277868270874
Loss :  1.666251301765442 3.3617970943450928 5.028048515319824
Loss :  1.652109980583191 2.846876382827759 4.49898624420166
Loss :  1.655263066291809 2.9866933822631836 4.641956329345703
Loss :  1.6799063682556152 3.487617254257202 5.167523384094238
Loss :  1.662889838218689 3.0264780521392822 4.689367771148682
Loss :  1.661041259765625 3.1065847873687744 4.76762580871582
Loss :  1.6499333381652832 2.7134110927581787 4.363344192504883
Loss :  1.6545815467834473 2.5524215698242188 4.207003116607666
Loss :  1.6090419292449951 2.798380136489868 4.407422065734863
Loss :  1.6686586141586304 3.612133264541626 5.280791759490967
Loss :  1.7291969060897827 2.9330012798309326 4.662198066711426
Loss :  1.6761990785598755 3.0736048221588135 4.7498040199279785
Loss :  1.6644054651260376 3.3327016830444336 4.997107028961182
Loss :  1.6425830125808716 2.661965847015381 4.304548740386963
Loss :  1.6521464586257935 3.2150208950042725 4.8671674728393555
Loss :  1.6611418724060059 2.709770917892456 4.370912551879883
Loss :  1.6595317125320435 3.305844306945801 4.965375900268555
Loss :  1.6675503253936768 2.995055675506592 4.662606239318848
Loss :  1.623997688293457 2.8679611682891846 4.4919586181640625
  batch 20 loss: 1.623997688293457, 2.8679611682891846, 4.4919586181640625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.659273386001587 3.1737887859344482 4.833062171936035
Loss :  1.675007939338684 2.9356424808502197 4.610650539398193
Loss :  1.6463755369186401 3.2778499126434326 4.924225330352783
Loss :  1.6873865127563477 2.767490863800049 4.4548773765563965
Loss :  1.6848433017730713 3.5685460567474365 5.253389358520508
Loss :  1.6484259366989136 3.2345786094665527 4.883004665374756
Loss :  1.6973578929901123 2.874521493911743 4.5718793869018555
Loss :  1.6398838758468628 3.1841061115264893 4.8239898681640625
Loss :  1.6880652904510498 2.8530633449554443 4.541128635406494
Loss :  1.6414066553115845 2.9455649852752686 4.586971759796143
Loss :  1.7226508855819702 3.3489737510681152 5.071624755859375
Loss :  1.6690574884414673 3.606274127960205 5.275331497192383
Loss :  1.6544060707092285 2.665680170059204 4.320086479187012
Loss :  1.6568303108215332 2.891641855239868 4.5484724044799805
Loss :  1.6967508792877197 3.5007777214050293 5.197528839111328
Loss :  1.6888507604599 2.7562339305877686 4.445084571838379
Loss :  1.6664979457855225 2.98415207862854 4.6506500244140625
Loss :  1.6343157291412354 2.807283878326416 4.4415998458862305
Loss :  1.6596654653549194 2.7718889713287354 4.431554317474365
Loss :  1.652172565460205 3.2718350887298584 4.924007415771484
  batch 40 loss: 1.652172565460205, 3.2718350887298584, 4.924007415771484
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602660417556763 3.1854541301727295 4.845720291137695
Loss :  1.6485246419906616 2.8897156715393066 4.538240432739258
Loss :  1.665466547012329 3.2772810459136963 4.942747592926025
Loss :  1.6637823581695557 2.769317626953125 4.433099746704102
Loss :  1.666686773300171 2.6031334400177 4.269820213317871
Loss :  1.6581056118011475 2.912125587463379 4.5702314376831055
Loss :  1.6451501846313477 3.229004144668579 4.874154090881348
Loss :  1.6573169231414795 2.5703163146972656 4.227633476257324
Loss :  1.6301292181015015 3.4013969898223877 5.0315260887146
Loss :  1.681261420249939 3.2505762577056885 4.931837558746338
Loss :  1.6458386182785034 2.673801898956299 4.319640636444092
Loss :  1.6641325950622559 2.80774188041687 4.471874237060547
Loss :  1.6826642751693726 3.33011794090271 5.012782096862793
Loss :  1.6674089431762695 3.178945779800415 4.8463544845581055
Loss :  1.6706748008728027 3.323734998703003 4.994409561157227
Loss :  1.634961485862732 2.66867995262146 4.303641319274902
Loss :  1.684635877609253 3.069044589996338 4.753680229187012
Loss :  1.6821975708007812 3.2711594104766846 4.953356742858887
Loss :  1.6956121921539307 3.339071035385132 5.0346832275390625
Loss :  1.6709048748016357 2.9338576793670654 4.604762554168701
  batch 60 loss: 1.6709048748016357, 2.9338576793670654, 4.604762554168701
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726057529449463 3.3357584476470947 5.008364200592041
Loss :  1.6693761348724365 2.7414262294769287 4.410802364349365
Loss :  1.677411675453186 2.9756431579589844 4.653054714202881
Loss :  1.6580129861831665 2.8321163654327393 4.490129470825195
Loss :  1.655036211013794 3.318814277648926 4.973850250244141
Loss :  5.486873149871826 4.409454345703125 9.89632797241211
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.376705169677734 4.4590983390808105 9.835803985595703
Loss :  5.485869884490967 4.266961574554443 9.75283145904541
Loss :  4.5653862953186035 4.208189964294434 8.773576736450195
Total LOSS train 4.703877581082858 valid 9.564635038375854
CE LOSS train 1.6640650125650258 valid 1.1413465738296509
Contrastive LOSS train 3.039812605197613 valid 1.0520474910736084
EPOCH 250:
Loss :  1.6504161357879639 2.5581743717193604 4.208590507507324
Loss :  1.6662602424621582 3.229145050048828 4.895405292510986
Loss :  1.6521013975143433 2.9638636112213135 4.615964889526367
Loss :  1.6552579402923584 2.8802502155303955 4.535508155822754
Loss :  1.679917573928833 3.1167497634887695 4.796667098999023
Loss :  1.6628881692886353 3.168572187423706 4.831460475921631
Loss :  1.6610554456710815 3.3664512634277344 5.0275068283081055
Loss :  1.649937629699707 2.740262269973755 4.390199661254883
Loss :  1.6545848846435547 2.1548142433166504 3.809399127960205
Loss :  1.6090428829193115 2.68521785736084 4.2942609786987305
Loss :  1.6686522960662842 3.4896883964538574 5.1583404541015625
Loss :  1.7292070388793945 2.8976352214813232 4.626842498779297
Loss :  1.6761586666107178 3.440538167953491 5.116696834564209
Loss :  1.6644142866134644 3.419332265853882 5.083746433258057
Loss :  1.6425930261611938 3.1536080837249756 4.796201229095459
Loss :  1.6521556377410889 2.9726402759552 4.624795913696289
Loss :  1.661141037940979 2.7398531436920166 4.400994300842285
Loss :  1.659510612487793 3.2193832397460938 4.878893852233887
Loss :  1.6675410270690918 2.6172804832458496 4.284821510314941
Loss :  1.6240050792694092 3.108099937438965 4.732105255126953
  batch 20 loss: 1.6240050792694092, 3.108099937438965, 4.732105255126953
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592801809310913 3.2727484703063965 4.932028770446777
Loss :  1.6750046014785767 2.6088857650756836 4.283890247344971
Loss :  1.6463875770568848 3.437788963317871 5.084176540374756
Loss :  1.6873985528945923 2.5866806507110596 4.274079322814941
Loss :  1.68484628200531 3.413839817047119 5.098686218261719
Loss :  1.648433804512024 3.301408529281616 4.94984245300293
Loss :  1.69735848903656 3.2004330158233643 4.897791385650635
Loss :  1.6398835182189941 2.907505750656128 4.547389030456543
Loss :  1.6880629062652588 2.7693140506744385 4.457376956939697
Loss :  1.6414101123809814 3.45157527923584 5.092985153198242
Loss :  1.7226563692092896 3.4665913581848145 5.1892476081848145
Loss :  1.669052004814148 3.2427914142608643 4.911843299865723
Loss :  1.6544111967086792 3.2227041721343994 4.877115249633789
Loss :  1.6568377017974854 2.707785129547119 4.364623069763184
Loss :  1.6967496871948242 3.5098061561584473 5.2065558433532715
Loss :  1.688858985900879 2.5931828022003174 4.282041549682617
Loss :  1.6664986610412598 2.8200058937072754 4.486504554748535
Loss :  1.6343196630477905 2.735471487045288 4.369791030883789
Loss :  1.6596654653549194 2.9626755714416504 4.622341156005859
Loss :  1.6521704196929932 3.205719232559204 4.857889652252197
  batch 40 loss: 1.6521704196929932, 3.205719232559204, 4.857889652252197
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602725982666016 3.42616868019104 5.0864410400390625
Loss :  1.6485254764556885 2.736325740814209 4.384851455688477
Loss :  1.6654608249664307 3.155815839767456 4.821276664733887
Loss :  1.6637662649154663 2.877086877822876 4.540853023529053
Loss :  1.6666905879974365 2.884906768798828 4.551597595214844
Loss :  1.6581114530563354 3.2210726737976074 4.879184246063232
Loss :  1.645142674446106 3.298976421356201 4.944118976593018
Loss :  1.657315969467163 2.6485788822174072 4.30589485168457
Loss :  1.630138874053955 3.4791295528411865 5.1092681884765625
Loss :  1.6812646389007568 3.037799835205078 4.719064712524414
Loss :  1.6458370685577393 2.8565382957458496 4.502375602722168
Loss :  1.6641255617141724 2.9732444286346436 4.6373701095581055
Loss :  1.6826621294021606 3.290440797805786 4.973103046417236
Loss :  1.667399287223816 3.1044363975524902 4.771835803985596
Loss :  1.6706770658493042 3.5208868980407715 5.191564083099365
Loss :  1.6349453926086426 2.8325257301330566 4.467471122741699
Loss :  1.6846321821212769 3.1403441429138184 4.824976444244385
Loss :  1.6822012662887573 3.3364107608795166 5.018611907958984
Loss :  1.6956124305725098 3.3206841945648193 5.01629638671875
Loss :  1.670906901359558 2.76487135887146 4.4357781410217285
  batch 60 loss: 1.670906901359558, 2.76487135887146, 4.4357781410217285
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726094484329224 3.5317609310150146 5.204370498657227
Loss :  1.669371485710144 2.6809475421905518 4.350318908691406
Loss :  1.6774113178253174 3.4926366806030273 5.170047760009766
Loss :  1.6580125093460083 2.960965871810913 4.618978500366211
Loss :  1.6550427675247192 3.060718059539795 4.715760707855225
Loss :  5.4847893714904785 4.4605488777160645 9.945338249206543
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.374854564666748 4.350724220275879 9.725578308105469
Loss :  5.483573913574219 4.283959865570068 9.767534255981445
Loss :  4.563053607940674 4.29010009765625 8.853153228759766
Total LOSS train 4.7251693872305065 valid 9.572901010513306
CE LOSS train 1.664065590271583 valid 1.1407634019851685
Contrastive LOSS train 3.0611037987929124 valid 1.0725250244140625
EPOCH 251:
Loss :  1.6504144668579102 2.8815791606903076 4.531993865966797
Loss :  1.6662578582763672 3.1218137741088867 4.788071632385254
Loss :  1.6520969867706299 3.3859546184539795 5.038051605224609
Loss :  1.6552505493164062 2.9182097911834717 4.573460578918457
Loss :  1.6799159049987793 3.2899868488311768 4.969902992248535
Loss :  1.6628704071044922 2.869300365447998 4.53217077255249
Loss :  1.6610560417175293 3.6700985431671143 5.331154823303223
Loss :  1.6499309539794922 2.39555287361145 4.045483589172363
Loss :  1.6545807123184204 2.417309284210205 4.071889877319336
Loss :  1.6090439558029175 2.8662304878234863 4.475274562835693
Loss :  1.66865873336792 3.3297183513641357 4.998376846313477
Loss :  1.7292033433914185 2.949430227279663 4.678633689880371
Loss :  1.6761772632598877 3.032266139984131 4.708443641662598
Loss :  1.6644084453582764 3.384718179702759 5.049126625061035
Loss :  1.6425883769989014 3.148221254348755 4.790809631347656
Loss :  1.652160882949829 3.0541107654571533 4.706271648406982
Loss :  1.6611425876617432 2.917762041091919 4.578904628753662
Loss :  1.659524917602539 2.9867982864379883 4.646323204040527
Loss :  1.6675410270690918 2.8653039932250977 4.5328450202941895
Loss :  1.6240049600601196 3.1325199604034424 4.756525039672852
  batch 20 loss: 1.6240049600601196, 3.1325199604034424, 4.756525039672852
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.659279465675354 3.0521814823150635 4.711461067199707
Loss :  1.6750051975250244 3.081023931503296 4.75602912902832
Loss :  1.6463847160339355 3.2566375732421875 4.903022289276123
Loss :  1.6873899698257446 2.9814417362213135 4.668831825256348
Loss :  1.6848335266113281 3.4605367183685303 5.1453704833984375
Loss :  1.6484324932098389 3.315612316131592 4.964044570922852
Loss :  1.6973598003387451 3.1265876293182373 4.823947429656982
Loss :  1.6398873329162598 2.971813440322876 4.611701011657715
Loss :  1.6880658864974976 2.520038604736328 4.208104610443115
Loss :  1.6413977146148682 2.752821445465088 4.394219398498535
Loss :  1.7226585149765015 3.306877374649048 5.02953577041626
Loss :  1.6690579652786255 3.282599925994873 4.951657772064209
Loss :  1.6544088125228882 2.9167609214782715 4.571169853210449
Loss :  1.656831979751587 2.9900832176208496 4.646915435791016
Loss :  1.696746826171875 3.728738307952881 5.425485134124756
Loss :  1.688855767250061 2.803062915802002 4.491918563842773
Loss :  1.6665016412734985 2.6990785598754883 4.365580081939697
Loss :  1.6343212127685547 2.835832118988037 4.470153331756592
Loss :  1.6596708297729492 3.1197314262390137 4.779402256011963
Loss :  1.6521748304367065 3.002223491668701 4.654398441314697
  batch 40 loss: 1.6521748304367065, 3.002223491668701, 4.654398441314697
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602734327316284 3.2100508213043213 4.87032413482666
Loss :  1.6485244035720825 2.5878796577453613 4.236403942108154
Loss :  1.665458083152771 2.9932668209075928 4.658724784851074
Loss :  1.663775086402893 3.0180745124816895 4.681849479675293
Loss :  1.666689395904541 2.810880184173584 4.477569580078125
Loss :  1.6581076383590698 2.989372968673706 4.647480487823486
Loss :  1.645145058631897 3.2950658798217773 4.940210819244385
Loss :  1.657309889793396 2.5202972888946533 4.17760705947876
Loss :  1.6301389932632446 3.269667625427246 4.899806499481201
Loss :  1.6812578439712524 3.042139768600464 4.723397731781006
Loss :  1.6458386182785034 2.7260429859161377 4.371881484985352
Loss :  1.6641297340393066 3.0147438049316406 4.678873538970947
Loss :  1.6826672554016113 3.071256399154663 4.753923416137695
Loss :  1.6674026250839233 3.0964810848236084 4.763883590698242
Loss :  1.6706769466400146 3.0827865600585938 4.7534637451171875
Loss :  1.6349488496780396 2.5751638412475586 4.210112571716309
Loss :  1.684632420539856 3.5126028060913086 5.197235107421875
Loss :  1.6822047233581543 3.1128811836242676 4.795085906982422
Loss :  1.6956158876419067 3.4114813804626465 5.107097148895264
Loss :  1.6709073781967163 3.1767995357513428 4.8477067947387695
  batch 60 loss: 1.6709073781967163, 3.1767995357513428, 4.8477067947387695
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726096868515015 3.2149674892425537 4.887577056884766
Loss :  1.6693710088729858 2.711946725845337 4.381317615509033
Loss :  1.677416443824768 3.0396902561187744 4.717106819152832
Loss :  1.6580127477645874 2.8659374713897705 4.523950099945068
Loss :  1.6550463438034058 2.844714403152466 4.499760627746582
Loss :  5.484562397003174 4.408332824707031 9.892894744873047
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.374650001525879 4.351778507232666 9.726428985595703
Loss :  5.483393669128418 4.180521011352539 9.663914680480957
Loss :  4.563170909881592 4.122106075286865 8.685276985168457
Total LOSS train 4.695061735006479 valid 9.492128849029541
CE LOSS train 1.664065436216501 valid 1.140792727470398
Contrastive LOSS train 3.030996300623967 valid 1.0305265188217163
EPOCH 252:
Loss :  1.6504145860671997 2.4911344051361084 4.141549110412598
Loss :  1.6662577390670776 3.1815550327301025 4.847812652587891
Loss :  1.6520994901657104 2.735093116760254 4.387192726135254
Loss :  1.6552517414093018 2.7436397075653076 4.398891448974609
Loss :  1.6799160242080688 3.681083917617798 5.361000061035156
Loss :  1.66287100315094 2.857102394104004 4.519973278045654
Loss :  1.6610559225082397 3.2144432067871094 4.875499248504639
Loss :  1.6499302387237549 2.5639984607696533 4.213928699493408
Loss :  1.6545796394348145 2.403175115585327 4.0577545166015625
Loss :  1.6090441942214966 2.621659517288208 4.230703830718994
Loss :  1.6686604022979736 3.4674289226531982 5.136089324951172
Loss :  1.729202151298523 2.8489134311676025 4.578115463256836
Loss :  1.6761844158172607 3.2687032222747803 4.944887638092041
Loss :  1.6644062995910645 3.126861095428467 4.791267395019531
Loss :  1.6425858736038208 3.1147003173828125 4.757286071777344
Loss :  1.6521612405776978 3.0457656383514404 4.697926998138428
Loss :  1.661143183708191 2.949467420578003 4.610610485076904
Loss :  1.6595282554626465 2.778418779373169 4.4379472732543945
Loss :  1.6675441265106201 2.7389771938323975 4.406521320343018
Loss :  1.6240040063858032 2.757934808731079 4.381938934326172
  batch 20 loss: 1.6240040063858032, 2.757934808731079, 4.381938934326172
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.659278392791748 3.347841501235962 5.007120132446289
Loss :  1.6750051975250244 2.70495867729187 4.3799638748168945
Loss :  1.646384596824646 3.384263277053833 5.0306477546691895
Loss :  1.6873887777328491 2.777981758117676 4.4653706550598145
Loss :  1.6848359107971191 3.2628180980682373 4.947653770446777
Loss :  1.6484326124191284 3.3676552772521973 5.016088008880615
Loss :  1.6973592042922974 3.2866287231445312 4.983987808227539
Loss :  1.6398870944976807 2.9825544357299805 4.622441291809082
Loss :  1.6880662441253662 2.772907257080078 4.460973739624023
Loss :  1.6413967609405518 2.7686567306518555 4.410053253173828
Loss :  1.722658395767212 3.6687893867492676 5.391448020935059
Loss :  1.6690595149993896 3.1546638011932373 4.823723316192627
Loss :  1.6544101238250732 3.031294345855713 4.685704231262207
Loss :  1.6568323373794556 2.8406448364257812 4.497477054595947
Loss :  1.6967493295669556 3.5580055713653564 5.254755020141602
Loss :  1.6888582706451416 2.720332622528076 4.409191131591797
Loss :  1.6665011644363403 3.08243465423584 4.748935699462891
Loss :  1.634323239326477 2.925337314605713 4.5596604347229
Loss :  1.6596711874008179 2.873704433441162 4.5333757400512695
Loss :  1.6521745920181274 3.318532705307007 4.970707416534424
  batch 40 loss: 1.6521745920181274, 3.318532705307007, 4.970707416534424
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602747440338135 3.288313627243042 4.9485883712768555
Loss :  1.6485251188278198 2.5266993045806885 4.175224304199219
Loss :  1.6654592752456665 3.0875296592712402 4.752988815307617
Loss :  1.6637732982635498 3.0488626956939697 4.7126359939575195
Loss :  1.6666899919509888 2.739902973175049 4.406592845916748
Loss :  1.6581085920333862 3.0057952404022217 4.663903713226318
Loss :  1.6451458930969238 3.0158796310424805 4.661025524139404
Loss :  1.6573107242584229 2.7085494995117188 4.3658599853515625
Loss :  1.630139708518982 3.2676212787628174 4.89776086807251
Loss :  1.681260585784912 3.0903396606445312 4.771600246429443
Loss :  1.6458388566970825 2.727607250213623 4.373445987701416
Loss :  1.6641303300857544 3.2392780780792236 4.903408527374268
Loss :  1.6826682090759277 3.1151654720306396 4.797833442687988
Loss :  1.667402744293213 2.917712450027466 4.585115432739258
Loss :  1.6706769466400146 3.3746178150177 5.045294761657715
Loss :  1.6349458694458008 2.6777000427246094 4.31264591217041
Loss :  1.6846317052841187 3.281226396560669 4.965857982635498
Loss :  1.682206630706787 3.148470401763916 4.830677032470703
Loss :  1.6956168413162231 3.3422629833221436 5.037879943847656
Loss :  1.670906901359558 2.958684206008911 4.62959098815918
  batch 60 loss: 1.670906901359558, 2.958684206008911, 4.62959098815918
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726101636886597 3.396692991256714 5.069303035736084
Loss :  1.6693724393844604 2.8402438163757324 4.509616374969482
Loss :  1.677416443824768 3.0443830490112305 4.721799373626709
Loss :  1.6580127477645874 2.9264683723449707 4.584481239318848
Loss :  1.65505051612854 2.965470314025879 4.62052059173584
Loss :  5.484057903289795 4.426680088043213 9.910737991333008
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.3741607666015625 4.390597343444824 9.764758110046387
Loss :  5.482937335968018 4.27447509765625 9.75741195678711
Loss :  4.5628886222839355 4.094821929931641 8.657711029052734
Total LOSS train 4.681843478863056 valid 9.52265477180481
CE LOSS train 1.6640659809112548 valid 1.1407221555709839
Contrastive LOSS train 3.0177775126237134 valid 1.0237054824829102
EPOCH 253:
Loss :  1.6504132747650146 2.427097797393799 4.077510833740234
Loss :  1.6662567853927612 3.419813871383667 5.086070537567139
Loss :  1.6520999670028687 2.825953722000122 4.478053569793701
Loss :  1.655251145362854 2.8531370162963867 4.508388042449951
Loss :  1.6799167394638062 3.4023048877716064 5.082221508026123
Loss :  1.6628707647323608 2.7542519569396973 4.417122840881348
Loss :  1.6610568761825562 3.440222978591919 5.1012797355651855
Loss :  1.6499290466308594 2.669313669204712 4.319242477416992
Loss :  1.6545780897140503 2.265983819961548 3.9205617904663086
Loss :  1.6090443134307861 2.942857027053833 4.551901340484619
Loss :  1.6686604022979736 3.2658212184906006 4.934481620788574
Loss :  1.729201078414917 2.951118230819702 4.680319309234619
Loss :  1.6761900186538696 3.221416473388672 4.897606372833252
Loss :  1.6644073724746704 3.402308702468872 5.066716194152832
Loss :  1.6425845623016357 3.0490729808807373 4.691657543182373
Loss :  1.6521601676940918 3.0462687015533447 4.698429107666016
Loss :  1.66114342212677 2.757056474685669 4.4182000160217285
Loss :  1.659528374671936 2.5969109535217285 4.256439208984375
Loss :  1.6675443649291992 2.9009673595428467 4.568511962890625
Loss :  1.624003291130066 2.934866428375244 4.5588698387146
  batch 20 loss: 1.624003291130066, 2.934866428375244, 4.5588698387146
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.659278392791748 3.1732518672943115 4.8325300216674805
Loss :  1.675005316734314 2.85585880279541 4.530864238739014
Loss :  1.6463847160339355 3.120335102081299 4.766719818115234
Loss :  1.6873869895935059 2.9911177158355713 4.678504943847656
Loss :  1.6848360300064087 3.618377208709717 5.303213119506836
Loss :  1.6484317779541016 3.1876327991485596 4.836064338684082
Loss :  1.6973596811294556 3.0471601486206055 4.7445197105407715
Loss :  1.6398873329162598 2.84273362159729 4.482621192932129
Loss :  1.6880662441253662 2.7838189601898193 4.4718852043151855
Loss :  1.6413956880569458 2.8764615058898926 4.517857074737549
Loss :  1.7226588726043701 3.2020246982574463 4.924683570861816
Loss :  1.669061303138733 3.2611515522003174 4.93021297454834
Loss :  1.6544100046157837 3.4210474491119385 5.075457572937012
Loss :  1.656831979751587 2.814502000808716 4.471333980560303
Loss :  1.6967507600784302 3.5510871410369873 5.247838020324707
Loss :  1.6888585090637207 2.9084508419036865 4.597309112548828
Loss :  1.6665006875991821 2.857642889022827 4.524143695831299
Loss :  1.6343244314193726 3.03739595413208 4.671720504760742
Loss :  1.6596715450286865 3.1277999877929688 4.787471771240234
Loss :  1.6521745920181274 3.115340232849121 4.767514705657959
  batch 40 loss: 1.6521745920181274, 3.115340232849121, 4.767514705657959
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602751016616821 3.52329158782959 5.183566570281982
Loss :  1.6485249996185303 2.6644210815429688 4.312946319580078
Loss :  1.6654592752456665 3.260464906692505 4.925924301147461
Loss :  1.663771152496338 2.913832664489746 4.577603816986084
Loss :  1.6666899919509888 2.67209792137146 4.338788032531738
Loss :  1.6581082344055176 2.9433329105377197 4.601441383361816
Loss :  1.6451454162597656 3.2040960788726807 4.849241256713867
Loss :  1.6573102474212646 2.6175172328948975 4.274827480316162
Loss :  1.6301393508911133 3.2795281410217285 4.909667491912842
Loss :  1.681260585784912 3.0142037868499756 4.695464134216309
Loss :  1.6458383798599243 2.8115134239196777 4.4573516845703125
Loss :  1.6641298532485962 3.024932384490967 4.689062118530273
Loss :  1.6826674938201904 3.6083216667175293 5.290988922119141
Loss :  1.6674022674560547 3.115379571914673 4.782781600952148
Loss :  1.6706758737564087 3.2794554233551025 4.950131416320801
Loss :  1.6349443197250366 2.7505152225494385 4.3854594230651855
Loss :  1.6846312284469604 3.454998016357422 5.139629364013672
Loss :  1.6822071075439453 3.185551881790161 4.867758750915527
Loss :  1.695615530014038 3.2893967628479004 4.985012054443359
Loss :  1.670906662940979 2.9088735580444336 4.579780101776123
  batch 60 loss: 1.670906662940979, 2.9088735580444336, 4.579780101776123
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726100444793701 3.3576979637145996 5.030307769775391
Loss :  1.6693730354309082 2.59121036529541 4.260583400726318
Loss :  1.6774154901504517 3.436941623687744 5.114356994628906
Loss :  1.6580127477645874 2.7464406490325928 4.404453277587891
Loss :  1.655052661895752 2.9836275577545166 4.638680458068848
Loss :  5.483886241912842 4.3428568840026855 9.826743125915527
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.374048709869385 4.344153881072998 9.718202590942383
Loss :  5.482805252075195 4.302878379821777 9.785683631896973
Loss :  4.562838077545166 4.247331619262695 8.810169219970703
Total LOSS train 4.703413193042461 valid 9.535199642181396
CE LOSS train 1.6640658763738778 valid 1.1407095193862915
Contrastive LOSS train 3.039347340510442 valid 1.0618329048156738
EPOCH 254:
Loss :  1.6504130363464355 2.6573355197906494 4.307748794555664
Loss :  1.666256070137024 3.2298710346221924 4.896127223968506
Loss :  1.652099370956421 2.864741325378418 4.516840934753418
Loss :  1.6552512645721436 2.9884214401245117 4.643672943115234
Loss :  1.6799161434173584 3.6112444400787354 5.291160583496094
Loss :  1.6628715991973877 2.9759912490844727 4.638862609863281
Loss :  1.661056637763977 3.3553690910339355 5.016425609588623
Loss :  1.6499288082122803 2.906515598297119 4.55644416809082
Loss :  1.6545774936676025 2.219193935394287 3.8737714290618896
Loss :  1.609044075012207 2.705216884613037 4.314260959625244
Loss :  1.6686605215072632 3.470947742462158 5.139608383178711
Loss :  1.7292009592056274 2.824486494064331 4.553687572479248
Loss :  1.676192283630371 3.0112080574035645 4.6874003410339355
Loss :  1.66440749168396 3.475409984588623 5.139817237854004
Loss :  1.6425837278366089 2.879201650619507 4.521785259246826
Loss :  1.6521596908569336 2.788424253463745 4.440584182739258
Loss :  1.66114342212677 2.77081561088562 4.43195915222168
Loss :  1.6595287322998047 2.5459201335906982 4.205449104309082
Loss :  1.6675447225570679 2.9954025745391846 4.662947177886963
Loss :  1.624003291130066 3.2387351989746094 4.862738609313965
  batch 20 loss: 1.624003291130066, 3.2387351989746094, 4.862738609313965
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592785120010376 3.0061426162719727 4.665421009063721
Loss :  1.675005316734314 2.5975029468536377 4.272508144378662
Loss :  1.6463844776153564 3.210660696029663 4.8570451736450195
Loss :  1.6873866319656372 3.00447678565979 4.691863536834717
Loss :  1.684836745262146 3.37532377243042 5.0601606369018555
Loss :  1.648431420326233 3.4104530811309814 5.058884620666504
Loss :  1.6973600387573242 3.0987205505371094 4.796080589294434
Loss :  1.639886498451233 3.0269601345062256 4.666846752166748
Loss :  1.6880662441253662 2.8443732261657715 4.532439231872559
Loss :  1.641396164894104 2.878434658050537 4.519830703735352
Loss :  1.722658634185791 3.1900365352630615 4.912694931030273
Loss :  1.6690620183944702 3.2664220333099365 4.935483932495117
Loss :  1.6544095277786255 3.086021661758423 4.740431308746338
Loss :  1.6568315029144287 2.977984666824341 4.6348161697387695
Loss :  1.6967504024505615 3.3005764484405518 4.997326850891113
Loss :  1.6888586282730103 2.741485595703125 4.430344104766846
Loss :  1.666500210762024 3.003413438796997 4.6699137687683105
Loss :  1.6343246698379517 2.9689300060272217 4.603254795074463
Loss :  1.6596713066101074 2.7659006118774414 4.425571918487549
Loss :  1.6521742343902588 3.2256622314453125 4.877836227416992
  batch 40 loss: 1.6521742343902588, 3.2256622314453125, 4.877836227416992
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602753400802612 3.303342580795288 4.96361780166626
Loss :  1.6485240459442139 2.3395798206329346 3.9881038665771484
Loss :  1.6654585599899292 3.387927532196045 5.053386211395264
Loss :  1.6637699604034424 3.0921244621276855 4.755894660949707
Loss :  1.6666895151138306 2.6868393421173096 4.35352897644043
Loss :  1.6581082344055176 2.9850497245788574 4.643157958984375
Loss :  1.6451443433761597 3.1629300117492676 4.808074474334717
Loss :  1.657309889793396 2.7073874473571777 4.364697456359863
Loss :  1.6301389932632446 3.2035155296325684 4.833654403686523
Loss :  1.6812604665756226 2.8211662769317627 4.502426624298096
Loss :  1.6458379030227661 2.7457430362701416 4.391581058502197
Loss :  1.6641297340393066 3.1825811862945557 4.846711158752441
Loss :  1.6826670169830322 3.5903711318969727 5.273037910461426
Loss :  1.667401909828186 2.9797236919403076 4.647125720977783
Loss :  1.670676350593567 3.2385644912719727 4.90924072265625
Loss :  1.6349437236785889 2.621967077255249 4.256910800933838
Loss :  1.6846307516098022 3.4435722827911377 5.12820291519165
Loss :  1.682207703590393 3.214110851287842 4.896318435668945
Loss :  1.6956151723861694 3.3320834636688232 5.027698516845703
Loss :  1.6709059476852417 2.728496551513672 4.399402618408203
  batch 60 loss: 1.6709059476852417, 2.728496551513672, 4.399402618408203
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726102828979492 3.435793161392212 5.108403205871582
Loss :  1.669372797012329 2.866342306137085 4.535715103149414
Loss :  1.677415132522583 3.3637847900390625 5.041199684143066
Loss :  1.6580127477645874 3.0048153400421143 4.662827968597412
Loss :  1.6550469398498535 2.8877627849578857 4.54280948638916
Loss :  5.483851909637451 4.3970160484313965 9.880867958068848
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.3740315437316895 4.369201183319092 9.743232727050781
Loss :  5.482792854309082 4.219282150268555 9.702075004577637
Loss :  4.562690258026123 4.214103698730469 8.77679443359375
Total LOSS train 4.692058068055373 valid 9.525742530822754
CE LOSS train 1.6640656306193424 valid 1.1406725645065308
Contrastive LOSS train 3.0279924429379976 valid 1.0535259246826172
EPOCH 255:
Loss :  1.65041184425354 2.5433199405670166 4.193731784820557
Loss :  1.666255235671997 3.278877019882202 4.945132255554199
Loss :  1.6520991325378418 2.675520658493042 4.327619552612305
Loss :  1.6552517414093018 2.708029270172119 4.36328125
Loss :  1.679915428161621 3.262589454650879 4.9425048828125
Loss :  1.6628719568252563 2.7354509830474854 4.398323059082031
Loss :  1.6610561609268188 3.4102368354797363 5.071292877197266
Loss :  1.6499284505844116 2.639988660812378 4.2899169921875
Loss :  1.654577612876892 2.2231955528259277 3.8777732849121094
Loss :  1.609044075012207 3.0110366344451904 4.620080947875977
Loss :  1.668660283088684 3.3654680252075195 5.034128189086914
Loss :  1.7292009592056274 2.9443156719207764 4.673516750335693
Loss :  1.6761925220489502 3.2395710945129395 4.915763854980469
Loss :  1.6644083261489868 3.3104283809661865 4.974836826324463
Loss :  1.6425836086273193 2.8528504371643066 4.495433807373047
Loss :  1.6521594524383545 3.1413567066192627 4.793516159057617
Loss :  1.66114342212677 2.9335391521453857 4.594682693481445
Loss :  1.6595299243927002 2.701732873916626 4.361262798309326
Loss :  1.6675463914871216 2.8369061946868896 4.504452705383301
Loss :  1.624003291130066 2.940814733505249 4.564817905426025
  batch 20 loss: 1.624003291130066, 2.940814733505249, 4.564817905426025
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592780351638794 3.0953898429870605 4.75466775894165
Loss :  1.6750054359436035 2.6408820152282715 4.315887451171875
Loss :  1.6463838815689087 3.3410558700561523 4.9874396324157715
Loss :  1.687386393547058 2.991664409637451 4.679050922393799
Loss :  1.684836983680725 3.3621749877929688 5.047011852264404
Loss :  1.6484298706054688 3.4473657608032227 5.095795631408691
Loss :  1.6973599195480347 3.1735050678253174 4.8708648681640625
Loss :  1.6398873329162598 3.0212152004241943 4.661102294921875
Loss :  1.6880664825439453 2.874404191970825 4.562470436096191
Loss :  1.641395092010498 2.8310396671295166 4.472434997558594
Loss :  1.722659707069397 3.0906753540039062 4.813334941864014
Loss :  1.669062852859497 3.1145668029785156 4.783629417419434
Loss :  1.6544102430343628 3.148582696914673 4.802992820739746
Loss :  1.6568316221237183 2.786010980606079 4.442842483520508
Loss :  1.6967498064041138 3.453932523727417 5.15068244934082
Loss :  1.688859224319458 2.9256951808929443 4.614554405212402
Loss :  1.6664996147155762 3.0341880321502686 4.700687408447266
Loss :  1.6343261003494263 3.0951292514801025 4.729455471038818
Loss :  1.659671664237976 3.0186541080474854 4.678325653076172
Loss :  1.652174472808838 3.1430861949920654 4.795260429382324
  batch 40 loss: 1.652174472808838, 3.1430861949920654, 4.795260429382324
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.660276174545288 3.525271415710449 5.185547828674316
Loss :  1.6485244035720825 2.710369825363159 4.358894348144531
Loss :  1.6654586791992188 3.173428773880005 4.8388872146606445
Loss :  1.6637705564498901 3.213320255279541 4.877090930938721
Loss :  1.6666898727416992 2.7924702167510986 4.459159851074219
Loss :  1.6581079959869385 2.9898457527160645 4.647953987121582
Loss :  1.645145058631897 3.1533920764923096 4.798537254333496
Loss :  1.6573097705841064 2.798973560333252 4.4562835693359375
Loss :  1.6301382780075073 3.470116376876831 5.100254535675049
Loss :  1.6812604665756226 3.06681752204895 4.748077869415283
Loss :  1.6458386182785034 2.771658420562744 4.417497158050537
Loss :  1.6641310453414917 3.011014699935913 4.675145626068115
Loss :  1.6826677322387695 3.2600033283233643 4.942670822143555
Loss :  1.6674036979675293 2.927396535873413 4.594799995422363
Loss :  1.6706770658493042 3.3688924312591553 5.03956937789917
Loss :  1.6349449157714844 2.8683996200561523 4.503344535827637
Loss :  1.6846306324005127 3.3896501064300537 5.074280738830566
Loss :  1.682208776473999 3.2371652126312256 4.919373989105225
Loss :  1.6956170797348022 3.4997048377990723 5.195322036743164
Loss :  1.6709064245224 2.9162726402282715 4.587179183959961
  batch 60 loss: 1.6709064245224, 2.9162726402282715, 4.587179183959961
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726096868515015 3.080793857574463 4.753403663635254
Loss :  1.6693730354309082 2.6100287437438965 4.279401779174805
Loss :  1.677415370941162 3.2897300720214844 4.9671454429626465
Loss :  1.6580138206481934 3.0068376064300537 4.664851188659668
Loss :  1.655045747756958 3.0780701637268066 4.733116149902344
Loss :  5.483891010284424 4.433332920074463 9.917223930358887
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.37412166595459 4.373660564422607 9.747781753540039
Loss :  5.482850551605225 4.2545247077941895 9.737375259399414
Loss :  4.562811851501465 4.283863544464111 8.846675872802734
Total LOSS train 4.703359215076153 valid 9.562264204025269
CE LOSS train 1.6640658378601074 valid 1.1407029628753662
Contrastive LOSS train 3.0392933918879583 valid 1.0709658861160278
EPOCH 256:
Loss :  1.650412917137146 2.7218117713928223 4.372224807739258
Loss :  1.6662553548812866 3.164522409439087 4.830777645111084
Loss :  1.6520999670028687 2.8179678916931152 4.470067977905273
Loss :  1.6552512645721436 2.879826545715332 4.535078048706055
Loss :  1.6799145936965942 3.305203676223755 4.985118389129639
Loss :  1.6628711223602295 2.9559128284454346 4.618783950805664
Loss :  1.661055564880371 3.318894147872925 4.979949951171875
Loss :  1.6499279737472534 2.511688709259033 4.161616802215576
Loss :  1.6545771360397339 2.1724331378936768 3.827010154724121
Loss :  1.6090441942214966 2.7541239261627197 4.363168239593506
Loss :  1.6686607599258423 3.470139741897583 5.138800621032715
Loss :  1.7292016744613647 2.9832143783569336 4.712416172027588
Loss :  1.6761966943740845 3.233351707458496 4.909548282623291
Loss :  1.6644080877304077 3.362006902694702 5.02641487121582
Loss :  1.6425837278366089 3.1355676651000977 4.778151512145996
Loss :  1.6521589756011963 3.108879804611206 4.761038780212402
Loss :  1.6611437797546387 2.764321804046631 4.4254655838012695
Loss :  1.6595312356948853 2.700347900390625 4.359879016876221
Loss :  1.667547583580017 2.9031760692596436 4.570723533630371
Loss :  1.6240028142929077 2.9495131969451904 4.573515892028809
  batch 20 loss: 1.6240028142929077, 2.9495131969451904, 4.573515892028809
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.659277319908142 3.0052826404571533 4.664559841156006
Loss :  1.6750047206878662 2.5505011081695557 4.225505828857422
Loss :  1.6463837623596191 3.3928091526031494 5.039193153381348
Loss :  1.6873856782913208 3.1165969371795654 4.803982734680176
Loss :  1.6848371028900146 3.381270170211792 5.066107273101807
Loss :  1.6484301090240479 3.098736524581909 4.747166633605957
Loss :  1.6973598003387451 3.1419079303741455 4.839267730712891
Loss :  1.6398873329162598 2.7667534351348877 4.406641006469727
Loss :  1.688066840171814 2.6910784244537354 4.37914514541626
Loss :  1.6413949728012085 3.0894954204559326 4.730890274047852
Loss :  1.722659945487976 3.3751637935638428 5.097823619842529
Loss :  1.6690634489059448 3.0426833629608154 4.711746692657471
Loss :  1.6544111967086792 3.248563051223755 4.9029741287231445
Loss :  1.6568323373794556 2.942401647567749 4.599234104156494
Loss :  1.6967504024505615 3.368863582611084 5.065613746643066
Loss :  1.6888601779937744 2.8087103366851807 4.497570514678955
Loss :  1.666499376296997 3.03597354888916 4.702472686767578
Loss :  1.6343257427215576 3.0683248043060303 4.702650547027588
Loss :  1.659672737121582 2.997252941131592 4.656925678253174
Loss :  1.6521748304367065 3.3463327884674072 4.998507499694824
  batch 40 loss: 1.6521748304367065, 3.3463327884674072, 4.998507499694824
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602766513824463 3.387082576751709 5.047359466552734
Loss :  1.6485241651535034 2.3953559398651123 4.043879985809326
Loss :  1.6654585599899292 3.290846824645996 4.956305503845215
Loss :  1.6637704372406006 3.0277912616729736 4.691561698913574
Loss :  1.6666892766952515 2.7570881843566895 4.4237775802612305
Loss :  1.6581082344055176 3.11012864112854 4.768237113952637
Loss :  1.6451467275619507 3.1523141860961914 4.797461032867432
Loss :  1.6573095321655273 3.0229711532592773 4.680280685424805
Loss :  1.6301389932632446 3.481078863143921 5.111217975616455
Loss :  1.6812610626220703 3.0052480697631836 4.686509132385254
Loss :  1.6458392143249512 2.778714895248413 4.424553871154785
Loss :  1.6641310453414917 2.9756357669830322 4.639766693115234
Loss :  1.6826684474945068 3.6202821731567383 5.302950859069824
Loss :  1.6674047708511353 2.924304246902466 4.591709136962891
Loss :  1.6706770658493042 3.4312703609466553 5.10194730758667
Loss :  1.634945034980774 2.804328441619873 4.439273357391357
Loss :  1.6846305131912231 3.2648861408233643 4.949516773223877
Loss :  1.6822086572647095 3.1754634380340576 4.857672214508057
Loss :  1.6956186294555664 3.4001033306121826 5.095722198486328
Loss :  1.6709061861038208 3.0057907104492188 4.67669677734375
  batch 60 loss: 1.6709061861038208, 3.0057907104492188, 4.67669677734375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726094484329224 3.0710904598236084 4.74370002746582
Loss :  1.6693735122680664 2.613009452819824 4.282382965087891
Loss :  1.6774171590805054 3.1539435386657715 4.831360816955566
Loss :  1.6580145359039307 2.947340488433838 4.605355262756348
Loss :  1.6550474166870117 2.8663415908813477 4.521389007568359
Loss :  5.483933448791504 4.447115898132324 9.931049346923828
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.374120235443115 4.35943603515625 9.733556747436523
Loss :  5.482878684997559 4.3018999099731445 9.784778594970703
Loss :  4.56297492980957 4.278043270111084 8.841018676757812
Total LOSS train 4.70012794641348 valid 9.572600841522217
CE LOSS train 1.6640661312983587 valid 1.1407437324523926
Contrastive LOSS train 3.036061793107253 valid 1.069510817527771
EPOCH 257:
Loss :  1.6504156589508057 2.705518960952759 4.3559346199035645
Loss :  1.6662575006484985 3.3911244869232178 5.057382106781006
Loss :  1.652099370956421 2.6933109760284424 4.345410346984863
Loss :  1.6552507877349854 3.020656108856201 4.675907135009766
Loss :  1.679916262626648 3.4840190410614014 5.16393518447876
Loss :  1.6628706455230713 2.7981209754943848 4.460991859436035
Loss :  1.661056637763977 3.219439744949341 4.880496501922607
Loss :  1.6499279737472534 2.57063364982605 4.220561504364014
Loss :  1.6545779705047607 2.186588764190674 3.8411667346954346
Loss :  1.6090433597564697 2.9200069904327393 4.529050350189209
Loss :  1.668661117553711 3.421175956726074 5.089837074279785
Loss :  1.7292048931121826 2.922091007232666 4.6512956619262695
Loss :  1.6761987209320068 3.316131114959717 4.9923295974731445
Loss :  1.6644083261489868 3.2880923748016357 4.952500820159912
Loss :  1.6425857543945312 3.048583507537842 4.691169261932373
Loss :  1.6521598100662231 3.110361337661743 4.762521266937256
Loss :  1.661144733428955 3.0048820972442627 4.666027069091797
Loss :  1.6595300436019897 2.7926487922668457 4.452178955078125
Loss :  1.6675482988357544 2.9588422775268555 4.62639045715332
Loss :  1.6240040063858032 3.07771635055542 4.701720237731934
  batch 20 loss: 1.6240040063858032, 3.07771635055542, 4.701720237731934
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592769622802734 3.2200427055358887 4.879319667816162
Loss :  1.675004005432129 2.5253183841705322 4.200322151184082
Loss :  1.646383285522461 3.325366973876953 4.971750259399414
Loss :  1.6873871088027954 2.998587131500244 4.68597412109375
Loss :  1.6848368644714355 3.484830141067505 5.1696672439575195
Loss :  1.6484299898147583 3.3430871963500977 4.991517066955566
Loss :  1.6973599195480347 3.0575830936431885 4.754942893981934
Loss :  1.6398875713348389 2.8083794116973877 4.448266983032227
Loss :  1.6880666017532349 2.853198528289795 4.54126501083374
Loss :  1.6413944959640503 2.7776424884796143 4.419036865234375
Loss :  1.7226594686508179 3.3271119594573975 5.049771308898926
Loss :  1.6690622568130493 3.164862632751465 4.833924770355225
Loss :  1.654412865638733 2.9336252212524414 4.588037967681885
Loss :  1.6568324565887451 2.828259229660034 4.485091686248779
Loss :  1.6967488527297974 3.432326555252075 5.129075527191162
Loss :  1.688860297203064 2.7662463188171387 4.455106735229492
Loss :  1.666499376296997 2.963578224182129 4.630077362060547
Loss :  1.6343255043029785 2.9010961055755615 4.535421371459961
Loss :  1.6596734523773193 2.892918825149536 4.5525922775268555
Loss :  1.6521750688552856 3.321934223175049 4.974109172821045
  batch 40 loss: 1.6521750688552856, 3.321934223175049, 4.974109172821045
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602764129638672 3.2823684215545654 4.942645072937012
Loss :  1.6485246419906616 2.496891736984253 4.145416259765625
Loss :  1.6654589176177979 3.1417579650878906 4.807216644287109
Loss :  1.6637732982635498 3.1165895462036133 4.780363082885742
Loss :  1.6666901111602783 2.8444888591766357 4.511178970336914
Loss :  1.6581069231033325 3.055030584335327 4.713137626647949
Loss :  1.645147681236267 3.152097225189209 4.797245025634766
Loss :  1.6573102474212646 2.7009923458099365 4.358302593231201
Loss :  1.6301382780075073 3.4425265789031982 5.072664737701416
Loss :  1.6812602281570435 3.2722651958465576 4.953525543212891
Loss :  1.645840048789978 2.578373908996582 4.22421407699585
Loss :  1.6641312837600708 3.0321860313415527 4.696317195892334
Loss :  1.6826688051223755 3.330738067626953 5.013406753540039
Loss :  1.6674079895019531 2.9124722480773926 4.579880237579346
Loss :  1.670676589012146 3.423211097717285 5.093887805938721
Loss :  1.6349469423294067 2.701179027557373 4.33612585067749
Loss :  1.6846307516098022 3.3514084815979004 5.036039352416992
Loss :  1.682209849357605 3.0479366779327393 4.730146408081055
Loss :  1.6956194639205933 3.1954452991485596 4.891064643859863
Loss :  1.6709067821502686 2.972806453704834 4.643712997436523
  batch 60 loss: 1.6709067821502686, 2.972806453704834, 4.643712997436523
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726093292236328 3.2243664264678955 4.896975517272949
Loss :  1.6693731546401978 2.771324634552002 4.44069766998291
Loss :  1.6774189472198486 3.094660997390747 4.772079944610596
Loss :  1.6580153703689575 2.821528196334839 4.479543685913086
Loss :  1.6550475358963013 3.3095028400421143 4.964550495147705
Loss :  5.484035015106201 4.397667407989502 9.881702423095703
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.374303817749023 4.351000785827637 9.72530460357666
Loss :  5.483034610748291 4.259528160095215 9.742563247680664
Loss :  4.562958240509033 4.3219380378723145 8.884896278381348
Total LOSS train 4.696806390468891 valid 9.558616638183594
CE LOSS train 1.6640665824596699 valid 1.1407395601272583
Contrastive LOSS train 3.0327398263491117 valid 1.0804845094680786
EPOCH 258:
Loss :  1.6504173278808594 2.729478120803833 4.379895210266113
Loss :  1.666257619857788 3.1094810962677 4.775738716125488
Loss :  1.6521013975143433 2.8513123989105225 4.503413677215576
Loss :  1.655252456665039 2.905258893966675 4.560511589050293
Loss :  1.6799159049987793 3.0921671390533447 4.772083282470703
Loss :  1.6628715991973877 2.89711594581604 4.559987545013428
Loss :  1.661055564880371 3.3868370056152344 5.0478925704956055
Loss :  1.6499274969100952 2.764016628265381 4.413944244384766
Loss :  1.654577612876892 2.2486732006073 3.9032506942749023
Loss :  1.6090426445007324 2.524505138397217 4.133547782897949
Loss :  1.6686617136001587 3.393221378326416 5.061882972717285
Loss :  1.7292057275772095 2.936837911605835 4.666043758392334
Loss :  1.6762025356292725 3.220423460006714 4.896625995635986
Loss :  1.6644073724746704 3.393599033355713 5.058006286621094
Loss :  1.6425849199295044 3.1657705307006836 4.808355331420898
Loss :  1.6521594524383545 3.0882017612457275 4.740361213684082
Loss :  1.6611449718475342 2.906806707382202 4.567951679229736
Loss :  1.6595327854156494 2.557595729827881 4.217128753662109
Loss :  1.6675498485565186 2.7479538917541504 4.41550350189209
Loss :  1.6240030527114868 3.1679930686950684 4.791996002197266
  batch 20 loss: 1.6240030527114868, 3.1679930686950684, 4.791996002197266
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592762470245361 2.997760534286499 4.657036781311035
Loss :  1.6750036478042603 2.521608591079712 4.196612358093262
Loss :  1.646382212638855 3.1706247329711914 4.817007064819336
Loss :  1.6873865127563477 2.9219632148742676 4.609349727630615
Loss :  1.6848381757736206 3.4174675941467285 5.102305889129639
Loss :  1.6484307050704956 3.2165191173553467 4.864949703216553
Loss :  1.6973594427108765 3.1007325649261475 4.798091888427734
Loss :  1.6398875713348389 2.8564491271972656 4.496336936950684
Loss :  1.6880669593811035 2.843997001647949 4.532063961029053
Loss :  1.6413941383361816 2.7528231143951416 4.394217491149902
Loss :  1.7226593494415283 3.142382860183716 4.865042209625244
Loss :  1.6690642833709717 3.4092185497283936 5.078282833099365
Loss :  1.6544142961502075 3.4260897636413574 5.080503940582275
Loss :  1.6568323373794556 2.698136329650879 4.354968547821045
Loss :  1.6967508792877197 3.647374153137207 5.344124794006348
Loss :  1.6888610124588013 2.815415620803833 4.504276752471924
Loss :  1.666499137878418 2.9228146076202393 4.589313507080078
Loss :  1.6343276500701904 2.8661599159240723 4.500487327575684
Loss :  1.6596739292144775 2.860793113708496 4.5204668045043945
Loss :  1.6521741151809692 3.2734293937683105 4.92560338973999
  batch 40 loss: 1.6521741151809692, 3.2734293937683105, 4.92560338973999
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602778434753418 3.412008762359619 5.072286605834961
Loss :  1.6485260725021362 2.5354859828948975 4.184011936187744
Loss :  1.6654595136642456 3.1827590465545654 4.8482184410095215
Loss :  1.663773775100708 2.8274614810943604 4.491235256195068
Loss :  1.6666927337646484 2.7370493412017822 4.403741836547852
Loss :  1.658107042312622 3.0759596824645996 4.734066963195801
Loss :  1.6451480388641357 3.0601696968078613 4.705317497253418
Loss :  1.657312273979187 2.7925541400909424 4.44986629486084
Loss :  1.6301382780075073 3.599853038787842 5.229991436004639
Loss :  1.6812626123428345 3.0839123725891113 4.765174865722656
Loss :  1.6458395719528198 2.586238145828247 4.232077598571777
Loss :  1.6641302108764648 2.9152777194976807 4.579407691955566
Loss :  1.6826682090759277 3.522364377975464 5.2050323486328125
Loss :  1.6674093008041382 2.940690279006958 4.608099460601807
Loss :  1.6706753969192505 3.5763282775878906 5.247003555297852
Loss :  1.634944200515747 2.673945665359497 4.308889865875244
Loss :  1.6846288442611694 3.30703067779541 4.991659641265869
Loss :  1.6822112798690796 3.250223398208618 4.932434558868408
Loss :  1.6956168413162231 3.270352363586426 4.965969085693359
Loss :  1.6709063053131104 2.7983267307281494 4.46923303604126
  batch 60 loss: 1.6709063053131104, 2.7983267307281494, 4.46923303604126
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.672609806060791 3.3093361854553223 4.981945991516113
Loss :  1.6693750619888306 2.666135549545288 4.335510730743408
Loss :  1.6774206161499023 3.1814417839050293 4.858862400054932
Loss :  1.658015251159668 2.857581853866577 4.515597343444824
Loss :  1.655050277709961 2.6122405529022217 4.267291069030762
Loss :  5.4842400550842285 4.362301349639893 9.846541404724121
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.3744940757751465 4.377871990203857 9.752366065979004
Loss :  5.483203887939453 4.316950798034668 9.800154685974121
Loss :  4.563228607177734 4.226247310638428 8.78947639465332
Total LOSS train 4.675201357327975 valid 9.547134637832642
CE LOSS train 1.6640669841032762 valid 1.1408071517944336
Contrastive LOSS train 3.0111344007345346 valid 1.056561827659607
EPOCH 259:
Loss :  1.6504182815551758 2.7370359897613525 4.387454032897949
Loss :  1.666257619857788 3.1043577194213867 4.770615577697754
Loss :  1.6521021127700806 2.794858694076538 4.446960926055908
Loss :  1.6552542448043823 3.1242477893829346 4.779501914978027
Loss :  1.6799156665802002 3.5850930213928223 5.265008926391602
Loss :  1.6628726720809937 2.88287615776062 4.545748710632324
Loss :  1.661054015159607 3.075371265411377 4.736425399780273
Loss :  1.6499276161193848 2.722303867340088 4.372231483459473
Loss :  1.654578685760498 2.3309991359710693 3.9855778217315674
Loss :  1.6090419292449951 2.6037778854370117 4.212820053100586
Loss :  1.6686619520187378 3.3034486770629883 4.972110748291016
Loss :  1.729207158088684 3.04288911819458 4.772096157073975
Loss :  1.6762042045593262 3.068066358566284 4.744270324707031
Loss :  1.6644079685211182 3.331303596496582 4.995711326599121
Loss :  1.6425843238830566 3.0358054637908936 4.678389549255371
Loss :  1.6521600484848022 3.0594897270202637 4.7116498947143555
Loss :  1.6611460447311401 3.0239787101745605 4.68512487411499
Loss :  1.6595350503921509 2.669959545135498 4.329494476318359
Loss :  1.6675519943237305 2.9590048789978027 4.626556873321533
Loss :  1.6240042448043823 3.2765703201293945 4.900574684143066
  batch 20 loss: 1.6240042448043823, 3.2765703201293945, 4.900574684143066
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592750549316406 2.8834776878356934 4.542752742767334
Loss :  1.6750047206878662 2.6414895057678223 4.316493988037109
Loss :  1.646381139755249 3.29548716545105 4.941868305206299
Loss :  1.6873877048492432 2.8424429893493652 4.5298309326171875
Loss :  1.6848384141921997 3.2995383739471436 4.984376907348633
Loss :  1.6484302282333374 3.2428154945373535 4.8912458419799805
Loss :  1.6973594427108765 3.3380355834960938 5.03539514541626
Loss :  1.6398875713348389 3.0502142906188965 4.690101623535156
Loss :  1.6880669593811035 2.7811150550842285 4.469182014465332
Loss :  1.6413938999176025 2.898746967315674 4.5401411056518555
Loss :  1.7226591110229492 3.275684356689453 4.998343467712402
Loss :  1.6690648794174194 3.210813045501709 4.879878044128418
Loss :  1.6544134616851807 2.9834702014923096 4.63788366317749
Loss :  1.656831979751587 2.8062350749969482 4.463067054748535
Loss :  1.6967512369155884 3.3760862350463867 5.0728373527526855
Loss :  1.6888610124588013 2.555476427078247 4.244337558746338
Loss :  1.6664987802505493 2.8566384315490723 4.523137092590332
Loss :  1.6343278884887695 2.6893208026885986 4.323648452758789
Loss :  1.6596742868423462 3.071535348892212 4.731209754943848
Loss :  1.6521745920181274 3.073122262954712 4.725296974182129
  batch 40 loss: 1.6521745920181274, 3.073122262954712, 4.725296974182129
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602779626846313 3.4166197776794434 5.076897621154785
Loss :  1.6485258340835571 2.5896573066711426 4.23818302154541
Loss :  1.6654590368270874 3.2679903507232666 4.9334492683410645
Loss :  1.6637747287750244 2.7752442359924316 4.439019203186035
Loss :  1.666691780090332 2.761988639831543 4.428680419921875
Loss :  1.6581066846847534 3.0609660148620605 4.7190728187561035
Loss :  1.6451482772827148 3.20455002784729 4.849698066711426
Loss :  1.6573116779327393 2.7351458072662354 4.392457485198975
Loss :  1.6301385164260864 3.3874776363372803 5.017616271972656
Loss :  1.681261658668518 2.8616573810577393 4.542919158935547
Loss :  1.6458392143249512 2.6301093101501465 4.275948524475098
Loss :  1.6641310453414917 2.6983160972595215 4.362447261810303
Loss :  1.682667851448059 3.3178374767303467 5.000505447387695
Loss :  1.6674102544784546 3.1839399337768555 4.8513503074646
Loss :  1.6706753969192505 3.2515509128570557 4.922226428985596
Loss :  1.6349462270736694 2.7079434394836426 4.342889785766602
Loss :  1.6846296787261963 3.220583438873291 4.905213356018066
Loss :  1.6822105646133423 3.1494593620300293 4.831669807434082
Loss :  1.695617437362671 3.321063280105591 5.016680717468262
Loss :  1.6709065437316895 2.782942056655884 4.453848838806152
  batch 60 loss: 1.6709065437316895, 2.782942056655884, 4.453848838806152
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726086139678955 3.5066559314727783 5.179264545440674
Loss :  1.6693748235702515 2.6441376209259033 4.313512325286865
Loss :  1.677419900894165 3.13582444190979 4.813244342803955
Loss :  1.6580148935317993 2.833760976791382 4.491775989532471
Loss :  1.6550467014312744 2.7628417015075684 4.417888641357422
Loss :  5.484334945678711 4.371754169464111 9.856088638305664
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.374567031860352 4.328190326690674 9.702756881713867
Loss :  5.4832282066345215 4.25974702835083 9.742975234985352
Loss :  4.5632500648498535 4.296171188354492 8.859420776367188
Total LOSS train 4.665781714366033 valid 9.540310382843018
CE LOSS train 1.6640671308224018 valid 1.1408125162124634
Contrastive LOSS train 3.0017145597017727 valid 1.074042797088623
EPOCH 260:
Loss :  1.6504184007644653 2.522294282913208 4.172712802886963
Loss :  1.6662579774856567 3.0519769191741943 4.718235015869141
Loss :  1.6521015167236328 2.755558490753174 4.407660007476807
Loss :  1.6552540063858032 2.9856350421905518 4.6408891677856445
Loss :  1.679915189743042 3.219700574874878 4.89961576461792
Loss :  1.6628721952438354 2.6990792751312256 4.3619513511657715
Loss :  1.6610549688339233 3.522089719772339 5.183144569396973
Loss :  1.6499271392822266 2.887955665588379 4.5378828048706055
Loss :  1.654578685760498 2.189847469329834 3.844426155090332
Loss :  1.609041690826416 2.948806047439575 4.55784797668457
Loss :  1.6686618328094482 3.483097791671753 5.151759624481201
Loss :  1.7292064428329468 2.8728458881378174 4.602052211761475
Loss :  1.6762055158615112 3.209792137145996 4.885997772216797
Loss :  1.6644072532653809 3.154930353164673 4.819337844848633
Loss :  1.6425844430923462 3.3426096439361572 4.985194206237793
Loss :  1.6521590948104858 3.16009521484375 4.812254428863525
Loss :  1.6611456871032715 2.935126543045044 4.5962724685668945
Loss :  1.659534215927124 2.767184019088745 4.426718235015869
Loss :  1.6675519943237305 3.085989236831665 4.753540992736816
Loss :  1.6240034103393555 2.7773001194000244 4.401303291320801
  batch 20 loss: 1.6240034103393555, 2.7773001194000244, 4.401303291320801
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592762470245361 3.1463468074798584 4.8056230545043945
Loss :  1.6750037670135498 2.697730302810669 4.372734069824219
Loss :  1.6463810205459595 3.404343366622925 5.050724506378174
Loss :  1.6873873472213745 2.8347578048706055 4.5221452713012695
Loss :  1.6848384141921997 3.387730598449707 5.072568893432617
Loss :  1.6484309434890747 3.3777554035186768 5.026186466217041
Loss :  1.6973587274551392 3.2907707691192627 4.988129615783691
Loss :  1.6398879289627075 3.152947425842285 4.792835235595703
Loss :  1.6880667209625244 2.749786853790283 4.437853813171387
Loss :  1.6413949728012085 3.363271474838257 5.004666328430176
Loss :  1.7226601839065552 3.4231011867523193 5.145761489868164
Loss :  1.6690647602081299 3.069634199142456 4.738698959350586
Loss :  1.6544134616851807 2.923731565475464 4.5781450271606445
Loss :  1.6568328142166138 2.7581634521484375 4.414996147155762
Loss :  1.6967495679855347 3.399376153945923 5.096125602722168
Loss :  1.6888612508773804 2.95001482963562 4.638875961303711
Loss :  1.6664986610412598 3.004741668701172 4.671240329742432
Loss :  1.6343278884887695 2.828721523284912 4.463049411773682
Loss :  1.6596741676330566 2.8997697830200195 4.559443950653076
Loss :  1.6521751880645752 3.18483567237854 4.837010860443115
  batch 40 loss: 1.6521751880645752, 3.18483567237854, 4.837010860443115
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602773666381836 3.559988260269165 5.2202653884887695
Loss :  1.6485251188278198 2.5383989810943604 4.186923980712891
Loss :  1.6654595136642456 2.9900307655334473 4.655490398406982
Loss :  1.663774013519287 2.778010845184326 4.441784858703613
Loss :  1.6666918992996216 2.806631565093994 4.473323345184326
Loss :  1.6581071615219116 2.941032886505127 4.599140167236328
Loss :  1.6451491117477417 3.229057550430298 4.87420654296875
Loss :  1.6573119163513184 2.8646557331085205 4.521967887878418
Loss :  1.6301385164260864 3.332831859588623 4.96297025680542
Loss :  1.6812634468078613 3.128221035003662 4.809484481811523
Loss :  1.645840048789978 2.6658198833465576 4.311659812927246
Loss :  1.6641312837600708 2.8319668769836426 4.496098041534424
Loss :  1.6826674938201904 3.2623136043548584 4.944981098175049
Loss :  1.6674095392227173 2.9126203060150146 4.5800299644470215
Loss :  1.6706749200820923 3.0511550903320312 4.721829891204834
Loss :  1.634944200515747 2.743070363998413 4.37801456451416
Loss :  1.6846295595169067 3.1592512130737305 4.843880653381348
Loss :  1.68221116065979 2.8872933387756348 4.569504737854004
Loss :  1.6956177949905396 3.23866868019104 4.934286594390869
Loss :  1.6709059476852417 3.103045701980591 4.773951530456543
  batch 60 loss: 1.6709059476852417, 3.103045701980591, 4.773951530456543
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726094484329224 3.4556188583374023 5.128228187561035
Loss :  1.669376015663147 2.6396052837371826 4.308981418609619
Loss :  1.6774219274520874 3.0386059284210205 4.716027736663818
Loss :  1.658015251159668 2.887084722518921 4.545100212097168
Loss :  1.65505051612854 3.073837995529175 4.728888511657715
Loss :  5.484127044677734 4.387838363647461 9.871965408325195
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.374361038208008 4.380551338195801 9.754912376403809
Loss :  5.482997417449951 4.285679817199707 9.7686767578125
Loss :  4.5631585121154785 4.077847480773926 8.641006469726562
Total LOSS train 4.687732799236591 valid 9.509140253067017
CE LOSS train 1.6640672133519099 valid 1.1407896280288696
Contrastive LOSS train 3.0236655785487248 valid 1.0194618701934814
EPOCH 261:
Loss :  1.650418758392334 2.7982940673828125 4.4487128257751465
Loss :  1.6662579774856567 3.3134331703186035 4.979691028594971
Loss :  1.652101755142212 2.660825252532959 4.31292724609375
Loss :  1.6552544832229614 3.117462396621704 4.772716999053955
Loss :  1.6799159049987793 3.3430724143981934 5.022988319396973
Loss :  1.6628718376159668 2.886169195175171 4.549040794372559
Loss :  1.6610560417175293 3.2436740398406982 4.904729843139648
Loss :  1.6499272584915161 2.5315868854522705 4.181514263153076
Loss :  1.6545783281326294 2.240910291671753 3.895488739013672
Loss :  1.6090420484542847 2.8638691902160645 4.472911357879639
Loss :  1.6686625480651855 3.389728307723999 5.0583906173706055
Loss :  1.7292062044143677 3.072944164276123 4.802150249481201
Loss :  1.6762077808380127 3.1812245845794678 4.8574323654174805
Loss :  1.6644084453582764 3.762016773223877 5.426424980163574
Loss :  1.6425840854644775 3.028024196624756 4.6706085205078125
Loss :  1.6521601676940918 3.0461552143096924 4.698315620422363
Loss :  1.6611460447311401 2.9262053966522217 4.587351322174072
Loss :  1.6595338582992554 2.593921661376953 4.253455638885498
Loss :  1.6675517559051514 3.3165979385375977 4.984149932861328
Loss :  1.6240034103393555 2.879098653793335 4.5031023025512695
  batch 20 loss: 1.6240034103393555, 2.879098653793335, 4.5031023025512695
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592766046524048 2.929870843887329 4.589147567749023
Loss :  1.6750041246414185 2.830825090408325 4.505829334259033
Loss :  1.6463819742202759 3.2853732109069824 4.931755065917969
Loss :  1.6873877048492432 3.1094634532928467 4.79685115814209
Loss :  1.6848392486572266 3.489081859588623 5.17392110824585
Loss :  1.6484315395355225 3.451019048690796 5.099450588226318
Loss :  1.6973590850830078 3.162130832672119 4.859489917755127
Loss :  1.639887809753418 2.940330743789673 4.580218315124512
Loss :  1.688066840171814 2.6955366134643555 4.383603572845459
Loss :  1.641395926475525 2.7784438133239746 4.419839859008789
Loss :  1.7226601839065552 3.3642241954803467 5.086884498596191
Loss :  1.6690654754638672 3.22369384765625 4.892759323120117
Loss :  1.6544134616851807 3.4018900394439697 5.05630350112915
Loss :  1.6568334102630615 2.9400813579559326 4.596914768218994
Loss :  1.6967512369155884 3.264747381210327 4.961498737335205
Loss :  1.6888619661331177 2.7922568321228027 4.481118679046631
Loss :  1.6664983034133911 2.8545310497283936 4.521029472351074
Loss :  1.6343283653259277 3.0309503078460693 4.665278434753418
Loss :  1.6596742868423462 3.4208145141601562 5.080488681793213
Loss :  1.6521755456924438 3.0572667121887207 4.709442138671875
  batch 40 loss: 1.6521755456924438, 3.0572667121887207, 4.709442138671875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602777242660522 3.2855303287506104 4.945807933807373
Loss :  1.6485261917114258 2.575632333755493 4.22415828704834
Loss :  1.6654607057571411 3.2100164890289307 4.875477313995361
Loss :  1.6637732982635498 2.870814561843872 4.534587860107422
Loss :  1.6666916608810425 2.7571165561676025 4.4238080978393555
Loss :  1.6581075191497803 3.0033421516418457 4.661449432373047
Loss :  1.6451492309570312 3.1388156414031982 4.783965110778809
Loss :  1.657310962677002 2.9660518169403076 4.6233625411987305
Loss :  1.6301391124725342 3.2166717052459717 4.846810817718506
Loss :  1.6812634468078613 3.249433755874634 4.930697441101074
Loss :  1.6458401679992676 2.66021466255188 4.306055068969727
Loss :  1.6641312837600708 3.1203391551971436 4.784470558166504
Loss :  1.6826683282852173 3.682521343231201 5.365189552307129
Loss :  1.6674085855484009 3.0349831581115723 4.702391624450684
Loss :  1.6706743240356445 3.23174786567688 4.902421951293945
Loss :  1.6349430084228516 2.705915927886963 4.3408589363098145
Loss :  1.6846284866333008 3.3270106315612793 5.01163911819458
Loss :  1.682210922241211 3.3484933376312256 5.030704498291016
Loss :  1.6956186294555664 3.372249126434326 5.067867755889893
Loss :  1.6709046363830566 2.89395809173584 4.5648627281188965
  batch 60 loss: 1.6709046363830566, 2.89395809173584, 4.5648627281188965
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726101636886597 2.963402509689331 4.636012554168701
Loss :  1.669374942779541 2.6812896728515625 4.3506646156311035
Loss :  1.6774227619171143 3.162104845046997 4.839527606964111
Loss :  1.6580150127410889 2.9372589588165283 4.595273971557617
Loss :  1.6550521850585938 3.0781869888305664 4.73323917388916
Loss :  5.483736038208008 4.384108066558838 9.867843627929688
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.3740363121032715 4.362824440002441 9.736860275268555
Loss :  5.482651233673096 4.301897048950195 9.784547805786133
Loss :  4.562401294708252 4.169707298278809 8.732109069824219
Total LOSS train 4.7208497267503 valid 9.530340194702148
CE LOSS train 1.6640674627744234 valid 1.140600323677063
Contrastive LOSS train 3.056782263975877 valid 1.0424268245697021
EPOCH 262:
Loss :  1.6504185199737549 3.005756378173828 4.656174659729004
Loss :  1.666258692741394 3.251969337463379 4.9182281494140625
Loss :  1.652101755142212 3.209261178970337 4.861362934112549
Loss :  1.655252456665039 2.913408041000366 4.568660736083984
Loss :  1.6799155473709106 3.4892961978912354 5.1692118644714355
Loss :  1.6628705263137817 2.7801144123077393 4.4429850578308105
Loss :  1.6610560417175293 3.522329092025757 5.183384895324707
Loss :  1.6499266624450684 2.626044750213623 4.275971412658691
Loss :  1.654577612876892 2.1704797744750977 3.8250575065612793
Loss :  1.6090420484542847 2.945590019226074 4.554632186889648
Loss :  1.668662667274475 3.2372148036956787 4.905877590179443
Loss :  1.7292065620422363 2.91477370262146 4.643980026245117
Loss :  1.6762127876281738 3.0182220935821533 4.694435119628906
Loss :  1.664408802986145 3.3258399963378906 4.990248680114746
Loss :  1.6425849199295044 2.86367130279541 4.506256103515625
Loss :  1.6521596908569336 3.064845085144043 4.717004776000977
Loss :  1.6611456871032715 2.975374698638916 4.6365203857421875
Loss :  1.6595325469970703 2.71150279045105 4.371035575866699
Loss :  1.6675529479980469 2.7365787029266357 4.404131889343262
Loss :  1.6240031719207764 2.8944523334503174 4.518455505371094
  batch 20 loss: 1.6240031719207764, 2.8944523334503174, 4.518455505371094
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592762470245361 2.929450035095215 4.588726043701172
Loss :  1.6750032901763916 2.686049699783325 4.361052989959717
Loss :  1.6463829278945923 2.9296770095825195 4.576059818267822
Loss :  1.6873859167099 2.880143642425537 4.567529678344727
Loss :  1.6848390102386475 3.4306273460388184 5.115466117858887
Loss :  1.6484310626983643 3.256618022918701 4.9050493240356445
Loss :  1.6973576545715332 3.1976826190948486 4.895040512084961
Loss :  1.6398882865905762 2.8507542610168457 4.490642547607422
Loss :  1.688067078590393 2.694631814956665 4.382699012756348
Loss :  1.6413936614990234 2.7776670455932617 4.419060707092285
Loss :  1.7226591110229492 3.233914613723755 4.956573486328125
Loss :  1.6690630912780762 3.152409076690674 4.82147216796875
Loss :  1.6544142961502075 3.1533150672912598 4.807729244232178
Loss :  1.6568330526351929 2.7365002632141113 4.393333435058594
Loss :  1.6967499256134033 3.7272653579711914 5.424015045166016
Loss :  1.6888620853424072 2.5057473182678223 4.194609642028809
Loss :  1.666498064994812 2.996344804763794 4.662842750549316
Loss :  1.6343276500701904 3.2904200553894043 4.924747467041016
Loss :  1.6596741676330566 3.083444118499756 4.7431182861328125
Loss :  1.6521745920181274 3.405999183654785 5.058173656463623
  batch 40 loss: 1.6521745920181274, 3.405999183654785, 5.058173656463623
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.660277247428894 3.451683759689331 5.1119608879089355
Loss :  1.6485259532928467 2.692403554916382 4.3409295082092285
Loss :  1.6654609441757202 3.2548184394836426 4.920279502868652
Loss :  1.6637730598449707 3.058790683746338 4.722563743591309
Loss :  1.6666913032531738 2.8674867153167725 4.534177780151367
Loss :  1.658106803894043 3.0546836853027344 4.712790489196777
Loss :  1.6451480388641357 3.0234265327453613 4.668574333190918
Loss :  1.6573106050491333 2.833695888519287 4.491006374359131
Loss :  1.6301395893096924 3.5730302333831787 5.203169822692871
Loss :  1.6812642812728882 3.1615638732910156 4.842828273773193
Loss :  1.6458396911621094 2.636991024017334 4.282830715179443
Loss :  1.6641311645507812 2.9855904579162598 4.649721622467041
Loss :  1.68266761302948 3.2290737628936768 4.911741256713867
Loss :  1.6674087047576904 3.0598788261413574 4.727287292480469
Loss :  1.6706738471984863 3.165325880050659 4.835999488830566
Loss :  1.6349427700042725 2.7235641479492188 4.35850715637207
Loss :  1.6846286058425903 3.4430384635925293 5.12766695022583
Loss :  1.6822103261947632 3.3690555095672607 5.051265716552734
Loss :  1.6956183910369873 3.3083627223968506 5.003981113433838
Loss :  1.6709052324295044 2.9201204776763916 4.5910258293151855
  batch 60 loss: 1.6709052324295044, 2.9201204776763916, 4.5910258293151855
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726092100143433 3.2083284854888916 4.880937576293945
Loss :  1.6693741083145142 2.634658098220825 4.304032325744629
Loss :  1.6774213314056396 3.0373823642730713 4.714803695678711
Loss :  1.6580147743225098 2.8704721927642822 4.528487205505371
Loss :  1.6550512313842773 3.1455326080322266 4.800583839416504
Loss :  5.4837822914123535 4.436381816864014 9.920164108276367
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.374056339263916 4.386334419250488 9.760391235351562
Loss :  5.4826812744140625 4.195712566375732 9.678394317626953
Loss :  4.562373161315918 4.127869606018066 8.690242767333984
Total LOSS train 4.699210915198693 valid 9.512298107147217
CE LOSS train 1.664067163834205 valid 1.1405932903289795
Contrastive LOSS train 3.0351437605344334 valid 1.0319674015045166
EPOCH 263:
Loss :  1.6504185199737549 2.8431057929992676 4.493524551391602
Loss :  1.6662580966949463 3.333117723464966 4.999375820159912
Loss :  1.6521005630493164 2.8788630962371826 4.530963897705078
Loss :  1.6552528142929077 3.0581412315368652 4.7133941650390625
Loss :  1.6799153089523315 3.337554931640625 5.017470359802246
Loss :  1.662870168685913 2.849147081375122 4.512017250061035
Loss :  1.6610571146011353 3.2507164478302 4.911773681640625
Loss :  1.6499266624450684 2.6991729736328125 4.349099636077881
Loss :  1.6545783281326294 2.409154176712036 4.063732624053955
Loss :  1.609041690826416 2.9619154930114746 4.570957183837891
Loss :  1.6686620712280273 3.2820677757263184 4.950729846954346
Loss :  1.7292066812515259 2.7324697971343994 4.461676597595215
Loss :  1.6762107610702515 3.2979700565338135 4.974180698394775
Loss :  1.6644090414047241 3.459324598312378 5.1237335205078125
Loss :  1.6425858736038208 3.2731544971466064 4.915740489959717
Loss :  1.6521586179733276 3.012392997741699 4.664551734924316
Loss :  1.6611454486846924 2.8868653774261475 4.54801082611084
Loss :  1.6595321893692017 3.0737578868865967 4.733290195465088
Loss :  1.6675512790679932 3.109137773513794 4.776689052581787
Loss :  1.6240038871765137 3.0711605548858643 4.695164680480957
  batch 20 loss: 1.6240038871765137, 3.0711605548858643, 4.695164680480957
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592769622802734 3.017080783843994 4.676357746124268
Loss :  1.6750032901763916 2.440941333770752 4.115944862365723
Loss :  1.6463819742202759 3.1013336181640625 4.747715473175049
Loss :  1.687387228012085 2.866919994354248 4.554306983947754
Loss :  1.6848393678665161 3.4700727462768555 5.154911994934082
Loss :  1.6484309434890747 3.225389003753662 4.873819828033447
Loss :  1.6973575353622437 3.053731679916382 4.751089096069336
Loss :  1.6398874521255493 2.6867518424987793 4.326639175415039
Loss :  1.6880664825439453 2.6277976036071777 4.315864086151123
Loss :  1.6413947343826294 2.962064027786255 4.603458881378174
Loss :  1.722659707069397 3.36500883102417 5.087668418884277
Loss :  1.669062852859497 3.3521313667297363 5.0211944580078125
Loss :  1.6544135808944702 3.063917875289917 4.718331336975098
Loss :  1.6568328142166138 2.8895959854125977 4.546428680419922
Loss :  1.696748971939087 3.584223508834839 5.280972480773926
Loss :  1.6888617277145386 2.8188867568969727 4.507748603820801
Loss :  1.6664986610412598 2.894881248474121 4.561379909515381
Loss :  1.634326696395874 2.5991501808166504 4.233476638793945
Loss :  1.6596742868423462 3.0482633113861084 4.707937717437744
Loss :  1.6521750688552856 3.182546377182007 4.834721565246582
  batch 40 loss: 1.6521750688552856, 3.182546377182007, 4.834721565246582
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602765321731567 3.4451119899749756 5.105388641357422
Loss :  1.6485258340835571 2.488912343978882 4.1374382972717285
Loss :  1.6654609441757202 3.000544309616089 4.6660051345825195
Loss :  1.6637728214263916 2.876523494720459 4.54029655456543
Loss :  1.6666902303695679 2.688084840774536 4.3547749519348145
Loss :  1.6581075191497803 3.188978672027588 4.847085952758789
Loss :  1.6451480388641357 2.911489725112915 4.556637763977051
Loss :  1.6573107242584229 2.7984578609466553 4.455768585205078
Loss :  1.630139708518982 3.4380006790161133 5.068140506744385
Loss :  1.6812636852264404 2.999159336090088 4.680422782897949
Loss :  1.645838975906372 2.7653470039367676 4.411186218261719
Loss :  1.6641314029693604 3.05716609954834 4.721297264099121
Loss :  1.6826679706573486 3.333888530731201 5.016556739807129
Loss :  1.6674089431762695 3.090402841567993 4.757811546325684
Loss :  1.670674204826355 3.0924079418182373 4.763082027435303
Loss :  1.6349434852600098 2.7185134887695312 4.353456974029541
Loss :  1.684629201889038 3.4213552474975586 5.105984687805176
Loss :  1.682210087776184 3.186784267425537 4.868994235992432
Loss :  1.6956188678741455 3.4535059928894043 5.149125099182129
Loss :  1.6709064245224 2.7273435592651367 4.398250102996826
  batch 60 loss: 1.6709064245224, 2.7273435592651367, 4.398250102996826
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.672608733177185 3.200896739959717 4.873505592346191
Loss :  1.6693745851516724 2.83242130279541 4.501795768737793
Loss :  1.6774214506149292 3.2875421047210693 4.964963436126709
Loss :  1.6580148935317993 2.7653656005859375 4.423380374908447
Loss :  1.6550489664077759 2.8958370685577393 4.550886154174805
Loss :  5.483817100524902 4.376190185546875 9.860007286071777
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.374088287353516 4.316956043243408 9.691043853759766
Loss :  5.482751846313477 4.205759048461914 9.68851089477539
Loss :  4.562504768371582 4.307663917541504 8.870168685913086
Total LOSS train 4.69074277144212 valid 9.527432680130005
CE LOSS train 1.6640670721347515 valid 1.1406261920928955
Contrastive LOSS train 3.026675682801467 valid 1.076915979385376
EPOCH 264:
Loss :  1.6504191160202026 2.711101770401001 4.361520767211914
Loss :  1.6662590503692627 3.1948115825653076 4.86107063293457
Loss :  1.6521005630493164 2.930109739303589 4.582210540771484
Loss :  1.6552529335021973 2.8160767555236816 4.471329689025879
Loss :  1.679915189743042 3.5560615062713623 5.235976696014404
Loss :  1.6628711223602295 2.9171273708343506 4.57999849319458
Loss :  1.6610567569732666 3.272700309753418 4.9337568283081055
Loss :  1.6499266624450684 2.40594482421875 4.055871486663818
Loss :  1.6545790433883667 2.50376296043396 4.158341884613037
Loss :  1.6090421676635742 2.839993715286255 4.44903564453125
Loss :  1.6686620712280273 3.300834894180298 4.969496726989746
Loss :  1.7292065620422363 3.020165205001831 4.749371528625488
Loss :  1.6762113571166992 3.116325855255127 4.792537212371826
Loss :  1.6644095182418823 3.333662271499634 4.998071670532227
Loss :  1.6425861120224 3.0599093437194824 4.702495574951172
Loss :  1.6521588563919067 2.823364496231079 4.475523471832275
Loss :  1.661145567893982 2.8792777061462402 4.540423393249512
Loss :  1.6595321893692017 2.6380600929260254 4.2975921630859375
Loss :  1.6675519943237305 2.7816057205200195 4.44915771484375
Loss :  1.6240042448043823 2.969012975692749 4.593017101287842
  batch 20 loss: 1.6240042448043823, 2.969012975692749, 4.593017101287842
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592758893966675 2.9743142127990723 4.633590221405029
Loss :  1.675003170967102 2.8238959312438965 4.498898983001709
Loss :  1.646382212638855 3.386544704437256 5.0329270362854
Loss :  1.6873868703842163 2.8026037216186523 4.489990711212158
Loss :  1.6848396062850952 3.347228765487671 5.032068252563477
Loss :  1.6484307050704956 3.300860643386841 4.949291229248047
Loss :  1.6973577737808228 3.2008469104766846 4.898204803466797
Loss :  1.639887809753418 2.941096067428589 4.580984115600586
Loss :  1.6880667209625244 2.7897329330444336 4.477799415588379
Loss :  1.6413946151733398 3.0188794136047363 4.660274028778076
Loss :  1.7226595878601074 3.2959299087524414 5.018589496612549
Loss :  1.669063925743103 3.245549440383911 4.914613246917725
Loss :  1.6544145345687866 3.129605770111084 4.78402042388916
Loss :  1.6568331718444824 2.6894772052764893 4.346310615539551
Loss :  1.6967504024505615 3.5317676067352295 5.228518009185791
Loss :  1.6888614892959595 2.7620599269866943 4.450921535491943
Loss :  1.6664983034133911 2.8489880561828613 4.515486240386963
Loss :  1.6343281269073486 2.975626230239868 4.609954357147217
Loss :  1.6596752405166626 3.079345941543579 4.739021301269531
Loss :  1.6521750688552856 3.083162546157837 4.735337734222412
  batch 40 loss: 1.6521750688552856, 3.083162546157837, 4.735337734222412
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.660277247428894 3.4898569583892822 5.150134086608887
Loss :  1.6485265493392944 2.7217304706573486 4.3702569007873535
Loss :  1.6654613018035889 2.910400390625 4.575861930847168
Loss :  1.663773536682129 2.903289556503296 4.567063331604004
Loss :  1.666690468788147 2.7986791133880615 4.465369701385498
Loss :  1.6581077575683594 3.0316247940063477 4.689732551574707
Loss :  1.6451483964920044 3.029287099838257 4.674435615539551
Loss :  1.6573115587234497 2.71977162361145 4.3770833015441895
Loss :  1.6301405429840088 3.5277445316314697 5.1578850746154785
Loss :  1.6812639236450195 2.8295047283172607 4.510768890380859
Loss :  1.645838737487793 2.6277527809143066 4.2735915184021
Loss :  1.664131999015808 2.911320686340332 4.57545280456543
Loss :  1.6826680898666382 3.292952060699463 4.975620269775391
Loss :  1.667410135269165 3.01182222366333 4.679232597351074
Loss :  1.6706739664077759 3.0841453075408936 4.754819393157959
Loss :  1.6349434852600098 2.6389882564544678 4.273931503295898
Loss :  1.684628963470459 3.280773639678955 4.965402603149414
Loss :  1.6822099685668945 3.2866079807281494 4.968817710876465
Loss :  1.6956182718276978 3.188530921936035 4.884149074554443
Loss :  1.670906901359558 2.76253342628479 4.433440208435059
  batch 60 loss: 1.670906901359558, 2.76253342628479, 4.433440208435059
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726090908050537 3.2178893089294434 4.890498161315918
Loss :  1.6693744659423828 2.697319984436035 4.366694450378418
Loss :  1.6774206161499023 2.9962306022644043 4.673651218414307
Loss :  1.6580150127410889 2.901772975921631 4.559787750244141
Loss :  1.6550489664077759 3.0668764114379883 4.721925258636475
Loss :  5.48372745513916 4.418728351593018 9.902456283569336
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.3739728927612305 4.382015705108643 9.755989074707031
Loss :  5.48267936706543 4.277738094329834 9.760417938232422
Loss :  4.56235933303833 4.266837120056152 8.82919692993164
Total LOSS train 4.6675262597891 valid 9.562015056610107
CE LOSS train 1.6640673270592323 valid 1.1405898332595825
Contrastive LOSS train 3.003458936397846 valid 1.066709280014038
EPOCH 265:
Loss :  1.650418758392334 2.893568992614746 4.54398775100708
Loss :  1.6662589311599731 3.2712247371673584 4.937483787536621
Loss :  1.6521000862121582 2.9576337337493896 4.609733581542969
Loss :  1.6552526950836182 2.9322168827056885 4.587469577789307
Loss :  1.679914116859436 3.449869394302368 5.129783630371094
Loss :  1.6628711223602295 2.7867813110351562 4.449652671813965
Loss :  1.661056637763977 3.5341427326202393 5.195199489593506
Loss :  1.6499269008636475 2.650195598602295 4.300122261047363
Loss :  1.6545798778533936 2.2674248218536377 3.9220046997070312
Loss :  1.6090422868728638 2.7270257472991943 4.336068153381348
Loss :  1.6686614751815796 3.4453256130218506 5.113986968994141
Loss :  1.729205846786499 2.948442220687866 4.677648067474365
Loss :  1.6762105226516724 2.967766761779785 4.643977165222168
Loss :  1.6644099950790405 3.4110779762268066 5.075488090515137
Loss :  1.6425855159759521 2.9888391494750977 4.631424903869629
Loss :  1.652159571647644 3.2550923824310303 4.907251834869385
Loss :  1.6611454486846924 2.7879862785339355 4.449131965637207
Loss :  1.6595304012298584 2.8086886405944824 4.468218803405762
Loss :  1.667551875114441 2.935114860534668 4.602666854858398
Loss :  1.6240044832229614 2.813056707382202 4.437061309814453
  batch 20 loss: 1.6240044832229614, 2.813056707382202, 4.437061309814453
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592752933502197 3.1131889820098877 4.772464275360107
Loss :  1.675003170967102 2.693894386291504 4.368897438049316
Loss :  1.646382212638855 3.1550912857055664 4.801473617553711
Loss :  1.687386155128479 2.9338393211364746 4.621225357055664
Loss :  1.6848384141921997 3.602217674255371 5.287055969238281
Loss :  1.6484302282333374 3.1590042114257812 4.807434558868408
Loss :  1.6973576545715332 3.218369960784912 4.915727615356445
Loss :  1.639887809753418 3.0254719257354736 4.6653594970703125
Loss :  1.688067078590393 2.846730947494507 4.5347981452941895
Loss :  1.6413938999176025 2.9304444789886475 4.57183837890625
Loss :  1.7226598262786865 3.2405765056610107 4.963236331939697
Loss :  1.669061541557312 3.1785571575164795 4.847618579864502
Loss :  1.6544145345687866 3.2673354148864746 4.921750068664551
Loss :  1.6568331718444824 2.824000358581543 4.480833530426025
Loss :  1.69674813747406 3.4764621257781982 5.173210144042969
Loss :  1.6888614892959595 2.774629831314087 4.463491439819336
Loss :  1.6664986610412598 3.018836498260498 4.685335159301758
Loss :  1.6343272924423218 3.042041301727295 4.676368713378906
Loss :  1.6596746444702148 2.846004009246826 4.505678653717041
Loss :  1.652174949645996 3.2990617752075195 4.951236724853516
  batch 40 loss: 1.652174949645996, 3.2990617752075195, 4.951236724853516
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602760553359985 3.3616204261779785 5.0218963623046875
Loss :  1.6485260725021362 2.5316174030303955 4.180143356323242
Loss :  1.6654608249664307 3.372001886367798 5.0374627113342285
Loss :  1.6637743711471558 3.1280031204223633 4.791777610778809
Loss :  1.6666899919509888 2.439730405807495 4.106420516967773
Loss :  1.658106803894043 3.2615272998809814 4.919633865356445
Loss :  1.6451473236083984 3.27128267288208 4.9164299964904785
Loss :  1.6573103666305542 2.758544445037842 4.4158549308776855
Loss :  1.630139946937561 3.1976282596588135 4.827768325805664
Loss :  1.681262731552124 3.1335816383361816 4.814844131469727
Loss :  1.6458390951156616 2.774078845977783 4.419918060302734
Loss :  1.6641318798065186 2.9230034351348877 4.587135314941406
Loss :  1.6826680898666382 3.4189107418060303 5.101578712463379
Loss :  1.667409896850586 2.8832807540893555 4.550690650939941
Loss :  1.6706733703613281 3.5001516342163086 5.170825004577637
Loss :  1.6349444389343262 2.678921937942505 4.31386661529541
Loss :  1.6846294403076172 3.379312753677368 5.063941955566406
Loss :  1.6822092533111572 2.981956720352173 4.66416597366333
Loss :  1.6956192255020142 3.389707088470459 5.085326194763184
Loss :  1.6709072589874268 3.015669584274292 4.686576843261719
  batch 60 loss: 1.6709072589874268, 3.015669584274292, 4.686576843261719
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726086139678955 3.2479248046875 4.920533180236816
Loss :  1.6693748235702515 2.5745317935943604 4.243906497955322
Loss :  1.6774206161499023 2.8760883808135986 4.553508758544922
Loss :  1.6580159664154053 2.7475578784942627 4.405573844909668
Loss :  1.6550487279891968 3.151599884033203 4.8066487312316895
Loss :  5.483745098114014 4.393016338348389 9.876761436462402
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.37402868270874 4.350187301635742 9.72421646118164
Loss :  5.482686996459961 4.238888263702393 9.721574783325195
Loss :  4.562258243560791 4.073879718780518 8.636137962341309
Total LOSS train 4.702151137131911 valid 9.489672660827637
CE LOSS train 1.6640670446249155 valid 1.1405645608901978
Contrastive LOSS train 3.0380840998429517 valid 1.0184699296951294
EPOCH 266:
Loss :  1.6504207849502563 2.7031071186065674 4.353528022766113
Loss :  1.6662590503692627 3.2445387840270996 4.910798072814941
Loss :  1.6521002054214478 3.01533579826355 4.667436122894287
Loss :  1.6552530527114868 2.755549430847168 4.410802364349365
Loss :  1.6799135208129883 3.228156328201294 4.908069610595703
Loss :  1.6628719568252563 2.61167049407959 4.274542331695557
Loss :  1.6610569953918457 3.0944552421569824 4.755512237548828
Loss :  1.6499260663986206 2.746868371963501 4.396794319152832
Loss :  1.654579520225525 2.2201998233795166 3.874779224395752
Loss :  1.6090413331985474 3.0172839164733887 4.6263251304626465
Loss :  1.6686617136001587 3.4822821617126465 5.150943756103516
Loss :  1.7292065620422363 2.9749221801757812 4.704128742218018
Loss :  1.676211953163147 3.0828161239624023 4.75902795791626
Loss :  1.6644083261489868 3.1116654872894287 4.776073932647705
Loss :  1.6425864696502686 3.228001832962036 4.870588302612305
Loss :  1.652159333229065 3.1307923793792725 4.782951831817627
Loss :  1.6611460447311401 2.9245924949645996 4.585738658905029
Loss :  1.6595317125320435 2.7222683429718018 4.381800174713135
Loss :  1.6675525903701782 2.7471044063568115 4.414657115936279
Loss :  1.6240044832229614 3.236957311630249 4.8609619140625
  batch 20 loss: 1.6240044832229614, 3.236957311630249, 4.8609619140625
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.659274935722351 3.271787166595459 4.9310622215271
Loss :  1.675002932548523 2.840907096862793 4.5159101486206055
Loss :  1.6463807821273804 3.085031270980835 4.731411933898926
Loss :  1.6873866319656372 2.9960217475891113 4.683408260345459
Loss :  1.6848393678665161 3.1671955585479736 4.852035045623779
Loss :  1.6484298706054688 3.1497480869293213 4.798177719116211
Loss :  1.6973565816879272 3.1436960697174072 4.841052532196045
Loss :  1.6398881673812866 3.0776076316833496 4.717495918273926
Loss :  1.688066840171814 2.8650906085968018 4.553157329559326
Loss :  1.6413941383361816 2.8681178092956543 4.509511947631836
Loss :  1.722661018371582 3.3988983631134033 5.121559143066406
Loss :  1.6690630912780762 3.369595766067505 5.03865909576416
Loss :  1.654415249824524 3.1947529315948486 4.849168300628662
Loss :  1.6568336486816406 2.78135085105896 4.43818473815918
Loss :  1.696747899055481 3.3021838665008545 4.998931884765625
Loss :  1.6888617277145386 2.9108755588531494 4.599737167358398
Loss :  1.666498064994812 2.8594517707824707 4.525949954986572
Loss :  1.6343278884887695 2.95744252204895 4.591770172119141
Loss :  1.6596752405166626 2.8291265964508057 4.488801956176758
Loss :  1.6521750688552856 3.1431307792663574 4.7953057289123535
  batch 40 loss: 1.6521750688552856, 3.1431307792663574, 4.7953057289123535
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602765321731567 3.305929660797119 4.966206073760986
Loss :  1.6485261917114258 2.416515588760376 4.065041542053223
Loss :  1.6654613018035889 2.98119854927063 4.646659851074219
Loss :  1.663774847984314 3.032392740249634 4.696167469024658
Loss :  1.666690468788147 2.826718330383301 4.493408679962158
Loss :  1.6581064462661743 2.950517177581787 4.608623504638672
Loss :  1.6451481580734253 3.3253960609436035 4.970544338226318
Loss :  1.6573108434677124 2.7381880283355713 4.395498752593994
Loss :  1.630138874053955 3.360002040863037 4.990140914916992
Loss :  1.6812634468078613 3.1634364128112793 4.844699859619141
Loss :  1.645838975906372 2.6751859188079834 4.3210248947143555
Loss :  1.6641314029693604 2.885531187057495 4.5496625900268555
Loss :  1.6826688051223755 3.471935510635376 5.154604434967041
Loss :  1.6674107313156128 2.8112356662750244 4.478646278381348
Loss :  1.6706727743148804 3.329946994781494 5.000619888305664
Loss :  1.6349438428878784 2.7876386642456055 4.422582626342773
Loss :  1.6846287250518799 3.243321418762207 4.927949905395508
Loss :  1.6822099685668945 2.8544278144836426 4.536637783050537
Loss :  1.6956183910369873 3.4472010135650635 5.142819404602051
Loss :  1.6709063053131104 2.8270063400268555 4.497912406921387
  batch 60 loss: 1.6709063053131104, 2.8270063400268555, 4.497912406921387
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726089715957642 3.2467644214630127 4.919373512268066
Loss :  1.6693758964538574 2.7074155807495117 4.376791477203369
Loss :  1.6774218082427979 3.3696985244750977 5.047120094299316
Loss :  1.6580157279968262 2.9227495193481445 4.580765247344971
Loss :  1.6550489664077759 3.209221363067627 4.864270210266113
Loss :  5.4839372634887695 4.368458271026611 9.852396011352539
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.374203205108643 4.358259201049805 9.732461929321289
Loss :  5.482876300811768 4.226841926574707 9.709718704223633
Loss :  4.562450408935547 4.240828514099121 8.803278923034668
Total LOSS train 4.685300350189209 valid 9.524463891983032
CE LOSS train 1.6640672188538772 valid 1.1406126022338867
Contrastive LOSS train 3.0212331478412335 valid 1.0602071285247803
EPOCH 267:
Loss :  1.650421142578125 2.668330669403076 4.318751811981201
Loss :  1.666259527206421 3.417379856109619 5.083639144897461
Loss :  1.6521005630493164 2.80669903755188 4.458799362182617
Loss :  1.6552540063858032 3.02394700050354 4.679201126098633
Loss :  1.679914116859436 3.5680251121520996 5.247939109802246
Loss :  1.6628715991973877 2.829495668411255 4.492367267608643
Loss :  1.6610569953918457 3.2049849033355713 4.866042137145996
Loss :  1.6499263048171997 2.670982599258423 4.320909023284912
Loss :  1.654579997062683 2.297351360321045 3.9519314765930176
Loss :  1.6090415716171265 2.8140132427215576 4.4230546951293945
Loss :  1.6686619520187378 3.528320074081421 5.196981906890869
Loss :  1.7292076349258423 2.9498608112335205 4.679068565368652
Loss :  1.676214575767517 3.0731379985809326 4.74935245513916
Loss :  1.664408564567566 3.1468000411987305 4.811208724975586
Loss :  1.6425857543945312 2.905301094055176 4.547886848449707
Loss :  1.6521601676940918 3.0350306034088135 4.687191009521484
Loss :  1.6611462831497192 2.934695243835449 4.595841407775879
Loss :  1.6595317125320435 2.6105027198791504 4.270034313201904
Loss :  1.6675525903701782 2.76379656791687 4.431349277496338
Loss :  1.6240047216415405 2.8115804195404053 4.435585021972656
  batch 20 loss: 1.6240047216415405, 2.8115804195404053, 4.435585021972656
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592751741409302 2.902115821838379 4.5613908767700195
Loss :  1.6750035285949707 2.72259521484375 4.397598743438721
Loss :  1.6463810205459595 2.8450353145599365 4.4914164543151855
Loss :  1.6873862743377686 2.8491361141204834 4.536522388458252
Loss :  1.6848405599594116 3.477262020111084 5.162102699279785
Loss :  1.6484296321868896 3.327975034713745 4.976404666900635
Loss :  1.6973568201065063 3.2629740238189697 4.960330963134766
Loss :  1.6398872137069702 3.196141481399536 4.836028575897217
Loss :  1.688066840171814 2.8781704902648926 4.566237449645996
Loss :  1.6413958072662354 2.899712085723877 4.541108131408691
Loss :  1.722661018371582 3.251382350921631 4.974043369293213
Loss :  1.669063687324524 3.5468931198120117 5.215956687927246
Loss :  1.6544150114059448 3.1700849533081055 4.82450008392334
Loss :  1.6568349599838257 2.907323122024536 4.564157962799072
Loss :  1.696749210357666 3.6607980728149414 5.357547283172607
Loss :  1.6888617277145386 2.62233304977417 4.311194896697998
Loss :  1.666497826576233 3.117144823074341 4.783642768859863
Loss :  1.6343276500701904 2.918682813644409 4.5530104637146
Loss :  1.6596750020980835 3.0293428897857666 4.6890177726745605
Loss :  1.6521754264831543 3.3562710285186768 5.00844669342041
  batch 40 loss: 1.6521754264831543, 3.3562710285186768, 5.00844669342041
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602765321731567 3.410829782485962 5.071106433868408
Loss :  1.648525595664978 2.5922253131866455 4.240750789642334
Loss :  1.665461778640747 3.2413527965545654 4.9068145751953125
Loss :  1.6637741327285767 2.8466711044311523 4.5104451179504395
Loss :  1.6666903495788574 2.5979292392730713 4.264619827270508
Loss :  1.6581075191497803 3.352677345275879 5.010785102844238
Loss :  1.6451478004455566 3.0473005771636963 4.692448616027832
Loss :  1.6573113203048706 2.7359020709991455 4.393213272094727
Loss :  1.630138635635376 3.3885281085968018 5.018666744232178
Loss :  1.68126380443573 3.0733773708343506 4.754641056060791
Loss :  1.6458388566970825 2.8772854804992676 4.5231242179870605
Loss :  1.6641314029693604 2.922560691833496 4.586691856384277
Loss :  1.6826682090759277 3.710585832595825 5.393254280090332
Loss :  1.6674104928970337 2.83681321144104 4.504223823547363
Loss :  1.670674204826355 3.3982880115509033 5.068962097167969
Loss :  1.6349432468414307 2.878943920135498 4.513887405395508
Loss :  1.684628963470459 3.3891961574554443 5.073824882507324
Loss :  1.6822106838226318 2.9319419860839844 4.614152908325195
Loss :  1.695618987083435 3.4322075843811035 5.127826690673828
Loss :  1.6709054708480835 2.8149192333221436 4.4858245849609375
  batch 60 loss: 1.6709054708480835, 2.8149192333221436, 4.4858245849609375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726094484329224 2.9876601696014404 4.660269737243652
Loss :  1.6693758964538574 2.65238356590271 4.321759223937988
Loss :  1.6774225234985352 3.3650598526000977 5.042482376098633
Loss :  1.658015489578247 2.7594616413116455 4.417477130889893
Loss :  1.6550496816635132 2.8489830493927 4.504032611846924
Loss :  5.483640670776367 4.35114860534668 9.834789276123047
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.373882293701172 4.3512959480285645 9.725177764892578
Loss :  5.482558250427246 4.231302738189697 9.713861465454102
Loss :  4.562228679656982 4.274387836456299 8.836616516113281
Total LOSS train 4.696293522761418 valid 9.527611255645752
CE LOSS train 1.6640674646084126 valid 1.1405571699142456
Contrastive LOSS train 3.0322260453150824 valid 1.0685969591140747
EPOCH 268:
Loss :  1.6504207849502563 2.7127397060394287 4.363160610198975
Loss :  1.666259527206421 3.1393883228302 4.805647850036621
Loss :  1.6521005630493164 2.883725881576538 4.535826683044434
Loss :  1.6552537679672241 2.882542610168457 4.537796497344971
Loss :  1.6799143552780151 3.4703166484832764 5.150230884552002
Loss :  1.6628714799880981 2.958437204360962 4.62130880355835
Loss :  1.6610562801361084 3.5328543186187744 5.193910598754883
Loss :  1.6499258279800415 2.509040117263794 4.158966064453125
Loss :  1.6545798778533936 2.2586395740509033 3.913219451904297
Loss :  1.609041452407837 2.9985291957855225 4.607570648193359
Loss :  1.6686620712280273 3.671290636062622 5.33995246887207
Loss :  1.7292072772979736 2.9505956172943115 4.679802894592285
Loss :  1.6762151718139648 3.000931739807129 4.677146911621094
Loss :  1.6644083261489868 3.5168702602386475 5.181278705596924
Loss :  1.6425851583480835 3.2399978637695312 4.882583141326904
Loss :  1.6521600484848022 3.0773890018463135 4.729548931121826
Loss :  1.6611465215682983 2.75986909866333 4.421015739440918
Loss :  1.6595317125320435 2.5038375854492188 4.163369178771973
Loss :  1.667553424835205 2.869636297225952 4.537189483642578
Loss :  1.6240042448043823 3.083346128463745 4.707350254058838
  batch 20 loss: 1.6240042448043823, 3.083346128463745, 4.707350254058838
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592748165130615 3.3178324699401855 4.977107048034668
Loss :  1.675003170967102 2.8030447959899902 4.478047847747803
Loss :  1.6463797092437744 3.394505262374878 5.040884971618652
Loss :  1.6873866319656372 2.852416515350342 4.5398030281066895
Loss :  1.684840440750122 3.558702230453491 5.243542671203613
Loss :  1.6484285593032837 3.4290499687194824 5.077478408813477
Loss :  1.6973572969436646 3.2698779106140137 4.967235088348389
Loss :  1.6398873329162598 3.081165313720703 4.721052646636963
Loss :  1.6880666017532349 2.9451398849487305 4.633206367492676
Loss :  1.6413952112197876 2.766428232192993 4.40782356262207
Loss :  1.7226612567901611 3.152268409729004 4.874929428100586
Loss :  1.6690648794174194 3.18575382232666 4.854818820953369
Loss :  1.654414415359497 3.2609448432922363 4.9153594970703125
Loss :  1.6568341255187988 2.948967933654785 4.605802059173584
Loss :  1.69674813747406 3.256556510925293 4.953304767608643
Loss :  1.6888624429702759 2.939323663711548 4.628186225891113
Loss :  1.6664977073669434 2.9529881477355957 4.619485855102539
Loss :  1.6343278884887695 2.9583868980407715 4.592714786529541
Loss :  1.6596742868423462 3.134270668029785 4.793944835662842
Loss :  1.6521751880645752 3.356239080429077 5.008414268493652
  batch 40 loss: 1.6521751880645752, 3.356239080429077, 5.008414268493652
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.660276174545288 3.498230457305908 5.158506393432617
Loss :  1.648525595664978 2.9472899436950684 4.595815658569336
Loss :  1.665461540222168 2.8733034133911133 4.538764953613281
Loss :  1.663773536682129 2.9533870220184326 4.617160797119141
Loss :  1.6666897535324097 2.4718880653381348 4.138577938079834
Loss :  1.6581065654754639 2.8795700073242188 4.537676811218262
Loss :  1.645147442817688 3.0338006019592285 4.678947925567627
Loss :  1.657309889793396 2.7852120399475098 4.442522048950195
Loss :  1.6301382780075073 3.315361976623535 4.945500373840332
Loss :  1.6812639236450195 3.2106614112854004 4.89192533493042
Loss :  1.6458383798599243 2.739602565765381 4.385440826416016
Loss :  1.6641318798065186 3.000969409942627 4.665101051330566
Loss :  1.6826677322387695 3.3948452472686768 5.077512741088867
Loss :  1.6674094200134277 2.7661004066467285 4.433509826660156
Loss :  1.6706736087799072 3.178278684616089 4.848952293395996
Loss :  1.634942889213562 2.6932053565979004 4.328148365020752
Loss :  1.6846288442611694 3.365745782852173 5.050374507904053
Loss :  1.6822103261947632 3.4358785152435303 5.118088722229004
Loss :  1.695618987083435 3.272305965423584 4.967925071716309
Loss :  1.6709061861038208 2.8282535076141357 4.499159812927246
  batch 60 loss: 1.6709061861038208, 2.8282535076141357, 4.499159812927246
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726094484329224 3.2271361351013184 4.899745464324951
Loss :  1.6693753004074097 2.742588520050049 4.411963939666748
Loss :  1.6774218082427979 3.3450517654418945 5.022473335266113
Loss :  1.6580157279968262 2.874457359313965 4.532473087310791
Loss :  1.6550475358963013 2.830502510070801 4.4855499267578125
Loss :  5.483631134033203 4.400606632232666 9.884237289428711
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.3739333152771 4.330672740936279 9.704606056213379
Loss :  5.4825615882873535 4.263520240783691 9.746082305908203
Loss :  4.562219619750977 4.263546943664551 8.825766563415527
Total LOSS train 4.714028233748216 valid 9.540173053741455
CE LOSS train 1.664067211517921 valid 1.1405549049377441
Contrastive LOSS train 3.0499610314002403 valid 1.0658867359161377
EPOCH 269:
Loss :  1.6504201889038086 2.814560890197754 4.4649810791015625
Loss :  1.6662594079971313 3.3154847621917725 4.981744289398193
Loss :  1.652099847793579 2.8787009716033936 4.530800819396973
Loss :  1.6552534103393555 3.0759291648864746 4.73118257522583
Loss :  1.679914116859436 3.1151864528656006 4.795100688934326
Loss :  1.6628719568252563 2.733494520187378 4.396366596221924
Loss :  1.661056637763977 3.3170242309570312 4.978080749511719
Loss :  1.649925708770752 2.471168041229248 4.12109375
Loss :  1.654579758644104 2.2172601222991943 3.871840000152588
Loss :  1.6090412139892578 2.926849603652954 4.535890579223633
Loss :  1.6686615943908691 3.5914952754974365 5.260156631469727
Loss :  1.729207992553711 2.9375712871551514 4.666779518127441
Loss :  1.6762142181396484 3.122204065322876 4.798418045043945
Loss :  1.6644092798233032 3.1398613452911377 4.8042707443237305
Loss :  1.6425857543945312 3.0788733959198 4.72145938873291
Loss :  1.6521596908569336 2.991251230239868 4.643410682678223
Loss :  1.6611462831497192 2.8515217304229736 4.512668132781982
Loss :  1.6595302820205688 2.7778589725494385 4.437389373779297
Loss :  1.6675529479980469 2.641796350479126 4.309349060058594
Loss :  1.6240043640136719 3.0066659450531006 4.630670547485352
  batch 20 loss: 1.6240043640136719, 3.0066659450531006, 4.630670547485352
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592752933502197 3.0669729709625244 4.726248264312744
Loss :  1.6750025749206543 2.6251347064971924 4.300137519836426
Loss :  1.6463794708251953 2.9674429893493652 4.6138224601745605
Loss :  1.6873867511749268 2.8946125507354736 4.5819993019104
Loss :  1.6848409175872803 3.1940579414367676 4.878898620605469
Loss :  1.6484283208847046 3.278803825378418 4.927232265472412
Loss :  1.697357177734375 2.9232001304626465 4.6205573081970215
Loss :  1.6398869752883911 3.0426204204559326 4.682507514953613
Loss :  1.6880669593811035 2.7204782962799072 4.40854549407959
Loss :  1.6413953304290771 2.831143856048584 4.472538948059082
Loss :  1.722660779953003 3.4602386951446533 5.182899475097656
Loss :  1.6690641641616821 3.3979148864746094 5.066978931427002
Loss :  1.6544148921966553 3.186872959136963 4.841287612915039
Loss :  1.6568336486816406 2.953066349029541 4.609899997711182
Loss :  1.6967476606369019 3.5077459812164307 5.204493522644043
Loss :  1.6888625621795654 2.75525164604187 4.4441142082214355
Loss :  1.6664977073669434 3.129822015762329 4.796319961547852
Loss :  1.6343283653259277 2.798304796218872 4.432633399963379
Loss :  1.6596745252609253 2.832045078277588 4.491719722747803
Loss :  1.6521753072738647 3.039262294769287 4.691437721252441
  batch 40 loss: 1.6521753072738647, 3.039262294769287, 4.691437721252441
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602762937545776 3.1812753677368164 4.841551780700684
Loss :  1.6485259532928467 2.4935860633850098 4.142111778259277
Loss :  1.6654623746871948 3.1442205905914307 4.809682846069336
Loss :  1.6637749671936035 2.97694730758667 4.640722274780273
Loss :  1.666690468788147 3.0630507469177246 4.729741096496582
Loss :  1.658107042312622 2.9231178760528564 4.5812249183654785
Loss :  1.6451483964920044 2.9254298210144043 4.570578098297119
Loss :  1.6573104858398438 2.7270288467407227 4.384339332580566
Loss :  1.6301381587982178 3.196505546569824 4.826643943786621
Loss :  1.68126380443573 2.9400887489318848 4.621352672576904
Loss :  1.645838975906372 2.656588077545166 4.302427291870117
Loss :  1.6641321182250977 2.894810438156128 4.558942794799805
Loss :  1.682668685913086 3.1445305347442627 4.8271989822387695
Loss :  1.6674119234085083 3.102290391921997 4.769702434539795
Loss :  1.6706740856170654 3.272221326828003 4.942895412445068
Loss :  1.6349438428878784 2.775996685028076 4.410940647125244
Loss :  1.684628963470459 3.1409192085266113 4.82554817199707
Loss :  1.68221116065979 3.320507049560547 5.002717971801758
Loss :  1.6956193447113037 3.296318531036377 4.991937637329102
Loss :  1.6709057092666626 3.107696533203125 4.778602123260498
  batch 60 loss: 1.6709057092666626, 3.107696533203125, 4.778602123260498
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726090908050537 3.124423027038574 4.797032356262207
Loss :  1.6693750619888306 2.75077748298645 4.42015266418457
Loss :  1.6774221658706665 3.308901309967041 4.986323356628418
Loss :  1.658016562461853 2.708146572113037 4.36616325378418
Loss :  1.655046820640564 2.7689907550811768 4.424037456512451
Loss :  5.483864784240723 4.3890581130981445 9.872922897338867
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.374194622039795 4.318022727966309 9.692216873168945
Loss :  5.48282527923584 4.214021682739258 9.696846961975098
Loss :  4.562547206878662 4.170926570892334 8.733473777770996
Total LOSS train 4.6572076430687535 valid 9.498865127563477
CE LOSS train 1.6640673307272105 valid 1.1406368017196655
Contrastive LOSS train 2.993140301337609 valid 1.0427316427230835
EPOCH 270:
Loss :  1.6504205465316772 2.68757700920105 4.3379974365234375
Loss :  1.666259765625 3.3215479850769043 4.987807750701904
Loss :  1.6521008014678955 3.0596389770507812 4.711739540100098
Loss :  1.6552530527114868 2.983283519744873 4.63853645324707
Loss :  1.6799131631851196 3.410151720046997 5.090065002441406
Loss :  1.6628717184066772 2.7681500911712646 4.431021690368652
Loss :  1.661056637763977 3.3123154640197754 4.973371982574463
Loss :  1.6499254703521729 2.549546957015991 4.199472427368164
Loss :  1.6545791625976562 2.2073545455932617 3.861933708190918
Loss :  1.60904061794281 2.884398937225342 4.493439674377441
Loss :  1.6686623096466064 3.4032716751098633 5.071933746337891
Loss :  1.7292077541351318 2.8916943073272705 4.620902061462402
Loss :  1.6762187480926514 3.010716199874878 4.686934947967529
Loss :  1.6644089221954346 3.2884743213653564 4.952883243560791
Loss :  1.6425853967666626 3.1047563552856445 4.747341632843018
Loss :  1.6521600484848022 3.009171724319458 4.661331653594971
Loss :  1.661146640777588 3.041133403778076 4.702280044555664
Loss :  1.6595311164855957 2.8404643535614014 4.499995231628418
Loss :  1.6675546169281006 2.9337756633758545 4.601330280303955
Loss :  1.624003529548645 2.967914581298828 4.591917991638184
  batch 20 loss: 1.624003529548645, 2.967914581298828, 4.591917991638184
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592748165130615 3.094245433807373 4.7535200119018555
Loss :  1.6750022172927856 2.4863979816436768 4.161400318145752
Loss :  1.646378993988037 3.3331079483032227 4.97948694229126
Loss :  1.6873857975006104 2.899393320083618 4.5867791175842285
Loss :  1.684840202331543 3.3780059814453125 5.0628461837768555
Loss :  1.6484287977218628 2.913508653640747 4.56193733215332
Loss :  1.6973564624786377 3.0935935974121094 4.790949821472168
Loss :  1.6398876905441284 3.1188509464263916 4.7587385177612305
Loss :  1.688067078590393 2.9045755863189697 4.592642784118652
Loss :  1.6413943767547607 2.859128475189209 4.500522613525391
Loss :  1.7226611375808716 3.3432888984680176 5.0659499168396
Loss :  1.6690641641616821 3.3372318744659424 5.006296157836914
Loss :  1.6544150114059448 3.0912604331970215 4.745675563812256
Loss :  1.6568341255187988 2.8865933418273926 4.543427467346191
Loss :  1.6967473030090332 3.545037269592285 5.241784572601318
Loss :  1.6888632774353027 2.740474224090576 4.429337501525879
Loss :  1.6664975881576538 2.816405773162842 4.482903480529785
Loss :  1.634328007698059 2.977175712585449 4.611503601074219
Loss :  1.6596752405166626 3.0231521129608154 4.682827472686768
Loss :  1.6521754264831543 3.1781322956085205 4.830307960510254
  batch 40 loss: 1.6521754264831543, 3.1781322956085205, 4.830307960510254
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.660277247428894 3.362018585205078 5.022295951843262
Loss :  1.6485261917114258 2.4364099502563477 4.084936141967773
Loss :  1.6654623746871948 2.95052170753479 4.615983963012695
Loss :  1.6637744903564453 2.8879928588867188 4.551767349243164
Loss :  1.6666903495788574 2.8582756519317627 4.524966239929199
Loss :  1.6581065654754639 2.845566511154175 4.503673076629639
Loss :  1.6451473236083984 2.6146538257598877 4.259800910949707
Loss :  1.6573094129562378 2.949223756790161 4.606533050537109
Loss :  1.6301374435424805 3.4678943157196045 5.098031997680664
Loss :  1.6812630891799927 3.220578908920288 4.90184211730957
Loss :  1.6458388566970825 2.71472430229187 4.360563278198242
Loss :  1.6641312837600708 2.961132287979126 4.625263690948486
Loss :  1.6826688051223755 3.57680082321167 5.259469509124756
Loss :  1.667410969734192 2.9445881843566895 4.611999034881592
Loss :  1.670673131942749 3.4415805339813232 5.112253665924072
Loss :  1.6349432468414307 2.6316134929656982 4.266556739807129
Loss :  1.6846287250518799 3.635629892349243 5.320258617401123
Loss :  1.6822112798690796 3.247227430343628 4.929438591003418
Loss :  1.6956197023391724 3.256836414337158 4.952455997467041
Loss :  1.6709052324295044 2.984628915786743 4.655534267425537
  batch 60 loss: 1.6709052324295044, 2.984628915786743, 4.655534267425537
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726089715957642 2.9920287132263184 4.664637565612793
Loss :  1.6693744659423828 2.8390116691589355 4.508386135101318
Loss :  1.6774216890335083 3.0678725242614746 4.745294094085693
Loss :  1.658016562461853 2.7723541259765625 4.430370807647705
Loss :  1.6550483703613281 3.1719789505004883 4.827027320861816
Loss :  5.483357906341553 4.380645751953125 9.864004135131836
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.373723030090332 4.318531513214111 9.692255020141602
Loss :  5.482354164123535 4.139040946960449 9.621395111083984
Loss :  4.561929225921631 4.334400653839111 8.896329879760742
Total LOSS train 4.687544830028827 valid 9.518496036529541
CE LOSS train 1.6640672848774836 valid 1.1404823064804077
Contrastive LOSS train 3.0234775689932016 valid 1.0836001634597778
EPOCH 271:
Loss :  1.6504194736480713 2.4952492713928223 4.145668983459473
Loss :  1.666259765625 3.2011282444000244 4.867387771606445
Loss :  1.652100682258606 2.8334908485412598 4.485591411590576
Loss :  1.6552518606185913 2.854525566101074 4.509777545928955
Loss :  1.679913878440857 3.2893218994140625 4.969235897064209
Loss :  1.662871241569519 2.9865732192993164 4.649444580078125
Loss :  1.6610575914382935 3.281233787536621 4.942291259765625
Loss :  1.6499249935150146 2.7223167419433594 4.372241973876953
Loss :  1.6545785665512085 2.1985206604003906 3.8530993461608887
Loss :  1.6090408563613892 2.6628105640411377 4.271851539611816
Loss :  1.668662190437317 3.547633409500122 5.2162957191467285
Loss :  1.729207992553711 2.895128011703491 4.624336242675781
Loss :  1.6762208938598633 3.4693055152893066 5.14552640914917
Loss :  1.6644096374511719 3.3056719303131104 4.970081329345703
Loss :  1.6425853967666626 3.003659725189209 4.646245002746582
Loss :  1.6521594524383545 3.151033878326416 4.803193092346191
Loss :  1.6611465215682983 2.6864802837371826 4.347626686096191
Loss :  1.6595299243927002 2.7637534141540527 4.423283576965332
Loss :  1.6675536632537842 2.674286127090454 4.341839790344238
Loss :  1.6240040063858032 2.9522082805633545 4.576212406158447
  batch 20 loss: 1.6240040063858032, 2.9522082805633545, 4.576212406158447
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592761278152466 2.932969331741333 4.592245578765869
Loss :  1.6750019788742065 2.749957799911499 4.424959659576416
Loss :  1.6463795900344849 3.242920398712158 4.8892998695373535
Loss :  1.6873855590820312 2.9333608150482178 4.620746612548828
Loss :  1.6848409175872803 3.174513578414917 4.859354496002197
Loss :  1.648429274559021 3.3391172885894775 4.987546443939209
Loss :  1.6973572969436646 3.0190365314483643 4.716393947601318
Loss :  1.6398872137069702 3.02466082572937 4.664547920227051
Loss :  1.6880667209625244 2.7422878742218018 4.430354595184326
Loss :  1.6413949728012085 2.837749719619751 4.47914457321167
Loss :  1.722662091255188 3.212841033935547 4.935503005981445
Loss :  1.669063687324524 3.6327896118164062 5.301853179931641
Loss :  1.6544146537780762 3.043788433074951 4.698203086853027
Loss :  1.6568342447280884 2.7284915447235107 4.385325908660889
Loss :  1.69674813747406 3.7756505012512207 5.47239875793457
Loss :  1.6888631582260132 2.7888615131378174 4.477724552154541
Loss :  1.6664973497390747 3.092332601547241 4.7588300704956055
Loss :  1.6343282461166382 3.0471670627593994 4.681495189666748
Loss :  1.6596753597259521 3.0941877365112305 4.753863334655762
Loss :  1.6521755456924438 3.3162035942077637 4.968379020690918
  batch 40 loss: 1.6521755456924438, 3.3162035942077637, 4.968379020690918
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602773666381836 3.117912769317627 4.7781901359558105
Loss :  1.6485267877578735 2.701049566268921 4.349576473236084
Loss :  1.6654630899429321 3.2003581523895264 4.865821361541748
Loss :  1.6637742519378662 2.936389684677124 4.60016393661499
Loss :  1.6666902303695679 2.7901511192321777 4.456841468811035
Loss :  1.658107876777649 3.198085308074951 4.8561930656433105
Loss :  1.6451475620269775 2.8573684692382812 4.50251579284668
Loss :  1.6573100090026855 2.8050596714019775 4.462369918823242
Loss :  1.6301385164260864 3.4806697368621826 5.110808372497559
Loss :  1.6812639236450195 3.0072059631347656 4.688469886779785
Loss :  1.6458394527435303 2.6901557445526123 4.335995197296143
Loss :  1.664131760597229 2.893749952316284 4.557881832122803
Loss :  1.6826682090759277 3.4914917945861816 5.174160003662109
Loss :  1.6674108505249023 2.852952718734741 4.520363807678223
Loss :  1.6706727743148804 3.3722710609436035 5.042943954467773
Loss :  1.6349432468414307 2.7414000034332275 4.376343250274658
Loss :  1.684629201889038 3.4132168292999268 5.097846031188965
Loss :  1.6822105646133423 3.147064685821533 4.829275131225586
Loss :  1.6956197023391724 3.40714430809021 5.102764129638672
Loss :  1.6709060668945312 2.7126524448394775 4.38355827331543
  batch 60 loss: 1.6709060668945312, 2.7126524448394775, 4.38355827331543
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726094484329224 3.1197333335876465 4.792342662811279
Loss :  1.6693748235702515 2.6788039207458496 4.348178863525391
Loss :  1.6774225234985352 3.0962915420532227 4.773714065551758
Loss :  1.6580166816711426 2.609828472137451 4.267845153808594
Loss :  1.655051827430725 3.160493850708008 4.815545558929443
Loss :  5.483269691467285 4.3711981773376465 9.854467391967773
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.373568534851074 4.361077785491943 9.73464584350586
Loss :  5.482250213623047 4.186313152313232 9.668563842773438
Loss :  4.562236785888672 4.314033508300781 8.876270294189453
Total LOSS train 4.682293979938214 valid 9.53348684310913
CE LOSS train 1.6640674994542048 valid 1.140559196472168
Contrastive LOSS train 3.0182264658120963 valid 1.0785083770751953
EPOCH 272:
Loss :  1.6504194736480713 2.7486300468444824 4.399049758911133
Loss :  1.6662602424621582 3.243163585662842 4.909423828125
Loss :  1.6521004438400269 3.2720329761505127 4.92413330078125
Loss :  1.6552520990371704 3.1092681884765625 4.764520168304443
Loss :  1.6799132823944092 3.3672661781311035 5.047179222106934
Loss :  1.66287100315094 2.868609666824341 4.53148078918457
Loss :  1.6610569953918457 3.3018391132354736 4.962896347045898
Loss :  1.6499258279800415 2.5651369094848633 4.215062618255615
Loss :  1.6545789241790771 2.363192558288574 4.0177717208862305
Loss :  1.6090408563613892 2.82507061958313 4.434111595153809
Loss :  1.6686618328094482 3.798373222351074 5.467035293579102
Loss :  1.7292073965072632 2.8883650302886963 4.61757230758667
Loss :  1.6762193441390991 3.159487009048462 4.8357062339782715
Loss :  1.6644099950790405 3.1717121601104736 4.836122035980225
Loss :  1.6425857543945312 3.1443254947662354 4.7869110107421875
Loss :  1.6521599292755127 3.205993175506592 4.858153343200684
Loss :  1.6611462831497192 2.8161234855651855 4.477269649505615
Loss :  1.6595282554626465 2.6688761711120605 4.328404426574707
Loss :  1.6675535440444946 2.952986240386963 4.620539665222168
Loss :  1.624003529548645 3.1309523582458496 4.754955768585205
  batch 20 loss: 1.624003529548645, 3.1309523582458496, 4.754955768585205
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592750549316406 3.115572929382324 4.774847984313965
Loss :  1.6750017404556274 2.6739516258239746 4.3489532470703125
Loss :  1.646379828453064 2.987484931945801 4.633864879608154
Loss :  1.6873847246170044 2.8448197841644287 4.532204627990723
Loss :  1.684839129447937 3.5069782733917236 5.191817283630371
Loss :  1.6484283208847046 3.163644313812256 4.81207275390625
Loss :  1.6973567008972168 2.9655468463897705 4.662903785705566
Loss :  1.6398870944976807 2.8110058307647705 4.450892925262451
Loss :  1.688066840171814 2.6970808506011963 4.385147571563721
Loss :  1.6413952112197876 2.9090142250061035 4.550409317016602
Loss :  1.7226606607437134 3.244185209274292 4.966845989227295
Loss :  1.669062852859497 3.3383893966674805 5.007452011108398
Loss :  1.654414176940918 2.9969184398651123 4.651332855224609
Loss :  1.6568338871002197 2.8099114894866943 4.466745376586914
Loss :  1.6967493295669556 3.7029776573181152 5.399726867675781
Loss :  1.6888625621795654 2.773967742919922 4.462830543518066
Loss :  1.6664971113204956 3.030245780944824 4.696743011474609
Loss :  1.6343278884887695 3.099123954772949 4.733451843261719
Loss :  1.6596741676330566 2.8487820625305176 4.508456230163574
Loss :  1.6521751880645752 3.1094489097595215 4.761624336242676
  batch 40 loss: 1.6521751880645752, 3.1094489097595215, 4.761624336242676
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602765321731567 3.4210479259490967 5.081324577331543
Loss :  1.6485261917114258 2.683568000793457 4.332094192504883
Loss :  1.6654633283615112 3.2002432346343994 4.865706443786621
Loss :  1.6637728214263916 3.0021262168884277 4.665899276733398
Loss :  1.6666895151138306 2.7725491523742676 4.439238548278809
Loss :  1.6581075191497803 2.890085458755493 4.548192977905273
Loss :  1.6451466083526611 3.393024206161499 5.03817081451416
Loss :  1.6573094129562378 2.882479190826416 4.539788722991943
Loss :  1.6301383972167969 3.0946428775787354 4.724781036376953
Loss :  1.6812634468078613 3.278303384780884 4.959567070007324
Loss :  1.6458396911621094 2.738163471221924 4.384003162384033
Loss :  1.6641312837600708 3.0119009017944336 4.676032066345215
Loss :  1.682668924331665 3.3324263095855713 5.015095233917236
Loss :  1.6674095392227173 3.1650655269622803 4.832475185394287
Loss :  1.6706738471984863 3.3882553577423096 5.058929443359375
Loss :  1.6349424123764038 2.7974395751953125 4.432382106781006
Loss :  1.6846290826797485 3.2869763374328613 4.97160530090332
Loss :  1.682210922241211 3.23881196975708 4.921022891998291
Loss :  1.6956195831298828 3.4924588203430176 5.1880784034729
Loss :  1.6709049940109253 2.7961227893829346 4.46702766418457
  batch 60 loss: 1.6709049940109253, 2.7961227893829346, 4.46702766418457
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726100444793701 3.1349446773529053 4.807554721832275
Loss :  1.6693754196166992 2.855719566345215 4.525094985961914
Loss :  1.6774219274520874 3.0598297119140625 4.7372517585754395
Loss :  1.6580157279968262 2.7963638305664062 4.454379558563232
Loss :  1.6550531387329102 3.1506664752960205 4.805719375610352
Loss :  5.483112335205078 4.399285793304443 9.88239860534668
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.3734450340271 4.347886562347412 9.721331596374512
Loss :  5.482153415679932 4.270794868469238 9.752948760986328
Loss :  4.562380313873291 4.254535675048828 8.816915512084961
Total LOSS train 4.711662123753475 valid 9.54339861869812
CE LOSS train 1.6640671968460083 valid 1.1405950784683228
Contrastive LOSS train 3.0475949140695424 valid 1.063633918762207
EPOCH 273:
Loss :  1.6504192352294922 2.6350440979003906 4.285463333129883
Loss :  1.6662594079971313 3.197178602218628 4.863438129425049
Loss :  1.6521003246307373 3.3409740924835205 4.993074417114258
Loss :  1.6552515029907227 3.005608558654785 4.660860061645508
Loss :  1.67991304397583 3.328263998031616 5.008176803588867
Loss :  1.6628702878952026 3.0446629524230957 4.707533359527588
Loss :  1.661057710647583 3.257026433944702 4.918084144592285
Loss :  1.6499251127243042 2.5559985637664795 4.205923557281494
Loss :  1.6545783281326294 2.2471578121185303 3.901736259460449
Loss :  1.6090413331985474 2.9883193969726562 4.597360610961914
Loss :  1.6686620712280273 3.6316771507263184 5.300339221954346
Loss :  1.729205846786499 2.9548230171203613 4.684028625488281
Loss :  1.6762217283248901 3.2001774311065674 4.876399040222168
Loss :  1.6644102334976196 3.6700894832611084 5.334499835968018
Loss :  1.6425838470458984 3.008469820022583 4.651053428649902
Loss :  1.652159571647644 2.969808340072632 4.621967792510986
Loss :  1.661145806312561 2.881406307220459 4.5425519943237305
Loss :  1.6595290899276733 2.754960298538208 4.414489269256592
Loss :  1.6675541400909424 3.1434240341186523 4.810977935791016
Loss :  1.6240026950836182 3.1870110034942627 4.811013698577881
  batch 20 loss: 1.6240026950836182, 3.1870110034942627, 4.811013698577881
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592752933502197 3.1828832626342773 4.842158317565918
Loss :  1.6750019788742065 2.640911817550659 4.315913677215576
Loss :  1.6463805437088013 3.164283275604248 4.81066370010376
Loss :  1.687384009361267 2.97320818901062 4.660592079162598
Loss :  1.684840202331543 3.529186487197876 5.21402645111084
Loss :  1.6484280824661255 3.281118869781494 4.92954683303833
Loss :  1.6973572969436646 3.17378568649292 4.871142864227295
Loss :  1.6398862600326538 3.120513439178467 4.76039981842041
Loss :  1.6880669593811035 2.644662380218506 4.332729339599609
Loss :  1.6413947343826294 3.009446620941162 4.650841236114502
Loss :  1.722661018371582 3.1168458461761475 4.839507102966309
Loss :  1.669062852859497 3.1775879859924316 4.846651077270508
Loss :  1.6544137001037598 3.1851277351379395 4.839541435241699
Loss :  1.6568344831466675 2.801223039627075 4.458057403564453
Loss :  1.6967493295669556 3.4841907024383545 5.1809401512146
Loss :  1.6888628005981445 2.8686373233795166 4.557499885559082
Loss :  1.6664968729019165 2.915151357650757 4.581648349761963
Loss :  1.6343282461166382 3.035677194595337 4.6700053215026855
Loss :  1.6596741676330566 2.963458299636841 4.623132705688477
Loss :  1.6521751880645752 3.067883253097534 4.720058441162109
  batch 40 loss: 1.6521751880645752, 3.067883253097534, 4.720058441162109
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602767705917358 3.3059163093566895 4.966193199157715
Loss :  1.6485261917114258 2.402174949645996 4.050701141357422
Loss :  1.6654634475708008 3.4418768882751465 5.107340335845947
Loss :  1.6637712717056274 3.075061082839966 4.738832473754883
Loss :  1.6666896343231201 2.544114351272583 4.210803985595703
Loss :  1.6581076383590698 3.042607545852661 4.700715065002441
Loss :  1.6451469659805298 2.9762156009674072 4.621362686157227
Loss :  1.6573095321655273 2.986260414123535 4.6435699462890625
Loss :  1.6301380395889282 3.3293542861938477 4.959492206573486
Loss :  1.6812645196914673 3.115079641342163 4.79634428024292
Loss :  1.6458394527435303 2.730607509613037 4.376446723937988
Loss :  1.6641322374343872 2.908066511154175 4.572198867797852
Loss :  1.682667851448059 3.4060721397399902 5.08873987197876
Loss :  1.6674096584320068 2.8966145515441895 4.564023971557617
Loss :  1.6706737279891968 3.540613889694214 5.211287498474121
Loss :  1.634941577911377 2.7336883544921875 4.3686299324035645
Loss :  1.6846293210983276 3.1397247314453125 4.82435417175293
Loss :  1.68221116065979 3.2847604751586914 4.966971397399902
Loss :  1.6956197023391724 3.178649663925171 4.874269485473633
Loss :  1.6709046363830566 2.8648407459259033 4.535745620727539
  batch 60 loss: 1.6709046363830566, 2.8648407459259033, 4.535745620727539
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726092100143433 3.311957836151123 4.984567165374756
Loss :  1.6693754196166992 2.557894468307495 4.227270126342773
Loss :  1.6774215698242188 3.369414806365967 5.0468363761901855
Loss :  1.658015251159668 2.7070019245147705 4.365016937255859
Loss :  1.6550549268722534 3.2156434059143066 4.87069845199585
Loss :  5.483320713043213 4.3845648765563965 9.86788558959961
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.373681545257568 4.322253227233887 9.695934295654297
Loss :  5.482347011566162 4.279415607452393 9.761762619018555
Loss :  4.56227445602417 4.25816535949707 8.820440292358398
Total LOSS train 4.7164067635169395 valid 9.536505699157715
CE LOSS train 1.6640671546642596 valid 1.1405686140060425
Contrastive LOSS train 3.052339634528527 valid 1.0645413398742676
EPOCH 274:
Loss :  1.6504195928573608 2.7087438106536865 4.359163284301758
Loss :  1.6662578582763672 3.3831629753112793 5.0494208335876465
Loss :  1.6521008014678955 3.06501841545105 4.717119216918945
Loss :  1.6552520990371704 3.17193865776062 4.82719087600708
Loss :  1.6799125671386719 3.3294284343719482 5.009341239929199
Loss :  1.6628705263137817 2.992307186126709 4.655177593231201
Loss :  1.6610575914382935 3.6918118000030518 5.352869510650635
Loss :  1.6499245166778564 2.885646343231201 4.535571098327637
Loss :  1.6545777320861816 2.4446003437042236 4.099178314208984
Loss :  1.609041690826416 2.7298741340637207 4.338915824890137
Loss :  1.6686623096466064 3.3231849670410156 4.991847038269043
Loss :  1.7292059659957886 2.707991600036621 4.437197685241699
Loss :  1.6762195825576782 3.0304322242736816 4.70665168762207
Loss :  1.6644095182418823 3.21122145652771 4.875630855560303
Loss :  1.6425834894180298 3.1170542240142822 4.759637832641602
Loss :  1.652159333229065 3.0596396923065186 4.711799144744873
Loss :  1.6611459255218506 2.9596261978149414 4.620772361755371
Loss :  1.6595299243927002 2.7530887126922607 4.412618637084961
Loss :  1.6675541400909424 3.1089558601379395 4.776510238647461
Loss :  1.62400221824646 3.137065887451172 4.761068344116211
  batch 20 loss: 1.62400221824646, 3.137065887451172, 4.761068344116211
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592752933502197 2.8964169025421143 4.555692195892334
Loss :  1.675002098083496 2.4444684982299805 4.119470596313477
Loss :  1.646378993988037 3.6327121257781982 5.279090881347656
Loss :  1.6873847246170044 2.795393466949463 4.482778072357178
Loss :  1.6848400831222534 3.3177707195281982 5.002610683441162
Loss :  1.6484273672103882 3.179729461669922 4.8281569480896
Loss :  1.6973576545715332 2.9622292518615723 4.6595869064331055
Loss :  1.639886498451233 3.1223275661468506 4.762214183807373
Loss :  1.6880667209625244 2.8861899375915527 4.574256896972656
Loss :  1.6413938999176025 3.1673359870910645 4.808730125427246
Loss :  1.7226614952087402 3.1403985023498535 4.863059997558594
Loss :  1.6690642833709717 3.29286789894104 4.961932182312012
Loss :  1.6544138193130493 3.0134427547454834 4.667856693267822
Loss :  1.6568337678909302 2.774928092956543 4.431761741638184
Loss :  1.6967496871948242 3.3571360111236572 5.053885459899902
Loss :  1.6888624429702759 2.709219217300415 4.3980817794799805
Loss :  1.6664974689483643 2.9379093647003174 4.604406833648682
Loss :  1.6343283653259277 2.8696343898773193 4.503962516784668
Loss :  1.6596744060516357 3.115217685699463 4.7748918533325195
Loss :  1.652174711227417 3.2776031494140625 4.929778099060059
  batch 40 loss: 1.652174711227417, 3.2776031494140625, 4.929778099060059
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602765321731567 3.3476245403289795 5.007901191711426
Loss :  1.6485265493392944 2.6812782287597656 4.32980489730835
Loss :  1.6654638051986694 3.0302364826202393 4.695700168609619
Loss :  1.6637712717056274 2.8066036701202393 4.470375061035156
Loss :  1.6666903495788574 2.470968723297119 4.137659072875977
Loss :  1.658107042312622 2.9002938270568848 4.558401107788086
Loss :  1.6451469659805298 3.261420488357544 4.906567573547363
Loss :  1.657309651374817 2.849916696548462 4.507226467132568
Loss :  1.6301379203796387 3.251566171646118 4.881704330444336
Loss :  1.6812649965286255 3.046570062637329 4.727835178375244
Loss :  1.6458388566970825 2.7888705730438232 4.434709548950195
Loss :  1.66413152217865 3.0515644550323486 4.715695858001709
Loss :  1.6826673746109009 3.488860607147217 5.171527862548828
Loss :  1.6674102544784546 2.8644044399261475 4.5318145751953125
Loss :  1.6706734895706177 3.4336230754852295 5.104296684265137
Loss :  1.6349408626556396 2.5310304164886475 4.165971279144287
Loss :  1.6846290826797485 3.3631510734558105 5.0477800369262695
Loss :  1.6822118759155273 3.4659392833709717 5.148151397705078
Loss :  1.695618748664856 3.4790306091308594 5.174649238586426
Loss :  1.6709048748016357 2.8957200050354004 4.566624641418457
  batch 60 loss: 1.6709048748016357, 2.8957200050354004, 4.566624641418457
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726094484329224 2.750725030899048 4.42333459854126
Loss :  1.6693754196166992 2.8922743797302246 4.561649799346924
Loss :  1.6774230003356934 3.1849446296691895 4.862367630004883
Loss :  1.6580160856246948 3.146563768386841 4.804579734802246
Loss :  1.655053734779358 2.962132215499878 4.617186069488525
Loss :  5.483518600463867 4.295234203338623 9.778753280639648
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.373855113983154 4.375792026519775 9.74964714050293
Loss :  5.48251485824585 4.197201728820801 9.679716110229492
Loss :  4.562584400177002 4.042149543762207 8.604734420776367
Total LOSS train 4.704821542593149 valid 9.45321273803711
CE LOSS train 1.6640670904746422 valid 1.1406461000442505
Contrastive LOSS train 3.0407544209406927 valid 1.0105373859405518
EPOCH 275:
Loss :  1.6504203081130981 2.7500486373901367 4.400468826293945
Loss :  1.6662594079971313 3.3392205238342285 5.00547981262207
Loss :  1.6520999670028687 2.8642630577087402 4.516363143920898
Loss :  1.65525221824646 3.011629343032837 4.666881561279297
Loss :  1.679913878440857 3.4356582164764404 5.115571975708008
Loss :  1.6628711223602295 2.894111156463623 4.556982040405273
Loss :  1.6610583066940308 3.2712602615356445 4.932318687438965
Loss :  1.649924874305725 2.5197906494140625 4.169715404510498
Loss :  1.6545780897140503 2.247657299041748 3.902235507965088
Loss :  1.6090413331985474 2.615452766418457 4.224493980407715
Loss :  1.6686627864837646 3.2935495376586914 4.962212562561035
Loss :  1.7292070388793945 2.9281678199768066 4.657374858856201
Loss :  1.6762186288833618 3.187103033065796 4.863321781158447
Loss :  1.6644099950790405 3.279771089553833 4.944180965423584
Loss :  1.6425845623016357 2.8345108032226562 4.477095603942871
Loss :  1.6521594524383545 3.098771333694458 4.7509307861328125
Loss :  1.6611464023590088 3.055853843688965 4.7170000076293945
Loss :  1.6595302820205688 2.948023796081543 4.607553958892822
Loss :  1.6675547361373901 2.565983772277832 4.233538627624512
Loss :  1.624002456665039 3.099282741546631 4.72328519821167
  batch 20 loss: 1.624002456665039, 3.099282741546631, 4.72328519821167
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592761278152466 3.1728203296661377 4.832096576690674
Loss :  1.6750019788742065 2.569310188293457 4.244312286376953
Loss :  1.646378755569458 3.3310658931732178 4.977444648742676
Loss :  1.6873862743377686 2.717725992202759 4.405112266540527
Loss :  1.6848403215408325 3.3501851558685303 5.035025596618652
Loss :  1.648427963256836 3.1848793029785156 4.833307266235352
Loss :  1.697356939315796 3.0300912857055664 4.727448463439941
Loss :  1.6398866176605225 2.906810998916626 4.546697616577148
Loss :  1.6880664825439453 2.8251500129699707 4.513216495513916
Loss :  1.6413955688476562 2.6596601009368896 4.301055908203125
Loss :  1.7226612567901611 3.2492477893829346 4.971909046173096
Loss :  1.6690642833709717 3.1496851444244385 4.81874942779541
Loss :  1.6544139385223389 2.9348480701446533 4.589262008666992
Loss :  1.6568338871002197 2.8061234951019287 4.462957382202148
Loss :  1.6967487335205078 3.1620945930480957 4.8588433265686035
Loss :  1.6888618469238281 2.6741013526916504 4.3629631996154785
Loss :  1.6664973497390747 3.0116257667541504 4.6781229972839355
Loss :  1.6343286037445068 2.8941004276275635 4.52842903137207
Loss :  1.6596754789352417 2.792713165283203 4.452388763427734
Loss :  1.6521745920181274 3.288949728012085 4.941124439239502
  batch 40 loss: 1.6521745920181274, 3.288949728012085, 4.941124439239502
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602765321731567 3.3916423320770264 5.051918983459473
Loss :  1.648526668548584 2.5976033210754395 4.246129989624023
Loss :  1.6654636859893799 2.9630987644195557 4.6285624504089355
Loss :  1.663771629333496 2.7174038887023926 4.381175518035889
Loss :  1.6666903495788574 2.5995004177093506 4.266190528869629
Loss :  1.6581072807312012 3.0982840061187744 4.756391525268555
Loss :  1.6451467275619507 3.0636916160583496 4.70883846282959
Loss :  1.6573100090026855 2.813201665878296 4.470511436462402
Loss :  1.6301379203796387 3.560556650161743 5.190694808959961
Loss :  1.6812646389007568 3.125394344329834 4.806658744812012
Loss :  1.6458394527435303 2.745349168777466 4.391188621520996
Loss :  1.664131999015808 2.893409252166748 4.557541370391846
Loss :  1.6826667785644531 3.413675546646118 5.096342086791992
Loss :  1.6674104928970337 2.7535955905914307 4.421006202697754
Loss :  1.67067289352417 3.5184924602508545 5.189165115356445
Loss :  1.6349420547485352 2.8254520893096924 4.460393905639648
Loss :  1.6846295595169067 3.4410083293914795 5.125638008117676
Loss :  1.68221116065979 3.2468576431274414 4.929068565368652
Loss :  1.6956188678741455 3.4362757205963135 5.131894588470459
Loss :  1.6709052324295044 2.5242555141448975 4.195160865783691
  batch 60 loss: 1.6709052324295044, 2.5242555141448975, 4.195160865783691
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.672609567642212 3.3662400245666504 5.038849830627441
Loss :  1.6693758964538574 2.6958296298980713 4.365205764770508
Loss :  1.677423357963562 3.1758134365081787 4.853236675262451
Loss :  1.6580162048339844 2.712002992630005 4.37001895904541
Loss :  1.6550501585006714 2.8785128593444824 4.533563137054443
Loss :  5.483486175537109 4.3935418128967285 9.87702751159668
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.37384557723999 4.415277481079102 9.78912353515625
Loss :  5.482506275177002 4.1647047996521 9.647211074829102
Loss :  4.562569618225098 4.328685283660889 8.891254425048828
Total LOSS train 4.656504895136907 valid 9.551154136657715
CE LOSS train 1.6640672610356257 valid 1.1406424045562744
Contrastive LOSS train 2.9924376267653243 valid 1.0821713209152222
EPOCH 276:
Loss :  1.6504210233688354 2.8938684463500977 4.544289588928223
Loss :  1.6662592887878418 3.3138136863708496 4.980072975158691
Loss :  1.652099609375 3.0140812397003174 4.666180610656738
Loss :  1.6552523374557495 3.164429187774658 4.819681644439697
Loss :  1.6799129247665405 3.2461016178131104 4.926014423370361
Loss :  1.66287100315094 2.8037216663360596 4.466592788696289
Loss :  1.661057949066162 3.0562994480133057 4.717357635498047
Loss :  1.6499249935150146 2.709768533706665 4.35969352722168
Loss :  1.6545790433883667 2.392129421234131 4.046708583831787
Loss :  1.6090409755706787 2.4459941387176514 4.05503511428833
Loss :  1.668662190437317 3.149216651916504 4.817878723144531
Loss :  1.7292077541351318 3.0070786476135254 4.736286163330078
Loss :  1.676217794418335 3.04502534866333 4.721242904663086
Loss :  1.664410948753357 3.2051517963409424 4.86956262588501
Loss :  1.6425851583480835 3.0186104774475098 4.661195755004883
Loss :  1.652159333229065 3.105968713760376 4.7581281661987305
Loss :  1.6611467599868774 2.683027982711792 4.344174861907959
Loss :  1.6595295667648315 2.730071783065796 4.389601230621338
Loss :  1.6675549745559692 2.7157037258148193 4.383258819580078
Loss :  1.624002456665039 3.037672758102417 4.661675453186035
  batch 20 loss: 1.624002456665039, 3.037672758102417, 4.661675453186035
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592758893966675 3.037508249282837 4.696784019470215
Loss :  1.6750028133392334 2.7411375045776367 4.416140556335449
Loss :  1.646378517150879 3.2791378498077393 4.925516128540039
Loss :  1.6873853206634521 2.794177770614624 4.481563091278076
Loss :  1.6848403215408325 3.454425096511841 5.139265537261963
Loss :  1.6484273672103882 3.179607391357422 4.8280348777771
Loss :  1.6973563432693481 3.1050546169281006 4.802411079406738
Loss :  1.639886736869812 3.157017946243286 4.796904563903809
Loss :  1.688066840171814 2.7774927616119385 4.465559482574463
Loss :  1.6413947343826294 2.9508159160614014 4.59221076965332
Loss :  1.7226613759994507 3.30788516998291 5.03054666519165
Loss :  1.6690638065338135 3.6004815101623535 5.269545555114746
Loss :  1.654414415359497 3.367553234100342 5.021967887878418
Loss :  1.6568338871002197 2.8525633811950684 4.509397506713867
Loss :  1.6967501640319824 3.4402878284454346 5.137038230895996
Loss :  1.6888624429702759 2.7734358310699463 4.462298393249512
Loss :  1.6664973497390747 3.2340917587280273 4.9005889892578125
Loss :  1.6343297958374023 3.0989036560058594 4.733233451843262
Loss :  1.6596757173538208 2.988088369369507 4.647764205932617
Loss :  1.6521743535995483 3.0334346294403076 4.685608863830566
  batch 40 loss: 1.6521743535995483, 3.0334346294403076, 4.685608863830566
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602767705917358 3.5161828994750977 5.176459789276123
Loss :  1.6485265493392944 2.6491875648498535 4.2977142333984375
Loss :  1.6654642820358276 3.1629016399383545 4.828365802764893
Loss :  1.6637697219848633 2.9872066974639893 4.650976181030273
Loss :  1.6666902303695679 2.562253713607788 4.228943824768066
Loss :  1.6581071615219116 3.2731502056121826 4.931257247924805
Loss :  1.6451460123062134 3.329432487487793 4.974578380584717
Loss :  1.6573107242584229 2.68157696723938 4.338887691497803
Loss :  1.6301381587982178 3.4298055171966553 5.059943675994873
Loss :  1.6812645196914673 3.1521520614624023 4.83341646194458
Loss :  1.645838975906372 2.6276981830596924 4.2735371589660645
Loss :  1.664131999015808 3.033710479736328 4.697842597961426
Loss :  1.682666540145874 3.912276268005371 5.594943046569824
Loss :  1.6674113273620605 3.0174434185028076 4.684854507446289
Loss :  1.6706734895706177 3.4801442623138428 5.15081787109375
Loss :  1.6349406242370605 2.786252021789551 4.421192646026611
Loss :  1.6846295595169067 3.1499688625335693 4.834598541259766
Loss :  1.6822118759155273 2.9668004512786865 4.649012565612793
Loss :  1.6956192255020142 3.340385675430298 5.036005020141602
Loss :  1.6709048748016357 3.046271800994873 4.71717643737793
  batch 60 loss: 1.6709048748016357, 3.046271800994873, 4.71717643737793
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726089715957642 3.654909372329712 5.327518463134766
Loss :  1.6693757772445679 2.734457492828369 4.403833389282227
Loss :  1.6774216890335083 3.059572696685791 4.73699426651001
Loss :  1.658015489578247 2.788363456726074 4.446378707885742
Loss :  1.6550506353378296 3.163562536239624 4.818613052368164
Loss :  5.4833292961120605 4.368477821350098 9.851806640625
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.373685836791992 4.37628698348999 9.74997329711914
Loss :  5.482359886169434 4.227308750152588 9.70966911315918
Loss :  4.562585353851318 4.079582691192627 8.642168045043945
Total LOSS train 4.716628815577581 valid 9.488404273986816
CE LOSS train 1.6640672225218553 valid 1.1406463384628296
Contrastive LOSS train 3.0525615765498233 valid 1.0198956727981567
EPOCH 277:
Loss :  1.6504212617874146 2.4821786880493164 4.132599830627441
Loss :  1.6662575006484985 3.280259132385254 4.946516513824463
Loss :  1.6521000862121582 2.9996862411499023 4.6517863273620605
Loss :  1.6552530527114868 2.781416893005371 4.436669826507568
Loss :  1.6799120903015137 3.2249133586883545 4.904825210571289
Loss :  1.66287100315094 2.8699629306793213 4.532834053039551
Loss :  1.6610573530197144 3.4325804710388184 5.093637943267822
Loss :  1.6499240398406982 2.591169834136963 4.241093635559082
Loss :  1.6545789241790771 2.3531293869018555 4.007708549499512
Loss :  1.6090409755706787 2.628309726715088 4.2373504638671875
Loss :  1.6686620712280273 3.707975149154663 5.3766374588012695
Loss :  1.729207158088684 2.8523101806640625 4.581517219543457
Loss :  1.6762198209762573 3.237304210662842 4.913524150848389
Loss :  1.6644092798233032 3.0674591064453125 4.731868267059326
Loss :  1.6425843238830566 3.0397963523864746 4.682380676269531
Loss :  1.652158260345459 2.9727325439453125 4.6248908042907715
Loss :  1.6611464023590088 2.776076078414917 4.437222480773926
Loss :  1.6595312356948853 2.938570499420166 4.598101615905762
Loss :  1.6675547361373901 2.7478487491607666 4.415403366088867
Loss :  1.6240030527114868 2.9120965003967285 4.536099433898926
  batch 20 loss: 1.6240030527114868, 2.9120965003967285, 4.536099433898926
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592748165130615 3.0354390144348145 4.694713592529297
Loss :  1.675003170967102 2.4420106410980225 4.117013931274414
Loss :  1.646376609802246 3.264500617980957 4.910877227783203
Loss :  1.687385082244873 2.635039806365967 4.32242488861084
Loss :  1.6848396062850952 3.56994366645813 5.2547831535339355
Loss :  1.6484262943267822 3.4114670753479004 5.059893608093262
Loss :  1.6973562240600586 3.06308650970459 4.760442733764648
Loss :  1.6398863792419434 3.008441209793091 4.648327827453613
Loss :  1.6880669593811035 3.045656681060791 4.7337236404418945
Loss :  1.641392707824707 2.849984884262085 4.491377830505371
Loss :  1.722660779953003 3.218419075012207 4.941080093383789
Loss :  1.6690640449523926 3.2381629943847656 4.907227039337158
Loss :  1.654414415359497 2.9560861587524414 4.610500335693359
Loss :  1.6568336486816406 2.6070024967193604 4.263835906982422
Loss :  1.6967494487762451 3.4741671085357666 5.170916557312012
Loss :  1.6888620853424072 2.6790215969085693 4.367883682250977
Loss :  1.6664968729019165 3.322190284729004 4.988687038421631
Loss :  1.6343296766281128 2.9392459392547607 4.573575496673584
Loss :  1.6596759557724 3.147488594055176 4.807164669036865
Loss :  1.6521743535995483 3.3095972537994385 4.961771488189697
  batch 40 loss: 1.6521743535995483, 3.3095972537994385, 4.961771488189697
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602767705917358 3.601738691329956 5.262015342712402
Loss :  1.6485261917114258 2.6537020206451416 4.302227973937988
Loss :  1.6654638051986694 3.2923007011413574 4.957764625549316
Loss :  1.6637707948684692 2.7008016109466553 4.364572525024414
Loss :  1.666689395904541 2.575146436691284 4.241835594177246
Loss :  1.6581065654754639 3.17242693901062 4.830533504486084
Loss :  1.6451473236083984 3.0718941688537598 4.717041492462158
Loss :  1.6573097705841064 2.7926254272460938 4.449934959411621
Loss :  1.6301381587982178 3.219646453857422 4.849784851074219
Loss :  1.68126380443573 3.1744754314422607 4.855739116668701
Loss :  1.6458386182785034 2.6388871669769287 4.284725666046143
Loss :  1.6641324758529663 2.7719476222991943 4.436079978942871
Loss :  1.682666540145874 3.467297315597534 5.149963855743408
Loss :  1.6674119234085083 2.7663140296936035 4.433725833892822
Loss :  1.6706736087799072 3.3041279315948486 4.974801540374756
Loss :  1.6349416971206665 2.590681552886963 4.22562313079834
Loss :  1.684629201889038 3.011303186416626 4.695932388305664
Loss :  1.6822121143341064 3.262134313583374 4.9443464279174805
Loss :  1.695619821548462 3.376854658126831 5.072474479675293
Loss :  1.670905590057373 3.1106247901916504 4.781530380249023
  batch 60 loss: 1.670905590057373, 3.1106247901916504, 4.781530380249023
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.672608494758606 2.99346661567688 4.666075229644775
Loss :  1.6693750619888306 2.5651354789733887 4.23451042175293
Loss :  1.6774210929870605 3.141108989715576 4.818530082702637
Loss :  1.658015489578247 2.84928560256958 4.507301330566406
Loss :  1.6550573110580444 2.9551267623901367 4.610184192657471
Loss :  5.483124732971191 4.408217430114746 9.891342163085938
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.373506546020508 4.419281482696533 9.792787551879883
Loss :  5.482151985168457 4.279046058654785 9.761198043823242
Loss :  4.562464714050293 4.34005069732666 8.902515411376953
Total LOSS train 4.666709899902344 valid 9.586960792541504
CE LOSS train 1.6640671289884128 valid 1.1406161785125732
Contrastive LOSS train 3.0026427929218 valid 1.085012674331665
EPOCH 278:
Loss :  1.650421142578125 2.7551686763763428 4.405590057373047
Loss :  1.6662575006484985 3.2010302543640137 4.867287635803223
Loss :  1.6521004438400269 3.2342000007629395 4.886300563812256
Loss :  1.6552526950836182 3.0161643028259277 4.671417236328125
Loss :  1.6799124479293823 3.2618086338043213 4.941720962524414
Loss :  1.6628705263137817 2.8306386470794678 4.493509292602539
Loss :  1.6610571146011353 3.342707872390747 5.003765106201172
Loss :  1.6499238014221191 2.8259315490722656 4.475855350494385
Loss :  1.654578685760498 2.2593207359313965 3.9138994216918945
Loss :  1.6090401411056519 2.73895001411438 4.347990036010742
Loss :  1.6686625480651855 3.2199270725250244 4.888589859008789
Loss :  1.7292075157165527 2.9289278984069824 4.658135414123535
Loss :  1.6762217283248901 3.0695528984069824 4.745774745941162
Loss :  1.6644086837768555 3.410888671875 5.0752973556518555
Loss :  1.6425836086273193 3.132659435272217 4.775242805480957
Loss :  1.652159333229065 3.0257985591888428 4.677958011627197
Loss :  1.6611469984054565 2.798236608505249 4.459383487701416
Loss :  1.6595317125320435 2.886847972869873 4.546379566192627
Loss :  1.6675556898117065 3.000476598739624 4.668032169342041
Loss :  1.6240031719207764 3.041586399078369 4.665589332580566
  batch 20 loss: 1.6240031719207764, 3.041586399078369, 4.665589332580566
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592742204666138 3.0301356315612793 4.6894097328186035
Loss :  1.6750025749206543 2.6854004859924316 4.360403060913086
Loss :  1.6463760137557983 3.269266366958618 4.915642261505127
Loss :  1.6873849630355835 2.9610605239868164 4.6484456062316895
Loss :  1.6848397254943848 3.2491061687469482 4.933945655822754
Loss :  1.648425817489624 3.2551982402801514 4.903624057769775
Loss :  1.697356104850769 2.9998910427093506 4.69724702835083
Loss :  1.6398861408233643 2.781172275543213 4.421058654785156
Loss :  1.6880671977996826 2.737640619277954 4.425707817077637
Loss :  1.6413929462432861 2.940025806427002 4.581418991088867
Loss :  1.7226603031158447 3.3665482997894287 5.089208602905273
Loss :  1.6690651178359985 3.111981153488159 4.781046390533447
Loss :  1.6544142961502075 3.018495798110962 4.672910213470459
Loss :  1.6568331718444824 2.8068795204162598 4.463712692260742
Loss :  1.6967499256134033 3.535252332687378 5.232002258300781
Loss :  1.6888628005981445 2.855942726135254 4.544805526733398
Loss :  1.6664971113204956 3.2244668006896973 4.890964031219482
Loss :  1.6343309879302979 2.8018853664398193 4.436216354370117
Loss :  1.659676432609558 3.1531472206115723 4.81282377243042
Loss :  1.6521743535995483 3.057677745819092 4.70985221862793
  batch 40 loss: 1.6521743535995483, 3.057677745819092, 4.70985221862793
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602773666381836 3.6611592769622803 5.321436882019043
Loss :  1.648526668548584 2.606665849685669 4.255192756652832
Loss :  1.665464162826538 3.315044403076172 4.980508804321289
Loss :  1.6637705564498901 2.844672441482544 4.5084428787231445
Loss :  1.6666903495788574 2.7069921493530273 4.373682498931885
Loss :  1.6581069231033325 3.096989393234253 4.755096435546875
Loss :  1.6451475620269775 3.251561403274536 4.896708965301514
Loss :  1.6573106050491333 2.7507283687591553 4.408039093017578
Loss :  1.630138874053955 3.536480188369751 5.166619300842285
Loss :  1.6812652349472046 3.328341245651245 5.00960636138916
Loss :  1.6458390951156616 3.0102813243865967 4.656120300292969
Loss :  1.664131999015808 2.9582345485687256 4.622366428375244
Loss :  1.682666540145874 3.47719407081604 5.159860610961914
Loss :  1.6674119234085083 2.946669816970825 4.614081859588623
Loss :  1.6706734895706177 3.5551106929779053 5.2257843017578125
Loss :  1.6349406242370605 2.8020083904266357 4.436948776245117
Loss :  1.6846290826797485 3.3884687423706055 5.0730977058410645
Loss :  1.682212471961975 3.252021312713623 4.934233665466309
Loss :  1.6956188678741455 3.3797011375427246 5.075320243835449
Loss :  1.6709049940109253 2.8136041164398193 4.484508991241455
  batch 60 loss: 1.6709049940109253, 2.8136041164398193, 4.484508991241455
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.672608733177185 3.280529737472534 4.95313835144043
Loss :  1.669376254081726 2.688295841217041 4.357672214508057
Loss :  1.6774215698242188 3.2167952060699463 4.894216537475586
Loss :  1.658015489578247 2.7465834617614746 4.404599189758301
Loss :  1.6550517082214355 3.1124563217163086 4.767508029937744
Loss :  5.4832963943481445 4.354851245880127 9.83814811706543
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.3736138343811035 4.3681206703186035 9.741734504699707
Loss :  5.482310771942139 4.2956461906433105 9.77795696258545
Loss :  4.562609672546387 4.2599592208862305 8.822568893432617
Total LOSS train 4.718660838787372 valid 9.5451021194458
CE LOSS train 1.6640671821740958 valid 1.1406524181365967
Contrastive LOSS train 3.054593636439397 valid 1.0649898052215576
EPOCH 279:
Loss :  1.6504204273223877 2.7323238849639893 4.382744312286377
Loss :  1.6662577390670776 3.2790770530700684 4.9453349113464355
Loss :  1.6521005630493164 3.0191826820373535 4.67128324508667
Loss :  1.6552534103393555 2.9487791061401367 4.604032516479492
Loss :  1.679912805557251 3.2337357997894287 4.91364860534668
Loss :  1.6628705263137817 2.863846778869629 4.526717185974121
Loss :  1.661057710647583 3.3492605686187744 5.010318279266357
Loss :  1.6499238014221191 2.6889419555664062 4.338865756988525
Loss :  1.6545783281326294 2.393984794616699 4.048563003540039
Loss :  1.60904061794281 2.5272114276885986 4.136251926422119
Loss :  1.668662190437317 3.5713982582092285 5.240060329437256
Loss :  1.7292081117630005 2.834408760070801 4.563616752624512
Loss :  1.6762202978134155 3.109524965286255 4.785745143890381
Loss :  1.6644095182418823 3.5927271842956543 5.257136821746826
Loss :  1.6425834894180298 3.110564708709717 4.753148078918457
Loss :  1.6521596908569336 3.2640020847320557 4.91616153717041
Loss :  1.6611467599868774 2.7144010066986084 4.375547885894775
Loss :  1.6595304012298584 2.8295698165893555 4.489100456237793
Loss :  1.6675546169281006 2.9587182998657227 4.626273155212402
Loss :  1.624003291130066 2.7689666748046875 4.392970085144043
  batch 20 loss: 1.624003291130066, 2.7689666748046875, 4.392970085144043
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592748165130615 3.065666437149048 4.724941253662109
Loss :  1.6750026941299438 2.510941505432129 4.185944080352783
Loss :  1.6463760137557983 3.2104547023773193 4.856830596923828
Loss :  1.6873854398727417 2.721592903137207 4.408978462219238
Loss :  1.6848398447036743 3.6403093338012695 5.325149059295654
Loss :  1.6484252214431763 3.455687999725342 5.1041131019592285
Loss :  1.6973564624786377 2.8874027729034424 4.58475923538208
Loss :  1.6398853063583374 2.7890138626098633 4.42889928817749
Loss :  1.688066840171814 3.1063544750213623 4.794421195983887
Loss :  1.6413928270339966 2.7758164405822754 4.417209148406982
Loss :  1.722659945487976 3.2946553230285645 5.01731538772583
Loss :  1.6690653562545776 3.2587320804595947 4.927797317504883
Loss :  1.6544134616851807 2.9184231758117676 4.572836875915527
Loss :  1.6568328142166138 2.6939163208007812 4.3507490158081055
Loss :  1.696750283241272 3.3001174926757812 4.996867656707764
Loss :  1.6888623237609863 2.763598918914795 4.452461242675781
Loss :  1.6664974689483643 3.035933494567871 4.702430725097656
Loss :  1.6343309879302979 2.9225785732269287 4.556909561157227
Loss :  1.6596754789352417 2.999945640563965 4.659621238708496
Loss :  1.652173638343811 3.2354776859283447 4.887651443481445
  batch 40 loss: 1.652173638343811, 3.2354776859283447, 4.887651443481445
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.660277247428894 3.434098243713379 5.0943756103515625
Loss :  1.6485259532928467 2.4207050800323486 4.069231033325195
Loss :  1.6654642820358276 3.1234703063964844 4.788934707641602
Loss :  1.6637691259384155 2.7930455207824707 4.456814765930176
Loss :  1.6666899919509888 2.6548476219177246 4.321537494659424
Loss :  1.6581069231033325 3.122042417526245 4.780149459838867
Loss :  1.645146369934082 3.360530138015747 5.00567626953125
Loss :  1.6573103666305542 2.74458384513855 4.4018940925598145
Loss :  1.630138874053955 3.5147528648376465 5.144891738891602
Loss :  1.6812653541564941 3.077571392059326 4.75883674621582
Loss :  1.6458386182785034 2.767016887664795 4.412855625152588
Loss :  1.6641308069229126 2.873319387435913 4.537450313568115
Loss :  1.6826661825180054 3.645054578781128 5.327720642089844
Loss :  1.6674104928970337 2.8593122959136963 4.5267229080200195
Loss :  1.670674443244934 3.283538341522217 4.954212665557861
Loss :  1.6349396705627441 2.614954710006714 4.249894142150879
Loss :  1.6846290826797485 3.493014335632324 5.177643299102783
Loss :  1.6822127103805542 3.1184241771698 4.8006367683410645
Loss :  1.6956186294555664 3.2662477493286133 4.96186637878418
Loss :  1.6709039211273193 3.1121277809143066 4.783031463623047
  batch 60 loss: 1.6709039211273193, 3.1121277809143066, 4.783031463623047
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.672608494758606 3.116715431213379 4.789323806762695
Loss :  1.6693758964538574 2.7942593097686768 4.463635444641113
Loss :  1.677420735359192 3.2500150203704834 4.927435874938965
Loss :  1.658015251159668 2.911881446838379 4.569896697998047
Loss :  1.655048131942749 3.0427732467651367 4.697821617126465
Loss :  5.483213424682617 4.388365745544434 9.87157917022705
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.373532295227051 4.382079124450684 9.755611419677734
Loss :  5.482202053070068 4.275025367736816 9.757226943969727
Loss :  4.562536716461182 4.110720634460449 8.673257827758789
Total LOSS train 4.691321468353271 valid 9.514418840408325
CE LOSS train 1.6640669089097244 valid 1.1406341791152954
Contrastive LOSS train 3.027254570447482 valid 1.0276801586151123
EPOCH 280:
Loss :  1.6504203081130981 2.612421751022339 4.262842178344727
Loss :  1.6662572622299194 3.238037586212158 4.904294967651367
Loss :  1.6520999670028687 3.024587869644165 4.676687717437744
Loss :  1.6552519798278809 2.9688756465911865 4.624127388000488
Loss :  1.6799125671386719 3.2181990146636963 4.898111343383789
Loss :  1.6628695726394653 2.929947853088379 4.592817306518555
Loss :  1.6610578298568726 3.1343894004821777 4.79544734954834
Loss :  1.64992356300354 2.4487502574920654 4.0986738204956055
Loss :  1.6545785665512085 2.311972141265869 3.966550827026367
Loss :  1.6090404987335205 2.687457323074341 4.296497821807861
Loss :  1.6686620712280273 3.4674715995788574 5.136133670806885
Loss :  1.7292089462280273 2.881861925125122 4.61107063293457
Loss :  1.676220417022705 3.1400794982910156 4.816299915313721
Loss :  1.6644104719161987 3.2681071758270264 4.9325175285339355
Loss :  1.642583966255188 2.991335153579712 4.6339192390441895
Loss :  1.6521599292755127 3.1294198036193848 4.781579971313477
Loss :  1.6611467599868774 2.813662528991699 4.474809169769287
Loss :  1.6595301628112793 3.2143750190734863 4.873905181884766
Loss :  1.667554497718811 2.6969428062438965 4.364497184753418
Loss :  1.624003529548645 3.175168752670288 4.799172401428223
  batch 20 loss: 1.624003529548645, 3.175168752670288, 4.799172401428223
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592750549316406 3.0906565189361572 4.749931335449219
Loss :  1.6750024557113647 2.6662936210632324 4.341296195983887
Loss :  1.6463764905929565 3.167299509048462 4.813675880432129
Loss :  1.687386155128479 2.9009993076324463 4.588385581970215
Loss :  1.6848403215408325 3.3148984909057617 4.999738693237305
Loss :  1.6484251022338867 3.1637308597564697 4.812155723571777
Loss :  1.6973564624786377 2.982588768005371 4.67994499206543
Loss :  1.639885425567627 2.7812905311584473 4.421175956726074
Loss :  1.6880667209625244 2.632779121398926 4.320845603942871
Loss :  1.6413928270339966 2.835256814956665 4.476649761199951
Loss :  1.7226598262786865 3.3503975868225098 5.073057174682617
Loss :  1.6690647602081299 3.0547728538513184 4.723837852478027
Loss :  1.6544134616851807 3.0354273319244385 4.689840793609619
Loss :  1.6568326950073242 2.6222798824310303 4.279112815856934
Loss :  1.6967486143112183 3.3892576694488525 5.086006164550781
Loss :  1.6888628005981445 2.809479236602783 4.498342037200928
Loss :  1.6664975881576538 2.647386312484741 4.3138837814331055
Loss :  1.6343306303024292 2.941532850265503 4.575863361358643
Loss :  1.6596759557724 3.0930533409118652 4.752729415893555
Loss :  1.6521743535995483 3.4405324459075928 5.092706680297852
  batch 40 loss: 1.6521743535995483, 3.4405324459075928, 5.092706680297852
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602774858474731 3.5144951343536377 5.1747727394104
Loss :  1.6485264301300049 2.3303866386413574 3.9789130687713623
Loss :  1.6654635667800903 2.724626302719116 4.390089988708496
Loss :  1.6637705564498901 2.8231470584869385 4.486917495727539
Loss :  1.666690468788147 2.637188196182251 4.3038787841796875
Loss :  1.6581071615219116 2.8805439472198486 4.538650989532471
Loss :  1.6451469659805298 3.3193717002868652 4.9645185470581055
Loss :  1.6573108434677124 2.6552107334136963 4.312521457672119
Loss :  1.6301387548446655 3.4219563007354736 5.05209493637085
Loss :  1.6812655925750732 3.0675101280212402 4.748775482177734
Loss :  1.6458395719528198 2.9075369834899902 4.5533766746521
Loss :  1.6641308069229126 2.8423171043395996 4.506447792053223
Loss :  1.682666301727295 3.6321499347686768 5.314816474914551
Loss :  1.6674120426177979 2.977234363555908 4.644646644592285
Loss :  1.6706732511520386 3.5255088806152344 5.1961822509765625
Loss :  1.6349400281906128 2.7149481773376465 4.349888324737549
Loss :  1.6846294403076172 3.280726671218872 4.96535587310791
Loss :  1.6822125911712646 3.318835496902466 5.0010480880737305
Loss :  1.6956193447113037 3.2874598503112793 4.983078956604004
Loss :  1.6709046363830566 2.6443750858306885 4.315279960632324
  batch 60 loss: 1.6709046363830566, 2.6443750858306885, 4.315279960632324
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.672608494758606 3.0419795513153076 4.714588165283203
Loss :  1.6693761348724365 2.4676642417907715 4.137040138244629
Loss :  1.6774208545684814 3.081233263015747 4.7586541175842285
Loss :  1.658015251159668 2.9438107013702393 4.601825714111328
Loss :  1.6550546884536743 3.0213520526885986 4.6764068603515625
Loss :  5.482980728149414 4.369096279144287 9.85207748413086
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.3733320236206055 4.361854076385498 9.735185623168945
Loss :  5.481962203979492 4.284466743469238 9.76642894744873
Loss :  4.5622100830078125 4.3296709060668945 8.891880989074707
Total LOSS train 4.653829306822557 valid 9.56139326095581
CE LOSS train 1.6640671051465548 valid 1.1405525207519531
Contrastive LOSS train 2.9897622255178598 valid 1.0824177265167236
EPOCH 281:
Loss :  1.6504210233688354 2.7820303440093994 4.432451248168945
Loss :  1.6662572622299194 3.2233474254608154 4.889604568481445
Loss :  1.6521008014678955 3.3723692893981934 5.024470329284668
Loss :  1.6552529335021973 3.130772352218628 4.786025047302246
Loss :  1.679911494255066 3.442434072494507 5.122345447540283
Loss :  1.662870168685913 2.820387363433838 4.483257293701172
Loss :  1.6610572338104248 3.481801748275757 5.142858982086182
Loss :  1.6499230861663818 2.789738178253174 4.439661026000977
Loss :  1.6545783281326294 2.1869590282440186 3.8415374755859375
Loss :  1.6090408563613892 2.595832109451294 4.204873085021973
Loss :  1.668662667274475 3.3321988582611084 5.000861644744873
Loss :  1.7292081117630005 2.9404332637786865 4.669641494750977
Loss :  1.6762205362319946 3.037026882171631 4.713247299194336
Loss :  1.6644096374511719 3.1802995204925537 4.844709396362305
Loss :  1.6425834894180298 2.9779577255249023 4.620541095733643
Loss :  1.6521598100662231 2.9596896171569824 4.611849308013916
Loss :  1.6611464023590088 2.77744197845459 4.4385881423950195
Loss :  1.6595314741134644 2.7262887954711914 4.385820388793945
Loss :  1.6675562858581543 2.97082257270813 4.638379096984863
Loss :  1.624002456665039 2.8523738384246826 4.476376533508301
  batch 20 loss: 1.624002456665039, 2.8523738384246826, 4.476376533508301
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592745780944824 2.9480979442596436 4.607372283935547
Loss :  1.6750025749206543 2.6167447566986084 4.291747093200684
Loss :  1.6463762521743774 3.4551548957824707 5.101531028747559
Loss :  1.6873849630355835 2.762995481491089 4.450380325317383
Loss :  1.6848405599594116 3.386752128601074 5.071592807769775
Loss :  1.6484248638153076 3.1796557903289795 4.828080654144287
Loss :  1.69735586643219 3.0417912006378174 4.739147186279297
Loss :  1.6398853063583374 2.7625367641448975 4.402421951293945
Loss :  1.6880667209625244 2.977940797805786 4.6660075187683105
Loss :  1.6413931846618652 2.8210976123809814 4.462491035461426
Loss :  1.7226600646972656 3.2636733055114746 4.98633337020874
Loss :  1.669065237045288 3.3172049522399902 4.986269950866699
Loss :  1.6544139385223389 3.196568727493286 4.850982666015625
Loss :  1.656833291053772 2.6858956813812256 4.342729091644287
Loss :  1.6967484951019287 3.420677423477173 5.117425918579102
Loss :  1.6888631582260132 2.7429263591766357 4.431789398193359
Loss :  1.6664965152740479 3.1957457065582275 4.862242221832275
Loss :  1.6343309879302979 3.1362407207489014 4.770571708679199
Loss :  1.6596763134002686 3.125335216522217 4.785011291503906
Loss :  1.652174472808838 3.0873749256134033 4.73954963684082
  batch 40 loss: 1.652174472808838, 3.0873749256134033, 4.73954963684082
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602774858474731 3.206319570541382 4.8665971755981445
Loss :  1.6485254764556885 2.52608060836792 4.1746063232421875
Loss :  1.6654630899429321 3.2139892578125 4.879452228546143
Loss :  1.6637707948684692 2.569237470626831 4.23300838470459
Loss :  1.666690468788147 2.736231565475464 4.4029221534729
Loss :  1.658107042312622 3.1439497470855713 4.802056789398193
Loss :  1.6451473236083984 3.1082355976104736 4.753382682800293
Loss :  1.6573104858398438 2.713433265686035 4.370743751525879
Loss :  1.63013756275177 3.340338945388794 4.9704766273498535
Loss :  1.6812646389007568 3.200913906097412 4.88217830657959
Loss :  1.6458401679992676 2.511009931564331 4.1568498611450195
Loss :  1.66413152217865 2.7603514194488525 4.424482822418213
Loss :  1.6826664209365845 3.4643545150756836 5.1470208168029785
Loss :  1.6674124002456665 2.8911004066467285 4.5585126876831055
Loss :  1.6706733703613281 3.48850154876709 5.159174919128418
Loss :  1.6349406242370605 2.843473196029663 4.4784135818481445
Loss :  1.6846297979354858 3.3603596687316895 5.044989585876465
Loss :  1.6822129487991333 3.160428047180176 4.8426408767700195
Loss :  1.6956188678741455 3.374218225479126 5.0698370933532715
Loss :  1.670904278755188 2.9938576221466064 4.664762020111084
  batch 60 loss: 1.670904278755188, 2.9938576221466064, 4.664762020111084
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726083755493164 3.09978985786438 4.772397994995117
Loss :  1.6693766117095947 2.579483985900879 4.2488603591918945
Loss :  1.6774218082427979 3.1498162746429443 4.827238082885742
Loss :  1.658015251159668 2.901625156402588 4.559640407562256
Loss :  1.6550548076629639 2.9585087299346924 4.613563537597656
Loss :  5.483153820037842 4.390128135681152 9.873281478881836
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.37352180480957 4.374665260314941 9.748187065124512
Loss :  5.48213005065918 4.280379772186279 9.762510299682617
Loss :  4.562217712402344 4.25869083404541 8.820908546447754
Total LOSS train 4.679455155592699 valid 9.55122184753418
CE LOSS train 1.6640671234864455 valid 1.140554428100586
Contrastive LOSS train 3.0153880596160887 valid 1.0646727085113525
EPOCH 282:
Loss :  1.6504215002059937 2.6336629390716553 4.284084320068359
Loss :  1.666257381439209 3.5107057094573975 5.176962852478027
Loss :  1.6521008014678955 3.4262404441833496 5.078341484069824
Loss :  1.6552529335021973 2.945059299468994 4.600312232971191
Loss :  1.6799112558364868 3.355809450149536 5.0357208251953125
Loss :  1.6628706455230713 3.167137384414673 4.830008029937744
Loss :  1.6610567569732666 3.168517827987671 4.8295745849609375
Loss :  1.6499229669570923 2.4029953479766846 4.052918434143066
Loss :  1.6545780897140503 2.2116315364837646 3.8662095069885254
Loss :  1.6090412139892578 2.829493999481201 4.438535213470459
Loss :  1.6686625480651855 3.0271124839782715 4.695775032043457
Loss :  1.7292076349258423 2.8444554805755615 4.573663234710693
Loss :  1.676220417022705 3.059262752532959 4.735483169555664
Loss :  1.664409875869751 3.4437053203582764 5.108115196228027
Loss :  1.6425836086273193 3.2012429237365723 4.8438262939453125
Loss :  1.6521599292755127 3.1305253505706787 4.782685279846191
Loss :  1.6611462831497192 2.7656657695770264 4.426812171936035
Loss :  1.6595314741134644 3.065553903579712 4.725085258483887
Loss :  1.667556881904602 2.936634063720703 4.604190826416016
Loss :  1.6240029335021973 3.1033143997192383 4.7273173332214355
  batch 20 loss: 1.6240029335021973, 3.1033143997192383, 4.7273173332214355
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592748165130615 3.007798194885254 4.6670732498168945
Loss :  1.6750017404556274 2.758124828338623 4.433126449584961
Loss :  1.6463762521743774 3.2548651695251465 4.901241302490234
Loss :  1.6873846054077148 2.8274362087249756 4.5148210525512695
Loss :  1.6848397254943848 3.5044336318969727 5.189273357391357
Loss :  1.6484242677688599 3.1040403842926025 4.752464771270752
Loss :  1.6973559856414795 3.0939126014709473 4.791268348693848
Loss :  1.6398857831954956 2.77017879486084 4.410064697265625
Loss :  1.6880671977996826 2.9640209674835205 4.652088165283203
Loss :  1.6413925886154175 2.9369184970855713 4.578310966491699
Loss :  1.7226603031158447 3.381972312927246 5.104632377624512
Loss :  1.669065237045288 3.059100389480591 4.728165626525879
Loss :  1.6544139385223389 2.988290548324585 4.642704486846924
Loss :  1.656833291053772 2.7560925483703613 4.412925720214844
Loss :  1.69674813747406 3.35699462890625 5.0537428855896
Loss :  1.6888631582260132 2.762596368789673 4.4514594078063965
Loss :  1.6664973497390747 3.04422664642334 4.710723876953125
Loss :  1.634331226348877 3.0373997688293457 4.671730995178223
Loss :  1.6596758365631104 2.967404842376709 4.627080917358398
Loss :  1.6521742343902588 3.077989101409912 4.73016357421875
  batch 40 loss: 1.6521742343902588, 3.077989101409912, 4.73016357421875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602771282196045 3.2849276065826416 4.945204734802246
Loss :  1.6485260725021362 2.553800106048584 4.20232629776001
Loss :  1.6654635667800903 3.186241388320923 4.851705074310303
Loss :  1.663770318031311 2.9829635620117188 4.64673376083374
Loss :  1.6666914224624634 2.5239837169647217 4.190675258636475
Loss :  1.6581071615219116 2.909703016281128 4.56781005859375
Loss :  1.6451472043991089 3.1719725131988525 4.817119598388672
Loss :  1.6573106050491333 2.9743359088897705 4.631646633148193
Loss :  1.6301381587982178 3.2075393199920654 4.837677478790283
Loss :  1.6812660694122314 3.2189407348632812 4.900206565856934
Loss :  1.6458396911621094 2.7881133556365967 4.433953285217285
Loss :  1.66413152217865 2.9234626293182373 4.587594032287598
Loss :  1.6826664209365845 3.582366943359375 5.26503324508667
Loss :  1.6674126386642456 3.023402214050293 4.690814971923828
Loss :  1.6706739664077759 3.220458984375 4.891132831573486
Loss :  1.634940266609192 2.726331949234009 4.36127233505249
Loss :  1.6846293210983276 3.317852258682251 5.002481460571289
Loss :  1.6822129487991333 3.072237253189087 4.75445032119751
Loss :  1.6956185102462769 3.1512486934661865 4.846867084503174
Loss :  1.6709041595458984 2.9922730922698975 4.663177490234375
  batch 60 loss: 1.6709041595458984, 2.9922730922698975, 4.663177490234375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726078987121582 3.2190818786621094 4.891689777374268
Loss :  1.6693764925003052 2.7558772563934326 4.425253868103027
Loss :  1.6774227619171143 3.0171632766723633 4.694585800170898
Loss :  1.6580148935317993 2.8555052280426025 4.513520240783691
Loss :  1.6550544500350952 2.8363027572631836 4.491357326507568
Loss :  5.483236789703369 4.334616184234619 9.817852973937988
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.3735527992248535 4.361734867095947 9.7352876663208
Loss :  5.482173919677734 4.2978715896606445 9.780045509338379
Loss :  4.562295436859131 4.162203788757324 8.724498748779297
Total LOSS train 4.685276508331299 valid 9.514421224594116
CE LOSS train 1.6640671454943143 valid 1.1405738592147827
Contrastive LOSS train 3.0212093610029953 valid 1.040550947189331
EPOCH 283:
Loss :  1.6504216194152832 2.721808910369873 4.372230529785156
Loss :  1.6662577390670776 3.1601204872131348 4.826378345489502
Loss :  1.6521013975143433 3.009615659713745 4.661716938018799
Loss :  1.6552540063858032 3.2481236457824707 4.903377532958984
Loss :  1.6799126863479614 3.4682319164276123 5.148144721984863
Loss :  1.6628706455230713 3.1510961055755615 4.813966751098633
Loss :  1.6610569953918457 3.2898898124694824 4.950946807861328
Loss :  1.6499226093292236 2.5561022758483887 4.206025123596191
Loss :  1.6545782089233398 2.3115622997283936 3.9661405086517334
Loss :  1.609041452407837 2.5621957778930664 4.171236991882324
Loss :  1.668662190437317 3.5399231910705566 5.208585262298584
Loss :  1.7292085886001587 2.8631794452667236 4.592388153076172
Loss :  1.6762185096740723 2.8714330196380615 4.547651290893555
Loss :  1.6644095182418823 3.082104444503784 4.746513843536377
Loss :  1.6425838470458984 3.1042141914367676 4.746798038482666
Loss :  1.6521605253219604 3.095874786376953 4.748035430908203
Loss :  1.6611464023590088 2.942591905593872 4.603738307952881
Loss :  1.6595314741134644 2.6329541206359863 4.29248571395874
Loss :  1.667556881904602 2.8914878368377686 4.55904483795166
Loss :  1.6240028142929077 3.262054443359375 4.886057376861572
  batch 20 loss: 1.6240028142929077, 3.262054443359375, 4.886057376861572
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592750549316406 3.303999185562134 4.963274002075195
Loss :  1.6750026941299438 2.7341132164001465 4.409115791320801
Loss :  1.6463747024536133 3.466656446456909 5.113031387329102
Loss :  1.6873859167099 2.8838508129119873 4.571236610412598
Loss :  1.684840202331543 3.3176329135894775 5.002472877502441
Loss :  1.6484242677688599 3.097057819366455 4.745481967926025
Loss :  1.6973562240600586 2.863762855529785 4.561119079589844
Loss :  1.6398851871490479 2.9219162464141846 4.561801433563232
Loss :  1.6880666017532349 2.697435140609741 4.385501861572266
Loss :  1.6413925886154175 2.8424742221832275 4.4838666915893555
Loss :  1.7226605415344238 3.3174750804901123 5.040135383605957
Loss :  1.669066309928894 3.190082311630249 4.8591485023498535
Loss :  1.6544137001037598 3.134004831314087 4.788418769836426
Loss :  1.656833291053772 2.8583438396453857 4.515177249908447
Loss :  1.6967483758926392 3.5042526721954346 5.201001167297363
Loss :  1.688862681388855 2.6788249015808105 4.367687702178955
Loss :  1.6664972305297852 3.17849063873291 4.844987869262695
Loss :  1.6343319416046143 3.1771674156188965 4.81149959564209
Loss :  1.659676194190979 3.0926320552825928 4.752308368682861
Loss :  1.6521741151809692 2.8715975284576416 4.5237717628479
  batch 40 loss: 1.6521741151809692, 2.8715975284576416, 4.5237717628479
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602771282196045 3.455946922302246 5.11622428894043
Loss :  1.6485261917114258 2.737056255340576 4.385582447052002
Loss :  1.6654640436172485 3.047915458679199 4.713379383087158
Loss :  1.6637701988220215 2.8487374782562256 4.512507438659668
Loss :  1.6666908264160156 2.710782766342163 4.377473831176758
Loss :  1.6581076383590698 3.0070366859436035 4.665144443511963
Loss :  1.6451475620269775 3.1564152240753174 4.801562786102295
Loss :  1.6573121547698975 2.560429573059082 4.217741966247559
Loss :  1.6301381587982178 3.1373603343963623 4.76749849319458
Loss :  1.681265950202942 3.0733706951141357 4.754636764526367
Loss :  1.6458394527435303 2.724982261657715 4.370821952819824
Loss :  1.6641312837600708 2.875521421432495 4.5396528244018555
Loss :  1.682666301727295 3.3426756858825684 5.025341987609863
Loss :  1.6674127578735352 3.0361390113830566 4.703551769256592
Loss :  1.6706738471984863 3.0637435913085938 4.73441743850708
Loss :  1.634940266609192 2.7685601711273193 4.403500556945801
Loss :  1.6846293210983276 3.3979105949401855 5.082540035247803
Loss :  1.6822134256362915 3.288522958755493 4.970736503601074
Loss :  1.6956181526184082 3.410520315170288 5.106138229370117
Loss :  1.6709043979644775 2.9205033779144287 4.591407775878906
  batch 60 loss: 1.6709043979644775, 2.9205033779144287, 4.591407775878906
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726090908050537 3.1105377674102783 4.783146858215332
Loss :  1.6693776845932007 2.7776007652282715 4.446978569030762
Loss :  1.6774232387542725 3.2926535606384277 4.970076560974121
Loss :  1.6580157279968262 2.8131327629089355 4.471148490905762
Loss :  1.6550543308258057 3.1772749423980713 4.832329273223877
Loss :  5.483244895935059 4.389866352081299 9.873111724853516
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.373575687408447 4.317417621612549 9.690993309020996
Loss :  5.482203960418701 4.3327460289001465 9.814949989318848
Loss :  4.562193393707275 4.215723514556885 8.77791690826416
Total LOSS train 4.689169865388137 valid 9.53924298286438
CE LOSS train 1.6640673087193416 valid 1.1405483484268188
Contrastive LOSS train 3.0251025383289045 valid 1.0539308786392212
EPOCH 284:
Loss :  1.6504223346710205 2.765254259109497 4.415676593780518
Loss :  1.6662585735321045 3.2393009662628174 4.905559539794922
Loss :  1.6521005630493164 3.1002933979034424 4.75239372253418
Loss :  1.6552540063858032 3.215832471847534 4.871086597442627
Loss :  1.6799132823944092 3.3811819553375244 5.061095237731934
Loss :  1.6628715991973877 2.87125563621521 4.534127235412598
Loss :  1.6610573530197144 3.543177604675293 5.204235076904297
Loss :  1.6499226093292236 2.4741594791412354 4.124082088470459
Loss :  1.6545785665512085 2.2569501399993896 3.9115285873413086
Loss :  1.6090412139892578 2.703446865081787 4.312488079071045
Loss :  1.6686625480651855 3.453583240509033 5.122245788574219
Loss :  1.7292098999023438 2.914469003677368 4.643678665161133
Loss :  1.6762194633483887 3.15273118019104 4.828950881958008
Loss :  1.6644102334976196 3.116111993789673 4.780522346496582
Loss :  1.6425833702087402 2.9226014614105225 4.565184593200684
Loss :  1.6521614789962769 3.072209119796753 4.72437047958374
Loss :  1.6611474752426147 2.831207513809204 4.492354869842529
Loss :  1.6595314741134644 2.9709889888763428 4.630520343780518
Loss :  1.6675575971603394 3.0318479537963867 4.699405670166016
Loss :  1.6240031719207764 2.8917784690856934 4.515781402587891
  batch 20 loss: 1.6240031719207764, 2.8917784690856934, 4.515781402587891
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.659274935722351 3.104780912399292 4.7640557289123535
Loss :  1.675003170967102 2.687894582748413 4.362897872924805
Loss :  1.6463751792907715 3.271292209625244 4.917667388916016
Loss :  1.687386393547058 2.725862979888916 4.413249492645264
Loss :  1.6848413944244385 3.137484550476074 4.822325706481934
Loss :  1.6484239101409912 3.0176124572753906 4.666036605834961
Loss :  1.6973570585250854 3.006173849105835 4.703530788421631
Loss :  1.6398855447769165 3.178408145904541 4.818293571472168
Loss :  1.688066840171814 2.69911527633667 4.387182235717773
Loss :  1.6413934230804443 2.8608574867248535 4.502250671386719
Loss :  1.7226606607437134 3.4910497665405273 5.213710308074951
Loss :  1.669066071510315 3.1833035945892334 4.852369785308838
Loss :  1.6544135808944702 2.9981846809387207 4.6525983810424805
Loss :  1.656833291053772 2.774977922439575 4.431811332702637
Loss :  1.6967483758926392 3.3576502799987793 5.054398536682129
Loss :  1.688862681388855 2.733937978744507 4.422800540924072
Loss :  1.6664971113204956 2.7898809909820557 4.456377983093262
Loss :  1.6343321800231934 3.1011288166046143 4.735461235046387
Loss :  1.6596760749816895 2.9504923820495605 4.61016845703125
Loss :  1.6521741151809692 3.031078577041626 4.683252811431885
  batch 40 loss: 1.6521741151809692, 3.031078577041626, 4.683252811431885
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602767705917358 3.4580800533294678 5.118356704711914
Loss :  1.6485260725021362 2.6535069942474365 4.302032947540283
Loss :  1.665463924407959 2.9067800045013428 4.572243690490723
Loss :  1.6637693643569946 2.9031004905700684 4.566869735717773
Loss :  1.6666905879974365 2.638392925262451 4.305083274841309
Loss :  1.658107876777649 3.3832545280456543 5.041362285614014
Loss :  1.645147442817688 3.1687169075012207 4.813864231109619
Loss :  1.657310962677002 2.7986977100372314 4.4560089111328125
Loss :  1.6301385164260864 3.2443807125091553 4.874519348144531
Loss :  1.6812667846679688 3.365894079208374 5.047161102294922
Loss :  1.6458386182785034 2.6478235721588135 4.293662071228027
Loss :  1.664131760597229 2.9304542541503906 4.59458589553833
Loss :  1.6826647520065308 3.3420028686523438 5.024667739868164
Loss :  1.667410969734192 2.877026319503784 4.544437408447266
Loss :  1.670673131942749 3.489079713821411 5.15975284576416
Loss :  1.634940266609192 2.5811262130737305 4.216066360473633
Loss :  1.6846296787261963 3.5076043605804443 5.192234039306641
Loss :  1.6822123527526855 2.9163448810577393 4.598557472229004
Loss :  1.6956183910369873 3.333071708679199 5.028690338134766
Loss :  1.6709046363830566 2.5938351154327393 4.264739990234375
  batch 60 loss: 1.6709046363830566, 2.5938351154327393, 4.264739990234375
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726094484329224 3.197730779647827 4.870340347290039
Loss :  1.6693764925003052 2.7944488525390625 4.463825225830078
Loss :  1.6774221658706665 3.2244977951049805 4.901919841766357
Loss :  1.6580160856246948 2.8774468898773193 4.535462856292725
Loss :  1.6550474166870117 3.095822334289551 4.7508697509765625
Loss :  5.483382701873779 4.409405708312988 9.89278793334961
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.3737311363220215 4.304976940155029 9.67870807647705
Loss :  5.482325553894043 4.2291412353515625 9.711466789245605
Loss :  4.562312602996826 4.1804633140563965 8.742775917053223
Total LOSS train 4.678508333059457 valid 9.506434679031372
CE LOSS train 1.6640672812095054 valid 1.1405781507492065
Contrastive LOSS train 3.0144410646878756 valid 1.0451158285140991
EPOCH 285:
Loss :  1.6504204273223877 2.648850202560425 4.2992706298828125
Loss :  1.6662575006484985 3.4244043827056885 5.090662002563477
Loss :  1.6520992517471313 3.165877342224121 4.817976474761963
Loss :  1.6552523374557495 2.8897013664245605 4.5449538230896
Loss :  1.6799122095108032 3.3692269325256348 5.049139022827148
Loss :  1.6628706455230713 2.8907012939453125 4.553571701049805
Loss :  1.661057710647583 3.2921359539031982 4.953193664550781
Loss :  1.649922251701355 2.6696555614471436 4.319577693939209
Loss :  1.654577612876892 2.3645193576812744 4.019096851348877
Loss :  1.609041452407837 2.425062656402588 4.034104347229004
Loss :  1.6686619520187378 3.178227424621582 4.846889495849609
Loss :  1.72920823097229 2.921194553375244 4.650403022766113
Loss :  1.6762171983718872 2.8451473712921143 4.521364688873291
Loss :  1.6644110679626465 3.3933193683624268 5.057730674743652
Loss :  1.6425837278366089 3.2285842895507812 4.87116813659668
Loss :  1.6521599292755127 3.1807548999786377 4.83291482925415
Loss :  1.6611462831497192 2.9069619178771973 4.568108081817627
Loss :  1.6595287322998047 2.892564058303833 4.552092552185059
Loss :  1.6675561666488647 2.6786515712738037 4.346207618713379
Loss :  1.624002456665039 3.017047643661499 4.641050338745117
  batch 20 loss: 1.624002456665039, 3.017047643661499, 4.641050338745117
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592756509780884 3.1337902545928955 4.793066024780273
Loss :  1.675002098083496 2.6926558017730713 4.367657661437988
Loss :  1.6463762521743774 3.124377965927124 4.770754337310791
Loss :  1.6873852014541626 2.7616348266601562 4.449019908905029
Loss :  1.6848400831222534 3.20086932182312 4.885709285736084
Loss :  1.648423671722412 2.98455810546875 4.632981777191162
Loss :  1.6973563432693481 3.0900774002075195 4.787433624267578
Loss :  1.6398850679397583 3.0352659225463867 4.6751508712768555
Loss :  1.688066840171814 2.834296464920044 4.522363185882568
Loss :  1.6413923501968384 2.929652452468872 4.571044921875
Loss :  1.7226593494415283 3.6504931449890137 5.373152732849121
Loss :  1.6690633296966553 3.090406894683838 4.759469985961914
Loss :  1.6544127464294434 2.879300594329834 4.533713340759277
Loss :  1.6568326950073242 2.7901275157928467 4.44696044921875
Loss :  1.696746826171875 3.7038776874542236 5.4006242752075195
Loss :  1.6888624429702759 2.706878423690796 4.395740985870361
Loss :  1.6664972305297852 3.2901132106781006 4.956610679626465
Loss :  1.63433039188385 3.005474090576172 4.639804363250732
Loss :  1.6596758365631104 3.0247254371643066 4.684401512145996
Loss :  1.6521742343902588 3.03000545501709 4.6821794509887695
  batch 40 loss: 1.6521742343902588, 3.03000545501709, 4.6821794509887695
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602758169174194 3.295637369155884 4.955913066864014
Loss :  1.6485257148742676 2.739933729171753 4.388459205627441
Loss :  1.6654636859893799 2.923513650894165 4.588977336883545
Loss :  1.663769006729126 2.986135721206665 4.649904727935791
Loss :  1.6666896343231201 2.7956326007843018 4.462322235107422
Loss :  1.6581076383590698 3.1424741744995117 4.800581932067871
Loss :  1.6451456546783447 3.3855483531951904 5.030694007873535
Loss :  1.657309889793396 2.825942277908325 4.483252048492432
Loss :  1.6301385164260864 3.1030349731445312 4.733173370361328
Loss :  1.681266188621521 3.11154842376709 4.7928147315979
Loss :  1.645838975906372 2.760474681854248 4.406313896179199
Loss :  1.6641311645507812 2.724957227706909 4.3890886306762695
Loss :  1.6826658248901367 3.32584810256958 5.008513927459717
Loss :  1.667409896850586 3.1550161838531494 4.822425842285156
Loss :  1.670673131942749 3.306736707687378 4.977409839630127
Loss :  1.6349401473999023 2.8429722785949707 4.477912425994873
Loss :  1.6846297979354858 3.189894437789917 4.874524116516113
Loss :  1.6822127103805542 3.2425389289855957 4.9247517585754395
Loss :  1.6956186294555664 3.3533904552459717 5.049009323120117
Loss :  1.6709046363830566 2.8274331092834473 4.498337745666504
  batch 60 loss: 1.6709046363830566, 2.8274331092834473, 4.498337745666504
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.672609567642212 3.100163221359253 4.772772789001465
Loss :  1.669376254081726 2.7662911415100098 4.435667514801025
Loss :  1.6774227619171143 2.9262826442718506 4.603705406188965
Loss :  1.6580168008804321 2.7342169284820557 4.392233848571777
Loss :  1.6550475358963013 2.949028968811035 4.604076385498047
Loss :  5.483344078063965 4.397671222686768 9.88101577758789
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.373696327209473 4.4379987716674805 9.811695098876953
Loss :  5.482292175292969 4.294197082519531 9.7764892578125
Loss :  4.562283992767334 4.304814338684082 8.867097854614258
Total LOSS train 4.677233094435472 valid 9.5840744972229
CE LOSS train 1.6640666980009813 valid 1.1405709981918335
Contrastive LOSS train 3.0131663909325233 valid 1.0762035846710205
EPOCH 286:
Loss :  1.650421380996704 2.614264488220215 4.26468563079834
Loss :  1.666258454322815 3.1942191123962402 4.860477447509766
Loss :  1.6520994901657104 3.0979843139648438 4.750083923339844
Loss :  1.6552530527114868 2.969269275665283 4.6245222091674805
Loss :  1.6799129247665405 3.213948965072632 4.893861770629883
Loss :  1.662870168685913 2.933565378189087 4.596435546875
Loss :  1.661058783531189 3.3099365234375 4.9709954261779785
Loss :  1.649922251701355 2.64172625541687 4.2916483879089355
Loss :  1.6545777320861816 2.206855058670044 3.8614327907562256
Loss :  1.6090415716171265 2.870224952697754 4.47926664352417
Loss :  1.668662667274475 3.2943942546844482 4.963057041168213
Loss :  1.7292083501815796 3.0339953899383545 4.7632036209106445
Loss :  1.6762183904647827 3.193986177444458 4.870204448699951
Loss :  1.6644110679626465 3.079901933670044 4.7443132400512695
Loss :  1.6425832509994507 3.2907752990722656 4.933358669281006
Loss :  1.6521600484848022 3.1282784938812256 4.780438423156738
Loss :  1.661146640777588 2.605100393295288 4.266246795654297
Loss :  1.6595289707183838 3.0333662033081055 4.69289493560791
Loss :  1.6675560474395752 2.919769048690796 4.587325096130371
Loss :  1.6240015029907227 2.941446542739868 4.565447807312012
  batch 20 loss: 1.6240015029907227, 2.941446542739868, 4.565447807312012
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.659276008605957 3.1087357997894287 4.768012046813965
Loss :  1.6750026941299438 2.4572105407714844 4.132213115692139
Loss :  1.6463762521743774 3.169806718826294 4.816183090209961
Loss :  1.6873854398727417 2.7606475353240967 4.448032855987549
Loss :  1.6848403215408325 3.2203481197357178 4.90518856048584
Loss :  1.648423671722412 3.2352068424224854 4.883630752563477
Loss :  1.6973562240600586 3.002167224884033 4.699523448944092
Loss :  1.6398853063583374 3.0142886638641357 4.654173851013184
Loss :  1.6880667209625244 2.844794750213623 4.532861709594727
Loss :  1.6413929462432861 2.9030649662017822 4.544457912445068
Loss :  1.722659945487976 3.4089529514312744 5.131612777709961
Loss :  1.6690648794174194 3.1841206550598145 4.853185653686523
Loss :  1.6544122695922852 3.118776559829712 4.773188591003418
Loss :  1.6568329334259033 2.791926622390747 4.44875955581665
Loss :  1.696747064590454 3.443321704864502 5.140069007873535
Loss :  1.6888623237609863 2.791785478591919 4.480648040771484
Loss :  1.6664968729019165 2.8310511112213135 4.4975481033325195
Loss :  1.6343309879302979 2.8094518184661865 4.443782806396484
Loss :  1.6596755981445312 3.1310694217681885 4.790744781494141
Loss :  1.6521739959716797 3.3560569286346436 5.008231163024902
  batch 40 loss: 1.6521739959716797, 3.3560569286346436, 5.008231163024902
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602762937545776 3.257885456085205 4.918161869049072
Loss :  1.648525595664978 2.763990879058838 4.4125165939331055
Loss :  1.6654636859893799 2.9473307132720947 4.612794399261475
Loss :  1.6637682914733887 2.802414655685425 4.466182708740234
Loss :  1.6666901111602783 2.5289931297302246 4.195683479309082
Loss :  1.6581079959869385 3.048774003982544 4.706881999969482
Loss :  1.6451449394226074 3.204719066619873 4.8498640060424805
Loss :  1.6573092937469482 2.812706470489502 4.470015525817871
Loss :  1.6301392316818237 3.263680934906006 4.893820285797119
Loss :  1.6812666654586792 3.268756866455078 4.950023651123047
Loss :  1.6458377838134766 2.7422425746917725 4.388080596923828
Loss :  1.6641310453414917 2.7402000427246094 4.404331207275391
Loss :  1.6826646327972412 3.4660260677337646 5.148690700531006
Loss :  1.6674078702926636 2.9691414833068848 4.636549472808838
Loss :  1.670671820640564 3.400811195373535 5.071483135223389
Loss :  1.634939193725586 2.770660161972046 4.405599594116211
Loss :  1.6846301555633545 3.1343443393707275 4.818974494934082
Loss :  1.6822115182876587 3.113194465637207 4.795405864715576
Loss :  1.6956177949905396 3.431345224380493 5.126963138580322
Loss :  1.670904517173767 2.764251708984375 4.435156345367432
  batch 60 loss: 1.670904517173767, 2.764251708984375, 4.435156345367432
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726100444793701 3.4551665782928467 5.127776622772217
Loss :  1.6693756580352783 2.5991146564483643 4.268490314483643
Loss :  1.6774221658706665 3.144432306289673 4.821854591369629
Loss :  1.6580159664154053 3.1531333923339844 4.811149597167969
Loss :  1.655048131942749 2.9758918285369873 4.630939960479736
Loss :  5.4829607009887695 4.401259899139404 9.884220123291016
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.373303413391113 4.424964427947998 9.798267364501953
Loss :  5.481847763061523 4.257184982299805 9.739032745361328
Loss :  4.5612945556640625 4.289392471313477 8.850687026977539
Total LOSS train 4.678143659004799 valid 9.568051815032959
CE LOSS train 1.6640667016689594 valid 1.1403236389160156
Contrastive LOSS train 3.014076933493981 valid 1.0723481178283691
EPOCH 287:
Loss :  1.6504192352294922 2.64957594871521 4.299995422363281
Loss :  1.6662579774856567 3.368652105331421 5.034910202026367
Loss :  1.652098298072815 3.63932204246521 5.2914204597473145
Loss :  1.655251145362854 3.0410096645355225 4.696260929107666
Loss :  1.6799125671386719 3.2134954929351807 4.893407821655273
Loss :  1.6628694534301758 2.874267816543579 4.537137031555176
Loss :  1.6610583066940308 3.6686861515045166 5.329744338989258
Loss :  1.649922490119934 2.6480095386505127 4.297932147979736
Loss :  1.6545768976211548 2.554197072982788 4.208774089813232
Loss :  1.6090420484542847 2.815169334411621 4.424211502075195
Loss :  1.6686620712280273 3.458071231842041 5.126733303070068
Loss :  1.729206919670105 2.9288928508758545 4.65809965133667
Loss :  1.676216959953308 3.1429381370544434 4.819155216217041
Loss :  1.6644119024276733 3.3014440536499023 4.965856075286865
Loss :  1.6425834894180298 3.1327219009399414 4.775305271148682
Loss :  1.6521602869033813 3.13038969039917 4.782549858093262
Loss :  1.6611462831497192 2.7873497009277344 4.448495864868164
Loss :  1.6595268249511719 2.906320810317993 4.565847396850586
Loss :  1.6675562858581543 3.080332040786743 4.747888565063477
Loss :  1.624001145362854 2.8611085414886475 4.485109806060791
  batch 20 loss: 1.624001145362854, 2.8611085414886475, 4.485109806060791
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.659275770187378 3.012397289276123 4.671672821044922
Loss :  1.6750022172927856 2.604160785675049 4.279162883758545
Loss :  1.6463782787322998 3.1936583518981934 4.840036392211914
Loss :  1.6873843669891357 3.0590593814849854 4.746443748474121
Loss :  1.684840440750122 3.661454200744629 5.346294403076172
Loss :  1.648423671722412 3.082859516143799 4.731283187866211
Loss :  1.6973568201065063 3.0261566638946533 4.723513603210449
Loss :  1.639885425567627 2.922236680984497 4.562122344970703
Loss :  1.6880667209625244 2.809161901473999 4.497228622436523
Loss :  1.6413942575454712 2.8152432441711426 4.456637382507324
Loss :  1.722659707069397 3.179698944091797 4.902358531951904
Loss :  1.669063687324524 3.493422031402588 5.162485599517822
Loss :  1.6544122695922852 3.064580202102661 4.718992233276367
Loss :  1.656834363937378 2.9323134422302246 4.589147567749023
Loss :  1.6967473030090332 3.189990282058716 4.886737823486328
Loss :  1.688862681388855 2.7018749713897705 4.390737533569336
Loss :  1.666496753692627 3.0441792011260986 4.710676193237305
Loss :  1.6343307495117188 2.6863722801208496 4.320703029632568
Loss :  1.6596753597259521 3.1236274242401123 4.7833027839660645
Loss :  1.6521743535995483 3.2073681354522705 4.859542369842529
  batch 40 loss: 1.6521743535995483, 3.2073681354522705, 4.859542369842529
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602765321731567 3.231539487838745 4.891816139221191
Loss :  1.648525357246399 2.4518518447875977 4.100377082824707
Loss :  1.6654636859893799 2.9479286670684814 4.613392353057861
Loss :  1.663766860961914 2.9872124195098877 4.650979042053223
Loss :  1.6666898727416992 2.4916367530822754 4.158326625823975
Loss :  1.658108115196228 3.088393211364746 4.746501445770264
Loss :  1.6451456546783447 3.2019782066345215 4.847124099731445
Loss :  1.6573097705841064 3.0087342262268066 4.666044235229492
Loss :  1.6301392316818237 3.4297947883605957 5.059934139251709
Loss :  1.6812679767608643 3.163978338241577 4.845246315002441
Loss :  1.645838737487793 2.7959470748901367 4.44178581237793
Loss :  1.664131999015808 3.1557846069335938 4.819916725158691
Loss :  1.6826649904251099 3.348945379257202 5.031610488891602
Loss :  1.667407751083374 2.9599192142486572 4.627326965332031
Loss :  1.6706726551055908 3.4826505184173584 5.153323173522949
Loss :  1.6349393129348755 2.631096601486206 4.266036033630371
Loss :  1.6846301555633545 3.42248797416687 5.107118129730225
Loss :  1.682212471961975 3.036668062210083 4.718880653381348
Loss :  1.6956181526184082 3.1650280952453613 4.8606462478637695
Loss :  1.6709040403366089 2.710057020187378 4.380960941314697
  batch 60 loss: 1.6709040403366089, 2.710057020187378, 4.380960941314697
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726102828979492 3.315682888031006 4.988293170928955
Loss :  1.669376254081726 2.831993818283081 4.501369953155518
Loss :  1.6774227619171143 3.2312002182006836 4.908622741699219
Loss :  1.6580156087875366 2.9801509380340576 4.638166427612305
Loss :  1.6550474166870117 2.8279640674591064 4.483011245727539
Loss :  5.483002662658691 4.4442138671875 9.927216529846191
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.373336315155029 4.361058712005615 9.734395027160645
Loss :  5.481873035430908 4.232079982757568 9.713953018188477
Loss :  4.561372756958008 4.242636680603027 8.804009437561035
Total LOSS train 4.708841910729041 valid 9.544893503189087
CE LOSS train 1.664066637479342 valid 1.140343189239502
Contrastive LOSS train 3.044775284253634 valid 1.0606591701507568
EPOCH 288:
Loss :  1.650418996810913 2.7841098308563232 4.434528827667236
Loss :  1.6662575006484985 3.384634494781494 5.050891876220703
Loss :  1.6520991325378418 3.0850069522857666 4.7371063232421875
Loss :  1.6552528142929077 2.9526407718658447 4.607893466949463
Loss :  1.679912805557251 3.0962140560150146 4.776126861572266
Loss :  1.6628690958023071 2.8049733638763428 4.4678425788879395
Loss :  1.661058783531189 3.643850564956665 5.3049092292785645
Loss :  1.6499217748641968 2.6691205501556396 4.319042205810547
Loss :  1.6545767784118652 2.3796072006225586 4.034183979034424
Loss :  1.6090419292449951 2.5495622158050537 4.158604145050049
Loss :  1.6686620712280273 3.5717060565948486 5.240367889404297
Loss :  1.7292077541351318 2.8721771240234375 4.601385116577148
Loss :  1.6762170791625977 3.265108585357666 4.941325664520264
Loss :  1.6644110679626465 3.04717755317688 4.7115888595581055
Loss :  1.6425834894180298 3.0367467403411865 4.679330348968506
Loss :  1.6521600484848022 3.20184326171875 4.854003429412842
Loss :  1.6611465215682983 2.900249719619751 4.56139612197876
Loss :  1.6595277786254883 2.787433385848999 4.446961402893066
Loss :  1.6675570011138916 2.9601950645446777 4.627752304077148
Loss :  1.6240012645721436 2.921729564666748 4.5457305908203125
  batch 20 loss: 1.6240012645721436, 2.921729564666748, 4.5457305908203125
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592751741409302 3.0494425296783447 4.7087178230285645
Loss :  1.6750022172927856 2.5687506198883057 4.243752956390381
Loss :  1.6463767290115356 3.0806262493133545 4.72700309753418
Loss :  1.6873838901519775 2.904961347579956 4.592345237731934
Loss :  1.6848398447036743 3.376187324523926 5.0610270500183105
Loss :  1.6484228372573853 3.066286563873291 4.714709281921387
Loss :  1.6973570585250854 3.1999294757843018 4.897286415100098
Loss :  1.6398855447769165 2.8397560119628906 4.479641437530518
Loss :  1.6880671977996826 2.701392650604248 4.389459609985352
Loss :  1.6413931846618652 3.3893074989318848 5.03070068359375
Loss :  1.722659707069397 3.2963976860046387 5.019057273864746
Loss :  1.6690646409988403 3.344517707824707 5.013582229614258
Loss :  1.6544123888015747 3.2211241722106934 4.8755364418029785
Loss :  1.6568342447280884 2.8212313652038574 4.478065490722656
Loss :  1.6967474222183228 3.4652297496795654 5.161977291107178
Loss :  1.6888624429702759 2.926581382751465 4.615443706512451
Loss :  1.6664966344833374 2.9426496028900146 4.6091461181640625
Loss :  1.6343318223953247 3.00256085395813 4.636892795562744
Loss :  1.6596755981445312 2.9830033779144287 4.642679214477539
Loss :  1.6521742343902588 2.9994165897369385 4.651590824127197
  batch 40 loss: 1.6521742343902588, 2.9994165897369385, 4.651590824127197
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602762937545776 3.137834310531616 4.798110485076904
Loss :  1.6485249996185303 2.805691719055176 4.454216957092285
Loss :  1.6654638051986694 3.030231237411499 4.695694923400879
Loss :  1.6637675762176514 2.811626672744751 4.475394248962402
Loss :  1.666689395904541 2.927706718444824 4.594396114349365
Loss :  1.6581075191497803 3.243561267852783 4.901668548583984
Loss :  1.645146369934082 2.977473020553589 4.62261962890625
Loss :  1.657309889793396 2.6256821155548096 4.282991886138916
Loss :  1.6301393508911133 3.2407748699188232 4.870914459228516
Loss :  1.6812671422958374 3.452631711959839 5.133898735046387
Loss :  1.645839810371399 2.652118444442749 4.2979583740234375
Loss :  1.6641323566436768 2.7830419540405273 4.447174072265625
Loss :  1.682665228843689 3.133863925933838 4.816529273986816
Loss :  1.6674089431762695 3.188000202178955 4.855409145355225
Loss :  1.67067289352417 3.6318747997283936 5.302547454833984
Loss :  1.6349395513534546 2.6837430000305176 4.318682670593262
Loss :  1.6846293210983276 3.659066915512085 5.343696117401123
Loss :  1.6822131872177124 3.0894837379455566 4.771697044372559
Loss :  1.69561767578125 3.4750823974609375 5.1707000732421875
Loss :  1.6709036827087402 2.9566147327423096 4.627518653869629
  batch 60 loss: 1.6709036827087402, 2.9566147327423096, 4.627518653869629
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.672609806060791 3.237774610519409 4.910384178161621
Loss :  1.669376015663147 2.617767095565796 4.287143230438232
Loss :  1.6774227619171143 3.0875346660614014 4.764957427978516
Loss :  1.6580162048339844 2.7507638931274414 4.408780097961426
Loss :  1.6550475358963013 3.0744752883911133 4.729522705078125
Loss :  5.483112812042236 4.387805461883545 9.870918273925781
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.37350606918335 4.366254806518555 9.739761352539062
Loss :  5.481952667236328 4.303491592407227 9.785444259643555
Loss :  4.561829090118408 4.0759782791137695 8.637807846069336
Total LOSS train 4.700495272416335 valid 9.508482933044434
CE LOSS train 1.6640666741591232 valid 1.140457272529602
Contrastive LOSS train 3.0364286019251896 valid 1.0189945697784424
EPOCH 289:
Loss :  1.6504194736480713 2.535639524459839 4.18605899810791
Loss :  1.6662577390670776 3.4704203605651855 5.136678218841553
Loss :  1.6521000862121582 3.578272819519043 5.230372905731201
Loss :  1.6552536487579346 2.9041695594787598 4.559423446655273
Loss :  1.6799122095108032 3.482320547103882 5.162232875823975
Loss :  1.6628694534301758 3.100268840789795 4.763138294219971
Loss :  1.661057949066162 3.3346049785614014 4.995662689208984
Loss :  1.6499216556549072 2.7945988178253174 4.444520473480225
Loss :  1.6545777320861816 2.2615792751312256 3.9161570072174072
Loss :  1.6090412139892578 2.8689699172973633 4.478011131286621
Loss :  1.668662667274475 3.4073357582092285 5.075998306274414
Loss :  1.72920823097229 3.001983642578125 4.731191635131836
Loss :  1.6762192249298096 3.249871015548706 4.926090240478516
Loss :  1.6644097566604614 3.246427059173584 4.910836696624756
Loss :  1.6425837278366089 2.9300646781921387 4.572648525238037
Loss :  1.6521602869033813 3.0746726989746094 4.726832866668701
Loss :  1.6611472368240356 2.980559825897217 4.641706943511963
Loss :  1.659529447555542 2.6726531982421875 4.332182884216309
Loss :  1.6675583124160767 2.8776767253875732 4.5452351570129395
Loss :  1.6240017414093018 3.0296506881713867 4.653652191162109
  batch 20 loss: 1.6240017414093018, 3.0296506881713867, 4.653652191162109
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592739820480347 3.11362624168396 4.772900104522705
Loss :  1.675001859664917 2.8266515731811523 4.501653671264648
Loss :  1.6463764905929565 3.1122400760650635 4.7586164474487305
Loss :  1.687383770942688 2.7784645557403564 4.465848445892334
Loss :  1.6848394870758057 3.252861738204956 4.937701225280762
Loss :  1.648422360420227 3.051445484161377 4.6998677253723145
Loss :  1.6973568201065063 3.059783458709717 4.757140159606934
Loss :  1.6398855447769165 3.207284450531006 4.847169876098633
Loss :  1.6880675554275513 2.5180695056915283 4.206137180328369
Loss :  1.641392469406128 2.6951260566711426 4.336518287658691
Loss :  1.7226600646972656 3.2919414043426514 5.014601707458496
Loss :  1.6690644025802612 3.188150644302368 4.85721492767334
Loss :  1.654411792755127 2.9054551124572754 4.559866905212402
Loss :  1.656833529472351 2.6200790405273438 4.276912689208984
Loss :  1.6967461109161377 3.495466470718384 5.1922125816345215
Loss :  1.6888631582260132 2.828803300857544 4.517666339874268
Loss :  1.666497826576233 2.9884302616119385 4.654928207397461
Loss :  1.634332299232483 3.1715967655181885 4.805929183959961
Loss :  1.6596754789352417 3.156560182571411 4.816235542297363
Loss :  1.6521741151809692 3.4951741695404053 5.147348403930664
  batch 40 loss: 1.6521741151809692, 3.4951741695404053, 5.147348403930664
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.660277009010315 3.202406406402588 4.862683296203613
Loss :  1.6485252380371094 2.7734954357147217 4.42202091217041
Loss :  1.6654640436172485 3.098344326019287 4.763808250427246
Loss :  1.6637665033340454 2.8717143535614014 4.535480976104736
Loss :  1.6666897535324097 2.5563013553619385 4.222990989685059
Loss :  1.658107876777649 3.078047752380371 4.7361555099487305
Loss :  1.645146131515503 3.299332618713379 4.944478988647461
Loss :  1.6573092937469482 3.152805805206299 4.810114860534668
Loss :  1.630139946937561 3.606612205505371 5.236752033233643
Loss :  1.6812691688537598 3.006263017654419 4.687532424926758
Loss :  1.6458384990692139 2.752793073654175 4.398631572723389
Loss :  1.664131760597229 3.1314852237701416 4.79561710357666
Loss :  1.6826647520065308 3.2199349403381348 4.902599811553955
Loss :  1.6674079895019531 3.0576109886169434 4.7250189781188965
Loss :  1.6706726551055908 3.349802255630493 5.020474910736084
Loss :  1.6349375247955322 2.7587409019470215 4.393678665161133
Loss :  1.6846288442611694 3.3779449462890625 5.0625739097595215
Loss :  1.6822136640548706 3.145904541015625 4.828118324279785
Loss :  1.695617437362671 3.1855201721191406 4.881137847900391
Loss :  1.670903205871582 2.8549318313598633 4.525835037231445
  batch 60 loss: 1.670903205871582, 2.8549318313598633, 4.525835037231445
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726099252700806 3.011237382888794 4.683847427368164
Loss :  1.6693753004074097 2.6482901573181152 4.3176655769348145
Loss :  1.6774227619171143 3.0400471687316895 4.717470169067383
Loss :  1.6580153703689575 3.019062042236328 4.677077293395996
Loss :  1.655052900314331 2.9705810546875 4.62563419342041
Loss :  5.482590675354004 4.420693397521973 9.903284072875977
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.372926235198975 4.296398639678955 9.66932487487793
Loss :  5.481424331665039 4.27746057510376 9.75888442993164
Loss :  4.561379909515381 4.474077224731445 9.035457611083984
Total LOSS train 4.706038464032686 valid 9.591737747192383
CE LOSS train 1.6640667456846971 valid 1.1403449773788452
Contrastive LOSS train 3.04197169817411 valid 1.1185193061828613
EPOCH 290:
Loss :  1.6504179239273071 2.633117198944092 4.283535003662109
Loss :  1.6662578582763672 3.552523374557495 5.218781471252441
Loss :  1.6520988941192627 3.1415011882781982 4.793600082397461
Loss :  1.6552534103393555 2.823697805404663 4.478951454162598
Loss :  1.6799135208129883 3.299982786178589 4.979896545410156
Loss :  1.6628693342208862 2.881826162338257 4.5446953773498535
Loss :  1.6610585451126099 3.1886963844299316 4.849754810333252
Loss :  1.6499214172363281 2.3821563720703125 4.032077789306641
Loss :  1.6545768976211548 2.305720090866089 3.960297107696533
Loss :  1.6090409755706787 2.7987427711486816 4.407783508300781
Loss :  1.6686618328094482 3.4461569786071777 5.114818572998047
Loss :  1.7292085886001587 2.7248072624206543 4.454015731811523
Loss :  1.6762176752090454 3.242825508117676 4.919043064117432
Loss :  1.664410948753357 3.196932792663574 4.861343860626221
Loss :  1.6425837278366089 3.137709140777588 4.780292987823486
Loss :  1.6521602869033813 2.8740391731262207 4.5261993408203125
Loss :  1.661147117614746 2.8934295177459717 4.554576873779297
Loss :  1.65952730178833 2.7912821769714355 4.450809478759766
Loss :  1.667556643486023 2.966817617416382 4.634374141693115
Loss :  1.6240018606185913 2.7348501682281494 4.358851909637451
  batch 20 loss: 1.6240018606185913, 2.7348501682281494, 4.358851909637451
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.659274935722351 3.097896099090576 4.757171154022217
Loss :  1.675001621246338 2.51818585395813 4.193187713623047
Loss :  1.6463768482208252 3.453639030456543 5.100015640258789
Loss :  1.6873835325241089 3.0943589210510254 4.781742572784424
Loss :  1.6848396062850952 3.438377618789673 5.1232171058654785
Loss :  1.6484224796295166 3.2747838497161865 4.923206329345703
Loss :  1.697357416152954 2.9404683113098145 4.637825965881348
Loss :  1.6398847103118896 3.000532388687134 4.640417098999023
Loss :  1.6880671977996826 2.7194581031799316 4.407525062561035
Loss :  1.6413925886154175 3.124126434326172 4.765519142150879
Loss :  1.7226601839065552 3.4635746479034424 5.186234951019287
Loss :  1.6690646409988403 2.9844024181365967 4.653467178344727
Loss :  1.6544114351272583 2.8114137649536133 4.465825080871582
Loss :  1.656833529472351 2.8186428546905518 4.475476264953613
Loss :  1.6967480182647705 3.6645596027374268 5.361307621002197
Loss :  1.6888630390167236 2.8288776874542236 4.517740726470947
Loss :  1.666497826576233 3.0788447856903076 4.74534273147583
Loss :  1.6343311071395874 3.0073373317718506 4.641668319702148
Loss :  1.659675121307373 2.794603109359741 4.454277992248535
Loss :  1.6521737575531006 3.2797341346740723 4.931907653808594
  batch 40 loss: 1.6521737575531006, 3.2797341346740723, 4.931907653808594
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602764129638672 3.606616735458374 5.26689338684082
Loss :  1.6485257148742676 2.632999897003174 4.281525611877441
Loss :  1.6654642820358276 2.8964805603027344 4.561944961547852
Loss :  1.663765788078308 2.7903337478637695 4.454099655151367
Loss :  1.6666896343231201 2.637387275695801 4.3040771484375
Loss :  1.6581085920333862 2.988222360610962 4.646330833435059
Loss :  1.6451451778411865 3.2583000659942627 4.903445243835449
Loss :  1.6573097705841064 2.963717460632324 4.621026992797852
Loss :  1.630140781402588 3.395846128463745 5.025986671447754
Loss :  1.6812684535980225 3.396387815475464 5.077656269073486
Loss :  1.6458380222320557 2.5004868507385254 4.14632511138916
Loss :  1.6641310453414917 2.77105450630188 4.435185432434082
Loss :  1.6826653480529785 3.269928216934204 4.952593803405762
Loss :  1.667406439781189 2.76452898979187 4.4319353103637695
Loss :  1.670671820640564 3.4457848072052 5.116456508636475
Loss :  1.6349371671676636 2.883667230606079 4.518604278564453
Loss :  1.6846293210983276 3.245300531387329 4.929929733276367
Loss :  1.6822131872177124 3.2350404262542725 4.917253494262695
Loss :  1.6956181526184082 3.502316474914551 5.197934627532959
Loss :  1.670902967453003 2.6186156272888184 4.289518356323242
  batch 60 loss: 1.670902967453003, 2.6186156272888184, 4.289518356323242
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726105213165283 3.2195611000061035 4.892171859741211
Loss :  1.6693748235702515 2.6581077575683594 4.3274827003479
Loss :  1.6774227619171143 3.1483166217803955 4.82573938369751
Loss :  1.658015489578247 2.7133495807647705 4.371365070343018
Loss :  1.6550514698028564 3.131586790084839 4.786638259887695
Loss :  5.482547283172607 4.3896331787109375 9.872180938720703
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.372896671295166 4.358585834503174 9.73148250579834
Loss :  5.481409072875977 4.239899158477783 9.721307754516602
Loss :  4.561227321624756 4.261346340179443 8.8225736618042
Total LOSS train 4.680752248030442 valid 9.536886215209961
CE LOSS train 1.6640665769577025 valid 1.140306830406189
Contrastive LOSS train 3.016685676574707 valid 1.0653365850448608
EPOCH 291:
Loss :  1.6504167318344116 2.6084437370300293 4.2588605880737305
Loss :  1.666257619857788 3.3121416568756104 4.978399276733398
Loss :  1.6520991325378418 3.5991017818450928 5.2512006759643555
Loss :  1.6552505493164062 2.8403849601745605 4.495635509490967
Loss :  1.679912805557251 3.487457275390625 5.167369842529297
Loss :  1.6628674268722534 2.8355841636657715 4.4984517097473145
Loss :  1.6610586643218994 3.1051011085510254 4.766160011291504
Loss :  1.6499210596084595 2.840574026107788 4.490495204925537
Loss :  1.654576301574707 2.4061472415924072 4.060723304748535
Loss :  1.6090428829193115 2.6122310161590576 4.221273899078369
Loss :  1.668662428855896 3.198357582092285 4.867020130157471
Loss :  1.7292054891586304 2.9957892894744873 4.724994659423828
Loss :  1.6762219667434692 3.2831835746765137 4.959405422210693
Loss :  1.6644114255905151 3.2779433727264404 4.942354679107666
Loss :  1.6425821781158447 3.0184967517852783 4.661078929901123
Loss :  1.6521596908569336 3.1135902404785156 4.765749931335449
Loss :  1.6611462831497192 2.936678886413574 4.597825050354004
Loss :  1.6595261096954346 3.2055065631866455 4.86503267288208
Loss :  1.6675565242767334 2.8601582050323486 4.527714729309082
Loss :  1.6240001916885376 2.935783624649048 4.559783935546875
  batch 20 loss: 1.6240001916885376, 2.935783624649048, 4.559783935546875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592755317687988 3.061160087585449 4.720435619354248
Loss :  1.6750009059906006 2.7677175998687744 4.442718505859375
Loss :  1.646378755569458 3.0493295192718506 4.695708274841309
Loss :  1.6873817443847656 2.751507043838501 4.4388885498046875
Loss :  1.6848394870758057 3.257631778717041 4.942471504211426
Loss :  1.648422360420227 3.207320213317871 4.855742454528809
Loss :  1.697357177734375 3.0889480113983154 4.7863054275512695
Loss :  1.6398848295211792 2.8798775672912598 4.5197625160217285
Loss :  1.6880675554275513 2.933626890182495 4.621694564819336
Loss :  1.6413928270339966 2.8909378051757812 4.532330513000488
Loss :  1.722659707069397 3.2804598808288574 5.003119468688965
Loss :  1.6690633296966553 3.23502516746521 4.904088497161865
Loss :  1.6544111967086792 2.7440872192382812 4.39849853515625
Loss :  1.6568337678909302 3.067810297012329 4.724644184112549
Loss :  1.696746826171875 3.6584129333496094 5.355159759521484
Loss :  1.6888628005981445 2.7722666263580322 4.461129188537598
Loss :  1.6664981842041016 2.9101943969726562 4.576692581176758
Loss :  1.6343294382095337 3.154674530029297 4.789003849029541
Loss :  1.6596746444702148 2.8675546646118164 4.527229309082031
Loss :  1.6521741151809692 3.5280110836029053 5.180185317993164
  batch 40 loss: 1.6521741151809692, 3.5280110836029053, 5.180185317993164
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602754592895508 3.4620630741119385 5.12233829498291
Loss :  1.648524522781372 2.5548923015594482 4.20341682434082
Loss :  1.6654635667800903 3.022477626800537 4.687941074371338
Loss :  1.66376531124115 3.0118906497955322 4.675655841827393
Loss :  1.6666882038116455 2.6074283123016357 4.274116516113281
Loss :  1.6581082344055176 2.780827045440674 4.438935279846191
Loss :  1.6451447010040283 3.552692174911499 5.197836875915527
Loss :  1.6573090553283691 2.7689998149871826 4.426308631896973
Loss :  1.6301401853561401 3.6733334064483643 5.303473472595215
Loss :  1.6812673807144165 3.3031859397888184 4.984453201293945
Loss :  1.6458382606506348 2.598674774169922 4.244513034820557
Loss :  1.6641305685043335 3.1191916465759277 4.783322334289551
Loss :  1.682665228843689 3.674808979034424 5.357474327087402
Loss :  1.6674059629440308 2.9184906482696533 4.5858964920043945
Loss :  1.6706717014312744 3.5288479328155518 5.199519634246826
Loss :  1.63493812084198 2.522775173187256 4.157713413238525
Loss :  1.6846295595169067 3.5518646240234375 5.236494064331055
Loss :  1.6822128295898438 3.0620689392089844 4.744281768798828
Loss :  1.695617914199829 3.2924160957336426 4.988034248352051
Loss :  1.6709024906158447 2.6346542835235596 4.305556774139404
  batch 60 loss: 1.6709024906158447, 2.6346542835235596, 4.305556774139404
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726104021072388 3.0535457134246826 4.726156234741211
Loss :  1.669373869895935 2.6949028968811035 4.364276885986328
Loss :  1.6774218082427979 3.2397053241729736 4.9171271324157715
Loss :  1.6580153703689575 2.983922243118286 4.641937732696533
Loss :  1.6550527811050415 2.907270669937134 4.562323570251465
Loss :  5.482697486877441 4.430877208709717 9.91357421875
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.37302303314209 4.398791313171387 9.771814346313477
Loss :  5.481569766998291 4.309006214141846 9.790575981140137
Loss :  4.561341762542725 4.073806285858154 8.635148048400879
Total LOSS train 4.711760652982272 valid 9.527778148651123
CE LOSS train 1.664066217495845 valid 1.1403354406356812
Contrastive LOSS train 3.047694440988394 valid 1.0184515714645386
EPOCH 292:
Loss :  1.6504164934158325 2.6952667236328125 4.3456830978393555
Loss :  1.666258692741394 3.385507345199585 5.0517659187316895
Loss :  1.6520991325378418 3.1874380111694336 4.839537143707275
Loss :  1.6552518606185913 2.8515446186065674 4.506796360015869
Loss :  1.6799129247665405 3.6288022994995117 5.308715343475342
Loss :  1.6628680229187012 2.907653331756592 4.570521354675293
Loss :  1.6610594987869263 2.9651944637298584 4.626254081726074
Loss :  1.649921178817749 2.4901442527770996 4.1400651931762695
Loss :  1.6545767784118652 2.501335620880127 4.155912399291992
Loss :  1.6090426445007324 2.8278863430023193 4.436928749084473
Loss :  1.6686625480651855 3.431082010269165 5.09974479675293
Loss :  1.7292073965072632 2.758685827255249 4.487893104553223
Loss :  1.6762192249298096 3.2164864540100098 4.892705917358398
Loss :  1.664411187171936 3.540160655975342 5.204571723937988
Loss :  1.642583966255188 3.01816463470459 4.660748481750488
Loss :  1.6521601676940918 3.215712308883667 4.86787223815918
Loss :  1.661147117614746 2.692950963973999 4.354098320007324
Loss :  1.659525990486145 3.090331792831421 4.7498579025268555
Loss :  1.6675562858581543 2.905576467514038 4.573132514953613
Loss :  1.6240006685256958 3.1805222034454346 4.80452299118042
  batch 20 loss: 1.6240006685256958, 3.1805222034454346, 4.80452299118042
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592758893966675 3.2744665145874023 4.933742523193359
Loss :  1.6750012636184692 2.5625343322753906 4.23753547668457
Loss :  1.6463772058486938 2.991220712661743 4.637598037719727
Loss :  1.6873838901519775 2.732797861099243 4.420181751251221
Loss :  1.6848403215408325 3.327645778656006 5.012485980987549
Loss :  1.6484229564666748 2.953472852706909 4.601895809173584
Loss :  1.697357177734375 3.0966758728027344 4.794033050537109
Loss :  1.639885425567627 3.1863715648651123 4.82625675201416
Loss :  1.688066840171814 2.742046356201172 4.430113315582275
Loss :  1.6413933038711548 2.7672460079193115 4.408639430999756
Loss :  1.7226606607437134 3.307705879211426 5.03036642074585
Loss :  1.6690646409988403 3.276106357574463 4.945170879364014
Loss :  1.6544123888015747 3.0146725177764893 4.6690850257873535
Loss :  1.6568347215652466 2.869126319885254 4.525960922241211
Loss :  1.696749210357666 3.3496899604797363 5.046439170837402
Loss :  1.688863754272461 2.8850293159484863 4.573893070220947
Loss :  1.6664974689483643 3.183436870574951 4.8499345779418945
Loss :  1.6343317031860352 2.8429107666015625 4.477242469787598
Loss :  1.6596744060516357 3.154252052307129 4.813926696777344
Loss :  1.6521737575531006 3.2448394298553467 4.897013187408447
  batch 40 loss: 1.6521737575531006, 3.2448394298553467, 4.897013187408447
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602755784988403 3.475651979446411 5.135927677154541
Loss :  1.6485251188278198 2.61348032951355 4.26200532913208
Loss :  1.6654634475708008 3.1326818466186523 4.798145294189453
Loss :  1.66376531124115 2.952507734298706 4.616272926330566
Loss :  1.6666879653930664 2.535499095916748 4.2021870613098145
Loss :  1.6581079959869385 2.8937718868255615 4.5518798828125
Loss :  1.6451444625854492 2.998173952102661 4.643318176269531
Loss :  1.6573100090026855 2.619054079055786 4.276364326477051
Loss :  1.6301403045654297 3.624053478240967 5.2541937828063965
Loss :  1.681267261505127 3.3925602436065674 5.073827743530273
Loss :  1.6458377838134766 2.724879264831543 4.3707170486450195
Loss :  1.664130687713623 2.9956541061401367 4.65978479385376
Loss :  1.682665228843689 3.3624565601348877 5.045121669769287
Loss :  1.667406678199768 3.0754897594451904 4.742896556854248
Loss :  1.670672059059143 3.3443994522094727 5.015071392059326
Loss :  1.6349384784698486 2.800837993621826 4.435776710510254
Loss :  1.6846293210983276 3.1582300662994385 4.842859268188477
Loss :  1.6822131872177124 3.266899347305298 4.949112415313721
Loss :  1.695617914199829 3.281005620956421 4.97662353515625
Loss :  1.6709030866622925 2.8785812854766846 4.5494842529296875
  batch 60 loss: 1.6709030866622925, 2.8785812854766846, 4.5494842529296875
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726106405258179 3.222815990447998 4.8954267501831055
Loss :  1.6693732738494873 2.8365836143493652 4.505956649780273
Loss :  1.6774208545684814 3.2090327739715576 4.886453628540039
Loss :  1.6580156087875366 3.1446821689605713 4.802697658538818
Loss :  1.6550527811050415 3.146662712097168 4.80171537399292
Loss :  5.482630729675293 4.351845741271973 9.834476470947266
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.372962951660156 4.300029277801514 9.672992706298828
Loss :  5.481563091278076 4.2670416831970215 9.748604774475098
Loss :  4.561375617980957 4.225879192352295 8.787254333496094
Total LOSS train 4.70927172440749 valid 9.510832071304321
CE LOSS train 1.6640665201040414 valid 1.1403439044952393
Contrastive LOSS train 3.0452052153073823 valid 1.0564697980880737
EPOCH 293:
Loss :  1.6504160165786743 2.462430715560913 4.112846851348877
Loss :  1.6662577390670776 3.3957018852233887 5.061959743499756
Loss :  1.6520988941192627 2.945817470550537 4.597916603088379
Loss :  1.6552515029907227 3.084789991378784 4.740041732788086
Loss :  1.6799134016036987 3.0766613483428955 4.756574630737305
Loss :  1.6628679037094116 2.791813850402832 4.454681873321533
Loss :  1.6610597372055054 3.364752769470215 5.02581262588501
Loss :  1.6499208211898804 2.3751444816589355 4.0250654220581055
Loss :  1.654576301574707 2.4217469692230225 4.076323509216309
Loss :  1.609042763710022 2.8144402503967285 4.423482894897461
Loss :  1.6686615943908691 3.356241464614868 5.024903297424316
Loss :  1.7292068004608154 2.9176313877105713 4.646838188171387
Loss :  1.676218032836914 3.214508533477783 4.890726566314697
Loss :  1.664412021636963 3.230860471725464 4.895272254943848
Loss :  1.6425837278366089 3.0749635696411133 4.717547416687012
Loss :  1.6521602869033813 3.3191823959350586 4.97134256362915
Loss :  1.6611467599868774 2.622614860534668 4.283761501312256
Loss :  1.65952467918396 2.8705527782440186 4.5300774574279785
Loss :  1.6675556898117065 2.8480846881866455 4.5156402587890625
Loss :  1.6240006685256958 2.9512147903442383 4.5752153396606445
  batch 20 loss: 1.6240006685256958, 2.9512147903442383, 4.5752153396606445
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592756509780884 3.019526958465576 4.678802490234375
Loss :  1.6750015020370483 2.9110348224639893 4.586036205291748
Loss :  1.6463778018951416 3.242502450942993 4.888880252838135
Loss :  1.6873829364776611 2.8584816455841064 4.545864582061768
Loss :  1.6848393678665161 3.3494181632995605 5.034257411956787
Loss :  1.6484217643737793 3.073570728302002 4.721992492675781
Loss :  1.6973567008972168 3.052417516708374 4.749773979187012
Loss :  1.6398848295211792 2.690915822982788 4.330800533294678
Loss :  1.6880674362182617 2.6402854919433594 4.328352928161621
Loss :  1.6413925886154175 3.0019350051879883 4.643327713012695
Loss :  1.722659707069397 3.3703713417053223 5.09303092956543
Loss :  1.6690638065338135 3.362534761428833 5.0315985679626465
Loss :  1.6544116735458374 2.963383674621582 4.617795467376709
Loss :  1.6568344831466675 2.879866123199463 4.53670072555542
Loss :  1.69674813747406 3.3005082607269287 4.997256278991699
Loss :  1.6888631582260132 2.8429582118988037 4.531821250915527
Loss :  1.6664977073669434 3.0365021228790283 4.703000068664551
Loss :  1.6343317031860352 3.0800082683563232 4.7143402099609375
Loss :  1.6596741676330566 3.062330484390259 4.7220048904418945
Loss :  1.6521738767623901 3.2827203273773193 4.93489408493042
  batch 40 loss: 1.6521738767623901, 3.2827203273773193, 4.93489408493042
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602756977081299 3.611403465270996 5.271678924560547
Loss :  1.6485247611999512 3.0430986881256104 4.691623687744141
Loss :  1.6654632091522217 3.148895502090454 4.814358711242676
Loss :  1.6637647151947021 2.9157755374908447 4.579540252685547
Loss :  1.666688084602356 2.7000954151153564 4.366783618927002
Loss :  1.6581077575683594 2.8720240592956543 4.530131816864014
Loss :  1.6451447010040283 3.0532050132751465 4.698349952697754
Loss :  1.6573092937469482 2.5471081733703613 4.2044172286987305
Loss :  1.6301401853561401 3.315615653991699 4.945755958557129
Loss :  1.6812677383422852 2.928288698196411 4.609556198120117
Loss :  1.6458383798599243 2.5865278244018555 4.23236608505249
Loss :  1.6641310453414917 3.193308115005493 4.857439041137695
Loss :  1.682665228843689 3.707390785217285 5.390056133270264
Loss :  1.6674054861068726 2.974193572998047 4.641599178314209
Loss :  1.6706727743148804 3.4130520820617676 5.0837249755859375
Loss :  1.6349382400512695 2.569462537765503 4.204401016235352
Loss :  1.6846296787261963 3.1111230850219727 4.79575252532959
Loss :  1.6822137832641602 3.280559778213501 4.962773323059082
Loss :  1.6956177949905396 3.4602813720703125 5.1558990478515625
Loss :  1.670902132987976 2.9597575664520264 4.630659580230713
  batch 60 loss: 1.670902132987976, 2.9597575664520264, 4.630659580230713
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.672610878944397 3.218651294708252 4.891262054443359
Loss :  1.669374704360962 2.6741068363189697 4.343481540679932
Loss :  1.677420973777771 3.247626304626465 4.925047397613525
Loss :  1.6580157279968262 2.93375301361084 4.591768741607666
Loss :  1.6550440788269043 2.838271141052246 4.49331521987915
Loss :  5.482588291168213 4.441179275512695 9.92376708984375
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.372917175292969 4.338847637176514 9.71176528930664
Loss :  5.48150110244751 4.168904781341553 9.650405883789062
Loss :  4.561374187469482 4.195207118988037 8.75658130645752
Total LOSS train 4.686589292379526 valid 9.510629892349243
CE LOSS train 1.6640661753140964 valid 1.1403435468673706
Contrastive LOSS train 3.0225231133974515 valid 1.0488017797470093
EPOCH 294:
Loss :  1.6504155397415161 2.9332492351531982 4.583664894104004
Loss :  1.6662575006484985 3.389073371887207 5.055330753326416
Loss :  1.6520986557006836 3.2403414249420166 4.892439842224121
Loss :  1.65525221824646 2.9011924266815186 4.5564446449279785
Loss :  1.679912805557251 3.659273386001587 5.339186191558838
Loss :  1.6628682613372803 2.7190980911254883 4.381966590881348
Loss :  1.6610597372055054 3.3243906497955322 4.985450267791748
Loss :  1.6499202251434326 2.7565767765045166 4.406497001647949
Loss :  1.6545767784118652 2.2856833934783936 3.940260171890259
Loss :  1.6090425252914429 2.5640883445739746 4.173130989074707
Loss :  1.6686608791351318 3.4629929065704346 5.131653785705566
Loss :  1.7292073965072632 2.9361181259155273 4.66532564163208
Loss :  1.6762142181396484 3.22350811958313 4.899722099304199
Loss :  1.664412021636963 3.230041265487671 4.894453048706055
Loss :  1.6425843238830566 3.1109588146209717 4.753542900085449
Loss :  1.6521600484848022 3.2872941493988037 4.939454078674316
Loss :  1.6611472368240356 2.8358161449432373 4.4969635009765625
Loss :  1.659523606300354 2.961207628250122 4.620731353759766
Loss :  1.667555332183838 2.8258705139160156 4.4934258460998535
Loss :  1.6240005493164062 2.850186824798584 4.47418737411499
  batch 20 loss: 1.6240005493164062, 2.850186824798584, 4.47418737411499
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592766046524048 2.9927866458892822 4.652063369750977
Loss :  1.6750019788742065 2.6373746395111084 4.312376499176025
Loss :  1.6463772058486938 2.8816683292388916 4.528045654296875
Loss :  1.6873831748962402 2.950995922088623 4.638379096984863
Loss :  1.6848393678665161 3.4723947048187256 5.157234191894531
Loss :  1.648421287536621 3.220585584640503 4.869007110595703
Loss :  1.6973578929901123 2.962263345718384 4.659621238708496
Loss :  1.6398851871490479 3.0998713970184326 4.7397565841674805
Loss :  1.6880669593811035 2.7982065677642822 4.486273765563965
Loss :  1.6413930654525757 2.8494343757629395 4.490827560424805
Loss :  1.7226601839065552 3.3629698753356934 5.085629940032959
Loss :  1.6690648794174194 3.2069740295410156 4.876039028167725
Loss :  1.654410481452942 3.293121576309204 4.9475321769714355
Loss :  1.6568344831466675 2.7052361965179443 4.362070560455322
Loss :  1.6967469453811646 3.614673614501953 5.311420440673828
Loss :  1.688862919807434 2.76798415184021 4.456847190856934
Loss :  1.6664975881576538 3.0979301929473877 4.764427661895752
Loss :  1.6343324184417725 2.9670298099517822 4.601362228393555
Loss :  1.6596745252609253 3.0690841674804688 4.728758811950684
Loss :  1.6521737575531006 3.088735342025757 4.740909099578857
  batch 40 loss: 1.6521737575531006, 3.088735342025757, 4.740909099578857
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602755784988403 3.296271324157715 4.956546783447266
Loss :  1.6485235691070557 2.6821115016937256 4.330635070800781
Loss :  1.6654630899429321 2.922633647918701 4.588096618652344
Loss :  1.6637639999389648 2.9501917362213135 4.613955497741699
Loss :  1.666687250137329 2.9463727474212646 4.613059997558594
Loss :  1.6581069231033325 3.0963351726531982 4.75444221496582
Loss :  1.645145297050476 3.564255475997925 5.209400653839111
Loss :  1.6573079824447632 2.854637384414673 4.5119452476501465
Loss :  1.630139946937561 3.66672945022583 5.296869277954102
Loss :  1.6812666654586792 2.9390008449554443 4.620267391204834
Loss :  1.6458380222320557 2.635781764984131 4.281620025634766
Loss :  1.6641312837600708 2.7745280265808105 4.438659191131592
Loss :  1.6826648712158203 3.2699766159057617 4.952641487121582
Loss :  1.6674057245254517 3.3006227016448975 4.968028545379639
Loss :  1.6706725358963013 3.3812177181243896 5.0518903732299805
Loss :  1.6349377632141113 2.699936628341675 4.334874153137207
Loss :  1.6846294403076172 3.1007401943206787 4.785369873046875
Loss :  1.682213306427002 3.2914750576019287 4.973688125610352
Loss :  1.695617437362671 3.2327558994293213 4.928373336791992
Loss :  1.6709023714065552 3.052921772003174 4.7238240242004395
  batch 60 loss: 1.6709023714065552, 3.052921772003174, 4.7238240242004395
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726102828979492 3.445390462875366 5.1180009841918945
Loss :  1.6693735122680664 2.7365520000457764 4.405925750732422
Loss :  1.6774208545684814 3.3495419025421143 5.026962757110596
Loss :  1.6580156087875366 3.3359086513519287 4.993924140930176
Loss :  1.6550452709197998 3.2409470081329346 4.895992279052734
Loss :  5.482574462890625 4.415553569793701 9.898128509521484
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.372941970825195 4.40549373626709 9.778435707092285
Loss :  5.48142671585083 4.254281520843506 9.735708236694336
Loss :  4.561312198638916 4.259182929992676 8.82049560546875
Total LOSS train 4.730267799817598 valid 9.558192014694214
CE LOSS train 1.6640659900812003 valid 1.140328049659729
Contrastive LOSS train 3.0662018115703877 valid 1.064795732498169
EPOCH 295:
Loss :  1.6504157781600952 2.480968713760376 4.131384372711182
Loss :  1.6662572622299194 3.1841859817504883 4.850443363189697
Loss :  1.6520991325378418 3.3910133838653564 5.043112754821777
Loss :  1.6552520990371704 2.7227988243103027 4.378050804138184
Loss :  1.6799122095108032 3.3320436477661133 5.011955738067627
Loss :  1.6628683805465698 2.920398235321045 4.583266735076904
Loss :  1.6610596179962158 3.480891466140747 5.141951084136963
Loss :  1.6499199867248535 2.7086410522460938 4.358561038970947
Loss :  1.6545755863189697 2.257598400115967 3.9121739864349365
Loss :  1.6090422868728638 2.6523287296295166 4.26137113571167
Loss :  1.6686614751815796 3.6227426528930664 5.2914042472839355
Loss :  1.7292065620422363 2.9084861278533936 4.637692451477051
Loss :  1.6762146949768066 3.023314952850342 4.699529647827148
Loss :  1.664411187171936 2.9327006340026855 4.597111701965332
Loss :  1.642583966255188 3.1085870265960693 4.751171112060547
Loss :  1.652159571647644 3.056765556335449 4.708925247192383
Loss :  1.6611467599868774 2.8241326808929443 4.485279560089111
Loss :  1.6595245599746704 3.002448797225952 4.661973476409912
Loss :  1.6675561666488647 2.723483085632324 4.3910393714904785
Loss :  1.6240001916885376 3.2336297035217285 4.857629776000977
  batch 20 loss: 1.6240001916885376, 3.2336297035217285, 4.857629776000977
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.659276008605957 3.1495485305786133 4.80882453918457
Loss :  1.6750006675720215 2.777855634689331 4.452856063842773
Loss :  1.6463764905929565 3.4066548347473145 5.0530314445495605
Loss :  1.6873831748962402 2.814894676208496 4.502277851104736
Loss :  1.6848390102386475 3.525258779525757 5.210097789764404
Loss :  1.648421287536621 2.994946002960205 4.643367290496826
Loss :  1.697356104850769 3.15183687210083 4.849193096160889
Loss :  1.6398857831954956 3.1350491046905518 4.774934768676758
Loss :  1.6880669593811035 2.8868494033813477 4.574916362762451
Loss :  1.6413917541503906 2.9089505672454834 4.550342559814453
Loss :  1.722659945487976 3.3320460319519043 5.05470609664917
Loss :  1.6690646409988403 3.493865966796875 5.162930488586426
Loss :  1.654411792755127 2.9109108448028564 4.5653228759765625
Loss :  1.6568341255187988 2.7174394130706787 4.374273300170898
Loss :  1.6967459917068481 3.5758132934570312 5.27255916595459
Loss :  1.688863754272461 2.583615303039551 4.272479057312012
Loss :  1.6664973497390747 3.198899030685425 4.865396499633789
Loss :  1.6343332529067993 2.9863200187683105 4.62065315246582
Loss :  1.659674048423767 3.1253750324249268 4.785048961639404
Loss :  1.6521738767623901 3.4676084518432617 5.119782447814941
  batch 40 loss: 1.6521738767623901, 3.4676084518432617, 5.119782447814941
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602753400802612 3.3173980712890625 4.977673530578613
Loss :  1.6485244035720825 2.874223232269287 4.52274751663208
Loss :  1.6654633283615112 3.3518142700195312 5.017277717590332
Loss :  1.6637650728225708 2.665735960006714 4.329501152038574
Loss :  1.6666887998580933 2.5857326984405518 4.2524213790893555
Loss :  1.6581071615219116 3.1474356651306152 4.805542945861816
Loss :  1.6451464891433716 3.259124994277954 4.904271602630615
Loss :  1.6573092937469482 2.8944270610809326 4.551736354827881
Loss :  1.6301401853561401 3.4146509170532227 5.044791221618652
Loss :  1.6812691688537598 3.005826473236084 4.687095642089844
Loss :  1.6458383798599243 2.652853488922119 4.298691749572754
Loss :  1.6641314029693604 3.0844082832336426 4.748539924621582
Loss :  1.6826647520065308 3.26926326751709 4.95192813873291
Loss :  1.667407512664795 2.9911391735076904 4.658546447753906
Loss :  1.6706715822219849 3.4074933528900146 5.078165054321289
Loss :  1.6349364519119263 2.7990262508392334 4.433962821960449
Loss :  1.6846294403076172 3.4409849643707275 5.125614166259766
Loss :  1.6822139024734497 3.4694700241088867 5.151683807373047
Loss :  1.6956177949905396 3.319746971130371 5.015364646911621
Loss :  1.6709024906158447 2.758373498916626 4.429275989532471
  batch 60 loss: 1.6709024906158447, 2.758373498916626, 4.429275989532471
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726102828979492 3.3053665161132812 4.9779767990112305
Loss :  1.6693741083145142 2.5252881050109863 4.194662094116211
Loss :  1.6774215698242188 3.104001045227051 4.7814226150512695
Loss :  1.6580162048339844 2.9386074542999268 4.596623420715332
Loss :  1.6550458669662476 2.9855215549468994 4.640567302703857
Loss :  5.482445240020752 4.360382080078125 9.842826843261719
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.372867107391357 4.391329765319824 9.764196395874023
Loss :  5.481298923492432 4.25455904006958 9.735857963562012
Loss :  4.561417102813721 4.187163829803467 8.748580932617188
Total LOSS train 4.714509314757127 valid 9.522865533828735
CE LOSS train 1.6640660689427302 valid 1.1403542757034302
Contrastive LOSS train 3.0504432421464185 valid 1.0467909574508667
EPOCH 296:
Loss :  1.650416612625122 2.9325740337371826 4.582990646362305
Loss :  1.6662579774856567 3.4389264583587646 5.105184555053711
Loss :  1.6520987749099731 3.0770928859710693 4.729191780090332
Loss :  1.6552515029907227 3.2128591537475586 4.868110656738281
Loss :  1.6799116134643555 3.2906250953674316 4.970536708831787
Loss :  1.6628687381744385 2.988665819168091 4.651534557342529
Loss :  1.661059021949768 3.1593780517578125 4.820436954498291
Loss :  1.6499204635620117 2.557387113571167 4.207307815551758
Loss :  1.654575228691101 2.3170294761657715 3.971604824066162
Loss :  1.6090421676635742 2.6606667041778564 4.269708633422852
Loss :  1.6686620712280273 3.4180655479431152 5.086727619171143
Loss :  1.7292062044143677 2.9552600383758545 4.684466361999512
Loss :  1.6762175559997559 3.0211565494537354 4.69737434387207
Loss :  1.6644114255905151 3.439929246902466 5.104340553283691
Loss :  1.642583966255188 3.0042576789855957 4.646841526031494
Loss :  1.65216064453125 3.2579827308654785 4.9101433753967285
Loss :  1.6611469984054565 2.872647285461426 4.533794403076172
Loss :  1.6595252752304077 3.0590226650238037 4.718547821044922
Loss :  1.667556881904602 2.852689504623413 4.520246505737305
Loss :  1.624000072479248 3.2195961475372314 4.843596458435059
  batch 20 loss: 1.624000072479248, 3.2195961475372314, 4.843596458435059
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592752933502197 3.054433584213257 4.713708877563477
Loss :  1.6750011444091797 2.64121413230896 4.316215515136719
Loss :  1.6463768482208252 3.1665198802948 4.812896728515625
Loss :  1.6873822212219238 2.9143311977386475 4.601713180541992
Loss :  1.6848384141921997 3.3700642585754395 5.05490255355835
Loss :  1.648421049118042 3.2061398029327393 4.854560852050781
Loss :  1.697356939315796 3.257481813430786 4.954838752746582
Loss :  1.6398859024047852 2.7571704387664795 4.397056579589844
Loss :  1.6880671977996826 2.883126974105835 4.571194171905518
Loss :  1.6413925886154175 2.9539883136749268 4.595380783081055
Loss :  1.7226594686508179 3.2601122856140137 4.982771873474121
Loss :  1.6690642833709717 3.0767388343811035 4.745802879333496
Loss :  1.6544116735458374 2.994636297225952 4.6490478515625
Loss :  1.6568336486816406 2.86023211479187 4.51706600189209
Loss :  1.6967467069625854 3.578094959259033 5.274841785430908
Loss :  1.6888631582260132 2.699802875518799 4.388666152954102
Loss :  1.6664974689483643 2.922651529312134 4.589148998260498
Loss :  1.6343332529067993 2.937035322189331 4.57136869430542
Loss :  1.6596742868423462 2.9860475063323975 4.645721912384033
Loss :  1.652173638343811 3.4025912284851074 5.054764747619629
  batch 40 loss: 1.652173638343811, 3.4025912284851074, 5.054764747619629
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602756977081299 3.357952117919922 5.018227577209473
Loss :  1.6485240459442139 2.7361912727355957 4.3847150802612305
Loss :  1.6654632091522217 2.795287847518921 4.460751056671143
Loss :  1.6637637615203857 3.190065622329712 4.853829383850098
Loss :  1.6666879653930664 2.8257546424865723 4.492442607879639
Loss :  1.658108115196228 2.8811421394348145 4.539250373840332
Loss :  1.6451451778411865 3.1610090732574463 4.806154251098633
Loss :  1.6573079824447632 2.9062793254852295 4.563587188720703
Loss :  1.6301406621932983 3.5019001960754395 5.132040977478027
Loss :  1.6812682151794434 3.234609365463257 4.915877342224121
Loss :  1.6458375453948975 2.750131368637085 4.395968914031982
Loss :  1.66413152217865 2.8842613697052 4.5483927726745605
Loss :  1.6826651096343994 3.132873773574829 4.8155388832092285
Loss :  1.667405366897583 3.0424091815948486 4.709814548492432
Loss :  1.6706715822219849 3.372087240219116 5.042758941650391
Loss :  1.6349364519119263 2.6777992248535156 4.312735557556152
Loss :  1.6846296787261963 3.305077314376831 4.989706993103027
Loss :  1.6822139024734497 3.225231409072876 4.907445430755615
Loss :  1.69561767578125 3.34470534324646 5.040323257446289
Loss :  1.670902132987976 2.8054070472717285 4.476309299468994
  batch 60 loss: 1.670902132987976, 2.8054070472717285, 4.476309299468994
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.672611117362976 3.308203935623169 4.9808149337768555
Loss :  1.6693732738494873 2.733797311782837 4.403170585632324
Loss :  1.6774216890335083 3.257882595062256 4.935304164886475
Loss :  1.6580157279968262 2.7404417991638184 4.3984575271606445
Loss :  1.6550567150115967 3.169950485229492 4.825007438659668
Loss :  5.482422351837158 4.409492015838623 9.891914367675781
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.3728437423706055 4.3859543800354 9.758798599243164
Loss :  5.481297016143799 4.253802299499512 9.735099792480469
Loss :  4.561202049255371 4.2164835929870605 8.777685165405273
Total LOSS train 4.710199693533091 valid 9.540874481201172
CE LOSS train 1.664066195487976 valid 1.1403005123138428
Contrastive LOSS train 3.046133485207191 valid 1.0541208982467651
EPOCH 297:
Loss :  1.6504148244857788 2.6076455116271973 4.258060455322266
Loss :  1.6662575006484985 3.370246171951294 5.036503791809082
Loss :  1.6520987749099731 3.2589733600616455 4.911072254180908
Loss :  1.6552507877349854 2.6238362789154053 4.279087066650391
Loss :  1.6799119710922241 3.040698528289795 4.720610618591309
Loss :  1.6628680229187012 2.7853400707244873 4.448207855224609
Loss :  1.6610597372055054 3.4576222896575928 5.118681907653809
Loss :  1.6499202251434326 2.3590617179870605 4.008981704711914
Loss :  1.6545747518539429 2.3837404251098633 4.038315296173096
Loss :  1.6090426445007324 2.4579837322235107 4.067026138305664
Loss :  1.6686618328094482 3.440347909927368 5.109009742736816
Loss :  1.7292042970657349 3.076474905014038 4.8056793212890625
Loss :  1.6762173175811768 3.3424737453460693 5.018691062927246
Loss :  1.6644115447998047 3.1316184997558594 4.796030044555664
Loss :  1.6425832509994507 3.067087411880493 4.709670543670654
Loss :  1.6521599292755127 3.076105833053589 4.728265762329102
Loss :  1.6611460447311401 2.9822030067443848 4.6433491706848145
Loss :  1.6595244407653809 2.7914352416992188 4.4509596824646
Loss :  1.6675561666488647 2.8970630168914795 4.564619064331055
Loss :  1.623998999595642 2.9386489391326904 4.562647819519043
  batch 20 loss: 1.623998999595642, 2.9386489391326904, 4.562647819519043
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592761278152466 3.1943018436431885 4.853578090667725
Loss :  1.6750000715255737 2.587531566619873 4.262531757354736
Loss :  1.6463772058486938 3.3525688648223877 4.998946189880371
Loss :  1.6873818635940552 2.851592779159546 4.538974761962891
Loss :  1.6848384141921997 3.6446874141693115 5.329525947570801
Loss :  1.648420810699463 3.2834038734436035 4.931824684143066
Loss :  1.697357177734375 3.2284443378448486 4.9258012771606445
Loss :  1.6398855447769165 2.9746451377868652 4.614530563354492
Loss :  1.6880669593811035 2.9974663257598877 4.68553352355957
Loss :  1.6413923501968384 2.98785662651062 4.629249095916748
Loss :  1.7226594686508179 3.340209484100342 5.062869071960449
Loss :  1.6690641641616821 3.2050161361694336 4.874080181121826
Loss :  1.654410719871521 2.916731357574463 4.571142196655273
Loss :  1.656833529472351 2.8900468349456787 4.54688024520874
Loss :  1.6967456340789795 3.637341022491455 5.3340864181518555
Loss :  1.6888635158538818 2.5476105213165283 4.23647403717041
Loss :  1.6664972305297852 2.9408493041992188 4.607346534729004
Loss :  1.6343330144882202 2.917170524597168 4.551503658294678
Loss :  1.6596739292144775 2.9967639446258545 4.656437873840332
Loss :  1.6521732807159424 3.323758363723755 4.975931644439697
  batch 40 loss: 1.6521732807159424, 3.323758363723755, 4.975931644439697
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602754592895508 3.125652313232422 4.785927772521973
Loss :  1.6485238075256348 2.6879796981811523 4.336503505706787
Loss :  1.6654629707336426 2.7538344860076904 4.419297218322754
Loss :  1.663763165473938 2.749218225479126 4.4129815101623535
Loss :  1.6666876077651978 2.60562801361084 4.272315502166748
Loss :  1.658107876777649 3.101224660873413 4.759332656860352
Loss :  1.645145058631897 3.2892611026763916 4.934406280517578
Loss :  1.6573079824447632 2.5032756328582764 4.16058349609375
Loss :  1.630139946937561 3.5857839584350586 5.21592378616333
Loss :  1.6812682151794434 3.191581964492798 4.87285041809082
Loss :  1.6458382606506348 2.7395455837249756 4.385383605957031
Loss :  1.6641297340393066 2.9309616088867188 4.595091342926025
Loss :  1.6826648712158203 3.197350263595581 4.8800153732299805
Loss :  1.6674048900604248 3.073803186416626 4.741208076477051
Loss :  1.670670986175537 3.2860281467437744 4.956699371337891
Loss :  1.634935975074768 2.78397536277771 4.418911457061768
Loss :  1.6846297979354858 3.204674243927002 4.889304161071777
Loss :  1.6822129487991333 3.442018747329712 5.124231815338135
Loss :  1.6956180334091187 3.496737480163574 5.192355632781982
Loss :  1.6709022521972656 2.703857421875 4.374759674072266
  batch 60 loss: 1.6709022521972656, 2.703857421875, 4.374759674072266
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726106405258179 3.323167324066162 4.9957780838012695
Loss :  1.6693729162216187 2.7351737022399902 4.404546737670898
Loss :  1.677420973777771 3.1498947143554688 4.827315807342529
Loss :  1.658016562461853 2.836127519607544 4.494143962860107
Loss :  1.6550471782684326 3.053097724914551 4.7081451416015625
Loss :  5.482131481170654 4.355001449584961 9.837133407592773
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.372569561004639 4.3523664474487305 9.724935531616211
Loss :  5.481003761291504 4.208349704742432 9.689353942871094
Loss :  4.560998439788818 4.15435266494751 8.715351104736328
Total LOSS train 4.686472760714017 valid 9.491693496704102
CE LOSS train 1.6640657259867742 valid 1.1402496099472046
Contrastive LOSS train 3.0224070145533637 valid 1.0385881662368774
EPOCH 298:
Loss :  1.6504156589508057 2.8103671073913574 4.460783004760742
Loss :  1.6662577390670776 3.200753927230835 4.867011547088623
Loss :  1.652098536491394 3.0119545459747314 4.664052963256836
Loss :  1.6552505493164062 2.9921655654907227 4.647416114807129
Loss :  1.6799119710922241 3.304516077041626 4.9844279289245605
Loss :  1.6628676652908325 2.9223365783691406 4.585204124450684
Loss :  1.6610603332519531 3.259868860244751 4.920928955078125
Loss :  1.6499205827713013 2.372340679168701 4.022261142730713
Loss :  1.6545751094818115 2.2487881183624268 3.9033632278442383
Loss :  1.6090425252914429 2.696192502975464 4.305234909057617
Loss :  1.66866135597229 3.4680721759796143 5.136733531951904
Loss :  1.729205846786499 2.89184832572937 4.621054172515869
Loss :  1.6762162446975708 3.2190144062042236 4.895230770111084
Loss :  1.664412021636963 3.398108720779419 5.062520980834961
Loss :  1.6425843238830566 3.1699395179748535 4.81252384185791
Loss :  1.6521601676940918 3.228010654449463 4.880170822143555
Loss :  1.6611464023590088 2.992387294769287 4.653533935546875
Loss :  1.6595242023468018 3.140791893005371 4.800315856933594
Loss :  1.6675552129745483 2.7052102088928223 4.37276554107666
Loss :  1.6239994764328003 2.9787588119506836 4.602758407592773
  batch 20 loss: 1.6239994764328003, 2.9787588119506836, 4.602758407592773
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.659275770187378 2.892151355743408 4.551426887512207
Loss :  1.6749999523162842 2.7310702800750732 4.406070232391357
Loss :  1.6463768482208252 3.2399439811706543 4.886321067810059
Loss :  1.6873815059661865 3.0042200088500977 4.691601753234863
Loss :  1.6848382949829102 3.445110559463501 5.129948616027832
Loss :  1.648420810699463 3.027360677719116 4.67578125
Loss :  1.6973567008972168 3.118081569671631 4.815438270568848
Loss :  1.6398860216140747 3.2518439292907715 4.891729831695557
Loss :  1.688067078590393 3.1642844676971436 4.852351665496826
Loss :  1.641392707824707 2.804408550262451 4.445801258087158
Loss :  1.7226592302322388 3.322598695755005 5.045258045196533
Loss :  1.669063925743103 3.154569387435913 4.823633193969727
Loss :  1.6544113159179688 2.9468252658843994 4.601236343383789
Loss :  1.6568338871002197 3.0356671810150146 4.692501068115234
Loss :  1.6967459917068481 3.5806961059570312 5.27744197845459
Loss :  1.6888633966445923 2.667762517929077 4.356626033782959
Loss :  1.666496753692627 3.1893582344055176 4.8558549880981445
Loss :  1.6343337297439575 2.941873788833618 4.576207637786865
Loss :  1.6596742868423462 3.322489023208618 4.982163429260254
Loss :  1.6521737575531006 3.1190860271453857 4.771259784698486
  batch 40 loss: 1.6521737575531006, 3.1190860271453857, 4.771259784698486
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602752208709717 3.3037612438201904 4.964036464691162
Loss :  1.6485236883163452 2.681593894958496 4.330117702484131
Loss :  1.665462851524353 2.8060014247894287 4.471464157104492
Loss :  1.6637639999389648 2.9442789554595947 4.6080427169799805
Loss :  1.666686773300171 2.4366953372955322 4.103382110595703
Loss :  1.6581075191497803 2.9909119606018066 4.649019241333008
Loss :  1.645146369934082 3.289742946624756 4.934889316558838
Loss :  1.6573078632354736 2.7069480419158936 4.364255905151367
Loss :  1.6301405429840088 3.3235573768615723 4.95369815826416
Loss :  1.6812678575515747 3.1274220943450928 4.808690071105957
Loss :  1.6458381414413452 2.661308526992798 4.3071465492248535
Loss :  1.6641308069229126 3.0216400623321533 4.6857709884643555
Loss :  1.6826655864715576 3.342362403869629 5.025028228759766
Loss :  1.667405128479004 3.1411523818969727 4.808557510375977
Loss :  1.670670986175537 3.315870523452759 4.986541748046875
Loss :  1.634937047958374 2.7887258529663086 4.423663139343262
Loss :  1.6846295595169067 3.198972463607788 4.883602142333984
Loss :  1.6822128295898438 3.2415122985839844 4.923725128173828
Loss :  1.6956182718276978 3.5038673877716064 5.199485778808594
Loss :  1.6709023714065552 2.5881035327911377 4.259006023406982
  batch 60 loss: 1.6709023714065552, 2.5881035327911377, 4.259006023406982
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726106405258179 3.2634735107421875 4.936084270477295
Loss :  1.669372797012329 2.5993268489837646 4.268699645996094
Loss :  1.6774200201034546 3.009469747543335 4.6868896484375
Loss :  1.6580164432525635 2.949646472930908 4.607663154602051
Loss :  1.6550503969192505 2.8827903270721436 4.537840843200684
Loss :  5.482703685760498 4.4034013748168945 9.886104583740234
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.373165607452393 4.330313205718994 9.703478813171387
Loss :  5.481605052947998 4.236477851867676 9.718082427978516
Loss :  4.561369895935059 4.180233478546143 8.74160385131836
Total LOSS train 4.696219165508563 valid 9.512317419052124
CE LOSS train 1.6640658708719107 valid 1.1403424739837646
Contrastive LOSS train 3.0321532799647404 valid 1.0450583696365356
EPOCH 299:
Loss :  1.6504157781600952 2.6508638858795166 4.301279544830322
Loss :  1.6662571430206299 3.4415502548217773 5.107807159423828
Loss :  1.6520986557006836 3.208184242248535 4.860282897949219
Loss :  1.655251383781433 2.856602907180786 4.51185417175293
Loss :  1.679911494255066 3.497418165206909 5.1773295402526855
Loss :  1.6628679037094116 2.9252967834472656 4.588164806365967
Loss :  1.661059856414795 3.3045904636383057 4.96565055847168
Loss :  1.6499195098876953 2.7115373611450195 4.361456871032715
Loss :  1.6545748710632324 2.1780688762664795 3.832643747329712
Loss :  1.6090426445007324 2.6319527626037598 4.240995407104492
Loss :  1.668661117553711 3.4375810623168945 5.1062421798706055
Loss :  1.7292051315307617 3.0051629543304443 4.734368324279785
Loss :  1.6762161254882812 3.0762085914611816 4.752424716949463
Loss :  1.6644110679626465 3.152381658554077 4.8167924880981445
Loss :  1.6425840854644775 3.105773448944092 4.748357772827148
Loss :  1.6521596908569336 3.11570143699646 4.767861366271973
Loss :  1.6611462831497192 2.8430848121643066 4.504230976104736
Loss :  1.6595244407653809 3.0557539463043213 4.715278625488281
Loss :  1.6675559282302856 2.7473864555358887 4.414942264556885
Loss :  1.623998999595642 2.9656035900115967 4.589602470397949
  batch 20 loss: 1.623998999595642, 2.9656035900115967, 4.589602470397949
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592750549316406 3.060476541519165 4.719751358032227
Loss :  1.674999713897705 2.4712584018707275 4.146258354187012
Loss :  1.6463764905929565 3.3557586669921875 5.002135276794434
Loss :  1.6873810291290283 3.1623170375823975 4.849698066711426
Loss :  1.6848371028900146 3.32283878326416 5.007676124572754
Loss :  1.648420810699463 3.143265962600708 4.79168701171875
Loss :  1.6973567008972168 3.0852856636047363 4.782642364501953
Loss :  1.6398863792419434 3.082728624343872 4.7226152420043945
Loss :  1.6880676746368408 2.628676652908325 4.316744327545166
Loss :  1.641391396522522 3.130340337753296 4.771731853485107
Loss :  1.7226581573486328 3.3275551795959473 5.05021333694458
Loss :  1.6690645217895508 3.3707821369171143 5.039846420288086
Loss :  1.6544116735458374 3.053698778152466 4.708110332489014
Loss :  1.6568337678909302 2.6811139583587646 4.337947845458984
Loss :  1.696745753288269 3.385084867477417 5.0818305015563965
Loss :  1.688863754272461 2.6798923015594482 4.368756294250488
Loss :  1.666496753692627 3.076127529144287 4.742624282836914
Loss :  1.634334683418274 2.822108030319214 4.456442832946777
Loss :  1.6596741676330566 3.0539393424987793 4.713613510131836
Loss :  1.6521735191345215 3.1252832412719727 4.777456760406494
  batch 40 loss: 1.6521735191345215, 3.1252832412719727, 4.777456760406494
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602747440338135 3.307408571243286 4.9676833152771
Loss :  1.6485234498977661 2.7956387996673584 4.444162368774414
Loss :  1.6654632091522217 3.045619487762451 4.711082458496094
Loss :  1.6637630462646484 3.035104513168335 4.6988677978515625
Loss :  1.66668701171875 2.7322537899017334 4.3989410400390625
Loss :  1.6581066846847534 3.126471996307373 4.784578800201416
Loss :  1.645147442817688 3.4552502632141113 5.10039758682251
Loss :  1.657307505607605 2.8548331260681152 4.51214075088501
Loss :  1.630140781402588 3.368063449859619 4.998204231262207
Loss :  1.681268334388733 3.1172194480895996 4.798487663269043
Loss :  1.6458375453948975 2.8874783515930176 4.533315658569336
Loss :  1.6641308069229126 2.746577501296997 4.410708427429199
Loss :  1.6826648712158203 3.7467944622039795 5.429459571838379
Loss :  1.667405128479004 3.0753164291381836 4.7427215576171875
Loss :  1.6706708669662476 3.3854620456695557 5.056132793426514
Loss :  1.634936809539795 2.7736265659332275 4.408563613891602
Loss :  1.684628963470459 3.5339431762695312 5.21857213973999
Loss :  1.6822137832641602 3.2146122455596924 4.896825790405273
Loss :  1.6956171989440918 3.2006611824035645 4.896278381347656
Loss :  1.6709020137786865 2.724591016769409 4.395493030548096
  batch 60 loss: 1.6709020137786865, 2.724591016769409, 4.395493030548096
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726106405258179 3.112466335296631 4.785077095031738
Loss :  1.669372797012329 2.677659273147583 4.347032070159912
Loss :  1.6774201393127441 3.1709177494049072 4.8483381271362305
Loss :  1.6580159664154053 2.7519125938415527 4.409928321838379
Loss :  1.655050277709961 2.825432777404785 4.480483055114746
Loss :  5.48275089263916 4.404776096343994 9.887527465820312
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.37319278717041 4.334733009338379 9.707925796508789
Loss :  5.48161506652832 4.232144832611084 9.713760375976562
Loss :  4.561460018157959 4.309851169586182 8.87131118774414
Total LOSS train 4.703981902049138 valid 9.545131206512451
CE LOSS train 1.6640657113148616 valid 1.1403650045394897
Contrastive LOSS train 3.0399161668924184 valid 1.0774627923965454
EPOCH 300:
Loss :  1.6504161357879639 2.9463021755218506 4.5967183113098145
Loss :  1.6662571430206299 3.5538065433502197 5.22006368637085
Loss :  1.6520979404449463 3.0206730365753174 4.672770977020264
Loss :  1.6552516222000122 3.58154559135437 5.236797332763672
Loss :  1.679911732673645 3.168738842010498 4.8486504554748535
Loss :  1.6628684997558594 3.021515369415283 4.684383869171143
Loss :  1.661059856414795 3.094916820526123 4.755976676940918
Loss :  1.6499196290969849 2.6021814346313477 4.252100944519043
Loss :  1.6545754671096802 2.3966381549835205 4.05121374130249
Loss :  1.609041690826416 2.6914281845092773 4.300469875335693
Loss :  1.6686612367630005 3.5849952697753906 5.253656387329102
Loss :  1.7292070388793945 2.957557201385498 4.686764240264893
Loss :  1.6762155294418335 3.125056505203247 4.801271915435791
Loss :  1.6644110679626465 3.1847989559173584 4.849209785461426
Loss :  1.6425851583480835 3.24253249168396 4.885117530822754
Loss :  1.6521600484848022 3.2770347595214844 4.929194927215576
Loss :  1.6611465215682983 2.805802345275879 4.466948986053467
Loss :  1.6595242023468018 2.9371731281280518 4.5966973304748535
Loss :  1.6675554513931274 2.8793833255767822 4.546938896179199
Loss :  1.624000072479248 3.004241704940796 4.628241539001465
  batch 20 loss: 1.624000072479248, 3.004241704940796, 4.628241539001465
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6592758893966675 3.1535654067993164 4.812841415405273
Loss :  1.6749998331069946 2.6305642127990723 4.305563926696777
Loss :  1.646376132965088 3.298259973526001 4.944636344909668
Loss :  1.6873829364776611 2.70645809173584 4.393840789794922
Loss :  1.6848384141921997 3.5174033641815186 5.202241897583008
Loss :  1.6484206914901733 3.0280234813690186 4.676444053649902
Loss :  1.6973565816879272 2.985610008239746 4.682966709136963
Loss :  1.6398853063583374 2.790320873260498 4.430206298828125
Loss :  1.6880669593811035 2.7131755352020264 4.401242256164551
Loss :  1.6413936614990234 2.9683125019073486 4.609705924987793
Loss :  1.722658634185791 3.5794994831085205 5.302158355712891
Loss :  1.6690640449523926 3.5027427673339844 5.171806812286377
Loss :  1.6544109582901 2.957254648208618 4.611665725708008
Loss :  1.6568336486816406 3.057067394256592 4.713901042938232
Loss :  1.696744680404663 3.5776355266571045 5.274380207061768
Loss :  1.6888624429702759 2.668267250061035 4.3571295738220215
Loss :  1.6664963960647583 2.9753880500793457 4.6418843269348145
Loss :  1.6343340873718262 2.852048397064209 4.486382484436035
Loss :  1.6596742868423462 3.046922445297241 4.706596851348877
Loss :  1.6521728038787842 3.3573575019836426 5.009530067443848
  batch 40 loss: 1.6521728038787842, 3.3573575019836426, 5.009530067443848
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6602745056152344 3.2735702991485596 4.933844566345215
Loss :  1.6485233306884766 2.933438539505005 4.581961631774902
Loss :  1.6654633283615112 2.966904878616333 4.632368087768555
Loss :  1.663762092590332 2.94631290435791 4.610074996948242
Loss :  1.666688084602356 2.8493762016296387 4.516064167022705
Loss :  1.6581072807312012 3.085843086242676 4.743950366973877
Loss :  1.6451464891433716 3.3579747676849365 5.003121376037598
Loss :  1.6573090553283691 2.5868752002716064 4.244184494018555
Loss :  1.6301400661468506 3.1499264240264893 4.78006649017334
Loss :  1.6812684535980225 3.470644950866699 5.151913642883301
Loss :  1.6458383798599243 2.5854177474975586 4.231256008148193
Loss :  1.664129614830017 3.031071662902832 4.695201396942139
Loss :  1.6826645135879517 3.266170024871826 4.948834419250488
Loss :  1.6674047708511353 2.9560821056365967 4.6234869956970215
Loss :  1.6706711053848267 3.2317636013031006 4.902434825897217
Loss :  1.6349356174468994 2.7828357219696045 4.417771339416504
Loss :  1.684630036354065 3.259044647216797 4.943674564361572
Loss :  1.6822142601013184 3.2746005058288574 4.956814765930176
Loss :  1.695617437362671 3.130812883377075 4.826430320739746
Loss :  1.6709020137786865 2.818772077560425 4.489674091339111
  batch 60 loss: 1.6709020137786865, 2.818772077560425, 4.489674091339111
  torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
Loss :  1.6726112365722656 3.2804505825042725 4.953062057495117
Loss :  1.6693733930587769 2.8410425186157227 4.510416030883789
Loss :  1.6774204969406128 3.159604549407959 4.837025165557861
Loss :  1.6580157279968262 2.9369587898254395 4.594974517822266
Loss :  1.655044436454773 3.1512656211853027 4.806310176849365
Loss :  5.482590675354004 4.3442063331604 9.826797485351562
  val done :  0
torch.unique(gt) :  tensor([0, 1, 2, 3, 4, 5], device='cuda:0')
  pred unique :  tensor([0, 1, 2, 5], device='cuda:0')
Loss :  5.372991561889648 4.452596664428711 9.82558822631836
Loss :  5.481456756591797 4.205883979797363 9.68734073638916
Loss :  4.561258792877197 4.285147190093994 8.846405982971191
Total LOSS train 4.722049676454985 valid 9.546533107757568
CE LOSS train 1.66406569480896 valid 1.1403146982192993
Contrastive LOSS train 3.0579839853140025 valid 1.0712867975234985
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 1.536 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: / 0.056 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: - 0.103 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: \ 0.103 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: | 0.157 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: / 0.157 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: - 0.212 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: \ 0.259 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: | 0.259 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: / 0.259 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: - 0.306 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: \ 0.353 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: | 0.400 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: / 0.454 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: - 0.454 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: \ 0.501 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: | 0.548 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: / 0.634 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: - 0.673 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: \ 0.712 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: | 0.759 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: / 0.806 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: - 0.845 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: \ 0.884 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: | 0.931 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: / 0.931 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: - 1.025 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: \ 1.056 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: | 1.103 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: / 1.150 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: - 1.196 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: \ 1.298 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: | 1.337 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: / 1.431 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: - 1.431 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: \ 1.525 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: | 1.537 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: / 1.537 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: - 1.537 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: \ 1.537 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: | 1.537 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: / 1.537 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: - 1.537 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: \ 1.537 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: | 1.537 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: / 1.537 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: - 1.537 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: \ 1.537 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: | 1.537 MB of 1.537 MB uploaded (0.000 MB deduped)wandb: / 1.537 MB of 1.537 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                    epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:                       lr ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   train contrastive loss ‚ñà‚ñá‚ñá‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb: train cross-entropy loss ‚ñà‚ñá‚ñÖ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:               train loss ‚ñà‚ñá‚ñÜ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:     val contrastive loss ‚ñÑ‚ñÑ‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÜ‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñà‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÜ‚ñÉ‚ñÖ‚ñÖ‚ñÜ
wandb:   val cross-entropy loss ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                 val loss ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:            best_val_loss 5.66073
wandb:                    epoch 300
wandb:                       lr 0.0
wandb:   train contrastive loss 3.05798
wandb: train cross-entropy loss 1.66407
wandb:               train loss 4.72205
wandb:     val contrastive loss 1.07129
wandb:   val cross-entropy loss 1.14031
wandb:                 val loss 9.54653
wandb: 
wandb: Synced bright-lamp-25: https://wandb.ai/harsh21122/part_segmentation/runs/29kcf1mc
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230205_144935-29kcf1mc/logs
